[
  {
    "discussion_id": "2024127496",
    "pr_number": 5492,
    "pr_file": "packages/ai/core/generate-text/stream-text.test.ts",
    "created_at": "2025-04-02T06:06:10+00:00",
    "commented_code": "});\n   });\n \n+  describe('result.consumeStream', () => {\n+    it('should ignore AbortError during stream consumption', async () => {\n+      const result = streamText({\n+        model: createTestModel({\n+          stream: new ReadableStream({\n+            start(controller) {\n+              controller.enqueue({ type: 'text-delta', textDelta: 'Hello' });\n+              queueMicrotask(() => {\n+                controller.error(\n+                  Object.assign(new Error('Stream aborted'), {\n+                    name: 'AbortError',\n+                  }),\n+                );\n+              });\n+            },\n+          }),\n+        }),\n+        prompt: 'test-input',\n+      });\n+\n+      await expect(result.consumeStream()).resolves.not.toThrow();\n+    });\n+\n+    it('should ignore ResponseAborted error during stream consumption', async () => {\n+      const result = streamText({\n+        model: createTestModel({\n+          stream: new ReadableStream({\n+            start(controller) {\n+              controller.enqueue({ type: 'text-delta', textDelta: 'Hello' });\n+              queueMicrotask(() => {\n+                controller.error(\n+                  Object.assign(new Error('Response aborted'), {\n+                    name: 'ResponseAborted',\n+                  }),\n+                );\n+              });\n+            },\n+          }),\n+        }),\n+        prompt: 'test-input',\n+      });\n+\n+      await expect(result.consumeStream()).resolves.not.toThrow();\n+    });\n+\n+    it('should propagate other errors during stream consumption', async () => {",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2024127496",
        "repo_full_name": "vercel/ai",
        "pr_number": 5492,
        "pr_file": "packages/ai/core/generate-text/stream-text.test.ts",
        "discussion_id": "2024127496",
        "commented_code": "@@ -1542,6 +1542,70 @@ describe('streamText', () => {\n     });\n   });\n \n+  describe('result.consumeStream', () => {\n+    it('should ignore AbortError during stream consumption', async () => {\n+      const result = streamText({\n+        model: createTestModel({\n+          stream: new ReadableStream({\n+            start(controller) {\n+              controller.enqueue({ type: 'text-delta', textDelta: 'Hello' });\n+              queueMicrotask(() => {\n+                controller.error(\n+                  Object.assign(new Error('Stream aborted'), {\n+                    name: 'AbortError',\n+                  }),\n+                );\n+              });\n+            },\n+          }),\n+        }),\n+        prompt: 'test-input',\n+      });\n+\n+      await expect(result.consumeStream()).resolves.not.toThrow();\n+    });\n+\n+    it('should ignore ResponseAborted error during stream consumption', async () => {\n+      const result = streamText({\n+        model: createTestModel({\n+          stream: new ReadableStream({\n+            start(controller) {\n+              controller.enqueue({ type: 'text-delta', textDelta: 'Hello' });\n+              queueMicrotask(() => {\n+                controller.error(\n+                  Object.assign(new Error('Response aborted'), {\n+                    name: 'ResponseAborted',\n+                  }),\n+                );\n+              });\n+            },\n+          }),\n+        }),\n+        prompt: 'test-input',\n+      });\n+\n+      await expect(result.consumeStream()).resolves.not.toThrow();\n+    });\n+\n+    it('should propagate other errors during stream consumption', async () => {",
        "comment_created_at": "2025-04-02T06:06:10+00:00",
        "comment_author": "lgrammel",
        "comment_body": "Wondering if that's the desired behavior - might be good to think through use cases",
        "pr_file_module": null
      },
      {
        "comment_id": "2025529661",
        "repo_full_name": "vercel/ai",
        "pr_number": 5492,
        "pr_file": "packages/ai/core/generate-text/stream-text.test.ts",
        "discussion_id": "2024127496",
        "commented_code": "@@ -1542,6 +1542,70 @@ describe('streamText', () => {\n     });\n   });\n \n+  describe('result.consumeStream', () => {\n+    it('should ignore AbortError during stream consumption', async () => {\n+      const result = streamText({\n+        model: createTestModel({\n+          stream: new ReadableStream({\n+            start(controller) {\n+              controller.enqueue({ type: 'text-delta', textDelta: 'Hello' });\n+              queueMicrotask(() => {\n+                controller.error(\n+                  Object.assign(new Error('Stream aborted'), {\n+                    name: 'AbortError',\n+                  }),\n+                );\n+              });\n+            },\n+          }),\n+        }),\n+        prompt: 'test-input',\n+      });\n+\n+      await expect(result.consumeStream()).resolves.not.toThrow();\n+    });\n+\n+    it('should ignore ResponseAborted error during stream consumption', async () => {\n+      const result = streamText({\n+        model: createTestModel({\n+          stream: new ReadableStream({\n+            start(controller) {\n+              controller.enqueue({ type: 'text-delta', textDelta: 'Hello' });\n+              queueMicrotask(() => {\n+                controller.error(\n+                  Object.assign(new Error('Response aborted'), {\n+                    name: 'ResponseAborted',\n+                  }),\n+                );\n+              });\n+            },\n+          }),\n+        }),\n+        prompt: 'test-input',\n+      });\n+\n+      await expect(result.consumeStream()).resolves.not.toThrow();\n+    });\n+\n+    it('should propagate other errors during stream consumption', async () => {",
        "comment_created_at": "2025-04-02T20:20:29+00:00",
        "comment_author": "iteratetograceness",
        "comment_body": "esp. given the network error finding - i like the idea of swallowing all errors and exposing an onError callback option!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2025699445",
    "pr_number": 5492,
    "pr_file": "examples/next-openai/app/api/use-chat-resilient-persistence/route.ts",
    "created_at": "2025-04-02T22:43:06+00:00",
    "commented_code": "// consume the stream to ensure it runs to completion and triggers onFinish\n   // even when the client response is aborted (e.g. when the browser tab is closed).\n-  result.consumeStream(); // no await\n+  // no await\n+  result.consumeStream(error => {\n+    console.log('Error during background stream consumption: ', error); // optional error callback\n+  });",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2025699445",
        "repo_full_name": "vercel/ai",
        "pr_number": 5492,
        "pr_file": "examples/next-openai/app/api/use-chat-resilient-persistence/route.ts",
        "discussion_id": "2025699445",
        "commented_code": "@@ -26,7 +26,10 @@ export async function POST(req: Request) {\n \n   // consume the stream to ensure it runs to completion and triggers onFinish\n   // even when the client response is aborted (e.g. when the browser tab is closed).\n-  result.consumeStream(); // no await\n+  // no await\n+  result.consumeStream(error => {\n+    console.log('Error during background stream consumption: ', error); // optional error callback\n+  });",
        "comment_created_at": "2025-04-02T22:43:06+00:00",
        "comment_author": "iteratetograceness",
        "comment_body": "Added to illustrated optional error callback",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2026249586",
    "pr_number": 5492,
    "pr_file": "packages/ai/core/generate-text/stream-text.ts",
    "created_at": "2025-04-03T05:48:42+00:00",
    "commented_code": ");\n   }\n \n-  async consumeStream(): Promise<void> {\n-    const stream = this.fullStream;\n-    for await (const part of stream) {\n-      // no op\n+  async consumeStream(onError?: (error: unknown) => void): Promise<void> {\n+    try {\n+      await consumeStream(this.fullStream);",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2026249586",
        "repo_full_name": "vercel/ai",
        "pr_number": 5492,
        "pr_file": "packages/ai/core/generate-text/stream-text.ts",
        "discussion_id": "2026249586",
        "commented_code": "@@ -1591,10 +1592,11 @@ However, the LLM results are expected to be small enough to not cause issues.\n     );\n   }\n \n-  async consumeStream(): Promise<void> {\n-    const stream = this.fullStream;\n-    for await (const part of stream) {\n-      // no op\n+  async consumeStream(onError?: (error: unknown) => void): Promise<void> {\n+    try {\n+      await consumeStream(this.fullStream);",
        "comment_created_at": "2025-04-03T05:48:42+00:00",
        "comment_author": "lgrammel",
        "comment_body": "wonder if it's worth pushing the onError object into `consumeStream` to keep the behavior between `consumeStream` here and the general helper aligned?",
        "pr_file_module": null
      },
      {
        "comment_id": "2026250146",
        "repo_full_name": "vercel/ai",
        "pr_number": 5492,
        "pr_file": "packages/ai/core/generate-text/stream-text.ts",
        "discussion_id": "2026249586",
        "commented_code": "@@ -1591,10 +1592,11 @@ However, the LLM results are expected to be small enough to not cause issues.\n     );\n   }\n \n-  async consumeStream(): Promise<void> {\n-    const stream = this.fullStream;\n-    for await (const part of stream) {\n-      // no op\n+  async consumeStream(onError?: (error: unknown) => void): Promise<void> {\n+    try {\n+      await consumeStream(this.fullStream);",
        "comment_created_at": "2025-04-03T05:49:23+00:00",
        "comment_author": "lgrammel",
        "comment_body": "the helper is afaik only used internally so we can change the main `stream` object to become a parameter object \r\n\r\n```\r\n{\r\n  stream,\r\n  onError?\r\n}\r\n```",
        "pr_file_module": null
      }
    ]
  }
]