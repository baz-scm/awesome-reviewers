[
  {
    "discussion_id": "2069812843",
    "pr_number": 6083,
    "pr_file": "packages/luma/src/luma-image-model.ts",
    "created_at": "2025-05-01T03:46:44+00:00",
    "commented_code": "});\n     }\n \n+    // remove non-request options from providerOptions\n+    const { pollIntervalMillis, maxPollAttempts, ...providerRequestOptions } =\n+      providerOptions.luma ?? {};",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2069812843",
        "repo_full_name": "vercel/ai",
        "pr_number": 6083,
        "pr_file": "packages/luma/src/luma-image-model.ts",
        "discussion_id": "2069812843",
        "commented_code": "@@ -86,6 +76,10 @@ export class LumaImageModel implements ImageModelV2 {\n       });\n     }\n \n+    // remove non-request options from providerOptions\n+    const { pollIntervalMillis, maxPollAttempts, ...providerRequestOptions } =\n+      providerOptions.luma ?? {};",
        "comment_created_at": "2025-05-01T03:46:44+00:00",
        "comment_author": "gr2m",
        "comment_body": "When providers want to support options that should not be passed through to the API calls then they have to manually extract them before sending the request. As an alternative we could use a special key like `providerOptions._.*` for options that should not be sent as part of the request body.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1849838028",
    "pr_number": 3781,
    "pr_file": "packages/openai-compatible/src/openai-compatible-error.ts",
    "created_at": "2024-11-20T08:30:04+00:00",
    "commented_code": "+import { z } from 'zod';\n+import { createJsonErrorResponseHandler } from '@ai-sdk/provider-utils';\n+\n+// TODO(shaper): Reconcile this with openai-error.ts. We derived from `xai`.\n+\n+export const openAICompatibleErrorDataSchema = z.object({\n+  code: z.string(),\n+  error: z.string(),\n+});\n+\n+export type OpenAICompatibleErrorData = z.infer<\n+  typeof openAICompatibleErrorDataSchema\n+>;\n+\n+export const openAICompatibleFailedResponseHandler =\n+  createJsonErrorResponseHandler({\n+    errorSchema: openAICompatibleErrorDataSchema,\n+    errorToMessage: data => data.error,\n+  });",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "1849838028",
        "repo_full_name": "vercel/ai",
        "pr_number": 3781,
        "pr_file": "packages/openai-compatible/src/openai-compatible-error.ts",
        "discussion_id": "1849838028",
        "commented_code": "@@ -0,0 +1,19 @@\n+import { z } from 'zod';\n+import { createJsonErrorResponseHandler } from '@ai-sdk/provider-utils';\n+\n+// TODO(shaper): Reconcile this with openai-error.ts. We derived from `xai`.\n+\n+export const openAICompatibleErrorDataSchema = z.object({\n+  code: z.string(),\n+  error: z.string(),\n+});\n+\n+export type OpenAICompatibleErrorData = z.infer<\n+  typeof openAICompatibleErrorDataSchema\n+>;\n+\n+export const openAICompatibleFailedResponseHandler =\n+  createJsonErrorResponseHandler({\n+    errorSchema: openAICompatibleErrorDataSchema,\n+    errorToMessage: data => data.error,\n+  });",
        "comment_created_at": "2024-11-20T08:30:04+00:00",
        "comment_author": "lgrammel",
        "comment_body": "make provider-specific. different providers have different structures. have default that matches openai",
        "pr_file_module": null
      },
      {
        "comment_id": "1851174798",
        "repo_full_name": "vercel/ai",
        "pr_number": 3781,
        "pr_file": "packages/openai-compatible/src/openai-compatible-error.ts",
        "discussion_id": "1849838028",
        "commented_code": "@@ -0,0 +1,19 @@\n+import { z } from 'zod';\n+import { createJsonErrorResponseHandler } from '@ai-sdk/provider-utils';\n+\n+// TODO(shaper): Reconcile this with openai-error.ts. We derived from `xai`.\n+\n+export const openAICompatibleErrorDataSchema = z.object({\n+  code: z.string(),\n+  error: z.string(),\n+});\n+\n+export type OpenAICompatibleErrorData = z.infer<\n+  typeof openAICompatibleErrorDataSchema\n+>;\n+\n+export const openAICompatibleFailedResponseHandler =\n+  createJsonErrorResponseHandler({\n+    errorSchema: openAICompatibleErrorDataSchema,\n+    errorToMessage: data => data.error,\n+  });",
        "comment_created_at": "2024-11-21T00:59:56+00:00",
        "comment_author": "shaper",
        "comment_body": "I can see a couple of different ways to do this. The error data schema is further embedded in the schemas used in the chat/completion model classes, so just making the failed response handler configurable isn't sufficient for the goal.\r\n\r\nI have in mind to break this PR into two so that we can start landing parts and work more incrementally:\r\n1. initial openai-compatible package work\r\n2. togetherai package atop it\r\n\r\nI expect we can land (1) tomorrow after I iterate on your next feedback. We can then put a focused change together to make the error schema/response handler configurable. And we can iterate on (2) until it supports the model/feature combos we feel is good enough for a first ship target.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2041062190",
    "pr_number": 5720,
    "pr_file": "packages/gladia/src/gladia-transcription-model.ts",
    "created_at": "2025-04-13T07:03:46+00:00",
    "commented_code": "+import {\n+  TranscriptionModelV1,\n+  TranscriptionModelV1CallWarning,\n+} from '@ai-sdk/provider';\n+import {\n+  combineHeaders,\n+  convertBase64ToUint8Array,\n+  createJsonResponseHandler,\n+  getFromApi,\n+  parseProviderOptions,\n+  postFormDataToApi,\n+  postJsonToApi,\n+} from '@ai-sdk/provider-utils';\n+import { z } from 'zod';\n+import { GladiaConfig } from './gladia-config';\n+import { gladiaFailedResponseHandler } from './gladia-error';\n+import { GladiaTranscriptionInitiateAPITypes } from './gladia-api-types';\n+\n+// https://docs.gladia.io/api-reference/v2/pre-recorded/init\n+const gladiaProviderOptionsSchema = z.object({\n+  /**\n+   * Optional context prompt to guide the transcription.\n+   */\n+  contextPrompt: z.string().nullish(),\n+\n+  /**\n+   * Custom vocabulary to improve transcription accuracy.\n+   * Can be a boolean or an array of custom terms.\n+   */\n+  customVocabulary: z.union([z.boolean(), z.array(z.any())]).nullish(),\n+\n+  /**\n+   * Configuration for custom vocabulary.\n+   */\n+  customVocabularyConfig: z\n+    .object({\n+      /**\n+       * Array of vocabulary terms or objects with pronunciation details.\n+       */\n+      vocabulary: z.array(\n+        z.union([\n+          z.string(),\n+          z.object({\n+            /**\n+             * The vocabulary term.\n+             */\n+            value: z.string(),\n+            /**\n+             * Intensity of the term in recognition (optional).\n+             */\n+            intensity: z.number().nullish(),\n+            /**\n+             * Alternative pronunciations for the term (optional).\n+             */\n+            pronunciations: z.array(z.string()).nullish(),\n+            /**\n+             * Language of the term (optional).\n+             */\n+            language: z.string().nullish(),\n+          }),\n+        ]),\n+      ),\n+      /**\n+       * Default intensity for all vocabulary terms.\n+       */\n+      defaultIntensity: z.number().nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to automatically detect the language of the audio.\n+   */\n+  detectLanguage: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable code switching (multiple languages in the same audio).\n+   */\n+  enableCodeSwitching: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for code switching.\n+   */\n+  codeSwitchingConfig: z\n+    .object({\n+      /**\n+       * Languages to consider for code switching.\n+       */\n+      languages: z.array(z.string()).nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Specific language for transcription.\n+   */\n+  language: z.string().nullish(),\n+\n+  /**\n+   * Whether to enable callback when transcription is complete.\n+   */\n+  callback: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for callback.\n+   */\n+  callbackConfig: z\n+    .object({\n+      /**\n+       * URL to send the callback to.\n+       */\n+      url: z.string(),\n+      /**\n+       * HTTP method for the callback.\n+       */\n+      method: z.enum(['POST', 'PUT']).nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to generate subtitles.\n+   */\n+  subtitles: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for subtitles generation.\n+   */\n+  subtitlesConfig: z\n+    .object({\n+      /**\n+       * Subtitle file formats to generate.\n+       */\n+      formats: z.array(z.enum(['srt', 'vtt'])).nullish(),\n+      /**\n+       * Minimum duration for subtitle segments.\n+       */\n+      minimumDuration: z.number().nullish(),\n+      /**\n+       * Maximum duration for subtitle segments.\n+       */\n+      maximumDuration: z.number().nullish(),\n+      /**\n+       * Maximum characters per row in subtitles.\n+       */\n+      maximumCharactersPerRow: z.number().nullish(),\n+      /**\n+       * Maximum rows per caption in subtitles.\n+       */\n+      maximumRowsPerCaption: z.number().nullish(),\n+      /**\n+       * Style of subtitles.\n+       */\n+      style: z.enum(['default', 'compliance']).nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to enable speaker diarization (speaker identification).\n+   */\n+  diarization: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for diarization.\n+   */\n+  diarizationConfig: z\n+    .object({\n+      /**\n+       * Exact number of speakers to identify.\n+       */\n+      numberOfSpeakers: z.number().nullish(),\n+      /**\n+       * Minimum number of speakers to identify.\n+       */\n+      minSpeakers: z.number().nullish(),\n+      /**\n+       * Maximum number of speakers to identify.\n+       */\n+      maxSpeakers: z.number().nullish(),\n+      /**\n+       * Whether to use enhanced diarization.\n+       */\n+      enhanced: z.boolean().nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to translate the transcription.\n+   */\n+  translation: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for translation.\n+   */\n+  translationConfig: z\n+    .object({\n+      /**\n+       * Target languages for translation.\n+       */\n+      targetLanguages: z.array(z.string()),\n+      /**\n+       * Translation model to use.\n+       */\n+      model: z.enum(['base', 'enhanced']).nullish(),\n+      /**\n+       * Whether to match original utterances in translation.\n+       */\n+      matchOriginalUtterances: z.boolean().nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to generate a summary of the transcription.\n+   */\n+  summarization: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for summarization.\n+   */\n+  summarizationConfig: z\n+    .object({\n+      /**\n+       * Type of summary to generate.\n+       */\n+      type: z.enum(['general', 'bullet_points', 'concise']).nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to enable content moderation.\n+   */\n+  moderation: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable named entity recognition.\n+   */\n+  namedEntityRecognition: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable automatic chapter creation.\n+   */\n+  chapterization: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to ensure consistent naming of entities.\n+   */\n+  nameConsistency: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable custom spelling.\n+   */\n+  customSpelling: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for custom spelling.\n+   */\n+  customSpellingConfig: z\n+    .object({\n+      /**\n+       * Dictionary of custom spellings.\n+       */\n+      spellingDictionary: z.record(z.array(z.string())),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to extract structured data from the transcription.\n+   */\n+  structuredDataExtraction: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for structured data extraction.\n+   */\n+  structuredDataExtractionConfig: z\n+    .object({\n+      /**\n+       * Classes of data to extract.\n+       */\n+      classes: z.array(z.string()),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to perform sentiment analysis on the transcription.\n+   */\n+  sentimentAnalysis: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to send audio to a language model for processing.\n+   */\n+  audioToLlm: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for audio to language model processing.\n+   */\n+  audioToLlmConfig: z\n+    .object({\n+      /**\n+       * Prompts to send to the language model.\n+       */\n+      prompts: z.array(z.string()),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Custom metadata to include with the transcription.\n+   */\n+  customMetadata: z.record(z.any()).nullish(),\n+\n+  /**\n+   * Whether to include sentence-level segmentation.\n+   */\n+  sentences: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable display mode.\n+   */\n+  displayMode: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enhance punctuation in the transcription.\n+   */\n+  punctuationEnhanced: z.boolean().nullish(),\n+});\n+\n+export type GladiaTranscriptionCallOptions = z.infer<\n+  typeof gladiaProviderOptionsSchema\n+>;\n+\n+interface GladiaTranscriptionModelConfig extends GladiaConfig {\n+  _internal?: {\n+    currentDate?: () => Date;\n+  };\n+}\n+\n+export class GladiaTranscriptionModel implements TranscriptionModelV1 {\n+  readonly specificationVersion = 'v1';\n+\n+  get provider(): string {\n+    return this.config.provider;\n+  }\n+\n+  constructor(\n+    readonly modelId: '',\n+    private readonly config: GladiaTranscriptionModelConfig,\n+  ) {}\n+\n+  private async getArgs({\n+    providerOptions,\n+  }: Parameters<TranscriptionModelV1['doGenerate']>[0]) {\n+    const warnings: TranscriptionModelV1CallWarning[] = [];\n+\n+    // Parse provider options\n+    const gladiaOptions = parseProviderOptions({\n+      provider: 'gladia',\n+      providerOptions,\n+      schema: gladiaProviderOptionsSchema,\n+    });\n+\n+    const body: Omit<GladiaTranscriptionInitiateAPITypes, 'audio_url'> = {};\n+\n+    // Add provider-specific options\n+    if (gladiaOptions) {\n+      body.context_prompt = gladiaOptions.contextPrompt ?? undefined;\n+      body.custom_vocabulary = gladiaOptions.customVocabulary ?? undefined;\n+      body.detect_language = gladiaOptions.detectLanguage ?? undefined;\n+      body.enable_code_switching =\n+        gladiaOptions.enableCodeSwitching ?? undefined;\n+      body.language = gladiaOptions.language ?? undefined;\n+      body.callback = gladiaOptions.callback ?? undefined;\n+      body.subtitles = gladiaOptions.subtitles ?? undefined;\n+      body.diarization = gladiaOptions.diarization ?? undefined;\n+      body.translation = gladiaOptions.translation ?? undefined;\n+      body.summarization = gladiaOptions.summarization ?? undefined;\n+      body.moderation = gladiaOptions.moderation ?? undefined;\n+      body.named_entity_recognition =\n+        gladiaOptions.namedEntityRecognition ?? undefined;\n+      body.chapterization = gladiaOptions.chapterization ?? undefined;\n+      body.name_consistency = gladiaOptions.nameConsistency ?? undefined;\n+      body.custom_spelling = gladiaOptions.customSpelling ?? undefined;\n+      body.structured_data_extraction =\n+        gladiaOptions.structuredDataExtraction ?? undefined;\n+      body.structured_data_extraction_config =\n+        gladiaOptions.structuredDataExtractionConfig ?? undefined;\n+      body.sentiment_analysis = gladiaOptions.sentimentAnalysis ?? undefined;\n+      body.audio_to_llm = gladiaOptions.audioToLlm ?? undefined;\n+      body.audio_to_llm_config = gladiaOptions.audioToLlmConfig ?? undefined;\n+      body.custom_metadata = gladiaOptions.customMetadata ?? undefined;\n+      body.sentences = gladiaOptions.sentences ?? undefined;\n+      body.display_mode = gladiaOptions.displayMode ?? undefined;\n+      body.punctuation_enhanced =\n+        gladiaOptions.punctuationEnhanced ?? undefined;\n+\n+      if (gladiaOptions.customVocabularyConfig) {\n+        body.custom_vocabulary_config = {\n+          vocabulary: gladiaOptions.customVocabularyConfig.vocabulary.map(\n+            item => {\n+              if (typeof item === 'string') return item;\n+              return {\n+                value: item.value,\n+                intensity: item.intensity ?? undefined,\n+                pronunciations: item.pronunciations ?? undefined,\n+                language: item.language ?? undefined,\n+              };\n+            },\n+          ),\n+          default_intensity:\n+            gladiaOptions.customVocabularyConfig.defaultIntensity ?? undefined,\n+        };\n+      }\n+\n+      // Handle code switching config\n+      if (gladiaOptions.codeSwitchingConfig) {\n+        body.code_switching_config = {\n+          languages: gladiaOptions.codeSwitchingConfig.languages ?? undefined,\n+        };\n+      }\n+\n+      // Handle callback config\n+      if (gladiaOptions.callbackConfig) {\n+        body.callback_config = {\n+          url: gladiaOptions.callbackConfig.url,\n+          method: gladiaOptions.callbackConfig.method ?? undefined,\n+        };\n+      }\n+\n+      // Handle subtitles config\n+      if (gladiaOptions.subtitlesConfig) {\n+        body.subtitles_config = {\n+          formats: gladiaOptions.subtitlesConfig.formats ?? undefined,\n+          minimum_duration:\n+            gladiaOptions.subtitlesConfig.minimumDuration ?? undefined,\n+          maximum_duration:\n+            gladiaOptions.subtitlesConfig.maximumDuration ?? undefined,\n+          maximum_characters_per_row:\n+            gladiaOptions.subtitlesConfig.maximumCharactersPerRow ?? undefined,\n+          maximum_rows_per_caption:\n+            gladiaOptions.subtitlesConfig.maximumRowsPerCaption ?? undefined,\n+          style: gladiaOptions.subtitlesConfig.style ?? undefined,\n+        };\n+      }\n+\n+      // Handle diarization config\n+      if (gladiaOptions.diarizationConfig) {\n+        body.diarization_config = {\n+          number_of_speakers:\n+            gladiaOptions.diarizationConfig.numberOfSpeakers ?? undefined,\n+          min_speakers:\n+            gladiaOptions.diarizationConfig.minSpeakers ?? undefined,\n+          max_speakers:\n+            gladiaOptions.diarizationConfig.maxSpeakers ?? undefined,\n+          enhanced: gladiaOptions.diarizationConfig.enhanced ?? undefined,\n+        };\n+      }\n+\n+      // Handle translation config\n+      if (gladiaOptions.translationConfig) {\n+        body.translation_config = {\n+          target_languages: gladiaOptions.translationConfig.targetLanguages,\n+          model: gladiaOptions.translationConfig.model ?? undefined,\n+          match_original_utterances:\n+            gladiaOptions.translationConfig.matchOriginalUtterances ??\n+            undefined,\n+        };\n+      }\n+\n+      // Handle summarization config\n+      if (gladiaOptions.summarizationConfig) {\n+        body.summarization_config = {\n+          type: gladiaOptions.summarizationConfig.type ?? undefined,\n+        };\n+      }\n+\n+      // Handle custom spelling config\n+      if (gladiaOptions.customSpellingConfig) {\n+        body.custom_spelling_config = {\n+          spelling_dictionary:\n+            gladiaOptions.customSpellingConfig.spellingDictionary,\n+        };\n+      }\n+    }\n+\n+    return {\n+      body,\n+      warnings,\n+    };\n+  }\n+\n+  async doGenerate(\n+    options: Parameters<TranscriptionModelV1['doGenerate']>[0],\n+  ): Promise<Awaited<ReturnType<TranscriptionModelV1['doGenerate']>>> {\n+    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n+\n+    // Create form data with base fields\n+    const formData = new FormData();\n+    const blob =\n+      options.audio instanceof Uint8Array\n+        ? new Blob([options.audio])\n+        : new Blob([convertBase64ToUint8Array(options.audio)]);\n+\n+    formData.append('model', this.modelId);\n+    formData.append(\n+      'audio',\n+      new File([blob], 'audio', { type: options.mediaType }),\n+    );\n+\n+    const { value: uploadResponse } = await postFormDataToApi({\n+      url: this.config.url({\n+        path: '/v2/upload',\n+        modelId: '',\n+      }),\n+      headers: combineHeaders(this.config.headers(), options.headers),\n+      formData,\n+      failedResponseHandler: gladiaFailedResponseHandler,\n+      successfulResponseHandler: createJsonResponseHandler(\n+        gladiaUploadResponseSchema,\n+      ),\n+      abortSignal: options.abortSignal,\n+      fetch: this.config.fetch,\n+    });\n+\n+    const { body, warnings } = await this.getArgs(options);\n+\n+    const { value: transcriptionInitResponse } = await postJsonToApi({\n+      url: this.config.url({\n+        path: '/v2/pre-recorded',\n+        modelId: '',\n+      }),\n+      headers: combineHeaders(this.config.headers(), options.headers),\n+      body: {\n+        ...body,\n+        audio_url: uploadResponse.audio_url,\n+      },\n+      failedResponseHandler: gladiaFailedResponseHandler,\n+      successfulResponseHandler: createJsonResponseHandler(\n+        gladiaTranscriptionInitializeResponseSchema,\n+      ),\n+      abortSignal: options.abortSignal,\n+      fetch: this.config.fetch,\n+    });\n+\n+    // Poll the result URL until the transcription is done or an error occurs\n+    const resultUrl = transcriptionInitResponse.result_url;\n+    let transcriptionResult;\n+    let transcriptionResultHeaders;\n+\n+    while (true) {\n+      const response = await getFromApi({\n+        url: resultUrl,\n+        headers: combineHeaders(this.config.headers(), options.headers),\n+        failedResponseHandler: gladiaFailedResponseHandler,\n+        successfulResponseHandler: createJsonResponseHandler(\n+          gladiaTranscriptionResultResponseSchema,\n+        ),\n+        abortSignal: options.abortSignal,\n+        fetch: this.config.fetch,\n+      });\n+\n+      transcriptionResult = response.value;\n+      transcriptionResultHeaders = response.responseHeaders;\n+\n+      if (\n+        transcriptionResult.status === 'done' ||\n+        transcriptionResult.status === 'error'\n+      ) {\n+        break;\n+      }\n+\n+      // Wait for 1 second before polling again\n+      await new Promise(resolve => setTimeout(resolve, 1000));\n+    }\n+\n+    // Handle error status\n+    if (transcriptionResult.status === 'error') {\n+      throw new Error(\n+        `Transcription failed with error code: ${transcriptionResult.error_code}`,\n+      );\n+    }\n+\n+    if (!transcriptionResult.result) {\n+      throw new Error('Transcription result is empty');\n+    }\n+\n+    // Process the successful result\n+    return {\n+      text: transcriptionResult.result.transcription.full_transcript,\n+      durationInSeconds: transcriptionResult.result.metadata.audio_duration,\n+      language: transcriptionResult.result.transcription.languages.at(0),\n+      segments: transcriptionResult.result.transcription.utterances.map(\n+        utterance => ({\n+          text: utterance.text,\n+          startSecond: utterance.start,\n+          endSecond: utterance.end,\n+        }),\n+      ),\n+      response: {\n+        timestamp: currentDate,\n+        modelId: '',\n+        headers: transcriptionResultHeaders,\n+      },\n+      providerMetadata: {\n+        gladia: transcriptionResult.result,\n+      },\n+      warnings,\n+    };\n+  }\n+}\n+\n+const gladiaUploadResponseSchema = z.object({\n+  audio_url: z.string(),\n+  audio_metadata: z.object({\n+    id: z.string(),\n+    filename: z.string(),\n+    extension: z.string(),\n+    size: z.number(),\n+    audio_duration: z.number(),\n+    number_of_channels: z.number(),\n+  }),\n+});\n+\n+const gladiaTranscriptionInitializeResponseSchema = z.object({\n+  id: z.string(),\n+  result_url: z.string(),\n+});\n+\n+const gladiaTranscriptionResultResponseSchema = z.object({",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2041062190",
        "repo_full_name": "vercel/ai",
        "pr_number": 5720,
        "pr_file": "packages/gladia/src/gladia-transcription-model.ts",
        "discussion_id": "2041062190",
        "commented_code": "@@ -0,0 +1,929 @@\n+import {\n+  TranscriptionModelV1,\n+  TranscriptionModelV1CallWarning,\n+} from '@ai-sdk/provider';\n+import {\n+  combineHeaders,\n+  convertBase64ToUint8Array,\n+  createJsonResponseHandler,\n+  getFromApi,\n+  parseProviderOptions,\n+  postFormDataToApi,\n+  postJsonToApi,\n+} from '@ai-sdk/provider-utils';\n+import { z } from 'zod';\n+import { GladiaConfig } from './gladia-config';\n+import { gladiaFailedResponseHandler } from './gladia-error';\n+import { GladiaTranscriptionInitiateAPITypes } from './gladia-api-types';\n+\n+// https://docs.gladia.io/api-reference/v2/pre-recorded/init\n+const gladiaProviderOptionsSchema = z.object({\n+  /**\n+   * Optional context prompt to guide the transcription.\n+   */\n+  contextPrompt: z.string().nullish(),\n+\n+  /**\n+   * Custom vocabulary to improve transcription accuracy.\n+   * Can be a boolean or an array of custom terms.\n+   */\n+  customVocabulary: z.union([z.boolean(), z.array(z.any())]).nullish(),\n+\n+  /**\n+   * Configuration for custom vocabulary.\n+   */\n+  customVocabularyConfig: z\n+    .object({\n+      /**\n+       * Array of vocabulary terms or objects with pronunciation details.\n+       */\n+      vocabulary: z.array(\n+        z.union([\n+          z.string(),\n+          z.object({\n+            /**\n+             * The vocabulary term.\n+             */\n+            value: z.string(),\n+            /**\n+             * Intensity of the term in recognition (optional).\n+             */\n+            intensity: z.number().nullish(),\n+            /**\n+             * Alternative pronunciations for the term (optional).\n+             */\n+            pronunciations: z.array(z.string()).nullish(),\n+            /**\n+             * Language of the term (optional).\n+             */\n+            language: z.string().nullish(),\n+          }),\n+        ]),\n+      ),\n+      /**\n+       * Default intensity for all vocabulary terms.\n+       */\n+      defaultIntensity: z.number().nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to automatically detect the language of the audio.\n+   */\n+  detectLanguage: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable code switching (multiple languages in the same audio).\n+   */\n+  enableCodeSwitching: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for code switching.\n+   */\n+  codeSwitchingConfig: z\n+    .object({\n+      /**\n+       * Languages to consider for code switching.\n+       */\n+      languages: z.array(z.string()).nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Specific language for transcription.\n+   */\n+  language: z.string().nullish(),\n+\n+  /**\n+   * Whether to enable callback when transcription is complete.\n+   */\n+  callback: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for callback.\n+   */\n+  callbackConfig: z\n+    .object({\n+      /**\n+       * URL to send the callback to.\n+       */\n+      url: z.string(),\n+      /**\n+       * HTTP method for the callback.\n+       */\n+      method: z.enum(['POST', 'PUT']).nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to generate subtitles.\n+   */\n+  subtitles: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for subtitles generation.\n+   */\n+  subtitlesConfig: z\n+    .object({\n+      /**\n+       * Subtitle file formats to generate.\n+       */\n+      formats: z.array(z.enum(['srt', 'vtt'])).nullish(),\n+      /**\n+       * Minimum duration for subtitle segments.\n+       */\n+      minimumDuration: z.number().nullish(),\n+      /**\n+       * Maximum duration for subtitle segments.\n+       */\n+      maximumDuration: z.number().nullish(),\n+      /**\n+       * Maximum characters per row in subtitles.\n+       */\n+      maximumCharactersPerRow: z.number().nullish(),\n+      /**\n+       * Maximum rows per caption in subtitles.\n+       */\n+      maximumRowsPerCaption: z.number().nullish(),\n+      /**\n+       * Style of subtitles.\n+       */\n+      style: z.enum(['default', 'compliance']).nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to enable speaker diarization (speaker identification).\n+   */\n+  diarization: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for diarization.\n+   */\n+  diarizationConfig: z\n+    .object({\n+      /**\n+       * Exact number of speakers to identify.\n+       */\n+      numberOfSpeakers: z.number().nullish(),\n+      /**\n+       * Minimum number of speakers to identify.\n+       */\n+      minSpeakers: z.number().nullish(),\n+      /**\n+       * Maximum number of speakers to identify.\n+       */\n+      maxSpeakers: z.number().nullish(),\n+      /**\n+       * Whether to use enhanced diarization.\n+       */\n+      enhanced: z.boolean().nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to translate the transcription.\n+   */\n+  translation: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for translation.\n+   */\n+  translationConfig: z\n+    .object({\n+      /**\n+       * Target languages for translation.\n+       */\n+      targetLanguages: z.array(z.string()),\n+      /**\n+       * Translation model to use.\n+       */\n+      model: z.enum(['base', 'enhanced']).nullish(),\n+      /**\n+       * Whether to match original utterances in translation.\n+       */\n+      matchOriginalUtterances: z.boolean().nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to generate a summary of the transcription.\n+   */\n+  summarization: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for summarization.\n+   */\n+  summarizationConfig: z\n+    .object({\n+      /**\n+       * Type of summary to generate.\n+       */\n+      type: z.enum(['general', 'bullet_points', 'concise']).nullish(),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to enable content moderation.\n+   */\n+  moderation: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable named entity recognition.\n+   */\n+  namedEntityRecognition: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable automatic chapter creation.\n+   */\n+  chapterization: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to ensure consistent naming of entities.\n+   */\n+  nameConsistency: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable custom spelling.\n+   */\n+  customSpelling: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for custom spelling.\n+   */\n+  customSpellingConfig: z\n+    .object({\n+      /**\n+       * Dictionary of custom spellings.\n+       */\n+      spellingDictionary: z.record(z.array(z.string())),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to extract structured data from the transcription.\n+   */\n+  structuredDataExtraction: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for structured data extraction.\n+   */\n+  structuredDataExtractionConfig: z\n+    .object({\n+      /**\n+       * Classes of data to extract.\n+       */\n+      classes: z.array(z.string()),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Whether to perform sentiment analysis on the transcription.\n+   */\n+  sentimentAnalysis: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to send audio to a language model for processing.\n+   */\n+  audioToLlm: z.boolean().nullish(),\n+\n+  /**\n+   * Configuration for audio to language model processing.\n+   */\n+  audioToLlmConfig: z\n+    .object({\n+      /**\n+       * Prompts to send to the language model.\n+       */\n+      prompts: z.array(z.string()),\n+    })\n+    .nullish(),\n+\n+  /**\n+   * Custom metadata to include with the transcription.\n+   */\n+  customMetadata: z.record(z.any()).nullish(),\n+\n+  /**\n+   * Whether to include sentence-level segmentation.\n+   */\n+  sentences: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enable display mode.\n+   */\n+  displayMode: z.boolean().nullish(),\n+\n+  /**\n+   * Whether to enhance punctuation in the transcription.\n+   */\n+  punctuationEnhanced: z.boolean().nullish(),\n+});\n+\n+export type GladiaTranscriptionCallOptions = z.infer<\n+  typeof gladiaProviderOptionsSchema\n+>;\n+\n+interface GladiaTranscriptionModelConfig extends GladiaConfig {\n+  _internal?: {\n+    currentDate?: () => Date;\n+  };\n+}\n+\n+export class GladiaTranscriptionModel implements TranscriptionModelV1 {\n+  readonly specificationVersion = 'v1';\n+\n+  get provider(): string {\n+    return this.config.provider;\n+  }\n+\n+  constructor(\n+    readonly modelId: '',\n+    private readonly config: GladiaTranscriptionModelConfig,\n+  ) {}\n+\n+  private async getArgs({\n+    providerOptions,\n+  }: Parameters<TranscriptionModelV1['doGenerate']>[0]) {\n+    const warnings: TranscriptionModelV1CallWarning[] = [];\n+\n+    // Parse provider options\n+    const gladiaOptions = parseProviderOptions({\n+      provider: 'gladia',\n+      providerOptions,\n+      schema: gladiaProviderOptionsSchema,\n+    });\n+\n+    const body: Omit<GladiaTranscriptionInitiateAPITypes, 'audio_url'> = {};\n+\n+    // Add provider-specific options\n+    if (gladiaOptions) {\n+      body.context_prompt = gladiaOptions.contextPrompt ?? undefined;\n+      body.custom_vocabulary = gladiaOptions.customVocabulary ?? undefined;\n+      body.detect_language = gladiaOptions.detectLanguage ?? undefined;\n+      body.enable_code_switching =\n+        gladiaOptions.enableCodeSwitching ?? undefined;\n+      body.language = gladiaOptions.language ?? undefined;\n+      body.callback = gladiaOptions.callback ?? undefined;\n+      body.subtitles = gladiaOptions.subtitles ?? undefined;\n+      body.diarization = gladiaOptions.diarization ?? undefined;\n+      body.translation = gladiaOptions.translation ?? undefined;\n+      body.summarization = gladiaOptions.summarization ?? undefined;\n+      body.moderation = gladiaOptions.moderation ?? undefined;\n+      body.named_entity_recognition =\n+        gladiaOptions.namedEntityRecognition ?? undefined;\n+      body.chapterization = gladiaOptions.chapterization ?? undefined;\n+      body.name_consistency = gladiaOptions.nameConsistency ?? undefined;\n+      body.custom_spelling = gladiaOptions.customSpelling ?? undefined;\n+      body.structured_data_extraction =\n+        gladiaOptions.structuredDataExtraction ?? undefined;\n+      body.structured_data_extraction_config =\n+        gladiaOptions.structuredDataExtractionConfig ?? undefined;\n+      body.sentiment_analysis = gladiaOptions.sentimentAnalysis ?? undefined;\n+      body.audio_to_llm = gladiaOptions.audioToLlm ?? undefined;\n+      body.audio_to_llm_config = gladiaOptions.audioToLlmConfig ?? undefined;\n+      body.custom_metadata = gladiaOptions.customMetadata ?? undefined;\n+      body.sentences = gladiaOptions.sentences ?? undefined;\n+      body.display_mode = gladiaOptions.displayMode ?? undefined;\n+      body.punctuation_enhanced =\n+        gladiaOptions.punctuationEnhanced ?? undefined;\n+\n+      if (gladiaOptions.customVocabularyConfig) {\n+        body.custom_vocabulary_config = {\n+          vocabulary: gladiaOptions.customVocabularyConfig.vocabulary.map(\n+            item => {\n+              if (typeof item === 'string') return item;\n+              return {\n+                value: item.value,\n+                intensity: item.intensity ?? undefined,\n+                pronunciations: item.pronunciations ?? undefined,\n+                language: item.language ?? undefined,\n+              };\n+            },\n+          ),\n+          default_intensity:\n+            gladiaOptions.customVocabularyConfig.defaultIntensity ?? undefined,\n+        };\n+      }\n+\n+      // Handle code switching config\n+      if (gladiaOptions.codeSwitchingConfig) {\n+        body.code_switching_config = {\n+          languages: gladiaOptions.codeSwitchingConfig.languages ?? undefined,\n+        };\n+      }\n+\n+      // Handle callback config\n+      if (gladiaOptions.callbackConfig) {\n+        body.callback_config = {\n+          url: gladiaOptions.callbackConfig.url,\n+          method: gladiaOptions.callbackConfig.method ?? undefined,\n+        };\n+      }\n+\n+      // Handle subtitles config\n+      if (gladiaOptions.subtitlesConfig) {\n+        body.subtitles_config = {\n+          formats: gladiaOptions.subtitlesConfig.formats ?? undefined,\n+          minimum_duration:\n+            gladiaOptions.subtitlesConfig.minimumDuration ?? undefined,\n+          maximum_duration:\n+            gladiaOptions.subtitlesConfig.maximumDuration ?? undefined,\n+          maximum_characters_per_row:\n+            gladiaOptions.subtitlesConfig.maximumCharactersPerRow ?? undefined,\n+          maximum_rows_per_caption:\n+            gladiaOptions.subtitlesConfig.maximumRowsPerCaption ?? undefined,\n+          style: gladiaOptions.subtitlesConfig.style ?? undefined,\n+        };\n+      }\n+\n+      // Handle diarization config\n+      if (gladiaOptions.diarizationConfig) {\n+        body.diarization_config = {\n+          number_of_speakers:\n+            gladiaOptions.diarizationConfig.numberOfSpeakers ?? undefined,\n+          min_speakers:\n+            gladiaOptions.diarizationConfig.minSpeakers ?? undefined,\n+          max_speakers:\n+            gladiaOptions.diarizationConfig.maxSpeakers ?? undefined,\n+          enhanced: gladiaOptions.diarizationConfig.enhanced ?? undefined,\n+        };\n+      }\n+\n+      // Handle translation config\n+      if (gladiaOptions.translationConfig) {\n+        body.translation_config = {\n+          target_languages: gladiaOptions.translationConfig.targetLanguages,\n+          model: gladiaOptions.translationConfig.model ?? undefined,\n+          match_original_utterances:\n+            gladiaOptions.translationConfig.matchOriginalUtterances ??\n+            undefined,\n+        };\n+      }\n+\n+      // Handle summarization config\n+      if (gladiaOptions.summarizationConfig) {\n+        body.summarization_config = {\n+          type: gladiaOptions.summarizationConfig.type ?? undefined,\n+        };\n+      }\n+\n+      // Handle custom spelling config\n+      if (gladiaOptions.customSpellingConfig) {\n+        body.custom_spelling_config = {\n+          spelling_dictionary:\n+            gladiaOptions.customSpellingConfig.spellingDictionary,\n+        };\n+      }\n+    }\n+\n+    return {\n+      body,\n+      warnings,\n+    };\n+  }\n+\n+  async doGenerate(\n+    options: Parameters<TranscriptionModelV1['doGenerate']>[0],\n+  ): Promise<Awaited<ReturnType<TranscriptionModelV1['doGenerate']>>> {\n+    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n+\n+    // Create form data with base fields\n+    const formData = new FormData();\n+    const blob =\n+      options.audio instanceof Uint8Array\n+        ? new Blob([options.audio])\n+        : new Blob([convertBase64ToUint8Array(options.audio)]);\n+\n+    formData.append('model', this.modelId);\n+    formData.append(\n+      'audio',\n+      new File([blob], 'audio', { type: options.mediaType }),\n+    );\n+\n+    const { value: uploadResponse } = await postFormDataToApi({\n+      url: this.config.url({\n+        path: '/v2/upload',\n+        modelId: '',\n+      }),\n+      headers: combineHeaders(this.config.headers(), options.headers),\n+      formData,\n+      failedResponseHandler: gladiaFailedResponseHandler,\n+      successfulResponseHandler: createJsonResponseHandler(\n+        gladiaUploadResponseSchema,\n+      ),\n+      abortSignal: options.abortSignal,\n+      fetch: this.config.fetch,\n+    });\n+\n+    const { body, warnings } = await this.getArgs(options);\n+\n+    const { value: transcriptionInitResponse } = await postJsonToApi({\n+      url: this.config.url({\n+        path: '/v2/pre-recorded',\n+        modelId: '',\n+      }),\n+      headers: combineHeaders(this.config.headers(), options.headers),\n+      body: {\n+        ...body,\n+        audio_url: uploadResponse.audio_url,\n+      },\n+      failedResponseHandler: gladiaFailedResponseHandler,\n+      successfulResponseHandler: createJsonResponseHandler(\n+        gladiaTranscriptionInitializeResponseSchema,\n+      ),\n+      abortSignal: options.abortSignal,\n+      fetch: this.config.fetch,\n+    });\n+\n+    // Poll the result URL until the transcription is done or an error occurs\n+    const resultUrl = transcriptionInitResponse.result_url;\n+    let transcriptionResult;\n+    let transcriptionResultHeaders;\n+\n+    while (true) {\n+      const response = await getFromApi({\n+        url: resultUrl,\n+        headers: combineHeaders(this.config.headers(), options.headers),\n+        failedResponseHandler: gladiaFailedResponseHandler,\n+        successfulResponseHandler: createJsonResponseHandler(\n+          gladiaTranscriptionResultResponseSchema,\n+        ),\n+        abortSignal: options.abortSignal,\n+        fetch: this.config.fetch,\n+      });\n+\n+      transcriptionResult = response.value;\n+      transcriptionResultHeaders = response.responseHeaders;\n+\n+      if (\n+        transcriptionResult.status === 'done' ||\n+        transcriptionResult.status === 'error'\n+      ) {\n+        break;\n+      }\n+\n+      // Wait for 1 second before polling again\n+      await new Promise(resolve => setTimeout(resolve, 1000));\n+    }\n+\n+    // Handle error status\n+    if (transcriptionResult.status === 'error') {\n+      throw new Error(\n+        `Transcription failed with error code: ${transcriptionResult.error_code}`,\n+      );\n+    }\n+\n+    if (!transcriptionResult.result) {\n+      throw new Error('Transcription result is empty');\n+    }\n+\n+    // Process the successful result\n+    return {\n+      text: transcriptionResult.result.transcription.full_transcript,\n+      durationInSeconds: transcriptionResult.result.metadata.audio_duration,\n+      language: transcriptionResult.result.transcription.languages.at(0),\n+      segments: transcriptionResult.result.transcription.utterances.map(\n+        utterance => ({\n+          text: utterance.text,\n+          startSecond: utterance.start,\n+          endSecond: utterance.end,\n+        }),\n+      ),\n+      response: {\n+        timestamp: currentDate,\n+        modelId: '',\n+        headers: transcriptionResultHeaders,\n+      },\n+      providerMetadata: {\n+        gladia: transcriptionResult.result,\n+      },\n+      warnings,\n+    };\n+  }\n+}\n+\n+const gladiaUploadResponseSchema = z.object({\n+  audio_url: z.string(),\n+  audio_metadata: z.object({\n+    id: z.string(),\n+    filename: z.string(),\n+    extension: z.string(),\n+    size: z.number(),\n+    audio_duration: z.number(),\n+    number_of_channels: z.number(),\n+  }),\n+});\n+\n+const gladiaTranscriptionInitializeResponseSchema = z.object({\n+  id: z.string(),\n+  result_url: z.string(),\n+});\n+\n+const gladiaTranscriptionResultResponseSchema = z.object({",
        "comment_created_at": "2025-04-13T07:03:46+00:00",
        "comment_author": "lgrammel",
        "comment_body": "this is going to be brittle as they change their APIs. Can we expose the raw body from the final result and strip this to a minimum?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2040691266",
    "pr_number": 5698,
    "pr_file": "packages/provider/src/language-model/v2/language-model-v2-call-options.ts",
    "created_at": "2025-04-12T16:39:42+00:00",
    "commented_code": "* to the provider from the AI SDK and enable provider-specific\n    * functionality that can be fully encapsulated in the provider.\n    */\n-  providerOptions?: LanguageModelV2ProviderOptions;\n+  providerOptions?: ProviderV2Options;",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2040691266",
        "repo_full_name": "vercel/ai",
        "pr_number": 5698,
        "pr_file": "packages/provider/src/language-model/v2/language-model-v2-call-options.ts",
        "discussion_id": "2040691266",
        "commented_code": "@@ -130,5 +130,5 @@ Only applicable for HTTP-based providers.\n    * to the provider from the AI SDK and enable provider-specific\n    * functionality that can be fully encapsulated in the provider.\n    */\n-  providerOptions?: LanguageModelV2ProviderOptions;\n+  providerOptions?: ProviderV2Options;",
        "comment_created_at": "2025-04-12T16:39:42+00:00",
        "comment_author": "lgrammel",
        "comment_body": "Please keep specs fully separate for now.",
        "pr_file_module": null
      },
      {
        "comment_id": "2041112071",
        "repo_full_name": "vercel/ai",
        "pr_number": 5698,
        "pr_file": "packages/provider/src/language-model/v2/language-model-v2-call-options.ts",
        "discussion_id": "2040691266",
        "commented_code": "@@ -130,5 +130,5 @@ Only applicable for HTTP-based providers.\n    * to the provider from the AI SDK and enable provider-specific\n    * functionality that can be fully encapsulated in the provider.\n    */\n-  providerOptions?: LanguageModelV2ProviderOptions;\n+  providerOptions?: ProviderV2Options;",
        "comment_created_at": "2025-04-13T12:15:32+00:00",
        "comment_author": "lgrammel",
        "comment_body": "I've extracted `SharedV2ProviderOptions` and `SharedV2ProviderMetadata` that we can use in all model specs: https://github.com/vercel/ai/pull/5733",
        "pr_file_module": null
      },
      {
        "comment_id": "2041142476",
        "repo_full_name": "vercel/ai",
        "pr_number": 5698,
        "pr_file": "packages/provider/src/language-model/v2/language-model-v2-call-options.ts",
        "discussion_id": "2040691266",
        "commented_code": "@@ -130,5 +130,5 @@ Only applicable for HTTP-based providers.\n    * to the provider from the AI SDK and enable provider-specific\n    * functionality that can be fully encapsulated in the provider.\n    */\n-  providerOptions?: LanguageModelV2ProviderOptions;\n+  providerOptions?: ProviderV2Options;",
        "comment_created_at": "2025-04-13T14:56:50+00:00",
        "comment_author": "samdenty",
        "comment_body": "awesome, thanks - yeah think it makes sense",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2028154514",
    "pr_number": 5496,
    "pr_file": "packages/openai/src/openai-transcription-model.ts",
    "created_at": "2025-04-04T06:14:02+00:00",
    "commented_code": "+import { TranscriptionModelV1 } from '@ai-sdk/provider';\n+import {\n+  combineHeaders,\n+  createJsonResponseHandler,\n+  postFormDataToApi,\n+} from '@ai-sdk/provider-utils';\n+import { z } from 'zod';\n+import { OpenAIConfig } from './openai-config';\n+import { openaiFailedResponseHandler } from './openai-error';\n+import { OpenAITranscriptionModelId } from './openai-transcription-settings';\n+\n+interface OpenAITranscriptionModelConfig extends OpenAIConfig {\n+  _internal?: {\n+    currentDate?: () => Date;\n+  };\n+}\n+\n+// https://platform.openai.com/docs/api-reference/audio/createTranscription\n+const providerOptionsMapping = {\n+  include: 'include',\n+  language: 'language',\n+  prompt: 'prompt',\n+  responseFormat: 'response_format',\n+  temperature: 'temperature',\n+  timestampGranularities: 'timestamp_granularities',\n+};\n+\n+// https://platform.openai.com/docs/guides/speech-to-text#supported-languages\n+const languageMap = {\n+  afrikaans: 'af',\n+  arabic: 'ar',\n+  armenian: 'hy',\n+  azerbaijani: 'az',\n+  belarusian: 'be',\n+  bosnian: 'bs',\n+  bulgarian: 'bg',\n+  catalan: 'ca',\n+  chinese: 'zh',\n+  croatian: 'hr',\n+  czech: 'cs',\n+  danish: 'da',\n+  dutch: 'nl',\n+  english: 'en',\n+  estonian: 'et',\n+  finnish: 'fi',\n+  french: 'fr',\n+  galician: 'gl',\n+  german: 'de',\n+  greek: 'el',\n+  hebrew: 'he',\n+  hindi: 'hi',\n+  hungarian: 'hu',\n+  icelandic: 'is',\n+  indonesian: 'id',\n+  italian: 'it',\n+  japanese: 'ja',\n+  kannada: 'kn',\n+  kazakh: 'kk',\n+  korean: 'ko',\n+  latvian: 'lv',\n+  lithuanian: 'lt',\n+  macedonian: 'mk',\n+  malay: 'ms',\n+  marathi: 'mr',\n+  maori: 'mi',\n+  nepali: 'ne',\n+  norwegian: 'no',\n+  persian: 'fa',\n+  polish: 'pl',\n+  portuguese: 'pt',\n+  romanian: 'ro',\n+  russian: 'ru',\n+  serbian: 'sr',\n+  slovak: 'sk',\n+  slovenian: 'sl',\n+  spanish: 'es',\n+  swahili: 'sw',\n+  swedish: 'sv',\n+  tagalog: 'tl',\n+  tamil: 'ta',\n+  thai: 'th',\n+  turkish: 'tr',\n+  ukrainian: 'uk',\n+  urdu: 'ur',\n+  vietnamese: 'vi',\n+  welsh: 'cy',\n+};\n+\n+export class OpenAITranscriptionModel implements TranscriptionModelV1 {\n+  readonly specificationVersion = 'v1';\n+\n+  get provider(): string {\n+    return this.config.provider;\n+  }\n+\n+  constructor(\n+    readonly modelId: OpenAITranscriptionModelId,\n+    private readonly config: OpenAITranscriptionModelConfig,\n+  ) {}\n+\n+  async doGenerate({\n+    audio,\n+    providerOptions,\n+    headers,\n+    abortSignal,\n+  }: Parameters<TranscriptionModelV1['doGenerate']>[0]): Promise<\n+    Awaited<ReturnType<TranscriptionModelV1['doGenerate']>>\n+  > {\n+    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n+    const formData = new FormData();\n+\n+    let blob: Blob | undefined;\n+\n+    if (audio instanceof Uint8Array) {\n+      // Convert Uint8Array to Blob and then to File\n+      blob = new Blob([audio]);\n+    } else if (typeof audio === 'string') {\n+      // Convert base64 string to Blob and then to File\n+      const byteCharacters = atob(audio);\n+      const byteNumbers = new Array(byteCharacters.length);\n+      for (let i = 0; i < byteCharacters.length; i++) {\n+        byteNumbers[i] = byteCharacters.charCodeAt(i);\n+      }\n+      const byteArray = new Uint8Array(byteNumbers);\n+      blob = new Blob([byteArray]);\n+    } else {\n+      throw new Error(\n+        'Invalid audio format. Must be Uint8Array or base64 string.',\n+      );\n+    }\n+\n+    formData.append('model', this.modelId);\n+    formData.append(\n+      'file',\n+      new File([blob], 'audio.wav', { type: 'audio/wav' }),\n+    );\n+\n+    // Add any additional provider options\n+    if (providerOptions?.openai) {\n+      for (const [key, value] of Object.entries(providerOptions.openai)) {\n+        if (key in providerOptionsMapping) {\n+          const newKey =\n+            providerOptionsMapping[key as keyof typeof providerOptionsMapping];\n+\n+          formData.append(newKey, String(value));\n+        }\n+      }\n+    }",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2028154514",
        "repo_full_name": "vercel/ai",
        "pr_number": 5496,
        "pr_file": "packages/openai/src/openai-transcription-model.ts",
        "discussion_id": "2028154514",
        "commented_code": "@@ -0,0 +1,208 @@\n+import { TranscriptionModelV1 } from '@ai-sdk/provider';\n+import {\n+  combineHeaders,\n+  createJsonResponseHandler,\n+  postFormDataToApi,\n+} from '@ai-sdk/provider-utils';\n+import { z } from 'zod';\n+import { OpenAIConfig } from './openai-config';\n+import { openaiFailedResponseHandler } from './openai-error';\n+import { OpenAITranscriptionModelId } from './openai-transcription-settings';\n+\n+interface OpenAITranscriptionModelConfig extends OpenAIConfig {\n+  _internal?: {\n+    currentDate?: () => Date;\n+  };\n+}\n+\n+// https://platform.openai.com/docs/api-reference/audio/createTranscription\n+const providerOptionsMapping = {\n+  include: 'include',\n+  language: 'language',\n+  prompt: 'prompt',\n+  responseFormat: 'response_format',\n+  temperature: 'temperature',\n+  timestampGranularities: 'timestamp_granularities',\n+};\n+\n+// https://platform.openai.com/docs/guides/speech-to-text#supported-languages\n+const languageMap = {\n+  afrikaans: 'af',\n+  arabic: 'ar',\n+  armenian: 'hy',\n+  azerbaijani: 'az',\n+  belarusian: 'be',\n+  bosnian: 'bs',\n+  bulgarian: 'bg',\n+  catalan: 'ca',\n+  chinese: 'zh',\n+  croatian: 'hr',\n+  czech: 'cs',\n+  danish: 'da',\n+  dutch: 'nl',\n+  english: 'en',\n+  estonian: 'et',\n+  finnish: 'fi',\n+  french: 'fr',\n+  galician: 'gl',\n+  german: 'de',\n+  greek: 'el',\n+  hebrew: 'he',\n+  hindi: 'hi',\n+  hungarian: 'hu',\n+  icelandic: 'is',\n+  indonesian: 'id',\n+  italian: 'it',\n+  japanese: 'ja',\n+  kannada: 'kn',\n+  kazakh: 'kk',\n+  korean: 'ko',\n+  latvian: 'lv',\n+  lithuanian: 'lt',\n+  macedonian: 'mk',\n+  malay: 'ms',\n+  marathi: 'mr',\n+  maori: 'mi',\n+  nepali: 'ne',\n+  norwegian: 'no',\n+  persian: 'fa',\n+  polish: 'pl',\n+  portuguese: 'pt',\n+  romanian: 'ro',\n+  russian: 'ru',\n+  serbian: 'sr',\n+  slovak: 'sk',\n+  slovenian: 'sl',\n+  spanish: 'es',\n+  swahili: 'sw',\n+  swedish: 'sv',\n+  tagalog: 'tl',\n+  tamil: 'ta',\n+  thai: 'th',\n+  turkish: 'tr',\n+  ukrainian: 'uk',\n+  urdu: 'ur',\n+  vietnamese: 'vi',\n+  welsh: 'cy',\n+};\n+\n+export class OpenAITranscriptionModel implements TranscriptionModelV1 {\n+  readonly specificationVersion = 'v1';\n+\n+  get provider(): string {\n+    return this.config.provider;\n+  }\n+\n+  constructor(\n+    readonly modelId: OpenAITranscriptionModelId,\n+    private readonly config: OpenAITranscriptionModelConfig,\n+  ) {}\n+\n+  async doGenerate({\n+    audio,\n+    providerOptions,\n+    headers,\n+    abortSignal,\n+  }: Parameters<TranscriptionModelV1['doGenerate']>[0]): Promise<\n+    Awaited<ReturnType<TranscriptionModelV1['doGenerate']>>\n+  > {\n+    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n+    const formData = new FormData();\n+\n+    let blob: Blob | undefined;\n+\n+    if (audio instanceof Uint8Array) {\n+      // Convert Uint8Array to Blob and then to File\n+      blob = new Blob([audio]);\n+    } else if (typeof audio === 'string') {\n+      // Convert base64 string to Blob and then to File\n+      const byteCharacters = atob(audio);\n+      const byteNumbers = new Array(byteCharacters.length);\n+      for (let i = 0; i < byteCharacters.length; i++) {\n+        byteNumbers[i] = byteCharacters.charCodeAt(i);\n+      }\n+      const byteArray = new Uint8Array(byteNumbers);\n+      blob = new Blob([byteArray]);\n+    } else {\n+      throw new Error(\n+        'Invalid audio format. Must be Uint8Array or base64 string.',\n+      );\n+    }\n+\n+    formData.append('model', this.modelId);\n+    formData.append(\n+      'file',\n+      new File([blob], 'audio.wav', { type: 'audio/wav' }),\n+    );\n+\n+    // Add any additional provider options\n+    if (providerOptions?.openai) {\n+      for (const [key, value] of Object.entries(providerOptions.openai)) {\n+        if (key in providerOptionsMapping) {\n+          const newKey =\n+            providerOptionsMapping[key as keyof typeof providerOptionsMapping];\n+\n+          formData.append(newKey, String(value));\n+        }\n+      }\n+    }",
        "comment_created_at": "2025-04-04T06:14:02+00:00",
        "comment_author": "lgrammel",
        "comment_body": "Please use the validation approach for provider options that we use elsewhere (so we get a type for type checking and can throw errors before the request). See https://github.com/vercel/ai/blob/main/packages/anthropic/src/anthropic-messages-language-model.ts#L122 and https://github.com/vercel/ai/blob/main/packages/anthropic/src/anthropic-messages-language-model.ts#L724 for an example",
        "pr_file_module": null
      },
      {
        "comment_id": "2031828977",
        "repo_full_name": "vercel/ai",
        "pr_number": 5496,
        "pr_file": "packages/openai/src/openai-transcription-model.ts",
        "discussion_id": "2028154514",
        "commented_code": "@@ -0,0 +1,208 @@\n+import { TranscriptionModelV1 } from '@ai-sdk/provider';\n+import {\n+  combineHeaders,\n+  createJsonResponseHandler,\n+  postFormDataToApi,\n+} from '@ai-sdk/provider-utils';\n+import { z } from 'zod';\n+import { OpenAIConfig } from './openai-config';\n+import { openaiFailedResponseHandler } from './openai-error';\n+import { OpenAITranscriptionModelId } from './openai-transcription-settings';\n+\n+interface OpenAITranscriptionModelConfig extends OpenAIConfig {\n+  _internal?: {\n+    currentDate?: () => Date;\n+  };\n+}\n+\n+// https://platform.openai.com/docs/api-reference/audio/createTranscription\n+const providerOptionsMapping = {\n+  include: 'include',\n+  language: 'language',\n+  prompt: 'prompt',\n+  responseFormat: 'response_format',\n+  temperature: 'temperature',\n+  timestampGranularities: 'timestamp_granularities',\n+};\n+\n+// https://platform.openai.com/docs/guides/speech-to-text#supported-languages\n+const languageMap = {\n+  afrikaans: 'af',\n+  arabic: 'ar',\n+  armenian: 'hy',\n+  azerbaijani: 'az',\n+  belarusian: 'be',\n+  bosnian: 'bs',\n+  bulgarian: 'bg',\n+  catalan: 'ca',\n+  chinese: 'zh',\n+  croatian: 'hr',\n+  czech: 'cs',\n+  danish: 'da',\n+  dutch: 'nl',\n+  english: 'en',\n+  estonian: 'et',\n+  finnish: 'fi',\n+  french: 'fr',\n+  galician: 'gl',\n+  german: 'de',\n+  greek: 'el',\n+  hebrew: 'he',\n+  hindi: 'hi',\n+  hungarian: 'hu',\n+  icelandic: 'is',\n+  indonesian: 'id',\n+  italian: 'it',\n+  japanese: 'ja',\n+  kannada: 'kn',\n+  kazakh: 'kk',\n+  korean: 'ko',\n+  latvian: 'lv',\n+  lithuanian: 'lt',\n+  macedonian: 'mk',\n+  malay: 'ms',\n+  marathi: 'mr',\n+  maori: 'mi',\n+  nepali: 'ne',\n+  norwegian: 'no',\n+  persian: 'fa',\n+  polish: 'pl',\n+  portuguese: 'pt',\n+  romanian: 'ro',\n+  russian: 'ru',\n+  serbian: 'sr',\n+  slovak: 'sk',\n+  slovenian: 'sl',\n+  spanish: 'es',\n+  swahili: 'sw',\n+  swedish: 'sv',\n+  tagalog: 'tl',\n+  tamil: 'ta',\n+  thai: 'th',\n+  turkish: 'tr',\n+  ukrainian: 'uk',\n+  urdu: 'ur',\n+  vietnamese: 'vi',\n+  welsh: 'cy',\n+};\n+\n+export class OpenAITranscriptionModel implements TranscriptionModelV1 {\n+  readonly specificationVersion = 'v1';\n+\n+  get provider(): string {\n+    return this.config.provider;\n+  }\n+\n+  constructor(\n+    readonly modelId: OpenAITranscriptionModelId,\n+    private readonly config: OpenAITranscriptionModelConfig,\n+  ) {}\n+\n+  async doGenerate({\n+    audio,\n+    providerOptions,\n+    headers,\n+    abortSignal,\n+  }: Parameters<TranscriptionModelV1['doGenerate']>[0]): Promise<\n+    Awaited<ReturnType<TranscriptionModelV1['doGenerate']>>\n+  > {\n+    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n+    const formData = new FormData();\n+\n+    let blob: Blob | undefined;\n+\n+    if (audio instanceof Uint8Array) {\n+      // Convert Uint8Array to Blob and then to File\n+      blob = new Blob([audio]);\n+    } else if (typeof audio === 'string') {\n+      // Convert base64 string to Blob and then to File\n+      const byteCharacters = atob(audio);\n+      const byteNumbers = new Array(byteCharacters.length);\n+      for (let i = 0; i < byteCharacters.length; i++) {\n+        byteNumbers[i] = byteCharacters.charCodeAt(i);\n+      }\n+      const byteArray = new Uint8Array(byteNumbers);\n+      blob = new Blob([byteArray]);\n+    } else {\n+      throw new Error(\n+        'Invalid audio format. Must be Uint8Array or base64 string.',\n+      );\n+    }\n+\n+    formData.append('model', this.modelId);\n+    formData.append(\n+      'file',\n+      new File([blob], 'audio.wav', { type: 'audio/wav' }),\n+    );\n+\n+    // Add any additional provider options\n+    if (providerOptions?.openai) {\n+      for (const [key, value] of Object.entries(providerOptions.openai)) {\n+        if (key in providerOptionsMapping) {\n+          const newKey =\n+            providerOptionsMapping[key as keyof typeof providerOptionsMapping];\n+\n+          formData.append(newKey, String(value));\n+        }\n+      }\n+    }",
        "comment_created_at": "2025-04-07T18:58:38+00:00",
        "comment_author": "haydenbleasel",
        "comment_body": "Added validation for both the raw options and provider options + extended the `providerOptions` for better type safety.",
        "pr_file_module": null
      }
    ]
  }
]