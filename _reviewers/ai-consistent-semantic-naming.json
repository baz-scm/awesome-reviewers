[
  {
    "discussion_id": "2191705187",
    "pr_number": 7071,
    "pr_file": "packages/google/src/tool/google-search.ts",
    "created_at": "2025-07-08T07:28:08+00:00",
    "commented_code": "+import { createProviderDefinedToolFactoryWithOutputSchema } from '@ai-sdk/provider-utils';\n+import { z } from 'zod/v4';\n+\n+// https://ai.google.dev/gemini-api/docs/google-search\n+// https://ai.google.dev/api/generate-content#GroundingSupport\n+// https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search\n+const groundingChunkSchema = z.object({\n+  web: z.object({ uri: z.string(), title: z.string() }).nullish(),\n+  retrievedContext: z.object({ uri: z.string(), title: z.string() }).nullish(),\n+});\n+\n+export const groundingMetadataSchema = z.object({\n+  webSearchQueries: z.array(z.string()).nullish(),\n+  retrievalQueries: z.array(z.string()).nullish(),\n+  searchEntryPoint: z.object({ renderedContent: z.string() }).nullish(),\n+  groundingChunks: z.array(groundingChunkSchema).nullish(),\n+  groundingSupports: z\n+    .array(\n+      z.object({\n+        segment: z.object({\n+          startIndex: z.number().nullish(),\n+          endIndex: z.number().nullish(),\n+          text: z.string().nullish(),\n+        }),\n+        segment_text: z.string().nullish(),\n+        groundingChunkIndices: z.array(z.number()).nullish(),\n+        supportChunkIndices: z.array(z.number()).nullish(),\n+        confidenceScores: z.array(z.number()).nullish(),\n+        confidenceScore: z.array(z.number()).nullish(),\n+      }),\n+    )\n+    .nullish(),\n+  retrievalMetadata: z\n+    .union([\n+      z.object({\n+        webDynamicRetrievalScore: z.number(),\n+      }),\n+      z.object({}),\n+    ])\n+    .nullish(),\n+});\n+\n+const factory = createProviderDefinedToolFactoryWithOutputSchema<",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2191705187",
        "repo_full_name": "vercel/ai",
        "pr_number": 7071,
        "pr_file": "packages/google/src/tool/google-search.ts",
        "discussion_id": "2191705187",
        "commented_code": "@@ -0,0 +1,61 @@\n+import { createProviderDefinedToolFactoryWithOutputSchema } from '@ai-sdk/provider-utils';\n+import { z } from 'zod/v4';\n+\n+// https://ai.google.dev/gemini-api/docs/google-search\n+// https://ai.google.dev/api/generate-content#GroundingSupport\n+// https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search\n+const groundingChunkSchema = z.object({\n+  web: z.object({ uri: z.string(), title: z.string() }).nullish(),\n+  retrievedContext: z.object({ uri: z.string(), title: z.string() }).nullish(),\n+});\n+\n+export const groundingMetadataSchema = z.object({\n+  webSearchQueries: z.array(z.string()).nullish(),\n+  retrievalQueries: z.array(z.string()).nullish(),\n+  searchEntryPoint: z.object({ renderedContent: z.string() }).nullish(),\n+  groundingChunks: z.array(groundingChunkSchema).nullish(),\n+  groundingSupports: z\n+    .array(\n+      z.object({\n+        segment: z.object({\n+          startIndex: z.number().nullish(),\n+          endIndex: z.number().nullish(),\n+          text: z.string().nullish(),\n+        }),\n+        segment_text: z.string().nullish(),\n+        groundingChunkIndices: z.array(z.number()).nullish(),\n+        supportChunkIndices: z.array(z.number()).nullish(),\n+        confidenceScores: z.array(z.number()).nullish(),\n+        confidenceScore: z.array(z.number()).nullish(),\n+      }),\n+    )\n+    .nullish(),\n+  retrievalMetadata: z\n+    .union([\n+      z.object({\n+        webDynamicRetrievalScore: z.number(),\n+      }),\n+      z.object({}),\n+    ])\n+    .nullish(),\n+});\n+\n+const factory = createProviderDefinedToolFactoryWithOutputSchema<",
        "comment_created_at": "2025-07-08T07:28:08+00:00",
        "comment_author": "lgrammel",
        "comment_body": "rn to `create...` (start function w/ verb)",
        "pr_file_module": null
      },
      {
        "comment_id": "2191903370",
        "repo_full_name": "vercel/ai",
        "pr_number": 7071,
        "pr_file": "packages/google/src/tool/google-search.ts",
        "discussion_id": "2191705187",
        "commented_code": "@@ -0,0 +1,61 @@\n+import { createProviderDefinedToolFactoryWithOutputSchema } from '@ai-sdk/provider-utils';\n+import { z } from 'zod/v4';\n+\n+// https://ai.google.dev/gemini-api/docs/google-search\n+// https://ai.google.dev/api/generate-content#GroundingSupport\n+// https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search\n+const groundingChunkSchema = z.object({\n+  web: z.object({ uri: z.string(), title: z.string() }).nullish(),\n+  retrievedContext: z.object({ uri: z.string(), title: z.string() }).nullish(),\n+});\n+\n+export const groundingMetadataSchema = z.object({\n+  webSearchQueries: z.array(z.string()).nullish(),\n+  retrievalQueries: z.array(z.string()).nullish(),\n+  searchEntryPoint: z.object({ renderedContent: z.string() }).nullish(),\n+  groundingChunks: z.array(groundingChunkSchema).nullish(),\n+  groundingSupports: z\n+    .array(\n+      z.object({\n+        segment: z.object({\n+          startIndex: z.number().nullish(),\n+          endIndex: z.number().nullish(),\n+          text: z.string().nullish(),\n+        }),\n+        segment_text: z.string().nullish(),\n+        groundingChunkIndices: z.array(z.number()).nullish(),\n+        supportChunkIndices: z.array(z.number()).nullish(),\n+        confidenceScores: z.array(z.number()).nullish(),\n+        confidenceScore: z.array(z.number()).nullish(),\n+      }),\n+    )\n+    .nullish(),\n+  retrievalMetadata: z\n+    .union([\n+      z.object({\n+        webDynamicRetrievalScore: z.number(),\n+      }),\n+      z.object({}),\n+    ])\n+    .nullish(),\n+});\n+\n+const factory = createProviderDefinedToolFactoryWithOutputSchema<",
        "comment_created_at": "2025-07-08T08:58:13+00:00",
        "comment_author": "patelvivekdev",
        "comment_body": "I have changed it to `createProviderDefinedToolFactory`, as we are not using the output result from the tools.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1849838745",
    "pr_number": 3781,
    "pr_file": "packages/openai-compatible/src/openai-compatible-provider.ts",
    "created_at": "2024-11-20T08:30:34+00:00",
    "commented_code": "+import {\n+  EmbeddingModelV1,\n+  LanguageModelV1,\n+  ProviderV1,\n+} from '@ai-sdk/provider';\n+import {\n+  FetchFunction,\n+  loadApiKey,\n+  withoutTrailingSlash,\n+} from '@ai-sdk/provider-utils';\n+import { OpenAICompatibleChatLanguageModel } from './openai-compatible-chat-language-model';\n+import { OpenAICompatibleChatSettings } from './openai-compatible-chat-settings';\n+import { OpenAICompatibleCompletionLanguageModel } from './openai-compatible-completion-language-model';\n+import { OpenAICompatibleCompletionSettings } from './openai-compatible-completion-settings';\n+import { OpenAICompatibleEmbeddingSettings } from './openai-compatible-embedding-settings';\n+import { OpenAICompatibleEmbeddingModel } from './openai-compatible-embedding-model';\n+\n+export interface OpenAICompatibleProvider<\n+  L extends string = string,\n+  C extends string = string,\n+  E extends string = string,",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "1849838745",
        "repo_full_name": "vercel/ai",
        "pr_number": 3781,
        "pr_file": "packages/openai-compatible/src/openai-compatible-provider.ts",
        "discussion_id": "1849838745",
        "commented_code": "@@ -0,0 +1,163 @@\n+import {\n+  EmbeddingModelV1,\n+  LanguageModelV1,\n+  ProviderV1,\n+} from '@ai-sdk/provider';\n+import {\n+  FetchFunction,\n+  loadApiKey,\n+  withoutTrailingSlash,\n+} from '@ai-sdk/provider-utils';\n+import { OpenAICompatibleChatLanguageModel } from './openai-compatible-chat-language-model';\n+import { OpenAICompatibleChatSettings } from './openai-compatible-chat-settings';\n+import { OpenAICompatibleCompletionLanguageModel } from './openai-compatible-completion-language-model';\n+import { OpenAICompatibleCompletionSettings } from './openai-compatible-completion-settings';\n+import { OpenAICompatibleEmbeddingSettings } from './openai-compatible-embedding-settings';\n+import { OpenAICompatibleEmbeddingModel } from './openai-compatible-embedding-model';\n+\n+export interface OpenAICompatibleProvider<\n+  L extends string = string,\n+  C extends string = string,\n+  E extends string = string,",
        "comment_created_at": "2024-11-20T08:30:34+00:00",
        "comment_author": "lgrammel",
        "comment_body": "Prefer full words for generic in upper case, e.g. `CONTEXT`",
        "pr_file_module": null
      },
      {
        "comment_id": "1851049496",
        "repo_full_name": "vercel/ai",
        "pr_number": 3781,
        "pr_file": "packages/openai-compatible/src/openai-compatible-provider.ts",
        "discussion_id": "1849838745",
        "commented_code": "@@ -0,0 +1,163 @@\n+import {\n+  EmbeddingModelV1,\n+  LanguageModelV1,\n+  ProviderV1,\n+} from '@ai-sdk/provider';\n+import {\n+  FetchFunction,\n+  loadApiKey,\n+  withoutTrailingSlash,\n+} from '@ai-sdk/provider-utils';\n+import { OpenAICompatibleChatLanguageModel } from './openai-compatible-chat-language-model';\n+import { OpenAICompatibleChatSettings } from './openai-compatible-chat-settings';\n+import { OpenAICompatibleCompletionLanguageModel } from './openai-compatible-completion-language-model';\n+import { OpenAICompatibleCompletionSettings } from './openai-compatible-completion-settings';\n+import { OpenAICompatibleEmbeddingSettings } from './openai-compatible-embedding-settings';\n+import { OpenAICompatibleEmbeddingModel } from './openai-compatible-embedding-model';\n+\n+export interface OpenAICompatibleProvider<\n+  L extends string = string,\n+  C extends string = string,\n+  E extends string = string,",
        "comment_created_at": "2024-11-20T22:14:24+00:00",
        "comment_author": "shaper",
        "comment_body": "Done.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2099240791",
    "pr_number": 6406,
    "pr_file": "packages/fal/src/fal-image-model.ts",
    "created_at": "2025-05-21T03:44:38+00:00",
    "commented_code": "const falImageSchema = z.object({\n   url: z.string(),\n-  content_type: z.string(),\n+  width: z.number().optional(),\n+  height: z.number().optional(),\n+  content_type: z.string().optional(),\n+  // e.g. https://fal.ai/models/fal-ai/flowedit/api#schema-output\n+  file_name: z.string().optional(),\n+  file_data: z.string().optional(),\n+  file_size: z.number().optional(),\n+});\n+\n+// https://fal.ai/models/fal-ai/lora/api#type-File\n+const LoraFileSchema = z.object({",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2099240791",
        "repo_full_name": "vercel/ai",
        "pr_number": 6406,
        "pr_file": "packages/fal/src/fal-image-model.ts",
        "discussion_id": "2099240791",
        "commented_code": "@@ -165,16 +197,54 @@ const falErrorSchema = z.union([falValidationErrorSchema, falHttpErrorSchema]);\n \n const falImageSchema = z.object({\n   url: z.string(),\n-  content_type: z.string(),\n+  width: z.number().optional(),\n+  height: z.number().optional(),\n+  content_type: z.string().optional(),\n+  // e.g. https://fal.ai/models/fal-ai/flowedit/api#schema-output\n+  file_name: z.string().optional(),\n+  file_data: z.string().optional(),\n+  file_size: z.number().optional(),\n+});\n+\n+// https://fal.ai/models/fal-ai/lora/api#type-File\n+const LoraFileSchema = z.object({",
        "comment_created_at": "2025-05-21T03:44:38+00:00",
        "comment_author": "lgrammel",
        "comment_body": "convention: start non-classes/types (ie regular vars like schemas) with lowercase letter",
        "pr_file_module": null
      },
      {
        "comment_id": "2099247756",
        "repo_full_name": "vercel/ai",
        "pr_number": 6406,
        "pr_file": "packages/fal/src/fal-image-model.ts",
        "discussion_id": "2099240791",
        "commented_code": "@@ -165,16 +197,54 @@ const falErrorSchema = z.union([falValidationErrorSchema, falHttpErrorSchema]);\n \n const falImageSchema = z.object({\n   url: z.string(),\n-  content_type: z.string(),\n+  width: z.number().optional(),\n+  height: z.number().optional(),\n+  content_type: z.string().optional(),\n+  // e.g. https://fal.ai/models/fal-ai/flowedit/api#schema-output\n+  file_name: z.string().optional(),\n+  file_data: z.string().optional(),\n+  file_size: z.number().optional(),\n+});\n+\n+// https://fal.ai/models/fal-ai/lora/api#type-File\n+const LoraFileSchema = z.object({",
        "comment_created_at": "2025-05-21T03:54:02+00:00",
        "comment_author": "gr2m",
        "comment_body": "resolved via 101cd0a0c",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2099249577",
    "pr_number": 6406,
    "pr_file": "packages/fal/src/fal-image-model.ts",
    "created_at": "2025-05-21T03:56:39+00:00",
    "commented_code": "timestamp: currentDate,\n         headers: responseHeaders,\n       },\n+      providerMetadata: {\n+        fal: {\n+          images: targetImages.map((image, index) => {\n+            const {\n+              url,\n+              content_type: contentType,\n+              file_name: fileName,\n+              file_data: fileData,\n+              file_size: fileSize,\n+              ...imageMetaData\n+            } = image;\n+\n+            const nsfw =\n+              value.has_nsfw_concepts?.[index] ??\n+              value.nsfw_content_detected?.[index];\n+\n+            return {\n+              ...imageMetaData,\n+              ...(contentType !== undefined ? { contentType } : undefined),\n+              ...(fileName !== undefined ? { fileName } : undefined),\n+              ...(fileData !== undefined ? { fileData } : undefined),\n+              ...(fileSize !== undefined ? { fileSize } : undefined),",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2099249577",
        "repo_full_name": "vercel/ai",
        "pr_number": 6406,
        "pr_file": "packages/fal/src/fal-image-model.ts",
        "discussion_id": "2099249577",
        "commented_code": "@@ -92,6 +109,34 @@ export class FalImageModel implements ImageModelV2 {\n         timestamp: currentDate,\n         headers: responseHeaders,\n       },\n+      providerMetadata: {\n+        fal: {\n+          images: targetImages.map((image, index) => {\n+            const {\n+              url,\n+              content_type: contentType,\n+              file_name: fileName,\n+              file_data: fileData,\n+              file_size: fileSize,\n+              ...imageMetaData\n+            } = image;\n+\n+            const nsfw =\n+              value.has_nsfw_concepts?.[index] ??\n+              value.nsfw_content_detected?.[index];\n+\n+            return {\n+              ...imageMetaData,\n+              ...(contentType !== undefined ? { contentType } : undefined),\n+              ...(fileName !== undefined ? { fileName } : undefined),\n+              ...(fileData !== undefined ? { fileData } : undefined),\n+              ...(fileSize !== undefined ? { fileSize } : undefined),",
        "comment_created_at": "2025-05-21T03:56:39+00:00",
        "comment_author": "gr2m",
        "comment_body": "I normalized some of the `providerMetaData` properties that I think the way we normalized `revised_prompt` to `revisedPrompt` in the Open AI Image model. \r\n\r\nShall I also camelize `debug_latents`, `debug_per_pass_latents`, and their respective properties?\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2099398261",
        "repo_full_name": "vercel/ai",
        "pr_number": 6406,
        "pr_file": "packages/fal/src/fal-image-model.ts",
        "discussion_id": "2099249577",
        "commented_code": "@@ -92,6 +109,34 @@ export class FalImageModel implements ImageModelV2 {\n         timestamp: currentDate,\n         headers: responseHeaders,\n       },\n+      providerMetadata: {\n+        fal: {\n+          images: targetImages.map((image, index) => {\n+            const {\n+              url,\n+              content_type: contentType,\n+              file_name: fileName,\n+              file_data: fileData,\n+              file_size: fileSize,\n+              ...imageMetaData\n+            } = image;\n+\n+            const nsfw =\n+              value.has_nsfw_concepts?.[index] ??\n+              value.nsfw_content_detected?.[index];\n+\n+            return {\n+              ...imageMetaData,\n+              ...(contentType !== undefined ? { contentType } : undefined),\n+              ...(fileName !== undefined ? { fileName } : undefined),\n+              ...(fileData !== undefined ? { fileData } : undefined),\n+              ...(fileSize !== undefined ? { fileSize } : undefined),",
        "comment_created_at": "2025-05-21T05:41:13+00:00",
        "comment_author": "lgrammel",
        "comment_body": "wonder how far we should to here. usually we do this but it is also overhead",
        "pr_file_module": null
      },
      {
        "comment_id": "2100717164",
        "repo_full_name": "vercel/ai",
        "pr_number": 6406,
        "pr_file": "packages/fal/src/fal-image-model.ts",
        "discussion_id": "2099249577",
        "commented_code": "@@ -92,6 +109,34 @@ export class FalImageModel implements ImageModelV2 {\n         timestamp: currentDate,\n         headers: responseHeaders,\n       },\n+      providerMetadata: {\n+        fal: {\n+          images: targetImages.map((image, index) => {\n+            const {\n+              url,\n+              content_type: contentType,\n+              file_name: fileName,\n+              file_data: fileData,\n+              file_size: fileSize,\n+              ...imageMetaData\n+            } = image;\n+\n+            const nsfw =\n+              value.has_nsfw_concepts?.[index] ??\n+              value.nsfw_content_detected?.[index];\n+\n+            return {\n+              ...imageMetaData,\n+              ...(contentType !== undefined ? { contentType } : undefined),\n+              ...(fileName !== undefined ? { fileName } : undefined),\n+              ...(fileData !== undefined ? { fileData } : undefined),\n+              ...(fileSize !== undefined ? { fileSize } : undefined),",
        "comment_created_at": "2025-05-21T16:26:36+00:00",
        "comment_author": "gr2m",
        "comment_body": "> wonder how far we should to here. usually we do this but it is also overhead \r\n\r\nI agree. I think we should do it for properties that we see repeatedly across providers and models. But e.g. not go overboard with things like `debug_latents` which is specific to a single model from `fal`\r\n\r\nI also wonder if we should pass through provider options that we don't handle ourselves. Otherwise we will always play catch up whenever some model from some provider exposes a new information. We could do it in a way that clearly communicates that we don't take responsibility, like `.providerMetaData.fal.unsafe_otherData`  or similar.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2071730463",
    "pr_number": 6108,
    "pr_file": "packages/ai/core/embed/embed-many.ts",
    "created_at": "2025-05-02T14:51:09+00:00",
    "commented_code": "functionality that can be fully encapsulated in the provider.\n   */\n   providerOptions?: ProviderOptions;\n+\n+  /**\n+   * Maximum number of concurrent requests.\n+   *\n+   * @default Infinity\n+   */\n+  concurrency?: number;",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2071730463",
        "repo_full_name": "vercel/ai",
        "pr_number": 6108,
        "pr_file": "packages/ai/core/embed/embed-many.ts",
        "discussion_id": "2071730463",
        "commented_code": "@@ -73,6 +74,13 @@ Only applicable for HTTP-based providers.\n   functionality that can be fully encapsulated in the provider.\n   */\n   providerOptions?: ProviderOptions;\n+\n+  /**\n+   * Maximum number of concurrent requests.\n+   *\n+   * @default Infinity\n+   */\n+  concurrency?: number;",
        "comment_created_at": "2025-05-02T14:51:09+00:00",
        "comment_author": "lgrammel",
        "comment_body": "rename to `maxParallelCalls` or `maxParallelRequests` or `maxConcurrentRequests`. If we rename to `Concurrent` here it would be good to do the same in the model spec",
        "pr_file_module": null
      },
      {
        "comment_id": "2071737240",
        "repo_full_name": "vercel/ai",
        "pr_number": 6108,
        "pr_file": "packages/ai/core/embed/embed-many.ts",
        "discussion_id": "2071730463",
        "commented_code": "@@ -73,6 +74,13 @@ Only applicable for HTTP-based providers.\n   functionality that can be fully encapsulated in the provider.\n   */\n   providerOptions?: ProviderOptions;\n+\n+  /**\n+   * Maximum number of concurrent requests.\n+   *\n+   * @default Infinity\n+   */\n+  concurrency?: number;",
        "comment_created_at": "2025-05-02T14:55:49+00:00",
        "comment_author": "samdenty",
        "comment_body": "yep that's a clearer name, updated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2060171758",
    "pr_number": 5987,
    "pr_file": "packages/ai/core/generate-object/generate-object.ts",
    "created_at": "2025-04-25T12:42:55+00:00",
    "commented_code": "currentDate?: () => Date;\n       };\n     },\n-): Promise<GenerateObjectResult<TYPE>>;\n-\n-export async function generateObject<SCHEMA, RESULT>({\n-  model,\n-  enum: enumValues, // rename bc enum is reserved by typescript\n-  schema: inputSchema,\n-  schemaName,\n-  schemaDescription,\n-  output = 'object',\n-  system,\n-  prompt,\n-  messages,\n-  maxRetries: maxRetriesArg,\n-  abortSignal,\n-  headers,\n-  experimental_repairText: repairText,\n-  experimental_telemetry: telemetry,\n-  providerOptions,\n-  _internal: {\n-    generateId = originalGenerateId,\n-    currentDate = () => new Date(),\n-  } = {},\n-  ...settings\n-}: Omit<CallSettings, 'stopSequences'> &\n-  Prompt & {\n-    /**\n-     * The expected structure of the output.\n-     *\n-     * - 'object': Generate a single object that conforms to the schema.\n-     * - 'array': Generate an array of objects that conform to the schema.\n-     * - 'no-schema': Generate any JSON object. No schema is specified.\n-     *\n-     * Default is 'object' if not specified.\n-     */\n-    output?: 'object' | 'array' | 'enum' | 'no-schema';\n-\n-    model: LanguageModel;\n-    enum?: Array<SCHEMA>;\n-    schema?: z.Schema<SCHEMA> | Schema<SCHEMA>;\n-    schemaName?: string;\n-    schemaDescription?: string;\n-    experimental_repairText?: RepairTextFunction;\n-    experimental_telemetry?: TelemetrySettings;\n-    providerOptions?: ProviderOptions;\n-\n-    /**\n-     * Internal. For test use only. May change without notice.\n-     */\n-    _internal?: {\n-      generateId?: () => string;\n-      currentDate?: () => Date;\n-    };\n-  }): Promise<GenerateObjectResult<RESULT>> {\n+): Promise<DefaultGenerateObjectResult<TYPE>> {",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2060171758",
        "repo_full_name": "vercel/ai",
        "pr_number": 5987,
        "pr_file": "packages/ai/core/generate-object/generate-object.ts",
        "discussion_id": "2060171758",
        "commented_code": "@@ -152,59 +152,33 @@ Default and recommended: 'auto' (best mode for the model).\n         currentDate?: () => Date;\n       };\n     },\n-): Promise<GenerateObjectResult<TYPE>>;\n-\n-export async function generateObject<SCHEMA, RESULT>({\n-  model,\n-  enum: enumValues, // rename bc enum is reserved by typescript\n-  schema: inputSchema,\n-  schemaName,\n-  schemaDescription,\n-  output = 'object',\n-  system,\n-  prompt,\n-  messages,\n-  maxRetries: maxRetriesArg,\n-  abortSignal,\n-  headers,\n-  experimental_repairText: repairText,\n-  experimental_telemetry: telemetry,\n-  providerOptions,\n-  _internal: {\n-    generateId = originalGenerateId,\n-    currentDate = () => new Date(),\n-  } = {},\n-  ...settings\n-}: Omit<CallSettings, 'stopSequences'> &\n-  Prompt & {\n-    /**\n-     * The expected structure of the output.\n-     *\n-     * - 'object': Generate a single object that conforms to the schema.\n-     * - 'array': Generate an array of objects that conform to the schema.\n-     * - 'no-schema': Generate any JSON object. No schema is specified.\n-     *\n-     * Default is 'object' if not specified.\n-     */\n-    output?: 'object' | 'array' | 'enum' | 'no-schema';\n-\n-    model: LanguageModel;\n-    enum?: Array<SCHEMA>;\n-    schema?: z.Schema<SCHEMA> | Schema<SCHEMA>;\n-    schemaName?: string;\n-    schemaDescription?: string;\n-    experimental_repairText?: RepairTextFunction;\n-    experimental_telemetry?: TelemetrySettings;\n-    providerOptions?: ProviderOptions;\n-\n-    /**\n-     * Internal. For test use only. May change without notice.\n-     */\n-    _internal?: {\n-      generateId?: () => string;\n-      currentDate?: () => Date;\n-    };\n-  }): Promise<GenerateObjectResult<RESULT>> {\n+): Promise<DefaultGenerateObjectResult<TYPE>> {",
        "comment_created_at": "2025-04-25T12:42:55+00:00",
        "comment_author": "lgrammel",
        "comment_body": "We should return `GenerateObjectResult<TYPE>` (not the default, that's internal)",
        "pr_file_module": null
      },
      {
        "comment_id": "2060173277",
        "repo_full_name": "vercel/ai",
        "pr_number": 5987,
        "pr_file": "packages/ai/core/generate-object/generate-object.ts",
        "discussion_id": "2060171758",
        "commented_code": "@@ -152,59 +152,33 @@ Default and recommended: 'auto' (best mode for the model).\n         currentDate?: () => Date;\n       };\n     },\n-): Promise<GenerateObjectResult<TYPE>>;\n-\n-export async function generateObject<SCHEMA, RESULT>({\n-  model,\n-  enum: enumValues, // rename bc enum is reserved by typescript\n-  schema: inputSchema,\n-  schemaName,\n-  schemaDescription,\n-  output = 'object',\n-  system,\n-  prompt,\n-  messages,\n-  maxRetries: maxRetriesArg,\n-  abortSignal,\n-  headers,\n-  experimental_repairText: repairText,\n-  experimental_telemetry: telemetry,\n-  providerOptions,\n-  _internal: {\n-    generateId = originalGenerateId,\n-    currentDate = () => new Date(),\n-  } = {},\n-  ...settings\n-}: Omit<CallSettings, 'stopSequences'> &\n-  Prompt & {\n-    /**\n-     * The expected structure of the output.\n-     *\n-     * - 'object': Generate a single object that conforms to the schema.\n-     * - 'array': Generate an array of objects that conform to the schema.\n-     * - 'no-schema': Generate any JSON object. No schema is specified.\n-     *\n-     * Default is 'object' if not specified.\n-     */\n-    output?: 'object' | 'array' | 'enum' | 'no-schema';\n-\n-    model: LanguageModel;\n-    enum?: Array<SCHEMA>;\n-    schema?: z.Schema<SCHEMA> | Schema<SCHEMA>;\n-    schemaName?: string;\n-    schemaDescription?: string;\n-    experimental_repairText?: RepairTextFunction;\n-    experimental_telemetry?: TelemetrySettings;\n-    providerOptions?: ProviderOptions;\n-\n-    /**\n-     * Internal. For test use only. May change without notice.\n-     */\n-    _internal?: {\n-      generateId?: () => string;\n-      currentDate?: () => Date;\n-    };\n-  }): Promise<GenerateObjectResult<RESULT>> {\n+): Promise<DefaultGenerateObjectResult<TYPE>> {",
        "comment_created_at": "2025-04-25T12:43:57+00:00",
        "comment_author": "samdenty",
        "comment_body": "Aha yeah I wasn't sure, will update 🙏 I think I got a type error will see why 👍",
        "pr_file_module": null
      },
      {
        "comment_id": "2060175062",
        "repo_full_name": "vercel/ai",
        "pr_number": 5987,
        "pr_file": "packages/ai/core/generate-object/generate-object.ts",
        "discussion_id": "2060171758",
        "commented_code": "@@ -152,59 +152,33 @@ Default and recommended: 'auto' (best mode for the model).\n         currentDate?: () => Date;\n       };\n     },\n-): Promise<GenerateObjectResult<TYPE>>;\n-\n-export async function generateObject<SCHEMA, RESULT>({\n-  model,\n-  enum: enumValues, // rename bc enum is reserved by typescript\n-  schema: inputSchema,\n-  schemaName,\n-  schemaDescription,\n-  output = 'object',\n-  system,\n-  prompt,\n-  messages,\n-  maxRetries: maxRetriesArg,\n-  abortSignal,\n-  headers,\n-  experimental_repairText: repairText,\n-  experimental_telemetry: telemetry,\n-  providerOptions,\n-  _internal: {\n-    generateId = originalGenerateId,\n-    currentDate = () => new Date(),\n-  } = {},\n-  ...settings\n-}: Omit<CallSettings, 'stopSequences'> &\n-  Prompt & {\n-    /**\n-     * The expected structure of the output.\n-     *\n-     * - 'object': Generate a single object that conforms to the schema.\n-     * - 'array': Generate an array of objects that conform to the schema.\n-     * - 'no-schema': Generate any JSON object. No schema is specified.\n-     *\n-     * Default is 'object' if not specified.\n-     */\n-    output?: 'object' | 'array' | 'enum' | 'no-schema';\n-\n-    model: LanguageModel;\n-    enum?: Array<SCHEMA>;\n-    schema?: z.Schema<SCHEMA> | Schema<SCHEMA>;\n-    schemaName?: string;\n-    schemaDescription?: string;\n-    experimental_repairText?: RepairTextFunction;\n-    experimental_telemetry?: TelemetrySettings;\n-    providerOptions?: ProviderOptions;\n-\n-    /**\n-     * Internal. For test use only. May change without notice.\n-     */\n-    _internal?: {\n-      generateId?: () => string;\n-      currentDate?: () => Date;\n-    };\n-  }): Promise<GenerateObjectResult<RESULT>> {\n+): Promise<DefaultGenerateObjectResult<TYPE>> {",
        "comment_created_at": "2025-04-25T12:45:14+00:00",
        "comment_author": "lgrammel",
        "comment_body": "also can you change `TYPE` to `RESULT` (which is more specific, a generic will always be a type)",
        "pr_file_module": null
      },
      {
        "comment_id": "2060248122",
        "repo_full_name": "vercel/ai",
        "pr_number": 5987,
        "pr_file": "packages/ai/core/generate-object/generate-object.ts",
        "discussion_id": "2060171758",
        "commented_code": "@@ -152,59 +152,33 @@ Default and recommended: 'auto' (best mode for the model).\n         currentDate?: () => Date;\n       };\n     },\n-): Promise<GenerateObjectResult<TYPE>>;\n-\n-export async function generateObject<SCHEMA, RESULT>({\n-  model,\n-  enum: enumValues, // rename bc enum is reserved by typescript\n-  schema: inputSchema,\n-  schemaName,\n-  schemaDescription,\n-  output = 'object',\n-  system,\n-  prompt,\n-  messages,\n-  maxRetries: maxRetriesArg,\n-  abortSignal,\n-  headers,\n-  experimental_repairText: repairText,\n-  experimental_telemetry: telemetry,\n-  providerOptions,\n-  _internal: {\n-    generateId = originalGenerateId,\n-    currentDate = () => new Date(),\n-  } = {},\n-  ...settings\n-}: Omit<CallSettings, 'stopSequences'> &\n-  Prompt & {\n-    /**\n-     * The expected structure of the output.\n-     *\n-     * - 'object': Generate a single object that conforms to the schema.\n-     * - 'array': Generate an array of objects that conform to the schema.\n-     * - 'no-schema': Generate any JSON object. No schema is specified.\n-     *\n-     * Default is 'object' if not specified.\n-     */\n-    output?: 'object' | 'array' | 'enum' | 'no-schema';\n-\n-    model: LanguageModel;\n-    enum?: Array<SCHEMA>;\n-    schema?: z.Schema<SCHEMA> | Schema<SCHEMA>;\n-    schemaName?: string;\n-    schemaDescription?: string;\n-    experimental_repairText?: RepairTextFunction;\n-    experimental_telemetry?: TelemetrySettings;\n-    providerOptions?: ProviderOptions;\n-\n-    /**\n-     * Internal. For test use only. May change without notice.\n-     */\n-    _internal?: {\n-      generateId?: () => string;\n-      currentDate?: () => Date;\n-    };\n-  }): Promise<GenerateObjectResult<RESULT>> {\n+): Promise<DefaultGenerateObjectResult<TYPE>> {",
        "comment_created_at": "2025-04-25T13:29:43+00:00",
        "comment_author": "samdenty",
        "comment_body": "done 🙏 ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2036723206",
    "pr_number": 5649,
    "pr_file": "packages/provider/src/speech-model/v1/speech-model-v1.ts",
    "created_at": "2025-04-10T07:44:49+00:00",
    "commented_code": "+import { JSONValue } from '../../json-value';\n+import { SpeechModelV1CallOptions } from './speech-model-v1-call-options';\n+import { SpeechModelV1CallWarning } from './speech-model-v1-call-warning';\n+\n+/**\n+ * Speech model specification version 1.\n+ */\n+export type SpeechModelV1 = {\n+  /**\n+   * The speech model must specify which speech model interface\n+   * version it implements. This will allow us to evolve the speech\n+   * model interface and retain backwards compatibility. The different\n+   * implementation versions can be handled as a discriminated union\n+   * on our side.\n+   */\n+  readonly specificationVersion: 'v1';\n+\n+  /**\n+   * Name of the provider for logging purposes.\n+   */\n+  readonly provider: string;\n+\n+  /**\n+   * Provider-specific model ID for logging purposes.\n+   */\n+  readonly modelId: string;\n+\n+  /**\n+   * Generates speech audio from text.\n+   */\n+  doGenerate(options: SpeechModelV1CallOptions): PromiseLike<{\n+    /**\n+     * The audio data as a binary buffer.\n+     */\n+    audioData: ArrayBuffer;\n+\n+    /**\n+     * The MIME type of the audio data.\n+     * For example: 'audio/mp3', 'audio/wav', etc.\n+     */\n+    contentType: string;",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2036723206",
        "repo_full_name": "vercel/ai",
        "pr_number": 5649,
        "pr_file": "packages/provider/src/speech-model/v1/speech-model-v1.ts",
        "discussion_id": "2036723206",
        "commented_code": "@@ -0,0 +1,91 @@\n+import { JSONValue } from '../../json-value';\n+import { SpeechModelV1CallOptions } from './speech-model-v1-call-options';\n+import { SpeechModelV1CallWarning } from './speech-model-v1-call-warning';\n+\n+/**\n+ * Speech model specification version 1.\n+ */\n+export type SpeechModelV1 = {\n+  /**\n+   * The speech model must specify which speech model interface\n+   * version it implements. This will allow us to evolve the speech\n+   * model interface and retain backwards compatibility. The different\n+   * implementation versions can be handled as a discriminated union\n+   * on our side.\n+   */\n+  readonly specificationVersion: 'v1';\n+\n+  /**\n+   * Name of the provider for logging purposes.\n+   */\n+  readonly provider: string;\n+\n+  /**\n+   * Provider-specific model ID for logging purposes.\n+   */\n+  readonly modelId: string;\n+\n+  /**\n+   * Generates speech audio from text.\n+   */\n+  doGenerate(options: SpeechModelV1CallOptions): PromiseLike<{\n+    /**\n+     * The audio data as a binary buffer.\n+     */\n+    audioData: ArrayBuffer;\n+\n+    /**\n+     * The MIME type of the audio data.\n+     * For example: 'audio/mp3', 'audio/wav', etc.\n+     */\n+    contentType: string;",
        "comment_created_at": "2025-04-10T07:44:49+00:00",
        "comment_author": "lgrammel",
        "comment_body": "rename to `mediaType` and refer to IANA media types (see other examples esp. on v5 branch)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2037439616",
    "pr_number": 5666,
    "pr_file": "packages/amazon-bedrock/src/bedrock-chat-language-model.ts",
    "created_at": "2025-04-10T13:41:04+00:00",
    "commented_code": "const { system, messages } = convertToBedrockChatMessages(prompt);\n \n-    // Parse thinking options from provider metadata\n-    const reasoningConfigOptions =\n-      BedrockReasoningConfigOptionsSchema.safeParse(\n-        providerOptions?.bedrock?.reasoning_config,",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2037439616",
        "repo_full_name": "vercel/ai",
        "pr_number": 5666,
        "pr_file": "packages/amazon-bedrock/src/bedrock-chat-language-model.ts",
        "discussion_id": "2037439616",
        "commented_code": "@@ -111,24 +116,10 @@ export class BedrockChatLanguageModel implements LanguageModelV2 {\n \n     const { system, messages } = convertToBedrockChatMessages(prompt);\n \n-    // Parse thinking options from provider metadata\n-    const reasoningConfigOptions =\n-      BedrockReasoningConfigOptionsSchema.safeParse(\n-        providerOptions?.bedrock?.reasoning_config,",
        "comment_created_at": "2025-04-10T13:41:04+00:00",
        "comment_author": "samdenty",
        "comment_body": "here we weren't using camelCase even though docs said it",
        "pr_file_module": null
      },
      {
        "comment_id": "2037442410",
        "repo_full_name": "vercel/ai",
        "pr_number": 5666,
        "pr_file": "packages/amazon-bedrock/src/bedrock-chat-language-model.ts",
        "discussion_id": "2037439616",
        "commented_code": "@@ -111,24 +116,10 @@ export class BedrockChatLanguageModel implements LanguageModelV2 {\n \n     const { system, messages } = convertToBedrockChatMessages(prompt);\n \n-    // Parse thinking options from provider metadata\n-    const reasoningConfigOptions =\n-      BedrockReasoningConfigOptionsSchema.safeParse(\n-        providerOptions?.bedrock?.reasoning_config,",
        "comment_created_at": "2025-04-10T13:41:52+00:00",
        "comment_author": "lgrammel",
        "comment_body": "let's fix that in `v5` and move to a pattern similar to groq where we parse all options first",
        "pr_file_module": null
      },
      {
        "comment_id": "2037446694",
        "repo_full_name": "vercel/ai",
        "pr_number": 5666,
        "pr_file": "packages/amazon-bedrock/src/bedrock-chat-language-model.ts",
        "discussion_id": "2037439616",
        "commented_code": "@@ -111,24 +116,10 @@ export class BedrockChatLanguageModel implements LanguageModelV2 {\n \n     const { system, messages } = convertToBedrockChatMessages(prompt);\n \n-    // Parse thinking options from provider metadata\n-    const reasoningConfigOptions =\n-      BedrockReasoningConfigOptionsSchema.safeParse(\n-        providerOptions?.bedrock?.reasoning_config,",
        "comment_created_at": "2025-04-10T13:43:10+00:00",
        "comment_author": "samdenty",
        "comment_body": "cool, done that in this PR lmk what you think about them. I'm going to pass them into AI to get it to do the rest of them hopefully",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2024199910",
    "pr_number": 5496,
    "pr_file": "packages/provider/src/transcription-model/v1/transcription-model-v1.ts",
    "created_at": "2025-04-02T07:08:28+00:00",
    "commented_code": "+import { TranscriptionModelV1CallOptions } from './transcription-model-v1-call-options';\n+import { TranscriptionModelV1CallWarning } from './transcription-model-v1-call-warning';\n+\n+/**\n+Transcription model specification version 1.\n+ */\n+export type TranscriptionModelV1 = {\n+  /**\n+The transcription model must specify which transcription model interface\n+version it implements. This will allow us to evolve the transcription\n+model interface and retain backwards compatibility. The different\n+implementation versions can be handled as a discriminated union\n+on our side.\n+   */\n+  readonly specificationVersion: 'v1';\n+\n+  /**\n+Name of the provider for logging purposes.\n+   */\n+  readonly provider: string;\n+\n+  /**\n+Provider-specific model ID for logging purposes.\n+   */\n+  readonly modelId: string;\n+\n+  /**\n+Generates an array of transcripts.\n+   */\n+  doGenerate(options: TranscriptionModelV1CallOptions): PromiseLike<{\n+    /**\n+Generated transcript as a string.\n+The transcript contains the text that was transcribed from the audio.\n+     */\n+    transcript: {\n+      text: string;\n+      segments: Array<{\n+        start: number;\n+        end: number;\n+        text: string;\n+      }>;\n+      language: string | undefined;\n+      duration: number | undefined;",
    "repo_full_name": "vercel/ai",
    "discussion_comments": [
      {
        "comment_id": "2024199910",
        "repo_full_name": "vercel/ai",
        "pr_number": 5496,
        "pr_file": "packages/provider/src/transcription-model/v1/transcription-model-v1.ts",
        "discussion_id": "2024199910",
        "commented_code": "@@ -0,0 +1,71 @@\n+import { TranscriptionModelV1CallOptions } from './transcription-model-v1-call-options';\n+import { TranscriptionModelV1CallWarning } from './transcription-model-v1-call-warning';\n+\n+/**\n+Transcription model specification version 1.\n+ */\n+export type TranscriptionModelV1 = {\n+  /**\n+The transcription model must specify which transcription model interface\n+version it implements. This will allow us to evolve the transcription\n+model interface and retain backwards compatibility. The different\n+implementation versions can be handled as a discriminated union\n+on our side.\n+   */\n+  readonly specificationVersion: 'v1';\n+\n+  /**\n+Name of the provider for logging purposes.\n+   */\n+  readonly provider: string;\n+\n+  /**\n+Provider-specific model ID for logging purposes.\n+   */\n+  readonly modelId: string;\n+\n+  /**\n+Generates an array of transcripts.\n+   */\n+  doGenerate(options: TranscriptionModelV1CallOptions): PromiseLike<{\n+    /**\n+Generated transcript as a string.\n+The transcript contains the text that was transcribed from the audio.\n+     */\n+    transcript: {\n+      text: string;\n+      segments: Array<{\n+        start: number;\n+        end: number;\n+        text: string;\n+      }>;\n+      language: string | undefined;\n+      duration: number | undefined;",
        "comment_created_at": "2025-04-02T07:08:28+00:00",
        "comment_author": "lgrammel",
        "comment_body": "what is the unit? can we include it in the var name to avoid confusion, e.g. `durationInSeconds`",
        "pr_file_module": null
      }
    ]
  }
]