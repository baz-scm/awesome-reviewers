[
  {
    "discussion_id": "2106204660",
    "pr_number": 3480,
    "pr_file": "aider/models.py",
    "created_at": "2025-05-25T13:35:11+00:00",
    "commented_code": "class Model(ModelSettings):\n+    COPY_PASTE_PREFIX = \"cp:\"\n+\n     def __init__(\n-        self, model, weak_model=None, editor_model=None, editor_edit_format=None, verbose=False\n+        self,\n+        model,\n+        weak_model=None,\n+        editor_model=None,\n+        editor_edit_format=None,\n+        verbose=False,\n+        io=None,\n     ):\n+        self.io = io\n+        self.copy_paste_instead_of_api = model.startswith(self.COPY_PASTE_PREFIX)\n+        model = model.removeprefix(self.COPY_PASTE_PREFIX)",
    "repo_full_name": "Aider-AI/aider",
    "discussion_comments": [
      {
        "comment_id": "2106204660",
        "repo_full_name": "Aider-AI/aider",
        "pr_number": 3480,
        "pr_file": "aider/models.py",
        "discussion_id": "2106204660",
        "commented_code": "@@ -304,9 +308,21 @@ def fetch_openrouter_model_info(self, model):\n \n \n class Model(ModelSettings):\n+    COPY_PASTE_PREFIX = \"cp:\"\n+\n     def __init__(\n-        self, model, weak_model=None, editor_model=None, editor_edit_format=None, verbose=False\n+        self,\n+        model,\n+        weak_model=None,\n+        editor_model=None,\n+        editor_edit_format=None,\n+        verbose=False,\n+        io=None,\n     ):\n+        self.io = io\n+        self.copy_paste_instead_of_api = model.startswith(self.COPY_PASTE_PREFIX)\n+        model = model.removeprefix(self.COPY_PASTE_PREFIX)",
        "comment_created_at": "2025-05-25T13:35:11+00:00",
        "comment_author": "robbintt",
        "comment_body": "Just thinking this through:\r\n\r\nModel parameters might still be used because Aider has defaults for each mode. However, chat modes often do not behave identically to API mode of the same model, for various reasons. It might be best to use aider defaults for that model now, which would require setting the model still.\r\n\r\n`--copy-paste` model settings may be required, otherwise aider will fallback to the user default, which may have the wrong settings.\r\n\r\nAs I was thinking through the original --copy-paste I realized that it splits the `/code` model features in a way not typically possible with API. My current understanding is that in `/code` mode, the editor model plans and codes the feature, whereas in --copy-paste, the chat model plans, and the --editor-model codes.",
        "pr_file_module": null
      },
      {
        "comment_id": "2110079882",
        "repo_full_name": "Aider-AI/aider",
        "pr_number": 3480,
        "pr_file": "aider/models.py",
        "discussion_id": "2106204660",
        "commented_code": "@@ -304,9 +308,21 @@ def fetch_openrouter_model_info(self, model):\n \n \n class Model(ModelSettings):\n+    COPY_PASTE_PREFIX = \"cp:\"\n+\n     def __init__(\n-        self, model, weak_model=None, editor_model=None, editor_edit_format=None, verbose=False\n+        self,\n+        model,\n+        weak_model=None,\n+        editor_model=None,\n+        editor_edit_format=None,\n+        verbose=False,\n+        io=None,\n     ):\n+        self.io = io\n+        self.copy_paste_instead_of_api = model.startswith(self.COPY_PASTE_PREFIX)\n+        model = model.removeprefix(self.COPY_PASTE_PREFIX)",
        "comment_created_at": "2025-05-27T19:55:01+00:00",
        "comment_author": "ther0bster",
        "comment_body": "> However, chat modes often do not behave identically to API mode of the same model, for various reasons.\r\n\r\nI agree, is there some documentation about what parameters chat models use compared to their API counterpart? would be interesting.\r\n\r\n> It might be best to use aider defaults for that model now, which would require setting the model still.\r\n\r\nwithout documentation about chat model parameters, it seems best to explicitly set the model and assume the same parameters as for the API.\r\n\r\n> As I was thinking through the original --copy-paste I realized that it splits the /code model features in a way not typically possible with API. My current understanding is that in /code mode, the editor model plans and codes the feature, whereas in --copy-paste, the chat model plans, and the --editor-model codes.\r\n\r\nyeah, the original copy-paste mode has a different architectural approach. as far as I understand, the chat model comes up with a plan, the user copy/pastes the plan, and the plan is injected into aider's chat. this chat containing the plan is sent to the editor model via the litellm API backend. and the editor model diffs are applied to the code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1565643318",
    "pr_number": 549,
    "pr_file": "aider/models/model.py",
    "created_at": "2024-04-15T11:43:40+00:00",
    "commented_code": "def create(cls, name, client=None):\n         from .openai import OpenAIModel\n         from .openrouter import OpenRouterModel\n+        from .litellm import LiteLLMModel\n \n+        if client and not hasattr(client, \"base_url\"):",
    "repo_full_name": "Aider-AI/aider",
    "discussion_comments": [
      {
        "comment_id": "1565643318",
        "repo_full_name": "Aider-AI/aider",
        "pr_number": 549,
        "pr_file": "aider/models/model.py",
        "discussion_id": "1565643318",
        "commented_code": "@@ -22,7 +22,10 @@ class Model:\n     def create(cls, name, client=None):\n         from .openai import OpenAIModel\n         from .openrouter import OpenRouterModel\n+        from .litellm import LiteLLMModel\n \n+        if client and not hasattr(client, \"base_url\"):",
        "comment_created_at": "2024-04-15T11:43:40+00:00",
        "comment_author": "nkeilar",
        "comment_body": "Not sure about this... I setup LiteLLM in docker on a VM for my team to use, the idea being we can setup the models in one place any everyone has access (and all our projects), but this would require setting a URL or I guess a hacky port forward, would be nice to be able to set a URL for litellm in-case its not running locally. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1566180641",
        "repo_full_name": "Aider-AI/aider",
        "pr_number": 549,
        "pr_file": "aider/models/model.py",
        "discussion_id": "1565643318",
        "commented_code": "@@ -22,7 +22,10 @@ class Model:\n     def create(cls, name, client=None):\n         from .openai import OpenAIModel\n         from .openrouter import OpenRouterModel\n+        from .litellm import LiteLLMModel\n \n+        if client and not hasattr(client, \"base_url\"):",
        "comment_created_at": "2024-04-15T17:12:57+00:00",
        "comment_author": "aleclarson",
        "comment_body": "This condition doesn't preclude setting a base_url for LiteLLM, but I understand why you'd think that. It's just duck-typing to detect LiteLLM client, which doesn't have a `base_url` property. There's probably a better way (with more clarity) to detect that, so I'll see what I can do.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1567812563",
    "pr_number": 549,
    "pr_file": "aider/main.py",
    "created_at": "2024-04-16T18:53:10+00:00",
    "commented_code": "io.tool_output(*map(scrub_sensitive_info, sys.argv), log_only=True)\n \n-    if not args.openai_api_key:\n-        if os.name == \"nt\":\n-            io.tool_error(\n-                \"No OpenAI API key provided. Use --openai-api-key or setx OPENAI_API_KEY.\"\n-            )\n-        else:\n-            io.tool_error(\n-                \"No OpenAI API key provided. Use --openai-api-key or export OPENAI_API_KEY.\"\n-            )\n+    if args.litellm:\n+        if not args.model:\n+            io.tool_error(\"You must specify --model or AIDER_MODEL environment variable when using --litellm.\")\n+            return 1\n+\n+    elif not (args.openai_api_key or args.openai_api_base) and \\\n+            args.model is not None and \\\n+            LITELLM_SPEC is not None:\n+        io.tool_output(f\"OpenAI key not provided, using LiteLLM instead.\")\n+        args.litellm = True\n+\n+    elif not args.openai_api_key:\n+        export_kw = \"setx\" if os.name == \"nt\" else \"export\"\n+        io.tool_error(\n+            f\"No OpenAI API key provided. Use --openai-api-key or {export_kw} OPENAI_API_KEY.\"\n+        )\n         return 1\n \n-    if args.openai_api_type == \"azure\":\n+    if not args.model:\n+        args.model = default_model\n+\n+    if args.litellm:\n+        if LITELLM_SPEC is None:\n+            io.tool_error(\"LiteLLM is not installed. Install it with `pip install litellm`.\")\n+            return 1\n+\n+        io.tool_output(\"LiteLLM is enabled.\")\n+\n+        packageRootDir = os.path.dirname(LITELLM_SPEC.origin)\n+        modelPricesBackupName = \"model_prices_and_context_window_backup.json\"\n+        modelPricesPath = os.path.join(packageRootDir, modelPricesBackupName)\n+\n+        # LiteLLM keeps a backup of the model prices for when the network is\n+        # down, but it's otherwise not used unless LITELLM_LOCAL_MODEL_COST_MAP\n+        # is set to True. To keep Aider's startup time fast, prevent the network\n+        # request unless the model prices havenâ€˜t been updated in 12 hours.\n+        from datetime import datetime, timedelta\n+        timeSinceModified = datetime.now() - datetime.fromtimestamp(os.path.getmtime(modelPricesPath))\n+\n+        if timeSinceModified < timedelta(hours=12):\n+            os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n+            io.tool_output(\"LiteLLM model list backup is recent, using it instead of network request.\")\n+        else:\n+            def strfdelta(delta):\n+                days = delta.days\n+                hours, remainder = divmod(delta.seconds, 3600)\n+                minutes, seconds = divmod(remainder, 60)\n+                time_parts = [f\"{days} day{'' if days == 1 else 's'}\" if days else \"\",\n+                                f\"{hours} hour{'' if hours == 1 else 's'}\" if hours else \"\",\n+                                f\"{minutes} minute{'' if minutes == 1 else 's'}\" if minutes else \"\",\n+                                f\"{seconds} second{'' if seconds == 1 else 's'}\"]\n+                time_parts = [part for part in time_parts if part]\n+                return \", \".join(time_parts)\n+\n+            formattedTime = strfdelta(timeSinceModified)\n+            io.tool_output(f\"LiteLLM model list backup last updated {formattedTime} ago, updating now.\")\n+\n+            # Since LiteLLM never updates its backup after hitting the\n+            # network, we'll have to do it ourselves.\n+            from litellm import model_cost\n+            with open(modelPricesPath, \"w\") as f:\n+                import json\n+                json.dump(model_cost, f)\n+\n+        # Strangely, LiteLLM has DEBUG-level logging enabled by default. This is\n+        # a rather fragile solution, so we should report the issue upstream.\n+        import logging\n+        from litellm._logging import handler\n+        handler.setLevel(logging.ERROR)\n+\n+        # Support LITELLM_API_KEY, LITELLM_BASE_URL, etc.\n+        litellm_kwargs = {\n+            key.replace(\"LITELLM_\", \"\").lower(): value\n+            for key, value in os.environ.items()\n+            if key.startswith(\"LITELLM_\")\n+        }\n+\n+        from litellm import LiteLLM\n+        client = LiteLLM(**litellm_kwargs)",
    "repo_full_name": "Aider-AI/aider",
    "discussion_comments": [
      {
        "comment_id": "1567812563",
        "repo_full_name": "Aider-AI/aider",
        "pr_number": 549,
        "pr_file": "aider/main.py",
        "discussion_id": "1567812563",
        "commented_code": "@@ -557,18 +564,86 @@ def scrub_sensitive_info(text):\n \n     io.tool_output(*map(scrub_sensitive_info, sys.argv), log_only=True)\n \n-    if not args.openai_api_key:\n-        if os.name == \"nt\":\n-            io.tool_error(\n-                \"No OpenAI API key provided. Use --openai-api-key or setx OPENAI_API_KEY.\"\n-            )\n-        else:\n-            io.tool_error(\n-                \"No OpenAI API key provided. Use --openai-api-key or export OPENAI_API_KEY.\"\n-            )\n+    if args.litellm:\n+        if not args.model:\n+            io.tool_error(\"You must specify --model or AIDER_MODEL environment variable when using --litellm.\")\n+            return 1\n+\n+    elif not (args.openai_api_key or args.openai_api_base) and \\\n+            args.model is not None and \\\n+            LITELLM_SPEC is not None:\n+        io.tool_output(f\"OpenAI key not provided, using LiteLLM instead.\")\n+        args.litellm = True\n+\n+    elif not args.openai_api_key:\n+        export_kw = \"setx\" if os.name == \"nt\" else \"export\"\n+        io.tool_error(\n+            f\"No OpenAI API key provided. Use --openai-api-key or {export_kw} OPENAI_API_KEY.\"\n+        )\n         return 1\n \n-    if args.openai_api_type == \"azure\":\n+    if not args.model:\n+        args.model = default_model\n+\n+    if args.litellm:\n+        if LITELLM_SPEC is None:\n+            io.tool_error(\"LiteLLM is not installed. Install it with `pip install litellm`.\")\n+            return 1\n+\n+        io.tool_output(\"LiteLLM is enabled.\")\n+\n+        packageRootDir = os.path.dirname(LITELLM_SPEC.origin)\n+        modelPricesBackupName = \"model_prices_and_context_window_backup.json\"\n+        modelPricesPath = os.path.join(packageRootDir, modelPricesBackupName)\n+\n+        # LiteLLM keeps a backup of the model prices for when the network is\n+        # down, but it's otherwise not used unless LITELLM_LOCAL_MODEL_COST_MAP\n+        # is set to True. To keep Aider's startup time fast, prevent the network\n+        # request unless the model prices havenâ€˜t been updated in 12 hours.\n+        from datetime import datetime, timedelta\n+        timeSinceModified = datetime.now() - datetime.fromtimestamp(os.path.getmtime(modelPricesPath))\n+\n+        if timeSinceModified < timedelta(hours=12):\n+            os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n+            io.tool_output(\"LiteLLM model list backup is recent, using it instead of network request.\")\n+        else:\n+            def strfdelta(delta):\n+                days = delta.days\n+                hours, remainder = divmod(delta.seconds, 3600)\n+                minutes, seconds = divmod(remainder, 60)\n+                time_parts = [f\"{days} day{'' if days == 1 else 's'}\" if days else \"\",\n+                                f\"{hours} hour{'' if hours == 1 else 's'}\" if hours else \"\",\n+                                f\"{minutes} minute{'' if minutes == 1 else 's'}\" if minutes else \"\",\n+                                f\"{seconds} second{'' if seconds == 1 else 's'}\"]\n+                time_parts = [part for part in time_parts if part]\n+                return \", \".join(time_parts)\n+\n+            formattedTime = strfdelta(timeSinceModified)\n+            io.tool_output(f\"LiteLLM model list backup last updated {formattedTime} ago, updating now.\")\n+\n+            # Since LiteLLM never updates its backup after hitting the\n+            # network, we'll have to do it ourselves.\n+            from litellm import model_cost\n+            with open(modelPricesPath, \"w\") as f:\n+                import json\n+                json.dump(model_cost, f)\n+\n+        # Strangely, LiteLLM has DEBUG-level logging enabled by default. This is\n+        # a rather fragile solution, so we should report the issue upstream.\n+        import logging\n+        from litellm._logging import handler\n+        handler.setLevel(logging.ERROR)\n+\n+        # Support LITELLM_API_KEY, LITELLM_BASE_URL, etc.\n+        litellm_kwargs = {\n+            key.replace(\"LITELLM_\", \"\").lower(): value\n+            for key, value in os.environ.items()\n+            if key.startswith(\"LITELLM_\")\n+        }\n+\n+        from litellm import LiteLLM\n+        client = LiteLLM(**litellm_kwargs)",
        "comment_created_at": "2024-04-16T18:53:10+00:00",
        "comment_author": "aleclarson",
        "comment_body": "@nkeilar please try setting `LITELLM_BASE_URL` environment variable and let me know if that works for you\r\n\r\n```\r\nLITELLM_BASE_URL=http://localhost:4000 aider --model gemini\r\n```",
        "pr_file_module": null
      }
    ]
  }
]