[
  {
    "discussion_id": "2163604768",
    "pr_number": 52001,
    "pr_file": "providers/google/src/airflow/providers/google/cloud/operators/cloud_logging_sink.py",
    "created_at": "2025-06-24T10:46:28+00:00",
    "commented_code": "+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING, Any\n+\n+import google.cloud.exceptions\n+from google.api_core.exceptions import AlreadyExists\n+from google.cloud import logging_v2\n+\n+from airflow.exceptions import AirflowException\n+from airflow.providers.google.cloud.hooks.cloud_logging import CloudLoggingHook\n+from airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\n+\n+if TYPE_CHECKING:\n+    from airflow.utils.context import Context\n+\n+\n+def _handle_excluison_filter(exclusion_filter):\n+    exclusion_filter_config = []\n+    if isinstance(exclusion_filter, dict):\n+        exclusion_filter_config.append(logging_v2.types.LogExclusion(**exclusion_filter))\n+    elif isinstance(exclusion_filter, list):\n+        for f in exclusion_filter:\n+            if isinstance(f, dict):\n+                exclusion_filter_config.append(logging_v2.types.LogExclusion(**f))\n+            else:\n+                exclusion_filter_config.append(f)\n+    return exclusion_filter_config\n+\n+\n+class CloudLoggingCreateSinkOperator(GoogleCloudBaseOperator):\n+    \"\"\"\n+    Creates a Cloud Logging export sink in a GCP project.\n+\n+    This operator creates a sink that exports log entries from Cloud Logging\n+    to destinations like Cloud Storage, BigQuery, or Pub/Sub.\n+\n+    :param sink_name: Required. Name of the sink to create.\n+    :param destination: Required. Destination URI. Examples:\n+        - Cloud Storage: 'storage.googleapis.com/my-bucket'\n+        - BigQuery: 'bigquery.googleapis.com/projects/my-project/datasets/my_dataset'\n+        - Pub/Sub: 'pubsub.googleapis.com/projects/my-project/topics/my-topic'\n+    :param project_id: Required. The ID of the Google Cloud project.\n+    :param filter_: Optional filter expression for selecting log entries.\n+        If None, all log entries are exported.\n+    :param exclusion_filter: Optional filter expressions for excluding logs.\n+    :param unique_writer_identity: If True, creates a unique service account for the sink.\n+    :param description: Optional description for the sink.\n+    :param disabled: If True, creates the sink in disabled state.\n+    :param bigquery_options: Optional,for destination bigquery, allow passing bigquery related configuration.\n+    :param include_children: Whether to export logs from child resources.\n+    :param gcp_conn_id: The connection ID used to connect to Google Cloud.\n+    :param impersonation_chain: Optional service account to impersonate using short-term\n+        credentials, or chained list of accounts required to get the access_token\n+        of the last account in the list, which will be impersonated in the request.\n+        If set as a string, the account must grant the originating account\n+        the Service Account Token Creator IAM role.\n+        If set as a sequence, the identities from the list must grant\n+        Service Account Token Creator IAM role to the directly preceding identity, with first\n+        account from the list granting this role to the originating account (templated).\n+    \"\"\"\n+\n+    template_fields: Sequence[str] = (\n+        \"sink_name\",\n+        \"destination\",\n+        \"filter_\",\n+        \"exclusion_filter\",\n+        \"project_id\",\n+        \"description\",\n+        \"gcp_conn_id\",\n+        \"impersonation_chain\",\n+    )\n+\n+    def __init__(\n+        self,\n+        sink_name: str,\n+        destination: str,\n+        project_id: str,\n+        filter_: str | None = None,\n+        exclusion_filter: Sequence[dict] | dict | None = None,\n+        unique_writer_identity: bool = True,\n+        description: str | None = None,\n+        disabled: bool = False,\n+        bigquery_options: dict | None = None,\n+        include_children: bool = False,\n+        gcp_conn_id: str = \"google_cloud_default\",\n+        impersonation_chain: str | Sequence[str] | None = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.sink_name = sink_name\n+        self.destination = destination\n+        self.project_id = project_id\n+        self.filter_ = filter_\n+        self.exclusion_filter = exclusion_filter\n+        self.unique_writer_identity = unique_writer_identity\n+        self.description = description\n+        self.disabled = disabled\n+        self.bigquery_options = bigquery_options\n+        self.include_children = include_children\n+        self.gcp_conn_id = gcp_conn_id\n+        self.impersonation_chain = impersonation_chain\n+\n+    def _validate_inputs(self):\n+        \"\"\"Validate required inputs.\"\"\"\n+        missing_fields = []\n+        for field_name in [\"sink_name\", \"destination\", \"project_id\"]:\n+            if not getattr(self, field_name):\n+                missing_fields.append(field_name)\n+\n+        if missing_fields:\n+            raise AirflowException(\n+                f\"Required parameters are missing: {missing_fields}. These parameters must be passed as \"\n+                \"keyword parameters or as extra fields in Airflow connection definition.\"\n+            )\n+\n+    def execute(self, context: Context) -> dict[str, Any]:\n+        \"\"\"Execute the operator.\"\"\"\n+        self._validate_inputs()\n+        hook = CloudLoggingHook(gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain)\n+\n+        client = hook.get_conn()\n+        parent = f\"projects/{self.project_id}\"\n+\n+        # Build the sink configuration\n+        sink_config = {\n+            \"name\": self.sink_name,\n+            \"destination\": self.destination,\n+            \"disabled\": self.disabled,\n+            \"include_children\": self.include_children,\n+        }\n+\n+        if self.filter_:\n+            sink_config[\"filter\"] = self.filter_\n+        if self.description:\n+            sink_config[\"description\"] = self.description\n+        if self.exclusion_filter:\n+            sink_config[\"exclusions\"] = _handle_excluison_filter(self.exclusion_filter)\n+        if self.bigquery_options:\n+            if isinstance(self.bigquery_options, dict):\n+                bigquery_options = logging_v2.types.BigQueryOptions(**self.bigquery_options)\n+            sink_config[\"bigquery_options\"] = bigquery_options\n+\n+        sink = logging_v2.types.LogSink(**sink_config)\n+\n+        try:\n+            self.log.info(\"Creating log sink '%s' in project '%s'\", self.sink_name, self.project_id)\n+            self.log.info(\"Destination: %s\", self.destination)\n+            if self.filter_:\n+                self.log.info(\"Filter: %s\", self.filter_)\n+\n+            response = client.create_sink(\n+                request={\n+                    \"parent\": parent,\n+                    \"sink\": sink,\n+                    \"unique_writer_identity\": self.unique_writer_identity,\n+                }\n+            )\n+\n+            self.log.info(\"Log sink created successfully: %s\", response.name)\n+\n+            if self.unique_writer_identity and hasattr(response, \"writer_identity\"):\n+                self.log.info(\"Writer identity: %s\", response.writer_identity)\n+                self.log.info(\"Remember to grant appropriate permissions to the writer identity\")\n+\n+            return logging_v2.types.LogSink.to_dict(response)\n+\n+        except AlreadyExists:\n+            self.log.info(\n+                \"Already existed log sink, sink_name=%s, project_id=%s\",\n+                self.sink_name,\n+                self.project_id,\n+            )\n+            sink_path = f\"projects/{self.project_id}/sinks/{self.sink_name}\"\n+            existing_sink = client.get_sink(request={\"sink_name\": sink_path})\n+            return logging_v2.types.LogSink.to_dict(existing_sink)\n+\n+        except google.cloud.exceptions.GoogleCloudError as e:\n+            self.log.error(\"An error occurred. Exiting.\")\n+            raise e\n+\n+\n+class CloudLoggingDeleteSinkOperator(GoogleCloudBaseOperator):\n+    \"\"\"\n+    Deletes a Cloud Logging export sink from a GCP project.\n+\n+    :param sink_name: Required. Name of the sink to delete.\n+    :param project_id: Required. The ID of the Google Cloud project.\n+    :param gcp_conn_id: The connection ID used to connect to Google Cloud.\n+    :param impersonation_chain: Optional service account to impersonate using short-term\n+        credentials, or chained list of accounts required to get the access_token\n+        of the last account in the list, which will be impersonated in the request.\n+        If set as a string, the account must grant the originating account\n+        the Service Account Token Creator IAM role.\n+        If set as a sequence, the identities from the list must grant\n+        Service Account Token Creator IAM role to the directly preceding identity, with first\n+        account from the list granting this role to the originating account (templated).\n+    \"\"\"\n+\n+    template_fields: Sequence[str] = (\"sink_name\", \"project_id\", \"gcp_conn_id\", \"impersonation_chain\")\n+\n+    def __init__(\n+        self,\n+        sink_name: str,\n+        project_id: str,\n+        gcp_conn_id: str = \"google_cloud_default\",\n+        impersonation_chain: str | Sequence[str] | None = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.sink_name = sink_name\n+        self.project_id = project_id\n+        self.gcp_conn_id = gcp_conn_id\n+        self.impersonation_chain = impersonation_chain\n+\n+    def _validate_inputs(self):\n+        \"\"\"Validate required inputs.\"\"\"\n+        missing_fields = []\n+        for field_name in [\"sink_name\", \"project_id\"]:\n+            if not getattr(self, field_name):\n+                missing_fields.append(field_name)\n+\n+        if missing_fields:\n+            raise AirflowException(\n+                f\"Required parameters are missing: {missing_fields}. These parameters must be passed as \"\n+                \"keyword parameters or as extra fields in Airflow connection definition.\"\n+            )\n+\n+    def execute(self, context: Context) -> dict[str, Any]:\n+        \"\"\"Execute the operator.\"\"\"\n+        self._validate_inputs()\n+        hook = CloudLoggingHook(gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain)\n+\n+        client = hook.get_conn()\n+        sink_path = f\"projects/{self.project_id}/sinks/{self.sink_name}\"\n+\n+        try:\n+            sink_to_delete = client.get_sink(request={\"sink_name\": sink_path})\n+\n+            self.log.info(\"Deleting log sink '%s' from project '%s'\", self.sink_name, self.project_id)\n+            client.delete_sink(request={\"sink_name\": sink_path})\n+            self.log.info(\"Log sink '%s' deleted successfully\", self.sink_name)\n+\n+            return logging_v2.types.LogSink.to_dict(sink_to_delete)\n+\n+        except google.cloud.exceptions.NotFound as e:\n+            self.log.error(\"An error occurred. Not Found.\")\n+            raise e\n+        except google.cloud.exceptions.GoogleCloudError as e:\n+            self.log.error(\"An error occurred. Exiting.\")\n+            raise e\n+\n+\n+class CloudLoggingUpdateSinkOperator(GoogleCloudBaseOperator):\n+    \"\"\"\n+    Updates an existing Cloud Logging export sink.\n+\n+    :param sink_name: Required. Name of the sink to update.\n+    :param project_id: Required. The ID of the Google Cloud project.\n+    :param destination: New destination URI.\n+    :param filter_: New filter expression for selecting log entries.\n+    :param exclusion_filter: New exclusion filter. It will override old exclusion filter\n+    :param description: New description for the sink.\n+    :param disabled: Whether to disable/enable the sink.\n+    :param bigquery_options: New bigquery related configuration.\n+    :param include_children: Whether to export logs from child resources.\n+    :param unique_writer_identity: If True, updates the writer identity.\n+    :param gcp_conn_id: The connection ID used to connect to Google Cloud.\n+    :param impersonation_chain: Optional service account to impersonate using short-term\n+        credentials, or chained list of accounts required to get the access_token\n+        of the last account in the list, which will be impersonated in the request.\n+        If set as a string, the account must grant the originating account\n+        the Service Account Token Creator IAM role.\n+        If set as a sequence, the identities from the list must grant\n+        Service Account Token Creator IAM role to the directly preceding identity, with first\n+        account from the list granting this role to the originating account (templated).\n+    \"\"\"\n+\n+    template_fields: Sequence[str] = (\n+        \"sink_name\",\n+        \"destination\",\n+        \"filter_\",\n+        \"exclusion_filter\",\n+        \"project_id\",\n+        \"description\",\n+        \"gcp_conn_id\",\n+        \"impersonation_chain\",\n+    )\n+\n+    def __init__(\n+        self,\n+        sink_name: str,\n+        project_id: str,\n+        destination: str | None = None,\n+        filter_: str | None = None,\n+        exclusion_filter: Sequence[dict] | dict | None = None,\n+        description: str | None = None,\n+        disabled: bool | None = None,\n+        bigquery_options: dict | None = None,\n+        include_children: bool | None = None,\n+        unique_writer_identity: bool = False,\n+        gcp_conn_id: str = \"google_cloud_default\",\n+        impersonation_chain: str | Sequence[str] | None = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.sink_name = sink_name\n+        self.project_id = project_id\n+        self.destination = destination\n+        self.filter_ = filter_\n+        self.exclusion_filter = exclusion_filter\n+        self.description = description\n+        self.disabled = disabled\n+        self.bigquery_options = bigquery_options\n+        self.include_children = include_children\n+        self.unique_writer_identity = unique_writer_identity\n+        self.gcp_conn_id = gcp_conn_id\n+        self.impersonation_chain = impersonation_chain\n+\n+    def _validate_inputs(self):",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2163604768",
        "repo_full_name": "apache/airflow",
        "pr_number": 52001,
        "pr_file": "providers/google/src/airflow/providers/google/cloud/operators/cloud_logging_sink.py",
        "discussion_id": "2163604768",
        "commented_code": "@@ -0,0 +1,490 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING, Any\n+\n+import google.cloud.exceptions\n+from google.api_core.exceptions import AlreadyExists\n+from google.cloud import logging_v2\n+\n+from airflow.exceptions import AirflowException\n+from airflow.providers.google.cloud.hooks.cloud_logging import CloudLoggingHook\n+from airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\n+\n+if TYPE_CHECKING:\n+    from airflow.utils.context import Context\n+\n+\n+def _handle_excluison_filter(exclusion_filter):\n+    exclusion_filter_config = []\n+    if isinstance(exclusion_filter, dict):\n+        exclusion_filter_config.append(logging_v2.types.LogExclusion(**exclusion_filter))\n+    elif isinstance(exclusion_filter, list):\n+        for f in exclusion_filter:\n+            if isinstance(f, dict):\n+                exclusion_filter_config.append(logging_v2.types.LogExclusion(**f))\n+            else:\n+                exclusion_filter_config.append(f)\n+    return exclusion_filter_config\n+\n+\n+class CloudLoggingCreateSinkOperator(GoogleCloudBaseOperator):\n+    \"\"\"\n+    Creates a Cloud Logging export sink in a GCP project.\n+\n+    This operator creates a sink that exports log entries from Cloud Logging\n+    to destinations like Cloud Storage, BigQuery, or Pub/Sub.\n+\n+    :param sink_name: Required. Name of the sink to create.\n+    :param destination: Required. Destination URI. Examples:\n+        - Cloud Storage: 'storage.googleapis.com/my-bucket'\n+        - BigQuery: 'bigquery.googleapis.com/projects/my-project/datasets/my_dataset'\n+        - Pub/Sub: 'pubsub.googleapis.com/projects/my-project/topics/my-topic'\n+    :param project_id: Required. The ID of the Google Cloud project.\n+    :param filter_: Optional filter expression for selecting log entries.\n+        If None, all log entries are exported.\n+    :param exclusion_filter: Optional filter expressions for excluding logs.\n+    :param unique_writer_identity: If True, creates a unique service account for the sink.\n+    :param description: Optional description for the sink.\n+    :param disabled: If True, creates the sink in disabled state.\n+    :param bigquery_options: Optional,for destination bigquery, allow passing bigquery related configuration.\n+    :param include_children: Whether to export logs from child resources.\n+    :param gcp_conn_id: The connection ID used to connect to Google Cloud.\n+    :param impersonation_chain: Optional service account to impersonate using short-term\n+        credentials, or chained list of accounts required to get the access_token\n+        of the last account in the list, which will be impersonated in the request.\n+        If set as a string, the account must grant the originating account\n+        the Service Account Token Creator IAM role.\n+        If set as a sequence, the identities from the list must grant\n+        Service Account Token Creator IAM role to the directly preceding identity, with first\n+        account from the list granting this role to the originating account (templated).\n+    \"\"\"\n+\n+    template_fields: Sequence[str] = (\n+        \"sink_name\",\n+        \"destination\",\n+        \"filter_\",\n+        \"exclusion_filter\",\n+        \"project_id\",\n+        \"description\",\n+        \"gcp_conn_id\",\n+        \"impersonation_chain\",\n+    )\n+\n+    def __init__(\n+        self,\n+        sink_name: str,\n+        destination: str,\n+        project_id: str,\n+        filter_: str | None = None,\n+        exclusion_filter: Sequence[dict] | dict | None = None,\n+        unique_writer_identity: bool = True,\n+        description: str | None = None,\n+        disabled: bool = False,\n+        bigquery_options: dict | None = None,\n+        include_children: bool = False,\n+        gcp_conn_id: str = \"google_cloud_default\",\n+        impersonation_chain: str | Sequence[str] | None = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.sink_name = sink_name\n+        self.destination = destination\n+        self.project_id = project_id\n+        self.filter_ = filter_\n+        self.exclusion_filter = exclusion_filter\n+        self.unique_writer_identity = unique_writer_identity\n+        self.description = description\n+        self.disabled = disabled\n+        self.bigquery_options = bigquery_options\n+        self.include_children = include_children\n+        self.gcp_conn_id = gcp_conn_id\n+        self.impersonation_chain = impersonation_chain\n+\n+    def _validate_inputs(self):\n+        \"\"\"Validate required inputs.\"\"\"\n+        missing_fields = []\n+        for field_name in [\"sink_name\", \"destination\", \"project_id\"]:\n+            if not getattr(self, field_name):\n+                missing_fields.append(field_name)\n+\n+        if missing_fields:\n+            raise AirflowException(\n+                f\"Required parameters are missing: {missing_fields}. These parameters must be passed as \"\n+                \"keyword parameters or as extra fields in Airflow connection definition.\"\n+            )\n+\n+    def execute(self, context: Context) -> dict[str, Any]:\n+        \"\"\"Execute the operator.\"\"\"\n+        self._validate_inputs()\n+        hook = CloudLoggingHook(gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain)\n+\n+        client = hook.get_conn()\n+        parent = f\"projects/{self.project_id}\"\n+\n+        # Build the sink configuration\n+        sink_config = {\n+            \"name\": self.sink_name,\n+            \"destination\": self.destination,\n+            \"disabled\": self.disabled,\n+            \"include_children\": self.include_children,\n+        }\n+\n+        if self.filter_:\n+            sink_config[\"filter\"] = self.filter_\n+        if self.description:\n+            sink_config[\"description\"] = self.description\n+        if self.exclusion_filter:\n+            sink_config[\"exclusions\"] = _handle_excluison_filter(self.exclusion_filter)\n+        if self.bigquery_options:\n+            if isinstance(self.bigquery_options, dict):\n+                bigquery_options = logging_v2.types.BigQueryOptions(**self.bigquery_options)\n+            sink_config[\"bigquery_options\"] = bigquery_options\n+\n+        sink = logging_v2.types.LogSink(**sink_config)\n+\n+        try:\n+            self.log.info(\"Creating log sink '%s' in project '%s'\", self.sink_name, self.project_id)\n+            self.log.info(\"Destination: %s\", self.destination)\n+            if self.filter_:\n+                self.log.info(\"Filter: %s\", self.filter_)\n+\n+            response = client.create_sink(\n+                request={\n+                    \"parent\": parent,\n+                    \"sink\": sink,\n+                    \"unique_writer_identity\": self.unique_writer_identity,\n+                }\n+            )\n+\n+            self.log.info(\"Log sink created successfully: %s\", response.name)\n+\n+            if self.unique_writer_identity and hasattr(response, \"writer_identity\"):\n+                self.log.info(\"Writer identity: %s\", response.writer_identity)\n+                self.log.info(\"Remember to grant appropriate permissions to the writer identity\")\n+\n+            return logging_v2.types.LogSink.to_dict(response)\n+\n+        except AlreadyExists:\n+            self.log.info(\n+                \"Already existed log sink, sink_name=%s, project_id=%s\",\n+                self.sink_name,\n+                self.project_id,\n+            )\n+            sink_path = f\"projects/{self.project_id}/sinks/{self.sink_name}\"\n+            existing_sink = client.get_sink(request={\"sink_name\": sink_path})\n+            return logging_v2.types.LogSink.to_dict(existing_sink)\n+\n+        except google.cloud.exceptions.GoogleCloudError as e:\n+            self.log.error(\"An error occurred. Exiting.\")\n+            raise e\n+\n+\n+class CloudLoggingDeleteSinkOperator(GoogleCloudBaseOperator):\n+    \"\"\"\n+    Deletes a Cloud Logging export sink from a GCP project.\n+\n+    :param sink_name: Required. Name of the sink to delete.\n+    :param project_id: Required. The ID of the Google Cloud project.\n+    :param gcp_conn_id: The connection ID used to connect to Google Cloud.\n+    :param impersonation_chain: Optional service account to impersonate using short-term\n+        credentials, or chained list of accounts required to get the access_token\n+        of the last account in the list, which will be impersonated in the request.\n+        If set as a string, the account must grant the originating account\n+        the Service Account Token Creator IAM role.\n+        If set as a sequence, the identities from the list must grant\n+        Service Account Token Creator IAM role to the directly preceding identity, with first\n+        account from the list granting this role to the originating account (templated).\n+    \"\"\"\n+\n+    template_fields: Sequence[str] = (\"sink_name\", \"project_id\", \"gcp_conn_id\", \"impersonation_chain\")\n+\n+    def __init__(\n+        self,\n+        sink_name: str,\n+        project_id: str,\n+        gcp_conn_id: str = \"google_cloud_default\",\n+        impersonation_chain: str | Sequence[str] | None = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.sink_name = sink_name\n+        self.project_id = project_id\n+        self.gcp_conn_id = gcp_conn_id\n+        self.impersonation_chain = impersonation_chain\n+\n+    def _validate_inputs(self):\n+        \"\"\"Validate required inputs.\"\"\"\n+        missing_fields = []\n+        for field_name in [\"sink_name\", \"project_id\"]:\n+            if not getattr(self, field_name):\n+                missing_fields.append(field_name)\n+\n+        if missing_fields:\n+            raise AirflowException(\n+                f\"Required parameters are missing: {missing_fields}. These parameters must be passed as \"\n+                \"keyword parameters or as extra fields in Airflow connection definition.\"\n+            )\n+\n+    def execute(self, context: Context) -> dict[str, Any]:\n+        \"\"\"Execute the operator.\"\"\"\n+        self._validate_inputs()\n+        hook = CloudLoggingHook(gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain)\n+\n+        client = hook.get_conn()\n+        sink_path = f\"projects/{self.project_id}/sinks/{self.sink_name}\"\n+\n+        try:\n+            sink_to_delete = client.get_sink(request={\"sink_name\": sink_path})\n+\n+            self.log.info(\"Deleting log sink '%s' from project '%s'\", self.sink_name, self.project_id)\n+            client.delete_sink(request={\"sink_name\": sink_path})\n+            self.log.info(\"Log sink '%s' deleted successfully\", self.sink_name)\n+\n+            return logging_v2.types.LogSink.to_dict(sink_to_delete)\n+\n+        except google.cloud.exceptions.NotFound as e:\n+            self.log.error(\"An error occurred. Not Found.\")\n+            raise e\n+        except google.cloud.exceptions.GoogleCloudError as e:\n+            self.log.error(\"An error occurred. Exiting.\")\n+            raise e\n+\n+\n+class CloudLoggingUpdateSinkOperator(GoogleCloudBaseOperator):\n+    \"\"\"\n+    Updates an existing Cloud Logging export sink.\n+\n+    :param sink_name: Required. Name of the sink to update.\n+    :param project_id: Required. The ID of the Google Cloud project.\n+    :param destination: New destination URI.\n+    :param filter_: New filter expression for selecting log entries.\n+    :param exclusion_filter: New exclusion filter. It will override old exclusion filter\n+    :param description: New description for the sink.\n+    :param disabled: Whether to disable/enable the sink.\n+    :param bigquery_options: New bigquery related configuration.\n+    :param include_children: Whether to export logs from child resources.\n+    :param unique_writer_identity: If True, updates the writer identity.\n+    :param gcp_conn_id: The connection ID used to connect to Google Cloud.\n+    :param impersonation_chain: Optional service account to impersonate using short-term\n+        credentials, or chained list of accounts required to get the access_token\n+        of the last account in the list, which will be impersonated in the request.\n+        If set as a string, the account must grant the originating account\n+        the Service Account Token Creator IAM role.\n+        If set as a sequence, the identities from the list must grant\n+        Service Account Token Creator IAM role to the directly preceding identity, with first\n+        account from the list granting this role to the originating account (templated).\n+    \"\"\"\n+\n+    template_fields: Sequence[str] = (\n+        \"sink_name\",\n+        \"destination\",\n+        \"filter_\",\n+        \"exclusion_filter\",\n+        \"project_id\",\n+        \"description\",\n+        \"gcp_conn_id\",\n+        \"impersonation_chain\",\n+    )\n+\n+    def __init__(\n+        self,\n+        sink_name: str,\n+        project_id: str,\n+        destination: str | None = None,\n+        filter_: str | None = None,\n+        exclusion_filter: Sequence[dict] | dict | None = None,\n+        description: str | None = None,\n+        disabled: bool | None = None,\n+        bigquery_options: dict | None = None,\n+        include_children: bool | None = None,\n+        unique_writer_identity: bool = False,\n+        gcp_conn_id: str = \"google_cloud_default\",\n+        impersonation_chain: str | Sequence[str] | None = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.sink_name = sink_name\n+        self.project_id = project_id\n+        self.destination = destination\n+        self.filter_ = filter_\n+        self.exclusion_filter = exclusion_filter\n+        self.description = description\n+        self.disabled = disabled\n+        self.bigquery_options = bigquery_options\n+        self.include_children = include_children\n+        self.unique_writer_identity = unique_writer_identity\n+        self.gcp_conn_id = gcp_conn_id\n+        self.impersonation_chain = impersonation_chain\n+\n+    def _validate_inputs(self):",
        "comment_created_at": "2025-06-24T10:46:28+00:00",
        "comment_author": "VladaZakharova",
        "comment_body": "if we have some fields in common for verifying, I would suggest making one function and then override it with the parameters we need to check. But in general I see that mostly we are checking sink_name and project_id in the operators, so we can definitely move this outside the operator and just call it to not duplicate the code",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2151424114",
    "pr_number": 49470,
    "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
    "created_at": "2025-06-17T06:28:05+00:00",
    "commented_code": "log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n \n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n+def _sort_key(timestamp: datetime | None, line_num: int) -> int:\n+    \"\"\"\n+    Generate a sort key for log record, to be used in K-way merge.\n \n-    records = itertools.chain.from_iterable(_parse_log_lines(log) for log in logs)\n+    :param timestamp: timestamp of the log line\n+    :param line_num: line number of the log line\n+    :return: a integer as sort key to avoid overhead of memory usage\n+    \"\"\"\n+    return int((timestamp or DEFAULT_SORT_DATETIME).timestamp() * 1000) * SORT_KEY_OFFSET + line_num\n+\n+\n+def _is_sort_key_with_default_timestamp(sort_key: int) -> bool:\n+    \"\"\"\n+    Check if the sort key was generated with the DEFAULT_SORT_TIMESTAMP.\n+\n+    This is used to identify log records that don't have timestamp.\n+\n+    :param sort_key: The sort key to check\n+    :return: True if the sort key was generated with DEFAULT_SORT_TIMESTAMP, False otherwise\n+    \"\"\"\n+    # Extract the timestamp part from the sort key (remove the line number part)\n+    timestamp_part = sort_key // SORT_KEY_OFFSET\n+    return timestamp_part == DEFAULT_SORT_TIMESTAMP\n+\n+\n+def _add_log_from_parsed_log_streams_to_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    parsed_log_streams: dict[int, ParsedLogStream],\n+) -> None:\n+    \"\"\"\n+    Add one log record from each parsed log stream to the heap, and will remove empty log stream from the dict after iterating.\n+\n+    :param heap: heap to store log records\n+    :param parsed_log_streams: dict of parsed log streams\n+    \"\"\"\n+    log_stream_to_remove: list[int] | None = None\n+    for idx, log_stream in parsed_log_streams.items():\n+        record: ParsedLog | None = next(log_stream, None)\n+        if record is None:\n+            if log_stream_to_remove is None:\n+                log_stream_to_remove = []\n+            log_stream_to_remove.append(idx)\n+            continue\n+        # add type hint to avoid mypy error\n+        record = cast(\"ParsedLog\", record)\n+        timestamp, line_num, line = record\n+        # take int as sort key to avoid overhead of memory usage\n+        heapq.heappush(heap, (_sort_key(timestamp, line_num), line))\n+    # remove empty log stream from the dict\n+    if log_stream_to_remove is not None:\n+        for idx in log_stream_to_remove:\n+            del parsed_log_streams[idx]\n+\n+\n+def _interleave_logs(*log_streams: RawLogStream) -> StructuredLogStream:\n+    \"\"\"\n+    Merge parsed log streams using K-way merge.\n+\n+    By yielding HALF_CHUNK_SIZE records when heap size exceeds CHUNK_SIZE, we can reduce the chance of messing up the global order.\n+    Since there are multiple log streams, we can't guarantee that the records are in global order.\n+\n+    e.g.\n+\n+    log_stream1: ----------\n+    log_stream2:   ----\n+    log_stream3:     --------\n+\n+    The first record of log_stream3 is later than the fourth record of log_stream1 !\n+    :param parsed_log_streams: parsed log streams\n+    :return: interleaved log stream\n+    \"\"\"\n+    # don't need to push whole tuple into heap, which increases too much overhead\n+    # push only sort_key and line into heap\n+    heap: list[tuple[int, StructuredLogMessage]] = []\n+    # to allow removing empty streams while iterating, also turn the str stream into parsed log stream\n+    parsed_log_streams: dict[int, ParsedLogStream] = {\n+        idx: _log_stream_to_parsed_log_stream(log_stream) for idx, log_stream in enumerate(log_streams)\n+    }\n+\n+    # keep adding records from logs until all logs are empty\n     last = None\n-    for timestamp, _, msg in sorted(records, key=lambda x: (x[0] or min_date, x[1])):\n-        if msg != last or not timestamp:  # dedupe\n-            yield msg\n-        last = msg\n+    while parsed_log_streams:\n+        _add_log_from_parsed_log_streams_to_heap(heap, parsed_log_streams)\n+\n+        # yield HALF_HEAP_DUMP_SIZE records when heap size exceeds HEAP_DUMP_SIZE\n+        if len(heap) >= HEAP_DUMP_SIZE:\n+            for _ in range(HALF_HEAP_DUMP_SIZE):\n+                sort_key, line = heapq.heappop(heap)\n+                if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+                    yield line\n+                last = line\n+\n+    # yield remaining records\n+    for _ in range(len(heap)):\n+        sort_key, line = heapq.heappop(heap)\n+        if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+            yield line\n+        last = line",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2151424114",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
        "discussion_id": "2151424114",
        "commented_code": "@@ -166,17 +242,133 @@ def _parse_log_lines(\n                 log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n \n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n+def _sort_key(timestamp: datetime | None, line_num: int) -> int:\n+    \"\"\"\n+    Generate a sort key for log record, to be used in K-way merge.\n \n-    records = itertools.chain.from_iterable(_parse_log_lines(log) for log in logs)\n+    :param timestamp: timestamp of the log line\n+    :param line_num: line number of the log line\n+    :return: a integer as sort key to avoid overhead of memory usage\n+    \"\"\"\n+    return int((timestamp or DEFAULT_SORT_DATETIME).timestamp() * 1000) * SORT_KEY_OFFSET + line_num\n+\n+\n+def _is_sort_key_with_default_timestamp(sort_key: int) -> bool:\n+    \"\"\"\n+    Check if the sort key was generated with the DEFAULT_SORT_TIMESTAMP.\n+\n+    This is used to identify log records that don't have timestamp.\n+\n+    :param sort_key: The sort key to check\n+    :return: True if the sort key was generated with DEFAULT_SORT_TIMESTAMP, False otherwise\n+    \"\"\"\n+    # Extract the timestamp part from the sort key (remove the line number part)\n+    timestamp_part = sort_key // SORT_KEY_OFFSET\n+    return timestamp_part == DEFAULT_SORT_TIMESTAMP\n+\n+\n+def _add_log_from_parsed_log_streams_to_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    parsed_log_streams: dict[int, ParsedLogStream],\n+) -> None:\n+    \"\"\"\n+    Add one log record from each parsed log stream to the heap, and will remove empty log stream from the dict after iterating.\n+\n+    :param heap: heap to store log records\n+    :param parsed_log_streams: dict of parsed log streams\n+    \"\"\"\n+    log_stream_to_remove: list[int] | None = None\n+    for idx, log_stream in parsed_log_streams.items():\n+        record: ParsedLog | None = next(log_stream, None)\n+        if record is None:\n+            if log_stream_to_remove is None:\n+                log_stream_to_remove = []\n+            log_stream_to_remove.append(idx)\n+            continue\n+        # add type hint to avoid mypy error\n+        record = cast(\"ParsedLog\", record)\n+        timestamp, line_num, line = record\n+        # take int as sort key to avoid overhead of memory usage\n+        heapq.heappush(heap, (_sort_key(timestamp, line_num), line))\n+    # remove empty log stream from the dict\n+    if log_stream_to_remove is not None:\n+        for idx in log_stream_to_remove:\n+            del parsed_log_streams[idx]\n+\n+\n+def _interleave_logs(*log_streams: RawLogStream) -> StructuredLogStream:\n+    \"\"\"\n+    Merge parsed log streams using K-way merge.\n+\n+    By yielding HALF_CHUNK_SIZE records when heap size exceeds CHUNK_SIZE, we can reduce the chance of messing up the global order.\n+    Since there are multiple log streams, we can't guarantee that the records are in global order.\n+\n+    e.g.\n+\n+    log_stream1: ----------\n+    log_stream2:   ----\n+    log_stream3:     --------\n+\n+    The first record of log_stream3 is later than the fourth record of log_stream1 !\n+    :param parsed_log_streams: parsed log streams\n+    :return: interleaved log stream\n+    \"\"\"\n+    # don't need to push whole tuple into heap, which increases too much overhead\n+    # push only sort_key and line into heap\n+    heap: list[tuple[int, StructuredLogMessage]] = []\n+    # to allow removing empty streams while iterating, also turn the str stream into parsed log stream\n+    parsed_log_streams: dict[int, ParsedLogStream] = {\n+        idx: _log_stream_to_parsed_log_stream(log_stream) for idx, log_stream in enumerate(log_streams)\n+    }\n+\n+    # keep adding records from logs until all logs are empty\n     last = None\n-    for timestamp, _, msg in sorted(records, key=lambda x: (x[0] or min_date, x[1])):\n-        if msg != last or not timestamp:  # dedupe\n-            yield msg\n-        last = msg\n+    while parsed_log_streams:\n+        _add_log_from_parsed_log_streams_to_heap(heap, parsed_log_streams)\n+\n+        # yield HALF_HEAP_DUMP_SIZE records when heap size exceeds HEAP_DUMP_SIZE\n+        if len(heap) >= HEAP_DUMP_SIZE:\n+            for _ in range(HALF_HEAP_DUMP_SIZE):\n+                sort_key, line = heapq.heappop(heap)\n+                if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+                    yield line\n+                last = line\n+\n+    # yield remaining records\n+    for _ in range(len(heap)):\n+        sort_key, line = heapq.heappop(heap)\n+        if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+            yield line\n+        last = line",
        "comment_created_at": "2025-06-17T06:28:05+00:00",
        "comment_author": "Lee-W",
        "comment_body": "```suggestion\r\n        sort_key, line = heapq.heappop(heap)\r\n        if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\r\n            yield line\r\n        last = line\r\n```\r\n\r\nnit: we might be able to extract it as a function as it looks similar to the lines above",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2154373428",
    "pr_number": 51780,
    "pr_file": "task-sdk/src/airflow/sdk/execution_time/task_runner.py",
    "created_at": "2025-06-18T11:35:55+00:00",
    "commented_code": "ti.log_url = get_log_url_from_ti(ti)\n     log.debug(\"DAG file parsed\", file=msg.dag_rel_path)\n \n+    try:\n+        run_as_user = getattr(ti.task, \"run_as_user\", None) or conf.get(\"core\", \"default_impersonation\")\n+    except AirflowConfigException:\n+        run_as_user = None\n+\n+    if os.environ.get(\"_AIRFLOW__REEXECUTED_PROCESS\") != \"1\" and run_as_user:\n+        # enters here for re-exec process\n+        os.environ[\"_AIRFLOW__REEXECUTED_PROCESS\"] = \"1\"\n+        # store startup message in environment for re-exec process\n+        os.environ[\"_AIRFLOW__STARTUP_MSG\"] = msg.model_dump_json()\n+        os.set_inheritable(SUPERVISOR_COMMS.socket.fileno(), True)\n+\n+        # Import main directly from the module instead of re-executing the file.\n+        # This ensures that when other parts modules import\n+        # airflow.sdk.execution_time.task_runner, they get the same module instance\n+        # with the properly initialized SUPERVISOR_COMMS global variable.\n+        # If we re-executed the script, it would load as __main__ and future\n+        # imports would get a fresh copy without the initialized globals.\n+        rexec_python_code = \"from airflow.sdk.execution_time.task_runner import main; main()\"\n+        log.info(\n+            \"Running command\",\n+            command=[\"sudo\", \"-E\", \"-H\", \"-u\", run_as_user, sys.executable, \"-c\", rexec_python_code],\n+        )\n+        os.execvp(\"sudo\", [\"sudo\", \"-E\", \"-H\", \"-u\", run_as_user, sys.executable, \"-c\", rexec_python_code])",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2154373428",
        "repo_full_name": "apache/airflow",
        "pr_number": 51780,
        "pr_file": "task-sdk/src/airflow/sdk/execution_time/task_runner.py",
        "discussion_id": "2154373428",
        "commented_code": "@@ -677,6 +685,34 @@ def startup() -> tuple[RuntimeTaskInstance, Context, Logger]:\n         ti.log_url = get_log_url_from_ti(ti)\n     log.debug(\"DAG file parsed\", file=msg.dag_rel_path)\n \n+    try:\n+        run_as_user = getattr(ti.task, \"run_as_user\", None) or conf.get(\"core\", \"default_impersonation\")\n+    except AirflowConfigException:\n+        run_as_user = None\n+\n+    if os.environ.get(\"_AIRFLOW__REEXECUTED_PROCESS\") != \"1\" and run_as_user:\n+        # enters here for re-exec process\n+        os.environ[\"_AIRFLOW__REEXECUTED_PROCESS\"] = \"1\"\n+        # store startup message in environment for re-exec process\n+        os.environ[\"_AIRFLOW__STARTUP_MSG\"] = msg.model_dump_json()\n+        os.set_inheritable(SUPERVISOR_COMMS.socket.fileno(), True)\n+\n+        # Import main directly from the module instead of re-executing the file.\n+        # This ensures that when other parts modules import\n+        # airflow.sdk.execution_time.task_runner, they get the same module instance\n+        # with the properly initialized SUPERVISOR_COMMS global variable.\n+        # If we re-executed the script, it would load as __main__ and future\n+        # imports would get a fresh copy without the initialized globals.\n+        rexec_python_code = \"from airflow.sdk.execution_time.task_runner import main; main()\"\n+        log.info(\n+            \"Running command\",\n+            command=[\"sudo\", \"-E\", \"-H\", \"-u\", run_as_user, sys.executable, \"-c\", rexec_python_code],\n+        )\n+        os.execvp(\"sudo\", [\"sudo\", \"-E\", \"-H\", \"-u\", run_as_user, sys.executable, \"-c\", rexec_python_code])",
        "comment_created_at": "2025-06-18T11:35:55+00:00",
        "comment_author": "ashb",
        "comment_body": "Nit: command is duplicated, we should store it in a variable used in both places.",
        "pr_file_module": null
      },
      {
        "comment_id": "2154741414",
        "repo_full_name": "apache/airflow",
        "pr_number": 51780,
        "pr_file": "task-sdk/src/airflow/sdk/execution_time/task_runner.py",
        "discussion_id": "2154373428",
        "commented_code": "@@ -677,6 +685,34 @@ def startup() -> tuple[RuntimeTaskInstance, Context, Logger]:\n         ti.log_url = get_log_url_from_ti(ti)\n     log.debug(\"DAG file parsed\", file=msg.dag_rel_path)\n \n+    try:\n+        run_as_user = getattr(ti.task, \"run_as_user\", None) or conf.get(\"core\", \"default_impersonation\")\n+    except AirflowConfigException:\n+        run_as_user = None\n+\n+    if os.environ.get(\"_AIRFLOW__REEXECUTED_PROCESS\") != \"1\" and run_as_user:\n+        # enters here for re-exec process\n+        os.environ[\"_AIRFLOW__REEXECUTED_PROCESS\"] = \"1\"\n+        # store startup message in environment for re-exec process\n+        os.environ[\"_AIRFLOW__STARTUP_MSG\"] = msg.model_dump_json()\n+        os.set_inheritable(SUPERVISOR_COMMS.socket.fileno(), True)\n+\n+        # Import main directly from the module instead of re-executing the file.\n+        # This ensures that when other parts modules import\n+        # airflow.sdk.execution_time.task_runner, they get the same module instance\n+        # with the properly initialized SUPERVISOR_COMMS global variable.\n+        # If we re-executed the script, it would load as __main__ and future\n+        # imports would get a fresh copy without the initialized globals.\n+        rexec_python_code = \"from airflow.sdk.execution_time.task_runner import main; main()\"\n+        log.info(\n+            \"Running command\",\n+            command=[\"sudo\", \"-E\", \"-H\", \"-u\", run_as_user, sys.executable, \"-c\", rexec_python_code],\n+        )\n+        os.execvp(\"sudo\", [\"sudo\", \"-E\", \"-H\", \"-u\", run_as_user, sys.executable, \"-c\", rexec_python_code])",
        "comment_created_at": "2025-06-18T14:20:54+00:00",
        "comment_author": "amoghrajesh",
        "comment_body": "Handled",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178711514",
    "pr_number": 52431,
    "pr_file": "providers/standard/src/airflow/providers/standard/sensors/external_task.py",
    "created_at": "2025-07-01T23:18:30+00:00",
    "commented_code": "from airflow.utils.operator_helpers import make_kwargs_callable\n \n         # Remove \"logical_date\" because it is already a mandatory positional argument\n-        logical_date = context[\"logical_date\"]\n+        logical_date = (",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2178711514",
        "repo_full_name": "apache/airflow",
        "pr_number": 52431,
        "pr_file": "providers/standard/src/airflow/providers/standard/sensors/external_task.py",
        "discussion_id": "2178711514",
        "commented_code": "@@ -534,7 +534,9 @@ def _handle_execution_date_fn(self, context) -> Any:\n         from airflow.utils.operator_helpers import make_kwargs_callable\n \n         # Remove \"logical_date\" because it is already a mandatory positional argument\n-        logical_date = context[\"logical_date\"]\n+        logical_date = (",
        "comment_created_at": "2025-07-01T23:18:30+00:00",
        "comment_author": "gopidesupavan",
        "comment_body": "I think it would be better to move this logical_date get from context to a function and work out and return logical_date. there is some duplicity, this line and here https://github.com/apache/airflow/pull/52431/files#diff-cd0a81a3fe4e14f46d41d402eabcb273554245c7e676e549b8b84030a99d4870R256, can you do something like this please? and you can call the function with context?\r\n\r\n\r\n\r\n\r\n```\r\n    def _get_logical_date(self, context):\r\n\r\n        if AIRFLOW_V_3_0_PLUS:\r\n            logical_date = context.get(\"logical_date\")\r\n            dag_run = context.get(\"dag_run\")\r\n\r\n            if not (logical_date or (dag_run and dag_run.run_after)):\r\n                raise ValueError(\r\n                    \"Either `logical_date` or `run_after` should be provided in the task context\"\r\n                )\r\n            return logical_date or dag_run.run_after\r\n        else:\r\n            execution_date = context.get(\"execution_date\")\r\n            if not execution_date:\r\n                raise ValueError(\r\n                    \"Either `execution_date` should be provided in the task context\"\r\n                )\r\n            return execution_date\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2178974491",
        "repo_full_name": "apache/airflow",
        "pr_number": 52431,
        "pr_file": "providers/standard/src/airflow/providers/standard/sensors/external_task.py",
        "discussion_id": "2178711514",
        "commented_code": "@@ -534,7 +534,9 @@ def _handle_execution_date_fn(self, context) -> Any:\n         from airflow.utils.operator_helpers import make_kwargs_callable\n \n         # Remove \"logical_date\" because it is already a mandatory positional argument\n-        logical_date = context[\"logical_date\"]\n+        logical_date = (",
        "comment_created_at": "2025-07-02T04:11:05+00:00",
        "comment_author": "yeswanth-bf",
        "comment_body": "Thanks for the feedback, @gopidesupavan!\r\n\r\n\r\nI've pulled that date-fallback logic into a new private helper _get_logical_date(context), and updated _handle_execution_date_fn to call it.\r\n\t\u2022\tAdded _get_logical_date in external_task.py\r\n\t\u2022\tRefactored _handle_execution_date_fn to delegate there\r\n\t\u2022\tUpdated and added unit tests in test_external_task_sensor.py\r\n\t\u2022\tRan pre-commit and fixed formatting/imports\r\n\r\n\r\nAll tests are now passing locally and CI is green. Please let me know if you'd like any more tweaks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2164319969",
    "pr_number": 51738,
    "pr_file": "airflow-core/src/airflow/cli/commands/backfill_command.py",
    "created_at": "2025-06-24T15:25:16+00:00",
    "commented_code": "console.print(f\"    - {d}\")\n         return\n \n+    try:\n+        user = getuser()\n+    except AirflowConfigException as e:\n+        log.warning(\"Failed to get user name from os: %s\", e)\n+        user = None",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2164319969",
        "repo_full_name": "apache/airflow",
        "pr_number": 51738,
        "pr_file": "airflow-core/src/airflow/cli/commands/backfill_command.py",
        "discussion_id": "2164319969",
        "commented_code": "@@ -70,12 +74,18 @@ def create_backfill(args) -> None:\n             console.print(f\"    - {d}\")\n         return\n \n+    try:\n+        user = getuser()\n+    except AirflowConfigException as e:\n+        log.warning(\"Failed to get user name from os: %s\", e)\n+        user = None",
        "comment_created_at": "2025-06-24T15:25:16+00:00",
        "comment_author": "jason810496",
        "comment_body": "Should we modularize the this try-except block for getting user from platform, as this utility will be reused in `assets`, `backfills`, `dag_run`, `task` commands .",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1982933744",
    "pr_number": 47440,
    "pr_file": "airflow/api_fastapi/common/parameters.py",
    "created_at": "2025-03-06T08:45:19+00:00",
    "commented_code": "self.model = model\n         self.to_replace = to_replace\n \n-    def to_orm(self, select: Select) -> Select:\n-        if self.skip_none is False:\n-            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+    def get_order_by_columns(self, order_by_list: list[str]) -> list:\n+        \"\"\"Generates order_by conditions based on the given sorting parameters.\"\"\"\n+        if len(order_by_list) > MAX_SORT_PARAMS:\n+            raise HTTPException(\n+                400, \n+                f\"Ordering with more than two parameters is not allowed. Provided: {order_by_list}\"\n+            )\n \n-        if self.value is None:\n-            self.value = self.get_primary_key_string()\n+        order_by_columns = []\n+\n+        if len(order_by_list) >= 1:\n+            order_by_columns.append(self.get_column_with_sort(order_by_list[0]))\n+\n+        if len(order_by_list) == 2:\n+            order_by_columns.append(self.get_column_with_sort(order_by_list[1]))",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "1982933744",
        "repo_full_name": "apache/airflow",
        "pr_number": 47440,
        "pr_file": "airflow/api_fastapi/common/parameters.py",
        "discussion_id": "1982933744",
        "commented_code": "@@ -173,14 +174,31 @@ def __init__(\n         self.model = model\n         self.to_replace = to_replace\n \n-    def to_orm(self, select: Select) -> Select:\n-        if self.skip_none is False:\n-            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+    def get_order_by_columns(self, order_by_list: list[str]) -> list:\n+        \"\"\"Generates order_by conditions based on the given sorting parameters.\"\"\"\n+        if len(order_by_list) > MAX_SORT_PARAMS:\n+            raise HTTPException(\n+                400, \n+                f\"Ordering with more than two parameters is not allowed. Provided: {order_by_list}\"\n+            )\n \n-        if self.value is None:\n-            self.value = self.get_primary_key_string()\n+        order_by_columns = []\n+\n+        if len(order_by_list) >= 1:\n+            order_by_columns.append(self.get_column_with_sort(order_by_list[0]))\n+\n+        if len(order_by_list) == 2:\n+            order_by_columns.append(self.get_column_with_sort(order_by_list[1]))",
        "comment_created_at": "2025-03-06T08:45:19+00:00",
        "comment_author": "pierrejeambrun",
        "comment_body": "for loop (because if MAX_SORT_PARAMS == 5, we don't want to do copy/past this 5 times)",
        "pr_file_module": null
      },
      {
        "comment_id": "1984973081",
        "repo_full_name": "apache/airflow",
        "pr_number": 47440,
        "pr_file": "airflow/api_fastapi/common/parameters.py",
        "discussion_id": "1982933744",
        "commented_code": "@@ -173,14 +174,31 @@ def __init__(\n         self.model = model\n         self.to_replace = to_replace\n \n-    def to_orm(self, select: Select) -> Select:\n-        if self.skip_none is False:\n-            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+    def get_order_by_columns(self, order_by_list: list[str]) -> list:\n+        \"\"\"Generates order_by conditions based on the given sorting parameters.\"\"\"\n+        if len(order_by_list) > MAX_SORT_PARAMS:\n+            raise HTTPException(\n+                400, \n+                f\"Ordering with more than two parameters is not allowed. Provided: {order_by_list}\"\n+            )\n \n-        if self.value is None:\n-            self.value = self.get_primary_key_string()\n+        order_by_columns = []\n+\n+        if len(order_by_list) >= 1:\n+            order_by_columns.append(self.get_column_with_sort(order_by_list[0]))\n+\n+        if len(order_by_list) == 2:\n+            order_by_columns.append(self.get_column_with_sort(order_by_list[1]))",
        "comment_created_at": "2025-03-07T12:25:19+00:00",
        "comment_author": "prasad-madine",
        "comment_body": "fixed",
        "pr_file_module": null
      }
    ]
  }
]