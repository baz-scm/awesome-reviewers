[
  {
    "discussion_id": "2162450379",
    "pr_number": 51657,
    "pr_file": "airflow-core/src/airflow/api_fastapi/auth/managers/middleware/refresh_token.py",
    "created_at": "2025-06-23T20:12:48+00:00",
    "commented_code": "+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import logging\n+\n+from fastapi import Request\n+from starlette.middleware.base import BaseHTTPMiddleware\n+\n+from airflow.api_fastapi.auth.managers.base_auth_manager import COOKIE_NAME_JWT_TOKEN, BaseAuthManager\n+from airflow.configuration import conf\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class RefreshTokenMiddleware(BaseHTTPMiddleware):\n+    \"\"\"Middleware that refresh auth manager token and update JWT Token.\"\"\"\n+\n+    def __init__(self, app, auth_manager: BaseAuthManager):\n+        super().__init__(app)\n+        self.auth_manager = auth_manager\n+\n+    async def dispatch(self, request: Request, call_next):\n+        # Extract Authorization header\n+        auth = request.headers.get(\"authorization\")\n+\n+        if auth and auth.lower().startswith(\"bearer \"):\n+            token_str = auth.split(\" \", 1)[1]\n+            if token_str != \"null\":\n+                user = await self.auth_manager.get_user_from_token(token_str)\n+                if new_user := self.auth_manager.refresh_token(user=user):\n+                    new_token_with_updated_user = self.auth_manager.generate_jwt(user=new_user)\n+                    response = await call_next(request)\n+                    # Set the new access token in the cookies to update the state of the user in the token\n+                    secure = bool(conf.get(\"api\", \"ssl_cert\", fallback=\"\"))\n+                    response.set_cookie(COOKIE_NAME_JWT_TOKEN, new_token_with_updated_user, secure=secure)\n+                    return response\n+                log.warning(\"User does not have a refresh token, cannot refresh.\")",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2162450379",
        "repo_full_name": "apache/airflow",
        "pr_number": 51657,
        "pr_file": "airflow-core/src/airflow/api_fastapi/auth/managers/middleware/refresh_token.py",
        "discussion_id": "2162450379",
        "commented_code": "@@ -0,0 +1,54 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import logging\n+\n+from fastapi import Request\n+from starlette.middleware.base import BaseHTTPMiddleware\n+\n+from airflow.api_fastapi.auth.managers.base_auth_manager import COOKIE_NAME_JWT_TOKEN, BaseAuthManager\n+from airflow.configuration import conf\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class RefreshTokenMiddleware(BaseHTTPMiddleware):\n+    \"\"\"Middleware that refresh auth manager token and update JWT Token.\"\"\"\n+\n+    def __init__(self, app, auth_manager: BaseAuthManager):\n+        super().__init__(app)\n+        self.auth_manager = auth_manager\n+\n+    async def dispatch(self, request: Request, call_next):\n+        # Extract Authorization header\n+        auth = request.headers.get(\"authorization\")\n+\n+        if auth and auth.lower().startswith(\"bearer \"):\n+            token_str = auth.split(\" \", 1)[1]\n+            if token_str != \"null\":\n+                user = await self.auth_manager.get_user_from_token(token_str)\n+                if new_user := self.auth_manager.refresh_token(user=user):\n+                    new_token_with_updated_user = self.auth_manager.generate_jwt(user=new_user)\n+                    response = await call_next(request)\n+                    # Set the new access token in the cookies to update the state of the user in the token\n+                    secure = bool(conf.get(\"api\", \"ssl_cert\", fallback=\"\"))\n+                    response.set_cookie(COOKIE_NAME_JWT_TOKEN, new_token_with_updated_user, secure=secure)\n+                    return response\n+                log.warning(\"User does not have a refresh token, cannot refresh.\")",
        "comment_created_at": "2025-06-23T20:12:48+00:00",
        "comment_author": "vincbeck",
        "comment_body": "I am not sure we should have this log statement, in particular a warning one. `refresh_token` is an optional method of many auth manager will not implement this method because simply it is not needed. Having a warning in that case feels wrong to me",
        "pr_file_module": null
      },
      {
        "comment_id": "2164781356",
        "repo_full_name": "apache/airflow",
        "pr_number": 51657,
        "pr_file": "airflow-core/src/airflow/api_fastapi/auth/managers/middleware/refresh_token.py",
        "discussion_id": "2162450379",
        "commented_code": "@@ -0,0 +1,54 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import logging\n+\n+from fastapi import Request\n+from starlette.middleware.base import BaseHTTPMiddleware\n+\n+from airflow.api_fastapi.auth.managers.base_auth_manager import COOKIE_NAME_JWT_TOKEN, BaseAuthManager\n+from airflow.configuration import conf\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class RefreshTokenMiddleware(BaseHTTPMiddleware):\n+    \"\"\"Middleware that refresh auth manager token and update JWT Token.\"\"\"\n+\n+    def __init__(self, app, auth_manager: BaseAuthManager):\n+        super().__init__(app)\n+        self.auth_manager = auth_manager\n+\n+    async def dispatch(self, request: Request, call_next):\n+        # Extract Authorization header\n+        auth = request.headers.get(\"authorization\")\n+\n+        if auth and auth.lower().startswith(\"bearer \"):\n+            token_str = auth.split(\" \", 1)[1]\n+            if token_str != \"null\":\n+                user = await self.auth_manager.get_user_from_token(token_str)\n+                if new_user := self.auth_manager.refresh_token(user=user):\n+                    new_token_with_updated_user = self.auth_manager.generate_jwt(user=new_user)\n+                    response = await call_next(request)\n+                    # Set the new access token in the cookies to update the state of the user in the token\n+                    secure = bool(conf.get(\"api\", \"ssl_cert\", fallback=\"\"))\n+                    response.set_cookie(COOKIE_NAME_JWT_TOKEN, new_token_with_updated_user, secure=secure)\n+                    return response\n+                log.warning(\"User does not have a refresh token, cannot refresh.\")",
        "comment_created_at": "2025-06-24T19:53:44+00:00",
        "comment_author": "bugraoz93",
        "comment_body": "In that sense, it makes sense! Let me remove it",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160707004",
    "pr_number": 48557,
    "pr_file": "airflow-core/src/airflow/models/taskinstance.py",
    "created_at": "2025-06-23T04:46:57+00:00",
    "commented_code": "delay = self.task.retry_delay\n         if self.task.retry_exponential_backoff:\n-            # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n-            # we must round up prior to converting to an int, otherwise a divide by zero error\n-            # will occur in the modded_hash calculation.\n-            # this probably gives unexpected results if a task instance has previously been cleared,\n-            # because try_number can increase without bound\n-            min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            try:\n+                # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n+                # we must round up prior to converting to an int, otherwise a divide by zero error\n+                # will occur in the modded_hash calculation.\n+                # this probably gives unexpected results if a task instance has previously been cleared,\n+                # because try_number can increase without bound\n+                min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            except OverflowError:\n+                min_backoff = MAX_RETRY_DELAY",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2160707004",
        "repo_full_name": "apache/airflow",
        "pr_number": 48557,
        "pr_file": "airflow-core/src/airflow/models/taskinstance.py",
        "discussion_id": "2160707004",
        "commented_code": "@@ -1114,12 +1114,15 @@ def next_retry_datetime(self):\n \n         delay = self.task.retry_delay\n         if self.task.retry_exponential_backoff:\n-            # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n-            # we must round up prior to converting to an int, otherwise a divide by zero error\n-            # will occur in the modded_hash calculation.\n-            # this probably gives unexpected results if a task instance has previously been cleared,\n-            # because try_number can increase without bound\n-            min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            try:\n+                # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n+                # we must round up prior to converting to an int, otherwise a divide by zero error\n+                # will occur in the modded_hash calculation.\n+                # this probably gives unexpected results if a task instance has previously been cleared,\n+                # because try_number can increase without bound\n+                min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            except OverflowError:\n+                min_backoff = MAX_RETRY_DELAY",
        "comment_created_at": "2025-06-23T04:46:57+00:00",
        "comment_author": "uranusjr",
        "comment_body": "Let\u2019s add a log message here so people know better what\u2019s going on if they get unexpected behaviour.",
        "pr_file_module": null
      },
      {
        "comment_id": "2162175204",
        "repo_full_name": "apache/airflow",
        "pr_number": 48557,
        "pr_file": "airflow-core/src/airflow/models/taskinstance.py",
        "discussion_id": "2160707004",
        "commented_code": "@@ -1114,12 +1114,15 @@ def next_retry_datetime(self):\n \n         delay = self.task.retry_delay\n         if self.task.retry_exponential_backoff:\n-            # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n-            # we must round up prior to converting to an int, otherwise a divide by zero error\n-            # will occur in the modded_hash calculation.\n-            # this probably gives unexpected results if a task instance has previously been cleared,\n-            # because try_number can increase without bound\n-            min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            try:\n+                # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n+                # we must round up prior to converting to an int, otherwise a divide by zero error\n+                # will occur in the modded_hash calculation.\n+                # this probably gives unexpected results if a task instance has previously been cleared,\n+                # because try_number can increase without bound\n+                min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            except OverflowError:\n+                min_backoff = MAX_RETRY_DELAY",
        "comment_created_at": "2025-06-23T18:01:59+00:00",
        "comment_author": "perry2of5",
        "comment_body": "We don't warn when limiting the retry time for other reasons. I think it should be consistent one way or the other. I'm happy to add a message here and add another ticket to log when limiting the time until the next retry at other locations. Thoughts?",
        "pr_file_module": null
      },
      {
        "comment_id": "2162960997",
        "repo_full_name": "apache/airflow",
        "pr_number": 48557,
        "pr_file": "airflow-core/src/airflow/models/taskinstance.py",
        "discussion_id": "2160707004",
        "commented_code": "@@ -1114,12 +1114,15 @@ def next_retry_datetime(self):\n \n         delay = self.task.retry_delay\n         if self.task.retry_exponential_backoff:\n-            # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n-            # we must round up prior to converting to an int, otherwise a divide by zero error\n-            # will occur in the modded_hash calculation.\n-            # this probably gives unexpected results if a task instance has previously been cleared,\n-            # because try_number can increase without bound\n-            min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            try:\n+                # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n+                # we must round up prior to converting to an int, otherwise a divide by zero error\n+                # will occur in the modded_hash calculation.\n+                # this probably gives unexpected results if a task instance has previously been cleared,\n+                # because try_number can increase without bound\n+                min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            except OverflowError:\n+                min_backoff = MAX_RETRY_DELAY",
        "comment_created_at": "2025-06-24T04:57:03+00:00",
        "comment_author": "uranusjr",
        "comment_body": "Can you point me to another instance we do this kind of limiting? (We seem to only catch OverflowError in one other place and it\u2019s for a different purpose)",
        "pr_file_module": null
      },
      {
        "comment_id": "2164711654",
        "repo_full_name": "apache/airflow",
        "pr_number": 48557,
        "pr_file": "airflow-core/src/airflow/models/taskinstance.py",
        "discussion_id": "2160707004",
        "commented_code": "@@ -1114,12 +1114,15 @@ def next_retry_datetime(self):\n \n         delay = self.task.retry_delay\n         if self.task.retry_exponential_backoff:\n-            # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n-            # we must round up prior to converting to an int, otherwise a divide by zero error\n-            # will occur in the modded_hash calculation.\n-            # this probably gives unexpected results if a task instance has previously been cleared,\n-            # because try_number can increase without bound\n-            min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            try:\n+                # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n+                # we must round up prior to converting to an int, otherwise a divide by zero error\n+                # will occur in the modded_hash calculation.\n+                # this probably gives unexpected results if a task instance has previously been cleared,\n+                # because try_number can increase without bound\n+                min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            except OverflowError:\n+                min_backoff = MAX_RETRY_DELAY",
        "comment_created_at": "2025-06-24T19:07:31+00:00",
        "comment_author": "perry2of5",
        "comment_body": "Lower in the function the code ensures the delay isn\u2019t longer than a day, but it doesn\u2019t log anything when it does so. I'm talking about this:\r\n\r\n`min(modded_hash, MAX_RETRY_DELAY)` around line 1149\r\n\r\nWe don't inform the user we are putting a cap on the exponential time, so it seems inconsistent to log about capping the value due to overflow but not due to exceeding the MAX_RETRY_DELAY. Overflow is just a implementation detail due number representation limits.... I think capping the value due to overflow and due to exceeded the MAX_RETRY_DELAY could both be unexpected.",
        "pr_file_module": null
      },
      {
        "comment_id": "2178644517",
        "repo_full_name": "apache/airflow",
        "pr_number": 48557,
        "pr_file": "airflow-core/src/airflow/models/taskinstance.py",
        "discussion_id": "2160707004",
        "commented_code": "@@ -1114,12 +1114,15 @@ def next_retry_datetime(self):\n \n         delay = self.task.retry_delay\n         if self.task.retry_exponential_backoff:\n-            # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n-            # we must round up prior to converting to an int, otherwise a divide by zero error\n-            # will occur in the modded_hash calculation.\n-            # this probably gives unexpected results if a task instance has previously been cleared,\n-            # because try_number can increase without bound\n-            min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            try:\n+                # If the min_backoff calculation is below 1, it will be converted to 0 via int. Thus,\n+                # we must round up prior to converting to an int, otherwise a divide by zero error\n+                # will occur in the modded_hash calculation.\n+                # this probably gives unexpected results if a task instance has previously been cleared,\n+                # because try_number can increase without bound\n+                min_backoff = math.ceil(delay.total_seconds() * (2 ** (self.try_number - 1)))\n+            except OverflowError:\n+                min_backoff = MAX_RETRY_DELAY",
        "comment_created_at": "2025-07-01T22:28:01+00:00",
        "comment_author": "perry2of5",
        "comment_body": "I added a log message. I think it is inconsistent to log when the calculation is capped due to overflow, but not when capped due to exceeding the maximum, but it does no harm. Better than crashing!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2162950217",
    "pr_number": 51738,
    "pr_file": "airflow-core/src/airflow/cli/commands/asset_command.py",
    "created_at": "2025-06-24T04:46:10+00:00",
    "commented_code": "if next(dag_id_it, None) is not None:\n         raise SystemExit(f\"More than one DAG materializes asset with {select_message}.\")\n \n-    dagrun = trigger_dag(dag_id=dag_id, triggered_by=DagRunTriggeredByType.CLI, session=session)\n+    try:\n+        user = getuser()\n+    except AirflowConfigException as e:\n+        log.warning(\"Failed to get user name from os: %s\", e)",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2162950217",
        "repo_full_name": "apache/airflow",
        "pr_number": 51738,
        "pr_file": "airflow-core/src/airflow/cli/commands/asset_command.py",
        "discussion_id": "2162950217",
        "commented_code": "@@ -149,7 +151,14 @@ def asset_materialize(args, *, session: Session = NEW_SESSION) -> None:\n     if next(dag_id_it, None) is not None:\n         raise SystemExit(f\"More than one DAG materializes asset with {select_message}.\")\n \n-    dagrun = trigger_dag(dag_id=dag_id, triggered_by=DagRunTriggeredByType.CLI, session=session)\n+    try:\n+        user = getuser()\n+    except AirflowConfigException as e:\n+        log.warning(\"Failed to get user name from os: %s\", e)",
        "comment_created_at": "2025-06-24T04:46:10+00:00",
        "comment_author": "amoghrajesh",
        "comment_body": "```suggestion\r\n        log.warning(\"Failed to get user name from os: %s, not setting the triggering user\", e)\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2162950321",
        "repo_full_name": "apache/airflow",
        "pr_number": 51738,
        "pr_file": "airflow-core/src/airflow/cli/commands/asset_command.py",
        "discussion_id": "2162950217",
        "commented_code": "@@ -149,7 +151,14 @@ def asset_materialize(args, *, session: Session = NEW_SESSION) -> None:\n     if next(dag_id_it, None) is not None:\n         raise SystemExit(f\"More than one DAG materializes asset with {select_message}.\")\n \n-    dagrun = trigger_dag(dag_id=dag_id, triggered_by=DagRunTriggeredByType.CLI, session=session)\n+    try:\n+        user = getuser()\n+    except AirflowConfigException as e:\n+        log.warning(\"Failed to get user name from os: %s\", e)",
        "comment_created_at": "2025-06-24T04:46:17+00:00",
        "comment_author": "amoghrajesh",
        "comment_body": "Same in other usages too",
        "pr_file_module": null
      },
      {
        "comment_id": "2172902907",
        "repo_full_name": "apache/airflow",
        "pr_number": 51738,
        "pr_file": "airflow-core/src/airflow/cli/commands/asset_command.py",
        "discussion_id": "2162950217",
        "commented_code": "@@ -149,7 +151,14 @@ def asset_materialize(args, *, session: Session = NEW_SESSION) -> None:\n     if next(dag_id_it, None) is not None:\n         raise SystemExit(f\"More than one DAG materializes asset with {select_message}.\")\n \n-    dagrun = trigger_dag(dag_id=dag_id, triggered_by=DagRunTriggeredByType.CLI, session=session)\n+    try:\n+        user = getuser()\n+    except AirflowConfigException as e:\n+        log.warning(\"Failed to get user name from os: %s\", e)",
        "comment_created_at": "2025-06-27T21:37:43+00:00",
        "comment_author": "jscheffl",
        "comment_body": "Adjusted, thanks for the review.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089772446",
    "pr_number": 50516,
    "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
    "created_at": "2025-05-14T21:19:53+00:00",
    "commented_code": "+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    if TYPE_CHECKING and AIRFLOW_V_3_0_PLUS:\n+        # In the v3 path, we store workloads, not commands as strings.\n+        # TODO: TaskSDK: move this type change into BaseExecutor\n+        queued_tasks: dict[TaskInstanceKey, workloads.All]  # type: ignore[assignment]\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.pending_tasks: deque = deque()\n+        self.running_tasks: dict[str, TaskInstanceKey] = {}\n+        self.lambda_function_name = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.FUNCTION_NAME)\n+        self.sqs_queue_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUEUE_URL)\n+        self.dlq_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.DLQ_URL)\n+        self.qualifier = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUALIFIER, fallback=None)\n+        # Maximum number of retries to invoke Lambda.\n+        self.max_invoke_attempts = conf.get(\n+            CONFIG_GROUP_NAME,\n+            AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        )\n+\n+        self.attempts_since_last_successful_connection = 0\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+        self.load_connections(check_connection=False)\n+\n+    def start(self):\n+        \"\"\"Call this when the Executor is run for the first time by the scheduler.\"\"\"\n+        check_health = conf.getboolean(CONFIG_GROUP_NAME, AllLambdaConfigKeys.CHECK_HEALTH_ON_STARTUP)\n+\n+        if not check_health:\n+            return\n+\n+        self.log.info(\"Starting Lambda Executor and determining health...\")\n+        try:\n+            self.check_health()\n+        except AirflowException:\n+            self.log.error(\"Stopping the Airflow Scheduler from starting until the issue is resolved.\")\n+            raise\n+\n+    def check_health(self):\n+        \"\"\"\n+        Check the health of the Lambda and SQS connections.\n+\n+        For lambda: Use get_function to test if the lambda connection works and the function can be\n+        described.\n+        For SQS: Use get_queue_attributes is used as a close analog to describe to test if the SQS\n+        connection is working.\n+        \"\"\"\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+\n+        try:\n+            self.log.info(\"Checking Lambda and SQS connections\")\n+            # Check Lambda health\n+            lambda_get_response = self.lambda_client.get_function(FunctionName=self.lambda_function_name)\n+            if self.lambda_function_name not in lambda_get_response[\"Configuration\"][\"FunctionName\"]:\n+                raise AirflowException(\"Lambda function %s not found.\", self.lambda_function_name)\n+            self.log.info(\n+                \"Lambda connection is healthy and function %s is present.\", self.lambda_function_name\n+            )\n+\n+            def _check_queue(queue_url):\n+                sqs_get_queue_attrs_response = self.sqs_client.get_queue_attributes(\n+                    QueueUrl=queue_url, AttributeNames=[\"ApproximateNumberOfMessages\"]\n+                )\n+                try:\n+                    approx_num_msgs = sqs_get_queue_attrs_response.get(\"Attributes\").get(\n+                        \"ApproximateNumberOfMessages\"\n+                    )\n+                    self.log.info(\n+                        \"SQS connection is healthy and queue %s is present with %s messages.\",\n+                        queue_url,\n+                        approx_num_msgs,\n+                    )\n+                except Exception:",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2089772446",
        "repo_full_name": "apache/airflow",
        "pr_number": 50516,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
        "discussion_id": "2089772446",
        "commented_code": "@@ -0,0 +1,484 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    if TYPE_CHECKING and AIRFLOW_V_3_0_PLUS:\n+        # In the v3 path, we store workloads, not commands as strings.\n+        # TODO: TaskSDK: move this type change into BaseExecutor\n+        queued_tasks: dict[TaskInstanceKey, workloads.All]  # type: ignore[assignment]\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.pending_tasks: deque = deque()\n+        self.running_tasks: dict[str, TaskInstanceKey] = {}\n+        self.lambda_function_name = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.FUNCTION_NAME)\n+        self.sqs_queue_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUEUE_URL)\n+        self.dlq_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.DLQ_URL)\n+        self.qualifier = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUALIFIER, fallback=None)\n+        # Maximum number of retries to invoke Lambda.\n+        self.max_invoke_attempts = conf.get(\n+            CONFIG_GROUP_NAME,\n+            AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        )\n+\n+        self.attempts_since_last_successful_connection = 0\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+        self.load_connections(check_connection=False)\n+\n+    def start(self):\n+        \"\"\"Call this when the Executor is run for the first time by the scheduler.\"\"\"\n+        check_health = conf.getboolean(CONFIG_GROUP_NAME, AllLambdaConfigKeys.CHECK_HEALTH_ON_STARTUP)\n+\n+        if not check_health:\n+            return\n+\n+        self.log.info(\"Starting Lambda Executor and determining health...\")\n+        try:\n+            self.check_health()\n+        except AirflowException:\n+            self.log.error(\"Stopping the Airflow Scheduler from starting until the issue is resolved.\")\n+            raise\n+\n+    def check_health(self):\n+        \"\"\"\n+        Check the health of the Lambda and SQS connections.\n+\n+        For lambda: Use get_function to test if the lambda connection works and the function can be\n+        described.\n+        For SQS: Use get_queue_attributes is used as a close analog to describe to test if the SQS\n+        connection is working.\n+        \"\"\"\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+\n+        try:\n+            self.log.info(\"Checking Lambda and SQS connections\")\n+            # Check Lambda health\n+            lambda_get_response = self.lambda_client.get_function(FunctionName=self.lambda_function_name)\n+            if self.lambda_function_name not in lambda_get_response[\"Configuration\"][\"FunctionName\"]:\n+                raise AirflowException(\"Lambda function %s not found.\", self.lambda_function_name)\n+            self.log.info(\n+                \"Lambda connection is healthy and function %s is present.\", self.lambda_function_name\n+            )\n+\n+            def _check_queue(queue_url):\n+                sqs_get_queue_attrs_response = self.sqs_client.get_queue_attributes(\n+                    QueueUrl=queue_url, AttributeNames=[\"ApproximateNumberOfMessages\"]\n+                )\n+                try:\n+                    approx_num_msgs = sqs_get_queue_attrs_response.get(\"Attributes\").get(\n+                        \"ApproximateNumberOfMessages\"\n+                    )\n+                    self.log.info(\n+                        \"SQS connection is healthy and queue %s is present with %s messages.\",\n+                        queue_url,\n+                        approx_num_msgs,\n+                    )\n+                except Exception:",
        "comment_created_at": "2025-05-14T21:19:53+00:00",
        "comment_author": "gopidesupavan",
        "comment_body": "Logging exception will give user better debugging option, what exactly the error thrown, in this case mostly we get permission errors/ or queue itself is wrong  or not exists :)",
        "pr_file_module": null
      },
      {
        "comment_id": "2089845211",
        "repo_full_name": "apache/airflow",
        "pr_number": 50516,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
        "discussion_id": "2089772446",
        "commented_code": "@@ -0,0 +1,484 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    if TYPE_CHECKING and AIRFLOW_V_3_0_PLUS:\n+        # In the v3 path, we store workloads, not commands as strings.\n+        # TODO: TaskSDK: move this type change into BaseExecutor\n+        queued_tasks: dict[TaskInstanceKey, workloads.All]  # type: ignore[assignment]\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.pending_tasks: deque = deque()\n+        self.running_tasks: dict[str, TaskInstanceKey] = {}\n+        self.lambda_function_name = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.FUNCTION_NAME)\n+        self.sqs_queue_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUEUE_URL)\n+        self.dlq_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.DLQ_URL)\n+        self.qualifier = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUALIFIER, fallback=None)\n+        # Maximum number of retries to invoke Lambda.\n+        self.max_invoke_attempts = conf.get(\n+            CONFIG_GROUP_NAME,\n+            AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        )\n+\n+        self.attempts_since_last_successful_connection = 0\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+        self.load_connections(check_connection=False)\n+\n+    def start(self):\n+        \"\"\"Call this when the Executor is run for the first time by the scheduler.\"\"\"\n+        check_health = conf.getboolean(CONFIG_GROUP_NAME, AllLambdaConfigKeys.CHECK_HEALTH_ON_STARTUP)\n+\n+        if not check_health:\n+            return\n+\n+        self.log.info(\"Starting Lambda Executor and determining health...\")\n+        try:\n+            self.check_health()\n+        except AirflowException:\n+            self.log.error(\"Stopping the Airflow Scheduler from starting until the issue is resolved.\")\n+            raise\n+\n+    def check_health(self):\n+        \"\"\"\n+        Check the health of the Lambda and SQS connections.\n+\n+        For lambda: Use get_function to test if the lambda connection works and the function can be\n+        described.\n+        For SQS: Use get_queue_attributes is used as a close analog to describe to test if the SQS\n+        connection is working.\n+        \"\"\"\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+\n+        try:\n+            self.log.info(\"Checking Lambda and SQS connections\")\n+            # Check Lambda health\n+            lambda_get_response = self.lambda_client.get_function(FunctionName=self.lambda_function_name)\n+            if self.lambda_function_name not in lambda_get_response[\"Configuration\"][\"FunctionName\"]:\n+                raise AirflowException(\"Lambda function %s not found.\", self.lambda_function_name)\n+            self.log.info(\n+                \"Lambda connection is healthy and function %s is present.\", self.lambda_function_name\n+            )\n+\n+            def _check_queue(queue_url):\n+                sqs_get_queue_attrs_response = self.sqs_client.get_queue_attributes(\n+                    QueueUrl=queue_url, AttributeNames=[\"ApproximateNumberOfMessages\"]\n+                )\n+                try:\n+                    approx_num_msgs = sqs_get_queue_attrs_response.get(\"Attributes\").get(\n+                        \"ApproximateNumberOfMessages\"\n+                    )\n+                    self.log.info(\n+                        \"SQS connection is healthy and queue %s is present with %s messages.\",\n+                        queue_url,\n+                        approx_num_msgs,\n+                    )\n+                except Exception:",
        "comment_created_at": "2025-05-14T22:18:16+00:00",
        "comment_author": "o-nikolas",
        "comment_body": "Yeah that's fair, the context of the original exception is getting dropped here which would be helpful to have for the user to fix the issue. Let me log it",
        "pr_file_module": null
      },
      {
        "comment_id": "2089854225",
        "repo_full_name": "apache/airflow",
        "pr_number": 50516,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
        "discussion_id": "2089772446",
        "commented_code": "@@ -0,0 +1,484 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    if TYPE_CHECKING and AIRFLOW_V_3_0_PLUS:\n+        # In the v3 path, we store workloads, not commands as strings.\n+        # TODO: TaskSDK: move this type change into BaseExecutor\n+        queued_tasks: dict[TaskInstanceKey, workloads.All]  # type: ignore[assignment]\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.pending_tasks: deque = deque()\n+        self.running_tasks: dict[str, TaskInstanceKey] = {}\n+        self.lambda_function_name = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.FUNCTION_NAME)\n+        self.sqs_queue_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUEUE_URL)\n+        self.dlq_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.DLQ_URL)\n+        self.qualifier = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUALIFIER, fallback=None)\n+        # Maximum number of retries to invoke Lambda.\n+        self.max_invoke_attempts = conf.get(\n+            CONFIG_GROUP_NAME,\n+            AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        )\n+\n+        self.attempts_since_last_successful_connection = 0\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+        self.load_connections(check_connection=False)\n+\n+    def start(self):\n+        \"\"\"Call this when the Executor is run for the first time by the scheduler.\"\"\"\n+        check_health = conf.getboolean(CONFIG_GROUP_NAME, AllLambdaConfigKeys.CHECK_HEALTH_ON_STARTUP)\n+\n+        if not check_health:\n+            return\n+\n+        self.log.info(\"Starting Lambda Executor and determining health...\")\n+        try:\n+            self.check_health()\n+        except AirflowException:\n+            self.log.error(\"Stopping the Airflow Scheduler from starting until the issue is resolved.\")\n+            raise\n+\n+    def check_health(self):\n+        \"\"\"\n+        Check the health of the Lambda and SQS connections.\n+\n+        For lambda: Use get_function to test if the lambda connection works and the function can be\n+        described.\n+        For SQS: Use get_queue_attributes is used as a close analog to describe to test if the SQS\n+        connection is working.\n+        \"\"\"\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+\n+        try:\n+            self.log.info(\"Checking Lambda and SQS connections\")\n+            # Check Lambda health\n+            lambda_get_response = self.lambda_client.get_function(FunctionName=self.lambda_function_name)\n+            if self.lambda_function_name not in lambda_get_response[\"Configuration\"][\"FunctionName\"]:\n+                raise AirflowException(\"Lambda function %s not found.\", self.lambda_function_name)\n+            self.log.info(\n+                \"Lambda connection is healthy and function %s is present.\", self.lambda_function_name\n+            )\n+\n+            def _check_queue(queue_url):\n+                sqs_get_queue_attrs_response = self.sqs_client.get_queue_attributes(\n+                    QueueUrl=queue_url, AttributeNames=[\"ApproximateNumberOfMessages\"]\n+                )\n+                try:\n+                    approx_num_msgs = sqs_get_queue_attrs_response.get(\"Attributes\").get(\n+                        \"ApproximateNumberOfMessages\"\n+                    )\n+                    self.log.info(\n+                        \"SQS connection is healthy and queue %s is present with %s messages.\",\n+                        queue_url,\n+                        approx_num_msgs,\n+                    )\n+                except Exception:",
        "comment_created_at": "2025-05-14T22:28:57+00:00",
        "comment_author": "o-nikolas",
        "comment_body": "I simplified the try/catches, removing all but the outer try/catch. It will except all exceptions, log the exception with the context that the lambda executor is not healthy and the issue needs rectifying ",
        "pr_file_module": null
      },
      {
        "comment_id": "2089871284",
        "repo_full_name": "apache/airflow",
        "pr_number": 50516,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
        "discussion_id": "2089772446",
        "commented_code": "@@ -0,0 +1,484 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    if TYPE_CHECKING and AIRFLOW_V_3_0_PLUS:\n+        # In the v3 path, we store workloads, not commands as strings.\n+        # TODO: TaskSDK: move this type change into BaseExecutor\n+        queued_tasks: dict[TaskInstanceKey, workloads.All]  # type: ignore[assignment]\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.pending_tasks: deque = deque()\n+        self.running_tasks: dict[str, TaskInstanceKey] = {}\n+        self.lambda_function_name = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.FUNCTION_NAME)\n+        self.sqs_queue_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUEUE_URL)\n+        self.dlq_url = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.DLQ_URL)\n+        self.qualifier = conf.get(CONFIG_GROUP_NAME, AllLambdaConfigKeys.QUALIFIER, fallback=None)\n+        # Maximum number of retries to invoke Lambda.\n+        self.max_invoke_attempts = conf.get(\n+            CONFIG_GROUP_NAME,\n+            AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        )\n+\n+        self.attempts_since_last_successful_connection = 0\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+        self.load_connections(check_connection=False)\n+\n+    def start(self):\n+        \"\"\"Call this when the Executor is run for the first time by the scheduler.\"\"\"\n+        check_health = conf.getboolean(CONFIG_GROUP_NAME, AllLambdaConfigKeys.CHECK_HEALTH_ON_STARTUP)\n+\n+        if not check_health:\n+            return\n+\n+        self.log.info(\"Starting Lambda Executor and determining health...\")\n+        try:\n+            self.check_health()\n+        except AirflowException:\n+            self.log.error(\"Stopping the Airflow Scheduler from starting until the issue is resolved.\")\n+            raise\n+\n+    def check_health(self):\n+        \"\"\"\n+        Check the health of the Lambda and SQS connections.\n+\n+        For lambda: Use get_function to test if the lambda connection works and the function can be\n+        described.\n+        For SQS: Use get_queue_attributes is used as a close analog to describe to test if the SQS\n+        connection is working.\n+        \"\"\"\n+        self.IS_BOTO_CONNECTION_HEALTHY = False\n+\n+        try:\n+            self.log.info(\"Checking Lambda and SQS connections\")\n+            # Check Lambda health\n+            lambda_get_response = self.lambda_client.get_function(FunctionName=self.lambda_function_name)\n+            if self.lambda_function_name not in lambda_get_response[\"Configuration\"][\"FunctionName\"]:\n+                raise AirflowException(\"Lambda function %s not found.\", self.lambda_function_name)\n+            self.log.info(\n+                \"Lambda connection is healthy and function %s is present.\", self.lambda_function_name\n+            )\n+\n+            def _check_queue(queue_url):\n+                sqs_get_queue_attrs_response = self.sqs_client.get_queue_attributes(\n+                    QueueUrl=queue_url, AttributeNames=[\"ApproximateNumberOfMessages\"]\n+                )\n+                try:\n+                    approx_num_msgs = sqs_get_queue_attrs_response.get(\"Attributes\").get(\n+                        \"ApproximateNumberOfMessages\"\n+                    )\n+                    self.log.info(\n+                        \"SQS connection is healthy and queue %s is present with %s messages.\",\n+                        queue_url,\n+                        approx_num_msgs,\n+                    )\n+                except Exception:",
        "comment_created_at": "2025-05-14T22:48:44+00:00",
        "comment_author": "gopidesupavan",
        "comment_body": "cool :)",
        "pr_file_module": null
      }
    ]
  }
]