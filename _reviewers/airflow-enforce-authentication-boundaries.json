[
  {
    "discussion_id": "2174148315",
    "pr_number": 51756,
    "pr_file": "providers/tableau/docs/connections/tableau.rst",
    "created_at": "2025-06-30T03:37:54+00:00",
    "commented_code": "2. Use a `Token Authentication\n    <https://tableau.github.io/server-client-python/docs/api-ref#personalaccesstokenauth-class>`_\n    i.e. add a ``token_name`` and ``personal_access_token`` to the Airflow connection (deprecated).\n+3. Use `JSON Web Token (JWT) Authentication\n+   <https://tableau.github.io/server-client-python/docs/sign-in-out.html#sign-in-with-json-web-token-jwt>`_\n+   i.e add a ``jwt_file`` or a ``jwt_token`` to the Airflow connection extras.\n+\n+If both Password and Username authentication and JWT authentication are used simultaneously,\n+Password and Username authentication is preferred.",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2174148315",
        "repo_full_name": "apache/airflow",
        "pr_number": 51756,
        "pr_file": "providers/tableau/docs/connections/tableau.rst",
        "discussion_id": "2174148315",
        "commented_code": "@@ -35,6 +35,12 @@ There are two ways to connect to Tableau using Airflow.\n 2. Use a `Token Authentication\n    <https://tableau.github.io/server-client-python/docs/api-ref#personalaccesstokenauth-class>`_\n    i.e. add a ``token_name`` and ``personal_access_token`` to the Airflow connection (deprecated).\n+3. Use `JSON Web Token (JWT) Authentication\n+   <https://tableau.github.io/server-client-python/docs/sign-in-out.html#sign-in-with-json-web-token-jwt>`_\n+   i.e add a ``jwt_file`` or a ``jwt_token`` to the Airflow connection extras.\n+\n+If both Password and Username authentication and JWT authentication are used simultaneously,\n+Password and Username authentication is preferred.",
        "comment_created_at": "2025-06-30T03:37:54+00:00",
        "comment_author": "eladkal",
        "comment_body": "How can both be allowed at the same time?\r\nIsn't it better to raise mutually exclusive exception?",
        "pr_file_module": null
      },
      {
        "comment_id": "2174544046",
        "repo_full_name": "apache/airflow",
        "pr_number": 51756,
        "pr_file": "providers/tableau/docs/connections/tableau.rst",
        "discussion_id": "2174148315",
        "commented_code": "@@ -35,6 +35,12 @@ There are two ways to connect to Tableau using Airflow.\n 2. Use a `Token Authentication\n    <https://tableau.github.io/server-client-python/docs/api-ref#personalaccesstokenauth-class>`_\n    i.e. add a ``token_name`` and ``personal_access_token`` to the Airflow connection (deprecated).\n+3. Use `JSON Web Token (JWT) Authentication\n+   <https://tableau.github.io/server-client-python/docs/sign-in-out.html#sign-in-with-json-web-token-jwt>`_\n+   i.e add a ``jwt_file`` or a ``jwt_token`` to the Airflow connection extras.\n+\n+If both Password and Username authentication and JWT authentication are used simultaneously,\n+Password and Username authentication is preferred.",
        "comment_created_at": "2025-06-30T08:41:42+00:00",
        "comment_author": "dominikhei",
        "comment_body": "My point of view was that as long as we document the behavior and only one method is used, there is no harm in allowing credentials for both to be included at the same time. But I'll adjust it, as one second thought there is more speaking against than for it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2175040989",
        "repo_full_name": "apache/airflow",
        "pr_number": 51756,
        "pr_file": "providers/tableau/docs/connections/tableau.rst",
        "discussion_id": "2174148315",
        "commented_code": "@@ -35,6 +35,12 @@ There are two ways to connect to Tableau using Airflow.\n 2. Use a `Token Authentication\n    <https://tableau.github.io/server-client-python/docs/api-ref#personalaccesstokenauth-class>`_\n    i.e. add a ``token_name`` and ``personal_access_token`` to the Airflow connection (deprecated).\n+3. Use `JSON Web Token (JWT) Authentication\n+   <https://tableau.github.io/server-client-python/docs/sign-in-out.html#sign-in-with-json-web-token-jwt>`_\n+   i.e add a ``jwt_file`` or a ``jwt_token`` to the Airflow connection extras.\n+\n+If both Password and Username authentication and JWT authentication are used simultaneously,\n+Password and Username authentication is preferred.",
        "comment_created_at": "2025-06-30T13:11:12+00:00",
        "comment_author": "dominikhei",
        "comment_body": "@eladkal I\u2019ve updated the behavior to raise an error when both authentication methods are configured. However, the [Trino provider](https://github.com/apache/airflow/blob/main/providers/trino/src/airflow/providers/trino/hooks/trino.py#L153) allows the same behavior:\r\nJwt and password auth can be set simultaneously, but password is silently preferred. Moreover it raises an error if cert / kerberos auth and a password is set.\r\n\r\nFor consistency, should we consider updating it as well?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2181905823",
    "pr_number": 52196,
    "pr_file": "airflow-core/docs/installation/upgrading_to_airflow3.rst",
    "created_at": "2025-07-03T05:55:35+00:00",
    "commented_code": "Apache Airflow 3 is a major release and contains :ref:`breaking changes<breaking-changes>`. This guide walks you through the steps required to upgrade from Airflow 2.x to Airflow 3.0.\n \n+Understanding Airflow 3.x Architecture Changes\n+-----------------------------------------------\n+\n+Airflow 3.x introduces significant architectural changes that improve security, scalability, and maintainability. Understanding these changes helps you prepare for the upgrade and adapt your workflows accordingly.\n+\n+Airflow 2.x Architecture\n+^^^^^^^^^^^^^^^^^^^^^^^^\n+.. image:: ../img/airflow-2-arch.png\n+   :alt: Airflow 2.x architecture diagram showing scheduler, metadata database, and worker\n+   :align: center\n+\n+- All components communicate directly with the Airflow metadata database.\n+- Airflow 2 was designed to run all components within the same network space: task code and task execution code (airflow package code that runs user code) run in the same process.\n+- Workers communicate directly with the Airflow database and execute all user code.\n+- User code could import sessions and perform malicious actions on the Airflow metadata database.\n+- The number of connections to the database was excessive, leading to scaling challenges.\n+\n+Airflow 3.x Architecture\n+^^^^^^^^^^^^^^^^^^^^^^^^\n+.. image:: ../img/airflow-3-arch.png\n+   :alt: Airflow 3.x architecture diagram showing the decoupled Execution API Server and worker subprocesses\n+   :align: center\n+\n+- The API server is currently the sole access point for the metadata DB for tasks and workers.\n+- It supports several applications: the Airflow REST API, an internal API for the Airflow UI that hosts static JS, and an API for workers to interact with when executing TIs via the task execution interface.\n+- Workers communicate with the API server instead of directly with the database.\n+- DAG processor and Triggerer utilize the task execution mechanism for their tasks, especially when they require variables or connections.\n+\n+Database Access Restrictions\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+In Airflow 3.x, direct metadata database access from task code is now restricted. This is a key security and architectural improvement that affects how DAG authors interact with Airflow resources:",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2181905823",
        "repo_full_name": "apache/airflow",
        "pr_number": 52196,
        "pr_file": "airflow-core/docs/installation/upgrading_to_airflow3.rst",
        "discussion_id": "2181905823",
        "commented_code": "@@ -20,6 +20,43 @@ Upgrading to Airflow 3\n \n Apache Airflow 3 is a major release and contains :ref:`breaking changes<breaking-changes>`. This guide walks you through the steps required to upgrade from Airflow 2.x to Airflow 3.0.\n \n+Understanding Airflow 3.x Architecture Changes\n+-----------------------------------------------\n+\n+Airflow 3.x introduces significant architectural changes that improve security, scalability, and maintainability. Understanding these changes helps you prepare for the upgrade and adapt your workflows accordingly.\n+\n+Airflow 2.x Architecture\n+^^^^^^^^^^^^^^^^^^^^^^^^\n+.. image:: ../img/airflow-2-arch.png\n+   :alt: Airflow 2.x architecture diagram showing scheduler, metadata database, and worker\n+   :align: center\n+\n+- All components communicate directly with the Airflow metadata database.\n+- Airflow 2 was designed to run all components within the same network space: task code and task execution code (airflow package code that runs user code) run in the same process.\n+- Workers communicate directly with the Airflow database and execute all user code.\n+- User code could import sessions and perform malicious actions on the Airflow metadata database.\n+- The number of connections to the database was excessive, leading to scaling challenges.\n+\n+Airflow 3.x Architecture\n+^^^^^^^^^^^^^^^^^^^^^^^^\n+.. image:: ../img/airflow-3-arch.png\n+   :alt: Airflow 3.x architecture diagram showing the decoupled Execution API Server and worker subprocesses\n+   :align: center\n+\n+- The API server is currently the sole access point for the metadata DB for tasks and workers.\n+- It supports several applications: the Airflow REST API, an internal API for the Airflow UI that hosts static JS, and an API for workers to interact with when executing TIs via the task execution interface.\n+- Workers communicate with the API server instead of directly with the database.\n+- DAG processor and Triggerer utilize the task execution mechanism for their tasks, especially when they require variables or connections.\n+\n+Database Access Restrictions\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+In Airflow 3.x, direct metadata database access from task code is now restricted. This is a key security and architectural improvement that affects how DAG authors interact with Airflow resources:",
        "comment_created_at": "2025-07-03T05:55:35+00:00",
        "comment_author": "amoghrajesh",
        "comment_body": "```suggestion\r\nIn Airflow 3, direct metadata database access from task code is now restricted. This is a key security and architectural improvement that affects how DAG authors interact with Airflow resources:\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2181918584",
    "pr_number": 52196,
    "pr_file": "task-sdk/docs/concepts.rst",
    "created_at": "2025-07-03T05:58:55+00:00",
    "commented_code": "+.. Licensed to the Apache Software Foundation (ASF) under one\n+   or more contributor license agreements.  See the NOTICE file\n+   distributed with this work for additional information\n+   regarding copyright ownership.  The ASF licenses this file\n+   to you under the Apache License, Version 2.0 (the\n+   \"License\"); you may not use this file except in compliance\n+   with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+   software distributed under the License is distributed on an\n+   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+   KIND, either express or implied.  See the License for the\n+   specific language governing permissions and limitations\n+   under the License.\n+\n+Concepts\n+========\n+\n+This section covers the fundamental concepts that DAG authors need to understand when working with the Task SDK.\n+\n+.. note::\n+\n+    For information about Airflow 3.x architectural changes and database access restrictions, see the \"Upgrading to Airflow 3\" guide in the main Airflow documentation.\n+\n+Terminology\n+-----------\n+- **Task**: a Python function (decorated with ``@task``) or Operator invocation representing a unit of work in a DAG.\n+- **Task Execution**: the runtime machinery that executes user tasks in isolated subprocesses, managed via the Supervisor and Execution API.\n+\n+Task Lifecycle\n+--------------\n+\n+Understanding the task lifecycle helps DAG authors write more effective tasks and debug issues:\n+\n+- **Scheduled**: The Airflow scheduler enqueues the task instance. The Executor assigns a workload token used for subsequent API authentication and validation.\n+- **Queued**: Workers poll the queue to retrieve and reserve queued task instances.\n+- **Subprocess Launch**: The worker's Supervisor process spawns a dedicated subprocess (Task Runner) for the task instance, isolating its execution.\n+- **Run API Call**: The Supervisor sends a ``POST /run`` call to the Execution API to mark the task as running; the API server responds with a ``TIRunContext`` (including retry limits, fail-fast flags, etc.).\n+- **Resource Fetch**: During execution, if the task code requests Airflow resources (variables, connections, etc.), it writes a request to STDOUT. The Supervisor intercepts it, issues a corresponding API call, and writes the API response into the subprocess's STDIN.\n+- **Heartbeats & Token Renewal**: The Task Runner periodically emits ``POST /heartbeat`` calls. Each call authenticates via JWT; if the token has expired, the API server returns a refreshed token in the ``Refreshed-API-Token`` header.",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2181918584",
        "repo_full_name": "apache/airflow",
        "pr_number": 52196,
        "pr_file": "task-sdk/docs/concepts.rst",
        "discussion_id": "2181918584",
        "commented_code": "@@ -0,0 +1,62 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+   or more contributor license agreements.  See the NOTICE file\n+   distributed with this work for additional information\n+   regarding copyright ownership.  The ASF licenses this file\n+   to you under the Apache License, Version 2.0 (the\n+   \"License\"); you may not use this file except in compliance\n+   with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+   software distributed under the License is distributed on an\n+   \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+   KIND, either express or implied.  See the License for the\n+   specific language governing permissions and limitations\n+   under the License.\n+\n+Concepts\n+========\n+\n+This section covers the fundamental concepts that DAG authors need to understand when working with the Task SDK.\n+\n+.. note::\n+\n+    For information about Airflow 3.x architectural changes and database access restrictions, see the \"Upgrading to Airflow 3\" guide in the main Airflow documentation.\n+\n+Terminology\n+-----------\n+- **Task**: a Python function (decorated with ``@task``) or Operator invocation representing a unit of work in a DAG.\n+- **Task Execution**: the runtime machinery that executes user tasks in isolated subprocesses, managed via the Supervisor and Execution API.\n+\n+Task Lifecycle\n+--------------\n+\n+Understanding the task lifecycle helps DAG authors write more effective tasks and debug issues:\n+\n+- **Scheduled**: The Airflow scheduler enqueues the task instance. The Executor assigns a workload token used for subsequent API authentication and validation.\n+- **Queued**: Workers poll the queue to retrieve and reserve queued task instances.\n+- **Subprocess Launch**: The worker's Supervisor process spawns a dedicated subprocess (Task Runner) for the task instance, isolating its execution.\n+- **Run API Call**: The Supervisor sends a ``POST /run`` call to the Execution API to mark the task as running; the API server responds with a ``TIRunContext`` (including retry limits, fail-fast flags, etc.).\n+- **Resource Fetch**: During execution, if the task code requests Airflow resources (variables, connections, etc.), it writes a request to STDOUT. The Supervisor intercepts it, issues a corresponding API call, and writes the API response into the subprocess's STDIN.\n+- **Heartbeats & Token Renewal**: The Task Runner periodically emits ``POST /heartbeat`` calls. Each call authenticates via JWT; if the token has expired, the API server returns a refreshed token in the ``Refreshed-API-Token`` header.",
        "comment_created_at": "2025-07-03T05:58:55+00:00",
        "comment_author": "amoghrajesh",
        "comment_body": "```suggestion\r\n- **Heartbeats & Token Renewal**: The Task Runner periodically emits ``POST /heartbeat`` calls through the Supervisor. Each call authenticates via JWT; if the token has expired, the API server returns a refreshed token in the ``Refreshed-API-Token`` header.\r\n```",
        "pr_file_module": null
      }
    ]
  }
]