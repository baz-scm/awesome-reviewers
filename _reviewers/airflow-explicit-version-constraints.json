[
  {
    "discussion_id": "2159418851",
    "pr_number": 46891,
    "pr_file": "providers/amazon/pyproject.toml",
    "created_at": "2025-06-20T17:25:17+00:00",
    "commented_code": "\"s3fs>=2023.10.0\",\n ]\n \"python3-saml\" = [\n-    \"python3-saml>=1.16.0\",\n+    # Python 3 saml is not compatible with Python 3.13 yet, so we pin it to < 3.13\n+    \"python3-saml>=1.16.0; python_version < \\\"3.13\\\"\",",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2159418851",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/amazon/pyproject.toml",
        "discussion_id": "2159418851",
        "commented_code": "@@ -96,11 +97,12 @@ dependencies = [\n     \"s3fs>=2023.10.0\",\n ]\n \"python3-saml\" = [\n-    \"python3-saml>=1.16.0\",\n+    # Python 3 saml is not compatible with Python 3.13 yet, so we pin it to < 3.13\n+    \"python3-saml>=1.16.0; python_version < \\\"3.13\\\"\",",
        "comment_created_at": "2025-06-20T17:25:17+00:00",
        "comment_author": "jscheffl",
        "comment_body": "But with this pinning the dependency is then NOT installed when using Python 3.13? What limitations would we have then in Amazon provider?\r\nDo we need to add notes in README of provider for functional limitations? Or should Amazon provider in general be restricted for 3.13 use until the dependency is 3.13 installable?",
        "pr_file_module": null
      },
      {
        "comment_id": "2160046955",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/amazon/pyproject.toml",
        "discussion_id": "2159418851",
        "commented_code": "@@ -96,11 +97,12 @@ dependencies = [\n     \"s3fs>=2023.10.0\",\n ]\n \"python3-saml\" = [\n-    \"python3-saml>=1.16.0\",\n+    # Python 3 saml is not compatible with Python 3.13 yet, so we pin it to < 3.13\n+    \"python3-saml>=1.16.0; python_version < \\\"3.13\\\"\",",
        "comment_created_at": "2025-06-21T14:06:36+00:00",
        "comment_author": "potiuk",
        "comment_body": "No - it's far too small of a feature, it's only needed for one Amazon tool - can't remember which one - but yes we should likely make a comment. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2160048083",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/amazon/pyproject.toml",
        "discussion_id": "2159418851",
        "commented_code": "@@ -96,11 +97,12 @@ dependencies = [\n     \"s3fs>=2023.10.0\",\n ]\n \"python3-saml\" = [\n-    \"python3-saml>=1.16.0\",\n+    # Python 3 saml is not compatible with Python 3.13 yet, so we pin it to < 3.13\n+    \"python3-saml>=1.16.0; python_version < \\\"3.13\\\"\",",
        "comment_created_at": "2025-06-21T14:11:24+00:00",
        "comment_author": "potiuk",
        "comment_body": "Also this saml has been problematic already https://github.com/xmlsec/python-xmlsec/issues/344 -> depends on xml-sec which appears to be somewhat abandoned. ... I will gladly get rid of it :). cc: @ferruzzi @vincbeck @o-nikolas - is it still needed for Amazon ?  I believe it is used for one of the authentication modes in AWS - but since it is not going to work for Python 3.13, likely another way of running it is necessary. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2159437361",
    "pr_number": 46891,
    "pr_file": "providers/apache/hdfs/pyproject.toml",
    "created_at": "2025-06-20T17:41:11+00:00",
    "commented_code": "\"apache-airflow>=2.10.0\",\n     'hdfs[avro,dataframe,kerberos]>=2.5.4;python_version<\"3.12\"',\n     'hdfs[avro,dataframe,kerberos]>=2.7.3;python_version>=\"3.12\"',\n-    \"pandas>=2.1.2,<2.2\",\n+    'pandas>=2.1.2; python_version <\"3.13\"',\n+    'pandas>=2.2.0; python_version >=\"3.13\"',",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2159437361",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/apache/hdfs/pyproject.toml",
        "discussion_id": "2159437361",
        "commented_code": "@@ -60,7 +61,8 @@ dependencies = [\n     \"apache-airflow>=2.10.0\",\n     'hdfs[avro,dataframe,kerberos]>=2.5.4;python_version<\"3.12\"',\n     'hdfs[avro,dataframe,kerberos]>=2.7.3;python_version>=\"3.12\"',\n-    \"pandas>=2.1.2,<2.2\",\n+    'pandas>=2.1.2; python_version <\"3.13\"',\n+    'pandas>=2.2.0; python_version >=\"3.13\"',",
        "comment_created_at": "2025-06-20T17:41:11+00:00",
        "comment_author": "jscheffl",
        "comment_body": "Just checked release notes... Pandas team claims that 2.2.3 is the first version supporting Python 3.13. See https://pandas.pydata.org/docs/whatsnew/v2.2.3.html#pandas-2-2-3-is-now-compatible-with-python-3-13\r\n\r\nInterestingly the 3.13 compatability tag was just added for 2.3.0 in Pypi.\r\n\r\nNote I also attempted to `uv pip install pandas==2.2.0` in a Python 3.13 venv and this fails (on x86 as least).\r\n```suggestion\r\n    'pandas>=2.3.0; python_version >=\"3.13\"',\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2159438482",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/apache/hdfs/pyproject.toml",
        "discussion_id": "2159437361",
        "commented_code": "@@ -60,7 +61,8 @@ dependencies = [\n     \"apache-airflow>=2.10.0\",\n     'hdfs[avro,dataframe,kerberos]>=2.5.4;python_version<\"3.12\"',\n     'hdfs[avro,dataframe,kerberos]>=2.7.3;python_version>=\"3.12\"',\n-    \"pandas>=2.1.2,<2.2\",\n+    'pandas>=2.1.2; python_version <\"3.13\"',\n+    'pandas>=2.2.0; python_version >=\"3.13\"',",
        "comment_created_at": "2025-06-20T17:42:04+00:00",
        "comment_author": "jscheffl",
        "comment_body": "Do we actually need to add this Python 3.13 specifc pinning here or will the pandas meta data not already select 2.3.0 when installing with Python 3.13?",
        "pr_file_module": null
      },
      {
        "comment_id": "2160049635",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/apache/hdfs/pyproject.toml",
        "discussion_id": "2159437361",
        "commented_code": "@@ -60,7 +61,8 @@ dependencies = [\n     \"apache-airflow>=2.10.0\",\n     'hdfs[avro,dataframe,kerberos]>=2.5.4;python_version<\"3.12\"',\n     'hdfs[avro,dataframe,kerberos]>=2.7.3;python_version>=\"3.12\"',\n-    \"pandas>=2.1.2,<2.2\",\n+    'pandas>=2.1.2; python_version <\"3.13\"',\n+    'pandas>=2.2.0; python_version >=\"3.13\"',",
        "comment_created_at": "2025-06-21T14:19:02+00:00",
        "comment_author": "potiuk",
        "comment_body": "2.2.3 is fine - but I would leave it lower for lower version. We cannot entirely rely on metadata because when go to \"lowest\" version - it coul be that that version did not yet have != 3.13 exclusion. Most libraries do not add exclusions in advance - so something relased a year ago will not have !=3.13 and when we try to get the lowest version it might fail when building.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160049841",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/apache/hdfs/pyproject.toml",
        "discussion_id": "2159437361",
        "commented_code": "@@ -60,7 +61,8 @@ dependencies = [\n     \"apache-airflow>=2.10.0\",\n     'hdfs[avro,dataframe,kerberos]>=2.5.4;python_version<\"3.12\"',\n     'hdfs[avro,dataframe,kerberos]>=2.7.3;python_version>=\"3.12\"',\n-    \"pandas>=2.1.2,<2.2\",\n+    'pandas>=2.1.2; python_version <\"3.13\"',\n+    'pandas>=2.2.0; python_version >=\"3.13\"',",
        "comment_created_at": "2025-06-21T14:19:43+00:00",
        "comment_author": "potiuk",
        "comment_body": "I will just bump min version to 2.3.3 for 3.13. That should be fine.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2159449822",
    "pr_number": 46891,
    "pr_file": "providers/google/pyproject.toml",
    "created_at": "2025-06-20T17:52:20+00:00",
    "commented_code": "# See https://github.com/looker-open-source/sdk-codegen/issues/1518\n     \"looker-sdk>=22.4.0,!=24.18.0\",\n     \"pandas-gbq>=0.7.0\",\n-    # A transient dependency of google-cloud-bigquery-datatransfer, but we\n-    # further constrain it since older versions are buggy.\n-    \"proto-plus>=1.19.6\",\n+    'pandas>=2.1.2; python_version <\"3.13\"',\n+    'pandas>=2.2.0; python_version >=\"3.13\"',\n+    \"proto-plus>=1.26.0\",\n     # Used to write parquet files by BaseSqlToGCSOperator\n-    \"pyarrow>=14.0.1\",\n+    # TODO: remove it when apache-beam allows pyarrow 18+",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2159449822",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/google/pyproject.toml",
        "discussion_id": "2159449822",
        "commented_code": "@@ -127,11 +128,13 @@ dependencies = [\n     # See https://github.com/looker-open-source/sdk-codegen/issues/1518\n     \"looker-sdk>=22.4.0,!=24.18.0\",\n     \"pandas-gbq>=0.7.0\",\n-    # A transient dependency of google-cloud-bigquery-datatransfer, but we\n-    # further constrain it since older versions are buggy.\n-    \"proto-plus>=1.19.6\",\n+    'pandas>=2.1.2; python_version <\"3.13\"',\n+    'pandas>=2.2.0; python_version >=\"3.13\"',\n+    \"proto-plus>=1.26.0\",\n     # Used to write parquet files by BaseSqlToGCSOperator\n-    \"pyarrow>=14.0.1\",\n+    # TODO: remove it when apache-beam allows pyarrow 18+",
        "comment_created_at": "2025-06-20T17:52:20+00:00",
        "comment_author": "jscheffl",
        "comment_body": "Do we need to constraint it here explicitly? Would not uv/pip install check the dependency basedon what is installed? Or is this buggy?",
        "pr_file_module": null
      },
      {
        "comment_id": "2160050811",
        "repo_full_name": "apache/airflow",
        "pr_number": 46891,
        "pr_file": "providers/google/pyproject.toml",
        "discussion_id": "2159449822",
        "commented_code": "@@ -127,11 +128,13 @@ dependencies = [\n     # See https://github.com/looker-open-source/sdk-codegen/issues/1518\n     \"looker-sdk>=22.4.0,!=24.18.0\",\n     \"pandas-gbq>=0.7.0\",\n-    # A transient dependency of google-cloud-bigquery-datatransfer, but we\n-    # further constrain it since older versions are buggy.\n-    \"proto-plus>=1.19.6\",\n+    'pandas>=2.1.2; python_version <\"3.13\"',\n+    'pandas>=2.2.0; python_version >=\"3.13\"',\n+    \"proto-plus>=1.26.0\",\n     # Used to write parquet files by BaseSqlToGCSOperator\n-    \"pyarrow>=14.0.1\",\n+    # TODO: remove it when apache-beam allows pyarrow 18+",
        "comment_created_at": "2025-06-21T14:23:41+00:00",
        "comment_author": "potiuk",
        "comment_body": "Same reasons - no full metadata and uv fails trying to build the package on incompatible version because the package does not specify incompatibility.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172467430",
    "pr_number": 52356,
    "pr_file": "providers/snowflake/pyproject.toml",
    "created_at": "2025-06-27T16:58:23+00:00",
    "commented_code": "\"snowflake-connector-python>=3.7.1\",\n     \"snowflake-sqlalchemy>=1.4.0\",\n     \"snowflake-snowpark-python>=1.17.0;python_version<'3.12'\",\n+    \"snowflake-snowpark-python>=1.27.0;python_version>='3.12' and python_version<'3.13'\",",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2172467430",
        "repo_full_name": "apache/airflow",
        "pr_number": 52356,
        "pr_file": "providers/snowflake/pyproject.toml",
        "discussion_id": "2172467430",
        "commented_code": "@@ -65,6 +65,7 @@ dependencies = [\n     \"snowflake-connector-python>=3.7.1\",\n     \"snowflake-sqlalchemy>=1.4.0\",\n     \"snowflake-snowpark-python>=1.17.0;python_version<'3.12'\",\n+    \"snowflake-snowpark-python>=1.27.0;python_version>='3.12' and python_version<'3.13'\",",
        "comment_created_at": "2025-06-27T16:58:23+00:00",
        "comment_author": "potiuk",
        "comment_body": "This is not a good idea to limit it to <3.13 - unless we also specify version >= 3.13. There is a high chance that library supports Python 3.13 and we should not prevent it \"proactively\" - the Python 3.13 PR of mine seems to pass with snowpark tests after I modified few other deps - like pyarrow etc.. \r\n\r\nAnd there are a lot of libraries that do not update their trove classifiers specifying that they are now compatible (because this is more of a metadata hint that real statement). So unless we know that snowpark does not work with Python 3.13, we should not preemptively limit it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172479285",
        "repo_full_name": "apache/airflow",
        "pr_number": 52356,
        "pr_file": "providers/snowflake/pyproject.toml",
        "discussion_id": "2172467430",
        "commented_code": "@@ -65,6 +65,7 @@ dependencies = [\n     \"snowflake-connector-python>=3.7.1\",\n     \"snowflake-sqlalchemy>=1.4.0\",\n     \"snowflake-snowpark-python>=1.17.0;python_version<'3.12'\",\n+    \"snowflake-snowpark-python>=1.27.0;python_version>='3.12' and python_version<'3.13'\",",
        "comment_created_at": "2025-06-27T17:01:53+00:00",
        "comment_author": "eladkal",
        "comment_body": "We also didn't have a version for 3.12\r\nI checked and the library is not present in our 3.12 constraints\r\n\r\nFor 3.12 I know it wasn't supported before classifier was added https://github.com/snowflakedb/snowpark-python/pull/2852 they also explicitly limited \r\n`REQUIRED_PYTHON_VERSION = \">=3.8, <3.13\"`\r\n\r\nOfficial docs also don't specify Python 3.13 as supported\r\nhttps://docs.snowflake.com/en/developer-guide/snowpark/python/setup",
        "pr_file_module": null
      },
      {
        "comment_id": "2172495110",
        "repo_full_name": "apache/airflow",
        "pr_number": 52356,
        "pr_file": "providers/snowflake/pyproject.toml",
        "discussion_id": "2172467430",
        "commented_code": "@@ -65,6 +65,7 @@ dependencies = [\n     \"snowflake-connector-python>=3.7.1\",\n     \"snowflake-sqlalchemy>=1.4.0\",\n     \"snowflake-snowpark-python>=1.17.0;python_version<'3.12'\",\n+    \"snowflake-snowpark-python>=1.27.0;python_version>='3.12' and python_version<'3.13'\",",
        "comment_created_at": "2025-06-27T17:11:11+00:00",
        "comment_author": "potiuk",
        "comment_body": "Many libraries do not mention it because they \"just work\". We only learn about those libraries that declare their support because they had some incompatibility. And while there are `more` incompatibilities in 3.13 (mostly removal of deprecated stuff) than they were before, by default we should assume that the library works also for 3.13, unless our tests start failing. They do not fail for snowflake provider, so I assume it works. And knowing how snowpark works, I would not be suprised if it does because all that it does is it derives an SQL from the execution plan and sends it to Snowflake to execute.\r\n\r\nDo you have some indication or report that it does not work for 3.13 ?",
        "pr_file_module": null
      },
      {
        "comment_id": "2172497853",
        "repo_full_name": "apache/airflow",
        "pr_number": 52356,
        "pr_file": "providers/snowflake/pyproject.toml",
        "discussion_id": "2172467430",
        "commented_code": "@@ -65,6 +65,7 @@ dependencies = [\n     \"snowflake-connector-python>=3.7.1\",\n     \"snowflake-sqlalchemy>=1.4.0\",\n     \"snowflake-snowpark-python>=1.17.0;python_version<'3.12'\",\n+    \"snowflake-snowpark-python>=1.27.0;python_version>='3.12' and python_version<'3.13'\",",
        "comment_created_at": "2025-06-27T17:13:03+00:00",
        "comment_author": "potiuk",
        "comment_body": "(What I also mean - we have > 700 dependencies and if we do it for snowpark - that we look at their docs and see the exact compatibility documentation, why we do not do it for all 700 of them? I bet about 50% of them will noit have 3.13 declared officially now - what's so special about snowpark that we want to limit it?",
        "pr_file_module": null
      },
      {
        "comment_id": "2172506527",
        "repo_full_name": "apache/airflow",
        "pr_number": 52356,
        "pr_file": "providers/snowflake/pyproject.toml",
        "discussion_id": "2172467430",
        "commented_code": "@@ -65,6 +65,7 @@ dependencies = [\n     \"snowflake-connector-python>=3.7.1\",\n     \"snowflake-sqlalchemy>=1.4.0\",\n     \"snowflake-snowpark-python>=1.17.0;python_version<'3.12'\",\n+    \"snowflake-snowpark-python>=1.27.0;python_version>='3.12' and python_version<'3.13'\",",
        "comment_created_at": "2025-06-27T17:17:24+00:00",
        "comment_author": "eladkal",
        "comment_body": "i have no objection",
        "pr_file_module": null
      },
      {
        "comment_id": "2172506543",
        "repo_full_name": "apache/airflow",
        "pr_number": 52356,
        "pr_file": "providers/snowflake/pyproject.toml",
        "discussion_id": "2172467430",
        "commented_code": "@@ -65,6 +65,7 @@ dependencies = [\n     \"snowflake-connector-python>=3.7.1\",\n     \"snowflake-sqlalchemy>=1.4.0\",\n     \"snowflake-snowpark-python>=1.17.0;python_version<'3.12'\",\n+    \"snowflake-snowpark-python>=1.27.0;python_version>='3.12' and python_version<'3.13'\",",
        "comment_created_at": "2025-06-27T17:17:25+00:00",
        "comment_author": "potiuk",
        "comment_body": "Also - our general approach is \"optimistic\" - when it comes to upper-binding,  precisely because we test and verify a lot of things in our CI and there we can find issues. So when we make airfow Python 3.13 compliant in my PR - it already contains all the limitaions that we need to put in order for our tests to pass - and for us, this is \"decision point we have\" - passes our tests -> works. So when we actually merge Py 3.13, the limits in main will be already \"good\" and by the time we  release Airfow 3.1, we will also release the updated providers, and it means that it will be \"consistent\" with Py 3.13.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172512798",
        "repo_full_name": "apache/airflow",
        "pr_number": 52356,
        "pr_file": "providers/snowflake/pyproject.toml",
        "discussion_id": "2172467430",
        "commented_code": "@@ -65,6 +65,7 @@ dependencies = [\n     \"snowflake-connector-python>=3.7.1\",\n     \"snowflake-sqlalchemy>=1.4.0\",\n     \"snowflake-snowpark-python>=1.17.0;python_version<'3.12'\",\n+    \"snowflake-snowpark-python>=1.27.0;python_version>='3.12' and python_version<'3.13'\",",
        "comment_created_at": "2025-06-27T17:20:26+00:00",
        "comment_author": "potiuk",
        "comment_body": "And also another reason -> if we now limit < 3.13, we will have to actively monitor, when snowpark releases version that \"officialy\" support 3.13 to add it back, and this is something we really do not want to do (especially for 700 dependencies). It's actually more likely that even if currently snowpark passes the tests but there is \"some\" 3.13 incompatibility, by the time we release 3.1 they will have version that fixes it (and our constraints will generally get updated to it automatically).",
        "pr_file_module": null
      }
    ]
  }
]