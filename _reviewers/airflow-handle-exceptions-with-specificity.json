[
  {
    "discussion_id": "2084836196",
    "pr_number": 50371,
    "pr_file": "airflow-core/src/airflow/dag_processing/processor.py",
    "created_at": "2025-05-12T14:37:42+00:00",
    "commented_code": "def _parse_file(msg: DagFileParseRequest, log: FilteringBoundLogger) -> DagFileParsingResult | None:\n     # TODO: Set known_pool names on DagBag!\n+\n+    # Pre-import modules\n+    # Read the file to pre-import airflow modules used.\n+    # This prevents them from being re-imported from zero in each \"processing\" process\n+    # and saves CPU time and memory.\n+\n+    if conf.getboolean(\"scheduler\", \"parsing_pre_import_modules\", fallback=True):\n+        for module in iter_airflow_imports(msg.file):\n+            try:\n+                importlib.import_module(module)\n+            except Exception as e:",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2084836196",
        "repo_full_name": "apache/airflow",
        "pr_number": 50371,
        "pr_file": "airflow-core/src/airflow/dag_processing/processor.py",
        "discussion_id": "2084836196",
        "commented_code": "@@ -94,6 +96,21 @@ def _parse_file_entrypoint():\n \n def _parse_file(msg: DagFileParseRequest, log: FilteringBoundLogger) -> DagFileParsingResult | None:\n     # TODO: Set known_pool names on DagBag!\n+\n+    # Pre-import modules\n+    # Read the file to pre-import airflow modules used.\n+    # This prevents them from being re-imported from zero in each \"processing\" process\n+    # and saves CPU time and memory.\n+\n+    if conf.getboolean(\"scheduler\", \"parsing_pre_import_modules\", fallback=True):\n+        for module in iter_airflow_imports(msg.file):\n+            try:\n+                importlib.import_module(module)\n+            except Exception as e:",
        "comment_created_at": "2025-05-12T14:37:42+00:00",
        "comment_author": "Lee-W",
        "comment_body": "```suggestion\r\n            except Exception as e:\r\n```\r\n\r\nWe probably should use concrete exception here instead",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2173819092",
    "pr_number": 52053,
    "pr_file": "providers/standard/src/airflow/providers/standard/models/__init__.py",
    "created_at": "2025-06-29T16:13:49+00:00",
    "commented_code": "+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any\n+\n+import sqlalchemy_jsonfield\n+from sqlalchemy import Boolean, Column, ForeignKeyConstraint, Integer, MetaData, String, Text\n+from sqlalchemy.dialects import postgresql\n+from sqlalchemy.orm import registry, relationship\n+\n+from airflow.models.base import _get_schema, naming_convention\n+from airflow.settings import json\n+from airflow.utils import timezone\n+from airflow.utils.sqlalchemy import UtcDateTime\n+\n+metadata = MetaData(schema=_get_schema(), naming_convention=naming_convention)\n+mapper_registry = registry(metadata=metadata)\n+\n+if TYPE_CHECKING:\n+    Base = Any  # type: ignore[misc]\n+else:\n+    Base = mapper_registry.generate_base()\n+\n+\n+class HITLInputRequestModel(Base):\n+    \"\"\"Human-in-the-loop input request.\"\"\"\n+\n+    __tablename__ = \"hitl_input_request\"\n+    id = Column(Integer, primary_key=True, autoincrement=True)\n+    options = Column(sqlalchemy_jsonfield.JSONField(json=json), nullable=False, default=[])",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2173819092",
        "repo_full_name": "apache/airflow",
        "pr_number": 52053,
        "pr_file": "providers/standard/src/airflow/providers/standard/models/__init__.py",
        "discussion_id": "2173819092",
        "commented_code": "@@ -0,0 +1,91 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any\n+\n+import sqlalchemy_jsonfield\n+from sqlalchemy import Boolean, Column, ForeignKeyConstraint, Integer, MetaData, String, Text\n+from sqlalchemy.dialects import postgresql\n+from sqlalchemy.orm import registry, relationship\n+\n+from airflow.models.base import _get_schema, naming_convention\n+from airflow.settings import json\n+from airflow.utils import timezone\n+from airflow.utils.sqlalchemy import UtcDateTime\n+\n+metadata = MetaData(schema=_get_schema(), naming_convention=naming_convention)\n+mapper_registry = registry(metadata=metadata)\n+\n+if TYPE_CHECKING:\n+    Base = Any  # type: ignore[misc]\n+else:\n+    Base = mapper_registry.generate_base()\n+\n+\n+class HITLInputRequestModel(Base):\n+    \"\"\"Human-in-the-loop input request.\"\"\"\n+\n+    __tablename__ = \"hitl_input_request\"\n+    id = Column(Integer, primary_key=True, autoincrement=True)\n+    options = Column(sqlalchemy_jsonfield.JSONField(json=json), nullable=False, default=[])",
        "comment_created_at": "2025-06-29T16:13:49+00:00",
        "comment_author": "jscheffl",
        "comment_body": "I would propose not providing a default, [] would not be usable later. Rather raise an exception if no options provided.",
        "pr_file_module": null
      },
      {
        "comment_id": "2174049356",
        "repo_full_name": "apache/airflow",
        "pr_number": 52053,
        "pr_file": "providers/standard/src/airflow/providers/standard/models/__init__.py",
        "discussion_id": "2173819092",
        "commented_code": "@@ -0,0 +1,91 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any\n+\n+import sqlalchemy_jsonfield\n+from sqlalchemy import Boolean, Column, ForeignKeyConstraint, Integer, MetaData, String, Text\n+from sqlalchemy.dialects import postgresql\n+from sqlalchemy.orm import registry, relationship\n+\n+from airflow.models.base import _get_schema, naming_convention\n+from airflow.settings import json\n+from airflow.utils import timezone\n+from airflow.utils.sqlalchemy import UtcDateTime\n+\n+metadata = MetaData(schema=_get_schema(), naming_convention=naming_convention)\n+mapper_registry = registry(metadata=metadata)\n+\n+if TYPE_CHECKING:\n+    Base = Any  # type: ignore[misc]\n+else:\n+    Base = mapper_registry.generate_base()\n+\n+\n+class HITLInputRequestModel(Base):\n+    \"\"\"Human-in-the-loop input request.\"\"\"\n+\n+    __tablename__ = \"hitl_input_request\"\n+    id = Column(Integer, primary_key=True, autoincrement=True)\n+    options = Column(sqlalchemy_jsonfield.JSONField(json=json), nullable=False, default=[])",
        "comment_created_at": "2025-06-30T01:17:42+00:00",
        "comment_author": "Lee-W",
        "comment_body": "sure, will do. Thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2159080790",
    "pr_number": 51657,
    "pr_file": "providers/keycloak/src/airflow/providers/keycloak/auth_manager/keycloak_auth_manager.py",
    "created_at": "2025-06-20T14:08:12+00:00",
    "commented_code": "\"Authorization\": f\"Bearer {access_token}\",\n             \"Content-Type\": \"application/x-www-form-urlencoded\",\n         }\n+\n+    @classmethod\n+    def _get_keycloak_client(cls) -> KeycloakOpenID:\n+        client_id = conf.get(CONF_SECTION_NAME, CONF_CLIENT_ID_KEY)\n+        client_secret = conf.get(CONF_SECTION_NAME, CONF_CLIENT_SECRET_KEY)\n+        realm = conf.get(CONF_SECTION_NAME, CONF_REALM_KEY)\n+        server_url = conf.get(CONF_SECTION_NAME, CONF_SERVER_URL_KEY)\n+\n+        return KeycloakOpenID(\n+            server_url=server_url,\n+            client_id=client_id,\n+            client_secret_key=client_secret,\n+            realm_name=realm,\n+        )\n+\n+    def refresh_token(self, token: str) -> str | None:\n+        \"\"\"Refresh the access token for the user.\"\"\"\n+        client = self._get_keycloak_client()\n+        try:\n+            tokens = client.refresh_token(token)\n+            log.info(\"Token refreshed successfully\")\n+            return tokens[\"access_token\"]\n+        except InvalidTokenError:\n+            log.error(\"Invalid refresh token\")\n+            return None\n+        except Exception as e:\n+            log.error(\"Error refreshing token: %s\", e)\n+            return None",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2159080790",
        "repo_full_name": "apache/airflow",
        "pr_number": 51657,
        "pr_file": "providers/keycloak/src/airflow/providers/keycloak/auth_manager/keycloak_auth_manager.py",
        "discussion_id": "2159080790",
        "commented_code": "@@ -282,3 +289,47 @@ def _get_headers(access_token):\n             \"Authorization\": f\"Bearer {access_token}\",\n             \"Content-Type\": \"application/x-www-form-urlencoded\",\n         }\n+\n+    @classmethod\n+    def _get_keycloak_client(cls) -> KeycloakOpenID:\n+        client_id = conf.get(CONF_SECTION_NAME, CONF_CLIENT_ID_KEY)\n+        client_secret = conf.get(CONF_SECTION_NAME, CONF_CLIENT_SECRET_KEY)\n+        realm = conf.get(CONF_SECTION_NAME, CONF_REALM_KEY)\n+        server_url = conf.get(CONF_SECTION_NAME, CONF_SERVER_URL_KEY)\n+\n+        return KeycloakOpenID(\n+            server_url=server_url,\n+            client_id=client_id,\n+            client_secret_key=client_secret,\n+            realm_name=realm,\n+        )\n+\n+    def refresh_token(self, token: str) -> str | None:\n+        \"\"\"Refresh the access token for the user.\"\"\"\n+        client = self._get_keycloak_client()\n+        try:\n+            tokens = client.refresh_token(token)\n+            log.info(\"Token refreshed successfully\")\n+            return tokens[\"access_token\"]\n+        except InvalidTokenError:\n+            log.error(\"Invalid refresh token\")\n+            return None\n+        except Exception as e:\n+            log.error(\"Error refreshing token: %s\", e)\n+            return None",
        "comment_created_at": "2025-06-20T14:08:12+00:00",
        "comment_author": "vincbeck",
        "comment_body": "I dont think we want to swallow the exceptions. If these exceptions happen, this is not normal, so we should let them through.\r\n\r\n```suggestion\r\n    def refresh_token(self, refresh_token: str) -> str | None:\r\n        \"\"\"Refresh the access token for the user.\"\"\"\r\n        client = self._get_keycloak_client()\r\n        tokens = client.refresh_token(refresh_token)\r\n        log.info(\"Token refreshed successfully\")\r\n        return tokens[\"access_token\"]\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2160061540",
        "repo_full_name": "apache/airflow",
        "pr_number": 51657,
        "pr_file": "providers/keycloak/src/airflow/providers/keycloak/auth_manager/keycloak_auth_manager.py",
        "discussion_id": "2159080790",
        "commented_code": "@@ -282,3 +289,47 @@ def _get_headers(access_token):\n             \"Authorization\": f\"Bearer {access_token}\",\n             \"Content-Type\": \"application/x-www-form-urlencoded\",\n         }\n+\n+    @classmethod\n+    def _get_keycloak_client(cls) -> KeycloakOpenID:\n+        client_id = conf.get(CONF_SECTION_NAME, CONF_CLIENT_ID_KEY)\n+        client_secret = conf.get(CONF_SECTION_NAME, CONF_CLIENT_SECRET_KEY)\n+        realm = conf.get(CONF_SECTION_NAME, CONF_REALM_KEY)\n+        server_url = conf.get(CONF_SECTION_NAME, CONF_SERVER_URL_KEY)\n+\n+        return KeycloakOpenID(\n+            server_url=server_url,\n+            client_id=client_id,\n+            client_secret_key=client_secret,\n+            realm_name=realm,\n+        )\n+\n+    def refresh_token(self, token: str) -> str | None:\n+        \"\"\"Refresh the access token for the user.\"\"\"\n+        client = self._get_keycloak_client()\n+        try:\n+            tokens = client.refresh_token(token)\n+            log.info(\"Token refreshed successfully\")\n+            return tokens[\"access_token\"]\n+        except InvalidTokenError:\n+            log.error(\"Invalid refresh token\")\n+            return None\n+        except Exception as e:\n+            log.error(\"Error refreshing token: %s\", e)\n+            return None",
        "comment_created_at": "2025-06-21T15:11:02+00:00",
        "comment_author": "bugraoz93",
        "comment_body": "Updated",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2163069081",
    "pr_number": 52051,
    "pr_file": "providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py",
    "created_at": "2025-06-24T06:29:34+00:00",
    "commented_code": "def custom_obj_api(self) -> CustomObjectsApi:\n         return CustomObjectsApi()\n \n+    def update_pod_spec_add_xcom_sidecar(self):\n+        if self.do_xcom_push:\n+            try:\n+                self.log.debug(\"Adding xcom sidecar to driver pod spec in task %s\", self.task_id)\n+                driver_template = self.template_body[\"spark\"][\"spec\"]\n+                driver_with_xcom_template = add_sidecar_to_spark_operator_pod_spec(\n+                    driver_template,\n+                    sidecar_container_image=self.hook.get_xcom_sidecar_container_image(),\n+                    sidecar_container_resources=self.hook.get_xcom_sidecar_container_resources(),\n+                )\n+                self.template_body[\"spark\"][\"spec\"]= driver_with_xcom_template\n+            except KeyError as e:\n+                raise AirflowException(\"Spec missing in SparkApplication template\") from e",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2163069081",
        "repo_full_name": "apache/airflow",
        "pr_number": 52051,
        "pr_file": "providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py",
        "discussion_id": "2163069081",
        "commented_code": "@@ -293,10 +293,24 @@ def client(self) -> CoreV1Api:\n     def custom_obj_api(self) -> CustomObjectsApi:\n         return CustomObjectsApi()\n \n+    def update_pod_spec_add_xcom_sidecar(self):\n+        if self.do_xcom_push:\n+            try:\n+                self.log.debug(\"Adding xcom sidecar to driver pod spec in task %s\", self.task_id)\n+                driver_template = self.template_body[\"spark\"][\"spec\"]\n+                driver_with_xcom_template = add_sidecar_to_spark_operator_pod_spec(\n+                    driver_template,\n+                    sidecar_container_image=self.hook.get_xcom_sidecar_container_image(),\n+                    sidecar_container_resources=self.hook.get_xcom_sidecar_container_resources(),\n+                )\n+                self.template_body[\"spark\"][\"spec\"]= driver_with_xcom_template\n+            except KeyError as e:\n+                raise AirflowException(\"Spec missing in SparkApplication template\") from e",
        "comment_created_at": "2025-06-24T06:29:34+00:00",
        "comment_author": "Lee-W",
        "comment_body": "let's not use `AirflowException`, we can create a customized exception or just use KeyError",
        "pr_file_module": null
      },
      {
        "comment_id": "2173856282",
        "repo_full_name": "apache/airflow",
        "pr_number": 52051,
        "pr_file": "providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py",
        "discussion_id": "2163069081",
        "commented_code": "@@ -293,10 +293,24 @@ def client(self) -> CoreV1Api:\n     def custom_obj_api(self) -> CustomObjectsApi:\n         return CustomObjectsApi()\n \n+    def update_pod_spec_add_xcom_sidecar(self):\n+        if self.do_xcom_push:\n+            try:\n+                self.log.debug(\"Adding xcom sidecar to driver pod spec in task %s\", self.task_id)\n+                driver_template = self.template_body[\"spark\"][\"spec\"]\n+                driver_with_xcom_template = add_sidecar_to_spark_operator_pod_spec(\n+                    driver_template,\n+                    sidecar_container_image=self.hook.get_xcom_sidecar_container_image(),\n+                    sidecar_container_resources=self.hook.get_xcom_sidecar_container_resources(),\n+                )\n+                self.template_body[\"spark\"][\"spec\"]= driver_with_xcom_template\n+            except KeyError as e:\n+                raise AirflowException(\"Spec missing in SparkApplication template\") from e",
        "comment_created_at": "2025-06-29T18:40:39+00:00",
        "comment_author": "kesem0811",
        "comment_body": "Why?\r\nIn all the file, when something doesn't work as expected, an AirflowException is raised",
        "pr_file_module": null
      },
      {
        "comment_id": "2174446793",
        "repo_full_name": "apache/airflow",
        "pr_number": 52051,
        "pr_file": "providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py",
        "discussion_id": "2163069081",
        "commented_code": "@@ -293,10 +293,24 @@ def client(self) -> CoreV1Api:\n     def custom_obj_api(self) -> CustomObjectsApi:\n         return CustomObjectsApi()\n \n+    def update_pod_spec_add_xcom_sidecar(self):\n+        if self.do_xcom_push:\n+            try:\n+                self.log.debug(\"Adding xcom sidecar to driver pod spec in task %s\", self.task_id)\n+                driver_template = self.template_body[\"spark\"][\"spec\"]\n+                driver_with_xcom_template = add_sidecar_to_spark_operator_pod_spec(\n+                    driver_template,\n+                    sidecar_container_image=self.hook.get_xcom_sidecar_container_image(),\n+                    sidecar_container_resources=self.hook.get_xcom_sidecar_container_resources(),\n+                )\n+                self.template_body[\"spark\"][\"spec\"]= driver_with_xcom_template\n+            except KeyError as e:\n+                raise AirflowException(\"Spec missing in SparkApplication template\") from e",
        "comment_created_at": "2025-06-30T07:49:49+00:00",
        "comment_author": "Lee-W",
        "comment_body": "`AirflowException` is broad and not infomative",
        "pr_file_module": null
      },
      {
        "comment_id": "2174449403",
        "repo_full_name": "apache/airflow",
        "pr_number": 52051,
        "pr_file": "providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py",
        "discussion_id": "2163069081",
        "commented_code": "@@ -293,10 +293,24 @@ def client(self) -> CoreV1Api:\n     def custom_obj_api(self) -> CustomObjectsApi:\n         return CustomObjectsApi()\n \n+    def update_pod_spec_add_xcom_sidecar(self):\n+        if self.do_xcom_push:\n+            try:\n+                self.log.debug(\"Adding xcom sidecar to driver pod spec in task %s\", self.task_id)\n+                driver_template = self.template_body[\"spark\"][\"spec\"]\n+                driver_with_xcom_template = add_sidecar_to_spark_operator_pod_spec(\n+                    driver_template,\n+                    sidecar_container_image=self.hook.get_xcom_sidecar_container_image(),\n+                    sidecar_container_resources=self.hook.get_xcom_sidecar_container_resources(),\n+                )\n+                self.template_body[\"spark\"][\"spec\"]= driver_with_xcom_template\n+            except KeyError as e:\n+                raise AirflowException(\"Spec missing in SparkApplication template\") from e",
        "comment_created_at": "2025-06-30T07:51:14+00:00",
        "comment_author": "eladkal",
        "comment_body": "lets switch to KeyError",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2159634181",
    "pr_number": 51699,
    "pr_file": "task-sdk/src/airflow/sdk/execution_time/supervisor.py",
    "created_at": "2025-06-20T20:38:34+00:00",
    "commented_code": "self.selector.register(\n             requests,\n             selectors.EVENT_READ,\n-            make_buffered_socket_reader(self.handle_requests(log), on_close=self._on_socket_closed),\n+            length_prefixed_frame_reader(self.handle_requests(log), on_close=self._on_socket_closed),\n         )\n \n-    def _create_socket_handler(self, loggers, channel, log_level=logging.INFO) -> Callable[[socket], bool]:\n+    def _create_log_forwarder(self, loggers, channel, log_level=logging.INFO) -> Callable[[socket], bool]:\n         \"\"\"Create a socket handler that forwards logs to a logger.\"\"\"\n         return make_buffered_socket_reader(\n             forward_to_log(loggers, chan=channel, level=log_level), on_close=self._on_socket_closed\n         )\n \n-    def _on_socket_closed(self):\n+    def _on_socket_closed(self, sock: socket):\n         # We want to keep servicing this process until we've read up to EOF from all the sockets.\n-        self._num_open_sockets -= 1\n \n-    def send_msg(self, msg: BaseModel, **dump_opts):\n-        \"\"\"Send the given pydantic message to the subprocess at once by encoding it and adding a line break.\"\"\"\n-        b = msg.model_dump_json(**dump_opts).encode() + b\"\n\"\n-        self.stdin.sendall(b)\n+        with suppress(KeyError):\n+            self.selector.unregister(sock)\n+            del self._open_sockets[sock]\n+\n+    def send_msg(\n+        self, msg: BaseModel | None, request_id: int, error: ErrorResponse | None = None, **dump_opts\n+    ):\n+        \"\"\"\n+        Send the msg as a length-prefixed response frame.\n+\n+        ``request_id`` is the ID that the client sent in it's request, and has no meaning to the server\n+\n+        \"\"\"\n+        if msg:\n+            frame = _ResponseFrame(id=request_id, body=msg.model_dump(**dump_opts))\n+        else:\n+            err_resp = error.model_dump() if error else None\n+            frame = _ResponseFrame(id=request_id, error=err_resp)\n+\n+        self.stdin.sendall(frame.as_bytes())\n \n-    def handle_requests(self, log: FilteringBoundLogger) -> Generator[None, bytes, None]:\n+    def handle_requests(self, log: FilteringBoundLogger) -> Generator[None, _RequestFrame, None]:\n         \"\"\"Handle incoming requests from the task process, respond with the appropriate data.\"\"\"\n         while True:\n-            line = yield\n+            request = yield\n \n             try:\n-                msg = self.decoder.validate_json(line)\n+                msg = self.decoder.validate_python(request.body)\n             except Exception:\n-                log.exception(\"Unable to decode message\", line=line)\n+                log.exception(\"Unable to decode message\", body=request.body)\n                 continue",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2159634181",
        "repo_full_name": "apache/airflow",
        "pr_number": 51699,
        "pr_file": "task-sdk/src/airflow/sdk/execution_time/supervisor.py",
        "discussion_id": "2159634181",
        "commented_code": "@@ -552,37 +542,52 @@ def _register_pipe_readers(self, stdout: socket, stderr: socket, requests: socke\n         self.selector.register(\n             requests,\n             selectors.EVENT_READ,\n-            make_buffered_socket_reader(self.handle_requests(log), on_close=self._on_socket_closed),\n+            length_prefixed_frame_reader(self.handle_requests(log), on_close=self._on_socket_closed),\n         )\n \n-    def _create_socket_handler(self, loggers, channel, log_level=logging.INFO) -> Callable[[socket], bool]:\n+    def _create_log_forwarder(self, loggers, channel, log_level=logging.INFO) -> Callable[[socket], bool]:\n         \"\"\"Create a socket handler that forwards logs to a logger.\"\"\"\n         return make_buffered_socket_reader(\n             forward_to_log(loggers, chan=channel, level=log_level), on_close=self._on_socket_closed\n         )\n \n-    def _on_socket_closed(self):\n+    def _on_socket_closed(self, sock: socket):\n         # We want to keep servicing this process until we've read up to EOF from all the sockets.\n-        self._num_open_sockets -= 1\n \n-    def send_msg(self, msg: BaseModel, **dump_opts):\n-        \"\"\"Send the given pydantic message to the subprocess at once by encoding it and adding a line break.\"\"\"\n-        b = msg.model_dump_json(**dump_opts).encode() + b\"\\n\"\n-        self.stdin.sendall(b)\n+        with suppress(KeyError):\n+            self.selector.unregister(sock)\n+            del self._open_sockets[sock]\n+\n+    def send_msg(\n+        self, msg: BaseModel | None, request_id: int, error: ErrorResponse | None = None, **dump_opts\n+    ):\n+        \"\"\"\n+        Send the msg as a length-prefixed response frame.\n+\n+        ``request_id`` is the ID that the client sent in it's request, and has no meaning to the server\n+\n+        \"\"\"\n+        if msg:\n+            frame = _ResponseFrame(id=request_id, body=msg.model_dump(**dump_opts))\n+        else:\n+            err_resp = error.model_dump() if error else None\n+            frame = _ResponseFrame(id=request_id, error=err_resp)\n+\n+        self.stdin.sendall(frame.as_bytes())\n \n-    def handle_requests(self, log: FilteringBoundLogger) -> Generator[None, bytes, None]:\n+    def handle_requests(self, log: FilteringBoundLogger) -> Generator[None, _RequestFrame, None]:\n         \"\"\"Handle incoming requests from the task process, respond with the appropriate data.\"\"\"\n         while True:\n-            line = yield\n+            request = yield\n \n             try:\n-                msg = self.decoder.validate_json(line)\n+                msg = self.decoder.validate_python(request.body)\n             except Exception:\n-                log.exception(\"Unable to decode message\", line=line)\n+                log.exception(\"Unable to decode message\", body=request.body)\n                 continue",
        "comment_created_at": "2025-06-20T20:38:34+00:00",
        "comment_author": "gopidesupavan",
        "comment_body": "Why we are continuing? i think if we unable to decode we should respond ? ",
        "pr_file_module": null
      }
    ]
  }
]