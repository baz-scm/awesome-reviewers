[
  {
    "discussion_id": "850110542",
    "pr_number": 22958,
    "pr_file": "airflow/api/common/mark_tasks.py",
    "created_at": "2022-04-14T06:05:52+00:00",
    "commented_code": "if execution_date and not timezone.is_localized(execution_date):\n         raise ValueError(f\"Received non-localized date {execution_date}\")\n \n-    task_dags = {task.dag for task in tasks}\n+    t_dags = {task.dag for task in tasks if not isinstance(task, tuple)}\n+    t_dags_2 = {item[0].dag for item in tasks if isinstance(item, tuple)}\n+    task_dags = t_dags | t_dags_2",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "850110542",
        "repo_full_name": "apache/airflow",
        "pr_number": 22958,
        "pr_file": "airflow/api/common/mark_tasks.py",
        "discussion_id": "850110542",
        "commented_code": "@@ -118,7 +119,9 @@ def set_state(\n     if execution_date and not timezone.is_localized(execution_date):\n         raise ValueError(f\"Received non-localized date {execution_date}\")\n \n-    task_dags = {task.dag for task in tasks}\n+    t_dags = {task.dag for task in tasks if not isinstance(task, tuple)}\n+    t_dags_2 = {item[0].dag for item in tasks if isinstance(item, tuple)}\n+    task_dags = t_dags | t_dags_2",
        "comment_created_at": "2022-04-14T06:05:52+00:00",
        "comment_author": "uranusjr",
        "comment_body": "```suggestion\r\n    task_dags = {\r\n        task[0].dag if isinstance(task, tuple) else task.dag\r\n        for task in tasks\r\n    }\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2151425060",
    "pr_number": 49470,
    "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
    "created_at": "2025-06-17T06:28:44+00:00",
    "commented_code": "log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n \n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n+def _sort_key(timestamp: datetime | None, line_num: int) -> int:\n+    \"\"\"\n+    Generate a sort key for log record, to be used in K-way merge.\n \n-    records = itertools.chain.from_iterable(_parse_log_lines(log) for log in logs)\n+    :param timestamp: timestamp of the log line\n+    :param line_num: line number of the log line\n+    :return: a integer as sort key to avoid overhead of memory usage\n+    \"\"\"\n+    return int((timestamp or DEFAULT_SORT_DATETIME).timestamp() * 1000) * SORT_KEY_OFFSET + line_num\n+\n+\n+def _is_sort_key_with_default_timestamp(sort_key: int) -> bool:\n+    \"\"\"\n+    Check if the sort key was generated with the DEFAULT_SORT_TIMESTAMP.\n+\n+    This is used to identify log records that don't have timestamp.\n+\n+    :param sort_key: The sort key to check\n+    :return: True if the sort key was generated with DEFAULT_SORT_TIMESTAMP, False otherwise\n+    \"\"\"\n+    # Extract the timestamp part from the sort key (remove the line number part)\n+    timestamp_part = sort_key // SORT_KEY_OFFSET\n+    return timestamp_part == DEFAULT_SORT_TIMESTAMP\n+\n+\n+def _add_log_from_parsed_log_streams_to_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    parsed_log_streams: dict[int, ParsedLogStream],\n+) -> None:\n+    \"\"\"\n+    Add one log record from each parsed log stream to the heap, and will remove empty log stream from the dict after iterating.\n+\n+    :param heap: heap to store log records\n+    :param parsed_log_streams: dict of parsed log streams\n+    \"\"\"\n+    log_stream_to_remove: list[int] | None = None\n+    for idx, log_stream in parsed_log_streams.items():\n+        record: ParsedLog | None = next(log_stream, None)\n+        if record is None:\n+            if log_stream_to_remove is None:\n+                log_stream_to_remove = []\n+            log_stream_to_remove.append(idx)\n+            continue\n+        # add type hint to avoid mypy error\n+        record = cast(\"ParsedLog\", record)\n+        timestamp, line_num, line = record\n+        # take int as sort key to avoid overhead of memory usage\n+        heapq.heappush(heap, (_sort_key(timestamp, line_num), line))\n+    # remove empty log stream from the dict\n+    if log_stream_to_remove is not None:\n+        for idx in log_stream_to_remove:\n+            del parsed_log_streams[idx]\n+\n+\n+def _interleave_logs(*log_streams: RawLogStream) -> StructuredLogStream:\n+    \"\"\"\n+    Merge parsed log streams using K-way merge.\n+\n+    By yielding HALF_CHUNK_SIZE records when heap size exceeds CHUNK_SIZE, we can reduce the chance of messing up the global order.\n+    Since there are multiple log streams, we can't guarantee that the records are in global order.\n+\n+    e.g.\n+\n+    log_stream1: ----------\n+    log_stream2:   ----\n+    log_stream3:     --------\n+\n+    The first record of log_stream3 is later than the fourth record of log_stream1 !\n+    :param parsed_log_streams: parsed log streams\n+    :return: interleaved log stream\n+    \"\"\"\n+    # don't need to push whole tuple into heap, which increases too much overhead\n+    # push only sort_key and line into heap\n+    heap: list[tuple[int, StructuredLogMessage]] = []\n+    # to allow removing empty streams while iterating, also turn the str stream into parsed log stream\n+    parsed_log_streams: dict[int, ParsedLogStream] = {\n+        idx: _log_stream_to_parsed_log_stream(log_stream) for idx, log_stream in enumerate(log_streams)\n+    }\n+\n+    # keep adding records from logs until all logs are empty\n     last = None\n-    for timestamp, _, msg in sorted(records, key=lambda x: (x[0] or min_date, x[1])):\n-        if msg != last or not timestamp:  # dedupe\n-            yield msg\n-        last = msg\n+    while parsed_log_streams:\n+        _add_log_from_parsed_log_streams_to_heap(heap, parsed_log_streams)\n+\n+        # yield HALF_HEAP_DUMP_SIZE records when heap size exceeds HEAP_DUMP_SIZE\n+        if len(heap) >= HEAP_DUMP_SIZE:\n+            for _ in range(HALF_HEAP_DUMP_SIZE):\n+                sort_key, line = heapq.heappop(heap)\n+                if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+                    yield line\n+                last = line\n+\n+    # yield remaining records\n+    for _ in range(len(heap)):\n+        sort_key, line = heapq.heappop(heap)\n+        if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+            yield line\n+        last = line\n+    # free memory\n+    del heap\n+    del parsed_log_streams\n+\n+\n+def _is_logs_stream_like(log):\n+    \"\"\"Check if the logs are stream-like.\"\"\"\n+    return isinstance(log, chain) or isinstance(log, GeneratorType)",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2151425060",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
        "discussion_id": "2151425060",
        "commented_code": "@@ -166,17 +242,133 @@ def _parse_log_lines(\n                 log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n \n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n+def _sort_key(timestamp: datetime | None, line_num: int) -> int:\n+    \"\"\"\n+    Generate a sort key for log record, to be used in K-way merge.\n \n-    records = itertools.chain.from_iterable(_parse_log_lines(log) for log in logs)\n+    :param timestamp: timestamp of the log line\n+    :param line_num: line number of the log line\n+    :return: a integer as sort key to avoid overhead of memory usage\n+    \"\"\"\n+    return int((timestamp or DEFAULT_SORT_DATETIME).timestamp() * 1000) * SORT_KEY_OFFSET + line_num\n+\n+\n+def _is_sort_key_with_default_timestamp(sort_key: int) -> bool:\n+    \"\"\"\n+    Check if the sort key was generated with the DEFAULT_SORT_TIMESTAMP.\n+\n+    This is used to identify log records that don't have timestamp.\n+\n+    :param sort_key: The sort key to check\n+    :return: True if the sort key was generated with DEFAULT_SORT_TIMESTAMP, False otherwise\n+    \"\"\"\n+    # Extract the timestamp part from the sort key (remove the line number part)\n+    timestamp_part = sort_key // SORT_KEY_OFFSET\n+    return timestamp_part == DEFAULT_SORT_TIMESTAMP\n+\n+\n+def _add_log_from_parsed_log_streams_to_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    parsed_log_streams: dict[int, ParsedLogStream],\n+) -> None:\n+    \"\"\"\n+    Add one log record from each parsed log stream to the heap, and will remove empty log stream from the dict after iterating.\n+\n+    :param heap: heap to store log records\n+    :param parsed_log_streams: dict of parsed log streams\n+    \"\"\"\n+    log_stream_to_remove: list[int] | None = None\n+    for idx, log_stream in parsed_log_streams.items():\n+        record: ParsedLog | None = next(log_stream, None)\n+        if record is None:\n+            if log_stream_to_remove is None:\n+                log_stream_to_remove = []\n+            log_stream_to_remove.append(idx)\n+            continue\n+        # add type hint to avoid mypy error\n+        record = cast(\"ParsedLog\", record)\n+        timestamp, line_num, line = record\n+        # take int as sort key to avoid overhead of memory usage\n+        heapq.heappush(heap, (_sort_key(timestamp, line_num), line))\n+    # remove empty log stream from the dict\n+    if log_stream_to_remove is not None:\n+        for idx in log_stream_to_remove:\n+            del parsed_log_streams[idx]\n+\n+\n+def _interleave_logs(*log_streams: RawLogStream) -> StructuredLogStream:\n+    \"\"\"\n+    Merge parsed log streams using K-way merge.\n+\n+    By yielding HALF_CHUNK_SIZE records when heap size exceeds CHUNK_SIZE, we can reduce the chance of messing up the global order.\n+    Since there are multiple log streams, we can't guarantee that the records are in global order.\n+\n+    e.g.\n+\n+    log_stream1: ----------\n+    log_stream2:   ----\n+    log_stream3:     --------\n+\n+    The first record of log_stream3 is later than the fourth record of log_stream1 !\n+    :param parsed_log_streams: parsed log streams\n+    :return: interleaved log stream\n+    \"\"\"\n+    # don't need to push whole tuple into heap, which increases too much overhead\n+    # push only sort_key and line into heap\n+    heap: list[tuple[int, StructuredLogMessage]] = []\n+    # to allow removing empty streams while iterating, also turn the str stream into parsed log stream\n+    parsed_log_streams: dict[int, ParsedLogStream] = {\n+        idx: _log_stream_to_parsed_log_stream(log_stream) for idx, log_stream in enumerate(log_streams)\n+    }\n+\n+    # keep adding records from logs until all logs are empty\n     last = None\n-    for timestamp, _, msg in sorted(records, key=lambda x: (x[0] or min_date, x[1])):\n-        if msg != last or not timestamp:  # dedupe\n-            yield msg\n-        last = msg\n+    while parsed_log_streams:\n+        _add_log_from_parsed_log_streams_to_heap(heap, parsed_log_streams)\n+\n+        # yield HALF_HEAP_DUMP_SIZE records when heap size exceeds HEAP_DUMP_SIZE\n+        if len(heap) >= HEAP_DUMP_SIZE:\n+            for _ in range(HALF_HEAP_DUMP_SIZE):\n+                sort_key, line = heapq.heappop(heap)\n+                if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+                    yield line\n+                last = line\n+\n+    # yield remaining records\n+    for _ in range(len(heap)):\n+        sort_key, line = heapq.heappop(heap)\n+        if line != last or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+            yield line\n+        last = line\n+    # free memory\n+    del heap\n+    del parsed_log_streams\n+\n+\n+def _is_logs_stream_like(log):\n+    \"\"\"Check if the logs are stream-like.\"\"\"\n+    return isinstance(log, chain) or isinstance(log, GeneratorType)",
        "comment_created_at": "2025-06-17T06:28:44+00:00",
        "comment_author": "Lee-W",
        "comment_body": "```suggestion\r\n    return isinstance(log, (chain, GeneratorType))\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2159892851",
    "pr_number": 49470,
    "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
    "created_at": "2025-06-21T06:57:44+00:00",
    "commented_code": "log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n+\n \n+def _create_sort_key(timestamp: datetime | None, line_num: int) -> int:\n+    \"\"\"\n+    Create a sort key for log record, to be used in K-way merge.\n+\n+    :param timestamp: timestamp of the log line\n+    :param line_num: line number of the log line\n+    :return: a integer as sort key to avoid overhead of memory usage\n+    \"\"\"\n+    return int((timestamp or DEFAULT_SORT_DATETIME).timestamp() * 1000) * SORT_KEY_OFFSET + line_num\n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n \n-    records = itertools.chain.from_iterable(_parse_log_lines(log) for log in logs)\n-    last = None\n-    for timestamp, _, msg in sorted(records, key=lambda x: (x[0] or min_date, x[1])):\n-        if msg != last or not timestamp:  # dedupe\n-            yield msg\n-        last = msg\n+def _is_sort_key_with_default_timestamp(sort_key: int) -> bool:\n+    \"\"\"\n+    Check if the sort key was generated with the DEFAULT_SORT_TIMESTAMP.\n+\n+    This is used to identify log records that don't have timestamp.\n+\n+    :param sort_key: The sort key to check\n+    :return: True if the sort key was generated with DEFAULT_SORT_TIMESTAMP, False otherwise\n+    \"\"\"\n+    # Extract the timestamp part from the sort key (remove the line number part)\n+    timestamp_part = sort_key // SORT_KEY_OFFSET\n+    return timestamp_part == DEFAULT_SORT_TIMESTAMP\n+\n+\n+def _add_log_from_parsed_log_streams_to_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    parsed_log_streams: dict[int, ParsedLogStream],\n+) -> None:\n+    \"\"\"\n+    Add one log record from each parsed log stream to the heap, and will remove empty log stream from the dict after iterating.\n+\n+    :param heap: heap to store log records\n+    :param parsed_log_streams: dict of parsed log streams\n+    \"\"\"\n+    log_stream_to_remove: list[int] | None = None\n+    for idx, log_stream in parsed_log_streams.items():\n+        record: ParsedLog | None = next(log_stream, None)\n+        if record is None:\n+            if log_stream_to_remove is None:\n+                log_stream_to_remove = []\n+            log_stream_to_remove.append(idx)\n+            continue\n+        # add type hint to avoid mypy error\n+        record = cast(\"ParsedLog\", record)\n+        timestamp, line_num, line = record\n+        # take int as sort key to avoid overhead of memory usage\n+        heapq.heappush(heap, (_create_sort_key(timestamp, line_num), line))\n+    # remove empty log stream from the dict\n+    if log_stream_to_remove is not None:\n+        for idx in log_stream_to_remove:\n+            del parsed_log_streams[idx]\n+\n+\n+def _flush_logs_out_of_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    flush_size: int,\n+    last_log_container: list[StructuredLogMessage | None],\n+) -> Generator[StructuredLogMessage, None, None]:\n+    \"\"\"\n+    Flush logs out of the heap, deduplicating them based on the last log.\n+\n+    :param heap: heap to flush logs from\n+    :param flush_size: number of logs to flush\n+    :param last_log_container: a container to store the last log, to avoid duplicate logs\n+    :return: a generator that yields deduplicated logs\n+    \"\"\"\n+    last_log = last_log_container[0]\n+    for _ in range(flush_size):\n+        sort_key, line = heapq.heappop(heap)\n+        if line != last_log or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+            yield line\n+        last_log = line\n+    # update the last log container with the last log\n+    last_log_container[0] = last_log\n+\n+\n+def _interleave_logs(*log_streams: RawLogStream) -> StructuredLogStream:\n+    \"\"\"\n+    Merge parsed log streams using K-way merge.\n+\n+    By yielding HALF_CHUNK_SIZE records when heap size exceeds CHUNK_SIZE, we can reduce the chance of messing up the global order.\n+    Since there are multiple log streams, we can't guarantee that the records are in global order.\n+\n+    e.g.\n+\n+    log_stream1: ----------\n+    log_stream2:   ----\n+    log_stream3:     --------\n+\n+    The first record of log_stream3 is later than the fourth record of log_stream1 !\n+    :param parsed_log_streams: parsed log streams\n+    :return: interleaved log stream\n+    \"\"\"\n+    # don't need to push whole tuple into heap, which increases too much overhead\n+    # push only sort_key and line into heap\n+    heap: list[tuple[int, StructuredLogMessage]] = []\n+    # to allow removing empty streams while iterating, also turn the str stream into parsed log stream\n+    parsed_log_streams: dict[int, ParsedLogStream] = {\n+        idx: _log_stream_to_parsed_log_stream(log_stream) for idx, log_stream in enumerate(log_streams)\n+    }\n+\n+    # keep adding records from logs until all logs are empty\n+    last_log_container: list[StructuredLogMessage | None] = [None]\n+    while parsed_log_streams:\n+        _add_log_from_parsed_log_streams_to_heap(heap, parsed_log_streams)\n+\n+        # yield HALF_HEAP_DUMP_SIZE records when heap size exceeds HEAP_DUMP_SIZE\n+        if len(heap) >= HEAP_DUMP_SIZE:\n+            yield from _flush_logs_out_of_heap(heap, HALF_HEAP_DUMP_SIZE, last_log_container)\n+\n+    # yield remaining records\n+    yield from _flush_logs_out_of_heap(heap, len(heap), last_log_container)\n+    # free memory\n+    del heap\n+    del parsed_log_streams\n+\n+\n+def _is_logs_stream_like(log) -> bool:\n+    \"\"\"Check if the logs are stream-like.\"\"\"\n+    return isinstance(log, (chain, GeneratorType))\n+\n+\n+def _get_compatible_log_stream(\n+    log_messages: LogMessages,\n+) -> RawLogStream:\n+    \"\"\"\n+    Convert legacy log message blobs into a generator that yields log lines.\n+\n+    :param log_messages: List of legacy log message strings.\n+    :return: A generator that yields interleaved log lines.\n+    \"\"\"\n+    log_streams: list[RawLogStream] = [\n+        _stream_lines_by_chunk(io.StringIO(log_message)) for log_message in log_messages\n+    ]\n+\n+    for log_stream in log_streams:\n+        yield from log_stream",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2159892851",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
        "discussion_id": "2159892851",
        "commented_code": "@@ -166,17 +248,147 @@ def _parse_log_lines(\n                 log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n+\n \n+def _create_sort_key(timestamp: datetime | None, line_num: int) -> int:\n+    \"\"\"\n+    Create a sort key for log record, to be used in K-way merge.\n+\n+    :param timestamp: timestamp of the log line\n+    :param line_num: line number of the log line\n+    :return: a integer as sort key to avoid overhead of memory usage\n+    \"\"\"\n+    return int((timestamp or DEFAULT_SORT_DATETIME).timestamp() * 1000) * SORT_KEY_OFFSET + line_num\n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n \n-    records = itertools.chain.from_iterable(_parse_log_lines(log) for log in logs)\n-    last = None\n-    for timestamp, _, msg in sorted(records, key=lambda x: (x[0] or min_date, x[1])):\n-        if msg != last or not timestamp:  # dedupe\n-            yield msg\n-        last = msg\n+def _is_sort_key_with_default_timestamp(sort_key: int) -> bool:\n+    \"\"\"\n+    Check if the sort key was generated with the DEFAULT_SORT_TIMESTAMP.\n+\n+    This is used to identify log records that don't have timestamp.\n+\n+    :param sort_key: The sort key to check\n+    :return: True if the sort key was generated with DEFAULT_SORT_TIMESTAMP, False otherwise\n+    \"\"\"\n+    # Extract the timestamp part from the sort key (remove the line number part)\n+    timestamp_part = sort_key // SORT_KEY_OFFSET\n+    return timestamp_part == DEFAULT_SORT_TIMESTAMP\n+\n+\n+def _add_log_from_parsed_log_streams_to_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    parsed_log_streams: dict[int, ParsedLogStream],\n+) -> None:\n+    \"\"\"\n+    Add one log record from each parsed log stream to the heap, and will remove empty log stream from the dict after iterating.\n+\n+    :param heap: heap to store log records\n+    :param parsed_log_streams: dict of parsed log streams\n+    \"\"\"\n+    log_stream_to_remove: list[int] | None = None\n+    for idx, log_stream in parsed_log_streams.items():\n+        record: ParsedLog | None = next(log_stream, None)\n+        if record is None:\n+            if log_stream_to_remove is None:\n+                log_stream_to_remove = []\n+            log_stream_to_remove.append(idx)\n+            continue\n+        # add type hint to avoid mypy error\n+        record = cast(\"ParsedLog\", record)\n+        timestamp, line_num, line = record\n+        # take int as sort key to avoid overhead of memory usage\n+        heapq.heappush(heap, (_create_sort_key(timestamp, line_num), line))\n+    # remove empty log stream from the dict\n+    if log_stream_to_remove is not None:\n+        for idx in log_stream_to_remove:\n+            del parsed_log_streams[idx]\n+\n+\n+def _flush_logs_out_of_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    flush_size: int,\n+    last_log_container: list[StructuredLogMessage | None],\n+) -> Generator[StructuredLogMessage, None, None]:\n+    \"\"\"\n+    Flush logs out of the heap, deduplicating them based on the last log.\n+\n+    :param heap: heap to flush logs from\n+    :param flush_size: number of logs to flush\n+    :param last_log_container: a container to store the last log, to avoid duplicate logs\n+    :return: a generator that yields deduplicated logs\n+    \"\"\"\n+    last_log = last_log_container[0]\n+    for _ in range(flush_size):\n+        sort_key, line = heapq.heappop(heap)\n+        if line != last_log or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+            yield line\n+        last_log = line\n+    # update the last log container with the last log\n+    last_log_container[0] = last_log\n+\n+\n+def _interleave_logs(*log_streams: RawLogStream) -> StructuredLogStream:\n+    \"\"\"\n+    Merge parsed log streams using K-way merge.\n+\n+    By yielding HALF_CHUNK_SIZE records when heap size exceeds CHUNK_SIZE, we can reduce the chance of messing up the global order.\n+    Since there are multiple log streams, we can't guarantee that the records are in global order.\n+\n+    e.g.\n+\n+    log_stream1: ----------\n+    log_stream2:   ----\n+    log_stream3:     --------\n+\n+    The first record of log_stream3 is later than the fourth record of log_stream1 !\n+    :param parsed_log_streams: parsed log streams\n+    :return: interleaved log stream\n+    \"\"\"\n+    # don't need to push whole tuple into heap, which increases too much overhead\n+    # push only sort_key and line into heap\n+    heap: list[tuple[int, StructuredLogMessage]] = []\n+    # to allow removing empty streams while iterating, also turn the str stream into parsed log stream\n+    parsed_log_streams: dict[int, ParsedLogStream] = {\n+        idx: _log_stream_to_parsed_log_stream(log_stream) for idx, log_stream in enumerate(log_streams)\n+    }\n+\n+    # keep adding records from logs until all logs are empty\n+    last_log_container: list[StructuredLogMessage | None] = [None]\n+    while parsed_log_streams:\n+        _add_log_from_parsed_log_streams_to_heap(heap, parsed_log_streams)\n+\n+        # yield HALF_HEAP_DUMP_SIZE records when heap size exceeds HEAP_DUMP_SIZE\n+        if len(heap) >= HEAP_DUMP_SIZE:\n+            yield from _flush_logs_out_of_heap(heap, HALF_HEAP_DUMP_SIZE, last_log_container)\n+\n+    # yield remaining records\n+    yield from _flush_logs_out_of_heap(heap, len(heap), last_log_container)\n+    # free memory\n+    del heap\n+    del parsed_log_streams\n+\n+\n+def _is_logs_stream_like(log) -> bool:\n+    \"\"\"Check if the logs are stream-like.\"\"\"\n+    return isinstance(log, (chain, GeneratorType))\n+\n+\n+def _get_compatible_log_stream(\n+    log_messages: LogMessages,\n+) -> RawLogStream:\n+    \"\"\"\n+    Convert legacy log message blobs into a generator that yields log lines.\n+\n+    :param log_messages: List of legacy log message strings.\n+    :return: A generator that yields interleaved log lines.\n+    \"\"\"\n+    log_streams: list[RawLogStream] = [\n+        _stream_lines_by_chunk(io.StringIO(log_message)) for log_message in log_messages\n+    ]\n+\n+    for log_stream in log_streams:\n+        yield from log_stream",
        "comment_created_at": "2025-06-21T06:57:44+00:00",
        "comment_author": "Lee-W",
        "comment_body": "```suggestion\r\n    yield from chain.from_iterable(\r\n        _stream_lines_by_chunk(io.StringIO(log_message)) for log_message in log_messages\r\n   )\r\n```\r\n\r\nwe can make it a standalone variable if we want to keep the type",
        "pr_file_module": null
      },
      {
        "comment_id": "2160351048",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
        "discussion_id": "2159892851",
        "commented_code": "@@ -166,17 +248,147 @@ def _parse_log_lines(\n                 log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n+\n \n+def _create_sort_key(timestamp: datetime | None, line_num: int) -> int:\n+    \"\"\"\n+    Create a sort key for log record, to be used in K-way merge.\n+\n+    :param timestamp: timestamp of the log line\n+    :param line_num: line number of the log line\n+    :return: a integer as sort key to avoid overhead of memory usage\n+    \"\"\"\n+    return int((timestamp or DEFAULT_SORT_DATETIME).timestamp() * 1000) * SORT_KEY_OFFSET + line_num\n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n \n-    records = itertools.chain.from_iterable(_parse_log_lines(log) for log in logs)\n-    last = None\n-    for timestamp, _, msg in sorted(records, key=lambda x: (x[0] or min_date, x[1])):\n-        if msg != last or not timestamp:  # dedupe\n-            yield msg\n-        last = msg\n+def _is_sort_key_with_default_timestamp(sort_key: int) -> bool:\n+    \"\"\"\n+    Check if the sort key was generated with the DEFAULT_SORT_TIMESTAMP.\n+\n+    This is used to identify log records that don't have timestamp.\n+\n+    :param sort_key: The sort key to check\n+    :return: True if the sort key was generated with DEFAULT_SORT_TIMESTAMP, False otherwise\n+    \"\"\"\n+    # Extract the timestamp part from the sort key (remove the line number part)\n+    timestamp_part = sort_key // SORT_KEY_OFFSET\n+    return timestamp_part == DEFAULT_SORT_TIMESTAMP\n+\n+\n+def _add_log_from_parsed_log_streams_to_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    parsed_log_streams: dict[int, ParsedLogStream],\n+) -> None:\n+    \"\"\"\n+    Add one log record from each parsed log stream to the heap, and will remove empty log stream from the dict after iterating.\n+\n+    :param heap: heap to store log records\n+    :param parsed_log_streams: dict of parsed log streams\n+    \"\"\"\n+    log_stream_to_remove: list[int] | None = None\n+    for idx, log_stream in parsed_log_streams.items():\n+        record: ParsedLog | None = next(log_stream, None)\n+        if record is None:\n+            if log_stream_to_remove is None:\n+                log_stream_to_remove = []\n+            log_stream_to_remove.append(idx)\n+            continue\n+        # add type hint to avoid mypy error\n+        record = cast(\"ParsedLog\", record)\n+        timestamp, line_num, line = record\n+        # take int as sort key to avoid overhead of memory usage\n+        heapq.heappush(heap, (_create_sort_key(timestamp, line_num), line))\n+    # remove empty log stream from the dict\n+    if log_stream_to_remove is not None:\n+        for idx in log_stream_to_remove:\n+            del parsed_log_streams[idx]\n+\n+\n+def _flush_logs_out_of_heap(\n+    heap: list[tuple[int, StructuredLogMessage]],\n+    flush_size: int,\n+    last_log_container: list[StructuredLogMessage | None],\n+) -> Generator[StructuredLogMessage, None, None]:\n+    \"\"\"\n+    Flush logs out of the heap, deduplicating them based on the last log.\n+\n+    :param heap: heap to flush logs from\n+    :param flush_size: number of logs to flush\n+    :param last_log_container: a container to store the last log, to avoid duplicate logs\n+    :return: a generator that yields deduplicated logs\n+    \"\"\"\n+    last_log = last_log_container[0]\n+    for _ in range(flush_size):\n+        sort_key, line = heapq.heappop(heap)\n+        if line != last_log or _is_sort_key_with_default_timestamp(sort_key):  # dedupe\n+            yield line\n+        last_log = line\n+    # update the last log container with the last log\n+    last_log_container[0] = last_log\n+\n+\n+def _interleave_logs(*log_streams: RawLogStream) -> StructuredLogStream:\n+    \"\"\"\n+    Merge parsed log streams using K-way merge.\n+\n+    By yielding HALF_CHUNK_SIZE records when heap size exceeds CHUNK_SIZE, we can reduce the chance of messing up the global order.\n+    Since there are multiple log streams, we can't guarantee that the records are in global order.\n+\n+    e.g.\n+\n+    log_stream1: ----------\n+    log_stream2:   ----\n+    log_stream3:     --------\n+\n+    The first record of log_stream3 is later than the fourth record of log_stream1 !\n+    :param parsed_log_streams: parsed log streams\n+    :return: interleaved log stream\n+    \"\"\"\n+    # don't need to push whole tuple into heap, which increases too much overhead\n+    # push only sort_key and line into heap\n+    heap: list[tuple[int, StructuredLogMessage]] = []\n+    # to allow removing empty streams while iterating, also turn the str stream into parsed log stream\n+    parsed_log_streams: dict[int, ParsedLogStream] = {\n+        idx: _log_stream_to_parsed_log_stream(log_stream) for idx, log_stream in enumerate(log_streams)\n+    }\n+\n+    # keep adding records from logs until all logs are empty\n+    last_log_container: list[StructuredLogMessage | None] = [None]\n+    while parsed_log_streams:\n+        _add_log_from_parsed_log_streams_to_heap(heap, parsed_log_streams)\n+\n+        # yield HALF_HEAP_DUMP_SIZE records when heap size exceeds HEAP_DUMP_SIZE\n+        if len(heap) >= HEAP_DUMP_SIZE:\n+            yield from _flush_logs_out_of_heap(heap, HALF_HEAP_DUMP_SIZE, last_log_container)\n+\n+    # yield remaining records\n+    yield from _flush_logs_out_of_heap(heap, len(heap), last_log_container)\n+    # free memory\n+    del heap\n+    del parsed_log_streams\n+\n+\n+def _is_logs_stream_like(log) -> bool:\n+    \"\"\"Check if the logs are stream-like.\"\"\"\n+    return isinstance(log, (chain, GeneratorType))\n+\n+\n+def _get_compatible_log_stream(\n+    log_messages: LogMessages,\n+) -> RawLogStream:\n+    \"\"\"\n+    Convert legacy log message blobs into a generator that yields log lines.\n+\n+    :param log_messages: List of legacy log message strings.\n+    :return: A generator that yields interleaved log lines.\n+    \"\"\"\n+    log_streams: list[RawLogStream] = [\n+        _stream_lines_by_chunk(io.StringIO(log_message)) for log_message in log_messages\n+    ]\n+\n+    for log_stream in log_streams:\n+        yield from log_stream",
        "comment_created_at": "2025-06-22T13:35:05+00:00",
        "comment_author": "jason810496",
        "comment_body": "Adapt with `chain.from_iterable`, first time to use this utility! ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2162965905",
    "pr_number": 51949,
    "pr_file": "airflow-core/src/airflow/models/dagrun.py",
    "created_at": "2025-06-24T05:02:25+00:00",
    "commented_code": "def handle_dag_callback(self, dag: SDKDAG, success: bool = True, reason: str = \"success\"):\n         \"\"\"Only needed for `dag.test` where `execute_callbacks=True` is passed to `update_state`.\"\"\"\n+        task_instances = self.get_task_instances()\n+\n+        # Identify the most relevant task instance\n+        last_relevant_ti = None\n+        if not success:\n+            failed_tis = [ti for ti in task_instances if ti.state in State.failed_states and ti.end_date]\n+            failed_tis.sort(key=lambda x: x.end_date, reverse=True)\n+            last_relevant_ti = failed_tis[0] if failed_tis else None\n+        else:\n+            success_tis = [ti for ti in task_instances if ti.state in State.success_states and ti.end_date]\n+            success_tis.sort(key=lambda x: x.end_date, reverse=True)\n+            last_relevant_ti = success_tis[0] if success_tis else None",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2162965905",
        "repo_full_name": "apache/airflow",
        "pr_number": 51949,
        "pr_file": "airflow-core/src/airflow/models/dagrun.py",
        "discussion_id": "2162965905",
        "commented_code": "@@ -1352,21 +1352,48 @@ def notify_dagrun_state_changed(self, msg: str = \"\"):\n \n     def handle_dag_callback(self, dag: SDKDAG, success: bool = True, reason: str = \"success\"):\n         \"\"\"Only needed for `dag.test` where `execute_callbacks=True` is passed to `update_state`.\"\"\"\n+        task_instances = self.get_task_instances()\n+\n+        # Identify the most relevant task instance\n+        last_relevant_ti = None\n+        if not success:\n+            failed_tis = [ti for ti in task_instances if ti.state in State.failed_states and ti.end_date]\n+            failed_tis.sort(key=lambda x: x.end_date, reverse=True)\n+            last_relevant_ti = failed_tis[0] if failed_tis else None\n+        else:\n+            success_tis = [ti for ti in task_instances if ti.state in State.success_states and ti.end_date]\n+            success_tis.sort(key=lambda x: x.end_date, reverse=True)\n+            last_relevant_ti = success_tis[0] if success_tis else None",
        "comment_created_at": "2025-06-24T05:02:25+00:00",
        "comment_author": "uranusjr",
        "comment_body": "This can be something like\r\n\r\n```python\r\nlast_relevant_ti = max(success_tis, ...) if success_tis else None\r\n```\r\n\r\ninstead.\r\n\r\nI kind of wonder if we can even avoid building the list at all.\r\n\r\nAlso is simply sorting by end_date correct? Especially with trigger_rule, the last success/failed ti might not necessarily be the ti that causes the dag run to be marked as success/failed. Can you check the logic in 2.x to see how the ti is selected?",
        "pr_file_module": null
      }
    ]
  }
]