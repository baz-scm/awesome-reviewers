[
  {
    "discussion_id": "2161136883",
    "pr_number": 51682,
    "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/ui/dependencies.py",
    "created_at": "2025-06-23T09:23:37+00:00",
    "commented_code": "),\n     dependencies=[Depends(requires_access_dag(\"GET\", DagAccessEntity.DEPENDENCIES))],\n )\n-def get_dependencies(session: SessionDep, node_id: str | None = None) -> BaseGraphResponse:\n-    \"\"\"Dependencies graph.\"\"\"\n+def get_dependencies(\n+    node_id: str | None = None,\n+    node_ids: str | None = Query(None, description=\"Comma-separated list of node ids\"),\n+) -> BaseGraphResponse:\n+    \"\"\"Dependencies graph. Supports a single node_id or multiple node_ids separated by commas.\"\"\"\n+    # Parse node_ids (priority to node_ids, fallback to node_id)\n+    ids_to_fetch: list[str] = []\n+    # If node_id contains commas, treat as multiple IDs (extra protection)\n+    if node_ids:\n+        ids_to_fetch = [nid.strip() for nid in node_ids.split(\",\") if nid.strip()]",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2161136883",
        "repo_full_name": "apache/airflow",
        "pr_number": 51682,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/ui/dependencies.py",
        "discussion_id": "2161136883",
        "commented_code": "@@ -41,12 +40,28 @@\n     ),\n     dependencies=[Depends(requires_access_dag(\"GET\", DagAccessEntity.DEPENDENCIES))],\n )\n-def get_dependencies(session: SessionDep, node_id: str | None = None) -> BaseGraphResponse:\n-    \"\"\"Dependencies graph.\"\"\"\n+def get_dependencies(\n+    node_id: str | None = None,\n+    node_ids: str | None = Query(None, description=\"Comma-separated list of node ids\"),\n+) -> BaseGraphResponse:\n+    \"\"\"Dependencies graph. Supports a single node_id or multiple node_ids separated by commas.\"\"\"\n+    # Parse node_ids (priority to node_ids, fallback to node_id)\n+    ids_to_fetch: list[str] = []\n+    # If node_id contains commas, treat as multiple IDs (extra protection)\n+    if node_ids:\n+        ids_to_fetch = [nid.strip() for nid in node_ids.split(\",\") if nid.strip()]",
        "comment_created_at": "2025-06-23T09:23:37+00:00",
        "comment_author": "pierrejeambrun",
        "comment_body": "No, FastAPI handles `node_ids: list[str] | None = None` natively. FastAPI chose exploded form query params, i.e `?node_ids=1&node_ids=2` you will directly receive `node_ids=[\"1\",\"2\"] in your function.",
        "pr_file_module": null
      },
      {
        "comment_id": "2168650683",
        "repo_full_name": "apache/airflow",
        "pr_number": 51682,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/ui/dependencies.py",
        "discussion_id": "2161136883",
        "commented_code": "@@ -41,12 +40,28 @@\n     ),\n     dependencies=[Depends(requires_access_dag(\"GET\", DagAccessEntity.DEPENDENCIES))],\n )\n-def get_dependencies(session: SessionDep, node_id: str | None = None) -> BaseGraphResponse:\n-    \"\"\"Dependencies graph.\"\"\"\n+def get_dependencies(\n+    node_id: str | None = None,\n+    node_ids: str | None = Query(None, description=\"Comma-separated list of node ids\"),\n+) -> BaseGraphResponse:\n+    \"\"\"Dependencies graph. Supports a single node_id or multiple node_ids separated by commas.\"\"\"\n+    # Parse node_ids (priority to node_ids, fallback to node_id)\n+    ids_to_fetch: list[str] = []\n+    # If node_id contains commas, treat as multiple IDs (extra protection)\n+    if node_ids:\n+        ids_to_fetch = [nid.strip() for nid in node_ids.split(\",\") if nid.strip()]",
        "comment_created_at": "2025-06-26T09:40:07+00:00",
        "comment_author": "davidfgcorreia",
        "comment_body": "I chose to send the parameters as a string specifically to avoid the exploded format in the URL. node_ids=1&node_ids=2, I send something like ?node_ids=1,2 (or another delimited format).",
        "pr_file_module": null
      },
      {
        "comment_id": "2172070190",
        "repo_full_name": "apache/airflow",
        "pr_number": 51682,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/ui/dependencies.py",
        "discussion_id": "2161136883",
        "commented_code": "@@ -41,12 +40,28 @@\n     ),\n     dependencies=[Depends(requires_access_dag(\"GET\", DagAccessEntity.DEPENDENCIES))],\n )\n-def get_dependencies(session: SessionDep, node_id: str | None = None) -> BaseGraphResponse:\n-    \"\"\"Dependencies graph.\"\"\"\n+def get_dependencies(\n+    node_id: str | None = None,\n+    node_ids: str | None = Query(None, description=\"Comma-separated list of node ids\"),\n+) -> BaseGraphResponse:\n+    \"\"\"Dependencies graph. Supports a single node_id or multiple node_ids separated by commas.\"\"\"\n+    # Parse node_ids (priority to node_ids, fallback to node_id)\n+    ids_to_fetch: list[str] = []\n+    # If node_id contains commas, treat as multiple IDs (extra protection)\n+    if node_ids:\n+        ids_to_fetch = [nid.strip() for nid in node_ids.split(\",\") if nid.strip()]",
        "comment_created_at": "2025-06-27T13:38:29+00:00",
        "comment_author": "pierrejeambrun",
        "comment_body": "Please stay consistent with the rest of the API, we don't want some endpoints doing that and some other doing something else, also this will remove the need for you to manually parse that string.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1982929013",
    "pr_number": 47440,
    "pr_file": "airflow/api_fastapi/common/parameters.py",
    "created_at": "2025-03-06T08:42:09+00:00",
    "commented_code": "# MySQL does not support `nullslast`, and True/False ordering depends on the\n         # database implementation.\n         nullscheck = case((column.isnot(None), 0), else_=1)\n+        \n+        return (nullscheck, column.desc() if order_by.startswith(\"-\") else column.asc())\n+\n+    def to_orm(self, select: Select) -> Select:\n+        if self.skip_none is False:\n+            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+\n+        if self.value is None:\n+            self.value = self.get_primary_key_string()\n+\n+        order_by_list = self.value.split(\",\") if self.value else []\n+\n+        order_by_columns = self.get_order_by_columns(order_by_list)",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "1982929013",
        "repo_full_name": "apache/airflow",
        "pr_number": 47440,
        "pr_file": "airflow/api_fastapi/common/parameters.py",
        "discussion_id": "1982929013",
        "commented_code": "@@ -201,16 +219,30 @@ def to_orm(self, select: Select) -> Select:\n         # MySQL does not support `nullslast`, and True/False ordering depends on the\n         # database implementation.\n         nullscheck = case((column.isnot(None), 0), else_=1)\n+        \n+        return (nullscheck, column.desc() if order_by.startswith(\"-\") else column.asc())\n+\n+    def to_orm(self, select: Select) -> Select:\n+        if self.skip_none is False:\n+            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+\n+        if self.value is None:\n+            self.value = self.get_primary_key_string()\n+\n+        order_by_list = self.value.split(\",\") if self.value else []\n+\n+        order_by_columns = self.get_order_by_columns(order_by_list)",
        "comment_created_at": "2025-03-06T08:42:09+00:00",
        "comment_author": "pierrejeambrun",
        "comment_body": "The multiple query param orders will not be passed like this `order_by=-criteria1,criteria2`, but `order_by=-criteria1&order_by=criteria2` to be consistent with the `exploded` way passing query parameters list that FastAPI is defaulting too.",
        "pr_file_module": null
      },
      {
        "comment_id": "1984971893",
        "repo_full_name": "apache/airflow",
        "pr_number": 47440,
        "pr_file": "airflow/api_fastapi/common/parameters.py",
        "discussion_id": "1982929013",
        "commented_code": "@@ -201,16 +219,30 @@ def to_orm(self, select: Select) -> Select:\n         # MySQL does not support `nullslast`, and True/False ordering depends on the\n         # database implementation.\n         nullscheck = case((column.isnot(None), 0), else_=1)\n+        \n+        return (nullscheck, column.desc() if order_by.startswith(\"-\") else column.asc())\n+\n+    def to_orm(self, select: Select) -> Select:\n+        if self.skip_none is False:\n+            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+\n+        if self.value is None:\n+            self.value = self.get_primary_key_string()\n+\n+        order_by_list = self.value.split(\",\") if self.value else []\n+\n+        order_by_columns = self.get_order_by_columns(order_by_list)",
        "comment_created_at": "2025-03-07T12:24:23+00:00",
        "comment_author": "prasad-madine",
        "comment_body": "fixed",
        "pr_file_module": null
      },
      {
        "comment_id": "1984971991",
        "repo_full_name": "apache/airflow",
        "pr_number": 47440,
        "pr_file": "airflow/api_fastapi/common/parameters.py",
        "discussion_id": "1982929013",
        "commented_code": "@@ -201,16 +219,30 @@ def to_orm(self, select: Select) -> Select:\n         # MySQL does not support `nullslast`, and True/False ordering depends on the\n         # database implementation.\n         nullscheck = case((column.isnot(None), 0), else_=1)\n+        \n+        return (nullscheck, column.desc() if order_by.startswith(\"-\") else column.asc())\n+\n+    def to_orm(self, select: Select) -> Select:\n+        if self.skip_none is False:\n+            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+\n+        if self.value is None:\n+            self.value = self.get_primary_key_string()\n+\n+        order_by_list = self.value.split(\",\") if self.value else []\n+\n+        order_by_columns = self.get_order_by_columns(order_by_list)",
        "comment_created_at": "2025-03-07T12:24:27+00:00",
        "comment_author": "prasad-madine",
        "comment_body": "fixed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1982932604",
    "pr_number": 47440,
    "pr_file": "airflow/api_fastapi/common/parameters.py",
    "created_at": "2025-03-06T08:44:35+00:00",
    "commented_code": "self.model = model\n         self.to_replace = to_replace\n \n-    def to_orm(self, select: Select) -> Select:\n-        if self.skip_none is False:\n-            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+    def get_order_by_columns(self, order_by_list: list[str]) -> list:\n+        \"\"\"Generates order_by conditions based on the given sorting parameters.\"\"\"\n+        if len(order_by_list) > MAX_SORT_PARAMS:\n+            raise HTTPException(\n+                400, \n+                f\"Ordering with more than two parameters is not allowed. Provided: {order_by_list}\"",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "1982932604",
        "repo_full_name": "apache/airflow",
        "pr_number": 47440,
        "pr_file": "airflow/api_fastapi/common/parameters.py",
        "discussion_id": "1982932604",
        "commented_code": "@@ -173,14 +174,31 @@ def __init__(\n         self.model = model\n         self.to_replace = to_replace\n \n-    def to_orm(self, select: Select) -> Select:\n-        if self.skip_none is False:\n-            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+    def get_order_by_columns(self, order_by_list: list[str]) -> list:\n+        \"\"\"Generates order_by conditions based on the given sorting parameters.\"\"\"\n+        if len(order_by_list) > MAX_SORT_PARAMS:\n+            raise HTTPException(\n+                400, \n+                f\"Ordering with more than two parameters is not allowed. Provided: {order_by_list}\"",
        "comment_created_at": "2025-03-06T08:44:35+00:00",
        "comment_author": "pierrejeambrun",
        "comment_body": "`two` shouldn't be hardcoded but come from the `MAX_SORT_PARAMS` value.\r\n\r\nWe need a test for that.",
        "pr_file_module": null
      },
      {
        "comment_id": "1984972459",
        "repo_full_name": "apache/airflow",
        "pr_number": 47440,
        "pr_file": "airflow/api_fastapi/common/parameters.py",
        "discussion_id": "1982932604",
        "commented_code": "@@ -173,14 +174,31 @@ def __init__(\n         self.model = model\n         self.to_replace = to_replace\n \n-    def to_orm(self, select: Select) -> Select:\n-        if self.skip_none is False:\n-            raise ValueError(f\"Cannot set 'skip_none' to False on a {type(self)}\")\n+    def get_order_by_columns(self, order_by_list: list[str]) -> list:\n+        \"\"\"Generates order_by conditions based on the given sorting parameters.\"\"\"\n+        if len(order_by_list) > MAX_SORT_PARAMS:\n+            raise HTTPException(\n+                400, \n+                f\"Ordering with more than two parameters is not allowed. Provided: {order_by_list}\"",
        "comment_created_at": "2025-03-07T12:24:50+00:00",
        "comment_author": "prasad-madine",
        "comment_body": "fixed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2020608835",
    "pr_number": 47440,
    "pr_file": "airflow-core/src/airflow/api_fastapi/common/parameters.py",
    "created_at": "2025-03-31T08:31:29+00:00",
    "commented_code": "def depends(cls, *args: Any, **kwargs: Any) -> Self:\n         raise NotImplementedError(\"Use dynamic_depends, depends not implemented.\")\n \n-    def dynamic_depends(self, default: str | None = None) -> Callable:\n-        def inner(order_by: str = default or self.get_primary_key_string()) -> SortParam:\n-            return self.set_value(self.get_primary_key_string() if order_by == \"\" else order_by)\n+    def dynamic_depends(self, default: list[str] | None = None) -> Callable:\n+        def inner(order_by: list[str] | str | None = Query(default)) -> SortParam:\n+            if order_by is None:\n+                order_by = [self.get_primary_key_string()]\n+            elif isinstance(order_by, str):\n+                order_by = [order_by]\n+            return self.set_value(order_by)",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2020608835",
        "repo_full_name": "apache/airflow",
        "pr_number": 47440,
        "pr_file": "airflow-core/src/airflow/api_fastapi/common/parameters.py",
        "discussion_id": "2020608835",
        "commented_code": "@@ -221,9 +246,13 @@ def get_primary_key_string(self) -> str:\n     def depends(cls, *args: Any, **kwargs: Any) -> Self:\n         raise NotImplementedError(\"Use dynamic_depends, depends not implemented.\")\n \n-    def dynamic_depends(self, default: str | None = None) -> Callable:\n-        def inner(order_by: str = default or self.get_primary_key_string()) -> SortParam:\n-            return self.set_value(self.get_primary_key_string() if order_by == \"\" else order_by)\n+    def dynamic_depends(self, default: list[str] | None = None) -> Callable:\n+        def inner(order_by: list[str] | str | None = Query(default)) -> SortParam:\n+            if order_by is None:\n+                order_by = [self.get_primary_key_string()]\n+            elif isinstance(order_by, str):\n+                order_by = [order_by]\n+            return self.set_value(order_by)",
        "comment_created_at": "2025-03-31T08:31:29+00:00",
        "comment_author": "pierrejeambrun",
        "comment_body": "I'm not sure it's exactly equivalent. If there is no `order_by` query param specified, we shouldn't add any by default and let the query default order operate:\r\n- order_by = None => don't update the query at all and let the default ordering of the query operate.\r\n- order_by = [] or [\"\"] -> fill with [primary_key_string] (or raise validation error)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2079018180",
    "pr_number": 49470,
    "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/log.py",
    "created_at": "2025-05-08T06:49:27+00:00",
    "commented_code": "with contextlib.suppress(TaskNotFound):\n             ti.task = dag.get_task(ti.task_id)\n \n-    if accept == Mimetype.JSON or accept == Mimetype.ANY:  # default\n-        logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n+    if accept == Mimetype.JSON:  # only specified application/json will return JSON\n+        structured_log_stream, out_metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n         encoded_token = None\n-        if not metadata.get(\"end_of_log\", False):\n-            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(metadata)\n-        return TaskInstancesLogResponse.model_construct(continuation_token=encoded_token, content=logs)\n-    # text/plain, or something else we don't understand. Return raw log content\n-\n-    # We need to exhaust the iterator before we can generate the continuation token.\n-    # We could improve this by making it a streaming/async response, and by then setting the header using\n-    # HTTP Trailers\n-    logs = \"\".join(task_log_reader.read_log_stream(ti, try_number, metadata))\n+        if not out_metadata.get(\"end_of_log\", False):\n+            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(out_metadata)\n+        return TaskInstancesLogResponse.model_construct(\n+            continuation_token=encoded_token, content=list(structured_log_stream)\n+        )\n+\n+    # text/plain, or something else we don't understand. Return raw log content in ndjson format with StreamingResponse",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2079018180",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/log.py",
        "discussion_id": "2079018180",
        "commented_code": "@@ -133,24 +135,23 @@ def get_log(\n         with contextlib.suppress(TaskNotFound):\n             ti.task = dag.get_task(ti.task_id)\n \n-    if accept == Mimetype.JSON or accept == Mimetype.ANY:  # default\n-        logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n+    if accept == Mimetype.JSON:  # only specified application/json will return JSON\n+        structured_log_stream, out_metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n         encoded_token = None\n-        if not metadata.get(\"end_of_log\", False):\n-            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(metadata)\n-        return TaskInstancesLogResponse.model_construct(continuation_token=encoded_token, content=logs)\n-    # text/plain, or something else we don't understand. Return raw log content\n-\n-    # We need to exhaust the iterator before we can generate the continuation token.\n-    # We could improve this by making it a streaming/async response, and by then setting the header using\n-    # HTTP Trailers\n-    logs = \"\".join(task_log_reader.read_log_stream(ti, try_number, metadata))\n+        if not out_metadata.get(\"end_of_log\", False):\n+            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(out_metadata)\n+        return TaskInstancesLogResponse.model_construct(\n+            continuation_token=encoded_token, content=list(structured_log_stream)\n+        )\n+\n+    # text/plain, or something else we don't understand. Return raw log content in ndjson format with StreamingResponse",
        "comment_created_at": "2025-05-08T06:49:27+00:00",
        "comment_author": "uranusjr",
        "comment_body": "Is it a better idea to default to ndjson (and streaming), or a plain json response? I wonder if defaulting to a simple response is easier for simple cases.",
        "pr_file_module": null
      },
      {
        "comment_id": "2079191295",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/log.py",
        "discussion_id": "2079018180",
        "commented_code": "@@ -133,24 +135,23 @@ def get_log(\n         with contextlib.suppress(TaskNotFound):\n             ti.task = dag.get_task(ti.task_id)\n \n-    if accept == Mimetype.JSON or accept == Mimetype.ANY:  # default\n-        logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n+    if accept == Mimetype.JSON:  # only specified application/json will return JSON\n+        structured_log_stream, out_metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n         encoded_token = None\n-        if not metadata.get(\"end_of_log\", False):\n-            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(metadata)\n-        return TaskInstancesLogResponse.model_construct(continuation_token=encoded_token, content=logs)\n-    # text/plain, or something else we don't understand. Return raw log content\n-\n-    # We need to exhaust the iterator before we can generate the continuation token.\n-    # We could improve this by making it a streaming/async response, and by then setting the header using\n-    # HTTP Trailers\n-    logs = \"\".join(task_log_reader.read_log_stream(ti, try_number, metadata))\n+        if not out_metadata.get(\"end_of_log\", False):\n+            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(out_metadata)\n+        return TaskInstancesLogResponse.model_construct(\n+            continuation_token=encoded_token, content=list(structured_log_stream)\n+        )\n+\n+    # text/plain, or something else we don't understand. Return raw log content in ndjson format with StreamingResponse",
        "comment_created_at": "2025-05-08T08:40:34+00:00",
        "comment_author": "jason810496",
        "comment_body": "Actually, I was initially thinking of only providing a streaming `ndjson` response. However, for backward compatibility, I decided to retain the plain `json` format as well.\r\n\r\nThe last `list(structured_log_stream)` part breaks the memory-efficient streaming approach. The `json` serialization is resource-intensive and has, in some cases, caused the worker to be killed due to memory pressure. Because of this, I consider the `json` format more of a special-case fallback.\r\n\r\nIn my opinion, it's more appropriate for the frontend UI to adapt by specifying the corresponding `Accept` header format. ( That's why I opened issue #50333 to propose switching to the `ndjson` response format in the frontend )",
        "pr_file_module": null
      },
      {
        "comment_id": "2081116652",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/log.py",
        "discussion_id": "2079018180",
        "commented_code": "@@ -133,24 +135,23 @@ def get_log(\n         with contextlib.suppress(TaskNotFound):\n             ti.task = dag.get_task(ti.task_id)\n \n-    if accept == Mimetype.JSON or accept == Mimetype.ANY:  # default\n-        logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n+    if accept == Mimetype.JSON:  # only specified application/json will return JSON\n+        structured_log_stream, out_metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n         encoded_token = None\n-        if not metadata.get(\"end_of_log\", False):\n-            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(metadata)\n-        return TaskInstancesLogResponse.model_construct(continuation_token=encoded_token, content=logs)\n-    # text/plain, or something else we don't understand. Return raw log content\n-\n-    # We need to exhaust the iterator before we can generate the continuation token.\n-    # We could improve this by making it a streaming/async response, and by then setting the header using\n-    # HTTP Trailers\n-    logs = \"\".join(task_log_reader.read_log_stream(ti, try_number, metadata))\n+        if not out_metadata.get(\"end_of_log\", False):\n+            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(out_metadata)\n+        return TaskInstancesLogResponse.model_construct(\n+            continuation_token=encoded_token, content=list(structured_log_stream)\n+        )\n+\n+    # text/plain, or something else we don't understand. Return raw log content in ndjson format with StreamingResponse",
        "comment_created_at": "2025-05-09T07:39:44+00:00",
        "comment_author": "uranusjr",
        "comment_body": "Iâ€™m not thinking about cases where the client asks for a format correctly, but those catch-all fallbacks. Our front _shouldnâ€™t_ use those in any cases, but a custom client might (like when you want to debug the endpoint directly with postman or something). Defaulting to streaming on an unrecognised Accept header might make those cases more difficult, but I guess itâ€™s not a big problem since you should just set the header correctly anyway. But in that case, why do we do this at all? Why not just return 406 Not Acceptable instead?",
        "pr_file_module": null
      },
      {
        "comment_id": "2081151947",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/log.py",
        "discussion_id": "2079018180",
        "commented_code": "@@ -133,24 +135,23 @@ def get_log(\n         with contextlib.suppress(TaskNotFound):\n             ti.task = dag.get_task(ti.task_id)\n \n-    if accept == Mimetype.JSON or accept == Mimetype.ANY:  # default\n-        logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n+    if accept == Mimetype.JSON:  # only specified application/json will return JSON\n+        structured_log_stream, out_metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n         encoded_token = None\n-        if not metadata.get(\"end_of_log\", False):\n-            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(metadata)\n-        return TaskInstancesLogResponse.model_construct(continuation_token=encoded_token, content=logs)\n-    # text/plain, or something else we don't understand. Return raw log content\n-\n-    # We need to exhaust the iterator before we can generate the continuation token.\n-    # We could improve this by making it a streaming/async response, and by then setting the header using\n-    # HTTP Trailers\n-    logs = \"\".join(task_log_reader.read_log_stream(ti, try_number, metadata))\n+        if not out_metadata.get(\"end_of_log\", False):\n+            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(out_metadata)\n+        return TaskInstancesLogResponse.model_construct(\n+            continuation_token=encoded_token, content=list(structured_log_stream)\n+        )\n+\n+    # text/plain, or something else we don't understand. Return raw log content in ndjson format with StreamingResponse",
        "comment_created_at": "2025-05-09T08:04:07+00:00",
        "comment_author": "jason810496",
        "comment_body": "Agreed, I see your point. Setting the default format to JSON will indeed make it easier for users to debug.  \nI'll update the default format to JSON, thanks for bringing this up!",
        "pr_file_module": null
      },
      {
        "comment_id": "2081593626",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/log.py",
        "discussion_id": "2079018180",
        "commented_code": "@@ -133,24 +135,23 @@ def get_log(\n         with contextlib.suppress(TaskNotFound):\n             ti.task = dag.get_task(ti.task_id)\n \n-    if accept == Mimetype.JSON or accept == Mimetype.ANY:  # default\n-        logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n+    if accept == Mimetype.JSON:  # only specified application/json will return JSON\n+        structured_log_stream, out_metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)\n         encoded_token = None\n-        if not metadata.get(\"end_of_log\", False):\n-            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(metadata)\n-        return TaskInstancesLogResponse.model_construct(continuation_token=encoded_token, content=logs)\n-    # text/plain, or something else we don't understand. Return raw log content\n-\n-    # We need to exhaust the iterator before we can generate the continuation token.\n-    # We could improve this by making it a streaming/async response, and by then setting the header using\n-    # HTTP Trailers\n-    logs = \"\".join(task_log_reader.read_log_stream(ti, try_number, metadata))\n+        if not out_metadata.get(\"end_of_log\", False):\n+            encoded_token = URLSafeSerializer(request.app.state.secret_key).dumps(out_metadata)\n+        return TaskInstancesLogResponse.model_construct(\n+            continuation_token=encoded_token, content=list(structured_log_stream)\n+        )\n+\n+    # text/plain, or something else we don't understand. Return raw log content in ndjson format with StreamingResponse",
        "comment_created_at": "2025-05-09T12:43:58+00:00",
        "comment_author": "jason810496",
        "comment_body": "Just resolved, thanks for the review.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2173813659",
    "pr_number": 52053,
    "pr_file": "airflow-core/src/airflow/api_fastapi/execution_api/routes/__init__.py",
    "created_at": "2025-06-29T15:53:57+00:00",
    "commented_code": "authenticated_router.include_router(variables.router, prefix=\"/variables\", tags=[\"Variables\"])\n authenticated_router.include_router(xcoms.router, prefix=\"/xcoms\", tags=[\"XComs\"])\n \n+# TODO: Remove this block once we can make the execution API pluggable.",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2173813659",
        "repo_full_name": "apache/airflow",
        "pr_number": 52053,
        "pr_file": "airflow-core/src/airflow/api_fastapi/execution_api/routes/__init__.py",
        "discussion_id": "2173813659",
        "commented_code": "@@ -49,4 +51,10 @@\n authenticated_router.include_router(variables.router, prefix=\"/variables\", tags=[\"Variables\"])\n authenticated_router.include_router(xcoms.router, prefix=\"/xcoms\", tags=[\"XComs\"])\n \n+# TODO: Remove this block once we can make the execution API pluggable.",
        "comment_created_at": "2025-06-29T15:53:57+00:00",
        "comment_author": "jscheffl",
        "comment_body": "Should already be \"pluggable\" today. You can take a look to FAB or Edge3 where via a Plugin another API endpoint is exposed. No need to patch core for this.\r\n\r\nSee: https://github.com/apache/airflow/blob/main/providers/edge3/src/airflow/providers/edge3/plugins/edge_executor_plugin.py#L223",
        "pr_file_module": null
      },
      {
        "comment_id": "2174042473",
        "repo_full_name": "apache/airflow",
        "pr_number": 52053,
        "pr_file": "airflow-core/src/airflow/api_fastapi/execution_api/routes/__init__.py",
        "discussion_id": "2173813659",
        "commented_code": "@@ -49,4 +51,10 @@\n authenticated_router.include_router(variables.router, prefix=\"/variables\", tags=[\"Variables\"])\n authenticated_router.include_router(xcoms.router, prefix=\"/xcoms\", tags=[\"XComs\"])\n \n+# TODO: Remove this block once we can make the execution API pluggable.",
        "comment_created_at": "2025-06-30T01:09:45+00:00",
        "comment_author": "Lee-W",
        "comment_body": "I think this is only doable in the public API, which Iâ€™m already using in a few places. ðŸ¤” \r\n\r\nThe execution API is meant for the operator and trigger to access the database. When I last chatted with @ashb and @kaxil, it seemed like this functionality isnâ€™t supported yet. But please correct me if Iâ€™m wrong. Thanks!",
        "pr_file_module": null
      },
      {
        "comment_id": "2175776164",
        "repo_full_name": "apache/airflow",
        "pr_number": 52053,
        "pr_file": "airflow-core/src/airflow/api_fastapi/execution_api/routes/__init__.py",
        "discussion_id": "2173813659",
        "commented_code": "@@ -49,4 +51,10 @@\n authenticated_router.include_router(variables.router, prefix=\"/variables\", tags=[\"Variables\"])\n authenticated_router.include_router(xcoms.router, prefix=\"/xcoms\", tags=[\"XComs\"])\n \n+# TODO: Remove this block once we can make the execution API pluggable.",
        "comment_created_at": "2025-06-30T19:45:37+00:00",
        "comment_author": "jscheffl",
        "comment_body": "I understand. I would have assumed to use \"public API\" for stuff that maybe is already existing, \"Task SDK API\" for all \"normal\" communication like deferring but would have not considered to extend the TaskSDK just for the human operator purpose. Not blocking it but I would have assumed the Human operator would use a HITL specific API to make the DB entries and do the needed task level <-> DB communication to be independent of Task SDK.\r\n\r\nBut maybe I was thinking too much provider in mind, that a feature in provider should not touch the core. But it if is in standard provider we can affort a bit more tighter bundling... but it would bundle us tight as well and as we potentially can mix provider and code versions it might make changes a bit more in-flexible (would implicitly create a strong coupling from standard provider to core/task SDK - if we do a feature increment might mean that certain features will only work in Airflow 3.2.0 if we need to change a core thing later...)\r\n\r\nI was running through this excercise with Edge Executor as well and was a bit of a stretch to implement it independent of core stuff.",
        "pr_file_module": null
      },
      {
        "comment_id": "2176370527",
        "repo_full_name": "apache/airflow",
        "pr_number": 52053,
        "pr_file": "airflow-core/src/airflow/api_fastapi/execution_api/routes/__init__.py",
        "discussion_id": "2173813659",
        "commented_code": "@@ -49,4 +51,10 @@\n authenticated_router.include_router(variables.router, prefix=\"/variables\", tags=[\"Variables\"])\n authenticated_router.include_router(xcoms.router, prefix=\"/xcoms\", tags=[\"XComs\"])\n \n+# TODO: Remove this block once we can make the execution API pluggable.",
        "comment_created_at": "2025-07-01T04:16:37+00:00",
        "comment_author": "Lee-W",
        "comment_body": "> would use a HITL specific API to make the DB entries and do the needed task level <-> DB communication to be independent of Task SDK.\r\n\r\nAFAIK, we don't have a way to do this for the time being. The last time I heard, we might allow such extension around 3.2 or 3.3.",
        "pr_file_module": null
      },
      {
        "comment_id": "2182618017",
        "repo_full_name": "apache/airflow",
        "pr_number": 52053,
        "pr_file": "airflow-core/src/airflow/api_fastapi/execution_api/routes/__init__.py",
        "discussion_id": "2173813659",
        "commented_code": "@@ -49,4 +51,10 @@\n authenticated_router.include_router(variables.router, prefix=\"/variables\", tags=[\"Variables\"])\n authenticated_router.include_router(xcoms.router, prefix=\"/xcoms\", tags=[\"XComs\"])\n \n+# TODO: Remove this block once we can make the execution API pluggable.",
        "comment_created_at": "2025-07-03T12:06:06+00:00",
        "comment_author": "kaxil",
        "comment_body": "Yeah this one feels weird, because this is probably one of the few things where we are a feature is spread across Provider & Core.\r\n\r\nIdeally, since technically it is a core feature -- everything is in Core.\r\n\r\nOr we could have considered it like Setup/teardown, DecoratedOperator etc and have it in Task SDK -- where the Base is in Task SDK and other providers can extend it.\r\n\r\n\r\n(Haven't deeply thought it yet -- but yes looking at the implementation -- there is certainly a coupling between Core + Standard provider which does seem odd. Especially thinking of a scenario where we fix a bug or add another feature on top of it -- what parts do we need to change).",
        "pr_file_module": null
      }
    ]
  }
]