[
  {
    "discussion_id": "2167048145",
    "pr_number": 52224,
    "pr_file": "providers/postgres/src/airflow/providers/postgres/hooks/postgres.py",
    "created_at": "2025-06-25T15:45:24+00:00",
    "commented_code": "self.conn = psycopg2.connect(**conn_args)\n         return self.conn\n \n+    def _get_pandas_df(\n+        self,\n+        sql,",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2167048145",
        "repo_full_name": "apache/airflow",
        "pr_number": 52224,
        "pr_file": "providers/postgres/src/airflow/providers/postgres/hooks/postgres.py",
        "discussion_id": "2167048145",
        "commented_code": "@@ -173,6 +178,24 @@ def get_conn(self) -> connection:\n         self.conn = psycopg2.connect(**conn_args)\n         return self.conn\n \n+    def _get_pandas_df(\n+        self,\n+        sql,",
        "comment_created_at": "2025-06-25T15:45:24+00:00",
        "comment_author": "bugraoz93",
        "comment_body": "I think this seems shadow naming with below import, could cause problems even though you added as psql there\n`from pandas.io import sql as psql`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2124893126",
    "pr_number": 46621,
    "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/hooks/s3.py",
    "created_at": "2025-06-03T20:44:16+00:00",
    "commented_code": "\"\"\"\n         s3_client = self.get_conn()\n         s3_client.delete_bucket_tagging(Bucket=bucket_name)\n+\n+    def _download_s3_delete_stale_local_files(self, current_s3_objects: list[Path], local_dir: Path):\n+        current_s3_keys = {key for key in current_s3_objects}\n+\n+        for item in local_dir.iterdir():\n+            item: Path  # type: ignore[no-redef]\n+            absolute_item_path = item.resolve()\n+\n+            if absolute_item_path not in current_s3_keys:\n+                try:\n+                    if item.is_file():\n+                        item.unlink(missing_ok=True)\n+                        self.log.debug(\"Deleted stale local file: %s\", item)\n+                    elif item.is_dir():\n+                        # delete only when the folder is empty\n+                        if not os.listdir(item):\n+                            item.rmdir()\n+                            self.log.debug(\"Deleted stale empty directory: %s\", item)\n+                    else:\n+                        self.log.debug(\"Skipping stale item of unknown type: %s\", item)\n+                except OSError as e:\n+                    self.log.error(\"Error deleting stale item %s: %s\", item, e)\n+                    raise e\n+\n+    def _download_s3_object_if_changed(self, s3_bucket, s3_object, local_target_path: Path):\n+        should_download = False\n+        download_msg = \"\"\n+        if not local_target_path.exists():\n+            should_download = True\n+            download_msg = f\"Local file {local_target_path} does not exist.\"\n+        else:\n+            local_stats = local_target_path.stat()\n+\n+            if s3_object.size != local_stats.st_size:\n+                should_download = True\n+                download_msg = (\n+                    f\"S3 object size ({s3_object.size}) and local file size ({local_stats.st_size}) differ.\"\n+                )\n+\n+            s3_last_modified = s3_object.last_modified\n+            if local_stats.st_mtime < s3_last_modified.microsecond:\n+                should_download = True\n+                download_msg = f\"S3 object last modified ({s3_last_modified.microsecond}) and local file last modified ({local_stats.st_mtime}) differ.\"\n+\n+        if should_download:\n+            s3_bucket.download_file(s3_object.key, local_target_path)\n+            self.log.debug(\n+                \"%s Downloaded %s to %s\", download_msg, s3_object.key, local_target_path.as_posix()\n+            )\n+        else:\n+            self.log.debug(\n+                \"Local file %s is up-to-date with S3 object %s. Skipping download.\",\n+                local_target_path.as_posix(),\n+                s3_object.key,\n+            )\n+\n+    def download_s3(self, bucket_name: str, local_dir: Path, s3_prefix=\"\", delete_stale: bool = True):",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2124893126",
        "repo_full_name": "apache/airflow",
        "pr_number": 46621,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/hooks/s3.py",
        "discussion_id": "2124893126",
        "commented_code": "@@ -1616,3 +1616,80 @@ def delete_bucket_tagging(self, bucket_name: str | None = None) -> None:\n         \"\"\"\n         s3_client = self.get_conn()\n         s3_client.delete_bucket_tagging(Bucket=bucket_name)\n+\n+    def _download_s3_delete_stale_local_files(self, current_s3_objects: list[Path], local_dir: Path):\n+        current_s3_keys = {key for key in current_s3_objects}\n+\n+        for item in local_dir.iterdir():\n+            item: Path  # type: ignore[no-redef]\n+            absolute_item_path = item.resolve()\n+\n+            if absolute_item_path not in current_s3_keys:\n+                try:\n+                    if item.is_file():\n+                        item.unlink(missing_ok=True)\n+                        self.log.debug(\"Deleted stale local file: %s\", item)\n+                    elif item.is_dir():\n+                        # delete only when the folder is empty\n+                        if not os.listdir(item):\n+                            item.rmdir()\n+                            self.log.debug(\"Deleted stale empty directory: %s\", item)\n+                    else:\n+                        self.log.debug(\"Skipping stale item of unknown type: %s\", item)\n+                except OSError as e:\n+                    self.log.error(\"Error deleting stale item %s: %s\", item, e)\n+                    raise e\n+\n+    def _download_s3_object_if_changed(self, s3_bucket, s3_object, local_target_path: Path):\n+        should_download = False\n+        download_msg = \"\"\n+        if not local_target_path.exists():\n+            should_download = True\n+            download_msg = f\"Local file {local_target_path} does not exist.\"\n+        else:\n+            local_stats = local_target_path.stat()\n+\n+            if s3_object.size != local_stats.st_size:\n+                should_download = True\n+                download_msg = (\n+                    f\"S3 object size ({s3_object.size}) and local file size ({local_stats.st_size}) differ.\"\n+                )\n+\n+            s3_last_modified = s3_object.last_modified\n+            if local_stats.st_mtime < s3_last_modified.microsecond:\n+                should_download = True\n+                download_msg = f\"S3 object last modified ({s3_last_modified.microsecond}) and local file last modified ({local_stats.st_mtime}) differ.\"\n+\n+        if should_download:\n+            s3_bucket.download_file(s3_object.key, local_target_path)\n+            self.log.debug(\n+                \"%s Downloaded %s to %s\", download_msg, s3_object.key, local_target_path.as_posix()\n+            )\n+        else:\n+            self.log.debug(\n+                \"Local file %s is up-to-date with S3 object %s. Skipping download.\",\n+                local_target_path.as_posix(),\n+                s3_object.key,\n+            )\n+\n+    def download_s3(self, bucket_name: str, local_dir: Path, s3_prefix=\"\", delete_stale: bool = True):",
        "comment_created_at": "2025-06-03T20:44:16+00:00",
        "comment_author": "jedcunningham",
        "comment_body": "Might be better to name this something like \"sync_to_local_dir\" or something?",
        "pr_file_module": null
      },
      {
        "comment_id": "2147365399",
        "repo_full_name": "apache/airflow",
        "pr_number": 46621,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/hooks/s3.py",
        "discussion_id": "2124893126",
        "commented_code": "@@ -1616,3 +1616,80 @@ def delete_bucket_tagging(self, bucket_name: str | None = None) -> None:\n         \"\"\"\n         s3_client = self.get_conn()\n         s3_client.delete_bucket_tagging(Bucket=bucket_name)\n+\n+    def _download_s3_delete_stale_local_files(self, current_s3_objects: list[Path], local_dir: Path):\n+        current_s3_keys = {key for key in current_s3_objects}\n+\n+        for item in local_dir.iterdir():\n+            item: Path  # type: ignore[no-redef]\n+            absolute_item_path = item.resolve()\n+\n+            if absolute_item_path not in current_s3_keys:\n+                try:\n+                    if item.is_file():\n+                        item.unlink(missing_ok=True)\n+                        self.log.debug(\"Deleted stale local file: %s\", item)\n+                    elif item.is_dir():\n+                        # delete only when the folder is empty\n+                        if not os.listdir(item):\n+                            item.rmdir()\n+                            self.log.debug(\"Deleted stale empty directory: %s\", item)\n+                    else:\n+                        self.log.debug(\"Skipping stale item of unknown type: %s\", item)\n+                except OSError as e:\n+                    self.log.error(\"Error deleting stale item %s: %s\", item, e)\n+                    raise e\n+\n+    def _download_s3_object_if_changed(self, s3_bucket, s3_object, local_target_path: Path):\n+        should_download = False\n+        download_msg = \"\"\n+        if not local_target_path.exists():\n+            should_download = True\n+            download_msg = f\"Local file {local_target_path} does not exist.\"\n+        else:\n+            local_stats = local_target_path.stat()\n+\n+            if s3_object.size != local_stats.st_size:\n+                should_download = True\n+                download_msg = (\n+                    f\"S3 object size ({s3_object.size}) and local file size ({local_stats.st_size}) differ.\"\n+                )\n+\n+            s3_last_modified = s3_object.last_modified\n+            if local_stats.st_mtime < s3_last_modified.microsecond:\n+                should_download = True\n+                download_msg = f\"S3 object last modified ({s3_last_modified.microsecond}) and local file last modified ({local_stats.st_mtime}) differ.\"\n+\n+        if should_download:\n+            s3_bucket.download_file(s3_object.key, local_target_path)\n+            self.log.debug(\n+                \"%s Downloaded %s to %s\", download_msg, s3_object.key, local_target_path.as_posix()\n+            )\n+        else:\n+            self.log.debug(\n+                \"Local file %s is up-to-date with S3 object %s. Skipping download.\",\n+                local_target_path.as_posix(),\n+                s3_object.key,\n+            )\n+\n+    def download_s3(self, bucket_name: str, local_dir: Path, s3_prefix=\"\", delete_stale: bool = True):",
        "comment_created_at": "2025-06-14T22:26:22+00:00",
        "comment_author": "ismailsimsek",
        "comment_body": "renamed to sync_to_local_dir",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160800660",
    "pr_number": 51638,
    "pr_file": "airflow-core/src/airflow/jobs/scheduler_job_runner.py",
    "created_at": "2025-06-23T06:20:30+00:00",
    "commented_code": "_update_state(dag, dag_run)\n             dag_run.notify_dagrun_state_changed()\n \n+            if (deadline := dag.deadline) and isinstance(\n+                deadline.reference, ReferenceModels.DagRunQueuedAtDeadline\n+            ):",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2160800660",
        "repo_full_name": "apache/airflow",
        "pr_number": 51638,
        "pr_file": "airflow-core/src/airflow/jobs/scheduler_job_runner.py",
        "discussion_id": "2160800660",
        "commented_code": "@@ -1769,6 +1770,25 @@ def _update_state(dag: DAG, dag_run: DagRun):\n             _update_state(dag, dag_run)\n             dag_run.notify_dagrun_state_changed()\n \n+            if (deadline := dag.deadline) and isinstance(\n+                deadline.reference, ReferenceModels.DagRunQueuedAtDeadline\n+            ):",
        "comment_created_at": "2025-06-23T06:20:30+00:00",
        "comment_author": "uranusjr",
        "comment_body": "Would it be a good idea to add a `get_dagrun_queued_at_deadline` on DAG like the creation deadline? Or remove the function for the creation deadline? It seems weird one has a function and one does not.\r\n\r\nIf this feels repetitive, maybe the function should be changed to like `get_deadline(kind: ReferenceModels) -> Deadline | None`",
        "pr_file_module": null
      },
      {
        "comment_id": "2162707409",
        "repo_full_name": "apache/airflow",
        "pr_number": 51638,
        "pr_file": "airflow-core/src/airflow/jobs/scheduler_job_runner.py",
        "discussion_id": "2160800660",
        "commented_code": "@@ -1769,6 +1770,25 @@ def _update_state(dag: DAG, dag_run: DagRun):\n             _update_state(dag, dag_run)\n             dag_run.notify_dagrun_state_changed()\n \n+            if (deadline := dag.deadline) and isinstance(\n+                deadline.reference, ReferenceModels.DagRunQueuedAtDeadline\n+            ):",
        "comment_created_at": "2025-06-23T23:57:22+00:00",
        "comment_author": "ramitkataria",
        "comment_body": "I agree, I've removed it for now and if it ends up being repetitive, we could add a function to get deadline by reference type",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2135330938",
    "pr_number": 49470,
    "pr_file": "airflow-core/src/airflow/utils/log/log_stream_counter.py",
    "created_at": "2025-06-09T08:57:52+00:00",
    "commented_code": "+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import os\n+import tempfile\n+from typing import TYPE_CHECKING\n+\n+if TYPE_CHECKING:\n+    from airflow.typing_compat import Self\n+    from airflow.utils.log.file_task_handler import (\n+        LogHandlerOutputStream,\n+        StructuredLogMessage,\n+        StructuredLogStream,\n+    )\n+\n+\n+class LogStreamCounter:",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2135330938",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/log_stream_counter.py",
        "discussion_id": "2135330938",
        "commented_code": "@@ -0,0 +1,151 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import os\n+import tempfile\n+from typing import TYPE_CHECKING\n+\n+if TYPE_CHECKING:\n+    from airflow.typing_compat import Self\n+    from airflow.utils.log.file_task_handler import (\n+        LogHandlerOutputStream,\n+        StructuredLogMessage,\n+        StructuredLogStream,\n+    )\n+\n+\n+class LogStreamCounter:",
        "comment_created_at": "2025-06-09T08:57:52+00:00",
        "comment_author": "Lee-W",
        "comment_body": "I don't quite understand this naming. This class seems to do more than just counting. Is this convention learned from somewhere else?",
        "pr_file_module": null
      },
      {
        "comment_id": "2135402447",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/log_stream_counter.py",
        "discussion_id": "2135330938",
        "commented_code": "@@ -0,0 +1,151 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import os\n+import tempfile\n+from typing import TYPE_CHECKING\n+\n+if TYPE_CHECKING:\n+    from airflow.typing_compat import Self\n+    from airflow.utils.log.file_task_handler import (\n+        LogHandlerOutputStream,\n+        StructuredLogMessage,\n+        StructuredLogStream,\n+    )\n+\n+\n+class LogStreamCounter:",
        "comment_created_at": "2025-06-09T09:39:31+00:00",
        "comment_author": "jason810496",
        "comment_body": "The only way to get the total log line is to iterating through the whole log stream.\n\nHowever, to make it memory efficient, we need to flush partial of logs to the temp file and also have the ability to replay the log stream after counting.\n\nThis is the only purpose of this class, and I come out with LogStreamCount as name. Maybe LogStreamLineCounter is more comprehensible?",
        "pr_file_module": null
      },
      {
        "comment_id": "2135415020",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/log_stream_counter.py",
        "discussion_id": "2135330938",
        "commented_code": "@@ -0,0 +1,151 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import os\n+import tempfile\n+from typing import TYPE_CHECKING\n+\n+if TYPE_CHECKING:\n+    from airflow.typing_compat import Self\n+    from airflow.utils.log.file_task_handler import (\n+        LogHandlerOutputStream,\n+        StructuredLogMessage,\n+        StructuredLogStream,\n+    )\n+\n+\n+class LogStreamCounter:",
        "comment_created_at": "2025-06-09T09:47:45+00:00",
        "comment_author": "Lee-W",
        "comment_body": "The counter is the part I'm confused about \ud83e\udd14. What about  Accumulator? Collector?",
        "pr_file_module": null
      },
      {
        "comment_id": "2135423216",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/log_stream_counter.py",
        "discussion_id": "2135330938",
        "commented_code": "@@ -0,0 +1,151 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import os\n+import tempfile\n+from typing import TYPE_CHECKING\n+\n+if TYPE_CHECKING:\n+    from airflow.typing_compat import Self\n+    from airflow.utils.log.file_task_handler import (\n+        LogHandlerOutputStream,\n+        StructuredLogMessage,\n+        StructuredLogStream,\n+    )\n+\n+\n+class LogStreamCounter:",
        "comment_created_at": "2025-06-09T09:53:05+00:00",
        "comment_author": "jason810496",
        "comment_body": "Accumulator is indeed more accurate in this scenario.\n\nI will rename Counter with Accumulator, thanks for suggestion.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2151418512",
    "pr_number": 49470,
    "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
    "created_at": "2025-06-17T06:24:17+00:00",
    "commented_code": "log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n \n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n+def _sort_key(timestamp: datetime | None, line_num: int) -> int:",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2151418512",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
        "discussion_id": "2151418512",
        "commented_code": "@@ -166,17 +242,133 @@ def _parse_log_lines(\n                 log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n \n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n+def _sort_key(timestamp: datetime | None, line_num: int) -> int:",
        "comment_created_at": "2025-06-17T06:24:17+00:00",
        "comment_author": "Lee-W",
        "comment_body": "```suggestion\r\ndef _create_sort_key(timestamp: datetime | None, line_num: int) -> int:\r\n```\r\n\r\nnot sure whether it's better \ud83e\udd14 it kinda confused me when I saw `sort_key` is a none and `_sort_key` looks like a verb that returns `sort_key`",
        "pr_file_module": null
      },
      {
        "comment_id": "2158244021",
        "repo_full_name": "apache/airflow",
        "pr_number": 49470,
        "pr_file": "airflow-core/src/airflow/utils/log/file_task_handler.py",
        "discussion_id": "2151418512",
        "commented_code": "@@ -166,17 +242,133 @@ def _parse_log_lines(\n                 log.timestamp = coerce_datetime(log.timestamp)\n                 timestamp = log.timestamp\n             yield timestamp, idx, log\n+        idx += 1\n \n \n-def _interleave_logs(*logs: str | LogMessages) -> Iterable[StructuredLogMessage]:\n-    min_date = pendulum.datetime(2000, 1, 1)\n+def _sort_key(timestamp: datetime | None, line_num: int) -> int:",
        "comment_created_at": "2025-06-20T07:28:52+00:00",
        "comment_author": "jason810496",
        "comment_body": "Agree, adding the verb for prefix is more readable",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2059933570",
    "pr_number": 49180,
    "pr_file": "airflow-core/src/airflow/traces/tracer.py",
    "created_at": "2025-04-25T09:54:56+00:00",
    "commented_code": "@wraps(func)\n     def wrapper(*args, **kwargs):\n-        with Trace.start_span(span_name=func_name, component=component):\n+        with DebugTrace.start_span(span_name=func_name, component=component):",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2059933570",
        "repo_full_name": "apache/airflow",
        "pr_number": 49180,
        "pr_file": "airflow-core/src/airflow/traces/tracer.py",
        "discussion_id": "2059933570",
        "commented_code": "@@ -53,7 +53,7 @@ def add_span(func):\n \n     @wraps(func)\n     def wrapper(*args, **kwargs):\n-        with Trace.start_span(span_name=func_name, component=component):\n+        with DebugTrace.start_span(span_name=func_name, component=component):",
        "comment_created_at": "2025-04-25T09:54:56+00:00",
        "comment_author": "ashb",
        "comment_body": "Why is `add_span` always setting things as a DebugTrace? If it's only ever used for internal things I think we should rename this to `add_debug_span` to make it clearer",
        "pr_file_module": null
      },
      {
        "comment_id": "2066657362",
        "repo_full_name": "apache/airflow",
        "pr_number": 49180,
        "pr_file": "airflow-core/src/airflow/traces/tracer.py",
        "discussion_id": "2059933570",
        "commented_code": "@@ -53,7 +53,7 @@ def add_span(func):\n \n     @wraps(func)\n     def wrapper(*args, **kwargs):\n-        with Trace.start_span(span_name=func_name, component=component):\n+        with DebugTrace.start_span(span_name=func_name, component=component):",
        "comment_created_at": "2025-04-29T14:17:56+00:00",
        "comment_author": "xBis7",
        "comment_body": "Yeah, I don't think that users will ever need to annotate a dag function with the `add_span`. I'll rename it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2169194110",
    "pr_number": 52164,
    "pr_file": "providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/utils/pod_manager.py",
    "created_at": "2025-06-26T14:20:08+00:00",
    "commented_code": "message_timestamp = line_timestamp\n                                 progress_callback_lines.append(line)\n                             else:  # previous log line is complete\n-                                for line in progress_callback_lines:\n+                                for progress_line in progress_callback_lines:",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2169194110",
        "repo_full_name": "apache/airflow",
        "pr_number": 52164,
        "pr_file": "providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/utils/pod_manager.py",
        "discussion_id": "2169194110",
        "commented_code": "@@ -524,10 +524,10 @@ def consume_logs(*, since_time: DateTime | None = None) -> tuple[DateTime | None\n                                 message_timestamp = line_timestamp\n                                 progress_callback_lines.append(line)\n                             else:  # previous log line is complete\n-                                for line in progress_callback_lines:\n+                                for progress_line in progress_callback_lines:",
        "comment_created_at": "2025-06-26T14:20:08+00:00",
        "comment_author": "potiuk",
        "comment_body": "That does not seem like an eror. It's just a variable name. Are you sure that if you come back to the original code your test is failing ? It should not.",
        "pr_file_module": null
      },
      {
        "comment_id": "2173821267",
        "repo_full_name": "apache/airflow",
        "pr_number": 52164,
        "pr_file": "providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/utils/pod_manager.py",
        "discussion_id": "2169194110",
        "commented_code": "@@ -524,10 +524,10 @@ def consume_logs(*, since_time: DateTime | None = None) -> tuple[DateTime | None\n                                 message_timestamp = line_timestamp\n                                 progress_callback_lines.append(line)\n                             else:  # previous log line is complete\n-                                for line in progress_callback_lines:\n+                                for progress_line in progress_callback_lines:",
        "comment_created_at": "2025-06-29T16:21:28+00:00",
        "comment_author": "msimmoteit-neozo",
        "comment_body": "> It's just a variable name\r\n\r\nThat is correct. And I intend for this PR to do nothing but to change this variable name (and maybe add the unit test).\r\n\r\nThe reason this variable name needs to be changed and why I'm creating this MR is that in line 519, the variable `line` is assigned to `raw_line.decode(\"utf-8\", errors=\"backslashreplace\")` and in line 540 this variable is used: `progress_callback_lines = [line]`. But in Python, `for`-loops are not scoped. This means that the variable `line` is overwritten with the last value of the loop iterator. Leading to, in my opinion, undesired behaviour for using the `progress_callback`.\r\n\r\nTo illustrate this behaviour, I have created this Python fiddle: [Link](https://python-fiddle.com/saved/17dfa659-d320-4393-8b81-238bfb255625)\r\nOr you can execute this in a Python interpreter of your choice:\r\n```py\r\nline = \"Outer line\"\r\n\r\nfor line in [\"Log 1\", \"Log 2\", \"Log 3\"]:\r\n    x = line\r\n    \r\nprint(f\"Line after Loop: {line}\")\r\n\r\ndef test():\r\n    f_line = \"Outer Line Function\"\r\n    \r\n    for f_line in [\"FLog 1\", \"FLog 2\", \"FLog 3\"]:\r\n        y = f_line\r\n        \r\n    return f_line\r\n    \r\nret = test()\r\n\r\nprint(f\"Result of Function line: {ret}\")\r\n```\r\n\r\nAnd lastly, if I revert commit e1d67dd34b623bbb9bf3ce20af553f38355ad85a and execute the unit test added for this PR, this is the result:\r\n<img width=\"1130\" alt=\"grafik\" src=\"https://github.com/user-attachments/assets/ab266858-71bc-4ef7-9dd2-50587fb7b9a3\" />",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160030219",
    "pr_number": 51889,
    "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/datamodels/plugins.py",
    "created_at": "2025-06-21T12:52:13+00:00",
    "commented_code": "model_config = ConfigDict(extra=\"allow\")\n \n     name: str\n-    href: str | None = None\n+    href: str\n     category: str | None = None\n \n \n-class IFrameViewsResponse(BaseModel):\n+class ExternalViewResponse(BaseModel):\n     \"\"\"Serializer for IFrame Plugin responses.\"\"\"\n \n     model_config = ConfigDict(extra=\"allow\")\n \n     name: str\n-    src: str\n+    href: str\n     icon: str | None = None\n+    dark_mode_icon: str | None = None",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2160030219",
        "repo_full_name": "apache/airflow",
        "pr_number": 51889,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/datamodels/plugins.py",
        "discussion_id": "2160030219",
        "commented_code": "@@ -65,20 +65,22 @@ class AppBuilderMenuItemResponse(BaseModel):\n     model_config = ConfigDict(extra=\"allow\")\n \n     name: str\n-    href: str | None = None\n+    href: str\n     category: str | None = None\n \n \n-class IFrameViewsResponse(BaseModel):\n+class ExternalViewResponse(BaseModel):\n     \"\"\"Serializer for IFrame Plugin responses.\"\"\"\n \n     model_config = ConfigDict(extra=\"allow\")\n \n     name: str\n-    src: str\n+    href: str\n     icon: str | None = None\n+    dark_mode_icon: str | None = None",
        "comment_created_at": "2025-06-21T12:52:13+00:00",
        "comment_author": "jscheffl",
        "comment_body": "Reading multiple times I feel a \"dark mode\" should rather be a suffic for the property, would bring the benefit that a sorting lists light and dark mode together\r\n```suggestion\r\n    icon_dark_mode: str | None = None\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2161978260",
        "repo_full_name": "apache/airflow",
        "pr_number": 51889,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/datamodels/plugins.py",
        "discussion_id": "2160030219",
        "commented_code": "@@ -65,20 +65,22 @@ class AppBuilderMenuItemResponse(BaseModel):\n     model_config = ConfigDict(extra=\"allow\")\n \n     name: str\n-    href: str | None = None\n+    href: str\n     category: str | None = None\n \n \n-class IFrameViewsResponse(BaseModel):\n+class ExternalViewResponse(BaseModel):\n     \"\"\"Serializer for IFrame Plugin responses.\"\"\"\n \n     model_config = ConfigDict(extra=\"allow\")\n \n     name: str\n-    src: str\n+    href: str\n     icon: str | None = None\n+    dark_mode_icon: str | None = None",
        "comment_created_at": "2025-06-23T16:05:15+00:00",
        "comment_author": "pierrejeambrun",
        "comment_body": "Good point, updated!",
        "pr_file_module": null
      }
    ]
  }
]