[
  {
    "discussion_id": "2185235873",
    "pr_number": 52871,
    "pr_file": "task-sdk/src/airflow/sdk/definitions/_internal/abstractoperator.py",
    "created_at": "2025-07-04T12:25:43+00:00",
    "commented_code": "MINIMUM_PRIORITY_WEIGHT: int = -2147483648\n MAXIMUM_PRIORITY_WEIGHT: int = 2147483647\n DEFAULT_EXECUTOR: str | None = None\n-DEFAULT_QUEUE: str = conf.get(\"operators\", \"default_queue\", \"default\")\n+DEFAULT_QUEUE: str = conf.get_mandatory_value(\"operators\", \"default_queue\")",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2185235873",
        "repo_full_name": "apache/airflow",
        "pr_number": 52871,
        "pr_file": "task-sdk/src/airflow/sdk/definitions/_internal/abstractoperator.py",
        "discussion_id": "2185235873",
        "commented_code": "@@ -62,7 +62,7 @@\n MINIMUM_PRIORITY_WEIGHT: int = -2147483648\n MAXIMUM_PRIORITY_WEIGHT: int = 2147483647\n DEFAULT_EXECUTOR: str | None = None\n-DEFAULT_QUEUE: str = conf.get(\"operators\", \"default_queue\", \"default\")\n+DEFAULT_QUEUE: str = conf.get_mandatory_value(\"operators\", \"default_queue\")",
        "comment_created_at": "2025-07-04T12:25:43+00:00",
        "comment_author": "ashb",
        "comment_body": "No point to this -- we've provided a default here in the code.",
        "pr_file_module": null
      },
      {
        "comment_id": "2185237559",
        "repo_full_name": "apache/airflow",
        "pr_number": 52871,
        "pr_file": "task-sdk/src/airflow/sdk/definitions/_internal/abstractoperator.py",
        "discussion_id": "2185235873",
        "commented_code": "@@ -62,7 +62,7 @@\n MINIMUM_PRIORITY_WEIGHT: int = -2147483648\n MAXIMUM_PRIORITY_WEIGHT: int = 2147483647\n DEFAULT_EXECUTOR: str | None = None\n-DEFAULT_QUEUE: str = conf.get(\"operators\", \"default_queue\", \"default\")\n+DEFAULT_QUEUE: str = conf.get_mandatory_value(\"operators\", \"default_queue\")",
        "comment_created_at": "2025-07-04T12:26:27+00:00",
        "comment_author": "ashb",
        "comment_body": "Also I don't even know why that function exists -- `get` will fail if it's not set with:\r\n\r\n```\r\n        raise AirflowConfigException(f\"section/key [{section}/{key}] not found in config\")\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2185245946",
        "repo_full_name": "apache/airflow",
        "pr_number": 52871,
        "pr_file": "task-sdk/src/airflow/sdk/definitions/_internal/abstractoperator.py",
        "discussion_id": "2185235873",
        "commented_code": "@@ -62,7 +62,7 @@\n MINIMUM_PRIORITY_WEIGHT: int = -2147483648\n MAXIMUM_PRIORITY_WEIGHT: int = 2147483647\n DEFAULT_EXECUTOR: str | None = None\n-DEFAULT_QUEUE: str = conf.get(\"operators\", \"default_queue\", \"default\")\n+DEFAULT_QUEUE: str = conf.get_mandatory_value(\"operators\", \"default_queue\")",
        "comment_created_at": "2025-07-04T12:30:18+00:00",
        "comment_author": "kaxil",
        "comment_body": "> No point to this -- we've provided a default here in the code.\r\n\r\nWe don't have the default in the code anymore:\r\n\r\n```py\r\nDEFAULT_QUEUE: str = conf.get_mandatory_value(\"operators\", \"default_queue\")\r\n```\r\n\r\n>Also I don't even know why that function exists -- get will fail if it's not set with:\r\n\r\nYeah, no clue \ud83e\udd37 \r\n\r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2149589600",
    "pr_number": 50371,
    "pr_file": "airflow-core/src/airflow/configuration.py",
    "created_at": "2025-06-16T10:18:03+00:00",
    "commented_code": "# DeprecationWarning will be issued and the old option will be used instead\n     deprecated_options: dict[tuple[str, str], tuple[str, str, str]] = {\n         (\"dag_processor\", \"refresh_interval\"): (\"scheduler\", \"dag_dir_list_interval\", \"3.0\"),\n+        (\"dag_processor\", \"parsing_pre_import_modules\"): (\"scheduler\", \"parsing_pre_import_modules\", \"3.0.2\"),",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2149589600",
        "repo_full_name": "apache/airflow",
        "pr_number": 50371,
        "pr_file": "airflow-core/src/airflow/configuration.py",
        "discussion_id": "2149589600",
        "commented_code": "@@ -347,6 +347,7 @@ def sensitive_config_values(self) -> set[tuple[str, str]]:\n     # DeprecationWarning will be issued and the old option will be used instead\n     deprecated_options: dict[tuple[str, str], tuple[str, str, str]] = {\n         (\"dag_processor\", \"refresh_interval\"): (\"scheduler\", \"dag_dir_list_interval\", \"3.0\"),\n+        (\"dag_processor\", \"parsing_pre_import_modules\"): (\"scheduler\", \"parsing_pre_import_modules\", \"3.0.2\"),",
        "comment_created_at": "2025-06-16T10:18:03+00:00",
        "comment_author": "ephraimbuddy",
        "comment_body": "```suggestion\r\n        (\"dag_processor\", \"parsing_pre_import_modules\"): (\"scheduler\", \"parsing_pre_import_modules\", \"3.1.0\"),\r\n```\r\nLet's plan this for 3.1.0 since it's not a bug fix but an improvement",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2059939140",
    "pr_number": 49180,
    "pr_file": "airflow-core/tests/unit/core/test_otel_tracer.py",
    "created_at": "2025-04-25T09:57:38+00:00",
    "commented_code": "class TestOtelTrace:\n     def test_get_otel_tracer_from_trace_metaclass(self):\n         \"\"\"Test that `Trace.some_method()`, uses an `OtelTrace` instance when otel is configured.\"\"\"\n-        conf.add_section(\"traces\")\n+        if conf.getsection(\"traces\") is None:\n+            conf.add_section(\"traces\")",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2059939140",
        "repo_full_name": "apache/airflow",
        "pr_number": 49180,
        "pr_file": "airflow-core/tests/unit/core/test_otel_tracer.py",
        "discussion_id": "2059939140",
        "commented_code": "@@ -42,7 +42,8 @@ def name():\n class TestOtelTrace:\n     def test_get_otel_tracer_from_trace_metaclass(self):\n         \"\"\"Test that `Trace.some_method()`, uses an `OtelTrace` instance when otel is configured.\"\"\"\n-        conf.add_section(\"traces\")\n+        if conf.getsection(\"traces\") is None:\n+            conf.add_section(\"traces\")",
        "comment_created_at": "2025-04-25T09:57:38+00:00",
        "comment_author": "ashb",
        "comment_body": "```suggestion\r\n```\r\n\r\nNot needed anymore, `conf.set` now creates the section if needed.",
        "pr_file_module": null
      },
      {
        "comment_id": "2064328809",
        "repo_full_name": "apache/airflow",
        "pr_number": 49180,
        "pr_file": "airflow-core/tests/unit/core/test_otel_tracer.py",
        "discussion_id": "2059939140",
        "commented_code": "@@ -42,7 +42,8 @@ def name():\n class TestOtelTrace:\n     def test_get_otel_tracer_from_trace_metaclass(self):\n         \"\"\"Test that `Trace.some_method()`, uses an `OtelTrace` instance when otel is configured.\"\"\"\n-        conf.add_section(\"traces\")\n+        if conf.getsection(\"traces\") is None:\n+            conf.add_section(\"traces\")",
        "comment_created_at": "2025-04-28T18:56:11+00:00",
        "comment_author": "xBis7",
        "comment_body": "ok, I removed it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2157418819",
    "pr_number": 49180,
    "pr_file": "airflow-core/src/airflow/traces/tracer.py",
    "created_at": "2025-06-19T17:12:44+00:00",
    "commented_code": "cls._initialize_instance()\n         return cls.instance\n \n-    @classmethod\n     def configure_factory(cls):\n         \"\"\"Configure the trace factory based on settings.\"\"\"\n-        if conf.has_option(\"traces\", \"otel_on\") and conf.getboolean(\"traces\", \"otel_on\"):\n+        otel_on = conf.getboolean(\"traces\", \"otel_on\")\n+\n+        if cls.check_debug_traces_flag:\n+            debug_traces_on = conf.getboolean(\"traces\", \"otel_debug_traces_on\")\n+        else:\n+            # Set to true so that it will be ignored during the evaluation for the factory instance.\n+            debug_traces_on = True",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2157418819",
        "repo_full_name": "apache/airflow",
        "pr_number": 49180,
        "pr_file": "airflow-core/src/airflow/traces/tracer.py",
        "discussion_id": "2157418819",
        "commented_code": "@@ -276,29 +283,45 @@ def __call__(cls, *args, **kwargs):\n             cls._initialize_instance()\n         return cls.instance\n \n-    @classmethod\n     def configure_factory(cls):\n         \"\"\"Configure the trace factory based on settings.\"\"\"\n-        if conf.has_option(\"traces\", \"otel_on\") and conf.getboolean(\"traces\", \"otel_on\"):\n+        otel_on = conf.getboolean(\"traces\", \"otel_on\")\n+\n+        if cls.check_debug_traces_flag:\n+            debug_traces_on = conf.getboolean(\"traces\", \"otel_debug_traces_on\")\n+        else:\n+            # Set to true so that it will be ignored during the evaluation for the factory instance.\n+            debug_traces_on = True",
        "comment_created_at": "2025-06-19T17:12:44+00:00",
        "comment_author": "potiuk",
        "comment_body": "Could you explain the logic of this True here - I am not sure i follow why debugging should default to True here?",
        "pr_file_module": null
      },
      {
        "comment_id": "2157487359",
        "repo_full_name": "apache/airflow",
        "pr_number": 49180,
        "pr_file": "airflow-core/src/airflow/traces/tracer.py",
        "discussion_id": "2157418819",
        "commented_code": "@@ -276,29 +283,45 @@ def __call__(cls, *args, **kwargs):\n             cls._initialize_instance()\n         return cls.instance\n \n-    @classmethod\n     def configure_factory(cls):\n         \"\"\"Configure the trace factory based on settings.\"\"\"\n-        if conf.has_option(\"traces\", \"otel_on\") and conf.getboolean(\"traces\", \"otel_on\"):\n+        otel_on = conf.getboolean(\"traces\", \"otel_on\")\n+\n+        if cls.check_debug_traces_flag:\n+            debug_traces_on = conf.getboolean(\"traces\", \"otel_debug_traces_on\")\n+        else:\n+            # Set to true so that it will be ignored during the evaluation for the factory instance.\n+            debug_traces_on = True",
        "comment_created_at": "2025-06-19T18:19:10+00:00",
        "comment_author": "xBis7",
        "comment_body": "`debug_traces_on` is only used in line 296\r\n\r\n```python\r\nif otel_on and debug_traces_on:\r\n```\r\n\r\nif we don't want to check that flag\r\n```python\r\nif cls.check_debug_traces_flag:\r\n```\r\n\r\nthen by setting it to `True` it's always ignored in the next evaluation. The condition will always evaluate to whatever value `otel_on` has.\r\n\r\n* if `otel_on` is true and `debug_traces_on` is true, then it will be true\r\n* if `otel_on` is false and `debug_traces_on` is true, then it will be false",
        "pr_file_module": null
      },
      {
        "comment_id": "2157493538",
        "repo_full_name": "apache/airflow",
        "pr_number": 49180,
        "pr_file": "airflow-core/src/airflow/traces/tracer.py",
        "discussion_id": "2157418819",
        "commented_code": "@@ -276,29 +283,45 @@ def __call__(cls, *args, **kwargs):\n             cls._initialize_instance()\n         return cls.instance\n \n-    @classmethod\n     def configure_factory(cls):\n         \"\"\"Configure the trace factory based on settings.\"\"\"\n-        if conf.has_option(\"traces\", \"otel_on\") and conf.getboolean(\"traces\", \"otel_on\"):\n+        otel_on = conf.getboolean(\"traces\", \"otel_on\")\n+\n+        if cls.check_debug_traces_flag:\n+            debug_traces_on = conf.getboolean(\"traces\", \"otel_debug_traces_on\")\n+        else:\n+            # Set to true so that it will be ignored during the evaluation for the factory instance.\n+            debug_traces_on = True",
        "comment_created_at": "2025-06-19T18:24:55+00:00",
        "comment_author": "xBis7",
        "comment_body": "I extended the comment to explain it better.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089400833",
    "pr_number": 50516,
    "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
    "created_at": "2025-05-14T17:19:14+00:00",
    "commented_code": "+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    # Maximum number of retries to invoke Lambda.\n+    MAX_INVOKE_ATTEMPTS = conf.get(\n+        CONFIG_GROUP_NAME,\n+        AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        fallback=3,",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2089400833",
        "repo_full_name": "apache/airflow",
        "pr_number": 50516,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
        "discussion_id": "2089400833",
        "commented_code": "@@ -0,0 +1,488 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    # Maximum number of retries to invoke Lambda.\n+    MAX_INVOKE_ATTEMPTS = conf.get(\n+        CONFIG_GROUP_NAME,\n+        AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        fallback=3,",
        "comment_created_at": "2025-05-14T17:19:14+00:00",
        "comment_author": "vincbeck",
        "comment_body": "It applies to all `conf.get` calls, the `fallback` should be necessary. The default is already defined in config",
        "pr_file_module": null
      },
      {
        "comment_id": "2089413495",
        "repo_full_name": "apache/airflow",
        "pr_number": 50516,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
        "discussion_id": "2089400833",
        "commented_code": "@@ -0,0 +1,488 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    # Maximum number of retries to invoke Lambda.\n+    MAX_INVOKE_ATTEMPTS = conf.get(\n+        CONFIG_GROUP_NAME,\n+        AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        fallback=3,",
        "comment_created_at": "2025-05-14T17:27:27+00:00",
        "comment_author": "o-nikolas",
        "comment_body": "This one is weird, I thought so too, but my testing has shown that this does not work as expected? Maybe it's something on my end?",
        "pr_file_module": null
      },
      {
        "comment_id": "2089425146",
        "repo_full_name": "apache/airflow",
        "pr_number": 50516,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
        "discussion_id": "2089400833",
        "commented_code": "@@ -0,0 +1,488 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    # Maximum number of retries to invoke Lambda.\n+    MAX_INVOKE_ATTEMPTS = conf.get(\n+        CONFIG_GROUP_NAME,\n+        AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        fallback=3,",
        "comment_created_at": "2025-05-14T17:35:06+00:00",
        "comment_author": "vincbeck",
        "comment_body": "Did you test it after you run `pre-commits`? `pre-commits` generate `get_provider_info.py` which are used to get configuration for providers. So you must run pre-commits to generate these file and then after these configuration are getting picked-up.\r\n\r\nAnother idea would be, `@providers_configuration_loaded` is missing. This decorator loads the configuration from providers. Since the executor is instantiated in the scheduler component, we do not load configuration from providers in scheduler?",
        "pr_file_module": null
      },
      {
        "comment_id": "2089432166",
        "repo_full_name": "apache/airflow",
        "pr_number": 50516,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/executors/aws_lambda/lambda_executor.py",
        "discussion_id": "2089400833",
        "commented_code": "@@ -0,0 +1,488 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import json\n+import time\n+from collections import deque\n+from collections.abc import Sequence\n+from typing import TYPE_CHECKING\n+\n+from boto3.session import NoCredentialsError\n+from botocore.utils import ClientError\n+\n+from airflow.configuration import conf\n+from airflow.exceptions import AirflowException\n+from airflow.executors.base_executor import BaseExecutor\n+from airflow.models.taskinstancekey import TaskInstanceKey\n+from airflow.providers.amazon.aws.executors.aws_lambda.utils import (\n+    CONFIG_GROUP_NAME,\n+    INVALID_CREDENTIALS_EXCEPTIONS,\n+    AllLambdaConfigKeys,\n+    CommandType,\n+    LambdaQueuedTask,\n+)\n+from airflow.providers.amazon.aws.executors.utils.exponential_backoff_retry import (\n+    calculate_next_attempt_delay,\n+    exponential_backoff_retry,\n+)\n+from airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\n+from airflow.providers.amazon.aws.hooks.sqs import SqsHook\n+from airflow.stats import Stats\n+from airflow.utils import timezone\n+\n+from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n+    from airflow.executors import workloads\n+    from airflow.models.taskinstance import TaskInstance\n+\n+\n+class AwsLambdaExecutor(BaseExecutor):\n+    \"\"\"\n+    An Airflow Executor that submits tasks to AWS Lambda asynchronously.\n+\n+    When execute_async() is called, the executor invokes a specified AWS Lambda function (asynchronously)\n+    with a payload that includes the task command and a unique task key.\n+\n+    The Lambda function writes its result directly to an SQS queue, which is then polled by this executor\n+    to update task state in Airflow.\n+    \"\"\"\n+\n+    # Maximum number of retries to invoke Lambda.\n+    MAX_INVOKE_ATTEMPTS = conf.get(\n+        CONFIG_GROUP_NAME,\n+        AllLambdaConfigKeys.MAX_INVOKE_ATTEMPTS,\n+        fallback=3,",
        "comment_created_at": "2025-05-14T17:38:38+00:00",
        "comment_author": "o-nikolas",
        "comment_body": "Interesting, I'll go have a look at these two options!",
        "pr_file_module": null
      }
    ]
  }
]