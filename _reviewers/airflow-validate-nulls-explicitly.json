[
  {
    "discussion_id": "2141316967",
    "pr_number": 51638,
    "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/dag_run.py",
    "created_at": "2025-06-12T00:46:26+00:00",
    "commented_code": "state=DagRunState.QUEUED,\n             session=session,\n         )\n+\n+        if dag.deadline and dag.has_dagrun_deadline():",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2141316967",
        "repo_full_name": "apache/airflow",
        "pr_number": 51638,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/dag_run.py",
        "discussion_id": "2141316967",
        "commented_code": "@@ -421,6 +421,18 @@ def trigger_dag_run(\n             state=DagRunState.QUEUED,\n             session=session,\n         )\n+\n+        if dag.deadline and dag.has_dagrun_deadline():",
        "comment_created_at": "2025-06-12T00:46:26+00:00",
        "comment_author": "ferruzzi",
        "comment_body": "This is a little redundant, but mypy required the `if dag.deadline` check to be separate like that.",
        "pr_file_module": null
      },
      {
        "comment_id": "2141627395",
        "repo_full_name": "apache/airflow",
        "pr_number": 51638,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/dag_run.py",
        "discussion_id": "2141316967",
        "commented_code": "@@ -421,6 +421,18 @@ def trigger_dag_run(\n             state=DagRunState.QUEUED,\n             session=session,\n         )\n+\n+        if dag.deadline and dag.has_dagrun_deadline():",
        "comment_created_at": "2025-06-12T03:41:22+00:00",
        "comment_author": "uranusjr",
        "comment_body": "I usually write this sort of things like this\r\n\r\n```python\r\ndef get_dagrun_deadline(self) -> Deadline:\r\n    ...\r\n\r\nif deadline := dag.get_dagrun_deadline():\r\n    ...\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2141764844",
        "repo_full_name": "apache/airflow",
        "pr_number": 51638,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/dag_run.py",
        "discussion_id": "2141316967",
        "commented_code": "@@ -421,6 +421,18 @@ def trigger_dag_run(\n             state=DagRunState.QUEUED,\n             session=session,\n         )\n+\n+        if dag.deadline and dag.has_dagrun_deadline():",
        "comment_created_at": "2025-06-12T05:47:02+00:00",
        "comment_author": "ferruzzi",
        "comment_body": "interesting, I wonder why mypy would allow that but not  `if dag.get_dagrun_deadline():`?    I'll give it a try, thanks.",
        "pr_file_module": null
      },
      {
        "comment_id": "2141813662",
        "repo_full_name": "apache/airflow",
        "pr_number": 51638,
        "pr_file": "airflow-core/src/airflow/api_fastapi/core_api/routes/public/dag_run.py",
        "discussion_id": "2141316967",
        "commented_code": "@@ -421,6 +421,18 @@ def trigger_dag_run(\n             state=DagRunState.QUEUED,\n             session=session,\n         )\n+\n+        if dag.deadline and dag.has_dagrun_deadline():",
        "comment_created_at": "2025-06-12T06:26:34+00:00",
        "comment_author": "ferruzzi",
        "comment_body": "oh, are you talking about replacing the boolen `has_dagrun_deadline` with `get_dagrun_deadline` instead?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2174944110",
    "pr_number": 51873,
    "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/hooks/chime.py",
    "created_at": "2025-06-30T12:20:29+00:00",
    "commented_code": "token = conn.password\n         if token is None:\n             raise AirflowException(\"Webhook token field is missing and is required.\")\n-        url = conn.schema + \"://\" + conn.host\n+        url = cast(\"str\", conn.schema) + \"://\" + cast(\"str\", conn.host)",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2174944110",
        "repo_full_name": "apache/airflow",
        "pr_number": 51873,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/hooks/chime.py",
        "discussion_id": "2174944110",
        "commented_code": "@@ -69,7 +69,7 @@ def _get_webhook_endpoint(self, conn_id: str) -> str:\n         token = conn.password\n         if token is None:\n             raise AirflowException(\"Webhook token field is missing and is required.\")\n-        url = conn.schema + \"://\" + conn.host\n+        url = cast(\"str\", conn.schema) + \"://\" + cast(\"str\", conn.host)",
        "comment_created_at": "2025-06-30T12:20:29+00:00",
        "comment_author": "kaxil",
        "comment_body": "You can add an error above, since you expect `conn.schema` and `conn.host` to be non-None -- similar to `token` field above",
        "pr_file_module": null
      },
      {
        "comment_id": "2176865021",
        "repo_full_name": "apache/airflow",
        "pr_number": 51873,
        "pr_file": "providers/amazon/src/airflow/providers/amazon/aws/hooks/chime.py",
        "discussion_id": "2174944110",
        "commented_code": "@@ -69,7 +69,7 @@ def _get_webhook_endpoint(self, conn_id: str) -> str:\n         token = conn.password\n         if token is None:\n             raise AirflowException(\"Webhook token field is missing and is required.\")\n-        url = conn.schema + \"://\" + conn.host\n+        url = cast(\"str\", conn.schema) + \"://\" + cast(\"str\", conn.host)",
        "comment_created_at": "2025-07-01T08:42:59+00:00",
        "comment_author": "amoghrajesh",
        "comment_body": "Makes sense, added null check",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2123205748",
    "pr_number": 50825,
    "pr_file": "airflow-core/src/airflow/cli/commands/task_command.py",
    "created_at": "2025-06-03T09:05:36+00:00",
    "commented_code": "f\"run_id or logical_date of {logical_date_or_run_id!r} not found\"\n             )\n         # TODO: Validate map_index is in range?\n-        ti = TaskInstance(task, run_id=dag_run.run_id, map_index=map_index)\n+        dag_version = DagVersion.get_latest_version(dag.dag_id, session=session)\n+        if TYPE_CHECKING:\n+            assert dag_version",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2123205748",
        "repo_full_name": "apache/airflow",
        "pr_number": 50825,
        "pr_file": "airflow-core/src/airflow/cli/commands/task_command.py",
        "discussion_id": "2123205748",
        "commented_code": "@@ -193,7 +194,10 @@ def _get_ti(\n                 f\"run_id or logical_date of {logical_date_or_run_id!r} not found\"\n             )\n         # TODO: Validate map_index is in range?\n-        ti = TaskInstance(task, run_id=dag_run.run_id, map_index=map_index)\n+        dag_version = DagVersion.get_latest_version(dag.dag_id, session=session)\n+        if TYPE_CHECKING:\n+            assert dag_version",
        "comment_created_at": "2025-06-03T09:05:36+00:00",
        "comment_author": "pierrejeambrun",
        "comment_body": "That type checking assert feels really weird. Maybe just handle the None case `if ... is None` -> `raise dag version not found`, while we wait for the follow up PR to clean that up.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2179479479",
    "pr_number": 52693,
    "pr_file": "airflow-core/src/airflow/jobs/triggerer_job_runner.py",
    "created_at": "2025-07-02T08:42:25+00:00",
    "commented_code": "await asyncio.sleep(0)\n \n             try:\n-                kwargs = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n-                trigger_instance = trigger_class(**kwargs)\n+                from airflow.serialization.serialized_objects import smart_decode_trigger_kwargs\n+\n+                # Decrypt and clean trigger kwargs before for execution\n+                # Note: We only clean up serialization artifacts (__var, __type keys) here,\n+                # not in `_decrypt_kwargs` because it is used during hash comparison in\n+                # add_asset_trigger_references and could lead to adverse effects like hash mismatches\n+                # that could cause None values in collections.\n+                kw = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n+                deserialised_kwargs = {k: smart_decode_trigger_kwargs(v) for k, v in kw.items()}",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2179479479",
        "repo_full_name": "apache/airflow",
        "pr_number": 52693,
        "pr_file": "airflow-core/src/airflow/jobs/triggerer_job_runner.py",
        "discussion_id": "2179479479",
        "commented_code": "@@ -870,8 +870,16 @@ async def create_triggers(self):\n             await asyncio.sleep(0)\n \n             try:\n-                kwargs = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n-                trigger_instance = trigger_class(**kwargs)\n+                from airflow.serialization.serialized_objects import smart_decode_trigger_kwargs\n+\n+                # Decrypt and clean trigger kwargs before for execution\n+                # Note: We only clean up serialization artifacts (__var, __type keys) here,\n+                # not in `_decrypt_kwargs` because it is used during hash comparison in\n+                # add_asset_trigger_references and could lead to adverse effects like hash mismatches\n+                # that could cause None values in collections.\n+                kw = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n+                deserialised_kwargs = {k: smart_decode_trigger_kwargs(v) for k, v in kw.items()}",
        "comment_created_at": "2025-07-02T08:42:25+00:00",
        "comment_author": "uranusjr",
        "comment_body": "Can this not just use `BaseSerialization.deserialize`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2179485699",
        "repo_full_name": "apache/airflow",
        "pr_number": 52693,
        "pr_file": "airflow-core/src/airflow/jobs/triggerer_job_runner.py",
        "discussion_id": "2179479479",
        "commented_code": "@@ -870,8 +870,16 @@ async def create_triggers(self):\n             await asyncio.sleep(0)\n \n             try:\n-                kwargs = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n-                trigger_instance = trigger_class(**kwargs)\n+                from airflow.serialization.serialized_objects import smart_decode_trigger_kwargs\n+\n+                # Decrypt and clean trigger kwargs before for execution\n+                # Note: We only clean up serialization artifacts (__var, __type keys) here,\n+                # not in `_decrypt_kwargs` because it is used during hash comparison in\n+                # add_asset_trigger_references and could lead to adverse effects like hash mismatches\n+                # that could cause None values in collections.\n+                kw = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n+                deserialised_kwargs = {k: smart_decode_trigger_kwargs(v) for k, v in kw.items()}",
        "comment_created_at": "2025-07-02T08:45:13+00:00",
        "comment_author": "amoghrajesh",
        "comment_body": "I think the only extra thing the `smart_decode_trigger_kwargs` utility does it to: check whether d is a dictionary and contains a special key, Encoding.TYPE. Returns if not. \r\n\r\nI think its worth having the safety check",
        "pr_file_module": null
      },
      {
        "comment_id": "2179563944",
        "repo_full_name": "apache/airflow",
        "pr_number": 52693,
        "pr_file": "airflow-core/src/airflow/jobs/triggerer_job_runner.py",
        "discussion_id": "2179479479",
        "commented_code": "@@ -870,8 +870,16 @@ async def create_triggers(self):\n             await asyncio.sleep(0)\n \n             try:\n-                kwargs = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n-                trigger_instance = trigger_class(**kwargs)\n+                from airflow.serialization.serialized_objects import smart_decode_trigger_kwargs\n+\n+                # Decrypt and clean trigger kwargs before for execution\n+                # Note: We only clean up serialization artifacts (__var, __type keys) here,\n+                # not in `_decrypt_kwargs` because it is used during hash comparison in\n+                # add_asset_trigger_references and could lead to adverse effects like hash mismatches\n+                # that could cause None values in collections.\n+                kw = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n+                deserialised_kwargs = {k: smart_decode_trigger_kwargs(v) for k, v in kw.items()}",
        "comment_created_at": "2025-07-02T09:21:00+00:00",
        "comment_author": "uranusjr",
        "comment_body": "We should probably fix the docstring of `smart_decode_trigger_kwargs` in this case since it\u2019s no longer just used for UI.",
        "pr_file_module": null
      },
      {
        "comment_id": "2179564977",
        "repo_full_name": "apache/airflow",
        "pr_number": 52693,
        "pr_file": "airflow-core/src/airflow/jobs/triggerer_job_runner.py",
        "discussion_id": "2179479479",
        "commented_code": "@@ -870,8 +870,16 @@ async def create_triggers(self):\n             await asyncio.sleep(0)\n \n             try:\n-                kwargs = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n-                trigger_instance = trigger_class(**kwargs)\n+                from airflow.serialization.serialized_objects import smart_decode_trigger_kwargs\n+\n+                # Decrypt and clean trigger kwargs before for execution\n+                # Note: We only clean up serialization artifacts (__var, __type keys) here,\n+                # not in `_decrypt_kwargs` because it is used during hash comparison in\n+                # add_asset_trigger_references and could lead to adverse effects like hash mismatches\n+                # that could cause None values in collections.\n+                kw = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n+                deserialised_kwargs = {k: smart_decode_trigger_kwargs(v) for k, v in kw.items()}",
        "comment_created_at": "2025-07-02T09:21:29+00:00",
        "comment_author": "uranusjr",
        "comment_body": "Oh I saw you already discussed that below.",
        "pr_file_module": null
      },
      {
        "comment_id": "2179565619",
        "repo_full_name": "apache/airflow",
        "pr_number": 52693,
        "pr_file": "airflow-core/src/airflow/jobs/triggerer_job_runner.py",
        "discussion_id": "2179479479",
        "commented_code": "@@ -870,8 +870,16 @@ async def create_triggers(self):\n             await asyncio.sleep(0)\n \n             try:\n-                kwargs = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n-                trigger_instance = trigger_class(**kwargs)\n+                from airflow.serialization.serialized_objects import smart_decode_trigger_kwargs\n+\n+                # Decrypt and clean trigger kwargs before for execution\n+                # Note: We only clean up serialization artifacts (__var, __type keys) here,\n+                # not in `_decrypt_kwargs` because it is used during hash comparison in\n+                # add_asset_trigger_references and could lead to adverse effects like hash mismatches\n+                # that could cause None values in collections.\n+                kw = Trigger._decrypt_kwargs(workload.encrypted_kwargs)\n+                deserialised_kwargs = {k: smart_decode_trigger_kwargs(v) for k, v in kw.items()}",
        "comment_created_at": "2025-07-02T09:21:47+00:00",
        "comment_author": "amoghrajesh",
        "comment_body": "Yeah :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2099906836",
    "pr_number": 50730,
    "pr_file": "airflow-core/src/airflow/dag_processing/manager.py",
    "created_at": "2025-05-21T10:13:07+00:00",
    "commented_code": ")\n                 self._file_stats[file] = stat\n \n-        # Clean up `self._processors` after iterating over it\n+        # Clean up timed-out `self._processors` after iterating over it\n         for proc in processors_to_remove:\n-            processor = self._processors.pop(proc)\n-            processor.logger_filehandle.close()\n+            processor = self._processors.pop(proc, None)\n+            if processor:\n+                processor.logger_filehandle.close()",
    "repo_full_name": "apache/airflow",
    "discussion_comments": [
      {
        "comment_id": "2099906836",
        "repo_full_name": "apache/airflow",
        "pr_number": 50730,
        "pr_file": "airflow-core/src/airflow/dag_processing/manager.py",
        "discussion_id": "2099906836",
        "commented_code": "@@ -1034,10 +1054,11 @@ def _kill_timed_out_processors(self):\n                 )\n                 self._file_stats[file] = stat\n \n-        # Clean up `self._processors` after iterating over it\n+        # Clean up timed-out `self._processors` after iterating over it\n         for proc in processors_to_remove:\n-            processor = self._processors.pop(proc)\n-            processor.logger_filehandle.close()\n+            processor = self._processors.pop(proc, None)\n+            if processor:\n+                processor.logger_filehandle.close()",
        "comment_created_at": "2025-05-21T10:13:07+00:00",
        "comment_author": "uranusjr",
        "comment_body": "```suggestion\r\n            if processor := self._processors.pop(proc, None):\r\n                processor.logger_filehandle.close()\r\n```\r\n\r\nThere are elsewhere that can benefit from this pattern too.",
        "pr_file_module": null
      }
    ]
  }
]