[
  {
    "discussion_id": "2161816859",
    "pr_number": 62139,
    "pr_file": "adev/src/content/tutorials/learn-angular/intro/README.md",
    "created_at": "2025-06-23T14:48:05+00:00",
    "commented_code": "If you get stuck, click \"Reveal answer\" at the top.\n \n Alright, let's [get started](/tutorials/learn-angular/1-components-in-angular).\n+\n+## Using AI for Development\n+\n+In case you're following this tutorial in your favorite AI powered IDE, check out Angular [prompt rules and best practices](/ai/develop-with-ai).",
    "repo_full_name": "angular/angular",
    "discussion_comments": [
      {
        "comment_id": "2161816859",
        "repo_full_name": "angular/angular",
        "pr_number": 62139,
        "pr_file": "adev/src/content/tutorials/learn-angular/intro/README.md",
        "discussion_id": "2161816859",
        "commented_code": "@@ -11,3 +11,7 @@ Each step represents a concept in Angular. You can do one, or all of them.\n If you get stuck, click \"Reveal answer\" at the top.\n \n Alright, let's [get started](/tutorials/learn-angular/1-components-in-angular).\n+\n+## Using AI for Development\n+\n+In case you're following this tutorial in your favorite AI powered IDE, check out Angular [prompt rules and best practices](/ai/develop-with-ai).",
        "comment_created_at": "2025-06-23T14:48:05+00:00",
        "comment_author": "MarkTechson",
        "comment_body": "```suggestion\r\nIn case you're following this tutorial in your preferred AI powered IDE, [check out Angular prompt rules and best practices](/ai/develop-with-ai).\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2222965234",
    "pr_number": 62746,
    "pr_file": "adev/src/content/ai/develop-with-ai.md",
    "created_at": "2025-07-22T15:34:45+00:00",
    "commented_code": "| VS Code | <a download=\".instructions.md\" href=\"/assets/context/guidelines.md\" target=\"_blank\">.instructions.md</a>  | <a href=\"https://code.visualstudio.com/docs/copilot/copilot-customization#_custom-instructions\" target=\"_blank\">Configure `.instructions.md`</a> |\n | Windsurf | <a download href=\"/assets/context/guidelines.md\" target=\"_blank\">guidelines.md</a>  | <a href=\"https://docs.windsurf.com/windsurf/cascade/memories#rules\" target=\"_blank\">Configure `guidelines.md`</a> |\n \n+## Angular CLI MCP Server setup\n+The Angular CLI includes a Model Context Protocol (MCP) server. This allows AI assistants in your IDE to interact with the Angular CLI, enabling them to generate code, add packages, and more.",
    "repo_full_name": "angular/angular",
    "discussion_comments": [
      {
        "comment_id": "2222965234",
        "repo_full_name": "angular/angular",
        "pr_number": 62746,
        "pr_file": "adev/src/content/ai/develop-with-ai.md",
        "discussion_id": "2222965234",
        "commented_code": "@@ -26,6 +26,62 @@ Several editors, such as <a href=\"https://studio.firebase.google.com?utm_source=\n | VS Code | <a download=\".instructions.md\" href=\"/assets/context/guidelines.md\" target=\"_blank\">.instructions.md</a>  | <a href=\"https://code.visualstudio.com/docs/copilot/copilot-customization#_custom-instructions\" target=\"_blank\">Configure `.instructions.md`</a> |\n | Windsurf | <a download href=\"/assets/context/guidelines.md\" target=\"_blank\">guidelines.md</a>  | <a href=\"https://docs.windsurf.com/windsurf/cascade/memories#rules\" target=\"_blank\">Configure `guidelines.md`</a> |\n \n+## Angular CLI MCP Server setup\n+The Angular CLI includes a Model Context Protocol (MCP) server. This allows AI assistants in your IDE to interact with the Angular CLI, enabling them to generate code, add packages, and more.",
        "comment_created_at": "2025-07-22T15:34:45+00:00",
        "comment_author": "MarkTechson",
        "comment_body": "```suggestion\r\nThe Angular CLI includes an experimental [Model Context Protocol (MCP) server](https://modelcontextprotocol.io/) enabling AI assistants in your development environment to interact with the Angular CLI. We've included support for CLI powered code generation, adding packages, and more.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2283313508",
    "pr_number": 63095,
    "pr_file": "adev/src/content/ai/design-patterns.md",
    "created_at": "2025-08-18T19:54:04+00:00",
    "commented_code": "+# Design patterns for AI SDKs and signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular [signals](guide/signals) and the [`resource`](guide/signals/resource) API provide powerful tools to solve these problems elegantly.\n+\n+## Triggering requests with signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type\n+2. When the user submits (e.g., by clicking a button), update a second signal with contents of the first signal.\n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can use additional signal parameters, like a `sessionId` or `userId` (which can be useful for creating persistent LLM sessions), in the `loader` field. This way, the request always uses these parameters' current values without re-triggering the asyncronous function defined in the `loader` field.\n+\n+Many AI SDKs provide helper methods for making API calls. For example, the Genkit client library exposes a `runFlow` method for calling Genkit flows, which you can\n+call from a resource's `loader`. For other APIs, you can use the [`httpResource`](guide/signals/resource#reactive-data-fetching-with-httpresource).\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+## Preparing LLM data for templates\n+\n+LLM APIs can be configured to return structured data. Strongly typing your `resource` to match the expected output from the LLM provides better type safety and editor autocompletion.",
    "repo_full_name": "angular/angular",
    "discussion_comments": [
      {
        "comment_id": "2283313508",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/design-patterns.md",
        "discussion_id": "2283313508",
        "commented_code": "@@ -0,0 +1,173 @@\n+# Design patterns for AI SDKs and signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular [signals](guide/signals) and the [`resource`](guide/signals/resource) API provide powerful tools to solve these problems elegantly.\n+\n+## Triggering requests with signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type\n+2. When the user submits (e.g., by clicking a button), update a second signal with contents of the first signal.\n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can use additional signal parameters, like a `sessionId` or `userId` (which can be useful for creating persistent LLM sessions), in the `loader` field. This way, the request always uses these parameters' current values without re-triggering the asyncronous function defined in the `loader` field.\n+\n+Many AI SDKs provide helper methods for making API calls. For example, the Genkit client library exposes a `runFlow` method for calling Genkit flows, which you can\n+call from a resource's `loader`. For other APIs, you can use the [`httpResource`](guide/signals/resource#reactive-data-fetching-with-httpresource).\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+## Preparing LLM data for templates\n+\n+LLM APIs can be configured to return structured data. Strongly typing your `resource` to match the expected output from the LLM provides better type safety and editor autocompletion.",
        "comment_created_at": "2025-08-18T19:54:04+00:00",
        "comment_author": "jelbourn",
        "comment_body": "```suggestion\nYou can configure LLM APIs to return structured data. Strongly typing your `resource` to match the expected output from the LLM provides better type safety and editor autocompletion.\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2283329602",
    "pr_number": 63095,
    "pr_file": "adev/src/content/ai/design-patterns.md",
    "created_at": "2025-08-18T20:00:52+00:00",
    "commented_code": "+# Design patterns for AI SDKs and signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular [signals](guide/signals) and the [`resource`](guide/signals/resource) API provide powerful tools to solve these problems elegantly.\n+\n+## Triggering requests with signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type\n+2. When the user submits (e.g., by clicking a button), update a second signal with contents of the first signal.\n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can use additional signal parameters, like a `sessionId` or `userId` (which can be useful for creating persistent LLM sessions), in the `loader` field. This way, the request always uses these parameters' current values without re-triggering the asyncronous function defined in the `loader` field.\n+\n+Many AI SDKs provide helper methods for making API calls. For example, the Genkit client library exposes a `runFlow` method for calling Genkit flows, which you can\n+call from a resource's `loader`. For other APIs, you can use the [`httpResource`](guide/signals/resource#reactive-data-fetching-with-httpresource).\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+## Preparing LLM data for templates\n+\n+LLM APIs can be configured to return structured data. Strongly typing your `resource` to match the expected output from the LLM provides better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. Becuase `linkedSignal` [provides access to prior values](guide/signals/linked-signal), it can serve a variety of AI-related use cases, including\n+  * building a chat history\n+  * preserving or customizing data that templates display while LLMs generate content\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+## Performance and user experience\n+\n+LLM APIs may be slower and more error-prone than typical, more deterministic APIs. You can use several Angular features to build a performant and user-friendly interface.\n+\n+* **Scoped Loading:** place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** use the `resource` `LOADING` [status](guide/signals/resource#resource-status) to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** use the `resource` [**`reload()`**](guide/signals/resource#reloading) method as a simple way for users to retry failed requests, may be more prevalent when relying on AI generated content.\n+\n+The following example demonstrates how to create a responsive UI to dynamically display an AI generated image with loading and retry functionality.\n+\n+```html\n+<!-- Display a loading spinner while the LLM generates the image -->\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+<!-- Dynamically populates the src attribute with the generated image URL -->\n+} @else if (imgResource.hasValue()) {\n+    <img [src]=\"imgResource.value()\" />\n+<!-- Provides a retry option if the request fails  -->\n+} @else {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\" />\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+}\n+```\n+\n+\n+## AI patterns in action: streaming chat responses\n+Having text appear as the response is received from the model is a common UI pattern for web apps using AI. You can achieve this asynchronous task with Angular's `resource` API. The `stream` property of `resource` accepts an asynchronous function you can use to apply updates to a signal value over time. The signal being updated represents the data being streamed.",
    "repo_full_name": "angular/angular",
    "discussion_comments": [
      {
        "comment_id": "2283329602",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/design-patterns.md",
        "discussion_id": "2283329602",
        "commented_code": "@@ -0,0 +1,173 @@\n+# Design patterns for AI SDKs and signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular [signals](guide/signals) and the [`resource`](guide/signals/resource) API provide powerful tools to solve these problems elegantly.\n+\n+## Triggering requests with signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type\n+2. When the user submits (e.g., by clicking a button), update a second signal with contents of the first signal.\n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can use additional signal parameters, like a `sessionId` or `userId` (which can be useful for creating persistent LLM sessions), in the `loader` field. This way, the request always uses these parameters' current values without re-triggering the asyncronous function defined in the `loader` field.\n+\n+Many AI SDKs provide helper methods for making API calls. For example, the Genkit client library exposes a `runFlow` method for calling Genkit flows, which you can\n+call from a resource's `loader`. For other APIs, you can use the [`httpResource`](guide/signals/resource#reactive-data-fetching-with-httpresource).\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+## Preparing LLM data for templates\n+\n+LLM APIs can be configured to return structured data. Strongly typing your `resource` to match the expected output from the LLM provides better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. Becuase `linkedSignal` [provides access to prior values](guide/signals/linked-signal), it can serve a variety of AI-related use cases, including\n+  * building a chat history\n+  * preserving or customizing data that templates display while LLMs generate content\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+## Performance and user experience\n+\n+LLM APIs may be slower and more error-prone than typical, more deterministic APIs. You can use several Angular features to build a performant and user-friendly interface.\n+\n+* **Scoped Loading:** place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** use the `resource` `LOADING` [status](guide/signals/resource#resource-status) to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** use the `resource` [**`reload()`**](guide/signals/resource#reloading) method as a simple way for users to retry failed requests, may be more prevalent when relying on AI generated content.\n+\n+The following example demonstrates how to create a responsive UI to dynamically display an AI generated image with loading and retry functionality.\n+\n+```html\n+<!-- Display a loading spinner while the LLM generates the image -->\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+<!-- Dynamically populates the src attribute with the generated image URL -->\n+} @else if (imgResource.hasValue()) {\n+    <img [src]=\"imgResource.value()\" />\n+<!-- Provides a retry option if the request fails  -->\n+} @else {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\" />\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+}\n+```\n+\n+\n+## AI patterns in action: streaming chat responses\n+Having text appear as the response is received from the model is a common UI pattern for web apps using AI. You can achieve this asynchronous task with Angular's `resource` API. The `stream` property of `resource` accepts an asynchronous function you can use to apply updates to a signal value over time. The signal being updated represents the data being streamed.",
        "comment_created_at": "2025-08-18T20:00:52+00:00",
        "comment_author": "jelbourn",
        "comment_body": "I think the wording in the opening here is a bit awkward to parse. Maybe something like\n\n\"Interfaces often display partial results from LLM-based APIs incrementally as response data arrives. Angular's `resource` API supports patterns like this with its support for streaming responses.\"",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2286295059",
    "pr_number": 63095,
    "pr_file": "adev/src/content/ai/design-patterns.md",
    "created_at": "2025-08-19T20:45:42+00:00",
    "commented_code": "+# Design patterns for AI SDKs and signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular [signals](guide/signals) and the [`resource`](guide/signals/resource) API provide powerful tools to solve these problems elegantly.\n+\n+## Triggering requests with signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type\n+2. When the user submits (e.g., by clicking a button), update a second signal with contents of the first signal.\n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the resource's **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can use additional signal parameters, like a `sessionId` or `userId` (which can be useful for creating persistent LLM sessions), in the `loader` field. This way, the request always uses these parameters' current values without re-triggering the asyncronous function defined in the `loader` field.",
    "repo_full_name": "angular/angular",
    "discussion_comments": [
      {
        "comment_id": "2286295059",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/design-patterns.md",
        "discussion_id": "2286295059",
        "commented_code": "@@ -0,0 +1,172 @@\n+# Design patterns for AI SDKs and signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular [signals](guide/signals) and the [`resource`](guide/signals/resource) API provide powerful tools to solve these problems elegantly.\n+\n+## Triggering requests with signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type\n+2. When the user submits (e.g., by clicking a button), update a second signal with contents of the first signal.\n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the resource's **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can use additional signal parameters, like a `sessionId` or `userId` (which can be useful for creating persistent LLM sessions), in the `loader` field. This way, the request always uses these parameters' current values without re-triggering the asyncronous function defined in the `loader` field.",
        "comment_created_at": "2025-08-19T20:45:42+00:00",
        "comment_author": "JeanMeche",
        "comment_body": "```suggestion\r\nThis setup ensures the resource's **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can use additional signal parameters, like a `sessionId` or `userId` (which can be useful for creating persistent LLM sessions), in the `loader` field. This way, the request always uses these parameters' current values without re-triggering the asynchronous function defined in the `loader` field.\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2286305537",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/design-patterns.md",
        "discussion_id": "2286295059",
        "commented_code": "@@ -0,0 +1,172 @@\n+# Design patterns for AI SDKs and signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular [signals](guide/signals) and the [`resource`](guide/signals/resource) API provide powerful tools to solve these problems elegantly.\n+\n+## Triggering requests with signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type\n+2. When the user submits (e.g., by clicking a button), update a second signal with contents of the first signal.\n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the resource's **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can use additional signal parameters, like a `sessionId` or `userId` (which can be useful for creating persistent LLM sessions), in the `loader` field. This way, the request always uses these parameters' current values without re-triggering the asyncronous function defined in the `loader` field.",
        "comment_created_at": "2025-08-19T20:50:40+00:00",
        "comment_author": "MarkTechson",
        "comment_body": "good catch, a misspelling is a big enough deal to not merge.",
        "pr_file_module": null
      }
    ]
  }
]