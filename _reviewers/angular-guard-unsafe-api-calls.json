[
  {
    "discussion_id": "2267696499",
    "pr_number": 63095,
    "pr_file": "adev/src/content/ai/overview.md",
    "created_at": "2025-08-11T18:51:41+00:00",
    "commented_code": "* [AI Chatbot app template](https://github.com/FirebaseExtended/firebase-framework-tools/tree/main/starters/angular/ai-chatbot) - This template starts with a chatbot user interface that communicates with the Gemini API via HTTP. \n \n+## Design Patterns for AI SDKs and Signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular signals and the new `resource` API provide powerful tools to solve these problems elegantly.\n+\n+### Leverage the `resource` API\n+\n+The **`resource`** API is a container for data fetched asynchronously. It provides a streamlined, declarative interface for managing the various states of an async operation: `LOADING`, `LOADED`, and `ERROR`.\n+\n+#### Triggering Requests with Signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type.  \n+2. When the user submits the form (e.g., by clicking a button), update a second signal with the final prompt.  \n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can also pass other parameters, like a `sessionId` or `userId`, into the `loader` by reading their values from separate signals. This way, the request always uses the latest values for these parameters without re-triggering the `loader` if they change.\n+\n+Many AI SDKs provide helper methods for making API calls. The Genkit SDK, for example, exposes a `runFlow` method for making requests to a Genkit flow. For other APIs, you can use the `httpResource` or configure a standard `resource` to make a `POST` request.\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+#### Structuring and Storing Data\n+\n+LLM APIs often return structured data. You can strongly type your `resource` to match the expected output from the LLM, providing better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. A `linkedSignal` lets you compute a new state based on a source signal while also giving you access to the previous value of the `linkedSignal` itself to do things like appending new data to an existing list, which can be useful building a chat history, or preserving the current UI state while a new value is loading, preventing the display from clearing out during a refresh.\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+### Performance and User Experience\n+\n+LLM APIs can be slower and more error-prone than typical, more deterministic APIs. You can use several features of the `resource` API to build a robust and user-friendly interface.\n+\n+* **Scoped Loading:** Place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** Use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** Use the `resource` `LOADING` status to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** Since LLM requests can fail, provide a simple way for users to retry. The `resource` **`reload()`** method re-runs the `loader` function with the same configuration, making it easy to add a \"Retry\" button.\n+\n+The following example shows how to use the `resource` statuses to build a responsive UI for generating an image. In this scenario, the LLM server returns an empty string in the case of an error, in which case we present the user with a retry button. Here, we also demonstrate usage of the `resource.isLoading()` method to show a loading spinner to the user while the image is being generated, and how to dynamically update the image element's `src` attribute to dynamically load the image following a successful request.\n+\n+```html\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+} @else if (imgResource.value() === '') {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\"></mat-icon>\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+} @else {",
    "repo_full_name": "angular/angular",
    "discussion_comments": [
      {
        "comment_id": "2267696499",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/overview.md",
        "discussion_id": "2267696499",
        "commented_code": "@@ -60,6 +60,96 @@ The [Gemini API](https://ai.google.dev/gemini-api/docs) provides access to state\n \n * [AI Chatbot app template](https://github.com/FirebaseExtended/firebase-framework-tools/tree/main/starters/angular/ai-chatbot) - This template starts with a chatbot user interface that communicates with the Gemini API via HTTP. \n \n+## Design Patterns for AI SDKs and Signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular signals and the new `resource` API provide powerful tools to solve these problems elegantly.\n+\n+### Leverage the `resource` API\n+\n+The **`resource`** API is a container for data fetched asynchronously. It provides a streamlined, declarative interface for managing the various states of an async operation: `LOADING`, `LOADED`, and `ERROR`.\n+\n+#### Triggering Requests with Signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type.  \n+2. When the user submits the form (e.g., by clicking a button), update a second signal with the final prompt.  \n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can also pass other parameters, like a `sessionId` or `userId`, into the `loader` by reading their values from separate signals. This way, the request always uses the latest values for these parameters without re-triggering the `loader` if they change.\n+\n+Many AI SDKs provide helper methods for making API calls. The Genkit SDK, for example, exposes a `runFlow` method for making requests to a Genkit flow. For other APIs, you can use the `httpResource` or configure a standard `resource` to make a `POST` request.\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+#### Structuring and Storing Data\n+\n+LLM APIs often return structured data. You can strongly type your `resource` to match the expected output from the LLM, providing better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. A `linkedSignal` lets you compute a new state based on a source signal while also giving you access to the previous value of the `linkedSignal` itself to do things like appending new data to an existing list, which can be useful building a chat history, or preserving the current UI state while a new value is loading, preventing the display from clearing out during a refresh.\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+### Performance and User Experience\n+\n+LLM APIs can be slower and more error-prone than typical, more deterministic APIs. You can use several features of the `resource` API to build a robust and user-friendly interface.\n+\n+* **Scoped Loading:** Place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** Use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** Use the `resource` `LOADING` status to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** Since LLM requests can fail, provide a simple way for users to retry. The `resource` **`reload()`** method re-runs the `loader` function with the same configuration, making it easy to add a \"Retry\" button.\n+\n+The following example shows how to use the `resource` statuses to build a responsive UI for generating an image. In this scenario, the LLM server returns an empty string in the case of an error, in which case we present the user with a retry button. Here, we also demonstrate usage of the `resource.isLoading()` method to show a loading spinner to the user while the image is being generated, and how to dynamically update the image element's `src` attribute to dynamically load the image following a successful request.\n+\n+```html\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+} @else if (imgResource.value() === '') {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\"></mat-icon>\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+} @else {",
        "comment_created_at": "2025-08-11T18:51:41+00:00",
        "comment_author": "JeanMeche",
        "comment_body": "This lacks some error handling. `value()` should be guarded by `hasValue()` (else reading the value could throw). \r\n\r\n```suggestion\r\n} @else if (imgResource.value() === '') {\r\n    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\r\n        <mat-icon fontIcon=\"refresh\"></mat-icon>\r\n        <p>Failed to load image. Click to retry.</p>\r\n    </div>\r\n} @else {\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2270191615",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/overview.md",
        "discussion_id": "2267696499",
        "commented_code": "@@ -60,6 +60,96 @@ The [Gemini API](https://ai.google.dev/gemini-api/docs) provides access to state\n \n * [AI Chatbot app template](https://github.com/FirebaseExtended/firebase-framework-tools/tree/main/starters/angular/ai-chatbot) - This template starts with a chatbot user interface that communicates with the Gemini API via HTTP. \n \n+## Design Patterns for AI SDKs and Signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular signals and the new `resource` API provide powerful tools to solve these problems elegantly.\n+\n+### Leverage the `resource` API\n+\n+The **`resource`** API is a container for data fetched asynchronously. It provides a streamlined, declarative interface for managing the various states of an async operation: `LOADING`, `LOADED`, and `ERROR`.\n+\n+#### Triggering Requests with Signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type.  \n+2. When the user submits the form (e.g., by clicking a button), update a second signal with the final prompt.  \n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can also pass other parameters, like a `sessionId` or `userId`, into the `loader` by reading their values from separate signals. This way, the request always uses the latest values for these parameters without re-triggering the `loader` if they change.\n+\n+Many AI SDKs provide helper methods for making API calls. The Genkit SDK, for example, exposes a `runFlow` method for making requests to a Genkit flow. For other APIs, you can use the `httpResource` or configure a standard `resource` to make a `POST` request.\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+#### Structuring and Storing Data\n+\n+LLM APIs often return structured data. You can strongly type your `resource` to match the expected output from the LLM, providing better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. A `linkedSignal` lets you compute a new state based on a source signal while also giving you access to the previous value of the `linkedSignal` itself to do things like appending new data to an existing list, which can be useful building a chat history, or preserving the current UI state while a new value is loading, preventing the display from clearing out during a refresh.\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+### Performance and User Experience\n+\n+LLM APIs can be slower and more error-prone than typical, more deterministic APIs. You can use several features of the `resource` API to build a robust and user-friendly interface.\n+\n+* **Scoped Loading:** Place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** Use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** Use the `resource` `LOADING` status to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** Since LLM requests can fail, provide a simple way for users to retry. The `resource` **`reload()`** method re-runs the `loader` function with the same configuration, making it easy to add a \"Retry\" button.\n+\n+The following example shows how to use the `resource` statuses to build a responsive UI for generating an image. In this scenario, the LLM server returns an empty string in the case of an error, in which case we present the user with a retry button. Here, we also demonstrate usage of the `resource.isLoading()` method to show a loading spinner to the user while the image is being generated, and how to dynamically update the image element's `src` attribute to dynamically load the image following a successful request.\n+\n+```html\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+} @else if (imgResource.value() === '') {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\"></mat-icon>\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+} @else {",
        "comment_created_at": "2025-08-12T15:02:45+00:00",
        "comment_author": "devchas",
        "comment_body": "FYI @JeanMeche I've reverted back to the original pattern but replaced the empty string with a reference to the resource default value. This wasn't originally clear because I didn't show the `imgResource` but with the provided default value, `imgResource` always has a value.",
        "pr_file_module": null
      },
      {
        "comment_id": "2271730548",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/overview.md",
        "discussion_id": "2267696499",
        "commented_code": "@@ -60,6 +60,96 @@ The [Gemini API](https://ai.google.dev/gemini-api/docs) provides access to state\n \n * [AI Chatbot app template](https://github.com/FirebaseExtended/firebase-framework-tools/tree/main/starters/angular/ai-chatbot) - This template starts with a chatbot user interface that communicates with the Gemini API via HTTP. \n \n+## Design Patterns for AI SDKs and Signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular signals and the new `resource` API provide powerful tools to solve these problems elegantly.\n+\n+### Leverage the `resource` API\n+\n+The **`resource`** API is a container for data fetched asynchronously. It provides a streamlined, declarative interface for managing the various states of an async operation: `LOADING`, `LOADED`, and `ERROR`.\n+\n+#### Triggering Requests with Signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type.  \n+2. When the user submits the form (e.g., by clicking a button), update a second signal with the final prompt.  \n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can also pass other parameters, like a `sessionId` or `userId`, into the `loader` by reading their values from separate signals. This way, the request always uses the latest values for these parameters without re-triggering the `loader` if they change.\n+\n+Many AI SDKs provide helper methods for making API calls. The Genkit SDK, for example, exposes a `runFlow` method for making requests to a Genkit flow. For other APIs, you can use the `httpResource` or configure a standard `resource` to make a `POST` request.\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+#### Structuring and Storing Data\n+\n+LLM APIs often return structured data. You can strongly type your `resource` to match the expected output from the LLM, providing better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. A `linkedSignal` lets you compute a new state based on a source signal while also giving you access to the previous value of the `linkedSignal` itself to do things like appending new data to an existing list, which can be useful building a chat history, or preserving the current UI state while a new value is loading, preventing the display from clearing out during a refresh.\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+### Performance and User Experience\n+\n+LLM APIs can be slower and more error-prone than typical, more deterministic APIs. You can use several features of the `resource` API to build a robust and user-friendly interface.\n+\n+* **Scoped Loading:** Place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** Use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** Use the `resource` `LOADING` status to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** Since LLM requests can fail, provide a simple way for users to retry. The `resource` **`reload()`** method re-runs the `loader` function with the same configuration, making it easy to add a \"Retry\" button.\n+\n+The following example shows how to use the `resource` statuses to build a responsive UI for generating an image. In this scenario, the LLM server returns an empty string in the case of an error, in which case we present the user with a retry button. Here, we also demonstrate usage of the `resource.isLoading()` method to show a loading spinner to the user while the image is being generated, and how to dynamically update the image element's `src` attribute to dynamically load the image following a successful request.\n+\n+```html\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+} @else if (imgResource.value() === '') {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\"></mat-icon>\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+} @else {",
        "comment_created_at": "2025-08-13T00:24:33+00:00",
        "comment_author": "JeanMeche",
        "comment_body": "I probably wasn't clear enough here, `value()` is inherently unsafe to call without proper guarding. When a `resource` is in an error state, invoking `value()` will throw. In such cases, `hasValue()` correctly returns `false`, ensuring it serves as the appropriate guard before attempting a `value()` read.",
        "pr_file_module": null
      },
      {
        "comment_id": "2273906561",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/overview.md",
        "discussion_id": "2267696499",
        "commented_code": "@@ -60,6 +60,96 @@ The [Gemini API](https://ai.google.dev/gemini-api/docs) provides access to state\n \n * [AI Chatbot app template](https://github.com/FirebaseExtended/firebase-framework-tools/tree/main/starters/angular/ai-chatbot) - This template starts with a chatbot user interface that communicates with the Gemini API via HTTP. \n \n+## Design Patterns for AI SDKs and Signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular signals and the new `resource` API provide powerful tools to solve these problems elegantly.\n+\n+### Leverage the `resource` API\n+\n+The **`resource`** API is a container for data fetched asynchronously. It provides a streamlined, declarative interface for managing the various states of an async operation: `LOADING`, `LOADED`, and `ERROR`.\n+\n+#### Triggering Requests with Signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type.  \n+2. When the user submits the form (e.g., by clicking a button), update a second signal with the final prompt.  \n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can also pass other parameters, like a `sessionId` or `userId`, into the `loader` by reading their values from separate signals. This way, the request always uses the latest values for these parameters without re-triggering the `loader` if they change.\n+\n+Many AI SDKs provide helper methods for making API calls. The Genkit SDK, for example, exposes a `runFlow` method for making requests to a Genkit flow. For other APIs, you can use the `httpResource` or configure a standard `resource` to make a `POST` request.\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+#### Structuring and Storing Data\n+\n+LLM APIs often return structured data. You can strongly type your `resource` to match the expected output from the LLM, providing better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. A `linkedSignal` lets you compute a new state based on a source signal while also giving you access to the previous value of the `linkedSignal` itself to do things like appending new data to an existing list, which can be useful building a chat history, or preserving the current UI state while a new value is loading, preventing the display from clearing out during a refresh.\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+### Performance and User Experience\n+\n+LLM APIs can be slower and more error-prone than typical, more deterministic APIs. You can use several features of the `resource` API to build a robust and user-friendly interface.\n+\n+* **Scoped Loading:** Place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** Use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** Use the `resource` `LOADING` status to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** Since LLM requests can fail, provide a simple way for users to retry. The `resource` **`reload()`** method re-runs the `loader` function with the same configuration, making it easy to add a \"Retry\" button.\n+\n+The following example shows how to use the `resource` statuses to build a responsive UI for generating an image. In this scenario, the LLM server returns an empty string in the case of an error, in which case we present the user with a retry button. Here, we also demonstrate usage of the `resource.isLoading()` method to show a loading spinner to the user while the image is being generated, and how to dynamically update the image element's `src` attribute to dynamically load the image following a successful request.\n+\n+```html\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+} @else if (imgResource.value() === '') {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\"></mat-icon>\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+} @else {",
        "comment_created_at": "2025-08-13T15:58:40+00:00",
        "comment_author": "devchas",
        "comment_body": "Ah, thanks @JeanMeche - I was testing on v19 and forgot we made this change. I don't love how this requires me to add a catch-all condition at the end. I'd love to do something like\r\n\r\n```\r\n<!-- Display a loading spinner while the LLM generates the image -->\r\n@if (imgResource.isLoading()) {\r\n    <div class=\"img-placeholder\">\r\n        <mat-spinner [diameter]=\"50\" />\r\n    </div>\r\n    <!-- Dynamically populates the src attribute with the generated image URL -->\r\n} @else if (imgResource.hasValue()) {\r\n    <img [src]=\"imgResource.value()\" />\r\n<!-- Provides a retry option if the request fails  -->\r\n} @else {\r\n    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\r\n        <mat-icon fontIcon=\"refresh\" />\r\n        <p>Failed to load image. Click to retry.</p>\r\n    </div>\r\n}\r\n```\r\n\r\nbut you can't call `reload` in this context: `Property 'reload' does not exist on type 'never'.`\r\n\r\nOddly enough, the following appears to be valid code. It seems like one of these two scenarios is not the intended behavior (I'm guessing this one), but I'm not sure.\r\n\r\n```\r\n@let showImg = imgResource.hasValue();\r\n\r\n<!-- Display a loading spinner while the LLM generates the image -->\r\n@if (imgResource.isLoading()) {\r\n    <div class=\"img-placeholder\">\r\n        <mat-spinner [diameter]=\"50\" />\r\n    </div>\r\n    <!-- Dynamically populates the src attribute with the generated image URL -->\r\n} @else if (showImg) {\r\n    <img [src]=\"imgResource.value()\" />\r\n<!-- Provides a retry option if the request fails  -->\r\n} @else {\r\n    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\r\n        <mat-icon fontIcon=\"refresh\" />\r\n        <p>Failed to load image. Click to retry.</p>\r\n    </div>\r\n}\r\n``` ",
        "pr_file_module": null
      },
      {
        "comment_id": "2277817267",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/overview.md",
        "discussion_id": "2267696499",
        "commented_code": "@@ -60,6 +60,96 @@ The [Gemini API](https://ai.google.dev/gemini-api/docs) provides access to state\n \n * [AI Chatbot app template](https://github.com/FirebaseExtended/firebase-framework-tools/tree/main/starters/angular/ai-chatbot) - This template starts with a chatbot user interface that communicates with the Gemini API via HTTP. \n \n+## Design Patterns for AI SDKs and Signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular signals and the new `resource` API provide powerful tools to solve these problems elegantly.\n+\n+### Leverage the `resource` API\n+\n+The **`resource`** API is a container for data fetched asynchronously. It provides a streamlined, declarative interface for managing the various states of an async operation: `LOADING`, `LOADED`, and `ERROR`.\n+\n+#### Triggering Requests with Signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type.  \n+2. When the user submits the form (e.g., by clicking a button), update a second signal with the final prompt.  \n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can also pass other parameters, like a `sessionId` or `userId`, into the `loader` by reading their values from separate signals. This way, the request always uses the latest values for these parameters without re-triggering the `loader` if they change.\n+\n+Many AI SDKs provide helper methods for making API calls. The Genkit SDK, for example, exposes a `runFlow` method for making requests to a Genkit flow. For other APIs, you can use the `httpResource` or configure a standard `resource` to make a `POST` request.\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+#### Structuring and Storing Data\n+\n+LLM APIs often return structured data. You can strongly type your `resource` to match the expected output from the LLM, providing better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. A `linkedSignal` lets you compute a new state based on a source signal while also giving you access to the previous value of the `linkedSignal` itself to do things like appending new data to an existing list, which can be useful building a chat history, or preserving the current UI state while a new value is loading, preventing the display from clearing out during a refresh.\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+### Performance and User Experience\n+\n+LLM APIs can be slower and more error-prone than typical, more deterministic APIs. You can use several features of the `resource` API to build a robust and user-friendly interface.\n+\n+* **Scoped Loading:** Place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** Use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** Use the `resource` `LOADING` status to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** Since LLM requests can fail, provide a simple way for users to retry. The `resource` **`reload()`** method re-runs the `loader` function with the same configuration, making it easy to add a \"Retry\" button.\n+\n+The following example shows how to use the `resource` statuses to build a responsive UI for generating an image. In this scenario, the LLM server returns an empty string in the case of an error, in which case we present the user with a retry button. Here, we also demonstrate usage of the `resource.isLoading()` method to show a loading spinner to the user while the image is being generated, and how to dynamically update the image element's `src` attribute to dynamically load the image following a successful request.\n+\n+```html\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+} @else if (imgResource.value() === '') {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\"></mat-icon>\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+} @else {",
        "comment_created_at": "2025-08-14T22:14:22+00:00",
        "comment_author": "JeanMeche",
        "comment_body": "The first scenario should work. Could you try to reproduce the issue on a stackblitz ? (I wasn't able to reproduce your issue). ",
        "pr_file_module": null
      },
      {
        "comment_id": "2279035006",
        "repo_full_name": "angular/angular",
        "pr_number": 63095,
        "pr_file": "adev/src/content/ai/overview.md",
        "discussion_id": "2267696499",
        "commented_code": "@@ -60,6 +60,96 @@ The [Gemini API](https://ai.google.dev/gemini-api/docs) provides access to state\n \n * [AI Chatbot app template](https://github.com/FirebaseExtended/firebase-framework-tools/tree/main/starters/angular/ai-chatbot) - This template starts with a chatbot user interface that communicates with the Gemini API via HTTP. \n \n+## Design Patterns for AI SDKs and Signal APIs\n+\n+Interacting with AI and Large Language Model (LLM) APIs introduces unique challenges, such as managing asynchronous operations, handling streaming data, and designing a responsive user experience for potentially slow or unreliable network requests. Angular signals and the new `resource` API provide powerful tools to solve these problems elegantly.\n+\n+### Leverage the `resource` API\n+\n+The **`resource`** API is a container for data fetched asynchronously. It provides a streamlined, declarative interface for managing the various states of an async operation: `LOADING`, `LOADED`, and `ERROR`.\n+\n+#### Triggering Requests with Signals\n+\n+A common pattern when working with user-provided prompts is to separate the user's live input from the submitted value that triggers the API call.\n+\n+1. Store the user's raw input in one signal as they type.  \n+2. When the user submits the form (e.g., by clicking a button), update a second signal with the final prompt.  \n+3. Use the second signal in the **`params`** field of your `resource`.\n+\n+This setup ensures the `resource`'s **`loader`** function only runs when the user explicitly submits their prompt, not on every keystroke. You can also pass other parameters, like a `sessionId` or `userId`, into the `loader` by reading their values from separate signals. This way, the request always uses the latest values for these parameters without re-triggering the `loader` if they change.\n+\n+Many AI SDKs provide helper methods for making API calls. The Genkit SDK, for example, exposes a `runFlow` method for making requests to a Genkit flow. For other APIs, you can use the `httpResource` or configure a standard `resource` to make a `POST` request.\n+\n+The following example shows a `resource` that fetches parts of an AI-generated story. The `loader` is triggered only when the `storyInput` signal changes.\n+\n+```ts\n+// A resource that fetches three parts of an AI generated story\n+storyResource = resource({\n+  // The default value to use before the first request or on error\n+  defaultValue: DEFAULT_STORY,\n+  // The loader is re-triggered when this signal changes\n+  params: () => this.storyInput(),\n+  // The async function to fetch data\n+  loader: ({params}): Promise<StoryData> => {\n+    // The params value is the current value of the storyInput signal\n+    const url = this.endpoint();\n+    return runFlow({ url, input: {\n+      userInput: params,\n+      sessionId: this.storyService.sessionId() // Read from another signal\n+    }});\n+  }\n+});\n+```\n+\n+#### Structuring and Storing Data\n+\n+LLM APIs often return structured data. You can strongly type your `resource` to match the expected output from the LLM, providing better type safety and editor autocompletion.\n+\n+To manage state derived from a resource, you can use a `computed` signal or `linkedSignal`. A `linkedSignal` lets you compute a new state based on a source signal while also giving you access to the previous value of the `linkedSignal` itself to do things like appending new data to an existing list, which can be useful building a chat history, or preserving the current UI state while a new value is loading, preventing the display from clearing out during a refresh.\n+\n+In the example below, `storyParts` is a `linkedSignal` that appends the latest story parts returned from `storyResource` to the existing array of story parts.\n+\n+```ts\n+storyParts = linkedSignal<string[], string[]>({\n+  // The source signal that triggers the computation\n+  source: () => this.storyResource.value().storyParts,\n+  // The computation function\n+  computation: (newStoryParts, previous) => {\n+    // Get the previous value of this linkedSignal, or an empty array\n+    const existingStoryParts = previous?.value || [];\n+    // Return a new array with the old and new parts\n+    return [...existingStoryParts, ...newStoryParts];\n+  }\n+});\n+```\n+\n+### Performance and User Experience\n+\n+LLM APIs can be slower and more error-prone than typical, more deterministic APIs. You can use several features of the `resource` API to build a robust and user-friendly interface.\n+\n+* **Scoped Loading:** Place the `resource` in the component that directly uses the data. This helps limit change detection cycles (especially in zoneless applications) and prevents blocking other parts of your application. If data needs to be shared across multiple components, provide the `resource` from a service.  \n+* **SSR and Hydration:** Use Server-Side Rendering (SSR) with incremental hydration to render the initial page content quickly. You can show a placeholder for the AI-generated content and defer fetching the data until the component hydrates on the client.  \n+* **Loading State:** Use the `resource` `LOADING` status to show an indicator, like a spinner, while the request is in flight. This status covers both initial loads and reloads.  \n+* **Error Handling and Retries:** Since LLM requests can fail, provide a simple way for users to retry. The `resource` **`reload()`** method re-runs the `loader` function with the same configuration, making it easy to add a \"Retry\" button.\n+\n+The following example shows how to use the `resource` statuses to build a responsive UI for generating an image. In this scenario, the LLM server returns an empty string in the case of an error, in which case we present the user with a retry button. Here, we also demonstrate usage of the `resource.isLoading()` method to show a loading spinner to the user while the image is being generated, and how to dynamically update the image element's `src` attribute to dynamically load the image following a successful request.\n+\n+```html\n+@if (imgResource.isLoading()) {\n+    <div class=\"img-placeholder\">\n+        <mat-spinner [diameter]=\"50\" />\n+    </div>\n+} @else if (imgResource.value() === '') {\n+    <div class=\"img-placeholder\" (click)=\"imgResource.reload()\">\n+        <mat-icon fontIcon=\"refresh\"></mat-icon>\n+        <p>Failed to load image. Click to retry.</p>\n+    </div>\n+} @else {",
        "comment_created_at": "2025-08-15T13:53:27+00:00",
        "comment_author": "devchas",
        "comment_body": "It is valid, and I figured out why it wasn't working. The genkit `runFlow` method has an `any` return type. In that case, you have to add a return type to the async loader function, or you'll get that error. I've updated the example.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2198026466",
    "pr_number": 62250,
    "pr_file": "adev/src/content/guide/routing/data-resolvers.md",
    "created_at": "2025-07-10T15:13:49+00:00",
    "commented_code": "+# Data resolvers\n+\n+Data resolvers allow you to fetch data before navigating to a route, ensuring that your components receive the data they need before rendering. This can help prevent the need for loading states and improve the user experience by pre-loading essential data.\n+\n+## What are data resolvers?\n+\n+A data resolver is a service that implements the [`ResolveFn`](api/router/ResolveFn) function. It runs before a route activates and can fetch data from APIs, databases, or other sources. The resolved data becomes available to the component through the [`ActivatedRoute`](api/router/ActivatedRoute).\n+\n+## Why use data resolvers?\n+\n+Data resolvers solve common routing challenges:\n+\n+- **Prevent empty states**: Components receive data immediately upon loading\n+- **Better user experience**: No loading spinners for critical data\n+- **Error handling**: Handle data fetching errors before navigation\n+- **Data consistency**: Ensure required data is available before rendering which is important for SSR\n+\n+## Creating a resolver\n+\n+You create a resolver by writing a function with the [`ResolveFn`](api/router/ResolveFn) type.\n+\n+It receives the [`ActivatedRouteSnapshot`](api/router/ActivatedRouteSnapshot) and [`RouterStateSnapshot`](api/router/RouterStateSnapshot) as parameters.\n+\n+Here is a resolver that gets the user information before rendering a route using the [`inject`](api/core/inject) function:\n+\n+```ts\n+import { inject } from '@angular/core';\n+import { UserStore, SettingsStore } from './user-store';\n+import type { ActivatedRouteSnapshot, ResolveFn, RouterStateSnapshot } from '@angular/router';\n+import type { User, Settings } from './types';\n+\n+export const userResolver: ResolveFn<User> = (route: ActivatedRouteSnapshot, state: RouterStateSnapshot) => {\n+  const userStore = inject(UserStore);\n+  const userId = route.paramMap.get('id')!;\n+  return userStore.getUser(userId);\n+};\n+\n+export const settingsResolver: ResolveFn<Settings> = (route: ActivatedRouteSnapshot, state: RouterStateSnapshot) => {\n+  const settingsStore = inject(SettingsStore);\n+  const userId = route.paramMap.get('id')!;\n+  return settingsStore.getUserSettings(userId);\n+};\n+```\n+\n+## Configuring routes with resolvers\n+\n+When you want to add one or more data resolvers to a route, you can add it under the `resolve` key in the route configuration. The [`Routes`](api/router/Routes) type defines the structure for route configurations:\n+\n+```ts\n+import { Routes } from '@angular/router';\n+\n+export const routes: Routes = [\n+  {\n+    path: 'user/:id',\n+    component: UserDetail,\n+    resolve: {\n+      user: userResolver,\n+      settings: settingsResolver\n+    }\n+  }\n+];\n+```\n+\n+You can learn more about the [`resolve` configuration in the API docs](api/router/Route#resolve).\n+\n+## Accessing resolved data in components\n+\n+### Using ActivatedRoute\n+\n+You can access the resolved data in a component by accessing the snapshot data from the [`ActivatedRoute`](api/router/ActivatedRoute) using the [`signal`](api/core/signal) function:\n+\n+```angular-ts\n+import { Component, inject, computed } from '@angular/core';\n+import { ActivatedRoute } from '@angular/router';\n+import { toSignal } from '@angular/core/rxjs-interop';\n+import type { User, Settings } from './types';\n+\n+@Component({\n+  template: `\n+    <h1>{{ user().name }}</h1>\n+    <p>{{ user().email }}</p>\n+    <div>Theme: {{ settings().theme }}</div>\n+  `\n+})\n+export class UserDetail {\n+  private route = inject(ActivatedRoute);\n+  private data = toSignal(this.route.data);\n+  user = computed(() => this.data().user as User);\n+  settings = computed(() => this.data().settings as Settings);\n+}\n+```\n+\n+### Using withComponentInputBinding\n+\n+A different approach to accessing the resolved data is to use [`withComponentInputBinding()`](api/router/withComponentInputBinding) when configuring your router with [`provideRouter`](api/router/provideRouter). This allows resolved data to be passed directly as component inputs:\n+\n+```ts\n+import { bootstrapApplication } from '@angular/platform-browser';\n+import { provideRouter, withComponentInputBinding } from '@angular/router';\n+import { routes } from './app.routes';\n+\n+bootstrapApplication(App, {\n+  providers: [\n+    provideRouter(routes, withComponentInputBinding())\n+  ]\n+});\n+```\n+\n+With this configuration, you can define inputs in your component that match the resolver keys using the [`input`](api/core/input) function and [`input.required`](api/core/input#required) for required inputs:\n+\n+```angular-ts\n+import { Component, input } from '@angular/core';\n+import type { User, Settings } from './types';\n+\n+@Component({\n+  template: `\n+    <h1>{{ user().name }}</h1>\n+    <p>{{ user().email }}</p>\n+    <div>Theme: {{ settings().theme }}</div>\n+  `\n+})\n+export class UserDetail {\n+  user = input.required<User>();\n+  settings = input.required<Settings>();\n+}\n+```\n+\n+This approach provides better type safety and eliminates the need to inject `ActivatedRoute` just to access resolved data.\n+\n+## Error handling in resolvers\n+\n+In the event of navigation failures, it is important to handle errors gracefully in your data resolvers.\n+\n+Here's an updated example of the `userResolver` that logs the error and navigates back to the generic `/users` page using the [`Router`](api/router/Router) service:\n+\n+```ts\n+import { inject } from '@angular/core';\n+import { ResolveFn, RedirectCommand, Router } from '@angular/router';\n+import { catchError, EMPTY } from 'rxjs';\n+import { UserStore } from './user-store';\n+import type { User } from './types';\n+\n+export const userResolver: ResolveFn<User | RedirectCommand> = (route) => {\n+  const userStore = inject(UserStore);\n+  const router = inject(Router);\n+  const userId = route.paramMap.get('id')!;\n+\n+  return userService.getUser(userId).pipe(\n+    catchError(error => {\n+      console.error('Failed to load user:', error);\n+      return [new RedirectCommand(router.parseUrl('/users'))];",
    "repo_full_name": "angular/angular",
    "discussion_comments": [
      {
        "comment_id": "2198026466",
        "repo_full_name": "angular/angular",
        "pr_number": 62250,
        "pr_file": "adev/src/content/guide/routing/data-resolvers.md",
        "discussion_id": "2198026466",
        "commented_code": "@@ -0,0 +1,197 @@\n+# Data resolvers\n+\n+Data resolvers allow you to fetch data before navigating to a route, ensuring that your components receive the data they need before rendering. This can help prevent the need for loading states and improve the user experience by pre-loading essential data.\n+\n+## What are data resolvers?\n+\n+A data resolver is a service that implements the [`ResolveFn`](api/router/ResolveFn) function. It runs before a route activates and can fetch data from APIs, databases, or other sources. The resolved data becomes available to the component through the [`ActivatedRoute`](api/router/ActivatedRoute).\n+\n+## Why use data resolvers?\n+\n+Data resolvers solve common routing challenges:\n+\n+- **Prevent empty states**: Components receive data immediately upon loading\n+- **Better user experience**: No loading spinners for critical data\n+- **Error handling**: Handle data fetching errors before navigation\n+- **Data consistency**: Ensure required data is available before rendering which is important for SSR\n+\n+## Creating a resolver\n+\n+You create a resolver by writing a function with the [`ResolveFn`](api/router/ResolveFn) type.\n+\n+It receives the [`ActivatedRouteSnapshot`](api/router/ActivatedRouteSnapshot) and [`RouterStateSnapshot`](api/router/RouterStateSnapshot) as parameters.\n+\n+Here is a resolver that gets the user information before rendering a route using the [`inject`](api/core/inject) function:\n+\n+```ts\n+import { inject } from '@angular/core';\n+import { UserStore, SettingsStore } from './user-store';\n+import type { ActivatedRouteSnapshot, ResolveFn, RouterStateSnapshot } from '@angular/router';\n+import type { User, Settings } from './types';\n+\n+export const userResolver: ResolveFn<User> = (route: ActivatedRouteSnapshot, state: RouterStateSnapshot) => {\n+  const userStore = inject(UserStore);\n+  const userId = route.paramMap.get('id')!;\n+  return userStore.getUser(userId);\n+};\n+\n+export const settingsResolver: ResolveFn<Settings> = (route: ActivatedRouteSnapshot, state: RouterStateSnapshot) => {\n+  const settingsStore = inject(SettingsStore);\n+  const userId = route.paramMap.get('id')!;\n+  return settingsStore.getUserSettings(userId);\n+};\n+```\n+\n+## Configuring routes with resolvers\n+\n+When you want to add one or more data resolvers to a route, you can add it under the `resolve` key in the route configuration. The [`Routes`](api/router/Routes) type defines the structure for route configurations:\n+\n+```ts\n+import { Routes } from '@angular/router';\n+\n+export const routes: Routes = [\n+  {\n+    path: 'user/:id',\n+    component: UserDetail,\n+    resolve: {\n+      user: userResolver,\n+      settings: settingsResolver\n+    }\n+  }\n+];\n+```\n+\n+You can learn more about the [`resolve` configuration in the API docs](api/router/Route#resolve).\n+\n+## Accessing resolved data in components\n+\n+### Using ActivatedRoute\n+\n+You can access the resolved data in a component by accessing the snapshot data from the [`ActivatedRoute`](api/router/ActivatedRoute) using the [`signal`](api/core/signal) function:\n+\n+```angular-ts\n+import { Component, inject, computed } from '@angular/core';\n+import { ActivatedRoute } from '@angular/router';\n+import { toSignal } from '@angular/core/rxjs-interop';\n+import type { User, Settings } from './types';\n+\n+@Component({\n+  template: `\n+    <h1>{{ user().name }}</h1>\n+    <p>{{ user().email }}</p>\n+    <div>Theme: {{ settings().theme }}</div>\n+  `\n+})\n+export class UserDetail {\n+  private route = inject(ActivatedRoute);\n+  private data = toSignal(this.route.data);\n+  user = computed(() => this.data().user as User);\n+  settings = computed(() => this.data().settings as Settings);\n+}\n+```\n+\n+### Using withComponentInputBinding\n+\n+A different approach to accessing the resolved data is to use [`withComponentInputBinding()`](api/router/withComponentInputBinding) when configuring your router with [`provideRouter`](api/router/provideRouter). This allows resolved data to be passed directly as component inputs:\n+\n+```ts\n+import { bootstrapApplication } from '@angular/platform-browser';\n+import { provideRouter, withComponentInputBinding } from '@angular/router';\n+import { routes } from './app.routes';\n+\n+bootstrapApplication(App, {\n+  providers: [\n+    provideRouter(routes, withComponentInputBinding())\n+  ]\n+});\n+```\n+\n+With this configuration, you can define inputs in your component that match the resolver keys using the [`input`](api/core/input) function and [`input.required`](api/core/input#required) for required inputs:\n+\n+```angular-ts\n+import { Component, input } from '@angular/core';\n+import type { User, Settings } from './types';\n+\n+@Component({\n+  template: `\n+    <h1>{{ user().name }}</h1>\n+    <p>{{ user().email }}</p>\n+    <div>Theme: {{ settings().theme }}</div>\n+  `\n+})\n+export class UserDetail {\n+  user = input.required<User>();\n+  settings = input.required<Settings>();\n+}\n+```\n+\n+This approach provides better type safety and eliminates the need to inject `ActivatedRoute` just to access resolved data.\n+\n+## Error handling in resolvers\n+\n+In the event of navigation failures, it is important to handle errors gracefully in your data resolvers.\n+\n+Here's an updated example of the `userResolver` that logs the error and navigates back to the generic `/users` page using the [`Router`](api/router/Router) service:\n+\n+```ts\n+import { inject } from '@angular/core';\n+import { ResolveFn, RedirectCommand, Router } from '@angular/router';\n+import { catchError, EMPTY } from 'rxjs';\n+import { UserStore } from './user-store';\n+import type { User } from './types';\n+\n+export const userResolver: ResolveFn<User | RedirectCommand> = (route) => {\n+  const userStore = inject(UserStore);\n+  const router = inject(Router);\n+  const userId = route.paramMap.get('id')!;\n+\n+  return userService.getUser(userId).pipe(\n+    catchError(error => {\n+      console.error('Failed to load user:', error);\n+      return [new RedirectCommand(router.parseUrl('/users'))];",
        "comment_created_at": "2025-07-10T15:13:49+00:00",
        "comment_author": "atscott",
        "comment_body": "```suggestion\r\n      return new RedirectCommand(router.parseUrl('/users'));\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2198049261",
        "repo_full_name": "angular/angular",
        "pr_number": 62250,
        "pr_file": "adev/src/content/guide/routing/data-resolvers.md",
        "discussion_id": "2198026466",
        "commented_code": "@@ -0,0 +1,197 @@\n+# Data resolvers\n+\n+Data resolvers allow you to fetch data before navigating to a route, ensuring that your components receive the data they need before rendering. This can help prevent the need for loading states and improve the user experience by pre-loading essential data.\n+\n+## What are data resolvers?\n+\n+A data resolver is a service that implements the [`ResolveFn`](api/router/ResolveFn) function. It runs before a route activates and can fetch data from APIs, databases, or other sources. The resolved data becomes available to the component through the [`ActivatedRoute`](api/router/ActivatedRoute).\n+\n+## Why use data resolvers?\n+\n+Data resolvers solve common routing challenges:\n+\n+- **Prevent empty states**: Components receive data immediately upon loading\n+- **Better user experience**: No loading spinners for critical data\n+- **Error handling**: Handle data fetching errors before navigation\n+- **Data consistency**: Ensure required data is available before rendering which is important for SSR\n+\n+## Creating a resolver\n+\n+You create a resolver by writing a function with the [`ResolveFn`](api/router/ResolveFn) type.\n+\n+It receives the [`ActivatedRouteSnapshot`](api/router/ActivatedRouteSnapshot) and [`RouterStateSnapshot`](api/router/RouterStateSnapshot) as parameters.\n+\n+Here is a resolver that gets the user information before rendering a route using the [`inject`](api/core/inject) function:\n+\n+```ts\n+import { inject } from '@angular/core';\n+import { UserStore, SettingsStore } from './user-store';\n+import type { ActivatedRouteSnapshot, ResolveFn, RouterStateSnapshot } from '@angular/router';\n+import type { User, Settings } from './types';\n+\n+export const userResolver: ResolveFn<User> = (route: ActivatedRouteSnapshot, state: RouterStateSnapshot) => {\n+  const userStore = inject(UserStore);\n+  const userId = route.paramMap.get('id')!;\n+  return userStore.getUser(userId);\n+};\n+\n+export const settingsResolver: ResolveFn<Settings> = (route: ActivatedRouteSnapshot, state: RouterStateSnapshot) => {\n+  const settingsStore = inject(SettingsStore);\n+  const userId = route.paramMap.get('id')!;\n+  return settingsStore.getUserSettings(userId);\n+};\n+```\n+\n+## Configuring routes with resolvers\n+\n+When you want to add one or more data resolvers to a route, you can add it under the `resolve` key in the route configuration. The [`Routes`](api/router/Routes) type defines the structure for route configurations:\n+\n+```ts\n+import { Routes } from '@angular/router';\n+\n+export const routes: Routes = [\n+  {\n+    path: 'user/:id',\n+    component: UserDetail,\n+    resolve: {\n+      user: userResolver,\n+      settings: settingsResolver\n+    }\n+  }\n+];\n+```\n+\n+You can learn more about the [`resolve` configuration in the API docs](api/router/Route#resolve).\n+\n+## Accessing resolved data in components\n+\n+### Using ActivatedRoute\n+\n+You can access the resolved data in a component by accessing the snapshot data from the [`ActivatedRoute`](api/router/ActivatedRoute) using the [`signal`](api/core/signal) function:\n+\n+```angular-ts\n+import { Component, inject, computed } from '@angular/core';\n+import { ActivatedRoute } from '@angular/router';\n+import { toSignal } from '@angular/core/rxjs-interop';\n+import type { User, Settings } from './types';\n+\n+@Component({\n+  template: `\n+    <h1>{{ user().name }}</h1>\n+    <p>{{ user().email }}</p>\n+    <div>Theme: {{ settings().theme }}</div>\n+  `\n+})\n+export class UserDetail {\n+  private route = inject(ActivatedRoute);\n+  private data = toSignal(this.route.data);\n+  user = computed(() => this.data().user as User);\n+  settings = computed(() => this.data().settings as Settings);\n+}\n+```\n+\n+### Using withComponentInputBinding\n+\n+A different approach to accessing the resolved data is to use [`withComponentInputBinding()`](api/router/withComponentInputBinding) when configuring your router with [`provideRouter`](api/router/provideRouter). This allows resolved data to be passed directly as component inputs:\n+\n+```ts\n+import { bootstrapApplication } from '@angular/platform-browser';\n+import { provideRouter, withComponentInputBinding } from '@angular/router';\n+import { routes } from './app.routes';\n+\n+bootstrapApplication(App, {\n+  providers: [\n+    provideRouter(routes, withComponentInputBinding())\n+  ]\n+});\n+```\n+\n+With this configuration, you can define inputs in your component that match the resolver keys using the [`input`](api/core/input) function and [`input.required`](api/core/input#required) for required inputs:\n+\n+```angular-ts\n+import { Component, input } from '@angular/core';\n+import type { User, Settings } from './types';\n+\n+@Component({\n+  template: `\n+    <h1>{{ user().name }}</h1>\n+    <p>{{ user().email }}</p>\n+    <div>Theme: {{ settings().theme }}</div>\n+  `\n+})\n+export class UserDetail {\n+  user = input.required<User>();\n+  settings = input.required<Settings>();\n+}\n+```\n+\n+This approach provides better type safety and eliminates the need to inject `ActivatedRoute` just to access resolved data.\n+\n+## Error handling in resolvers\n+\n+In the event of navigation failures, it is important to handle errors gracefully in your data resolvers.\n+\n+Here's an updated example of the `userResolver` that logs the error and navigates back to the generic `/users` page using the [`Router`](api/router/Router) service:\n+\n+```ts\n+import { inject } from '@angular/core';\n+import { ResolveFn, RedirectCommand, Router } from '@angular/router';\n+import { catchError, EMPTY } from 'rxjs';\n+import { UserStore } from './user-store';\n+import type { User } from './types';\n+\n+export const userResolver: ResolveFn<User | RedirectCommand> = (route) => {\n+  const userStore = inject(UserStore);\n+  const router = inject(Router);\n+  const userId = route.paramMap.get('id')!;\n+\n+  return userService.getUser(userId).pipe(\n+    catchError(error => {\n+      console.error('Failed to load user:', error);\n+      return [new RedirectCommand(router.parseUrl('/users'))];",
        "comment_created_at": "2025-07-10T15:21:57+00:00",
        "comment_author": "JeanMeche",
        "comment_body": "`catchError` expects an `ObservableInput` (array, promise, observable). The alternative here would be returning `of(new RedirectCommand(router.parseUrl('/users'))`",
        "pr_file_module": null
      }
    ]
  }
]