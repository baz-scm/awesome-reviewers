[
  {
    "discussion_id": "1600374751",
    "pr_number": 4128,
    "pr_file": "boto3/s3/inject.py",
    "created_at": "2024-05-14T17:00:13+00:00",
    "commented_code": "Usage::\n \n         import boto3\n-        s3 = boto3.resource('s3')\n-        s3.meta.client.download_file('mybucket', 'hello.txt', '/tmp/hello.txt')\n+        s3 = boto3.client('s3')",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "1600374751",
        "repo_full_name": "boto/boto3",
        "pr_number": 4128,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "1600374751",
        "commented_code": "@@ -159,8 +159,8 @@ def download_file(\n     Usage::\n \n         import boto3\n-        s3 = boto3.resource('s3')\n-        s3.meta.client.download_file('mybucket', 'hello.txt', '/tmp/hello.txt')\n+        s3 = boto3.client('s3')",
        "comment_created_at": "2024-05-14T17:00:13+00:00",
        "comment_author": "nateprewitt",
        "comment_body": "While we're here can you fix these for `bucket_upload_file` and `bucket_download_file` too, Tim?",
        "pr_file_module": null
      },
      {
        "comment_id": "1600437999",
        "repo_full_name": "boto/boto3",
        "pr_number": 4128,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "1600374751",
        "commented_code": "@@ -159,8 +159,8 @@ def download_file(\n     Usage::\n \n         import boto3\n-        s3 = boto3.resource('s3')\n-        s3.meta.client.download_file('mybucket', 'hello.txt', '/tmp/hello.txt')\n+        s3 = boto3.client('s3')",
        "comment_created_at": "2024-05-14T17:52:19+00:00",
        "comment_author": "tim-finnigan",
        "comment_body": "Shouldn't the [bucket_upload_file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/bucket/upload_file.html) and [bucket_download_file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/bucket/download_file.html) examples stay the same since they're under the [Bucket](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/bucket/index.html) -> [Actions](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/bucket/index.html#actions) documentation for resources?",
        "pr_file_module": null
      },
      {
        "comment_id": "1600441960",
        "repo_full_name": "boto/boto3",
        "pr_number": 4128,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "1600374751",
        "commented_code": "@@ -159,8 +159,8 @@ def download_file(\n     Usage::\n \n         import boto3\n-        s3 = boto3.resource('s3')\n-        s3.meta.client.download_file('mybucket', 'hello.txt', '/tmp/hello.txt')\n+        s3 = boto3.client('s3')",
        "comment_created_at": "2024-05-14T17:55:50+00:00",
        "comment_author": "nateprewitt",
        "comment_body": "Yep, you're right, that was an oversight. Should be good to go.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1172887375",
    "pr_number": 3673,
    "pr_file": "boto3/docs/action.py",
    "created_at": "2023-04-20T17:17:42+00:00",
    "commented_code": "get_resource_public_actions,\n )\n \n+WARNING_MESSAGE_TEMPLATE = \"\"\"\n+.. warning::\n+    Please note that this method does not function as expected. It is\n+    recommended to use the :py:meth:`{suggested_py_method_name}`\n+    :doc:`client method <{suggested_py_method_rel_path}>` instead.\n+    {extra}\n+\"\"\"\n+DOCUMENT_ACTION_OVERRIDES = {\n+    \"Metric\": {\n+        \"put_data\": {\n+            \"ignore_params\": [\"Namespace\"],\n+            \"warning\": WARNING_MESSAGE_TEMPLATE.format(\n+                suggested_py_method_name=\"put_metric_data\",\n+                suggested_py_method_rel_path=(\n+                    \"../../cloudwatch/client/put_metric_data\"\n+                ),\n+                extra=(\n+                    \"If you would still like to use this method, please make sure \"\n+                    \"that `MetricData[].MetricName` is equal to the metric resource's \"\n+                    \"`name` attribute.\"",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "1172887375",
        "repo_full_name": "boto/boto3",
        "pr_number": 3673,
        "pr_file": "boto3/docs/action.py",
        "discussion_id": "1172887375",
        "commented_code": "@@ -29,6 +29,32 @@\n     get_resource_public_actions,\n )\n \n+WARNING_MESSAGE_TEMPLATE = \"\"\"\n+.. warning::\n+    Please note that this method does not function as expected. It is\n+    recommended to use the :py:meth:`{suggested_py_method_name}`\n+    :doc:`client method <{suggested_py_method_rel_path}>` instead.\n+    {extra}\n+\"\"\"\n+DOCUMENT_ACTION_OVERRIDES = {\n+    \"Metric\": {\n+        \"put_data\": {\n+            \"ignore_params\": [\"Namespace\"],\n+            \"warning\": WARNING_MESSAGE_TEMPLATE.format(\n+                suggested_py_method_name=\"put_metric_data\",\n+                suggested_py_method_rel_path=(\n+                    \"../../cloudwatch/client/put_metric_data\"\n+                ),\n+                extra=(\n+                    \"If you would still like to use this method, please make sure \"\n+                    \"that `MetricData[].MetricName` is equal to the metric resource's \"\n+                    \"`name` attribute.\"",
        "comment_created_at": "2023-04-20T17:17:42+00:00",
        "comment_author": "jonathan343",
        "comment_body": "Im assuming you might have wanted these to be inline-code instead of italicized. In restructuredText this is done by using ``` `` ``` instead of ``` ` ``` like in markdown. \r\nSource: [Sphinx Website](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html#:~:text=backquotes%3A%20%60%60text%60%60%20for%20code%20samples.)\r\n\r\nSo:\r\n* ``` `MetricData[].MetricName` ``` --> ``` ``MetricData[].MetricName`` ```\r\n* ``` `name` ``` --> ``` ``name`` ```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "71656610",
    "pr_number": 724,
    "pr_file": "boto3/s3/inject.py",
    "created_at": "2016-07-21T07:10:46+00:00",
    "commented_code": "CopySource=CopySource, Bucket=self.bucket_name, Key=self.key,\n         ExtraArgs=ExtraArgs, Callback=Callback, SourceClient=SourceClient,\n         Config=Config)\n+\n+\n+def upload_fileobj(self, Fileobj, Bucket, Key, ExtraArgs=None,\n+                   Callback=None, Config=None):\n+    \"\"\"Upload a file-like object to S3.\n+\n+    This is a managed transfer which will perform a multipart upload in\n+    multiple threads if necessary.\n+\n+    Usage::\n+\n+        import io",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "71656610",
        "repo_full_name": "boto/boto3",
        "pr_number": 724,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "71656610",
        "commented_code": "@@ -356,3 +367,267 @@ def object_copy(self, CopySource, ExtraArgs=None, Callback=None,\n         CopySource=CopySource, Bucket=self.bucket_name, Key=self.key,\n         ExtraArgs=ExtraArgs, Callback=Callback, SourceClient=SourceClient,\n         Config=Config)\n+\n+\n+def upload_fileobj(self, Fileobj, Bucket, Key, ExtraArgs=None,\n+                   Callback=None, Config=None):\n+    \"\"\"Upload a file-like object to S3.\n+\n+    This is a managed transfer which will perform a multipart upload in\n+    multiple threads if necessary.\n+\n+    Usage::\n+\n+        import io",
        "comment_created_at": "2016-07-21T07:10:46+00:00",
        "comment_author": "kyleknap",
        "comment_body": "I think it would be worth having two examples:\n1) For opening a file-like object with 'rb'\n2) And your io.BytesIO example\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "71656778",
    "pr_number": 724,
    "pr_file": "boto3/s3/inject.py",
    "created_at": "2016-07-21T07:12:19+00:00",
    "commented_code": "CopySource=CopySource, Bucket=self.bucket_name, Key=self.key,\n         ExtraArgs=ExtraArgs, Callback=Callback, SourceClient=SourceClient,\n         Config=Config)\n+\n+\n+def upload_fileobj(self, Fileobj, Bucket, Key, ExtraArgs=None,\n+                   Callback=None, Config=None):\n+    \"\"\"Upload a file-like object to S3.\n+\n+    This is a managed transfer which will perform a multipart upload in\n+    multiple threads if necessary.\n+\n+    Usage::\n+\n+        import io\n+        import boto3\n+        data = io.BytesIO(b'foo')\n+        s3 = boto3.client('s3')\n+        s3.upload_fileobj(data, 'mybucket', 'mykey')\n+\n+    :type Fileobj: a file-like object\n+    :param Fileobj: A file-like object to upload. At a minimum, it must\n+        implement the `read` method.\n+\n+    :type Bucket: str\n+    :param Bucket: The name of the bucket to upload to.\n+\n+    :type Key: str\n+    :param Key: The name of the key to upload to.\n+\n+    :type ExtraArgs: dict\n+    :param ExtraArgs: Extra arguments that may be passed to the\n+        client operation.\n+\n+    :type Callback: method\n+    :param Callback: A method which takes a number of bytes transferred to\n+        be periodically called during the upload.\n+\n+    :type Config: boto3.s3.transfer.TransferConfig\n+    :param Config: The transfer configuration to be used when performing the\n+        upload.\n+    \"\"\"\n+    subscribers = None\n+    if Callback is not None:\n+        subscribers = [ProgressCallbackInvoker(Callback)]\n+\n+    config = Config\n+    if config is None:\n+        config = TransferConfig()\n+\n+    with TransferManager(self, config) as manager:\n+        future = manager.upload(\n+            fileobj=Fileobj, bucket=Bucket, key=Key,\n+            extra_args=ExtraArgs, subscribers=subscribers)\n+        return future.result()\n+\n+\n+def bucket_upload_fileobj(self, Fileobj, Key, ExtraArgs=None,\n+                          Callback=None, Config=None):\n+    \"\"\"Upload a file-like object to this bucket.\n+\n+    This is a managed transfer which will perform a multipart upload in\n+    multiple threads if necessary.\n+\n+    Usage::\n+\n+        import io",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "71656778",
        "repo_full_name": "boto/boto3",
        "pr_number": 724,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "71656778",
        "commented_code": "@@ -356,3 +367,267 @@ def object_copy(self, CopySource, ExtraArgs=None, Callback=None,\n         CopySource=CopySource, Bucket=self.bucket_name, Key=self.key,\n         ExtraArgs=ExtraArgs, Callback=Callback, SourceClient=SourceClient,\n         Config=Config)\n+\n+\n+def upload_fileobj(self, Fileobj, Bucket, Key, ExtraArgs=None,\n+                   Callback=None, Config=None):\n+    \"\"\"Upload a file-like object to S3.\n+\n+    This is a managed transfer which will perform a multipart upload in\n+    multiple threads if necessary.\n+\n+    Usage::\n+\n+        import io\n+        import boto3\n+        data = io.BytesIO(b'foo')\n+        s3 = boto3.client('s3')\n+        s3.upload_fileobj(data, 'mybucket', 'mykey')\n+\n+    :type Fileobj: a file-like object\n+    :param Fileobj: A file-like object to upload. At a minimum, it must\n+        implement the `read` method.\n+\n+    :type Bucket: str\n+    :param Bucket: The name of the bucket to upload to.\n+\n+    :type Key: str\n+    :param Key: The name of the key to upload to.\n+\n+    :type ExtraArgs: dict\n+    :param ExtraArgs: Extra arguments that may be passed to the\n+        client operation.\n+\n+    :type Callback: method\n+    :param Callback: A method which takes a number of bytes transferred to\n+        be periodically called during the upload.\n+\n+    :type Config: boto3.s3.transfer.TransferConfig\n+    :param Config: The transfer configuration to be used when performing the\n+        upload.\n+    \"\"\"\n+    subscribers = None\n+    if Callback is not None:\n+        subscribers = [ProgressCallbackInvoker(Callback)]\n+\n+    config = Config\n+    if config is None:\n+        config = TransferConfig()\n+\n+    with TransferManager(self, config) as manager:\n+        future = manager.upload(\n+            fileobj=Fileobj, bucket=Bucket, key=Key,\n+            extra_args=ExtraArgs, subscribers=subscribers)\n+        return future.result()\n+\n+\n+def bucket_upload_fileobj(self, Fileobj, Key, ExtraArgs=None,\n+                          Callback=None, Config=None):\n+    \"\"\"Upload a file-like object to this bucket.\n+\n+    This is a managed transfer which will perform a multipart upload in\n+    multiple threads if necessary.\n+\n+    Usage::\n+\n+        import io",
        "comment_created_at": "2016-07-21T07:12:19+00:00",
        "comment_author": "kyleknap",
        "comment_body": "Same thing here. I think it would be useful to add an open file example with 'rb'\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "39897863",
    "pr_number": 243,
    "pr_file": "boto3/s3/inject.py",
    "created_at": "2015-09-18T20:33:14+00:00",
    "commented_code": "return transfer.download_file(\n         bucket=Bucket, key=Key, filename=Filename,\n         extra_args=ExtraArgs, callback=Callback)\n+\n+\n+def bucket_upload_file(self, Filename, Key,\n+                       ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Upload a file to an S3 object.\"\"\"",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "39897863",
        "repo_full_name": "boto/boto3",
        "pr_number": 243,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "39897863",
        "commented_code": "@@ -56,3 +65,35 @@ def download_file(self, Bucket, Key, Filename, ExtraArgs=None,\n     return transfer.download_file(\n         bucket=Bucket, key=Key, filename=Filename,\n         extra_args=ExtraArgs, callback=Callback)\n+\n+\n+def bucket_upload_file(self, Filename, Key,\n+                       ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Upload a file to an S3 object.\"\"\"",
        "comment_created_at": "2015-09-18T20:33:14+00:00",
        "comment_author": "kyleknap",
        "comment_body": "It would be cool if we can add a little more to all of these docstrings. These docstrings will get publicly exposed in our documentation and it would be nice to have parameters documented and maybe even an example of how to use it.\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "27617927",
    "pr_number": 80,
    "pr_file": "boto3/s3/transfer.py",
    "created_at": "2015-04-01T22:21:30+00:00",
    "commented_code": "+\"\"\"Abstractions over S3's upload/download operations.\n+\n+This module provides high level abstractions for efficient\n+uploads/downloads.  It handles several things for the user:\n+\n+* Automatically switching to multipart transfers when\n+  a file is over a specific size threshold\n+* Uploading/downloading a file in parallel\n+* Throttling based on max bandwidth\n+* Progress callbacks to monitor transfers\n+* Retries.  While botocore handles retries for streaming uploads,\n+  it is not possible for it to handle retries for streaming\n+  downloads.  This module handles retries for both cases so\n+  you don't need to implement any retry logic yourself.\n+\n+This module has a reasonable set of defaults.  It also allows you\n+to configure many aspects of the transfer process including:\n+\n+* Multipart threshold size\n+* Max parallel downloads\n+* Max bandwidth\n+* Socket timeouts\n+* Retry amounts\n+\n+There is no support for s3->s3 multipart copies at this\n+time.\n+\n+\n+Usage\n+=====\n+\n+The simplest way to use this module is:\n+\n+.. code-block:: python\n+\n+    client = boto3.client('s3', 'us-west-2')\n+    transfer = S3Transfer(client)\n+    # Upload /tmp/myfile to s3://bucket/key\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key')\n+\n+    # Download s3://bucket/key to /tmp/myfile\n+    transfer.download_file('bucket', 'key', '/tmp/myfile')\n+\n+The ``upload_file`` and ``download_file`` methods also accept\n+``**kwargs``, which will be forwarded through to the corresponding\n+client operation.  Here are a few examples using ``upload_file``::\n+\n+    # Making the object public\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         extra_args={'ACL': 'public-read'})\n+\n+    # Setting metadata\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         extra_args={'Metadata': {'a': 'b', 'c': 'd'}})\n+\n+    # Setting content type\n+    transfer.upload_file('/tmp/myfile.json', 'bucket', 'key',\n+                         extra_args={'ContentType': \"application/json\"})\n+\n+\n+The ``S3Transfer`` clas also supports progress callbacks so you can\n+provide transfer progress to users.  Both the ``upload_file`` and\n+``download_file`` methods take an optional ``callback`` parameter.\n+Here's an example of how to print a simple progress percentage\n+to the user:\n+\n+.. code-block:: python\n+\n+    class ProgressPercentage(object):\n+        def __init__(self, filename):\n+            self._filename = filename\n+            self._size = float(os.path.getsize(filename))\n+            self._seen_so_far = 0\n+            self._lock = threading.Lock()\n+\n+        def __call__(self, filename, bytes_amount):\n+            # To simplify we'll assume this is hooked up\n+            # to a single filename.\n+            with self._lock:\n+                self._seen_so_far += bytes_amount\n+                percentage = (self._seen_so_far / self._size) * 100\n+                sys.stdout.write(\n+                    \"\\r%s  %s / %s  (%.2f%%)\" % (filename, self._seen_so_far,\n+                                                 self._size, percentage))\n+                sys.stdout.flush()\n+\n+\n+    transfer = S3Transfer(boto3.client('s3', 'us-west-2'))\n+    # Upload /tmp/myfile to s3://bucket/key and print upload progress.\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         callback=ProgressPercentage('/tmp/myfile'))\n+\n+\n+\n+You can also provide an TransferConfig object to the S3Transfer\n+object that gives you more fine grained control over the\n+transfer.  For example:\n+\n+.. code-block:: python\n+\n+    client = boto3.client('s3', 'us-west-2')\n+    config = TransferConfig(\n+        multipart_threshold=8 * 1024 * 1024,\n+        max_concurrency=10,\n+        num_download_attempts=10,\n+    )\n+    transfer = S3Transfer(client, config)\n+    transfer.upload_file('/tmp/foo', 'bucket', 'key')\n+\n+\n+\"\"\"\n+import os\n+import math\n+import threading\n+import functools\n+import logging\n+import socket\n+from concurrent import futures\n+\n+from botocore.vendored.requests.packages.urllib3.exceptions import \\\n+    ReadTimeoutError\n+from botocore.exceptions import IncompleteReadError\n+\n+from boto3.exceptions import RetriesExceededError\n+\n+\n+logger = logging.getLogger(__name__)\n+MB = 1024 * 1024\n+\n+\n+class ReadFileChunk(object):\n+    def __init__(self, fileobj, start_byte, chunk_size, full_file_size,\n+                 callback=None):\n+        \"\"\"\n+\n+        Given a file object shown below:\n+\n+            |___________________________________________________|\n+            0          |                 |                 full_file_size\n+                       |----chunk_size---|\n+                 start_byte\n+\n+        :type fileobj: file\n+        :param fileobj: File like object\n+\n+        :type start_byte: int\n+        :param start_byte: The first byte from which to start reading.\n+\n+        :type chunk_size: int\n+        :param chunk_size: The max chunk size to read.  Trying to read\n+            pass the end of the chunk size will behave like you've\n+            reached the end of the file.\n+\n+        :type full_file_size: int\n+        :param full_file_size: The entire content length associated\n+            with ``fileobj``.\n+\n+        :type callback: function(amount_read)\n+        :param callback: Called whenever data is read from this object.\n+\n+        \"\"\"\n+        self._fileobj = fileobj\n+        self._start_byte = start_byte\n+        self._size = self._calculate_file_size(\n+            self._fileobj, requested_size=chunk_size,\n+            start_byte=start_byte, actual_file_size=full_file_size)\n+        self._fileobj.seek(self._start_byte)\n+        self._amount_read = 0\n+        self._callback = callback\n+\n+    @classmethod\n+    def from_filename(cls, filename, start_byte, chunk_size, callback=None):\n+        \"\"\"Convenience factory function to create from a filename.\"\"\"\n+        f = open(filename, 'rb')\n+        file_size = os.fstat(f.fileno()).st_size\n+        return cls(f, start_byte, chunk_size, file_size, callback)\n+\n+    def _calculate_file_size(self, fileobj, requested_size, start_byte,\n+                             actual_file_size):\n+        max_chunk_size = actual_file_size - start_byte\n+        return min(max_chunk_size, requested_size)\n+\n+    def read(self, amount=None):\n+        if amount is None:\n+            amount_to_read = self._size - self._amount_read\n+        else:\n+            amount_to_read = min(self._size - self._amount_read, amount)\n+        data = self._fileobj.read(amount_to_read)\n+        self._amount_read += len(data)\n+        if self._callback is not None:\n+            self._callback(len(data))\n+        return data\n+\n+    def seek(self, where):\n+        self._fileobj.seek(self._start_byte + where)\n+        self._amount_read = where\n+\n+    def close(self):\n+        self._fileobj.close()\n+\n+    def tell(self):\n+        return self._amount_read\n+\n+    def __len__(self):\n+        # __len__ is defined because requests will try to determine the length\n+        # of the stream to set a content length.  In the normal case\n+        # of the file it will just stat the file, but we need to change that\n+        # behavior.  By providing a __len__, requests will use that instead\n+        # of stat'ing the file.\n+        return self._size\n+\n+    def __enter__(self):\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        self.close()\n+\n+    def __iter__(self):\n+        # This is a workaround for http://bugs.python.org/issue17575\n+        # Basically httplib will try to iterate over the contents, even\n+        # if its a file like object.  This wasn't noticed because we've\n+        # already exhausted the stream so iterating over the file immediately\n+        # steps, which is what we're simulating here.\n+        return iter([])\n+\n+\n+class StreamReaderProgress(object):\n+    \"\"\"Wrapper for a read only stream that adds progress callbacks.\"\"\"\n+    def __init__(self, stream, callback=None):\n+        self._stream = stream\n+        self._callback = callback\n+\n+    def read(self, *args, **kwargs):\n+        value = self._stream.read(*args, **kwargs)\n+        if self._callback is not None:\n+            self._callback(len(value))\n+        return value\n+\n+\n+class ThreadSafeWriter(object):\n+    def __init__(self, write_stream):\n+        self._write_stream = write_stream\n+        self._lock = threading.Lock()\n+\n+    def pwrite(self, data, offset):\n+        with self._lock:\n+            self._write_stream.seek(offset)\n+            self._write_stream.write(data)\n+\n+    def close(self):\n+        return self._write_stream.close()\n+\n+\n+class OSUtils(object):\n+    def get_file_size(self, filename):\n+        return os.path.getsize(filename)\n+\n+    def open_file_chunk_reader(self, filename, start_byte, size, callback):\n+        return ReadFileChunk.from_filename(filename, start_byte,\n+                                           size, callback)\n+\n+    def open(self, filename, mode):\n+        return open(filename, mode)\n+\n+    def wrap_stream_with_callback(self, stream, callback):\n+        return StreamReaderProgress(stream, callback)\n+\n+    def wrap_thread_safe_writer(self, stream):\n+        return ThreadSafeWriter(stream)\n+\n+\n+class MultipartUploader(object):\n+    def __init__(self, client, config, osutil,\n+                 executor_cls=futures.ThreadPoolExecutor):\n+        self._client = client\n+        self._config = config\n+        self._os = osutil\n+        self._executor_cls = executor_cls\n+\n+    def upload_file(self, filename, bucket, key, callback, extra_args):\n+        response = self._client.create_multipart_upload(Bucket=bucket,\n+                                                        Key=key, **extra_args)",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "27617927",
        "repo_full_name": "boto/boto3",
        "pr_number": 80,
        "pr_file": "boto3/s3/transfer.py",
        "discussion_id": "27617927",
        "commented_code": "@@ -0,0 +1,448 @@\n+\"\"\"Abstractions over S3's upload/download operations.\n+\n+This module provides high level abstractions for efficient\n+uploads/downloads.  It handles several things for the user:\n+\n+* Automatically switching to multipart transfers when\n+  a file is over a specific size threshold\n+* Uploading/downloading a file in parallel\n+* Throttling based on max bandwidth\n+* Progress callbacks to monitor transfers\n+* Retries.  While botocore handles retries for streaming uploads,\n+  it is not possible for it to handle retries for streaming\n+  downloads.  This module handles retries for both cases so\n+  you don't need to implement any retry logic yourself.\n+\n+This module has a reasonable set of defaults.  It also allows you\n+to configure many aspects of the transfer process including:\n+\n+* Multipart threshold size\n+* Max parallel downloads\n+* Max bandwidth\n+* Socket timeouts\n+* Retry amounts\n+\n+There is no support for s3->s3 multipart copies at this\n+time.\n+\n+\n+Usage\n+=====\n+\n+The simplest way to use this module is:\n+\n+.. code-block:: python\n+\n+    client = boto3.client('s3', 'us-west-2')\n+    transfer = S3Transfer(client)\n+    # Upload /tmp/myfile to s3://bucket/key\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key')\n+\n+    # Download s3://bucket/key to /tmp/myfile\n+    transfer.download_file('bucket', 'key', '/tmp/myfile')\n+\n+The ``upload_file`` and ``download_file`` methods also accept\n+``**kwargs``, which will be forwarded through to the corresponding\n+client operation.  Here are a few examples using ``upload_file``::\n+\n+    # Making the object public\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         extra_args={'ACL': 'public-read'})\n+\n+    # Setting metadata\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         extra_args={'Metadata': {'a': 'b', 'c': 'd'}})\n+\n+    # Setting content type\n+    transfer.upload_file('/tmp/myfile.json', 'bucket', 'key',\n+                         extra_args={'ContentType': \"application/json\"})\n+\n+\n+The ``S3Transfer`` clas also supports progress callbacks so you can\n+provide transfer progress to users.  Both the ``upload_file`` and\n+``download_file`` methods take an optional ``callback`` parameter.\n+Here's an example of how to print a simple progress percentage\n+to the user:\n+\n+.. code-block:: python\n+\n+    class ProgressPercentage(object):\n+        def __init__(self, filename):\n+            self._filename = filename\n+            self._size = float(os.path.getsize(filename))\n+            self._seen_so_far = 0\n+            self._lock = threading.Lock()\n+\n+        def __call__(self, filename, bytes_amount):\n+            # To simplify we'll assume this is hooked up\n+            # to a single filename.\n+            with self._lock:\n+                self._seen_so_far += bytes_amount\n+                percentage = (self._seen_so_far / self._size) * 100\n+                sys.stdout.write(\n+                    \"\\r%s  %s / %s  (%.2f%%)\" % (filename, self._seen_so_far,\n+                                                 self._size, percentage))\n+                sys.stdout.flush()\n+\n+\n+    transfer = S3Transfer(boto3.client('s3', 'us-west-2'))\n+    # Upload /tmp/myfile to s3://bucket/key and print upload progress.\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         callback=ProgressPercentage('/tmp/myfile'))\n+\n+\n+\n+You can also provide an TransferConfig object to the S3Transfer\n+object that gives you more fine grained control over the\n+transfer.  For example:\n+\n+.. code-block:: python\n+\n+    client = boto3.client('s3', 'us-west-2')\n+    config = TransferConfig(\n+        multipart_threshold=8 * 1024 * 1024,\n+        max_concurrency=10,\n+        num_download_attempts=10,\n+    )\n+    transfer = S3Transfer(client, config)\n+    transfer.upload_file('/tmp/foo', 'bucket', 'key')\n+\n+\n+\"\"\"\n+import os\n+import math\n+import threading\n+import functools\n+import logging\n+import socket\n+from concurrent import futures\n+\n+from botocore.vendored.requests.packages.urllib3.exceptions import \\\n+    ReadTimeoutError\n+from botocore.exceptions import IncompleteReadError\n+\n+from boto3.exceptions import RetriesExceededError\n+\n+\n+logger = logging.getLogger(__name__)\n+MB = 1024 * 1024\n+\n+\n+class ReadFileChunk(object):\n+    def __init__(self, fileobj, start_byte, chunk_size, full_file_size,\n+                 callback=None):\n+        \"\"\"\n+\n+        Given a file object shown below:\n+\n+            |___________________________________________________|\n+            0          |                 |                 full_file_size\n+                       |----chunk_size---|\n+                 start_byte\n+\n+        :type fileobj: file\n+        :param fileobj: File like object\n+\n+        :type start_byte: int\n+        :param start_byte: The first byte from which to start reading.\n+\n+        :type chunk_size: int\n+        :param chunk_size: The max chunk size to read.  Trying to read\n+            pass the end of the chunk size will behave like you've\n+            reached the end of the file.\n+\n+        :type full_file_size: int\n+        :param full_file_size: The entire content length associated\n+            with ``fileobj``.\n+\n+        :type callback: function(amount_read)\n+        :param callback: Called whenever data is read from this object.\n+\n+        \"\"\"\n+        self._fileobj = fileobj\n+        self._start_byte = start_byte\n+        self._size = self._calculate_file_size(\n+            self._fileobj, requested_size=chunk_size,\n+            start_byte=start_byte, actual_file_size=full_file_size)\n+        self._fileobj.seek(self._start_byte)\n+        self._amount_read = 0\n+        self._callback = callback\n+\n+    @classmethod\n+    def from_filename(cls, filename, start_byte, chunk_size, callback=None):\n+        \"\"\"Convenience factory function to create from a filename.\"\"\"\n+        f = open(filename, 'rb')\n+        file_size = os.fstat(f.fileno()).st_size\n+        return cls(f, start_byte, chunk_size, file_size, callback)\n+\n+    def _calculate_file_size(self, fileobj, requested_size, start_byte,\n+                             actual_file_size):\n+        max_chunk_size = actual_file_size - start_byte\n+        return min(max_chunk_size, requested_size)\n+\n+    def read(self, amount=None):\n+        if amount is None:\n+            amount_to_read = self._size - self._amount_read\n+        else:\n+            amount_to_read = min(self._size - self._amount_read, amount)\n+        data = self._fileobj.read(amount_to_read)\n+        self._amount_read += len(data)\n+        if self._callback is not None:\n+            self._callback(len(data))\n+        return data\n+\n+    def seek(self, where):\n+        self._fileobj.seek(self._start_byte + where)\n+        self._amount_read = where\n+\n+    def close(self):\n+        self._fileobj.close()\n+\n+    def tell(self):\n+        return self._amount_read\n+\n+    def __len__(self):\n+        # __len__ is defined because requests will try to determine the length\n+        # of the stream to set a content length.  In the normal case\n+        # of the file it will just stat the file, but we need to change that\n+        # behavior.  By providing a __len__, requests will use that instead\n+        # of stat'ing the file.\n+        return self._size\n+\n+    def __enter__(self):\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        self.close()\n+\n+    def __iter__(self):\n+        # This is a workaround for http://bugs.python.org/issue17575\n+        # Basically httplib will try to iterate over the contents, even\n+        # if its a file like object.  This wasn't noticed because we've\n+        # already exhausted the stream so iterating over the file immediately\n+        # steps, which is what we're simulating here.\n+        return iter([])\n+\n+\n+class StreamReaderProgress(object):\n+    \"\"\"Wrapper for a read only stream that adds progress callbacks.\"\"\"\n+    def __init__(self, stream, callback=None):\n+        self._stream = stream\n+        self._callback = callback\n+\n+    def read(self, *args, **kwargs):\n+        value = self._stream.read(*args, **kwargs)\n+        if self._callback is not None:\n+            self._callback(len(value))\n+        return value\n+\n+\n+class ThreadSafeWriter(object):\n+    def __init__(self, write_stream):\n+        self._write_stream = write_stream\n+        self._lock = threading.Lock()\n+\n+    def pwrite(self, data, offset):\n+        with self._lock:\n+            self._write_stream.seek(offset)\n+            self._write_stream.write(data)\n+\n+    def close(self):\n+        return self._write_stream.close()\n+\n+\n+class OSUtils(object):\n+    def get_file_size(self, filename):\n+        return os.path.getsize(filename)\n+\n+    def open_file_chunk_reader(self, filename, start_byte, size, callback):\n+        return ReadFileChunk.from_filename(filename, start_byte,\n+                                           size, callback)\n+\n+    def open(self, filename, mode):\n+        return open(filename, mode)\n+\n+    def wrap_stream_with_callback(self, stream, callback):\n+        return StreamReaderProgress(stream, callback)\n+\n+    def wrap_thread_safe_writer(self, stream):\n+        return ThreadSafeWriter(stream)\n+\n+\n+class MultipartUploader(object):\n+    def __init__(self, client, config, osutil,\n+                 executor_cls=futures.ThreadPoolExecutor):\n+        self._client = client\n+        self._config = config\n+        self._os = osutil\n+        self._executor_cls = executor_cls\n+\n+    def upload_file(self, filename, bucket, key, callback, extra_args):\n+        response = self._client.create_multipart_upload(Bucket=bucket,\n+                                                        Key=key, **extra_args)",
        "comment_created_at": "2015-04-01T22:21:30+00:00",
        "comment_author": "danielgtaylor",
        "comment_body": ":+1: At the very least we should document which command the `extra_args` applies to in order to prevent confusion.\n",
        "pr_file_module": null
      }
    ]
  }
]