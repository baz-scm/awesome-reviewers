[
  {
    "discussion_id": "83226849",
    "pr_number": 349,
    "pr_file": "boto3/s3/inject.py",
    "created_at": "2016-10-13T14:20:17+00:00",
    "commented_code": "extra_args=ExtraArgs, callback=Callback)\n \n \n+def copy_file(self, SrcBucket, SrcKey, DestBucket, DestKey,\n+              ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Copy one S3 object to another s3 object.\n+\n+    Usage::\n+\n+        import boto3\n+        s3 = boto3.resource('s3')\n+        s3.meta.client.copy_file('bucket1', 'hello.txt', 'bucket2', 'hello.txt')",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "83226849",
        "repo_full_name": "boto/boto3",
        "pr_number": 349,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "83226849",
        "commented_code": "@@ -91,6 +94,27 @@ def download_file(self, Bucket, Key, Filename, ExtraArgs=None,\n         extra_args=ExtraArgs, callback=Callback)\n \n \n+def copy_file(self, SrcBucket, SrcKey, DestBucket, DestKey,\n+              ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Copy one S3 object to another s3 object.\n+\n+    Usage::\n+\n+        import boto3\n+        s3 = boto3.resource('s3')\n+        s3.meta.client.copy_file('bucket1', 'hello.txt', 'bucket2', 'hello.txt')",
        "comment_created_at": "2016-10-13T14:20:17+00:00",
        "comment_author": "srinivasbapathu",
        "comment_body": "Hi its nice for bucket to bucket transfer. I would like move the file completely from folder 'A' in bucket to different folder 'B' in same bucket. Means i want archived the file from one folder to another folder in same bucket. Could you please let us know, how can we able to do.\n\nThanks & regards,\nSrinivasa Reddy B\nPhNo: +91-8105856509\n",
        "pr_file_module": null
      },
      {
        "comment_id": "99555197",
        "repo_full_name": "boto/boto3",
        "pr_number": 349,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "83226849",
        "commented_code": "@@ -91,6 +94,27 @@ def download_file(self, Bucket, Key, Filename, ExtraArgs=None,\n         extra_args=ExtraArgs, callback=Callback)\n \n \n+def copy_file(self, SrcBucket, SrcKey, DestBucket, DestKey,\n+              ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Copy one S3 object to another s3 object.\n+\n+    Usage::\n+\n+        import boto3\n+        s3 = boto3.resource('s3')\n+        s3.meta.client.copy_file('bucket1', 'hello.txt', 'bucket2', 'hello.txt')",
        "comment_created_at": "2017-02-06T10:25:30+00:00",
        "comment_author": "dejlek",
        "comment_body": "Furthermore, I need to copy between accounts. So credentials for accessing bucket1 (source files) ARE NOT the same as credentials for bucket2 (dest files)...",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "39320756",
    "pr_number": 243,
    "pr_file": "boto3/s3/inject.py",
    "created_at": "2015-09-11T21:39:52+00:00",
    "commented_code": "return transfer.download_file(\n         bucket=Bucket, key=Key, filename=Filename,\n         extra_args=ExtraArgs, callback=Callback)\n+\n+\n+def bucket_upload_file(self, Filename, Key,\n+                       ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Upload a file to an S3 object.\"\"\"\n+    transfer = S3Transfer(self.meta.client, Config)\n+    return transfer.upload_file(\n+        filename=Filename, bucket=self.name, key=Key,\n+        extra_args=ExtraArgs, callback=Callback)\n+\n+\n+def bucket_download_file(self, Key, Filename,\n+                         ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Download an S3 object to a file.\"\"\"\n+    transfer = S3Transfer(self.meta.client, Config)",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "39320756",
        "repo_full_name": "boto/boto3",
        "pr_number": 243,
        "pr_file": "boto3/s3/inject.py",
        "discussion_id": "39320756",
        "commented_code": "@@ -56,3 +65,39 @@ def download_file(self, Bucket, Key, Filename, ExtraArgs=None,\n     return transfer.download_file(\n         bucket=Bucket, key=Key, filename=Filename,\n         extra_args=ExtraArgs, callback=Callback)\n+\n+\n+def bucket_upload_file(self, Filename, Key,\n+                       ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Upload a file to an S3 object.\"\"\"\n+    transfer = S3Transfer(self.meta.client, Config)\n+    return transfer.upload_file(\n+        filename=Filename, bucket=self.name, key=Key,\n+        extra_args=ExtraArgs, callback=Callback)\n+\n+\n+def bucket_download_file(self, Key, Filename,\n+                         ExtraArgs=None, Callback=None, Config=None):\n+    \"\"\"Download an S3 object to a file.\"\"\"\n+    transfer = S3Transfer(self.meta.client, Config)",
        "comment_created_at": "2015-09-11T21:39:52+00:00",
        "comment_author": "rayluo",
        "comment_body": "~~It seems the [S3Transfer provides more advanced features](http://boto3.readthedocs.org/en/latest/reference/customizations/s3.html#module-boto3.s3.transfer). Besides, the pre-existing upload_file() and download_file() also use S3Transfer so I follow the pattern.~~\n\nUnderstood. You are talking about the already-injected self.meta.client, not the raw botocore client. Testing. Will send out new PR soon.\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "32161960",
    "pr_number": 128,
    "pr_file": "boto3/session.py",
    "created_at": "2015-06-10T20:24:18+00:00",
    "commented_code": "boto3.utils.lazy_call(\n                 'boto3.s3.inject.inject_s3_transfer_methods'))",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "32161960",
        "repo_full_name": "boto/boto3",
        "pr_number": 128,
        "pr_file": "boto3/session.py",
        "discussion_id": "32161960",
        "commented_code": "@@ -280,6 +280,10 @@ def _register_default_handlers(self):\n             boto3.utils.lazy_call(\n                 'boto3.s3.inject.inject_s3_transfer_methods'))",
        "comment_created_at": "2015-06-10T20:24:18+00:00",
        "comment_author": "kyleknap",
        "comment_body": "On a complete side note, we should look into adding `upload_file` and `download_file` to the Bucket resource.\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "30847328",
    "pr_number": 109,
    "pr_file": "boto3/session.py",
    "created_at": "2015-05-21T21:02:37+00:00",
    "commented_code": ":param aws_session_token: The session token to use when creating\n             the client.  Same semantics as aws_access_key_id above.\n \n+        :type config: botocore.client.Config\n+        :param config: Advanced client configuration options. If region_name\n+            is specified in the client config, its value will take precedence\n+            over environment variables and configuration values, but not over\n+            a region_name value passed explicitly to the method.\n+\n         :return: Service client instance\n+\n         \"\"\"\n         return self._session.create_client(\n             service_name, region_name=region_name, api_version=api_version,\n             use_ssl=use_ssl, verify=verify, endpoint_url=endpoint_url,\n             aws_access_key_id=aws_access_key_id,\n             aws_secret_access_key=aws_secret_access_key,\n-            aws_session_token=aws_session_token)\n+            aws_session_token=aws_session_token, config=config)\n \n     def resource(self, service_name, region_name=None, api_version=None,",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "30847328",
        "repo_full_name": "boto/boto3",
        "pr_number": 109,
        "pr_file": "boto3/session.py",
        "discussion_id": "30847328",
        "commented_code": "@@ -174,14 +175,21 @@ def client(self, service_name, region_name=None, api_version=None,\n         :param aws_session_token: The session token to use when creating\n             the client.  Same semantics as aws_access_key_id above.\n \n+        :type config: botocore.client.Config\n+        :param config: Advanced client configuration options. If region_name\n+            is specified in the client config, its value will take precedence\n+            over environment variables and configuration values, but not over\n+            a region_name value passed explicitly to the method.\n+\n         :return: Service client instance\n+\n         \"\"\"\n         return self._session.create_client(\n             service_name, region_name=region_name, api_version=api_version,\n             use_ssl=use_ssl, verify=verify, endpoint_url=endpoint_url,\n             aws_access_key_id=aws_access_key_id,\n             aws_secret_access_key=aws_secret_access_key,\n-            aws_session_token=aws_session_token)\n+            aws_session_token=aws_session_token, config=config)\n \n     def resource(self, service_name, region_name=None, api_version=None,",
        "comment_created_at": "2015-05-21T21:02:37+00:00",
        "comment_author": "jamesls",
        "comment_body": "Yeah it seems we either add the client config arg to `resource()`, or we allow a user to specify a client object when creating a resource.  The latter seems more general and would allow you to use a single client across resources which could be nice.\n\nIf we do allow for a client config to be passed into `resource()`, what should be the behavior if the user already provided a non-None `user_agent_extra`?\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "27607816",
    "pr_number": 80,
    "pr_file": "boto3/s3/transfer.py",
    "created_at": "2015-04-01T20:26:13+00:00",
    "commented_code": "+\"\"\"Abstractions over S3's upload/download operations.\n+\n+This module provides high level abstractions for efficient\n+uploads/downloads.  It handles several things for the user:\n+\n+* Automatically switching to multipart transfers when\n+  a file is over a specific size threshold\n+* Uploading/downloading a file in parallel\n+* Throttling based on max bandwidth\n+* Progress callbacks to monitor transfers\n+* Retries.  While botocore handles retries for streaming uploads,\n+  it is not possible for it to handle retries for streaming\n+  downloads.  This module handles retries for both cases so\n+  you don't need to implement any retry logic yourself.\n+\n+This module has a reasonable set of defaults.  It also allows you\n+to configure many aspects of the transfer process including:\n+\n+* Multipart threshold size\n+* Max parallel downloads\n+* Max bandwidth\n+* Socket timeouts\n+* Retry amounts\n+\n+There is no support for s3->s3 multipart copies at this\n+time.\n+\n+\n+Usage\n+=====\n+\n+The simplest way to use this module is:\n+\n+.. code-block:: python\n+\n+    client = boto3.client('s3', 'us-west-2')\n+    transfer = S3Transfer(client)\n+    # Upload /tmp/myfile to s3://bucket/key\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key')\n+\n+    # Download s3://bucket/key to /tmp/myfile\n+    transfer.download_file('bucket', 'key', '/tmp/myfile')\n+\n+The ``upload_file`` and ``download_file`` methods also accept\n+``**kwargs``, which will be forwarded through to the corresponding\n+client operation.  Here are a few examples using ``upload_file``::\n+\n+    # Making the object public\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         extra_args={'ACL': 'public-read'})\n+\n+    # Setting metadata\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         extra_args={'Metadata': {'a': 'b', 'c': 'd'}})\n+\n+    # Setting content type\n+    transfer.upload_file('/tmp/myfile.json', 'bucket', 'key',\n+                         extra_args={'ContentType': \"application/json\"})\n+\n+\n+The ``S3Transfer`` clas also supports progress callbacks so you can\n+provide transfer progress to users.  Both the ``upload_file`` and\n+``download_file`` methods take an optional ``callback`` parameter.\n+Here's an example of how to print a simple progress percentage\n+to the user:\n+\n+.. code-block:: python\n+\n+    class ProgressPercentage(object):\n+        def __init__(self, filename):\n+            self._filename = filename\n+            self._size = float(os.path.getsize(filename))\n+            self._seen_so_far = 0\n+            self._lock = threading.Lock()\n+\n+        def __call__(self, filename, bytes_amount):\n+            # To simplify we'll assume this is hooked up\n+            # to a single filename.\n+            with self._lock:\n+                self._seen_so_far += bytes_amount\n+                percentage = (self._seen_so_far / self._size) * 100\n+                sys.stdout.write(\n+                    \"\\r%s  %s / %s  (%.2f%%)\" % (filename, self._seen_so_far,\n+                                                 self._size, percentage))\n+                sys.stdout.flush()\n+\n+\n+    transfer = S3Transfer(boto3.client('s3', 'us-west-2'))\n+    # Upload /tmp/myfile to s3://bucket/key and print upload progress.\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         callback=ProgressPercentage('/tmp/myfile'))\n+\n+\n+\n+You can also provide an TransferConfig object to the S3Transfer\n+object that gives you more fine grained control over the\n+transfer.  For example:\n+\n+.. code-block:: python\n+\n+    client = boto3.client('s3', 'us-west-2')\n+    config = TransferConfig(\n+        multipart_threshold=8 * 1024 * 1024,\n+        max_concurrency=10,\n+        num_download_attempts=10,\n+    )\n+    transfer = S3Transfer(client, config)\n+    transfer.upload_file('/tmp/foo', 'bucket', 'key')\n+\n+\n+\"\"\"\n+import os\n+import math\n+import threading\n+import functools\n+import logging\n+import socket\n+from concurrent import futures\n+\n+from botocore.vendored.requests.packages.urllib3.exceptions import \\\n+    ReadTimeoutError\n+from botocore.exceptions import IncompleteReadError\n+\n+from boto3.exceptions import RetriesExceededError\n+\n+\n+logger = logging.getLogger(__name__)\n+MB = 1024 * 1024\n+\n+\n+class ReadFileChunk(object):\n+    def __init__(self, fileobj, start_byte, chunk_size, full_file_size,\n+                 callback=None):\n+        \"\"\"\n+\n+        Given a file object shown below:\n+\n+            |___________________________________________________|\n+            0          |                 |                 full_file_size\n+                       |----chunk_size---|\n+                 start_byte\n+\n+        :type fileobj: file\n+        :param fileobj: File like object\n+\n+        :type start_byte: int\n+        :param start_byte: The first byte from which to start reading.\n+\n+        :type chunk_size: int\n+        :param chunk_size: The max chunk size to read.  Trying to read\n+            pass the end of the chunk size will behave like you've\n+            reached the end of the file.\n+\n+        :type full_file_size: int\n+        :param full_file_size: The entire content length associated\n+            with ``fileobj``.\n+\n+        :type callback: function(amount_read)\n+        :param callback: Called whenever data is read from this object.\n+\n+        \"\"\"\n+        self._fileobj = fileobj\n+        self._start_byte = start_byte\n+        self._size = self._calculate_file_size(\n+            self._fileobj, requested_size=chunk_size,\n+            start_byte=start_byte, actual_file_size=full_file_size)\n+        self._fileobj.seek(self._start_byte)\n+        self._amount_read = 0\n+        self._callback = callback\n+\n+    @classmethod\n+    def from_filename(cls, filename, start_byte, chunk_size, callback=None):\n+        \"\"\"Convenience factory function to create from a filename.\"\"\"\n+        f = open(filename, 'rb')\n+        file_size = os.fstat(f.fileno()).st_size\n+        return cls(f, start_byte, chunk_size, file_size, callback)\n+\n+    def _calculate_file_size(self, fileobj, requested_size, start_byte,\n+                             actual_file_size):\n+        max_chunk_size = actual_file_size - start_byte\n+        return min(max_chunk_size, requested_size)\n+\n+    def read(self, amount=None):\n+        if amount is None:\n+            amount_to_read = self._size - self._amount_read\n+        else:\n+            amount_to_read = min(self._size - self._amount_read, amount)\n+        data = self._fileobj.read(amount_to_read)\n+        self._amount_read += len(data)\n+        if self._callback is not None:\n+            self._callback(len(data))\n+        return data\n+\n+    def seek(self, where):\n+        self._fileobj.seek(self._start_byte + where)\n+        self._amount_read = where\n+\n+    def close(self):\n+        self._fileobj.close()\n+\n+    def tell(self):\n+        return self._amount_read\n+\n+    def __len__(self):\n+        # __len__ is defined because requests will try to determine the length\n+        # of the stream to set a content length.  In the normal case\n+        # of the file it will just stat the file, but we need to change that\n+        # behavior.  By providing a __len__, requests will use that instead\n+        # of stat'ing the file.\n+        return self._size\n+\n+    def __enter__(self):\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        self.close()\n+\n+    def __iter__(self):\n+        # This is a workaround for http://bugs.python.org/issue17575\n+        # Basically httplib will try to iterate over the contents, even\n+        # if its a file like object.  This wasn't noticed because we've\n+        # already exhausted the stream so iterating over the file immediately\n+        # steps, which is what we're simulating here.\n+        return iter([])\n+\n+\n+class StreamReaderProgress(object):\n+    \"\"\"Wrapper for a read only stream that adds progress callbacks.\"\"\"\n+    def __init__(self, stream, callback=None):\n+        self._stream = stream\n+        self._callback = callback\n+\n+    def read(self, *args, **kwargs):\n+        value = self._stream.read(*args, **kwargs)\n+        if self._callback is not None:\n+            self._callback(len(value))\n+        return value\n+\n+\n+class ThreadSafeWriter(object):\n+    def __init__(self, write_stream):\n+        self._write_stream = write_stream\n+        self._lock = threading.Lock()\n+\n+    def pwrite(self, data, offset):\n+        with self._lock:\n+            self._write_stream.seek(offset)\n+            self._write_stream.write(data)\n+\n+    def close(self):\n+        return self._write_stream.close()\n+\n+\n+class OSUtils(object):\n+    def get_file_size(self, filename):\n+        return os.path.getsize(filename)\n+\n+    def open_file_chunk_reader(self, filename, start_byte, size, callback):\n+        return ReadFileChunk.from_filename(filename, start_byte,\n+                                           size, callback)\n+\n+    def open(self, filename, mode):\n+        return open(filename, mode)\n+\n+    def wrap_stream_with_callback(self, stream, callback):\n+        return StreamReaderProgress(stream, callback)\n+\n+    def wrap_thread_safe_writer(self, stream):\n+        return ThreadSafeWriter(stream)\n+\n+\n+class MultipartUploader(object):\n+    def __init__(self, client, config, osutil,\n+                 executor_cls=futures.ThreadPoolExecutor):\n+        self._client = client\n+        self._config = config\n+        self._os = osutil\n+        self._executor_cls = executor_cls\n+\n+    def upload_file(self, filename, bucket, key, callback, extra_args):\n+        response = self._client.create_multipart_upload(Bucket=bucket,\n+                                                        Key=key, **extra_args)",
    "repo_full_name": "boto/boto3",
    "discussion_comments": [
      {
        "comment_id": "27607816",
        "repo_full_name": "boto/boto3",
        "pr_number": 80,
        "pr_file": "boto3/s3/transfer.py",
        "discussion_id": "27607816",
        "commented_code": "@@ -0,0 +1,448 @@\n+\"\"\"Abstractions over S3's upload/download operations.\n+\n+This module provides high level abstractions for efficient\n+uploads/downloads.  It handles several things for the user:\n+\n+* Automatically switching to multipart transfers when\n+  a file is over a specific size threshold\n+* Uploading/downloading a file in parallel\n+* Throttling based on max bandwidth\n+* Progress callbacks to monitor transfers\n+* Retries.  While botocore handles retries for streaming uploads,\n+  it is not possible for it to handle retries for streaming\n+  downloads.  This module handles retries for both cases so\n+  you don't need to implement any retry logic yourself.\n+\n+This module has a reasonable set of defaults.  It also allows you\n+to configure many aspects of the transfer process including:\n+\n+* Multipart threshold size\n+* Max parallel downloads\n+* Max bandwidth\n+* Socket timeouts\n+* Retry amounts\n+\n+There is no support for s3->s3 multipart copies at this\n+time.\n+\n+\n+Usage\n+=====\n+\n+The simplest way to use this module is:\n+\n+.. code-block:: python\n+\n+    client = boto3.client('s3', 'us-west-2')\n+    transfer = S3Transfer(client)\n+    # Upload /tmp/myfile to s3://bucket/key\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key')\n+\n+    # Download s3://bucket/key to /tmp/myfile\n+    transfer.download_file('bucket', 'key', '/tmp/myfile')\n+\n+The ``upload_file`` and ``download_file`` methods also accept\n+``**kwargs``, which will be forwarded through to the corresponding\n+client operation.  Here are a few examples using ``upload_file``::\n+\n+    # Making the object public\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         extra_args={'ACL': 'public-read'})\n+\n+    # Setting metadata\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         extra_args={'Metadata': {'a': 'b', 'c': 'd'}})\n+\n+    # Setting content type\n+    transfer.upload_file('/tmp/myfile.json', 'bucket', 'key',\n+                         extra_args={'ContentType': \"application/json\"})\n+\n+\n+The ``S3Transfer`` clas also supports progress callbacks so you can\n+provide transfer progress to users.  Both the ``upload_file`` and\n+``download_file`` methods take an optional ``callback`` parameter.\n+Here's an example of how to print a simple progress percentage\n+to the user:\n+\n+.. code-block:: python\n+\n+    class ProgressPercentage(object):\n+        def __init__(self, filename):\n+            self._filename = filename\n+            self._size = float(os.path.getsize(filename))\n+            self._seen_so_far = 0\n+            self._lock = threading.Lock()\n+\n+        def __call__(self, filename, bytes_amount):\n+            # To simplify we'll assume this is hooked up\n+            # to a single filename.\n+            with self._lock:\n+                self._seen_so_far += bytes_amount\n+                percentage = (self._seen_so_far / self._size) * 100\n+                sys.stdout.write(\n+                    \"\\r%s  %s / %s  (%.2f%%)\" % (filename, self._seen_so_far,\n+                                                 self._size, percentage))\n+                sys.stdout.flush()\n+\n+\n+    transfer = S3Transfer(boto3.client('s3', 'us-west-2'))\n+    # Upload /tmp/myfile to s3://bucket/key and print upload progress.\n+    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n+                         callback=ProgressPercentage('/tmp/myfile'))\n+\n+\n+\n+You can also provide an TransferConfig object to the S3Transfer\n+object that gives you more fine grained control over the\n+transfer.  For example:\n+\n+.. code-block:: python\n+\n+    client = boto3.client('s3', 'us-west-2')\n+    config = TransferConfig(\n+        multipart_threshold=8 * 1024 * 1024,\n+        max_concurrency=10,\n+        num_download_attempts=10,\n+    )\n+    transfer = S3Transfer(client, config)\n+    transfer.upload_file('/tmp/foo', 'bucket', 'key')\n+\n+\n+\"\"\"\n+import os\n+import math\n+import threading\n+import functools\n+import logging\n+import socket\n+from concurrent import futures\n+\n+from botocore.vendored.requests.packages.urllib3.exceptions import \\\n+    ReadTimeoutError\n+from botocore.exceptions import IncompleteReadError\n+\n+from boto3.exceptions import RetriesExceededError\n+\n+\n+logger = logging.getLogger(__name__)\n+MB = 1024 * 1024\n+\n+\n+class ReadFileChunk(object):\n+    def __init__(self, fileobj, start_byte, chunk_size, full_file_size,\n+                 callback=None):\n+        \"\"\"\n+\n+        Given a file object shown below:\n+\n+            |___________________________________________________|\n+            0          |                 |                 full_file_size\n+                       |----chunk_size---|\n+                 start_byte\n+\n+        :type fileobj: file\n+        :param fileobj: File like object\n+\n+        :type start_byte: int\n+        :param start_byte: The first byte from which to start reading.\n+\n+        :type chunk_size: int\n+        :param chunk_size: The max chunk size to read.  Trying to read\n+            pass the end of the chunk size will behave like you've\n+            reached the end of the file.\n+\n+        :type full_file_size: int\n+        :param full_file_size: The entire content length associated\n+            with ``fileobj``.\n+\n+        :type callback: function(amount_read)\n+        :param callback: Called whenever data is read from this object.\n+\n+        \"\"\"\n+        self._fileobj = fileobj\n+        self._start_byte = start_byte\n+        self._size = self._calculate_file_size(\n+            self._fileobj, requested_size=chunk_size,\n+            start_byte=start_byte, actual_file_size=full_file_size)\n+        self._fileobj.seek(self._start_byte)\n+        self._amount_read = 0\n+        self._callback = callback\n+\n+    @classmethod\n+    def from_filename(cls, filename, start_byte, chunk_size, callback=None):\n+        \"\"\"Convenience factory function to create from a filename.\"\"\"\n+        f = open(filename, 'rb')\n+        file_size = os.fstat(f.fileno()).st_size\n+        return cls(f, start_byte, chunk_size, file_size, callback)\n+\n+    def _calculate_file_size(self, fileobj, requested_size, start_byte,\n+                             actual_file_size):\n+        max_chunk_size = actual_file_size - start_byte\n+        return min(max_chunk_size, requested_size)\n+\n+    def read(self, amount=None):\n+        if amount is None:\n+            amount_to_read = self._size - self._amount_read\n+        else:\n+            amount_to_read = min(self._size - self._amount_read, amount)\n+        data = self._fileobj.read(amount_to_read)\n+        self._amount_read += len(data)\n+        if self._callback is not None:\n+            self._callback(len(data))\n+        return data\n+\n+    def seek(self, where):\n+        self._fileobj.seek(self._start_byte + where)\n+        self._amount_read = where\n+\n+    def close(self):\n+        self._fileobj.close()\n+\n+    def tell(self):\n+        return self._amount_read\n+\n+    def __len__(self):\n+        # __len__ is defined because requests will try to determine the length\n+        # of the stream to set a content length.  In the normal case\n+        # of the file it will just stat the file, but we need to change that\n+        # behavior.  By providing a __len__, requests will use that instead\n+        # of stat'ing the file.\n+        return self._size\n+\n+    def __enter__(self):\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        self.close()\n+\n+    def __iter__(self):\n+        # This is a workaround for http://bugs.python.org/issue17575\n+        # Basically httplib will try to iterate over the contents, even\n+        # if its a file like object.  This wasn't noticed because we've\n+        # already exhausted the stream so iterating over the file immediately\n+        # steps, which is what we're simulating here.\n+        return iter([])\n+\n+\n+class StreamReaderProgress(object):\n+    \"\"\"Wrapper for a read only stream that adds progress callbacks.\"\"\"\n+    def __init__(self, stream, callback=None):\n+        self._stream = stream\n+        self._callback = callback\n+\n+    def read(self, *args, **kwargs):\n+        value = self._stream.read(*args, **kwargs)\n+        if self._callback is not None:\n+            self._callback(len(value))\n+        return value\n+\n+\n+class ThreadSafeWriter(object):\n+    def __init__(self, write_stream):\n+        self._write_stream = write_stream\n+        self._lock = threading.Lock()\n+\n+    def pwrite(self, data, offset):\n+        with self._lock:\n+            self._write_stream.seek(offset)\n+            self._write_stream.write(data)\n+\n+    def close(self):\n+        return self._write_stream.close()\n+\n+\n+class OSUtils(object):\n+    def get_file_size(self, filename):\n+        return os.path.getsize(filename)\n+\n+    def open_file_chunk_reader(self, filename, start_byte, size, callback):\n+        return ReadFileChunk.from_filename(filename, start_byte,\n+                                           size, callback)\n+\n+    def open(self, filename, mode):\n+        return open(filename, mode)\n+\n+    def wrap_stream_with_callback(self, stream, callback):\n+        return StreamReaderProgress(stream, callback)\n+\n+    def wrap_thread_safe_writer(self, stream):\n+        return ThreadSafeWriter(stream)\n+\n+\n+class MultipartUploader(object):\n+    def __init__(self, client, config, osutil,\n+                 executor_cls=futures.ThreadPoolExecutor):\n+        self._client = client\n+        self._config = config\n+        self._os = osutil\n+        self._executor_cls = executor_cls\n+\n+    def upload_file(self, filename, bucket, key, callback, extra_args):\n+        response = self._client.create_multipart_upload(Bucket=bucket,\n+                                                        Key=key, **extra_args)",
        "comment_created_at": "2015-04-01T20:26:13+00:00",
        "comment_author": "kyleknap",
        "comment_body": "There is not an exact mapping between the `extra_args` for `PutObject` and `CreateMultipartUpload`. For example, `PutObject` has `ContentMD5` and `ContentLenght` options, but `CreateMultipartUpload` does not (so you may get a botocore validation error).\n\nAlso if I remember correctly for some arguments, it is not sufficient just to include them in the `CreateMultipartUpload` call, you need to include them in `UploadPart`. I believe this is the case for SSEC encryption.\n\nYou probably will have to do some custom logic in the mappings of the arguments.\n",
        "pr_file_module": null
      }
    ]
  }
]