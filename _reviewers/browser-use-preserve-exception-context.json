[
  {
    "discussion_id": "2047748335",
    "pr_number": 1395,
    "pr_file": "eval/service.py",
    "created_at": "2025-04-16T20:54:51+00:00",
    "commented_code": "+# ==============================================================================================================\n+# Documentation for this evaluation file.\n+# The import\n+\n+\n+# Here is the command to run the evaluation:\n+# python eval/service.py --parallel_runs 5 --parallel_evaluations 5 --max-steps 25 --start 0 --end 100\n+# options:\n+# --parallel_runs: Number of parallel tasks to run\n+# --max-steps: Maximum steps per task\n+# --start: Start index\n+# --end: End index (exclusive)\n+# --headless: Run in headless mode\n+\n+# Here is the command to run the evaluation only:\n+# python eval/service.py --evaluate-only\n+# options:\n+# --parallel_evaluations: Number of parallel evaluations to run\n+\n+# To run a new evaluation, you need to first clear the saved_trajectories folder.\n+# rm -rf saved_trajectories\n+# Otherwise, the evaluation will continue on from the last saved trajectory.\n+# ==============================================================================================================\n+\n+\n+# ==============================================================================================================\n+# This is the LLM as a judge evaluation system from the OSU-NLP Group paper\n+# Any adaptiations made should be explicitly stated here:\n+# Adaptations:\n+# We are using our langchain wrapper for the OpenAI API\n+# This means we changed model.generate to model.invoke. The behavior of the model should be identical.\n+# Added a Online_Mind2Web_eval_with_retry wrapper with retry logic in case of API rate limiting or other issues.\n+\n+\n+# @article{xue2025illusionprogressassessingcurrent,\n+#       title={An Illusion of Progress? Assessing the Current State of Web Agents},\n+#       author={Tianci Xue and Weijian Qi and Tianneng Shi and Chan Hee Song and Boyu Gou and Dawn Song and Huan Sun and Yu Su},\n+#       year={2025},\n+#       eprint={2504.01382},\n+#       archivePrefix={arXiv},\n+#       primaryClass={cs.AI},\n+#       url={https://arxiv.org/abs/2504.01382},\n+# }\n+\n+# @inproceedings{deng2023mind2web,\n+#  author = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},\n+#  booktitle = {Advances in Neural Information Processing Systems},\n+#  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},\n+#  pages = {28091--28114},\n+#  publisher = {Curran Associates, Inc.},\n+#  title = {Mind2Web: Towards a Generalist Agent for the Web},\n+#  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf},\n+#  volume = {36},\n+#  year = {2023}\n+# }\n+# ==============================================================================================================\n+import asyncio\n+import base64\n+import io\n+import logging\n+import re\n+\n+from PIL import Image\n+\n+MAX_IMAGE = 5\n+\n+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n+logger = logging.getLogger(__name__)\n+\n+\n+def encode_image(image):\n+\t\"\"\"Convert a PIL image to base64 string.\"\"\"\n+\tif image.mode == 'RGBA':\n+\t\timage = image.convert('RGB')\n+\tbuffered = io.BytesIO()\n+\timage.save(buffered, format='JPEG')\n+\treturn base64.b64encode(buffered.getvalue()).decode('utf-8')\n+\n+\n+async def identify_key_points(task, model):\n+\tsystem_msg = \"\"\"You are an expert tasked with analyzing a given task to identify the key points explicitly stated in the task description.\n+\n+**Objective**: Carefully analyze the task description and extract the critical elements explicitly mentioned in the task for achieving its goal.\n+\n+**Instructions**:\n+1. Read the task description carefully.\n+2. Identify and extract **key points** directly stated in the task description.\n+   - A **key point** is a critical element, condition, or step explicitly mentioned in the task description.\n+   - Do not infer or add any unstated elements.\n+   - Words such as \"best,\" \"highest,\" \"cheapest,\" \"latest,\" \"most recent,\" \"lowest,\" \"closest,\" \"highest-rated,\" \"largest,\" and \"newest\" must go through the sort function(e.g., the key point should be \"Filter by highest\").\n+\n+**Respond with**:\n+- **Key Points**: A numbered list of the explicit key points for completing this task, one per line, without explanations or additional details.\"\"\"\n+\tprompt = \"\"\"Task: {task}\"\"\"\n+\ttext = prompt.format(task=task)\n+\tmessages = [\n+\t\t{'role': 'system', 'content': system_msg},\n+\t\t{\n+\t\t\t'role': 'user',\n+\t\t\t'content': [{'type': 'text', 'text': text}],\n+\t\t},\n+\t]\n+\tresponse = await asyncio.to_thread(model.invoke, messages)\n+\treturn response.content\n+\n+\n+async def judge_image(task, image_path, key_points, model):\n+\tsystem_msg = \"\"\"You are an expert evaluator tasked with determining whether an image contains information about the necessary steps to complete a task.\n+\n+**Objective**: Analyze the provided image and decide if it shows essential steps or evidence required for completing the task. Use your reasoning to explain your decision before assigning a score.\n+\n+**Instructions**:\n+1. Provide a detailed description of the image, including its contents, visible elements, text (if any), and any notable features.\n+\n+2. Carefully examine the image and evaluate whether it contains necessary steps or evidence crucial to task completion:  \n+- Identify key points that could be relevant to task completion, such as actions, progress indicators, tool usage, applied filters, or step-by-step instructions.  \n+- Does the image show actions, progress indicators, or critical information directly related to completing the task?  \n+- Is this information indispensable for understanding or ensuring task success?\n+- If the image contains partial but relevant information, consider its usefulness rather than dismissing it outright.\n+\n+3. Provide your response in the following format:  \n+- **Reasoning**: Explain your thought process and observations. Mention specific elements in the image that indicate necessary steps, evidence, or lack thereof.  \n+- **Score**: Assign a score based on the reasoning, using the following scale:  \n+    - **1**: The image does not contain any necessary steps or relevant information.  \n+    - **2**: The image contains minimal or ambiguous information, unlikely to be essential.  \n+    - **3**: The image includes some relevant steps or hints but lacks clarity or completeness.  \n+    - **4**: The image contains important steps or evidence that are highly relevant but not fully comprehensive.  \n+    - **5**: The image clearly displays necessary steps or evidence crucial for completing the task.\n+\n+Respond with:  \n+1. **Reasoning**: [Your explanation]  \n+2. **Score**: [1-5]\"\"\"\n+\n+\tjpg_base64_str = encode_image(Image.open(image_path))\n+\n+\tprompt = \"\"\"**Task**: {task}\n+\n+**Key Points for Task Completion**: {key_points}\n+\n+The snapshot of the web page is shown in the image.\"\"\"\n+\ttext = prompt.format(task=task, key_points=key_points)\n+\n+\tmessages = [\n+\t\t{'role': 'system', 'content': system_msg},\n+\t\t{\n+\t\t\t'role': 'user',\n+\t\t\t'content': [\n+\t\t\t\t{'type': 'text', 'text': text},\n+\t\t\t\t{\n+\t\t\t\t\t'type': 'image_url',\n+\t\t\t\t\t'image_url': {'url': f'data:image/jpeg;base64,{jpg_base64_str}', 'detail': 'high'},\n+\t\t\t\t},\n+\t\t\t],\n+\t\t},\n+\t]\n+\tresponse = await asyncio.to_thread(model.invoke, messages)\n+\treturn response.content\n+\n+\n+async def Online_Mind2Web_eval(task, last_actions, images_path, model, score_threshold):\n+\tsystem_msg = \"\"\"You are an expert in evaluating the performance of a web navigation agent. The agent is designed to help a human user navigate a website to complete a task. Given the user's task, the agent's action history, key points for task completion, some potentially important web pages in the agent's trajectory and their reasons, your goal is to determine whether the agent has completed the task and achieved all requirements.\n+\n+Your response must strictly follow the following evaluation criteria!\n+*Important Evaluation Criteria*:\n+1: The filtered results must be displayed correctly. If filters were not properly applied (i.e., missing selection, missing confirmation, or no visible effect in results), the task is not considered successful.\n+2: You must carefully check whether these snapshots and action history meet these key points. Ensure that specific filter conditions, such as \"best,\" \"highest,\" \"cheapest,\" \"latest,\" \"most recent,\" \"lowest,\" \"closest,\" \"highest-rated,\" \"largest,\" and \"newest\" are correctly applied using the filter function(e.g., sort function).\n+3: Certain key points or requirements should be applied by the filter. Otherwise, a search with all requirements as input will be deemed a failure since it cannot guarantee that all results meet the requirements!\n+4: If the task requires filtering by a specific range of money, years, or the number of beds and bathrooms, the applied filter must exactly match the given requirement. Any deviation results in failure. To ensure the task is successful, the applied filter must precisely match the specified range without being too broad or too narrow.\n+Examples of Failure Cases:\n+- If the requirement is less than $50, but the applied filter is less than $25, it is a failure.\n+- If the requirement is $1500-$2500, but the applied filter is $2000-$2500, it is a failure.\n+- If the requirement is $25-$200, but the applied filter is $0-$200, it is a failure.\n+- If the required years are 2004-2012, but the filter applied is 2001-2012, it is a failure.\n+- If the required years are before 2015, but the applied filter is 2000-2014, it is a failure.\n+- If the task requires exactly 2 beds, but the filter applied is 2+ beds, it is a failure.\n+5: Some tasks require a submission action or a display of results to be considered successful.\n+6: If the retrieved information is invalid or empty(e.g., No match was found), but the agent has correctly performed the required action, it should still be considered successful.\n+7: If the current page already displays all available items, then applying a filter is not necessary. As long as the agent selects items that meet the requirements (e.g., the cheapest or lowest price), the task is still considered successful.\n+\n+*IMPORTANT*\n+Format your response into two lines as shown below:\n+\n+Thoughts: <your thoughts and reasoning process based on double-checking each key points and the evaluation criteria>\n+Status: \"success\" or \"failure\"\n+\"\"\"\n+\tprompt = \"\"\"User Task: {task}\n+\n+Key Points: {key_points}\n+\n+Action History:\n+{last_actions}\n+\n+The potentially important snapshots of the webpage in the agent's trajectory and their reasons:\n+{thoughts}\"\"\"\n+\n+\tkey_points = await identify_key_points(task, model)\n+\tkey_points = key_points.replace('\n\n', '\n')\n+\n+\ttry:\n+\t\tkey_points = key_points.split('**Key Points**:')[1]\n+\t\tkey_points = '\n'.join(line.lstrip() for line in key_points.splitlines())\n+\texcept IndexError:\n+\t\tkey_points = key_points.split('Key Points:')[-1]\n+\t\tkey_points = '\n'.join(line.lstrip() for line in key_points.splitlines())\n+\n+\ttasks = [judge_image(task, image_path, key_points, model) for image_path in images_path]\n+\timage_responses = await asyncio.gather(*tasks)\n+\n+\twhole_content_img = []\n+\twhole_thoughts = []\n+\trecord = []\n+\tpattern = r'[1-5]'\n+\tfor response, image_path in zip(image_responses, images_path):\n+\t\ttry:\n+\t\t\tscore_text = response.split('Score')[1]\n+\t\t\tthought = response.split('**Reasoning**:')[-1].strip().lstrip('\n').split('\n\n')[0].replace('\n', ' ')\n+\t\t\tscore = re.findall(pattern, score_text)[0]\n+\t\t\trecord.append({'Response': response, 'Score': int(score)})\n+\t\texcept Exception as e:\n+\t\t\tlogger.error(f'Error processing response: {e}')\n+\t\t\tscore = 0\n+\t\t\trecord.append({'Response': response, 'Score': 0})\n+\n+\t\tif int(score) >= score_threshold:\n+\t\t\tjpg_base64_str = encode_image(Image.open(image_path))\n+\t\t\twhole_content_img.append(\n+\t\t\t\t{'type': 'image_url', 'image_url': {'url': f'data:image/png;base64,{jpg_base64_str}', 'detail': 'high'}}\n+\t\t\t)\n+\t\t\tif thought != '':\n+\t\t\t\twhole_thoughts.append(thought)\n+\n+\twhole_content_img = whole_content_img[:MAX_IMAGE]\n+\twhole_thoughts = whole_thoughts[:MAX_IMAGE]\n+\tif len(whole_content_img) == 0:\n+\t\tprompt = \"\"\"User Task: {task}\n+\n+Key Points: {key_points}\n+\n+Action History:\n+{last_actions}\"\"\"\n+\ttext = prompt.format(\n+\t\ttask=task,\n+\t\tlast_actions='\n'.join(f'{i + 1}. {action}' for i, action in enumerate(last_actions)),\n+\t\tkey_points=key_points,\n+\t\tthoughts='\n'.join(f'{i + 1}. {thought}' for i, thought in enumerate(whole_thoughts)),\n+\t)\n+\n+\tmessages = [\n+\t\t{'role': 'system', 'content': system_msg},\n+\t\t{'role': 'user', 'content': [{'type': 'text', 'text': text}] + whole_content_img},\n+\t]\n+\treturn messages, text, system_msg, record, key_points\n+\n+\n+async def Online_Mind2Web_eval_with_retry(task, last_actions, images_path, model, score_threshold, max_retries=3):\n+\t\"\"\"\n+\tWrapper for Online_Mind2Web_eval with retry logic.\n+\n+\tArgs:\n+\t    task: The task description\n+\t    last_actions: List of actions taken\n+\t    images_path: List of image paths\n+\t    model: The model to use for evaluation\n+\t    score_threshold: Score threshold for image filtering\n+\t    max_retries: Maximum number of retry attempts\n+\n+\tReturns:\n+\t    Tuple of (messages, text, system_msg, record, key_points) or None if all retries fail\n+\t\"\"\"\n+\tfor attempt in range(max_retries):\n+\t\ttry:\n+\t\t\treturn await Online_Mind2Web_eval(task, last_actions, images_path, model, score_threshold)\n+\t\texcept Exception as e:\n+\t\t\tif attempt == max_retries - 1:  # Last attempt\n+\t\t\t\tlogger.error(f'Failed to evaluate after {max_retries} attempts. Error: {str(e)}')\n+\t\t\t\traise\n+\t\t\tlogger.warning(f'Attempt {attempt + 1} failed. Retrying... Error: {str(e)}')\n+\t\t\tawait asyncio.sleep(2**attempt)  # Exponential backoff\n+\n+\n+# ==============================================================================================================\n+\n+\n+# ==============================================================================================================\n+# A service for evaluating the performance of the agent\n+# ==============================================================================================================\n+import argparse\n+import json\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Dict, List, Optional\n+\n+from dotenv import load_dotenv\n+from langchain_openai import ChatOpenAI\n+\n+from browser_use import Agent, Browser, BrowserConfig\n+\n+\n+class Task:\n+\tdef __init__(self, task_id, confirmed_task, website, reference_length, level):\n+\t\tself.task_id = task_id\n+\t\tself.confirmed_task = confirmed_task\n+\t\tself.website = website\n+\t\tself.reference_length = reference_length\n+\t\tself.level = level\n+\n+\tdef __str__(self):\n+\t\treturn f'Task(task_id={self.task_id}, confirmed_task={self.confirmed_task}, website={self.website}, reference_length={self.reference_length}, level={self.level})'\n+\n+\tdef __repr__(self):\n+\t\treturn self.__str__()\n+\n+\n+class TaskTracker:\n+\tdef __init__(self, task_id: str, task_text: str):\n+\t\tself.task_id = task_id\n+\t\tself.task_text = task_text\n+\t\tself.result_folder = Path(f'saved_trajectories/{task_id}')\n+\t\tself.trajectory_folder = self.result_folder / 'trajectory'\n+\t\tself.step_results = []\n+\t\tself.step_counter = 0\n+\t\tself.screenshots = []\n+\t\tself.setup_folders()\n+\n+\tdef setup_folders(self):\n+\t\t\"\"\"Create the necessary folder structure\"\"\"\n+\t\tself.result_folder.mkdir(parents=True, exist_ok=True)\n+\t\tself.trajectory_folder.mkdir(parents=True, exist_ok=True)\n+\n+\tasync def on_step_start(self, agent):\n+\t\t\"\"\"Record information at the start of a step\"\"\"\n+\t\tself.current_step = {'step_number': self.step_counter, 'start_time': datetime.now().isoformat(), 'actions': []}\n+\n+\tasync def on_step_end(self, agent):\n+\t\t\"\"\"Record information at the end of a step\"\"\"\n+\t\t# Take screenshot\n+\t\tbrowser_context = agent.browser_context\n+\t\tscreenshot_b64 = await browser_context.take_screenshot()\n+\t\tscreenshot_path = self.trajectory_folder / f'step_{self.step_counter}.png'\n+\n+\t\t# Save screenshot to file\n+\t\twith open(screenshot_path, 'wb') as f:\n+\t\t\tf.write(base64.b64decode(screenshot_b64))\n+\n+\t\t# Save screenshot path\n+\t\tself.screenshots.append(str(screenshot_path))\n+\n+\t\t# Record action and result\n+\t\tif agent.state.last_result:\n+\t\t\tfor result in agent.state.last_result:\n+\t\t\t\tself.current_step['actions'].append(\n+\t\t\t\t\t{\n+\t\t\t\t\t\t'content': result.extracted_content,\n+\t\t\t\t\t\t'error': result.error,\n+\t\t\t\t\t\t'is_done': result.is_done,\n+\t\t\t\t\t\t'success': result.success,\n+\t\t\t\t\t}\n+\t\t\t\t)\n+\n+\t\t# Record end time\n+\t\tself.current_step['end_time'] = datetime.now().isoformat()\n+\t\tself.current_step['screenshot_path'] = str(screenshot_path)\n+\n+\t\t# Add to step results\n+\t\tself.step_results.append(self.current_step)\n+\t\tself.step_counter += 1\n+\n+\t\t# Save intermediate results\n+\t\tself.save_results()  # Save progress after each step\n+\n+\tdef save_results(self):\n+\t\t\"\"\"Save the consolidated results\"\"\"\n+\t\t# Create the final result object\n+\n+\t\tformatted_result = {\n+\t\t\t'task_id': self.task_id,\n+\t\t\t'task': self.task_text,\n+\t\t\t'steps': self.step_results,\n+\t\t\t'action_history': [step['actions'][-1]['content'] for step in self.step_results],\n+\t\t\t'screenshot_paths': self.screenshots,\n+\t\t\t'final_result_response': self.step_results[-1]['actions'][-1]['content']\n+\t\t\tif self.step_results[-1]['actions'][-1]['is_done']\n+\t\t\telse None,\n+\t\t\t'self_report_completed': self.step_results[-1]['actions'][-1]['is_done'],\n+\t\t\t'self_report_success': self.step_results[-1]['actions'][-1]['success'],\n+\t\t}\n+\n+\t\t# Save to file\n+\t\twith open(self.result_folder / 'result.json', 'w') as f:\n+\t\t\tjson.dump(formatted_result, f, indent=2)\n+\n+\t\treturn formatted_result\n+\n+\n+async def run_agent_with_tracing(task: Task, browser: Browser | None = None, max_steps: int = 25):\n+\ttry:\n+\t\t# Create task tracker\n+\t\ttracker = TaskTracker(task.task_id, task.confirmed_task)\n+\n+\t\tbrowser = browser or Browser()\n+\t\tllm = ChatOpenAI(\n+\t\t\tmodel='gpt-4o',\n+\t\t\ttemperature=0.0,\n+\t\t)\n+\n+\t\tagent = Agent(task=task.confirmed_task, llm=llm, browser=browser)\n+\n+\t\t# Pass our hook functions\n+\t\tresult = await agent.run(max_steps=max_steps, on_step_start=tracker.on_step_start, on_step_end=tracker.on_step_end)\n+\n+\t\t# Save final results\n+\t\tfinal_results = tracker.save_results()\n+\n+\t\treturn result\n+\tfinally:\n+\t\t# Ensure proper cleanup\n+\t\tawait asyncio.sleep(0.1)  # Give a moment for any pending tasks to complete\n+\t\tif not browser:\n+\t\t\tawait agent.close()  # This will close the browser if we created it\n+\n+\n+def judge_task_result(model, task_folder: Path, score_threshold: float = 3) -> Dict:\n+\t\"\"\"\n+\tJudge a single task result based on the success value of the final action.\n+\n+\tArgs:\n+\t    task_folder: Path to the task result folder\n+\n+\tReturns:\n+\t    Dictionary containing judgment results\n+\t\"\"\"\n+\tresult_file = task_folder / 'result.json'\n+\tif not result_file.exists():\n+\t\treturn {'task_id': task_folder.name, 'judgement': None, 'success': False, 'error': 'No result.json found', 'score': 0.0}\n+\n+\ttry:\n+\t\twith open(result_file) as f:\n+\t\t\tresult = json.load(f)\n+\n+\t\t# If a Online_Mind2Web_evaluation is already saved, we can skip the eval\n+\t\tif result.get('Online_Mind2Web_evaluation'):\n+\t\t\treturn result.get('Online_Mind2Web_evaluation')\n+\n+\t\t# Get the screenshot paths, task description, and action history\n+\t\tscreenshot_paths = result.get('screenshot_paths', [])\n+\t\ttask_description = result.get('task')\n+\t\taction_history = result.get('action_history', [])\n+\n+\t\t# Use the retry wrapper for evaluation\n+\t\ttry:\n+\t\t\teval_result = asyncio.run(\n+\t\t\t\tOnline_Mind2Web_eval_with_retry(task_description, action_history, screenshot_paths, model, score_threshold)\n+\t\t\t)\n+\n+\t\t\tif eval_result is None:\n+\t\t\t\traise Exception('Evaluation failed after all retries')\n+\n+\t\t\tmessages, text, system_msg, record, key_points = eval_result\n+\n+\t\t\t# Final steps to get judgement\n+\t\t\tjudgement = model.invoke(messages).content\n+\n+\t\t\tif 'success' in judgement.lower().split('status:')[1]:  # This is the official criteria for success\n+\t\t\t\tevaluation = {'task_id': task_folder.name, 'judgement': judgement, 'success': True, 'error': None, 'score': 1.0}\n+\t\t\telse:  # This is the official criteria for failure\n+\t\t\t\tevaluation = {'task_id': task_folder.name, 'judgement': judgement, 'success': False, 'error': None, 'score': 0.0}\n+\n+\t\t\t# Save the Online_Mind2Web_evaluation into the result.json file\n+\t\t\tresult['Online_Mind2Web_evaluation'] = evaluation\n+\t\t\twith open(result_file, 'w') as f:\n+\t\t\t\tjson.dump(result, f, indent=2)\n+\n+\t\t\treturn evaluation\n+\n+\t\texcept Exception as e:\n+\t\t\treturn {'task_id': task_folder.name, 'judgement': None, 'success': False, 'error': str(e), 'score': 0.0}",
    "repo_full_name": "browser-use/browser-use",
    "discussion_comments": [
      {
        "comment_id": "2047748335",
        "repo_full_name": "browser-use/browser-use",
        "pr_number": 1395,
        "pr_file": "eval/service.py",
        "discussion_id": "2047748335",
        "commented_code": "@@ -0,0 +1,695 @@\n+# ==============================================================================================================\n+# Documentation for this evaluation file.\n+# The import\n+\n+\n+# Here is the command to run the evaluation:\n+# python eval/service.py --parallel_runs 5 --parallel_evaluations 5 --max-steps 25 --start 0 --end 100\n+# options:\n+# --parallel_runs: Number of parallel tasks to run\n+# --max-steps: Maximum steps per task\n+# --start: Start index\n+# --end: End index (exclusive)\n+# --headless: Run in headless mode\n+\n+# Here is the command to run the evaluation only:\n+# python eval/service.py --evaluate-only\n+# options:\n+# --parallel_evaluations: Number of parallel evaluations to run\n+\n+# To run a new evaluation, you need to first clear the saved_trajectories folder.\n+# rm -rf saved_trajectories\n+# Otherwise, the evaluation will continue on from the last saved trajectory.\n+# ==============================================================================================================\n+\n+\n+# ==============================================================================================================\n+# This is the LLM as a judge evaluation system from the OSU-NLP Group paper\n+# Any adaptiations made should be explicitly stated here:\n+# Adaptations:\n+# We are using our langchain wrapper for the OpenAI API\n+# This means we changed model.generate to model.invoke. The behavior of the model should be identical.\n+# Added a Online_Mind2Web_eval_with_retry wrapper with retry logic in case of API rate limiting or other issues.\n+\n+\n+# @article{xue2025illusionprogressassessingcurrent,\n+#       title={An Illusion of Progress? Assessing the Current State of Web Agents},\n+#       author={Tianci Xue and Weijian Qi and Tianneng Shi and Chan Hee Song and Boyu Gou and Dawn Song and Huan Sun and Yu Su},\n+#       year={2025},\n+#       eprint={2504.01382},\n+#       archivePrefix={arXiv},\n+#       primaryClass={cs.AI},\n+#       url={https://arxiv.org/abs/2504.01382},\n+# }\n+\n+# @inproceedings{deng2023mind2web,\n+#  author = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},\n+#  booktitle = {Advances in Neural Information Processing Systems},\n+#  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},\n+#  pages = {28091--28114},\n+#  publisher = {Curran Associates, Inc.},\n+#  title = {Mind2Web: Towards a Generalist Agent for the Web},\n+#  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf},\n+#  volume = {36},\n+#  year = {2023}\n+# }\n+# ==============================================================================================================\n+import asyncio\n+import base64\n+import io\n+import logging\n+import re\n+\n+from PIL import Image\n+\n+MAX_IMAGE = 5\n+\n+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n+logger = logging.getLogger(__name__)\n+\n+\n+def encode_image(image):\n+\t\"\"\"Convert a PIL image to base64 string.\"\"\"\n+\tif image.mode == 'RGBA':\n+\t\timage = image.convert('RGB')\n+\tbuffered = io.BytesIO()\n+\timage.save(buffered, format='JPEG')\n+\treturn base64.b64encode(buffered.getvalue()).decode('utf-8')\n+\n+\n+async def identify_key_points(task, model):\n+\tsystem_msg = \"\"\"You are an expert tasked with analyzing a given task to identify the key points explicitly stated in the task description.\n+\n+**Objective**: Carefully analyze the task description and extract the critical elements explicitly mentioned in the task for achieving its goal.\n+\n+**Instructions**:\n+1. Read the task description carefully.\n+2. Identify and extract **key points** directly stated in the task description.\n+   - A **key point** is a critical element, condition, or step explicitly mentioned in the task description.\n+   - Do not infer or add any unstated elements.\n+   - Words such as \"best,\" \"highest,\" \"cheapest,\" \"latest,\" \"most recent,\" \"lowest,\" \"closest,\" \"highest-rated,\" \"largest,\" and \"newest\" must go through the sort function(e.g., the key point should be \"Filter by highest\").\n+\n+**Respond with**:\n+- **Key Points**: A numbered list of the explicit key points for completing this task, one per line, without explanations or additional details.\"\"\"\n+\tprompt = \"\"\"Task: {task}\"\"\"\n+\ttext = prompt.format(task=task)\n+\tmessages = [\n+\t\t{'role': 'system', 'content': system_msg},\n+\t\t{\n+\t\t\t'role': 'user',\n+\t\t\t'content': [{'type': 'text', 'text': text}],\n+\t\t},\n+\t]\n+\tresponse = await asyncio.to_thread(model.invoke, messages)\n+\treturn response.content\n+\n+\n+async def judge_image(task, image_path, key_points, model):\n+\tsystem_msg = \"\"\"You are an expert evaluator tasked with determining whether an image contains information about the necessary steps to complete a task.\n+\n+**Objective**: Analyze the provided image and decide if it shows essential steps or evidence required for completing the task. Use your reasoning to explain your decision before assigning a score.\n+\n+**Instructions**:\n+1. Provide a detailed description of the image, including its contents, visible elements, text (if any), and any notable features.\n+\n+2. Carefully examine the image and evaluate whether it contains necessary steps or evidence crucial to task completion:  \n+- Identify key points that could be relevant to task completion, such as actions, progress indicators, tool usage, applied filters, or step-by-step instructions.  \n+- Does the image show actions, progress indicators, or critical information directly related to completing the task?  \n+- Is this information indispensable for understanding or ensuring task success?\n+- If the image contains partial but relevant information, consider its usefulness rather than dismissing it outright.\n+\n+3. Provide your response in the following format:  \n+- **Reasoning**: Explain your thought process and observations. Mention specific elements in the image that indicate necessary steps, evidence, or lack thereof.  \n+- **Score**: Assign a score based on the reasoning, using the following scale:  \n+    - **1**: The image does not contain any necessary steps or relevant information.  \n+    - **2**: The image contains minimal or ambiguous information, unlikely to be essential.  \n+    - **3**: The image includes some relevant steps or hints but lacks clarity or completeness.  \n+    - **4**: The image contains important steps or evidence that are highly relevant but not fully comprehensive.  \n+    - **5**: The image clearly displays necessary steps or evidence crucial for completing the task.\n+\n+Respond with:  \n+1. **Reasoning**: [Your explanation]  \n+2. **Score**: [1-5]\"\"\"\n+\n+\tjpg_base64_str = encode_image(Image.open(image_path))\n+\n+\tprompt = \"\"\"**Task**: {task}\n+\n+**Key Points for Task Completion**: {key_points}\n+\n+The snapshot of the web page is shown in the image.\"\"\"\n+\ttext = prompt.format(task=task, key_points=key_points)\n+\n+\tmessages = [\n+\t\t{'role': 'system', 'content': system_msg},\n+\t\t{\n+\t\t\t'role': 'user',\n+\t\t\t'content': [\n+\t\t\t\t{'type': 'text', 'text': text},\n+\t\t\t\t{\n+\t\t\t\t\t'type': 'image_url',\n+\t\t\t\t\t'image_url': {'url': f'data:image/jpeg;base64,{jpg_base64_str}', 'detail': 'high'},\n+\t\t\t\t},\n+\t\t\t],\n+\t\t},\n+\t]\n+\tresponse = await asyncio.to_thread(model.invoke, messages)\n+\treturn response.content\n+\n+\n+async def Online_Mind2Web_eval(task, last_actions, images_path, model, score_threshold):\n+\tsystem_msg = \"\"\"You are an expert in evaluating the performance of a web navigation agent. The agent is designed to help a human user navigate a website to complete a task. Given the user's task, the agent's action history, key points for task completion, some potentially important web pages in the agent's trajectory and their reasons, your goal is to determine whether the agent has completed the task and achieved all requirements.\n+\n+Your response must strictly follow the following evaluation criteria!\n+*Important Evaluation Criteria*:\n+1: The filtered results must be displayed correctly. If filters were not properly applied (i.e., missing selection, missing confirmation, or no visible effect in results), the task is not considered successful.\n+2: You must carefully check whether these snapshots and action history meet these key points. Ensure that specific filter conditions, such as \"best,\" \"highest,\" \"cheapest,\" \"latest,\" \"most recent,\" \"lowest,\" \"closest,\" \"highest-rated,\" \"largest,\" and \"newest\" are correctly applied using the filter function(e.g., sort function).\n+3: Certain key points or requirements should be applied by the filter. Otherwise, a search with all requirements as input will be deemed a failure since it cannot guarantee that all results meet the requirements!\n+4: If the task requires filtering by a specific range of money, years, or the number of beds and bathrooms, the applied filter must exactly match the given requirement. Any deviation results in failure. To ensure the task is successful, the applied filter must precisely match the specified range without being too broad or too narrow.\n+Examples of Failure Cases:\n+- If the requirement is less than $50, but the applied filter is less than $25, it is a failure.\n+- If the requirement is $1500-$2500, but the applied filter is $2000-$2500, it is a failure.\n+- If the requirement is $25-$200, but the applied filter is $0-$200, it is a failure.\n+- If the required years are 2004-2012, but the filter applied is 2001-2012, it is a failure.\n+- If the required years are before 2015, but the applied filter is 2000-2014, it is a failure.\n+- If the task requires exactly 2 beds, but the filter applied is 2+ beds, it is a failure.\n+5: Some tasks require a submission action or a display of results to be considered successful.\n+6: If the retrieved information is invalid or empty(e.g., No match was found), but the agent has correctly performed the required action, it should still be considered successful.\n+7: If the current page already displays all available items, then applying a filter is not necessary. As long as the agent selects items that meet the requirements (e.g., the cheapest or lowest price), the task is still considered successful.\n+\n+*IMPORTANT*\n+Format your response into two lines as shown below:\n+\n+Thoughts: <your thoughts and reasoning process based on double-checking each key points and the evaluation criteria>\n+Status: \"success\" or \"failure\"\n+\"\"\"\n+\tprompt = \"\"\"User Task: {task}\n+\n+Key Points: {key_points}\n+\n+Action History:\n+{last_actions}\n+\n+The potentially important snapshots of the webpage in the agent's trajectory and their reasons:\n+{thoughts}\"\"\"\n+\n+\tkey_points = await identify_key_points(task, model)\n+\tkey_points = key_points.replace('\\n\\n', '\\n')\n+\n+\ttry:\n+\t\tkey_points = key_points.split('**Key Points**:')[1]\n+\t\tkey_points = '\\n'.join(line.lstrip() for line in key_points.splitlines())\n+\texcept IndexError:\n+\t\tkey_points = key_points.split('Key Points:')[-1]\n+\t\tkey_points = '\\n'.join(line.lstrip() for line in key_points.splitlines())\n+\n+\ttasks = [judge_image(task, image_path, key_points, model) for image_path in images_path]\n+\timage_responses = await asyncio.gather(*tasks)\n+\n+\twhole_content_img = []\n+\twhole_thoughts = []\n+\trecord = []\n+\tpattern = r'[1-5]'\n+\tfor response, image_path in zip(image_responses, images_path):\n+\t\ttry:\n+\t\t\tscore_text = response.split('Score')[1]\n+\t\t\tthought = response.split('**Reasoning**:')[-1].strip().lstrip('\\n').split('\\n\\n')[0].replace('\\n', ' ')\n+\t\t\tscore = re.findall(pattern, score_text)[0]\n+\t\t\trecord.append({'Response': response, 'Score': int(score)})\n+\t\texcept Exception as e:\n+\t\t\tlogger.error(f'Error processing response: {e}')\n+\t\t\tscore = 0\n+\t\t\trecord.append({'Response': response, 'Score': 0})\n+\n+\t\tif int(score) >= score_threshold:\n+\t\t\tjpg_base64_str = encode_image(Image.open(image_path))\n+\t\t\twhole_content_img.append(\n+\t\t\t\t{'type': 'image_url', 'image_url': {'url': f'data:image/png;base64,{jpg_base64_str}', 'detail': 'high'}}\n+\t\t\t)\n+\t\t\tif thought != '':\n+\t\t\t\twhole_thoughts.append(thought)\n+\n+\twhole_content_img = whole_content_img[:MAX_IMAGE]\n+\twhole_thoughts = whole_thoughts[:MAX_IMAGE]\n+\tif len(whole_content_img) == 0:\n+\t\tprompt = \"\"\"User Task: {task}\n+\n+Key Points: {key_points}\n+\n+Action History:\n+{last_actions}\"\"\"\n+\ttext = prompt.format(\n+\t\ttask=task,\n+\t\tlast_actions='\\n'.join(f'{i + 1}. {action}' for i, action in enumerate(last_actions)),\n+\t\tkey_points=key_points,\n+\t\tthoughts='\\n'.join(f'{i + 1}. {thought}' for i, thought in enumerate(whole_thoughts)),\n+\t)\n+\n+\tmessages = [\n+\t\t{'role': 'system', 'content': system_msg},\n+\t\t{'role': 'user', 'content': [{'type': 'text', 'text': text}] + whole_content_img},\n+\t]\n+\treturn messages, text, system_msg, record, key_points\n+\n+\n+async def Online_Mind2Web_eval_with_retry(task, last_actions, images_path, model, score_threshold, max_retries=3):\n+\t\"\"\"\n+\tWrapper for Online_Mind2Web_eval with retry logic.\n+\n+\tArgs:\n+\t    task: The task description\n+\t    last_actions: List of actions taken\n+\t    images_path: List of image paths\n+\t    model: The model to use for evaluation\n+\t    score_threshold: Score threshold for image filtering\n+\t    max_retries: Maximum number of retry attempts\n+\n+\tReturns:\n+\t    Tuple of (messages, text, system_msg, record, key_points) or None if all retries fail\n+\t\"\"\"\n+\tfor attempt in range(max_retries):\n+\t\ttry:\n+\t\t\treturn await Online_Mind2Web_eval(task, last_actions, images_path, model, score_threshold)\n+\t\texcept Exception as e:\n+\t\t\tif attempt == max_retries - 1:  # Last attempt\n+\t\t\t\tlogger.error(f'Failed to evaluate after {max_retries} attempts. Error: {str(e)}')\n+\t\t\t\traise\n+\t\t\tlogger.warning(f'Attempt {attempt + 1} failed. Retrying... Error: {str(e)}')\n+\t\t\tawait asyncio.sleep(2**attempt)  # Exponential backoff\n+\n+\n+# ==============================================================================================================\n+\n+\n+# ==============================================================================================================\n+# A service for evaluating the performance of the agent\n+# ==============================================================================================================\n+import argparse\n+import json\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Dict, List, Optional\n+\n+from dotenv import load_dotenv\n+from langchain_openai import ChatOpenAI\n+\n+from browser_use import Agent, Browser, BrowserConfig\n+\n+\n+class Task:\n+\tdef __init__(self, task_id, confirmed_task, website, reference_length, level):\n+\t\tself.task_id = task_id\n+\t\tself.confirmed_task = confirmed_task\n+\t\tself.website = website\n+\t\tself.reference_length = reference_length\n+\t\tself.level = level\n+\n+\tdef __str__(self):\n+\t\treturn f'Task(task_id={self.task_id}, confirmed_task={self.confirmed_task}, website={self.website}, reference_length={self.reference_length}, level={self.level})'\n+\n+\tdef __repr__(self):\n+\t\treturn self.__str__()\n+\n+\n+class TaskTracker:\n+\tdef __init__(self, task_id: str, task_text: str):\n+\t\tself.task_id = task_id\n+\t\tself.task_text = task_text\n+\t\tself.result_folder = Path(f'saved_trajectories/{task_id}')\n+\t\tself.trajectory_folder = self.result_folder / 'trajectory'\n+\t\tself.step_results = []\n+\t\tself.step_counter = 0\n+\t\tself.screenshots = []\n+\t\tself.setup_folders()\n+\n+\tdef setup_folders(self):\n+\t\t\"\"\"Create the necessary folder structure\"\"\"\n+\t\tself.result_folder.mkdir(parents=True, exist_ok=True)\n+\t\tself.trajectory_folder.mkdir(parents=True, exist_ok=True)\n+\n+\tasync def on_step_start(self, agent):\n+\t\t\"\"\"Record information at the start of a step\"\"\"\n+\t\tself.current_step = {'step_number': self.step_counter, 'start_time': datetime.now().isoformat(), 'actions': []}\n+\n+\tasync def on_step_end(self, agent):\n+\t\t\"\"\"Record information at the end of a step\"\"\"\n+\t\t# Take screenshot\n+\t\tbrowser_context = agent.browser_context\n+\t\tscreenshot_b64 = await browser_context.take_screenshot()\n+\t\tscreenshot_path = self.trajectory_folder / f'step_{self.step_counter}.png'\n+\n+\t\t# Save screenshot to file\n+\t\twith open(screenshot_path, 'wb') as f:\n+\t\t\tf.write(base64.b64decode(screenshot_b64))\n+\n+\t\t# Save screenshot path\n+\t\tself.screenshots.append(str(screenshot_path))\n+\n+\t\t# Record action and result\n+\t\tif agent.state.last_result:\n+\t\t\tfor result in agent.state.last_result:\n+\t\t\t\tself.current_step['actions'].append(\n+\t\t\t\t\t{\n+\t\t\t\t\t\t'content': result.extracted_content,\n+\t\t\t\t\t\t'error': result.error,\n+\t\t\t\t\t\t'is_done': result.is_done,\n+\t\t\t\t\t\t'success': result.success,\n+\t\t\t\t\t}\n+\t\t\t\t)\n+\n+\t\t# Record end time\n+\t\tself.current_step['end_time'] = datetime.now().isoformat()\n+\t\tself.current_step['screenshot_path'] = str(screenshot_path)\n+\n+\t\t# Add to step results\n+\t\tself.step_results.append(self.current_step)\n+\t\tself.step_counter += 1\n+\n+\t\t# Save intermediate results\n+\t\tself.save_results()  # Save progress after each step\n+\n+\tdef save_results(self):\n+\t\t\"\"\"Save the consolidated results\"\"\"\n+\t\t# Create the final result object\n+\n+\t\tformatted_result = {\n+\t\t\t'task_id': self.task_id,\n+\t\t\t'task': self.task_text,\n+\t\t\t'steps': self.step_results,\n+\t\t\t'action_history': [step['actions'][-1]['content'] for step in self.step_results],\n+\t\t\t'screenshot_paths': self.screenshots,\n+\t\t\t'final_result_response': self.step_results[-1]['actions'][-1]['content']\n+\t\t\tif self.step_results[-1]['actions'][-1]['is_done']\n+\t\t\telse None,\n+\t\t\t'self_report_completed': self.step_results[-1]['actions'][-1]['is_done'],\n+\t\t\t'self_report_success': self.step_results[-1]['actions'][-1]['success'],\n+\t\t}\n+\n+\t\t# Save to file\n+\t\twith open(self.result_folder / 'result.json', 'w') as f:\n+\t\t\tjson.dump(formatted_result, f, indent=2)\n+\n+\t\treturn formatted_result\n+\n+\n+async def run_agent_with_tracing(task: Task, browser: Browser | None = None, max_steps: int = 25):\n+\ttry:\n+\t\t# Create task tracker\n+\t\ttracker = TaskTracker(task.task_id, task.confirmed_task)\n+\n+\t\tbrowser = browser or Browser()\n+\t\tllm = ChatOpenAI(\n+\t\t\tmodel='gpt-4o',\n+\t\t\ttemperature=0.0,\n+\t\t)\n+\n+\t\tagent = Agent(task=task.confirmed_task, llm=llm, browser=browser)\n+\n+\t\t# Pass our hook functions\n+\t\tresult = await agent.run(max_steps=max_steps, on_step_start=tracker.on_step_start, on_step_end=tracker.on_step_end)\n+\n+\t\t# Save final results\n+\t\tfinal_results = tracker.save_results()\n+\n+\t\treturn result\n+\tfinally:\n+\t\t# Ensure proper cleanup\n+\t\tawait asyncio.sleep(0.1)  # Give a moment for any pending tasks to complete\n+\t\tif not browser:\n+\t\t\tawait agent.close()  # This will close the browser if we created it\n+\n+\n+def judge_task_result(model, task_folder: Path, score_threshold: float = 3) -> Dict:\n+\t\"\"\"\n+\tJudge a single task result based on the success value of the final action.\n+\n+\tArgs:\n+\t    task_folder: Path to the task result folder\n+\n+\tReturns:\n+\t    Dictionary containing judgment results\n+\t\"\"\"\n+\tresult_file = task_folder / 'result.json'\n+\tif not result_file.exists():\n+\t\treturn {'task_id': task_folder.name, 'judgement': None, 'success': False, 'error': 'No result.json found', 'score': 0.0}\n+\n+\ttry:\n+\t\twith open(result_file) as f:\n+\t\t\tresult = json.load(f)\n+\n+\t\t# If a Online_Mind2Web_evaluation is already saved, we can skip the eval\n+\t\tif result.get('Online_Mind2Web_evaluation'):\n+\t\t\treturn result.get('Online_Mind2Web_evaluation')\n+\n+\t\t# Get the screenshot paths, task description, and action history\n+\t\tscreenshot_paths = result.get('screenshot_paths', [])\n+\t\ttask_description = result.get('task')\n+\t\taction_history = result.get('action_history', [])\n+\n+\t\t# Use the retry wrapper for evaluation\n+\t\ttry:\n+\t\t\teval_result = asyncio.run(\n+\t\t\t\tOnline_Mind2Web_eval_with_retry(task_description, action_history, screenshot_paths, model, score_threshold)\n+\t\t\t)\n+\n+\t\t\tif eval_result is None:\n+\t\t\t\traise Exception('Evaluation failed after all retries')\n+\n+\t\t\tmessages, text, system_msg, record, key_points = eval_result\n+\n+\t\t\t# Final steps to get judgement\n+\t\t\tjudgement = model.invoke(messages).content\n+\n+\t\t\tif 'success' in judgement.lower().split('status:')[1]:  # This is the official criteria for success\n+\t\t\t\tevaluation = {'task_id': task_folder.name, 'judgement': judgement, 'success': True, 'error': None, 'score': 1.0}\n+\t\t\telse:  # This is the official criteria for failure\n+\t\t\t\tevaluation = {'task_id': task_folder.name, 'judgement': judgement, 'success': False, 'error': None, 'score': 0.0}\n+\n+\t\t\t# Save the Online_Mind2Web_evaluation into the result.json file\n+\t\t\tresult['Online_Mind2Web_evaluation'] = evaluation\n+\t\t\twith open(result_file, 'w') as f:\n+\t\t\t\tjson.dump(result, f, indent=2)\n+\n+\t\t\treturn evaluation\n+\n+\t\texcept Exception as e:\n+\t\t\treturn {'task_id': task_folder.name, 'judgement': None, 'success': False, 'error': str(e), 'score': 0.0}",
        "comment_created_at": "2025-04-16T20:54:51+00:00",
        "comment_author": "pirate",
        "comment_body": "As a rule I never use a bare `str(e)` or `f'{e}'`, it's hard to know what the actual error is from those because it doesn't including the error class name, this is usually better: `f'{type(e).__name__}: {e}'`\r\n\r\n```suggestion\r\n\t\t\treturn {'task_id': task_folder.name, 'judgement': None, 'success': False, 'error': f'{type(err).__name__}: {err}', 'score': 0.0}\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2015174123",
    "pr_number": 1146,
    "pr_file": "browser_use/agent/service.py",
    "created_at": "2025-03-27T00:33:53+00:00",
    "commented_code": "elif self.tool_calling_method is None:\n \t\t\tstructured_llm = self.llm.with_structured_output(self.AgentOutput, include_raw=True)\n-\t\t\tresponse: dict[str, Any] = await structured_llm.ainvoke(input_messages)  # type: ignore\n-\t\t\tparsed: AgentOutput | None = response['parsed']\n+\t\t\ttry:\n+\t\t\t\tresponse: dict[str, Any] = await structured_llm.ainvoke(input_messages)  # type: ignore\n+\t\t\t\tparsed: AgentOutput | None = response['parsed']\n+\n+\t\t\texcept Exception as e:\n+\t\t\t\tlogger.error(f'Failed to invoke model: {str(e)}')\n+\t\t\t\traise LLMException(401, 'Failed to invoke model')",
    "repo_full_name": "browser-use/browser-use",
    "discussion_comments": [
      {
        "comment_id": "2015174123",
        "repo_full_name": "browser-use/browser-use",
        "pr_number": 1146,
        "pr_file": "browser_use/agent/service.py",
        "discussion_id": "2015174123",
        "commented_code": "@@ -531,12 +545,24 @@ async def get_next_action(self, input_messages: list[BaseMessage]) -> AgentOutpu\n \n \t\telif self.tool_calling_method is None:\n \t\t\tstructured_llm = self.llm.with_structured_output(self.AgentOutput, include_raw=True)\n-\t\t\tresponse: dict[str, Any] = await structured_llm.ainvoke(input_messages)  # type: ignore\n-\t\t\tparsed: AgentOutput | None = response['parsed']\n+\t\t\ttry:\n+\t\t\t\tresponse: dict[str, Any] = await structured_llm.ainvoke(input_messages)  # type: ignore\n+\t\t\t\tparsed: AgentOutput | None = response['parsed']\n+\n+\t\t\texcept Exception as e:\n+\t\t\t\tlogger.error(f'Failed to invoke model: {str(e)}')\n+\t\t\t\traise LLMException(401, 'Failed to invoke model')",
        "comment_created_at": "2025-03-27T00:33:53+00:00",
        "comment_author": "pirate",
        "comment_body": "```suggestion\r\n\t\t\t\traise LLMException(401, 'LLM API call failed') from e\r\n```\r\nalways preserve the original error in the traceback with `from e`",
        "pr_file_module": null
      }
    ]
  }
]