[
  {
    "discussion_id": "2099686902",
    "pr_number": 1718,
    "pr_file": "browser_use/agent/service.py",
    "created_at": "2025-05-21T08:27:42+00:00",
    "commented_code": "self.DoneActionModel = self.controller.registry.create_action_model(include_actions=['done'])\n \t\tself.DoneAgentOutput = AgentOutput.type_with_custom_actions(self.DoneActionModel)\n \n-\tdef _set_tool_calling_method(self) -> ToolCallingMethod | None:\n-\t\ttool_calling_method = self.settings.tool_calling_method\n-\t\tif tool_calling_method == 'auto':\n-\t\t\tif is_model_without_tool_support(self.model_name):\n-\t\t\t\treturn 'raw'\n-\t\t\telif self.chat_model_library == 'ChatGoogleGenerativeAI':\n-\t\t\t\treturn None\n-\t\t\telif self.chat_model_library == 'ChatOpenAI':\n-\t\t\t\treturn 'function_calling'\n-\t\t\telif self.chat_model_library == 'AzureChatOpenAI':\n-\t\t\t\t# Azure OpenAI API requires 'tools' parameter for GPT-4\n-\t\t\t\t# The error 'content must be either a string or an array' occurs when\n-\t\t\t\t# the API expects a tools array but gets something else\n-\t\t\t\tif 'gpt-4' in self.model_name.lower():\n-\t\t\t\t\treturn 'tools'\n-\t\t\t\telse:\n-\t\t\t\t\treturn 'function_calling'\n+\tdef _test_tool_calling_method(self, method: str) -> bool:\n+\t\t\"\"\"Test if a specific tool calling method works with the current LLM.\"\"\"\n+\t\ttry:\n+\t\t\t# Create a simple test message\n+\t\t\ttest_message = HumanMessage(content='Hello, world!')\n+\n+\t\t\tif method == 'raw':\n+\t\t\t\t# For raw mode, we just check if we can invoke the model",
    "repo_full_name": "browser-use/browser-use",
    "discussion_comments": [
      {
        "comment_id": "2099686902",
        "repo_full_name": "browser-use/browser-use",
        "pr_number": 1718,
        "pr_file": "browser_use/agent/service.py",
        "discussion_id": "2099686902",
        "commented_code": "@@ -396,27 +394,78 @@ def _setup_action_models(self) -> None:\n \t\tself.DoneActionModel = self.controller.registry.create_action_model(include_actions=['done'])\n \t\tself.DoneAgentOutput = AgentOutput.type_with_custom_actions(self.DoneActionModel)\n \n-\tdef _set_tool_calling_method(self) -> ToolCallingMethod | None:\n-\t\ttool_calling_method = self.settings.tool_calling_method\n-\t\tif tool_calling_method == 'auto':\n-\t\t\tif is_model_without_tool_support(self.model_name):\n-\t\t\t\treturn 'raw'\n-\t\t\telif self.chat_model_library == 'ChatGoogleGenerativeAI':\n-\t\t\t\treturn None\n-\t\t\telif self.chat_model_library == 'ChatOpenAI':\n-\t\t\t\treturn 'function_calling'\n-\t\t\telif self.chat_model_library == 'AzureChatOpenAI':\n-\t\t\t\t# Azure OpenAI API requires 'tools' parameter for GPT-4\n-\t\t\t\t# The error 'content must be either a string or an array' occurs when\n-\t\t\t\t# the API expects a tools array but gets something else\n-\t\t\t\tif 'gpt-4' in self.model_name.lower():\n-\t\t\t\t\treturn 'tools'\n-\t\t\t\telse:\n-\t\t\t\t\treturn 'function_calling'\n+\tdef _test_tool_calling_method(self, method: str) -> bool:\n+\t\t\"\"\"Test if a specific tool calling method works with the current LLM.\"\"\"\n+\t\ttry:\n+\t\t\t# Create a simple test message\n+\t\t\ttest_message = HumanMessage(content='Hello, world!')\n+\n+\t\t\tif method == 'raw':\n+\t\t\t\t# For raw mode, we just check if we can invoke the model",
        "comment_created_at": "2025-05-21T08:27:42+00:00",
        "comment_author": "pirate",
        "comment_body": "the test for `raw` should probably ask the model to respond with some JSON manually in the same way we do in the actual system prompt in `raw` mode.\r\n\r\nI think for the same reason we check if the model knows the capital of France, basic sanity checks are very useful to weed out low power models that struggle to follow directions + return JSON",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2099693008",
    "pr_number": 1718,
    "pr_file": "browser_use/agent/service.py",
    "created_at": "2025-05-21T08:30:35+00:00",
    "commented_code": "self.DoneActionModel = self.controller.registry.create_action_model(include_actions=['done'])\n \t\tself.DoneAgentOutput = AgentOutput.type_with_custom_actions(self.DoneActionModel)\n \n-\tdef _set_tool_calling_method(self) -> ToolCallingMethod | None:\n-\t\ttool_calling_method = self.settings.tool_calling_method\n-\t\tif tool_calling_method == 'auto':\n-\t\t\tif is_model_without_tool_support(self.model_name):\n-\t\t\t\treturn 'raw'\n-\t\t\telif self.chat_model_library == 'ChatGoogleGenerativeAI':\n-\t\t\t\treturn None\n-\t\t\telif self.chat_model_library == 'ChatOpenAI':\n-\t\t\t\treturn 'function_calling'\n-\t\t\telif self.chat_model_library == 'AzureChatOpenAI':\n-\t\t\t\t# Azure OpenAI API requires 'tools' parameter for GPT-4\n-\t\t\t\t# The error 'content must be either a string or an array' occurs when\n-\t\t\t\t# the API expects a tools array but gets something else\n-\t\t\t\tif 'gpt-4' in self.model_name.lower():\n-\t\t\t\t\treturn 'tools'\n-\t\t\t\telse:\n-\t\t\t\t\treturn 'function_calling'\n+\tdef _test_tool_calling_method(self, method: str) -> bool:\n+\t\t\"\"\"Test if a specific tool calling method works with the current LLM.\"\"\"\n+\t\ttry:\n+\t\t\t# Create a simple test message\n+\t\t\ttest_message = HumanMessage(content='Hello, world!')\n+\n+\t\t\tif method == 'raw':\n+\t\t\t\t# For raw mode, we just check if we can invoke the model\n+\t\t\t\tresponse = self.llm.invoke([test_message])\n+\t\t\t\t# Basic validation of response\n+\t\t\t\tif not response or not hasattr(response, 'content'):\n+\t\t\t\t\treturn False\n+\t\t\t\treturn True\n \t\t\telse:\n-\t\t\t\treturn None\n-\t\telse:\n-\t\t\treturn tool_calling_method\n+\t\t\t\t# For other methods, try to use structured output\n+\t\t\t\tstructured_llm = self.llm.with_structured_output(self.ActionModel, include_raw=True, method=method)\n+\t\t\t\t# Try a simple invocation to test the method\n+\t\t\t\tresponse = structured_llm.invoke([test_message])\n+\t\t\t\t# TODO: add validation of response",
    "repo_full_name": "browser-use/browser-use",
    "discussion_comments": [
      {
        "comment_id": "2099693008",
        "repo_full_name": "browser-use/browser-use",
        "pr_number": 1718,
        "pr_file": "browser_use/agent/service.py",
        "discussion_id": "2099693008",
        "commented_code": "@@ -396,27 +394,78 @@ def _setup_action_models(self) -> None:\n \t\tself.DoneActionModel = self.controller.registry.create_action_model(include_actions=['done'])\n \t\tself.DoneAgentOutput = AgentOutput.type_with_custom_actions(self.DoneActionModel)\n \n-\tdef _set_tool_calling_method(self) -> ToolCallingMethod | None:\n-\t\ttool_calling_method = self.settings.tool_calling_method\n-\t\tif tool_calling_method == 'auto':\n-\t\t\tif is_model_without_tool_support(self.model_name):\n-\t\t\t\treturn 'raw'\n-\t\t\telif self.chat_model_library == 'ChatGoogleGenerativeAI':\n-\t\t\t\treturn None\n-\t\t\telif self.chat_model_library == 'ChatOpenAI':\n-\t\t\t\treturn 'function_calling'\n-\t\t\telif self.chat_model_library == 'AzureChatOpenAI':\n-\t\t\t\t# Azure OpenAI API requires 'tools' parameter for GPT-4\n-\t\t\t\t# The error 'content must be either a string or an array' occurs when\n-\t\t\t\t# the API expects a tools array but gets something else\n-\t\t\t\tif 'gpt-4' in self.model_name.lower():\n-\t\t\t\t\treturn 'tools'\n-\t\t\t\telse:\n-\t\t\t\t\treturn 'function_calling'\n+\tdef _test_tool_calling_method(self, method: str) -> bool:\n+\t\t\"\"\"Test if a specific tool calling method works with the current LLM.\"\"\"\n+\t\ttry:\n+\t\t\t# Create a simple test message\n+\t\t\ttest_message = HumanMessage(content='Hello, world!')\n+\n+\t\t\tif method == 'raw':\n+\t\t\t\t# For raw mode, we just check if we can invoke the model\n+\t\t\t\tresponse = self.llm.invoke([test_message])\n+\t\t\t\t# Basic validation of response\n+\t\t\t\tif not response or not hasattr(response, 'content'):\n+\t\t\t\t\treturn False\n+\t\t\t\treturn True\n \t\t\telse:\n-\t\t\t\treturn None\n-\t\telse:\n-\t\t\treturn tool_calling_method\n+\t\t\t\t# For other methods, try to use structured output\n+\t\t\t\tstructured_llm = self.llm.with_structured_output(self.ActionModel, include_raw=True, method=method)\n+\t\t\t\t# Try a simple invocation to test the method\n+\t\t\t\tresponse = structured_llm.invoke([test_message])\n+\t\t\t\t# TODO: add validation of response",
        "comment_created_at": "2025-05-21T08:30:35+00:00",
        "comment_author": "pirate",
        "comment_body": "can you try and keep the `what is the capital of France?` question + answer check here as part of the tool call prompt & validation logic? we've found it very useful in practice to set a minimum intelligence bar for models, it really reduces the support load from people trying to run very low-power or broken models",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2062710905",
    "pr_number": 1495,
    "pr_file": "browser_use/agent/service.py",
    "created_at": "2025-04-27T20:01:51+00:00",
    "commented_code": "try:\n \t\t\t\tmodel_output = await self.get_next_action(input_messages)\n+\t\t\t\tif (\n+\t\t\t\t\tnot model_output.action\n+\t\t\t\t\tor not isinstance(model_output.action, list)\n+\t\t\t\t\tor all(action.model_dump() == {} for action in model_output.action)\n+\t\t\t\t):\n+\t\t\t\t\tlogger.warning('Model returned empty action. Retrying...')\n+\n+\t\t\t\t\tclarification_message = HumanMessage(\n+\t\t\t\t\t\tcontent='You forgot to return an action. Please respond only with a valid JSON action according to the expected format.'\n+\t\t\t\t\t)\n+\n+\t\t\t\t\tretry_messages = input_messages + [clarification_message]\n+\t\t\t\t\tmodel_output = await self.get_next_action(retry_messages)\n+\n+\t\t\t\t\tif not model_output.action or all(action.model_dump() == {} for action in model_output.action):\n+\t\t\t\t\t\tlogger.warning('Model still returned empty after retry. Inserting safe noop action.')\n+\t\t\t\t\t\taction_instance = self.ActionModel(\n+\t\t\t\t\t\t\t**{'done': {'success': False, 'text': 'No action returned, safe exit.'}}\n+\t\t\t\t\t\t)",
    "repo_full_name": "browser-use/browser-use",
    "discussion_comments": [
      {
        "comment_id": "2062710905",
        "repo_full_name": "browser-use/browser-use",
        "pr_number": 1495,
        "pr_file": "browser_use/agent/service.py",
        "discussion_id": "2062710905",
        "commented_code": "@@ -472,6 +472,26 @@ async def step(self, step_info: Optional[AgentStepInfo] = None) -> None:\n \n \t\t\ttry:\n \t\t\t\tmodel_output = await self.get_next_action(input_messages)\n+\t\t\t\tif (\n+\t\t\t\t\tnot model_output.action\n+\t\t\t\t\tor not isinstance(model_output.action, list)\n+\t\t\t\t\tor all(action.model_dump() == {} for action in model_output.action)\n+\t\t\t\t):\n+\t\t\t\t\tlogger.warning('Model returned empty action. Retrying...')\n+\n+\t\t\t\t\tclarification_message = HumanMessage(\n+\t\t\t\t\t\tcontent='You forgot to return an action. Please respond only with a valid JSON action according to the expected format.'\n+\t\t\t\t\t)\n+\n+\t\t\t\t\tretry_messages = input_messages + [clarification_message]\n+\t\t\t\t\tmodel_output = await self.get_next_action(retry_messages)\n+\n+\t\t\t\t\tif not model_output.action or all(action.model_dump() == {} for action in model_output.action):\n+\t\t\t\t\t\tlogger.warning('Model still returned empty after retry. Inserting safe noop action.')\n+\t\t\t\t\t\taction_instance = self.ActionModel(\n+\t\t\t\t\t\t\t**{'done': {'success': False, 'text': 'No action returned, safe exit.'}}\n+\t\t\t\t\t\t)",
        "comment_created_at": "2025-04-27T20:01:51+00:00",
        "comment_author": "pirate",
        "comment_body": "```suggestion\r\n\t\t\t\t\t\taction_instance = self.ActionModel(done={\r\n\t\t\t\t\t\t\t'success': False,\r\n\t\t\t\t\t\t\t'text': 'No next action returned by LLM!',\r\n\t\t\t\t\t\t})\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1930108833",
    "pr_number": 267,
    "pr_file": "browser_use/agent/planning_manager/service.py",
    "created_at": "2025-01-27T08:02:19+00:00",
    "commented_code": "+from __future__ import annotations\n+\n+import logging\n+from typing import Optional, Any\n+\n+from langchain_core.language_models import BaseChatModel\n+from langchain_core.messages import SystemMessage, HumanMessage\n+\n+from browser_use.agent.planning_manager.views import TaskPlan\n+\n+logger = logging.getLogger(__name__)\n+\n+class PlanningManager:\n+    def __init__(\n+        self,\n+        llm: BaseChatModel,\n+    ):\n+        self.llm = llm\n+\n+    def _get_planning_system_prompt(self) -> SystemMessage:\n+        \"\"\"Get system prompt for task planning\"\"\"\n+        return SystemMessage(content=\"\"\"You are a task planning assistant. Your role is to:\n+1. Analyze the given task description\n+2. Create a well-structured, clear version of the task\n+3. Add relevant metadata like tags and difficulty estimation\n+\n+Respond with a structured task plan that includes:\n+- A reformulated task description that is clear and actionable\n+- A short summary of the task\n+- Relevant tags/categories\n+- Estimated difficulty (1-10)\n+\n+Keep the reformulated task focused and specific while maintaining all important details from the original task.\"\"\")\n+",
    "repo_full_name": "browser-use/browser-use",
    "discussion_comments": [
      {
        "comment_id": "1930108833",
        "repo_full_name": "browser-use/browser-use",
        "pr_number": 267,
        "pr_file": "browser_use/agent/planning_manager/service.py",
        "discussion_id": "1930108833",
        "commented_code": "@@ -0,0 +1,56 @@\n+from __future__ import annotations\n+\n+import logging\n+from typing import Optional, Any\n+\n+from langchain_core.language_models import BaseChatModel\n+from langchain_core.messages import SystemMessage, HumanMessage\n+\n+from browser_use.agent.planning_manager.views import TaskPlan\n+\n+logger = logging.getLogger(__name__)\n+\n+class PlanningManager:\n+    def __init__(\n+        self,\n+        llm: BaseChatModel,\n+    ):\n+        self.llm = llm\n+\n+    def _get_planning_system_prompt(self) -> SystemMessage:\n+        \"\"\"Get system prompt for task planning\"\"\"\n+        return SystemMessage(content=\"\"\"You are a task planning assistant. Your role is to:\n+1. Analyze the given task description\n+2. Create a well-structured, clear version of the task\n+3. Add relevant metadata like tags and difficulty estimation\n+\n+Respond with a structured task plan that includes:\n+- A reformulated task description that is clear and actionable\n+- A short summary of the task\n+- Relevant tags/categories\n+- Estimated difficulty (1-10)\n+\n+Keep the reformulated task focused and specific while maintaining all important details from the original task.\"\"\")\n+",
        "comment_created_at": "2025-01-27T08:02:19+00:00",
        "comment_author": "PaperBoardOfficial",
        "comment_body": "Also we can add a validation function just to have a check if the llm has added some new info:\r\n\r\n```\r\ndef _validate_task_plan(self, raw_task: str, task_plan: TaskPlan) -> bool:\r\n        \"\"\"Validate the task plan to ensure no new information is introduced\"\"\"\r\n        # Check if the reformulated task contains any new information\r\n        if raw_task not in task_plan.task:\r\n            logger.warning(\"Reformulated task contains new information\")\r\n            return False\r\n        return True\r\n```",
        "pr_file_module": null
      }
    ]
  }
]