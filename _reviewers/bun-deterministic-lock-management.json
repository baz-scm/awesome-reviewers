[
  {
    "discussion_id": "2132861567",
    "pr_number": 19940,
    "pr_file": "src/bun.js/web_worker.zig",
    "created_at": "2025-06-06T20:19:23+00:00",
    "commented_code": "vm_to_deinit = vm;\n     }\n     var arena = this.arena;\n-\n+    this.lifecycle_handle.?.onTermination();\n     WebWorker__dispatchExit(globalObject, cpp_worker, exit_code);\n     if (loop) |loop_| {\n         loop_.internal_loop_data.jsc_vm = null;\n     }\n \n     bun.uws.onThreadExit();\n-    this.deinit();\n+    this.freeWithoutDeinit();\n \n     if (vm_to_deinit) |vm| {\n         vm.deinit(); // NOTE: deinit here isn't implemented, so freeing workers will leak the vm.\n     }\n     bun.deleteAllPoolsForThreadExit();\n+\n     if (arena) |*arena_| {\n         arena_.deinit();\n     }\n \n+    this.deref();\n     bun.exitThread();\n }\n \n+pub export fn WebWorkerLifecycleHandle__requestTermination(handle: ?*WebWorkerLifecycleHandle) void {\n+    if (handle) |h| {\n+        h.requestTermination();\n+    }\n+}\n+\n+/// Manages the complex timing surrounding web worker creation and destruction\n+const WebWorkerLifecycleHandle = struct {\n+    const RefCount = bun.ptr.ThreadSafeRefCount(@This(), \"ref_count\", WebWorkerLifecycleHandle.deinit, .{});\n+    pub const ref = WebWorkerLifecycleHandle.RefCount.ref;\n+    pub const deref = WebWorkerLifecycleHandle.RefCount.deref;\n+\n+    mutex: bun.Mutex = .{},\n+    worker: ?*WebWorker = null,\n+    requested_terminate: std.atomic.Value(bool) = .init(false),\n+    ref_count: WebWorkerLifecycleHandle.RefCount,\n+\n+    pub const new = bun.TrivialNew(WebWorkerLifecycleHandle);\n+\n+    pub fn createWebWorker(\n+        cpp_worker: *void,\n+        parent: *jsc.VirtualMachine,\n+        name_str: bun.String,\n+        specifier_str: bun.String,\n+        error_message: *bun.String,\n+        parent_context_id: u32,\n+        this_context_id: u32,\n+        mini: bool,\n+        default_unref: bool,\n+        eval_mode: bool,\n+        argv_ptr: ?[*]WTFStringImpl,\n+        argv_len: usize,\n+        inherit_execArgv: bool,\n+        execArgv_ptr: ?[*]WTFStringImpl,\n+        execArgv_len: usize,\n+        preload_modules_ptr: ?[*]bun.String,\n+        preload_modules_len: usize,\n+    ) callconv(.c) *WebWorkerLifecycleHandle {\n+        const worker = create(cpp_worker, parent, name_str, specifier_str, error_message, parent_context_id, this_context_id, mini, default_unref, eval_mode, argv_ptr, argv_len, inherit_execArgv, execArgv_ptr, execArgv_len, preload_modules_ptr, preload_modules_len);\n+        const handle = WebWorkerLifecycleHandle.new(.{\n+            .worker = worker,\n+            .ref_count = .init(),\n+        });\n+        worker.?.lifecycle_handle = handle;\n+        return handle;\n+    }\n+\n+    pub fn deinit(this: *WebWorkerLifecycleHandle) void {\n+        bun.destroy(this);\n+    }\n+\n+    pub fn requestTermination(self: *WebWorkerLifecycleHandle) void {\n+        if (self.requested_terminate.load(.acquire)) {\n+            return;\n+        }\n+\n+        self.ref();\n+        self.mutex.lock();",
    "repo_full_name": "oven-sh/bun",
    "discussion_comments": [
      {
        "comment_id": "2132861567",
        "repo_full_name": "oven-sh/bun",
        "pr_number": 19940,
        "pr_file": "src/bun.js/web_worker.zig",
        "discussion_id": "2132861567",
        "commented_code": "@@ -593,31 +619,129 @@ pub fn exitAndDeinit(this: *WebWorker) noreturn {\n         vm_to_deinit = vm;\n     }\n     var arena = this.arena;\n-\n+    this.lifecycle_handle.?.onTermination();\n     WebWorker__dispatchExit(globalObject, cpp_worker, exit_code);\n     if (loop) |loop_| {\n         loop_.internal_loop_data.jsc_vm = null;\n     }\n \n     bun.uws.onThreadExit();\n-    this.deinit();\n+    this.freeWithoutDeinit();\n \n     if (vm_to_deinit) |vm| {\n         vm.deinit(); // NOTE: deinit here isn't implemented, so freeing workers will leak the vm.\n     }\n     bun.deleteAllPoolsForThreadExit();\n+\n     if (arena) |*arena_| {\n         arena_.deinit();\n     }\n \n+    this.deref();\n     bun.exitThread();\n }\n \n+pub export fn WebWorkerLifecycleHandle__requestTermination(handle: ?*WebWorkerLifecycleHandle) void {\n+    if (handle) |h| {\n+        h.requestTermination();\n+    }\n+}\n+\n+/// Manages the complex timing surrounding web worker creation and destruction\n+const WebWorkerLifecycleHandle = struct {\n+    const RefCount = bun.ptr.ThreadSafeRefCount(@This(), \"ref_count\", WebWorkerLifecycleHandle.deinit, .{});\n+    pub const ref = WebWorkerLifecycleHandle.RefCount.ref;\n+    pub const deref = WebWorkerLifecycleHandle.RefCount.deref;\n+\n+    mutex: bun.Mutex = .{},\n+    worker: ?*WebWorker = null,\n+    requested_terminate: std.atomic.Value(bool) = .init(false),\n+    ref_count: WebWorkerLifecycleHandle.RefCount,\n+\n+    pub const new = bun.TrivialNew(WebWorkerLifecycleHandle);\n+\n+    pub fn createWebWorker(\n+        cpp_worker: *void,\n+        parent: *jsc.VirtualMachine,\n+        name_str: bun.String,\n+        specifier_str: bun.String,\n+        error_message: *bun.String,\n+        parent_context_id: u32,\n+        this_context_id: u32,\n+        mini: bool,\n+        default_unref: bool,\n+        eval_mode: bool,\n+        argv_ptr: ?[*]WTFStringImpl,\n+        argv_len: usize,\n+        inherit_execArgv: bool,\n+        execArgv_ptr: ?[*]WTFStringImpl,\n+        execArgv_len: usize,\n+        preload_modules_ptr: ?[*]bun.String,\n+        preload_modules_len: usize,\n+    ) callconv(.c) *WebWorkerLifecycleHandle {\n+        const worker = create(cpp_worker, parent, name_str, specifier_str, error_message, parent_context_id, this_context_id, mini, default_unref, eval_mode, argv_ptr, argv_len, inherit_execArgv, execArgv_ptr, execArgv_len, preload_modules_ptr, preload_modules_len);\n+        const handle = WebWorkerLifecycleHandle.new(.{\n+            .worker = worker,\n+            .ref_count = .init(),\n+        });\n+        worker.?.lifecycle_handle = handle;\n+        return handle;\n+    }\n+\n+    pub fn deinit(this: *WebWorkerLifecycleHandle) void {\n+        bun.destroy(this);\n+    }\n+\n+    pub fn requestTermination(self: *WebWorkerLifecycleHandle) void {\n+        if (self.requested_terminate.load(.acquire)) {\n+            return;\n+        }\n+\n+        self.ref();\n+        self.mutex.lock();",
        "comment_created_at": "2025-06-06T20:19:23+00:00",
        "comment_author": "190n",
        "comment_body": "add\r\n\r\n```zig\r\ndefer self.mutex.unlock();\r\ndefer self.deref();\r\n```\r\n\r\nand then we can lose the copies of this cleanup along the exit paths",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2132866109",
    "pr_number": 19940,
    "pr_file": "src/bun.js/web_worker.zig",
    "created_at": "2025-06-06T20:23:31+00:00",
    "commented_code": "vm_to_deinit = vm;\n     }\n     var arena = this.arena;\n-\n+    this.lifecycle_handle.?.onTermination();\n     WebWorker__dispatchExit(globalObject, cpp_worker, exit_code);\n     if (loop) |loop_| {\n         loop_.internal_loop_data.jsc_vm = null;\n     }\n \n     bun.uws.onThreadExit();\n-    this.deinit();\n+    this.freeWithoutDeinit();\n \n     if (vm_to_deinit) |vm| {\n         vm.deinit(); // NOTE: deinit here isn't implemented, so freeing workers will leak the vm.\n     }\n     bun.deleteAllPoolsForThreadExit();\n+\n     if (arena) |*arena_| {\n         arena_.deinit();\n     }\n \n+    this.deref();\n     bun.exitThread();\n }\n \n+pub export fn WebWorkerLifecycleHandle__requestTermination(handle: ?*WebWorkerLifecycleHandle) void {\n+    if (handle) |h| {\n+        h.requestTermination();\n+    }\n+}\n+\n+/// Manages the complex timing surrounding web worker creation and destruction\n+const WebWorkerLifecycleHandle = struct {\n+    const RefCount = bun.ptr.ThreadSafeRefCount(@This(), \"ref_count\", WebWorkerLifecycleHandle.deinit, .{});\n+    pub const ref = WebWorkerLifecycleHandle.RefCount.ref;\n+    pub const deref = WebWorkerLifecycleHandle.RefCount.deref;\n+\n+    mutex: bun.Mutex = .{},\n+    worker: ?*WebWorker = null,\n+    requested_terminate: std.atomic.Value(bool) = .init(false),\n+    ref_count: WebWorkerLifecycleHandle.RefCount,\n+\n+    pub const new = bun.TrivialNew(WebWorkerLifecycleHandle);\n+\n+    pub fn createWebWorker(\n+        cpp_worker: *void,\n+        parent: *jsc.VirtualMachine,\n+        name_str: bun.String,\n+        specifier_str: bun.String,\n+        error_message: *bun.String,\n+        parent_context_id: u32,\n+        this_context_id: u32,\n+        mini: bool,\n+        default_unref: bool,\n+        eval_mode: bool,\n+        argv_ptr: ?[*]WTFStringImpl,\n+        argv_len: usize,\n+        inherit_execArgv: bool,\n+        execArgv_ptr: ?[*]WTFStringImpl,\n+        execArgv_len: usize,\n+        preload_modules_ptr: ?[*]bun.String,\n+        preload_modules_len: usize,\n+    ) callconv(.c) *WebWorkerLifecycleHandle {\n+        const worker = create(cpp_worker, parent, name_str, specifier_str, error_message, parent_context_id, this_context_id, mini, default_unref, eval_mode, argv_ptr, argv_len, inherit_execArgv, execArgv_ptr, execArgv_len, preload_modules_ptr, preload_modules_len);\n+        const handle = WebWorkerLifecycleHandle.new(.{\n+            .worker = worker,\n+            .ref_count = .init(),\n+        });\n+        worker.?.lifecycle_handle = handle;\n+        return handle;\n+    }\n+\n+    pub fn deinit(this: *WebWorkerLifecycleHandle) void {\n+        bun.destroy(this);\n+    }\n+\n+    pub fn requestTermination(self: *WebWorkerLifecycleHandle) void {\n+        if (self.requested_terminate.load(.acquire)) {\n+            return;\n+        }\n+\n+        self.ref();\n+        self.mutex.lock();\n+\n+        if (self.requested_terminate.swap(true, .monotonic)) {\n+            self.mutex.unlock();\n+            self.deref();\n+            return;\n+        }\n+\n+        if (self.worker) |worker| {\n+            self.worker = null;\n+            worker.notifyNeedTermination();\n+            self.mutex.unlock();\n+            worker.deref();\n+        } else {\n+            self.mutex.unlock();\n+            // Let the reference counting system handle deinitialization\n+            self.deref();\n+        }\n+\n+        self.deref();\n+    }\n+\n+    pub fn onTermination(self: *WebWorkerLifecycleHandle) void {\n+        self.ref();\n+        self.mutex.lock();",
    "repo_full_name": "oven-sh/bun",
    "discussion_comments": [
      {
        "comment_id": "2132866109",
        "repo_full_name": "oven-sh/bun",
        "pr_number": 19940,
        "pr_file": "src/bun.js/web_worker.zig",
        "discussion_id": "2132866109",
        "commented_code": "@@ -593,31 +619,129 @@ pub fn exitAndDeinit(this: *WebWorker) noreturn {\n         vm_to_deinit = vm;\n     }\n     var arena = this.arena;\n-\n+    this.lifecycle_handle.?.onTermination();\n     WebWorker__dispatchExit(globalObject, cpp_worker, exit_code);\n     if (loop) |loop_| {\n         loop_.internal_loop_data.jsc_vm = null;\n     }\n \n     bun.uws.onThreadExit();\n-    this.deinit();\n+    this.freeWithoutDeinit();\n \n     if (vm_to_deinit) |vm| {\n         vm.deinit(); // NOTE: deinit here isn't implemented, so freeing workers will leak the vm.\n     }\n     bun.deleteAllPoolsForThreadExit();\n+\n     if (arena) |*arena_| {\n         arena_.deinit();\n     }\n \n+    this.deref();\n     bun.exitThread();\n }\n \n+pub export fn WebWorkerLifecycleHandle__requestTermination(handle: ?*WebWorkerLifecycleHandle) void {\n+    if (handle) |h| {\n+        h.requestTermination();\n+    }\n+}\n+\n+/// Manages the complex timing surrounding web worker creation and destruction\n+const WebWorkerLifecycleHandle = struct {\n+    const RefCount = bun.ptr.ThreadSafeRefCount(@This(), \"ref_count\", WebWorkerLifecycleHandle.deinit, .{});\n+    pub const ref = WebWorkerLifecycleHandle.RefCount.ref;\n+    pub const deref = WebWorkerLifecycleHandle.RefCount.deref;\n+\n+    mutex: bun.Mutex = .{},\n+    worker: ?*WebWorker = null,\n+    requested_terminate: std.atomic.Value(bool) = .init(false),\n+    ref_count: WebWorkerLifecycleHandle.RefCount,\n+\n+    pub const new = bun.TrivialNew(WebWorkerLifecycleHandle);\n+\n+    pub fn createWebWorker(\n+        cpp_worker: *void,\n+        parent: *jsc.VirtualMachine,\n+        name_str: bun.String,\n+        specifier_str: bun.String,\n+        error_message: *bun.String,\n+        parent_context_id: u32,\n+        this_context_id: u32,\n+        mini: bool,\n+        default_unref: bool,\n+        eval_mode: bool,\n+        argv_ptr: ?[*]WTFStringImpl,\n+        argv_len: usize,\n+        inherit_execArgv: bool,\n+        execArgv_ptr: ?[*]WTFStringImpl,\n+        execArgv_len: usize,\n+        preload_modules_ptr: ?[*]bun.String,\n+        preload_modules_len: usize,\n+    ) callconv(.c) *WebWorkerLifecycleHandle {\n+        const worker = create(cpp_worker, parent, name_str, specifier_str, error_message, parent_context_id, this_context_id, mini, default_unref, eval_mode, argv_ptr, argv_len, inherit_execArgv, execArgv_ptr, execArgv_len, preload_modules_ptr, preload_modules_len);\n+        const handle = WebWorkerLifecycleHandle.new(.{\n+            .worker = worker,\n+            .ref_count = .init(),\n+        });\n+        worker.?.lifecycle_handle = handle;\n+        return handle;\n+    }\n+\n+    pub fn deinit(this: *WebWorkerLifecycleHandle) void {\n+        bun.destroy(this);\n+    }\n+\n+    pub fn requestTermination(self: *WebWorkerLifecycleHandle) void {\n+        if (self.requested_terminate.load(.acquire)) {\n+            return;\n+        }\n+\n+        self.ref();\n+        self.mutex.lock();\n+\n+        if (self.requested_terminate.swap(true, .monotonic)) {\n+            self.mutex.unlock();\n+            self.deref();\n+            return;\n+        }\n+\n+        if (self.worker) |worker| {\n+            self.worker = null;\n+            worker.notifyNeedTermination();\n+            self.mutex.unlock();\n+            worker.deref();\n+        } else {\n+            self.mutex.unlock();\n+            // Let the reference counting system handle deinitialization\n+            self.deref();\n+        }\n+\n+        self.deref();\n+    }\n+\n+    pub fn onTermination(self: *WebWorkerLifecycleHandle) void {\n+        self.ref();\n+        self.mutex.lock();",
        "comment_created_at": "2025-06-06T20:23:31+00:00",
        "comment_author": "190n",
        "comment_body": "add\r\n\r\n```zig\r\ndefer self.mutex.unlock();\r\ndefer self.deref();\r\n```\r\n\r\nthen delete the copies along the exit paths",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2122964527",
    "pr_number": 20152,
    "pr_file": "src/async/darwin_select_fallback_thread.zig",
    "created_at": "2025-06-03T07:27:28+00:00",
    "commented_code": "+const DarwinSelectFallbackThread = @This();\n+\n+select_thread: ?std.Thread = null,\n+should_stop: std.atomic.Value(bool) = .init(false),\n+lock: bun.Mutex = .{},\n+waker: bun.Async.Waker,\n+registered_event_loops: std.AutoArrayHashMapUnmanaged(i32, std.ArrayListUnmanaged(JSC.EventLoopHandle)) = .{},\n+registered_event_loops_lock: bun.Mutex = .{},\n+\n+extern \"c\" fn darwin_select_thread_is_needed_for_fd(fd: i32) bool;\n+\n+/// Simple map for fd -> FilePoll that works with any EventLoopHandle\n+pub const Map = struct {\n+    fd_to_file_poll: std.AutoHashMapUnmanaged(i32, *FilePoll) = .{},\n+\n+    pub fn put(self: *Map, fd: i32, file_poll: *FilePoll) void {\n+        self.fd_to_file_poll.put(bun.default_allocator, fd, file_poll) catch {};\n+    }\n+\n+    pub fn remove(self: *Map, fd: i32) void {\n+        _ = self.fd_to_file_poll.remove(fd);\n+    }\n+\n+    pub fn get(self: *Map, fd: i32) ?*FilePoll {\n+        return self.fd_to_file_poll.get(fd);\n+    }\n+\n+    pub fn deinit(self: *Map) void {\n+        self.fd_to_file_poll.deinit(bun.default_allocator);\n+    }\n+};\n+\n+pub fn get() *DarwinSelectFallbackThread {\n+    const Holder = struct {\n+        pub var thread: *DarwinSelectFallbackThread = undefined;\n+        pub var select_fallback_thread_once = std.once(initFallbackThread);\n+\n+        pub fn initFallbackThread() void {\n+            thread = bun.default_allocator.create(DarwinSelectFallbackThread) catch bun.outOfMemory();\n+            thread.* = .{\n+                .waker = bun.Async.Waker.init() catch @panic(\"Unexpected error: Failed to initialize waker for select fallback thread\"),\n+            };\n+        }\n+    };\n+    Holder.select_fallback_thread_once.call();\n+\n+    return Holder.thread;\n+}\n+\n+pub fn isNeededForStdin() bool {\n+    return darwin_select_thread_is_needed_for_fd(0);\n+}\n+\n+pub fn isNeededForFd(fd: i32) bool {\n+    return darwin_select_thread_is_needed_for_fd(fd);\n+}\n+\n+pub fn register(event_loop_handle: JSC.EventLoopHandle, fd: bun.FileDescriptor) void {\n+    const this = get();\n+\n+    var needs_wake = true;\n+\n+    {\n+        this.lock.lock();\n+        defer this.lock.unlock();\n+\n+        // Add to the list\n+        const entry = this.registered_event_loops.getOrPut(bun.default_allocator, fd.cast()) catch bun.outOfMemory();\n+        if (!entry.found_existing) {\n+            entry.value_ptr.* = .{};\n+        }\n+\n+        for (entry.value_ptr.*.items) |handle| {\n+            if (handle.eq(event_loop_handle)) {\n+                return;\n+            }\n+        }\n+\n+        entry.value_ptr.*.append(bun.default_allocator, event_loop_handle) catch bun.outOfMemory();\n+    }\n+\n+    // Start select thread if not running\n+    if (this.select_thread == null) {\n+        this.select_thread = std.Thread.spawn(.{}, selectThreadMain, .{this}) catch bun.outOfMemory();\n+        this.select_thread.?.setName(\"Bun stdin select() thread\") catch {};\n+        this.select_thread.?.detach();\n+        needs_wake = false;\n+    }\n+\n+    if (needs_wake) {\n+        this.waker.wake();\n+    }\n+}\n+\n+pub fn unregister(event_loop_handle: JSC.EventLoopHandle, fd: bun.FileDescriptor) void {\n+    const this = get();\n+    {\n+        this.lock.lock();\n+        defer this.lock.unlock();\n+\n+        const entry = this.registered_event_loops.getEntry(@intCast(fd.cast())) orelse return;\n+        const index = for (entry.value_ptr.*.items, 0..) |handle, i| {\n+            if (handle.eq(event_loop_handle)) break i;\n+        } else return;\n+        _ = entry.value_ptr.*.swapRemove(index);\n+\n+        if (entry.value_ptr.items.len == 0) {\n+            _ = this.registered_event_loops.swapRemove(entry.key_ptr.*);\n+        }\n+    }\n+    this.waker.wake();\n+}\n+\n+export fn darwin_select_thread_fd_is_readable(fd: i32) void {\n+    const this = get();\n+\n+    this.registered_event_loops_lock.lock();",
    "repo_full_name": "oven-sh/bun",
    "discussion_comments": [
      {
        "comment_id": "2122964527",
        "repo_full_name": "oven-sh/bun",
        "pr_number": 20152,
        "pr_file": "src/async/darwin_select_fallback_thread.zig",
        "discussion_id": "2122964527",
        "commented_code": "@@ -0,0 +1,168 @@\n+const DarwinSelectFallbackThread = @This();\n+\n+select_thread: ?std.Thread = null,\n+should_stop: std.atomic.Value(bool) = .init(false),\n+lock: bun.Mutex = .{},\n+waker: bun.Async.Waker,\n+registered_event_loops: std.AutoArrayHashMapUnmanaged(i32, std.ArrayListUnmanaged(JSC.EventLoopHandle)) = .{},\n+registered_event_loops_lock: bun.Mutex = .{},\n+\n+extern \"c\" fn darwin_select_thread_is_needed_for_fd(fd: i32) bool;\n+\n+/// Simple map for fd -> FilePoll that works with any EventLoopHandle\n+pub const Map = struct {\n+    fd_to_file_poll: std.AutoHashMapUnmanaged(i32, *FilePoll) = .{},\n+\n+    pub fn put(self: *Map, fd: i32, file_poll: *FilePoll) void {\n+        self.fd_to_file_poll.put(bun.default_allocator, fd, file_poll) catch {};\n+    }\n+\n+    pub fn remove(self: *Map, fd: i32) void {\n+        _ = self.fd_to_file_poll.remove(fd);\n+    }\n+\n+    pub fn get(self: *Map, fd: i32) ?*FilePoll {\n+        return self.fd_to_file_poll.get(fd);\n+    }\n+\n+    pub fn deinit(self: *Map) void {\n+        self.fd_to_file_poll.deinit(bun.default_allocator);\n+    }\n+};\n+\n+pub fn get() *DarwinSelectFallbackThread {\n+    const Holder = struct {\n+        pub var thread: *DarwinSelectFallbackThread = undefined;\n+        pub var select_fallback_thread_once = std.once(initFallbackThread);\n+\n+        pub fn initFallbackThread() void {\n+            thread = bun.default_allocator.create(DarwinSelectFallbackThread) catch bun.outOfMemory();\n+            thread.* = .{\n+                .waker = bun.Async.Waker.init() catch @panic(\"Unexpected error: Failed to initialize waker for select fallback thread\"),\n+            };\n+        }\n+    };\n+    Holder.select_fallback_thread_once.call();\n+\n+    return Holder.thread;\n+}\n+\n+pub fn isNeededForStdin() bool {\n+    return darwin_select_thread_is_needed_for_fd(0);\n+}\n+\n+pub fn isNeededForFd(fd: i32) bool {\n+    return darwin_select_thread_is_needed_for_fd(fd);\n+}\n+\n+pub fn register(event_loop_handle: JSC.EventLoopHandle, fd: bun.FileDescriptor) void {\n+    const this = get();\n+\n+    var needs_wake = true;\n+\n+    {\n+        this.lock.lock();\n+        defer this.lock.unlock();\n+\n+        // Add to the list\n+        const entry = this.registered_event_loops.getOrPut(bun.default_allocator, fd.cast()) catch bun.outOfMemory();\n+        if (!entry.found_existing) {\n+            entry.value_ptr.* = .{};\n+        }\n+\n+        for (entry.value_ptr.*.items) |handle| {\n+            if (handle.eq(event_loop_handle)) {\n+                return;\n+            }\n+        }\n+\n+        entry.value_ptr.*.append(bun.default_allocator, event_loop_handle) catch bun.outOfMemory();\n+    }\n+\n+    // Start select thread if not running\n+    if (this.select_thread == null) {\n+        this.select_thread = std.Thread.spawn(.{}, selectThreadMain, .{this}) catch bun.outOfMemory();\n+        this.select_thread.?.setName(\"Bun stdin select() thread\") catch {};\n+        this.select_thread.?.detach();\n+        needs_wake = false;\n+    }\n+\n+    if (needs_wake) {\n+        this.waker.wake();\n+    }\n+}\n+\n+pub fn unregister(event_loop_handle: JSC.EventLoopHandle, fd: bun.FileDescriptor) void {\n+    const this = get();\n+    {\n+        this.lock.lock();\n+        defer this.lock.unlock();\n+\n+        const entry = this.registered_event_loops.getEntry(@intCast(fd.cast())) orelse return;\n+        const index = for (entry.value_ptr.*.items, 0..) |handle, i| {\n+            if (handle.eq(event_loop_handle)) break i;\n+        } else return;\n+        _ = entry.value_ptr.*.swapRemove(index);\n+\n+        if (entry.value_ptr.items.len == 0) {\n+            _ = this.registered_event_loops.swapRemove(entry.key_ptr.*);\n+        }\n+    }\n+    this.waker.wake();\n+}\n+\n+export fn darwin_select_thread_fd_is_readable(fd: i32) void {\n+    const this = get();\n+\n+    this.registered_event_loops_lock.lock();",
        "comment_created_at": "2025-06-03T07:27:28+00:00",
        "comment_author": "graphite-app[bot]",
        "comment_body": "There appears to be a race condition in the `darwin_select_thread_fd_is_readable` function. The code accesses `this.registered_event_loops.get(fd)` without holding the lock, but then immediately acquires `registered_event_loops_lock`. This could lead to inconsistent state if the map is modified between these operations.\n\nConsider acquiring the lock before accessing the map:\n\n```zig\nthis.registered_event_loops_lock.lock();\nvar event_loop_handles: std.ArrayListUnmanaged(JSC.EventLoopHandle) = brk: {\n    if (this.registered_event_loops.get(fd)) |*loops| {\n        break :brk loops.clone(bun.default_allocator) catch bun.outOfMemory();\n    }\n    break :brk .{};\n};\nthis.registered_event_loops_lock.unlock();\n```\n\n*Spotted by [Diamond](https://app.graphite.dev/diamond/?org=oven-sh&ref=ai-review-comment)*<i class='graphite__hidden'><br /><br />Is this helpful? React \ud83d\udc4d or \ud83d\udc4e to let us know.</i>",
        "pr_file_module": null
      }
    ]
  }
]