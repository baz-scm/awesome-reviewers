[
  {
    "discussion_id": "1818074116",
    "pr_number": 6769,
    "pr_file": "checkov/arm/graph_builder/local_graph.py",
    "created_at": "2024-10-27T12:05:14+00:00",
    "commented_code": "path=file_path,\n                     block_type=BlockType.RESOURCE,\n                     attributes=attributes,\n-                    id=f\"{resource_type}.{resource_name}\",\n+                    id=f\"{resource_type}.{resource_name}\"\n                 )\n             )\n \n     def _create_edges(self) -> None:\n-        self._create_vars_and_parameters_edges()\n-        # todo add explicit references edges\n-\n-    def _create_edge(self, element_name: str, origin_vertex_index: int, label: str) -> None:\n-        vertex_name = element_name\n-        if \".\" in vertex_name:\n-            # special case for bicep and arm elements, when properties are accessed\n-            vertex_name = vertex_name.split(\".\")[0]\n-\n-        dest_vertex_index = self.vertices_by_name.get(vertex_name)\n-        if dest_vertex_index or dest_vertex_index == 0:\n-            if origin_vertex_index == dest_vertex_index:\n-                return\n-            edge = Edge(origin_vertex_index, dest_vertex_index, label)\n-            self.edges.append(edge)\n-            self.out_edges[origin_vertex_index].append(edge)\n-            self.in_edges[dest_vertex_index].append(edge)\n+        for origin_vertex_index, vertex in enumerate(self.vertices):\n+            if 'dependsOn' in vertex.attributes:\n+                self._create_explicit_edge(origin_vertex_index, vertex.name, vertex.attributes['dependsOn'])\n+            self._create_implicit_edges(origin_vertex_index, vertex.name, vertex.attributes)\n+\n+    def _create_explicit_edge(self, origin_vertex_index: int, resource_name: str, deps: list[str]) -> None:\n+        for dep in deps:\n+            if 'resourceId' in dep:\n+                processed_dep = extract_resource_name_from_resource_id_func(dep)\n+            else:\n+                processed_dep = dep.split('/')[-1]\n+            # Check if the processed dependency exists in the map\n+            if processed_dep in self.vertices_by_name:\n+                self._create_edge(processed_dep, origin_vertex_index, f'{resource_name}->{processed_dep}')\n+            else:\n+                # Dependency not found\n+                logging.debug(f\"[ArmLocalGraph] resource dependency {processed_dep} defined in {dep} for resource\"",
    "repo_full_name": "bridgecrewio/checkov",
    "discussion_comments": [
      {
        "comment_id": "1818074116",
        "repo_full_name": "bridgecrewio/checkov",
        "pr_number": 6769,
        "pr_file": "checkov/arm/graph_builder/local_graph.py",
        "discussion_id": "1818074116",
        "commented_code": "@@ -121,28 +123,30 @@ def _create_resource_vertices(self, file_path: str, resources: list[dict[str, An\n                     path=file_path,\n                     block_type=BlockType.RESOURCE,\n                     attributes=attributes,\n-                    id=f\"{resource_type}.{resource_name}\",\n+                    id=f\"{resource_type}.{resource_name}\"\n                 )\n             )\n \n     def _create_edges(self) -> None:\n-        self._create_vars_and_parameters_edges()\n-        # todo add explicit references edges\n-\n-    def _create_edge(self, element_name: str, origin_vertex_index: int, label: str) -> None:\n-        vertex_name = element_name\n-        if \".\" in vertex_name:\n-            # special case for bicep and arm elements, when properties are accessed\n-            vertex_name = vertex_name.split(\".\")[0]\n-\n-        dest_vertex_index = self.vertices_by_name.get(vertex_name)\n-        if dest_vertex_index or dest_vertex_index == 0:\n-            if origin_vertex_index == dest_vertex_index:\n-                return\n-            edge = Edge(origin_vertex_index, dest_vertex_index, label)\n-            self.edges.append(edge)\n-            self.out_edges[origin_vertex_index].append(edge)\n-            self.in_edges[dest_vertex_index].append(edge)\n+        for origin_vertex_index, vertex in enumerate(self.vertices):\n+            if 'dependsOn' in vertex.attributes:\n+                self._create_explicit_edge(origin_vertex_index, vertex.name, vertex.attributes['dependsOn'])\n+            self._create_implicit_edges(origin_vertex_index, vertex.name, vertex.attributes)\n+\n+    def _create_explicit_edge(self, origin_vertex_index: int, resource_name: str, deps: list[str]) -> None:\n+        for dep in deps:\n+            if 'resourceId' in dep:\n+                processed_dep = extract_resource_name_from_resource_id_func(dep)\n+            else:\n+                processed_dep = dep.split('/')[-1]\n+            # Check if the processed dependency exists in the map\n+            if processed_dep in self.vertices_by_name:\n+                self._create_edge(processed_dep, origin_vertex_index, f'{resource_name}->{processed_dep}')\n+            else:\n+                # Dependency not found\n+                logging.debug(f\"[ArmLocalGraph] resource dependency {processed_dep} defined in {dep} for resource\"",
        "comment_created_at": "2024-10-27T12:05:14+00:00",
        "comment_author": "bo156",
        "comment_body": "maybe warning is better than debug? just because we won't see debug logs by default",
        "pr_file_module": null
      },
      {
        "comment_id": "1818083828",
        "repo_full_name": "bridgecrewio/checkov",
        "pr_number": 6769,
        "pr_file": "checkov/arm/graph_builder/local_graph.py",
        "discussion_id": "1818074116",
        "commented_code": "@@ -121,28 +123,30 @@ def _create_resource_vertices(self, file_path: str, resources: list[dict[str, An\n                     path=file_path,\n                     block_type=BlockType.RESOURCE,\n                     attributes=attributes,\n-                    id=f\"{resource_type}.{resource_name}\",\n+                    id=f\"{resource_type}.{resource_name}\"\n                 )\n             )\n \n     def _create_edges(self) -> None:\n-        self._create_vars_and_parameters_edges()\n-        # todo add explicit references edges\n-\n-    def _create_edge(self, element_name: str, origin_vertex_index: int, label: str) -> None:\n-        vertex_name = element_name\n-        if \".\" in vertex_name:\n-            # special case for bicep and arm elements, when properties are accessed\n-            vertex_name = vertex_name.split(\".\")[0]\n-\n-        dest_vertex_index = self.vertices_by_name.get(vertex_name)\n-        if dest_vertex_index or dest_vertex_index == 0:\n-            if origin_vertex_index == dest_vertex_index:\n-                return\n-            edge = Edge(origin_vertex_index, dest_vertex_index, label)\n-            self.edges.append(edge)\n-            self.out_edges[origin_vertex_index].append(edge)\n-            self.in_edges[dest_vertex_index].append(edge)\n+        for origin_vertex_index, vertex in enumerate(self.vertices):\n+            if 'dependsOn' in vertex.attributes:\n+                self._create_explicit_edge(origin_vertex_index, vertex.name, vertex.attributes['dependsOn'])\n+            self._create_implicit_edges(origin_vertex_index, vertex.name, vertex.attributes)\n+\n+    def _create_explicit_edge(self, origin_vertex_index: int, resource_name: str, deps: list[str]) -> None:\n+        for dep in deps:\n+            if 'resourceId' in dep:\n+                processed_dep = extract_resource_name_from_resource_id_func(dep)\n+            else:\n+                processed_dep = dep.split('/')[-1]\n+            # Check if the processed dependency exists in the map\n+            if processed_dep in self.vertices_by_name:\n+                self._create_edge(processed_dep, origin_vertex_index, f'{resource_name}->{processed_dep}')\n+            else:\n+                # Dependency not found\n+                logging.debug(f\"[ArmLocalGraph] resource dependency {processed_dep} defined in {dep} for resource\"",
        "comment_created_at": "2024-10-27T12:53:30+00:00",
        "comment_author": "omriyoffe-panw",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1366957207",
    "pr_number": 5569,
    "pr_file": "checkov/common/output/ai.py",
    "created_at": "2023-10-20T13:11:30+00:00",
    "commented_code": "return\n \n         try:\n-            completion = await openai.ChatCompletion.acreate(  # type:ignore[no-untyped-call]\n-                model=OPENAI_MODEL,\n-                messages=[\n-                    {\"role\": \"system\", \"content\": \"You are a security tool\"},\n-                    {\n-                        \"role\": \"user\",\n-                        \"content\": \"\".join(\n-                            [\n-                                f\"fix following code, which violates checkov policy '{record.check_name}':\n\",\n-                                *[line for _, line in record.code_block],\n-                            ]\n-                        ),\n-                    },\n-                    {\"role\": \"user\", \"content\": \"Explain\"},\n-                ],\n-                temperature=0,\n-                max_tokens=OPENAI_MAX_TOKENS,\n-            )\n+            # define common messages array\n+            messages = [\n+                {\"role\": \"system\", \"content\": \"You are a security tool\"},\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": \"\".join(\n+                        [\n+                            f\"fix following code, which violates checkov policy '{record.check_name}':\n\",\n+                            *[line for _, line in record.code_block],\n+                        ]\n+                    ),\n+                },\n+                {\"role\": \"user\", \"content\": \"Explain\"},\n+            ],\n+            # depends on api_type, call ChatCompletion differently\n+            logging.info(f\"[_chat_complete]: self._api_type: {self._api_type}\")\n+            if (self._api_type == 'azure'):\n+                completion = await openai.ChatCompletion.acreate(  # type:ignore[no-untyped-call]\n+                    engine=self.AZURE_OPENAI_DEPLOYMENT_NAME,\n+                    messages=messages[0],\n+                    temperature=0,\n+                    max_tokens=OPENAI_MAX_TOKENS,\n+                )\n+            else:\n+                completion = await openai.ChatCompletion.acreate(  # type:ignore[no-untyped-call]\n+                    model=OPENAI_MODEL,\n+                    messages=messages[0],\n+                    temperature=0,\n+                    max_tokens=OPENAI_MAX_TOKENS,\n+                )\n+\n+            logging.info(f\"[COMPLETION]{completion}\")",
    "repo_full_name": "bridgecrewio/checkov",
    "discussion_comments": [
      {
        "comment_id": "1366957207",
        "repo_full_name": "bridgecrewio/checkov",
        "pr_number": 5569,
        "pr_file": "checkov/common/output/ai.py",
        "discussion_id": "1366957207",
        "commented_code": "@@ -70,24 +93,38 @@ async def _chat_complete(self, record: Record) -> None:\n             return\n \n         try:\n-            completion = await openai.ChatCompletion.acreate(  # type:ignore[no-untyped-call]\n-                model=OPENAI_MODEL,\n-                messages=[\n-                    {\"role\": \"system\", \"content\": \"You are a security tool\"},\n-                    {\n-                        \"role\": \"user\",\n-                        \"content\": \"\".join(\n-                            [\n-                                f\"fix following code, which violates checkov policy '{record.check_name}':\\n\",\n-                                *[line for _, line in record.code_block],\n-                            ]\n-                        ),\n-                    },\n-                    {\"role\": \"user\", \"content\": \"Explain\"},\n-                ],\n-                temperature=0,\n-                max_tokens=OPENAI_MAX_TOKENS,\n-            )\n+            # define common messages array\n+            messages = [\n+                {\"role\": \"system\", \"content\": \"You are a security tool\"},\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": \"\".join(\n+                        [\n+                            f\"fix following code, which violates checkov policy '{record.check_name}':\\n\",\n+                            *[line for _, line in record.code_block],\n+                        ]\n+                    ),\n+                },\n+                {\"role\": \"user\", \"content\": \"Explain\"},\n+            ],\n+            # depends on api_type, call ChatCompletion differently\n+            logging.info(f\"[_chat_complete]: self._api_type: {self._api_type}\")\n+            if (self._api_type == 'azure'):\n+                completion = await openai.ChatCompletion.acreate(  # type:ignore[no-untyped-call]\n+                    engine=self.AZURE_OPENAI_DEPLOYMENT_NAME,\n+                    messages=messages[0],\n+                    temperature=0,\n+                    max_tokens=OPENAI_MAX_TOKENS,\n+                )\n+            else:\n+                completion = await openai.ChatCompletion.acreate(  # type:ignore[no-untyped-call]\n+                    model=OPENAI_MODEL,\n+                    messages=messages[0],\n+                    temperature=0,\n+                    max_tokens=OPENAI_MAX_TOKENS,\n+                )\n+\n+            logging.info(f\"[COMPLETION]{completion}\")",
        "comment_created_at": "2023-10-20T13:11:30+00:00",
        "comment_author": "gruebel",
        "comment_body": "```suggestion\r\n            logging.debug(f\"OpenAI request returned: {completion}\")\r\n```\r\nor something similar to have a direct context. Also debug level is more than enough.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1343759234",
    "pr_number": 5612,
    "pr_file": "checkov/common/bridgecrew/wrapper.py",
    "created_at": "2023-10-03T08:59:51+00:00",
    "commented_code": "timeout=timeout\n         )\n     logging.info(f\"Done persisting {len(list(itertools.chain(*graphs.values())))} graphs\")\n+\n+\n+def persist_resource_subgraph_maps(\n+        resource_subgraph_maps: dict[str, dict[str, str]],\n+        s3_client: S3Client,\n+        bucket: str,\n+        full_repo_object_key: str,\n+        timeout: int\n+) -> None:\n+    def _upload_resource_subgraph_map(check_type: str, resource_subgraph_map: dict[str, str]) -> None:\n+        s3_key = os.path.join(graphs_repo_object_key, check_type, \"multi-graph/resource_subgraph_maps/resource_subgraph_map.json\")\n+        try:\n+            _put_json_object(s3_client, resource_subgraph_map, bucket, s3_key)\n+        except Exception:\n+            logging.error(f'failed to upload resource_subgraph_map from framework {check_type} to platform', exc_info=True)\n+\n+    graphs_repo_object_key = full_repo_object_key.replace('checkov', 'graphs')[:-4]\n+    with futures.ThreadPoolExecutor() as executor:\n+        futures.wait(\n+            [executor.submit(_upload_resource_subgraph_map, check_type, resource_subgraph_map) for\n+             check_type, resource_subgraph_map in resource_subgraph_maps.items()],\n+            return_when=futures.FIRST_EXCEPTION,\n+            timeout=timeout\n+        )\n+    logging.info(f\"Done persisting {len(resource_subgraph_maps)} resource_subgraph_maps\")",
    "repo_full_name": "bridgecrewio/checkov",
    "discussion_comments": [
      {
        "comment_id": "1343759234",
        "repo_full_name": "bridgecrewio/checkov",
        "pr_number": 5612,
        "pr_file": "checkov/common/bridgecrew/wrapper.py",
        "discussion_id": "1343759234",
        "commented_code": "@@ -190,3 +190,28 @@ def _upload_graph(check_type: str, graph: LibraryGraph, _absolute_root_folder: s\n             timeout=timeout\n         )\n     logging.info(f\"Done persisting {len(list(itertools.chain(*graphs.values())))} graphs\")\n+\n+\n+def persist_resource_subgraph_maps(\n+        resource_subgraph_maps: dict[str, dict[str, str]],\n+        s3_client: S3Client,\n+        bucket: str,\n+        full_repo_object_key: str,\n+        timeout: int\n+) -> None:\n+    def _upload_resource_subgraph_map(check_type: str, resource_subgraph_map: dict[str, str]) -> None:\n+        s3_key = os.path.join(graphs_repo_object_key, check_type, \"multi-graph/resource_subgraph_maps/resource_subgraph_map.json\")\n+        try:\n+            _put_json_object(s3_client, resource_subgraph_map, bucket, s3_key)\n+        except Exception:\n+            logging.error(f'failed to upload resource_subgraph_map from framework {check_type} to platform', exc_info=True)\n+\n+    graphs_repo_object_key = full_repo_object_key.replace('checkov', 'graphs')[:-4]\n+    with futures.ThreadPoolExecutor() as executor:\n+        futures.wait(\n+            [executor.submit(_upload_resource_subgraph_map, check_type, resource_subgraph_map) for\n+             check_type, resource_subgraph_map in resource_subgraph_maps.items()],\n+            return_when=futures.FIRST_EXCEPTION,\n+            timeout=timeout\n+        )\n+    logging.info(f\"Done persisting {len(resource_subgraph_maps)} resource_subgraph_maps\")",
        "comment_created_at": "2023-10-03T08:59:51+00:00",
        "comment_author": "bo156",
        "comment_body": "Wouldn't the log be now `Done persisting <random-number> resource_subgraph_maps`? this might confuse us as we only save 1 map. maybe instead log the bucket/key/repo or something like that",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1234351382",
    "pr_number": 5217,
    "pr_file": "checkov/common/runners/runner_registry.py",
    "created_at": "2023-06-19T18:26:14+00:00",
    "commented_code": "exit_code = 1 if 1 in exit_codes else 0\n         return cast(Literal[0, 1], exit_code)\n \n+    # Define the function that will get the relay state from the Prisma Cloud Platform.\n+    def get_sso_prismacloud_url(self, report_url: str, prisma_api_url: str, token: str) -> str:\n+        url_saml_config = f\"{prisma_api_url}/saml/config\"\n+        headers = {'x-redlock-auth': token}\n+        try:\n+            response = requests.get(url_saml_config, headers=headers, timeout=10)\n+            response.raise_for_status()  # Raises an HTTPError if the response status is 4xx, 5xx\n+            data = response.json()\n+        except requests.exceptions.Timeout:\n+            print(\"The request timed out\")\n+        except requests.exceptions.TooManyRedirects:\n+            print(\"Too many redirects\")\n+        except requests.exceptions.RequestException as e:\n+            print(f\"Error while calling Prisma Cloud API: {e}\")\n+            return report_url",
    "repo_full_name": "bridgecrewio/checkov",
    "discussion_comments": [
      {
        "comment_id": "1234351382",
        "repo_full_name": "bridgecrewio/checkov",
        "pr_number": 5217,
        "pr_file": "checkov/common/runners/runner_registry.py",
        "discussion_id": "1234351382",
        "commented_code": "@@ -577,16 +579,54 @@ def print_reports(\n         exit_code = 1 if 1 in exit_codes else 0\n         return cast(Literal[0, 1], exit_code)\n \n+    # Define the function that will get the relay state from the Prisma Cloud Platform.\n+    def get_sso_prismacloud_url(self, report_url: str, prisma_api_url: str, token: str) -> str:\n+        url_saml_config = f\"{prisma_api_url}/saml/config\"\n+        headers = {'x-redlock-auth': token}\n+        try:\n+            response = requests.get(url_saml_config, headers=headers, timeout=10)\n+            response.raise_for_status()  # Raises an HTTPError if the response status is 4xx, 5xx\n+            data = response.json()\n+        except requests.exceptions.Timeout:\n+            print(\"The request timed out\")\n+        except requests.exceptions.TooManyRedirects:\n+            print(\"Too many redirects\")\n+        except requests.exceptions.RequestException as e:\n+            print(f\"Error while calling Prisma Cloud API: {e}\")\n+            return report_url",
        "comment_created_at": "2023-06-19T18:26:14+00:00",
        "comment_author": "gruebel",
        "comment_body": "instead of print use `logging.info(...)`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1269419140",
    "pr_number": 5362,
    "pr_file": "checkov/terraform/tf_parser.py",
    "created_at": "2023-07-20T12:53:55+00:00",
    "commented_code": "logging.warning(e, exc_info=False)\n         return module, tf_definitions\n \n+    def parse_hcl_module_from_multi_tf_definitions(\n+        self,\n+        tf_definitions: list[dict[TFDefinitionKey, dict[str, Any]]],\n+        source_dir: str,\n+        source: str,\n+    ) -> tuple[Module, list[dict[TFDefinitionKey, dict[str, Any]]]]:\n+        module = self.get_new_module(\n+            source_dir=source_dir,\n+            module_address_map=self.module_address_map,\n+            external_modules_source_map=self.external_modules_source_map,\n+        )\n+        self.add_tfvars_with_source_dir(module, source, source_dir)\n+        copy_of_tf_definitions = pickle_deepcopy(tf_definitions)\n+        for tf_def in copy_of_tf_definitions:\n+            for file_path, blocks in tf_def.items():\n+                for block_type in blocks:\n+                    try:\n+                        module.add_blocks(block_type, blocks[block_type], file_path, source)\n+                    except Exception as e:\n+                        logging.warning(f'Failed to add block {blocks[block_type]}. Error:')\n+                        logging.warning(e, exc_info=False)",
    "repo_full_name": "bridgecrewio/checkov",
    "discussion_comments": [
      {
        "comment_id": "1269419140",
        "repo_full_name": "bridgecrewio/checkov",
        "pr_number": 5362,
        "pr_file": "checkov/terraform/tf_parser.py",
        "discussion_id": "1269419140",
        "commented_code": "@@ -391,6 +453,29 @@ def parse_hcl_module_from_tf_definitions(\n                     logging.warning(e, exc_info=False)\n         return module, tf_definitions\n \n+    def parse_hcl_module_from_multi_tf_definitions(\n+        self,\n+        tf_definitions: list[dict[TFDefinitionKey, dict[str, Any]]],\n+        source_dir: str,\n+        source: str,\n+    ) -> tuple[Module, list[dict[TFDefinitionKey, dict[str, Any]]]]:\n+        module = self.get_new_module(\n+            source_dir=source_dir,\n+            module_address_map=self.module_address_map,\n+            external_modules_source_map=self.external_modules_source_map,\n+        )\n+        self.add_tfvars_with_source_dir(module, source, source_dir)\n+        copy_of_tf_definitions = pickle_deepcopy(tf_definitions)\n+        for tf_def in copy_of_tf_definitions:\n+            for file_path, blocks in tf_def.items():\n+                for block_type in blocks:\n+                    try:\n+                        module.add_blocks(block_type, blocks[block_type], file_path, source)\n+                    except Exception as e:\n+                        logging.warning(f'Failed to add block {blocks[block_type]}. Error:')\n+                        logging.warning(e, exc_info=False)",
        "comment_created_at": "2023-07-20T12:53:55+00:00",
        "comment_author": "gruebel",
        "comment_body": "just thinking about the log level, if maybe `info` is enough, depends on how critical it is to the user. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1269462872",
        "repo_full_name": "bridgecrewio/checkov",
        "pr_number": 5362,
        "pr_file": "checkov/terraform/tf_parser.py",
        "discussion_id": "1269419140",
        "commented_code": "@@ -391,6 +453,29 @@ def parse_hcl_module_from_tf_definitions(\n                     logging.warning(e, exc_info=False)\n         return module, tf_definitions\n \n+    def parse_hcl_module_from_multi_tf_definitions(\n+        self,\n+        tf_definitions: list[dict[TFDefinitionKey, dict[str, Any]]],\n+        source_dir: str,\n+        source: str,\n+    ) -> tuple[Module, list[dict[TFDefinitionKey, dict[str, Any]]]]:\n+        module = self.get_new_module(\n+            source_dir=source_dir,\n+            module_address_map=self.module_address_map,\n+            external_modules_source_map=self.external_modules_source_map,\n+        )\n+        self.add_tfvars_with_source_dir(module, source, source_dir)\n+        copy_of_tf_definitions = pickle_deepcopy(tf_definitions)\n+        for tf_def in copy_of_tf_definitions:\n+            for file_path, blocks in tf_def.items():\n+                for block_type in blocks:\n+                    try:\n+                        module.add_blocks(block_type, blocks[block_type], file_path, source)\n+                    except Exception as e:\n+                        logging.warning(f'Failed to add block {blocks[block_type]}. Error:')\n+                        logging.warning(e, exc_info=False)",
        "comment_created_at": "2023-07-20T13:29:05+00:00",
        "comment_author": "maxamel",
        "comment_body": "Since it shouldn't happen normally I'm OK with warning, this way we can also search for it in our logs more easily. It's not critical enough to log error in my opinion.\r\nAlso, in the original function it's logged as warning as well.",
        "pr_file_module": null
      }
    ]
  }
]