[
  {
    "discussion_id": "2268184100",
    "pr_number": 85394,
    "pr_file": "src/Server/PrometheusRequestHandler.cpp",
    "created_at": "2025-08-11T22:59:33+00:00",
    "commented_code": "response.setStatusAndReason(Poco::Net::HTTPResponse::HTTPStatus::HTTP_NO_CONTENT, Poco::Net::HTTPResponse::HTTP_REASON_NO_CONTENT);\n         response.setChunkedTransferEncoding(false);\n-        response.send();",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2268184100",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85394,
        "pr_file": "src/Server/PrometheusRequestHandler.cpp",
        "discussion_id": "2268184100",
        "commented_code": "@@ -238,7 +238,6 @@ class PrometheusRequestHandler::RemoteWriteImpl : public ImplWithContext\n \n         response.setStatusAndReason(Poco::Net::HTTPResponse::HTTPStatus::HTTP_NO_CONTENT, Poco::Net::HTTPResponse::HTTP_REASON_NO_CONTENT);\n         response.setChunkedTransferEncoding(false);\n-        response.send();",
        "comment_created_at": "2025-08-11T22:59:33+00:00",
        "comment_author": "vitlibar",
        "comment_body": "This call is superfluous because this call sends headers to the client and then these header are sent again in https://github.com/ClickHouse/ClickHouse/blob/345d553ad78932af36d1d67d7b7c127a1516ab1b/src/Server/PrometheusRequestHandler.cpp#L373\r\n\r\nThat's not allowed, so the following message occurs many times in the logs:\r\n```\r\n2025.08.11 22:11:31.233188 [ 17 ] {} <Error> PrometheusRequestHandler: Code: 210. DB::NetException: I/O error: Broken pipe, while writing to socket (172.18.0.4:9092 -> 172.18.0.3:32772). (NETWORK_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. /clickhouse/contrib/llvm-project/libcxx/include/__exception/exception.h:113: Poco::Exception::Exception(String const&, int) @ 0x0000000024a15132\r\n1. /clickhouse/src/Common/Exception.cpp:119: DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000001227ab1e\r\n2. /clickhouse/src/Common/Exception.h:122: DB::Exception::Exception(String&&, int, String, bool) @ 0x000000000b218bce\r\n3. /clickhouse/src/Common/NetException.h:26: DB::NetException::NetException<String, String, String>(int, FormatStringHelperImpl<std::type_identity<String>::type, std::type_identity<String>::type, std::type_identity<String>::type>, String&&, String&&, String&&) @ 0x000000001245c6bf\r\n4. /clickhouse/src/IO/WriteBufferFromPocoSocket.cpp:116: DB::WriteBufferFromPocoSocket::socketSendBytes(char const*, unsigned long) @ 0x000000001245d472\r\n5. /clickhouse/src/Server/HTTP/WriteBufferFromHTTPServerResponse.cpp:56: DB::WriteBufferFromHTTPServerResponse::writeHeaderProgressImpl(char const*, DB::Progress::DisplayMode) @ 0x000000001cb08dde\r\n6. /clickhouse/src/Server/HTTP/WriteBufferFromHTTPServerResponse.cpp:66: DB::WriteBufferFromHTTPServerResponse::finishSendHeaders() @ 0x000000001cb0923b\r\n7. /clickhouse/src/Server/HTTP/WriteBufferFromHTTPServerResponse.cpp:193: DB::WriteBufferFromHTTPServerResponse::finalizeImpl() @ 0x000000001cb099e6\r\n8. /clickhouse/src/IO/WriteBuffer.cpp:95: DB::WriteBuffer::finalize() @ 0x000000001237adf0\r\n9. /clickhouse/src/Server/PrometheusRequestHandler.cpp:374: DB::PrometheusRequestHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&, StrongTypedef<unsigned long, ProfileEvents::EventTag> const&) @ 0x000000001cab868f\r\n10. /clickhouse/src/Server/HTTP/HTTPServerConnection.cpp:104: DB::HTTPServerConnection::run() @ 0x000000001cb0285b\r\n11. /clickhouse/base/poco/Net/src/TCPServerConnection.cpp:40: Poco::Net::TCPServerConnection::start() @ 0x0000000024ac8c07\r\n12. /clickhouse/base/poco/Net/src/TCPServerDispatcher.cpp:115: Poco::Net::TCPServerDispatcher::run() @ 0x0000000024ac919e\r\n13. /clickhouse/base/poco/Foundation/src/ThreadPool.cpp:205: Poco::PooledThread::run() @ 0x0000000024a69abf\r\n14. /clickhouse/base/poco/Foundation/src/Thread_POSIX.cpp:341: Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000024a6724f\r\n15. ? @ 0x0000000000094ac3\r\n16. ? @ 0x0000000000126850\r\n (version 25.8.1.1)\r\n```\r\n\r\nThis PR fixes that.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2269049029",
    "pr_number": 85400,
    "pr_file": "src/Coordination/KeeperServer.cpp",
    "created_at": "2025-08-12T08:08:17+00:00",
    "commented_code": "{\n         auto asio_listener = asio_service->create_rpc_listener(state_manager->getPort(), logger, enable_ipv6);\n         if (!asio_listener)\n-            throw Exception(ErrorCodes::RAFT_ERROR, \"Cannot create interserver listener on port {}\", state_manager->getPort());\n+        {\n+            LOG_WARNING(log, \"Failed to create listener with IPv6 enabled, falling back to IPv4 only.\");\n+            asio_listener = asio_service->create_rpc_listener(\"0.0.0.0\", state_manager->getPort(), logger);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2269049029",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85400,
        "pr_file": "src/Coordination/KeeperServer.cpp",
        "discussion_id": "2269049029",
        "commented_code": "@@ -526,7 +526,13 @@ void KeeperServer::launchRaftServer(const Poco::Util::AbstractConfiguration & co\n     {\n         auto asio_listener = asio_service->create_rpc_listener(state_manager->getPort(), logger, enable_ipv6);\n         if (!asio_listener)\n-            throw Exception(ErrorCodes::RAFT_ERROR, \"Cannot create interserver listener on port {}\", state_manager->getPort());\n+        {\n+            LOG_WARNING(log, \"Failed to create listener with IPv6 enabled, falling back to IPv4 only.\");\n+            asio_listener = asio_service->create_rpc_listener(\"0.0.0.0\", state_manager->getPort(), logger);",
        "comment_created_at": "2025-08-12T08:08:17+00:00",
        "comment_author": "antonio2368",
        "comment_body": "```suggestion\r\n            asio_listener = asio_service->create_rpc_listener(state_manager->getPort(), logger, /*_enable_ipv6=*/false);\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2048532793",
    "pr_number": 79215,
    "pr_file": "src/Processors/QueryPlan/ReadFromRemote.cpp",
    "created_at": "2025-04-17T08:54:44+00:00",
    "commented_code": "pushed_down_filters]() mutable\n         -> QueryPipelineBuilder\n     {\n-        auto current_settings = my_context->getSettingsRef();",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2048532793",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 79215,
        "pr_file": "src/Processors/QueryPlan/ReadFromRemote.cpp",
        "discussion_id": "2048532793",
        "commented_code": "@@ -500,75 +500,6 @@ void ReadFromRemote::addLazyPipe(Pipes & pipes, const ClusterProxy::SelectStream\n             pushed_down_filters]() mutable\n         -> QueryPipelineBuilder\n     {\n-        auto current_settings = my_context->getSettingsRef();",
        "comment_created_at": "2025-04-17T08:54:44+00:00",
        "comment_author": "MikhailBurdukov",
        "comment_body": "`my_shard.shard_info.pool->getManyChecked..` are blocking the thread until we will get responses(or connection failures ) from all replicas in the shard. This may cause delays in the execution of requests, in case of timeouts + retries. So the `use_hedged_requests` doesn't works here. IMO we can just use  `RemoteQueryExecutor` instead of  complicated logic, my thoughts down below.\r\n\r\n\r\nPS. Changes here are not part the main goal of the PR, but it related and the test coverage this case as well. \r\nSo do not copy-paste the tests and resolving merge conflicts in the future and due to there not really much changes, decided to put it here. It is on reviewers, we can move changes to separate PR.",
        "pr_file_module": null
      },
      {
        "comment_id": "2100574430",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 79215,
        "pr_file": "src/Processors/QueryPlan/ReadFromRemote.cpp",
        "discussion_id": "2048532793",
        "commented_code": "@@ -500,75 +500,6 @@ void ReadFromRemote::addLazyPipe(Pipes & pipes, const ClusterProxy::SelectStream\n             pushed_down_filters]() mutable\n         -> QueryPipelineBuilder\n     {\n-        auto current_settings = my_context->getSettingsRef();",
        "comment_created_at": "2025-05-21T15:23:24+00:00",
        "comment_author": "MikhailBurdukov",
        "comment_body": "Moved it to separate PR #80634",
        "pr_file_module": null
      }
    ]
  }
]