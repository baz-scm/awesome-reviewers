[
  {
    "discussion_id": "2228785223",
    "pr_number": 83777,
    "pr_file": "src/DataTypes/Serializations/SerializationObjectSharedData.cpp",
    "created_at": "2025-07-24T15:01:27+00:00",
    "commented_code": "+#include <DataTypes/Serializations/SerializationObjectSharedData.h>\n+#include <DataTypes/Serializations/SerializationObjectHelpers.h>\n+#include <DataTypes/Serializations/SerializationArray.h>\n+#include <DataTypes/Serializations/SerializationNumber.h>\n+#include <DataTypes/Serializations/SerializationString.h>\n+#include <DataTypes/Serializations/getSubcolumnsDeserializationOrder.h>\n+#include <DataTypes/DataTypeObject.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Columns/ColumnMap.h>\n+#include <Columns/ColumnArray.h>\n+#include <Columns/ColumnTuple.h>\n+#include <Storages/MergeTree/ColumnsSubstreams.h>\n+#include <Core/NamesAndTypes.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+    extern const int INCORRECT_DATA;\n+    extern const int NOT_IMPLEMENTED;\n+}\n+\n+SerializationObjectSharedData::SerializationObjectSharedData(SerializationVersion serialization_version_, const DataTypePtr & dynamic_type_, size_t buckets_)\n+    : serialization_version(serialization_version_)\n+    , dynamic_type(dynamic_type_)\n+    , dynamic_serialization(dynamic_type_->getDefaultSerialization())\n+    , buckets(buckets_)\n+    , serialization_map(DataTypeObject::getTypeOfSharedData()->getDefaultSerialization())\n+{\n+}\n+\n+SerializationObjectSharedData::SerializationVersion::SerializationVersion(UInt64 version) : value(static_cast<Value>(version))\n+{\n+    checkVersion(version);\n+}\n+\n+SerializationObjectSharedData::SerializationVersion::SerializationVersion(DB::MergeTreeObjectSharedDataSerializationVersion version)\n+{\n+    switch (version)\n+    {\n+        case MergeTreeObjectSharedDataSerializationVersion::MAP:\n+            value = MAP;\n+            break;\n+        case MergeTreeObjectSharedDataSerializationVersion::MAP_WITH_BUCKETS:\n+            value = MAP_WITH_BUCKETS;\n+            break;\n+        case MergeTreeObjectSharedDataSerializationVersion::ADVANCED:\n+            value = ADVANCED;\n+            break;\n+    }\n+}\n+\n+void SerializationObjectSharedData::SerializationVersion::checkVersion(UInt64 version)\n+{\n+    if (version != MAP && version != MAP_WITH_BUCKETS && version != ADVANCED)\n+        throw Exception(ErrorCodes::INCORRECT_DATA, \"Invalid version for Object shared data serialization: {}\", version);\n+}\n+\n+struct SerializeBinaryBulkStateObjectSharedData : public ISerialization::SerializeBinaryBulkState\n+{\n+    ISerialization::SerializeBinaryBulkStatePtr map_state;\n+    std::vector<ISerialization::SerializeBinaryBulkStatePtr> bucket_map_states;\n+};\n+\n+struct DeserializeBinaryBulkStateObjectSharedData : public ISerialization::DeserializeBinaryBulkState\n+{\n+    ISerialization::DeserializeBinaryBulkStatePtr map_state;\n+    std::vector<ISerialization::DeserializeBinaryBulkStatePtr> bucket_map_states;\n+    std::vector<ISerialization::DeserializeBinaryBulkStatePtr> bucket_structure_states;\n+    /// Some granules can be partially read, we need to remember how many rows\n+    /// were already read from the last incomplete granule.\n+    size_t last_incomplete_granule_offset = 0;\n+\n+    ISerialization::DeserializeBinaryBulkStatePtr clone() const override\n+    {\n+        auto new_state = std::make_shared<DeserializeBinaryBulkStateObjectSharedData>(*this);\n+        for (size_t bucket = 0; bucket != bucket_map_states.size(); ++bucket)\n+            new_state->bucket_map_states[bucket] = bucket_map_states[bucket] ? bucket_map_states[bucket]->clone() : nullptr;\n+        for (size_t bucket = 0; bucket != bucket_structure_states.size(); ++bucket)\n+            new_state->bucket_structure_states[bucket] = bucket_structure_states[bucket] ? bucket_structure_states[bucket]->clone() : nullptr;\n+        return new_state;\n+    }\n+};\n+\n+void SerializationObjectSharedData::enumerateStreams(\n+    ISerialization::EnumerateStreamsSettings & settings,\n+    const ISerialization::StreamCallback & callback,\n+    const ISerialization::SubstreamData & data) const\n+{\n+    const auto * shared_data_state = data.deserialize_state ? checkAndGetState<DeserializeBinaryBulkStateObjectSharedData>(data.deserialize_state) : nullptr;\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        auto map_data = SubstreamData(serialization_map)\n+                            .withColumn(data.column)\n+                            .withType(data.type)\n+                            .withSerializationInfo(data.serialization_info)\n+                            .withDeserializeState(shared_data_state ? shared_data_state->map_state : nullptr);\n+\n+        serialization_map->enumerateStreams(settings, callback, map_data);\n+        return;\n+    }\n+\n+    /// Other 2 serializations MAP_WITH_BUCKETS and ADVAMCED support buckets.\n+    for (size_t bucket = 0; bucket != buckets; ++bucket)\n+    {\n+        settings.path.push_back(Substream::ObjectSharedDataBucket);\n+        settings.path.back().object_shared_data_bucket = bucket;\n+        if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+        {\n+            auto map_data = SubstreamData(serialization_map)\n+                                .withColumn(data.column)\n+                                .withType(data.type)\n+                                .withSerializationInfo(data.serialization_info)\n+                                .withDeserializeState(shared_data_state ? shared_data_state->bucket_map_states[bucket] : nullptr);\n+            serialization_map->enumerateStreams(settings, callback, map_data);\n+        }\n+        else if (serialization_version.value == SerializationObjectSharedData::SerializationVersion::ADVANCED)\n+        {\n+            if (settings.use_specialized_prefixes_and_suffixes_substreams)\n+            {\n+                settings.path.push_back(Substream::ObjectSharedDataStructurePrefix);\n+                callback(settings.path);\n+                settings.path.pop_back();\n+            }\n+            else\n+            {\n+                settings.path.push_back(Substream::ObjectSharedDataStructure);\n+                callback(settings.path);\n+                settings.path.pop_back();\n+            }\n+\n+            settings.path.push_back(Substream::ObjectSharedDataData);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            settings.path.push_back(Substream::ObjectSharedDataPathsMarks);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            settings.path.push_back(Substream::ObjectSharedDataSubstreams);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            settings.path.push_back(Substream::ObjectSharedDataSubstreamsMarks);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            settings.path.push_back(Substream::ObjectSharedDataPathsSubstreamsMetadata);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            if (settings.use_specialized_prefixes_and_suffixes_substreams)\n+            {\n+                settings.path.push_back(Substream::ObjectSharedDataStructureSuffix);\n+                callback(settings.path);\n+                settings.path.pop_back();\n+            }\n+        }\n+        else\n+        {\n+            /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+            throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"enumerateStreams is not implemented for shared data serialization version {}\", serialization_version.value);\n+        }\n+\n+        settings.path.pop_back();\n+    }\n+\n+    /// Streams related to shared data copy in ADVANCED serialization.\n+    if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        settings.path.push_back(Substream::ObjectSharedDataCopy);\n+\n+        settings.path.push_back(Substream::ObjectSharedDataCopySizes);\n+        callback(settings.path);\n+        settings.path.pop_back();\n+\n+        settings.path.push_back(Substream::ObjectSharedDataCopyPathsIndexes);\n+        callback(settings.path);\n+        settings.path.pop_back();\n+\n+        settings.path.push_back(Substream::ObjectSharedDataCopyValues);\n+        callback(settings.path);\n+        settings.path.pop_back();\n+\n+        settings.path.pop_back();\n+    }\n+}\n+\n+void SerializationObjectSharedData::serializeBinaryBulkStatePrefix(\n+    const IColumn & column,\n+    ISerialization::SerializeBinaryBulkSettings & settings,\n+    ISerialization::SerializeBinaryBulkStatePtr & state) const\n+{\n+    auto shared_data_state = std::make_shared<SerializeBinaryBulkStateObjectSharedData>();\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        serialization_map->serializeBinaryBulkStatePrefix(column, settings, shared_data_state->map_state);\n+    }\n+    else if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+    {\n+        shared_data_state->bucket_map_states.resize(buckets);\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            serialization_map->serializeBinaryBulkStatePrefix(column, settings, shared_data_state->bucket_map_states[bucket]);\n+            settings.path.pop_back();\n+        }\n+    }\n+    else if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        /// ADVANCED serialization doesn't have serialization prefix.\n+    }\n+    else\n+    {\n+        /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"enumerateStreams is not implemented for shared data serialization version {}\", serialization_version.value);\n+    }\n+\n+\n+    state = std::move(shared_data_state);\n+}\n+\n+void SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams(\n+    const IColumn & column,\n+    size_t offset,\n+    size_t limit,\n+    ISerialization::SerializeBinaryBulkSettings & settings,\n+    ISerialization::SerializeBinaryBulkStatePtr & state) const\n+{\n+    auto * shared_data_state = checkAndGetState<SerializeBinaryBulkStateObjectSharedData>(state);\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        serialization_map->serializeBinaryBulkWithMultipleStreams(column, offset, limit, settings, shared_data_state->map_state);\n+    }\n+    else if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+    {\n+        size_t end = limit && offset + limit < column.size() ? offset + limit : column.size();\n+        auto shared_data_buckets = splitSharedDataPathsToBuckets(column, offset, end, buckets);\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            serialization_map->serializeBinaryBulkWithMultipleStreams(*shared_data_buckets[bucket], 0, 0, settings, shared_data_state->bucket_map_states[bucket]);\n+            settings.path.pop_back();\n+        }\n+    }\n+    else if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        size_t end = limit && offset + limit < column.size() ? offset + limit : column.size();\n+        /// First we need to flatten all paths stored in the shared data and separate them into buckets.\n+        auto flattened_paths_buckets = flattenAndBucketSharedDataPaths(column, offset, end, dynamic_type, buckets);\n+        /// Second, write paths in each bucket separately.\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            const auto & flattened_paths = flattened_paths_buckets[bucket];\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+\n+            /// Write structure of this granule.\n+            Substream structure_stream_type = settings.use_specialized_prefixes_and_suffixes_substreams ? Substream::ObjectSharedDataStructurePrefix\n+                                                                                           : Substream::ObjectSharedDataStructure;\n+            settings.path.push_back(structure_stream_type);\n+            auto * structure_stream = settings.getter(settings.path);\n+            settings.path.pop_back();\n+\n+            if (!structure_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data structure in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            /// Write number of rows in this granule and list of all paths that we will serialize.\n+            writeVarUInt(end - offset, *structure_stream);\n+            writeVarUInt(flattened_paths.size(), *structure_stream);\n+            for (const auto & [path, _] : flattened_paths)\n+                writeStringBinary(path, *structure_stream);\n+\n+            /// Write data of flattened paths.\n+            settings.path.push_back(Substream::ObjectSharedDataData);\n+            auto * data_stream = settings.getter(settings.path);\n+            auto data_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!data_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data data in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            if (!settings.stream_mark_getter)\n+                throw Exception(ErrorCodes::LOGICAL_ERROR, \"Mark getter is not set for ADVANCED shared data serialization\");\n+\n+            /// Remember the mark of the ObjectSharedDataData stream, we will write it in the structure stream later.\n+            MarkInCompressedFile data_stream_mark = settings.stream_mark_getter(data_stream_path);\n+\n+            /// Collect mark of the ObjectSharedDataData stream for each path.\n+            std::vector<MarkInCompressedFile> paths_marks;\n+            paths_marks.reserve(flattened_paths.size());\n+            /// Collect list of substreams for each path.\n+            std::vector<std::vector<String>> paths_substreams;\n+            /// Collect mark of the ObjectSharedDataData stream for each substream of each path.\n+            std::vector<std::vector<MarkInCompressedFile>> paths_substreams_marks;\n+\n+            /// Configure serialization settings as in Compact part.\n+            SerializeBinaryBulkSettings data_serialization_settings;\n+            data_serialization_settings.data_part_type = MergeTreeDataPartType::Compact;\n+            data_serialization_settings.position_independent_encoding = true;\n+            data_serialization_settings.low_cardinality_max_dictionary_size = 0;\n+            data_serialization_settings.use_specialized_prefixes_and_suffixes_substreams = true;\n+            data_serialization_settings.use_compact_variant_discriminators_serialization = true;\n+            data_serialization_settings.dynamic_serialization_version = MergeTreeDynamicSerializationVersion::V3;\n+            data_serialization_settings.object_serialization_version = MergeTreeObjectSerializationVersion::V3;\n+            /// Also use ADVANCED serialization for nested Object types.\n+            data_serialization_settings.object_shared_data_serialization_version = MergeTreeObjectSharedDataSerializationVersion::ADVANCED;\n+            /// Don't write any dynamic statistics.\n+            data_serialization_settings.object_and_dynamic_write_statistics = ISerialization::SerializeBinaryBulkSettings::ObjectAndDynamicStatisticsMode::NONE;\n+            data_serialization_settings.stream_mark_getter = [&](const SubstreamPath &) -> MarkInCompressedFile { return settings.stream_mark_getter(data_stream_path); };\n+\n+            for (const auto & [path, path_column] : flattened_paths)\n+            {\n+                paths_substreams.emplace_back();\n+                paths_substreams_marks.emplace_back();\n+                data_serialization_settings.getter = [&](const SubstreamPath & substream_path) -> WriteBuffer *\n+                {\n+                    /// Add new substream and its mark for current path.\n+                    paths_substreams.back().push_back(ISerialization::getFileNameForStream(NameAndTypePair(\"\", dynamic_type), substream_path));\n+                    paths_substreams_marks.back().push_back(settings.stream_mark_getter(data_stream_path));\n+                    return data_stream;\n+                };\n+\n+                SerializeBinaryBulkStatePtr path_state;\n+                /// Remember the mark of ObjectSharedDataData stream for this path before writing any data.\n+                paths_marks.push_back(settings.stream_mark_getter(data_stream_path));\n+                dynamic_serialization->serializeBinaryBulkStatePrefix(*path_column, data_serialization_settings, path_state);\n+                dynamic_serialization->serializeBinaryBulkWithMultipleStreams(*path_column, 0, 0, data_serialization_settings, path_state);\n+                dynamic_serialization->serializeBinaryBulkStateSuffix(data_serialization_settings, path_state);\n+            }\n+\n+            /// Write paths marks of the ObjectSharedDataData stream.\n+            settings.path.push_back(Substream::ObjectSharedDataPathsMarks);\n+            auto * paths_marks_stream = settings.getter(settings.path);\n+            auto paths_marks_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!paths_marks_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data paths marks in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            /// Remember the mark of the ObjectSharedDataPathsMarks stream, we will write it in the structure stream later.\n+            MarkInCompressedFile paths_marks_stream_mark = settings.stream_mark_getter(paths_marks_stream_path);\n+\n+            for (const auto & mark : paths_marks)\n+            {\n+                writeBinaryLittleEndian(mark.offset_in_compressed_file, *paths_marks_stream);\n+                writeBinaryLittleEndian(mark.offset_in_decompressed_block, *paths_marks_stream);\n+            }\n+\n+            /// Write paths substreams.\n+            settings.path.push_back(Substream::ObjectSharedDataSubstreams);\n+            auto * paths_substreams_stream = settings.getter(settings.path);\n+            auto paths_substreams_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!paths_substreams_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data paths substreams in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            /// Collect marks of the ObjectSharedDataSubstreams stream for each path so\n+            /// we will be able to seek to the start of the substreams list of the specific path.\n+            std::vector<MarkInCompressedFile> marks_of_paths_substreams;\n+            for (const auto & path_substreams : paths_substreams)\n+            {\n+                marks_of_paths_substreams.push_back(settings.stream_mark_getter(paths_substreams_stream_path));\n+                /// Write number of substreams of this path and the list of substreams.\n+                writeVarUInt(path_substreams.size(), *paths_substreams_stream);\n+                for (const auto & substream : path_substreams)\n+                    writeStringBinary(substream, *paths_substreams_stream);\n+            }\n+\n+            /// Write paths substreams marks of the ObjectSharedDataData stream.\n+            settings.path.push_back(Substream::ObjectSharedDataSubstreamsMarks);\n+            auto * substreams_marks_stream = settings.getter(settings.path);\n+            auto substreams_marks_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!substreams_marks_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data substreams marks in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            /// Collect mark of the ObjectSharedDataSubstreamsMarks stream for each path so\n+            /// we will be able to seek to the start of the substreams marks of the specific path.\n+            std::vector<MarkInCompressedFile> marks_of_paths_substreams_marks;\n+            for (const auto & substreams_marks : paths_substreams_marks)\n+            {\n+                marks_of_paths_substreams_marks.push_back(settings.stream_mark_getter(substreams_marks_stream_path));\n+                for (const auto & mark : substreams_marks)\n+                {\n+                    writeBinaryLittleEndian(mark.offset_in_compressed_file, *substreams_marks_stream);\n+                    writeBinaryLittleEndian(mark.offset_in_decompressed_block, *substreams_marks_stream);\n+                }\n+            }\n+\n+            /// For each path write mark of its substreams in ObjectSharedDataSubstreams/ObjectSharedDataSubstreamsMarks streams.\n+            settings.path.push_back(Substream::ObjectSharedDataPathsSubstreamsMetadata);\n+            auto * paths_substreams_metadata_stream = settings.getter(settings.path);\n+            auto paths_substreams_metadata_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!paths_substreams_metadata_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data paths substreams metadata in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            MarkInCompressedFile paths_substreams_metadata_stream_mark = settings.stream_mark_getter(paths_substreams_metadata_stream_path);\n+            for (size_t i = 0; i != marks_of_paths_substreams.size(); ++i)\n+            {\n+                writeBinaryLittleEndian(marks_of_paths_substreams[i].offset_in_compressed_file, *paths_substreams_metadata_stream);\n+                writeBinaryLittleEndian(marks_of_paths_substreams[i].offset_in_decompressed_block, *paths_substreams_metadata_stream);\n+                writeBinaryLittleEndian(marks_of_paths_substreams_marks[i].offset_in_compressed_file, *paths_substreams_metadata_stream);\n+                writeBinaryLittleEndian(marks_of_paths_substreams_marks[i].offset_in_decompressed_block, *paths_substreams_metadata_stream);\n+            }\n+\n+            /// Write collected marks of other streams into structure stream.\n+            structure_stream_type = settings.use_specialized_prefixes_and_suffixes_substreams ? Substream::ObjectSharedDataStructureSuffix\n+                                                                                 : Substream::ObjectSharedDataStructure;\n+            settings.path.push_back(structure_stream_type);\n+            structure_stream = settings.getter(settings.path);\n+            settings.path.pop_back();\n+\n+            if (!structure_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data structure in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            writeBinaryLittleEndian(data_stream_mark.offset_in_compressed_file, *structure_stream);\n+            writeBinaryLittleEndian(data_stream_mark.offset_in_decompressed_block, *structure_stream);\n+            writeBinaryLittleEndian(paths_marks_stream_mark.offset_in_compressed_file, *structure_stream);\n+            writeBinaryLittleEndian(paths_marks_stream_mark.offset_in_decompressed_block, *structure_stream);\n+            writeBinaryLittleEndian(paths_substreams_metadata_stream_mark.offset_in_compressed_file, *structure_stream);\n+            writeBinaryLittleEndian(paths_substreams_metadata_stream_mark.offset_in_decompressed_block, *structure_stream);\n+\n+            settings.path.pop_back();\n+        }\n+\n+        /// Write a modified copy of the shared data for faster shared data reading.\n+        settings.path.push_back(Substream::ObjectSharedDataCopy);\n+\n+        const auto & shared_data_array_column = assert_cast<const ColumnArray &>(column);\n+        const auto & shared_data_tuple_column = assert_cast<const ColumnTuple &>(shared_data_array_column.getData());\n+        const auto & shared_data_offsets_column = shared_data_array_column.getOffsetsColumn();\n+        const auto & shared_data_offsets = shared_data_array_column.getOffsets();\n+\n+        /// Write array sizes.\n+        settings.path.push_back(Substream::ObjectSharedDataCopySizes);\n+        SerializationArray::serializeOffsetsBinaryBulk(shared_data_offsets_column, offset, limit, settings);\n+        settings.path.pop_back();\n+\n+        size_t nested_offset = offset ? shared_data_offsets[offset - 1] : 0;\n+        size_t nested_limit = limit ? shared_data_offsets[end - 1] - nested_offset : shared_data_offsets.back() - nested_offset;\n+        size_t nested_end = nested_offset + nested_limit;\n+\n+        settings.path.push_back(Substream::ObjectSharedDataCopyPathsIndexes);\n+        auto * copy_indexes_stream = settings.getter(settings.path);\n+\n+        if (!copy_indexes_stream)\n+            throw Exception(\n+                ErrorCodes::LOGICAL_ERROR,\n+                \"Got empty stream for shared data copy indexes in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+        /// We already serialized all paths in the structure streams of buckets.\n+        /// Instead of writing all paths again we create a column that contains indexes\n+        /// of paths in the total list of paths that we serialized for buckets.\n+        std::unordered_map<std::string_view, size_t> path_to_index;\n+        size_t index = 0;\n+        for (const auto & bucket_paths : flattened_paths_buckets)\n+        {\n+            for (const auto & [path, _] : bucket_paths)\n+            {\n+                path_to_index[path] = index;\n+                ++index;\n+            }\n+        }\n+\n+        auto [indexes_column, indexes_type] = createPathsIndexes(path_to_index, shared_data_tuple_column.getColumn(0), nested_offset, nested_end);\n+        indexes_type->getDefaultSerialization()->serializeBinaryBulk(*indexes_column, *copy_indexes_stream, 0, nested_limit);\n+        settings.path.pop_back();\n+\n+        /// Write paths values.\n+        settings.path.push_back(Substream::ObjectSharedDataCopyValues);\n+        auto * copy_values_stream = settings.getter(settings.path);\n+\n+        if (!copy_values_stream)\n+            throw Exception(\n+                ErrorCodes::LOGICAL_ERROR,\n+                \"Got empty stream for shared data copy values in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+        const auto & values_column = shared_data_tuple_column.getColumn(1);\n+        if (nested_limit)\n+            SerializationString().serializeBinaryBulk(values_column, *copy_values_stream, nested_offset, nested_limit);\n+        settings.path.pop_back();\n+\n+        settings.path.pop_back();\n+    }\n+    else\n+    {\n+        /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"serializeBinaryBulkWithMultipleStreams is not implemented for shared data serialization version {}\", serialization_version.value);\n+    }\n+}\n+\n+void SerializationObjectSharedData::serializeBinaryBulkStateSuffix(\n+    ISerialization::SerializeBinaryBulkSettings & settings, ISerialization::SerializeBinaryBulkStatePtr & state) const\n+{\n+    auto * shared_data_state = checkAndGetState<SerializeBinaryBulkStateObjectSharedData>(state);\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        serialization_map->serializeBinaryBulkStateSuffix(settings, shared_data_state->map_state);\n+    }\n+    else if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+    {\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            serialization_map->serializeBinaryBulkStateSuffix(settings, shared_data_state->bucket_map_states[bucket]);\n+            settings.path.pop_back();\n+        }\n+    }\n+    else if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        /// ADVANCED doesn't have suffix.\n+    }\n+    else\n+    {\n+        /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"serializeBinaryBulkStateSuffix is not implemented for shared data serialization version {}\", serialization_version.value);\n+    }\n+\n+}\n+\n+void SerializationObjectSharedData::deserializeBinaryBulkStatePrefix(\n+    ISerialization::DeserializeBinaryBulkSettings & settings,\n+    ISerialization::DeserializeBinaryBulkStatePtr & state,\n+    ISerialization::SubstreamsDeserializeStatesCache * cache) const\n+{\n+    auto shared_data_state = std::make_shared<DeserializeBinaryBulkStateObjectSharedData>();\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        serialization_map->deserializeBinaryBulkStatePrefix(settings, shared_data_state->map_state, cache);\n+    }\n+    else if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+    {\n+        shared_data_state->bucket_map_states.resize(buckets);\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            serialization_map->deserializeBinaryBulkStatePrefix(settings, shared_data_state->bucket_map_states[bucket], cache);\n+            settings.path.pop_back();\n+        }\n+    }\n+    else if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        shared_data_state->bucket_structure_states.resize(buckets);\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            shared_data_state->bucket_structure_states[bucket] = deserializeStructureStatePrefix(settings, cache);\n+            auto * structure_state_concrete = checkAndGetState<DeserializeBinaryBulkStateObjectSharedDataStructure>(shared_data_state->bucket_structure_states[bucket]);\n+            structure_state_concrete->need_all_paths = true;\n+            settings.path.pop_back();\n+        }\n+    }\n+    else\n+    {\n+        /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"deserializeBinaryBulkStatePrefix is not implemented for shared data serialization version {}\", serialization_version.value);\n+    }\n+\n+    state = std::move(shared_data_state);\n+}\n+\n+ISerialization::DeserializeBinaryBulkStatePtr SerializationObjectSharedData::deserializeStructureStatePrefix(DeserializeBinaryBulkSettings & settings, SubstreamsDeserializeStatesCache * cache)\n+{\n+    settings.path.push_back(Substream::ObjectSharedDataStructure);\n+    DeserializeBinaryBulkStatePtr state = nullptr;\n+    if (auto cached_state = getFromSubstreamsDeserializeStatesCache(cache, settings.path))\n+    {\n+        state = cached_state;\n+    }\n+    else\n+    {\n+        state = std::make_shared<DeserializeBinaryBulkStateObjectSharedDataStructure>();\n+        /// Add state to cache so all columns/subcolumns that read from this stream will share the same state.\n+        addToSubstreamsDeserializeStatesCache(cache, settings.path, state);\n+    }\n+\n+    settings.path.pop_back();\n+    return state;\n+}\n+\n+void SerializationObjectSharedData::deserializeStructureGranulePrefix(\n+    ReadBuffer & buf,\n+    SerializationObjectSharedData::StructureGranule & structure_granule,\n+    const DeserializeBinaryBulkStateObjectSharedDataStructure & structure_state)\n+{\n+    /// Read number of rows in this granule.\n+    readVarUInt(structure_granule.num_rows, buf);\n+    String path;\n+    /// Read number of paths stored in this granule.\n+    readVarUInt(structure_granule.num_paths, buf);\n+    /// Read list of paths.\n+    for (size_t i = 0; i != structure_granule.num_paths; ++i)\n+    {\n+        readStringBinary(path, buf);\n+        if (structure_state.requested_paths.contains(path) || structure_state.requested_paths_subcolumns.contains(path) || structure_state.checkIfPathMatchesAnyRequestedPrefix(path))\n+            structure_granule.position_to_requested_path[i] = path;\n+\n+        if (structure_state.need_all_paths)\n+            structure_granule.all_paths.push_back(path);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2228785223",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83777,
        "pr_file": "src/DataTypes/Serializations/SerializationObjectSharedData.cpp",
        "discussion_id": "2228785223",
        "commented_code": "@@ -0,0 +1,1429 @@\n+#include <DataTypes/Serializations/SerializationObjectSharedData.h>\n+#include <DataTypes/Serializations/SerializationObjectHelpers.h>\n+#include <DataTypes/Serializations/SerializationArray.h>\n+#include <DataTypes/Serializations/SerializationNumber.h>\n+#include <DataTypes/Serializations/SerializationString.h>\n+#include <DataTypes/Serializations/getSubcolumnsDeserializationOrder.h>\n+#include <DataTypes/DataTypeObject.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Columns/ColumnMap.h>\n+#include <Columns/ColumnArray.h>\n+#include <Columns/ColumnTuple.h>\n+#include <Storages/MergeTree/ColumnsSubstreams.h>\n+#include <Core/NamesAndTypes.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int LOGICAL_ERROR;\n+    extern const int INCORRECT_DATA;\n+    extern const int NOT_IMPLEMENTED;\n+}\n+\n+SerializationObjectSharedData::SerializationObjectSharedData(SerializationVersion serialization_version_, const DataTypePtr & dynamic_type_, size_t buckets_)\n+    : serialization_version(serialization_version_)\n+    , dynamic_type(dynamic_type_)\n+    , dynamic_serialization(dynamic_type_->getDefaultSerialization())\n+    , buckets(buckets_)\n+    , serialization_map(DataTypeObject::getTypeOfSharedData()->getDefaultSerialization())\n+{\n+}\n+\n+SerializationObjectSharedData::SerializationVersion::SerializationVersion(UInt64 version) : value(static_cast<Value>(version))\n+{\n+    checkVersion(version);\n+}\n+\n+SerializationObjectSharedData::SerializationVersion::SerializationVersion(DB::MergeTreeObjectSharedDataSerializationVersion version)\n+{\n+    switch (version)\n+    {\n+        case MergeTreeObjectSharedDataSerializationVersion::MAP:\n+            value = MAP;\n+            break;\n+        case MergeTreeObjectSharedDataSerializationVersion::MAP_WITH_BUCKETS:\n+            value = MAP_WITH_BUCKETS;\n+            break;\n+        case MergeTreeObjectSharedDataSerializationVersion::ADVANCED:\n+            value = ADVANCED;\n+            break;\n+    }\n+}\n+\n+void SerializationObjectSharedData::SerializationVersion::checkVersion(UInt64 version)\n+{\n+    if (version != MAP && version != MAP_WITH_BUCKETS && version != ADVANCED)\n+        throw Exception(ErrorCodes::INCORRECT_DATA, \"Invalid version for Object shared data serialization: {}\", version);\n+}\n+\n+struct SerializeBinaryBulkStateObjectSharedData : public ISerialization::SerializeBinaryBulkState\n+{\n+    ISerialization::SerializeBinaryBulkStatePtr map_state;\n+    std::vector<ISerialization::SerializeBinaryBulkStatePtr> bucket_map_states;\n+};\n+\n+struct DeserializeBinaryBulkStateObjectSharedData : public ISerialization::DeserializeBinaryBulkState\n+{\n+    ISerialization::DeserializeBinaryBulkStatePtr map_state;\n+    std::vector<ISerialization::DeserializeBinaryBulkStatePtr> bucket_map_states;\n+    std::vector<ISerialization::DeserializeBinaryBulkStatePtr> bucket_structure_states;\n+    /// Some granules can be partially read, we need to remember how many rows\n+    /// were already read from the last incomplete granule.\n+    size_t last_incomplete_granule_offset = 0;\n+\n+    ISerialization::DeserializeBinaryBulkStatePtr clone() const override\n+    {\n+        auto new_state = std::make_shared<DeserializeBinaryBulkStateObjectSharedData>(*this);\n+        for (size_t bucket = 0; bucket != bucket_map_states.size(); ++bucket)\n+            new_state->bucket_map_states[bucket] = bucket_map_states[bucket] ? bucket_map_states[bucket]->clone() : nullptr;\n+        for (size_t bucket = 0; bucket != bucket_structure_states.size(); ++bucket)\n+            new_state->bucket_structure_states[bucket] = bucket_structure_states[bucket] ? bucket_structure_states[bucket]->clone() : nullptr;\n+        return new_state;\n+    }\n+};\n+\n+void SerializationObjectSharedData::enumerateStreams(\n+    ISerialization::EnumerateStreamsSettings & settings,\n+    const ISerialization::StreamCallback & callback,\n+    const ISerialization::SubstreamData & data) const\n+{\n+    const auto * shared_data_state = data.deserialize_state ? checkAndGetState<DeserializeBinaryBulkStateObjectSharedData>(data.deserialize_state) : nullptr;\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        auto map_data = SubstreamData(serialization_map)\n+                            .withColumn(data.column)\n+                            .withType(data.type)\n+                            .withSerializationInfo(data.serialization_info)\n+                            .withDeserializeState(shared_data_state ? shared_data_state->map_state : nullptr);\n+\n+        serialization_map->enumerateStreams(settings, callback, map_data);\n+        return;\n+    }\n+\n+    /// Other 2 serializations MAP_WITH_BUCKETS and ADVAMCED support buckets.\n+    for (size_t bucket = 0; bucket != buckets; ++bucket)\n+    {\n+        settings.path.push_back(Substream::ObjectSharedDataBucket);\n+        settings.path.back().object_shared_data_bucket = bucket;\n+        if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+        {\n+            auto map_data = SubstreamData(serialization_map)\n+                                .withColumn(data.column)\n+                                .withType(data.type)\n+                                .withSerializationInfo(data.serialization_info)\n+                                .withDeserializeState(shared_data_state ? shared_data_state->bucket_map_states[bucket] : nullptr);\n+            serialization_map->enumerateStreams(settings, callback, map_data);\n+        }\n+        else if (serialization_version.value == SerializationObjectSharedData::SerializationVersion::ADVANCED)\n+        {\n+            if (settings.use_specialized_prefixes_and_suffixes_substreams)\n+            {\n+                settings.path.push_back(Substream::ObjectSharedDataStructurePrefix);\n+                callback(settings.path);\n+                settings.path.pop_back();\n+            }\n+            else\n+            {\n+                settings.path.push_back(Substream::ObjectSharedDataStructure);\n+                callback(settings.path);\n+                settings.path.pop_back();\n+            }\n+\n+            settings.path.push_back(Substream::ObjectSharedDataData);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            settings.path.push_back(Substream::ObjectSharedDataPathsMarks);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            settings.path.push_back(Substream::ObjectSharedDataSubstreams);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            settings.path.push_back(Substream::ObjectSharedDataSubstreamsMarks);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            settings.path.push_back(Substream::ObjectSharedDataPathsSubstreamsMetadata);\n+            callback(settings.path);\n+            settings.path.pop_back();\n+\n+            if (settings.use_specialized_prefixes_and_suffixes_substreams)\n+            {\n+                settings.path.push_back(Substream::ObjectSharedDataStructureSuffix);\n+                callback(settings.path);\n+                settings.path.pop_back();\n+            }\n+        }\n+        else\n+        {\n+            /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+            throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"enumerateStreams is not implemented for shared data serialization version {}\", serialization_version.value);\n+        }\n+\n+        settings.path.pop_back();\n+    }\n+\n+    /// Streams related to shared data copy in ADVANCED serialization.\n+    if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        settings.path.push_back(Substream::ObjectSharedDataCopy);\n+\n+        settings.path.push_back(Substream::ObjectSharedDataCopySizes);\n+        callback(settings.path);\n+        settings.path.pop_back();\n+\n+        settings.path.push_back(Substream::ObjectSharedDataCopyPathsIndexes);\n+        callback(settings.path);\n+        settings.path.pop_back();\n+\n+        settings.path.push_back(Substream::ObjectSharedDataCopyValues);\n+        callback(settings.path);\n+        settings.path.pop_back();\n+\n+        settings.path.pop_back();\n+    }\n+}\n+\n+void SerializationObjectSharedData::serializeBinaryBulkStatePrefix(\n+    const IColumn & column,\n+    ISerialization::SerializeBinaryBulkSettings & settings,\n+    ISerialization::SerializeBinaryBulkStatePtr & state) const\n+{\n+    auto shared_data_state = std::make_shared<SerializeBinaryBulkStateObjectSharedData>();\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        serialization_map->serializeBinaryBulkStatePrefix(column, settings, shared_data_state->map_state);\n+    }\n+    else if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+    {\n+        shared_data_state->bucket_map_states.resize(buckets);\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            serialization_map->serializeBinaryBulkStatePrefix(column, settings, shared_data_state->bucket_map_states[bucket]);\n+            settings.path.pop_back();\n+        }\n+    }\n+    else if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        /// ADVANCED serialization doesn't have serialization prefix.\n+    }\n+    else\n+    {\n+        /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"enumerateStreams is not implemented for shared data serialization version {}\", serialization_version.value);\n+    }\n+\n+\n+    state = std::move(shared_data_state);\n+}\n+\n+void SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams(\n+    const IColumn & column,\n+    size_t offset,\n+    size_t limit,\n+    ISerialization::SerializeBinaryBulkSettings & settings,\n+    ISerialization::SerializeBinaryBulkStatePtr & state) const\n+{\n+    auto * shared_data_state = checkAndGetState<SerializeBinaryBulkStateObjectSharedData>(state);\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        serialization_map->serializeBinaryBulkWithMultipleStreams(column, offset, limit, settings, shared_data_state->map_state);\n+    }\n+    else if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+    {\n+        size_t end = limit && offset + limit < column.size() ? offset + limit : column.size();\n+        auto shared_data_buckets = splitSharedDataPathsToBuckets(column, offset, end, buckets);\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            serialization_map->serializeBinaryBulkWithMultipleStreams(*shared_data_buckets[bucket], 0, 0, settings, shared_data_state->bucket_map_states[bucket]);\n+            settings.path.pop_back();\n+        }\n+    }\n+    else if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        size_t end = limit && offset + limit < column.size() ? offset + limit : column.size();\n+        /// First we need to flatten all paths stored in the shared data and separate them into buckets.\n+        auto flattened_paths_buckets = flattenAndBucketSharedDataPaths(column, offset, end, dynamic_type, buckets);\n+        /// Second, write paths in each bucket separately.\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            const auto & flattened_paths = flattened_paths_buckets[bucket];\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+\n+            /// Write structure of this granule.\n+            Substream structure_stream_type = settings.use_specialized_prefixes_and_suffixes_substreams ? Substream::ObjectSharedDataStructurePrefix\n+                                                                                           : Substream::ObjectSharedDataStructure;\n+            settings.path.push_back(structure_stream_type);\n+            auto * structure_stream = settings.getter(settings.path);\n+            settings.path.pop_back();\n+\n+            if (!structure_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data structure in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            /// Write number of rows in this granule and list of all paths that we will serialize.\n+            writeVarUInt(end - offset, *structure_stream);\n+            writeVarUInt(flattened_paths.size(), *structure_stream);\n+            for (const auto & [path, _] : flattened_paths)\n+                writeStringBinary(path, *structure_stream);\n+\n+            /// Write data of flattened paths.\n+            settings.path.push_back(Substream::ObjectSharedDataData);\n+            auto * data_stream = settings.getter(settings.path);\n+            auto data_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!data_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data data in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            if (!settings.stream_mark_getter)\n+                throw Exception(ErrorCodes::LOGICAL_ERROR, \"Mark getter is not set for ADVANCED shared data serialization\");\n+\n+            /// Remember the mark of the ObjectSharedDataData stream, we will write it in the structure stream later.\n+            MarkInCompressedFile data_stream_mark = settings.stream_mark_getter(data_stream_path);\n+\n+            /// Collect mark of the ObjectSharedDataData stream for each path.\n+            std::vector<MarkInCompressedFile> paths_marks;\n+            paths_marks.reserve(flattened_paths.size());\n+            /// Collect list of substreams for each path.\n+            std::vector<std::vector<String>> paths_substreams;\n+            /// Collect mark of the ObjectSharedDataData stream for each substream of each path.\n+            std::vector<std::vector<MarkInCompressedFile>> paths_substreams_marks;\n+\n+            /// Configure serialization settings as in Compact part.\n+            SerializeBinaryBulkSettings data_serialization_settings;\n+            data_serialization_settings.data_part_type = MergeTreeDataPartType::Compact;\n+            data_serialization_settings.position_independent_encoding = true;\n+            data_serialization_settings.low_cardinality_max_dictionary_size = 0;\n+            data_serialization_settings.use_specialized_prefixes_and_suffixes_substreams = true;\n+            data_serialization_settings.use_compact_variant_discriminators_serialization = true;\n+            data_serialization_settings.dynamic_serialization_version = MergeTreeDynamicSerializationVersion::V3;\n+            data_serialization_settings.object_serialization_version = MergeTreeObjectSerializationVersion::V3;\n+            /// Also use ADVANCED serialization for nested Object types.\n+            data_serialization_settings.object_shared_data_serialization_version = MergeTreeObjectSharedDataSerializationVersion::ADVANCED;\n+            /// Don't write any dynamic statistics.\n+            data_serialization_settings.object_and_dynamic_write_statistics = ISerialization::SerializeBinaryBulkSettings::ObjectAndDynamicStatisticsMode::NONE;\n+            data_serialization_settings.stream_mark_getter = [&](const SubstreamPath &) -> MarkInCompressedFile { return settings.stream_mark_getter(data_stream_path); };\n+\n+            for (const auto & [path, path_column] : flattened_paths)\n+            {\n+                paths_substreams.emplace_back();\n+                paths_substreams_marks.emplace_back();\n+                data_serialization_settings.getter = [&](const SubstreamPath & substream_path) -> WriteBuffer *\n+                {\n+                    /// Add new substream and its mark for current path.\n+                    paths_substreams.back().push_back(ISerialization::getFileNameForStream(NameAndTypePair(\"\", dynamic_type), substream_path));\n+                    paths_substreams_marks.back().push_back(settings.stream_mark_getter(data_stream_path));\n+                    return data_stream;\n+                };\n+\n+                SerializeBinaryBulkStatePtr path_state;\n+                /// Remember the mark of ObjectSharedDataData stream for this path before writing any data.\n+                paths_marks.push_back(settings.stream_mark_getter(data_stream_path));\n+                dynamic_serialization->serializeBinaryBulkStatePrefix(*path_column, data_serialization_settings, path_state);\n+                dynamic_serialization->serializeBinaryBulkWithMultipleStreams(*path_column, 0, 0, data_serialization_settings, path_state);\n+                dynamic_serialization->serializeBinaryBulkStateSuffix(data_serialization_settings, path_state);\n+            }\n+\n+            /// Write paths marks of the ObjectSharedDataData stream.\n+            settings.path.push_back(Substream::ObjectSharedDataPathsMarks);\n+            auto * paths_marks_stream = settings.getter(settings.path);\n+            auto paths_marks_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!paths_marks_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data paths marks in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            /// Remember the mark of the ObjectSharedDataPathsMarks stream, we will write it in the structure stream later.\n+            MarkInCompressedFile paths_marks_stream_mark = settings.stream_mark_getter(paths_marks_stream_path);\n+\n+            for (const auto & mark : paths_marks)\n+            {\n+                writeBinaryLittleEndian(mark.offset_in_compressed_file, *paths_marks_stream);\n+                writeBinaryLittleEndian(mark.offset_in_decompressed_block, *paths_marks_stream);\n+            }\n+\n+            /// Write paths substreams.\n+            settings.path.push_back(Substream::ObjectSharedDataSubstreams);\n+            auto * paths_substreams_stream = settings.getter(settings.path);\n+            auto paths_substreams_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!paths_substreams_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data paths substreams in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            /// Collect marks of the ObjectSharedDataSubstreams stream for each path so\n+            /// we will be able to seek to the start of the substreams list of the specific path.\n+            std::vector<MarkInCompressedFile> marks_of_paths_substreams;\n+            for (const auto & path_substreams : paths_substreams)\n+            {\n+                marks_of_paths_substreams.push_back(settings.stream_mark_getter(paths_substreams_stream_path));\n+                /// Write number of substreams of this path and the list of substreams.\n+                writeVarUInt(path_substreams.size(), *paths_substreams_stream);\n+                for (const auto & substream : path_substreams)\n+                    writeStringBinary(substream, *paths_substreams_stream);\n+            }\n+\n+            /// Write paths substreams marks of the ObjectSharedDataData stream.\n+            settings.path.push_back(Substream::ObjectSharedDataSubstreamsMarks);\n+            auto * substreams_marks_stream = settings.getter(settings.path);\n+            auto substreams_marks_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!substreams_marks_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data substreams marks in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            /// Collect mark of the ObjectSharedDataSubstreamsMarks stream for each path so\n+            /// we will be able to seek to the start of the substreams marks of the specific path.\n+            std::vector<MarkInCompressedFile> marks_of_paths_substreams_marks;\n+            for (const auto & substreams_marks : paths_substreams_marks)\n+            {\n+                marks_of_paths_substreams_marks.push_back(settings.stream_mark_getter(substreams_marks_stream_path));\n+                for (const auto & mark : substreams_marks)\n+                {\n+                    writeBinaryLittleEndian(mark.offset_in_compressed_file, *substreams_marks_stream);\n+                    writeBinaryLittleEndian(mark.offset_in_decompressed_block, *substreams_marks_stream);\n+                }\n+            }\n+\n+            /// For each path write mark of its substreams in ObjectSharedDataSubstreams/ObjectSharedDataSubstreamsMarks streams.\n+            settings.path.push_back(Substream::ObjectSharedDataPathsSubstreamsMetadata);\n+            auto * paths_substreams_metadata_stream = settings.getter(settings.path);\n+            auto paths_substreams_metadata_stream_path = settings.path;\n+            settings.path.pop_back();\n+\n+            if (!paths_substreams_metadata_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data paths substreams metadata in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            MarkInCompressedFile paths_substreams_metadata_stream_mark = settings.stream_mark_getter(paths_substreams_metadata_stream_path);\n+            for (size_t i = 0; i != marks_of_paths_substreams.size(); ++i)\n+            {\n+                writeBinaryLittleEndian(marks_of_paths_substreams[i].offset_in_compressed_file, *paths_substreams_metadata_stream);\n+                writeBinaryLittleEndian(marks_of_paths_substreams[i].offset_in_decompressed_block, *paths_substreams_metadata_stream);\n+                writeBinaryLittleEndian(marks_of_paths_substreams_marks[i].offset_in_compressed_file, *paths_substreams_metadata_stream);\n+                writeBinaryLittleEndian(marks_of_paths_substreams_marks[i].offset_in_decompressed_block, *paths_substreams_metadata_stream);\n+            }\n+\n+            /// Write collected marks of other streams into structure stream.\n+            structure_stream_type = settings.use_specialized_prefixes_and_suffixes_substreams ? Substream::ObjectSharedDataStructureSuffix\n+                                                                                 : Substream::ObjectSharedDataStructure;\n+            settings.path.push_back(structure_stream_type);\n+            structure_stream = settings.getter(settings.path);\n+            settings.path.pop_back();\n+\n+            if (!structure_stream)\n+                throw Exception(\n+                    ErrorCodes::LOGICAL_ERROR,\n+                    \"Got empty stream for shared data structure in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+            writeBinaryLittleEndian(data_stream_mark.offset_in_compressed_file, *structure_stream);\n+            writeBinaryLittleEndian(data_stream_mark.offset_in_decompressed_block, *structure_stream);\n+            writeBinaryLittleEndian(paths_marks_stream_mark.offset_in_compressed_file, *structure_stream);\n+            writeBinaryLittleEndian(paths_marks_stream_mark.offset_in_decompressed_block, *structure_stream);\n+            writeBinaryLittleEndian(paths_substreams_metadata_stream_mark.offset_in_compressed_file, *structure_stream);\n+            writeBinaryLittleEndian(paths_substreams_metadata_stream_mark.offset_in_decompressed_block, *structure_stream);\n+\n+            settings.path.pop_back();\n+        }\n+\n+        /// Write a modified copy of the shared data for faster shared data reading.\n+        settings.path.push_back(Substream::ObjectSharedDataCopy);\n+\n+        const auto & shared_data_array_column = assert_cast<const ColumnArray &>(column);\n+        const auto & shared_data_tuple_column = assert_cast<const ColumnTuple &>(shared_data_array_column.getData());\n+        const auto & shared_data_offsets_column = shared_data_array_column.getOffsetsColumn();\n+        const auto & shared_data_offsets = shared_data_array_column.getOffsets();\n+\n+        /// Write array sizes.\n+        settings.path.push_back(Substream::ObjectSharedDataCopySizes);\n+        SerializationArray::serializeOffsetsBinaryBulk(shared_data_offsets_column, offset, limit, settings);\n+        settings.path.pop_back();\n+\n+        size_t nested_offset = offset ? shared_data_offsets[offset - 1] : 0;\n+        size_t nested_limit = limit ? shared_data_offsets[end - 1] - nested_offset : shared_data_offsets.back() - nested_offset;\n+        size_t nested_end = nested_offset + nested_limit;\n+\n+        settings.path.push_back(Substream::ObjectSharedDataCopyPathsIndexes);\n+        auto * copy_indexes_stream = settings.getter(settings.path);\n+\n+        if (!copy_indexes_stream)\n+            throw Exception(\n+                ErrorCodes::LOGICAL_ERROR,\n+                \"Got empty stream for shared data copy indexes in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+        /// We already serialized all paths in the structure streams of buckets.\n+        /// Instead of writing all paths again we create a column that contains indexes\n+        /// of paths in the total list of paths that we serialized for buckets.\n+        std::unordered_map<std::string_view, size_t> path_to_index;\n+        size_t index = 0;\n+        for (const auto & bucket_paths : flattened_paths_buckets)\n+        {\n+            for (const auto & [path, _] : bucket_paths)\n+            {\n+                path_to_index[path] = index;\n+                ++index;\n+            }\n+        }\n+\n+        auto [indexes_column, indexes_type] = createPathsIndexes(path_to_index, shared_data_tuple_column.getColumn(0), nested_offset, nested_end);\n+        indexes_type->getDefaultSerialization()->serializeBinaryBulk(*indexes_column, *copy_indexes_stream, 0, nested_limit);\n+        settings.path.pop_back();\n+\n+        /// Write paths values.\n+        settings.path.push_back(Substream::ObjectSharedDataCopyValues);\n+        auto * copy_values_stream = settings.getter(settings.path);\n+\n+        if (!copy_values_stream)\n+            throw Exception(\n+                ErrorCodes::LOGICAL_ERROR,\n+                \"Got empty stream for shared data copy values in SerializationObjectSharedData::serializeBinaryBulkWithMultipleStreams\");\n+\n+        const auto & values_column = shared_data_tuple_column.getColumn(1);\n+        if (nested_limit)\n+            SerializationString().serializeBinaryBulk(values_column, *copy_values_stream, nested_offset, nested_limit);\n+        settings.path.pop_back();\n+\n+        settings.path.pop_back();\n+    }\n+    else\n+    {\n+        /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"serializeBinaryBulkWithMultipleStreams is not implemented for shared data serialization version {}\", serialization_version.value);\n+    }\n+}\n+\n+void SerializationObjectSharedData::serializeBinaryBulkStateSuffix(\n+    ISerialization::SerializeBinaryBulkSettings & settings, ISerialization::SerializeBinaryBulkStatePtr & state) const\n+{\n+    auto * shared_data_state = checkAndGetState<SerializeBinaryBulkStateObjectSharedData>(state);\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        serialization_map->serializeBinaryBulkStateSuffix(settings, shared_data_state->map_state);\n+    }\n+    else if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+    {\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            serialization_map->serializeBinaryBulkStateSuffix(settings, shared_data_state->bucket_map_states[bucket]);\n+            settings.path.pop_back();\n+        }\n+    }\n+    else if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        /// ADVANCED doesn't have suffix.\n+    }\n+    else\n+    {\n+        /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"serializeBinaryBulkStateSuffix is not implemented for shared data serialization version {}\", serialization_version.value);\n+    }\n+\n+}\n+\n+void SerializationObjectSharedData::deserializeBinaryBulkStatePrefix(\n+    ISerialization::DeserializeBinaryBulkSettings & settings,\n+    ISerialization::DeserializeBinaryBulkStatePtr & state,\n+    ISerialization::SubstreamsDeserializeStatesCache * cache) const\n+{\n+    auto shared_data_state = std::make_shared<DeserializeBinaryBulkStateObjectSharedData>();\n+\n+    if (serialization_version.value == SerializationVersion::MAP)\n+    {\n+        serialization_map->deserializeBinaryBulkStatePrefix(settings, shared_data_state->map_state, cache);\n+    }\n+    else if (serialization_version.value == SerializationVersion::MAP_WITH_BUCKETS)\n+    {\n+        shared_data_state->bucket_map_states.resize(buckets);\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            serialization_map->deserializeBinaryBulkStatePrefix(settings, shared_data_state->bucket_map_states[bucket], cache);\n+            settings.path.pop_back();\n+        }\n+    }\n+    else if (serialization_version.value == SerializationVersion::ADVANCED)\n+    {\n+        shared_data_state->bucket_structure_states.resize(buckets);\n+        for (size_t bucket = 0; bucket != buckets; ++bucket)\n+        {\n+            settings.path.push_back(Substream::ObjectSharedDataBucket);\n+            settings.path.back().object_shared_data_bucket = bucket;\n+            shared_data_state->bucket_structure_states[bucket] = deserializeStructureStatePrefix(settings, cache);\n+            auto * structure_state_concrete = checkAndGetState<DeserializeBinaryBulkStateObjectSharedDataStructure>(shared_data_state->bucket_structure_states[bucket]);\n+            structure_state_concrete->need_all_paths = true;\n+            settings.path.pop_back();\n+        }\n+    }\n+    else\n+    {\n+        /// If we add new serialization version in future and forget to implement something, better to get an exception instead of doing nothing.\n+        throw Exception(ErrorCodes::NOT_IMPLEMENTED, \"deserializeBinaryBulkStatePrefix is not implemented for shared data serialization version {}\", serialization_version.value);\n+    }\n+\n+    state = std::move(shared_data_state);\n+}\n+\n+ISerialization::DeserializeBinaryBulkStatePtr SerializationObjectSharedData::deserializeStructureStatePrefix(DeserializeBinaryBulkSettings & settings, SubstreamsDeserializeStatesCache * cache)\n+{\n+    settings.path.push_back(Substream::ObjectSharedDataStructure);\n+    DeserializeBinaryBulkStatePtr state = nullptr;\n+    if (auto cached_state = getFromSubstreamsDeserializeStatesCache(cache, settings.path))\n+    {\n+        state = cached_state;\n+    }\n+    else\n+    {\n+        state = std::make_shared<DeserializeBinaryBulkStateObjectSharedDataStructure>();\n+        /// Add state to cache so all columns/subcolumns that read from this stream will share the same state.\n+        addToSubstreamsDeserializeStatesCache(cache, settings.path, state);\n+    }\n+\n+    settings.path.pop_back();\n+    return state;\n+}\n+\n+void SerializationObjectSharedData::deserializeStructureGranulePrefix(\n+    ReadBuffer & buf,\n+    SerializationObjectSharedData::StructureGranule & structure_granule,\n+    const DeserializeBinaryBulkStateObjectSharedDataStructure & structure_state)\n+{\n+    /// Read number of rows in this granule.\n+    readVarUInt(structure_granule.num_rows, buf);\n+    String path;\n+    /// Read number of paths stored in this granule.\n+    readVarUInt(structure_granule.num_paths, buf);\n+    /// Read list of paths.\n+    for (size_t i = 0; i != structure_granule.num_paths; ++i)\n+    {\n+        readStringBinary(path, buf);\n+        if (structure_state.requested_paths.contains(path) || structure_state.requested_paths_subcolumns.contains(path) || structure_state.checkIfPathMatchesAnyRequestedPrefix(path))\n+            structure_granule.position_to_requested_path[i] = path;\n+\n+        if (structure_state.need_all_paths)\n+            structure_granule.all_paths.push_back(path);",
        "comment_created_at": "2025-07-24T15:01:27+00:00",
        "comment_author": "antonio2368",
        "comment_body": "we can reserve upfront `structure_granule.all_paths`\r\n\r\nI'm like a reserve bot :triumph: ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2277087603",
    "pr_number": 85640,
    "pr_file": "src/Functions/searchAnyAll.cpp",
    "created_at": "2025-08-14T16:16:37+00:00",
    "commented_code": "const Needles & needles,\n     PaddedPODArray<UInt8> & col_result)\n {\n+    std::vector<std::string_view> tokens;\n     for (size_t i = 0; i < input_rows_count; ++i)\n     {\n         const auto value = col_input.getDataAt(i);\n         col_result[i] = false;\n \n-        const auto & tokens = token_extractor->getTokens(value.data, value.size);\n+        tokens = token_extractor->getTokensView(value.data, value.size);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2277087603",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85640,
        "pr_file": "src/Functions/searchAnyAll.cpp",
        "discussion_id": "2277087603",
        "commented_code": "@@ -84,12 +84,13 @@ void executeSearchAny(\n     const Needles & needles,\n     PaddedPODArray<UInt8> & col_result)\n {\n+    std::vector<std::string_view> tokens;\n     for (size_t i = 0; i < input_rows_count; ++i)\n     {\n         const auto value = col_input.getDataAt(i);\n         col_result[i] = false;\n \n-        const auto & tokens = token_extractor->getTokens(value.data, value.size);\n+        tokens = token_extractor->getTokensView(value.data, value.size);",
        "comment_created_at": "2025-08-14T16:16:37+00:00",
        "comment_author": "Ergus",
        "comment_body": "Considering what this does maybe we should use an std::set instead of a vector to avoid repetitions?",
        "pr_file_module": null
      },
      {
        "comment_id": "2277487453",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85640,
        "pr_file": "src/Functions/searchAnyAll.cpp",
        "discussion_id": "2277087603",
        "commented_code": "@@ -84,12 +84,13 @@ void executeSearchAny(\n     const Needles & needles,\n     PaddedPODArray<UInt8> & col_result)\n {\n+    std::vector<std::string_view> tokens;\n     for (size_t i = 0; i < input_rows_count; ++i)\n     {\n         const auto value = col_input.getDataAt(i);\n         col_result[i] = false;\n \n-        const auto & tokens = token_extractor->getTokens(value.data, value.size);\n+        tokens = token_extractor->getTokensView(value.data, value.size);",
        "comment_created_at": "2025-08-14T18:59:19+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "If we go that route (up to you, @ahmadov), we rather use an `std::unordered_set`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2277664236",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85640,
        "pr_file": "src/Functions/searchAnyAll.cpp",
        "discussion_id": "2277087603",
        "commented_code": "@@ -84,12 +84,13 @@ void executeSearchAny(\n     const Needles & needles,\n     PaddedPODArray<UInt8> & col_result)\n {\n+    std::vector<std::string_view> tokens;\n     for (size_t i = 0; i < input_rows_count; ++i)\n     {\n         const auto value = col_input.getDataAt(i);\n         col_result[i] = false;\n \n-        const auto & tokens = token_extractor->getTokens(value.data, value.size);\n+        tokens = token_extractor->getTokensView(value.data, value.size);",
        "comment_created_at": "2025-08-14T20:31:24+00:00",
        "comment_author": "Ergus",
        "comment_body": "There is a traded off here.\r\n\r\n1. Container construction: hashing a string is usually more expensive than lexicographic order. But O(1) is better than log(N) when N is big enough.\r\n\r\n2. The set allocates the nodes, the hash map doesn't, but it rehashes and copies everything from time to time when N gets big enough.\r\n\r\n3. To iterate is better to use and ordered (deterministic) set of tokens. And iterating a binary tree is a bit cheaper because the iterators are just a wrapper around a pointer, but it is less localized in memory.\r\n\r\nWe also have flat_set ;p \r\n\r\nSumming up: we need to test each in case we want to optimize this part.\r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2240352742",
    "pr_number": 84435,
    "pr_file": "src/Storages/MergeTree/GinIndexStore.cpp",
    "created_at": "2025-07-29T16:18:40+00:00",
    "commented_code": "UInt64 GinIndexPostingsBuilder::serialize(WriteBuffer & buffer)\n {\n     rowids.runOptimize();\n-\n     const UInt64 cardinality = rowids.cardinality();\n \n-    if (cardinality < MIN_SIZE_FOR_ROARING_ENCODING)\n+    PaddedPODArray<UInt32> deltas(cardinality);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2240352742",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84435,
        "pr_file": "src/Storages/MergeTree/GinIndexStore.cpp",
        "discussion_id": "2240352742",
        "commented_code": "@@ -48,110 +52,67 @@ void GinIndexPostingsBuilder::add(UInt32 row_id)\n UInt64 GinIndexPostingsBuilder::serialize(WriteBuffer & buffer)\n {\n     rowids.runOptimize();\n-\n     const UInt64 cardinality = rowids.cardinality();\n \n-    if (cardinality < MIN_SIZE_FOR_ROARING_ENCODING)\n+    PaddedPODArray<UInt32> deltas(cardinality);",
        "comment_created_at": "2025-07-29T16:18:40+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "Are we still getting memory corruptions with FastPFOR?",
        "pr_file_module": null
      },
      {
        "comment_id": "2240369201",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84435,
        "pr_file": "src/Storages/MergeTree/GinIndexStore.cpp",
        "discussion_id": "2240352742",
        "commented_code": "@@ -48,110 +52,67 @@ void GinIndexPostingsBuilder::add(UInt32 row_id)\n UInt64 GinIndexPostingsBuilder::serialize(WriteBuffer & buffer)\n {\n     rowids.runOptimize();\n-\n     const UInt64 cardinality = rowids.cardinality();\n \n-    if (cardinality < MIN_SIZE_FOR_ROARING_ENCODING)\n+    PaddedPODArray<UInt32> deltas(cardinality);",
        "comment_created_at": "2025-07-29T16:26:11+00:00",
        "comment_author": "ahmadov",
        "comment_body": "no, the case `compressed size > uncompressed size` happened due to a few elements in array, after I added a threshold it didn't occur then",
        "pr_file_module": null
      },
      {
        "comment_id": "2258060546",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84435,
        "pr_file": "src/Storages/MergeTree/GinIndexStore.cpp",
        "discussion_id": "2240352742",
        "commented_code": "@@ -48,110 +52,67 @@ void GinIndexPostingsBuilder::add(UInt32 row_id)\n UInt64 GinIndexPostingsBuilder::serialize(WriteBuffer & buffer)\n {\n     rowids.runOptimize();\n-\n     const UInt64 cardinality = rowids.cardinality();\n \n-    if (cardinality < MIN_SIZE_FOR_ROARING_ENCODING)\n+    PaddedPODArray<UInt32> deltas(cardinality);",
        "comment_created_at": "2025-08-06T19:17:55+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "Alright. It would be cool to add a comment to l. 60 like this: `FastPFOR just says that the output buffer should be \"big enough\". +20% is our attempt to comply with that.`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219882333",
    "pr_number": 82368,
    "pr_file": "src/Functions/array/arrayExcept.cpp",
    "created_at": "2025-07-21T17:57:07+00:00",
    "commented_code": "+#include <base/StringRef.h>\n+#include <base/types.h>\n+\n+#include <Columns/ColumnArray.h>\n+#include <Columns/ColumnFixedString.h>\n+#include <Columns/ColumnNullable.h>\n+#include <Columns/ColumnsNumber.h>\n+#include <Columns/ColumnString.h>\n+#include <Columns/ColumnVector.h>\n+#include <Columns/IColumn.h>\n+#include <Common/assert_cast.h>\n+#include <Common/Exception.h>\n+#include <Common/FunctionDocumentation.h>\n+#include <Common/HashTable/ClearableHashSet.h>\n+#include <Common/HashTable/Hash.h>\n+#include <Common/PODArray_fwd.h>\n+#include <Common/register_objects.h>\n+#include <Common/typeid_cast.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/IDataType.h>\n+#include <Functions/FunctionFactory.h>\n+#include <Functions/FunctionHelpers.h>\n+#include <Functions/IFunction.h>\n+#include <Interpreters/Context_fwd.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int ILLEGAL_TYPE_OF_ARGUMENT;\n+    extern const int LOGICAL_ERROR;\n+}\n+\n+namespace\n+{\n+\n+template <typename ColT, bool is_const = false>\n+struct ColumnInfoImpl\n+{\n+    using ColumnType = std::conditional_t<is_const, const ColT, ColT>;\n+    using OffsetsType = std::conditional_t<is_const, const ColumnArray::Offsets, ColumnArray::Offsets>;\n+\n+    ColumnType & col;\n+    OffsetsType & offsets;\n+    std::conditional_t<is_const, const PaddedPODArray<UInt8>, PaddedPODArray<UInt8>> * null_map;\n+};\n+\n+template <typename T>\n+using ColVecType = ColumnVector<T>;\n+\n+template <typename T>\n+using ColumnInfo = ColumnInfoImpl<T, false>;\n+\n+template <typename T>\n+using ConstColumnInfo = ColumnInfoImpl<T, true>;\n+\n+\n+template <typename ColumnType>\n+struct ValueHandler;\n+\n+template <typename T>\n+struct ValueHandler<ColumnVector<T>>\n+{\n+    using ValueType = T;\n+\n+    static ValueType getValue(const ColumnVector<T> & col, size_t pos) { return col.getData()[pos]; }\n+\n+    static void insertValue(ColumnVector<T> & col, ValueType value) { col.insertValue(value); }\n+\n+    static void insertDefault(ColumnVector<T> & col) { col.insertDefault(); }\n+};\n+\n+template <>\n+struct ValueHandler<ColumnString>\n+{\n+    using ValueType = StringRef;\n+\n+    static StringRef getValue(const ColumnString & col, size_t pos) { return col.getDataAt(pos); }\n+\n+    static void insertValue(ColumnString & col, StringRef value) { col.insertData(value.data, value.size); }\n+\n+    static void insertDefault(ColumnString & col) { col.insertDefault(); }\n+};\n+\n+template <>\n+struct ValueHandler<ColumnFixedString>\n+{\n+    using ValueType = StringRef;\n+\n+    static StringRef getValue(const ColumnFixedString & col, size_t pos)\n+    {\n+        const size_t fixed_size = col.getN();\n+        return StringRef(&col.getChars()[pos * fixed_size], fixed_size);\n+    }\n+\n+    static void insertValue(ColumnFixedString & col, StringRef value) { col.insertData(value.data, value.size); }\n+\n+    static void insertDefault(ColumnFixedString & col) { col.insertDefault(); }\n+};\n+\n+template <bool nullable, bool exclude_single_row, typename ColumnT>\n+void processImpl(ConstColumnInfo<ColumnT> source, ConstColumnInfo<ColumnT> exclude, ColumnInfo<ColumnT> result, size_t input_rows_count)\n+{\n+    using ValueType = typename ValueHandler<ColumnT>::ValueType;\n+    using Handler = ValueHandler<ColumnT>;\n+    constexpr size_t initial_size_degree = 9;",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2219882333",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 82368,
        "pr_file": "src/Functions/array/arrayExcept.cpp",
        "discussion_id": "2219882333",
        "commented_code": "@@ -0,0 +1,411 @@\n+#include <base/StringRef.h>\n+#include <base/types.h>\n+\n+#include <Columns/ColumnArray.h>\n+#include <Columns/ColumnFixedString.h>\n+#include <Columns/ColumnNullable.h>\n+#include <Columns/ColumnsNumber.h>\n+#include <Columns/ColumnString.h>\n+#include <Columns/ColumnVector.h>\n+#include <Columns/IColumn.h>\n+#include <Common/assert_cast.h>\n+#include <Common/Exception.h>\n+#include <Common/FunctionDocumentation.h>\n+#include <Common/HashTable/ClearableHashSet.h>\n+#include <Common/HashTable/Hash.h>\n+#include <Common/PODArray_fwd.h>\n+#include <Common/register_objects.h>\n+#include <Common/typeid_cast.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <DataTypes/DataTypeNullable.h>\n+#include <DataTypes/IDataType.h>\n+#include <Functions/FunctionFactory.h>\n+#include <Functions/FunctionHelpers.h>\n+#include <Functions/IFunction.h>\n+#include <Interpreters/Context_fwd.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int ILLEGAL_TYPE_OF_ARGUMENT;\n+    extern const int LOGICAL_ERROR;\n+}\n+\n+namespace\n+{\n+\n+template <typename ColT, bool is_const = false>\n+struct ColumnInfoImpl\n+{\n+    using ColumnType = std::conditional_t<is_const, const ColT, ColT>;\n+    using OffsetsType = std::conditional_t<is_const, const ColumnArray::Offsets, ColumnArray::Offsets>;\n+\n+    ColumnType & col;\n+    OffsetsType & offsets;\n+    std::conditional_t<is_const, const PaddedPODArray<UInt8>, PaddedPODArray<UInt8>> * null_map;\n+};\n+\n+template <typename T>\n+using ColVecType = ColumnVector<T>;\n+\n+template <typename T>\n+using ColumnInfo = ColumnInfoImpl<T, false>;\n+\n+template <typename T>\n+using ConstColumnInfo = ColumnInfoImpl<T, true>;\n+\n+\n+template <typename ColumnType>\n+struct ValueHandler;\n+\n+template <typename T>\n+struct ValueHandler<ColumnVector<T>>\n+{\n+    using ValueType = T;\n+\n+    static ValueType getValue(const ColumnVector<T> & col, size_t pos) { return col.getData()[pos]; }\n+\n+    static void insertValue(ColumnVector<T> & col, ValueType value) { col.insertValue(value); }\n+\n+    static void insertDefault(ColumnVector<T> & col) { col.insertDefault(); }\n+};\n+\n+template <>\n+struct ValueHandler<ColumnString>\n+{\n+    using ValueType = StringRef;\n+\n+    static StringRef getValue(const ColumnString & col, size_t pos) { return col.getDataAt(pos); }\n+\n+    static void insertValue(ColumnString & col, StringRef value) { col.insertData(value.data, value.size); }\n+\n+    static void insertDefault(ColumnString & col) { col.insertDefault(); }\n+};\n+\n+template <>\n+struct ValueHandler<ColumnFixedString>\n+{\n+    using ValueType = StringRef;\n+\n+    static StringRef getValue(const ColumnFixedString & col, size_t pos)\n+    {\n+        const size_t fixed_size = col.getN();\n+        return StringRef(&col.getChars()[pos * fixed_size], fixed_size);\n+    }\n+\n+    static void insertValue(ColumnFixedString & col, StringRef value) { col.insertData(value.data, value.size); }\n+\n+    static void insertDefault(ColumnFixedString & col) { col.insertDefault(); }\n+};\n+\n+template <bool nullable, bool exclude_single_row, typename ColumnT>\n+void processImpl(ConstColumnInfo<ColumnT> source, ConstColumnInfo<ColumnT> exclude, ColumnInfo<ColumnT> result, size_t input_rows_count)\n+{\n+    using ValueType = typename ValueHandler<ColumnT>::ValueType;\n+    using Handler = ValueHandler<ColumnT>;\n+    constexpr size_t initial_size_degree = 9;",
        "comment_created_at": "2025-07-21T17:57:07+00:00",
        "comment_author": "yariks5s",
        "comment_body": "initial_size_degree decides the first (stack) buffer that the hash table starts with -- ClearableHashSetWithStackMemory pre\u2011allocates\n2^initial_size_degree cells directly inside the object\n\n---\n\nMotivation behind this value (in case anyone wonders why 9) -- I wouldn't say that it is an arbitrary number, as it was stated in the line comment.\n\n9 is conservative default already used in arrayDistinct. it costs from around 8 to 16 kb of stack (depending on the type) and avoids rehashing for up to around 450 distinct values. That is usually still OK for most cases, unless we know our except arrays are almost always tiny, which is not true for real production use-cases",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1987120269",
    "pr_number": 77029,
    "pr_file": "src/Common/benchmarks/radix_sort.cpp",
    "created_at": "2025-03-10T11:40:50+00:00",
    "commented_code": "+#include <DataTypes/DataTypesNumber.h>\n+#include <Functions/FunctionHelpers.h>\n+#include <Storages/StorageGenerateRandom.h>\n+#include <benchmark/benchmark.h>\n+#include \"pcg_random.hpp\"",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "1987120269",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 77029,
        "pr_file": "src/Common/benchmarks/radix_sort.cpp",
        "discussion_id": "1987120269",
        "commented_code": "@@ -0,0 +1,24 @@\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Functions/FunctionHelpers.h>\n+#include <Storages/StorageGenerateRandom.h>\n+#include <benchmark/benchmark.h>\n+#include \"pcg_random.hpp\"",
        "comment_created_at": "2025-03-10T11:40:50+00:00",
        "comment_author": "Algunenano",
        "comment_body": "I see a degradation in performance in this benchmark after the changes to RadixSort.h. Without the changes it's faster:\r\n\r\n```\r\n/mnt/ch/ClickHouse/build $ ./src/Common/benchmarks/radix_sort\r\n2025-03-10T12:39:15+01:00\r\nRunning ./src/Common/benchmarks/radix_sort\r\nRun on (32 X 4119.81 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x16)\r\n  L1 Instruction 32 KiB (x16)\r\n  L2 Unified 1024 KiB (x16)\r\n  L3 Unified 98304 KiB (x2)\r\nLoad Average: 0.84, 2.65, 2.47\r\n--------------------------------------------------------\r\nBenchmark              Time             CPU   Iterations\r\n--------------------------------------------------------\r\nBM_RadixSort1     566352 ns       563924 ns         1218\r\n/mnt/ch/ClickHouse/build $ git checkout ../src/Common/RadixSort.h\r\nUpdated 1 path from the index\r\n/mnt/ch/ClickHouse/build $ ninja src/Common/benchmarks/radix_sort\r\n[0/2] Re-checking globbed directories...\r\n[0/14] cd /mnt/ch/ClickHouse/rust/workspace/prql && /usr/bin/cmake -E env CARGO_TARGET_X86_64_UNK...fig=/mnt/ch/ClickHouse/build/contrib/corrosion-cmake/config.toml -- -Cdefault-linker-libraries=no\r\n    Finished `release` profile [optimized] target(s) in 0.09s\r\n[1/14] cd /mnt/ch/ClickHouse/contrib/delta-kernel-rs/ffi && /usr/bin/cmake -E env CARGO_TARGET_DI...fig=/mnt/ch/ClickHouse/build/contrib/corrosion-cmake/config.toml -- -Cdefault-linker-libraries=no\r\n    Finished `release` profile [optimized] target(s) in 0.09s\r\n[3/14] cd /mnt/ch/ClickHouse/rust/workspace/skim && /usr/bin/cmake -E env CARGO_TARGET_X86_64_UNK...fig=/mnt/ch/ClickHouse/build/contrib/corrosion-cmake/config.toml -- -Cdefault-linker-libraries=no\r\n    Finished `release` profile [optimized] target(s) in 0.09s\r\n[13/14] Linking CXX executable src/Common/benchmarks/radix_sort\r\n/mnt/ch/ClickHouse/build $ ./src/Common/benchmarks/radix_sort\r\n2025-03-10T12:40:39+01:00\r\nRunning ./src/Common/benchmarks/radix_sort\r\nRun on (32 X 5049.31 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x16)\r\n  L1 Instruction 32 KiB (x16)\r\n  L2 Unified 1024 KiB (x16)\r\n  L3 Unified 98304 KiB (x2)\r\nLoad Average: 0.82, 2.20, 2.32\r\n--------------------------------------------------------\r\nBenchmark              Time             CPU   Iterations\r\n--------------------------------------------------------\r\nBM_RadixSort1     472904 ns       470935 ns         1436\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1990415356",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 77029,
        "pr_file": "src/Common/benchmarks/radix_sort.cpp",
        "discussion_id": "1987120269",
        "commented_code": "@@ -0,0 +1,24 @@\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Functions/FunctionHelpers.h>\n+#include <Storages/StorageGenerateRandom.h>\n+#include <benchmark/benchmark.h>\n+#include \"pcg_random.hpp\"",
        "comment_created_at": "2025-03-12T02:00:38+00:00",
        "comment_author": "taiyang-li",
        "comment_body": "That's strange... This is my test result:\r\n\r\nbaseline\r\n```\r\n$ git checkout master -- src/Common/RadixSort.h\r\n$ ninja benchmark_radix_sort\r\n$ ./build_gcc/src/Common/benchmarks/benchmark_radix_sort\r\nRunning ./build_gcc/src/Common/benchmarks/benchmark_radix_sort\r\nRun on (32 X 2100 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x16)\r\n  L1 Instruction 32 KiB (x16)\r\n  L2 Unified 1024 KiB (x16)\r\n  L3 Unified 11264 KiB (x2)\r\nLoad Average: 6.39, 6.25, 6.11\r\n--------------------------------------------------------\r\nBenchmark              Time             CPU   Iterations\r\n--------------------------------------------------------\r\nBM_RadixSort1    2583340 ns      2583180 ns          272\r\n```\r\n\r\nafter changes:\r\n```\r\n$ git restore --staged src/Common/RadixSort.h\r\n$ git restore src/Common/RadixSort.h\r\n$ ninja ./build_gcc/src/Common/benchmarks/benchmark_radix_sort\r\nRunning ./build_gcc/src/Common/benchmarks/benchmark_radix_sort\r\nRun on (32 X 2100 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x16)\r\n  L1 Instruction 32 KiB (x16)\r\n  L2 Unified 1024 KiB (x16)\r\n  L3 Unified 11264 KiB (x2)\r\nLoad Average: 7.30, 7.68, 6.36\r\n--------------------------------------------------------\r\nBenchmark              Time             CPU   Iterations\r\n--------------------------------------------------------\r\nBM_RadixSort1    2006556 ns      2006513 ns          349\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1999099633",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 77029,
        "pr_file": "src/Common/benchmarks/radix_sort.cpp",
        "discussion_id": "1987120269",
        "commented_code": "@@ -0,0 +1,24 @@\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Functions/FunctionHelpers.h>\n+#include <Storages/StorageGenerateRandom.h>\n+#include <benchmark/benchmark.h>\n+#include \"pcg_random.hpp\"",
        "comment_created_at": "2025-03-17T16:03:09+00:00",
        "comment_author": "Algunenano",
        "comment_body": "I've tested in 2 different systems.\r\n\r\n* In one there are no differences:\r\n```\r\n$ cat /proc/cpuinfo  | grep \"model name\"\r\nmodel name      : AMD EPYC 7R13 Processor\r\n$ ./src/Common/benchmarks/benchmark_radix_sort --benchmark_min_time=2\r\n2025-03-17T15:58:13+00:00\r\nRunning ./src/Common/benchmarks/benchmark_radix_sort\r\nRun on (32 X 2878.31 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x16)\r\n  L1 Instruction 32 KiB (x16)\r\n  L2 Unified 512 KiB (x16)\r\n  L3 Unified 32768 KiB (x2)\r\nLoad Average: 0.64, 6.01, 14.44\r\n--------------------------------------------------------\r\nBenchmark              Time             CPU   Iterations\r\n--------------------------------------------------------\r\nBM_RadixSort1    1018836 ns      1018808 ns         2754\r\n\r\n$ git stash push ../src/Common/RadixSort.h\r\n$ ninja ./src/Common/benchmarks/benchmark_radix_sort\r\n$ ./src/Common/benchmarks/benchmark_radix_sort --benchmark_min_time=2\r\n2025-03-17T15:58:55+00:00\r\nRunning ./src/Common/benchmarks/benchmark_radix_sort\r\nRun on (32 X 2878.94 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x16)\r\n  L1 Instruction 32 KiB (x16)\r\n  L2 Unified 512 KiB (x16)\r\n  L3 Unified 32768 KiB (x2)\r\nLoad Average: 0.58, 5.32, 13.85\r\n--------------------------------------------------------\r\nBenchmark              Time             CPU   Iterations\r\n--------------------------------------------------------\r\nBM_RadixSort1    1026243 ns      1026205 ns         2724\r\n```\r\n\r\nIn another one the code becomes 20% slower:\r\n\r\n```\r\n$ cat /proc/cpuinfo  | grep \"model name\"\r\nmodel name      : AMD Ryzen 9 7950X3D 16-Core Processor\r\n$ ./src/Common/benchmarks/benchmark_radix_sort --benchmark_min_time=2\r\n2025-03-17T16:57:27+01:00\r\nRunning ./src/Common/benchmarks/benchmark_radix_sort\r\nRun on (32 X 4190.06 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x16)\r\n  L1 Instruction 32 KiB (x16)\r\n  L2 Unified 1024 KiB (x16)\r\n  L3 Unified 98304 KiB (x2)\r\nLoad Average: 0.40, 0.91, 1.59\r\n--------------------------------------------------------\r\nBenchmark              Time             CPU   Iterations\r\n--------------------------------------------------------\r\nBM_RadixSort1     552767 ns       550552 ns         5107\r\n$ git stash push ../src/Common/RadixSort.h\r\n$ ninja ./src/Common/benchmarks/benchmark_radix_sort\r\n$ ./src/Common/benchmarks/benchmark_radix_sort --benchmark_min_time=2\r\n2025-03-17T16:58:18+01:00\r\nRunning ./src/Common/benchmarks/benchmark_radix_sort\r\nRun on (32 X 2982 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32 KiB (x16)\r\n  L1 Instruction 32 KiB (x16)\r\n  L2 Unified 1024 KiB (x16)\r\n  L3 Unified 98304 KiB (x2)\r\nLoad Average: 0.46, 0.85, 1.53\r\n--------------------------------------------------------\r\nBenchmark              Time             CPU   Iterations\r\n--------------------------------------------------------\r\nBM_RadixSort1     454596 ns       452763 ns         6222\r\n```\r\n\r\n\r\nLet's move the changes to RadixSort to a different PR because they obviously have some CPU specific changes and are not always beneficial, so they need to be reviewed in more detail",
        "pr_file_module": null
      },
      {
        "comment_id": "2000474979",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 77029,
        "pr_file": "src/Common/benchmarks/radix_sort.cpp",
        "discussion_id": "1987120269",
        "commented_code": "@@ -0,0 +1,24 @@\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Functions/FunctionHelpers.h>\n+#include <Storages/StorageGenerateRandom.h>\n+#include <benchmark/benchmark.h>\n+#include \"pcg_random.hpp\"",
        "comment_created_at": "2025-03-18T08:37:32+00:00",
        "comment_author": "taiyang-li",
        "comment_body": "done.  Refer to https://github.com/ClickHouse/ClickHouse/pull/77789",
        "pr_file_module": null
      }
    ]
  }
]