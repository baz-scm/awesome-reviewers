[
  {
    "discussion_id": "2282312878",
    "pr_number": 85455,
    "pr_file": "src/Planner/PlannerCorrelatedSubqueries.cpp",
    "created_at": "2025-08-18T12:52:15+00:00",
    "commented_code": "namespace\n {\n \n-void projectCorrelatedColumns(\n+[[maybe_unused]] void projectCorrelatedColumns(",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2282312878",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85455,
        "pr_file": "src/Planner/PlannerCorrelatedSubqueries.cpp",
        "discussion_id": "2282312878",
        "commented_code": "@@ -188,7 +198,7 @@ struct DecorrelationContext\n namespace\n {\n \n-void projectCorrelatedColumns(\n+[[maybe_unused]] void projectCorrelatedColumns(",
        "comment_created_at": "2025-08-18T12:52:15+00:00",
        "comment_author": "KochetovNicolai",
        "comment_body": "Semantically, this function makes sense without correlation, and `lhs` has no meaning.\r\nWe want to project columns from the `identifiers` list to the `plan`.\r\n\r\nMaybe rename here, cause we call `projectCorrelatedColumns(rhs_plan, ...)` now.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2246309799",
    "pr_number": 82807,
    "pr_file": "src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp",
    "created_at": "2025-07-31T20:25:22+00:00",
    "commented_code": "from->getServerSettings()[ServerSetting::max_entries_for_hash_table_stats],\n         from->getInitialQueryId(),\n         ExpressionActionsSettings(from),\n-        from->getPreparedSetsCache())\n+        from->getPreparedSetsCache(),\n+        from->canUseParallelReplicasOnInitiator()\n+            && from->getSettingsRef()[Setting::parallel_replicas_local_plan]\n+            && from->getSettingsRef()[Setting::parallel_replicas_support_projection])",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2246309799",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 82807,
        "pr_file": "src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp",
        "discussion_id": "2246309799",
        "commented_code": "@@ -166,7 +170,10 @@ QueryPlanOptimizationSettings::QueryPlanOptimizationSettings(ContextPtr from)\n         from->getServerSettings()[ServerSetting::max_entries_for_hash_table_stats],\n         from->getInitialQueryId(),\n         ExpressionActionsSettings(from),\n-        from->getPreparedSetsCache())\n+        from->getPreparedSetsCache(),\n+        from->canUseParallelReplicasOnInitiator()\n+            && from->getSettingsRef()[Setting::parallel_replicas_local_plan]\n+            && from->getSettingsRef()[Setting::parallel_replicas_support_projection])",
        "comment_created_at": "2025-07-31T20:25:22+00:00",
        "comment_author": "nickitat",
        "comment_body": "If I'm reading this right, we later pass this value under the name `is_parallel_replicas_initiator`. This name doesn't sound accurate, given that we also account here for things like `parallel_replicas_support_projection` setting.",
        "pr_file_module": null
      },
      {
        "comment_id": "2246724193",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 82807,
        "pr_file": "src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp",
        "discussion_id": "2246309799",
        "commented_code": "@@ -166,7 +170,10 @@ QueryPlanOptimizationSettings::QueryPlanOptimizationSettings(ContextPtr from)\n         from->getServerSettings()[ServerSetting::max_entries_for_hash_table_stats],\n         from->getInitialQueryId(),\n         ExpressionActionsSettings(from),\n-        from->getPreparedSetsCache())\n+        from->getPreparedSetsCache(),\n+        from->canUseParallelReplicasOnInitiator()\n+            && from->getSettingsRef()[Setting::parallel_replicas_local_plan]\n+            && from->getSettingsRef()[Setting::parallel_replicas_support_projection])",
        "comment_created_at": "2025-08-01T02:00:41+00:00",
        "comment_author": "zoomxi",
        "comment_body": "ok, done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2210258631",
    "pr_number": 82885,
    "pr_file": "src/Formats/ProtobufSerializer.cpp",
    "created_at": "2025-07-16T12:34:35+00:00",
    "commented_code": "};\n \n \n+    /// Wraps a structute (field, Message, etc) which is a member of OneOf (protobuf union)\n+    class ProtobufSerializerOneOf : public ProtobufSerializer\n+    {\n+    public:\n+        explicit ProtobufSerializerOneOf(std::unique_ptr<ProtobufSerializer> nested_serializer_, std::string_view one_of_column_name_, size_t column_idx_, int field_tag_)\n+            : nested_serializer(std::move(nested_serializer_))\n+            , one_of_column_name(one_of_column_name_)\n+            , column_idx(column_idx_)\n+            , field_tag(field_tag_)\n+        {\n+        }\n+\n+        void setColumns(const ColumnPtr * columns, size_t num_columns) override\n+        {\n+            assert(num_columns > column_idx);\n+\n+            Columns cols;\n+            cols.reserve(num_columns - 1);\n+            for (size_t i : collections::range(num_columns))\n+            {\n+                if (i == column_idx)\n+                {\n+                    presence_column = columns[column_idx]->assumeMutable();\n+                }\n+                else\n+                {\n+                    cols.push_back(columns[i]->getPtr());\n+                }\n+            }\n+            nested_serializer->setColumns(cols.data(), cols.size());\n+        }\n+\n+        void setColumns(const MutableColumnPtr * columns, size_t num_columns) override\n+        {\n+            Columns cols;\n+            cols.reserve(num_columns);\n+            for (size_t i : collections::range(num_columns))\n+                cols.push_back(columns[i]->getPtr());\n+            setColumns(cols.data(), cols.size());\n+        }\n+\n+        void writeRow([[maybe_unused]] size_t row_num) override\n+        {\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"OneOf is not implemented for writes\");\n+            // Should be as simple as\n+            //   if (static_cast<int>(presence_column->getInt(row_num)) == field_tag)\n+            //       nested_serializer->writeRow(row_num);\n+            // but disabled so far\n+        }\n+\n+        void readRow(size_t row_num) override\n+        {\n+            if (presence_column)\n+            {\n+                presence_column->insert(field_tag);\n+            }\n+            nested_serializer->readRow(row_num);\n+        }\n+\n+        void insertDefaults(size_t row_num) override\n+        {\n+            if (row_num >= presence_column->size())\n+                presence_column->insert(0);\n+\n+            nested_serializer->insertDefaults(row_num);\n+        }\n+\n+        void describeTree(WriteBuffer & out, size_t indent) const override\n+        {\n+            writeIndent(out, indent) << \"ProtobufSerializerOneOf \" << one_of_column_name << \", idx \" << column_idx << \"->\n\";\n+            nested_serializer->describeTree(out, indent + 1);\n+        }\n+\n+    private:\n+        const std::unique_ptr<ProtobufSerializer> nested_serializer;\n+        ColumnPtr column; // ???\n+        std::string_view one_of_column_name;  // Is it safe?\n+        size_t column_idx;",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2210258631",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 82885,
        "pr_file": "src/Formats/ProtobufSerializer.cpp",
        "discussion_id": "2210258631",
        "commented_code": "@@ -1969,6 +1971,89 @@ namespace\n     };\n \n \n+    /// Wraps a structute (field, Message, etc) which is a member of OneOf (protobuf union)\n+    class ProtobufSerializerOneOf : public ProtobufSerializer\n+    {\n+    public:\n+        explicit ProtobufSerializerOneOf(std::unique_ptr<ProtobufSerializer> nested_serializer_, std::string_view one_of_column_name_, size_t column_idx_, int field_tag_)\n+            : nested_serializer(std::move(nested_serializer_))\n+            , one_of_column_name(one_of_column_name_)\n+            , column_idx(column_idx_)\n+            , field_tag(field_tag_)\n+        {\n+        }\n+\n+        void setColumns(const ColumnPtr * columns, size_t num_columns) override\n+        {\n+            assert(num_columns > column_idx);\n+\n+            Columns cols;\n+            cols.reserve(num_columns - 1);\n+            for (size_t i : collections::range(num_columns))\n+            {\n+                if (i == column_idx)\n+                {\n+                    presence_column = columns[column_idx]->assumeMutable();\n+                }\n+                else\n+                {\n+                    cols.push_back(columns[i]->getPtr());\n+                }\n+            }\n+            nested_serializer->setColumns(cols.data(), cols.size());\n+        }\n+\n+        void setColumns(const MutableColumnPtr * columns, size_t num_columns) override\n+        {\n+            Columns cols;\n+            cols.reserve(num_columns);\n+            for (size_t i : collections::range(num_columns))\n+                cols.push_back(columns[i]->getPtr());\n+            setColumns(cols.data(), cols.size());\n+        }\n+\n+        void writeRow([[maybe_unused]] size_t row_num) override\n+        {\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"OneOf is not implemented for writes\");\n+            // Should be as simple as\n+            //   if (static_cast<int>(presence_column->getInt(row_num)) == field_tag)\n+            //       nested_serializer->writeRow(row_num);\n+            // but disabled so far\n+        }\n+\n+        void readRow(size_t row_num) override\n+        {\n+            if (presence_column)\n+            {\n+                presence_column->insert(field_tag);\n+            }\n+            nested_serializer->readRow(row_num);\n+        }\n+\n+        void insertDefaults(size_t row_num) override\n+        {\n+            if (row_num >= presence_column->size())\n+                presence_column->insert(0);\n+\n+            nested_serializer->insertDefaults(row_num);\n+        }\n+\n+        void describeTree(WriteBuffer & out, size_t indent) const override\n+        {\n+            writeIndent(out, indent) << \"ProtobufSerializerOneOf \" << one_of_column_name << \", idx \" << column_idx << \"->\\n\";\n+            nested_serializer->describeTree(out, indent + 1);\n+        }\n+\n+    private:\n+        const std::unique_ptr<ProtobufSerializer> nested_serializer;\n+        ColumnPtr column; // ???\n+        std::string_view one_of_column_name;  // Is it safe?\n+        size_t column_idx;",
        "comment_created_at": "2025-07-16T12:34:35+00:00",
        "comment_author": "Avogar",
        "comment_body": "Is it an index of presence column? If yes, please, rename the field to `presence_column_idx`",
        "pr_file_module": null
      },
      {
        "comment_id": "2222153945",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 82885,
        "pr_file": "src/Formats/ProtobufSerializer.cpp",
        "discussion_id": "2210258631",
        "commented_code": "@@ -1969,6 +1971,89 @@ namespace\n     };\n \n \n+    /// Wraps a structute (field, Message, etc) which is a member of OneOf (protobuf union)\n+    class ProtobufSerializerOneOf : public ProtobufSerializer\n+    {\n+    public:\n+        explicit ProtobufSerializerOneOf(std::unique_ptr<ProtobufSerializer> nested_serializer_, std::string_view one_of_column_name_, size_t column_idx_, int field_tag_)\n+            : nested_serializer(std::move(nested_serializer_))\n+            , one_of_column_name(one_of_column_name_)\n+            , column_idx(column_idx_)\n+            , field_tag(field_tag_)\n+        {\n+        }\n+\n+        void setColumns(const ColumnPtr * columns, size_t num_columns) override\n+        {\n+            assert(num_columns > column_idx);\n+\n+            Columns cols;\n+            cols.reserve(num_columns - 1);\n+            for (size_t i : collections::range(num_columns))\n+            {\n+                if (i == column_idx)\n+                {\n+                    presence_column = columns[column_idx]->assumeMutable();\n+                }\n+                else\n+                {\n+                    cols.push_back(columns[i]->getPtr());\n+                }\n+            }\n+            nested_serializer->setColumns(cols.data(), cols.size());\n+        }\n+\n+        void setColumns(const MutableColumnPtr * columns, size_t num_columns) override\n+        {\n+            Columns cols;\n+            cols.reserve(num_columns);\n+            for (size_t i : collections::range(num_columns))\n+                cols.push_back(columns[i]->getPtr());\n+            setColumns(cols.data(), cols.size());\n+        }\n+\n+        void writeRow([[maybe_unused]] size_t row_num) override\n+        {\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"OneOf is not implemented for writes\");\n+            // Should be as simple as\n+            //   if (static_cast<int>(presence_column->getInt(row_num)) == field_tag)\n+            //       nested_serializer->writeRow(row_num);\n+            // but disabled so far\n+        }\n+\n+        void readRow(size_t row_num) override\n+        {\n+            if (presence_column)\n+            {\n+                presence_column->insert(field_tag);\n+            }\n+            nested_serializer->readRow(row_num);\n+        }\n+\n+        void insertDefaults(size_t row_num) override\n+        {\n+            if (row_num >= presence_column->size())\n+                presence_column->insert(0);\n+\n+            nested_serializer->insertDefaults(row_num);\n+        }\n+\n+        void describeTree(WriteBuffer & out, size_t indent) const override\n+        {\n+            writeIndent(out, indent) << \"ProtobufSerializerOneOf \" << one_of_column_name << \", idx \" << column_idx << \"->\\n\";\n+            nested_serializer->describeTree(out, indent + 1);\n+        }\n+\n+    private:\n+        const std::unique_ptr<ProtobufSerializer> nested_serializer;\n+        ColumnPtr column; // ???\n+        std::string_view one_of_column_name;  // Is it safe?\n+        size_t column_idx;",
        "comment_created_at": "2025-07-22T10:50:40+00:00",
        "comment_author": "Avogar",
        "comment_body": "Still relevant",
        "pr_file_module": null
      },
      {
        "comment_id": "2222173095",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 82885,
        "pr_file": "src/Formats/ProtobufSerializer.cpp",
        "discussion_id": "2210258631",
        "commented_code": "@@ -1969,6 +1971,89 @@ namespace\n     };\n \n \n+    /// Wraps a structute (field, Message, etc) which is a member of OneOf (protobuf union)\n+    class ProtobufSerializerOneOf : public ProtobufSerializer\n+    {\n+    public:\n+        explicit ProtobufSerializerOneOf(std::unique_ptr<ProtobufSerializer> nested_serializer_, std::string_view one_of_column_name_, size_t column_idx_, int field_tag_)\n+            : nested_serializer(std::move(nested_serializer_))\n+            , one_of_column_name(one_of_column_name_)\n+            , column_idx(column_idx_)\n+            , field_tag(field_tag_)\n+        {\n+        }\n+\n+        void setColumns(const ColumnPtr * columns, size_t num_columns) override\n+        {\n+            assert(num_columns > column_idx);\n+\n+            Columns cols;\n+            cols.reserve(num_columns - 1);\n+            for (size_t i : collections::range(num_columns))\n+            {\n+                if (i == column_idx)\n+                {\n+                    presence_column = columns[column_idx]->assumeMutable();\n+                }\n+                else\n+                {\n+                    cols.push_back(columns[i]->getPtr());\n+                }\n+            }\n+            nested_serializer->setColumns(cols.data(), cols.size());\n+        }\n+\n+        void setColumns(const MutableColumnPtr * columns, size_t num_columns) override\n+        {\n+            Columns cols;\n+            cols.reserve(num_columns);\n+            for (size_t i : collections::range(num_columns))\n+                cols.push_back(columns[i]->getPtr());\n+            setColumns(cols.data(), cols.size());\n+        }\n+\n+        void writeRow([[maybe_unused]] size_t row_num) override\n+        {\n+            throw Exception(ErrorCodes::LOGICAL_ERROR, \"OneOf is not implemented for writes\");\n+            // Should be as simple as\n+            //   if (static_cast<int>(presence_column->getInt(row_num)) == field_tag)\n+            //       nested_serializer->writeRow(row_num);\n+            // but disabled so far\n+        }\n+\n+        void readRow(size_t row_num) override\n+        {\n+            if (presence_column)\n+            {\n+                presence_column->insert(field_tag);\n+            }\n+            nested_serializer->readRow(row_num);\n+        }\n+\n+        void insertDefaults(size_t row_num) override\n+        {\n+            if (row_num >= presence_column->size())\n+                presence_column->insert(0);\n+\n+            nested_serializer->insertDefaults(row_num);\n+        }\n+\n+        void describeTree(WriteBuffer & out, size_t indent) const override\n+        {\n+            writeIndent(out, indent) << \"ProtobufSerializerOneOf \" << one_of_column_name << \", idx \" << column_idx << \"->\\n\";\n+            nested_serializer->describeTree(out, indent + 1);\n+        }\n+\n+    private:\n+        const std::unique_ptr<ProtobufSerializer> nested_serializer;\n+        ColumnPtr column; // ???\n+        std::string_view one_of_column_name;  // Is it safe?\n+        size_t column_idx;",
        "comment_created_at": "2025-07-22T10:58:53+00:00",
        "comment_author": "ilejn",
        "comment_body": "Yes, will rename to presence_column_idx.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1631040171",
    "pr_number": 63665,
    "pr_file": "src/Planner/PlannerJoinTree.cpp",
    "created_at": "2024-06-07T11:06:23+00:00",
    "commented_code": "return alias_column_step;\n }\n \n+std::unique_ptr<ExpressionStep> createAddingVirtualColumnsStep(",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "1631040171",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 63665,
        "pr_file": "src/Planner/PlannerJoinTree.cpp",
        "discussion_id": "1631040171",
        "commented_code": "@@ -606,6 +607,93 @@ std::unique_ptr<ExpressionStep> createComputeAliasColumnsStep(\n     return alias_column_step;\n }\n \n+std::unique_ptr<ExpressionStep> createAddingVirtualColumnsStep(",
        "comment_created_at": "2024-06-07T11:06:23+00:00",
        "comment_author": "divanik",
        "comment_body": "Name should reflect connection with _table column",
        "pr_file_module": null
      },
      {
        "comment_id": "1632353343",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 63665,
        "pr_file": "src/Planner/PlannerJoinTree.cpp",
        "discussion_id": "1631040171",
        "commented_code": "@@ -606,6 +607,93 @@ std::unique_ptr<ExpressionStep> createComputeAliasColumnsStep(\n     return alias_column_step;\n }\n \n+std::unique_ptr<ExpressionStep> createAddingVirtualColumnsStep(",
        "comment_created_at": "2024-06-09T17:05:09+00:00",
        "comment_author": "wudidapaopao",
        "comment_body": "I've relocated this logic to the common AddingTableNameVirtualColumnStep.cpp file and add \"TableName\" to the function's name to elucidate its functionality.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2270701096",
    "pr_number": 83837,
    "pr_file": "src/Core/Settings.cpp",
    "created_at": "2025-08-12T17:58:13+00:00",
    "commented_code": "DECLARE(UInt64, hnsw_candidate_list_size_for_search, 256, R\"(\n The size of the dynamic candidate list when searching the vector similarity index, also known as 'ef_search'.\n )\", BETA) \\\n+    DECLARE(Bool, allow_experimental_text_index_pipeline, true, R\"(",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2270701096",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83837,
        "pr_file": "src/Core/Settings.cpp",
        "discussion_id": "2270701096",
        "commented_code": "@@ -6745,6 +6745,9 @@ SELECT queries with LIMIT bigger than this setting cannot use vector similarity\n     DECLARE(UInt64, hnsw_candidate_list_size_for_search, 256, R\"(\n The size of the dynamic candidate list when searching the vector similarity index, also known as 'ef_search'.\n )\", BETA) \\\n+    DECLARE(Bool, allow_experimental_text_index_pipeline, true, R\"(",
        "comment_created_at": "2025-08-12T17:58:13+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "Let's move to after l. 6792 (after `allow_experimental_full_text_index`) and rename to `query_plan_direct_read_from_text_index` (most query plan optimizations start with this prefix, see \"src/Processors/QueryPlan/Optimizations/QueryPlanOptimizationSettings.cpp\").\nNote the feature itself is already marked \"experimental\", there is no need to name a related setting \"experimental\" as well..",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2244856409",
    "pr_number": 84547,
    "pr_file": "src/Common/ErrorCodes.cpp",
    "created_at": "2025-07-31T09:29:56+00:00",
    "commented_code": "M(744, SESSION_ID_EMPTY) \\\n     M(745, SERVER_OVERLOADED) \\\n     M(746, DEPENDENCIES_NOT_FOUND) \\\n-    M(747, FILECACHE_CANNOT_WRITE_THROUGH_CACHE_WITH_CONCURRENT_READS) /* private error code */ \\\n+    M(747, FILECACHE_CANNOT_WRITE_THROUGH_CACHE_WITH_CONCURRENT_READS) \\\n+    M(748, SHELL_COMMAND_CANNOT_WRITE_TO_FILE_DESCRIPTOR) /* private error code */ \\",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2244856409",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84547,
        "pr_file": "src/Common/ErrorCodes.cpp",
        "discussion_id": "2244856409",
        "commented_code": "@@ -626,7 +626,8 @@\n     M(744, SESSION_ID_EMPTY) \\\n     M(745, SERVER_OVERLOADED) \\\n     M(746, DEPENDENCIES_NOT_FOUND) \\\n-    M(747, FILECACHE_CANNOT_WRITE_THROUGH_CACHE_WITH_CONCURRENT_READS) /* private error code */ \\\n+    M(747, FILECACHE_CANNOT_WRITE_THROUGH_CACHE_WITH_CONCURRENT_READS) \\\n+    M(748, SHELL_COMMAND_CANNOT_WRITE_TO_FILE_DESCRIPTOR) /* private error code */ \\",
        "comment_created_at": "2025-07-31T09:29:56+00:00",
        "comment_author": "alexey-milovidov",
        "comment_body": "The name of this error code is ambiguous and allows misinterpretation.\r\nThe user might think that the shell command cannot write to a file descriptor.\r\nWhile this error code is thrown when the ClickHouse server cannot write data into a pipe from where the shell command reads.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2010424356",
    "pr_number": 77884,
    "pr_file": "src/Disks/IO/ThreadPoolRemoteFSReader.cpp",
    "created_at": "2025-03-24T15:34:36+00:00",
    "commented_code": "auto * fd = assert_cast<RemoteFSFileDescriptor *>(request.descriptor.get());\n     auto & reader = fd->getReader();\n+    auto * cached_reader = typeid_cast<CachedInMemoryReadBufferFromFile *>(&reader);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2010424356",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 77884,
        "pr_file": "src/Disks/IO/ThreadPoolRemoteFSReader.cpp",
        "discussion_id": "2010424356",
        "commented_code": "@@ -121,6 +124,7 @@ IAsynchronousReader::Result ThreadPoolRemoteFSReader::execute(Request request, b\n \n     auto * fd = assert_cast<RemoteFSFileDescriptor *>(request.descriptor.get());\n     auto & reader = fd->getReader();\n+    auto * cached_reader = typeid_cast<CachedInMemoryReadBufferFromFile *>(&reader);",
        "comment_created_at": "2025-03-24T15:34:36+00:00",
        "comment_author": "kssenii",
        "comment_body": "```suggestion\r\n    auto * page_cache_reader = typeid_cast<CachedInMemoryReadBufferFromFile *>(&reader);\r\n```\r\nTechnically `cached_reader` could also mean `CachedOnDiskReadBufferFromFile`, not only `CachedInMemoryReadBufferFromFile`, so `cached_reader` here is misleading.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2142334373",
    "pr_number": 81040,
    "pr_file": "src/Storages/ObjectStorageQueue/ObjectStorageQueueOrderedFileMetadata.cpp",
    "created_at": "2025-06-12T10:47:34+00:00",
    "commented_code": "paths = std::move(result);\n }\n \n+std::string ObjectStorageQueueOrderedFileMetadata::getHivePart(const std::string & file_path)\n+{\n+    std::string hive_part(VirtualColumnUtils::findHivePartitioningInPath(file_path));\n+    boost::replace_all(hive_part, \"_\", \"__\");\n+    boost::replace_all(hive_part, \"/\", \"_\");",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2142334373",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 81040,
        "pr_file": "src/Storages/ObjectStorageQueue/ObjectStorageQueueOrderedFileMetadata.cpp",
        "discussion_id": "2142334373",
        "commented_code": "@@ -642,4 +774,12 @@ void ObjectStorageQueueOrderedFileMetadata::filterOutProcessedAndFailed(\n     paths = std::move(result);\n }\n \n+std::string ObjectStorageQueueOrderedFileMetadata::getHivePart(const std::string & file_path)\n+{\n+    std::string hive_part(VirtualColumnUtils::findHivePartitioningInPath(file_path));\n+    boost::replace_all(hive_part, \"_\", \"__\");\n+    boost::replace_all(hive_part, \"/\", \"_\");",
        "comment_created_at": "2025-06-12T10:47:34+00:00",
        "comment_author": "kssenii",
        "comment_body": "As I see we do this as we use it as a node name in keeper so we want to kind of normalize it, right? The method name implies that we return hive part as is without normalization, so let's split it into two methods to make it more explicit and understandable",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2270540239",
    "pr_number": 85329,
    "pr_file": "src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp",
    "created_at": "2025-08-12T16:45:06+00:00",
    "commented_code": "}\n }\n \n+\n+void IcebergStreamingPositionDeleteTransform::initialize()\n+{\n+    for (size_t i = 0; i < delete_sources.size(); ++i)\n+    {\n+        auto & delete_source = delete_sources[i];\n+        size_t position_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::positions_column_name);\n+        size_t filename_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::data_file_path_column_name);\n+\n+        delete_source_column_indices.push_back(PositionDeleteFileIndexes{\n+            .filename_index = filename_index,\n+            .position_index = position_index\n+        });\n+        auto latest_chunk = delete_source->read();\n+        iterator_at_latest_chunks.push_back(0);\n+        if (latest_chunk.hasRows())\n+            latest_positions.insert(std::pair<size_t, size_t>{latest_chunk.getColumns()[delete_source_column_indices.back().position_index]->get64(0), i});\n+        latest_chunks.push_back(std::move(latest_chunk));\n+    }\n+}\n+\n+void IcebergStreamingPositionDeleteTransform::fetchNewChunkFromSource(size_t delete_source_index)\n+{\n+    auto latest_chunk = delete_sources[delete_source_index]->read();\n+    if (latest_chunk.hasRows())\n+        latest_positions.insert(std::pair<size_t, size_t>{latest_chunk.getColumns()[delete_source_column_indices.back().position_index]->get64(0), delete_source_index});",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2270540239",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85329,
        "pr_file": "src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp",
        "discussion_id": "2270540239",
        "commented_code": "@@ -159,6 +159,86 @@ void IcebergBitmapPositionDeleteTransform::initialize()\n     }\n }\n \n+\n+void IcebergStreamingPositionDeleteTransform::initialize()\n+{\n+    for (size_t i = 0; i < delete_sources.size(); ++i)\n+    {\n+        auto & delete_source = delete_sources[i];\n+        size_t position_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::positions_column_name);\n+        size_t filename_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::data_file_path_column_name);\n+\n+        delete_source_column_indices.push_back(PositionDeleteFileIndexes{\n+            .filename_index = filename_index,\n+            .position_index = position_index\n+        });\n+        auto latest_chunk = delete_source->read();\n+        iterator_at_latest_chunks.push_back(0);\n+        if (latest_chunk.hasRows())\n+            latest_positions.insert(std::pair<size_t, size_t>{latest_chunk.getColumns()[delete_source_column_indices.back().position_index]->get64(0), i});\n+        latest_chunks.push_back(std::move(latest_chunk));\n+    }\n+}\n+\n+void IcebergStreamingPositionDeleteTransform::fetchNewChunkFromSource(size_t delete_source_index)\n+{\n+    auto latest_chunk = delete_sources[delete_source_index]->read();\n+    if (latest_chunk.hasRows())\n+        latest_positions.insert(std::pair<size_t, size_t>{latest_chunk.getColumns()[delete_source_column_indices.back().position_index]->get64(0), delete_source_index});",
        "comment_created_at": "2025-08-12T16:45:06+00:00",
        "comment_author": "divanik",
        "comment_body": "Create a descriptive name for the expression: latest_chunk.getColumns()[delete_source_column_indices.back().position_index]->get64(0)",
        "pr_file_module": null
      },
      {
        "comment_id": "2270542533",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85329,
        "pr_file": "src/Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.cpp",
        "discussion_id": "2270540239",
        "commented_code": "@@ -159,6 +159,86 @@ void IcebergBitmapPositionDeleteTransform::initialize()\n     }\n }\n \n+\n+void IcebergStreamingPositionDeleteTransform::initialize()\n+{\n+    for (size_t i = 0; i < delete_sources.size(); ++i)\n+    {\n+        auto & delete_source = delete_sources[i];\n+        size_t position_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::positions_column_name);\n+        size_t filename_index = getColumnIndex(delete_source, IcebergPositionDeleteTransform::data_file_path_column_name);\n+\n+        delete_source_column_indices.push_back(PositionDeleteFileIndexes{\n+            .filename_index = filename_index,\n+            .position_index = position_index\n+        });\n+        auto latest_chunk = delete_source->read();\n+        iterator_at_latest_chunks.push_back(0);\n+        if (latest_chunk.hasRows())\n+            latest_positions.insert(std::pair<size_t, size_t>{latest_chunk.getColumns()[delete_source_column_indices.back().position_index]->get64(0), i});\n+        latest_chunks.push_back(std::move(latest_chunk));\n+    }\n+}\n+\n+void IcebergStreamingPositionDeleteTransform::fetchNewChunkFromSource(size_t delete_source_index)\n+{\n+    auto latest_chunk = delete_sources[delete_source_index]->read();\n+    if (latest_chunk.hasRows())\n+        latest_positions.insert(std::pair<size_t, size_t>{latest_chunk.getColumns()[delete_source_column_indices.back().position_index]->get64(0), delete_source_index});",
        "comment_created_at": "2025-08-12T16:46:09+00:00",
        "comment_author": "divanik",
        "comment_body": "delete_source_column_indices.back().position_index should be delete_source_column_indices[delete_source_index].position_index ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1300309904",
    "pr_number": 48941,
    "pr_file": "src/Storages/StorageMergeTree.cpp",
    "created_at": "2023-08-21T15:49:18+00:00",
    "commented_code": "{\n     std::lock_guard lock(currently_processing_in_background_mutex);\n \n-    std::vector<PartVersionWithName> part_versions_with_names;\n+    std::vector<PartVersionWithPartitionIdAndName> part_versions;\n     auto data_parts = getDataPartsVectorForInternalUsage();\n-    part_versions_with_names.reserve(data_parts.size());\n+    part_versions.reserve(data_parts.size());\n     for (const auto & part : data_parts)\n-        part_versions_with_names.emplace_back(PartVersionWithName{part->info.getDataVersion(), part->name});\n-    std::sort(part_versions_with_names.begin(), part_versions_with_names.end(), comparator);\n+        part_versions.emplace_back(PartVersionWithPartitionIdAndName{part->info.getDataVersion(), part->info.partition_id, part->name});\n+    std::sort(part_versions.begin(), part_versions.end(), comparator);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "1300309904",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 48941,
        "pr_file": "src/Storages/StorageMergeTree.cpp",
        "discussion_id": "1300309904",
        "commented_code": "@@ -710,27 +714,32 @@ std::vector<MergeTreeMutationStatus> StorageMergeTree::getMutationsStatus() cons\n {\n     std::lock_guard lock(currently_processing_in_background_mutex);\n \n-    std::vector<PartVersionWithName> part_versions_with_names;\n+    std::vector<PartVersionWithPartitionIdAndName> part_versions;\n     auto data_parts = getDataPartsVectorForInternalUsage();\n-    part_versions_with_names.reserve(data_parts.size());\n+    part_versions.reserve(data_parts.size());\n     for (const auto & part : data_parts)\n-        part_versions_with_names.emplace_back(PartVersionWithName{part->info.getDataVersion(), part->name});\n-    std::sort(part_versions_with_names.begin(), part_versions_with_names.end(), comparator);\n+        part_versions.emplace_back(PartVersionWithPartitionIdAndName{part->info.getDataVersion(), part->info.partition_id, part->name});\n+    std::sort(part_versions.begin(), part_versions.end(), comparator);",
        "comment_created_at": "2023-08-21T15:49:18+00:00",
        "comment_author": "Enmk",
        "comment_body": "IMO it might be a good time to rename `comparator` to something like `orderByVersion`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2254202851",
    "pr_number": 84710,
    "pr_file": "src/Interpreters/DatabaseCatalog.cpp",
    "created_at": "2025-08-05T12:30:59+00:00",
    "commented_code": "table.table->drop();\n     }\n \n+    /// Check if we are interested in a particular disk\n+    ///   or it is better to bypass it e.g. to avoid interactions with a remote storage\n+    auto is_disk_eligible_for_search =\n+        [this](DiskPtr disk, std::shared_ptr<MergeTreeData> tbl)",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2254202851",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84710,
        "pr_file": "src/Interpreters/DatabaseCatalog.cpp",
        "discussion_id": "2254202851",
        "commented_code": "@@ -1459,11 +1466,34 @@ void DatabaseCatalog::dropTableFinally(const TableMarkedAsDropped & table)\n         table.table->drop();\n     }\n \n+    /// Check if we are interested in a particular disk\n+    ///   or it is better to bypass it e.g. to avoid interactions with a remote storage\n+    auto is_disk_eligible_for_search =\n+        [this](DiskPtr disk, std::shared_ptr<MergeTreeData> tbl)",
        "comment_created_at": "2025-08-05T12:30:59+00:00",
        "comment_author": "azat",
        "comment_body": "AFAIR this style is more common\r\n\r\n```suggestion\r\n    auto is_disk_eligible_for_search = [this](DiskPtr disk, std::shared_ptr<MergeTreeData> table)\r\n```\r\n\r\nAlso consider using `storage` or `table` over `tbl`, these abbrev is not worth it, and it is more common to avoid such in the code",
        "pr_file_module": null
      },
      {
        "comment_id": "2254222289",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84710,
        "pr_file": "src/Interpreters/DatabaseCatalog.cpp",
        "discussion_id": "2254202851",
        "commented_code": "@@ -1459,11 +1466,34 @@ void DatabaseCatalog::dropTableFinally(const TableMarkedAsDropped & table)\n         table.table->drop();\n     }\n \n+    /// Check if we are interested in a particular disk\n+    ///   or it is better to bypass it e.g. to avoid interactions with a remote storage\n+    auto is_disk_eligible_for_search =\n+        [this](DiskPtr disk, std::shared_ptr<MergeTreeData> tbl)",
        "comment_created_at": "2025-08-05T12:39:14+00:00",
        "comment_author": "ilejn",
        "comment_body": "I guess that using 'table' can raise a warning like \"Variable shadowed\".\r\nOk, let it be \"storage\". ",
        "pr_file_module": null
      }
    ]
  }
]