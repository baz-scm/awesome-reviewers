[
  {
    "discussion_id": "2281251739",
    "pr_number": 85637,
    "pr_file": "src/Interpreters/InterpreterSystemQuery.cpp",
    "created_at": "2025-08-18T04:07:39+00:00",
    "commented_code": "throw Exception(ErrorCodes::LOGICAL_ERROR, \"Invalid query\");\n }\n \n-bool InterpreterSystemQuery::dropReplicaImpl(ASTSystemQuery & query, const StoragePtr & table)\n+bool InterpreterSystemQuery::dropStorageReplica(const String & query_replica, const StoragePtr & storage)\n {\n-    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(table.get());\n+    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(storage.get());\n     if (!storage_replicated)\n         return false;\n \n     const auto & replica_name = storage_replicated->getReplicaName();\n \n     /// Do not allow to drop local replicas and active remote replicas\n-    if (query.replica == replica_name)\n+    if (query_replica == replica_name)\n         throw Exception(ErrorCodes::TABLE_WAS_NOT_DROPPED,\n                         \"We can't drop local replica, please use `DROP TABLE` if you want \"\n                         \"to clean the data and drop this replica\");\n \n-    storage_replicated->dropReplica(query.replica, log);\n-    LOG_TRACE(log, \"Dropped replica {} of {}\", query.replica, table->getStorageID().getNameForLogs());\n+    storage_replicated->dropReplica(query_replica, log);\n+    LOG_TRACE(log, \"Dropped replica {} of {}\", query_replica, storage->getStorageID().getNameForLogs());\n \n     return true;\n }\n \n+void InterpreterSystemQuery::dropStorageReplicasFromDatabase(const String & query_replica, DatabasePtr database)\n+{\n+    for (auto iterator = database->getTablesIterator(getContext()); iterator->isValid(); iterator->next())\n+        dropStorageReplica(query_replica, iterator->table());\n+\n+    LOG_TRACE(\n+        log, \"Dropped storage replica from {} of Replicated database {}\", query_replica, backQuoteIfNeed(database->getDatabaseName()));\n+}\n+\n+DatabasePtr InterpreterSystemQuery::restoreDatabaseFromKeeperPath(const String & zookeeper_path, const String & restoring_database_name)\n+{\n+    auto zookeeper = getContext()->getZooKeeper();\n+\n+    if (!zookeeper->exists(zookeeper_path + \"/metadata\"))\n+        return {};\n+\n+    Strings escaped_table_names;\n+    escaped_table_names = zookeeper->getChildren(zookeeper_path + \"/metadata\");\n+\n+    std::vector<String> paths_to_fetch;\n+    paths_to_fetch.reserve(escaped_table_names.size());\n+\n+    for (const auto & table : escaped_table_names)\n+        paths_to_fetch.push_back(zookeeper_path + \"/metadata/\" + table);\n+\n+    auto table_metadata = zookeeper->tryGet(paths_to_fetch);\n+    std::map<String, String> table_name_to_metadata;\n+    for (size_t i = 0; i < escaped_table_names.size(); ++i)\n+    {\n+        auto & res = table_metadata[i];\n+        if (res.error != Coordination::Error::ZOK)\n+            throw zkutil::KeeperException::fromPath(res.error, fs::path(zookeeper_path) / \"metadata\");\n+\n+        table_name_to_metadata.emplace(unescapeForFileName(escaped_table_names[i]), std::move(res.data));\n+    }\n+    String create_query = fmt::format(\"CREATE DATABASE `{}` ENGINE=Atomic\", restoring_database_name);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2281251739",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85637,
        "pr_file": "src/Interpreters/InterpreterSystemQuery.cpp",
        "discussion_id": "2281251739",
        "commented_code": "@@ -1219,26 +1229,193 @@ void InterpreterSystemQuery::dropReplica(ASTSystemQuery & query)\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Invalid query\");\n }\n \n-bool InterpreterSystemQuery::dropReplicaImpl(ASTSystemQuery & query, const StoragePtr & table)\n+bool InterpreterSystemQuery::dropStorageReplica(const String & query_replica, const StoragePtr & storage)\n {\n-    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(table.get());\n+    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(storage.get());\n     if (!storage_replicated)\n         return false;\n \n     const auto & replica_name = storage_replicated->getReplicaName();\n \n     /// Do not allow to drop local replicas and active remote replicas\n-    if (query.replica == replica_name)\n+    if (query_replica == replica_name)\n         throw Exception(ErrorCodes::TABLE_WAS_NOT_DROPPED,\n                         \"We can't drop local replica, please use `DROP TABLE` if you want \"\n                         \"to clean the data and drop this replica\");\n \n-    storage_replicated->dropReplica(query.replica, log);\n-    LOG_TRACE(log, \"Dropped replica {} of {}\", query.replica, table->getStorageID().getNameForLogs());\n+    storage_replicated->dropReplica(query_replica, log);\n+    LOG_TRACE(log, \"Dropped replica {} of {}\", query_replica, storage->getStorageID().getNameForLogs());\n \n     return true;\n }\n \n+void InterpreterSystemQuery::dropStorageReplicasFromDatabase(const String & query_replica, DatabasePtr database)\n+{\n+    for (auto iterator = database->getTablesIterator(getContext()); iterator->isValid(); iterator->next())\n+        dropStorageReplica(query_replica, iterator->table());\n+\n+    LOG_TRACE(\n+        log, \"Dropped storage replica from {} of Replicated database {}\", query_replica, backQuoteIfNeed(database->getDatabaseName()));\n+}\n+\n+DatabasePtr InterpreterSystemQuery::restoreDatabaseFromKeeperPath(const String & zookeeper_path, const String & restoring_database_name)\n+{\n+    auto zookeeper = getContext()->getZooKeeper();\n+\n+    if (!zookeeper->exists(zookeeper_path + \"/metadata\"))\n+        return {};\n+\n+    Strings escaped_table_names;\n+    escaped_table_names = zookeeper->getChildren(zookeeper_path + \"/metadata\");\n+\n+    std::vector<String> paths_to_fetch;\n+    paths_to_fetch.reserve(escaped_table_names.size());\n+\n+    for (const auto & table : escaped_table_names)\n+        paths_to_fetch.push_back(zookeeper_path + \"/metadata/\" + table);\n+\n+    auto table_metadata = zookeeper->tryGet(paths_to_fetch);\n+    std::map<String, String> table_name_to_metadata;\n+    for (size_t i = 0; i < escaped_table_names.size(); ++i)\n+    {\n+        auto & res = table_metadata[i];\n+        if (res.error != Coordination::Error::ZOK)\n+            throw zkutil::KeeperException::fromPath(res.error, fs::path(zookeeper_path) / \"metadata\");\n+\n+        table_name_to_metadata.emplace(unescapeForFileName(escaped_table_names[i]), std::move(res.data));\n+    }\n+    String create_query = fmt::format(\"CREATE DATABASE `{}` ENGINE=Atomic\", restoring_database_name);",
        "comment_created_at": "2025-08-18T04:07:39+00:00",
        "comment_author": "evillique",
        "comment_body": "If I understand correctly, this would create a persistent database with metadata on disk. That means if we restart the server while executing this query, then this database will be left on disk, and the table data will never actually be dropped.\r\n\r\nFor RMT, considering that there is a static method to drop the data in ZK, creating a full table looks just a bit overkill, as it will probably start downloading actual parts from other servers and stuff.",
        "pr_file_module": null
      },
      {
        "comment_id": "2281274663",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85637,
        "pr_file": "src/Interpreters/InterpreterSystemQuery.cpp",
        "discussion_id": "2281251739",
        "commented_code": "@@ -1219,26 +1229,193 @@ void InterpreterSystemQuery::dropReplica(ASTSystemQuery & query)\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Invalid query\");\n }\n \n-bool InterpreterSystemQuery::dropReplicaImpl(ASTSystemQuery & query, const StoragePtr & table)\n+bool InterpreterSystemQuery::dropStorageReplica(const String & query_replica, const StoragePtr & storage)\n {\n-    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(table.get());\n+    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(storage.get());\n     if (!storage_replicated)\n         return false;\n \n     const auto & replica_name = storage_replicated->getReplicaName();\n \n     /// Do not allow to drop local replicas and active remote replicas\n-    if (query.replica == replica_name)\n+    if (query_replica == replica_name)\n         throw Exception(ErrorCodes::TABLE_WAS_NOT_DROPPED,\n                         \"We can't drop local replica, please use `DROP TABLE` if you want \"\n                         \"to clean the data and drop this replica\");\n \n-    storage_replicated->dropReplica(query.replica, log);\n-    LOG_TRACE(log, \"Dropped replica {} of {}\", query.replica, table->getStorageID().getNameForLogs());\n+    storage_replicated->dropReplica(query_replica, log);\n+    LOG_TRACE(log, \"Dropped replica {} of {}\", query_replica, storage->getStorageID().getNameForLogs());\n \n     return true;\n }\n \n+void InterpreterSystemQuery::dropStorageReplicasFromDatabase(const String & query_replica, DatabasePtr database)\n+{\n+    for (auto iterator = database->getTablesIterator(getContext()); iterator->isValid(); iterator->next())\n+        dropStorageReplica(query_replica, iterator->table());\n+\n+    LOG_TRACE(\n+        log, \"Dropped storage replica from {} of Replicated database {}\", query_replica, backQuoteIfNeed(database->getDatabaseName()));\n+}\n+\n+DatabasePtr InterpreterSystemQuery::restoreDatabaseFromKeeperPath(const String & zookeeper_path, const String & restoring_database_name)\n+{\n+    auto zookeeper = getContext()->getZooKeeper();\n+\n+    if (!zookeeper->exists(zookeeper_path + \"/metadata\"))\n+        return {};\n+\n+    Strings escaped_table_names;\n+    escaped_table_names = zookeeper->getChildren(zookeeper_path + \"/metadata\");\n+\n+    std::vector<String> paths_to_fetch;\n+    paths_to_fetch.reserve(escaped_table_names.size());\n+\n+    for (const auto & table : escaped_table_names)\n+        paths_to_fetch.push_back(zookeeper_path + \"/metadata/\" + table);\n+\n+    auto table_metadata = zookeeper->tryGet(paths_to_fetch);\n+    std::map<String, String> table_name_to_metadata;\n+    for (size_t i = 0; i < escaped_table_names.size(); ++i)\n+    {\n+        auto & res = table_metadata[i];\n+        if (res.error != Coordination::Error::ZOK)\n+            throw zkutil::KeeperException::fromPath(res.error, fs::path(zookeeper_path) / \"metadata\");\n+\n+        table_name_to_metadata.emplace(unescapeForFileName(escaped_table_names[i]), std::move(res.data));\n+    }\n+    String create_query = fmt::format(\"CREATE DATABASE `{}` ENGINE=Atomic\", restoring_database_name);",
        "comment_created_at": "2025-08-18T04:33:16+00:00",
        "comment_author": "tuanpach",
        "comment_body": "> If I understand correctly, this would create a persistent database with metadata on disk. That means if we restart the server while executing this query, then this database will be left on disk, and the table data will never actually be dropped.\r\n\r\nYes. Should we add logic to clean up this database when the instance starts up?\r\n\r\n> For RMT, considering that there is a static method to drop the data in ZK, creating a full table looks just a bit overkill, as it will probably start downloading actual parts from other servers and stuff.\r\n\r\nDo you think we should only restore Shared*MergeTree tables, or storages containing Shared*MergeTree inner table?",
        "pr_file_module": null
      },
      {
        "comment_id": "2283971499",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85637,
        "pr_file": "src/Interpreters/InterpreterSystemQuery.cpp",
        "discussion_id": "2281251739",
        "commented_code": "@@ -1219,26 +1229,193 @@ void InterpreterSystemQuery::dropReplica(ASTSystemQuery & query)\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Invalid query\");\n }\n \n-bool InterpreterSystemQuery::dropReplicaImpl(ASTSystemQuery & query, const StoragePtr & table)\n+bool InterpreterSystemQuery::dropStorageReplica(const String & query_replica, const StoragePtr & storage)\n {\n-    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(table.get());\n+    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(storage.get());\n     if (!storage_replicated)\n         return false;\n \n     const auto & replica_name = storage_replicated->getReplicaName();\n \n     /// Do not allow to drop local replicas and active remote replicas\n-    if (query.replica == replica_name)\n+    if (query_replica == replica_name)\n         throw Exception(ErrorCodes::TABLE_WAS_NOT_DROPPED,\n                         \"We can't drop local replica, please use `DROP TABLE` if you want \"\n                         \"to clean the data and drop this replica\");\n \n-    storage_replicated->dropReplica(query.replica, log);\n-    LOG_TRACE(log, \"Dropped replica {} of {}\", query.replica, table->getStorageID().getNameForLogs());\n+    storage_replicated->dropReplica(query_replica, log);\n+    LOG_TRACE(log, \"Dropped replica {} of {}\", query_replica, storage->getStorageID().getNameForLogs());\n \n     return true;\n }\n \n+void InterpreterSystemQuery::dropStorageReplicasFromDatabase(const String & query_replica, DatabasePtr database)\n+{\n+    for (auto iterator = database->getTablesIterator(getContext()); iterator->isValid(); iterator->next())\n+        dropStorageReplica(query_replica, iterator->table());\n+\n+    LOG_TRACE(\n+        log, \"Dropped storage replica from {} of Replicated database {}\", query_replica, backQuoteIfNeed(database->getDatabaseName()));\n+}\n+\n+DatabasePtr InterpreterSystemQuery::restoreDatabaseFromKeeperPath(const String & zookeeper_path, const String & restoring_database_name)\n+{\n+    auto zookeeper = getContext()->getZooKeeper();\n+\n+    if (!zookeeper->exists(zookeeper_path + \"/metadata\"))\n+        return {};\n+\n+    Strings escaped_table_names;\n+    escaped_table_names = zookeeper->getChildren(zookeeper_path + \"/metadata\");\n+\n+    std::vector<String> paths_to_fetch;\n+    paths_to_fetch.reserve(escaped_table_names.size());\n+\n+    for (const auto & table : escaped_table_names)\n+        paths_to_fetch.push_back(zookeeper_path + \"/metadata/\" + table);\n+\n+    auto table_metadata = zookeeper->tryGet(paths_to_fetch);\n+    std::map<String, String> table_name_to_metadata;\n+    for (size_t i = 0; i < escaped_table_names.size(); ++i)\n+    {\n+        auto & res = table_metadata[i];\n+        if (res.error != Coordination::Error::ZOK)\n+            throw zkutil::KeeperException::fromPath(res.error, fs::path(zookeeper_path) / \"metadata\");\n+\n+        table_name_to_metadata.emplace(unescapeForFileName(escaped_table_names[i]), std::move(res.data));\n+    }\n+    String create_query = fmt::format(\"CREATE DATABASE `{}` ENGINE=Atomic\", restoring_database_name);",
        "comment_created_at": "2025-08-19T03:30:24+00:00",
        "comment_author": "evillique",
        "comment_body": "> Yes. Should we add logic to clean up this database when the instance starts up?\r\n\r\nWell, at least that is definitely needed, but I don't particularly like making a persistent DB with real tables at all.\r\n\r\n> Do you think we should only restore SharedMergeTree tables, or storages containing SharedMergeTree inner table?\r\n\r\nWell, the current option is ok and universal, but not optimal in terms of the things I think RMT will start doing as soon as we create it. For SMT there is no other way than to create the table.\r\n\r\nThe ideal way I see it is that we go through table definitions, call dropReplica for RMT tables, create SMT tables (without even creating a database, call drop replica, and then drop this table), and skip all the other engines. The only thing that needs investigation is whether it can be broken by not having some tables and the dependencies somehow breaking. This way this would execute as fast as it can and require the minimum amount of resources. But I don't know whether this level of optimization is needed for a SYSTEM query. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2283926579",
    "pr_number": 85637,
    "pr_file": "src/Interpreters/InterpreterSystemQuery.cpp",
    "created_at": "2025-08-19T02:53:05+00:00",
    "commented_code": "throw Exception(ErrorCodes::LOGICAL_ERROR, \"Invalid query\");\n }\n \n-bool InterpreterSystemQuery::dropReplicaImpl(ASTSystemQuery & query, const StoragePtr & table)\n+bool InterpreterSystemQuery::dropStorageReplica(const String & query_replica, const StoragePtr & storage)\n {\n-    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(table.get());\n+    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(storage.get());\n     if (!storage_replicated)\n         return false;\n \n     const auto & replica_name = storage_replicated->getReplicaName();\n \n     /// Do not allow to drop local replicas and active remote replicas\n-    if (query.replica == replica_name)\n+    if (query_replica == replica_name)\n         throw Exception(ErrorCodes::TABLE_WAS_NOT_DROPPED,\n                         \"We can't drop local replica, please use `DROP TABLE` if you want \"\n                         \"to clean the data and drop this replica\");\n \n-    storage_replicated->dropReplica(query.replica, log);\n-    LOG_TRACE(log, \"Dropped replica {} of {}\", query.replica, table->getStorageID().getNameForLogs());\n+    storage_replicated->dropReplica(query_replica, log);\n+    LOG_TRACE(log, \"Dropped replica {} of {}\", query_replica, storage->getStorageID().getNameForLogs());\n \n     return true;\n }\n \n+void InterpreterSystemQuery::dropStorageReplicasFromDatabase(const String & query_replica, DatabasePtr database)\n+{\n+    for (auto iterator = database->getTablesIterator(getContext()); iterator->isValid(); iterator->next())\n+        dropStorageReplica(query_replica, iterator->table());\n+\n+    LOG_TRACE(\n+        log, \"Dropped storage replica from {} of Replicated database {}\", query_replica, backQuoteIfNeed(database->getDatabaseName()));\n+}\n+\n+DatabasePtr InterpreterSystemQuery::restoreDatabaseFromKeeperPath(const String & zookeeper_path, const String & restoring_database_name)\n+{\n+    auto zookeeper = getContext()->getZooKeeper();\n+\n+    if (!zookeeper->exists(zookeeper_path + \"/metadata\"))\n+        return {};",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2283926579",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85637,
        "pr_file": "src/Interpreters/InterpreterSystemQuery.cpp",
        "discussion_id": "2283926579",
        "commented_code": "@@ -1219,26 +1229,193 @@ void InterpreterSystemQuery::dropReplica(ASTSystemQuery & query)\n         throw Exception(ErrorCodes::LOGICAL_ERROR, \"Invalid query\");\n }\n \n-bool InterpreterSystemQuery::dropReplicaImpl(ASTSystemQuery & query, const StoragePtr & table)\n+bool InterpreterSystemQuery::dropStorageReplica(const String & query_replica, const StoragePtr & storage)\n {\n-    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(table.get());\n+    auto * storage_replicated = dynamic_cast<StorageReplicatedMergeTree *>(storage.get());\n     if (!storage_replicated)\n         return false;\n \n     const auto & replica_name = storage_replicated->getReplicaName();\n \n     /// Do not allow to drop local replicas and active remote replicas\n-    if (query.replica == replica_name)\n+    if (query_replica == replica_name)\n         throw Exception(ErrorCodes::TABLE_WAS_NOT_DROPPED,\n                         \"We can't drop local replica, please use `DROP TABLE` if you want \"\n                         \"to clean the data and drop this replica\");\n \n-    storage_replicated->dropReplica(query.replica, log);\n-    LOG_TRACE(log, \"Dropped replica {} of {}\", query.replica, table->getStorageID().getNameForLogs());\n+    storage_replicated->dropReplica(query_replica, log);\n+    LOG_TRACE(log, \"Dropped replica {} of {}\", query_replica, storage->getStorageID().getNameForLogs());\n \n     return true;\n }\n \n+void InterpreterSystemQuery::dropStorageReplicasFromDatabase(const String & query_replica, DatabasePtr database)\n+{\n+    for (auto iterator = database->getTablesIterator(getContext()); iterator->isValid(); iterator->next())\n+        dropStorageReplica(query_replica, iterator->table());\n+\n+    LOG_TRACE(\n+        log, \"Dropped storage replica from {} of Replicated database {}\", query_replica, backQuoteIfNeed(database->getDatabaseName()));\n+}\n+\n+DatabasePtr InterpreterSystemQuery::restoreDatabaseFromKeeperPath(const String & zookeeper_path, const String & restoring_database_name)\n+{\n+    auto zookeeper = getContext()->getZooKeeper();\n+\n+    if (!zookeeper->exists(zookeeper_path + \"/metadata\"))\n+        return {};",
        "comment_created_at": "2025-08-19T02:53:05+00:00",
        "comment_author": "evillique",
        "comment_body": "I would check whether the replica we are trying to drop is in replicas of zookeeper_path + `/replicas` of the database. If it is not there, then either the previous attempt succeeded or this replica dropped itself -> dropped all the tables, so that we would not re-create all the tables for nothing.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2273465207",
    "pr_number": 85250,
    "pr_file": "src/Storages/ObjectStorage/DataLakes/Iceberg/Compaction.cpp",
    "created_at": "2025-08-13T13:24:28+00:00",
    "commented_code": "+#include <string>\n+#include <Formats/FormatFactory.h>\n+#include <IO/CompressionMethod.h>\n+#include <Interpreters/Context.h>\n+#include <Processors/Formats/IRowOutputFormat.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Compaction.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>\n+#include <fmt/format.h>\n+#include <Poco/JSON/Object.h>\n+#include <Poco/JSON/Stringifier.h>\n+#include <Common/Logger.h>\n+#include <Core/Settings.h>\n+#include <Disks/ObjectStorages/StoredObject.h>\n+#include <Columns/IColumn.h>\n+#include <Core/ColumnsWithTypeAndName.h>\n+#include <Interpreters/Cache/FileSegment.h>\n+#include <Storages/ColumnsDescription.h>\n+#include <Storages/ObjectStorage/DataLakes/Common.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Constant.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergWrites.h>\n+#include <Storages/ObjectStorage/StorageObjectStorageSource.h>\n+\n+#if USE_AVRO\n+\n+namespace DB::Setting\n+{\n+    extern const SettingsMilliseconds iceberg_compaction_backoff_time;\n+}\n+\n+namespace Iceberg\n+{\n+\n+using namespace DB;\n+\n+struct ManifestFilePlan\n+{\n+    String path;\n+    std::vector<String> manifest_lists_path;\n+\n+    FileNamesGenerator::Result patched_path;\n+};\n+\n+struct DataFilePlan\n+{\n+    ParsedDataFileInfo parsed_data_file_info;\n+    std::shared_ptr<ManifestFilePlan> manifest_list;\n+\n+    FileNamesGenerator::Result patched_path;\n+    UInt64 new_records_count = 0;\n+};\n+\n+class ParititonEncoder\n+{\n+public:\n+    size_t encodePartition(const Row & row)\n+    {\n+        if (auto it = partition_value_to_index.find(row); it != partition_value_to_index.end())\n+            return it->second;\n+\n+        partition_value_to_index[row] = partition_values.size();\n+        partition_values.push_back(row);\n+        return partition_value_to_index[row];\n+    }\n+\n+    const Row & getPartitionValue(size_t partition_index) const { return partition_values.at(partition_index); }\n+\n+private:\n+    struct PartitionValueHasher\n+    {\n+        std::hash<String> hasher;\n+        size_t operator()(const Row & row) const\n+        {\n+            size_t result = 0;\n+            for (const auto & value : row)\n+                result ^= hasher(value.dump());\n+            return result;\n+        }\n+    };\n+\n+    std::unordered_map<Row, size_t, PartitionValueHasher> partition_value_to_index;\n+    std::vector<Row> partition_values;\n+};\n+\n+/// Plan of compaction consists information about all data files and what delete files should be applied for them.\n+/// Also it contains some other information about previous metadata.\n+struct Plan\n+{\n+    bool need_optimize = false;\n+    using Partition = std::vector<std::shared_ptr<DataFilePlan>>;\n+    std::vector<Partition> partitions;\n+    IcebergMetadata::IcebergHistory history;\n+    std::unordered_map<String, Int64> manifest_file_to_first_snapshot;\n+    std::unordered_map<String, std::vector<String>> manifest_list_to_manifest_files;\n+    std::unordered_map<Int64, std::vector<std::shared_ptr<DataFilePlan>>> snapshot_id_to_data_files;\n+    std::unordered_map<String, std::shared_ptr<DataFilePlan>> path_to_data_file;\n+    std::unique_ptr<IcebergMetadata> iceberg_metadata;\n+    ParititonEncoder partition_encoder;\n+};\n+\n+Plan getPlan(\n+    ObjectStoragePtr object_storage, StorageObjectStorageConfigurationPtr configuration, ContextPtr context, FileNamesGenerator & generator)\n+{\n+    auto metadata = IcebergMetadata::create(object_storage, configuration, context);\n+    std::unique_ptr<IcebergMetadata> iceberg_metadata(static_cast<IcebergMetadata *>(metadata.release()));\n+    auto snapshots_info = iceberg_metadata->getHistory(context);\n+\n+    Plan plan;\n+\n+    std::vector<ManifestFileEntry> all_positional_delete_files;\n+    std::unordered_map<String, std::shared_ptr<ManifestFilePlan>> manifest_files;\n+    for (const auto & snapshot : snapshots_info)\n+    {\n+        auto manifest_list = iceberg_metadata->getManifestList(context, snapshot.manifest_list_path);\n+        for (const auto & manifest_file : manifest_list)\n+        {\n+            plan.manifest_list_to_manifest_files[snapshot.manifest_list_path].push_back(manifest_file.manifest_file_path);\n+            if (!plan.manifest_file_to_first_snapshot.contains(manifest_file.manifest_file_path))\n+                plan.manifest_file_to_first_snapshot[manifest_file.manifest_file_path] = snapshot.snapshot_id;\n+            auto manifest_file_content = iceberg_metadata->tryGetManifestFile(\n+                context, manifest_file.manifest_file_path, manifest_file.added_sequence_number, manifest_file.added_snapshot_id);\n+\n+            if (!manifest_files.contains(manifest_file.manifest_file_path))\n+            {\n+                manifest_files[manifest_file.manifest_file_path] = std::make_shared<ManifestFilePlan>();\n+                manifest_files[manifest_file.manifest_file_path]->path = manifest_file.manifest_file_path;\n+            }\n+            manifest_files[manifest_file.manifest_file_path]->manifest_lists_path.push_back(snapshot.manifest_list_path);\n+            auto data_files = manifest_file_content->getFiles(FileContentType::DATA);\n+            auto positional_delete_files = manifest_file_content->getFiles(FileContentType::POSITION_DELETE);\n+            for (const auto & pos_delete_file : positional_delete_files)\n+                all_positional_delete_files.push_back(pos_delete_file);\n+\n+            for (const auto & data_file : data_files)\n+            {\n+                auto partition_index = plan.partition_encoder.encodePartition(data_file.partition_key_value);\n+                if (plan.partitions.size() <= partition_index)\n+                    plan.partitions.push_back({});\n+\n+                ParsedDataFileInfo parsed_data_file_info(configuration, data_file, {});\n+                parsed_data_file_info.sequence_number = data_file.added_sequence_number;\n+                std::shared_ptr<DataFilePlan> data_file_ptr;\n+                if (!plan.path_to_data_file.contains(manifest_file.manifest_file_path))\n+                {\n+                    data_file_ptr = std::make_shared<DataFilePlan>(DataFilePlan{\n+                        .parsed_data_file_info = parsed_data_file_info,\n+                        .manifest_list = manifest_files[manifest_file.manifest_file_path],\n+                        .patched_path = generator.generateDataFileName()});\n+                    plan.path_to_data_file[manifest_file.manifest_file_path] = data_file_ptr;\n+                }\n+                else\n+                {\n+                    data_file_ptr = plan.path_to_data_file[manifest_file.manifest_file_path];\n+                }\n+                plan.partitions[partition_index].push_back(data_file_ptr);\n+                plan.snapshot_id_to_data_files[snapshot.snapshot_id].push_back(plan.partitions[partition_index].back());\n+            }\n+        }\n+    }\n+\n+    for (const auto & delete_file : all_positional_delete_files)\n+    {\n+        auto partition_index = plan.partition_encoder.encodePartition(delete_file.partition_key_value);\n+        if (partition_index >= plan.partitions.size())\n+            continue;\n+\n+        std::vector<Iceberg::ManifestFileEntry> result_delete_files;\n+        for (auto & data_file : plan.partitions[partition_index])\n+        {\n+            if (data_file->parsed_data_file_info.sequence_number <= delete_file.added_sequence_number)\n+                data_file->parsed_data_file_info.position_deletes_objects.push_back(delete_file);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2273465207",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85250,
        "pr_file": "src/Storages/ObjectStorage/DataLakes/Iceberg/Compaction.cpp",
        "discussion_id": "2273465207",
        "commented_code": "@@ -0,0 +1,515 @@\n+#include <string>\n+#include <Formats/FormatFactory.h>\n+#include <IO/CompressionMethod.h>\n+#include <Interpreters/Context.h>\n+#include <Processors/Formats/IRowOutputFormat.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Compaction.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>\n+#include <fmt/format.h>\n+#include <Poco/JSON/Object.h>\n+#include <Poco/JSON/Stringifier.h>\n+#include <Common/Logger.h>\n+#include <Core/Settings.h>\n+#include <Disks/ObjectStorages/StoredObject.h>\n+#include <Columns/IColumn.h>\n+#include <Core/ColumnsWithTypeAndName.h>\n+#include <Interpreters/Cache/FileSegment.h>\n+#include <Storages/ColumnsDescription.h>\n+#include <Storages/ObjectStorage/DataLakes/Common.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Constant.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergWrites.h>\n+#include <Storages/ObjectStorage/StorageObjectStorageSource.h>\n+\n+#if USE_AVRO\n+\n+namespace DB::Setting\n+{\n+    extern const SettingsMilliseconds iceberg_compaction_backoff_time;\n+}\n+\n+namespace Iceberg\n+{\n+\n+using namespace DB;\n+\n+struct ManifestFilePlan\n+{\n+    String path;\n+    std::vector<String> manifest_lists_path;\n+\n+    FileNamesGenerator::Result patched_path;\n+};\n+\n+struct DataFilePlan\n+{\n+    ParsedDataFileInfo parsed_data_file_info;\n+    std::shared_ptr<ManifestFilePlan> manifest_list;\n+\n+    FileNamesGenerator::Result patched_path;\n+    UInt64 new_records_count = 0;\n+};\n+\n+class ParititonEncoder\n+{\n+public:\n+    size_t encodePartition(const Row & row)\n+    {\n+        if (auto it = partition_value_to_index.find(row); it != partition_value_to_index.end())\n+            return it->second;\n+\n+        partition_value_to_index[row] = partition_values.size();\n+        partition_values.push_back(row);\n+        return partition_value_to_index[row];\n+    }\n+\n+    const Row & getPartitionValue(size_t partition_index) const { return partition_values.at(partition_index); }\n+\n+private:\n+    struct PartitionValueHasher\n+    {\n+        std::hash<String> hasher;\n+        size_t operator()(const Row & row) const\n+        {\n+            size_t result = 0;\n+            for (const auto & value : row)\n+                result ^= hasher(value.dump());\n+            return result;\n+        }\n+    };\n+\n+    std::unordered_map<Row, size_t, PartitionValueHasher> partition_value_to_index;\n+    std::vector<Row> partition_values;\n+};\n+\n+/// Plan of compaction consists information about all data files and what delete files should be applied for them.\n+/// Also it contains some other information about previous metadata.\n+struct Plan\n+{\n+    bool need_optimize = false;\n+    using Partition = std::vector<std::shared_ptr<DataFilePlan>>;\n+    std::vector<Partition> partitions;\n+    IcebergMetadata::IcebergHistory history;\n+    std::unordered_map<String, Int64> manifest_file_to_first_snapshot;\n+    std::unordered_map<String, std::vector<String>> manifest_list_to_manifest_files;\n+    std::unordered_map<Int64, std::vector<std::shared_ptr<DataFilePlan>>> snapshot_id_to_data_files;\n+    std::unordered_map<String, std::shared_ptr<DataFilePlan>> path_to_data_file;\n+    std::unique_ptr<IcebergMetadata> iceberg_metadata;\n+    ParititonEncoder partition_encoder;\n+};\n+\n+Plan getPlan(\n+    ObjectStoragePtr object_storage, StorageObjectStorageConfigurationPtr configuration, ContextPtr context, FileNamesGenerator & generator)\n+{\n+    auto metadata = IcebergMetadata::create(object_storage, configuration, context);\n+    std::unique_ptr<IcebergMetadata> iceberg_metadata(static_cast<IcebergMetadata *>(metadata.release()));\n+    auto snapshots_info = iceberg_metadata->getHistory(context);\n+\n+    Plan plan;\n+\n+    std::vector<ManifestFileEntry> all_positional_delete_files;\n+    std::unordered_map<String, std::shared_ptr<ManifestFilePlan>> manifest_files;\n+    for (const auto & snapshot : snapshots_info)\n+    {\n+        auto manifest_list = iceberg_metadata->getManifestList(context, snapshot.manifest_list_path);\n+        for (const auto & manifest_file : manifest_list)\n+        {\n+            plan.manifest_list_to_manifest_files[snapshot.manifest_list_path].push_back(manifest_file.manifest_file_path);\n+            if (!plan.manifest_file_to_first_snapshot.contains(manifest_file.manifest_file_path))\n+                plan.manifest_file_to_first_snapshot[manifest_file.manifest_file_path] = snapshot.snapshot_id;\n+            auto manifest_file_content = iceberg_metadata->tryGetManifestFile(\n+                context, manifest_file.manifest_file_path, manifest_file.added_sequence_number, manifest_file.added_snapshot_id);\n+\n+            if (!manifest_files.contains(manifest_file.manifest_file_path))\n+            {\n+                manifest_files[manifest_file.manifest_file_path] = std::make_shared<ManifestFilePlan>();\n+                manifest_files[manifest_file.manifest_file_path]->path = manifest_file.manifest_file_path;\n+            }\n+            manifest_files[manifest_file.manifest_file_path]->manifest_lists_path.push_back(snapshot.manifest_list_path);\n+            auto data_files = manifest_file_content->getFiles(FileContentType::DATA);\n+            auto positional_delete_files = manifest_file_content->getFiles(FileContentType::POSITION_DELETE);\n+            for (const auto & pos_delete_file : positional_delete_files)\n+                all_positional_delete_files.push_back(pos_delete_file);\n+\n+            for (const auto & data_file : data_files)\n+            {\n+                auto partition_index = plan.partition_encoder.encodePartition(data_file.partition_key_value);\n+                if (plan.partitions.size() <= partition_index)\n+                    plan.partitions.push_back({});\n+\n+                ParsedDataFileInfo parsed_data_file_info(configuration, data_file, {});\n+                parsed_data_file_info.sequence_number = data_file.added_sequence_number;\n+                std::shared_ptr<DataFilePlan> data_file_ptr;\n+                if (!plan.path_to_data_file.contains(manifest_file.manifest_file_path))\n+                {\n+                    data_file_ptr = std::make_shared<DataFilePlan>(DataFilePlan{\n+                        .parsed_data_file_info = parsed_data_file_info,\n+                        .manifest_list = manifest_files[manifest_file.manifest_file_path],\n+                        .patched_path = generator.generateDataFileName()});\n+                    plan.path_to_data_file[manifest_file.manifest_file_path] = data_file_ptr;\n+                }\n+                else\n+                {\n+                    data_file_ptr = plan.path_to_data_file[manifest_file.manifest_file_path];\n+                }\n+                plan.partitions[partition_index].push_back(data_file_ptr);\n+                plan.snapshot_id_to_data_files[snapshot.snapshot_id].push_back(plan.partitions[partition_index].back());\n+            }\n+        }\n+    }\n+\n+    for (const auto & delete_file : all_positional_delete_files)\n+    {\n+        auto partition_index = plan.partition_encoder.encodePartition(delete_file.partition_key_value);\n+        if (partition_index >= plan.partitions.size())\n+            continue;\n+\n+        std::vector<Iceberg::ManifestFileEntry> result_delete_files;\n+        for (auto & data_file : plan.partitions[partition_index])\n+        {\n+            if (data_file->parsed_data_file_info.sequence_number <= delete_file.added_sequence_number)\n+                data_file->parsed_data_file_info.position_deletes_objects.push_back(delete_file);",
        "comment_created_at": "2025-08-13T13:24:28+00:00",
        "comment_author": "hanfei1991",
        "comment_body": "Do we ignore the datafiles without any position deletes?",
        "pr_file_module": null
      },
      {
        "comment_id": "2274893186",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85250,
        "pr_file": "src/Storages/ObjectStorage/DataLakes/Iceberg/Compaction.cpp",
        "discussion_id": "2273465207",
        "commented_code": "@@ -0,0 +1,515 @@\n+#include <string>\n+#include <Formats/FormatFactory.h>\n+#include <IO/CompressionMethod.h>\n+#include <Interpreters/Context.h>\n+#include <Processors/Formats/IRowOutputFormat.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Compaction.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>\n+#include <fmt/format.h>\n+#include <Poco/JSON/Object.h>\n+#include <Poco/JSON/Stringifier.h>\n+#include <Common/Logger.h>\n+#include <Core/Settings.h>\n+#include <Disks/ObjectStorages/StoredObject.h>\n+#include <Columns/IColumn.h>\n+#include <Core/ColumnsWithTypeAndName.h>\n+#include <Interpreters/Cache/FileSegment.h>\n+#include <Storages/ColumnsDescription.h>\n+#include <Storages/ObjectStorage/DataLakes/Common.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Constant.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergWrites.h>\n+#include <Storages/ObjectStorage/StorageObjectStorageSource.h>\n+\n+#if USE_AVRO\n+\n+namespace DB::Setting\n+{\n+    extern const SettingsMilliseconds iceberg_compaction_backoff_time;\n+}\n+\n+namespace Iceberg\n+{\n+\n+using namespace DB;\n+\n+struct ManifestFilePlan\n+{\n+    String path;\n+    std::vector<String> manifest_lists_path;\n+\n+    FileNamesGenerator::Result patched_path;\n+};\n+\n+struct DataFilePlan\n+{\n+    ParsedDataFileInfo parsed_data_file_info;\n+    std::shared_ptr<ManifestFilePlan> manifest_list;\n+\n+    FileNamesGenerator::Result patched_path;\n+    UInt64 new_records_count = 0;\n+};\n+\n+class ParititonEncoder\n+{\n+public:\n+    size_t encodePartition(const Row & row)\n+    {\n+        if (auto it = partition_value_to_index.find(row); it != partition_value_to_index.end())\n+            return it->second;\n+\n+        partition_value_to_index[row] = partition_values.size();\n+        partition_values.push_back(row);\n+        return partition_value_to_index[row];\n+    }\n+\n+    const Row & getPartitionValue(size_t partition_index) const { return partition_values.at(partition_index); }\n+\n+private:\n+    struct PartitionValueHasher\n+    {\n+        std::hash<String> hasher;\n+        size_t operator()(const Row & row) const\n+        {\n+            size_t result = 0;\n+            for (const auto & value : row)\n+                result ^= hasher(value.dump());\n+            return result;\n+        }\n+    };\n+\n+    std::unordered_map<Row, size_t, PartitionValueHasher> partition_value_to_index;\n+    std::vector<Row> partition_values;\n+};\n+\n+/// Plan of compaction consists information about all data files and what delete files should be applied for them.\n+/// Also it contains some other information about previous metadata.\n+struct Plan\n+{\n+    bool need_optimize = false;\n+    using Partition = std::vector<std::shared_ptr<DataFilePlan>>;\n+    std::vector<Partition> partitions;\n+    IcebergMetadata::IcebergHistory history;\n+    std::unordered_map<String, Int64> manifest_file_to_first_snapshot;\n+    std::unordered_map<String, std::vector<String>> manifest_list_to_manifest_files;\n+    std::unordered_map<Int64, std::vector<std::shared_ptr<DataFilePlan>>> snapshot_id_to_data_files;\n+    std::unordered_map<String, std::shared_ptr<DataFilePlan>> path_to_data_file;\n+    std::unique_ptr<IcebergMetadata> iceberg_metadata;\n+    ParititonEncoder partition_encoder;\n+};\n+\n+Plan getPlan(\n+    ObjectStoragePtr object_storage, StorageObjectStorageConfigurationPtr configuration, ContextPtr context, FileNamesGenerator & generator)\n+{\n+    auto metadata = IcebergMetadata::create(object_storage, configuration, context);\n+    std::unique_ptr<IcebergMetadata> iceberg_metadata(static_cast<IcebergMetadata *>(metadata.release()));\n+    auto snapshots_info = iceberg_metadata->getHistory(context);\n+\n+    Plan plan;\n+\n+    std::vector<ManifestFileEntry> all_positional_delete_files;\n+    std::unordered_map<String, std::shared_ptr<ManifestFilePlan>> manifest_files;\n+    for (const auto & snapshot : snapshots_info)\n+    {\n+        auto manifest_list = iceberg_metadata->getManifestList(context, snapshot.manifest_list_path);\n+        for (const auto & manifest_file : manifest_list)\n+        {\n+            plan.manifest_list_to_manifest_files[snapshot.manifest_list_path].push_back(manifest_file.manifest_file_path);\n+            if (!plan.manifest_file_to_first_snapshot.contains(manifest_file.manifest_file_path))\n+                plan.manifest_file_to_first_snapshot[manifest_file.manifest_file_path] = snapshot.snapshot_id;\n+            auto manifest_file_content = iceberg_metadata->tryGetManifestFile(\n+                context, manifest_file.manifest_file_path, manifest_file.added_sequence_number, manifest_file.added_snapshot_id);\n+\n+            if (!manifest_files.contains(manifest_file.manifest_file_path))\n+            {\n+                manifest_files[manifest_file.manifest_file_path] = std::make_shared<ManifestFilePlan>();\n+                manifest_files[manifest_file.manifest_file_path]->path = manifest_file.manifest_file_path;\n+            }\n+            manifest_files[manifest_file.manifest_file_path]->manifest_lists_path.push_back(snapshot.manifest_list_path);\n+            auto data_files = manifest_file_content->getFiles(FileContentType::DATA);\n+            auto positional_delete_files = manifest_file_content->getFiles(FileContentType::POSITION_DELETE);\n+            for (const auto & pos_delete_file : positional_delete_files)\n+                all_positional_delete_files.push_back(pos_delete_file);\n+\n+            for (const auto & data_file : data_files)\n+            {\n+                auto partition_index = plan.partition_encoder.encodePartition(data_file.partition_key_value);\n+                if (plan.partitions.size() <= partition_index)\n+                    plan.partitions.push_back({});\n+\n+                ParsedDataFileInfo parsed_data_file_info(configuration, data_file, {});\n+                parsed_data_file_info.sequence_number = data_file.added_sequence_number;\n+                std::shared_ptr<DataFilePlan> data_file_ptr;\n+                if (!plan.path_to_data_file.contains(manifest_file.manifest_file_path))\n+                {\n+                    data_file_ptr = std::make_shared<DataFilePlan>(DataFilePlan{\n+                        .parsed_data_file_info = parsed_data_file_info,\n+                        .manifest_list = manifest_files[manifest_file.manifest_file_path],\n+                        .patched_path = generator.generateDataFileName()});\n+                    plan.path_to_data_file[manifest_file.manifest_file_path] = data_file_ptr;\n+                }\n+                else\n+                {\n+                    data_file_ptr = plan.path_to_data_file[manifest_file.manifest_file_path];\n+                }\n+                plan.partitions[partition_index].push_back(data_file_ptr);\n+                plan.snapshot_id_to_data_files[snapshot.snapshot_id].push_back(plan.partitions[partition_index].back());\n+            }\n+        }\n+    }\n+\n+    for (const auto & delete_file : all_positional_delete_files)\n+    {\n+        auto partition_index = plan.partition_encoder.encodePartition(delete_file.partition_key_value);\n+        if (partition_index >= plan.partitions.size())\n+            continue;\n+\n+        std::vector<Iceberg::ManifestFileEntry> result_delete_files;\n+        for (auto & data_file : plan.partitions[partition_index])\n+        {\n+            if (data_file->parsed_data_file_info.sequence_number <= delete_file.added_sequence_number)\n+                data_file->parsed_data_file_info.position_deletes_objects.push_back(delete_file);",
        "comment_created_at": "2025-08-13T23:54:50+00:00",
        "comment_author": "scanhex12",
        "comment_body": "No\r\nThis part of code just attaches delete files into data files. All data files (including files with no deletes) were created before this line.",
        "pr_file_module": null
      },
      {
        "comment_id": "2274898421",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85250,
        "pr_file": "src/Storages/ObjectStorage/DataLakes/Iceberg/Compaction.cpp",
        "discussion_id": "2273465207",
        "commented_code": "@@ -0,0 +1,515 @@\n+#include <string>\n+#include <Formats/FormatFactory.h>\n+#include <IO/CompressionMethod.h>\n+#include <Interpreters/Context.h>\n+#include <Processors/Formats/IRowOutputFormat.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Compaction.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/PositionDeleteTransform.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Utils.h>\n+#include <fmt/format.h>\n+#include <Poco/JSON/Object.h>\n+#include <Poco/JSON/Stringifier.h>\n+#include <Common/Logger.h>\n+#include <Core/Settings.h>\n+#include <Disks/ObjectStorages/StoredObject.h>\n+#include <Columns/IColumn.h>\n+#include <Core/ColumnsWithTypeAndName.h>\n+#include <Interpreters/Cache/FileSegment.h>\n+#include <Storages/ColumnsDescription.h>\n+#include <Storages/ObjectStorage/DataLakes/Common.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/Constant.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergMetadata.h>\n+#include <Storages/ObjectStorage/DataLakes/Iceberg/IcebergWrites.h>\n+#include <Storages/ObjectStorage/StorageObjectStorageSource.h>\n+\n+#if USE_AVRO\n+\n+namespace DB::Setting\n+{\n+    extern const SettingsMilliseconds iceberg_compaction_backoff_time;\n+}\n+\n+namespace Iceberg\n+{\n+\n+using namespace DB;\n+\n+struct ManifestFilePlan\n+{\n+    String path;\n+    std::vector<String> manifest_lists_path;\n+\n+    FileNamesGenerator::Result patched_path;\n+};\n+\n+struct DataFilePlan\n+{\n+    ParsedDataFileInfo parsed_data_file_info;\n+    std::shared_ptr<ManifestFilePlan> manifest_list;\n+\n+    FileNamesGenerator::Result patched_path;\n+    UInt64 new_records_count = 0;\n+};\n+\n+class ParititonEncoder\n+{\n+public:\n+    size_t encodePartition(const Row & row)\n+    {\n+        if (auto it = partition_value_to_index.find(row); it != partition_value_to_index.end())\n+            return it->second;\n+\n+        partition_value_to_index[row] = partition_values.size();\n+        partition_values.push_back(row);\n+        return partition_value_to_index[row];\n+    }\n+\n+    const Row & getPartitionValue(size_t partition_index) const { return partition_values.at(partition_index); }\n+\n+private:\n+    struct PartitionValueHasher\n+    {\n+        std::hash<String> hasher;\n+        size_t operator()(const Row & row) const\n+        {\n+            size_t result = 0;\n+            for (const auto & value : row)\n+                result ^= hasher(value.dump());\n+            return result;\n+        }\n+    };\n+\n+    std::unordered_map<Row, size_t, PartitionValueHasher> partition_value_to_index;\n+    std::vector<Row> partition_values;\n+};\n+\n+/// Plan of compaction consists information about all data files and what delete files should be applied for them.\n+/// Also it contains some other information about previous metadata.\n+struct Plan\n+{\n+    bool need_optimize = false;\n+    using Partition = std::vector<std::shared_ptr<DataFilePlan>>;\n+    std::vector<Partition> partitions;\n+    IcebergMetadata::IcebergHistory history;\n+    std::unordered_map<String, Int64> manifest_file_to_first_snapshot;\n+    std::unordered_map<String, std::vector<String>> manifest_list_to_manifest_files;\n+    std::unordered_map<Int64, std::vector<std::shared_ptr<DataFilePlan>>> snapshot_id_to_data_files;\n+    std::unordered_map<String, std::shared_ptr<DataFilePlan>> path_to_data_file;\n+    std::unique_ptr<IcebergMetadata> iceberg_metadata;\n+    ParititonEncoder partition_encoder;\n+};\n+\n+Plan getPlan(\n+    ObjectStoragePtr object_storage, StorageObjectStorageConfigurationPtr configuration, ContextPtr context, FileNamesGenerator & generator)\n+{\n+    auto metadata = IcebergMetadata::create(object_storage, configuration, context);\n+    std::unique_ptr<IcebergMetadata> iceberg_metadata(static_cast<IcebergMetadata *>(metadata.release()));\n+    auto snapshots_info = iceberg_metadata->getHistory(context);\n+\n+    Plan plan;\n+\n+    std::vector<ManifestFileEntry> all_positional_delete_files;\n+    std::unordered_map<String, std::shared_ptr<ManifestFilePlan>> manifest_files;\n+    for (const auto & snapshot : snapshots_info)\n+    {\n+        auto manifest_list = iceberg_metadata->getManifestList(context, snapshot.manifest_list_path);\n+        for (const auto & manifest_file : manifest_list)\n+        {\n+            plan.manifest_list_to_manifest_files[snapshot.manifest_list_path].push_back(manifest_file.manifest_file_path);\n+            if (!plan.manifest_file_to_first_snapshot.contains(manifest_file.manifest_file_path))\n+                plan.manifest_file_to_first_snapshot[manifest_file.manifest_file_path] = snapshot.snapshot_id;\n+            auto manifest_file_content = iceberg_metadata->tryGetManifestFile(\n+                context, manifest_file.manifest_file_path, manifest_file.added_sequence_number, manifest_file.added_snapshot_id);\n+\n+            if (!manifest_files.contains(manifest_file.manifest_file_path))\n+            {\n+                manifest_files[manifest_file.manifest_file_path] = std::make_shared<ManifestFilePlan>();\n+                manifest_files[manifest_file.manifest_file_path]->path = manifest_file.manifest_file_path;\n+            }\n+            manifest_files[manifest_file.manifest_file_path]->manifest_lists_path.push_back(snapshot.manifest_list_path);\n+            auto data_files = manifest_file_content->getFiles(FileContentType::DATA);\n+            auto positional_delete_files = manifest_file_content->getFiles(FileContentType::POSITION_DELETE);\n+            for (const auto & pos_delete_file : positional_delete_files)\n+                all_positional_delete_files.push_back(pos_delete_file);\n+\n+            for (const auto & data_file : data_files)\n+            {\n+                auto partition_index = plan.partition_encoder.encodePartition(data_file.partition_key_value);\n+                if (plan.partitions.size() <= partition_index)\n+                    plan.partitions.push_back({});\n+\n+                ParsedDataFileInfo parsed_data_file_info(configuration, data_file, {});\n+                parsed_data_file_info.sequence_number = data_file.added_sequence_number;\n+                std::shared_ptr<DataFilePlan> data_file_ptr;\n+                if (!plan.path_to_data_file.contains(manifest_file.manifest_file_path))\n+                {\n+                    data_file_ptr = std::make_shared<DataFilePlan>(DataFilePlan{\n+                        .parsed_data_file_info = parsed_data_file_info,\n+                        .manifest_list = manifest_files[manifest_file.manifest_file_path],\n+                        .patched_path = generator.generateDataFileName()});\n+                    plan.path_to_data_file[manifest_file.manifest_file_path] = data_file_ptr;\n+                }\n+                else\n+                {\n+                    data_file_ptr = plan.path_to_data_file[manifest_file.manifest_file_path];\n+                }\n+                plan.partitions[partition_index].push_back(data_file_ptr);\n+                plan.snapshot_id_to_data_files[snapshot.snapshot_id].push_back(plan.partitions[partition_index].back());\n+            }\n+        }\n+    }\n+\n+    for (const auto & delete_file : all_positional_delete_files)\n+    {\n+        auto partition_index = plan.partition_encoder.encodePartition(delete_file.partition_key_value);\n+        if (partition_index >= plan.partitions.size())\n+            continue;\n+\n+        std::vector<Iceberg::ManifestFileEntry> result_delete_files;\n+        for (auto & data_file : plan.partitions[partition_index])\n+        {\n+            if (data_file->parsed_data_file_info.sequence_number <= delete_file.added_sequence_number)\n+                data_file->parsed_data_file_info.position_deletes_objects.push_back(delete_file);",
        "comment_created_at": "2025-08-14T00:00:21+00:00",
        "comment_author": "scanhex12",
        "comment_body": "I already checked this case in tests (last query before `OPTIMIZE` is `INSERT`, so we have file with no deletes in test)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2247812206",
    "pr_number": 74344,
    "pr_file": "src/Parsers/ParserCopyQuery.cpp",
    "created_at": "2025-08-01T12:06:31+00:00",
    "commented_code": "+#include <Parsers/ParserCopyQuery.h>\n+\n+#include <Parsers/ASTAsterisk.h>\n+#include <Parsers/ASTCopyQuery.h>\n+#include <Parsers/ASTExpressionList.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTIdentifier.h>\n+#include <Parsers/ASTInsertQuery.h>\n+#include <Parsers/ASTInterpolateElement.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Parsers/ASTOrderByElement.h>\n+#include <Parsers/ASTSelectQuery.h>\n+#include <Parsers/ASTSelectWithUnionQuery.h>\n+#include <Parsers/ASTSubquery.h>\n+#include <Parsers/ASTTablesInSelectQuery.h>\n+#include <Parsers/ASTWithElement.h>\n+#include <Parsers/CommonParsers.h>\n+#include <Parsers/ExpressionElementParsers.h>\n+#include <Parsers/ExpressionListParsers.h>\n+#include <Parsers/IAST_fwd.h>\n+#include <Parsers/IParserBase.h>\n+#include <Parsers/ParserSampleRatio.h>\n+#include <Parsers/ParserSelectQuery.h>\n+#include <Parsers/ParserSetQuery.h>\n+#include <Parsers/ParserTablesInSelectQuery.h>\n+#include <Parsers/ParserWithElement.h>\n+\n+#include <algorithm>\n+#include <memory>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+extern const int BAD_ARGUMENTS;\n+}\n+\n+bool ParserCopyQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n+{\n+    ParserIdentifier s_ident;\n+    ParserKeyword s_copy(Keyword::COPY);\n+    ParserKeyword s_to(Keyword::TO);\n+    ParserKeyword s_from(Keyword::FROM);\n+    ParserToken open_bracket(TokenType::OpeningRoundBracket);\n+    ParserToken close_bracket(TokenType::ClosingRoundBracket);\n+\n+    ParserSubquery s_subquery;\n+\n+    auto copy_element = std::make_shared<ASTCopyQuery>();\n+    node = copy_element;\n+\n+    if (!s_copy.ignore(pos, expected))\n+        return false;\n+\n+    auto saved_pos = pos;\n+    if (!open_bracket.ignore(pos, expected))\n+    {\n+        ParserIdentifier s_table_identifier;\n+        ASTPtr table_name;\n+        if (!s_table_identifier.parse(pos, table_name, expected))\n+            return false;\n+\n+        if (open_bracket.ignore(pos, expected))",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2247812206",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 74344,
        "pr_file": "src/Parsers/ParserCopyQuery.cpp",
        "discussion_id": "2247812206",
        "commented_code": "@@ -0,0 +1,155 @@\n+#include <Parsers/ParserCopyQuery.h>\n+\n+#include <Parsers/ASTAsterisk.h>\n+#include <Parsers/ASTCopyQuery.h>\n+#include <Parsers/ASTExpressionList.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTIdentifier.h>\n+#include <Parsers/ASTInsertQuery.h>\n+#include <Parsers/ASTInterpolateElement.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Parsers/ASTOrderByElement.h>\n+#include <Parsers/ASTSelectQuery.h>\n+#include <Parsers/ASTSelectWithUnionQuery.h>\n+#include <Parsers/ASTSubquery.h>\n+#include <Parsers/ASTTablesInSelectQuery.h>\n+#include <Parsers/ASTWithElement.h>\n+#include <Parsers/CommonParsers.h>\n+#include <Parsers/ExpressionElementParsers.h>\n+#include <Parsers/ExpressionListParsers.h>\n+#include <Parsers/IAST_fwd.h>\n+#include <Parsers/IParserBase.h>\n+#include <Parsers/ParserSampleRatio.h>\n+#include <Parsers/ParserSelectQuery.h>\n+#include <Parsers/ParserSetQuery.h>\n+#include <Parsers/ParserTablesInSelectQuery.h>\n+#include <Parsers/ParserWithElement.h>\n+\n+#include <algorithm>\n+#include <memory>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+extern const int BAD_ARGUMENTS;\n+}\n+\n+bool ParserCopyQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)\n+{\n+    ParserIdentifier s_ident;\n+    ParserKeyword s_copy(Keyword::COPY);\n+    ParserKeyword s_to(Keyword::TO);\n+    ParserKeyword s_from(Keyword::FROM);\n+    ParserToken open_bracket(TokenType::OpeningRoundBracket);\n+    ParserToken close_bracket(TokenType::ClosingRoundBracket);\n+\n+    ParserSubquery s_subquery;\n+\n+    auto copy_element = std::make_shared<ASTCopyQuery>();\n+    node = copy_element;\n+\n+    if (!s_copy.ignore(pos, expected))\n+        return false;\n+\n+    auto saved_pos = pos;\n+    if (!open_bracket.ignore(pos, expected))\n+    {\n+        ParserIdentifier s_table_identifier;\n+        ASTPtr table_name;\n+        if (!s_table_identifier.parse(pos, table_name, expected))\n+            return false;\n+\n+        if (open_bracket.ignore(pos, expected))",
        "comment_created_at": "2025-08-01T12:06:31+00:00",
        "comment_author": "GrigoryPervakov",
        "comment_body": "Have I understood it correctly?\r\nIn queries like this, we silently ignore the column list? \r\n```sql\r\nCOPY table_name (a, b) ...\r\n```\r\nMaybe it's better to throw in such case?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2267564573",
    "pr_number": 84383,
    "pr_file": "src/Compression/CompressionCodecDelta.cpp",
    "created_at": "2025-08-11T17:53:42+00:00",
    "commented_code": "UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2267564573",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84383,
        "pr_file": "src/Compression/CompressionCodecDelta.cpp",
        "discussion_id": "2267564573",
        "commented_code": "@@ -179,13 +179,14 @@ namespace\n \n UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
        "comment_created_at": "2025-08-11T17:53:42+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "Thinking about this, restricting the eligible types for \\*Delta makes sense overall (\\*), but I'm sure that _someone_ out there is using \\*Delta compression on a now forbidden type (e.g. FixedString). In that case, affected tables will fail to load. That's a problem. I think we should approach the problem a bit differently: ClickHouse has a setting `allow_suspicious_codecs` for dubious codec combinations. What if we keep `isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion` as a condition in the codecs but throw an error if the data type does not satisfy `isValueRepresentedByNumber` (--> can be done in src/Compression/CompressionFactoryAdditions.cpp). Existing tables can then be loaded with setting `allow_suspicious_codecs = 1`.\n\n(*) even if not all combinations of a now-disabled type (FixedString) with different codec parameters (`Delta(1)`, `Delta(2)`) and data type lengths (`FixedString(1)`, `FixedString(2)`) might fail.",
        "pr_file_module": null
      },
      {
        "comment_id": "2268058002",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84383,
        "pr_file": "src/Compression/CompressionCodecDelta.cpp",
        "discussion_id": "2267564573",
        "commented_code": "@@ -179,13 +179,14 @@ namespace\n \n UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
        "comment_created_at": "2025-08-11T21:25:12+00:00",
        "comment_author": "Ergus",
        "comment_body": "@rschu1ze \r\n\r\nI would ask here is we have some kind of deprecation policy/process. Because ideally what we could do is (as you suggested) trigger a warning but specifying that the feature will be totally disabled in a future version x.x. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2268167636",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84383,
        "pr_file": "src/Compression/CompressionCodecDelta.cpp",
        "discussion_id": "2267564573",
        "commented_code": "@@ -179,13 +179,14 @@ namespace\n \n UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
        "comment_created_at": "2025-08-11T22:44:26+00:00",
        "comment_author": "Ergus",
        "comment_body": "Before this change there were a couple of issues that were not exposed because of the error that `delta is not a compression codec` triggered earlier:\r\n\r\nFor example these two were hidden\r\n\r\n```sql\r\nCREATE TABLE codecTest (c0 FixedString(9) CODEC(Delta)) ENGINE = MergeTree() ORDER BY tuple();\r\nCREATE TABLE codecTest (c0 FixedString(9) CODEC(Delta(1))) ENGINE = MergeTree() ORDER BY tuple();\r\n```\r\nbut this one:\r\n\r\n```sql\r\nset allow_suspicious_codecs = 1;\r\nCREATE TABLE codecTest (c0 FixedString(9) CODEC(Delta(2))) ENGINE = MergeTree() ORDER BY tuple();\r\n```\r\nworked because the user specified the size and the code totally ignored the column size. AFAIK that is an issue because `9%2 != 0`\r\n\r\nIn the current code we enforce to check the column size to be: `1, 2, 4, 8` even when we respect the user value latter. So the question now is if we are supposed to go back to the previous code even if we know that there is an issue? \r\n\r\nOr we must ensure some extra correctness check. like checking that the column size is divisible by the user size when provides? Is it correct (for example) `c0 FixedString(9) CODEC(Delta(2))`?\r\n\r\nWhat about `c0 FixedString(16) CODEC(Delta(4))`?\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2268944178",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84383,
        "pr_file": "src/Compression/CompressionCodecDelta.cpp",
        "discussion_id": "2267564573",
        "commented_code": "@@ -179,13 +179,14 @@ namespace\n \n UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
        "comment_created_at": "2025-08-12T07:35:44+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "> I would ask here is we have some kind of deprecation policy/process. Because ideally what we could do is (as you suggested) trigger a warning but specifying that the feature will be totally disabled in a future version x.x.\r\n\r\nBreaking existing usage must be avoided if possible. ClickHouse uses the \"compatibility setting\" mechanism for that (see [here](https://github.com/ClickHouse/ClickHouse/issues/35972) and [here](https://clickhouse.com/docs/operations/settings/settings#compatibility)). For example, customers can choose to upgrade a cluster but keep the compatibility level - the system will keep behaving like before the upgrade (in theory). The list of changed settings per version can be found in src/Core/SettingsChangesHistory.cpp.\r\n\r\n\"totally disabling\" can only be done at a future point in time when we know that no user has a compatibility level smaller than 25.8 configured.\r\n- for ClickHouse Cloud, that is usually the case after ~6 months.\r\n- for ClickHouse OSS, that will likely never be the case. We will need to wait until ancient compatibility levels are forcefully removed from the code (that never happened yet).\r\n\r\nThe problem here is that we'd need a new MergeTree setting `allow_delta_and_doubledelta_compression_with_non_numeric_types` with default = true before this PR and default = false after this PR. Actually, this would be the best solution but it requires support in the code for passing and checking MergeTree settings in the CodecsFactory as well as the Codecs ... this is currently not there. Compared to that, my proposal of making the load dependent `allow_suspicious_codecs` is slightly inferior - loading will _always_ break after upgrade and manual intervention is required (one could argue that likely only few files are affected but still).",
        "pr_file_module": null
      },
      {
        "comment_id": "2269015477",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84383,
        "pr_file": "src/Compression/CompressionCodecDelta.cpp",
        "discussion_id": "2267564573",
        "commented_code": "@@ -179,13 +179,14 @@ namespace\n \n UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
        "comment_created_at": "2025-08-12T07:54:37+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "Re your other comment:\r\n\r\nWith `set allow_suspicious_codecs = 1;`, all bets are off, i.e., codecs may produce arbitrary garbage with this setting. This is known and accepted behavior.\r\n\r\n> In the current code we enforce to check the column size to be: 1, 2, 4, 8 even when we respect the user value latter. So the question now is if we are supposed to go back to the previous code even if we know that there is an issue?\r\n\r\n> Or we must ensure some extra correctness check. like checking that the column size is divisible by the user size when provides? Is it correct (for example) c0 FixedString(9) CODEC(Delta(2))?\r\n\r\n> What about c0 FixedString(16) CODEC(Delta(4))?\r\n\r\nI don't know the answers but this PR eliminates support for FixedString and therefore the entire sub-class of problems you describe.",
        "pr_file_module": null
      },
      {
        "comment_id": "2269041472",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84383,
        "pr_file": "src/Compression/CompressionCodecDelta.cpp",
        "discussion_id": "2267564573",
        "commented_code": "@@ -179,13 +179,14 @@ namespace\n \n UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
        "comment_created_at": "2025-08-12T08:05:09+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "Thinking about this, the change from `isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion` to `isValueRepresentedByNumber` affects data types UUID, FixedString, and Array (these will no longer be Delta/DoubleDelta compressible). Out of these, UUID has a fixed byte sizes and it setting integer parameter `N` as in `Delta(N)` and `DoubleDelta(N)` makes no sense. Setting `N` only makes sense for FixedString (even there it is doubtful). For Array, I don't know.\r\n\r\nFor un-affected data types (integers, floats, decimals, enums), setting `N` also makes no sense.\r\n\r\nI guess this PR should also make setting `N` a no-op? (we can then also remove mentions of `N` from the docs).",
        "pr_file_module": null
      },
      {
        "comment_id": "2269639561",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84383,
        "pr_file": "src/Compression/CompressionCodecDelta.cpp",
        "discussion_id": "2267564573",
        "commented_code": "@@ -179,13 +179,14 @@ namespace\n \n UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
        "comment_created_at": "2025-08-12T12:10:48+00:00",
        "comment_author": "Ergus",
        "comment_body": "But doing that would break backward compatibility for tables already created with N. Even if that is wrong.\r\n\r\nBoth codecs have the N parameter as well, and some tests use them.\r\n\r\nDoes it worth doing the change and updating all those? \r\n\r\nThis will probably affect some other people around, should we ask someone from other group about this?",
        "pr_file_module": null
      },
      {
        "comment_id": "2270155732",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 84383,
        "pr_file": "src/Compression/CompressionCodecDelta.cpp",
        "discussion_id": "2267564573",
        "commented_code": "@@ -179,13 +179,14 @@ namespace\n \n UInt8 getDeltaBytesSize(const IDataType * column_type)\n {\n-    if (!column_type->isValueUnambiguouslyRepresentedInFixedSizeContiguousMemoryRegion())\n-        throw Exception(ErrorCodes::BAD_ARGUMENTS, \"Codec Delta is not applicable for {} because the data type is not of fixed size\",\n+    if (!column_type->isValueRepresentedByNumber())",
        "comment_created_at": "2025-08-12T14:53:44+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "Usage of codecs `Delta(N)` / `DoubleDelta(N)` (where `N` is != the type byte length) for integers, floats, decimals, enums is highly dubious, but indeed ... chances are that someone uses that. For CHC, we can check internally if someone uses that (I'll ping you). We can eventually move such combinations under setting `allow_suspicous_codecs` as well.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192931023",
    "pr_number": 76802,
    "pr_file": "src/Storages/PartitionStrategy.cpp",
    "created_at": "2025-07-08T16:15:58+00:00",
    "commented_code": "+#include <Storages/PartitionStrategy.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Interpreters/TreeRewriter.h>\n+#include <Interpreters/ExpressionAnalyzer.h>\n+#include <Storages/PartitionedSink.h>\n+#include <Functions/generateSnowflakeID.h>\n+#include <Interpreters/Context.h>\n+#include <Storages/KeyDescription.h>\n+#include <Poco/String.h>\n+#include <Core/Settings.h>\n+#include <Storages/ColumnsDescription.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+extern const int LOGICAL_ERROR;\n+extern const int BAD_ARGUMENTS;\n+}\n+\n+namespace\n+{\n+    HiveStylePartitionStrategy::PartitionExpressionActionsAndColumnName buildExpressionHive(",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2192931023",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 76802,
        "pr_file": "src/Storages/PartitionStrategy.cpp",
        "discussion_id": "2192931023",
        "commented_code": "@@ -0,0 +1,370 @@\n+#include <Storages/PartitionStrategy.h>\n+#include <Parsers/ASTFunction.h>\n+#include <Parsers/ASTLiteral.h>\n+#include <Interpreters/TreeRewriter.h>\n+#include <Interpreters/ExpressionAnalyzer.h>\n+#include <Storages/PartitionedSink.h>\n+#include <Functions/generateSnowflakeID.h>\n+#include <Interpreters/Context.h>\n+#include <Storages/KeyDescription.h>\n+#include <Poco/String.h>\n+#include <Core/Settings.h>\n+#include <Storages/ColumnsDescription.h>\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+extern const int LOGICAL_ERROR;\n+extern const int BAD_ARGUMENTS;\n+}\n+\n+namespace\n+{\n+    HiveStylePartitionStrategy::PartitionExpressionActionsAndColumnName buildExpressionHive(",
        "comment_created_at": "2025-07-08T16:15:58+00:00",
        "comment_author": "kssenii",
        "comment_body": "```suggestion\r\n    /// Creates Expression actions to create hive path part of format\r\n    ///  `partition_column_1=toString(partition_value_expr_1)/ ... /partition_column_N=toString(partition_value_expr_N)/`\r\n    /// for given partition columns list and a partition by AST.\r\n    /// The actions will be computed over chunk to convert partition values to string values.\r\n    HiveStylePartitionStrategy::PartitionExpressionActionsAndColumnName buildExpressionHive(\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2210850958",
    "pr_number": 83849,
    "pr_file": "src/Storages/StorageReplicatedMergeTree.cpp",
    "created_at": "2025-07-16T16:01:21+00:00",
    "commented_code": "const auto table_metadata = ReplicatedMergeTreeTableMetadata(*this, current_metadata);\n         auto metadata_diff = table_metadata.checkAndFindDiff(metadata_from_entry, current_metadata->getColumns(), getStorageID().getNameForLogs(), getContext());\n         auto adjusted_metadata = metadata_diff.getNewMetadata(columns_from_entry, getContext(), *current_metadata);\n-        applyMetadataChangesToCreateQuery(create_query, adjusted_metadata, getContext());\n+        applyMetadataChangesToCreateQuery(create_query, adjusted_metadata, getContext(), false);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2210850958",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83849,
        "pr_file": "src/Storages/StorageReplicatedMergeTree.cpp",
        "discussion_id": "2210850958",
        "commented_code": "@@ -11224,7 +11224,7 @@ void StorageReplicatedMergeTree::applyMetadataChangesToCreateQueryForBackup(ASTP\n         const auto table_metadata = ReplicatedMergeTreeTableMetadata(*this, current_metadata);\n         auto metadata_diff = table_metadata.checkAndFindDiff(metadata_from_entry, current_metadata->getColumns(), getStorageID().getNameForLogs(), getContext());\n         auto adjusted_metadata = metadata_diff.getNewMetadata(columns_from_entry, getContext(), *current_metadata);\n-        applyMetadataChangesToCreateQuery(create_query, adjusted_metadata, getContext());\n+        applyMetadataChangesToCreateQuery(create_query, adjusted_metadata, getContext(), false);",
        "comment_created_at": "2025-07-16T16:01:21+00:00",
        "comment_author": "antaljanosbenjamin",
        "comment_body": "When altering the create queries for backup, we shouldn't validate the create queries, there is no reason to do that.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2211033659",
    "pr_number": 83849,
    "pr_file": "src/Databases/DatabaseOrdinary.cpp",
    "created_at": "2025-07-16T17:14:48+00:00",
    "commented_code": "}\n \n void DatabaseOrdinary::alterTable(ContextPtr local_context, const StorageID & table_id, const StorageInMemoryMetadata & metadata)\n+{\n+    doAlterTable(local_context, table_id, metadata, true);\n+}",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2211033659",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83849,
        "pr_file": "src/Databases/DatabaseOrdinary.cpp",
        "discussion_id": "2211033659",
        "commented_code": "@@ -628,6 +628,12 @@ Strings DatabaseOrdinary::getAllTableNames(ContextPtr) const\n }\n \n void DatabaseOrdinary::alterTable(ContextPtr local_context, const StorageID & table_id, const StorageInMemoryMetadata & metadata)\n+{\n+    doAlterTable(local_context, table_id, metadata, true);\n+}",
        "comment_created_at": "2025-07-16T17:14:48+00:00",
        "comment_author": "tavplubix",
        "comment_body": "Looks like there's still the same problem for RMT with Ordinary/Atomic: secondary replicas will fail to apply the change when calling it from here: \r\nhttps://github.com/ClickHouse/ClickHouse/blob/7808fb188743a75fc30037d4f821543b31bc14e3/src/Storages/StorageReplicatedMergeTree.cpp#L6508\r\nhttps://github.com/ClickHouse/ClickHouse/blob/7808fb188743a75fc30037d4f821543b31bc14e3/src/Storages/StorageReplicatedMergeTree.cpp#L1717\r\n\r\nSo maybe we don't need `doAlterTable` and need this argument for `alterTable` ",
        "pr_file_module": null
      },
      {
        "comment_id": "2211588098",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83849,
        "pr_file": "src/Databases/DatabaseOrdinary.cpp",
        "discussion_id": "2211033659",
        "commented_code": "@@ -628,6 +628,12 @@ Strings DatabaseOrdinary::getAllTableNames(ContextPtr) const\n }\n \n void DatabaseOrdinary::alterTable(ContextPtr local_context, const StorageID & table_id, const StorageInMemoryMetadata & metadata)\n+{\n+    doAlterTable(local_context, table_id, metadata, true);\n+}",
        "comment_created_at": "2025-07-16T20:53:17+00:00",
        "comment_author": "antaljanosbenjamin",
        "comment_body": "> Looks like there's still the same problem for RMT with Ordinary/Atomic: secondary replicas will fail to apply the change when calling it from here:\r\n\r\nThat is true.\r\n\r\n> So maybe we don't need doAlterTable and need this argument for alterTable\r\n\r\nEither that, or we can disable the check in case of replicated/shared merge tree tables. However I feel that kind of wonky, because what if we add other replicated/shared tables that do the validation themselves. So I think the best is to add tihs to `IDatabase::alterTable`. Will do it tomorrow.",
        "pr_file_module": null
      }
    ]
  }
]