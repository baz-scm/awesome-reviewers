[
  {
    "discussion_id": "2137671383",
    "pr_number": 79636,
    "pr_file": "src/AggregateFunctions/AggregateFunctionWindowFunnel.cpp",
    "created_at": "2025-06-10T11:45:54+00:00",
    "commented_code": "/// Applies conditions only to events with strictly increasing timestamps\n     bool strict_increase;\n \n-    /// Loop through the entire events_list, update the event timestamp value\n-    /// The level path must be 1---2---3---...---check_events_size, find the max event level that satisfied the path in the sliding window.\n-    /// If found, returns the max event level, else return 0.\n+    /// When enabled, returns conversion times between funnel steps\n+    bool conversion_time;\n+\n+    struct FunnelResult\n+    {\n+        UInt8 max_level = 0;\n+        std::vector<T> conversion_times;\n+    };\n+\n     /// The algorithm works in O(n) time, but the overall function works in O(n * log(n)) due to sorting.\n-    UInt8 getEventLevelNonStrictOnce(const AggregateFunctionWindowFunnelData<T>::TimestampEvents & events_list) const\n+    FunnelResult getEventLevelNonStrictOnce(const AggregateFunctionWindowFunnelData<T>::TimestampEvents & events_list) const\n     {\n-        /// events_timestamp stores the timestamp of the first and previous i-th level event happen within time window\n-        std::vector<std::optional<std::pair<UInt64, UInt64>>> events_timestamp(events_size);\n-        bool first_event = false;\n-        for (size_t i = 0; i < events_list.size(); ++i)\n+        /// candidate_events_timestamp stores {first_event_ts, last_event_ts} for the latest candidate sequence reaching level i+1.\n+        std::vector<std::optional<std::pair<T, T>>> candidate_events_timestamp(events_size);\n+\n+        /// Stores the specific timestamps for each step of the sequence that achieved the current overall_max_level.\n+        std::vector<std::optional<T>> winning_sequence_timestamps(events_size);\n+        UInt8 overall_max_level = 0;\n+\n+        // Helper lambda calculates based on stored winning_sequence_timestamps for the best sequence found\n+        auto calculate_conversion_times = [&](UInt8 current_max_level) -> std::vector<T>\n         {\n-            const T & timestamp = events_list[i].first;\n-            const auto & event_idx = events_list[i].second - 1;\n-            if (strict_order && event_idx == -1)\n+            std::vector<T> times;\n+            if (conversion_time && current_max_level > 1)\n             {\n-                if (first_event)\n-                    break;\n-                continue;\n+                times.resize(events_size - 1, 0);\n+                for (size_t i = 0; i < current_max_level - 1; ++i)\n+                {\n+                    // Ensure both current and next step timestamps exist in the stored best sequence\n+                    if (winning_sequence_timestamps[i].has_value() && winning_sequence_timestamps[i+1].has_value())\n+                    {\n+                         // Calculate difference from the consistent sequence timestamps\n+                         times[i] = *winning_sequence_timestamps[i+1] - *winning_sequence_timestamps[i];\n+                    }\n+                }\n+            }\n+            else if (conversion_time) // Ensure vector has correct size even if max_level <= 1\n+            {\n+                // Resize to events_size - 1, initialized to 0\n+                times.resize(events_size - 1, 0);\n+            }\n+            // If not conversion_time, times remains empty, which is fine.\n+            return times;\n+        };\n+\n+        bool first_event_occurred = false; // Track if any event 1 has occurred in strict_order mode\n+\n+        for (const auto & event_pair : events_list)\n+        {\n+            const T & timestamp = event_pair.first;\n+            const auto & event_type = event_pair.second; // This is 1-based event type\n+            const UInt8 event_idx = event_type - 1; // 0-based index, max value is events_size-1 (<= 31)",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2137671383",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 79636,
        "pr_file": "src/AggregateFunctions/AggregateFunctionWindowFunnel.cpp",
        "discussion_id": "2137671383",
        "commented_code": "@@ -290,73 +291,192 @@ class AggregateFunctionWindowFunnel final\n     /// Applies conditions only to events with strictly increasing timestamps\n     bool strict_increase;\n \n-    /// Loop through the entire events_list, update the event timestamp value\n-    /// The level path must be 1---2---3---...---check_events_size, find the max event level that satisfied the path in the sliding window.\n-    /// If found, returns the max event level, else return 0.\n+    /// When enabled, returns conversion times between funnel steps\n+    bool conversion_time;\n+\n+    struct FunnelResult\n+    {\n+        UInt8 max_level = 0;\n+        std::vector<T> conversion_times;\n+    };\n+\n     /// The algorithm works in O(n) time, but the overall function works in O(n * log(n)) due to sorting.\n-    UInt8 getEventLevelNonStrictOnce(const AggregateFunctionWindowFunnelData<T>::TimestampEvents & events_list) const\n+    FunnelResult getEventLevelNonStrictOnce(const AggregateFunctionWindowFunnelData<T>::TimestampEvents & events_list) const\n     {\n-        /// events_timestamp stores the timestamp of the first and previous i-th level event happen within time window\n-        std::vector<std::optional<std::pair<UInt64, UInt64>>> events_timestamp(events_size);\n-        bool first_event = false;\n-        for (size_t i = 0; i < events_list.size(); ++i)\n+        /// candidate_events_timestamp stores {first_event_ts, last_event_ts} for the latest candidate sequence reaching level i+1.\n+        std::vector<std::optional<std::pair<T, T>>> candidate_events_timestamp(events_size);\n+\n+        /// Stores the specific timestamps for each step of the sequence that achieved the current overall_max_level.\n+        std::vector<std::optional<T>> winning_sequence_timestamps(events_size);\n+        UInt8 overall_max_level = 0;\n+\n+        // Helper lambda calculates based on stored winning_sequence_timestamps for the best sequence found\n+        auto calculate_conversion_times = [&](UInt8 current_max_level) -> std::vector<T>\n         {\n-            const T & timestamp = events_list[i].first;\n-            const auto & event_idx = events_list[i].second - 1;\n-            if (strict_order && event_idx == -1)\n+            std::vector<T> times;\n+            if (conversion_time && current_max_level > 1)\n             {\n-                if (first_event)\n-                    break;\n-                continue;\n+                times.resize(events_size - 1, 0);\n+                for (size_t i = 0; i < current_max_level - 1; ++i)\n+                {\n+                    // Ensure both current and next step timestamps exist in the stored best sequence\n+                    if (winning_sequence_timestamps[i].has_value() && winning_sequence_timestamps[i+1].has_value())\n+                    {\n+                         // Calculate difference from the consistent sequence timestamps\n+                         times[i] = *winning_sequence_timestamps[i+1] - *winning_sequence_timestamps[i];\n+                    }\n+                }\n+            }\n+            else if (conversion_time) // Ensure vector has correct size even if max_level <= 1\n+            {\n+                // Resize to events_size - 1, initialized to 0\n+                times.resize(events_size - 1, 0);\n+            }\n+            // If not conversion_time, times remains empty, which is fine.\n+            return times;\n+        };\n+\n+        bool first_event_occurred = false; // Track if any event 1 has occurred in strict_order mode\n+\n+        for (const auto & event_pair : events_list)\n+        {\n+            const T & timestamp = event_pair.first;\n+            const auto & event_type = event_pair.second; // This is 1-based event type\n+            const UInt8 event_idx = event_type - 1; // 0-based index, max value is events_size-1 (<= 31)",
        "comment_created_at": "2025-06-10T11:45:54+00:00",
        "comment_author": "vdimir",
        "comment_body": "if we consider `event_type = 0` as expected value for no match (and we check `event_type == 0)` below, we have underflow in `UInt8 event_idx`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2280745123",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 79636,
        "pr_file": "src/AggregateFunctions/AggregateFunctionWindowFunnel.cpp",
        "discussion_id": "2137671383",
        "commented_code": "@@ -290,73 +291,192 @@ class AggregateFunctionWindowFunnel final\n     /// Applies conditions only to events with strictly increasing timestamps\n     bool strict_increase;\n \n-    /// Loop through the entire events_list, update the event timestamp value\n-    /// The level path must be 1---2---3---...---check_events_size, find the max event level that satisfied the path in the sliding window.\n-    /// If found, returns the max event level, else return 0.\n+    /// When enabled, returns conversion times between funnel steps\n+    bool conversion_time;\n+\n+    struct FunnelResult\n+    {\n+        UInt8 max_level = 0;\n+        std::vector<T> conversion_times;\n+    };\n+\n     /// The algorithm works in O(n) time, but the overall function works in O(n * log(n)) due to sorting.\n-    UInt8 getEventLevelNonStrictOnce(const AggregateFunctionWindowFunnelData<T>::TimestampEvents & events_list) const\n+    FunnelResult getEventLevelNonStrictOnce(const AggregateFunctionWindowFunnelData<T>::TimestampEvents & events_list) const\n     {\n-        /// events_timestamp stores the timestamp of the first and previous i-th level event happen within time window\n-        std::vector<std::optional<std::pair<UInt64, UInt64>>> events_timestamp(events_size);\n-        bool first_event = false;\n-        for (size_t i = 0; i < events_list.size(); ++i)\n+        /// candidate_events_timestamp stores {first_event_ts, last_event_ts} for the latest candidate sequence reaching level i+1.\n+        std::vector<std::optional<std::pair<T, T>>> candidate_events_timestamp(events_size);\n+\n+        /// Stores the specific timestamps for each step of the sequence that achieved the current overall_max_level.\n+        std::vector<std::optional<T>> winning_sequence_timestamps(events_size);\n+        UInt8 overall_max_level = 0;\n+\n+        // Helper lambda calculates based on stored winning_sequence_timestamps for the best sequence found\n+        auto calculate_conversion_times = [&](UInt8 current_max_level) -> std::vector<T>\n         {\n-            const T & timestamp = events_list[i].first;\n-            const auto & event_idx = events_list[i].second - 1;\n-            if (strict_order && event_idx == -1)\n+            std::vector<T> times;\n+            if (conversion_time && current_max_level > 1)\n             {\n-                if (first_event)\n-                    break;\n-                continue;\n+                times.resize(events_size - 1, 0);\n+                for (size_t i = 0; i < current_max_level - 1; ++i)\n+                {\n+                    // Ensure both current and next step timestamps exist in the stored best sequence\n+                    if (winning_sequence_timestamps[i].has_value() && winning_sequence_timestamps[i+1].has_value())\n+                    {\n+                         // Calculate difference from the consistent sequence timestamps\n+                         times[i] = *winning_sequence_timestamps[i+1] - *winning_sequence_timestamps[i];\n+                    }\n+                }\n+            }\n+            else if (conversion_time) // Ensure vector has correct size even if max_level <= 1\n+            {\n+                // Resize to events_size - 1, initialized to 0\n+                times.resize(events_size - 1, 0);\n+            }\n+            // If not conversion_time, times remains empty, which is fine.\n+            return times;\n+        };\n+\n+        bool first_event_occurred = false; // Track if any event 1 has occurred in strict_order mode\n+\n+        for (const auto & event_pair : events_list)\n+        {\n+            const T & timestamp = event_pair.first;\n+            const auto & event_type = event_pair.second; // This is 1-based event type\n+            const UInt8 event_idx = event_type - 1; // 0-based index, max value is events_size-1 (<= 31)",
        "comment_created_at": "2025-08-17T06:44:12+00:00",
        "comment_author": "ataberkgrl",
        "comment_body": "Yeah you are right fixed it, thanks.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2271018486",
    "pr_number": 83837,
    "pr_file": "src/Processors/QueryPlan/Optimizations/useTextIndexSearch.cpp",
    "created_at": "2025-08-12T19:49:22+00:00",
    "commented_code": "+#include <Columns/ColumnConst.h>\n+#include <Core/Field.h>\n+#include <Core/SortDescription.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <Functions/IFunction.h>\n+#include <Processors/QueryPlan/ExpressionStep.h>\n+#include <Processors/QueryPlan/FilterStep.h>\n+#include <Processors/QueryPlan/LimitStep.h>\n+#include <Processors/QueryPlan/Optimizations/Optimizations.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n+#include <Processors/QueryPlan/SortingStep.h>\n+#include <Storages/MergeTree/MergeTreeIndices.h>\n+#include <Functions/FunctionFactory.h>\n+\n+#include <Functions/IFunctionAdaptors.h>\n+#include <Functions/FunctionsStringSearch.h>\n+#include <Interpreters/Context_fwd.h>\n+#include <Interpreters/ActionsDAG.h>\n+#include <__format/format_functions.h>\n+#include \"base/defines.h\"\n+#include <algorithm>\n+#include <memory>\n+#include <format>\n+\n+#include <cstdio>\n+\n+namespace DB::QueryPlanOptimizations\n+{\n+\n+static void printHierarchicalActions(const ActionsDAG::Node * node, int indent)\n+{\n+    if (indent == 0)\n+        std::println(\"=== Node: {} ===\", node->result_name);\n+    else\n+        for (int i = 0; i < 3 * indent; ++i)\n+            std::print(\" \");\n+\n+    std::print(\"{} \", static_cast<const void*>(node));\n+\n+    if (node->function_base)\n+        std::print(\"BaseFunc: {} -> \", node->function_base->getName());\n+\n+    if (node->function)\n+        std::print(\"ExecutableFunc: {} -> \", node->function->getName());\n+\n+    std::print(\" result {} (compiled: {}) (type: {}) (node type {})\",\n+        node->result_name,\n+        node->is_function_compiled,\n+        node->result_type->getName(),\n+        static_cast<int>(node->type)\n+    );\n+\n+    if (node->column)\n+        std::print(\" column {} (id: {})\",\n+            node->column->getName(),\n+            static_cast<int>(node->column->getDataType())\n+        );\n+\n+    std::println(\"\");\n+\n+    for (const ActionsDAG::Node * subnode : node->children)\n+        printHierarchicalActions(subnode, indent + 1);\n+}\n+\n+\n+class FunctionReplacerDAG {\n+\n+    ActionsDAG &dag;\n+    const std::map<std::string, std::string> &map_indexed_columns;\n+    std::vector<std::string> removed_columns;\n+\n+    // Check if these nodes are already there.\n+    const ActionsDAG::Node * pindex;\n+    const ActionsDAG::Node * poffsets;\n+\n+    /// Extract the context from the function. The functions need to store a reference to the context in order to be reemplazable with an\n+    /// index function.\n+    static ContextPtr extractFunctionContext(const ActionsDAG::Node &original_function_node)\n+    {\n+        chassert(original_function_node.type == ActionsDAG::ActionType::FUNCTION);",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2271018486",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83837,
        "pr_file": "src/Processors/QueryPlan/Optimizations/useTextIndexSearch.cpp",
        "discussion_id": "2271018486",
        "commented_code": "@@ -0,0 +1,464 @@\n+#include <Columns/ColumnConst.h>\n+#include <Core/Field.h>\n+#include <Core/SortDescription.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <Functions/IFunction.h>\n+#include <Processors/QueryPlan/ExpressionStep.h>\n+#include <Processors/QueryPlan/FilterStep.h>\n+#include <Processors/QueryPlan/LimitStep.h>\n+#include <Processors/QueryPlan/Optimizations/Optimizations.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n+#include <Processors/QueryPlan/SortingStep.h>\n+#include <Storages/MergeTree/MergeTreeIndices.h>\n+#include <Functions/FunctionFactory.h>\n+\n+#include <Functions/IFunctionAdaptors.h>\n+#include <Functions/FunctionsStringSearch.h>\n+#include <Interpreters/Context_fwd.h>\n+#include <Interpreters/ActionsDAG.h>\n+#include <__format/format_functions.h>\n+#include \"base/defines.h\"\n+#include <algorithm>\n+#include <memory>\n+#include <format>\n+\n+#include <cstdio>\n+\n+namespace DB::QueryPlanOptimizations\n+{\n+\n+static void printHierarchicalActions(const ActionsDAG::Node * node, int indent)\n+{\n+    if (indent == 0)\n+        std::println(\"=== Node: {} ===\", node->result_name);\n+    else\n+        for (int i = 0; i < 3 * indent; ++i)\n+            std::print(\" \");\n+\n+    std::print(\"{} \", static_cast<const void*>(node));\n+\n+    if (node->function_base)\n+        std::print(\"BaseFunc: {} -> \", node->function_base->getName());\n+\n+    if (node->function)\n+        std::print(\"ExecutableFunc: {} -> \", node->function->getName());\n+\n+    std::print(\" result {} (compiled: {}) (type: {}) (node type {})\",\n+        node->result_name,\n+        node->is_function_compiled,\n+        node->result_type->getName(),\n+        static_cast<int>(node->type)\n+    );\n+\n+    if (node->column)\n+        std::print(\" column {} (id: {})\",\n+            node->column->getName(),\n+            static_cast<int>(node->column->getDataType())\n+        );\n+\n+    std::println(\"\");\n+\n+    for (const ActionsDAG::Node * subnode : node->children)\n+        printHierarchicalActions(subnode, indent + 1);\n+}\n+\n+\n+class FunctionReplacerDAG {\n+\n+    ActionsDAG &dag;\n+    const std::map<std::string, std::string> &map_indexed_columns;\n+    std::vector<std::string> removed_columns;\n+\n+    // Check if these nodes are already there.\n+    const ActionsDAG::Node * pindex;\n+    const ActionsDAG::Node * poffsets;\n+\n+    /// Extract the context from the function. The functions need to store a reference to the context in order to be reemplazable with an\n+    /// index function.\n+    static ContextPtr extractFunctionContext(const ActionsDAG::Node &original_function_node)\n+    {\n+        chassert(original_function_node.type == ActionsDAG::ActionType::FUNCTION);",
        "comment_created_at": "2025-08-12T19:49:22+00:00",
        "comment_author": "rschu1ze",
        "comment_body": "Feel free to remove all pointer-checking asserts (the one in l. 83 is okay). The code will crash properly if there's a problem with pointers anyways.",
        "pr_file_module": null
      },
      {
        "comment_id": "2272698614",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83837,
        "pr_file": "src/Processors/QueryPlan/Optimizations/useTextIndexSearch.cpp",
        "discussion_id": "2271018486",
        "commented_code": "@@ -0,0 +1,464 @@\n+#include <Columns/ColumnConst.h>\n+#include <Core/Field.h>\n+#include <Core/SortDescription.h>\n+#include <DataTypes/DataTypeArray.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <Functions/IFunction.h>\n+#include <Processors/QueryPlan/ExpressionStep.h>\n+#include <Processors/QueryPlan/FilterStep.h>\n+#include <Processors/QueryPlan/LimitStep.h>\n+#include <Processors/QueryPlan/Optimizations/Optimizations.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n+#include <Processors/QueryPlan/ReadFromMergeTree.h>\n+#include <Processors/QueryPlan/SortingStep.h>\n+#include <Storages/MergeTree/MergeTreeIndices.h>\n+#include <Functions/FunctionFactory.h>\n+\n+#include <Functions/IFunctionAdaptors.h>\n+#include <Functions/FunctionsStringSearch.h>\n+#include <Interpreters/Context_fwd.h>\n+#include <Interpreters/ActionsDAG.h>\n+#include <__format/format_functions.h>\n+#include \"base/defines.h\"\n+#include <algorithm>\n+#include <memory>\n+#include <format>\n+\n+#include <cstdio>\n+\n+namespace DB::QueryPlanOptimizations\n+{\n+\n+static void printHierarchicalActions(const ActionsDAG::Node * node, int indent)\n+{\n+    if (indent == 0)\n+        std::println(\"=== Node: {} ===\", node->result_name);\n+    else\n+        for (int i = 0; i < 3 * indent; ++i)\n+            std::print(\" \");\n+\n+    std::print(\"{} \", static_cast<const void*>(node));\n+\n+    if (node->function_base)\n+        std::print(\"BaseFunc: {} -> \", node->function_base->getName());\n+\n+    if (node->function)\n+        std::print(\"ExecutableFunc: {} -> \", node->function->getName());\n+\n+    std::print(\" result {} (compiled: {}) (type: {}) (node type {})\",\n+        node->result_name,\n+        node->is_function_compiled,\n+        node->result_type->getName(),\n+        static_cast<int>(node->type)\n+    );\n+\n+    if (node->column)\n+        std::print(\" column {} (id: {})\",\n+            node->column->getName(),\n+            static_cast<int>(node->column->getDataType())\n+        );\n+\n+    std::println(\"\");\n+\n+    for (const ActionsDAG::Node * subnode : node->children)\n+        printHierarchicalActions(subnode, indent + 1);\n+}\n+\n+\n+class FunctionReplacerDAG {\n+\n+    ActionsDAG &dag;\n+    const std::map<std::string, std::string> &map_indexed_columns;\n+    std::vector<std::string> removed_columns;\n+\n+    // Check if these nodes are already there.\n+    const ActionsDAG::Node * pindex;\n+    const ActionsDAG::Node * poffsets;\n+\n+    /// Extract the context from the function. The functions need to store a reference to the context in order to be reemplazable with an\n+    /// index function.\n+    static ContextPtr extractFunctionContext(const ActionsDAG::Node &original_function_node)\n+    {\n+        chassert(original_function_node.type == ActionsDAG::ActionType::FUNCTION);",
        "comment_created_at": "2025-08-13T09:28:32+00:00",
        "comment_author": "Ergus",
        "comment_body": "These checks are recommended because these are `dynamic_casts` and they are the way to ensure that we are trying to cast what we expect. \r\n\r\nInitially these checks were used to return `nullptr` if any of the casts failed. But when the code was more completed I changed the if conditions with assertions (simpler to debug) because none of those should fail.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2224965139",
    "pr_number": 83408,
    "pr_file": "src/Storages/System/StorageSystemDatabaseReplicas.cpp",
    "created_at": "2025-07-23T09:28:42+00:00",
    "commented_code": "+#include <future>\n+#include <memory>\n+#include <Access/ContextAccess.h>\n+#include <Columns/ColumnString.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Databases/DatabaseReplicated.h>\n+#include <Databases/IDatabase.h>\n+#include <Databases/ReplicatedDatabaseStatus.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/DatabaseCatalog.h>\n+#include <Interpreters/formatWithPossiblyHidingSecrets.h>\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Storages/SelectQueryInfo.h>\n+#include <Storages/System/StorageSystemDatabaseReplicas.h>\n+#include <Storages/VirtualColumnUtils.h>\n+#include <Common/logger_useful.h>\n+#include <Parsers/Kusto/KustoFunctions/KQLDataTypeFunctions.h>\n+#include <Interpreters/ProcessList.h>\n+#include <Columns/ColumnsNumber.h>\n+#include <Processors/Sources/NullSource.h>\n+#include <QueryPipeline/Pipe.h>\n+#include <QueryPipeline/QueryPipelineBuilder.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n+#include <Processors/QueryPlan/SourceStepWithFilter.h>\n+\n+\n+namespace CurrentMetrics\n+{\n+    extern const Metric SystemDatabaseReplicasThreads;\n+    extern const Metric SystemDatabaseReplicasThreadsActive;\n+    extern const Metric SystemDatabaseReplicasThreadsScheduled;\n+}\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int ABORTED;\n+    extern const int QUERY_WAS_CANCELLED;\n+}\n+\n+namespace\n+{\n+\n+class StatusRequestsPool\n+{\n+public:\n+    struct RequestInfo\n+    {\n+        UInt64 request_id = 0;\n+        std::shared_future<ReplicatedDatabaseStatus> future;\n+    };\n+\n+private:\n+    ThreadPool thread_pool;\n+\n+    std::mutex mutex;\n+    std::unordered_map<DatabasePtr, RequestInfo> current_requests TSA_GUARDED_BY(mutex);\n+    std::deque<std::tuple<UInt64, DatabasePtr, std::shared_ptr<std::promise<ReplicatedDatabaseStatus>>, bool>> requests_to_schedule TSA_GUARDED_BY(mutex);\n+    UInt64 request_id TSA_GUARDED_BY(mutex) = 0;\n+\n+    LoggerPtr log;\n+\n+public:\n+    explicit StatusRequestsPool(size_t max_threads)\n+        : thread_pool(CurrentMetrics::SystemDatabaseReplicasThreads, CurrentMetrics::SystemDatabaseReplicasThreadsActive, CurrentMetrics::SystemDatabaseReplicasThreadsScheduled, max_threads)\n+        , log(getLogger(\"StatusRequestsPool\"))\n+    {}\n+\n+    ~StatusRequestsPool()\n+    {\n+        thread_pool.wait();\n+        for (auto & request : requests_to_schedule)\n+            std::get<2>(request)->set_exception(std::make_exception_ptr(\n+                DB::Exception(ErrorCodes::QUERY_WAS_CANCELLED, \"StatusRequestsPool is destroyed\")));\n+    }\n+\n+    RequestInfo addRequest(DatabasePtr database, const bool with_zk_fields)\n+    {\n+        std::shared_ptr<std::promise<ReplicatedDatabaseStatus>> promise;\n+        std::shared_future<ReplicatedDatabaseStatus> future;\n+        UInt64 this_request_id = 0;\n+\n+        {\n+            std::lock_guard lock(mutex);\n+\n+            auto existing_request = current_requests.find(database);\n+            if (existing_request != current_requests.end())\n+            {\n+                LOG_DEBUG(log, \"Attaching to existing request for database {}\", database->getDatabaseName());\n+                return existing_request->second;\n+            }\n+\n+            this_request_id = request_id;\n+            ++request_id;\n+\n+            promise = std::make_shared<std::promise<ReplicatedDatabaseStatus>>();\n+            future = promise->get_future().share();\n+\n+            current_requests[database] = { .request_id = this_request_id, .future = future };\n+\n+            LOG_DEBUG(log, \"Making new request for database {}\", database->getDatabaseName());\n+\n+            requests_to_schedule.emplace_back(this_request_id, database, promise, with_zk_fields);\n+        }\n+\n+        return {this_request_id, future};\n+    }\n+\n+    void scheduleRequests(UInt64 max_request_id, QueryStatusPtr query_status)\n+    {\n+        while (true)\n+        {\n+            if (query_status)\n+                query_status->checkTimeLimit();\n+\n+            std::tuple<UInt64, DatabasePtr, std::shared_ptr<std::promise<ReplicatedDatabaseStatus>>, bool> req;\n+            {\n+                std::lock_guard lock(mutex);\n+                if (requests_to_schedule.empty())\n+                    break;\n+\n+                req = requests_to_schedule.front();\n+\n+                if (std::get<0>(req) > max_request_id)\n+                    break;\n+\n+                requests_to_schedule.pop_front();\n+            }\n+\n+            auto get_status_task = [this, req, thread_group = CurrentThread::getGroup()]() mutable\n+            {\n+                ThreadGroupSwitcher switcher(thread_group, \"DBReplicas\");\n+\n+                auto & [_, database, promise, with_zk_fields] = req;\n+                try\n+                {\n+                    ReplicatedDatabaseStatus status;\n+\n+                    DatabaseReplicated * replicated_database = dynamic_cast<DatabaseReplicated *>(database.get());",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2224965139",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83408,
        "pr_file": "src/Storages/System/StorageSystemDatabaseReplicas.cpp",
        "discussion_id": "2224965139",
        "commented_code": "@@ -0,0 +1,484 @@\n+#include <future>\n+#include <memory>\n+#include <Access/ContextAccess.h>\n+#include <Columns/ColumnString.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Databases/DatabaseReplicated.h>\n+#include <Databases/IDatabase.h>\n+#include <Databases/ReplicatedDatabaseStatus.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/DatabaseCatalog.h>\n+#include <Interpreters/formatWithPossiblyHidingSecrets.h>\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Storages/SelectQueryInfo.h>\n+#include <Storages/System/StorageSystemDatabaseReplicas.h>\n+#include <Storages/VirtualColumnUtils.h>\n+#include <Common/logger_useful.h>\n+#include <Parsers/Kusto/KustoFunctions/KQLDataTypeFunctions.h>\n+#include <Interpreters/ProcessList.h>\n+#include <Columns/ColumnsNumber.h>\n+#include <Processors/Sources/NullSource.h>\n+#include <QueryPipeline/Pipe.h>\n+#include <QueryPipeline/QueryPipelineBuilder.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n+#include <Processors/QueryPlan/SourceStepWithFilter.h>\n+\n+\n+namespace CurrentMetrics\n+{\n+    extern const Metric SystemDatabaseReplicasThreads;\n+    extern const Metric SystemDatabaseReplicasThreadsActive;\n+    extern const Metric SystemDatabaseReplicasThreadsScheduled;\n+}\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int ABORTED;\n+    extern const int QUERY_WAS_CANCELLED;\n+}\n+\n+namespace\n+{\n+\n+class StatusRequestsPool\n+{\n+public:\n+    struct RequestInfo\n+    {\n+        UInt64 request_id = 0;\n+        std::shared_future<ReplicatedDatabaseStatus> future;\n+    };\n+\n+private:\n+    ThreadPool thread_pool;\n+\n+    std::mutex mutex;\n+    std::unordered_map<DatabasePtr, RequestInfo> current_requests TSA_GUARDED_BY(mutex);\n+    std::deque<std::tuple<UInt64, DatabasePtr, std::shared_ptr<std::promise<ReplicatedDatabaseStatus>>, bool>> requests_to_schedule TSA_GUARDED_BY(mutex);\n+    UInt64 request_id TSA_GUARDED_BY(mutex) = 0;\n+\n+    LoggerPtr log;\n+\n+public:\n+    explicit StatusRequestsPool(size_t max_threads)\n+        : thread_pool(CurrentMetrics::SystemDatabaseReplicasThreads, CurrentMetrics::SystemDatabaseReplicasThreadsActive, CurrentMetrics::SystemDatabaseReplicasThreadsScheduled, max_threads)\n+        , log(getLogger(\"StatusRequestsPool\"))\n+    {}\n+\n+    ~StatusRequestsPool()\n+    {\n+        thread_pool.wait();\n+        for (auto & request : requests_to_schedule)\n+            std::get<2>(request)->set_exception(std::make_exception_ptr(\n+                DB::Exception(ErrorCodes::QUERY_WAS_CANCELLED, \"StatusRequestsPool is destroyed\")));\n+    }\n+\n+    RequestInfo addRequest(DatabasePtr database, const bool with_zk_fields)\n+    {\n+        std::shared_ptr<std::promise<ReplicatedDatabaseStatus>> promise;\n+        std::shared_future<ReplicatedDatabaseStatus> future;\n+        UInt64 this_request_id = 0;\n+\n+        {\n+            std::lock_guard lock(mutex);\n+\n+            auto existing_request = current_requests.find(database);\n+            if (existing_request != current_requests.end())\n+            {\n+                LOG_DEBUG(log, \"Attaching to existing request for database {}\", database->getDatabaseName());\n+                return existing_request->second;\n+            }\n+\n+            this_request_id = request_id;\n+            ++request_id;\n+\n+            promise = std::make_shared<std::promise<ReplicatedDatabaseStatus>>();\n+            future = promise->get_future().share();\n+\n+            current_requests[database] = { .request_id = this_request_id, .future = future };\n+\n+            LOG_DEBUG(log, \"Making new request for database {}\", database->getDatabaseName());\n+\n+            requests_to_schedule.emplace_back(this_request_id, database, promise, with_zk_fields);\n+        }\n+\n+        return {this_request_id, future};\n+    }\n+\n+    void scheduleRequests(UInt64 max_request_id, QueryStatusPtr query_status)\n+    {\n+        while (true)\n+        {\n+            if (query_status)\n+                query_status->checkTimeLimit();\n+\n+            std::tuple<UInt64, DatabasePtr, std::shared_ptr<std::promise<ReplicatedDatabaseStatus>>, bool> req;\n+            {\n+                std::lock_guard lock(mutex);\n+                if (requests_to_schedule.empty())\n+                    break;\n+\n+                req = requests_to_schedule.front();\n+\n+                if (std::get<0>(req) > max_request_id)\n+                    break;\n+\n+                requests_to_schedule.pop_front();\n+            }\n+\n+            auto get_status_task = [this, req, thread_group = CurrentThread::getGroup()]() mutable\n+            {\n+                ThreadGroupSwitcher switcher(thread_group, \"DBReplicas\");\n+\n+                auto & [_, database, promise, with_zk_fields] = req;\n+                try\n+                {\n+                    ReplicatedDatabaseStatus status;\n+\n+                    DatabaseReplicated * replicated_database = dynamic_cast<DatabaseReplicated *>(database.get());",
        "comment_created_at": "2025-07-23T09:28:42+00:00",
        "comment_author": "aalexfvk",
        "comment_body": "Should `dynamic_cast` be checked for nullptr ?",
        "pr_file_module": null
      },
      {
        "comment_id": "2226883001",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 83408,
        "pr_file": "src/Storages/System/StorageSystemDatabaseReplicas.cpp",
        "discussion_id": "2224965139",
        "commented_code": "@@ -0,0 +1,484 @@\n+#include <future>\n+#include <memory>\n+#include <Access/ContextAccess.h>\n+#include <Columns/ColumnString.h>\n+#include <DataTypes/DataTypeString.h>\n+#include <DataTypes/DataTypesNumber.h>\n+#include <Databases/DatabaseReplicated.h>\n+#include <Databases/IDatabase.h>\n+#include <Databases/ReplicatedDatabaseStatus.h>\n+#include <Interpreters/Context.h>\n+#include <Interpreters/DatabaseCatalog.h>\n+#include <Interpreters/formatWithPossiblyHidingSecrets.h>\n+#include <Parsers/ASTCreateQuery.h>\n+#include <Storages/SelectQueryInfo.h>\n+#include <Storages/System/StorageSystemDatabaseReplicas.h>\n+#include <Storages/VirtualColumnUtils.h>\n+#include <Common/logger_useful.h>\n+#include <Parsers/Kusto/KustoFunctions/KQLDataTypeFunctions.h>\n+#include <Interpreters/ProcessList.h>\n+#include <Columns/ColumnsNumber.h>\n+#include <Processors/Sources/NullSource.h>\n+#include <QueryPipeline/Pipe.h>\n+#include <QueryPipeline/QueryPipelineBuilder.h>\n+#include <Processors/QueryPlan/QueryPlan.h>\n+#include <Processors/QueryPlan/SourceStepWithFilter.h>\n+\n+\n+namespace CurrentMetrics\n+{\n+    extern const Metric SystemDatabaseReplicasThreads;\n+    extern const Metric SystemDatabaseReplicasThreadsActive;\n+    extern const Metric SystemDatabaseReplicasThreadsScheduled;\n+}\n+\n+namespace DB\n+{\n+\n+namespace ErrorCodes\n+{\n+    extern const int ABORTED;\n+    extern const int QUERY_WAS_CANCELLED;\n+}\n+\n+namespace\n+{\n+\n+class StatusRequestsPool\n+{\n+public:\n+    struct RequestInfo\n+    {\n+        UInt64 request_id = 0;\n+        std::shared_future<ReplicatedDatabaseStatus> future;\n+    };\n+\n+private:\n+    ThreadPool thread_pool;\n+\n+    std::mutex mutex;\n+    std::unordered_map<DatabasePtr, RequestInfo> current_requests TSA_GUARDED_BY(mutex);\n+    std::deque<std::tuple<UInt64, DatabasePtr, std::shared_ptr<std::promise<ReplicatedDatabaseStatus>>, bool>> requests_to_schedule TSA_GUARDED_BY(mutex);\n+    UInt64 request_id TSA_GUARDED_BY(mutex) = 0;\n+\n+    LoggerPtr log;\n+\n+public:\n+    explicit StatusRequestsPool(size_t max_threads)\n+        : thread_pool(CurrentMetrics::SystemDatabaseReplicasThreads, CurrentMetrics::SystemDatabaseReplicasThreadsActive, CurrentMetrics::SystemDatabaseReplicasThreadsScheduled, max_threads)\n+        , log(getLogger(\"StatusRequestsPool\"))\n+    {}\n+\n+    ~StatusRequestsPool()\n+    {\n+        thread_pool.wait();\n+        for (auto & request : requests_to_schedule)\n+            std::get<2>(request)->set_exception(std::make_exception_ptr(\n+                DB::Exception(ErrorCodes::QUERY_WAS_CANCELLED, \"StatusRequestsPool is destroyed\")));\n+    }\n+\n+    RequestInfo addRequest(DatabasePtr database, const bool with_zk_fields)\n+    {\n+        std::shared_ptr<std::promise<ReplicatedDatabaseStatus>> promise;\n+        std::shared_future<ReplicatedDatabaseStatus> future;\n+        UInt64 this_request_id = 0;\n+\n+        {\n+            std::lock_guard lock(mutex);\n+\n+            auto existing_request = current_requests.find(database);\n+            if (existing_request != current_requests.end())\n+            {\n+                LOG_DEBUG(log, \"Attaching to existing request for database {}\", database->getDatabaseName());\n+                return existing_request->second;\n+            }\n+\n+            this_request_id = request_id;\n+            ++request_id;\n+\n+            promise = std::make_shared<std::promise<ReplicatedDatabaseStatus>>();\n+            future = promise->get_future().share();\n+\n+            current_requests[database] = { .request_id = this_request_id, .future = future };\n+\n+            LOG_DEBUG(log, \"Making new request for database {}\", database->getDatabaseName());\n+\n+            requests_to_schedule.emplace_back(this_request_id, database, promise, with_zk_fields);\n+        }\n+\n+        return {this_request_id, future};\n+    }\n+\n+    void scheduleRequests(UInt64 max_request_id, QueryStatusPtr query_status)\n+    {\n+        while (true)\n+        {\n+            if (query_status)\n+                query_status->checkTimeLimit();\n+\n+            std::tuple<UInt64, DatabasePtr, std::shared_ptr<std::promise<ReplicatedDatabaseStatus>>, bool> req;\n+            {\n+                std::lock_guard lock(mutex);\n+                if (requests_to_schedule.empty())\n+                    break;\n+\n+                req = requests_to_schedule.front();\n+\n+                if (std::get<0>(req) > max_request_id)\n+                    break;\n+\n+                requests_to_schedule.pop_front();\n+            }\n+\n+            auto get_status_task = [this, req, thread_group = CurrentThread::getGroup()]() mutable\n+            {\n+                ThreadGroupSwitcher switcher(thread_group, \"DBReplicas\");\n+\n+                auto & [_, database, promise, with_zk_fields] = req;\n+                try\n+                {\n+                    ReplicatedDatabaseStatus status;\n+\n+                    DatabaseReplicated * replicated_database = dynamic_cast<DatabaseReplicated *>(database.get());",
        "comment_created_at": "2025-07-23T22:43:28+00:00",
        "comment_author": "k-morozov",
        "comment_body": "No, we have here only replicated databases. \r\nHowever, I am going to refactoing the pools so this check will be added.",
        "pr_file_module": null
      }
    ]
  }
]