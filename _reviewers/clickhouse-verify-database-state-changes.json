[
  {
    "discussion_id": "2284382487",
    "pr_number": 85769,
    "pr_file": "tests/integration/test_storage_iceberg/test.py",
    "created_at": "2025-08-19T07:31:03+00:00",
    "commented_code": ")\n     # drop should not delete user data\n     assert len(files) > 0\n-    \n+\n+\n+@pytest.mark.parametrize(\"format_version\", [1, 2])\n+@pytest.mark.parametrize(\"storage_type\", [\"s3\", \"local\", \"azure\"])\n+def test_writes_schema_evolution(started_cluster, format_version, storage_type):\n+    instance = started_cluster.instances[\"node1\"]\n+    spark = started_cluster.spark_session\n+    TABLE_NAME = \"test_bucket_partition_pruning_\" + storage_type + \"_\" + get_uuid_str()\n+\n+    create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, \"(x Int32)\", format_version)\n+    assert instance.query(f\"SELECT * FROM {TABLE_NAME} ORDER BY ALL\") == ''\n+\n+    instance.query(f\"ALTER TABLE {TABLE_NAME} MODIFY COLUMN x Int64;\", settings={\"allow_experimental_insert_into_iceberg\": 1})\n+\n+    instance.query(f\"INSERT INTO {TABLE_NAME} VALUES (123);\", settings={\"allow_experimental_insert_into_iceberg\": 1})\n+    assert instance.query(f\"SELECT * FROM {TABLE_NAME} ORDER BY ALL\") == '123\n'",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2284382487",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85769,
        "pr_file": "tests/integration/test_storage_iceberg/test.py",
        "discussion_id": "2284382487",
        "commented_code": "@@ -2717,7 +2717,45 @@ def test_writes_drop_table(started_cluster, format_version, storage_type):\n     )\n     # drop should not delete user data\n     assert len(files) > 0\n-    \n+\n+\n+@pytest.mark.parametrize(\"format_version\", [1, 2])\n+@pytest.mark.parametrize(\"storage_type\", [\"s3\", \"local\", \"azure\"])\n+def test_writes_schema_evolution(started_cluster, format_version, storage_type):\n+    instance = started_cluster.instances[\"node1\"]\n+    spark = started_cluster.spark_session\n+    TABLE_NAME = \"test_bucket_partition_pruning_\" + storage_type + \"_\" + get_uuid_str()\n+\n+    create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, \"(x Int32)\", format_version)\n+    assert instance.query(f\"SELECT * FROM {TABLE_NAME} ORDER BY ALL\") == ''\n+\n+    instance.query(f\"ALTER TABLE {TABLE_NAME} MODIFY COLUMN x Int64;\", settings={\"allow_experimental_insert_into_iceberg\": 1})\n+\n+    instance.query(f\"INSERT INTO {TABLE_NAME} VALUES (123);\", settings={\"allow_experimental_insert_into_iceberg\": 1})\n+    assert instance.query(f\"SELECT * FROM {TABLE_NAME} ORDER BY ALL\") == '123\\n'",
        "comment_created_at": "2025-08-19T07:31:03+00:00",
        "comment_author": "kssenii",
        "comment_body": "Could be also useful to check `show create table`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2283935134",
    "pr_number": 85637,
    "pr_file": "tests/integration/test_drop_database_replica/test.py",
    "created_at": "2025-08-19T03:01:08+00:00",
    "commented_code": "+import logging\n+import os\n+import re\n+import shutil\n+import threading\n+import time\n+\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+test_recover_staled_replica_run = 1\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance(\n+    \"node1\",\n+    main_configs=[\n+        \"configs/config.xml\",\n+    ],\n+    user_configs=[\"configs/settings.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+    macros={\"shard\": \"s1\", \"replica\": \"r1\"},\n+    with_minio=True,\n+)\n+\n+node2 = cluster.add_instance(\n+    \"node2\",\n+    main_configs=[\n+        \"configs/config.xml\",\n+    ],\n+    user_configs=[\"configs/settings.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+    macros={\"shard\": \"s1\", \"replica\": \"r2\"},\n+    with_minio=True,\n+)\n+\n+node3 = cluster.add_instance(\n+    \"node3\",\n+    main_configs=[\n+        \"configs/config.xml\",\n+    ],\n+    user_configs=[\"configs/settings.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+    macros={\"shard\": \"s2\", \"replica\": \"r1\"},\n+    with_minio=True,\n+)\n+node4 = cluster.add_instance(\n+    \"node4\",\n+    main_configs=[\n+        \"configs/config.xml\",\n+    ],\n+    user_configs=[\"configs/settings.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+    macros={\"shard\": \"s2\", \"replica\": \"r2\"},\n+    with_minio=True,\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_drop_database_replica(started_cluster):\n+    node1.query(\"DROP DATABASE IF EXISTS db\")\n+    node2.query(\"DROP DATABASE IF EXISTS db\")\n+    node3.query(\"DROP DATABASE IF EXISTS db\")\n+    node4.query(\"DROP DATABASE IF EXISTS db\")\n+\n+    zk_path = \"/test/db\"\n+    node1.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+\n+    node1.query(f\"CREATE TABLE db.t (x INT) ENGINE=MergeTree ORDER BY x\")\n+\n+    assert \"SYNTAX_ERROR\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM TABLE t\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM DATABASE db\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 'r1' FROM SHARD 's1' FROM DATABASE db\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM ZKPATH '{zk_path}'\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 'r1' FROM SHARD 's1' FROM ZKPATH '{zk_path}'\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM ZKPATH '{zk_path}/'\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 'r1' FROM SHARD 's1' FROM ZKPATH '{zk_path}/'\"\n+    )\n+\n+    node2.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+    node3.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+\n+    node2.query(f\"SYSTEM SYNC DATABASE REPLICA db\")\n+    node3.query(f\"SYSTEM SYNC DATABASE REPLICA db\")\n+\n+    assert \"is active, cannot drop it\" in node2.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM DATABASE db\"\n+    )\n+\n+    node1.query(f\"CREATE TABLE db.t2 (x INT) ENGINE=Log\")\n+    node1.query(f\"CREATE TABLE db.t3 (x INT) ENGINE=Log\")\n+    node1.query(f\"CREATE TABLE db.t4 (x INT) ENGINE=Log\")\n+\n+    node4_uuid = node4.query(\"SELECT serverUUID()\").strip()\n+    node4.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+    node4.query(\"DETACH DATABASE db\")\n+    node4.query(\n+        f\"INSERT INTO system.zookeeper(name, path, value) VALUES ('active', '{zk_path}/replicas/s2|r2', '{node4_uuid}')\",\n+    )\n+\n+    assert \"TIMEOUT_EXCEEDED\" in node1.query_and_get_error(\n+        \"CREATE TABLE db.t22 (n int) ENGINE=Log\",\n+        settings={\n+            \"distributed_ddl_task_timeout\": 5,\n+            \"distributed_ddl_output_mode\": \"none_only_active\",\n+        },\n+    )\n+    assert \"TIMEOUT_EXCEEDED\" in node1.query_and_get_error(\n+        \"CREATE TABLE db.t33 (n int) ENGINE=Log\",\n+        settings={\n+            \"distributed_ddl_task_timeout\": 5,\n+            \"distributed_ddl_output_mode\": \"throw_only_active\",\n+        },\n+    )\n+    node1.query(\n+        \"CREATE TABLE db.t44 (n int) ENGINE=Log\",\n+        settings={\n+            \"distributed_ddl_task_timeout\": 5,\n+            \"distributed_ddl_output_mode\": \"null_status_on_timeout_only_active\",\n+        },\n+    )\n+    node4.query(\"ATTACH DATABASE db\")\n+\n+    node3.query(\"DETACH DATABASE db\")\n+    node4.query(\"SYSTEM DROP DATABASE replica 'r1' FROM SHARD 's2' FROM DATABASE db\")\n+    node3.query(\"ATTACH DATABASE db\")\n+    assert \"Database is in readonly mode\" in node3.query_and_get_error(\n+        \"CREATE TABLE db.t55 (n int) ENGINE=MergeTree\",\n+        settings={\n+            \"distributed_ddl_output_mode\": \"none\",\n+        },\n+    )\n+    node4.query(\"DROP DATABASE db SYNC\")\n+\n+    node2.query(\"DETACH DATABASE db\")\n+    node1.query(\"SYSTEM DROP DATABASE REPLICA 's1|r2' FROM DATABASE db\")\n+    node2.query(\"ATTACH DATABASE db\")\n+    assert \"Database is in readonly mode\" in node2.query_and_get_error(\n+        \"CREATE TABLE db.t55 (n int) ENGINE=MergeTree\",\n+        settings={\n+            \"distributed_ddl_output_mode\": \"none\",\n+        },\n+    )\n+\n+    node1.query(\"DETACH DATABASE db\")\n+    node4.query(f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM ZKPATH '{zk_path}'\")\n+    node1.query(\"ATTACH DATABASE db\")\n+    assert \"Database is in readonly mode\" in node1.query_and_get_error(\n+        \"CREATE TABLE db.t55 (n int) ENGINE=MergeTree\",\n+        settings={\n+            \"distributed_ddl_output_mode\": \"none\",\n+        },\n+    )\n+\n+    node1.query(f\"SYSTEM DROP DATABASE REPLICA 'dummy' FROM SHARD 'dummy'\")\n+\n+    node1.query(\"DROP DATABASE db SYNC\")\n+    node2.query(\"DROP DATABASE db SYNC\")\n+    node3.query(\"DROP DATABASE db SYNC\")\n+\n+    node4.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+\n+    node4.query(\n+        \"CREATE TABLE db.rmt (x INT) ENGINE=ReplicatedMergeTree ORDER BY x\",",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2283935134",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85637,
        "pr_file": "tests/integration/test_drop_database_replica/test.py",
        "discussion_id": "2283935134",
        "commented_code": "@@ -0,0 +1,215 @@\n+import logging\n+import os\n+import re\n+import shutil\n+import threading\n+import time\n+\n+import pytest\n+\n+from helpers.cluster import ClickHouseCluster\n+\n+test_recover_staled_replica_run = 1\n+\n+cluster = ClickHouseCluster(__file__)\n+\n+node1 = cluster.add_instance(\n+    \"node1\",\n+    main_configs=[\n+        \"configs/config.xml\",\n+    ],\n+    user_configs=[\"configs/settings.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+    macros={\"shard\": \"s1\", \"replica\": \"r1\"},\n+    with_minio=True,\n+)\n+\n+node2 = cluster.add_instance(\n+    \"node2\",\n+    main_configs=[\n+        \"configs/config.xml\",\n+    ],\n+    user_configs=[\"configs/settings.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+    macros={\"shard\": \"s1\", \"replica\": \"r2\"},\n+    with_minio=True,\n+)\n+\n+node3 = cluster.add_instance(\n+    \"node3\",\n+    main_configs=[\n+        \"configs/config.xml\",\n+    ],\n+    user_configs=[\"configs/settings.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+    macros={\"shard\": \"s2\", \"replica\": \"r1\"},\n+    with_minio=True,\n+)\n+node4 = cluster.add_instance(\n+    \"node4\",\n+    main_configs=[\n+        \"configs/config.xml\",\n+    ],\n+    user_configs=[\"configs/settings.xml\"],\n+    with_zookeeper=True,\n+    stay_alive=True,\n+    macros={\"shard\": \"s2\", \"replica\": \"r2\"},\n+    with_minio=True,\n+)\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def started_cluster():\n+    try:\n+        cluster.start()\n+\n+        yield cluster\n+\n+    finally:\n+        cluster.shutdown()\n+\n+\n+def test_drop_database_replica(started_cluster):\n+    node1.query(\"DROP DATABASE IF EXISTS db\")\n+    node2.query(\"DROP DATABASE IF EXISTS db\")\n+    node3.query(\"DROP DATABASE IF EXISTS db\")\n+    node4.query(\"DROP DATABASE IF EXISTS db\")\n+\n+    zk_path = \"/test/db\"\n+    node1.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+\n+    node1.query(f\"CREATE TABLE db.t (x INT) ENGINE=MergeTree ORDER BY x\")\n+\n+    assert \"SYNTAX_ERROR\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM TABLE t\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM DATABASE db\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 'r1' FROM SHARD 's1' FROM DATABASE db\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM ZKPATH '{zk_path}'\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 'r1' FROM SHARD 's1' FROM ZKPATH '{zk_path}'\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM ZKPATH '{zk_path}/'\"\n+    )\n+    assert \"There is a local database\" in node1.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 'r1' FROM SHARD 's1' FROM ZKPATH '{zk_path}/'\"\n+    )\n+\n+    node2.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+    node3.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+\n+    node2.query(f\"SYSTEM SYNC DATABASE REPLICA db\")\n+    node3.query(f\"SYSTEM SYNC DATABASE REPLICA db\")\n+\n+    assert \"is active, cannot drop it\" in node2.query_and_get_error(\n+        f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM DATABASE db\"\n+    )\n+\n+    node1.query(f\"CREATE TABLE db.t2 (x INT) ENGINE=Log\")\n+    node1.query(f\"CREATE TABLE db.t3 (x INT) ENGINE=Log\")\n+    node1.query(f\"CREATE TABLE db.t4 (x INT) ENGINE=Log\")\n+\n+    node4_uuid = node4.query(\"SELECT serverUUID()\").strip()\n+    node4.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+    node4.query(\"DETACH DATABASE db\")\n+    node4.query(\n+        f\"INSERT INTO system.zookeeper(name, path, value) VALUES ('active', '{zk_path}/replicas/s2|r2', '{node4_uuid}')\",\n+    )\n+\n+    assert \"TIMEOUT_EXCEEDED\" in node1.query_and_get_error(\n+        \"CREATE TABLE db.t22 (n int) ENGINE=Log\",\n+        settings={\n+            \"distributed_ddl_task_timeout\": 5,\n+            \"distributed_ddl_output_mode\": \"none_only_active\",\n+        },\n+    )\n+    assert \"TIMEOUT_EXCEEDED\" in node1.query_and_get_error(\n+        \"CREATE TABLE db.t33 (n int) ENGINE=Log\",\n+        settings={\n+            \"distributed_ddl_task_timeout\": 5,\n+            \"distributed_ddl_output_mode\": \"throw_only_active\",\n+        },\n+    )\n+    node1.query(\n+        \"CREATE TABLE db.t44 (n int) ENGINE=Log\",\n+        settings={\n+            \"distributed_ddl_task_timeout\": 5,\n+            \"distributed_ddl_output_mode\": \"null_status_on_timeout_only_active\",\n+        },\n+    )\n+    node4.query(\"ATTACH DATABASE db\")\n+\n+    node3.query(\"DETACH DATABASE db\")\n+    node4.query(\"SYSTEM DROP DATABASE replica 'r1' FROM SHARD 's2' FROM DATABASE db\")\n+    node3.query(\"ATTACH DATABASE db\")\n+    assert \"Database is in readonly mode\" in node3.query_and_get_error(\n+        \"CREATE TABLE db.t55 (n int) ENGINE=MergeTree\",\n+        settings={\n+            \"distributed_ddl_output_mode\": \"none\",\n+        },\n+    )\n+    node4.query(\"DROP DATABASE db SYNC\")\n+\n+    node2.query(\"DETACH DATABASE db\")\n+    node1.query(\"SYSTEM DROP DATABASE REPLICA 's1|r2' FROM DATABASE db\")\n+    node2.query(\"ATTACH DATABASE db\")\n+    assert \"Database is in readonly mode\" in node2.query_and_get_error(\n+        \"CREATE TABLE db.t55 (n int) ENGINE=MergeTree\",\n+        settings={\n+            \"distributed_ddl_output_mode\": \"none\",\n+        },\n+    )\n+\n+    node1.query(\"DETACH DATABASE db\")\n+    node4.query(f\"SYSTEM DROP DATABASE REPLICA 's1|r1' FROM ZKPATH '{zk_path}'\")\n+    node1.query(\"ATTACH DATABASE db\")\n+    assert \"Database is in readonly mode\" in node1.query_and_get_error(\n+        \"CREATE TABLE db.t55 (n int) ENGINE=MergeTree\",\n+        settings={\n+            \"distributed_ddl_output_mode\": \"none\",\n+        },\n+    )\n+\n+    node1.query(f\"SYSTEM DROP DATABASE REPLICA 'dummy' FROM SHARD 'dummy'\")\n+\n+    node1.query(\"DROP DATABASE db SYNC\")\n+    node2.query(\"DROP DATABASE db SYNC\")\n+    node3.query(\"DROP DATABASE db SYNC\")\n+\n+    node4.query(\n+        f\"CREATE DATABASE db ENGINE = Replicated('{zk_path}'\"\n+        + r\", '{shard}', '{replica}')\"\n+    )\n+\n+    node4.query(\n+        \"CREATE TABLE db.rmt (x INT) ENGINE=ReplicatedMergeTree ORDER BY x\",",
        "comment_created_at": "2025-08-19T03:01:08+00:00",
        "comment_author": "evillique",
        "comment_body": "I would also check whether `SYSTEM DROP REPLICA` would remove the ZK nodes of the replica and the table.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2282411786",
    "pr_number": 85395,
    "pr_file": "tests/integration/test_storage_iceberg/test.py",
    "created_at": "2025-08-18T13:33:46+00:00",
    "commented_code": "df = spark.read.format(\"iceberg\").load(f\"/iceberg_data/default/{TABLE_NAME}\").collect()\n     assert len(df) == 1\n+\n+\n+@pytest.mark.parametrize(\"format_version\", [1, 2])\n+@pytest.mark.parametrize(\"storage_type\", [\"local\"])\n+def test_writes_drop_table(started_cluster, format_version, storage_type):\n+    instance = started_cluster.instances[\"node1\"]\n+    spark = started_cluster.spark_session\n+    TABLE_NAME = \"test_bucket_partition_pruning_\" + storage_type + \"_\" + get_uuid_str()\n+\n+    create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, \"(x String, y Int64)\", format_version, use_version_hint=True)\n+\n+    instance.query(f\"INSERT INTO {TABLE_NAME} VALUES ('123', 1);\", settings={\"allow_experimental_insert_into_iceberg\": 1})\n+    assert instance.query(f\"SELECT * FROM {TABLE_NAME} ORDER BY ALL\", ) == '123\\t1\n'\n+\n+    drop_iceberg_table(instance, TABLE_NAME)\n+    with pytest.raises(Exception):\n+        drop_iceberg_table(instance, TABLE_NAME)\n+    drop_iceberg_table(instance, TABLE_NAME, True)\n+\n+    files = default_download_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+    )\n+    # drop should not delete user data\n+    assert len(files) > 0",
    "repo_full_name": "ClickHouse/ClickHouse",
    "discussion_comments": [
      {
        "comment_id": "2282411786",
        "repo_full_name": "ClickHouse/ClickHouse",
        "pr_number": 85395,
        "pr_file": "tests/integration/test_storage_iceberg/test.py",
        "discussion_id": "2282411786",
        "commented_code": "@@ -2555,3 +2556,30 @@ def test_writes_create_version_hint(started_cluster, format_version, storage_typ\n \n     df = spark.read.format(\"iceberg\").load(f\"/iceberg_data/default/{TABLE_NAME}\").collect()\n     assert len(df) == 1\n+\n+\n+@pytest.mark.parametrize(\"format_version\", [1, 2])\n+@pytest.mark.parametrize(\"storage_type\", [\"local\"])\n+def test_writes_drop_table(started_cluster, format_version, storage_type):\n+    instance = started_cluster.instances[\"node1\"]\n+    spark = started_cluster.spark_session\n+    TABLE_NAME = \"test_bucket_partition_pruning_\" + storage_type + \"_\" + get_uuid_str()\n+\n+    create_iceberg_table(storage_type, instance, TABLE_NAME, started_cluster, \"(x String, y Int64)\", format_version, use_version_hint=True)\n+\n+    instance.query(f\"INSERT INTO {TABLE_NAME} VALUES ('123', 1);\", settings={\"allow_experimental_insert_into_iceberg\": 1})\n+    assert instance.query(f\"SELECT * FROM {TABLE_NAME} ORDER BY ALL\", ) == '123\\t1\\n'\n+\n+    drop_iceberg_table(instance, TABLE_NAME)\n+    with pytest.raises(Exception):\n+        drop_iceberg_table(instance, TABLE_NAME)\n+    drop_iceberg_table(instance, TABLE_NAME, True)\n+\n+    files = default_download_directory(\n+        started_cluster,\n+        storage_type,\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+        f\"/iceberg_data/default/{TABLE_NAME}/\",\n+    )\n+    # drop should not delete user data\n+    assert len(files) > 0",
        "comment_created_at": "2025-08-18T13:33:46+00:00",
        "comment_author": "fm4v",
        "comment_body": "Should it drop the data or not on a catalog side? \nPlease add a check after this statement which asserts that data still exists and readable",
        "pr_file_module": null
      }
    ]
  }
]