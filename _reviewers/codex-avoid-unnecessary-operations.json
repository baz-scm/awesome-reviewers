[
  {
    "discussion_id": "2221396859",
    "pr_number": 1647,
    "pr_file": "codex-rs/core/src/rollout.rs",
    "created_at": "2025-07-22T06:48:47+00:00",
    "commented_code": "let _ = file.flush().await;\n                 }\n             }\n+            RolloutCmd::Sync { exit, ack } => {\n+                if let Err(e) = file.flush().await {",
    "repo_full_name": "openai/codex",
    "discussion_comments": [
      {
        "comment_id": "2221396859",
        "repo_full_name": "openai/codex",
        "pr_number": 1647,
        "pr_file": "codex-rs/core/src/rollout.rs",
        "discussion_id": "2221396859",
        "commented_code": "@@ -294,6 +335,15 @@ async fn rollout_writer(\n                     let _ = file.flush().await;\n                 }\n             }\n+            RolloutCmd::Sync { exit, ack } => {\n+                if let Err(e) = file.flush().await {",
        "comment_created_at": "2025-07-22T06:48:47+00:00",
        "comment_author": "bolinfest",
        "comment_body": "Since the other two cases always `flush()`, there should never be anything new to `flush()` in this case, no?",
        "pr_file_module": null
      },
      {
        "comment_id": "2223454350",
        "repo_full_name": "openai/codex",
        "pr_number": 1647,
        "pr_file": "codex-rs/core/src/rollout.rs",
        "discussion_id": "2221396859",
        "commented_code": "@@ -294,6 +335,15 @@ async fn rollout_writer(\n                     let _ = file.flush().await;\n                 }\n             }\n+            RolloutCmd::Sync { exit, ack } => {\n+                if let Err(e) = file.flush().await {",
        "comment_created_at": "2025-07-22T18:22:17+00:00",
        "comment_author": "aibrahim-oai",
        "comment_body": "supposedly ",
        "pr_file_module": null
      },
      {
        "comment_id": "2223975605",
        "repo_full_name": "openai/codex",
        "pr_number": 1647,
        "pr_file": "codex-rs/core/src/rollout.rs",
        "discussion_id": "2221396859",
        "commented_code": "@@ -294,6 +335,15 @@ async fn rollout_writer(\n                     let _ = file.flush().await;\n                 }\n             }\n+            RolloutCmd::Sync { exit, ack } => {\n+                if let Err(e) = file.flush().await {",
        "comment_created_at": "2025-07-22T22:54:11+00:00",
        "comment_author": "bolinfest",
        "comment_body": "I would skip the `flush()`: all this needs to do is `ack.send(())`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2211798072",
    "pr_number": 1594,
    "pr_file": "codex-rs/tui/src/chatwidget.rs",
    "created_at": "2025-07-16T23:16:02+00:00",
    "commented_code": "}\n \n     fn request_redraw(&mut self) {\n-        self.app_event_tx.send(AppEvent::Redraw);\n+        if Instant::now().duration_since(self.last_redraw_time) > Duration::from_millis(100) {",
    "repo_full_name": "openai/codex",
    "discussion_comments": [
      {
        "comment_id": "2211798072",
        "repo_full_name": "openai/codex",
        "pr_number": 1594,
        "pr_file": "codex-rs/tui/src/chatwidget.rs",
        "discussion_id": "2211798072",
        "commented_code": "@@ -398,7 +426,10 @@ impl ChatWidget<'_> {\n     }\n \n     fn request_redraw(&mut self) {\n-        self.app_event_tx.send(AppEvent::Redraw);\n+        if Instant::now().duration_since(self.last_redraw_time) > Duration::from_millis(100) {",
        "comment_created_at": "2025-07-16T23:16:02+00:00",
        "comment_author": "bolinfest",
        "comment_body": "If we are going to ad logic to control the render late, we should be doing this at a higher level in the TUI so it applies globally, no?",
        "pr_file_module": null
      },
      {
        "comment_id": "2211939965",
        "repo_full_name": "openai/codex",
        "pr_number": 1594,
        "pr_file": "codex-rs/tui/src/chatwidget.rs",
        "discussion_id": "2211798072",
        "commented_code": "@@ -398,7 +426,10 @@ impl ChatWidget<'_> {\n     }\n \n     fn request_redraw(&mut self) {\n-        self.app_event_tx.send(AppEvent::Redraw);\n+        if Instant::now().duration_since(self.last_redraw_time) > Duration::from_millis(100) {",
        "comment_created_at": "2025-07-17T01:21:09+00:00",
        "comment_author": "aibrahim-oai",
        "comment_body": "where exactly? the `self.last_redraw_time` should be updated for the `ChatWidget` in every render. We can remove it. I just added it in case it throttles someone's laptop.",
        "pr_file_module": null
      },
      {
        "comment_id": "2212065334",
        "repo_full_name": "openai/codex",
        "pr_number": 1594,
        "pr_file": "codex-rs/tui/src/chatwidget.rs",
        "discussion_id": "2211798072",
        "commented_code": "@@ -398,7 +426,10 @@ impl ChatWidget<'_> {\n     }\n \n     fn request_redraw(&mut self) {\n-        self.app_event_tx.send(AppEvent::Redraw);\n+        if Instant::now().duration_since(self.last_redraw_time) > Duration::from_millis(100) {",
        "comment_created_at": "2025-07-17T03:23:39+00:00",
        "comment_author": "bolinfest",
        "comment_body": "Right, but in general, it's possible the top-level event loop gets too many `AppEvent::Redraw` requests, so why not do the throttling there?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2167671770",
    "pr_number": 1387,
    "pr_file": "codex-rs/core/src/codex.rs",
    "created_at": "2025-06-25T21:31:53+00:00",
    "commented_code": "sub_id: &str,\n     prompt: &Prompt,\n ) -> CodexResult<Vec<ProcessedResponseItem>> {\n-    let mut stream = sess.client.clone().stream(prompt).await?;\n+    // call_ids that are part of this response.\n+    let completed_call_ids = prompt\n+        .input\n+        .iter()\n+        .filter_map(|ri| match ri {\n+            ResponseItem::FunctionCallOutput { call_id, .. } => Some(call_id),\n+            ResponseItem::LocalShellCall {\n+                call_id: Some(call_id),\n+                ..\n+            } => Some(call_id),\n+            _ => None,\n+        })\n+        .collect::<Vec<_>>();\n+\n+    // call_ids that were pending but are not part of this response.\n+    // This usually happens because the user interrupted the model before we responded to one of its tool calls\n+    // and then the user sent a follow-up message.\n+    let missing_calls = {\n+        sess.state\n+            .lock()\n+            .unwrap()\n+            .pending_call_ids\n+            .iter()\n+            .filter_map(|call_id| {\n+                if completed_call_ids.contains(&call_id) {\n+                    None\n+                } else {\n+                    Some(call_id.clone())\n+                }\n+            })\n+            .map(|call_id| ResponseItem::FunctionCallOutput {\n+                call_id: call_id.clone(),\n+                output: FunctionCallOutputPayload {\n+                    content: \"aborted\".to_string(),\n+                    success: Some(false),\n+                },\n+            })\n+            .collect::<Vec<_>>()\n+    };\n+    let prompt = if missing_calls.is_empty() {\n+        prompt.clone()",
    "repo_full_name": "openai/codex",
    "discussion_comments": [
      {
        "comment_id": "2167671770",
        "repo_full_name": "openai/codex",
        "pr_number": 1387,
        "pr_file": "codex-rs/core/src/codex.rs",
        "discussion_id": "2167671770",
        "commented_code": "@@ -1062,7 +1068,59 @@ async fn try_run_turn(\n     sub_id: &str,\n     prompt: &Prompt,\n ) -> CodexResult<Vec<ProcessedResponseItem>> {\n-    let mut stream = sess.client.clone().stream(prompt).await?;\n+    // call_ids that are part of this response.\n+    let completed_call_ids = prompt\n+        .input\n+        .iter()\n+        .filter_map(|ri| match ri {\n+            ResponseItem::FunctionCallOutput { call_id, .. } => Some(call_id),\n+            ResponseItem::LocalShellCall {\n+                call_id: Some(call_id),\n+                ..\n+            } => Some(call_id),\n+            _ => None,\n+        })\n+        .collect::<Vec<_>>();\n+\n+    // call_ids that were pending but are not part of this response.\n+    // This usually happens because the user interrupted the model before we responded to one of its tool calls\n+    // and then the user sent a follow-up message.\n+    let missing_calls = {\n+        sess.state\n+            .lock()\n+            .unwrap()\n+            .pending_call_ids\n+            .iter()\n+            .filter_map(|call_id| {\n+                if completed_call_ids.contains(&call_id) {\n+                    None\n+                } else {\n+                    Some(call_id.clone())\n+                }\n+            })\n+            .map(|call_id| ResponseItem::FunctionCallOutput {\n+                call_id: call_id.clone(),\n+                output: FunctionCallOutputPayload {\n+                    content: \"aborted\".to_string(),\n+                    success: Some(false),\n+                },\n+            })\n+            .collect::<Vec<_>>()\n+    };\n+    let prompt = if missing_calls.is_empty() {\n+        prompt.clone()",
        "comment_created_at": "2025-06-25T21:31:53+00:00",
        "comment_author": "bolinfest",
        "comment_body": "I think you want `let prompt: Cow<'a, Prompt>` if you can to avoid the `clone()`? So in the consequent, it's `Cow::Borrowed` and in the alternative, it's `Cow::Owned`?",
        "pr_file_module": null
      }
    ]
  }
]