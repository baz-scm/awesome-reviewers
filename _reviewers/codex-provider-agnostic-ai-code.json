[
  {
    "discussion_id": "2148829201",
    "pr_number": 1336,
    "pr_file": "codex-cli/src/utils/agent/agent-loop.ts",
    "created_at": "2025-06-15T21:16:50+00:00",
    "commented_code": "});\n     }\n \n+    if (this.provider.toLowerCase() === \"githubcopilot\") {",
    "repo_full_name": "openai/codex",
    "discussion_comments": [
      {
        "comment_id": "2148829201",
        "repo_full_name": "openai/codex",
        "pr_number": 1336,
        "pr_file": "codex-cli/src/utils/agent/agent-loop.ts",
        "discussion_id": "2148829201",
        "commented_code": "@@ -350,6 +351,24 @@ export class AgentLoop {\n       });\n     }\n \n+    if (this.provider.toLowerCase() === \"githubcopilot\") {",
        "comment_created_at": "2025-06-15T21:16:50+00:00",
        "comment_author": "Copilot",
        "comment_body": "This block re-instantiates `oai` for GitHub Copilot after it was already created by `createOpenAIClient`, leading to duplicated logic and potential configuration drift.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2054735223",
    "pr_number": 551,
    "pr_file": "codex-cli/src/utils/model-utils.ts",
    "created_at": "2025-04-22T19:32:54+00:00",
    "commented_code": "}\n \n /** Returns the maximum context length (in tokens) for a given model. */\n-function maxTokensForModel(model: string): number {\n-  // TODO: These numbers are best\u2011effort guesses and provide a basis for UI percentages. They\n-  // should be provider & model specific instead of being wild guesses.\n-\n-  const lower = model.toLowerCase();\n-  if (lower.includes(\"32k\")) {\n-    return 32000;\n-  }\n-  if (lower.includes(\"16k\")) {\n-    return 16000;\n-  }\n-  if (lower.includes(\"8k\")) {\n-    return 8000;\n-  }\n-  if (lower.includes(\"4k\")) {\n-    return 4000;\n-  }\n-  return 128000; // Default to 128k for any other model.\n+function maxTokensForModel(model: SupportedModelId): number {\n+  return openAiModelInfo[model].maxContextLength;",
    "repo_full_name": "openai/codex",
    "discussion_comments": [
      {
        "comment_id": "2054735223",
        "repo_full_name": "openai/codex",
        "pr_number": 551,
        "pr_file": "codex-cli/src/utils/model-utils.ts",
        "discussion_id": "2054735223",
        "commented_code": "@@ -89,24 +90,8 @@ export async function isModelSupportedForResponses(\n }\n \n /** Returns the maximum context length (in tokens) for a given model. */\n-function maxTokensForModel(model: string): number {\n-  // TODO: These numbers are best\u2011effort guesses and provide a basis for UI percentages. They\n-  // should be provider & model specific instead of being wild guesses.\n-\n-  const lower = model.toLowerCase();\n-  if (lower.includes(\"32k\")) {\n-    return 32000;\n-  }\n-  if (lower.includes(\"16k\")) {\n-    return 16000;\n-  }\n-  if (lower.includes(\"8k\")) {\n-    return 8000;\n-  }\n-  if (lower.includes(\"4k\")) {\n-    return 4000;\n-  }\n-  return 128000; // Default to 128k for any other model.\n+function maxTokensForModel(model: SupportedModelId): number {\n+  return openAiModelInfo[model].maxContextLength;",
        "comment_created_at": "2025-04-22T19:32:54+00:00",
        "comment_author": "tibo-openai",
        "comment_body": "Not all models will be openai models or in the explicit model registry that you added. Let's keep the existing logic as a fallback if it's not in the registry\r\n\r\n```\r\n  const lower = model.toLowerCase();\r\n  if (lower.includes(\"32k\")) {\r\n    return 32000;\r\n  }\r\n  if (lower.includes(\"16k\")) {\r\n    return 16000;\r\n  }\r\n  if (lower.includes(\"8k\")) {\r\n    return 8000;\r\n  }\r\n  if (lower.includes(\"4k\")) {\r\n    return 4000;\r\n  }\r\n  return 128000; // Default to 128k for any other model.\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2054748499",
        "repo_full_name": "openai/codex",
        "pr_number": 551,
        "pr_file": "codex-cli/src/utils/model-utils.ts",
        "discussion_id": "2054735223",
        "commented_code": "@@ -89,24 +90,8 @@ export async function isModelSupportedForResponses(\n }\n \n /** Returns the maximum context length (in tokens) for a given model. */\n-function maxTokensForModel(model: string): number {\n-  // TODO: These numbers are best\u2011effort guesses and provide a basis for UI percentages. They\n-  // should be provider & model specific instead of being wild guesses.\n-\n-  const lower = model.toLowerCase();\n-  if (lower.includes(\"32k\")) {\n-    return 32000;\n-  }\n-  if (lower.includes(\"16k\")) {\n-    return 16000;\n-  }\n-  if (lower.includes(\"8k\")) {\n-    return 8000;\n-  }\n-  if (lower.includes(\"4k\")) {\n-    return 4000;\n-  }\n-  return 128000; // Default to 128k for any other model.\n+function maxTokensForModel(model: SupportedModelId): number {\n+  return openAiModelInfo[model].maxContextLength;",
        "comment_created_at": "2025-04-22T19:42:37+00:00",
        "comment_author": "chunterb",
        "comment_body": "Good catch - I'll add the fallback.\r\n\r\nShall I go ahead and add a good base-set of models from other providers into the registry, or keep it to just OpenAI?\r\n\r\nEDIT: Providers like Openrouter and Groq might benefit more using API to get info. May be good to leave those out for now.",
        "pr_file_module": null
      },
      {
        "comment_id": "2054753971",
        "repo_full_name": "openai/codex",
        "pr_number": 551,
        "pr_file": "codex-cli/src/utils/model-utils.ts",
        "discussion_id": "2054735223",
        "commented_code": "@@ -89,24 +90,8 @@ export async function isModelSupportedForResponses(\n }\n \n /** Returns the maximum context length (in tokens) for a given model. */\n-function maxTokensForModel(model: string): number {\n-  // TODO: These numbers are best\u2011effort guesses and provide a basis for UI percentages. They\n-  // should be provider & model specific instead of being wild guesses.\n-\n-  const lower = model.toLowerCase();\n-  if (lower.includes(\"32k\")) {\n-    return 32000;\n-  }\n-  if (lower.includes(\"16k\")) {\n-    return 16000;\n-  }\n-  if (lower.includes(\"8k\")) {\n-    return 8000;\n-  }\n-  if (lower.includes(\"4k\")) {\n-    return 4000;\n-  }\n-  return 128000; // Default to 128k for any other model.\n+function maxTokensForModel(model: SupportedModelId): number {\n+  return openAiModelInfo[model].maxContextLength;",
        "comment_created_at": "2025-04-22T19:47:08+00:00",
        "comment_author": "tibo-openai",
        "comment_body": "Fine to keep it scoped to openai model registry in this PR, but don't want to regress.",
        "pr_file_module": null
      }
    ]
  }
]