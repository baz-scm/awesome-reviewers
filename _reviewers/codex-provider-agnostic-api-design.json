[
  {
    "discussion_id": "2214793461",
    "pr_number": 1601,
    "pr_file": "codex-rs/core/src/model_provider_info.rs",
    "created_at": "2025-07-18T03:25:59+00:00",
    "commented_code": "use serde::Serialize;\n use std::collections::HashMap;\n use std::env::VarError;\n+use std::time::Duration;\n \n use crate::error::EnvVarError;\n use crate::openai_api_key::get_openai_api_key;\n \n /// Value for the `OpenAI-Originator` header that is sent with requests to\n /// OpenAI.\n const OPENAI_ORIGINATOR_HEADER: &str = \"codex_cli_rs\";\n+const OPENAI_STREAM_IDLE_TIMEOUT_MS: u64 = 300_000;\n+const OPENAI_STREAM_MAX_RETRIES: u64 = 10;\n+const OPENAI_REQUEST_MAX_RETRIES: u64 = 4;",
    "repo_full_name": "openai/codex",
    "discussion_comments": [
      {
        "comment_id": "2214793461",
        "repo_full_name": "openai/codex",
        "pr_number": 1601,
        "pr_file": "codex-rs/core/src/model_provider_info.rs",
        "discussion_id": "2214793461",
        "commented_code": "@@ -9,13 +9,17 @@ use serde::Deserialize;\n use serde::Serialize;\n use std::collections::HashMap;\n use std::env::VarError;\n+use std::time::Duration;\n \n use crate::error::EnvVarError;\n use crate::openai_api_key::get_openai_api_key;\n \n /// Value for the `OpenAI-Originator` header that is sent with requests to\n /// OpenAI.\n const OPENAI_ORIGINATOR_HEADER: &str = \"codex_cli_rs\";\n+const OPENAI_STREAM_IDLE_TIMEOUT_MS: u64 = 300_000;\n+const OPENAI_STREAM_MAX_RETRIES: u64 = 10;\n+const OPENAI_REQUEST_MAX_RETRIES: u64 = 4;",
        "comment_created_at": "2025-07-18T03:25:59+00:00",
        "comment_author": "bolinfest",
        "comment_body": "I think it's more appropriate for these to be:\r\n\r\n```suggestion\r\nconst DEFAULT_STREAM_IDLE_TIMEOUT_MS: u64 = 300_000;\r\nconst DEFAULT_STREAM_MAX_RETRIES: u64 = 10;\r\nconst DEFAULT_REQUEST_MAX_RETRIES: u64 = 4;\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2196613542",
    "pr_number": 1497,
    "pr_file": "codex-rs/tui/src/compact.rs",
    "created_at": "2025-07-10T05:25:34+00:00",
    "commented_code": "+use anyhow::Result;\n+use anyhow::anyhow;\n+use codex_core::config::Config;\n+use codex_core::openai_api_key::get_openai_api_key;\n+use serde::Serialize;\n+\n+#[derive(Clone)]\n+pub enum Role {\n+    User,\n+    Assistant,\n+}\n+\n+#[derive(Clone)]\n+pub struct TranscriptEntry {\n+    pub role: Role,\n+    pub text: String,\n+}\n+\n+impl TranscriptEntry {\n+    fn role_str(&self) -> &'static str {\n+        match self.role {\n+            Role::User => \"user\",\n+            Role::Assistant => \"assistant\",\n+        }\n+    }\n+}\n+\n+#[derive(Serialize)]\n+struct Message<'a> {\n+    role: &'a str,\n+    content: String,\n+}\n+\n+#[derive(Serialize)]\n+struct Payload<'a> {\n+    model: &'a str,\n+    messages: Vec<Message<'a>>,\n+}\n+\n+/// Generate a concise summary of the provided transcript using the OpenAI chat\n+/// completions API.\n+pub async fn generate_compact_summary(\n+    transcript: &[TranscriptEntry],\n+    model: &str,\n+    config: &Config,\n+) -> Result<String> {\n+    let conversation_text = transcript\n+        .iter()\n+        .map(|e| format!(\"{}: {}\", e.role_str(), e.text))\n+        .collect::<Vec<_>>()\n+        .join(\"\n\");\n+\n+    let messages = vec![\n+        Message {\n+            role: \"assistant\",\n+            content: \"You are an expert coding assistant. Your goal is to generate a concise, structured summary of the conversation below that captures all essential information needed to continue development after context replacement. Include tasks performed, code areas modified or reviewed, key decisions or assumptions, test results or errors, and outstanding tasks or next steps.\".to_string(),\n+        },\n+        Message {\n+            role: \"user\",\n+            content: format!(\n+                \"Here is the conversation so far:\n{conversation_text}\n\nPlease summarize this conversation, covering:\n1. Tasks performed and outcomes\n2. Code files, modules, or functions modified or examined\n3. Important decisions or assumptions made\n4. Errors encountered and test or build results\n5. Remaining tasks, open questions, or next steps\nProvide the summary in a clear, concise format.\"\n+            ),\n+        },\n+    ];\n+\n+    let api_key = get_openai_api_key().ok_or_else(|| anyhow!(\"OpenAI API key not set\"))?;\n+    let client = reqwest::Client::new();\n+    let base = config.model_provider.base_url.trim_end_matches('/');\n+    let url = format!(\"{}/chat/completions\", base);\n+\n+    let payload = Payload { model, messages };\n+    let res = client\n+        .post(url)\n+        .bearer_auth(api_key)\n+        .json(&payload)\n+        .send()\n+        .await?;\n+\n+    let body: serde_json::Value = res.json().await?;\n+    if let Some(summary) = body\n+        .get(\"choices\")\n+        .and_then(|c| c.get(0))\n+        .and_then(|c| c.get(\"message\"))\n+        .and_then(|m| m.get(\"content\"))\n+        .and_then(|v| v.as_str())\n+    {\n+        Ok(summary.to_string())\n+    } else {\n+        Ok(\"Unable to generate summary.\".to_string())\n+    }\n+}",
    "repo_full_name": "openai/codex",
    "discussion_comments": [
      {
        "comment_id": "2196613542",
        "repo_full_name": "openai/codex",
        "pr_number": 1497,
        "pr_file": "codex-rs/tui/src/compact.rs",
        "discussion_id": "2196613542",
        "commented_code": "@@ -0,0 +1,91 @@\n+use anyhow::Result;\n+use anyhow::anyhow;\n+use codex_core::config::Config;\n+use codex_core::openai_api_key::get_openai_api_key;\n+use serde::Serialize;\n+\n+#[derive(Clone)]\n+pub enum Role {\n+    User,\n+    Assistant,\n+}\n+\n+#[derive(Clone)]\n+pub struct TranscriptEntry {\n+    pub role: Role,\n+    pub text: String,\n+}\n+\n+impl TranscriptEntry {\n+    fn role_str(&self) -> &'static str {\n+        match self.role {\n+            Role::User => \"user\",\n+            Role::Assistant => \"assistant\",\n+        }\n+    }\n+}\n+\n+#[derive(Serialize)]\n+struct Message<'a> {\n+    role: &'a str,\n+    content: String,\n+}\n+\n+#[derive(Serialize)]\n+struct Payload<'a> {\n+    model: &'a str,\n+    messages: Vec<Message<'a>>,\n+}\n+\n+/// Generate a concise summary of the provided transcript using the OpenAI chat\n+/// completions API.\n+pub async fn generate_compact_summary(\n+    transcript: &[TranscriptEntry],\n+    model: &str,\n+    config: &Config,\n+) -> Result<String> {\n+    let conversation_text = transcript\n+        .iter()\n+        .map(|e| format!(\"{}: {}\", e.role_str(), e.text))\n+        .collect::<Vec<_>>()\n+        .join(\"\\n\");\n+\n+    let messages = vec![\n+        Message {\n+            role: \"assistant\",\n+            content: \"You are an expert coding assistant. Your goal is to generate a concise, structured summary of the conversation below that captures all essential information needed to continue development after context replacement. Include tasks performed, code areas modified or reviewed, key decisions or assumptions, test results or errors, and outstanding tasks or next steps.\".to_string(),\n+        },\n+        Message {\n+            role: \"user\",\n+            content: format!(\n+                \"Here is the conversation so far:\\n{conversation_text}\\n\\nPlease summarize this conversation, covering:\\n1. Tasks performed and outcomes\\n2. Code files, modules, or functions modified or examined\\n3. Important decisions or assumptions made\\n4. Errors encountered and test or build results\\n5. Remaining tasks, open questions, or next steps\\nProvide the summary in a clear, concise format.\"\n+            ),\n+        },\n+    ];\n+\n+    let api_key = get_openai_api_key().ok_or_else(|| anyhow!(\"OpenAI API key not set\"))?;\n+    let client = reqwest::Client::new();\n+    let base = config.model_provider.base_url.trim_end_matches('/');\n+    let url = format!(\"{}/chat/completions\", base);\n+\n+    let payload = Payload { model, messages };\n+    let res = client\n+        .post(url)\n+        .bearer_auth(api_key)\n+        .json(&payload)\n+        .send()\n+        .await?;\n+\n+    let body: serde_json::Value = res.json().await?;\n+    if let Some(summary) = body\n+        .get(\"choices\")\n+        .and_then(|c| c.get(0))\n+        .and_then(|c| c.get(\"message\"))\n+        .and_then(|m| m.get(\"content\"))\n+        .and_then(|v| v.as_str())\n+    {\n+        Ok(summary.to_string())\n+    } else {\n+        Ok(\"Unable to generate summary.\".to_string())\n+    }\n+}",
        "comment_created_at": "2025-07-10T05:25:34+00:00",
        "comment_author": "bolinfest",
        "comment_body": "Some of this code duplicates code in `core/`. Note that the model provider might only support one of Responses or the chat completions API, so it is not safe to assume the above code is an option.\r\n\r\nThat said, I recently learned that there is a _non-stateful version of the Responses API_ where  you pass input as the array of all messages the same way you do for chat completions. So perhaps we could expose a function in `core/` that takes a `Config` and a list of messages like you have here and uses the `config.model_provider` and a `Client` to make the request?\r\n\r\nAdmittedly this is complex enough that it is probably appropriate to do it in a separate PR.",
        "pr_file_module": null
      }
    ]
  }
]