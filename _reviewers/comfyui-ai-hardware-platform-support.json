[
  {
    "discussion_id": "2157834371",
    "pr_number": 8585,
    "pr_file": "README.md",
    "created_at": "2025-06-20T00:51:42+00:00",
    "commented_code": "2. Next, install the PyTorch(torch_mlu) following the instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html)\n 3. Launch ComfyUI by running `python main.py`\n \n+#### Iluvatar Corex\n+\n+For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:\n+\n+1. Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the [Installation](https://support.iluvatar.com/#/DocumentCentre?id=1&nameCenter=2&productId=520117912052801536)\n+2. Launch ComfyUI by running `python main.py --disable-cuda-malloc`",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "2157834371",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8585,
        "pr_file": "README.md",
        "discussion_id": "2157834371",
        "commented_code": "@@ -292,6 +292,13 @@ For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a\n 2. Next, install the PyTorch(torch_mlu) following the instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html)\n 3. Launch ComfyUI by running `python main.py`\n \n+#### Iluvatar Corex\n+\n+For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:\n+\n+1. Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the [Installation](https://support.iluvatar.com/#/DocumentCentre?id=1&nameCenter=2&productId=520117912052801536)\n+2. Launch ComfyUI by running `python main.py --disable-cuda-malloc`",
        "comment_created_at": "2025-06-20T00:51:42+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "why do you need to --disable-cuda-malloc?",
        "pr_file_module": null
      },
      {
        "comment_id": "2160577013",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8585,
        "pr_file": "README.md",
        "discussion_id": "2157834371",
        "commented_code": "@@ -292,6 +292,13 @@ For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a\n 2. Next, install the PyTorch(torch_mlu) following the instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html)\n 3. Launch ComfyUI by running `python main.py`\n \n+#### Iluvatar Corex\n+\n+For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:\n+\n+1. Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the [Installation](https://support.iluvatar.com/#/DocumentCentre?id=1&nameCenter=2&productId=520117912052801536)\n+2. Launch ComfyUI by running `python main.py --disable-cuda-malloc`",
        "comment_created_at": "2025-06-23T01:33:40+00:00",
        "comment_author": "honglyua-il",
        "comment_body": "Because of not support cudaMallocAsync on Iluvatar GPU. I have updated the code and add check in cuda_malloc_supported, PTAL.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1968692112",
    "pr_number": 1469,
    "pr_file": "README.md",
    "created_at": "2025-02-25T01:52:15+00:00",
    "commented_code": "After this you should have everything installed and can proceed to running ComfyUI.\n \n+\n+## Docker\n+\n+There are prebuilt docker images for AMD and NVIDIA GPUs on [GitHub Packages](https://ghcr.io/comfyanonymous/comfyui).\n+\n+You can pull them to your local docker registry with:\n+\n+```shell\n+# For NVIDIA GPUs\n+docker pull ghcr.io/comfyanonymous/comfyui:latest-cu124\n+# For AMD GPUs\n+docker pull ghcr.io/comfyanonymous/comfyui:latest-rocm6.2\n+\n+# For CPU only\n+docker pull ghcr.io/comfyanonymous/comfyui:latest-cpu\n+```\n+\n+### Building images manually\n+\n+You can build a docker image with the `Dockerfile` in this repo.\n+Specify, `PYTORCH_INSTALL_ARGS` build arg with one of the PyTorch commands above to build for AMD or NVIDIA GPUs.\n+```shell\n+docker build --build-arg PYTORCH_INSTALL_ARGS=\"--index-url https://download.pytorch.org/whl/cu122\" .\n+```\n+```shell\n+docker build --build-arg PYTORCH_INSTALL_ARGS=\"--index-url https://download.pytorch.org/whl/rocm6.2\" .\n+```\n+The `Dockerfile` requires BuildKit to be enabled. If your Docker does not support the buildx command, you can\n+enable BuildKit by setting the `DOCKER_BUILDKIT` environment variable.\n+```shell\n+DOCKER_BUILDKIT=1 docker build --build-arg PYTORCH_INSTALL_ARGS=\"--index-url https://download.pytorch.org/whl/cu124\" .\n+```\n+> [!NOTE]\n+> For building the CPU-only image, it is recommended that you add the --cpu flag to the EXTRA_ARGS build arg:\n+>\n+> ```shell\n+> docker build --build-arg PYTORCH_INSTALL_ARGS=\"--index-url https://download.pytorch.org/whl/cpu\" --build-arg EXTRA_ARGS=--cpu .\n+> ```\n+",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1968692112",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 1469,
        "pr_file": "README.md",
        "discussion_id": "1968692112",
        "commented_code": "@@ -207,6 +207,45 @@ Install the dependencies by opening your terminal inside the ComfyUI folder and:\n \n After this you should have everything installed and can proceed to running ComfyUI.\n \n+\n+## Docker\n+\n+There are prebuilt docker images for AMD and NVIDIA GPUs on [GitHub Packages](https://ghcr.io/comfyanonymous/comfyui).\n+\n+You can pull them to your local docker registry with:\n+\n+```shell\n+# For NVIDIA GPUs\n+docker pull ghcr.io/comfyanonymous/comfyui:latest-cu124\n+# For AMD GPUs\n+docker pull ghcr.io/comfyanonymous/comfyui:latest-rocm6.2\n+\n+# For CPU only\n+docker pull ghcr.io/comfyanonymous/comfyui:latest-cpu\n+```\n+\n+### Building images manually\n+\n+You can build a docker image with the `Dockerfile` in this repo.\n+Specify, `PYTORCH_INSTALL_ARGS` build arg with one of the PyTorch commands above to build for AMD or NVIDIA GPUs.\n+```shell\n+docker build --build-arg PYTORCH_INSTALL_ARGS=\"--index-url https://download.pytorch.org/whl/cu122\" .\n+```\n+```shell\n+docker build --build-arg PYTORCH_INSTALL_ARGS=\"--index-url https://download.pytorch.org/whl/rocm6.2\" .\n+```\n+The `Dockerfile` requires BuildKit to be enabled. If your Docker does not support the buildx command, you can\n+enable BuildKit by setting the `DOCKER_BUILDKIT` environment variable.\n+```shell\n+DOCKER_BUILDKIT=1 docker build --build-arg PYTORCH_INSTALL_ARGS=\"--index-url https://download.pytorch.org/whl/cu124\" .\n+```\n+> [!NOTE]\n+> For building the CPU-only image, it is recommended that you add the --cpu flag to the EXTRA_ARGS build arg:\n+>\n+> ```shell\n+> docker build --build-arg PYTORCH_INSTALL_ARGS=\"--index-url https://download.pytorch.org/whl/cpu\" --build-arg EXTRA_ARGS=--cpu .\n+> ```\n+",
        "comment_created_at": "2025-02-25T01:52:15+00:00",
        "comment_author": "polarathene",
        "comment_body": "Is there any case where you would specify something other than the `--index url` here? Otherwise it seems you only need to change that suffix.\r\n\r\nWith my suggested `Dockerfile` revision, you'd instead just provide (_I've included `--tag` as it's better to have an actual image tag to reference the built image_):\r\n\r\n```bash\r\ndocker build --tag localhost/comfy-ui:cuda --build-arg VARIANT=cu122 .\r\n```\r\n\r\nMuch simpler? I left the `PYTORCH_INSTALL_ARGS` as a configurable `ARG` in case there is a valid reason for adjusting that, but it should simplify the instructions here, along with CI.\r\n\r\nI'd personally drop the `DOCKER_BUILDKIT` mention, that implies the user is building with a version of Docker prior to v23 (Released in [Feb 2023](https://docs.docker.com/engine/release-notes/23.0/#2300)). You could instead be more explicit about minimum required Docker, I doubt the maintainers here are interested in supporting users on old versions of Docker, let's keep it simple for them \ud83d\udc4d (_for reference, a project I maintain has [this section](https://docker-mailserver.github.io/docker-mailserver/v14.0/examples/tutorials/docker-build/#minimum-supported-version) that briefly mentions `DOCKER_BUILDKIT`_)\r\n\r\nYou can also drop the CPU note, since my revision handles that implicitly instead via `VARIANT`.\r\n\r\n````suggestion\r\n### Building images manually\r\n\r\nThis repo provides a `Dockerfile` that is intended to be built with Docker v23 (Feb 2023) or newer.\r\n\r\nThe build defaults to the `cpu` variant. To build with support for AMD or NVIDIA GPUs you should instead add the build arg `VARIANT` with a [PyTorch supported CUDA or ROCm version](https://pytorch.org/get-started/locally/).\r\n\r\n```bash\r\n# Setting `VARIANT` affects the package index used for PyTorch:\r\n# `--index-url https://download.pytorch.org/whl/<VARIANT>`\r\n\r\n# NVIDIA => CUDA 12.6:\r\ndocker build --tag localhost/comfy-ui:cuda --build-arg VARIANT=cu126 .\r\n\r\n# AMD => ROCm 6.2.4:\r\ndocker build --tag localhost/comfy-ui:rocm --build-arg VARIANT=rocm6.2.4 .\r\n```\r\n````",
        "pr_file_module": null
      }
    ]
  }
]