[
  {
    "discussion_id": "2216978086",
    "pr_number": 8890,
    "pr_file": "execution.py",
    "created_at": "2025-07-18T22:03:13+00:00",
    "commented_code": "else:\n                 return {}\n \n+    def get_ordered_history(self, max_items=None, offset=0):\n+        \"\"\"\n+        Retrieves execution history in chronological order with pagination support.\n+        Returns a lightweight list of history objects.\n+        Used by the /history_v2.\n+        \n+        API Output Structure:\n+        {\n+            \"history\": [\n+                {\n+                    \"prompt_id\": str,          # Unique identifier for this execution\n+                    \"outputs\": dict,           # Node outputs {node_id: ui_data}\n+                    \"meta\": dict,              # Node metadata {node_id: {node_id, display_node, parent_node, real_node_id}}\n+                    \"prompt\": {\n+                        \"priority\": int,       # Execution priority\n+                        \"prompt_id\": str,      # Same as root prompt_id\n+                        \"extra_data\": dict     # Additional metadata (workflow removed from extra_pnginfo)\n+                    } | None,                  # None if no prompt data available\n+                    \"status\": {\n+                        \"status_str\": str,     # \"success\" | \"error\" \n+                        \"messages\": [          # Filtered execution event messages\n+                            (event_name: str, event_data: dict)\n+                        ]\n+                    } | None                   # None if no status recorded\n+                },\n+                # ... more history items\n+            ]\n+        }\n+        \n+        Parameters:\n+        - max_items: Maximum number of items to return (None = all)\n+        - offset: Starting index (0-based, negative values calculated from end)\n+        \"\"\"\n+        with self.mutex:\n+            history_keys = list(self.history.keys())\n+\n+            if offset < 0 and max_items is not None:\n+                offset = max(0, len(history_keys) - max_items)\n+\n+            end_index = offset + max_items if max_items is not None else None\n+            selected_keys = history_keys[offset:end_index]\n+\n+            history_items = []\n+            for key in selected_keys:\n+                history_entry = copy.deepcopy(self.history[key])",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "2216978086",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8890,
        "pr_file": "execution.py",
        "discussion_id": "2216978086",
        "commented_code": "@@ -1116,6 +1116,126 @@ def get_history(self, prompt_id=None, max_items=None, offset=-1):\n             else:\n                 return {}\n \n+    def get_ordered_history(self, max_items=None, offset=0):\n+        \"\"\"\n+        Retrieves execution history in chronological order with pagination support.\n+        Returns a lightweight list of history objects.\n+        Used by the /history_v2.\n+        \n+        API Output Structure:\n+        {\n+            \"history\": [\n+                {\n+                    \"prompt_id\": str,          # Unique identifier for this execution\n+                    \"outputs\": dict,           # Node outputs {node_id: ui_data}\n+                    \"meta\": dict,              # Node metadata {node_id: {node_id, display_node, parent_node, real_node_id}}\n+                    \"prompt\": {\n+                        \"priority\": int,       # Execution priority\n+                        \"prompt_id\": str,      # Same as root prompt_id\n+                        \"extra_data\": dict     # Additional metadata (workflow removed from extra_pnginfo)\n+                    } | None,                  # None if no prompt data available\n+                    \"status\": {\n+                        \"status_str\": str,     # \"success\" | \"error\" \n+                        \"messages\": [          # Filtered execution event messages\n+                            (event_name: str, event_data: dict)\n+                        ]\n+                    } | None                   # None if no status recorded\n+                },\n+                # ... more history items\n+            ]\n+        }\n+        \n+        Parameters:\n+        - max_items: Maximum number of items to return (None = all)\n+        - offset: Starting index (0-based, negative values calculated from end)\n+        \"\"\"\n+        with self.mutex:\n+            history_keys = list(self.history.keys())\n+\n+            if offset < 0 and max_items is not None:\n+                offset = max(0, len(history_keys) - max_items)\n+\n+            end_index = offset + max_items if max_items is not None else None\n+            selected_keys = history_keys[offset:end_index]\n+\n+            history_items = []\n+            for key in selected_keys:\n+                history_entry = copy.deepcopy(self.history[key])",
        "comment_created_at": "2025-07-18T22:03:13+00:00",
        "comment_author": "christian-byrne",
        "comment_body": "Is it necessary to use `deepcopy`?\r\n\r\nSee https://github.com/comfyanonymous/ComfyUI/pull/8176 and https://github.com/Comfy-Org/ComfyUI_frontend/issues/2435",
        "pr_file_module": null
      },
      {
        "comment_id": "2216985711",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8890,
        "pr_file": "execution.py",
        "discussion_id": "2216978086",
        "commented_code": "@@ -1116,6 +1116,126 @@ def get_history(self, prompt_id=None, max_items=None, offset=-1):\n             else:\n                 return {}\n \n+    def get_ordered_history(self, max_items=None, offset=0):\n+        \"\"\"\n+        Retrieves execution history in chronological order with pagination support.\n+        Returns a lightweight list of history objects.\n+        Used by the /history_v2.\n+        \n+        API Output Structure:\n+        {\n+            \"history\": [\n+                {\n+                    \"prompt_id\": str,          # Unique identifier for this execution\n+                    \"outputs\": dict,           # Node outputs {node_id: ui_data}\n+                    \"meta\": dict,              # Node metadata {node_id: {node_id, display_node, parent_node, real_node_id}}\n+                    \"prompt\": {\n+                        \"priority\": int,       # Execution priority\n+                        \"prompt_id\": str,      # Same as root prompt_id\n+                        \"extra_data\": dict     # Additional metadata (workflow removed from extra_pnginfo)\n+                    } | None,                  # None if no prompt data available\n+                    \"status\": {\n+                        \"status_str\": str,     # \"success\" | \"error\" \n+                        \"messages\": [          # Filtered execution event messages\n+                            (event_name: str, event_data: dict)\n+                        ]\n+                    } | None                   # None if no status recorded\n+                },\n+                # ... more history items\n+            ]\n+        }\n+        \n+        Parameters:\n+        - max_items: Maximum number of items to return (None = all)\n+        - offset: Starting index (0-based, negative values calculated from end)\n+        \"\"\"\n+        with self.mutex:\n+            history_keys = list(self.history.keys())\n+\n+            if offset < 0 and max_items is not None:\n+                offset = max(0, len(history_keys) - max_items)\n+\n+            end_index = offset + max_items if max_items is not None else None\n+            selected_keys = history_keys[offset:end_index]\n+\n+            history_items = []\n+            for key in selected_keys:\n+                history_entry = copy.deepcopy(self.history[key])",
        "comment_created_at": "2025-07-18T22:12:08+00:00",
        "comment_author": "ric-yu",
        "comment_body": "We can build a new dict and copy values we need instead of deepcopying the whole thing and popping values we don't need. SGTY? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2216991979",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8890,
        "pr_file": "execution.py",
        "discussion_id": "2216978086",
        "commented_code": "@@ -1116,6 +1116,126 @@ def get_history(self, prompt_id=None, max_items=None, offset=-1):\n             else:\n                 return {}\n \n+    def get_ordered_history(self, max_items=None, offset=0):\n+        \"\"\"\n+        Retrieves execution history in chronological order with pagination support.\n+        Returns a lightweight list of history objects.\n+        Used by the /history_v2.\n+        \n+        API Output Structure:\n+        {\n+            \"history\": [\n+                {\n+                    \"prompt_id\": str,          # Unique identifier for this execution\n+                    \"outputs\": dict,           # Node outputs {node_id: ui_data}\n+                    \"meta\": dict,              # Node metadata {node_id: {node_id, display_node, parent_node, real_node_id}}\n+                    \"prompt\": {\n+                        \"priority\": int,       # Execution priority\n+                        \"prompt_id\": str,      # Same as root prompt_id\n+                        \"extra_data\": dict     # Additional metadata (workflow removed from extra_pnginfo)\n+                    } | None,                  # None if no prompt data available\n+                    \"status\": {\n+                        \"status_str\": str,     # \"success\" | \"error\" \n+                        \"messages\": [          # Filtered execution event messages\n+                            (event_name: str, event_data: dict)\n+                        ]\n+                    } | None                   # None if no status recorded\n+                },\n+                # ... more history items\n+            ]\n+        }\n+        \n+        Parameters:\n+        - max_items: Maximum number of items to return (None = all)\n+        - offset: Starting index (0-based, negative values calculated from end)\n+        \"\"\"\n+        with self.mutex:\n+            history_keys = list(self.history.keys())\n+\n+            if offset < 0 and max_items is not None:\n+                offset = max(0, len(history_keys) - max_items)\n+\n+            end_index = offset + max_items if max_items is not None else None\n+            selected_keys = history_keys[offset:end_index]\n+\n+            history_items = []\n+            for key in selected_keys:\n+                history_entry = copy.deepcopy(self.history[key])",
        "comment_created_at": "2025-07-18T22:20:52+00:00",
        "comment_author": "christian-byrne",
        "comment_body": "Seems good",
        "pr_file_module": null
      },
      {
        "comment_id": "2216999234",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8890,
        "pr_file": "execution.py",
        "discussion_id": "2216978086",
        "commented_code": "@@ -1116,6 +1116,126 @@ def get_history(self, prompt_id=None, max_items=None, offset=-1):\n             else:\n                 return {}\n \n+    def get_ordered_history(self, max_items=None, offset=0):\n+        \"\"\"\n+        Retrieves execution history in chronological order with pagination support.\n+        Returns a lightweight list of history objects.\n+        Used by the /history_v2.\n+        \n+        API Output Structure:\n+        {\n+            \"history\": [\n+                {\n+                    \"prompt_id\": str,          # Unique identifier for this execution\n+                    \"outputs\": dict,           # Node outputs {node_id: ui_data}\n+                    \"meta\": dict,              # Node metadata {node_id: {node_id, display_node, parent_node, real_node_id}}\n+                    \"prompt\": {\n+                        \"priority\": int,       # Execution priority\n+                        \"prompt_id\": str,      # Same as root prompt_id\n+                        \"extra_data\": dict     # Additional metadata (workflow removed from extra_pnginfo)\n+                    } | None,                  # None if no prompt data available\n+                    \"status\": {\n+                        \"status_str\": str,     # \"success\" | \"error\" \n+                        \"messages\": [          # Filtered execution event messages\n+                            (event_name: str, event_data: dict)\n+                        ]\n+                    } | None                   # None if no status recorded\n+                },\n+                # ... more history items\n+            ]\n+        }\n+        \n+        Parameters:\n+        - max_items: Maximum number of items to return (None = all)\n+        - offset: Starting index (0-based, negative values calculated from end)\n+        \"\"\"\n+        with self.mutex:\n+            history_keys = list(self.history.keys())\n+\n+            if offset < 0 and max_items is not None:\n+                offset = max(0, len(history_keys) - max_items)\n+\n+            end_index = offset + max_items if max_items is not None else None\n+            selected_keys = history_keys[offset:end_index]\n+\n+            history_items = []\n+            for key in selected_keys:\n+                history_entry = copy.deepcopy(self.history[key])",
        "comment_created_at": "2025-07-18T22:30:45+00:00",
        "comment_author": "ric-yu",
        "comment_body": "Updated! Attaching manual testing vids.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070885540",
    "pr_number": 7901,
    "pr_file": "comfy_extras/nodes_custom_sampler.py",
    "created_at": "2025-05-01T22:28:19+00:00",
    "commented_code": "sigmas[0] = sigma\n         return (sigmas, )\n \n+class ExtendIntermediateSigmas:\n+    @classmethod\n+    def INPUT_TYPES(s):\n+        return {\"required\":\n+                    {\"sigmas\": (\"SIGMAS\", ),\n+                     \"steps\": (\"INT\", {\"default\": 2, \"min\": 1, \"max\": 100}),\n+                     \"start_at_sigma\": (\"FLOAT\", {\"default\": -1.0, \"min\": -1.0, \"max\": 20000.0, \"step\": 0.01, \"round\": False}),\n+                     \"end_at_sigma\": (\"FLOAT\", {\"default\": 12.0, \"min\":  0.0, \"max\": 20000.0, \"step\": 0.01, \"round\": False}),\n+                     \"spacing\": (['linear', 'cosine', 'sine'],),\n+                    }\n+               }\n+    RETURN_TYPES = (\"SIGMAS\",)\n+    CATEGORY = \"sampling/custom_sampling/sigmas\"\n+\n+    FUNCTION = \"extend\"\n+\n+    def extend(self, sigmas, steps, start_at_sigma, end_at_sigma, spacing):\n+        if start_at_sigma < 0:\n+            start_at_sigma = float(\"inf\")\n+\n+        interpolator = {\n+            'linear': lambda x: x,\n+            'cosine': lambda x: torch.sin(x*math.pi/2),\n+            'sine':   lambda x: 1 - torch.cos(x*math.pi/2)\n+        }[spacing]\n+\n+        # linear space for our interpolation function\n+        x = torch.linspace(0, 1, steps + 1, device=sigmas.device)[1:-1]\n+\n+        extended_sigmas = []\n+        for i in range(len(sigmas) - 1):\n+            sigma_current = sigmas[i]\n+            sigma_next = sigmas[i+1]\n+\n+            extended_sigmas.append(sigma_current)\n+\n+            if end_at_sigma <= sigma_current <= start_at_sigma:\n+                interpolated_steps = interpolator(x) * (sigma_next - sigma_current) + sigma_current",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "2070885540",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 7901,
        "pr_file": "comfy_extras/nodes_custom_sampler.py",
        "discussion_id": "2070885540",
        "commented_code": "@@ -249,6 +250,54 @@ def set_first_sigma(self, sigmas, sigma):\n         sigmas[0] = sigma\n         return (sigmas, )\n \n+class ExtendIntermediateSigmas:\n+    @classmethod\n+    def INPUT_TYPES(s):\n+        return {\"required\":\n+                    {\"sigmas\": (\"SIGMAS\", ),\n+                     \"steps\": (\"INT\", {\"default\": 2, \"min\": 1, \"max\": 100}),\n+                     \"start_at_sigma\": (\"FLOAT\", {\"default\": -1.0, \"min\": -1.0, \"max\": 20000.0, \"step\": 0.01, \"round\": False}),\n+                     \"end_at_sigma\": (\"FLOAT\", {\"default\": 12.0, \"min\":  0.0, \"max\": 20000.0, \"step\": 0.01, \"round\": False}),\n+                     \"spacing\": (['linear', 'cosine', 'sine'],),\n+                    }\n+               }\n+    RETURN_TYPES = (\"SIGMAS\",)\n+    CATEGORY = \"sampling/custom_sampling/sigmas\"\n+\n+    FUNCTION = \"extend\"\n+\n+    def extend(self, sigmas, steps, start_at_sigma, end_at_sigma, spacing):\n+        if start_at_sigma < 0:\n+            start_at_sigma = float(\"inf\")\n+\n+        interpolator = {\n+            'linear': lambda x: x,\n+            'cosine': lambda x: torch.sin(x*math.pi/2),\n+            'sine':   lambda x: 1 - torch.cos(x*math.pi/2)\n+        }[spacing]\n+\n+        # linear space for our interpolation function\n+        x = torch.linspace(0, 1, steps + 1, device=sigmas.device)[1:-1]\n+\n+        extended_sigmas = []\n+        for i in range(len(sigmas) - 1):\n+            sigma_current = sigmas[i]\n+            sigma_next = sigmas[i+1]\n+\n+            extended_sigmas.append(sigma_current)\n+\n+            if end_at_sigma <= sigma_current <= start_at_sigma:\n+                interpolated_steps = interpolator(x) * (sigma_next - sigma_current) + sigma_current",
        "comment_created_at": "2025-05-01T22:28:19+00:00",
        "comment_author": "liesened",
        "comment_body": "Now that I look at it, putting `interpolator(x)` inside the loop is unnecessary. Can be moved to line 281 and assigned to `computed_spacing` or something.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1660396961",
    "pr_number": 3605,
    "pr_file": "comfy/sd.py",
    "created_at": "2024-07-01T02:57:44+00:00",
    "commented_code": "if model_config.clip_vision_prefix is not None:\n         if output_clipvision:\n-            clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n+            if output_clipvision:\n+                clipvision = model_cache.get_item(ckpt_path, 'clipvision')\n+                if clipvision is None:\n+                    clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n \n     if output_model:\n+        model_patcher = model_cache.get_item(ckpt_path, 'model')\n         inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)\n-        offload_device = model_management.unet_offload_device()\n-        model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n-        model.load_model_weights(sd, \"model.diffusion_model.\")\n+        if model_patcher is None:\n+            offload_device = model_management.unet_offload_device()\n+            model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n+            model.load_model_weights(sd, \"model.diffusion_model.\")\n+            model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device, current_device=inital_load_device)\n+\n+        if inital_load_device != torch.device(\"cpu\"):\n+            logging.info(\"loaded straight to GPU\")\n+            model_management.load_model_gpu(model_patcher)\n \n     if output_vae:\n-        vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n-        vae_sd = model_config.process_vae_state_dict(vae_sd)\n-        vae = VAE(sd=vae_sd)\n+        vae = model_cache.get_item(ckpt_path, 'vae')\n+        if vae is None:\n+            vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n+            vae_sd = model_config.process_vae_state_dict(vae_sd)\n+            vae = VAE(sd=vae_sd)\n \n+    clip_key = f\"clip_{embedding_directory}\"\n     if output_clip:\n-        clip_target = model_config.clip_target()\n-        if clip_target is not None:\n-            clip_sd = model_config.process_clip_state_dict(sd)\n-            if len(clip_sd) > 0:\n-                clip = CLIP(clip_target, embedding_directory=embedding_directory)\n-                m, u = clip.load_sd(clip_sd, full_model=True)\n-                if len(m) > 0:\n-                    m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n-                    if len(m_filter) > 0:\n-                        logging.warning(\"clip missing: {}\".format(m))\n-                    else:\n-                        logging.debug(\"clip missing: {}\".format(m))\n-\n-                if len(u) > 0:\n-                    logging.debug(\"clip unexpected {}:\".format(u))\n-            else:\n-                logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n+        clip = model_cache.get_item(ckpt_path, clip_key)\n+        if clip is None:\n+            clip_target = model_config.clip_target()\n+            if clip_target is not None:\n+                clip_sd = model_config.process_clip_state_dict(sd)\n+                if len(clip_sd) > 0:\n+                    clip = CLIP(clip_target, embedding_directory=embedding_directory)\n+                    m, u = clip.load_sd(clip_sd, full_model=True)\n+                    if len(m) > 0:\n+                        m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n+                        if len(m_filter) > 0:\n+                            logging.warning(\"clip missing: {}\".format(m))\n+                        else:\n+                            logging.debug(\"clip missing: {}\".format(m))\n+\n+                    if len(u) > 0:\n+                        logging.debug(\"clip unexpected {}:\".format(u))\n+                else:\n+                    logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n \n     left_over = sd.keys()\n     if len(left_over) > 0:\n         logging.debug(\"left over keys: {}\".format(left_over))\n-\n-    if output_model:\n-        model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=model_management.unet_offload_device(), current_device=inital_load_device)\n-        if inital_load_device != torch.device(\"cpu\"):\n-            logging.info(\"loaded straight to GPU\")\n-            model_management.load_model_gpu(model_patcher)\n-\n+    if model:\n+        logging.debug(f\"cache model of : {ckpt_path}\")\n+        model_cache.cache_model(ckpt_path, model_patcher)\n+    if clip:\n+        logging.debug(f\"cache clip of : {ckpt_path}\")\n+        model_cache.cache_clip(ckpt_path, clip_key, clip)\n+    if vae:\n+        logging.debug(f\"cache vae of : {ckpt_path}\")\n+        model_cache.cache_vae(ckpt_path, vae)\n+    if clipvision:\n+        logging.debug(f\"cache clipvision of : {ckpt_path}\")\n+        model_cache.cache_clipvision(ckpt_path, clipvision)",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1660396961",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 3605,
        "pr_file": "comfy/sd.py",
        "discussion_id": "1660396961",
        "commented_code": "@@ -464,48 +465,70 @@ def load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, o\n \n     if model_config.clip_vision_prefix is not None:\n         if output_clipvision:\n-            clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n+            if output_clipvision:\n+                clipvision = model_cache.get_item(ckpt_path, 'clipvision')\n+                if clipvision is None:\n+                    clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n \n     if output_model:\n+        model_patcher = model_cache.get_item(ckpt_path, 'model')\n         inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)\n-        offload_device = model_management.unet_offload_device()\n-        model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n-        model.load_model_weights(sd, \"model.diffusion_model.\")\n+        if model_patcher is None:\n+            offload_device = model_management.unet_offload_device()\n+            model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n+            model.load_model_weights(sd, \"model.diffusion_model.\")\n+            model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device, current_device=inital_load_device)\n+\n+        if inital_load_device != torch.device(\"cpu\"):\n+            logging.info(\"loaded straight to GPU\")\n+            model_management.load_model_gpu(model_patcher)\n \n     if output_vae:\n-        vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n-        vae_sd = model_config.process_vae_state_dict(vae_sd)\n-        vae = VAE(sd=vae_sd)\n+        vae = model_cache.get_item(ckpt_path, 'vae')\n+        if vae is None:\n+            vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n+            vae_sd = model_config.process_vae_state_dict(vae_sd)\n+            vae = VAE(sd=vae_sd)\n \n+    clip_key = f\"clip_{embedding_directory}\"\n     if output_clip:\n-        clip_target = model_config.clip_target()\n-        if clip_target is not None:\n-            clip_sd = model_config.process_clip_state_dict(sd)\n-            if len(clip_sd) > 0:\n-                clip = CLIP(clip_target, embedding_directory=embedding_directory)\n-                m, u = clip.load_sd(clip_sd, full_model=True)\n-                if len(m) > 0:\n-                    m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n-                    if len(m_filter) > 0:\n-                        logging.warning(\"clip missing: {}\".format(m))\n-                    else:\n-                        logging.debug(\"clip missing: {}\".format(m))\n-\n-                if len(u) > 0:\n-                    logging.debug(\"clip unexpected {}:\".format(u))\n-            else:\n-                logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n+        clip = model_cache.get_item(ckpt_path, clip_key)\n+        if clip is None:\n+            clip_target = model_config.clip_target()\n+            if clip_target is not None:\n+                clip_sd = model_config.process_clip_state_dict(sd)\n+                if len(clip_sd) > 0:\n+                    clip = CLIP(clip_target, embedding_directory=embedding_directory)\n+                    m, u = clip.load_sd(clip_sd, full_model=True)\n+                    if len(m) > 0:\n+                        m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n+                        if len(m_filter) > 0:\n+                            logging.warning(\"clip missing: {}\".format(m))\n+                        else:\n+                            logging.debug(\"clip missing: {}\".format(m))\n+\n+                    if len(u) > 0:\n+                        logging.debug(\"clip unexpected {}:\".format(u))\n+                else:\n+                    logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n \n     left_over = sd.keys()\n     if len(left_over) > 0:\n         logging.debug(\"left over keys: {}\".format(left_over))\n-\n-    if output_model:\n-        model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=model_management.unet_offload_device(), current_device=inital_load_device)\n-        if inital_load_device != torch.device(\"cpu\"):\n-            logging.info(\"loaded straight to GPU\")\n-            model_management.load_model_gpu(model_patcher)\n-\n+    if model:\n+        logging.debug(f\"cache model of : {ckpt_path}\")\n+        model_cache.cache_model(ckpt_path, model_patcher)\n+    if clip:\n+        logging.debug(f\"cache clip of : {ckpt_path}\")\n+        model_cache.cache_clip(ckpt_path, clip_key, clip)\n+    if vae:\n+        logging.debug(f\"cache vae of : {ckpt_path}\")\n+        model_cache.cache_vae(ckpt_path, vae)\n+    if clipvision:\n+        logging.debug(f\"cache clipvision of : {ckpt_path}\")\n+        model_cache.cache_clipvision(ckpt_path, clipvision)",
        "comment_created_at": "2024-07-01T02:57:44+00:00",
        "comment_author": "alboto",
        "comment_body": "A bug here\r\n`model_cache.cache_clipvision` needs to be changed to `model_cache.cache_clip_vision`",
        "pr_file_module": null
      },
      {
        "comment_id": "1665060560",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 3605,
        "pr_file": "comfy/sd.py",
        "discussion_id": "1660396961",
        "commented_code": "@@ -464,48 +465,70 @@ def load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, o\n \n     if model_config.clip_vision_prefix is not None:\n         if output_clipvision:\n-            clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n+            if output_clipvision:\n+                clipvision = model_cache.get_item(ckpt_path, 'clipvision')\n+                if clipvision is None:\n+                    clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n \n     if output_model:\n+        model_patcher = model_cache.get_item(ckpt_path, 'model')\n         inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)\n-        offload_device = model_management.unet_offload_device()\n-        model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n-        model.load_model_weights(sd, \"model.diffusion_model.\")\n+        if model_patcher is None:\n+            offload_device = model_management.unet_offload_device()\n+            model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n+            model.load_model_weights(sd, \"model.diffusion_model.\")\n+            model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device, current_device=inital_load_device)\n+\n+        if inital_load_device != torch.device(\"cpu\"):\n+            logging.info(\"loaded straight to GPU\")\n+            model_management.load_model_gpu(model_patcher)\n \n     if output_vae:\n-        vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n-        vae_sd = model_config.process_vae_state_dict(vae_sd)\n-        vae = VAE(sd=vae_sd)\n+        vae = model_cache.get_item(ckpt_path, 'vae')\n+        if vae is None:\n+            vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n+            vae_sd = model_config.process_vae_state_dict(vae_sd)\n+            vae = VAE(sd=vae_sd)\n \n+    clip_key = f\"clip_{embedding_directory}\"\n     if output_clip:\n-        clip_target = model_config.clip_target()\n-        if clip_target is not None:\n-            clip_sd = model_config.process_clip_state_dict(sd)\n-            if len(clip_sd) > 0:\n-                clip = CLIP(clip_target, embedding_directory=embedding_directory)\n-                m, u = clip.load_sd(clip_sd, full_model=True)\n-                if len(m) > 0:\n-                    m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n-                    if len(m_filter) > 0:\n-                        logging.warning(\"clip missing: {}\".format(m))\n-                    else:\n-                        logging.debug(\"clip missing: {}\".format(m))\n-\n-                if len(u) > 0:\n-                    logging.debug(\"clip unexpected {}:\".format(u))\n-            else:\n-                logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n+        clip = model_cache.get_item(ckpt_path, clip_key)\n+        if clip is None:\n+            clip_target = model_config.clip_target()\n+            if clip_target is not None:\n+                clip_sd = model_config.process_clip_state_dict(sd)\n+                if len(clip_sd) > 0:\n+                    clip = CLIP(clip_target, embedding_directory=embedding_directory)\n+                    m, u = clip.load_sd(clip_sd, full_model=True)\n+                    if len(m) > 0:\n+                        m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n+                        if len(m_filter) > 0:\n+                            logging.warning(\"clip missing: {}\".format(m))\n+                        else:\n+                            logging.debug(\"clip missing: {}\".format(m))\n+\n+                    if len(u) > 0:\n+                        logging.debug(\"clip unexpected {}:\".format(u))\n+                else:\n+                    logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n \n     left_over = sd.keys()\n     if len(left_over) > 0:\n         logging.debug(\"left over keys: {}\".format(left_over))\n-\n-    if output_model:\n-        model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=model_management.unet_offload_device(), current_device=inital_load_device)\n-        if inital_load_device != torch.device(\"cpu\"):\n-            logging.info(\"loaded straight to GPU\")\n-            model_management.load_model_gpu(model_patcher)\n-\n+    if model:\n+        logging.debug(f\"cache model of : {ckpt_path}\")\n+        model_cache.cache_model(ckpt_path, model_patcher)\n+    if clip:\n+        logging.debug(f\"cache clip of : {ckpt_path}\")\n+        model_cache.cache_clip(ckpt_path, clip_key, clip)\n+    if vae:\n+        logging.debug(f\"cache vae of : {ckpt_path}\")\n+        model_cache.cache_vae(ckpt_path, vae)\n+    if clipvision:\n+        logging.debug(f\"cache clipvision of : {ckpt_path}\")\n+        model_cache.cache_clipvision(ckpt_path, clipvision)",
        "comment_created_at": "2024-07-04T03:27:23+00:00",
        "comment_author": "jasstionzyf",
        "comment_body": "It is better to cache clipVision model also within CLIPVisionLoader\r\nWhen use ip adapter every time will need to load CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors \r\nUse model cache can directly copy it each time from cpu memory",
        "pr_file_module": null
      },
      {
        "comment_id": "1666159314",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 3605,
        "pr_file": "comfy/sd.py",
        "discussion_id": "1660396961",
        "commented_code": "@@ -464,48 +465,70 @@ def load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True, o\n \n     if model_config.clip_vision_prefix is not None:\n         if output_clipvision:\n-            clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n+            if output_clipvision:\n+                clipvision = model_cache.get_item(ckpt_path, 'clipvision')\n+                if clipvision is None:\n+                    clipvision = clip_vision.load_clipvision_from_sd(sd, model_config.clip_vision_prefix, True)\n \n     if output_model:\n+        model_patcher = model_cache.get_item(ckpt_path, 'model')\n         inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)\n-        offload_device = model_management.unet_offload_device()\n-        model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n-        model.load_model_weights(sd, \"model.diffusion_model.\")\n+        if model_patcher is None:\n+            offload_device = model_management.unet_offload_device()\n+            model = model_config.get_model(sd, \"model.diffusion_model.\", device=inital_load_device)\n+            model.load_model_weights(sd, \"model.diffusion_model.\")\n+            model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=offload_device, current_device=inital_load_device)\n+\n+        if inital_load_device != torch.device(\"cpu\"):\n+            logging.info(\"loaded straight to GPU\")\n+            model_management.load_model_gpu(model_patcher)\n \n     if output_vae:\n-        vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n-        vae_sd = model_config.process_vae_state_dict(vae_sd)\n-        vae = VAE(sd=vae_sd)\n+        vae = model_cache.get_item(ckpt_path, 'vae')\n+        if vae is None:\n+            vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: \"\" for k in model_config.vae_key_prefix}, filter_keys=True)\n+            vae_sd = model_config.process_vae_state_dict(vae_sd)\n+            vae = VAE(sd=vae_sd)\n \n+    clip_key = f\"clip_{embedding_directory}\"\n     if output_clip:\n-        clip_target = model_config.clip_target()\n-        if clip_target is not None:\n-            clip_sd = model_config.process_clip_state_dict(sd)\n-            if len(clip_sd) > 0:\n-                clip = CLIP(clip_target, embedding_directory=embedding_directory)\n-                m, u = clip.load_sd(clip_sd, full_model=True)\n-                if len(m) > 0:\n-                    m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n-                    if len(m_filter) > 0:\n-                        logging.warning(\"clip missing: {}\".format(m))\n-                    else:\n-                        logging.debug(\"clip missing: {}\".format(m))\n-\n-                if len(u) > 0:\n-                    logging.debug(\"clip unexpected {}:\".format(u))\n-            else:\n-                logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n+        clip = model_cache.get_item(ckpt_path, clip_key)\n+        if clip is None:\n+            clip_target = model_config.clip_target()\n+            if clip_target is not None:\n+                clip_sd = model_config.process_clip_state_dict(sd)\n+                if len(clip_sd) > 0:\n+                    clip = CLIP(clip_target, embedding_directory=embedding_directory)\n+                    m, u = clip.load_sd(clip_sd, full_model=True)\n+                    if len(m) > 0:\n+                        m_filter = list(filter(lambda a: \".logit_scale\" not in a and \".transformer.text_projection.weight\" not in a, m))\n+                        if len(m_filter) > 0:\n+                            logging.warning(\"clip missing: {}\".format(m))\n+                        else:\n+                            logging.debug(\"clip missing: {}\".format(m))\n+\n+                    if len(u) > 0:\n+                        logging.debug(\"clip unexpected {}:\".format(u))\n+                else:\n+                    logging.warning(\"no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\")\n \n     left_over = sd.keys()\n     if len(left_over) > 0:\n         logging.debug(\"left over keys: {}\".format(left_over))\n-\n-    if output_model:\n-        model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device, offload_device=model_management.unet_offload_device(), current_device=inital_load_device)\n-        if inital_load_device != torch.device(\"cpu\"):\n-            logging.info(\"loaded straight to GPU\")\n-            model_management.load_model_gpu(model_patcher)\n-\n+    if model:\n+        logging.debug(f\"cache model of : {ckpt_path}\")\n+        model_cache.cache_model(ckpt_path, model_patcher)\n+    if clip:\n+        logging.debug(f\"cache clip of : {ckpt_path}\")\n+        model_cache.cache_clip(ckpt_path, clip_key, clip)\n+    if vae:\n+        logging.debug(f\"cache vae of : {ckpt_path}\")\n+        model_cache.cache_vae(ckpt_path, vae)\n+    if clipvision:\n+        logging.debug(f\"cache clipvision of : {ckpt_path}\")\n+        model_cache.cache_clipvision(ckpt_path, clipvision)",
        "comment_created_at": "2024-07-05T02:07:24+00:00",
        "comment_author": "efwfe",
        "comment_body": "copy that.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1724406163",
    "pr_number": 4295,
    "pr_file": "api_server/utils/file_operations.py",
    "created_at": "2024-08-21T05:15:13+00:00",
    "commented_code": "+import os\n+from typing import List, Union, TypedDict, Literal\n+from typing_extensions import TypeGuard\n+class FileInfo(TypedDict):\n+    name: str\n+    path: str\n+    type: Literal[\"file\"]\n+    size: int\n+\n+class DirectoryInfo(TypedDict):\n+    name: str\n+    path: str\n+    type: Literal[\"directory\"]\n+\n+FileSystemItem = Union[FileInfo, DirectoryInfo]\n+\n+def is_file_info(item: FileSystemItem) -> TypeGuard[FileInfo]:\n+    return item[\"type\"] == \"file\"\n+\n+class FileSystemOperations:\n+    @staticmethod\n+    def walk_directory(directory: str) -> List[FileSystemItem]:\n+        file_list: List[FileSystemItem] = []\n+        for root, dirs, files in os.walk(directory):\n+            for name in files:\n+                file_path = os.path.join(root, name)\n+                relative_path = os.path.relpath(file_path, directory)\n+                file_list.append({\n+                    \"name\": name,\n+                    \"path\": relative_path,\n+                    \"type\": \"file\",\n+                    \"size\": os.path.getsize(file_path)",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1724406163",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 4295,
        "pr_file": "api_server/utils/file_operations.py",
        "discussion_id": "1724406163",
        "commented_code": "@@ -0,0 +1,42 @@\n+import os\n+from typing import List, Union, TypedDict, Literal\n+from typing_extensions import TypeGuard\n+class FileInfo(TypedDict):\n+    name: str\n+    path: str\n+    type: Literal[\"file\"]\n+    size: int\n+\n+class DirectoryInfo(TypedDict):\n+    name: str\n+    path: str\n+    type: Literal[\"directory\"]\n+\n+FileSystemItem = Union[FileInfo, DirectoryInfo]\n+\n+def is_file_info(item: FileSystemItem) -> TypeGuard[FileInfo]:\n+    return item[\"type\"] == \"file\"\n+\n+class FileSystemOperations:\n+    @staticmethod\n+    def walk_directory(directory: str) -> List[FileSystemItem]:\n+        file_list: List[FileSystemItem] = []\n+        for root, dirs, files in os.walk(directory):\n+            for name in files:\n+                file_path = os.path.join(root, name)\n+                relative_path = os.path.relpath(file_path, directory)\n+                file_list.append({\n+                    \"name\": name,\n+                    \"path\": relative_path,\n+                    \"type\": \"file\",\n+                    \"size\": os.path.getsize(file_path)",
        "comment_created_at": "2024-08-21T05:15:13+00:00",
        "comment_author": "mcmonkey4eva",
        "comment_body": "Maybe out of scope for this PR, but this function is going to be very slow without heavy caching of some form on any decently sized folder",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1193084281",
    "pr_number": 654,
    "pr_file": "comfy/sd.py",
    "created_at": "2023-05-14T06:37:39+00:00",
    "commented_code": "if self.cond_hint is None or x_noisy.shape[2] * 8 != self.cond_hint.shape[2] or x_noisy.shape[3] * 8 != self.cond_hint.shape[3]:\n             if self.cond_hint is not None:\n                 del self.cond_hint\n+            self.control_input = None\n             self.cond_hint = None\n-            self.cond_hint = resize_image_to(self.cond_hint_original, x_noisy, batched_number).float().to(self.device)\n+            self.cond_hint = utils.common_upscale(self.cond_hint_original, x_noisy.shape[3] * 8, x_noisy.shape[2] * 8, 'nearest-exact', \"center\").float().to(self.device)\n             if self.channels_in == 1 and self.cond_hint.shape[1] > 1:\n                 self.cond_hint = torch.mean(self.cond_hint, 1, keepdim=True)\n+        if x_noisy.shape[0] != self.cond_hint.shape[0]:\n+            self.cond_hint = broadcast_image_to(self.cond_hint, x_noisy.shape[0], batched_number)",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1193084281",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 654,
        "pr_file": "comfy/sd.py",
        "discussion_id": "1193084281",
        "commented_code": "@@ -794,10 +793,14 @@ def get_control(self, x_noisy, t, cond_txt, batched_number):\n         if self.cond_hint is None or x_noisy.shape[2] * 8 != self.cond_hint.shape[2] or x_noisy.shape[3] * 8 != self.cond_hint.shape[3]:\n             if self.cond_hint is not None:\n                 del self.cond_hint\n+            self.control_input = None\n             self.cond_hint = None\n-            self.cond_hint = resize_image_to(self.cond_hint_original, x_noisy, batched_number).float().to(self.device)\n+            self.cond_hint = utils.common_upscale(self.cond_hint_original, x_noisy.shape[3] * 8, x_noisy.shape[2] * 8, 'nearest-exact', \"center\").float().to(self.device)\n             if self.channels_in == 1 and self.cond_hint.shape[1] > 1:\n                 self.cond_hint = torch.mean(self.cond_hint, 1, keepdim=True)\n+        if x_noisy.shape[0] != self.cond_hint.shape[0]:\n+            self.cond_hint = broadcast_image_to(self.cond_hint, x_noisy.shape[0], batched_number)",
        "comment_created_at": "2023-05-14T06:37:39+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "Is there any reason why you need to do this every run instead of only when the t2i model is actually executed?",
        "pr_file_module": null
      },
      {
        "comment_id": "1193109708",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 654,
        "pr_file": "comfy/sd.py",
        "discussion_id": "1193084281",
        "commented_code": "@@ -794,10 +793,14 @@ def get_control(self, x_noisy, t, cond_txt, batched_number):\n         if self.cond_hint is None or x_noisy.shape[2] * 8 != self.cond_hint.shape[2] or x_noisy.shape[3] * 8 != self.cond_hint.shape[3]:\n             if self.cond_hint is not None:\n                 del self.cond_hint\n+            self.control_input = None\n             self.cond_hint = None\n-            self.cond_hint = resize_image_to(self.cond_hint_original, x_noisy, batched_number).float().to(self.device)\n+            self.cond_hint = utils.common_upscale(self.cond_hint_original, x_noisy.shape[3] * 8, x_noisy.shape[2] * 8, 'nearest-exact', \"center\").float().to(self.device)\n             if self.channels_in == 1 and self.cond_hint.shape[1] > 1:\n                 self.cond_hint = torch.mean(self.cond_hint, 1, keepdim=True)\n+        if x_noisy.shape[0] != self.cond_hint.shape[0]:\n+            self.cond_hint = broadcast_image_to(self.cond_hint, x_noisy.shape[0], batched_number)",
        "comment_created_at": "2023-05-14T09:29:38+00:00",
        "comment_author": "BlenderNeko",
        "comment_body": "the `control_input ` has to change for every tile, I can make sure that all of the stuff above this if statements is dealt with once per tile, but I don't have access to the batch size of `x_noise` beforehand so the t2i model has to check if the batch size is correct and if not make it so. As a result the normal sampler now has to deal with 3 checks every step instead of one, but each of these checks is still O(1)",
        "pr_file_module": null
      }
    ]
  }
]