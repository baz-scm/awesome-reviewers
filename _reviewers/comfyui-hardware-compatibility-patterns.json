[
  {
    "discussion_id": "2162784196",
    "pr_number": 8585,
    "pr_file": "cuda_malloc.py",
    "created_at": "2025-06-24T01:15:24+00:00",
    "commented_code": "\"GeForce GTX 1650\", \"GeForce GTX 1630\", \"Tesla M4\", \"Tesla M6\", \"Tesla M10\", \"Tesla M40\", \"Tesla M60\"\n                 }\n \n+def is_ixuca():\n+    try:\n+        import torch",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "2162784196",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8585,
        "pr_file": "cuda_malloc.py",
        "discussion_id": "2162784196",
        "commented_code": "@@ -50,7 +50,17 @@ def enum_display_devices():\n                 \"GeForce GTX 1650\", \"GeForce GTX 1630\", \"Tesla M4\", \"Tesla M6\", \"Tesla M10\", \"Tesla M40\", \"Tesla M60\"\n                 }\n \n+def is_ixuca():\n+    try:\n+        import torch",
        "comment_created_at": "2025-06-24T01:15:24+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "If you import torch before setting the cuda malloc option it completely breaks cuda malloc on nvidia gpus.",
        "pr_file_module": null
      },
      {
        "comment_id": "2162828430",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8585,
        "pr_file": "cuda_malloc.py",
        "discussion_id": "2162784196",
        "commented_code": "@@ -50,7 +50,17 @@ def enum_display_devices():\n                 \"GeForce GTX 1650\", \"GeForce GTX 1630\", \"Tesla M4\", \"Tesla M6\", \"Tesla M10\", \"Tesla M40\", \"Tesla M60\"\n                 }\n \n+def is_ixuca():\n+    try:\n+        import torch",
        "comment_created_at": "2025-06-24T02:13:19+00:00",
        "comment_author": "honglyua-il",
        "comment_body": "I have updated the code. Using _load_torch_submodule to check if is_ixuca like the get version code, and return early in cuda_malloc_supported if is_ixuca. \r\n\r\n1. Created shared _load_torch_submodule() helper to handle all the importlib boilerplate like version.py, corex.py and so on.\r\n2. Updated is_ixuca to use the helper and not import torch directly to break cuda malloc on nvidia gpus.\r\n3. Made the version check code more concise and robust by using the helper",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2082215653",
    "pr_number": 8011,
    "pr_file": "comfy/clip_vision.py",
    "created_at": "2025-05-09T17:47:36+00:00",
    "commented_code": "else:\n             scale_size = (size, size)\n \n-        image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bicubic\", antialias=True)\n+        image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bilinear\")",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "2082215653",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8011,
        "pr_file": "comfy/clip_vision.py",
        "discussion_id": "2082215653",
        "commented_code": "@@ -29,7 +29,7 @@ def clip_preprocess(image, size=224, mean=[0.48145466, 0.4578275, 0.40821073], s\n         else:\n             scale_size = (size, size)\n \n-        image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bicubic\", antialias=True)\n+        image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bilinear\")",
        "comment_created_at": "2025-05-09T17:47:36+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "This cannot be changed because bicubic is the correct way to downscale images for the clip vision models.",
        "pr_file_module": null
      },
      {
        "comment_id": "2082391646",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8011,
        "pr_file": "comfy/clip_vision.py",
        "discussion_id": "2082215653",
        "commented_code": "@@ -29,7 +29,7 @@ def clip_preprocess(image, size=224, mean=[0.48145466, 0.4578275, 0.40821073], s\n         else:\n             scale_size = (size, size)\n \n-        image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bicubic\", antialias=True)\n+        image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bilinear\")",
        "comment_created_at": "2025-05-09T19:34:35+00:00",
        "comment_author": "hben35096",
        "comment_body": "If I use it on a computer with a Moore Threads GPU:\r\n`image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bicubic\", antialias=True`\r\nI receive the following error:\r\n```\r\nVAE load device: musa:0, offload device: cpu, dtype: torch.bfloat16\r\nCLIP/text encoder model load device: musa:0, offload device: cpu, current: cpu, dtype: torch.float16\r\nRequested to load SD1ClipModel\r\nloaded completely 47836.78046875 235.84423828125 True\r\nFETCH ComfyRegistry Data: 15/84\r\nINFO: Clip Vision model loaded from /root/autodl-tmp/ComfyUI/models/clip_vision/CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors\r\nINFO: IPAdapter model loaded from /root/autodl-tmp/ComfyUI/models/ipadapter/ip-adapter_sd15.safetensors\r\nINFO: the IPAdapter reference image is not a square, CLIPImageProcessor will resize and crop it at the center. If the main focus of the picture is not in the middle the result might not be what you are expecting.\r\nRequested to load CLIPVisionModelProjection\r\nloaded completely 47552.3732421875 1208.09814453125 True\r\n!!! Exception during processing !!! Could not run 'aten::_upsample_bicubic2d_aa.out' with arguments from the 'musa' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_upsample_bicubic2d_aa.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, AutocastPrivateUse1, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\r\n\r\nCPU: registered at /home/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31411 [kernel]\r\nMeta: registered at /home/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\r\nBackendSelect: fallthrough registered at /home/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\r\nPython: registered at /home/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\r\nFuncTorchDynamicLayerBackMode: registered at /home/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\r\nFunctionalize: registered at /home/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:21981 [kernel]\r\nNamed: registered at /home/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\r\nConjugate: registered at /home/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\r\nNegative: registered at /home/pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\r\nZeroTensor: registered at /home/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\r\nADInplaceOrView: registered at /home/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4832 [kernel]\r\nAutogradOther: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradCPU: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradCUDA: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradHIP: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradXLA: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradMPS: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradIPU: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradXPU: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradHPU: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradVE: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradLazy: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradMTIA: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradPrivateUse1: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradPrivateUse2: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradPrivateUse3: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradMeta: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradNestedTensor: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nTracer: registered at /home/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:17001 [kernel]\r\nAutocastCPU: fallthrough registered at /home/pytorch/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\r\nAutocastCUDA: fallthrough registered at /home/pytorch/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\r\nAutocastPrivateUse1: fallthrough registered at /home/torch_musa/torch_musa/csrc/amp/autocast_mode.cpp:412 [backend fallback]\r\nFuncTorchBatched: registered at /home/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\r\nBatchedNestedTensor: registered at /home/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\r\nFuncTorchVmapMode: fallthrough registered at /home/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\r\nBatched: registered at /home/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\r\nVmapMode: fallthrough registered at /home/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\nFuncTorchGradWrapper: registered at /home/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\r\nPythonTLSSnapshot: registered at /home/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\r\nFuncTorchDynamicLayerFrontMode: registered at /home/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\r\nPreDispatch: registered at /home/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\r\nPythonDispatcher: registered at /home/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/autodl-tmp/ComfyUI/execution.py\", line 347, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"/root/autodl-tmp/ComfyUI/execution.py\", line 222, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"/root/autodl-tmp/ComfyUI/execution.py\", line 194, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"/root/autodl-tmp/ComfyUI/execution.py\", line 183, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n  File \"/root/autodl-tmp/ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/IPAdapterPlus.py\", line 752, in apply_ipadapter\r\n    return ipadapter_execute(model.clone(), ipadapter['ipadapter']['model'], ipadapter['clipvision']['model'], **ipa_args)\r\n  File \"/root/autodl-tmp/ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/IPAdapterPlus.py\", line 376, in ipadapter_execute\r\n    img_cond_embeds = encode_image_masked(clipvision, image, batch_size=encode_batch_size, tiles=enhance_tiles, ratio=enhance_ratio, clipvision_size=clipvision_size)\r\n  File \"/root/autodl-tmp/ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/utils.py\", line 242, in encode_image_masked\r\n    embeds = encode_image_masked_(clip_vision, image, mask, batch_size, clipvision_size=clipvision_size)\r\n  File \"/root/autodl-tmp/ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/utils.py\", line 293, in encode_image_masked_\r\n    pixel_values = clip_preprocess(img, size=clipvision_size).float()\r\n  File \"/root/autodl-tmp/ComfyUI/comfy/clip_vision.py\", line 32, in clip_preprocess\r\n    image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bicubic\", antialias=True)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py\", line 4059, in interpolate\r\n    return torch._C._nn._upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors)\r\nNotImplementedError: Could not run 'aten::_upsample_bicubic2d_aa.out' with arguments from the 'musa' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_upsample_bicubic2d_aa.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, AutocastPrivateUse1, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\r\n\r\nCPU: registered at /home/pytorch/build/aten/src/ATen/RegisterCPU.cpp:31411 [kernel]\r\nMeta: registered at /home/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\r\nBackendSelect: fallthrough registered at /home/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\r\nPython: registered at /home/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\r\nFuncTorchDynamicLayerBackMode: registered at /home/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\r\nFunctionalize: registered at /home/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:21981 [kernel]\r\nNamed: registered at /home/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\r\nConjugate: registered at /home/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\r\nNegative: registered at /home/pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\r\nZeroTensor: registered at /home/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\r\nADInplaceOrView: registered at /home/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4832 [kernel]\r\nAutogradOther: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradCPU: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradCUDA: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradHIP: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradXLA: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradMPS: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradIPU: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradXPU: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradHPU: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradVE: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradLazy: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradMTIA: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradPrivateUse1: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradPrivateUse2: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradPrivateUse3: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradMeta: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nAutogradNestedTensor: registered at /home/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:19039 [autograd kernel]\r\nTracer: registered at /home/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:17001 [kernel]\r\nAutocastCPU: fallthrough registered at /home/pytorch/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\r\nAutocastCUDA: fallthrough registered at /home/pytorch/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\r\nAutocastPrivateUse1: fallthrough registered at /home/torch_musa/torch_musa/csrc/amp/autocast_mode.cpp:412 [backend fallback]\r\nFuncTorchBatched: registered at /home/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\r\nBatchedNestedTensor: registered at /home/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\r\nFuncTorchVmapMode: fallthrough registered at /home/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\r\nBatched: registered at /home/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\r\nVmapMode: fallthrough registered at /home/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\nFuncTorchGradWrapper: registered at /home/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\r\nPythonTLSSnapshot: registered at /home/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\r\nFuncTorchDynamicLayerFrontMode: registered at /home/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\r\nPreDispatch: registered at /home/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\r\nPythonDispatcher: registered at /home/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\r\n```\r\nSo I used \"bilinear\" to make it run, and judging from the results, it seems acceptable, haha.\r\nOf course, it would be even better if we could determine which method to use based on the device being used, or if there were other solutions.\r\n\r\n![PixPin_2025-05-10_03-09-1822](https://github.com/user-attachments/assets/c5f756d8-a86c-48b2-a080-3c8520634bba)\r\n\r\n![PixPin_2025-05-10_03-09-182](https://github.com/user-attachments/assets/cc0b875a-b1eb-4d11-b792-ab9066fd1763)\r\n\r\n\r\nI\u2019m not very familiar with programming; I can only use the most amateur methods. I hope you can understand.\r\n\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2082870302",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8011,
        "pr_file": "comfy/clip_vision.py",
        "discussion_id": "2082215653",
        "commented_code": "@@ -29,7 +29,7 @@ def clip_preprocess(image, size=224, mean=[0.48145466, 0.4578275, 0.40821073], s\n         else:\n             scale_size = (size, size)\n \n-        image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bicubic\", antialias=True)\n+        image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bilinear\")",
        "comment_created_at": "2025-05-10T05:53:05+00:00",
        "comment_author": "hben35096",
        "comment_body": "You're right, I changed it to:\r\n```\r\n        if image.device.type == 'musa':\r\n            image = image.cpu()\r\n            image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bicubic\", antialias=True)\r\n            image = image.to('musa')\r\n        else:\r\n            image = torch.nn.functional.interpolate(image, size=scale_size, mode=\"bicubic\", antialias=True)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1947389700",
    "pr_number": 6542,
    "pr_file": "comfy/clip_model.py",
    "created_at": "2025-02-08T01:10:02+00:00",
    "commented_code": "mask = 1.0 - attention_mask.to(x.dtype).reshape((attention_mask.shape[0], 1, -1, attention_mask.shape[-1])).expand(attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1])\n             mask = mask.masked_fill(mask.to(torch.bool), -torch.finfo(x.dtype).max)\n \n-        causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).fill_(-torch.finfo(x.dtype).max).triu_(1)\n+        if comfy.model_management.is_directml_enabled():\n+            causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).triu_(1)\n+        else:",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1947389700",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6542,
        "pr_file": "comfy/clip_model.py",
        "discussion_id": "1947389700",
        "commented_code": "@@ -104,7 +104,11 @@ def forward(self, input_tokens, attention_mask=None, intermediate_output=None, f\n             mask = 1.0 - attention_mask.to(x.dtype).reshape((attention_mask.shape[0], 1, -1, attention_mask.shape[-1])).expand(attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1])\n             mask = mask.masked_fill(mask.to(torch.bool), -torch.finfo(x.dtype).max)\n \n-        causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).fill_(-torch.finfo(x.dtype).max).triu_(1)\n+        if comfy.model_management.is_directml_enabled():\n+            causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).triu_(1)\n+        else:",
        "comment_created_at": "2025-02-08T01:10:02+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "Is this change still needed on the latest code?",
        "pr_file_module": null
      },
      {
        "comment_id": "1947483739",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6542,
        "pr_file": "comfy/clip_model.py",
        "discussion_id": "1947389700",
        "commented_code": "@@ -104,7 +104,11 @@ def forward(self, input_tokens, attention_mask=None, intermediate_output=None, f\n             mask = 1.0 - attention_mask.to(x.dtype).reshape((attention_mask.shape[0], 1, -1, attention_mask.shape[-1])).expand(attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1])\n             mask = mask.masked_fill(mask.to(torch.bool), -torch.finfo(x.dtype).max)\n \n-        causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).fill_(-torch.finfo(x.dtype).max).triu_(1)\n+        if comfy.model_management.is_directml_enabled():\n+            causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).triu_(1)\n+        else:",
        "comment_created_at": "2025-02-08T05:25:15+00:00",
        "comment_author": "hisham-hchowdhu",
        "comment_body": "yes, without this change image generated is blank",
        "pr_file_module": null
      },
      {
        "comment_id": "1948494279",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6542,
        "pr_file": "comfy/clip_model.py",
        "discussion_id": "1947389700",
        "commented_code": "@@ -104,7 +104,11 @@ def forward(self, input_tokens, attention_mask=None, intermediate_output=None, f\n             mask = 1.0 - attention_mask.to(x.dtype).reshape((attention_mask.shape[0], 1, -1, attention_mask.shape[-1])).expand(attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1])\n             mask = mask.masked_fill(mask.to(torch.bool), -torch.finfo(x.dtype).max)\n \n-        causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).fill_(-torch.finfo(x.dtype).max).triu_(1)\n+        if comfy.model_management.is_directml_enabled():\n+            causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).triu_(1)\n+        else:",
        "comment_created_at": "2025-02-10T07:06:58+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "This isn't setting the causal mask correctly.\r\n\r\nDoes doing this work?\r\n\r\n`torch.full((x.shape[1], x.shape[1]), -torch.finfo(x.dtype).max, dtype=x.dtype, device=x.device).triu_(1)`\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1951487763",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6542,
        "pr_file": "comfy/clip_model.py",
        "discussion_id": "1947389700",
        "commented_code": "@@ -104,7 +104,11 @@ def forward(self, input_tokens, attention_mask=None, intermediate_output=None, f\n             mask = 1.0 - attention_mask.to(x.dtype).reshape((attention_mask.shape[0], 1, -1, attention_mask.shape[-1])).expand(attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1])\n             mask = mask.masked_fill(mask.to(torch.bool), -torch.finfo(x.dtype).max)\n \n-        causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).fill_(-torch.finfo(x.dtype).max).triu_(1)\n+        if comfy.model_management.is_directml_enabled():\n+            causal_mask = torch.empty(x.shape[1], x.shape[1], dtype=x.dtype, device=x.device).triu_(1)\n+        else:",
        "comment_created_at": "2025-02-11T19:40:16+00:00",
        "comment_author": "hisham-hchowdhu",
        "comment_body": "yes, this seem to work....do you want me to make this change under directml path or make it default?\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1905499889",
    "pr_number": 6300,
    "pr_file": "latent_preview.py",
    "created_at": "2025-01-07T14:01:41+00:00",
    "commented_code": "def preview_to_image(latent_image):\n         latents_ubyte = (((latent_image + 1.0) / 2.0).clamp(0, 1)  # change scale from -1..1 to 0..1\n                             .mul(0xFF)  # to 0..255\n-                            ).to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n+                            )\n+        latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\n+        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1905499889",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6300,
        "pr_file": "latent_preview.py",
        "discussion_id": "1905499889",
        "commented_code": "@@ -12,7 +12,9 @@\n def preview_to_image(latent_image):\n         latents_ubyte = (((latent_image + 1.0) / 2.0).clamp(0, 1)  # change scale from -1..1 to 0..1\n                             .mul(0xFF)  # to 0..255\n-                            ).to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n+                            )\n+        latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\n+        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))",
        "comment_created_at": "2025-01-07T14:01:41+00:00",
        "comment_author": "shenanigansd",
        "comment_body": "```suggestion\r\n        if comfy.model_management.directml_enabled:\r\n            latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\r\n            latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1905609926",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6300,
        "pr_file": "latent_preview.py",
        "discussion_id": "1905499889",
        "commented_code": "@@ -12,7 +12,9 @@\n def preview_to_image(latent_image):\n         latents_ubyte = (((latent_image + 1.0) / 2.0).clamp(0, 1)  # change scale from -1..1 to 0..1\n                             .mul(0xFF)  # to 0..255\n-                            ).to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n+                            )\n+        latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\n+        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))",
        "comment_created_at": "2025-01-07T15:17:06+00:00",
        "comment_author": "mcmonkey4eva",
        "comment_body": "no that would block the conversion from happening at all on nvidia GPUs. You want only the first line inside the if",
        "pr_file_module": null
      },
      {
        "comment_id": "1911551955",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6300,
        "pr_file": "latent_preview.py",
        "discussion_id": "1905499889",
        "commented_code": "@@ -12,7 +12,9 @@\n def preview_to_image(latent_image):\n         latents_ubyte = (((latent_image + 1.0) / 2.0).clamp(0, 1)  # change scale from -1..1 to 0..1\n                             .mul(0xFF)  # to 0..255\n-                            ).to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n+                            )\n+        latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\n+        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))",
        "comment_created_at": "2025-01-10T22:20:40+00:00",
        "comment_author": "Kosinkadink",
        "comment_body": "Newest commit by @shenanigansd resolves conversation.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1894488730",
    "pr_number": 6112,
    "pr_file": "comfy/model_management.py",
    "created_at": "2024-12-20T23:09:36+00:00",
    "commented_code": "if args.cpu_vae:\n     VAE_DTYPES = [torch.float32]\n \n-\n if ENABLE_PYTORCH_ATTENTION:\n     torch.backends.cuda.enable_math_sdp(True)\n     torch.backends.cuda.enable_flash_sdp(True)\n     torch.backends.cuda.enable_mem_efficient_sdp(True)\n \n+if int(torch_version[0]) == 2 and int(torch_version[2]) >= 5:\n+    torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1894488730",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6112,
        "pr_file": "comfy/model_management.py",
        "discussion_id": "1894488730",
        "commented_code": "@@ -214,12 +214,14 @@ def is_nvidia():\n if args.cpu_vae:\n     VAE_DTYPES = [torch.float32]\n \n-\n if ENABLE_PYTORCH_ATTENTION:\n     torch.backends.cuda.enable_math_sdp(True)\n     torch.backends.cuda.enable_flash_sdp(True)\n     torch.backends.cuda.enable_mem_efficient_sdp(True)\n \n+if int(torch_version[0]) == 2 and int(torch_version[2]) >= 5:\n+    torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)",
        "comment_created_at": "2024-12-20T23:09:36+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "There are no situations where the math backend is actually used by ComfyUI unless you force it.",
        "pr_file_module": null
      },
      {
        "comment_id": "1894530402",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 6112,
        "pr_file": "comfy/model_management.py",
        "discussion_id": "1894488730",
        "commented_code": "@@ -214,12 +214,14 @@ def is_nvidia():\n if args.cpu_vae:\n     VAE_DTYPES = [torch.float32]\n \n-\n if ENABLE_PYTORCH_ATTENTION:\n     torch.backends.cuda.enable_math_sdp(True)\n     torch.backends.cuda.enable_flash_sdp(True)\n     torch.backends.cuda.enable_mem_efficient_sdp(True)\n \n+if int(torch_version[0]) == 2 and int(torch_version[2]) >= 5:\n+    torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)",
        "comment_created_at": "2024-12-21T01:39:12+00:00",
        "comment_author": "simonlui",
        "comment_body": "I should've explained this better. For any non-Nvidia GPUs, the math backend is what ends up being used if Pytorch Attention is selected since the Flash Attention and mem efficient backends are CUDA only, with AMD's implementations for both only making it around 1-2 weeks ago in the nightly packages. I can try and gate this off a bit better if you want, but the Pytorch change mentioned does slow things down for GPUs that are stuck in that situation like Intel right now.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1494000641",
    "pr_number": 2829,
    "pr_file": "comfy/model_management.py",
    "created_at": "2024-02-19T04:42:46+00:00",
    "commented_code": "return \"CUDA {}: {}\".format(device, torch.cuda.get_device_name(device))\n \n try:\n-    print(\"Device:\", get_torch_device_name(get_torch_device()))\n+    torch_device_name = get_torch_device_name(get_torch_device())\n+\n+    if \"[ZLUDA]\" in torch_device_name:\n+        print(\"Detected ZLUDA, support for it is experimental and comfy may not work properly.\")\n+\n+        if torch.backends.cudnn.enabled:\n+            torch.backends.cudnn.enabled = False\n+            print(\"Disabling cuDNN because ZLUDA does currently not support it.\")\n+\n+        torch.backends.cuda.enable_flash_sdp(True)\n+        torch.backends.cuda.enable_math_sdp(False)\n+        torch.backends.cuda.enable_mem_efficient_sdp(False)\n+\n+        if ENABLE_PYTORCH_ATTENTION:\n+            print(\"Disabling pytorch cross attention because ZLUDA does currently not support it.\")\n+            ENABLE_PYTORCH_ATTENTION = False",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1494000641",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 2829,
        "pr_file": "comfy/model_management.py",
        "discussion_id": "1494000641",
        "commented_code": "@@ -254,7 +254,24 @@ def get_torch_device_name(device):\n         return \"CUDA {}: {}\".format(device, torch.cuda.get_device_name(device))\n \n try:\n-    print(\"Device:\", get_torch_device_name(get_torch_device()))\n+    torch_device_name = get_torch_device_name(get_torch_device())\n+\n+    if \"[ZLUDA]\" in torch_device_name:\n+        print(\"Detected ZLUDA, support for it is experimental and comfy may not work properly.\")\n+\n+        if torch.backends.cudnn.enabled:\n+            torch.backends.cudnn.enabled = False\n+            print(\"Disabling cuDNN because ZLUDA does currently not support it.\")\n+\n+        torch.backends.cuda.enable_flash_sdp(True)\n+        torch.backends.cuda.enable_math_sdp(False)\n+        torch.backends.cuda.enable_mem_efficient_sdp(False)\n+\n+        if ENABLE_PYTORCH_ATTENTION:\n+            print(\"Disabling pytorch cross attention because ZLUDA does currently not support it.\")\n+            ENABLE_PYTORCH_ATTENTION = False",
        "comment_created_at": "2024-02-19T04:42:46+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "Why are you disabling pytorch attention after doing torch.backends.cuda.enable_flash_sdp(True) above?",
        "pr_file_module": null
      },
      {
        "comment_id": "1494175939",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 2829,
        "pr_file": "comfy/model_management.py",
        "discussion_id": "1494000641",
        "commented_code": "@@ -254,7 +254,24 @@ def get_torch_device_name(device):\n         return \"CUDA {}: {}\".format(device, torch.cuda.get_device_name(device))\n \n try:\n-    print(\"Device:\", get_torch_device_name(get_torch_device()))\n+    torch_device_name = get_torch_device_name(get_torch_device())\n+\n+    if \"[ZLUDA]\" in torch_device_name:\n+        print(\"Detected ZLUDA, support for it is experimental and comfy may not work properly.\")\n+\n+        if torch.backends.cudnn.enabled:\n+            torch.backends.cudnn.enabled = False\n+            print(\"Disabling cuDNN because ZLUDA does currently not support it.\")\n+\n+        torch.backends.cuda.enable_flash_sdp(True)\n+        torch.backends.cuda.enable_math_sdp(False)\n+        torch.backends.cuda.enable_mem_efficient_sdp(False)\n+\n+        if ENABLE_PYTORCH_ATTENTION:\n+            print(\"Disabling pytorch cross attention because ZLUDA does currently not support it.\")\n+            ENABLE_PYTORCH_ATTENTION = False",
        "comment_created_at": "2024-02-19T08:32:44+00:00",
        "comment_author": "LeagueRaINi",
        "comment_body": "That's from vlad, they can all 3 be disabled atm i guess cause using sdp causes a driver timeout\n<https://github.com/vladmandic/automatic/blob/2785af1bc4bff2041d09f5cd9ff7877709c75761/modules/zluda.py>",
        "pr_file_module": null
      },
      {
        "comment_id": "1494236745",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 2829,
        "pr_file": "comfy/model_management.py",
        "discussion_id": "1494000641",
        "commented_code": "@@ -254,7 +254,24 @@ def get_torch_device_name(device):\n         return \"CUDA {}: {}\".format(device, torch.cuda.get_device_name(device))\n \n try:\n-    print(\"Device:\", get_torch_device_name(get_torch_device()))\n+    torch_device_name = get_torch_device_name(get_torch_device())\n+\n+    if \"[ZLUDA]\" in torch_device_name:\n+        print(\"Detected ZLUDA, support for it is experimental and comfy may not work properly.\")\n+\n+        if torch.backends.cudnn.enabled:\n+            torch.backends.cudnn.enabled = False\n+            print(\"Disabling cuDNN because ZLUDA does currently not support it.\")\n+\n+        torch.backends.cuda.enable_flash_sdp(True)\n+        torch.backends.cuda.enable_math_sdp(False)\n+        torch.backends.cuda.enable_mem_efficient_sdp(False)\n+\n+        if ENABLE_PYTORCH_ATTENTION:\n+            print(\"Disabling pytorch cross attention because ZLUDA does currently not support it.\")\n+            ENABLE_PYTORCH_ATTENTION = False",
        "comment_created_at": "2024-02-19T09:20:03+00:00",
        "comment_author": "comfyanonymous",
        "comment_body": "Oh I see he only enables the math one:  torch.backends.cuda.enable_math_sdp(True)\r\nAnd disables the others that makes more sense.",
        "pr_file_module": null
      },
      {
        "comment_id": "1494269902",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 2829,
        "pr_file": "comfy/model_management.py",
        "discussion_id": "1494000641",
        "commented_code": "@@ -254,7 +254,24 @@ def get_torch_device_name(device):\n         return \"CUDA {}: {}\".format(device, torch.cuda.get_device_name(device))\n \n try:\n-    print(\"Device:\", get_torch_device_name(get_torch_device()))\n+    torch_device_name = get_torch_device_name(get_torch_device())\n+\n+    if \"[ZLUDA]\" in torch_device_name:\n+        print(\"Detected ZLUDA, support for it is experimental and comfy may not work properly.\")\n+\n+        if torch.backends.cudnn.enabled:\n+            torch.backends.cudnn.enabled = False\n+            print(\"Disabling cuDNN because ZLUDA does currently not support it.\")\n+\n+        torch.backends.cuda.enable_flash_sdp(True)\n+        torch.backends.cuda.enable_math_sdp(False)\n+        torch.backends.cuda.enable_mem_efficient_sdp(False)\n+\n+        if ENABLE_PYTORCH_ATTENTION:\n+            print(\"Disabling pytorch cross attention because ZLUDA does currently not support it.\")\n+            ENABLE_PYTORCH_ATTENTION = False",
        "comment_created_at": "2024-02-19T09:41:40+00:00",
        "comment_author": "LeagueRaINi",
        "comment_body": "Does indeed, dont know how I didn't spot that I switched that up",
        "pr_file_module": null
      }
    ]
  }
]