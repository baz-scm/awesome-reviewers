[
  {
    "discussion_id": "1990406519",
    "pr_number": 7032,
    "pr_file": "comfy/ldm/modules/attention.py",
    "created_at": "2025-03-12T01:50:19+00:00",
    "commented_code": "for p in patch:\n                 n = p(n, extra_options)\n \n-        x += n\n+        x = n + x\n         if self.is_res:\n             x_skip = x\n         x = self.ff(self.norm3(x))\n         if self.is_res:\n-            x += x_skip\n+            x = x_skip + x",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "1990406519",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 7032,
        "pr_file": "comfy/ldm/modules/attention.py",
        "discussion_id": "1990406519",
        "commented_code": "@@ -713,12 +713,12 @@ def forward(self, x, context=None, transformer_options={}):\n             for p in patch:\n                 n = p(n, extra_options)\n \n-        x += n\n+        x = n + x\n         if self.is_res:\n             x_skip = x\n         x = self.ff(self.norm3(x))\n         if self.is_res:\n-            x += x_skip\n+            x = x_skip + x",
        "comment_created_at": "2025-03-12T01:50:19+00:00",
        "comment_author": "yewentao256",
        "comment_body": "I am thinking if we could still use the inplace operator when it is not a leaf node.\r\n\r\nEg:\r\n\r\n```py\r\nimport torch\r\n\r\ndef add_1_optimized(x: torch.Tensor) -> torch.Tensor:\r\n    if x.is_leaf:\r\n        x = x + 1\r\n    else:\r\n        x += 1\r\n    return x\r\n\r\ndef add_1(x: torch.Tensor) -> torch.Tensor:\r\n    return x + 1\r\n\r\nimport time\r\n\r\nx = torch.randn(100, 200, 200, 200, requires_grad=True)\r\nstart = time.time()\r\ny = add_1_optimized(x)\r\nz = add_1_optimized(y)\r\nassert id(y) == id(z)\r\ng = add_1_optimized(z)\r\nh = add_1_optimized(g)\r\ni = add_1_optimized(h)\r\nprint('Time taken for add_1_optimized:', time.time() - start)\r\n\r\nstart = time.time()\r\ni.sum().backward()\r\nprint('Time taken for add_1_optimized backward:', time.time() - start)\r\n\r\nx = torch.randn(100, 200, 200, 200, requires_grad=True)\r\nstart = time.time()\r\ny = add_1(x)\r\nz = add_1(y)\r\ng = add_1(z)\r\nh = add_1(g)\r\ni = add_1(h)\r\nprint('Time taken for add_1:', time.time() - start)\r\nstart = time.time()\r\ni.sum().backward()\r\nprint('Time taken for add_1 backward:', time.time() - start)\r\n```\r\n\r\nResult:\r\n\r\n```bash\r\nTime taken for add_1_optimized: 1.166808843612671\r\nTime taken for add_1_optimized backward: 0.3505527973175049\r\nTime taken for add_1: 1.8314952850341797\r\nTime taken for add_1 backward: 0.395704984664917\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2106220769",
    "pr_number": 8258,
    "pr_file": "latent_preview.py",
    "created_at": "2025-05-25T14:37:49+00:00",
    "commented_code": "import folder_paths\n import comfy.utils\n import logging\n+from contextlib import nullcontext\n+import threading\n \n MAX_PREVIEW_RESOLUTION = args.preview_size\n \n-def preview_to_image(latent_image):\n-        latents_ubyte = (((latent_image + 1.0) / 2.0).clamp(0, 1)  # change scale from -1..1 to 0..1\n-                            .mul(0xFF)  # to 0..255\n-                            )\n-        if comfy.model_management.directml_enabled:\n-                latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\n-        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n+if args.preview_stream:\n+    preview_stream = torch.cuda.Stream()\n+    preview_context = torch.cuda.stream(preview_stream)\n+else:\n+    preview_context = nullcontext()\n \n-        return Image.fromarray(latents_ubyte.numpy())\n+def preview_to_image(preview_image: torch.Tensor):\n+        # no reason why any of this has to happen on GPU, also non-blocking transfers to cpu aren't safe ever\n+        # but we don't care about it blocking because the main stream is fine\n+        preview_image = preview_image.cpu()\n+\n+        preview_image.add_(1.0)\n+        preview_image.div_(2.0)\n+        preview_image.clamp_(0, 1) # change scale from -1..1 to 0..1 and clamp\n+        preview_image.mul_(255.) # change to uint8 range\n+        preview_image.round_() # default behavior when casting is truncate which is wrong for image processing\n+",
    "repo_full_name": "comfyanonymous/ComfyUI",
    "discussion_comments": [
      {
        "comment_id": "2106220769",
        "repo_full_name": "comfyanonymous/ComfyUI",
        "pr_number": 8258,
        "pr_file": "latent_preview.py",
        "discussion_id": "2106220769",
        "commented_code": "@@ -6,18 +6,29 @@\n import folder_paths\n import comfy.utils\n import logging\n+from contextlib import nullcontext\n+import threading\n \n MAX_PREVIEW_RESOLUTION = args.preview_size\n \n-def preview_to_image(latent_image):\n-        latents_ubyte = (((latent_image + 1.0) / 2.0).clamp(0, 1)  # change scale from -1..1 to 0..1\n-                            .mul(0xFF)  # to 0..255\n-                            )\n-        if comfy.model_management.directml_enabled:\n-                latents_ubyte = latents_ubyte.to(dtype=torch.uint8)\n-        latents_ubyte = latents_ubyte.to(device=\"cpu\", dtype=torch.uint8, non_blocking=comfy.model_management.device_supports_non_blocking(latent_image.device))\n+if args.preview_stream:\n+    preview_stream = torch.cuda.Stream()\n+    preview_context = torch.cuda.stream(preview_stream)\n+else:\n+    preview_context = nullcontext()\n \n-        return Image.fromarray(latents_ubyte.numpy())\n+def preview_to_image(preview_image: torch.Tensor):\n+        # no reason why any of this has to happen on GPU, also non-blocking transfers to cpu aren't safe ever\n+        # but we don't care about it blocking because the main stream is fine\n+        preview_image = preview_image.cpu()\n+\n+        preview_image.add_(1.0)\n+        preview_image.div_(2.0)\n+        preview_image.clamp_(0, 1) # change scale from -1..1 to 0..1 and clamp\n+        preview_image.mul_(255.) # change to uint8 range\n+        preview_image.round_() # default behavior when casting is truncate which is wrong for image processing\n+",
        "comment_created_at": "2025-05-25T14:37:49+00:00",
        "comment_author": "blepping",
        "comment_body": "I think you can avoid an operation here if you do `.add_(1.0).clamp_(2.0).mul_(127.5).round_()` instead.\r\n\r\n```plaintext\r\n>>> x = torch.randn(10000)\r\n>>> r1 = x.add(1.0).div(2.0).clamp(0,1).mul(255.).round()\r\n>>> r2 = x.add(1.0).clamp(0,2).mul(127.5).round()\r\n>>> torch.equal(r1, r2)\r\nTrue\r\n```\r\n```",
        "pr_file_module": null
      }
    ]
  }
]