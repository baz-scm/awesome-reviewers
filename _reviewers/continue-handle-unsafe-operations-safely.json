[
  {
    "discussion_id": "2151828591",
    "pr_number": 6164,
    "pr_file": "packages/fetch/src/certs.ts",
    "created_at": "2025-06-17T09:46:03+00:00",
    "commented_code": "+import { globalAgent } from \"https\";\n+import * as fs from \"node:fs\";\n+import tls from \"node:tls\";\n+\n+/**\n+ * Extracts content from either a file path or data URI\n+ */\n+export function getCertificateContent(input: string): string {\n+  if (input.startsWith(\"data:\")) {\n+    // Parse data URI: data:[<mediatype>][;base64],<data>\n+    const [header, data] = input.split(\",\");\n+    if (header.includes(\"base64\")) {\n+      return Buffer.from(data, \"base64\").toString(\"utf8\");",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2151828591",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6164,
        "pr_file": "packages/fetch/src/certs.ts",
        "discussion_id": "2151828591",
        "commented_code": "@@ -0,0 +1,129 @@\n+import { globalAgent } from \"https\";\n+import * as fs from \"node:fs\";\n+import tls from \"node:tls\";\n+\n+/**\n+ * Extracts content from either a file path or data URI\n+ */\n+export function getCertificateContent(input: string): string {\n+  if (input.startsWith(\"data:\")) {\n+    // Parse data URI: data:[<mediatype>][;base64],<data>\n+    const [header, data] = input.split(\",\");\n+    if (header.includes(\"base64\")) {\n+      return Buffer.from(data, \"base64\").toString(\"utf8\");",
        "comment_created_at": "2025-06-17T09:46:03+00:00",
        "comment_author": "recurseml[bot]",
        "comment_body": "No error handling for invalid base64 data. Buffer.from() can throw an exception if the input is not valid base64, which could crash the application. Should wrap in try/catch and handle malformed input gracefully.\n\n\ud83d\udcda [Relevant Docs](https://nodejs.org/api/buffer.html#static-method-bufferfromstring-encoding)\n\n---\n\n> *React with \ud83d\udc4d to tell me that this comment was useful, or \ud83d\udc4e if not (and I'll stop posting more comments like this in the future)*",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2216444647",
    "pr_number": 6395,
    "pr_file": "gui/src/redux/util/constructMessages.ts",
    "created_at": "2025-07-18T16:26:56+00:00",
    "commented_code": "message: item.message,\n       });\n     } else if (item.message.role === \"assistant\") {\n-      msgs.push({\n-        ctxItems: item.contextItems,\n-        message: item.message,\n-      });\n-\n-      // Add a tool message for each tool call\n-      if (item.message.toolCalls?.length) {\n-        // If the assistant message has tool calls, we need to insert tool messages\n-        for (const toolCall of item.message.toolCalls) {\n-          let content: string = NO_TOOL_CALL_OUTPUT_MESSAGE;\n-\n-          // Find the corresponding tool call state for this specific tool call\n-          const toolCallState = item.toolCallStates?.find(\n-            (state) => state.toolCallId === toolCall.id,\n+      if (item.message.toolCalls?.length && useSystemMessageTools) {",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2216444647",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6395,
        "pr_file": "gui/src/redux/util/constructMessages.ts",
        "discussion_id": "2216444647",
        "commented_code": "@@ -98,50 +95,66 @@ export function constructMessages(\n         message: item.message,\n       });\n     } else if (item.message.role === \"assistant\") {\n-      msgs.push({\n-        ctxItems: item.contextItems,\n-        message: item.message,\n-      });\n-\n-      // Add a tool message for each tool call\n-      if (item.message.toolCalls?.length) {\n-        // If the assistant message has tool calls, we need to insert tool messages\n-        for (const toolCall of item.message.toolCalls) {\n-          let content: string = NO_TOOL_CALL_OUTPUT_MESSAGE;\n-\n-          // Find the corresponding tool call state for this specific tool call\n-          const toolCallState = item.toolCallStates?.find(\n-            (state) => state.toolCallId === toolCall.id,\n+      if (item.message.toolCalls?.length && useSystemMessageTools) {",
        "comment_created_at": "2025-07-18T16:26:56+00:00",
        "comment_author": "cubic-dev-ai[bot]",
        "comment_body": "<!-- metadata:{\"confidence\":6,\"steps\":[{\"text\":\"\",\"toolCalls\":[{\"args\":{\"command\":\"grep -R \\\"export const NO_TOOL_CALL_OUTPUT_MESSAGE\\\" -n\"},\"toolName\":\"executeCommand\"}]},{\"text\":\"\",\"toolCalls\":[{\"args\":{\"command\":\"grep -n \\\"useSystemMessageTools = false\\\" -n gui/src/redux/util/constructMessages.ts\"},\"toolName\":\"executeCommand\"}]},{\"text\":\"\",\"toolCalls\":[{\"args\":{\"command\":\"grep -R \\\"convertToolCallStatesToSystemCallsAndOutput\\\" -n\"},\"toolName\":\"executeCommand\"}]},{\"text\":\"\",\"toolCalls\":[{\"args\":{\"command\":\"sed -n '35,100p' core/tools/systemMessageTools/textifySystemTools.ts\"},\"toolName\":\"executeCommand\"}]}]} -->\nWhen toolCallStates is undefined or empty, convertToolCallStatesToSystemCallsAndOutput is still invoked, which inserts a spurious user message containing \"Error: no tool output for tool calls\". Only convert when there is at least one toolCallState to avoid misleading messages.\n\n<details>\n<summary>Prompt for AI agents</summary>\n\n```\nAddress the following comment on gui/src/redux/util/constructMessages.ts at line 98:\n\n<comment>When toolCallStates is undefined or empty, convertToolCallStatesToSystemCallsAndOutput is still invoked, which inserts a spurious user message containing &quot;Error: no tool output for tool calls&quot;. Only convert when there is at least one toolCallState to avoid misleading messages.</comment>\n\n<file context>\n@@ -2,30 +2,26 @@ import {\n   ChatHistoryItem,\n   ChatMessage,\n   ContextItemWithId,\n-  ModelDescription,\n   RuleWithSource,\n   TextMessagePart,\n   ToolResultChatMessage,\n   UserChatMessage,\n } from &quot;core&quot;;\n-import {\n-  DEFAULT_AGENT_SYSTEM_MESSAGE,\n-  DEFAULT_CHAT_SYSTEM_MESSAGE,\n-  DEFAULT_PLAN_SYSTEM_MESSAGE,\n-} from &quot;core/llm/defaultSystemMessages&quot;;\n import { chatMessageIsEmpty } from &quot;core/llm/messages&quot;;\n import { getSystemMessageWithRules } from &quot;core/llm/rules/getSystemMessageWithRules&quot;;\n import { RulePolicies } from &quot;core/llm/rules/types&quot;;\n+import {\n+  CANCELLED_TOOL_CALL_MESSAGE,\n+  NO_TOOL_CALL_OUTPUT_MESSAGE,\n+} from &quot;core/tools/constants&quot;;\n+import { convertToolCallStatesToSystemCallsAndOutput } from &quot;core/tools/systemMessageTools/textifySystemTools&quot;;\n import { findLast, findLastIndex } from &quot;core/util/findLast&quot;;\n import {\n   normalizeToMessageParts,\n   renderContextItems,\n } from &quot;core/util/messageContent&quot;;\n import { toolCallStateToContextItems } from &quot;../../pages/gui/ToolCallDiv/utils&quot;;\n \n-export const NO_TOOL_CALL_OUTPUT_MESSAGE = &quot;No tool output&quot;;\n-export const CANCELLED_TOOL_CALL_MESSAGE = &quot;The user cancelled this tool call.&quot;;\n-\n interface MessageWithContextItems {\n   ctxItems: ContextItemWithId[];\n   message: ChatMessage;\n@@ -35,6 +31,7 @@ export function constructMessages(\n   baseSystemMessage: string | undefined,\n   availableRules: RuleWithSource[],\n   rulePolicies: RulePolicies,\n+  useSystemMessageTools = false,\n ): {\n   messages: ChatMessage[];\n   appliedRules: RuleWithSource[];\n@@ -98,50 +95,66 @@ export function constructMessages(\n         message: item.message,\n       });\n     } else if (item.message.role === &quot;assistant&quot;) {\n-      msgs.push({\n-        ctxItems: item.contextItems,\n-        message: item.message,\n-      });\n-\n-      // Add a tool message for each tool call\n-      if (item.message.toolCalls?.length) {\n-        // If the assistant message has tool calls, we need to insert tool messages\n-        for (const toolCall of item.message.toolCalls) {\n-          let content: string = NO_TOOL_CALL_OUTPUT_MESSAGE;\n-\n-          // Find the corresponding tool call state for this specific tool call\n-          const toolCallState = item.toolCallStates?.find(\n-            (state) =&gt; state.toolCallId === toolCall.id,\n+      if (item.message.toolCalls?.length &amp;&amp; useSystemMessageTools) {\n+        const { userMessage, assistantMessage } =\n+          convertToolCallStatesToSystemCallsAndOutput(\n+            item.message,\n+            item.toolCallStates ?? [],\n           );\n-\n-          if (toolCallState?.status === &quot;canceled&quot;) {\n-            content = CANCELLED_TOOL_CALL_MESSAGE;\n-          } else if (toolCallState?.output) {\n-            content = renderContextItems(toolCallState.output);\n+        msgs.push({\n+          message: assistantMessage,\n+          ctxItems: [],\n+        });\n+        msgs.push({\n+          message: userMessage,\n+          ctxItems: [],\n+        });\n+      } else {\n+        msgs.push({\n+          ctxItems: item.contextItems,\n+          message: item.message,\n+        });\n+\n+        // Add a tool message for each tool call\n+        if (item.message.toolCalls?.length) {\n+          // If the assistant message has tool calls, we need to insert tool messages\n+          for (const toolCall of item.message.toolCalls) {\n+            let content: string = NO_TOOL_CALL_OUTPUT_MESSAGE;\n+\n+            // Find the corresponding tool call state for this specific tool call\n+            const toolCallState = item.toolCallStates?.find(\n+              (state) =&gt; state.toolCallId === toolCall.id,\n+            );\n+\n+            if (toolCallState?.status === &quot;canceled&quot;) {\n+              content = CANCELLED_TOOL_CALL_MESSAGE;\n+            } else if (toolCallState?.output) {\n+              content = renderContextItems(toolCallState.output);\n+            }\n+\n+            msgs.push({\n+              ctxItems: toolCallStateToContextItems(toolCallState),\n+              message: {\n+                role: &quot;tool&quot;,\n+                content,\n+                toolCallId: toolCall.id!,\n+              },\n+            });\n           }\n-\n-          msgs.push({\n-            ctxItems: toolCallStateToContextItems(toolCallState),\n-            message: {\n-              role: &quot;tool&quot;,\n-              content,\n-              toolCallId: toolCall.id!,\n+        } else if (item.toolCallStates &amp;&amp; item.toolCallStates.length &gt; 0) {\n+          // This case indicates a potential mismatch - we have tool call states but no message.toolCalls\n+          console.error(\n+            &quot;ERROR constructMessages: Assistant message has toolCallStates but no message.toolCalls:&quot;,\n+            {\n+              toolCallStates: item.toolCallStates.length,\n+              toolCallIds: item.toolCallStates.map((s) =&gt; s.toolCallId),\n+              messageContent:\n+                typeof item.message.content === &quot;string&quot;\n+                  ? item.message.content?.substring(0, 50) + &quot;...&quot;\n+                  : &quot;Non-string content&quot;,\n             },\n-          });\n+          );\n         }\n-      } else if (item.toolCallStates &amp;&amp; item.toolCallStates.length &gt; 0) {\n-        // This case indicates a potential mismatch - we have tool call states but no message.toolCalls\n-        console.error(\n-          &quot;ERROR constructMessages: Assistant message has toolCallStates but no message.toolCalls:&quot;,\n-          {\n-            toolCallStates: item.toolCallStates.length,\n-            toolCallIds: item.toolCallStates.map((s) =&gt; s.toolCallId),\n-            messageContent:\n-              typeof item.message.content === &quot;string&quot;\n-                ? item.message.content?.substring(0, 50) + &quot;...&quot;\n-                : &quot;Non-string content&quot;,\n-          },\n-        );\n       }\n     }\n   }\n@@ -199,16 +212,3 @@ export function constructMessages(\n     appliedRuleIndex,\n   };\n }\n-\n-export function getBaseSystemMessage(\n-  messageMode: string,\n-  model: ModelDescription,\n-): string {\n-  if (messageMode === &quot;agent&quot;) {\n-    return model.baseAgentSystemMessage ?? DEFAULT_AGENT_SYSTEM_MESSAGE;\n-  } else if (messageMode === &quot;plan&quot;) {\n-    return model.basePlanSystemMessage ?? DEFAULT_PLAN_SYSTEM_MESSAGE;\n-  } else {\n-    return model.baseChatSystemMessage ?? DEFAULT_CHAT_SYSTEM_MESSAGE;\n-  }\n-}\n</file context>\n```\n\n</details>\n\n```suggestion\n      if (item.message.toolCalls?.length && useSystemMessageTools && item.toolCallStates?.length) {\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2163813041",
    "pr_number": 6274,
    "pr_file": "core/llm/llms/AICore/AICoreClaudeProvider.ts",
    "created_at": "2025-06-24T12:13:50+00:00",
    "commented_code": "+import fs from \"fs\";\n+import os from \"os\";\n+import path from \"path\";\n+import { ChatMessage, CompletionOptions, LLMOptions } from \"../../../index.js\";\n+import { BaseLLM } from \"../../index.js\";\n+import { stripImages } from \"../../../util/messageContent.js\";\n+import { ContentBlock, Message, ToolConfiguration } from \"@aws-sdk/client-bedrock-runtime\";\n+import { getSecureID } from \"../../utils/getSecureID.js\";\n+import SAPClaudeClient, { SAPClaudeClientOptions } from \"./SAPClaudeClient.js\";\n+\n+const AI_CORE_CREDS_FILENAME = \"ai-core-creds.json\"\n+\n+interface ToolUseState {\n+    toolUseId: string;\n+    name: string;\n+    input: string;\n+}\n+\n+export class AICoreClaudeProvider extends BaseLLM {\n+    private creds?: SAPClaudeClientOptions;\n+    static providerName = \"aiCore\";\n+    static defaultOptions: Partial<LLMOptions> = {\n+        model: \"anthropic--claude-3.7-sonnet\",\n+        contextLength: 128_000,\n+        completionOptions: {\n+            model: \"anthropic--claude-3.7-sonnet\",\n+            maxTokens: 4096,\n+        },\n+    };\n+    private _currentToolResponse: Partial<ToolUseState> | null = null;\n+\n+    constructor(options: LLMOptions) {\n+        super(options);\n+        this.setupAiCore()\n+    }\n+\n+\n+\n+    /**\n+     * Generates the input payload for the Bedrock Converse API\n+     * @param messages - Array of chat messages\n+     * @param options - Completion options\n+     * @returns Formatted input payload for the API\n+     */\n+    private _generateConverseInput(\n+        messages: ChatMessage[],\n+        options: CompletionOptions,\n+    ): any {\n+        const systemMessage = stripImages(\n+            messages.find((m) => m.role === \"system\")?.content ?? \"\",\n+        );\n+        const convertedMessages = this._convertMessages(messages);\n+\n+        const shouldCacheSystemMessage =\n+            (!!systemMessage && this.cacheBehavior?.cacheSystemMessage) ||\n+            this.completionOptions.promptCaching;\n+        const enablePromptCaching =\n+            shouldCacheSystemMessage ||\n+            this.cacheBehavior?.cacheConversation ||\n+            this.completionOptions.promptCaching;\n+        const shouldCacheToolsConfig = this.completionOptions.promptCaching;\n+\n+\n+\n+        const supportsTools = true;\n+\n+        let toolConfig =\n+            supportsTools && options.tools\n+                ? ({\n+                    tools: options.tools.map((tool) => ({\n+                        toolSpec: {\n+                            name: tool.function.name,\n+                            description: tool.function.description,\n+                            inputSchema: {\n+                                json: tool.function.parameters,\n+                            },\n+                        },\n+                    })),\n+                } as ToolConfiguration)\n+                : undefined;\n+\n+        if (toolConfig?.tools && shouldCacheToolsConfig) {\n+            toolConfig.tools.push({ cachePoint: { type: \"default\" } });\n+        }\n+\n+        if (toolConfig)\n+            return {\n+                toolConfig: toolConfig,\n+                system: systemMessage\n+                    ? shouldCacheSystemMessage\n+                        ? [{ text: systemMessage }, { cachePoint: { type: \"default\" } }]\n+                        : [{ text: systemMessage }]\n+                    : undefined,\n+                messages: convertedMessages,\n+                inferenceConfig: {\n+                    maxTokens: 4096,\n+                    temperature: 0,\n+\n+                },\n+            };\n+        else\n+            return {\n+                messages: convertedMessages,\n+                system: systemMessage\n+                    ? shouldCacheSystemMessage\n+                        ? [{ text: systemMessage }, { cachePoint: { type: \"default\" } }]\n+                        : [{ text: systemMessage }]\n+                    : undefined,\n+                inferenceConfig: {\n+                    maxTokens: 8192,\n+                    temperature: 0,\n+\n+                },\n+            };\n+    }\n+    private _convertMessages(messages: ChatMessage[]): any[] {\n+        const filteredmessages = messages.filter(\n+            (m) => m.role !== \"system\" && !!m.content,\n+        );\n+        const lastTwoUserMsgIndices = filteredmessages\n+            .map((msg, index) => (msg.role === \"user\" ? index : -1))\n+            .filter((index) => index !== -1)\n+            .slice(-2);\n+\n+        const converted = filteredmessages\n+            .map((message, filteredMsgIdx) => {\n+                // Add cache_control parameter to the last two user messages\n+                // The second-to-last because it retrieves potentially already cached contents,\n+                // The last one because we want it cached for later retrieval.\n+                // See: https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html\n+                const addCaching =\n+                    this.cacheBehavior?.cacheConversation &&\n+                    lastTwoUserMsgIndices.includes(filteredMsgIdx);\n+\n+                try {\n+                    return this._convertMessage(message, addCaching);\n+                } catch (error) {\n+                    console.error(`Failed to convert message: ${error}`);\n+                    return null;\n+                }\n+            })\n+            .filter(Boolean);\n+\n+        return converted;\n+    }\n+\n+    private _convertMessage(\n+        message: ChatMessage,\n+        addCaching: boolean = false,\n+    ): Message | null {\n+        // Handle system messages explicitly\n+        if (message.role === \"system\") {\n+            return null;\n+        }\n+\n+        const cachePoint = addCaching\n+            ? { cachePoint: { type: \"default\" } }\n+            : undefined;\n+\n+        // Tool response handling\n+        if (message.role === \"tool\") {\n+            return {\n+                role: \"user\",\n+                content: [\n+                    {\n+                        toolResult: {\n+                            toolUseId: message.toolCallId,\n+                            content: [\n+                                {\n+                                    text: message.content || \"\",\n+                                },\n+                            ],\n+                        },\n+                    },\n+                ],\n+            };\n+        }\n+\n+        // Tool calls handling\n+        if (message.role === \"assistant\" && message.toolCalls) {\n+            return {\n+                role: \"assistant\",\n+                content: message.toolCalls.map((toolCall) => ({\n+                    toolUse: {\n+                        toolUseId: toolCall.id,\n+                        name: toolCall.function?.name,\n+                        input: JSON.parse(toolCall.function?.arguments || \"{}\"),\n+                    },\n+                })),\n+            };\n+        }\n+\n+        if (message.role === \"thinking\") {\n+            if (message.redactedThinking) {\n+                const content: ContentBlock.ReasoningContentMember = {\n+                    reasoningContent: {\n+                        redactedContent: new Uint8Array(\n+                            Buffer.from(message.redactedThinking),\n+                        ),\n+                    },\n+                };\n+                return {\n+                    role: \"assistant\",\n+                    content: [content],\n+                };\n+            } else {\n+                const content: ContentBlock.ReasoningContentMember = {\n+                    reasoningContent: {\n+                        reasoningText: {\n+                            text: (message.content as string) || \"\",\n+                            signature: message.signature,\n+                        },\n+                    },\n+                };\n+                return {\n+                    role: \"assistant\",\n+                    content: [content],\n+                };\n+            }\n+        }\n+\n+        // Standard text message\n+        if (typeof message.content === \"string\") {\n+            if (addCaching) {\n+                message.content += getSecureID();\n+            }\n+            const content: any[] = [{ text: message.content }];\n+            if (addCaching) {\n+                content.push({ cachePoint: { type: \"default\" } });\n+            }\n+            return {\n+                role: message.role,\n+                content,\n+            };\n+        }\n+\n+        // Improved multimodal content handling\n+        if (Array.isArray(message.content)) {\n+            const content: any[] = [];\n+\n+            // Process all parts first\n+            message.content.forEach((part) => {\n+                if (part.type === \"text\") {\n+                    if (addCaching) {\n+                        part.text += getSecureID();\n+                    }\n+                    content.push({ text: part.text });\n+                } else if (part.type === \"imageUrl\" && part.imageUrl) {\n+                    try {\n+                        const [mimeType, base64Data] = part.imageUrl.url.split(\",\");\n+                        const format = mimeType.split(\"/\")[1]?.split(\";\")[0] || \"jpeg\";\n+                        content.push({\n+                            image: {\n+                                format,\n+                                source: {\n+                                    bytes: Buffer.from(base64Data, \"base64\"),\n+                                },\n+                            },\n+                        });\n+                    } catch (error) {\n+                        console.warn(`Failed to process image: ${error}`);\n+                    }\n+                }\n+            });\n+\n+            // Add cache point as a separate block at the end if needed\n+            if (addCaching && content.length > 0) {\n+                content.push({ cachePoint: { type: \"default\" } });\n+            }\n+\n+            return {\n+                role: message.role,\n+                content,\n+            } as Message;\n+        }\n+        return null;\n+    }\n+\n+\n+    protected async *_streamComplete(\n+        prompt: string,\n+        signal: AbortSignal,\n+        options: CompletionOptions,\n+    ): AsyncGenerator<string> {\n+        const messages = [{ role: \"user\" as const, content: prompt }];\n+        for await (const update of this._streamChat(messages, signal, options)) {\n+            const content = update.content;\n+            if (Array.isArray(content)) {\n+                for (const chunk of content) {\n+                    if (chunk.type === \"text\") {\n+                        yield chunk.text;\n+                    }\n+                }\n+            }\n+            else {\n+                yield content\n+            }\n+\n+        }\n+    }\n+    async *_streamChat(\n+        messages: ChatMessage[],\n+        signal: AbortSignal,\n+        options: CompletionOptions,\n+    ): AsyncGenerator<ChatMessage> {\n+        if(!this.creds){\n+            this.creds = this.setupAiCore()\n+        }\n+        const client = new SAPClaudeClient(this.creds);\n+        const input = this._generateConverseInput(messages, {\n+            ...options,\n+            stream: true,\n+        });\n+        const response = await client.sendMessage(input);\n+        const stream = response.data\n+\n+        function toStrictJson(str: string): string {\n+            // Wrap it in parentheses so JS will treat it as an expression\n+            const obj = new Function(\"return \" + str)()\n+            return JSON.stringify(obj)\n+        }\n+\n+\n+        // Buffer for accumulating stream data\n+        let buffer = '';\n+        \n+        try {\n+            for await (const chunk_str of stream) {\n+                buffer += chunk_str.toString();\n+                \n+                // Process complete JSON objects from the buffer\n+                let startIdx = 0;\n+                while (true) {\n+                    // Find the next \"data: \" marker\n+                    const dataPrefix = \"data: \";\n+                    const dataIdx = buffer.indexOf(dataPrefix, startIdx);\n+                    if (dataIdx === -1) break;\n+                    \n+                    // Find the end of this JSON object (next data marker or end of buffer)\n+                    const nextDataIdx = buffer.indexOf(dataPrefix, dataIdx + dataPrefix.length);\n+                    \n+                    // If we found a complete object or end of stream marker\n+                    if (nextDataIdx !== -1 || buffer.endsWith(\"\n\n\")) {\n+                        const endIdx = nextDataIdx !== -1 ? nextDataIdx : buffer.length;\n+                        const jsonData = buffer.substring(dataIdx + dataPrefix.length, endIdx).trim();\n+                        \n+                        try {\n+                            // Try to parse as regular JSON first\n+                            let chunk;\n+                            try {\n+                                chunk = JSON.parse(jsonData);\n+                            } catch (e) {\n+                                // If that fails, try with the toStrictJson function\n+                                try {\n+                                    chunk = JSON.parse(toStrictJson(jsonData));\n+                                } catch (e2) {\n+                                    console.warn(\"Failed to parse JSON:\", jsonData);\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+                            }\n+                            \n+                            // Process the chunk as before\n+                            if (chunk.metadata?.usage) {\n+                                console.log(`${JSON.stringify(chunk.metadata.usage)}`);\n+                            }\n+\n+                            if (chunk.contentBlockDelta?.delta) {\n+                                const delta: any = chunk.contentBlockDelta.delta;\n+\n+                                // Handle text content\n+                                if (chunk.contentBlockDelta.delta.text) {\n+                                    yield {\n+                                        role: \"assistant\",\n+                                        content: chunk.contentBlockDelta.delta.text,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                // Handle text content\n+                                if ((chunk.contentBlockDelta.delta as any).reasoningContent?.text) {\n+                                    yield {\n+                                        role: \"thinking\",\n+                                        content: (chunk.contentBlockDelta.delta as any).reasoningContent\n+                                            .text,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                // Handle signature for thinking\n+                                if (delta.reasoningContent?.signature) {\n+                                    yield {\n+                                        role: \"thinking\",\n+                                        content: \"\",\n+                                        signature: delta.reasoningContent.signature,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                // Handle redacted thinking\n+                                if (delta.redactedReasoning?.data) {\n+                                    yield {\n+                                        role: \"thinking\",\n+                                        content: \"\",\n+                                        redactedThinking: delta.redactedReasoning.data,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                if (\n+                                    chunk.contentBlockDelta.delta.toolUse?.input &&\n+                                    this._currentToolResponse\n+                                ) {\n+                                    // Append the new input to the existing string\n+                                    // eslint-disable-next-line max-depth\n+                                    if (this._currentToolResponse.input === undefined) {\n+                                        this._currentToolResponse.input = \"\";\n+                                    }\n+                                    this._currentToolResponse.input +=\n+                                        chunk.contentBlockDelta.delta.toolUse.input;\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+                            }\n+\n+                            if (chunk.contentBlockStart?.start) {\n+                                const start: any = chunk.contentBlockStart.start;\n+                                if (start.redactedReasoning) {\n+                                    yield {\n+                                        role: \"thinking\",\n+                                        content: \"\",\n+                                        redactedThinking: start.redactedReasoning.data,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                const toolUse = chunk.contentBlockStart.start.toolUse;\n+                                if (toolUse?.toolUseId && toolUse?.name) {\n+                                    this._currentToolResponse = {\n+                                        toolUseId: toolUse.toolUseId,\n+                                        name: toolUse.name,\n+                                        input: \"\",\n+                                    };\n+                                }\n+                                startIdx = endIdx;\n+                                continue;\n+                            }\n+\n+                            if (chunk.contentBlockStop) {\n+                                if (this._currentToolResponse) {\n+                                    yield {\n+                                        role: \"assistant\",\n+                                        content: \"\",\n+                                        toolCalls: [\n+                                            {\n+                                                id: this._currentToolResponse.toolUseId,\n+                                                type: \"function\",\n+                                                function: {\n+                                                    name: this._currentToolResponse.name,\n+                                                    arguments: this._currentToolResponse.input,\n+                                                },\n+                                            },\n+                                        ],\n+                                    };\n+                                    this._currentToolResponse = null;\n+                                }\n+                                startIdx = endIdx;\n+                                continue;\n+                            }\n+                            \n+                            // If we get here, update startIdx to process the next chunk\n+                            startIdx = endIdx;\n+                            \n+                        } catch (error: unknown) {\n+                            console.warn(`Error processing chunk: ${error instanceof Error ? error.message : String(error)}`);\n+                            startIdx = endIdx;\n+                        }\n+                    } else {\n+                        // We have an incomplete object, wait for more data\n+                        break;\n+                    }\n+                }\n+                \n+                // Keep only the unprocessed part of the buffer\n+                if (startIdx > 0) {\n+                    buffer = buffer.substring(startIdx);\n+                }\n+            }\n+        } catch (error: unknown) {\n+            this._currentToolResponse = null;\n+            if (error instanceof Error) {\n+                if (\"code\" in error) {\n+                    // AWS SDK specific errors\n+                    throw new Error(\n+                        `AWS Bedrock stream error (${(error as any).code}): ${error.message}`,\n+                    );\n+                }\n+                throw new Error(`Error processing Bedrock stream: ${error.message}`);\n+            }\n+            throw new Error(\n+                \"Error processing Bedrock stream: Unknown error occurred\",\n+            );\n+        }\n+    }\n+\n+\n+    loadAiCoreCredentials(): SAPClaudeClientOptions | undefined {\n+        const credsFilePath = path.join(os.homedir(), AI_CORE_CREDS_FILENAME)\n+\n+        if (!fs.existsSync(credsFilePath)) {\n+            return undefined\n+        }\n+\n+        const fileContents = fs.readFileSync(credsFilePath, \"utf-8\")\n+        try {\n+            const parsed = JSON.parse(fileContents)\n+\n+            // Check and report missing credentials\n+            const missingCredentials = []\n+            if (!parsed.clientid) {\n+                missingCredentials.push(\"clientid\")\n+            }\n+            if (!parsed.clientsecret) {\n+                missingCredentials.push(\"clientsecret\")\n+            }\n+            if (!parsed.url) {\n+                missingCredentials.push(\"url\")\n+            }\n+            if (!parsed.serviceurls) {\n+                missingCredentials.push(\"serviceurls\")\n+            } else if (!parsed.serviceurls.AI_API_URL) {\n+                missingCredentials.push(\"serviceurls.AI_API_URL\")\n+            }\n+\n+            if (missingCredentials.length > 0) {\n+                throw new Error(`Credentials file is missing required properties: ${missingCredentials.join(\", \")}`)\n+            }\n+\n+            return parsed\n+        } catch (e) {\n+            throw new Error(\"Failed to parse ai core credentials file:\", e as any)",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2163813041",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6274,
        "pr_file": "core/llm/llms/AICore/AICoreClaudeProvider.ts",
        "discussion_id": "2163813041",
        "commented_code": "@@ -0,0 +1,563 @@\n+import fs from \"fs\";\n+import os from \"os\";\n+import path from \"path\";\n+import { ChatMessage, CompletionOptions, LLMOptions } from \"../../../index.js\";\n+import { BaseLLM } from \"../../index.js\";\n+import { stripImages } from \"../../../util/messageContent.js\";\n+import { ContentBlock, Message, ToolConfiguration } from \"@aws-sdk/client-bedrock-runtime\";\n+import { getSecureID } from \"../../utils/getSecureID.js\";\n+import SAPClaudeClient, { SAPClaudeClientOptions } from \"./SAPClaudeClient.js\";\n+\n+const AI_CORE_CREDS_FILENAME = \"ai-core-creds.json\"\n+\n+interface ToolUseState {\n+    toolUseId: string;\n+    name: string;\n+    input: string;\n+}\n+\n+export class AICoreClaudeProvider extends BaseLLM {\n+    private creds?: SAPClaudeClientOptions;\n+    static providerName = \"aiCore\";\n+    static defaultOptions: Partial<LLMOptions> = {\n+        model: \"anthropic--claude-3.7-sonnet\",\n+        contextLength: 128_000,\n+        completionOptions: {\n+            model: \"anthropic--claude-3.7-sonnet\",\n+            maxTokens: 4096,\n+        },\n+    };\n+    private _currentToolResponse: Partial<ToolUseState> | null = null;\n+\n+    constructor(options: LLMOptions) {\n+        super(options);\n+        this.setupAiCore()\n+    }\n+\n+\n+\n+    /**\n+     * Generates the input payload for the Bedrock Converse API\n+     * @param messages - Array of chat messages\n+     * @param options - Completion options\n+     * @returns Formatted input payload for the API\n+     */\n+    private _generateConverseInput(\n+        messages: ChatMessage[],\n+        options: CompletionOptions,\n+    ): any {\n+        const systemMessage = stripImages(\n+            messages.find((m) => m.role === \"system\")?.content ?? \"\",\n+        );\n+        const convertedMessages = this._convertMessages(messages);\n+\n+        const shouldCacheSystemMessage =\n+            (!!systemMessage && this.cacheBehavior?.cacheSystemMessage) ||\n+            this.completionOptions.promptCaching;\n+        const enablePromptCaching =\n+            shouldCacheSystemMessage ||\n+            this.cacheBehavior?.cacheConversation ||\n+            this.completionOptions.promptCaching;\n+        const shouldCacheToolsConfig = this.completionOptions.promptCaching;\n+\n+\n+\n+        const supportsTools = true;\n+\n+        let toolConfig =\n+            supportsTools && options.tools\n+                ? ({\n+                    tools: options.tools.map((tool) => ({\n+                        toolSpec: {\n+                            name: tool.function.name,\n+                            description: tool.function.description,\n+                            inputSchema: {\n+                                json: tool.function.parameters,\n+                            },\n+                        },\n+                    })),\n+                } as ToolConfiguration)\n+                : undefined;\n+\n+        if (toolConfig?.tools && shouldCacheToolsConfig) {\n+            toolConfig.tools.push({ cachePoint: { type: \"default\" } });\n+        }\n+\n+        if (toolConfig)\n+            return {\n+                toolConfig: toolConfig,\n+                system: systemMessage\n+                    ? shouldCacheSystemMessage\n+                        ? [{ text: systemMessage }, { cachePoint: { type: \"default\" } }]\n+                        : [{ text: systemMessage }]\n+                    : undefined,\n+                messages: convertedMessages,\n+                inferenceConfig: {\n+                    maxTokens: 4096,\n+                    temperature: 0,\n+\n+                },\n+            };\n+        else\n+            return {\n+                messages: convertedMessages,\n+                system: systemMessage\n+                    ? shouldCacheSystemMessage\n+                        ? [{ text: systemMessage }, { cachePoint: { type: \"default\" } }]\n+                        : [{ text: systemMessage }]\n+                    : undefined,\n+                inferenceConfig: {\n+                    maxTokens: 8192,\n+                    temperature: 0,\n+\n+                },\n+            };\n+    }\n+    private _convertMessages(messages: ChatMessage[]): any[] {\n+        const filteredmessages = messages.filter(\n+            (m) => m.role !== \"system\" && !!m.content,\n+        );\n+        const lastTwoUserMsgIndices = filteredmessages\n+            .map((msg, index) => (msg.role === \"user\" ? index : -1))\n+            .filter((index) => index !== -1)\n+            .slice(-2);\n+\n+        const converted = filteredmessages\n+            .map((message, filteredMsgIdx) => {\n+                // Add cache_control parameter to the last two user messages\n+                // The second-to-last because it retrieves potentially already cached contents,\n+                // The last one because we want it cached for later retrieval.\n+                // See: https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html\n+                const addCaching =\n+                    this.cacheBehavior?.cacheConversation &&\n+                    lastTwoUserMsgIndices.includes(filteredMsgIdx);\n+\n+                try {\n+                    return this._convertMessage(message, addCaching);\n+                } catch (error) {\n+                    console.error(`Failed to convert message: ${error}`);\n+                    return null;\n+                }\n+            })\n+            .filter(Boolean);\n+\n+        return converted;\n+    }\n+\n+    private _convertMessage(\n+        message: ChatMessage,\n+        addCaching: boolean = false,\n+    ): Message | null {\n+        // Handle system messages explicitly\n+        if (message.role === \"system\") {\n+            return null;\n+        }\n+\n+        const cachePoint = addCaching\n+            ? { cachePoint: { type: \"default\" } }\n+            : undefined;\n+\n+        // Tool response handling\n+        if (message.role === \"tool\") {\n+            return {\n+                role: \"user\",\n+                content: [\n+                    {\n+                        toolResult: {\n+                            toolUseId: message.toolCallId,\n+                            content: [\n+                                {\n+                                    text: message.content || \"\",\n+                                },\n+                            ],\n+                        },\n+                    },\n+                ],\n+            };\n+        }\n+\n+        // Tool calls handling\n+        if (message.role === \"assistant\" && message.toolCalls) {\n+            return {\n+                role: \"assistant\",\n+                content: message.toolCalls.map((toolCall) => ({\n+                    toolUse: {\n+                        toolUseId: toolCall.id,\n+                        name: toolCall.function?.name,\n+                        input: JSON.parse(toolCall.function?.arguments || \"{}\"),\n+                    },\n+                })),\n+            };\n+        }\n+\n+        if (message.role === \"thinking\") {\n+            if (message.redactedThinking) {\n+                const content: ContentBlock.ReasoningContentMember = {\n+                    reasoningContent: {\n+                        redactedContent: new Uint8Array(\n+                            Buffer.from(message.redactedThinking),\n+                        ),\n+                    },\n+                };\n+                return {\n+                    role: \"assistant\",\n+                    content: [content],\n+                };\n+            } else {\n+                const content: ContentBlock.ReasoningContentMember = {\n+                    reasoningContent: {\n+                        reasoningText: {\n+                            text: (message.content as string) || \"\",\n+                            signature: message.signature,\n+                        },\n+                    },\n+                };\n+                return {\n+                    role: \"assistant\",\n+                    content: [content],\n+                };\n+            }\n+        }\n+\n+        // Standard text message\n+        if (typeof message.content === \"string\") {\n+            if (addCaching) {\n+                message.content += getSecureID();\n+            }\n+            const content: any[] = [{ text: message.content }];\n+            if (addCaching) {\n+                content.push({ cachePoint: { type: \"default\" } });\n+            }\n+            return {\n+                role: message.role,\n+                content,\n+            };\n+        }\n+\n+        // Improved multimodal content handling\n+        if (Array.isArray(message.content)) {\n+            const content: any[] = [];\n+\n+            // Process all parts first\n+            message.content.forEach((part) => {\n+                if (part.type === \"text\") {\n+                    if (addCaching) {\n+                        part.text += getSecureID();\n+                    }\n+                    content.push({ text: part.text });\n+                } else if (part.type === \"imageUrl\" && part.imageUrl) {\n+                    try {\n+                        const [mimeType, base64Data] = part.imageUrl.url.split(\",\");\n+                        const format = mimeType.split(\"/\")[1]?.split(\";\")[0] || \"jpeg\";\n+                        content.push({\n+                            image: {\n+                                format,\n+                                source: {\n+                                    bytes: Buffer.from(base64Data, \"base64\"),\n+                                },\n+                            },\n+                        });\n+                    } catch (error) {\n+                        console.warn(`Failed to process image: ${error}`);\n+                    }\n+                }\n+            });\n+\n+            // Add cache point as a separate block at the end if needed\n+            if (addCaching && content.length > 0) {\n+                content.push({ cachePoint: { type: \"default\" } });\n+            }\n+\n+            return {\n+                role: message.role,\n+                content,\n+            } as Message;\n+        }\n+        return null;\n+    }\n+\n+\n+    protected async *_streamComplete(\n+        prompt: string,\n+        signal: AbortSignal,\n+        options: CompletionOptions,\n+    ): AsyncGenerator<string> {\n+        const messages = [{ role: \"user\" as const, content: prompt }];\n+        for await (const update of this._streamChat(messages, signal, options)) {\n+            const content = update.content;\n+            if (Array.isArray(content)) {\n+                for (const chunk of content) {\n+                    if (chunk.type === \"text\") {\n+                        yield chunk.text;\n+                    }\n+                }\n+            }\n+            else {\n+                yield content\n+            }\n+\n+        }\n+    }\n+    async *_streamChat(\n+        messages: ChatMessage[],\n+        signal: AbortSignal,\n+        options: CompletionOptions,\n+    ): AsyncGenerator<ChatMessage> {\n+        if(!this.creds){\n+            this.creds = this.setupAiCore()\n+        }\n+        const client = new SAPClaudeClient(this.creds);\n+        const input = this._generateConverseInput(messages, {\n+            ...options,\n+            stream: true,\n+        });\n+        const response = await client.sendMessage(input);\n+        const stream = response.data\n+\n+        function toStrictJson(str: string): string {\n+            // Wrap it in parentheses so JS will treat it as an expression\n+            const obj = new Function(\"return \" + str)()\n+            return JSON.stringify(obj)\n+        }\n+\n+\n+        // Buffer for accumulating stream data\n+        let buffer = '';\n+        \n+        try {\n+            for await (const chunk_str of stream) {\n+                buffer += chunk_str.toString();\n+                \n+                // Process complete JSON objects from the buffer\n+                let startIdx = 0;\n+                while (true) {\n+                    // Find the next \"data: \" marker\n+                    const dataPrefix = \"data: \";\n+                    const dataIdx = buffer.indexOf(dataPrefix, startIdx);\n+                    if (dataIdx === -1) break;\n+                    \n+                    // Find the end of this JSON object (next data marker or end of buffer)\n+                    const nextDataIdx = buffer.indexOf(dataPrefix, dataIdx + dataPrefix.length);\n+                    \n+                    // If we found a complete object or end of stream marker\n+                    if (nextDataIdx !== -1 || buffer.endsWith(\"\\n\\n\")) {\n+                        const endIdx = nextDataIdx !== -1 ? nextDataIdx : buffer.length;\n+                        const jsonData = buffer.substring(dataIdx + dataPrefix.length, endIdx).trim();\n+                        \n+                        try {\n+                            // Try to parse as regular JSON first\n+                            let chunk;\n+                            try {\n+                                chunk = JSON.parse(jsonData);\n+                            } catch (e) {\n+                                // If that fails, try with the toStrictJson function\n+                                try {\n+                                    chunk = JSON.parse(toStrictJson(jsonData));\n+                                } catch (e2) {\n+                                    console.warn(\"Failed to parse JSON:\", jsonData);\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+                            }\n+                            \n+                            // Process the chunk as before\n+                            if (chunk.metadata?.usage) {\n+                                console.log(`${JSON.stringify(chunk.metadata.usage)}`);\n+                            }\n+\n+                            if (chunk.contentBlockDelta?.delta) {\n+                                const delta: any = chunk.contentBlockDelta.delta;\n+\n+                                // Handle text content\n+                                if (chunk.contentBlockDelta.delta.text) {\n+                                    yield {\n+                                        role: \"assistant\",\n+                                        content: chunk.contentBlockDelta.delta.text,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                // Handle text content\n+                                if ((chunk.contentBlockDelta.delta as any).reasoningContent?.text) {\n+                                    yield {\n+                                        role: \"thinking\",\n+                                        content: (chunk.contentBlockDelta.delta as any).reasoningContent\n+                                            .text,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                // Handle signature for thinking\n+                                if (delta.reasoningContent?.signature) {\n+                                    yield {\n+                                        role: \"thinking\",\n+                                        content: \"\",\n+                                        signature: delta.reasoningContent.signature,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                // Handle redacted thinking\n+                                if (delta.redactedReasoning?.data) {\n+                                    yield {\n+                                        role: \"thinking\",\n+                                        content: \"\",\n+                                        redactedThinking: delta.redactedReasoning.data,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                if (\n+                                    chunk.contentBlockDelta.delta.toolUse?.input &&\n+                                    this._currentToolResponse\n+                                ) {\n+                                    // Append the new input to the existing string\n+                                    // eslint-disable-next-line max-depth\n+                                    if (this._currentToolResponse.input === undefined) {\n+                                        this._currentToolResponse.input = \"\";\n+                                    }\n+                                    this._currentToolResponse.input +=\n+                                        chunk.contentBlockDelta.delta.toolUse.input;\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+                            }\n+\n+                            if (chunk.contentBlockStart?.start) {\n+                                const start: any = chunk.contentBlockStart.start;\n+                                if (start.redactedReasoning) {\n+                                    yield {\n+                                        role: \"thinking\",\n+                                        content: \"\",\n+                                        redactedThinking: start.redactedReasoning.data,\n+                                    };\n+                                    startIdx = endIdx;\n+                                    continue;\n+                                }\n+\n+                                const toolUse = chunk.contentBlockStart.start.toolUse;\n+                                if (toolUse?.toolUseId && toolUse?.name) {\n+                                    this._currentToolResponse = {\n+                                        toolUseId: toolUse.toolUseId,\n+                                        name: toolUse.name,\n+                                        input: \"\",\n+                                    };\n+                                }\n+                                startIdx = endIdx;\n+                                continue;\n+                            }\n+\n+                            if (chunk.contentBlockStop) {\n+                                if (this._currentToolResponse) {\n+                                    yield {\n+                                        role: \"assistant\",\n+                                        content: \"\",\n+                                        toolCalls: [\n+                                            {\n+                                                id: this._currentToolResponse.toolUseId,\n+                                                type: \"function\",\n+                                                function: {\n+                                                    name: this._currentToolResponse.name,\n+                                                    arguments: this._currentToolResponse.input,\n+                                                },\n+                                            },\n+                                        ],\n+                                    };\n+                                    this._currentToolResponse = null;\n+                                }\n+                                startIdx = endIdx;\n+                                continue;\n+                            }\n+                            \n+                            // If we get here, update startIdx to process the next chunk\n+                            startIdx = endIdx;\n+                            \n+                        } catch (error: unknown) {\n+                            console.warn(`Error processing chunk: ${error instanceof Error ? error.message : String(error)}`);\n+                            startIdx = endIdx;\n+                        }\n+                    } else {\n+                        // We have an incomplete object, wait for more data\n+                        break;\n+                    }\n+                }\n+                \n+                // Keep only the unprocessed part of the buffer\n+                if (startIdx > 0) {\n+                    buffer = buffer.substring(startIdx);\n+                }\n+            }\n+        } catch (error: unknown) {\n+            this._currentToolResponse = null;\n+            if (error instanceof Error) {\n+                if (\"code\" in error) {\n+                    // AWS SDK specific errors\n+                    throw new Error(\n+                        `AWS Bedrock stream error (${(error as any).code}): ${error.message}`,\n+                    );\n+                }\n+                throw new Error(`Error processing Bedrock stream: ${error.message}`);\n+            }\n+            throw new Error(\n+                \"Error processing Bedrock stream: Unknown error occurred\",\n+            );\n+        }\n+    }\n+\n+\n+    loadAiCoreCredentials(): SAPClaudeClientOptions | undefined {\n+        const credsFilePath = path.join(os.homedir(), AI_CORE_CREDS_FILENAME)\n+\n+        if (!fs.existsSync(credsFilePath)) {\n+            return undefined\n+        }\n+\n+        const fileContents = fs.readFileSync(credsFilePath, \"utf-8\")\n+        try {\n+            const parsed = JSON.parse(fileContents)\n+\n+            // Check and report missing credentials\n+            const missingCredentials = []\n+            if (!parsed.clientid) {\n+                missingCredentials.push(\"clientid\")\n+            }\n+            if (!parsed.clientsecret) {\n+                missingCredentials.push(\"clientsecret\")\n+            }\n+            if (!parsed.url) {\n+                missingCredentials.push(\"url\")\n+            }\n+            if (!parsed.serviceurls) {\n+                missingCredentials.push(\"serviceurls\")\n+            } else if (!parsed.serviceurls.AI_API_URL) {\n+                missingCredentials.push(\"serviceurls.AI_API_URL\")\n+            }\n+\n+            if (missingCredentials.length > 0) {\n+                throw new Error(`Credentials file is missing required properties: ${missingCredentials.join(\", \")}`)\n+            }\n+\n+            return parsed\n+        } catch (e) {\n+            throw new Error(\"Failed to parse ai core credentials file:\", e as any)",
        "comment_created_at": "2025-06-24T12:13:50+00:00",
        "comment_author": "recurseml[bot]",
        "comment_body": "Incorrect error construction that will lose the actual error details. The Error constructor takes only a single message parameter, so the error object 'e' is being incorrectly passed as a second parameter and will be lost. Should use string concatenation or template literals to include the error details in the message, e.g., `Failed to parse ai core credentials file: ${e.message}`\n\n\ud83d\udcda [Relevant Docs](https://www.typescriptlang.org/docs/handbook/2/understanding-errors.html)\n\n---\n\n> *React with \ud83d\udc4d to tell me that this comment was useful, or \ud83d\udc4e if not (and I'll stop posting more comments like this in the future)*",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2146081788",
    "pr_number": 5936,
    "pr_file": "packages/openai-adapters/src/apis/Bedrock.ts",
    "created_at": "2025-06-13T20:56:35+00:00",
    "commented_code": "+import { AnthropicBedrock } from \"@anthropic-ai/bedrock-sdk\";\n+import {\n+  ChatCompletion,\n+  ChatCompletionChunk,\n+  ChatCompletionCreateParamsNonStreaming,\n+  ChatCompletionCreateParamsStreaming,\n+  Completion,\n+  CompletionCreateParamsNonStreaming,\n+  CompletionCreateParamsStreaming,\n+  Model,\n+} from \"openai/resources/index\";\n+import { z } from \"zod\";\n+import { BedrockConfigSchema } from \"../types.js\";\n+import { chatChunk, chatChunkFromDelta } from \"../util.js\";\n+import { EMPTY_CHAT_COMPLETION } from \"../util/emptyChatCompletion.js\";\n+import {\n+  BaseLlmApi,\n+  CreateRerankResponse,\n+  FimCreateParamsStreaming,\n+  RerankCreateParams,\n+} from \"./base.js\";\n+\n+interface ToolUseState {\n+  toolUseId: string;\n+  name: string;\n+  input: string;\n+}\n+\n+export class BedrockApi implements BaseLlmApi {\n+  private client: AnthropicBedrock;\n+  private _currentToolResponse: Partial<ToolUseState> | null = null;\n+\n+  constructor(protected config: z.infer<typeof BedrockConfigSchema>) {\n+    this.client = new AnthropicBedrock({\n+      awsRegion: config.env.region ?? \"us-east-1\",\n+      awsAccessKey: config.env.awsAccessKey,\n+      awsSecretKey: config.env.awsSecretKey,\n+    });\n+  }\n+\n+  private _convertBody(oaiBody: any) {\n+    let stop = undefined;\n+    if (oaiBody.stop && Array.isArray(oaiBody.stop)) {\n+      stop = oaiBody.stop.filter((x: string) => x.trim() !== \"\");\n+    } else if (typeof oaiBody.stop === \"string\" && oaiBody.stop.trim() !== \"\") {\n+      stop = [oaiBody.stop];\n+    }\n+\n+    const systemMessage = oaiBody.messages.find(\n+      (msg: any) => msg.role === \"system\",\n+    )?.content;\n+\n+    // Convert tool_choice to proper Anthropic format\n+    let toolChoice: any = undefined;\n+    if (oaiBody.tool_choice) {\n+      if (typeof oaiBody.tool_choice === \"string\") {\n+        if (oaiBody.tool_choice === \"auto\") {\n+          toolChoice = { type: \"auto\" };\n+        } else if (oaiBody.tool_choice === \"required\") {\n+          toolChoice = { type: \"any\" };\n+        } else if (oaiBody.tool_choice === \"none\") {\n+          toolChoice = { type: \"none\" };\n+        }\n+      } else if (oaiBody.tool_choice?.function?.name) {\n+        toolChoice = {\n+          type: \"tool\",\n+          name: oaiBody.tool_choice.function.name,\n+        };\n+      }\n+    }\n+\n+    const anthropicBody: any = {\n+      messages: this._convertMessages(\n+        oaiBody.messages.filter((msg: any) => msg.role !== \"system\"),\n+      ),\n+      system: systemMessage\n+        ? [\n+            {\n+              type: \"text\",\n+              text: systemMessage,\n+              cache_control: { type: \"ephemeral\" },\n+            },\n+          ]\n+        : undefined,\n+      top_p: oaiBody.top_p,\n+      temperature: oaiBody.temperature,\n+      max_tokens: oaiBody.max_tokens ?? 4096,\n+      model: oaiBody.model,\n+      stop_sequences: stop?.slice(0, 4), // Bedrock supports max 4 stop sequences\n+      stream: oaiBody.stream,\n+      tools: oaiBody.tools?.map((tool: any) => ({\n+        name: tool.function.name,\n+        description: tool.function.description,\n+        input_schema: tool.function.parameters,\n+      })),\n+      tool_choice: toolChoice,\n+    };\n+\n+    return anthropicBody;\n+  }\n+\n+  private _convertMessages(msgs: any[]): any[] {\n+    const messages = msgs.map((message) => {\n+      if (message.role === \"tool\") {\n+        return {\n+          role: \"user\" as const,\n+          content: [\n+            {\n+              type: \"tool_result\" as const,\n+              tool_use_id: message.tool_call_id,\n+              content:\n+                typeof message.content === \"string\"\n+                  ? message.content\n+                  : Array.isArray(message.content)\n+                    ? message.content\n+                        .map(\n+                          (part: any) =>\n+                            part.text || part.content || JSON.stringify(part),\n+                        )\n+                        .join(\"\")\n+                    : String(message.content),\n+            },\n+          ],\n+        };\n+      } else if (message.role === \"assistant\" && message.tool_calls) {\n+        return {\n+          role: \"assistant\" as const,\n+          content: message.tool_calls.map((toolCall: any) => ({\n+            type: \"tool_use\" as const,\n+            id: toolCall.id,\n+            name: toolCall.function?.name,\n+            input: JSON.parse(toolCall.function?.arguments || \"{}\"),",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2146081788",
        "repo_full_name": "continuedev/continue",
        "pr_number": 5936,
        "pr_file": "packages/openai-adapters/src/apis/Bedrock.ts",
        "discussion_id": "2146081788",
        "commented_code": "@@ -0,0 +1,399 @@\n+import { AnthropicBedrock } from \"@anthropic-ai/bedrock-sdk\";\n+import {\n+  ChatCompletion,\n+  ChatCompletionChunk,\n+  ChatCompletionCreateParamsNonStreaming,\n+  ChatCompletionCreateParamsStreaming,\n+  Completion,\n+  CompletionCreateParamsNonStreaming,\n+  CompletionCreateParamsStreaming,\n+  Model,\n+} from \"openai/resources/index\";\n+import { z } from \"zod\";\n+import { BedrockConfigSchema } from \"../types.js\";\n+import { chatChunk, chatChunkFromDelta } from \"../util.js\";\n+import { EMPTY_CHAT_COMPLETION } from \"../util/emptyChatCompletion.js\";\n+import {\n+  BaseLlmApi,\n+  CreateRerankResponse,\n+  FimCreateParamsStreaming,\n+  RerankCreateParams,\n+} from \"./base.js\";\n+\n+interface ToolUseState {\n+  toolUseId: string;\n+  name: string;\n+  input: string;\n+}\n+\n+export class BedrockApi implements BaseLlmApi {\n+  private client: AnthropicBedrock;\n+  private _currentToolResponse: Partial<ToolUseState> | null = null;\n+\n+  constructor(protected config: z.infer<typeof BedrockConfigSchema>) {\n+    this.client = new AnthropicBedrock({\n+      awsRegion: config.env.region ?? \"us-east-1\",\n+      awsAccessKey: config.env.awsAccessKey,\n+      awsSecretKey: config.env.awsSecretKey,\n+    });\n+  }\n+\n+  private _convertBody(oaiBody: any) {\n+    let stop = undefined;\n+    if (oaiBody.stop && Array.isArray(oaiBody.stop)) {\n+      stop = oaiBody.stop.filter((x: string) => x.trim() !== \"\");\n+    } else if (typeof oaiBody.stop === \"string\" && oaiBody.stop.trim() !== \"\") {\n+      stop = [oaiBody.stop];\n+    }\n+\n+    const systemMessage = oaiBody.messages.find(\n+      (msg: any) => msg.role === \"system\",\n+    )?.content;\n+\n+    // Convert tool_choice to proper Anthropic format\n+    let toolChoice: any = undefined;\n+    if (oaiBody.tool_choice) {\n+      if (typeof oaiBody.tool_choice === \"string\") {\n+        if (oaiBody.tool_choice === \"auto\") {\n+          toolChoice = { type: \"auto\" };\n+        } else if (oaiBody.tool_choice === \"required\") {\n+          toolChoice = { type: \"any\" };\n+        } else if (oaiBody.tool_choice === \"none\") {\n+          toolChoice = { type: \"none\" };\n+        }\n+      } else if (oaiBody.tool_choice?.function?.name) {\n+        toolChoice = {\n+          type: \"tool\",\n+          name: oaiBody.tool_choice.function.name,\n+        };\n+      }\n+    }\n+\n+    const anthropicBody: any = {\n+      messages: this._convertMessages(\n+        oaiBody.messages.filter((msg: any) => msg.role !== \"system\"),\n+      ),\n+      system: systemMessage\n+        ? [\n+            {\n+              type: \"text\",\n+              text: systemMessage,\n+              cache_control: { type: \"ephemeral\" },\n+            },\n+          ]\n+        : undefined,\n+      top_p: oaiBody.top_p,\n+      temperature: oaiBody.temperature,\n+      max_tokens: oaiBody.max_tokens ?? 4096,\n+      model: oaiBody.model,\n+      stop_sequences: stop?.slice(0, 4), // Bedrock supports max 4 stop sequences\n+      stream: oaiBody.stream,\n+      tools: oaiBody.tools?.map((tool: any) => ({\n+        name: tool.function.name,\n+        description: tool.function.description,\n+        input_schema: tool.function.parameters,\n+      })),\n+      tool_choice: toolChoice,\n+    };\n+\n+    return anthropicBody;\n+  }\n+\n+  private _convertMessages(msgs: any[]): any[] {\n+    const messages = msgs.map((message) => {\n+      if (message.role === \"tool\") {\n+        return {\n+          role: \"user\" as const,\n+          content: [\n+            {\n+              type: \"tool_result\" as const,\n+              tool_use_id: message.tool_call_id,\n+              content:\n+                typeof message.content === \"string\"\n+                  ? message.content\n+                  : Array.isArray(message.content)\n+                    ? message.content\n+                        .map(\n+                          (part: any) =>\n+                            part.text || part.content || JSON.stringify(part),\n+                        )\n+                        .join(\"\")\n+                    : String(message.content),\n+            },\n+          ],\n+        };\n+      } else if (message.role === \"assistant\" && message.tool_calls) {\n+        return {\n+          role: \"assistant\" as const,\n+          content: message.tool_calls.map((toolCall: any) => ({\n+            type: \"tool_use\" as const,\n+            id: toolCall.id,\n+            name: toolCall.function?.name,\n+            input: JSON.parse(toolCall.function?.arguments || \"{}\"),",
        "comment_created_at": "2025-06-13T20:56:35+00:00",
        "comment_author": "recurseml[bot]",
        "comment_body": "Unsafe JSON parsing: The code directly parses potentially malformed JSON without error handling. If toolCall.function.arguments contains invalid JSON, this will throw an unhandled runtime error. Should wrap in try-catch and handle parsing failures gracefully.\n\n\ud83d\udcda [Relevant Docs](https://docs.anthropic.com/en/api/claude-on-amazon-bedrock)\n\n---\n\n> *React with \ud83d\udc4d to tell me that this comment was useful, or \ud83d\udc4e if not (and I'll stop posting more comments like this in the future)*",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2173116100",
    "pr_number": 6264,
    "pr_file": "core/llm/streamChat.ts",
    "created_at": "2025-06-28T04:20:29+00:00",
    "commented_code": "},\n       true,\n     );\n+    if (!slashCommand.run) {\n+      throw new Error(`Slash command ${command.name} is not a legacy command`);",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2173116100",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6264,
        "pr_file": "core/llm/streamChat.ts",
        "discussion_id": "2173116100",
        "commented_code": "@@ -59,6 +59,9 @@ export async function* llmStreamChat(\n       },\n       true,\n     );\n+    if (!slashCommand.run) {\n+      throw new Error(`Slash command ${command.name} is not a legacy command`);",
        "comment_created_at": "2025-06-28T04:20:29+00:00",
        "comment_author": "sestinj",
        "comment_body": "god forbid this error ever becomes visible to the user\r\n\r\nI don't think it should be possible, but could we pretend that we expect that and write an error message that makes more sense to them? e.g. \"Slash command not found\"",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2132427469",
    "pr_number": 6013,
    "pr_file": "packages/fetch/src/premature-close.ts",
    "created_at": "2025-06-06T15:40:30+00:00",
    "commented_code": "+export function getPrematureCloseErrorMessage(chunks: number): string {\n+  if (chunks === 0) {\n+    return \"Connection was closed before any data was received (Premature close). Try again.\";\n+  } else {\n+    return (",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2132427469",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6013,
        "pr_file": "packages/fetch/src/premature-close.ts",
        "discussion_id": "2132427469",
        "commented_code": "@@ -0,0 +1,14 @@\n+export function getPrematureCloseErrorMessage(chunks: number): string {\n+  if (chunks === 0) {\n+    return \"Connection was closed before any data was received (Premature close). Try again.\";\n+  } else {\n+    return (",
        "comment_created_at": "2025-06-06T15:40:30+00:00",
        "comment_author": "sestinj",
        "comment_body": "I really don't think the end user wants to see all of this. If they start trying to solve \"Keep alive\" on their own they will never succeed. Could it just be \"The response was cancelled mid-stream\"",
        "pr_file_module": null
      },
      {
        "comment_id": "2132611222",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6013,
        "pr_file": "packages/fetch/src/premature-close.ts",
        "discussion_id": "2132427469",
        "commented_code": "@@ -0,0 +1,14 @@\n+export function getPrematureCloseErrorMessage(chunks: number): string {\n+  if (chunks === 0) {\n+    return \"Connection was closed before any data was received (Premature close). Try again.\";\n+  } else {\n+    return (",
        "comment_created_at": "2025-06-06T17:45:23+00:00",
        "comment_author": "RomneyDa",
        "comment_body": "Agreed, updated",
        "pr_file_module": null
      }
    ]
  }
]