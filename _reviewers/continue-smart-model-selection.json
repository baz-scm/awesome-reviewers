[
  {
    "discussion_id": "1277173721",
    "pr_number": 318,
    "pr_file": "continuedev/src/continuedev/core/sdk.py",
    "created_at": "2023-07-28T06:46:22+00:00",
    "commented_code": "api_key = self.provider_keys[\"anthropic\"]\n         return AnthropicLLM(api_key, model, self.system_message)\n \n-    @cached_property\n-    def claude2(self):\n-        return self.__load_anthropic_model(\"claude-2\")\n-\n-    @cached_property\n-    def starcoder(self):\n-        return self.__load_hf_inference_api_model(\"bigcode/starcoder\")\n-\n-    @cached_property\n-    def gpt35(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo\")\n-\n-    @cached_property\n-    def gpt350613(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo-0613\")\n-\n-    @cached_property\n-    def gpt3516k(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo-16k\")\n-\n-    @cached_property\n-    def gpt4(self):\n-        return self.__load_openai_model(\"gpt-4\")\n-\n-    @cached_property\n-    def ggml(self):\n-        return GGML(system_message=self.system_message)\n-\n-    def __model_from_name(self, model_name: str):\n-        if model_name == \"starcoder\":\n-            return self.starcoder\n-        elif model_name == \"gpt-3.5-turbo\":\n-            return self.gpt35\n-        elif model_name == \"gpt-3.5-turbo-16k\":\n-            return self.gpt3516k\n-        elif model_name == \"gpt-4\":\n-            return self.gpt4\n-        elif model_name == \"claude-2\":\n-            return self.claude2\n-        elif model_name == \"ggml\":\n-            return self.ggml\n-        else:\n-            raise Exception(f\"Unknown model {model_name}\")\n-\n     @property\n     def default(self):\n-        default_model = self.sdk.config.default_model\n-        return self.__model_from_name(default_model) if default_model is not None else self.gpt4\n+        return sdk.config.llm if sdk.config.llm is not None else ProxyServer(default_model=\"gpt-4\")",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1277173721",
        "repo_full_name": "continuedev/continue",
        "pr_number": 318,
        "pr_file": "continuedev/src/continuedev/core/sdk.py",
        "discussion_id": "1277173721",
        "commented_code": "@@ -90,54 +80,9 @@ def __load_anthropic_model(self, model: str) -> AnthropicLLM:\n         api_key = self.provider_keys[\"anthropic\"]\n         return AnthropicLLM(api_key, model, self.system_message)\n \n-    @cached_property\n-    def claude2(self):\n-        return self.__load_anthropic_model(\"claude-2\")\n-\n-    @cached_property\n-    def starcoder(self):\n-        return self.__load_hf_inference_api_model(\"bigcode/starcoder\")\n-\n-    @cached_property\n-    def gpt35(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo\")\n-\n-    @cached_property\n-    def gpt350613(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo-0613\")\n-\n-    @cached_property\n-    def gpt3516k(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo-16k\")\n-\n-    @cached_property\n-    def gpt4(self):\n-        return self.__load_openai_model(\"gpt-4\")\n-\n-    @cached_property\n-    def ggml(self):\n-        return GGML(system_message=self.system_message)\n-\n-    def __model_from_name(self, model_name: str):\n-        if model_name == \"starcoder\":\n-            return self.starcoder\n-        elif model_name == \"gpt-3.5-turbo\":\n-            return self.gpt35\n-        elif model_name == \"gpt-3.5-turbo-16k\":\n-            return self.gpt3516k\n-        elif model_name == \"gpt-4\":\n-            return self.gpt4\n-        elif model_name == \"claude-2\":\n-            return self.claude2\n-        elif model_name == \"ggml\":\n-            return self.ggml\n-        else:\n-            raise Exception(f\"Unknown model {model_name}\")\n-\n     @property\n     def default(self):\n-        default_model = self.sdk.config.default_model\n-        return self.__model_from_name(default_model) if default_model is not None else self.gpt4\n+        return sdk.config.llm if sdk.config.llm is not None else ProxyServer(default_model=\"gpt-4\")",
        "comment_created_at": "2023-07-28T06:46:22+00:00",
        "comment_author": "sestinj",
        "comment_body": "This should respect the logic that says \"if there's an openai key, then use OpenAI, otherwise use ProxyServer\". I think the best way here is: In Models.create, start the sdk.config.llm, then set the property `self.default` to `sdk.config.llm if sdk.config.llm is not None else (<CHECK FOR API KEY> ... ProxyServer/OpenAI)`, and then there's no need for the @property decorator",
        "pr_file_module": null
      },
      {
        "comment_id": "1278001434",
        "repo_full_name": "continuedev/continue",
        "pr_number": 318,
        "pr_file": "continuedev/src/continuedev/core/sdk.py",
        "discussion_id": "1277173721",
        "commented_code": "@@ -90,54 +80,9 @@ def __load_anthropic_model(self, model: str) -> AnthropicLLM:\n         api_key = self.provider_keys[\"anthropic\"]\n         return AnthropicLLM(api_key, model, self.system_message)\n \n-    @cached_property\n-    def claude2(self):\n-        return self.__load_anthropic_model(\"claude-2\")\n-\n-    @cached_property\n-    def starcoder(self):\n-        return self.__load_hf_inference_api_model(\"bigcode/starcoder\")\n-\n-    @cached_property\n-    def gpt35(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo\")\n-\n-    @cached_property\n-    def gpt350613(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo-0613\")\n-\n-    @cached_property\n-    def gpt3516k(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo-16k\")\n-\n-    @cached_property\n-    def gpt4(self):\n-        return self.__load_openai_model(\"gpt-4\")\n-\n-    @cached_property\n-    def ggml(self):\n-        return GGML(system_message=self.system_message)\n-\n-    def __model_from_name(self, model_name: str):\n-        if model_name == \"starcoder\":\n-            return self.starcoder\n-        elif model_name == \"gpt-3.5-turbo\":\n-            return self.gpt35\n-        elif model_name == \"gpt-3.5-turbo-16k\":\n-            return self.gpt3516k\n-        elif model_name == \"gpt-4\":\n-            return self.gpt4\n-        elif model_name == \"claude-2\":\n-            return self.claude2\n-        elif model_name == \"ggml\":\n-            return self.ggml\n-        else:\n-            raise Exception(f\"Unknown model {model_name}\")\n-\n     @property\n     def default(self):\n-        default_model = self.sdk.config.default_model\n-        return self.__model_from_name(default_model) if default_model is not None else self.gpt4\n+        return sdk.config.llm if sdk.config.llm is not None else ProxyServer(default_model=\"gpt-4\")",
        "comment_created_at": "2023-07-28T19:20:22+00:00",
        "comment_author": "lun-4",
        "comment_body": "I think we could create a `MaybeProxyOpenAI()` (other name: `AutomaticOpenAIFallback`) LLM class that does the switch based on secret existence, rather than having that logic inside the SDK",
        "pr_file_module": null
      },
      {
        "comment_id": "1278012111",
        "repo_full_name": "continuedev/continue",
        "pr_number": 318,
        "pr_file": "continuedev/src/continuedev/core/sdk.py",
        "discussion_id": "1277173721",
        "commented_code": "@@ -90,54 +80,9 @@ def __load_anthropic_model(self, model: str) -> AnthropicLLM:\n         api_key = self.provider_keys[\"anthropic\"]\n         return AnthropicLLM(api_key, model, self.system_message)\n \n-    @cached_property\n-    def claude2(self):\n-        return self.__load_anthropic_model(\"claude-2\")\n-\n-    @cached_property\n-    def starcoder(self):\n-        return self.__load_hf_inference_api_model(\"bigcode/starcoder\")\n-\n-    @cached_property\n-    def gpt35(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo\")\n-\n-    @cached_property\n-    def gpt350613(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo-0613\")\n-\n-    @cached_property\n-    def gpt3516k(self):\n-        return self.__load_openai_model(\"gpt-3.5-turbo-16k\")\n-\n-    @cached_property\n-    def gpt4(self):\n-        return self.__load_openai_model(\"gpt-4\")\n-\n-    @cached_property\n-    def ggml(self):\n-        return GGML(system_message=self.system_message)\n-\n-    def __model_from_name(self, model_name: str):\n-        if model_name == \"starcoder\":\n-            return self.starcoder\n-        elif model_name == \"gpt-3.5-turbo\":\n-            return self.gpt35\n-        elif model_name == \"gpt-3.5-turbo-16k\":\n-            return self.gpt3516k\n-        elif model_name == \"gpt-4\":\n-            return self.gpt4\n-        elif model_name == \"claude-2\":\n-            return self.claude2\n-        elif model_name == \"ggml\":\n-            return self.ggml\n-        else:\n-            raise Exception(f\"Unknown model {model_name}\")\n-\n     @property\n     def default(self):\n-        default_model = self.sdk.config.default_model\n-        return self.__model_from_name(default_model) if default_model is not None else self.gpt4\n+        return sdk.config.llm if sdk.config.llm is not None else ProxyServer(default_model=\"gpt-4\")",
        "comment_created_at": "2023-07-28T19:34:28+00:00",
        "comment_author": "sestinj",
        "comment_body": "Not a bad idea. If it can be done without too much work that would make sense",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1278705696",
    "pr_number": 318,
    "pr_file": "continuedev/src/continuedev/plugins/steps/chat.py",
    "created_at": "2023-07-31T02:40:47+00:00",
    "commented_code": "msg_content = \"\"\n             msg_step = None\n \n-            async for msg_chunk in sdk.models.gpt350613.stream_chat(await sdk.get_chat_context(), functions=functions):\n+            gpt350613 = OpenAI(model=\"gpt-3.5-turbo-0613\")",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1278705696",
        "repo_full_name": "continuedev/continue",
        "pr_number": 318,
        "pr_file": "continuedev/src/continuedev/plugins/steps/chat.py",
        "discussion_id": "1278705696",
        "commented_code": "@@ -168,7 +169,10 @@ async def run(self, sdk: ContinueSDK):\n             msg_content = \"\"\n             msg_step = None\n \n-            async for msg_chunk in sdk.models.gpt350613.stream_chat(await sdk.get_chat_context(), functions=functions):\n+            gpt350613 = OpenAI(model=\"gpt-3.5-turbo-0613\")",
        "comment_created_at": "2023-07-31T02:40:47+00:00",
        "comment_author": "lun-4",
        "comment_body": "Shouldn't this be `sdk.models.medium` instead of starting gpt350613?",
        "pr_file_module": null
      },
      {
        "comment_id": "1278795897",
        "repo_full_name": "continuedev/continue",
        "pr_number": 318,
        "pr_file": "continuedev/src/continuedev/plugins/steps/chat.py",
        "discussion_id": "1278705696",
        "commented_code": "@@ -168,7 +169,10 @@ async def run(self, sdk: ContinueSDK):\n             msg_content = \"\"\n             msg_step = None\n \n-            async for msg_chunk in sdk.models.gpt350613.stream_chat(await sdk.get_chat_context(), functions=functions):\n+            gpt350613 = OpenAI(model=\"gpt-3.5-turbo-0613\")",
        "comment_created_at": "2023-07-31T05:04:24+00:00",
        "comment_author": "sestinj",
        "comment_body": "As far as I know, it's still only the 0613 version that is trained for [function calling](https://openai.com/blog/function-calling-and-other-api-updates), which is needed in this specific step.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1278715683",
    "pr_number": 318,
    "pr_file": "continuedev/src/continuedev/plugins/steps/core/core.py",
    "created_at": "2023-07-31T02:49:21+00:00",
    "commented_code": "# If using 3.5 and overflows, upgrade to 3.5.16k\n         if model_to_use.name == \"gpt-3.5-turbo\":\n-            if total_tokens > MAX_TOKENS_FOR_MODEL[\"gpt-3.5-turbo\"]:\n-                model_to_use = sdk.models.gpt3516k\n+            if total_tokens > model_to_use.context_length:",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1278715683",
        "repo_full_name": "continuedev/continue",
        "pr_number": 318,
        "pr_file": "continuedev/src/continuedev/plugins/steps/core/core.py",
        "discussion_id": "1278715683",
        "commented_code": "@@ -201,8 +200,9 @@ async def get_prompt_parts(self, rif: RangeInFileWithContents, sdk: ContinueSDK,\n \n         # If using 3.5 and overflows, upgrade to 3.5.16k\n         if model_to_use.name == \"gpt-3.5-turbo\":\n-            if total_tokens > MAX_TOKENS_FOR_MODEL[\"gpt-3.5-turbo\"]:\n-                model_to_use = sdk.models.gpt3516k\n+            if total_tokens > model_to_use.context_length:",
        "comment_created_at": "2023-07-31T02:49:21+00:00",
        "comment_author": "lun-4",
        "comment_body": "This is an interesting question in terms of replacement. On one hand, the idea is to decrease amount of hardcoded models on the steps, so that users can switch to anything. On the other, model upgrades may be wanted based on context size...\r\n\r\nI don't know if `sdk.models.large` would be a good replacement here unless we explicitly define \"large\" as a model that is \"actually large in size, expensive\" **and** \"has a large context size compared to the current models\" (that latter part being something that should be documented, in my opinion. it would be a footgun if someone sets their `large` model to something smaller in context to `model_to_use`).\r\n\r\nOne possible avenue to this is letting the LLM object decide if it should upgrade in the background, letting the user decide if that's behavior they want to have. I can see a future where someone does not want to use any OpenAI models, ever, and they want to upgrade from LLaMA-2 to [LLongMA-13B](https://huggingface.co/conceptofmind/LLongMA-13b) in the cases where the context is too large to warrant an upgrade.\r\n\r\nOne drawback of that is most likely we'd always blow past the normal context window and use the long model, which for the case of an OpenAI user it'd mean a giant hole in their wallet.\r\n\r\nMaybe `model_to_use = await model_to_use.maybe_upgrade()` would be a valid compromise. Let's LLMs decide the upgrade path, while only letting specific steps that desire large context windows signal that to the LLM.",
        "pr_file_module": null
      },
      {
        "comment_id": "1278796320",
        "repo_full_name": "continuedev/continue",
        "pr_number": 318,
        "pr_file": "continuedev/src/continuedev/plugins/steps/core/core.py",
        "discussion_id": "1278715683",
        "commented_code": "@@ -201,8 +200,9 @@ async def get_prompt_parts(self, rif: RangeInFileWithContents, sdk: ContinueSDK,\n \n         # If using 3.5 and overflows, upgrade to 3.5.16k\n         if model_to_use.name == \"gpt-3.5-turbo\":\n-            if total_tokens > MAX_TOKENS_FOR_MODEL[\"gpt-3.5-turbo\"]:\n-                model_to_use = sdk.models.gpt3516k\n+            if total_tokens > model_to_use.context_length:",
        "comment_created_at": "2023-07-31T05:05:20+00:00",
        "comment_author": "sestinj",
        "comment_body": "I do think this logic could be handled more automatically, but the uses are too sparse right now to justify adding that as a part of each LLM class. I think this is one of those that will become more apparent once it's required a few more times.",
        "pr_file_module": null
      }
    ]
  }
]