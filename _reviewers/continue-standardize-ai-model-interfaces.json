[
  {
    "discussion_id": "2163812419",
    "pr_number": 6274,
    "pr_file": "core/llm/llms/AICore.ts",
    "created_at": "2025-06-24T12:13:46+00:00",
    "commented_code": "+import { ChatMessage, CompletionOptions, LLMOptions, MessageContent, Tool, ToolCallDelta } from \"../../index.js\";\n+import { BaseLLM } from \"../index.js\";\n+import { AICoreClaudeProvider } from \"./AICore/AICoreClaudeProvider.js\";\n+import { AICoreGeneralProvider } from \"./AICore/AICoreGeneralProvider.js\";\n+const CAP_MESSAGE = `\n+  Instactions for SAP Cloud Application Programming Model (CAP):\n+    - DO:\n+      - After you createing the application run it!\n+      - Before you createing the application, using tools, mcp etc you need to give the user summry.\n+      - Summarize just the first time the user want to create an application.\n+      - Assume cds installed as global package.\n+      - Use the summery as information to build a prompt to the CAP MCP Server tools.\n+      - When CAP MCP Server tools passed successfully:\n+        1) setup the application directory as the git work directory.\n+        2) run git init.\n+        3) return answer to the user that we created application successfully and if he need more help.\n+  \n+    - DO NOT:\n+      - Dont summarize evry step.\n+      - Dont Create the application with out that the user Approve the application summry.\n+      - Dont use npx\n+    \n+    - Application Summry Instactions:\n+      - A high-level description of the application's purpose and main features. \n+      - A simplified storyboard-style diagram showing key entities and their relationships (this can reflect or be derived from the data model). \n+      - Example data models: define main entities and how data is structured and accessed. \n+      - Recommended technology stack (Node.js with CAP, SAP Fiori/UI5 or suitable low-code alternatives, database). \n+      - Project architecture outline (folder structure, main modules, and interactions).\n+      Keep the explanation concise, clear, and focused on system design. Do **not** generate implementation code or file scaffolding. Do **not** include a summary or closing paragraph.\n+`\n+\n+export class AICore extends BaseLLM {\n+    private aICoreClaudeProvider?: AICoreClaudeProvider;\n+    private aICoreGeneralProvider?: AICoreGeneralProvider;\n+    private llmOptions: LLMOptions;\n+    static providerName = \"aiCore\";\n+\n+    static defaultOptions: Partial<LLMOptions> = {\n+        model: \"anthropic--claude-3.7-sonnet\",",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2163812419",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6274,
        "pr_file": "core/llm/llms/AICore.ts",
        "discussion_id": "2163812419",
        "commented_code": "@@ -0,0 +1,101 @@\n+import { ChatMessage, CompletionOptions, LLMOptions, MessageContent, Tool, ToolCallDelta } from \"../../index.js\";\n+import { BaseLLM } from \"../index.js\";\n+import { AICoreClaudeProvider } from \"./AICore/AICoreClaudeProvider.js\";\n+import { AICoreGeneralProvider } from \"./AICore/AICoreGeneralProvider.js\";\n+const CAP_MESSAGE = `\n+  Instactions for SAP Cloud Application Programming Model (CAP):\n+    - DO:\n+      - After you createing the application run it!\n+      - Before you createing the application, using tools, mcp etc you need to give the user summry.\n+      - Summarize just the first time the user want to create an application.\n+      - Assume cds installed as global package.\n+      - Use the summery as information to build a prompt to the CAP MCP Server tools.\n+      - When CAP MCP Server tools passed successfully:\n+        1) setup the application directory as the git work directory.\n+        2) run git init.\n+        3) return answer to the user that we created application successfully and if he need more help.\n+  \n+    - DO NOT:\n+      - Dont summarize evry step.\n+      - Dont Create the application with out that the user Approve the application summry.\n+      - Dont use npx\n+    \n+    - Application Summry Instactions:\n+      - A high-level description of the application's purpose and main features. \n+      - A simplified storyboard-style diagram showing key entities and their relationships (this can reflect or be derived from the data model). \n+      - Example data models: define main entities and how data is structured and accessed. \n+      - Recommended technology stack (Node.js with CAP, SAP Fiori/UI5 or suitable low-code alternatives, database). \n+      - Project architecture outline (folder structure, main modules, and interactions).\n+      Keep the explanation concise, clear, and focused on system design. Do **not** generate implementation code or file scaffolding. Do **not** include a summary or closing paragraph.\n+`\n+\n+export class AICore extends BaseLLM {\n+    private aICoreClaudeProvider?: AICoreClaudeProvider;\n+    private aICoreGeneralProvider?: AICoreGeneralProvider;\n+    private llmOptions: LLMOptions;\n+    static providerName = \"aiCore\";\n+\n+    static defaultOptions: Partial<LLMOptions> = {\n+        model: \"anthropic--claude-3.7-sonnet\",",
        "comment_created_at": "2025-06-24T12:13:46+00:00",
        "comment_author": "recurseml[bot]",
        "comment_body": "Incorrect model name format. The model name uses wrong delimiter (double dash) and incorrect version format. According to Anthropic's documentation, the correct format should use single hyphens, like 'claude-3-7-sonnet'. The current format could cause model selection issues and API failures.\n\n\ud83d\udcda [Relevant Docs](https://docs.anthropic.com/en/docs/about-claude/models)\n\n---\n\n> *React with \ud83d\udc4d to tell me that this comment was useful, or \ud83d\udc4e if not (and I'll stop posting more comments like this in the future)*",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2146096128",
    "pr_number": 4524,
    "pr_file": "core/indexing/LanceDbIndex.ts",
    "created_at": "2025-06-13T21:09:49+00:00",
    "commented_code": "return [];\n     }\n     try {\n-      return await this.embeddingsProvider.embed(chunks.map((c) => c.content));\n+      return await this.embeddingsProvider.embed(chunks.map((c) => c.content), \"chunk\");",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2146096128",
        "repo_full_name": "continuedev/continue",
        "pr_number": 4524,
        "pr_file": "core/indexing/LanceDbIndex.ts",
        "discussion_id": "2146096128",
        "commented_code": "@@ -199,7 +199,7 @@ export class LanceDbIndex implements CodebaseIndex {\n       return [];\n     }\n     try {\n-      return await this.embeddingsProvider.embed(chunks.map((c) => c.content));\n+      return await this.embeddingsProvider.embed(chunks.map((c) => c.content), \"chunk\");",
        "comment_created_at": "2025-06-13T21:09:49+00:00",
        "comment_author": "recurseml[bot]",
        "comment_body": "Breaking change in ILLM interface implementation: The code is modifying embed() calls to include a new required parameter (task type string), but this breaks compatibility with existing ILLM interface definition which only expects chunks: string[] parameter. The interface in core/index.d.ts defines embed() as accepting chunks[] and EmbeddingTasks enum, not raw string literals. This change will cause runtime errors when the embeddings provider doesn't support the new parameter format.\n\n---\n\n> *React with \ud83d\udc4d to tell me that this comment was useful, or \ud83d\udc4e if not (and I'll stop posting more comments like this in the future)*",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1997703007",
    "pr_number": 4448,
    "pr_file": "core/llm/llms/index.ts",
    "created_at": "2025-03-16T20:40:44+00:00",
    "commented_code": "maxTokens:\n         finalCompletionOptions.maxTokens ??\n         cls.defaultOptions?.completionOptions?.maxTokens,\n+      autoCompleteMaxTokens:",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1997703007",
        "repo_full_name": "continuedev/continue",
        "pr_number": 4448,
        "pr_file": "core/llm/llms/index.ts",
        "discussion_id": "1997703007",
        "commented_code": "@@ -137,6 +138,11 @@ export async function llmFromDescription(\n       maxTokens:\n         finalCompletionOptions.maxTokens ??\n         cls.defaultOptions?.completionOptions?.maxTokens,\n+      autoCompleteMaxTokens:",
        "comment_created_at": "2025-03-16T20:40:44+00:00",
        "comment_author": "ferenci84",
        "comment_body": "We need this extra key in the LLMOptions, if we want to set different maxTokens limit for autocomplete than for any other use, because we cannot determine here whether this model is used for autocomplete or chat.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1986313875",
    "pr_number": 4559,
    "pr_file": "core/llm/openaiTypeConverters.ts",
    "created_at": "2025-03-09T12:45:05+00:00",
    "commented_code": "import { FimCreateParamsStreaming } from \"@continuedev/openai-adapters/dist/apis/base\";\n import {\n-  Chat,\n   ChatCompletion,\n   ChatCompletionAssistantMessageParam,\n   ChatCompletionChunk,\n   ChatCompletionCreateParams,\n   ChatCompletionMessageParam,\n-  ChatCompletionUserMessageParam,\n   CompletionCreateParams,\n } from \"openai/resources/index\";\n \n-import {\n-  ChatMessage,\n-  CompletionOptions,\n-  MessageContent,\n-  TextMessagePart,\n-} from \"..\";\n+import { ChatMessage, CompletionOptions, TextMessagePart } from \"..\";\n+\n+// Extend OpenAI API types to support DeepSeek reasoning_content field\n+interface DeepSeekDelta {",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1986313875",
        "repo_full_name": "continuedev/continue",
        "pr_number": 4559,
        "pr_file": "core/llm/openaiTypeConverters.ts",
        "discussion_id": "1986313875",
        "commented_code": "@@ -1,21 +1,32 @@\n import { FimCreateParamsStreaming } from \"@continuedev/openai-adapters/dist/apis/base\";\n import {\n-  Chat,\n   ChatCompletion,\n   ChatCompletionAssistantMessageParam,\n   ChatCompletionChunk,\n   ChatCompletionCreateParams,\n   ChatCompletionMessageParam,\n-  ChatCompletionUserMessageParam,\n   CompletionCreateParams,\n } from \"openai/resources/index\";\n \n-import {\n-  ChatMessage,\n-  CompletionOptions,\n-  MessageContent,\n-  TextMessagePart,\n-} from \"..\";\n+import { ChatMessage, CompletionOptions, TextMessagePart } from \"..\";\n+\n+// Extend OpenAI API types to support DeepSeek reasoning_content field\n+interface DeepSeekDelta {",
        "comment_created_at": "2025-03-09T12:45:05+00:00",
        "comment_author": "FallDownTheSystem",
        "comment_body": "The types for OpenAI's messages are imported from an external library, so to support DeepSeek's `reasoning_content` I needed to create those interfaces elsewhere. I'm not sure if this is the best place for them, but it works.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1847225928",
    "pr_number": 2974,
    "pr_file": "core/llm/llms/OpenAI.ts",
    "created_at": "2024-11-18T20:27:23+00:00",
    "commented_code": "suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1847225928",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-18T20:27:23+00:00",
        "comment_author": "sestinj",
        "comment_body": "@AnoyiX Can we make a dedicated `LLM` class instead of putting the logic in `OpenAI.ts`? A good example would be in `Deepseek.ts`, where we subclass `OpenAI` and adjust the FIM logic",
        "pr_file_module": null
      },
      {
        "comment_id": "1847545190",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-19T02:17:16+00:00",
        "comment_author": "AnoyiX",
        "comment_body": "It might be better to allow custom `endpoint`, there are so many providers, it\u2019s hard to support them all, what do you think?",
        "pr_file_module": null
      },
      {
        "comment_id": "1849597728",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-20T06:02:27+00:00",
        "comment_author": "sestinj",
        "comment_body": "I think that's fine as long as they have the exact same expected request body. I would just make it a function like `getFimEndpoint` so that it can be modified by subclasses, and then make a subclass of the endpoint. I'm not worried about having a large number of subclasses",
        "pr_file_module": null
      },
      {
        "comment_id": "1849607164",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-20T06:10:01+00:00",
        "comment_author": "AnoyiX",
        "comment_body": "Sounds good. It's convenient for subclasses to implement `getFimEndpoint`, this way we can avoid duplicating a lot of code in `*_streamFim`",
        "pr_file_module": null
      },
      {
        "comment_id": "1851725962",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-21T10:01:15+00:00",
        "comment_author": "AnoyiX",
        "comment_body": "you can review the latest code ~ \ud83d\ude0a\r\n\r\n<img width=\"1680\" alt=\"image\" src=\"https://github.com/user-attachments/assets/535ab566-0ff8-4a79-81ab-3f78306cbecf\">\r\n",
        "pr_file_module": null
      }
    ]
  }
]