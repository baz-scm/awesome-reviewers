[
  {
    "discussion_id": "1997703007",
    "pr_number": 4448,
    "pr_file": "core/llm/llms/index.ts",
    "created_at": "2025-03-16T20:40:44+00:00",
    "commented_code": "maxTokens:\n         finalCompletionOptions.maxTokens ??\n         cls.defaultOptions?.completionOptions?.maxTokens,\n+      autoCompleteMaxTokens:",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1997703007",
        "repo_full_name": "continuedev/continue",
        "pr_number": 4448,
        "pr_file": "core/llm/llms/index.ts",
        "discussion_id": "1997703007",
        "commented_code": "@@ -137,6 +138,11 @@ export async function llmFromDescription(\n       maxTokens:\n         finalCompletionOptions.maxTokens ??\n         cls.defaultOptions?.completionOptions?.maxTokens,\n+      autoCompleteMaxTokens:",
        "comment_created_at": "2025-03-16T20:40:44+00:00",
        "comment_author": "ferenci84",
        "comment_body": "We need this extra key in the LLMOptions, if we want to set different maxTokens limit for autocomplete than for any other use, because we cannot determine here whether this model is used for autocomplete or chat.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1986313875",
    "pr_number": 4559,
    "pr_file": "core/llm/openaiTypeConverters.ts",
    "created_at": "2025-03-09T12:45:05+00:00",
    "commented_code": "import { FimCreateParamsStreaming } from \"@continuedev/openai-adapters/dist/apis/base\";\n import {\n-  Chat,\n   ChatCompletion,\n   ChatCompletionAssistantMessageParam,\n   ChatCompletionChunk,\n   ChatCompletionCreateParams,\n   ChatCompletionMessageParam,\n-  ChatCompletionUserMessageParam,\n   CompletionCreateParams,\n } from \"openai/resources/index\";\n \n-import {\n-  ChatMessage,\n-  CompletionOptions,\n-  MessageContent,\n-  TextMessagePart,\n-} from \"..\";\n+import { ChatMessage, CompletionOptions, TextMessagePart } from \"..\";\n+\n+// Extend OpenAI API types to support DeepSeek reasoning_content field\n+interface DeepSeekDelta {",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1986313875",
        "repo_full_name": "continuedev/continue",
        "pr_number": 4559,
        "pr_file": "core/llm/openaiTypeConverters.ts",
        "discussion_id": "1986313875",
        "commented_code": "@@ -1,21 +1,32 @@\n import { FimCreateParamsStreaming } from \"@continuedev/openai-adapters/dist/apis/base\";\n import {\n-  Chat,\n   ChatCompletion,\n   ChatCompletionAssistantMessageParam,\n   ChatCompletionChunk,\n   ChatCompletionCreateParams,\n   ChatCompletionMessageParam,\n-  ChatCompletionUserMessageParam,\n   CompletionCreateParams,\n } from \"openai/resources/index\";\n \n-import {\n-  ChatMessage,\n-  CompletionOptions,\n-  MessageContent,\n-  TextMessagePart,\n-} from \"..\";\n+import { ChatMessage, CompletionOptions, TextMessagePart } from \"..\";\n+\n+// Extend OpenAI API types to support DeepSeek reasoning_content field\n+interface DeepSeekDelta {",
        "comment_created_at": "2025-03-09T12:45:05+00:00",
        "comment_author": "FallDownTheSystem",
        "comment_body": "The types for OpenAI's messages are imported from an external library, so to support DeepSeek's `reasoning_content` I needed to create those interfaces elsewhere. I'm not sure if this is the best place for them, but it works.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1847225928",
    "pr_number": 2974,
    "pr_file": "core/llm/llms/OpenAI.ts",
    "created_at": "2024-11-18T20:27:23+00:00",
    "commented_code": "suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1847225928",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-18T20:27:23+00:00",
        "comment_author": "sestinj",
        "comment_body": "@AnoyiX Can we make a dedicated `LLM` class instead of putting the logic in `OpenAI.ts`? A good example would be in `Deepseek.ts`, where we subclass `OpenAI` and adjust the FIM logic",
        "pr_file_module": null
      },
      {
        "comment_id": "1847545190",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-19T02:17:16+00:00",
        "comment_author": "AnoyiX",
        "comment_body": "It might be better to allow custom `endpoint`, there are so many providers, itâ€™s hard to support them all, what do you think?",
        "pr_file_module": null
      },
      {
        "comment_id": "1849597728",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-20T06:02:27+00:00",
        "comment_author": "sestinj",
        "comment_body": "I think that's fine as long as they have the exact same expected request body. I would just make it a function like `getFimEndpoint` so that it can be modified by subclasses, and then make a subclass of the endpoint. I'm not worried about having a large number of subclasses",
        "pr_file_module": null
      },
      {
        "comment_id": "1849607164",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-20T06:10:01+00:00",
        "comment_author": "AnoyiX",
        "comment_body": "Sounds good. It's convenient for subclasses to implement `getFimEndpoint`, this way we can avoid duplicating a lot of code in `*_streamFim`",
        "pr_file_module": null
      },
      {
        "comment_id": "1851725962",
        "repo_full_name": "continuedev/continue",
        "pr_number": 2974,
        "pr_file": "core/llm/llms/OpenAI.ts",
        "discussion_id": "1847225928",
        "commented_code": "@@ -288,7 +288,8 @@ class OpenAI extends BaseLLM {\n     suffix: string,\n     options: CompletionOptions,\n   ): AsyncGenerator<string> {\n-    const endpoint = new URL(\"fim/completions\", this.apiBase);\n+    const url = this.apiBase?.includes(\"api.siliconflow.cn\") ? \"completions\" : \"fim/completions\"",
        "comment_created_at": "2024-11-21T10:01:15+00:00",
        "comment_author": "AnoyiX",
        "comment_body": "you can review the latest code ~ ðŸ˜Š\r\n\r\n<img width=\"1680\" alt=\"image\" src=\"https://github.com/user-attachments/assets/535ab566-0ff8-4a79-81ab-3f78306cbecf\">\r\n",
        "pr_file_module": null
      }
    ]
  }
]