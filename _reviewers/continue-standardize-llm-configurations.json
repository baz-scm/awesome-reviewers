[
  {
    "discussion_id": "2176032869",
    "pr_number": 6349,
    "pr_file": "extensions/vscode/e2e/README.md",
    "created_at": "2025-06-30T21:54:27+00:00",
    "commented_code": "- Is your `data-testid` or selector actually just wrong?\n - Are you inconsistently getting different behaviors? You can try adding a `TestUtils.waitForTimeout` between two events if you think it's caused by a race condition. Note that this may lead to flake down the road.\n - Alternatively, you can add a `TestUtils.waitForSuccess`\n+\n+### How is it connecting to the LLM?\n+\n+- This depends on the model provider written inside the `config.json`. If the model provider is `mock`, it will use `MockLLM`, and if it uses `test`, it will use `TestLLM`.",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "2176032869",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6349,
        "pr_file": "extensions/vscode/e2e/README.md",
        "discussion_id": "2176032869",
        "commented_code": "@@ -31,3 +31,11 @@ All e2e tests are separated (by folder) into\n - Is your `data-testid` or selector actually just wrong?\n - Are you inconsistently getting different behaviors? You can try adding a `TestUtils.waitForTimeout` between two events if you think it's caused by a race condition. Note that this may lead to flake down the road.\n - Alternatively, you can add a `TestUtils.waitForSuccess`\n+\n+### How is it connecting to the LLM?\n+\n+- This depends on the model provider written inside the `config.json`. If the model provider is `mock`, it will use `MockLLM`, and if it uses `test`, it will use `TestLLM`.",
        "comment_created_at": "2025-06-30T21:54:27+00:00",
        "comment_author": "Patrick-Erichsen",
        "comment_body": "```suggestion\r\n- This depends on the model provider written inside the `config.yaml`. If the model provider is `mock`, it will use `MockLLM`, and if it uses `test`, it will use `TestLLM`.\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2176070269",
        "repo_full_name": "continuedev/continue",
        "pr_number": 6349,
        "pr_file": "extensions/vscode/e2e/README.md",
        "discussion_id": "2176032869",
        "commented_code": "@@ -31,3 +31,11 @@ All e2e tests are separated (by folder) into\n - Is your `data-testid` or selector actually just wrong?\n - Are you inconsistently getting different behaviors? You can try adding a `TestUtils.waitForTimeout` between two events if you think it's caused by a race condition. Note that this may lead to flake down the road.\n - Alternatively, you can add a `TestUtils.waitForSuccess`\n+\n+### How is it connecting to the LLM?\n+\n+- This depends on the model provider written inside the `config.json`. If the model provider is `mock`, it will use `MockLLM`, and if it uses `test`, it will use `TestLLM`.",
        "comment_created_at": "2025-06-30T22:24:34+00:00",
        "comment_author": "jpoly1219",
        "comment_body": "clarification: this should be `test-continue/config.json`. I haven't tested it with config.yaml.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1693236104",
    "pr_number": 1810,
    "pr_file": "docs/docs/setup/configuration.md",
    "created_at": "2024-07-26T15:10:22+00:00",
    "commented_code": "return config;\n }\n ```\n+\n+\n+## Customizing the LLM capability\n+Sometimes when you use certain third-party interface providers, such as [one api](https://github.com/songquanpeng/one-api), [openRouter](https://openrouter.ai/), etc., they offer openai-compatible interfaces, but the model names are highly different. \n+\n+Continue cannot determine the capabilities of the model, whether it supports uploading images or files, etc. You can clearly inform Continue about the capabilities of the model.\n+\n+```typescript title=\"~/.continue/config.json\"\n+{\n+  \"models\": [\n+    {\n+      \"title\": \"kimi\",\n+      \"provider\": \"openai\",\n+      \"model\": \"moonshot-kimi\",\n+      \"contextLength\": 8192,\n+      \"apiBase\": \"https://any-your-thrid-part-OpenAI-compatible-api-provider/v1\",\n+      \"capability\": {\n+        uploadImage: true",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1693236104",
        "repo_full_name": "continuedev/continue",
        "pr_number": 1810,
        "pr_file": "docs/docs/setup/configuration.md",
        "discussion_id": "1693236104",
        "commented_code": "@@ -322,3 +322,26 @@ export function modifyConfig(config: Config): Config {\n   return config;\n }\n ```\n+\n+\n+## Customizing the LLM capability\n+Sometimes when you use certain third-party interface providers, such as [one api](https://github.com/songquanpeng/one-api), [openRouter](https://openrouter.ai/), etc., they offer openai-compatible interfaces, but the model names are highly different. \n+\n+Continue cannot determine the capabilities of the model, whether it supports uploading images or files, etc. You can clearly inform Continue about the capabilities of the model.\n+\n+```typescript title=\"~/.continue/config.json\"\n+{\n+  \"models\": [\n+    {\n+      \"title\": \"kimi\",\n+      \"provider\": \"openai\",\n+      \"model\": \"moonshot-kimi\",\n+      \"contextLength\": 8192,\n+      \"apiBase\": \"https://any-your-thrid-part-OpenAI-compatible-api-provider/v1\",\n+      \"capability\": {\n+        uploadImage: true",
        "comment_created_at": "2024-07-26T15:10:22+00:00",
        "comment_author": "Patrick-Erichsen",
        "comment_body": "```suggestion\r\n        \"uploadImage\": true\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1986314693",
    "pr_number": 4559,
    "pr_file": "docs/docs/json-reference.md",
    "created_at": "2025-03-09T12:48:56+00:00",
    "commented_code": "- `engine`: Engine for Azure OpenAI requests.\n - `capabilities`: Override auto-detected capabilities:\n   - `uploadImage`: Boolean indicating if the model supports image uploads.\n+  - `tools`: Boolean indicating if the model supports tool use.",
    "repo_full_name": "continuedev/continue",
    "discussion_comments": [
      {
        "comment_id": "1986314693",
        "repo_full_name": "continuedev/continue",
        "pr_number": 4559,
        "pr_file": "docs/docs/json-reference.md",
        "discussion_id": "1986314693",
        "commented_code": "@@ -36,6 +36,8 @@ Each model has specific configuration options tailored to its provider and funct\n - `engine`: Engine for Azure OpenAI requests.\n - `capabilities`: Override auto-detected capabilities:\n   - `uploadImage`: Boolean indicating if the model supports image uploads.\n+  - `tools`: Boolean indicating if the model supports tool use.",
        "comment_created_at": "2025-03-09T12:48:56+00:00",
        "comment_author": "FallDownTheSystem",
        "comment_body": "Tools was missing so I added that along with the new thinking capability",
        "pr_file_module": null
      }
    ]
  }
]