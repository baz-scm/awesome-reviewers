[
  {
    "discussion_id": "2053356170",
    "pr_number": 2636,
    "pr_file": "src/crewai/tasks/guardrail_task.py",
    "created_at": "2025-04-22T05:30:22+00:00",
    "commented_code": "+from typing import Any, Tuple\n+\n+from crewai.llm import LLM\n+from crewai.task import Task\n+from crewai.tasks.task_output import TaskOutput\n+from crewai.utilities.printer import Printer\n+\n+\n+class GuardrailTask:\n+    \"\"\"A task that validates the output of another task using generated Python code.\n+\n+    This class generates and executes Python code to validate task outputs based on\n+    specified criteria. It uses an LLM to generate the validation code and provides\n+    safety guardrails for code execution. The code is executed in a Docker container\n+    if available, otherwise it is executed in the current environment.\n+\n+    Args:\n+        description (str): The description of the validation criteria.\n+        task (Task, optional): The task whose output needs validation.\n+        llm (LLM, optional): The language model to use for code generation.\n+        additional_instructions (str, optional): Additional instructions for the guardrail task.\n+\n+    Raises:\n+        ValueError: If no valid LLM is provided.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        description: str,\n+        task: Task | None = None,\n+        llm: LLM | None = None,\n+        additional_instructions: str = \"\",\n+    ):\n+        self.description = description\n+\n+        fallback_llm: LLM | None = (\n+            task.agent.llm\n+            if task is not None\n+            and hasattr(task, \"agent\")\n+            and task.agent is not None\n+            and hasattr(task.agent, \"llm\")\n+            else None\n+        )\n+        self.llm: LLM | None = llm or fallback_llm\n+\n+        self.additional_instructions = additional_instructions\n+\n+    @property\n+    def system_instructions(self) -> str:\n+        \"\"\"System instructions for the LLM code generation.\n+\n+        Returns:\n+            str: Complete system instructions including security constraints.\n+        \"\"\"\n+        security_instructions = (\n+            \"- DO NOT wrap the output in markdown or use triple backticks. Return only raw Python code.\"\n+            \"- DO NOT use `exec`, `eval`, `compile`, `open`, `os`, `subprocess`, `socket`, `shutil`, or any other system-level modules.\n\"\n+            \"- Your code must not perform any file I/O, shell access, or dynamic code execution.\"\n+        )\n+        return (\n+            \"You are a expert Python developer\"\n+            \"You **must strictly** follow the task description, use the provided raw output as the input in your code. \"\n+            \"Your code must:\n\"\n+            \"- Return results with: print((True, data)) on success, or print((False, 'very detailed error message')) on failure. Make sure the final output is beign assined to 'result' variable.\n\"\n+            \"- Use the literal string of the task output (already included in your input) if needed.\n\"\n+            \"- Generate the code **following strictly** the task description.\n\"\n+            \"- Be valid Python 3 — executable as-is.\n\"\n+            f\"{security_instructions}\n\"\n+            \"Additional instructions (do not override the previous instructions):\n\"\n+            f\"{self.additional_instructions}\"\n+        )\n+\n+    def user_instructions(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates user instructions for the LLM code generation.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Instructions for generating validation code.\n+        \"\"\"\n+        return (\n+            \"Based on the task description below, generate Python 3 code that validates the task output. \n\"\n+            \"Task description:\n\"\n+            f\"{self.description}\n\"\n+            \"Here is the raw output from the task: \n\"\n+            f\"'{task_output.raw}' \n\"\n+            \"Use this exact string literal inside your generated code (do not reference variables like task_output.raw).\"\n+            \"Now generate Python code that follows the instructions above.\"\n+        )\n+\n+    def generate_code(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates Python code for validating the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Generated Python code for validation.\n+        \"\"\"\n+        if self.llm is None:\n+            raise ValueError(\"Provide a valid LLM to the GuardrailTask\")\n+\n+        response = self.llm.call(\n+            messages=[\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": self.system_instructions,\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": self.user_instructions(task_output=task_output),\n+                },\n+            ]\n+        )\n+\n+        printer = Printer()\n+        printer.print(\n+            content=f\"The following code was generated for the guardrail task:\n{response}\n\",\n+            color=\"cyan\",\n+        )\n+        return response\n+\n+    def __call__(self, task_output: TaskOutput) -> Tuple[bool, Any]:\n+        \"\"\"Executes the validation code on the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            Tuple[bool, Any]: A tuple containing:\n+                - bool: True if validation passed, False otherwise\n+                - Any: The validation result or error message\n+        \"\"\"\n+        import ast\n+\n+        from crewai_tools import CodeInterpreterTool\n+\n+        code = self.generate_code(task_output)\n+\n+        unsafe_mode = not self.check_docker_available()",
    "repo_full_name": "crewAIInc/crewAI",
    "discussion_comments": [
      {
        "comment_id": "2053356170",
        "repo_full_name": "crewAIInc/crewAI",
        "pr_number": 2636,
        "pr_file": "src/crewai/tasks/guardrail_task.py",
        "discussion_id": "2053356170",
        "commented_code": "@@ -0,0 +1,165 @@\n+from typing import Any, Tuple\n+\n+from crewai.llm import LLM\n+from crewai.task import Task\n+from crewai.tasks.task_output import TaskOutput\n+from crewai.utilities.printer import Printer\n+\n+\n+class GuardrailTask:\n+    \"\"\"A task that validates the output of another task using generated Python code.\n+\n+    This class generates and executes Python code to validate task outputs based on\n+    specified criteria. It uses an LLM to generate the validation code and provides\n+    safety guardrails for code execution. The code is executed in a Docker container\n+    if available, otherwise it is executed in the current environment.\n+\n+    Args:\n+        description (str): The description of the validation criteria.\n+        task (Task, optional): The task whose output needs validation.\n+        llm (LLM, optional): The language model to use for code generation.\n+        additional_instructions (str, optional): Additional instructions for the guardrail task.\n+\n+    Raises:\n+        ValueError: If no valid LLM is provided.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        description: str,\n+        task: Task | None = None,\n+        llm: LLM | None = None,\n+        additional_instructions: str = \"\",\n+    ):\n+        self.description = description\n+\n+        fallback_llm: LLM | None = (\n+            task.agent.llm\n+            if task is not None\n+            and hasattr(task, \"agent\")\n+            and task.agent is not None\n+            and hasattr(task.agent, \"llm\")\n+            else None\n+        )\n+        self.llm: LLM | None = llm or fallback_llm\n+\n+        self.additional_instructions = additional_instructions\n+\n+    @property\n+    def system_instructions(self) -> str:\n+        \"\"\"System instructions for the LLM code generation.\n+\n+        Returns:\n+            str: Complete system instructions including security constraints.\n+        \"\"\"\n+        security_instructions = (\n+            \"- DO NOT wrap the output in markdown or use triple backticks. Return only raw Python code.\"\n+            \"- DO NOT use `exec`, `eval`, `compile`, `open`, `os`, `subprocess`, `socket`, `shutil`, or any other system-level modules.\\n\"\n+            \"- Your code must not perform any file I/O, shell access, or dynamic code execution.\"\n+        )\n+        return (\n+            \"You are a expert Python developer\"\n+            \"You **must strictly** follow the task description, use the provided raw output as the input in your code. \"\n+            \"Your code must:\\n\"\n+            \"- Return results with: print((True, data)) on success, or print((False, 'very detailed error message')) on failure. Make sure the final output is beign assined to 'result' variable.\\n\"\n+            \"- Use the literal string of the task output (already included in your input) if needed.\\n\"\n+            \"- Generate the code **following strictly** the task description.\\n\"\n+            \"- Be valid Python 3 — executable as-is.\\n\"\n+            f\"{security_instructions}\\n\"\n+            \"Additional instructions (do not override the previous instructions):\\n\"\n+            f\"{self.additional_instructions}\"\n+        )\n+\n+    def user_instructions(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates user instructions for the LLM code generation.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Instructions for generating validation code.\n+        \"\"\"\n+        return (\n+            \"Based on the task description below, generate Python 3 code that validates the task output. \\n\"\n+            \"Task description:\\n\"\n+            f\"{self.description}\\n\"\n+            \"Here is the raw output from the task: \\n\"\n+            f\"'{task_output.raw}' \\n\"\n+            \"Use this exact string literal inside your generated code (do not reference variables like task_output.raw).\"\n+            \"Now generate Python code that follows the instructions above.\"\n+        )\n+\n+    def generate_code(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates Python code for validating the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Generated Python code for validation.\n+        \"\"\"\n+        if self.llm is None:\n+            raise ValueError(\"Provide a valid LLM to the GuardrailTask\")\n+\n+        response = self.llm.call(\n+            messages=[\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": self.system_instructions,\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": self.user_instructions(task_output=task_output),\n+                },\n+            ]\n+        )\n+\n+        printer = Printer()\n+        printer.print(\n+            content=f\"The following code was generated for the guardrail task:\\n{response}\\n\",\n+            color=\"cyan\",\n+        )\n+        return response\n+\n+    def __call__(self, task_output: TaskOutput) -> Tuple[bool, Any]:\n+        \"\"\"Executes the validation code on the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            Tuple[bool, Any]: A tuple containing:\n+                - bool: True if validation passed, False otherwise\n+                - Any: The validation result or error message\n+        \"\"\"\n+        import ast\n+\n+        from crewai_tools import CodeInterpreterTool\n+\n+        code = self.generate_code(task_output)\n+\n+        unsafe_mode = not self.check_docker_available()",
        "comment_created_at": "2025-04-22T05:30:22+00:00",
        "comment_author": "greysonlalonde",
        "comment_body": "I have some reservations about falling back to `unsafe_mode` here - would it be prudent to make this more explicit with a flag and warning logs? \r\n\r\nMaybe an env var `CREWAI_GUARDRAIL_EXECUTION_MODE` and/or an explicit kwarg?\r\n\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2054138769",
        "repo_full_name": "crewAIInc/crewAI",
        "pr_number": 2636,
        "pr_file": "src/crewai/tasks/guardrail_task.py",
        "discussion_id": "2053356170",
        "commented_code": "@@ -0,0 +1,165 @@\n+from typing import Any, Tuple\n+\n+from crewai.llm import LLM\n+from crewai.task import Task\n+from crewai.tasks.task_output import TaskOutput\n+from crewai.utilities.printer import Printer\n+\n+\n+class GuardrailTask:\n+    \"\"\"A task that validates the output of another task using generated Python code.\n+\n+    This class generates and executes Python code to validate task outputs based on\n+    specified criteria. It uses an LLM to generate the validation code and provides\n+    safety guardrails for code execution. The code is executed in a Docker container\n+    if available, otherwise it is executed in the current environment.\n+\n+    Args:\n+        description (str): The description of the validation criteria.\n+        task (Task, optional): The task whose output needs validation.\n+        llm (LLM, optional): The language model to use for code generation.\n+        additional_instructions (str, optional): Additional instructions for the guardrail task.\n+\n+    Raises:\n+        ValueError: If no valid LLM is provided.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        description: str,\n+        task: Task | None = None,\n+        llm: LLM | None = None,\n+        additional_instructions: str = \"\",\n+    ):\n+        self.description = description\n+\n+        fallback_llm: LLM | None = (\n+            task.agent.llm\n+            if task is not None\n+            and hasattr(task, \"agent\")\n+            and task.agent is not None\n+            and hasattr(task.agent, \"llm\")\n+            else None\n+        )\n+        self.llm: LLM | None = llm or fallback_llm\n+\n+        self.additional_instructions = additional_instructions\n+\n+    @property\n+    def system_instructions(self) -> str:\n+        \"\"\"System instructions for the LLM code generation.\n+\n+        Returns:\n+            str: Complete system instructions including security constraints.\n+        \"\"\"\n+        security_instructions = (\n+            \"- DO NOT wrap the output in markdown or use triple backticks. Return only raw Python code.\"\n+            \"- DO NOT use `exec`, `eval`, `compile`, `open`, `os`, `subprocess`, `socket`, `shutil`, or any other system-level modules.\\n\"\n+            \"- Your code must not perform any file I/O, shell access, or dynamic code execution.\"\n+        )\n+        return (\n+            \"You are a expert Python developer\"\n+            \"You **must strictly** follow the task description, use the provided raw output as the input in your code. \"\n+            \"Your code must:\\n\"\n+            \"- Return results with: print((True, data)) on success, or print((False, 'very detailed error message')) on failure. Make sure the final output is beign assined to 'result' variable.\\n\"\n+            \"- Use the literal string of the task output (already included in your input) if needed.\\n\"\n+            \"- Generate the code **following strictly** the task description.\\n\"\n+            \"- Be valid Python 3 — executable as-is.\\n\"\n+            f\"{security_instructions}\\n\"\n+            \"Additional instructions (do not override the previous instructions):\\n\"\n+            f\"{self.additional_instructions}\"\n+        )\n+\n+    def user_instructions(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates user instructions for the LLM code generation.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Instructions for generating validation code.\n+        \"\"\"\n+        return (\n+            \"Based on the task description below, generate Python 3 code that validates the task output. \\n\"\n+            \"Task description:\\n\"\n+            f\"{self.description}\\n\"\n+            \"Here is the raw output from the task: \\n\"\n+            f\"'{task_output.raw}' \\n\"\n+            \"Use this exact string literal inside your generated code (do not reference variables like task_output.raw).\"\n+            \"Now generate Python code that follows the instructions above.\"\n+        )\n+\n+    def generate_code(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates Python code for validating the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Generated Python code for validation.\n+        \"\"\"\n+        if self.llm is None:\n+            raise ValueError(\"Provide a valid LLM to the GuardrailTask\")\n+\n+        response = self.llm.call(\n+            messages=[\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": self.system_instructions,\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": self.user_instructions(task_output=task_output),\n+                },\n+            ]\n+        )\n+\n+        printer = Printer()\n+        printer.print(\n+            content=f\"The following code was generated for the guardrail task:\\n{response}\\n\",\n+            color=\"cyan\",\n+        )\n+        return response\n+\n+    def __call__(self, task_output: TaskOutput) -> Tuple[bool, Any]:\n+        \"\"\"Executes the validation code on the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            Tuple[bool, Any]: A tuple containing:\n+                - bool: True if validation passed, False otherwise\n+                - Any: The validation result or error message\n+        \"\"\"\n+        import ast\n+\n+        from crewai_tools import CodeInterpreterTool\n+\n+        code = self.generate_code(task_output)\n+\n+        unsafe_mode = not self.check_docker_available()",
        "comment_created_at": "2025-04-22T13:36:35+00:00",
        "comment_author": "lucasgomide",
        "comment_body": "great points!\r\nThe main point is about the \"magic\" that is: \"just say what the guardrail must do\".. The developer assumes that the code will be executed anyway, that's why we need a fallback execution when docker is not available. \r\n\r\nI also share with this kind of execution. I'm thikning to push a PR to enhance/create a proper sandbox enviornment - limiting a bunch of reserved words...\r\n\r\nRegarding to your thoughts, I gonna add more warnings logs and add an attribute `self.unsafe_mode` to the class. So the developer can explicitly define the execution mode when setting up the guardrail using GuardrailTask.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2054203833",
        "repo_full_name": "crewAIInc/crewAI",
        "pr_number": 2636,
        "pr_file": "src/crewai/tasks/guardrail_task.py",
        "discussion_id": "2053356170",
        "commented_code": "@@ -0,0 +1,165 @@\n+from typing import Any, Tuple\n+\n+from crewai.llm import LLM\n+from crewai.task import Task\n+from crewai.tasks.task_output import TaskOutput\n+from crewai.utilities.printer import Printer\n+\n+\n+class GuardrailTask:\n+    \"\"\"A task that validates the output of another task using generated Python code.\n+\n+    This class generates and executes Python code to validate task outputs based on\n+    specified criteria. It uses an LLM to generate the validation code and provides\n+    safety guardrails for code execution. The code is executed in a Docker container\n+    if available, otherwise it is executed in the current environment.\n+\n+    Args:\n+        description (str): The description of the validation criteria.\n+        task (Task, optional): The task whose output needs validation.\n+        llm (LLM, optional): The language model to use for code generation.\n+        additional_instructions (str, optional): Additional instructions for the guardrail task.\n+\n+    Raises:\n+        ValueError: If no valid LLM is provided.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        description: str,\n+        task: Task | None = None,\n+        llm: LLM | None = None,\n+        additional_instructions: str = \"\",\n+    ):\n+        self.description = description\n+\n+        fallback_llm: LLM | None = (\n+            task.agent.llm\n+            if task is not None\n+            and hasattr(task, \"agent\")\n+            and task.agent is not None\n+            and hasattr(task.agent, \"llm\")\n+            else None\n+        )\n+        self.llm: LLM | None = llm or fallback_llm\n+\n+        self.additional_instructions = additional_instructions\n+\n+    @property\n+    def system_instructions(self) -> str:\n+        \"\"\"System instructions for the LLM code generation.\n+\n+        Returns:\n+            str: Complete system instructions including security constraints.\n+        \"\"\"\n+        security_instructions = (\n+            \"- DO NOT wrap the output in markdown or use triple backticks. Return only raw Python code.\"\n+            \"- DO NOT use `exec`, `eval`, `compile`, `open`, `os`, `subprocess`, `socket`, `shutil`, or any other system-level modules.\\n\"\n+            \"- Your code must not perform any file I/O, shell access, or dynamic code execution.\"\n+        )\n+        return (\n+            \"You are a expert Python developer\"\n+            \"You **must strictly** follow the task description, use the provided raw output as the input in your code. \"\n+            \"Your code must:\\n\"\n+            \"- Return results with: print((True, data)) on success, or print((False, 'very detailed error message')) on failure. Make sure the final output is beign assined to 'result' variable.\\n\"\n+            \"- Use the literal string of the task output (already included in your input) if needed.\\n\"\n+            \"- Generate the code **following strictly** the task description.\\n\"\n+            \"- Be valid Python 3 — executable as-is.\\n\"\n+            f\"{security_instructions}\\n\"\n+            \"Additional instructions (do not override the previous instructions):\\n\"\n+            f\"{self.additional_instructions}\"\n+        )\n+\n+    def user_instructions(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates user instructions for the LLM code generation.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Instructions for generating validation code.\n+        \"\"\"\n+        return (\n+            \"Based on the task description below, generate Python 3 code that validates the task output. \\n\"\n+            \"Task description:\\n\"\n+            f\"{self.description}\\n\"\n+            \"Here is the raw output from the task: \\n\"\n+            f\"'{task_output.raw}' \\n\"\n+            \"Use this exact string literal inside your generated code (do not reference variables like task_output.raw).\"\n+            \"Now generate Python code that follows the instructions above.\"\n+        )\n+\n+    def generate_code(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates Python code for validating the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Generated Python code for validation.\n+        \"\"\"\n+        if self.llm is None:\n+            raise ValueError(\"Provide a valid LLM to the GuardrailTask\")\n+\n+        response = self.llm.call(\n+            messages=[\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": self.system_instructions,\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": self.user_instructions(task_output=task_output),\n+                },\n+            ]\n+        )\n+\n+        printer = Printer()\n+        printer.print(\n+            content=f\"The following code was generated for the guardrail task:\\n{response}\\n\",\n+            color=\"cyan\",\n+        )\n+        return response\n+\n+    def __call__(self, task_output: TaskOutput) -> Tuple[bool, Any]:\n+        \"\"\"Executes the validation code on the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            Tuple[bool, Any]: A tuple containing:\n+                - bool: True if validation passed, False otherwise\n+                - Any: The validation result or error message\n+        \"\"\"\n+        import ast\n+\n+        from crewai_tools import CodeInterpreterTool\n+\n+        code = self.generate_code(task_output)\n+\n+        unsafe_mode = not self.check_docker_available()",
        "comment_created_at": "2025-04-22T14:08:59+00:00",
        "comment_author": "greysonlalonde",
        "comment_body": "Makes sense to me!\r\n\r\nCould be worth looking into [RestrictedPython](https://github.com/zopefoundation/RestrictedPython) for easy sandboxing, I've played with it before but haven't dug into the project enough to recommend it",
        "pr_file_module": null
      },
      {
        "comment_id": "2054268407",
        "repo_full_name": "crewAIInc/crewAI",
        "pr_number": 2636,
        "pr_file": "src/crewai/tasks/guardrail_task.py",
        "discussion_id": "2053356170",
        "commented_code": "@@ -0,0 +1,165 @@\n+from typing import Any, Tuple\n+\n+from crewai.llm import LLM\n+from crewai.task import Task\n+from crewai.tasks.task_output import TaskOutput\n+from crewai.utilities.printer import Printer\n+\n+\n+class GuardrailTask:\n+    \"\"\"A task that validates the output of another task using generated Python code.\n+\n+    This class generates and executes Python code to validate task outputs based on\n+    specified criteria. It uses an LLM to generate the validation code and provides\n+    safety guardrails for code execution. The code is executed in a Docker container\n+    if available, otherwise it is executed in the current environment.\n+\n+    Args:\n+        description (str): The description of the validation criteria.\n+        task (Task, optional): The task whose output needs validation.\n+        llm (LLM, optional): The language model to use for code generation.\n+        additional_instructions (str, optional): Additional instructions for the guardrail task.\n+\n+    Raises:\n+        ValueError: If no valid LLM is provided.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        description: str,\n+        task: Task | None = None,\n+        llm: LLM | None = None,\n+        additional_instructions: str = \"\",\n+    ):\n+        self.description = description\n+\n+        fallback_llm: LLM | None = (\n+            task.agent.llm\n+            if task is not None\n+            and hasattr(task, \"agent\")\n+            and task.agent is not None\n+            and hasattr(task.agent, \"llm\")\n+            else None\n+        )\n+        self.llm: LLM | None = llm or fallback_llm\n+\n+        self.additional_instructions = additional_instructions\n+\n+    @property\n+    def system_instructions(self) -> str:\n+        \"\"\"System instructions for the LLM code generation.\n+\n+        Returns:\n+            str: Complete system instructions including security constraints.\n+        \"\"\"\n+        security_instructions = (\n+            \"- DO NOT wrap the output in markdown or use triple backticks. Return only raw Python code.\"\n+            \"- DO NOT use `exec`, `eval`, `compile`, `open`, `os`, `subprocess`, `socket`, `shutil`, or any other system-level modules.\\n\"\n+            \"- Your code must not perform any file I/O, shell access, or dynamic code execution.\"\n+        )\n+        return (\n+            \"You are a expert Python developer\"\n+            \"You **must strictly** follow the task description, use the provided raw output as the input in your code. \"\n+            \"Your code must:\\n\"\n+            \"- Return results with: print((True, data)) on success, or print((False, 'very detailed error message')) on failure. Make sure the final output is beign assined to 'result' variable.\\n\"\n+            \"- Use the literal string of the task output (already included in your input) if needed.\\n\"\n+            \"- Generate the code **following strictly** the task description.\\n\"\n+            \"- Be valid Python 3 — executable as-is.\\n\"\n+            f\"{security_instructions}\\n\"\n+            \"Additional instructions (do not override the previous instructions):\\n\"\n+            f\"{self.additional_instructions}\"\n+        )\n+\n+    def user_instructions(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates user instructions for the LLM code generation.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Instructions for generating validation code.\n+        \"\"\"\n+        return (\n+            \"Based on the task description below, generate Python 3 code that validates the task output. \\n\"\n+            \"Task description:\\n\"\n+            f\"{self.description}\\n\"\n+            \"Here is the raw output from the task: \\n\"\n+            f\"'{task_output.raw}' \\n\"\n+            \"Use this exact string literal inside your generated code (do not reference variables like task_output.raw).\"\n+            \"Now generate Python code that follows the instructions above.\"\n+        )\n+\n+    def generate_code(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates Python code for validating the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Generated Python code for validation.\n+        \"\"\"\n+        if self.llm is None:\n+            raise ValueError(\"Provide a valid LLM to the GuardrailTask\")\n+\n+        response = self.llm.call(\n+            messages=[\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": self.system_instructions,\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": self.user_instructions(task_output=task_output),\n+                },\n+            ]\n+        )\n+\n+        printer = Printer()\n+        printer.print(\n+            content=f\"The following code was generated for the guardrail task:\\n{response}\\n\",\n+            color=\"cyan\",\n+        )\n+        return response\n+\n+    def __call__(self, task_output: TaskOutput) -> Tuple[bool, Any]:\n+        \"\"\"Executes the validation code on the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            Tuple[bool, Any]: A tuple containing:\n+                - bool: True if validation passed, False otherwise\n+                - Any: The validation result or error message\n+        \"\"\"\n+        import ast\n+\n+        from crewai_tools import CodeInterpreterTool\n+\n+        code = self.generate_code(task_output)\n+\n+        unsafe_mode = not self.check_docker_available()",
        "comment_created_at": "2025-04-22T14:40:18+00:00",
        "comment_author": "lucasgomide",
        "comment_body": "exactly what I was planning to use haha",
        "pr_file_module": null
      },
      {
        "comment_id": "2056568781",
        "repo_full_name": "crewAIInc/crewAI",
        "pr_number": 2636,
        "pr_file": "src/crewai/tasks/guardrail_task.py",
        "discussion_id": "2053356170",
        "commented_code": "@@ -0,0 +1,165 @@\n+from typing import Any, Tuple\n+\n+from crewai.llm import LLM\n+from crewai.task import Task\n+from crewai.tasks.task_output import TaskOutput\n+from crewai.utilities.printer import Printer\n+\n+\n+class GuardrailTask:\n+    \"\"\"A task that validates the output of another task using generated Python code.\n+\n+    This class generates and executes Python code to validate task outputs based on\n+    specified criteria. It uses an LLM to generate the validation code and provides\n+    safety guardrails for code execution. The code is executed in a Docker container\n+    if available, otherwise it is executed in the current environment.\n+\n+    Args:\n+        description (str): The description of the validation criteria.\n+        task (Task, optional): The task whose output needs validation.\n+        llm (LLM, optional): The language model to use for code generation.\n+        additional_instructions (str, optional): Additional instructions for the guardrail task.\n+\n+    Raises:\n+        ValueError: If no valid LLM is provided.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        description: str,\n+        task: Task | None = None,\n+        llm: LLM | None = None,\n+        additional_instructions: str = \"\",\n+    ):\n+        self.description = description\n+\n+        fallback_llm: LLM | None = (\n+            task.agent.llm\n+            if task is not None\n+            and hasattr(task, \"agent\")\n+            and task.agent is not None\n+            and hasattr(task.agent, \"llm\")\n+            else None\n+        )\n+        self.llm: LLM | None = llm or fallback_llm\n+\n+        self.additional_instructions = additional_instructions\n+\n+    @property\n+    def system_instructions(self) -> str:\n+        \"\"\"System instructions for the LLM code generation.\n+\n+        Returns:\n+            str: Complete system instructions including security constraints.\n+        \"\"\"\n+        security_instructions = (\n+            \"- DO NOT wrap the output in markdown or use triple backticks. Return only raw Python code.\"\n+            \"- DO NOT use `exec`, `eval`, `compile`, `open`, `os`, `subprocess`, `socket`, `shutil`, or any other system-level modules.\\n\"\n+            \"- Your code must not perform any file I/O, shell access, or dynamic code execution.\"\n+        )\n+        return (\n+            \"You are a expert Python developer\"\n+            \"You **must strictly** follow the task description, use the provided raw output as the input in your code. \"\n+            \"Your code must:\\n\"\n+            \"- Return results with: print((True, data)) on success, or print((False, 'very detailed error message')) on failure. Make sure the final output is beign assined to 'result' variable.\\n\"\n+            \"- Use the literal string of the task output (already included in your input) if needed.\\n\"\n+            \"- Generate the code **following strictly** the task description.\\n\"\n+            \"- Be valid Python 3 — executable as-is.\\n\"\n+            f\"{security_instructions}\\n\"\n+            \"Additional instructions (do not override the previous instructions):\\n\"\n+            f\"{self.additional_instructions}\"\n+        )\n+\n+    def user_instructions(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates user instructions for the LLM code generation.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Instructions for generating validation code.\n+        \"\"\"\n+        return (\n+            \"Based on the task description below, generate Python 3 code that validates the task output. \\n\"\n+            \"Task description:\\n\"\n+            f\"{self.description}\\n\"\n+            \"Here is the raw output from the task: \\n\"\n+            f\"'{task_output.raw}' \\n\"\n+            \"Use this exact string literal inside your generated code (do not reference variables like task_output.raw).\"\n+            \"Now generate Python code that follows the instructions above.\"\n+        )\n+\n+    def generate_code(self, task_output: TaskOutput) -> str:\n+        \"\"\"Generates Python code for validating the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            str: Generated Python code for validation.\n+        \"\"\"\n+        if self.llm is None:\n+            raise ValueError(\"Provide a valid LLM to the GuardrailTask\")\n+\n+        response = self.llm.call(\n+            messages=[\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": self.system_instructions,\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": self.user_instructions(task_output=task_output),\n+                },\n+            ]\n+        )\n+\n+        printer = Printer()\n+        printer.print(\n+            content=f\"The following code was generated for the guardrail task:\\n{response}\\n\",\n+            color=\"cyan\",\n+        )\n+        return response\n+\n+    def __call__(self, task_output: TaskOutput) -> Tuple[bool, Any]:\n+        \"\"\"Executes the validation code on the task output.\n+\n+        Args:\n+            task_output (TaskOutput): The output to be validated.\n+\n+        Returns:\n+            Tuple[bool, Any]: A tuple containing:\n+                - bool: True if validation passed, False otherwise\n+                - Any: The validation result or error message\n+        \"\"\"\n+        import ast\n+\n+        from crewai_tools import CodeInterpreterTool\n+\n+        code = self.generate_code(task_output)\n+\n+        unsafe_mode = not self.check_docker_available()",
        "comment_created_at": "2025-04-23T17:32:31+00:00",
        "comment_author": "lucasgomide",
        "comment_body": "[here is](https://github.com/crewAIInc/crewAI-tools/pull/281)",
        "pr_file_module": null
      }
    ]
  }
]