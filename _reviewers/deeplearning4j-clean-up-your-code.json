[
  {
    "discussion_id": "1705378940",
    "pr_number": 10109,
    "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/convolution/CnnLossLayer.java",
    "created_at": "2024-08-06T11:32:53+00:00",
    "commented_code": "INDArray in = workspaceMgr.dup(ArrayType.ACTIVATIONS, input, input.ordering());\n         INDArray input2d = ConvolutionUtils.reshape4dTo2d(in, format, workspaceMgr, ArrayType.ACTIVATIONS);\n         INDArray out2d = layerConf().getActivationFn().getActivation(input2d, training);\n+        //just  print all inputs and outputs + method name\n+        System.out.println(\"CnnLossLayer activation - forward pass input (\"\n+                + layerId() + \" - \" + this.layerConf().getLayerName() + \" )\");\n+        System.out.println(\"input: \" + input.toStringFull());\n+        System.out.println(\"CnnLossLayer activation - forward pass result (\"\n+                + layerId() + \" - \" + this.layerConf().getLayerName() + \" )\");\n+        System.out.println(\"Output shape: \" + Arrays.toString(out2d.shape()));\n+        System.out.println(\"Output: \" + out2d.toStringFull());\n+",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1705378940",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 10109,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/convolution/CnnLossLayer.java",
        "discussion_id": "1705378940",
        "commented_code": "@@ -157,6 +157,15 @@ public INDArray activate(boolean training, LayerWorkspaceMgr workspaceMgr) {\n         INDArray in = workspaceMgr.dup(ArrayType.ACTIVATIONS, input, input.ordering());\n         INDArray input2d = ConvolutionUtils.reshape4dTo2d(in, format, workspaceMgr, ArrayType.ACTIVATIONS);\n         INDArray out2d = layerConf().getActivationFn().getActivation(input2d, training);\n+        //just  print all inputs and outputs + method name\n+        System.out.println(\"CnnLossLayer activation - forward pass input (\"\n+                + layerId() + \" - \" + this.layerConf().getLayerName() + \" )\");\n+        System.out.println(\"input: \" + input.toStringFull());\n+        System.out.println(\"CnnLossLayer activation - forward pass result (\"\n+                + layerId() + \" - \" + this.layerConf().getLayerName() + \" )\");\n+        System.out.println(\"Output shape: \" + Arrays.toString(out2d.shape()));\n+        System.out.println(\"Output: \" + out2d.toStringFull());\n+",
        "comment_created_at": "2024-08-06T11:32:53+00:00",
        "comment_author": "treo",
        "comment_body": "left over debug prints?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1196406763",
    "pr_number": 9985,
    "pr_file": "nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda-preset/src/main/java/org/nd4j/presets/cuda/Nd4jCudaPresets.java",
    "created_at": "2023-05-17T12:08:27+00:00",
    "commented_code": "String platform = properties.getProperty(\"platform\");\r\n         List<String> preloads = properties.get(\"platform.preload\");\r\n         List<String> resources = properties.get(\"platform.preloadresource\");\r\n-\r\n+        boolean funcTrace = System.getProperty(\"libnd4j.calltrace\",\"OFF\").equalsIgnoreCase(\"ON\");\r\n+        System.out.println(\"Functrace on: \" + funcTrace);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1196406763",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9985,
        "pr_file": "nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda-preset/src/main/java/org/nd4j/presets/cuda/Nd4jCudaPresets.java",
        "discussion_id": "1196406763",
        "commented_code": "@@ -152,7 +152,8 @@ public void init(Logger logger, java.util.Properties properties, String encoding\n         String platform = properties.getProperty(\"platform\");\r\n         List<String> preloads = properties.get(\"platform.preload\");\r\n         List<String> resources = properties.get(\"platform.preloadresource\");\r\n-\r\n+        boolean funcTrace = System.getProperty(\"libnd4j.calltrace\",\"OFF\").equalsIgnoreCase(\"ON\");\r\n+        System.out.println(\"Functrace on: \" + funcTrace);\r",
        "comment_created_at": "2023-05-17T12:08:27+00:00",
        "comment_author": "treo",
        "comment_body": "I guess this print is a left over from debugging?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1187333215",
    "pr_number": 9980,
    "pr_file": "nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cpu-backend-common/src/main/java/org/nd4j/linalg/cpu/nativecpu/buffer/BaseCpuDataBuffer.java",
    "created_at": "2023-05-08T11:26:15+00:00",
    "commented_code": "@Override\n     protected void release() {\n-        ptrDataBuffer.closeBuffer();\n-        if(pointer != null && !pointer.isNull())\n-            pointer.close();\n-        if(addressPointer != null)\n-            addressPointer.deallocate();\n-        super.release();\n+       // ptrDataBuffer.closeBuffer();\n+     /*   if(pointer != null && !pointer.isNull())\n+            pointer.close();*/\n+   /*     if(addressPointer != null)\n+            addressPointer.deallocate();*/\n+      //  super.release();",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1187333215",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9980,
        "pr_file": "nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cpu-backend-common/src/main/java/org/nd4j/linalg/cpu/nativecpu/buffer/BaseCpuDataBuffer.java",
        "discussion_id": "1187333215",
        "commented_code": "@@ -873,12 +872,12 @@ public BaseCpuDataBuffer(float[] data, MemoryWorkspace workspace) {\n \n     @Override\n     protected void release() {\n-        ptrDataBuffer.closeBuffer();\n-        if(pointer != null && !pointer.isNull())\n-            pointer.close();\n-        if(addressPointer != null)\n-            addressPointer.deallocate();\n-        super.release();\n+       // ptrDataBuffer.closeBuffer();\n+     /*   if(pointer != null && !pointer.isNull())\n+            pointer.close();*/\n+   /*     if(addressPointer != null)\n+            addressPointer.deallocate();*/\n+      //  super.release();",
        "comment_created_at": "2023-05-08T11:26:15+00:00",
        "comment_author": "treo",
        "comment_body": "Please don't leave any commented out code around.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1187334695",
    "pr_number": 9980,
    "pr_file": "nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cpu-backend-common/src/main/java/org/nd4j/linalg/cpu/nativecpu/workspace/CpuWorkspace.java",
    "created_at": "2023-05-08T11:27:36+00:00",
    "commented_code": "}\n \n         workspace.setDevicePointer(null);\n-        workspace.setHostPointer(null);\n+        workspace.setHostPointer(null);*/",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1187334695",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9980,
        "pr_file": "nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cpu-backend-common/src/main/java/org/nd4j/linalg/cpu/nativecpu/workspace/CpuWorkspace.java",
        "discussion_id": "1187334695",
        "commented_code": "@@ -198,7 +198,7 @@ public synchronized void destroyWorkspace(boolean extended) {\n         }\n \n         workspace.setDevicePointer(null);\n-        workspace.setHostPointer(null);\n+        workspace.setHostPointer(null);*/",
        "comment_created_at": "2023-05-08T11:27:36+00:00",
        "comment_author": "treo",
        "comment_body": "Please don't leave commented out code around.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1169560259",
    "pr_number": 9959,
    "pr_file": "nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cpu-backend-common/src/main/java/org/nd4j/linalg/cpu/nativecpu/ops/NativeOpExecutioner.java",
    "created_at": "2023-04-18T06:38:31+00:00",
    "commented_code": "}\n \n         val name = op.opName();\n-        try (val context = buildContext()) {\n+     /*   try (val context = buildContext()) {",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1169560259",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9959,
        "pr_file": "nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cpu-backend-common/src/main/java/org/nd4j/linalg/cpu/nativecpu/ops/NativeOpExecutioner.java",
        "discussion_id": "1169560259",
        "commented_code": "@@ -1344,7 +1343,7 @@ public INDArray[] exec(@NonNull CustomOp op) {\n         }\n \n         val name = op.opName();\n-        try (val context = buildContext()) {\n+     /*   try (val context = buildContext()) {",
        "comment_created_at": "2023-04-18T06:38:31+00:00",
        "comment_author": "treo",
        "comment_body": "Please don't leave any commented out code to hang around",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1136623627",
    "pr_number": 9935,
    "pr_file": "deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/learning/impl/elements/CBOW.java",
    "created_at": "2023-03-15T07:19:53+00:00",
    "commented_code": "maxWinWordsCols = curr;\n                 }\n \n-                INDArray inputWindowWords = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n+            /*    INDArray inputWindowWords = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n                 INDArray inputWordsStatuses = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n                 INDArray randoms = Nd4j.createUninitializedDetached(DataType.INT64, items.size());\n                 INDArray alphas = Nd4j.createUninitializedDetached(DataType.DOUBLE, items.size());\n                 INDArray currentWindowIndexes = Nd4j.createUninitializedDetached(DataType.INT32, items.size());\n                 INDArray codes = Nd4j.createUninitializedDetached(DataType.INT8, items.size(), maxCols);\n                 INDArray indices = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxCols);\n                 INDArray numLabelsArray = Nd4j.createUninitializedDetached(DataType.INT32, items.size());",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1136623627",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9935,
        "pr_file": "deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/learning/impl/elements/CBOW.java",
        "discussion_id": "1136623627",
        "commented_code": "@@ -212,38 +222,81 @@ public double doExec(List<BatchItem<T>> items, INDArray inferenceVector) {\n                         maxWinWordsCols = curr;\n                 }\n \n-                INDArray inputWindowWords = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n+            /*    INDArray inputWindowWords = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n                 INDArray inputWordsStatuses = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n                 INDArray randoms = Nd4j.createUninitializedDetached(DataType.INT64, items.size());\n                 INDArray alphas = Nd4j.createUninitializedDetached(DataType.DOUBLE, items.size());\n                 INDArray currentWindowIndexes = Nd4j.createUninitializedDetached(DataType.INT32, items.size());\n                 INDArray codes = Nd4j.createUninitializedDetached(DataType.INT8, items.size(), maxCols);\n                 INDArray indices = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxCols);\n                 INDArray numLabelsArray = Nd4j.createUninitializedDetached(DataType.INT32, items.size());",
        "comment_created_at": "2023-03-15T07:19:53+00:00",
        "comment_author": "treo",
        "comment_body": "please don't leave code commented out like that.",
        "pr_file_module": null
      },
      {
        "comment_id": "1136630052",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9935,
        "pr_file": "deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/learning/impl/elements/CBOW.java",
        "discussion_id": "1136623627",
        "commented_code": "@@ -212,38 +222,81 @@ public double doExec(List<BatchItem<T>> items, INDArray inferenceVector) {\n                         maxWinWordsCols = curr;\n                 }\n \n-                INDArray inputWindowWords = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n+            /*    INDArray inputWindowWords = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n                 INDArray inputWordsStatuses = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxWinWordsCols);\n                 INDArray randoms = Nd4j.createUninitializedDetached(DataType.INT64, items.size());\n                 INDArray alphas = Nd4j.createUninitializedDetached(DataType.DOUBLE, items.size());\n                 INDArray currentWindowIndexes = Nd4j.createUninitializedDetached(DataType.INT32, items.size());\n                 INDArray codes = Nd4j.createUninitializedDetached(DataType.INT8, items.size(), maxCols);\n                 INDArray indices = Nd4j.createUninitializedDetached(DataType.INT32, items.size(), maxCols);\n                 INDArray numLabelsArray = Nd4j.createUninitializedDetached(DataType.INT32, items.size());",
        "comment_created_at": "2023-03-15T07:26:56+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "Done. Sorry missed that.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "926801541",
    "pr_number": 9735,
    "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/validation/listeners/NonInplaceValidationListener.java",
    "created_at": "2022-07-21T15:11:05+00:00",
    "commented_code": "\"for op %s - input %s\", op.getOp().getClass(), i);\n \n             //Deallocate:\n-            if(dealloc && after.closeable()) {\n+         /*   if(dealloc && after.closeable()) {\n                 after.close();\n-            }\n-            if(opInputs[i].closeable()){\n+            }*/\n+          /*  if(opInputs[i].closeable()){\n                 opInputs[i].close();\n-            }\n+            }*/",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "926801541",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9735,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/validation/listeners/NonInplaceValidationListener.java",
        "discussion_id": "926801541",
        "commented_code": "@@ -128,12 +128,12 @@ public void opExecution(SameDiff sd, At at, MultiDataSet batch, SameDiffOp op, O\n                     \"for op %s - input %s\", op.getOp().getClass(), i);\n \n             //Deallocate:\n-            if(dealloc && after.closeable()) {\n+         /*   if(dealloc && after.closeable()) {\n                 after.close();\n-            }\n-            if(opInputs[i].closeable()){\n+            }*/\n+          /*  if(opInputs[i].closeable()){\n                 opInputs[i].close();\n-            }\n+            }*/",
        "comment_created_at": "2022-07-21T15:11:05+00:00",
        "comment_author": "treo",
        "comment_body": "let's not have commented out code. If this is just temporary, put at least a TODO there, so it is obvious that this is still work in progress. Otherwise it is easy to miss this when you do the cleanup. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "892058815",
    "pr_number": 9706,
    "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/internal/InferenceSession.java",
    "created_at": "2022-06-08T08:16:12+00:00",
    "commented_code": "}\n                 }\n \n-                switch(value.getSdValueType()) {\n-                    case TENSOR:\n-                        mmgr.release(value.getTensorValue());\n-                        break;\n-                    case LIST:\n-                        for(INDArray arr : value.getListValue())\n-                            if(arr != null)\n-                                mmgr.release(arr);\n-                        break;\n-                }\n+      /*          if(!(op.getOp() instanceof Switch))\n+                    switch(value.getSdValueType()) {\n+                        case TENSOR:\n+                            if(!freedArrays.contains(value.getTensorValue().getId())) {\n+                                mmgr.release(value.getTensorValue());\n+                                freedArrays.add(value.getTensorValue().getId());\n+                            }\n+                            break;\n+                        case LIST:\n+                            for(INDArray arr : value.getListValue())\n+                                if(arr != null && !freedArrays.contains(arr.getId())) {\n+                                    mmgr.release(arr);\n+                                    freedArrays.add(arr.getId());\n+                                }\n+                            break;\n+                    }*/",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "892058815",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9706,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/internal/InferenceSession.java",
        "discussion_id": "892058815",
        "commented_code": "@@ -404,16 +417,22 @@ else if(out.hasValues()) {\n                     }\n                 }\n \n-                switch(value.getSdValueType()) {\n-                    case TENSOR:\n-                        mmgr.release(value.getTensorValue());\n-                        break;\n-                    case LIST:\n-                        for(INDArray arr : value.getListValue())\n-                            if(arr != null)\n-                                mmgr.release(arr);\n-                        break;\n-                }\n+      /*          if(!(op.getOp() instanceof Switch))\n+                    switch(value.getSdValueType()) {\n+                        case TENSOR:\n+                            if(!freedArrays.contains(value.getTensorValue().getId())) {\n+                                mmgr.release(value.getTensorValue());\n+                                freedArrays.add(value.getTensorValue().getId());\n+                            }\n+                            break;\n+                        case LIST:\n+                            for(INDArray arr : value.getListValue())\n+                                if(arr != null && !freedArrays.contains(arr.getId())) {\n+                                    mmgr.release(arr);\n+                                    freedArrays.add(arr.getId());\n+                                }\n+                            break;\n+                    }*/",
        "comment_created_at": "2022-06-08T08:16:12+00:00",
        "comment_author": "treo",
        "comment_body": "Please don't leave any commented out code behind.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "739385186",
    "pr_number": 9501,
    "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Reshape.java",
    "created_at": "2021-10-29T16:35:21+00:00",
    "commented_code": "public Reshape(SameDiff sameDiff, SDVariable i_v, SDVariable shape) {\n         super(null, sameDiff, new SDVariable[]{i_v, shape});\n+        addIArgument(-99);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "739385186",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9501,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Reshape.java",
        "discussion_id": "739385186",
        "commented_code": "@@ -59,6 +59,7 @@ public Reshape(SameDiff sameDiff, SDVariable i_v, long[] shape) {\n \n     public Reshape(SameDiff sameDiff, SDVariable i_v, SDVariable shape) {\n         super(null, sameDiff, new SDVariable[]{i_v, shape});\n+        addIArgument(-99);",
        "comment_created_at": "2021-10-29T16:35:21+00:00",
        "comment_author": "treo",
        "comment_body": "Can we turn this magic number into a constant? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "172058473",
    "pr_number": 4629,
    "pr_file": "deeplearning4j-manifold/deeplearning4j-largevis/src/main/java/org/deeplearning4j/largevis/LargeVis.java",
    "created_at": "2018-03-04T18:16:28+00:00",
    "commented_code": "+package org.deeplearning4j.largevis;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.commons.lang3.time.StopWatch;\n+import org.deeplearning4j.clustering.randomprojection.RPForest;\n+import org.deeplearning4j.clustering.randomprojection.RPUtils;\n+import org.deeplearning4j.nn.conf.GradientNormalization;\n+import org.deeplearning4j.nn.conf.WorkspaceMode;\n+import org.deeplearning4j.nndescent.NNDescent;\n+import org.nd4j.linalg.api.memory.MemoryWorkspace;\n+import org.nd4j.linalg.api.memory.conf.WorkspaceConfiguration;\n+import org.nd4j.linalg.api.memory.enums.*;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.api.ops.CustomOp;\n+import org.nd4j.linalg.api.ops.DynamicCustomOp;\n+import org.nd4j.linalg.api.ops.impl.accum.Norm2;\n+import org.nd4j.linalg.api.ops.impl.transforms.arithmetic.OldSubOp;\n+import org.nd4j.linalg.api.ops.impl.transforms.clip.ClipByValue;\n+import org.nd4j.linalg.dataset.DataSet;\n+import org.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize;\n+import org.nd4j.linalg.factory.Nd4j;\n+import org.nd4j.linalg.learning.config.IUpdater;\n+import org.nd4j.linalg.learning.config.Sgd;\n+import org.nd4j.linalg.memory.abstracts.DummyWorkspace;\n+import org.nd4j.linalg.primitives.Counter;\n+import org.nd4j.linalg.primitives.Pair;\n+import org.nd4j.linalg.util.MathUtils;\n+import org.nd4j.list.FloatNDArrayList;\n+import org.nd4j.list.IntNDArrayList;\n+import org.nd4j.list.matrix.IntMatrixNDArrayList;\n+import org.nd4j.weightinit.WeightInitScheme;\n+import org.nd4j.weightinit.impl.XavierFanInInitScheme;\n+\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.PriorityQueue;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.LockSupport;\n+\n+\n+/**\n+ * A port of the LargeVis algorithm:\n+ * https://github.com/lferry007/LargeVis\n+ *\n+ * to nd4j. This implementation uses\n+ * RPTrees rather than annoy as in the original implementation.\n+ *\n+ *\n+ * This algorithm also uses the nd4j updaters (to allow for more flexibility)\n+ * over static gradient clipping and the simpler learning rate schedule.\n+ *\n+ *\n+ * The algorithm has the following parameters:\n+ *      -fea: specify whether the input file is high-dimensional feature vectors (1) or networks (0). Default is 1.\n+ *      -input: Input file of feature vectors or networks\n+ *      -output: Output file of low-dimensional representations.\n+ *      -threads: Number of threads. Default is 8.\n+ *      -outdim: The lower dimensionality LargesVis learns for visualization (usually 2 or 3). Default is 2.\n+ *      -samples: Number of edge samples for graph layout (in millions). Default is set to data size / 100 (million).\n+ *      -prop: Number of times for neighbor propagations in the state of K      -NNG construction, usually less than 3. Default is 3.\n+ *      -alpha: Initial learning rate. Default is 1.0.\n+ *      -trees: Number of random-projection trees used for constructing K-NNG. 50 is sufficient for most cases.\n+ *      -neg: Number of negative samples used for negative sampling. Default is 5.\n+ *      -neigh: Number of neighbors (K) in K-NNG, which is usually set as three times of perplexity. Default is 150.\n+ *      -gamma: The weights assigned to negative edges. Default is 7.\n+ *      -perp: The perplexity used for deciding edge weights in K-NNG. Default is 50.\n+ *\n+ * @author Adam Gibson\n+ */\n+@Data\n+@Slf4j\n+public class LargeVis {\n+\n+\n+    private NNDescent nnDescent;\n+    @Builder.Default\n+    private int numWorkers = Runtime.getRuntime().availableProcessors();\n+    //vec.rows -> nVertices\n+    private INDArray vec,vis,prob;\n+    @Builder.Default\n+    private IUpdater updater = new Sgd(0.01);\n+    private WeightInitScheme weightInitScheme;\n+    private ThreadLocal<INDArray> scalars = new ThreadLocal<>();\n+    @Builder.Default\n+    private WorkspaceMode workspaceMode = WorkspaceMode.SINGLE;\n+    private ThreadLocal<MemoryWorkspace>  workspaceThread = new ThreadLocal<>();\n+    private WorkspaceConfiguration workspaceConfiguration;\n+\n+    @Builder.Default\n+    private String distanceFunction = \"euclidean\";\n+    private int nEdges;\n+    @Builder.Default\n+    private IntNDArrayList reverse = new IntNDArrayList();\n+    private ExecutorService threadExec;\n+    @Builder.Default\n+    private int outDim = 2;\n+    @Builder.Default\n+    private double initialAlpha = 1.0;\n+    @Builder.Default\n+    private int nNegatives = 5;\n+    @Builder.Default\n+    private double gamma = 7.0;\n+    @Builder.Default\n+    private double perplexity = 50.0;\n+    @Builder.Default\n+    private long seed = 42;\n+    @Builder.Default\n+    private Boolean normalize;\n+    private int negSize = (int) 1e8;\n+    @Builder.Default\n+    private double gradClipValue = 5.0;\n+    @Builder.Default\n+    private GradientNormalization gradientNormalization = GradientNormalization.ClipElementWiseAbsoluteValue;\n+    private AtomicInteger edgeCountActual = new AtomicInteger(0);\n+    private  MemoryWorkspace workspace;\n+    private AtomicInteger updateCount = new AtomicInteger(0);\n+    private AtomicInteger epochCount = new AtomicInteger(0);\n+    private IntNDArrayList edgeFrom = new IntNDArrayList();\n+    private IntNDArrayList edgeTo = new IntNDArrayList();\n+    private ExecutorService executorService;\n+    protected final AtomicInteger workerCounter = new AtomicInteger(0);\n+\n+    @Builder.Default\n+    private Boolean sample = true;\n+\n+    private ThreadLocal<INDArray> errors = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> grads = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> gradsFirstRow = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> gradsSecondRow = new ThreadLocal<>();\n+\n+\n+    private ThreadLocal<Norm2> norm2 = new ThreadLocal<>();\n+    private ThreadLocal<ClipByValue> clip = new ThreadLocal<>();\n+    private ThreadLocal<OldSubOp> visXMinusVisY = new ThreadLocal<>();\n+    private ThreadLocal<OldSubOp> visYMinusVisX = new ThreadLocal<>();\n+    // log uncaught exceptions\n+    Thread.UncaughtExceptionHandler handler = new Thread.UncaughtExceptionHandler() {\n+        public void uncaughtException(Thread th, Throwable ex) {\n+            log.error(\"Uncaught exception: \" + ex);\n+            ex.printStackTrace();\n+        }\n+    };\n+\n+\n+\n+\n+\n+    @Builder\n+    public LargeVis(INDArray vec,\n+                    int maxSize,\n+                    String distanceFunction,\n+                    int numTrees,\n+                    int outDims,\n+                    int nNegatives,\n+                    double gamma,\n+                    double initialAlpha,\n+                    double perplexity,\n+                    int nPropagations,\n+                    long seed,\n+                    int nNeighbors,\n+                    Boolean normalize,\n+                    int iterationCount,\n+                    IUpdater updater,\n+                    int nTrees,\n+                    WeightInitScheme weightInitScheme,\n+                    GradientNormalization gradientNormalization,\n+                    double gradClipValue,\n+                    WorkspaceMode workspaceMode,\n+                    int numWorkers,\n+                    int nSamples,\n+                    Boolean sample) {\n+\n+\n+\n+        if(workspaceMode != null) {\n+            this.workspaceMode = workspaceMode;\n+        }\n+\n+        if(numWorkers > 0) {\n+            this.numWorkers = numWorkers;\n+        }\n+\n+        if(workspaceMode != WorkspaceMode.NONE)\n+            workspaceConfiguration = WorkspaceConfiguration.builder().cyclesBeforeInitialization(1)\n+                    .policyAllocation(AllocationPolicy.STRICT).policyLearning(LearningPolicy.FIRST_LOOP)\n+                    .policyMirroring(MirroringPolicy.FULL).policyReset(ResetPolicy.BLOCK_LEFT)\n+                    .policySpill(SpillPolicy.REALLOCATE).build();\n+\n+\n+        if(sample != null) {\n+            this.sample = sample;\n+        }\n+\n+        if(gradientNormalization != null) {\n+            this.gradientNormalization = gradientNormalization;\n+        }\n+\n+        if(gradClipValue > 0) {\n+            this.gradClipValue = gradClipValue;\n+        }\n+\n+\n+\n+        if(normalize != null) {\n+            this.normalize = normalize;\n+        }\n+\n+        if(updater != null)\n+            this.updater = updater;\n+        this.normalize = normalize;\n+        this.vec = vec;\n+\n+        if(weightInitScheme != null) {\n+            this.weightInitScheme = weightInitScheme;\n+        }\n+\n+\n+        if(distanceFunction != null)\n+            this.distanceFunction = distanceFunction;\n+        if(outDims > 0)\n+            this.outDim = outDims;\n+        if(initialAlpha > 0)\n+            this.initialAlpha = initialAlpha;\n+\n+        if(nNegatives > 0)\n+            this.nNegatives = nNegatives;\n+        if(gamma > 0)\n+            this.gamma = gamma;\n+        if(perplexity > 0)\n+            this.perplexity = perplexity;\n+        if(seed > 0)\n+            this.seed = seed;\n+        if(outDims > 0)\n+            this.outDim = outDims;\n+\n+\n+        vis = Nd4j.create(vec.rows(),outDim);\n+\n+\n+\n+\n+\n+        this.executorService = Executors.newFixedThreadPool(this.numWorkers, new ThreadFactory() {\n+            @Override\n+            public Thread newThread( Runnable r) {\n+                Thread t = Executors.defaultThreadFactory().newThread(r);\n+\n+                int cThread = workerCounter.getAndIncrement();\n+\n+                t.setName(\"LargeVis thread \" + cThread);\n+                t.setDaemon(true);\n+                t.setUncaughtExceptionHandler(handler);\n+\n+                Nd4j.getAffinityManager().attachThreadToDevice(t,\n+                        cThread % Nd4j.getAffinityManager().getNumberOfDevices());\n+\n+                return t;\n+            }\n+        });\n+\n+        this.nnDescent = NNDescent.builder()\n+                .distanceFunction(distanceFunction)\n+                .gamma(gamma)\n+                .initialAlpha(initialAlpha)\n+                .maxSize(maxSize)\n+                .iterationCount(iterationCount)\n+                .nNegatives(nNegatives)\n+                .normalize(normalize)\n+                .nPropagations(nPropagations)\n+                .numWorkers(numWorkers)\n+                .perplexity(perplexity)\n+                .nSamples(nSamples)\n+                .sample(sample)\n+                .nTrees(numTrees)\n+                .nNeighbors(nNeighbors)\n+                .seed(seed)\n+                .workspaceMode(workspaceMode)\n+                .executorService(executorService)\n+                .build();\n+        nnDescent.fit();\n+\n+        // opening workspace\n+        MemoryWorkspace workspace = nnDescent.getWorkspace();\n+\n+        workspace.notifyScopeEntered();\n+        Nd4j.getRandom().setSeed(seed);\n+\n+        workspace.notifyScopeLeft();\n+        Nd4j.getMemoryManager().togglePeriodicGc(false);\n+    }\n+\n+    /**\n+     * Sample a random edge based on the 2\n+     * random numbers passed in.\n+     * The first number is used to compute a k\n+     * relative to the number of edges.\n+     * Depending on the probability of k\n+     * it returns the k or its alias.\n+     * @param rand1 the first number\n+     * @param rand2 the second number\n+     * @return the sampled edge\n+     */\n+\n+    public int sampleAnEdge(double rand1,double rand2) {\n+        int k = (int) ((nEdges - 0.1) * rand1);\n+        return rand2 <= prob.getDouble(k) ? k : nnDescent.getAlias()[k];\n+    }\n+\n+\n+\n+    /**\n+     * Return the gradients wrt the distances of x and y\n+     * relative to each other\n+     * @param visX the slice of vis to take the gradient of\n+     * @param visY the slice of y to take the gradient of\n+     * @param i the current iteration\n+     * @param currLr the current learning rate\n+     * @param normalize whether to normalize the gradient or not\n+     * @return the gradients wrt x and y (in that order)\n+     */\n+    public INDArray gradientsFor(INDArray grads,INDArray visX,INDArray visY,int i,double currLr,boolean normalize) {\n+        MemoryWorkspace workspace = nnDescent.getWorkspace();\n+        try(MemoryWorkspace w2 = workspace.notifyScopeEntered()) {\n+\n+            if (scalars.get() == null)\n+                scalars.set(Nd4j.scalar(0.0));\n+\n+            double g = grad(i,visX,visY);\n+\n+            //gradient wrt distance to x and y\n+            Nd4j.getExecutioner().execAndReturn(getVisXMinusVisY(visX,visY,createGradFirstRow().muli(g * currLr)));\n+            Nd4j.getExecutioner().execAndReturn(getVisYMinusVisX(visY,visX,createGradSecondRow()));\n+            if(normalize) {\n+                normalizeBatch(grads);\n+            }\n+\n+\n+            return grads;\n+        }\n+\n+\n+    }\n+\n+    public double grad(int i,INDArray visX,INDArray visY) {\n+        double g;\n+        if (scalars.get() == null)\n+            scalars.set(Nd4j.scalar(0.0));\n+\n+        double f = RPUtils.computeDistance(distanceFunction,visX,visY,scalars.get());\n+        if(i == 0) {\n+            g = (-2 / (1 + f));\n+        }\n+        else {\n+            g = 2 * gamma / (1 + f) / (0.1 + f);\n+        }\n+        return g;\n+    }\n+\n+\n+    public double grad(int i,int x,int y) {\n+        return grad(i,vis.slice(x),vis.slice(y));\n+    }\n+\n+\n+\n+\n+    /**\n+     * Compute the error wrt the given parameters given a sampled edge(p)\n+     * and 2 vectors to compute the distance and error for\n+     *\n+     * Note that there is a side effect of updating y as a part of calling this method.\n+     * This method should mainly be used in gradient checking tests or internally within this class\n+     * not directly by a user.\n+     *\n+     * @param visX the x component to compute the error for\n+     * @param y the index of y\n+     * @param p the sample edge for random access\n+     * @param currLr the current learning rate for the gradient update\n+     * @param updateY\n+     * @return the error wrt the given parameters\n+     */\n+    public INDArray errorWrt(INDArray visX,  int y, int p, double currLr, boolean updateY,boolean normalize) {\n+        MemoryWorkspace workspace = nnDescent.getWorkspace();\n+        int[] negTable = nnDescent.getNegTable();\n+\n+        try(MemoryWorkspace w2 = workspace.notifyScopeEntered()) {\n+            INDArray err = createError();\n+            INDArray grads = createGrad();\n+            for(int i = 0; i < nNegatives + 1; i++) {\n+                if(y > 0) {\n+                    y = negTable[(MathUtils.randomNumberBetween(0, negSize - 1,Nd4j.getRandom().nextDouble()))];\n+                    if (y == edgeTo.get(p)) continue;\n+                }\n+\n+                INDArray visY = vis.slice(y);\n+                //get the gradient wrt x and y\n+                gradientsFor(grads,visX,visY,i,currLr,normalize);\n+                INDArray gradsFirst = createGradFirstRow();\n+                INDArray gradsSecond = createGradSecondRow();\n+                err.addi(gradsFirst);\n+                if(updateY)\n+                    visY.addi(gradsSecond);\n+\n+\n+            }\n+\n+            return err;\n+\n+        }\n+\n+    }\n+\n+\n+    public INDArray createGradSecondRow() {\n+        if(gradsSecondRow.get() == null) {\n+            if(grads.get() == null) {\n+                INDArray grads = createGrad();\n+                gradsSecondRow.set(grads.slice(1));\n+            }\n+            else\n+                gradsSecondRow.set(grads.get().slice(1));\n+        }\n+\n+        return gradsSecondRow.get();\n+    }\n+\n+\n+    public OldSubOp getVisYMinusVisX(INDArray x,INDArray y,INDArray result) {\n+        return getSubOp(visYMinusVisX,x,y,result);\n+\n+    }\n+\n+\n+    public OldSubOp getVisXMinusVisY(INDArray x,INDArray y,INDArray result) {\n+        return getSubOp(visXMinusVisY,x,y,result);\n+    }\n+\n+    private OldSubOp getSubOp(ThreadLocal<OldSubOp> threadLocal,INDArray x,INDArray y,INDArray result) {\n+        if(threadLocal.get() == null) {\n+            OldSubOp clipByValue = new OldSubOp(x,y,result);\n+            threadLocal.set(clipByValue);\n+            return clipByValue;\n+        }\n+\n+        OldSubOp clipByValue = threadLocal.get();\n+        clipByValue.setX(x);\n+        clipByValue.setY(y);\n+        return clipByValue;\n+    }\n+\n+\n+    public ClipByValue getClipByValue(INDArray input) {\n+        if(clip.get() == null) {\n+            ClipByValue clipByValue = new ClipByValue(new INDArray[] {input},null,-gradClipValue,gradClipValue,true);\n+            clip.set(clipByValue);\n+            return clipByValue;\n+        }\n+\n+        ClipByValue clipByValue = clip.get();\n+        clipByValue.setInputArgument(0,input);\n+        return clipByValue;\n+    }\n+\n+\n+    public Norm2 getNorm2(INDArray input) {\n+        if(clip.get() == null) {\n+            Norm2 norm2 = new Norm2(input);\n+            this.norm2.set(norm2);\n+            return norm2;\n+        }\n+\n+        Norm2 ret = norm2.get();\n+        ret.setX(input);\n+        return ret;\n+    }\n+\n+\n+    public INDArray createGradFirstRow() {\n+        if(gradsFirstRow.get() == null) {\n+            if(grads.get() == null) {\n+                INDArray grads = createGrad();\n+                gradsFirstRow.set(grads.slice(0));\n+            }\n+            else\n+                gradsFirstRow.set(grads.get().slice(0));\n+        }\n+\n+        return gradsFirstRow.get();\n+    }\n+\n+    public INDArray createGrad() {\n+        if(grads.get() == null) {\n+            grads.set(Nd4j.create(2,vis.columns()));\n+        }\n+\n+        return grads.get();\n+    }\n+\n+    public INDArray createError() {\n+        if(errors.get() == null) {\n+            errors.set(Nd4j.create(outDim));\n+        }\n+\n+        return errors.get();\n+    }\n+\n+\n+\n+    public double distance(int x,int y) {\n+        return RPUtils.computeDistance(distanceFunction,vis.slice(x),vis.slice(y));\n+    }\n+\n+    /**\n+     * Compute the error wrt the given parameters given a sampled edge(p)\n+     * and 2 vectors to compute the distance and error for\n+     *\n+     * Note that there is a side effect of updating y as a part of calling this method.\n+     * This method should mainly be used in gradient checking tests or internally within this class\n+     * not directly by a user.\n+     *\n+     * @param x the x component to compute the error for\n+     * @param y the initial index of y (changes with various negative sampling\n+     * @param p the sampled edge for random access\n+     * @param currLr the current learning rate for the gradient update\n+     * @param updateY whether to update y or not\n+     * @param normalize whether to apply gradient normalization or not\n+     * @return the error wrt the given parameters\n+     */\n+    public INDArray errorWrt(int x, int y, int p, double currLr, boolean updateY,boolean normalize) {\n+        return errorWrt(vis.slice(x),y,p,currLr,updateY,normalize);\n+\n+    }\n+\n+\n+    private class VisualizeThread implements Runnable {\n+        private int id;\n+        private AtomicBoolean done = new AtomicBoolean(false);\n+\n+        public VisualizeThread(int id) {\n+            this.id = id;\n+        }\n+\n+        public boolean isDone() {\n+            return done.get();\n+        }\n+\n+        @Override\n+        public void run() {\n+\n+\n+            log.info(\"Starting visualize thread \" + id);\n+            int edgeCount = 0;\n+            int lastEdgeCount = 0;\n+            int p,x,y;\n+            // opening workspace\n+            MemoryWorkspace workspace = nnDescent.getWorkspace();\n+            try(MemoryWorkspace workspace2 = workspace.notifyScopeEntered()) {\n+                double currLr = initialAlpha;\n+                while (true) {\n+                    if (edgeCount > nnDescent.getNSamples() / numWorkers + 2) {\n+                        break;\n+                    }\n+\n+                    if (edgeCount - lastEdgeCount > 10000) {",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "172058473",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 4629,
        "pr_file": "deeplearning4j-manifold/deeplearning4j-largevis/src/main/java/org/deeplearning4j/largevis/LargeVis.java",
        "discussion_id": "172058473",
        "commented_code": "@@ -0,0 +1,713 @@\n+package org.deeplearning4j.largevis;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.commons.lang3.time.StopWatch;\n+import org.deeplearning4j.clustering.randomprojection.RPForest;\n+import org.deeplearning4j.clustering.randomprojection.RPUtils;\n+import org.deeplearning4j.nn.conf.GradientNormalization;\n+import org.deeplearning4j.nn.conf.WorkspaceMode;\n+import org.deeplearning4j.nndescent.NNDescent;\n+import org.nd4j.linalg.api.memory.MemoryWorkspace;\n+import org.nd4j.linalg.api.memory.conf.WorkspaceConfiguration;\n+import org.nd4j.linalg.api.memory.enums.*;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.api.ops.CustomOp;\n+import org.nd4j.linalg.api.ops.DynamicCustomOp;\n+import org.nd4j.linalg.api.ops.impl.accum.Norm2;\n+import org.nd4j.linalg.api.ops.impl.transforms.arithmetic.OldSubOp;\n+import org.nd4j.linalg.api.ops.impl.transforms.clip.ClipByValue;\n+import org.nd4j.linalg.dataset.DataSet;\n+import org.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize;\n+import org.nd4j.linalg.factory.Nd4j;\n+import org.nd4j.linalg.learning.config.IUpdater;\n+import org.nd4j.linalg.learning.config.Sgd;\n+import org.nd4j.linalg.memory.abstracts.DummyWorkspace;\n+import org.nd4j.linalg.primitives.Counter;\n+import org.nd4j.linalg.primitives.Pair;\n+import org.nd4j.linalg.util.MathUtils;\n+import org.nd4j.list.FloatNDArrayList;\n+import org.nd4j.list.IntNDArrayList;\n+import org.nd4j.list.matrix.IntMatrixNDArrayList;\n+import org.nd4j.weightinit.WeightInitScheme;\n+import org.nd4j.weightinit.impl.XavierFanInInitScheme;\n+\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.PriorityQueue;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.LockSupport;\n+\n+\n+/**\n+ * A port of the LargeVis algorithm:\n+ * https://github.com/lferry007/LargeVis\n+ *\n+ * to nd4j. This implementation uses\n+ * RPTrees rather than annoy as in the original implementation.\n+ *\n+ *\n+ * This algorithm also uses the nd4j updaters (to allow for more flexibility)\n+ * over static gradient clipping and the simpler learning rate schedule.\n+ *\n+ *\n+ * The algorithm has the following parameters:\n+ *      -fea: specify whether the input file is high-dimensional feature vectors (1) or networks (0). Default is 1.\n+ *      -input: Input file of feature vectors or networks\n+ *      -output: Output file of low-dimensional representations.\n+ *      -threads: Number of threads. Default is 8.\n+ *      -outdim: The lower dimensionality LargesVis learns for visualization (usually 2 or 3). Default is 2.\n+ *      -samples: Number of edge samples for graph layout (in millions). Default is set to data size / 100 (million).\n+ *      -prop: Number of times for neighbor propagations in the state of K      -NNG construction, usually less than 3. Default is 3.\n+ *      -alpha: Initial learning rate. Default is 1.0.\n+ *      -trees: Number of random-projection trees used for constructing K-NNG. 50 is sufficient for most cases.\n+ *      -neg: Number of negative samples used for negative sampling. Default is 5.\n+ *      -neigh: Number of neighbors (K) in K-NNG, which is usually set as three times of perplexity. Default is 150.\n+ *      -gamma: The weights assigned to negative edges. Default is 7.\n+ *      -perp: The perplexity used for deciding edge weights in K-NNG. Default is 50.\n+ *\n+ * @author Adam Gibson\n+ */\n+@Data\n+@Slf4j\n+public class LargeVis {\n+\n+\n+    private NNDescent nnDescent;\n+    @Builder.Default\n+    private int numWorkers = Runtime.getRuntime().availableProcessors();\n+    //vec.rows -> nVertices\n+    private INDArray vec,vis,prob;\n+    @Builder.Default\n+    private IUpdater updater = new Sgd(0.01);\n+    private WeightInitScheme weightInitScheme;\n+    private ThreadLocal<INDArray> scalars = new ThreadLocal<>();\n+    @Builder.Default\n+    private WorkspaceMode workspaceMode = WorkspaceMode.SINGLE;\n+    private ThreadLocal<MemoryWorkspace>  workspaceThread = new ThreadLocal<>();\n+    private WorkspaceConfiguration workspaceConfiguration;\n+\n+    @Builder.Default\n+    private String distanceFunction = \"euclidean\";\n+    private int nEdges;\n+    @Builder.Default\n+    private IntNDArrayList reverse = new IntNDArrayList();\n+    private ExecutorService threadExec;\n+    @Builder.Default\n+    private int outDim = 2;\n+    @Builder.Default\n+    private double initialAlpha = 1.0;\n+    @Builder.Default\n+    private int nNegatives = 5;\n+    @Builder.Default\n+    private double gamma = 7.0;\n+    @Builder.Default\n+    private double perplexity = 50.0;\n+    @Builder.Default\n+    private long seed = 42;\n+    @Builder.Default\n+    private Boolean normalize;\n+    private int negSize = (int) 1e8;\n+    @Builder.Default\n+    private double gradClipValue = 5.0;\n+    @Builder.Default\n+    private GradientNormalization gradientNormalization = GradientNormalization.ClipElementWiseAbsoluteValue;\n+    private AtomicInteger edgeCountActual = new AtomicInteger(0);\n+    private  MemoryWorkspace workspace;\n+    private AtomicInteger updateCount = new AtomicInteger(0);\n+    private AtomicInteger epochCount = new AtomicInteger(0);\n+    private IntNDArrayList edgeFrom = new IntNDArrayList();\n+    private IntNDArrayList edgeTo = new IntNDArrayList();\n+    private ExecutorService executorService;\n+    protected final AtomicInteger workerCounter = new AtomicInteger(0);\n+\n+    @Builder.Default\n+    private Boolean sample = true;\n+\n+    private ThreadLocal<INDArray> errors = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> grads = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> gradsFirstRow = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> gradsSecondRow = new ThreadLocal<>();\n+\n+\n+    private ThreadLocal<Norm2> norm2 = new ThreadLocal<>();\n+    private ThreadLocal<ClipByValue> clip = new ThreadLocal<>();\n+    private ThreadLocal<OldSubOp> visXMinusVisY = new ThreadLocal<>();\n+    private ThreadLocal<OldSubOp> visYMinusVisX = new ThreadLocal<>();\n+    // log uncaught exceptions\n+    Thread.UncaughtExceptionHandler handler = new Thread.UncaughtExceptionHandler() {\n+        public void uncaughtException(Thread th, Throwable ex) {\n+            log.error(\"Uncaught exception: \" + ex);\n+            ex.printStackTrace();\n+        }\n+    };\n+\n+\n+\n+\n+\n+    @Builder\n+    public LargeVis(INDArray vec,\n+                    int maxSize,\n+                    String distanceFunction,\n+                    int numTrees,\n+                    int outDims,\n+                    int nNegatives,\n+                    double gamma,\n+                    double initialAlpha,\n+                    double perplexity,\n+                    int nPropagations,\n+                    long seed,\n+                    int nNeighbors,\n+                    Boolean normalize,\n+                    int iterationCount,\n+                    IUpdater updater,\n+                    int nTrees,\n+                    WeightInitScheme weightInitScheme,\n+                    GradientNormalization gradientNormalization,\n+                    double gradClipValue,\n+                    WorkspaceMode workspaceMode,\n+                    int numWorkers,\n+                    int nSamples,\n+                    Boolean sample) {\n+\n+\n+\n+        if(workspaceMode != null) {\n+            this.workspaceMode = workspaceMode;\n+        }\n+\n+        if(numWorkers > 0) {\n+            this.numWorkers = numWorkers;\n+        }\n+\n+        if(workspaceMode != WorkspaceMode.NONE)\n+            workspaceConfiguration = WorkspaceConfiguration.builder().cyclesBeforeInitialization(1)\n+                    .policyAllocation(AllocationPolicy.STRICT).policyLearning(LearningPolicy.FIRST_LOOP)\n+                    .policyMirroring(MirroringPolicy.FULL).policyReset(ResetPolicy.BLOCK_LEFT)\n+                    .policySpill(SpillPolicy.REALLOCATE).build();\n+\n+\n+        if(sample != null) {\n+            this.sample = sample;\n+        }\n+\n+        if(gradientNormalization != null) {\n+            this.gradientNormalization = gradientNormalization;\n+        }\n+\n+        if(gradClipValue > 0) {\n+            this.gradClipValue = gradClipValue;\n+        }\n+\n+\n+\n+        if(normalize != null) {\n+            this.normalize = normalize;\n+        }\n+\n+        if(updater != null)\n+            this.updater = updater;\n+        this.normalize = normalize;\n+        this.vec = vec;\n+\n+        if(weightInitScheme != null) {\n+            this.weightInitScheme = weightInitScheme;\n+        }\n+\n+\n+        if(distanceFunction != null)\n+            this.distanceFunction = distanceFunction;\n+        if(outDims > 0)\n+            this.outDim = outDims;\n+        if(initialAlpha > 0)\n+            this.initialAlpha = initialAlpha;\n+\n+        if(nNegatives > 0)\n+            this.nNegatives = nNegatives;\n+        if(gamma > 0)\n+            this.gamma = gamma;\n+        if(perplexity > 0)\n+            this.perplexity = perplexity;\n+        if(seed > 0)\n+            this.seed = seed;\n+        if(outDims > 0)\n+            this.outDim = outDims;\n+\n+\n+        vis = Nd4j.create(vec.rows(),outDim);\n+\n+\n+\n+\n+\n+        this.executorService = Executors.newFixedThreadPool(this.numWorkers, new ThreadFactory() {\n+            @Override\n+            public Thread newThread( Runnable r) {\n+                Thread t = Executors.defaultThreadFactory().newThread(r);\n+\n+                int cThread = workerCounter.getAndIncrement();\n+\n+                t.setName(\"LargeVis thread \" + cThread);\n+                t.setDaemon(true);\n+                t.setUncaughtExceptionHandler(handler);\n+\n+                Nd4j.getAffinityManager().attachThreadToDevice(t,\n+                        cThread % Nd4j.getAffinityManager().getNumberOfDevices());\n+\n+                return t;\n+            }\n+        });\n+\n+        this.nnDescent = NNDescent.builder()\n+                .distanceFunction(distanceFunction)\n+                .gamma(gamma)\n+                .initialAlpha(initialAlpha)\n+                .maxSize(maxSize)\n+                .iterationCount(iterationCount)\n+                .nNegatives(nNegatives)\n+                .normalize(normalize)\n+                .nPropagations(nPropagations)\n+                .numWorkers(numWorkers)\n+                .perplexity(perplexity)\n+                .nSamples(nSamples)\n+                .sample(sample)\n+                .nTrees(numTrees)\n+                .nNeighbors(nNeighbors)\n+                .seed(seed)\n+                .workspaceMode(workspaceMode)\n+                .executorService(executorService)\n+                .build();\n+        nnDescent.fit();\n+\n+        // opening workspace\n+        MemoryWorkspace workspace = nnDescent.getWorkspace();\n+\n+        workspace.notifyScopeEntered();\n+        Nd4j.getRandom().setSeed(seed);\n+\n+        workspace.notifyScopeLeft();\n+        Nd4j.getMemoryManager().togglePeriodicGc(false);\n+    }\n+\n+    /**\n+     * Sample a random edge based on the 2\n+     * random numbers passed in.\n+     * The first number is used to compute a k\n+     * relative to the number of edges.\n+     * Depending on the probability of k\n+     * it returns the k or its alias.\n+     * @param rand1 the first number\n+     * @param rand2 the second number\n+     * @return the sampled edge\n+     */\n+\n+    public int sampleAnEdge(double rand1,double rand2) {\n+        int k = (int) ((nEdges - 0.1) * rand1);\n+        return rand2 <= prob.getDouble(k) ? k : nnDescent.getAlias()[k];\n+    }\n+\n+\n+\n+    /**\n+     * Return the gradients wrt the distances of x and y\n+     * relative to each other\n+     * @param visX the slice of vis to take the gradient of\n+     * @param visY the slice of y to take the gradient of\n+     * @param i the current iteration\n+     * @param currLr the current learning rate\n+     * @param normalize whether to normalize the gradient or not\n+     * @return the gradients wrt x and y (in that order)\n+     */\n+    public INDArray gradientsFor(INDArray grads,INDArray visX,INDArray visY,int i,double currLr,boolean normalize) {\n+        MemoryWorkspace workspace = nnDescent.getWorkspace();\n+        try(MemoryWorkspace w2 = workspace.notifyScopeEntered()) {\n+\n+            if (scalars.get() == null)\n+                scalars.set(Nd4j.scalar(0.0));\n+\n+            double g = grad(i,visX,visY);\n+\n+            //gradient wrt distance to x and y\n+            Nd4j.getExecutioner().execAndReturn(getVisXMinusVisY(visX,visY,createGradFirstRow().muli(g * currLr)));\n+            Nd4j.getExecutioner().execAndReturn(getVisYMinusVisX(visY,visX,createGradSecondRow()));\n+            if(normalize) {\n+                normalizeBatch(grads);\n+            }\n+\n+\n+            return grads;\n+        }\n+\n+\n+    }\n+\n+    public double grad(int i,INDArray visX,INDArray visY) {\n+        double g;\n+        if (scalars.get() == null)\n+            scalars.set(Nd4j.scalar(0.0));\n+\n+        double f = RPUtils.computeDistance(distanceFunction,visX,visY,scalars.get());\n+        if(i == 0) {\n+            g = (-2 / (1 + f));\n+        }\n+        else {\n+            g = 2 * gamma / (1 + f) / (0.1 + f);\n+        }\n+        return g;\n+    }\n+\n+\n+    public double grad(int i,int x,int y) {\n+        return grad(i,vis.slice(x),vis.slice(y));\n+    }\n+\n+\n+\n+\n+    /**\n+     * Compute the error wrt the given parameters given a sampled edge(p)\n+     * and 2 vectors to compute the distance and error for\n+     *\n+     * Note that there is a side effect of updating y as a part of calling this method.\n+     * This method should mainly be used in gradient checking tests or internally within this class\n+     * not directly by a user.\n+     *\n+     * @param visX the x component to compute the error for\n+     * @param y the index of y\n+     * @param p the sample edge for random access\n+     * @param currLr the current learning rate for the gradient update\n+     * @param updateY\n+     * @return the error wrt the given parameters\n+     */\n+    public INDArray errorWrt(INDArray visX,  int y, int p, double currLr, boolean updateY,boolean normalize) {\n+        MemoryWorkspace workspace = nnDescent.getWorkspace();\n+        int[] negTable = nnDescent.getNegTable();\n+\n+        try(MemoryWorkspace w2 = workspace.notifyScopeEntered()) {\n+            INDArray err = createError();\n+            INDArray grads = createGrad();\n+            for(int i = 0; i < nNegatives + 1; i++) {\n+                if(y > 0) {\n+                    y = negTable[(MathUtils.randomNumberBetween(0, negSize - 1,Nd4j.getRandom().nextDouble()))];\n+                    if (y == edgeTo.get(p)) continue;\n+                }\n+\n+                INDArray visY = vis.slice(y);\n+                //get the gradient wrt x and y\n+                gradientsFor(grads,visX,visY,i,currLr,normalize);\n+                INDArray gradsFirst = createGradFirstRow();\n+                INDArray gradsSecond = createGradSecondRow();\n+                err.addi(gradsFirst);\n+                if(updateY)\n+                    visY.addi(gradsSecond);\n+\n+\n+            }\n+\n+            return err;\n+\n+        }\n+\n+    }\n+\n+\n+    public INDArray createGradSecondRow() {\n+        if(gradsSecondRow.get() == null) {\n+            if(grads.get() == null) {\n+                INDArray grads = createGrad();\n+                gradsSecondRow.set(grads.slice(1));\n+            }\n+            else\n+                gradsSecondRow.set(grads.get().slice(1));\n+        }\n+\n+        return gradsSecondRow.get();\n+    }\n+\n+\n+    public OldSubOp getVisYMinusVisX(INDArray x,INDArray y,INDArray result) {\n+        return getSubOp(visYMinusVisX,x,y,result);\n+\n+    }\n+\n+\n+    public OldSubOp getVisXMinusVisY(INDArray x,INDArray y,INDArray result) {\n+        return getSubOp(visXMinusVisY,x,y,result);\n+    }\n+\n+    private OldSubOp getSubOp(ThreadLocal<OldSubOp> threadLocal,INDArray x,INDArray y,INDArray result) {\n+        if(threadLocal.get() == null) {\n+            OldSubOp clipByValue = new OldSubOp(x,y,result);\n+            threadLocal.set(clipByValue);\n+            return clipByValue;\n+        }\n+\n+        OldSubOp clipByValue = threadLocal.get();\n+        clipByValue.setX(x);\n+        clipByValue.setY(y);\n+        return clipByValue;\n+    }\n+\n+\n+    public ClipByValue getClipByValue(INDArray input) {\n+        if(clip.get() == null) {\n+            ClipByValue clipByValue = new ClipByValue(new INDArray[] {input},null,-gradClipValue,gradClipValue,true);\n+            clip.set(clipByValue);\n+            return clipByValue;\n+        }\n+\n+        ClipByValue clipByValue = clip.get();\n+        clipByValue.setInputArgument(0,input);\n+        return clipByValue;\n+    }\n+\n+\n+    public Norm2 getNorm2(INDArray input) {\n+        if(clip.get() == null) {\n+            Norm2 norm2 = new Norm2(input);\n+            this.norm2.set(norm2);\n+            return norm2;\n+        }\n+\n+        Norm2 ret = norm2.get();\n+        ret.setX(input);\n+        return ret;\n+    }\n+\n+\n+    public INDArray createGradFirstRow() {\n+        if(gradsFirstRow.get() == null) {\n+            if(grads.get() == null) {\n+                INDArray grads = createGrad();\n+                gradsFirstRow.set(grads.slice(0));\n+            }\n+            else\n+                gradsFirstRow.set(grads.get().slice(0));\n+        }\n+\n+        return gradsFirstRow.get();\n+    }\n+\n+    public INDArray createGrad() {\n+        if(grads.get() == null) {\n+            grads.set(Nd4j.create(2,vis.columns()));\n+        }\n+\n+        return grads.get();\n+    }\n+\n+    public INDArray createError() {\n+        if(errors.get() == null) {\n+            errors.set(Nd4j.create(outDim));\n+        }\n+\n+        return errors.get();\n+    }\n+\n+\n+\n+    public double distance(int x,int y) {\n+        return RPUtils.computeDistance(distanceFunction,vis.slice(x),vis.slice(y));\n+    }\n+\n+    /**\n+     * Compute the error wrt the given parameters given a sampled edge(p)\n+     * and 2 vectors to compute the distance and error for\n+     *\n+     * Note that there is a side effect of updating y as a part of calling this method.\n+     * This method should mainly be used in gradient checking tests or internally within this class\n+     * not directly by a user.\n+     *\n+     * @param x the x component to compute the error for\n+     * @param y the initial index of y (changes with various negative sampling\n+     * @param p the sampled edge for random access\n+     * @param currLr the current learning rate for the gradient update\n+     * @param updateY whether to update y or not\n+     * @param normalize whether to apply gradient normalization or not\n+     * @return the error wrt the given parameters\n+     */\n+    public INDArray errorWrt(int x, int y, int p, double currLr, boolean updateY,boolean normalize) {\n+        return errorWrt(vis.slice(x),y,p,currLr,updateY,normalize);\n+\n+    }\n+\n+\n+    private class VisualizeThread implements Runnable {\n+        private int id;\n+        private AtomicBoolean done = new AtomicBoolean(false);\n+\n+        public VisualizeThread(int id) {\n+            this.id = id;\n+        }\n+\n+        public boolean isDone() {\n+            return done.get();\n+        }\n+\n+        @Override\n+        public void run() {\n+\n+\n+            log.info(\"Starting visualize thread \" + id);\n+            int edgeCount = 0;\n+            int lastEdgeCount = 0;\n+            int p,x,y;\n+            // opening workspace\n+            MemoryWorkspace workspace = nnDescent.getWorkspace();\n+            try(MemoryWorkspace workspace2 = workspace.notifyScopeEntered()) {\n+                double currLr = initialAlpha;\n+                while (true) {\n+                    if (edgeCount > nnDescent.getNSamples() / numWorkers + 2) {\n+                        break;\n+                    }\n+\n+                    if (edgeCount - lastEdgeCount > 10000) {",
        "comment_created_at": "2018-03-04T18:16:28+00:00",
        "comment_author": "DzianisH",
        "comment_body": "Could you please describe why have you chosen exactly `10000` constant? Why haven't you choose neither `9999` nor `42` constants?\r\nSorry for being quite impolite, i just want to make you understand that it's not clear to some other people what does `10000` mean. It would be nice to move this magic number to class's constants.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "173277288",
    "pr_number": 4629,
    "pr_file": "deeplearning4j-manifold/deeplearning4j-nn-descent/src/main/java/org/deeplearning4j/nndescent/ABParams.java",
    "created_at": "2018-03-08T20:12:20+00:00",
    "commented_code": "+package org.deeplearning4j.nndescent;\n+\n+import lombok.AllArgsConstructor;\n+import lombok.Builder;\n+import org.apache.commons.math3.analysis.UnivariateVectorFunction;\n+import org.apache.commons.math3.analysis.differentiation.FiniteDifferencesDifferentiator;\n+import org.apache.commons.math3.fitting.leastsquares.LeastSquaresBuilder;\n+import org.apache.commons.math3.fitting.leastsquares.LeastSquaresOptimizer;\n+import org.apache.commons.math3.fitting.leastsquares.LevenbergMarquardtOptimizer;\n+import org.apache.commons.math3.fitting.leastsquares.MultivariateJacobianFunction;\n+import org.apache.commons.math3.linear.Array2DRowRealMatrix;\n+import org.apache.commons.math3.linear.ArrayRealVector;\n+import org.apache.commons.math3.linear.RealMatrix;\n+import org.apache.commons.math3.linear.RealVector;\n+import org.apache.commons.math3.util.Pair;\n+import org.bytedeco.javacpp.DoublePointer;\n+import org.bytedeco.javacpp.IntPointer;\n+import org.bytedeco.javacpp.Pointer;\n+import org.bytedeco.javacpp.indexer.DoubleIndexer;\n+import org.nd4j.finitedifferences.TwoPointApproximation;\n+import org.nd4j.linalg.api.blas.Level3;\n+import org.nd4j.linalg.api.buffer.DataBuffer;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.factory.Nd4j;\n+import org.nd4j.linalg.function.Function;\n+import org.nd4j.linalg.indexing.BooleanIndexing;\n+import org.nd4j.linalg.indexing.INDArrayIndex;\n+import org.nd4j.linalg.indexing.SpecifiedIndex;\n+import org.nd4j.linalg.indexing.conditions.GreaterThan;\n+import org.nd4j.linalg.indexing.conditions.GreaterThanOrEqual;\n+import org.nd4j.linalg.ops.transforms.Transforms;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+\n+import static org.nd4j.linalg.ops.transforms.Transforms.exp;\n+import static org.nd4j.linalg.ops.transforms.Transforms.max;\n+import static org.nd4j.linalg.ops.transforms.Transforms.pow;\n+import static org.bytedeco.javacpp.cminpack.*;\n+@Builder\n+public class ABParams {\n+    private double spread;\n+    private double minDistance;\n+\n+    public INDArray[] solve() {\n+        INDArray xv = Nd4j.linspace(0,spread * 3,300);\n+        INDArray yv = Nd4j.zeros(xv.shape());\n+        INDArray xvLtMinDist = xv.lt(minDistance);\n+\n+        yv = yv.putWhereWithMask(xvLtMinDist,1.0);\n+        INDArray xvGteMinDist = xv.getWhere(minDistance,new GreaterThanOrEqual());\n+        INDArray xvGteMinDistMinusMinDist = xvGteMinDist.sub(minDistance);\n+        INDArray neg = xvGteMinDistMinusMinDist.neg();\n+        INDArray divSpread = neg.div(spread);\n+\n+        INDArray toPut = exp(divSpread);\n+        INDArray xvGteMask = xv.gte(minDistance);\n+        yv = yv.putWhereWithMask(xvGteMask,toPut);\n+\n+\n+\n+        Function<INDArray,INDArray> f = new Function<INDArray, INDArray>() {\n+            @Override\n+            public INDArray apply(INDArray indArray) {\n+                INDArray add =  pow(indArray.mul(spread),2 * minDistance);\n+                INDArray arr =  add\n+                        .addi(1).rdivi(1.0);\n+                return arr;\n+            }\n+        };\n+\n+        final  INDArray yvRef = yv;\n+\n+       // LevenbergMarquardtOptimizer levenbergMarquardtOptimizer = new LevenbergMarquardtOptimizer();\n+        double[] params = xv.data().asDouble();\n+        double[] start =  {1,1};\n+\n+        Func func = new Func(xv,yv);\n+        DoublePointer doublePointer = new DoublePointer(start);\n+        double fTol = 1.49012e-08;\n+        double xTol = 1.49012e-08;\n+        double gTol = 0.0;\n+        int maxFev = 600;\n+        double epsfcn = 2.220446049250313e-16;\n+        double factor = 100;\n+        INDArray fVec = Nd4j.create(xv.shape());\n+        DoublePointer fVecPointer = (DoublePointer) fVec.data().pointer();\n+        IntPointer ipvt = new IntPointer(new int[start.length]);\n+        DataBuffer ipvtBuff = Nd4j.createBuffer(new int[]{start.length}, DataBuffer.Type.INT);\n+        INDArray ipvtArr = Nd4j.create(ipvtBuff);\n+        IntPointer nFev = new IntPointer(new int[1]);\n+        INDArray fjac = Nd4j.create(new int[] {start.length,start.length});\n+        DoublePointer fJacPointer = (DoublePointer) fjac.data().pointer();\n+        lmdif(func,doublePointer,\n+                xv.length(),\n+                params.length,\n+                doublePointer,\n+                fVecPointer,\n+                fTol,\n+                xTol,\n+                gTol,\n+                maxFev,\n+                epsfcn,\n+                null,\n+                1,\n+                factor,\n+                0,\n+                nFev,\n+                fJacPointer,\n+                0,\n+                ipvt,\n+                new DoublePointer(),\n+                new DoublePointer(),\n+                new DoublePointer(),\n+                new DoublePointer(),\n+                new DoublePointer());\n+   /*     final FiniteDifferencesDifferentiator finiteDifferencesDifferentiator = new FiniteDifferencesDifferentiator(params.length,1e-3);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "173277288",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 4629,
        "pr_file": "deeplearning4j-manifold/deeplearning4j-nn-descent/src/main/java/org/deeplearning4j/nndescent/ABParams.java",
        "discussion_id": "173277288",
        "commented_code": "@@ -0,0 +1,193 @@\n+package org.deeplearning4j.nndescent;\n+\n+import lombok.AllArgsConstructor;\n+import lombok.Builder;\n+import org.apache.commons.math3.analysis.UnivariateVectorFunction;\n+import org.apache.commons.math3.analysis.differentiation.FiniteDifferencesDifferentiator;\n+import org.apache.commons.math3.fitting.leastsquares.LeastSquaresBuilder;\n+import org.apache.commons.math3.fitting.leastsquares.LeastSquaresOptimizer;\n+import org.apache.commons.math3.fitting.leastsquares.LevenbergMarquardtOptimizer;\n+import org.apache.commons.math3.fitting.leastsquares.MultivariateJacobianFunction;\n+import org.apache.commons.math3.linear.Array2DRowRealMatrix;\n+import org.apache.commons.math3.linear.ArrayRealVector;\n+import org.apache.commons.math3.linear.RealMatrix;\n+import org.apache.commons.math3.linear.RealVector;\n+import org.apache.commons.math3.util.Pair;\n+import org.bytedeco.javacpp.DoublePointer;\n+import org.bytedeco.javacpp.IntPointer;\n+import org.bytedeco.javacpp.Pointer;\n+import org.bytedeco.javacpp.indexer.DoubleIndexer;\n+import org.nd4j.finitedifferences.TwoPointApproximation;\n+import org.nd4j.linalg.api.blas.Level3;\n+import org.nd4j.linalg.api.buffer.DataBuffer;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.factory.Nd4j;\n+import org.nd4j.linalg.function.Function;\n+import org.nd4j.linalg.indexing.BooleanIndexing;\n+import org.nd4j.linalg.indexing.INDArrayIndex;\n+import org.nd4j.linalg.indexing.SpecifiedIndex;\n+import org.nd4j.linalg.indexing.conditions.GreaterThan;\n+import org.nd4j.linalg.indexing.conditions.GreaterThanOrEqual;\n+import org.nd4j.linalg.ops.transforms.Transforms;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+\n+import static org.nd4j.linalg.ops.transforms.Transforms.exp;\n+import static org.nd4j.linalg.ops.transforms.Transforms.max;\n+import static org.nd4j.linalg.ops.transforms.Transforms.pow;\n+import static org.bytedeco.javacpp.cminpack.*;\n+@Builder\n+public class ABParams {\n+    private double spread;\n+    private double minDistance;\n+\n+    public INDArray[] solve() {\n+        INDArray xv = Nd4j.linspace(0,spread * 3,300);\n+        INDArray yv = Nd4j.zeros(xv.shape());\n+        INDArray xvLtMinDist = xv.lt(minDistance);\n+\n+        yv = yv.putWhereWithMask(xvLtMinDist,1.0);\n+        INDArray xvGteMinDist = xv.getWhere(minDistance,new GreaterThanOrEqual());\n+        INDArray xvGteMinDistMinusMinDist = xvGteMinDist.sub(minDistance);\n+        INDArray neg = xvGteMinDistMinusMinDist.neg();\n+        INDArray divSpread = neg.div(spread);\n+\n+        INDArray toPut = exp(divSpread);\n+        INDArray xvGteMask = xv.gte(minDistance);\n+        yv = yv.putWhereWithMask(xvGteMask,toPut);\n+\n+\n+\n+        Function<INDArray,INDArray> f = new Function<INDArray, INDArray>() {\n+            @Override\n+            public INDArray apply(INDArray indArray) {\n+                INDArray add =  pow(indArray.mul(spread),2 * minDistance);\n+                INDArray arr =  add\n+                        .addi(1).rdivi(1.0);\n+                return arr;\n+            }\n+        };\n+\n+        final  INDArray yvRef = yv;\n+\n+       // LevenbergMarquardtOptimizer levenbergMarquardtOptimizer = new LevenbergMarquardtOptimizer();\n+        double[] params = xv.data().asDouble();\n+        double[] start =  {1,1};\n+\n+        Func func = new Func(xv,yv);\n+        DoublePointer doublePointer = new DoublePointer(start);\n+        double fTol = 1.49012e-08;\n+        double xTol = 1.49012e-08;\n+        double gTol = 0.0;\n+        int maxFev = 600;\n+        double epsfcn = 2.220446049250313e-16;\n+        double factor = 100;\n+        INDArray fVec = Nd4j.create(xv.shape());\n+        DoublePointer fVecPointer = (DoublePointer) fVec.data().pointer();\n+        IntPointer ipvt = new IntPointer(new int[start.length]);\n+        DataBuffer ipvtBuff = Nd4j.createBuffer(new int[]{start.length}, DataBuffer.Type.INT);\n+        INDArray ipvtArr = Nd4j.create(ipvtBuff);\n+        IntPointer nFev = new IntPointer(new int[1]);\n+        INDArray fjac = Nd4j.create(new int[] {start.length,start.length});\n+        DoublePointer fJacPointer = (DoublePointer) fjac.data().pointer();\n+        lmdif(func,doublePointer,\n+                xv.length(),\n+                params.length,\n+                doublePointer,\n+                fVecPointer,\n+                fTol,\n+                xTol,\n+                gTol,\n+                maxFev,\n+                epsfcn,\n+                null,\n+                1,\n+                factor,\n+                0,\n+                nFev,\n+                fJacPointer,\n+                0,\n+                ipvt,\n+                new DoublePointer(),\n+                new DoublePointer(),\n+                new DoublePointer(),\n+                new DoublePointer(),\n+                new DoublePointer());\n+   /*     final FiniteDifferencesDifferentiator finiteDifferencesDifferentiator = new FiniteDifferencesDifferentiator(params.length,1e-3);",
        "comment_created_at": "2018-03-08T20:12:20+00:00",
        "comment_author": "DzianisH",
        "comment_body": "Please, remove commented code",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "259219558",
    "pr_number": 7218,
    "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/util/ValidationUtils.java",
    "created_at": "2019-02-22T05:50:10+00:00",
    "commented_code": "+package org.deeplearning4j.util;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Arrays;\n+\n+public class ValidationUtils {",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "259219558",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7218,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/util/ValidationUtils.java",
        "discussion_id": "259219558",
        "commented_code": "@@ -0,0 +1,332 @@\n+package org.deeplearning4j.util;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Arrays;\n+\n+public class ValidationUtils {",
        "comment_created_at": "2019-02-22T05:50:10+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Add private no-arg constructor to avoid users being able to do ```new ValidationUtils()```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "259219793",
    "pr_number": 7218,
    "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/util/ValidationUtils.java",
    "created_at": "2019-02-22T05:52:16+00:00",
    "commented_code": "+package org.deeplearning4j.util;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Arrays;\n+\n+public class ValidationUtils {\n+\n+    /**\n+     * Checks that the values is >= 0.\n+     *\n+     * @param data An int\n+     * @param paramName The param name, for error reporting\n+     */\n+    public static void validateNonNegative(int data, String paramName){\n+        Preconditions.checkArgument(data >= 0,\n+                \"Values for \" +\n+                        paramName + \" must be >= 0, got: \" +\n+                        data);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "259219793",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7218,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/util/ValidationUtils.java",
        "discussion_id": "259219793",
        "commented_code": "@@ -0,0 +1,332 @@\n+package org.deeplearning4j.util;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Arrays;\n+\n+public class ValidationUtils {\n+\n+    /**\n+     * Checks that the values is >= 0.\n+     *\n+     * @param data An int\n+     * @param paramName The param name, for error reporting\n+     */\n+    public static void validateNonNegative(int data, String paramName){\n+        Preconditions.checkArgument(data >= 0,\n+                \"Values for \" +\n+                        paramName + \" must be >= 0, got: \" +\n+                        data);",
        "comment_created_at": "2019-02-22T05:52:16+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "This comment applies to all of your uses of Preconditions case: don't create strings like this.\r\nThe correct way of using preconditions class doesn't (in most cases) result in any object creation unless the precondition fails:\r\n```\r\nPreconditions.checkArgument(data >= 0, \"Values for %s must be >= 0, got %s\", paramName, data);\r\n```",
        "pr_file_module": null
      }
    ]
  }
]