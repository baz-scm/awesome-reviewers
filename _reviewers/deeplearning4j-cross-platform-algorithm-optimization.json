[
  {
    "discussion_id": "655038677",
    "pr_number": 9379,
    "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/compilation_units/standardDeviation.cpp.in",
    "created_at": "2021-06-21T02:38:12+00:00",
    "commented_code": "+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf\n+//\n+\n+#cmakedefine LIBND4J_TYPE_GEN \n+\n+#include <ops/declarable/helpers/cpu/summaryReductions.hpp>\n+\n+namespace sd {",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "655038677",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9379,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/compilation_units/standardDeviation.cpp.in",
        "discussion_id": "655038677",
        "commented_code": "@@ -0,0 +1,32 @@\n+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf\n+//\n+\n+#cmakedefine LIBND4J_TYPE_GEN \n+\n+#include <ops/declarable/helpers/cpu/summaryReductions.hpp>\n+\n+namespace sd {",
        "comment_created_at": "2021-06-21T02:38:12+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "Could you layout where the in files are used and how it works?",
        "pr_file_module": null
      },
      {
        "comment_id": "656011652",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9379,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/compilation_units/standardDeviation.cpp.in",
        "discussion_id": "655038677",
        "commented_code": "@@ -0,0 +1,32 @@\n+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf\n+//\n+\n+#cmakedefine LIBND4J_TYPE_GEN \n+\n+#include <ops/declarable/helpers/cpu/summaryReductions.hpp>\n+\n+namespace sd {",
        "comment_created_at": "2021-06-22T08:48:30+00:00",
        "comment_author": "quickwritereader",
        "comment_body": "brief story: \r\nwe had multiple cpp files with explicit template instantiation. I believe they did it to speed up parallel compilation. Besides some compilers are choked when these template instantiations are many in one translation unit. (for example: nec).\r\n[to get rid of all these copy-paste files](https://github.com/KonduitAI/deeplearning4j/pull/478/files) I used CMake configure to decrease such CPP files. \r\nit will generate and put all these in the compilation_units folder.",
        "pr_file_module": null
      },
      {
        "comment_id": "656055616",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9379,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/compilation_units/standardDeviation.cpp.in",
        "discussion_id": "655038677",
        "commented_code": "@@ -0,0 +1,32 @@\n+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf\n+//\n+\n+#cmakedefine LIBND4J_TYPE_GEN \n+\n+#include <ops/declarable/helpers/cpu/summaryReductions.hpp>\n+\n+namespace sd {",
        "comment_created_at": "2021-06-22T09:44:00+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "@quickwritereader  just make sure to put this in the actual code itself, maybe even in a readme.",
        "pr_file_module": null
      },
      {
        "comment_id": "656103329",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9379,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/compilation_units/standardDeviation.cpp.in",
        "discussion_id": "655038677",
        "commented_code": "@@ -0,0 +1,32 @@\n+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf\n+//\n+\n+#cmakedefine LIBND4J_TYPE_GEN \n+\n+#include <ops/declarable/helpers/cpu/summaryReductions.hpp>\n+\n+namespace sd {",
        "comment_created_at": "2021-06-22T10:50:47+00:00",
        "comment_author": "quickwritereader",
        "comment_body": "done. also  ReadME section [AddingNewOps.md#explicit-template-instantiations-in-helper-methods](https://github.com/eclipse/deeplearning4j/blob/43a48685ba702aedc7dfdea96a49d4ea612d3022/libnd4j/AddingNewOps.md#explicit-template-instantiations-in-helper-methods)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "652557669",
    "pr_number": 9372,
    "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/cudnnUtils.h",
    "created_at": "2021-06-16T10:24:00+00:00",
    "commented_code": "DECLARE_PLATFORM(maxpool3dnew, ENGINE_CUDA);\n     DECLARE_PLATFORM(maxpool3dnew_bp, ENGINE_CUDA);\n \n+    DECLARE_PLATFORM(lstmLayer, ENGINE_CUDA);\n+\n     DECLARE_PLATFORM(ctc_loss, ENGINE_CUDA);\n     DECLARE_PLATFORM(ctc_loss_grad, ENGINE_CUDA);\n \n+\n+//////////////////////////////////////////////////////////////////////////\n+template<typename Op, typename ...Args>\n+FORCEINLINE void callCudnnIfNoErr(cudnnStatus_t &err, Op op, Args&&... args){\n+        if(err==CUDNN_STATUS_SUCCESS){\n+            err = op(std::forward<Args>(args)...);\n+            if(err){\n+                nd4j_printf(\"Cudnn error code %s\n\", cudnnGetErrorString(err));\n+            }\n+        }\n+}\n+\n+template <typename T>\n+FORCEINLINE const T* bufferInHost( const NDArray &array)  {\n+    array.syncToHost();\n+    return reinterpret_cast<const T*>(array.buffer());\n+}\n+\n+ \n+\n+struct CudnnTensor{\n+\n+    CudnnTensor(const CudnnTensor& s) = delete; \n+\n+    CudnnTensor& operator=(const CudnnTensor& other) = delete;\n+\n+    CudnnTensor(){ \n+       create();\n+    }\n+\n+    CudnnTensor(bool createDesc){ \n+       if(createDesc)  create();\n+       else { desc= nullptr; }\n+    }\n+\n+    void create(){\n+      desc_status = cudnnCreateTensorDescriptor(&desc); \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetTensorNdDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnTensorDescriptor_t() const { return desc; }\n+\n+    ~CudnnTensor(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyTensorDescriptor(desc);\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnTensorDescriptor_t desc;\n+};\n+\n+struct CudnnTensorList{\n+\n+    CudnnTensorList(const CudnnTensorList& s) = delete; \n+\n+    CudnnTensorList& operator=(const CudnnTensorList& other) = delete;\n+\n+    CudnnTensorList(int size){ \n+       descList.resize(size);\n+       desc_status = CUDNN_STATUS_SUCCESS;\n+       for(int i=0;i<size;i++){ \n+           desc_status = cudnnCreateTensorDescriptor(&descList[i]);\n+           if(desc_status != CUDNN_STATUS_SUCCESS) break;\n+       } \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, int index, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS && index<descList.size()){\n+            last_err = cudnnSetTensorNdDescriptor(descList[index], std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    cudnnTensorDescriptor_t get(int i) const { \n+        if(i<descList.size()) return descList[i];\n+        return nullptr;\n+     }\n+\n+    const cudnnTensorDescriptor_t* getDescriptors() const { return descList.data(); }\n+\n+    ~CudnnTensorList(){\n+      for(auto x: descList){\n+          cudnnDestroyTensorDescriptor(x);\n+      }\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    std::vector<cudnnTensorDescriptor_t> descList;\n+};\n+\n+struct FilterDesc{\n+\n+    FilterDesc(const FilterDesc& s) = delete; \n+\n+    FilterDesc& operator=(const FilterDesc& other) = delete;\n+\n+    FilterDesc(){ \n+       desc_status = cudnnCreateFilterDescriptor(&desc);\n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetFilterNdDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnFilterDescriptor_t() const { return desc; }\n+\n+    ~FilterDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyFilterDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnFilterDescriptor_t desc;\n+};\n+\n+struct DropoutDesc{\n+\n+    DropoutDesc(const DropoutDesc& s) = delete; \n+\n+    DropoutDesc& operator=(const DropoutDesc& other) = delete;\n+\n+    DropoutDesc(){ \n+       create();\n+    }\n+\n+    DropoutDesc(bool createDesc){ \n+       if(createDesc)  create();\n+       else { desc= nullptr; }\n+    }\n+\n+    void create(){\n+      desc_status = cudnnCreateDropoutDescriptor(&desc); \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetDropoutDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnDropoutDescriptor_t() const { return desc; }\n+\n+    ~DropoutDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyDropoutDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnDropoutDescriptor_t desc;\n+};\n+\n+#if CUDNN_VERSION > CUDNN_NEW_RNN_API_VER\n+struct RnnDataDesc{\n+\n+    RnnDataDesc(const RnnDataDesc& s) = delete; \n+\n+    RnnDataDesc& operator=(const RnnDataDesc& other) = delete;\n+\n+    RnnDataDesc(){ \n+       desc_status = cudnnCreateRNNDataDescriptor(&desc);\n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetRNNDataDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnRNNDataDescriptor_t() const { return desc; }\n+\n+    ~RnnDataDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyRNNDataDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnRNNDataDescriptor_t desc;\n+};\n+#endif\n+\n+FORCEINLINE cudnnStatus_t setRnnDescriptorOldApi(cudnnRNNDescriptor_t rnnDesc, \n+                            cudnnHandle_t handle,\n+                            cudnnRNNInputMode_t inputMode,\n+                            cudnnDirectionMode_t dirMode,\n+                            cudnnRNNMode_t cellMode,\n+                            cudnnRNNAlgo_t algo,\n+                            cudnnDataType_t mathPrec,\n+                            int32_t hiddenSize,\n+                            int32_t numLayers,\n+                            cudnnDropoutDescriptor_t dropoutDesc,  bool use_tensor_op=false){\n+\n+    cudnnStatus_t err = CUDNN_STATUS_SUCCESS;",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "652557669",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9372,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/cudnnUtils.h",
        "discussion_id": "652557669",
        "commented_code": "@@ -60,9 +63,325 @@ namespace platforms {\n     DECLARE_PLATFORM(maxpool3dnew, ENGINE_CUDA);\n     DECLARE_PLATFORM(maxpool3dnew_bp, ENGINE_CUDA);\n \n+    DECLARE_PLATFORM(lstmLayer, ENGINE_CUDA);\n+\n     DECLARE_PLATFORM(ctc_loss, ENGINE_CUDA);\n     DECLARE_PLATFORM(ctc_loss_grad, ENGINE_CUDA);\n \n+\n+//////////////////////////////////////////////////////////////////////////\n+template<typename Op, typename ...Args>\n+FORCEINLINE void callCudnnIfNoErr(cudnnStatus_t &err, Op op, Args&&... args){\n+        if(err==CUDNN_STATUS_SUCCESS){\n+            err = op(std::forward<Args>(args)...);\n+            if(err){\n+                nd4j_printf(\"Cudnn error code %s\\n\", cudnnGetErrorString(err));\n+            }\n+        }\n+}\n+\n+template <typename T>\n+FORCEINLINE const T* bufferInHost( const NDArray &array)  {\n+    array.syncToHost();\n+    return reinterpret_cast<const T*>(array.buffer());\n+}\n+\n+ \n+\n+struct CudnnTensor{\n+\n+    CudnnTensor(const CudnnTensor& s) = delete; \n+\n+    CudnnTensor& operator=(const CudnnTensor& other) = delete;\n+\n+    CudnnTensor(){ \n+       create();\n+    }\n+\n+    CudnnTensor(bool createDesc){ \n+       if(createDesc)  create();\n+       else { desc= nullptr; }\n+    }\n+\n+    void create(){\n+      desc_status = cudnnCreateTensorDescriptor(&desc); \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetTensorNdDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnTensorDescriptor_t() const { return desc; }\n+\n+    ~CudnnTensor(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyTensorDescriptor(desc);\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnTensorDescriptor_t desc;\n+};\n+\n+struct CudnnTensorList{\n+\n+    CudnnTensorList(const CudnnTensorList& s) = delete; \n+\n+    CudnnTensorList& operator=(const CudnnTensorList& other) = delete;\n+\n+    CudnnTensorList(int size){ \n+       descList.resize(size);\n+       desc_status = CUDNN_STATUS_SUCCESS;\n+       for(int i=0;i<size;i++){ \n+           desc_status = cudnnCreateTensorDescriptor(&descList[i]);\n+           if(desc_status != CUDNN_STATUS_SUCCESS) break;\n+       } \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, int index, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS && index<descList.size()){\n+            last_err = cudnnSetTensorNdDescriptor(descList[index], std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    cudnnTensorDescriptor_t get(int i) const { \n+        if(i<descList.size()) return descList[i];\n+        return nullptr;\n+     }\n+\n+    const cudnnTensorDescriptor_t* getDescriptors() const { return descList.data(); }\n+\n+    ~CudnnTensorList(){\n+      for(auto x: descList){\n+          cudnnDestroyTensorDescriptor(x);\n+      }\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    std::vector<cudnnTensorDescriptor_t> descList;\n+};\n+\n+struct FilterDesc{\n+\n+    FilterDesc(const FilterDesc& s) = delete; \n+\n+    FilterDesc& operator=(const FilterDesc& other) = delete;\n+\n+    FilterDesc(){ \n+       desc_status = cudnnCreateFilterDescriptor(&desc);\n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetFilterNdDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnFilterDescriptor_t() const { return desc; }\n+\n+    ~FilterDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyFilterDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnFilterDescriptor_t desc;\n+};\n+\n+struct DropoutDesc{\n+\n+    DropoutDesc(const DropoutDesc& s) = delete; \n+\n+    DropoutDesc& operator=(const DropoutDesc& other) = delete;\n+\n+    DropoutDesc(){ \n+       create();\n+    }\n+\n+    DropoutDesc(bool createDesc){ \n+       if(createDesc)  create();\n+       else { desc= nullptr; }\n+    }\n+\n+    void create(){\n+      desc_status = cudnnCreateDropoutDescriptor(&desc); \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetDropoutDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnDropoutDescriptor_t() const { return desc; }\n+\n+    ~DropoutDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyDropoutDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnDropoutDescriptor_t desc;\n+};\n+\n+#if CUDNN_VERSION > CUDNN_NEW_RNN_API_VER\n+struct RnnDataDesc{\n+\n+    RnnDataDesc(const RnnDataDesc& s) = delete; \n+\n+    RnnDataDesc& operator=(const RnnDataDesc& other) = delete;\n+\n+    RnnDataDesc(){ \n+       desc_status = cudnnCreateRNNDataDescriptor(&desc);\n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetRNNDataDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnRNNDataDescriptor_t() const { return desc; }\n+\n+    ~RnnDataDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyRNNDataDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnRNNDataDescriptor_t desc;\n+};\n+#endif\n+\n+FORCEINLINE cudnnStatus_t setRnnDescriptorOldApi(cudnnRNNDescriptor_t rnnDesc, \n+                            cudnnHandle_t handle,\n+                            cudnnRNNInputMode_t inputMode,\n+                            cudnnDirectionMode_t dirMode,\n+                            cudnnRNNMode_t cellMode,\n+                            cudnnRNNAlgo_t algo,\n+                            cudnnDataType_t mathPrec,\n+                            int32_t hiddenSize,\n+                            int32_t numLayers,\n+                            cudnnDropoutDescriptor_t dropoutDesc,  bool use_tensor_op=false){\n+\n+    cudnnStatus_t err = CUDNN_STATUS_SUCCESS;",
        "comment_created_at": "2021-06-16T10:24:00+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "Mind adding some declarations here so we know what versions of methods are being invoked? nd4j_debug should be fine here",
        "pr_file_module": null
      },
      {
        "comment_id": "652840111",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9372,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/cudnnUtils.h",
        "discussion_id": "652557669",
        "commented_code": "@@ -60,9 +63,325 @@ namespace platforms {\n     DECLARE_PLATFORM(maxpool3dnew, ENGINE_CUDA);\n     DECLARE_PLATFORM(maxpool3dnew_bp, ENGINE_CUDA);\n \n+    DECLARE_PLATFORM(lstmLayer, ENGINE_CUDA);\n+\n     DECLARE_PLATFORM(ctc_loss, ENGINE_CUDA);\n     DECLARE_PLATFORM(ctc_loss_grad, ENGINE_CUDA);\n \n+\n+//////////////////////////////////////////////////////////////////////////\n+template<typename Op, typename ...Args>\n+FORCEINLINE void callCudnnIfNoErr(cudnnStatus_t &err, Op op, Args&&... args){\n+        if(err==CUDNN_STATUS_SUCCESS){\n+            err = op(std::forward<Args>(args)...);\n+            if(err){\n+                nd4j_printf(\"Cudnn error code %s\\n\", cudnnGetErrorString(err));\n+            }\n+        }\n+}\n+\n+template <typename T>\n+FORCEINLINE const T* bufferInHost( const NDArray &array)  {\n+    array.syncToHost();\n+    return reinterpret_cast<const T*>(array.buffer());\n+}\n+\n+ \n+\n+struct CudnnTensor{\n+\n+    CudnnTensor(const CudnnTensor& s) = delete; \n+\n+    CudnnTensor& operator=(const CudnnTensor& other) = delete;\n+\n+    CudnnTensor(){ \n+       create();\n+    }\n+\n+    CudnnTensor(bool createDesc){ \n+       if(createDesc)  create();\n+       else { desc= nullptr; }\n+    }\n+\n+    void create(){\n+      desc_status = cudnnCreateTensorDescriptor(&desc); \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetTensorNdDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnTensorDescriptor_t() const { return desc; }\n+\n+    ~CudnnTensor(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyTensorDescriptor(desc);\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnTensorDescriptor_t desc;\n+};\n+\n+struct CudnnTensorList{\n+\n+    CudnnTensorList(const CudnnTensorList& s) = delete; \n+\n+    CudnnTensorList& operator=(const CudnnTensorList& other) = delete;\n+\n+    CudnnTensorList(int size){ \n+       descList.resize(size);\n+       desc_status = CUDNN_STATUS_SUCCESS;\n+       for(int i=0;i<size;i++){ \n+           desc_status = cudnnCreateTensorDescriptor(&descList[i]);\n+           if(desc_status != CUDNN_STATUS_SUCCESS) break;\n+       } \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, int index, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS && index<descList.size()){\n+            last_err = cudnnSetTensorNdDescriptor(descList[index], std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    cudnnTensorDescriptor_t get(int i) const { \n+        if(i<descList.size()) return descList[i];\n+        return nullptr;\n+     }\n+\n+    const cudnnTensorDescriptor_t* getDescriptors() const { return descList.data(); }\n+\n+    ~CudnnTensorList(){\n+      for(auto x: descList){\n+          cudnnDestroyTensorDescriptor(x);\n+      }\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    std::vector<cudnnTensorDescriptor_t> descList;\n+};\n+\n+struct FilterDesc{\n+\n+    FilterDesc(const FilterDesc& s) = delete; \n+\n+    FilterDesc& operator=(const FilterDesc& other) = delete;\n+\n+    FilterDesc(){ \n+       desc_status = cudnnCreateFilterDescriptor(&desc);\n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetFilterNdDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnFilterDescriptor_t() const { return desc; }\n+\n+    ~FilterDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyFilterDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnFilterDescriptor_t desc;\n+};\n+\n+struct DropoutDesc{\n+\n+    DropoutDesc(const DropoutDesc& s) = delete; \n+\n+    DropoutDesc& operator=(const DropoutDesc& other) = delete;\n+\n+    DropoutDesc(){ \n+       create();\n+    }\n+\n+    DropoutDesc(bool createDesc){ \n+       if(createDesc)  create();\n+       else { desc= nullptr; }\n+    }\n+\n+    void create(){\n+      desc_status = cudnnCreateDropoutDescriptor(&desc); \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetDropoutDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnDropoutDescriptor_t() const { return desc; }\n+\n+    ~DropoutDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyDropoutDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnDropoutDescriptor_t desc;\n+};\n+\n+#if CUDNN_VERSION > CUDNN_NEW_RNN_API_VER\n+struct RnnDataDesc{\n+\n+    RnnDataDesc(const RnnDataDesc& s) = delete; \n+\n+    RnnDataDesc& operator=(const RnnDataDesc& other) = delete;\n+\n+    RnnDataDesc(){ \n+       desc_status = cudnnCreateRNNDataDescriptor(&desc);\n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetRNNDataDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnRNNDataDescriptor_t() const { return desc; }\n+\n+    ~RnnDataDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyRNNDataDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnRNNDataDescriptor_t desc;\n+};\n+#endif\n+\n+FORCEINLINE cudnnStatus_t setRnnDescriptorOldApi(cudnnRNNDescriptor_t rnnDesc, \n+                            cudnnHandle_t handle,\n+                            cudnnRNNInputMode_t inputMode,\n+                            cudnnDirectionMode_t dirMode,\n+                            cudnnRNNMode_t cellMode,\n+                            cudnnRNNAlgo_t algo,\n+                            cudnnDataType_t mathPrec,\n+                            int32_t hiddenSize,\n+                            int32_t numLayers,\n+                            cudnnDropoutDescriptor_t dropoutDesc,  bool use_tensor_op=false){\n+\n+    cudnnStatus_t err = CUDNN_STATUS_SUCCESS;",
        "comment_created_at": "2021-06-16T16:09:54+00:00",
        "comment_author": "quickwritereader",
        "comment_body": "I already wrote nd4j_debug that informs it will use v6 but it's in the caller function.\r\n\r\nI tried to keep Scoped descriptors as simple as I can. Plus as history shows Cudnn side when upgrading they are not hesitant to change method signatures, introduce new functions, and so on. As they had newly introduced functions in v7 which were deprecated in v8. so I had to ignore some of them to make the code cleaner.\r\nwhen CUDNN < 8.0.1 it calls v6 (CUDNN version 6) API ( but still allowing some functionality (for example clipping) by comparing against some versions).\r\nfor > 8 I'm using new v8 API calls. \r\nWhat's good with v8 they simplified/merged API calls, now most argument-based arrays are allocated on the device ( probably for the Cuda graph thing).",
        "pr_file_module": null
      },
      {
        "comment_id": "653295768",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9372,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/cudnnUtils.h",
        "discussion_id": "652557669",
        "commented_code": "@@ -60,9 +63,325 @@ namespace platforms {\n     DECLARE_PLATFORM(maxpool3dnew, ENGINE_CUDA);\n     DECLARE_PLATFORM(maxpool3dnew_bp, ENGINE_CUDA);\n \n+    DECLARE_PLATFORM(lstmLayer, ENGINE_CUDA);\n+\n     DECLARE_PLATFORM(ctc_loss, ENGINE_CUDA);\n     DECLARE_PLATFORM(ctc_loss_grad, ENGINE_CUDA);\n \n+\n+//////////////////////////////////////////////////////////////////////////\n+template<typename Op, typename ...Args>\n+FORCEINLINE void callCudnnIfNoErr(cudnnStatus_t &err, Op op, Args&&... args){\n+        if(err==CUDNN_STATUS_SUCCESS){\n+            err = op(std::forward<Args>(args)...);\n+            if(err){\n+                nd4j_printf(\"Cudnn error code %s\\n\", cudnnGetErrorString(err));\n+            }\n+        }\n+}\n+\n+template <typename T>\n+FORCEINLINE const T* bufferInHost( const NDArray &array)  {\n+    array.syncToHost();\n+    return reinterpret_cast<const T*>(array.buffer());\n+}\n+\n+ \n+\n+struct CudnnTensor{\n+\n+    CudnnTensor(const CudnnTensor& s) = delete; \n+\n+    CudnnTensor& operator=(const CudnnTensor& other) = delete;\n+\n+    CudnnTensor(){ \n+       create();\n+    }\n+\n+    CudnnTensor(bool createDesc){ \n+       if(createDesc)  create();\n+       else { desc= nullptr; }\n+    }\n+\n+    void create(){\n+      desc_status = cudnnCreateTensorDescriptor(&desc); \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetTensorNdDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnTensorDescriptor_t() const { return desc; }\n+\n+    ~CudnnTensor(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyTensorDescriptor(desc);\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnTensorDescriptor_t desc;\n+};\n+\n+struct CudnnTensorList{\n+\n+    CudnnTensorList(const CudnnTensorList& s) = delete; \n+\n+    CudnnTensorList& operator=(const CudnnTensorList& other) = delete;\n+\n+    CudnnTensorList(int size){ \n+       descList.resize(size);\n+       desc_status = CUDNN_STATUS_SUCCESS;\n+       for(int i=0;i<size;i++){ \n+           desc_status = cudnnCreateTensorDescriptor(&descList[i]);\n+           if(desc_status != CUDNN_STATUS_SUCCESS) break;\n+       } \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, int index, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS && index<descList.size()){\n+            last_err = cudnnSetTensorNdDescriptor(descList[index], std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    cudnnTensorDescriptor_t get(int i) const { \n+        if(i<descList.size()) return descList[i];\n+        return nullptr;\n+     }\n+\n+    const cudnnTensorDescriptor_t* getDescriptors() const { return descList.data(); }\n+\n+    ~CudnnTensorList(){\n+      for(auto x: descList){\n+          cudnnDestroyTensorDescriptor(x);\n+      }\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    std::vector<cudnnTensorDescriptor_t> descList;\n+};\n+\n+struct FilterDesc{\n+\n+    FilterDesc(const FilterDesc& s) = delete; \n+\n+    FilterDesc& operator=(const FilterDesc& other) = delete;\n+\n+    FilterDesc(){ \n+       desc_status = cudnnCreateFilterDescriptor(&desc);\n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetFilterNdDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnFilterDescriptor_t() const { return desc; }\n+\n+    ~FilterDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyFilterDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnFilterDescriptor_t desc;\n+};\n+\n+struct DropoutDesc{\n+\n+    DropoutDesc(const DropoutDesc& s) = delete; \n+\n+    DropoutDesc& operator=(const DropoutDesc& other) = delete;\n+\n+    DropoutDesc(){ \n+       create();\n+    }\n+\n+    DropoutDesc(bool createDesc){ \n+       if(createDesc)  create();\n+       else { desc= nullptr; }\n+    }\n+\n+    void create(){\n+      desc_status = cudnnCreateDropoutDescriptor(&desc); \n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetDropoutDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnDropoutDescriptor_t() const { return desc; }\n+\n+    ~DropoutDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyDropoutDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnDropoutDescriptor_t desc;\n+};\n+\n+#if CUDNN_VERSION > CUDNN_NEW_RNN_API_VER\n+struct RnnDataDesc{\n+\n+    RnnDataDesc(const RnnDataDesc& s) = delete; \n+\n+    RnnDataDesc& operator=(const RnnDataDesc& other) = delete;\n+\n+    RnnDataDesc(){ \n+       desc_status = cudnnCreateRNNDataDescriptor(&desc);\n+    }\n+\n+    template<typename ...Args>\n+    void set(cudnnStatus_t &last_err, Args&&... args){ \n+        if(desc_status==CUDNN_STATUS_SUCCESS && last_err==CUDNN_STATUS_SUCCESS){\n+            last_err = cudnnSetRNNDataDescriptor(desc, std::forward<Args>(args)...);\n+            if(last_err){\n+                nd4j_printf(\"Cudnn error code %s\\n\",cudnnGetErrorString(last_err));\n+            }\n+        }\n+    }\n+\n+    cudnnStatus_t descStatus(){ return desc_status;}\n+\n+    operator cudnnRNNDataDescriptor_t() const { return desc; }\n+\n+    ~RnnDataDesc(){\n+      if(desc_status==CUDNN_STATUS_SUCCESS)  cudnnDestroyRNNDataDescriptor(desc); ;\n+    }\n+\n+    cudnnStatus_t desc_status;\n+    cudnnRNNDataDescriptor_t desc;\n+};\n+#endif\n+\n+FORCEINLINE cudnnStatus_t setRnnDescriptorOldApi(cudnnRNNDescriptor_t rnnDesc, \n+                            cudnnHandle_t handle,\n+                            cudnnRNNInputMode_t inputMode,\n+                            cudnnDirectionMode_t dirMode,\n+                            cudnnRNNMode_t cellMode,\n+                            cudnnRNNAlgo_t algo,\n+                            cudnnDataType_t mathPrec,\n+                            int32_t hiddenSize,\n+                            int32_t numLayers,\n+                            cudnnDropoutDescriptor_t dropoutDesc,  bool use_tensor_op=false){\n+\n+    cudnnStatus_t err = CUDNN_STATUS_SUCCESS;",
        "comment_created_at": "2021-06-17T07:22:11+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "The point was more to put this comment you just did in the comments so it's in the code and people don't have to search github to find context  :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "268015339",
    "pr_number": 7317,
    "pr_file": "libnd4j/include/graph/RandomGenerator.h",
    "created_at": "2019-03-22T01:47:33+00:00",
    "commented_code": "FORCEINLINE RandomGenerator::RandomGenerator(Nd4jLong rootSeed, Nd4jLong nodeSeed) {\n             // this seed is used graph-level state\n-            if (rootSeed == 0)\n-                rootSeed = currentMilliseconds();\n+            if (rootSeed == 0){\n+                auto s = std::chrono::system_clock::now().time_since_epoch();\n+                rootSeed  = std::chrono::duration_cast<std::chrono::milliseconds>(s).count();",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "268015339",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7317,
        "pr_file": "libnd4j/include/graph/RandomGenerator.h",
        "discussion_id": "268015339",
        "commented_code": "@@ -139,8 +139,11 @@ namespace nd4j {\n \n         FORCEINLINE RandomGenerator::RandomGenerator(Nd4jLong rootSeed, Nd4jLong nodeSeed) {\n             // this seed is used graph-level state\n-            if (rootSeed == 0)\n-                rootSeed = currentMilliseconds();\n+            if (rootSeed == 0){\n+                auto s = std::chrono::system_clock::now().time_since_epoch();\n+                rootSeed  = std::chrono::duration_cast<std::chrono::milliseconds>(s).count();",
        "comment_created_at": "2019-03-22T01:47:33+00:00",
        "comment_author": "saudet",
        "comment_body": "This is basically what currentMilliseconds() does, how is this different?",
        "pr_file_module": null
      },
      {
        "comment_id": "268078559",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7317,
        "pr_file": "libnd4j/include/graph/RandomGenerator.h",
        "discussion_id": "268015339",
        "commented_code": "@@ -139,8 +139,11 @@ namespace nd4j {\n \n         FORCEINLINE RandomGenerator::RandomGenerator(Nd4jLong rootSeed, Nd4jLong nodeSeed) {\n             // this seed is used graph-level state\n-            if (rootSeed == 0)\n-                rootSeed = currentMilliseconds();\n+            if (rootSeed == 0){\n+                auto s = std::chrono::system_clock::now().time_since_epoch();\n+                rootSeed  = std::chrono::duration_cast<std::chrono::milliseconds>(s).count();",
        "comment_created_at": "2019-03-22T08:47:34+00:00",
        "comment_author": "yquemener",
        "comment_body": "In cases when inlining is deactivated (happened with ncc at one point), this results in a constructor calling a member function, which is generally fine, except in one case: https://stackoverflow.com/questions/3091833/calling-member-functions-from-a-constructor/31757790#31757790\r\n\r\nI can try to retrieve the case where this was happening, one of the tests was failing because of that.",
        "pr_file_module": null
      },
      {
        "comment_id": "268079461",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7317,
        "pr_file": "libnd4j/include/graph/RandomGenerator.h",
        "discussion_id": "268015339",
        "commented_code": "@@ -139,8 +139,11 @@ namespace nd4j {\n \n         FORCEINLINE RandomGenerator::RandomGenerator(Nd4jLong rootSeed, Nd4jLong nodeSeed) {\n             // this seed is used graph-level state\n-            if (rootSeed == 0)\n-                rootSeed = currentMilliseconds();\n+            if (rootSeed == 0){\n+                auto s = std::chrono::system_clock::now().time_since_epoch();\n+                rootSeed  = std::chrono::duration_cast<std::chrono::milliseconds>(s).count();",
        "comment_created_at": "2019-03-22T08:50:32+00:00",
        "comment_author": "saudet",
        "comment_body": "We can make currentMilliseconds() static, that should be fine, right?",
        "pr_file_module": null
      },
      {
        "comment_id": "268089218",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7317,
        "pr_file": "libnd4j/include/graph/RandomGenerator.h",
        "discussion_id": "268015339",
        "commented_code": "@@ -139,8 +139,11 @@ namespace nd4j {\n \n         FORCEINLINE RandomGenerator::RandomGenerator(Nd4jLong rootSeed, Nd4jLong nodeSeed) {\n             // this seed is used graph-level state\n-            if (rootSeed == 0)\n-                rootSeed = currentMilliseconds();\n+            if (rootSeed == 0){\n+                auto s = std::chrono::system_clock::now().time_since_epoch();\n+                rootSeed  = std::chrono::duration_cast<std::chrono::milliseconds>(s).count();",
        "comment_created_at": "2019-03-22T09:21:15+00:00",
        "comment_author": "yquemener",
        "comment_body": "Probably. The constructor used to call SetStates, which was problematic, but currentMilliseconds() is probably no problem.",
        "pr_file_module": null
      },
      {
        "comment_id": "268093654",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7317,
        "pr_file": "libnd4j/include/graph/RandomGenerator.h",
        "discussion_id": "268015339",
        "commented_code": "@@ -139,8 +139,11 @@ namespace nd4j {\n \n         FORCEINLINE RandomGenerator::RandomGenerator(Nd4jLong rootSeed, Nd4jLong nodeSeed) {\n             // this seed is used graph-level state\n-            if (rootSeed == 0)\n-                rootSeed = currentMilliseconds();\n+            if (rootSeed == 0){\n+                auto s = std::chrono::system_clock::now().time_since_epoch();\n+                rootSeed  = std::chrono::duration_cast<std::chrono::milliseconds>(s).count();",
        "comment_created_at": "2019-03-22T09:34:14+00:00",
        "comment_author": "saudet",
        "comment_body": "Ok, let's try to make it static, and revert these lines to call that function again.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "197520428",
    "pr_number": 5686,
    "pr_file": "libnd4j/include/pointercast.h",
    "created_at": "2018-06-22T17:46:14+00:00",
    "commented_code": "typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "197520428",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-22T17:46:14+00:00",
        "comment_author": "raver119",
        "comment_body": "No way.",
        "pr_file_module": null
      },
      {
        "comment_id": "197526537",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-22T18:09:11+00:00",
        "comment_author": "emillynge",
        "comment_body": "I'm fine with removing this typedef, i was simply trying to stay as close as possible to the existing especially how LongBuffers are handles.",
        "pr_file_module": null
      },
      {
        "comment_id": "197527178",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-22T18:11:37+00:00",
        "comment_author": "raver119",
        "comment_body": "Nd4jLong is declared in this way due to legacy reasons. And FlatBuffers. Eventually it should just become int64_t",
        "pr_file_module": null
      },
      {
        "comment_id": "197528135",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-22T18:15:14+00:00",
        "comment_author": "emillynge",
        "comment_body": "So should I just use `int32_t`?",
        "pr_file_module": null
      },
      {
        "comment_id": "197574845",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-22T21:34:12+00:00",
        "comment_author": "raver119",
        "comment_body": "just int for now",
        "pr_file_module": null
      },
      {
        "comment_id": "197600892",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-23T03:22:33+00:00",
        "comment_author": "saudet",
        "comment_body": "Why do we need 32-bit here?",
        "pr_file_module": null
      },
      {
        "comment_id": "197600932",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-23T03:24:01+00:00",
        "comment_author": "raver119",
        "comment_body": "That's exactly we're talking about. One message above.",
        "pr_file_module": null
      },
      {
        "comment_id": "197601163",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-23T03:36:06+00:00",
        "comment_author": "saudet",
        "comment_body": "Why do we need concatInt in the first place?",
        "pr_file_module": null
      },
      {
        "comment_id": "197603234",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-23T05:25:34+00:00",
        "comment_author": "emillynge",
        "comment_body": "This is used to refer to a JavaCPP IntPointer. Those are 32bit as far as I am aware.\r\n\r\nI need concatlong for use with sparse indices. Implementing concatint is simply to be feature complete. ",
        "pr_file_module": null
      },
      {
        "comment_id": "197711721",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-25T08:13:50+00:00",
        "comment_author": "emillynge",
        "comment_body": "@saudet and @raver119 , I need to know whether or not I am correct in assuming that using `int` will not work.\r\n\r\n`int` in cpp is 16 bit AFAIK, whereas IntBuffer reports elementSize is 4 bytes => 32 bit.\r\nTherefore I must assume I shoud be using either `int_32` or `long`, correct?",
        "pr_file_module": null
      },
      {
        "comment_id": "197771784",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-25T12:06:43+00:00",
        "comment_author": "saudet",
        "comment_body": "Java SE doesn't work on platforms with less than 32 bits and `int` is pretty much guaranteed to be 32-bit long on those these days, so `int` is fine. Anyway, we're using a `typedef` in case we want to change it, so not a big issue.",
        "pr_file_module": null
      },
      {
        "comment_id": "197832892",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-25T15:09:37+00:00",
        "comment_author": "emillynge",
        "comment_body": "> Anyway, we're using a typedef in case we want to change it, so not a big issue.\r\n\r\nis int a typedef?  or are you referring to the `Nd4jInt` typedef that @raver119  told me to remove?",
        "pr_file_module": null
      },
      {
        "comment_id": "198396600",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-27T07:50:40+00:00",
        "comment_author": "saudet",
        "comment_body": "No, he asked you to change it to `typedef int Nd4jInt`.",
        "pr_file_module": null
      },
      {
        "comment_id": "198429314",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-27T09:38:14+00:00",
        "comment_author": "emillynge",
        "comment_body": "I'm sorry to drag this out, but in in an earlier [comment](https://github.com/deeplearning4j/deeplearning4j/pull/5686#discussion_r197520132) he said:\r\n> We don't need Nd4jInt definition.\r\n\r\nSo I'm still a bit confused :/",
        "pr_file_module": null
      },
      {
        "comment_id": "198485800",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-27T13:03:04+00:00",
        "comment_author": "saudet",
        "comment_body": "It's not a big deal, let's start by doing something that works! \"long\" is 64-bit on non-Windows system so that's not going to work for sure.",
        "pr_file_module": null
      },
      {
        "comment_id": "198508557",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5686,
        "pr_file": "libnd4j/include/pointercast.h",
        "discussion_id": "197520428",
        "commented_code": "@@ -9,6 +9,7 @@\n \n typedef void* Nd4jPointer;\n typedef long long Nd4jLong;\n+typedef long Nd4jInt;",
        "comment_created_at": "2018-06-27T14:07:44+00:00",
        "comment_author": "emillynge",
        "comment_body": "Gotcha! :)\r\nI will replace with  `typedef int Nd4jInt` and remove the cuda attr stuff.",
        "pr_file_module": null
      }
    ]
  }
]