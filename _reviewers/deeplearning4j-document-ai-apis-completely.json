[
  {
    "discussion_id": "336818255",
    "pr_number": 8301,
    "pr_file": "docs/samediff/templates/model-import.md",
    "created_at": "2019-10-21T02:16:10+00:00",
    "commented_code": "-# Getting started: importing TensorFlow and ONNX models into SameDiff\n\\ No newline at end of file\n+---\n+title: Getting started: importing TensorFlow and ONNX models into SameDiff\n+short_title: Model import\n+description: importing TensorFlow and ONNX models into SameDiff\n+category: SameDiff\n+weight: 3\n+---\n+\n+# Getting started: importing TensorFlow models into SameDiff\n+\n+## What models can be imported into samediff\n+\n+Currently samediff supports the import of Tensorflow frozen graphs through the various Samediff.importFrozenTF methods. \n+Tensorflow documentation on frozen models can be found [here](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk). \n+\n+    import org.nd4j.autodiff.samediff.SameDiff;\n+    \n+    SameDiff sd = SameDiff.importFrozenTF(modelFile);\n+    \n+ ## Finding the model input/outputs and running inference\n+ \n+ After you import the Tensorflow model there are 2 ways to find the inputs and outputs. The first method is to look at the output of\n+ \n+     sd.summary();\n+     \n+ Where the input variables are the output of no ops, and the output variables are the input of no ops.  The other way to find the inputs and outputs is\n+ \n+      List<String> inputs = sd.inputs();\n+      List<String> outputs = sd.outputs());",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "336818255",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 8301,
        "pr_file": "docs/samediff/templates/model-import.md",
        "discussion_id": "336818255",
        "commented_code": "@@ -1 +1,62 @@\n-# Getting started: importing TensorFlow and ONNX models into SameDiff\n\\ No newline at end of file\n+---\n+title: Getting started: importing TensorFlow and ONNX models into SameDiff\n+short_title: Model import\n+description: importing TensorFlow and ONNX models into SameDiff\n+category: SameDiff\n+weight: 3\n+---\n+\n+# Getting started: importing TensorFlow models into SameDiff\n+\n+## What models can be imported into samediff\n+\n+Currently samediff supports the import of Tensorflow frozen graphs through the various Samediff.importFrozenTF methods. \n+Tensorflow documentation on frozen models can be found [here](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk). \n+\n+    import org.nd4j.autodiff.samediff.SameDiff;\n+    \n+    SameDiff sd = SameDiff.importFrozenTF(modelFile);\n+    \n+ ## Finding the model input/outputs and running inference\n+ \n+ After you import the Tensorflow model there are 2 ways to find the inputs and outputs. The first method is to look at the output of\n+ \n+     sd.summary();\n+     \n+ Where the input variables are the output of no ops, and the output variables are the input of no ops.  The other way to find the inputs and outputs is\n+ \n+      List<String> inputs = sd.inputs();\n+      List<String> outputs = sd.outputs());",
        "comment_created_at": "2019-10-21T02:16:10+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "inputs() is good - that just tells you placeholders. Let's note placeholders, and maybe link to other SameDiff page that explains this.\r\nAs for outputs() - that will be going away. Yes it's in 1.0.0-beta5, but it's not robust enough - it's basically \"variables that don't feed into any other ops\" - which often won't be the real predictions that users want - but rather the loss function or some irrelevant unused output.\r\n\r\nSo users should just look at the summary instead and infer the outputs from that.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "338515008",
    "pr_number": 8301,
    "pr_file": "docs/samediff/templates/model-import.md",
    "created_at": "2019-10-24T11:13:48+00:00",
    "commented_code": "-# Getting started: importing TensorFlow and ONNX models into SameDiff\n\\ No newline at end of file\n+---\n+title: Getting started: importing TensorFlow models into SameDiff\n+short_title: Model import\n+description: importing TensorFlow models into SameDiff\n+category: SameDiff\n+weight: 3\n+---\n+\n+# Getting started: importing TensorFlow models into SameDiff\n+\n+## What models can be imported into SameDiff\n+\n+Currently SameDiff supports the import of TensorFlow frozen graphs through the various SameDiff.importFrozenTF methods. \n+TensorFlow documentation on frozen models can be found [here](https://www.TensorFlow.org/guide/saved_model#the_savedmodel_format_on_disk). \n+\n+    import org.nd4j.autodiff.SameDiff.SameDiff;\n+    \n+    SameDiff sd = SameDiff.importFrozenTF(modelFile);\n+    \n+ ## Finding the model input/outputs and running inference\n+ \n+ After you import the TensorFlow model there are 2 ways to find the inputs and outputs. The first method is to look at the output of\n+ \n+     sd.summary();\n+     \n+ Where the input variables are the output of no ops, and the output variables are the input of no ops.  Another way to find the inputs is\n+ \n+      List<String> inputs = sd.inputs();\n+    \n+ To run inference use:\n+ \n+    INDArray out = sd.batchOutput()\n+        .input(inputs, inputArray)\n+        .output(outputs)\n+        .execSingle();",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "338515008",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 8301,
        "pr_file": "docs/samediff/templates/model-import.md",
        "discussion_id": "338515008",
        "commented_code": "@@ -1 +1,61 @@\n-# Getting started: importing TensorFlow and ONNX models into SameDiff\n\\ No newline at end of file\n+---\n+title: Getting started: importing TensorFlow models into SameDiff\n+short_title: Model import\n+description: importing TensorFlow models into SameDiff\n+category: SameDiff\n+weight: 3\n+---\n+\n+# Getting started: importing TensorFlow models into SameDiff\n+\n+## What models can be imported into SameDiff\n+\n+Currently SameDiff supports the import of TensorFlow frozen graphs through the various SameDiff.importFrozenTF methods. \n+TensorFlow documentation on frozen models can be found [here](https://www.TensorFlow.org/guide/saved_model#the_savedmodel_format_on_disk). \n+\n+    import org.nd4j.autodiff.SameDiff.SameDiff;\n+    \n+    SameDiff sd = SameDiff.importFrozenTF(modelFile);\n+    \n+ ## Finding the model input/outputs and running inference\n+ \n+ After you import the TensorFlow model there are 2 ways to find the inputs and outputs. The first method is to look at the output of\n+ \n+     sd.summary();\n+     \n+ Where the input variables are the output of no ops, and the output variables are the input of no ops.  Another way to find the inputs is\n+ \n+      List<String> inputs = sd.inputs();\n+    \n+ To run inference use:\n+ \n+    INDArray out = sd.batchOutput()\n+        .input(inputs, inputArray)\n+        .output(outputs)\n+        .execSingle();",
        "comment_created_at": "2019-10-24T11:13:48+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "\"For multiple outputs, use exec() instead of execSingle(), to return a `Map<String,INDArray>` of outputs instead.\r\nAlternatively, you can use methods such as `SameDiff.output(Map<String, INDArray> placeholders, String... outputs)` to get the same output.\"",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "338517420",
    "pr_number": 8301,
    "pr_file": "docs/samediff/templates/model-import.md",
    "created_at": "2019-10-24T11:20:32+00:00",
    "commented_code": "-# Getting started: importing TensorFlow and ONNX models into SameDiff\n\\ No newline at end of file\n+---\n+title: Getting started: importing TensorFlow models into SameDiff\n+short_title: Model import\n+description: importing TensorFlow models into SameDiff\n+category: SameDiff\n+weight: 3\n+---\n+\n+# Getting started: importing TensorFlow models into SameDiff\n+\n+## What models can be imported into SameDiff\n+\n+Currently SameDiff supports the import of TensorFlow frozen graphs through the various SameDiff.importFrozenTF methods. \n+TensorFlow documentation on frozen models can be found [here](https://www.TensorFlow.org/guide/saved_model#the_savedmodel_format_on_disk). \n+\n+    import org.nd4j.autodiff.SameDiff.SameDiff;\n+    \n+    SameDiff sd = SameDiff.importFrozenTF(modelFile);\n+    \n+ ## Finding the model input/outputs and running inference\n+ \n+ After you import the TensorFlow model there are 2 ways to find the inputs and outputs. The first method is to look at the output of\n+ \n+     sd.summary();\n+     \n+ Where the input variables are the output of no ops, and the output variables are the input of no ops.  Another way to find the inputs is\n+ \n+      List<String> inputs = sd.inputs();\n+    \n+ To run inference use:\n+ \n+    INDArray out = sd.batchOutput()\n+        .input(inputs, inputArray)\n+        .output(outputs)\n+        .execSingle();\n+\n+##  Import Validation.\n+We have a TensorFlow graph analyzing utility which will report any missing operations (operations that still need to be implemented) [here](https://github.com/eclipse/deeplearning4j/blob/master/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/imports/TensorFlow/TensorFlowImportValidator.java)\n+\n+## Advanced: Node Skipping and Import Overrides\n+It is possible to remove nodes from the network. For example TensorFlow 1.x models can have hard coded dropout layers. \n+See the [BERT Graph test](https://github.com/eclipse/deeplearning4j/blob/master/nd4j/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/imports/TFGraphs/BERTGraphTest.java#L114-L150) for an example.\n+\n+## List of models known to work with SameDiff.\n+ \t\t\n+- [PorV-RNN](https://deeplearning4jblob.blob.core.windows.net/testresources/PorV-RNN_frozenmodel.pb)\n+- [alexnet](https://deeplearning4jblob.blob.core.windows.net/testresources/alexnet_frozenmodel.pb)\n+- [cifar10_gan_85](https://deeplearning4jblob.blob.core.windows.net/testresources/cifar10_gan_85_frozenmodel.pb)\n+- [deeplab_mobilenetv2_coco_voc_trainval](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz)\n+- [densenet_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/densenet_2018_04_27.tgz)\n+- [inception_resnet_v2_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_resnet_v2_2018_04_27.tgz)\n+- [inception_v4_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v4_2018_04_27.tgz)\n+- [labels](https://github.com/KonduitAI/dl4j-test-resources/tree/master/src/main/resources/tf_graphs/zoo_models/labels)\n+- [mobilenet_v1_0.5_128](http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_0.5_128.tgz)\n+- [mobilenet_v2_1.0_224](http://download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz)\n+- [nasnet_mobile_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/nasnet_mobile_2018_04_27.tgz)\n+- [resnetv2_imagenet_frozen_graph](http://download.tensorflow.org/models/official/resnetv2_imagenet_frozen_graph.pb)\n+- [squeezenet_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/squeezenet_2018_04_27.tgz)\n+- [temperature_bidirectional_63](https://deeplearning4jblob.blob.core.windows.net/testresources/temperature_bidirectional_63_frozenmodel.pb)\n+- [temperature_stacked_63](https://deeplearning4jblob.blob.core.windows.net/testresources/temperature_stacked_63_frozenmodel.pb)\n+- [text_gen_81](https://deeplearning4jblob.blob.core.windows.net/testresources/text_gen_81_frozenmodel.pb)",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "338517420",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 8301,
        "pr_file": "docs/samediff/templates/model-import.md",
        "discussion_id": "338517420",
        "commented_code": "@@ -1 +1,61 @@\n-# Getting started: importing TensorFlow and ONNX models into SameDiff\n\\ No newline at end of file\n+---\n+title: Getting started: importing TensorFlow models into SameDiff\n+short_title: Model import\n+description: importing TensorFlow models into SameDiff\n+category: SameDiff\n+weight: 3\n+---\n+\n+# Getting started: importing TensorFlow models into SameDiff\n+\n+## What models can be imported into SameDiff\n+\n+Currently SameDiff supports the import of TensorFlow frozen graphs through the various SameDiff.importFrozenTF methods. \n+TensorFlow documentation on frozen models can be found [here](https://www.TensorFlow.org/guide/saved_model#the_savedmodel_format_on_disk). \n+\n+    import org.nd4j.autodiff.SameDiff.SameDiff;\n+    \n+    SameDiff sd = SameDiff.importFrozenTF(modelFile);\n+    \n+ ## Finding the model input/outputs and running inference\n+ \n+ After you import the TensorFlow model there are 2 ways to find the inputs and outputs. The first method is to look at the output of\n+ \n+     sd.summary();\n+     \n+ Where the input variables are the output of no ops, and the output variables are the input of no ops.  Another way to find the inputs is\n+ \n+      List<String> inputs = sd.inputs();\n+    \n+ To run inference use:\n+ \n+    INDArray out = sd.batchOutput()\n+        .input(inputs, inputArray)\n+        .output(outputs)\n+        .execSingle();\n+\n+##  Import Validation.\n+We have a TensorFlow graph analyzing utility which will report any missing operations (operations that still need to be implemented) [here](https://github.com/eclipse/deeplearning4j/blob/master/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/imports/TensorFlow/TensorFlowImportValidator.java)\n+\n+## Advanced: Node Skipping and Import Overrides\n+It is possible to remove nodes from the network. For example TensorFlow 1.x models can have hard coded dropout layers. \n+See the [BERT Graph test](https://github.com/eclipse/deeplearning4j/blob/master/nd4j/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/imports/TFGraphs/BERTGraphTest.java#L114-L150) for an example.\n+\n+## List of models known to work with SameDiff.\n+ \t\t\n+- [PorV-RNN](https://deeplearning4jblob.blob.core.windows.net/testresources/PorV-RNN_frozenmodel.pb)\n+- [alexnet](https://deeplearning4jblob.blob.core.windows.net/testresources/alexnet_frozenmodel.pb)\n+- [cifar10_gan_85](https://deeplearning4jblob.blob.core.windows.net/testresources/cifar10_gan_85_frozenmodel.pb)\n+- [deeplab_mobilenetv2_coco_voc_trainval](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz)\n+- [densenet_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/densenet_2018_04_27.tgz)\n+- [inception_resnet_v2_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_resnet_v2_2018_04_27.tgz)\n+- [inception_v4_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v4_2018_04_27.tgz)\n+- [labels](https://github.com/KonduitAI/dl4j-test-resources/tree/master/src/main/resources/tf_graphs/zoo_models/labels)\n+- [mobilenet_v1_0.5_128](http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_0.5_128.tgz)\n+- [mobilenet_v2_1.0_224](http://download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz)\n+- [nasnet_mobile_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/nasnet_mobile_2018_04_27.tgz)\n+- [resnetv2_imagenet_frozen_graph](http://download.tensorflow.org/models/official/resnetv2_imagenet_frozen_graph.pb)\n+- [squeezenet_2018_04_27](https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/squeezenet_2018_04_27.tgz)\n+- [temperature_bidirectional_63](https://deeplearning4jblob.blob.core.windows.net/testresources/temperature_bidirectional_63_frozenmodel.pb)\n+- [temperature_stacked_63](https://deeplearning4jblob.blob.core.windows.net/testresources/temperature_stacked_63_frozenmodel.pb)\n+- [text_gen_81](https://deeplearning4jblob.blob.core.windows.net/testresources/text_gen_81_frozenmodel.pb)",
        "comment_created_at": "2019-10-24T11:20:32+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Let's add a section with this, which should be good enough for now. We'll get proper coverage info up at a later date.\r\n\r\n```\r\n## Operations Coverage\r\n\r\nSameDiff's TensorFlow import is still being developed, and does not yet have support for every single operation and datatype in TensorFlow.\r\nAlmost all of the common/standard operations are importable and tested, however - including almost everything in the tf, tf.math, tf.layers, tf.losses, tf.bitwise and tf.nn namespaces. The majority of existing pretrained models out there should be importable into SameDiff.\r\n\r\nIf you run into an operation that can't be imported, feel free to open an issue here: https://github.com/eclipse/deeplearning4j/issues\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "307208100",
    "pr_number": 8043,
    "pr_file": "docs/samediff/templates/ops.md",
    "created_at": "2019-07-25T09:44:16+00:00",
    "commented_code": "-# SameDiff operations\n-\n-{{autogenerated}}\n\\ No newline at end of file\n+---\r\n+title: Operations in SameDiff\r\n+short_title: Ops\r\n+description: What kind of operations is there in `SameDiff` and how to use them\r\n+category: SameDiff\r\n+weight: 4\r\n+---\r\n+\r\n+# SameDiff operations\r\n+\r\n+Operations in `SameDiff` work mostly the way you'd expect them to. You take variables - in our framework, those are \r\n+objects of type `SDVariable` - apply operations to them, and thus produce new variables. Before we proceed to the \r\n+overview of the available operations, let us list some of their common properties.\r\n+\r\n+## Common properties of operations\r\n+\r\n+- All operations should return a new `SDVariable`. Thus, a standalone operation like ```x.mul(2);``` will not work; one \r\n+needs to have something like\r\n+```java\r\n+SDVariable _2x = x.mul(2);\r\n+``` \r\n+- All variables in an operation have to belong to the same instance of `SamdeDiff` (see the [variables](./samediff/variables)\r\n+section on how variables are added to a `SameDiff` instance). In other words, the following code will also produce an \r\n+exception \r\n+```java\r\n+SDVariable x = sameDiff0.var(DataType.FLOAT, 1);\r\n+SDVariable y = sameDiff1.placeHolder(DataType.FLOAT, 1);\r\n+//The following code produces exception, because SameDiff instances are different\r\n+SDVariable z = x.add(y);\r\n+```\r\n+- Operations **may not** be used to redefine variables that were already introduced. The following code will produce an \r\n+exception\r\n+```java\r\n+SDVariable z = x.add(y);\r\n+//The following code produces an exception!!!\r\n+x = z.mul(y);\r\n+``` \r\n+To learn more why it is made like that, see our section on [graph](./samediff/graph).\r\n+- Variables of any *variable type* may be used in any operation, as long as their *data types* match those that are \r\n+required by the operation (again, see our [variables](./samediff/variables) section for what variable types are). Most\r\n+often an operation will require its `SDVariable` to have a floating point data type.\r\n+- Variables created by operations have `ARRAY` variable type.\r\n+- For all operations, you may define a `String` name of your resulting variable, although for most operations this\r\n+is not obligatory. The name goes as the first (optional) argument in each operation, like so:\r\n+```java\r\n+SDVariable linear = weights.mmul(\"matrix_product\", input).add(bias); \r\n+SDVariable output = sameDiff.nn.sigmoid(\"output\", linear);\r\n+``` \r\n+Named variables may be accessed from outside using a `SameDiff` method `getVariable(String name)`. For the code above, \r\n+this method will allow you to infer the value of both `output` as well as the result of `mmul` operation. Note that we \r\n+haven't even explicitly defined this result as a separate `SDVariable`, and yet a corresponding `SDVariable` will be \r\n+created internally and added to our instance of `SameDiff` under the `String` name `\"matrix_product\"`.\r\n+- All operations are ultimately produced by our [differential function factory](./samediff/function-factory).",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "307208100",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 8043,
        "pr_file": "docs/samediff/templates/ops.md",
        "discussion_id": "307208100",
        "commented_code": "@@ -1,3 +1,243 @@\n-# SameDiff operations\n-\n-{{autogenerated}}\n\\ No newline at end of file\n+---\r\n+title: Operations in SameDiff\r\n+short_title: Ops\r\n+description: What kind of operations is there in `SameDiff` and how to use them\r\n+category: SameDiff\r\n+weight: 4\r\n+---\r\n+\r\n+# SameDiff operations\r\n+\r\n+Operations in `SameDiff` work mostly the way you'd expect them to. You take variables - in our framework, those are \r\n+objects of type `SDVariable` - apply operations to them, and thus produce new variables. Before we proceed to the \r\n+overview of the available operations, let us list some of their common properties.\r\n+\r\n+## Common properties of operations\r\n+\r\n+- All operations should return a new `SDVariable`. Thus, a standalone operation like ```x.mul(2);``` will not work; one \r\n+needs to have something like\r\n+```java\r\n+SDVariable _2x = x.mul(2);\r\n+``` \r\n+- All variables in an operation have to belong to the same instance of `SamdeDiff` (see the [variables](./samediff/variables)\r\n+section on how variables are added to a `SameDiff` instance). In other words, the following code will also produce an \r\n+exception \r\n+```java\r\n+SDVariable x = sameDiff0.var(DataType.FLOAT, 1);\r\n+SDVariable y = sameDiff1.placeHolder(DataType.FLOAT, 1);\r\n+//The following code produces exception, because SameDiff instances are different\r\n+SDVariable z = x.add(y);\r\n+```\r\n+- Operations **may not** be used to redefine variables that were already introduced. The following code will produce an \r\n+exception\r\n+```java\r\n+SDVariable z = x.add(y);\r\n+//The following code produces an exception!!!\r\n+x = z.mul(y);\r\n+``` \r\n+To learn more why it is made like that, see our section on [graph](./samediff/graph).\r\n+- Variables of any *variable type* may be used in any operation, as long as their *data types* match those that are \r\n+required by the operation (again, see our [variables](./samediff/variables) section for what variable types are). Most\r\n+often an operation will require its `SDVariable` to have a floating point data type.\r\n+- Variables created by operations have `ARRAY` variable type.\r\n+- For all operations, you may define a `String` name of your resulting variable, although for most operations this\r\n+is not obligatory. The name goes as the first (optional) argument in each operation, like so:\r\n+```java\r\n+SDVariable linear = weights.mmul(\"matrix_product\", input).add(bias); \r\n+SDVariable output = sameDiff.nn.sigmoid(\"output\", linear);\r\n+``` \r\n+Named variables may be accessed from outside using a `SameDiff` method `getVariable(String name)`. For the code above, \r\n+this method will allow you to infer the value of both `output` as well as the result of `mmul` operation. Note that we \r\n+haven't even explicitly defined this result as a separate `SDVariable`, and yet a corresponding `SDVariable` will be \r\n+created internally and added to our instance of `SameDiff` under the `String` name `\"matrix_product\"`.\r\n+- All operations are ultimately produced by our [differential function factory](./samediff/function-factory).\r",
        "comment_created_at": "2019-07-25T09:44:16+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "DifferentialFunctionFactory is an internal detail most users will never need to touch, let's not mention this.",
        "pr_file_module": null
      }
    ]
  }
]