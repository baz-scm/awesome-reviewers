[
  {
    "discussion_id": "273860051",
    "pr_number": 7311,
    "pr_file": "libnd4j/include/ops/declarable/generic/nn/dot_product_attention.cpp",
    "created_at": "2019-04-10T09:08:19+00:00",
    "commented_code": "+/*******************************************************************************\n+ * Copyright (c) 2015-2019 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+//\n+// @author Paul Dubs\n+//\n+\n+#include <op_boilerplate.h>\n+#if NOT_EXCLUDED(OP_dot_product_attention)\n+\n+#include <ops/declarable/CustomOperations.h>\n+#include <ops/declarable/helpers/reverse.h>\n+\n+\n+namespace nd4j {\n+namespace ops  {\n+\n+    CUSTOM_OP_IMPL(dot_product_attention, 3, -1, false, 0, 2) {\n+        auto queries = INPUT_VARIABLE(0);\n+        auto keys = INPUT_VARIABLE(1);\n+        auto values = INPUT_VARIABLE(2);\n+        auto mask    = block.width() > 3 ? INPUT_VARIABLE(3) : nullptr;\n+\n+        auto output = OUTPUT_VARIABLE(0);\n+        NDArray* weights;\n+        bool outputWeights = INT_ARG(1);\n+        if(outputWeights){\n+            weights = OUTPUT_VARIABLE(1);\n+        }else{\n+            auto weightShape = ShapeUtils::evalShapeForMatmul(keys->getShapeInfo(), queries->getShapeInfo(), true, false);\n+            weights = NDArrayFactory::create_('c', weightShape, values->dataType(), block.workspace());\n+        }\n+\n+        int normalization = INT_ARG(0);\n+\n+        REQUIRE_TRUE(queries->rankOf() == keys->rankOf() && keys->rankOf() == values->rankOf(), 0,\n+                     \"dot_product_attention: Queries, Keys and Values must have same rank. \"\n+                     \"But got queries = %s, keys = %s, values = %s\", ShapeUtils::shapeAsString(queries).c_str(),\n+                     ShapeUtils::shapeAsString(keys).c_str(), ShapeUtils::shapeAsString(values).c_str());\n+\n+        REQUIRE_TRUE(queries->rankOf() == 3 || queries->rankOf() == 4, 0,\n+                     \"dot_product_attention: Queries, Keys and Values must be rank 3 arrays for single headed attention \"\n+                     \"or rank 4 arrays for multi headed attention. But got rank = %i\", queries->rankOf());\n+\n+        REQUIRE_TRUE(queries->sizeAt(0) == keys->sizeAt(0) && keys->sizeAt(0) == values->sizeAt(0), 0,\n+                \"dot_product_attention: Queries, Keys and Values must have the same mini batch size. \"\n+                \"But got queries = %i, keys = %i, values = %i\", queries->sizeAt(0), keys->sizeAt(0), values->sizeAt(0));\n+\n+        REQUIRE_TRUE(queries->sizeAt(-2) == keys->sizeAt(-2), 0,\n+                \"dot_product_attention: Queries and Keys must have the same feature size. \"\n+                \"But got queries = %i, keys = %i\", queries->sizeAt(-2), keys->sizeAt(-2));\n+\n+        REQUIRE_TRUE(keys->sizeAt(-1) == values->sizeAt(-1), 0,\n+                \"dot_product_attention: Keys and Values must have the same timestep length. \"\n+                \"But got keys = %i, values = %i\", keys->sizeAt(-1), values->sizeAt(-1));\n+\n+        nd4j::ops::matmul mmul;\n+        mmul.execute({keys, queries}, {weights}, {}, {1}, {});\n+        if(normalization) {\n+            *weights /= sqrt((double)keys->sizeAt(-2));\n+        }\n+\n+        if(mask != nullptr){\n+            NDArray* reshapedMask;\n+            if(weights->rankOf() == 4){\n+                reshapedMask = mask->reshape(mask->ordering(), {mask->sizeAt(0), 1, mask->sizeAt(1), 1});\n+            }else{\n+                reshapedMask = mask->reshape(mask->ordering(), {mask->sizeAt(0), mask->sizeAt(1), 1});\n+            }\n+            *weights += (*reshapedMask - 1) * 1e9;",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "273860051",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7311,
        "pr_file": "libnd4j/include/ops/declarable/generic/nn/dot_product_attention.cpp",
        "discussion_id": "273860051",
        "commented_code": "@@ -0,0 +1,223 @@\n+/*******************************************************************************\n+ * Copyright (c) 2015-2019 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+//\n+// @author Paul Dubs\n+//\n+\n+#include <op_boilerplate.h>\n+#if NOT_EXCLUDED(OP_dot_product_attention)\n+\n+#include <ops/declarable/CustomOperations.h>\n+#include <ops/declarable/helpers/reverse.h>\n+\n+\n+namespace nd4j {\n+namespace ops  {\n+\n+    CUSTOM_OP_IMPL(dot_product_attention, 3, -1, false, 0, 2) {\n+        auto queries = INPUT_VARIABLE(0);\n+        auto keys = INPUT_VARIABLE(1);\n+        auto values = INPUT_VARIABLE(2);\n+        auto mask    = block.width() > 3 ? INPUT_VARIABLE(3) : nullptr;\n+\n+        auto output = OUTPUT_VARIABLE(0);\n+        NDArray* weights;\n+        bool outputWeights = INT_ARG(1);\n+        if(outputWeights){\n+            weights = OUTPUT_VARIABLE(1);\n+        }else{\n+            auto weightShape = ShapeUtils::evalShapeForMatmul(keys->getShapeInfo(), queries->getShapeInfo(), true, false);\n+            weights = NDArrayFactory::create_('c', weightShape, values->dataType(), block.workspace());\n+        }\n+\n+        int normalization = INT_ARG(0);\n+\n+        REQUIRE_TRUE(queries->rankOf() == keys->rankOf() && keys->rankOf() == values->rankOf(), 0,\n+                     \"dot_product_attention: Queries, Keys and Values must have same rank. \"\n+                     \"But got queries = %s, keys = %s, values = %s\", ShapeUtils::shapeAsString(queries).c_str(),\n+                     ShapeUtils::shapeAsString(keys).c_str(), ShapeUtils::shapeAsString(values).c_str());\n+\n+        REQUIRE_TRUE(queries->rankOf() == 3 || queries->rankOf() == 4, 0,\n+                     \"dot_product_attention: Queries, Keys and Values must be rank 3 arrays for single headed attention \"\n+                     \"or rank 4 arrays for multi headed attention. But got rank = %i\", queries->rankOf());\n+\n+        REQUIRE_TRUE(queries->sizeAt(0) == keys->sizeAt(0) && keys->sizeAt(0) == values->sizeAt(0), 0,\n+                \"dot_product_attention: Queries, Keys and Values must have the same mini batch size. \"\n+                \"But got queries = %i, keys = %i, values = %i\", queries->sizeAt(0), keys->sizeAt(0), values->sizeAt(0));\n+\n+        REQUIRE_TRUE(queries->sizeAt(-2) == keys->sizeAt(-2), 0,\n+                \"dot_product_attention: Queries and Keys must have the same feature size. \"\n+                \"But got queries = %i, keys = %i\", queries->sizeAt(-2), keys->sizeAt(-2));\n+\n+        REQUIRE_TRUE(keys->sizeAt(-1) == values->sizeAt(-1), 0,\n+                \"dot_product_attention: Keys and Values must have the same timestep length. \"\n+                \"But got keys = %i, values = %i\", keys->sizeAt(-1), values->sizeAt(-1));\n+\n+        nd4j::ops::matmul mmul;\n+        mmul.execute({keys, queries}, {weights}, {}, {1}, {});\n+        if(normalization) {\n+            *weights /= sqrt((double)keys->sizeAt(-2));\n+        }\n+\n+        if(mask != nullptr){\n+            NDArray* reshapedMask;\n+            if(weights->rankOf() == 4){\n+                reshapedMask = mask->reshape(mask->ordering(), {mask->sizeAt(0), 1, mask->sizeAt(1), 1});\n+            }else{\n+                reshapedMask = mask->reshape(mask->ordering(), {mask->sizeAt(0), mask->sizeAt(1), 1});\n+            }\n+            *weights += (*reshapedMask - 1) * 1e9;",
        "comment_created_at": "2019-04-10T09:08:19+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Maybe add a comment here - seemed a little strange on first glance.\r\nI gather you want masked steps to have very large negative attention weights as a way of doing \"masked softmax\"?\r\nHow was the 1e9 chosen? (just wondering if we'll even have numerical stability issues...)",
        "pr_file_module": null
      },
      {
        "comment_id": "274258789",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7311,
        "pr_file": "libnd4j/include/ops/declarable/generic/nn/dot_product_attention.cpp",
        "discussion_id": "273860051",
        "commented_code": "@@ -0,0 +1,223 @@\n+/*******************************************************************************\n+ * Copyright (c) 2015-2019 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+//\n+// @author Paul Dubs\n+//\n+\n+#include <op_boilerplate.h>\n+#if NOT_EXCLUDED(OP_dot_product_attention)\n+\n+#include <ops/declarable/CustomOperations.h>\n+#include <ops/declarable/helpers/reverse.h>\n+\n+\n+namespace nd4j {\n+namespace ops  {\n+\n+    CUSTOM_OP_IMPL(dot_product_attention, 3, -1, false, 0, 2) {\n+        auto queries = INPUT_VARIABLE(0);\n+        auto keys = INPUT_VARIABLE(1);\n+        auto values = INPUT_VARIABLE(2);\n+        auto mask    = block.width() > 3 ? INPUT_VARIABLE(3) : nullptr;\n+\n+        auto output = OUTPUT_VARIABLE(0);\n+        NDArray* weights;\n+        bool outputWeights = INT_ARG(1);\n+        if(outputWeights){\n+            weights = OUTPUT_VARIABLE(1);\n+        }else{\n+            auto weightShape = ShapeUtils::evalShapeForMatmul(keys->getShapeInfo(), queries->getShapeInfo(), true, false);\n+            weights = NDArrayFactory::create_('c', weightShape, values->dataType(), block.workspace());\n+        }\n+\n+        int normalization = INT_ARG(0);\n+\n+        REQUIRE_TRUE(queries->rankOf() == keys->rankOf() && keys->rankOf() == values->rankOf(), 0,\n+                     \"dot_product_attention: Queries, Keys and Values must have same rank. \"\n+                     \"But got queries = %s, keys = %s, values = %s\", ShapeUtils::shapeAsString(queries).c_str(),\n+                     ShapeUtils::shapeAsString(keys).c_str(), ShapeUtils::shapeAsString(values).c_str());\n+\n+        REQUIRE_TRUE(queries->rankOf() == 3 || queries->rankOf() == 4, 0,\n+                     \"dot_product_attention: Queries, Keys and Values must be rank 3 arrays for single headed attention \"\n+                     \"or rank 4 arrays for multi headed attention. But got rank = %i\", queries->rankOf());\n+\n+        REQUIRE_TRUE(queries->sizeAt(0) == keys->sizeAt(0) && keys->sizeAt(0) == values->sizeAt(0), 0,\n+                \"dot_product_attention: Queries, Keys and Values must have the same mini batch size. \"\n+                \"But got queries = %i, keys = %i, values = %i\", queries->sizeAt(0), keys->sizeAt(0), values->sizeAt(0));\n+\n+        REQUIRE_TRUE(queries->sizeAt(-2) == keys->sizeAt(-2), 0,\n+                \"dot_product_attention: Queries and Keys must have the same feature size. \"\n+                \"But got queries = %i, keys = %i\", queries->sizeAt(-2), keys->sizeAt(-2));\n+\n+        REQUIRE_TRUE(keys->sizeAt(-1) == values->sizeAt(-1), 0,\n+                \"dot_product_attention: Keys and Values must have the same timestep length. \"\n+                \"But got keys = %i, values = %i\", keys->sizeAt(-1), values->sizeAt(-1));\n+\n+        nd4j::ops::matmul mmul;\n+        mmul.execute({keys, queries}, {weights}, {}, {1}, {});\n+        if(normalization) {\n+            *weights /= sqrt((double)keys->sizeAt(-2));\n+        }\n+\n+        if(mask != nullptr){\n+            NDArray* reshapedMask;\n+            if(weights->rankOf() == 4){\n+                reshapedMask = mask->reshape(mask->ordering(), {mask->sizeAt(0), 1, mask->sizeAt(1), 1});\n+            }else{\n+                reshapedMask = mask->reshape(mask->ordering(), {mask->sizeAt(0), mask->sizeAt(1), 1});\n+            }\n+            *weights += (*reshapedMask - 1) * 1e9;",
        "comment_created_at": "2019-04-11T05:05:16+00:00",
        "comment_author": "treo",
        "comment_body": "The 1e9 is used by the tensor2tensor package as well. Bert on the other hand uses just 1e4, while GPT-2 uses 1e10.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "243132481",
    "pr_number": 6896,
    "pr_file": "libnd4j/include/ops/declarable/generic/helpers/impl/convolutions.cpp",
    "created_at": "2018-12-20T01:33:12+00:00",
    "commented_code": "}\n \n \n+#ifdef HAVE_MKLDNN\n+using namespace mkldnn;\n+\n+void ConvolutionUtils::getMKLDNNMemoryDescConv2d(\n+        int kH, int kW, int sH, int sW, int pH, int pW, int dH, int dW, bool isSameMode, bool isNCHW,\n+        int bS, int iC, int iH, int iW, int oC, int oH, int oW, const NDArray* src, const NDArray* diff_src,\n+        const NDArray* weights, const NDArray* diff_weights, const NDArray* bias, const NDArray* dst,\n+        mkldnn::memory::desc* conv_src_md, mkldnn::memory::desc* conv_diff_src_md, mkldnn::memory::desc* conv_weights_md,\n+        mkldnn::memory::desc* conv_diff_weights_md, mkldnn::memory::desc* conv_bias_md, mkldnn::memory::desc* conv_dst_md,\n+        mkldnn::memory::dims& conv_strides, mkldnn::memory::dims& conv_padding, mkldnn::memory::dims& conv_padding_r) {\n+    mkldnn::memory::dims conv_src_tz = { bS, iC, iH, iW };\n+    mkldnn::memory::dims conv_weights_tz = { oC, iC, kH, kW };\n+    mkldnn::memory::dims conv_bias_tz = { oC };\n+    mkldnn::memory::dims conv_dst_tz = { bS, oC, oH, oW };\n+\n+    conv_strides = { sH, sW };\n+    conv_padding = { pH, pW };\n+    conv_padding_r = { (oH - 1) * sH - iH + kH - pH,\n+                       (oW - 1) * sW - iW + kW - pW };",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "243132481",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 6896,
        "pr_file": "libnd4j/include/ops/declarable/generic/helpers/impl/convolutions.cpp",
        "discussion_id": "243132481",
        "commented_code": "@@ -415,9 +415,161 @@ else\n }\n \n \n+#ifdef HAVE_MKLDNN\n+using namespace mkldnn;\n+\n+void ConvolutionUtils::getMKLDNNMemoryDescConv2d(\n+        int kH, int kW, int sH, int sW, int pH, int pW, int dH, int dW, bool isSameMode, bool isNCHW,\n+        int bS, int iC, int iH, int iW, int oC, int oH, int oW, const NDArray* src, const NDArray* diff_src,\n+        const NDArray* weights, const NDArray* diff_weights, const NDArray* bias, const NDArray* dst,\n+        mkldnn::memory::desc* conv_src_md, mkldnn::memory::desc* conv_diff_src_md, mkldnn::memory::desc* conv_weights_md,\n+        mkldnn::memory::desc* conv_diff_weights_md, mkldnn::memory::desc* conv_bias_md, mkldnn::memory::desc* conv_dst_md,\n+        mkldnn::memory::dims& conv_strides, mkldnn::memory::dims& conv_padding, mkldnn::memory::dims& conv_padding_r) {\n+    mkldnn::memory::dims conv_src_tz = { bS, iC, iH, iW };\n+    mkldnn::memory::dims conv_weights_tz = { oC, iC, kH, kW };\n+    mkldnn::memory::dims conv_bias_tz = { oC };\n+    mkldnn::memory::dims conv_dst_tz = { bS, oC, oH, oW };\n+\n+    conv_strides = { sH, sW };\n+    conv_padding = { pH, pW };\n+    conv_padding_r = { (oH - 1) * sH - iH + kH - pH,\n+                       (oW - 1) * sW - iW + kW - pW };",
        "comment_created_at": "2018-12-20T01:33:12+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "conv_padding is 'truncate' mode padding values, and conv_padding_r is same mode padding, right?\r\nIf so, not sure on the -pH and -pW here... Usually with same mode, we ignore the ph and pW args and just calculate what we should actually use.\r\nhttps://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ConvolutionMode.java#L62-L63\r\n\r\nAlso present in a bunch of other places.",
        "pr_file_module": null
      },
      {
        "comment_id": "243137279",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 6896,
        "pr_file": "libnd4j/include/ops/declarable/generic/helpers/impl/convolutions.cpp",
        "discussion_id": "243132481",
        "commented_code": "@@ -415,9 +415,161 @@ else\n }\n \n \n+#ifdef HAVE_MKLDNN\n+using namespace mkldnn;\n+\n+void ConvolutionUtils::getMKLDNNMemoryDescConv2d(\n+        int kH, int kW, int sH, int sW, int pH, int pW, int dH, int dW, bool isSameMode, bool isNCHW,\n+        int bS, int iC, int iH, int iW, int oC, int oH, int oW, const NDArray* src, const NDArray* diff_src,\n+        const NDArray* weights, const NDArray* diff_weights, const NDArray* bias, const NDArray* dst,\n+        mkldnn::memory::desc* conv_src_md, mkldnn::memory::desc* conv_diff_src_md, mkldnn::memory::desc* conv_weights_md,\n+        mkldnn::memory::desc* conv_diff_weights_md, mkldnn::memory::desc* conv_bias_md, mkldnn::memory::desc* conv_dst_md,\n+        mkldnn::memory::dims& conv_strides, mkldnn::memory::dims& conv_padding, mkldnn::memory::dims& conv_padding_r) {\n+    mkldnn::memory::dims conv_src_tz = { bS, iC, iH, iW };\n+    mkldnn::memory::dims conv_weights_tz = { oC, iC, kH, kW };\n+    mkldnn::memory::dims conv_bias_tz = { oC };\n+    mkldnn::memory::dims conv_dst_tz = { bS, oC, oH, oW };\n+\n+    conv_strides = { sH, sW };\n+    conv_padding = { pH, pW };\n+    conv_padding_r = { (oH - 1) * sH - iH + kH - pH,\n+                       (oW - 1) * sW - iW + kW - pW };",
        "comment_created_at": "2018-12-20T02:07:29+00:00",
        "comment_author": "saudet",
        "comment_body": "No, conv_padding_r is the padding on the \"right\", but it needs to match that equation exactly or MKL-DNN will complain: https://github.com/intel/mkl-dnn/blob/master/src/common/convolution.cpp#L116\r\n",
        "pr_file_module": null
      }
    ]
  }
]