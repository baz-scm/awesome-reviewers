[
  {
    "discussion_id": "655038978",
    "pr_number": 9379,
    "pr_file": "libnd4j/include/ops/declarable/helpers/cuda/summaryStatReductions.cu",
    "created_at": "2021-06-21T02:39:35+00:00",
    "commented_code": "+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf (rauf@konduit.ai)\n+//\n+\n+#include <system/op_enums.h>\n+#include <ops/declarable/helpers/reductions.h>\n+#include <legacy/NativeOpExecutioner.h>\n+#include <helpers/ConstantTadHelper.h>\n+\n+namespace sd {\n+    namespace ops {\n+        namespace helpers {\n+\n+            //////////////////////////////////////////////////////////////////////////\n+            void  variance(const NDArray& input, NDArray& output, const std::vector<int>& dimensions, bool biasCorrected) {\n+                NDArray::prepareSpecialUse({ &output }, { &input });\n+                if (output.isScalar()) {\n+                    NativeOpExecutioner::execSummaryStatsScalar(LaunchContext::defaultContext(), variance::SummaryStatsVariance, input.buffer(), input.shapeInfo(), input.specialBuffer(), input.specialShapeInfo(), nullptr, output.buffer(), output.shapeInfo(), output.specialBuffer(), output.specialShapeInfo(), biasCorrected);\n+                }\n+                else {\n+                    auto tadPack = sd::ConstantTadHelper::getInstance().tadForDimensions(input.shapeInfo(), dimensions);\n+\n+                    NativeOpExecutioner::execSummaryStats(LaunchContext::defaultContext(), variance::SummaryStatsVariance,\n+                        input.buffer(), input.shapeInfo(), input.specialBuffer(), input.specialShapeInfo(),\n+                        nullptr, output.buffer(), output.shapeInfo(), output.specialBuffer(), output.specialShapeInfo(),\n+                        (int*) nullptr, dimensions.size(),\n+                        tadPack.specialShapeInfo(), tadPack.specialOffsets(), biasCorrected);\n+                }\n+\n+                NDArray::registerSpecialUse({ &output }, { &input });\n+            }\n+\n+            //////////////////////////////////////////////////////////////////////////\n+            void  standardDeviation(const NDArray& input, NDArray& output, const std::vector<int>& dimensions, bool biasCorrected) {\n+                NDArray::prepareSpecialUse({ &output }, { &input });",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "655038978",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9379,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cuda/summaryStatReductions.cu",
        "discussion_id": "655038978",
        "commented_code": "@@ -0,0 +1,73 @@\n+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf (rauf@konduit.ai)\n+//\n+\n+#include <system/op_enums.h>\n+#include <ops/declarable/helpers/reductions.h>\n+#include <legacy/NativeOpExecutioner.h>\n+#include <helpers/ConstantTadHelper.h>\n+\n+namespace sd {\n+    namespace ops {\n+        namespace helpers {\n+\n+            //////////////////////////////////////////////////////////////////////////\n+            void  variance(const NDArray& input, NDArray& output, const std::vector<int>& dimensions, bool biasCorrected) {\n+                NDArray::prepareSpecialUse({ &output }, { &input });\n+                if (output.isScalar()) {\n+                    NativeOpExecutioner::execSummaryStatsScalar(LaunchContext::defaultContext(), variance::SummaryStatsVariance, input.buffer(), input.shapeInfo(), input.specialBuffer(), input.specialShapeInfo(), nullptr, output.buffer(), output.shapeInfo(), output.specialBuffer(), output.specialShapeInfo(), biasCorrected);\n+                }\n+                else {\n+                    auto tadPack = sd::ConstantTadHelper::getInstance().tadForDimensions(input.shapeInfo(), dimensions);\n+\n+                    NativeOpExecutioner::execSummaryStats(LaunchContext::defaultContext(), variance::SummaryStatsVariance,\n+                        input.buffer(), input.shapeInfo(), input.specialBuffer(), input.specialShapeInfo(),\n+                        nullptr, output.buffer(), output.shapeInfo(), output.specialBuffer(), output.specialShapeInfo(),\n+                        (int*) nullptr, dimensions.size(),\n+                        tadPack.specialShapeInfo(), tadPack.specialOffsets(), biasCorrected);\n+                }\n+\n+                NDArray::registerSpecialUse({ &output }, { &input });\n+            }\n+\n+            //////////////////////////////////////////////////////////////////////////\n+            void  standardDeviation(const NDArray& input, NDArray& output, const std::vector<int>& dimensions, bool biasCorrected) {\n+                NDArray::prepareSpecialUse({ &output }, { &input });",
        "comment_created_at": "2021-06-21T02:39:35+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "Describing some of the steps here would also be useful. Eg: what's \"special use\" for?",
        "pr_file_module": null
      },
      {
        "comment_id": "656017684",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9379,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cuda/summaryStatReductions.cu",
        "discussion_id": "655038978",
        "commented_code": "@@ -0,0 +1,73 @@\n+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf (rauf@konduit.ai)\n+//\n+\n+#include <system/op_enums.h>\n+#include <ops/declarable/helpers/reductions.h>\n+#include <legacy/NativeOpExecutioner.h>\n+#include <helpers/ConstantTadHelper.h>\n+\n+namespace sd {\n+    namespace ops {\n+        namespace helpers {\n+\n+            //////////////////////////////////////////////////////////////////////////\n+            void  variance(const NDArray& input, NDArray& output, const std::vector<int>& dimensions, bool biasCorrected) {\n+                NDArray::prepareSpecialUse({ &output }, { &input });\n+                if (output.isScalar()) {\n+                    NativeOpExecutioner::execSummaryStatsScalar(LaunchContext::defaultContext(), variance::SummaryStatsVariance, input.buffer(), input.shapeInfo(), input.specialBuffer(), input.specialShapeInfo(), nullptr, output.buffer(), output.shapeInfo(), output.specialBuffer(), output.specialShapeInfo(), biasCorrected);\n+                }\n+                else {\n+                    auto tadPack = sd::ConstantTadHelper::getInstance().tadForDimensions(input.shapeInfo(), dimensions);\n+\n+                    NativeOpExecutioner::execSummaryStats(LaunchContext::defaultContext(), variance::SummaryStatsVariance,\n+                        input.buffer(), input.shapeInfo(), input.specialBuffer(), input.specialShapeInfo(),\n+                        nullptr, output.buffer(), output.shapeInfo(), output.specialBuffer(), output.specialShapeInfo(),\n+                        (int*) nullptr, dimensions.size(),\n+                        tadPack.specialShapeInfo(), tadPack.specialOffsets(), biasCorrected);\n+                }\n+\n+                NDArray::registerSpecialUse({ &output }, { &input });\n+            }\n+\n+            //////////////////////////////////////////////////////////////////////////\n+            void  standardDeviation(const NDArray& input, NDArray& output, const std::vector<int>& dimensions, bool biasCorrected) {\n+                NDArray::prepareSpecialUse({ &output }, { &input });",
        "comment_created_at": "2021-06-22T08:55:51+00:00",
        "comment_author": "quickwritereader",
        "comment_body": "I've used (copy-pasted) the legacy code there. for Cuda as it was already good. specialUse is intended for synching and proper use of NdArrays buffer.  host memory -> device memory  then  device memory -> host memory ",
        "pr_file_module": null
      },
      {
        "comment_id": "656055793",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9379,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cuda/summaryStatReductions.cu",
        "discussion_id": "655038978",
        "commented_code": "@@ -0,0 +1,73 @@\n+/* ******************************************************************************\n+ *\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ *  See the NOTICE file distributed with this work for additional\n+ *  information regarding copyright ownership.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+//\n+// @author AbdelRauf (rauf@konduit.ai)\n+//\n+\n+#include <system/op_enums.h>\n+#include <ops/declarable/helpers/reductions.h>\n+#include <legacy/NativeOpExecutioner.h>\n+#include <helpers/ConstantTadHelper.h>\n+\n+namespace sd {\n+    namespace ops {\n+        namespace helpers {\n+\n+            //////////////////////////////////////////////////////////////////////////\n+            void  variance(const NDArray& input, NDArray& output, const std::vector<int>& dimensions, bool biasCorrected) {\n+                NDArray::prepareSpecialUse({ &output }, { &input });\n+                if (output.isScalar()) {\n+                    NativeOpExecutioner::execSummaryStatsScalar(LaunchContext::defaultContext(), variance::SummaryStatsVariance, input.buffer(), input.shapeInfo(), input.specialBuffer(), input.specialShapeInfo(), nullptr, output.buffer(), output.shapeInfo(), output.specialBuffer(), output.specialShapeInfo(), biasCorrected);\n+                }\n+                else {\n+                    auto tadPack = sd::ConstantTadHelper::getInstance().tadForDimensions(input.shapeInfo(), dimensions);\n+\n+                    NativeOpExecutioner::execSummaryStats(LaunchContext::defaultContext(), variance::SummaryStatsVariance,\n+                        input.buffer(), input.shapeInfo(), input.specialBuffer(), input.specialShapeInfo(),\n+                        nullptr, output.buffer(), output.shapeInfo(), output.specialBuffer(), output.specialShapeInfo(),\n+                        (int*) nullptr, dimensions.size(),\n+                        tadPack.specialShapeInfo(), tadPack.specialOffsets(), biasCorrected);\n+                }\n+\n+                NDArray::registerSpecialUse({ &output }, { &input });\n+            }\n+\n+            //////////////////////////////////////////////////////////////////////////\n+            void  standardDeviation(const NDArray& input, NDArray& output, const std::vector<int>& dimensions, bool biasCorrected) {\n+                NDArray::prepareSpecialUse({ &output }, { &input });",
        "comment_created_at": "2021-06-22T09:44:15+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "@quickwritereader same thing: put it in the code, not on github",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "628228306",
    "pr_number": 9295,
    "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/maxpool2d.cu",
    "created_at": "2021-05-07T13:53:18+00:00",
    "commented_code": "const auto goodType = input->dataType() == DataType::DOUBLE || input->dataType() == DataType::FLOAT32 || input->dataType() == DataType::HALF || input->dataType() == DataType::INT32;\n \n-    return goodType && (input->dataType() == gradO->dataType())\n-                    && (input->dataType() == gradI->dataType())\n-                    && shape::haveSameShapeAndStrides(input->shapeInfo(), gradI->shapeInfo());\n+    return  goodType && (input->dataType() == gradO->dataType())",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "628228306",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9295,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/maxpool2d.cu",
        "discussion_id": "628228306",
        "commented_code": "@@ -123,9 +136,12 @@ PLATFORM_CHECK(maxpool2d_bp, ENGINE_CUDA) {\n \n     const auto goodType = input->dataType() == DataType::DOUBLE || input->dataType() == DataType::FLOAT32 || input->dataType() == DataType::HALF || input->dataType() == DataType::INT32;\n \n-    return goodType && (input->dataType() == gradO->dataType())\n-                    && (input->dataType() == gradI->dataType())\n-                    && shape::haveSameShapeAndStrides(input->shapeInfo(), gradI->shapeInfo());\n+    return  goodType && (input->dataType() == gradO->dataType())",
        "comment_created_at": "2021-05-07T13:53:18+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "Mind adding a minor comment paragraph explaining the issues found here so we know why we're defaulting to our implementation in certain cases?",
        "pr_file_module": null
      },
      {
        "comment_id": "628279917",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9295,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/maxpool2d.cu",
        "discussion_id": "628228306",
        "commented_code": "@@ -123,9 +136,12 @@ PLATFORM_CHECK(maxpool2d_bp, ENGINE_CUDA) {\n \n     const auto goodType = input->dataType() == DataType::DOUBLE || input->dataType() == DataType::FLOAT32 || input->dataType() == DataType::HALF || input->dataType() == DataType::INT32;\n \n-    return goodType && (input->dataType() == gradO->dataType())\n-                    && (input->dataType() == gradI->dataType())\n-                    && shape::haveSameShapeAndStrides(input->shapeInfo(), gradI->shapeInfo());\n+    return  goodType && (input->dataType() == gradO->dataType())",
        "comment_created_at": "2021-05-07T15:01:21+00:00",
        "comment_author": "quickwritereader",
        "comment_body": "a short comment was added. but more detailed information is written in the commit message. it's usable in Github blames\r\n",
        "pr_file_module": null
      }
    ]
  }
]