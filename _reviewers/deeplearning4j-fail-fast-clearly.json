[
  {
    "discussion_id": "172056227",
    "pr_number": 4629,
    "pr_file": "deeplearning4j-manifold/deeplearning4j-largevis/src/main/java/org/deeplearning4j/largevis/LargeVis.java",
    "created_at": "2018-03-04T17:14:20+00:00",
    "commented_code": "+package org.deeplearning4j.largevis;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.commons.lang3.time.StopWatch;\n+import org.deeplearning4j.clustering.randomprojection.RPForest;\n+import org.deeplearning4j.clustering.randomprojection.RPUtils;\n+import org.deeplearning4j.nn.conf.GradientNormalization;\n+import org.deeplearning4j.nn.conf.WorkspaceMode;\n+import org.deeplearning4j.nndescent.NNDescent;\n+import org.nd4j.linalg.api.memory.MemoryWorkspace;\n+import org.nd4j.linalg.api.memory.conf.WorkspaceConfiguration;\n+import org.nd4j.linalg.api.memory.enums.*;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.api.ops.CustomOp;\n+import org.nd4j.linalg.api.ops.DynamicCustomOp;\n+import org.nd4j.linalg.api.ops.impl.accum.Norm2;\n+import org.nd4j.linalg.api.ops.impl.transforms.arithmetic.OldSubOp;\n+import org.nd4j.linalg.api.ops.impl.transforms.clip.ClipByValue;\n+import org.nd4j.linalg.dataset.DataSet;\n+import org.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize;\n+import org.nd4j.linalg.factory.Nd4j;\n+import org.nd4j.linalg.learning.config.IUpdater;\n+import org.nd4j.linalg.learning.config.Sgd;\n+import org.nd4j.linalg.memory.abstracts.DummyWorkspace;\n+import org.nd4j.linalg.primitives.Counter;\n+import org.nd4j.linalg.primitives.Pair;\n+import org.nd4j.linalg.util.MathUtils;\n+import org.nd4j.list.FloatNDArrayList;\n+import org.nd4j.list.IntNDArrayList;\n+import org.nd4j.list.matrix.IntMatrixNDArrayList;\n+import org.nd4j.weightinit.WeightInitScheme;\n+import org.nd4j.weightinit.impl.XavierFanInInitScheme;\n+\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.PriorityQueue;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.LockSupport;\n+\n+\n+/**\n+ * A port of the LargeVis algorithm:\n+ * https://github.com/lferry007/LargeVis\n+ *\n+ * to nd4j. This implementation uses\n+ * RPTrees rather than annoy as in the original implementation.\n+ *\n+ *\n+ * This algorithm also uses the nd4j updaters (to allow for more flexibility)\n+ * over static gradient clipping and the simpler learning rate schedule.\n+ *\n+ *\n+ * The algorithm has the following parameters:\n+ *      -fea: specify whether the input file is high-dimensional feature vectors (1) or networks (0). Default is 1.\n+ *      -input: Input file of feature vectors or networks\n+ *      -output: Output file of low-dimensional representations.\n+ *      -threads: Number of threads. Default is 8.\n+ *      -outdim: The lower dimensionality LargesVis learns for visualization (usually 2 or 3). Default is 2.\n+ *      -samples: Number of edge samples for graph layout (in millions). Default is set to data size / 100 (million).\n+ *      -prop: Number of times for neighbor propagations in the state of K      -NNG construction, usually less than 3. Default is 3.\n+ *      -alpha: Initial learning rate. Default is 1.0.\n+ *      -trees: Number of random-projection trees used for constructing K-NNG. 50 is sufficient for most cases.\n+ *      -neg: Number of negative samples used for negative sampling. Default is 5.\n+ *      -neigh: Number of neighbors (K) in K-NNG, which is usually set as three times of perplexity. Default is 150.\n+ *      -gamma: The weights assigned to negative edges. Default is 7.\n+ *      -perp: The perplexity used for deciding edge weights in K-NNG. Default is 50.\n+ *\n+ * @author Adam Gibson\n+ */\n+@Data\n+@Slf4j\n+public class LargeVis {\n+\n+\n+    private NNDescent nnDescent;\n+    @Builder.Default\n+    private int numWorkers = Runtime.getRuntime().availableProcessors();\n+    //vec.rows -> nVertices\n+    private INDArray vec,vis,prob;\n+    @Builder.Default\n+    private IUpdater updater = new Sgd(0.01);\n+    private WeightInitScheme weightInitScheme;\n+    private ThreadLocal<INDArray> scalars = new ThreadLocal<>();\n+    @Builder.Default\n+    private WorkspaceMode workspaceMode = WorkspaceMode.SINGLE;\n+    private ThreadLocal<MemoryWorkspace>  workspaceThread = new ThreadLocal<>();\n+    private WorkspaceConfiguration workspaceConfiguration;\n+\n+    @Builder.Default\n+    private String distanceFunction = \"euclidean\";\n+    private int nEdges;\n+    @Builder.Default\n+    private IntNDArrayList reverse = new IntNDArrayList();\n+    private ExecutorService threadExec;\n+    @Builder.Default\n+    private int outDim = 2;\n+    @Builder.Default\n+    private double initialAlpha = 1.0;\n+    @Builder.Default\n+    private int nNegatives = 5;\n+    @Builder.Default\n+    private double gamma = 7.0;\n+    @Builder.Default\n+    private double perplexity = 50.0;\n+    @Builder.Default\n+    private long seed = 42;\n+    @Builder.Default\n+    private Boolean normalize;\n+    private int negSize = (int) 1e8;\n+    @Builder.Default\n+    private double gradClipValue = 5.0;\n+    @Builder.Default\n+    private GradientNormalization gradientNormalization = GradientNormalization.ClipElementWiseAbsoluteValue;\n+    private AtomicInteger edgeCountActual = new AtomicInteger(0);\n+    private  MemoryWorkspace workspace;\n+    private AtomicInteger updateCount = new AtomicInteger(0);\n+    private AtomicInteger epochCount = new AtomicInteger(0);\n+    private IntNDArrayList edgeFrom = new IntNDArrayList();\n+    private IntNDArrayList edgeTo = new IntNDArrayList();\n+    private ExecutorService executorService;\n+    protected final AtomicInteger workerCounter = new AtomicInteger(0);\n+\n+    @Builder.Default\n+    private Boolean sample = true;\n+\n+    private ThreadLocal<INDArray> errors = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> grads = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> gradsFirstRow = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> gradsSecondRow = new ThreadLocal<>();\n+\n+\n+    private ThreadLocal<Norm2> norm2 = new ThreadLocal<>();\n+    private ThreadLocal<ClipByValue> clip = new ThreadLocal<>();\n+    private ThreadLocal<OldSubOp> visXMinusVisY = new ThreadLocal<>();\n+    private ThreadLocal<OldSubOp> visYMinusVisX = new ThreadLocal<>();\n+    // log uncaught exceptions\n+    Thread.UncaughtExceptionHandler handler = new Thread.UncaughtExceptionHandler() {\n+        public void uncaughtException(Thread th, Throwable ex) {\n+            log.error(\"Uncaught exception: \" + ex);\n+            ex.printStackTrace();\n+        }\n+    };\n+\n+\n+\n+\n+\n+    @Builder\n+    public LargeVis(INDArray vec,\n+                    int maxSize,\n+                    String distanceFunction,\n+                    int numTrees,\n+                    int outDims,\n+                    int nNegatives,\n+                    double gamma,\n+                    double initialAlpha,\n+                    double perplexity,\n+                    int nPropagations,\n+                    long seed,\n+                    int nNeighbors,\n+                    Boolean normalize,\n+                    int iterationCount,\n+                    IUpdater updater,\n+                    int nTrees,\n+                    WeightInitScheme weightInitScheme,\n+                    GradientNormalization gradientNormalization,\n+                    double gradClipValue,\n+                    WorkspaceMode workspaceMode,\n+                    int numWorkers,\n+                    int nSamples,\n+                    Boolean sample) {\n+\n+\n+\n+        if(workspaceMode != null) {\n+            this.workspaceMode = workspaceMode;\n+        }\n+\n+        if(numWorkers > 0) {\n+            this.numWorkers = numWorkers;\n+        }\n+\n+        if(workspaceMode != WorkspaceMode.NONE)\n+            workspaceConfiguration = WorkspaceConfiguration.builder().cyclesBeforeInitialization(1)\n+                    .policyAllocation(AllocationPolicy.STRICT).policyLearning(LearningPolicy.FIRST_LOOP)\n+                    .policyMirroring(MirroringPolicy.FULL).policyReset(ResetPolicy.BLOCK_LEFT)\n+                    .policySpill(SpillPolicy.REALLOCATE).build();\n+\n+\n+        if(sample != null) {\n+            this.sample = sample;\n+        }\n+\n+        if(gradientNormalization != null) {\n+            this.gradientNormalization = gradientNormalization;\n+        }\n+\n+        if(gradClipValue > 0) {\n+            this.gradClipValue = gradClipValue;\n+        }\n+\n+\n+\n+        if(normalize != null) {\n+            this.normalize = normalize;\n+        }\n+\n+        if(updater != null)\n+            this.updater = updater;\n+        this.normalize = normalize;\n+        this.vec = vec;\n+\n+        if(weightInitScheme != null) {\n+            this.weightInitScheme = weightInitScheme;\n+        }\n+\n+\n+        if(distanceFunction != null)\n+            this.distanceFunction = distanceFunction;\n+        if(outDims > 0)\n+            this.outDim = outDims;\n+        if(initialAlpha > 0)\n+            this.initialAlpha = initialAlpha;\n+\n+        if(nNegatives > 0)\n+            this.nNegatives = nNegatives;\n+        if(gamma > 0)",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "172056227",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 4629,
        "pr_file": "deeplearning4j-manifold/deeplearning4j-largevis/src/main/java/org/deeplearning4j/largevis/LargeVis.java",
        "discussion_id": "172056227",
        "commented_code": "@@ -0,0 +1,713 @@\n+package org.deeplearning4j.largevis;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.commons.lang3.time.StopWatch;\n+import org.deeplearning4j.clustering.randomprojection.RPForest;\n+import org.deeplearning4j.clustering.randomprojection.RPUtils;\n+import org.deeplearning4j.nn.conf.GradientNormalization;\n+import org.deeplearning4j.nn.conf.WorkspaceMode;\n+import org.deeplearning4j.nndescent.NNDescent;\n+import org.nd4j.linalg.api.memory.MemoryWorkspace;\n+import org.nd4j.linalg.api.memory.conf.WorkspaceConfiguration;\n+import org.nd4j.linalg.api.memory.enums.*;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.api.ops.CustomOp;\n+import org.nd4j.linalg.api.ops.DynamicCustomOp;\n+import org.nd4j.linalg.api.ops.impl.accum.Norm2;\n+import org.nd4j.linalg.api.ops.impl.transforms.arithmetic.OldSubOp;\n+import org.nd4j.linalg.api.ops.impl.transforms.clip.ClipByValue;\n+import org.nd4j.linalg.dataset.DataSet;\n+import org.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize;\n+import org.nd4j.linalg.factory.Nd4j;\n+import org.nd4j.linalg.learning.config.IUpdater;\n+import org.nd4j.linalg.learning.config.Sgd;\n+import org.nd4j.linalg.memory.abstracts.DummyWorkspace;\n+import org.nd4j.linalg.primitives.Counter;\n+import org.nd4j.linalg.primitives.Pair;\n+import org.nd4j.linalg.util.MathUtils;\n+import org.nd4j.list.FloatNDArrayList;\n+import org.nd4j.list.IntNDArrayList;\n+import org.nd4j.list.matrix.IntMatrixNDArrayList;\n+import org.nd4j.weightinit.WeightInitScheme;\n+import org.nd4j.weightinit.impl.XavierFanInInitScheme;\n+\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.PriorityQueue;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.locks.LockSupport;\n+\n+\n+/**\n+ * A port of the LargeVis algorithm:\n+ * https://github.com/lferry007/LargeVis\n+ *\n+ * to nd4j. This implementation uses\n+ * RPTrees rather than annoy as in the original implementation.\n+ *\n+ *\n+ * This algorithm also uses the nd4j updaters (to allow for more flexibility)\n+ * over static gradient clipping and the simpler learning rate schedule.\n+ *\n+ *\n+ * The algorithm has the following parameters:\n+ *      -fea: specify whether the input file is high-dimensional feature vectors (1) or networks (0). Default is 1.\n+ *      -input: Input file of feature vectors or networks\n+ *      -output: Output file of low-dimensional representations.\n+ *      -threads: Number of threads. Default is 8.\n+ *      -outdim: The lower dimensionality LargesVis learns for visualization (usually 2 or 3). Default is 2.\n+ *      -samples: Number of edge samples for graph layout (in millions). Default is set to data size / 100 (million).\n+ *      -prop: Number of times for neighbor propagations in the state of K      -NNG construction, usually less than 3. Default is 3.\n+ *      -alpha: Initial learning rate. Default is 1.0.\n+ *      -trees: Number of random-projection trees used for constructing K-NNG. 50 is sufficient for most cases.\n+ *      -neg: Number of negative samples used for negative sampling. Default is 5.\n+ *      -neigh: Number of neighbors (K) in K-NNG, which is usually set as three times of perplexity. Default is 150.\n+ *      -gamma: The weights assigned to negative edges. Default is 7.\n+ *      -perp: The perplexity used for deciding edge weights in K-NNG. Default is 50.\n+ *\n+ * @author Adam Gibson\n+ */\n+@Data\n+@Slf4j\n+public class LargeVis {\n+\n+\n+    private NNDescent nnDescent;\n+    @Builder.Default\n+    private int numWorkers = Runtime.getRuntime().availableProcessors();\n+    //vec.rows -> nVertices\n+    private INDArray vec,vis,prob;\n+    @Builder.Default\n+    private IUpdater updater = new Sgd(0.01);\n+    private WeightInitScheme weightInitScheme;\n+    private ThreadLocal<INDArray> scalars = new ThreadLocal<>();\n+    @Builder.Default\n+    private WorkspaceMode workspaceMode = WorkspaceMode.SINGLE;\n+    private ThreadLocal<MemoryWorkspace>  workspaceThread = new ThreadLocal<>();\n+    private WorkspaceConfiguration workspaceConfiguration;\n+\n+    @Builder.Default\n+    private String distanceFunction = \"euclidean\";\n+    private int nEdges;\n+    @Builder.Default\n+    private IntNDArrayList reverse = new IntNDArrayList();\n+    private ExecutorService threadExec;\n+    @Builder.Default\n+    private int outDim = 2;\n+    @Builder.Default\n+    private double initialAlpha = 1.0;\n+    @Builder.Default\n+    private int nNegatives = 5;\n+    @Builder.Default\n+    private double gamma = 7.0;\n+    @Builder.Default\n+    private double perplexity = 50.0;\n+    @Builder.Default\n+    private long seed = 42;\n+    @Builder.Default\n+    private Boolean normalize;\n+    private int negSize = (int) 1e8;\n+    @Builder.Default\n+    private double gradClipValue = 5.0;\n+    @Builder.Default\n+    private GradientNormalization gradientNormalization = GradientNormalization.ClipElementWiseAbsoluteValue;\n+    private AtomicInteger edgeCountActual = new AtomicInteger(0);\n+    private  MemoryWorkspace workspace;\n+    private AtomicInteger updateCount = new AtomicInteger(0);\n+    private AtomicInteger epochCount = new AtomicInteger(0);\n+    private IntNDArrayList edgeFrom = new IntNDArrayList();\n+    private IntNDArrayList edgeTo = new IntNDArrayList();\n+    private ExecutorService executorService;\n+    protected final AtomicInteger workerCounter = new AtomicInteger(0);\n+\n+    @Builder.Default\n+    private Boolean sample = true;\n+\n+    private ThreadLocal<INDArray> errors = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> grads = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> gradsFirstRow = new ThreadLocal<>();\n+    private ThreadLocal<INDArray> gradsSecondRow = new ThreadLocal<>();\n+\n+\n+    private ThreadLocal<Norm2> norm2 = new ThreadLocal<>();\n+    private ThreadLocal<ClipByValue> clip = new ThreadLocal<>();\n+    private ThreadLocal<OldSubOp> visXMinusVisY = new ThreadLocal<>();\n+    private ThreadLocal<OldSubOp> visYMinusVisX = new ThreadLocal<>();\n+    // log uncaught exceptions\n+    Thread.UncaughtExceptionHandler handler = new Thread.UncaughtExceptionHandler() {\n+        public void uncaughtException(Thread th, Throwable ex) {\n+            log.error(\"Uncaught exception: \" + ex);\n+            ex.printStackTrace();\n+        }\n+    };\n+\n+\n+\n+\n+\n+    @Builder\n+    public LargeVis(INDArray vec,\n+                    int maxSize,\n+                    String distanceFunction,\n+                    int numTrees,\n+                    int outDims,\n+                    int nNegatives,\n+                    double gamma,\n+                    double initialAlpha,\n+                    double perplexity,\n+                    int nPropagations,\n+                    long seed,\n+                    int nNeighbors,\n+                    Boolean normalize,\n+                    int iterationCount,\n+                    IUpdater updater,\n+                    int nTrees,\n+                    WeightInitScheme weightInitScheme,\n+                    GradientNormalization gradientNormalization,\n+                    double gradClipValue,\n+                    WorkspaceMode workspaceMode,\n+                    int numWorkers,\n+                    int nSamples,\n+                    Boolean sample) {\n+\n+\n+\n+        if(workspaceMode != null) {\n+            this.workspaceMode = workspaceMode;\n+        }\n+\n+        if(numWorkers > 0) {\n+            this.numWorkers = numWorkers;\n+        }\n+\n+        if(workspaceMode != WorkspaceMode.NONE)\n+            workspaceConfiguration = WorkspaceConfiguration.builder().cyclesBeforeInitialization(1)\n+                    .policyAllocation(AllocationPolicy.STRICT).policyLearning(LearningPolicy.FIRST_LOOP)\n+                    .policyMirroring(MirroringPolicy.FULL).policyReset(ResetPolicy.BLOCK_LEFT)\n+                    .policySpill(SpillPolicy.REALLOCATE).build();\n+\n+\n+        if(sample != null) {\n+            this.sample = sample;\n+        }\n+\n+        if(gradientNormalization != null) {\n+            this.gradientNormalization = gradientNormalization;\n+        }\n+\n+        if(gradClipValue > 0) {\n+            this.gradClipValue = gradClipValue;\n+        }\n+\n+\n+\n+        if(normalize != null) {\n+            this.normalize = normalize;\n+        }\n+\n+        if(updater != null)\n+            this.updater = updater;\n+        this.normalize = normalize;\n+        this.vec = vec;\n+\n+        if(weightInitScheme != null) {\n+            this.weightInitScheme = weightInitScheme;\n+        }\n+\n+\n+        if(distanceFunction != null)\n+            this.distanceFunction = distanceFunction;\n+        if(outDims > 0)\n+            this.outDim = outDims;\n+        if(initialAlpha > 0)\n+            this.initialAlpha = initialAlpha;\n+\n+        if(nNegatives > 0)\n+            this.nNegatives = nNegatives;\n+        if(gamma > 0)",
        "comment_created_at": "2018-03-04T17:14:20+00:00",
        "comment_author": "DzianisH",
        "comment_body": "If someone will try to set `gamma` with negative value (e.g. -42) he will not face any error/warning/exception so he would think that `gamma` actually gets into negative. So i would propose to either throw exception if user tries to set invalid value or at least log error/warning message.\r\nPlease, look through the same issues in the code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "195884216",
    "pr_number": 5603,
    "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/recurrent/GravesLSTM.java",
    "created_at": "2018-06-15T23:40:04+00:00",
    "commented_code": "private FwdPassReturn activateHelper(final boolean training, final INDArray prevOutputActivations,\n                     final INDArray prevMemCellState, boolean forBackprop, LayerWorkspaceMgr workspaceMgr) {\n         assertInputSet(false);\n+        Preconditions.checkState(input.shape().length == 3,\n+                \"3D input expected to RNN layer expected, got \" + input.shape().length);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "195884216",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5603,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/recurrent/GravesLSTM.java",
        "discussion_id": "195884216",
        "commented_code": "@@ -121,6 +122,8 @@ public INDArray activate(boolean training, LayerWorkspaceMgr workspaceMgr) {\n     private FwdPassReturn activateHelper(final boolean training, final INDArray prevOutputActivations,\n                     final INDArray prevMemCellState, boolean forBackprop, LayerWorkspaceMgr workspaceMgr) {\n         assertInputSet(false);\n+        Preconditions.checkState(input.shape().length == 3,\n+                \"3D input expected to RNN layer expected, got \" + input.shape().length);",
        "comment_created_at": "2018-06-15T23:40:04+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Good check, but this can be optimized (and is not the correct way to use preconditions). This always results in 3 objects being created (2xlong[] shape, and a String) regardless of whether an error is encountered. Instead, use this (no object creation unless an error is encountered)\r\n```\r\nPreconditions.checkState(input.rank() == 3, \"3D ... got %s\", input.rank())\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "214573944",
    "pr_number": 6115,
    "pr_file": "deeplearning4j/deeplearning4j-scaleout/deeplearning4j-scaleout-parallelwrapper/src/main/java/org/deeplearning4j/parallelism/ParallelWrapper.java",
    "created_at": "2018-09-03T04:31:27+00:00",
    "commented_code": "log.info(\"Starting ParallelWrapper training round...\");\n         long intcnt = 0;\n         while (iterator.hasNext() && !stopFit.get()) {\n+            if (modelParamsSupplier != null) {\n+                val params = modelParamsSupplier.get();\n+                if (params != null) {\n+                    // TODO: We should propagate params across the workers",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "214573944",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 6115,
        "pr_file": "deeplearning4j/deeplearning4j-scaleout/deeplearning4j-scaleout-parallelwrapper/src/main/java/org/deeplearning4j/parallelism/ParallelWrapper.java",
        "discussion_id": "214573944",
        "commented_code": "@@ -520,6 +539,20 @@ public synchronized void fit(@NonNull DataSetIterator source) {\n         log.info(\"Starting ParallelWrapper training round...\");\n         long intcnt = 0;\n         while (iterator.hasNext() && !stopFit.get()) {\n+            if (modelParamsSupplier != null) {\n+                val params = modelParamsSupplier.get();\n+                if (params != null) {\n+                    // TODO: We should propagate params across the workers",
        "comment_created_at": "2018-09-03T04:31:27+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Unimplemented? Get the params then don't do anything with them...\r\nIf planned to be added later, maybe add UnsupportedOperationException?\r\n(Better to get exception than obscure issues or silent failure)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "203337614",
    "pr_number": 5922,
    "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/factory/Nd4j.java",
    "created_at": "2018-07-18T11:07:13+00:00",
    "commented_code": "long[] indexes = new long[input.rank() + 1];\n         for (int i = 0; i < indexes.length; i++)\n             indexes[i] = i < dimension ? shape[i] : i == dimension ? 1 : shape[i - 1];\n-        return input.reshape(input.ordering(), indexes);\n+        return input.reshape(indexes);\n     }\n \n+    /**\n+     * Squeeze : removes a dimension of size 1\n+     * @param input the input array\n+     * @param dimension the dimension to remove\n+     * @return the array with dimension removed\n+     */\n+    public static INDArray squeeze(INDArray input, int dimension) {\n+        if (dimension < 0){\n+            dimension += input.rank();\n+        }\n+        long[] shape = input.shape();\n+        if (shape[dimension] != 1){\n+            throw new ND4JIllegalStateException(\"Can squeeze only dimensions of size 1.\");",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "203337614",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 5922,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/factory/Nd4j.java",
        "discussion_id": "203337614",
        "commented_code": "@@ -406,9 +406,34 @@ public static INDArray expandDims(INDArray input, int dimension) {\n         long[] indexes = new long[input.rank() + 1];\n         for (int i = 0; i < indexes.length; i++)\n             indexes[i] = i < dimension ? shape[i] : i == dimension ? 1 : shape[i - 1];\n-        return input.reshape(input.ordering(), indexes);\n+        return input.reshape(indexes);\n     }\n \n+    /**\n+     * Squeeze : removes a dimension of size 1\n+     * @param input the input array\n+     * @param dimension the dimension to remove\n+     * @return the array with dimension removed\n+     */\n+    public static INDArray squeeze(INDArray input, int dimension) {\n+        if (dimension < 0){\n+            dimension += input.rank();\n+        }\n+        long[] shape = input.shape();\n+        if (shape[dimension] != 1){\n+            throw new ND4JIllegalStateException(\"Can squeeze only dimensions of size 1.\");",
        "comment_created_at": "2018-07-18T11:07:13+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Couple of things here:\r\n(a) we can use Preconditions.checkArgument/checkState - less verbose\r\n(b) When throwing exceptions, I think it's good practice to include useful information. What dimension? What's the shape of the array? Without that, I need a debugger to get that information, which adds a lot of time required to fix it...",
        "pr_file_module": null
      }
    ]
  }
]