[
  {
    "discussion_id": "379996124",
    "pr_number": 8713,
    "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/lossfunctions/SameDiffLoss.java",
    "created_at": "2020-02-17T05:55:04+00:00",
    "commented_code": "+package org.nd4j.linalg.lossfunctions;\n+\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.base.Preconditions;\n+import org.nd4j.linalg.activations.IActivation;\n+import org.nd4j.linalg.api.buffer.DataType;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.primitives.Pair;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+\n+public abstract class SameDiffLoss implements ILossFunction {\n+\n+    SameDiff sd = SameDiff.create();\n+\n+\n+    protected SameDiffLoss() {\n+\n+\n+        SDVariable layerInput =  sd.placeHolder(\"layerInput\", DataType.FLOAT ,-1);\n+        SDVariable labels = sd.placeHolder(\"labels\", DataType.FLOAT ,-1);\n+        this.defineLoss(sd, layerInput, labels);\n+\n+\n+    }\n+\n+\n+    public abstract SDVariable defineLoss(SameDiff sd, SDVariable layerInput, SDVariable labels);\n+\n+    /**\n+     * Compute the score (loss function value) for the given inputs.\n+     * @param labels       Label/expected preOutput\n+     * @param preOutput    Output of the model (neural network)\n+     * @param activationFn Activation function that should be applied to preOutput\n+     * @param mask         Mask array; may be null\n+     * @param average      Whether the score should be averaged (divided by number of rows in labels/preOutput) or not   @return Loss function value\n+     */\n+    public double computeScore(INDArray labels, INDArray preOutput, IActivation activationFn, INDArray mask, boolean average) {\n+\n+            // The score overall consists of the\n+            // sum of the negative log likelihoods for each\n+            // of the individual labels.\n+\n+\n+\n+        INDArray scoreArr = computeScoreArray(labels, preOutput, activationFn, mask);\n+\n+        double score = scoreArr.sumNumber().doubleValue();\n+            if (average) {\n+                score /= scoreArr.size(0);\n+            }\n+            return score;\n+        }\n+\n+\n+\n+\n+\n+\n+    /**\n+     * Compute the score (loss function value) for each example individually.\n+     * For input [numExamples,nOut] returns scores as a column vector: [numExamples,1]\n+     * @param labels       Labels/expected output\n+     * @param preOutput    Output of the model (neural network)\n+     * @param activationFn Activation function that should be applied to preOutput\n+     * @param mask         @return Loss function value for each example; column vector\n+     */\n+    public INDArray computeScoreArray(INDArray labels, INDArray preOutput, IActivation activationFn, INDArray mask){\n+\n+        Preconditions.checkArgument((labels.size(1) != preOutput.size(1)),\"Labels array numColumns (size(1) = \" + labels.size(1) + \") does not match output layer\n\" +\n+                \"number of outputs (nOut = \" + preOutput.size(1) + \")\");",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "379996124",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 8713,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/lossfunctions/SameDiffLoss.java",
        "discussion_id": "379996124",
        "commented_code": "@@ -0,0 +1,146 @@\n+package org.nd4j.linalg.lossfunctions;\n+\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.base.Preconditions;\n+import org.nd4j.linalg.activations.IActivation;\n+import org.nd4j.linalg.api.buffer.DataType;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.primitives.Pair;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+\n+public abstract class SameDiffLoss implements ILossFunction {\n+\n+    SameDiff sd = SameDiff.create();\n+\n+\n+    protected SameDiffLoss() {\n+\n+\n+        SDVariable layerInput =  sd.placeHolder(\"layerInput\", DataType.FLOAT ,-1);\n+        SDVariable labels = sd.placeHolder(\"labels\", DataType.FLOAT ,-1);\n+        this.defineLoss(sd, layerInput, labels);\n+\n+\n+    }\n+\n+\n+    public abstract SDVariable defineLoss(SameDiff sd, SDVariable layerInput, SDVariable labels);\n+\n+    /**\n+     * Compute the score (loss function value) for the given inputs.\n+     * @param labels       Label/expected preOutput\n+     * @param preOutput    Output of the model (neural network)\n+     * @param activationFn Activation function that should be applied to preOutput\n+     * @param mask         Mask array; may be null\n+     * @param average      Whether the score should be averaged (divided by number of rows in labels/preOutput) or not   @return Loss function value\n+     */\n+    public double computeScore(INDArray labels, INDArray preOutput, IActivation activationFn, INDArray mask, boolean average) {\n+\n+            // The score overall consists of the\n+            // sum of the negative log likelihoods for each\n+            // of the individual labels.\n+\n+\n+\n+        INDArray scoreArr = computeScoreArray(labels, preOutput, activationFn, mask);\n+\n+        double score = scoreArr.sumNumber().doubleValue();\n+            if (average) {\n+                score /= scoreArr.size(0);\n+            }\n+            return score;\n+        }\n+\n+\n+\n+\n+\n+\n+    /**\n+     * Compute the score (loss function value) for each example individually.\n+     * For input [numExamples,nOut] returns scores as a column vector: [numExamples,1]\n+     * @param labels       Labels/expected output\n+     * @param preOutput    Output of the model (neural network)\n+     * @param activationFn Activation function that should be applied to preOutput\n+     * @param mask         @return Loss function value for each example; column vector\n+     */\n+    public INDArray computeScoreArray(INDArray labels, INDArray preOutput, IActivation activationFn, INDArray mask){\n+\n+        Preconditions.checkArgument((labels.size(1) != preOutput.size(1)),\"Labels array numColumns (size(1) = \" + labels.size(1) + \") does not match output layer\\n\" +\n+                \"number of outputs (nOut = \" + preOutput.size(1) + \")\");",
        "comment_created_at": "2020-02-17T05:55:04+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "When using the Preconditions class, do this instead:\r\nThis avoids object creation/garbage unless an error is actually thrown.\r\n```\r\nPreconditions.checkArgument((labels.size(1) != preOutput.size(1)),\"Labels array numColumns (size(1) = %s) does not match output layer number of outputs (nOut = %s)\",  labels.size(1), preOutput.size(1));\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "272029128",
    "pr_number": 7391,
    "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/CapsuleLayer.java",
    "created_at": "2019-04-04T06:13:15+00:00",
    "commented_code": "+/*******************************************************************************\n+ * Copyright (c) 2015-2018 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+package org.deeplearning4j.nn.conf.layers;\n+\n+import java.util.Map;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NoArgsConstructor;\n+import lombok.Setter;\n+import org.deeplearning4j.nn.conf.inputs.InputType;\n+import org.deeplearning4j.nn.conf.inputs.InputType.InputTypeRecurrent;\n+import org.deeplearning4j.nn.conf.inputs.InputType.Type;\n+import org.deeplearning4j.nn.conf.layers.samediff.SDLayerParams;\n+import org.deeplearning4j.nn.conf.layers.samediff.SameDiffLayer;\n+import org.deeplearning4j.nn.weights.WeightInitUtil;\n+import org.deeplearning4j.util.CapsuleUtils;\n+import org.deeplearning4j.util.ValidationUtils;\n+import org.nd4j.autodiff.samediff.SDIndex;\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.linalg.api.memory.MemoryWorkspace;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.factory.Nd4j;\n+\n+/**\n+ * An implementation of the DigiCaps layer from Dynamic Routing Between Capsules\n+ *\n+ * Input should come from a PrimaryCapsules layer and be of shape [mb, inputCaps, inputCapDims].\n+ *\n+ * From <a href=\"http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf\">Dynamic Routing Between Capsules</a>\n+ *\n+ * @author Ryan Nett\n+ */\n+@Data\n+@NoArgsConstructor\n+@EqualsAndHashCode(callSuper = true)\n+public class CapsuleLayer extends SameDiffLayer {\n+\n+    private static final String WEIGHT_PARAM = \"weight\";\n+    private static final String BIAS_PARAM = \"bias\";\n+\n+    private boolean hasBias = false;\n+    private long inputCapsules = 0;\n+    private long inputCapsuleDimensions = 0;\n+    private int capsules;\n+    private int capsuleDimensions;\n+    private int routings = 3;\n+\n+    public CapsuleLayer(Builder builder){\n+        super(builder);\n+        this.hasBias = builder.hasBias;\n+        this.inputCapsules = builder.inputCapsules;\n+        this.inputCapsuleDimensions = builder.inputCapsuleDimensions;\n+        this.capsules = builder.capsules;\n+        this.capsuleDimensions = builder.capsuleDimensions;\n+        this.routings = builder.routings;\n+\n+        if(capsules <= 0 || capsuleDimensions <= 0 || routings <= 0){\n+            throw new IllegalArgumentException(\"Invalid configuration for Capsule Layer (layer name = \\\"\"\n+                    + layerName + \"\\\"):\"\n+                    + \" capsules, capsuleDimensions, and routings must be > 0.  Got: \"\n+                    + capsules + \", \" + capsuleDimensions + \", \" + routings);\n+        }\n+\n+        if(inputCapsules < 0 || inputCapsuleDimensions < 0){\n+            throw new IllegalArgumentException(\"Invalid configuration for Capsule Layer (layer name = \\\"\"\n+                    + layerName + \"\\\"):\"\n+                    + \" inputCapsules and inputCapsuleDimensions must be >= 0 if set.  Got: \"\n+                    + inputCapsules + \", \" + inputCapsuleDimensions);\n+        }\n+\n+    }\n+\n+    @Override\n+    public void setNIn(InputType inputType, boolean override) {\n+        if(inputType == null || inputType.getType() != Type.RNN) {\n+            throw new IllegalStateException(\"Invalid input for Capsule layer (layer name = \\\"\"\n+                    + layerName + \"\\\"): expect RNN input.  Got: \" + inputType);\n+        }\n+\n+        if(inputCapsules <= 0 || inputCapsuleDimensions <= 0){\n+            InputType.InputTypeRecurrent ir = (InputTypeRecurrent) inputType;\n+            inputCapsules = ir.getSize();\n+            inputCapsuleDimensions = ir.getTimeSeriesLength();\n+        }\n+\n+    }\n+\n+    @Override\n+    public SDVariable defineLayer(SameDiff SD, SDVariable input, Map<String, SDVariable> paramTable) {\n+\n+        // input: [mb, inputCapsules, inputCapsuleDimensions]\n+\n+        // [mb, inputCapsules, 1, inputCapsuleDimensions, 1]\n+        SDVariable expanded = SD.expandDims(SD.expandDims(input, 2), 4);\n+\n+        // [mb, inputCapsules, capsules  * capsuleDimensions, inputCapsuleDimensions, 1]\n+        SDVariable tiled = SD.tile(expanded, 1, 1, capsules * capsuleDimensions, 1, 1);\n+\n+        // [1, inputCapsules, capsules * capsuleDimensions, inputCapsuleDimensions]\n+        SDVariable weights = paramTable.get(WEIGHT_PARAM);\n+\n+        // uHat is the matrix of prediction vectors between two capsules\n+        // [mb, inputCapsules, capsules, capsuleDimensions, 1]\n+        SDVariable uHat = weights.times(tiled).sum(true, 3)\n+                .reshape(-1, inputCapsules, capsules, capsuleDimensions, 1);\n+\n+        // b is the logits of the routing procedure\n+        // [mb, inputCapsules, capsules, 1, 1]\n+        SDVariable b = SD.zerosLike(uHat).get(SDIndex.all(), SDIndex.all(), SDIndex.all(), SDIndex.interval(0, 1), SDIndex.interval(0, 1));",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "272029128",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7391,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/CapsuleLayer.java",
        "discussion_id": "272029128",
        "commented_code": "@@ -0,0 +1,300 @@\n+/*******************************************************************************\n+ * Copyright (c) 2015-2018 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+package org.deeplearning4j.nn.conf.layers;\n+\n+import java.util.Map;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NoArgsConstructor;\n+import lombok.Setter;\n+import org.deeplearning4j.nn.conf.inputs.InputType;\n+import org.deeplearning4j.nn.conf.inputs.InputType.InputTypeRecurrent;\n+import org.deeplearning4j.nn.conf.inputs.InputType.Type;\n+import org.deeplearning4j.nn.conf.layers.samediff.SDLayerParams;\n+import org.deeplearning4j.nn.conf.layers.samediff.SameDiffLayer;\n+import org.deeplearning4j.nn.weights.WeightInitUtil;\n+import org.deeplearning4j.util.CapsuleUtils;\n+import org.deeplearning4j.util.ValidationUtils;\n+import org.nd4j.autodiff.samediff.SDIndex;\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.linalg.api.memory.MemoryWorkspace;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.factory.Nd4j;\n+\n+/**\n+ * An implementation of the DigiCaps layer from Dynamic Routing Between Capsules\n+ *\n+ * Input should come from a PrimaryCapsules layer and be of shape [mb, inputCaps, inputCapDims].\n+ *\n+ * From <a href=\"http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf\">Dynamic Routing Between Capsules</a>\n+ *\n+ * @author Ryan Nett\n+ */\n+@Data\n+@NoArgsConstructor\n+@EqualsAndHashCode(callSuper = true)\n+public class CapsuleLayer extends SameDiffLayer {\n+\n+    private static final String WEIGHT_PARAM = \"weight\";\n+    private static final String BIAS_PARAM = \"bias\";\n+\n+    private boolean hasBias = false;\n+    private long inputCapsules = 0;\n+    private long inputCapsuleDimensions = 0;\n+    private int capsules;\n+    private int capsuleDimensions;\n+    private int routings = 3;\n+\n+    public CapsuleLayer(Builder builder){\n+        super(builder);\n+        this.hasBias = builder.hasBias;\n+        this.inputCapsules = builder.inputCapsules;\n+        this.inputCapsuleDimensions = builder.inputCapsuleDimensions;\n+        this.capsules = builder.capsules;\n+        this.capsuleDimensions = builder.capsuleDimensions;\n+        this.routings = builder.routings;\n+\n+        if(capsules <= 0 || capsuleDimensions <= 0 || routings <= 0){\n+            throw new IllegalArgumentException(\"Invalid configuration for Capsule Layer (layer name = \\\"\"\n+                    + layerName + \"\\\"):\"\n+                    + \" capsules, capsuleDimensions, and routings must be > 0.  Got: \"\n+                    + capsules + \", \" + capsuleDimensions + \", \" + routings);\n+        }\n+\n+        if(inputCapsules < 0 || inputCapsuleDimensions < 0){\n+            throw new IllegalArgumentException(\"Invalid configuration for Capsule Layer (layer name = \\\"\"\n+                    + layerName + \"\\\"):\"\n+                    + \" inputCapsules and inputCapsuleDimensions must be >= 0 if set.  Got: \"\n+                    + inputCapsules + \", \" + inputCapsuleDimensions);\n+        }\n+\n+    }\n+\n+    @Override\n+    public void setNIn(InputType inputType, boolean override) {\n+        if(inputType == null || inputType.getType() != Type.RNN) {\n+            throw new IllegalStateException(\"Invalid input for Capsule layer (layer name = \\\"\"\n+                    + layerName + \"\\\"): expect RNN input.  Got: \" + inputType);\n+        }\n+\n+        if(inputCapsules <= 0 || inputCapsuleDimensions <= 0){\n+            InputType.InputTypeRecurrent ir = (InputTypeRecurrent) inputType;\n+            inputCapsules = ir.getSize();\n+            inputCapsuleDimensions = ir.getTimeSeriesLength();\n+        }\n+\n+    }\n+\n+    @Override\n+    public SDVariable defineLayer(SameDiff SD, SDVariable input, Map<String, SDVariable> paramTable) {\n+\n+        // input: [mb, inputCapsules, inputCapsuleDimensions]\n+\n+        // [mb, inputCapsules, 1, inputCapsuleDimensions, 1]\n+        SDVariable expanded = SD.expandDims(SD.expandDims(input, 2), 4);\n+\n+        // [mb, inputCapsules, capsules  * capsuleDimensions, inputCapsuleDimensions, 1]\n+        SDVariable tiled = SD.tile(expanded, 1, 1, capsules * capsuleDimensions, 1, 1);\n+\n+        // [1, inputCapsules, capsules * capsuleDimensions, inputCapsuleDimensions]\n+        SDVariable weights = paramTable.get(WEIGHT_PARAM);\n+\n+        // uHat is the matrix of prediction vectors between two capsules\n+        // [mb, inputCapsules, capsules, capsuleDimensions, 1]\n+        SDVariable uHat = weights.times(tiled).sum(true, 3)\n+                .reshape(-1, inputCapsules, capsules, capsuleDimensions, 1);\n+\n+        // b is the logits of the routing procedure\n+        // [mb, inputCapsules, capsules, 1, 1]\n+        SDVariable b = SD.zerosLike(uHat).get(SDIndex.all(), SDIndex.all(), SDIndex.all(), SDIndex.interval(0, 1), SDIndex.interval(0, 1));",
        "comment_created_at": "2019-04-04T06:13:15+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "This seems off.\r\nYou create a zeros array of shape [mb, inputCaps, caps, capsDimensions, 1]\r\nBut then proceed to immediately get a [mb, inputCaps, caps, 1, 1] subset from it?\r\nThat's unnecessarily inefficient. Why not make a ```[mb, inputCaps, caps, 1, 1]``` in the first place?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "262834981",
    "pr_number": 7251,
    "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DefaultParamInitializer.java",
    "created_at": "2019-03-06T08:30:28+00:00",
    "commented_code": "return biasParamView;\n     }\n \n+    protected INDArray createGain(NeuralNetConfiguration conf, INDArray gainParamView, boolean initializeParameters) {\n+        org.deeplearning4j.nn.conf.layers.FeedForwardLayer layerConf =\n+                (org.deeplearning4j.nn.conf.layers.FeedForwardLayer) conf.getLayer();\n+        return createGain(layerConf.getNOut(), layerConf.getGainInit(), gainParamView, initializeParameters);\n+    }\n+\n+    protected INDArray createGain(long nOut, double gainInit, INDArray gainParamView, boolean initializeParameters) {\n+        if (initializeParameters) {\n+            INDArray ret = Nd4j.valueArrayOf(new long[] {1, nOut}, gainInit);\n+            gainParamView.assign(ret);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "262834981",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7251,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DefaultParamInitializer.java",
        "discussion_id": "262834981",
        "commented_code": "@@ -162,6 +181,20 @@ protected INDArray createBias(long nOut, double biasInit, INDArray biasParamView\n         return biasParamView;\n     }\n \n+    protected INDArray createGain(NeuralNetConfiguration conf, INDArray gainParamView, boolean initializeParameters) {\n+        org.deeplearning4j.nn.conf.layers.FeedForwardLayer layerConf =\n+                (org.deeplearning4j.nn.conf.layers.FeedForwardLayer) conf.getLayer();\n+        return createGain(layerConf.getNOut(), layerConf.getGainInit(), gainParamView, initializeParameters);\n+    }\n+\n+    protected INDArray createGain(long nOut, double gainInit, INDArray gainParamView, boolean initializeParameters) {\n+        if (initializeParameters) {\n+            INDArray ret = Nd4j.valueArrayOf(new long[] {1, nOut}, gainInit);\n+            gainParamView.assign(ret);",
        "comment_created_at": "2019-03-06T08:30:28+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Don't create this temporary ret array. Use ```gainParamView.assign(gainInit)``` instead.",
        "pr_file_module": null
      },
      {
        "comment_id": "262845606",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7251,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DefaultParamInitializer.java",
        "discussion_id": "262834981",
        "commented_code": "@@ -162,6 +181,20 @@ protected INDArray createBias(long nOut, double biasInit, INDArray biasParamView\n         return biasParamView;\n     }\n \n+    protected INDArray createGain(NeuralNetConfiguration conf, INDArray gainParamView, boolean initializeParameters) {\n+        org.deeplearning4j.nn.conf.layers.FeedForwardLayer layerConf =\n+                (org.deeplearning4j.nn.conf.layers.FeedForwardLayer) conf.getLayer();\n+        return createGain(layerConf.getNOut(), layerConf.getGainInit(), gainParamView, initializeParameters);\n+    }\n+\n+    protected INDArray createGain(long nOut, double gainInit, INDArray gainParamView, boolean initializeParameters) {\n+        if (initializeParameters) {\n+            INDArray ret = Nd4j.valueArrayOf(new long[] {1, nOut}, gainInit);\n+            gainParamView.assign(ret);",
        "comment_created_at": "2019-03-06T09:00:55+00:00",
        "comment_author": "treo",
        "comment_body": "good idea, I'll change it accordingly in the createBias method as well then.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "255437049",
    "pr_number": 7141,
    "pr_file": "deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/text/tokenization/tokenizer/BertWordPieceStreamTokenizer.java",
    "created_at": "2019-02-11T09:57:04+00:00",
    "commented_code": "+/*******************************************************************************\n+ * Copyright (c) 2015-2018 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+package org.deeplearning4j.text.tokenization.tokenizer;\n+\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.*;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.NavigableMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * A tokenizer that works with a vocab from a published bert model and tokenizes a token at a time from a stream\n+ * @author Paul Dubs\n+ */\n+public class BertWordPieceStreamTokenizer implements Tokenizer {\n+\n+    private final Pattern splitPattern = Pattern.compile(\"(\\\\p{javaWhitespace}|((?<=\\\\p{Punct})|(?=\\\\p{Punct})))+\");",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "255437049",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7141,
        "pr_file": "deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/text/tokenization/tokenizer/BertWordPieceStreamTokenizer.java",
        "discussion_id": "255437049",
        "commented_code": "@@ -0,0 +1,182 @@\n+/*******************************************************************************\n+ * Copyright (c) 2015-2018 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+package org.deeplearning4j.text.tokenization.tokenizer;\n+\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.*;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.NavigableMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * A tokenizer that works with a vocab from a published bert model and tokenizes a token at a time from a stream\n+ * @author Paul Dubs\n+ */\n+public class BertWordPieceStreamTokenizer implements Tokenizer {\n+\n+    private final Pattern splitPattern = Pattern.compile(\"(\\\\p{javaWhitespace}|((?<=\\\\p{Punct})|(?=\\\\p{Punct})))+\");",
        "comment_created_at": "2019-02-11T09:57:04+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Nice regex :)\r\nA minor detail, but pattern is apparently immutable and thread safe... we can make it static? (Or make the one in BertWordPieceTokenizer static + public and use here too?)",
        "pr_file_module": null
      }
    ]
  }
]