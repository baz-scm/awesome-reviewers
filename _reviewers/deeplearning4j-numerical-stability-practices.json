[
  {
    "discussion_id": "579625773",
    "pr_number": 9186,
    "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/learning/config/AdaBelief.java",
    "created_at": "2021-02-20T08:51:44+00:00",
    "commented_code": "+/*\n+ *  ******************************************************************************\n+ *  *\n+ *  *\n+ *  * This program and the accompanying materials are made available under the\n+ *  * terms of the Apache License, Version 2.0 which is available at\n+ *  * https://www.apache.org/licenses/LICENSE-2.0.\n+ *  *\n+ *  *  See the NOTICE file distributed with this work for additional\n+ *  *  information regarding copyright ownership.\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ *  * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ *  * License for the specific language governing permissions and limitations\n+ *  * under the License.\n+ *  *\n+ *  * SPDX-License-Identifier: Apache-2.0\n+ *  *****************************************************************************\n+ */\n+\n+package org.nd4j.linalg.learning.config;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.learning.AdaBeliefUpdater;\n+import org.nd4j.linalg.learning.GradientUpdater;\n+import org.nd4j.linalg.schedule.ISchedule;\n+import org.nd4j.shade.jackson.annotation.JsonProperty;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+\n+/**\n+ * AdaBelief \n+ * https://arxiv.org/pdf/2010.07468.pdf\n+ */\n+@Data\n+@Builder(builderClassName = \"Builder\")\n+public class AdaBelief implements IUpdater {\n+\n+    public static final double DEFAULT_LEARNING_RATE = 1e-3;\n+    public static final double DEFAULT_EPSILON = 1e-8;",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "579625773",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9186,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/learning/config/AdaBelief.java",
        "discussion_id": "579625773",
        "commented_code": "@@ -0,0 +1,132 @@\n+/*\n+ *  ******************************************************************************\n+ *  *\n+ *  *\n+ *  * This program and the accompanying materials are made available under the\n+ *  * terms of the Apache License, Version 2.0 which is available at\n+ *  * https://www.apache.org/licenses/LICENSE-2.0.\n+ *  *\n+ *  *  See the NOTICE file distributed with this work for additional\n+ *  *  information regarding copyright ownership.\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ *  * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ *  * License for the specific language governing permissions and limitations\n+ *  * under the License.\n+ *  *\n+ *  * SPDX-License-Identifier: Apache-2.0\n+ *  *****************************************************************************\n+ */\n+\n+package org.nd4j.linalg.learning.config;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.learning.AdaBeliefUpdater;\n+import org.nd4j.linalg.learning.GradientUpdater;\n+import org.nd4j.linalg.schedule.ISchedule;\n+import org.nd4j.shade.jackson.annotation.JsonProperty;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+\n+/**\n+ * AdaBelief \n+ * https://arxiv.org/pdf/2010.07468.pdf\n+ */\n+@Data\n+@Builder(builderClassName = \"Builder\")\n+public class AdaBelief implements IUpdater {\n+\n+    public static final double DEFAULT_LEARNING_RATE = 1e-3;\n+    public static final double DEFAULT_EPSILON = 1e-8;",
        "comment_created_at": "2021-02-20T08:51:44+00:00",
        "comment_author": "treo",
        "comment_body": "The paper authors have found that a smaller default epsilon works better:\r\n\r\n> Epsilon in AdaBelief is different from Adam (typically eps_adabelief = eps_adam*eps_adam)\r\n> ( eps of Adam in Tensorflow is 1e-7, in PyTorch is 1e-8, need to consider this when use AdaBelief in Tensorflow)\r\n\r\nSo I guess we might want to use either 1e-14 or 1e-16 here. ",
        "pr_file_module": null
      },
      {
        "comment_id": "579971123",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9186,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/learning/config/AdaBelief.java",
        "discussion_id": "579625773",
        "commented_code": "@@ -0,0 +1,132 @@\n+/*\n+ *  ******************************************************************************\n+ *  *\n+ *  *\n+ *  * This program and the accompanying materials are made available under the\n+ *  * terms of the Apache License, Version 2.0 which is available at\n+ *  * https://www.apache.org/licenses/LICENSE-2.0.\n+ *  *\n+ *  *  See the NOTICE file distributed with this work for additional\n+ *  *  information regarding copyright ownership.\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ *  * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ *  * License for the specific language governing permissions and limitations\n+ *  * under the License.\n+ *  *\n+ *  * SPDX-License-Identifier: Apache-2.0\n+ *  *****************************************************************************\n+ */\n+\n+package org.nd4j.linalg.learning.config;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.learning.AdaBeliefUpdater;\n+import org.nd4j.linalg.learning.GradientUpdater;\n+import org.nd4j.linalg.schedule.ISchedule;\n+import org.nd4j.shade.jackson.annotation.JsonProperty;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+\n+/**\n+ * AdaBelief \n+ * https://arxiv.org/pdf/2010.07468.pdf\n+ */\n+@Data\n+@Builder(builderClassName = \"Builder\")\n+public class AdaBelief implements IUpdater {\n+\n+    public static final double DEFAULT_LEARNING_RATE = 1e-3;\n+    public static final double DEFAULT_EPSILON = 1e-8;",
        "comment_created_at": "2021-02-22T04:00:41+00:00",
        "comment_author": "quickwritereader",
        "comment_body": "changed to 1e-14 ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "272031362",
    "pr_number": 7391,
    "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/util/CapsuleUtils.java",
    "created_at": "2019-04-04T06:24:46+00:00",
    "commented_code": "+/*******************************************************************************\n+ * Copyright (c) 2015-2018 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+package org.deeplearning4j.util;\n+\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.linalg.util.ArrayUtil;\n+\n+/**\n+ * Utilities for CapsNet Layers\n+ * @see org.deeplearning4j.nn.conf.layers.CapsuleLayer\n+ * @see org.deeplearning4j.nn.conf.layers.PrimaryCapsules\n+ * @see org.deeplearning4j.nn.conf.layers.CapsuleStrengthLayer\n+ *\n+ * @author Ryan Nett\n+ */\n+public class CapsuleUtils {\n+\n+    /**\n+     *  Compute the squash operation used in CapsNet\n+     *  The formula is (||s||^2 / (1 + ||s||^2)) * (s / ||s||).\n+     *  Canceling one ||s|| gives ||s||*s/((1 + ||s||^2)\n+     *\n+     * @param SD The SameDiff environment\n+     * @param x The variable to squash\n+     * @return squash(x)\n+     */\n+    public static SDVariable squash(SameDiff SD, SDVariable x, int dim){\n+        SDVariable squaredNorm = SD.math.square(x).sum(true, dim);\n+        SDVariable scale = SD.math.sqrt(squaredNorm.plus(1e-5));\n+        return x.times(squaredNorm).div(squaredNorm.plus(1.0).times(scale));",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "272031362",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7391,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/util/CapsuleUtils.java",
        "discussion_id": "272031362",
        "commented_code": "@@ -0,0 +1,59 @@\n+/*******************************************************************************\n+ * Copyright (c) 2015-2018 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+package org.deeplearning4j.util;\n+\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.linalg.util.ArrayUtil;\n+\n+/**\n+ * Utilities for CapsNet Layers\n+ * @see org.deeplearning4j.nn.conf.layers.CapsuleLayer\n+ * @see org.deeplearning4j.nn.conf.layers.PrimaryCapsules\n+ * @see org.deeplearning4j.nn.conf.layers.CapsuleStrengthLayer\n+ *\n+ * @author Ryan Nett\n+ */\n+public class CapsuleUtils {\n+\n+    /**\n+     *  Compute the squash operation used in CapsNet\n+     *  The formula is (||s||^2 / (1 + ||s||^2)) * (s / ||s||).\n+     *  Canceling one ||s|| gives ||s||*s/((1 + ||s||^2)\n+     *\n+     * @param SD The SameDiff environment\n+     * @param x The variable to squash\n+     * @return squash(x)\n+     */\n+    public static SDVariable squash(SameDiff SD, SDVariable x, int dim){\n+        SDVariable squaredNorm = SD.math.square(x).sum(true, dim);\n+        SDVariable scale = SD.math.sqrt(squaredNorm.plus(1e-5));\n+        return x.times(squaredNorm).div(squaredNorm.plus(1.0).times(scale));",
        "comment_created_at": "2019-04-04T06:24:46+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Why addition of 1e-5? Normally this is for numerical precision, but I don't believe we risk underflowing here even if l2 norm is 0? (We get 0/1 in that case)",
        "pr_file_module": null
      },
      {
        "comment_id": "272315045",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7391,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/util/CapsuleUtils.java",
        "discussion_id": "272031362",
        "commented_code": "@@ -0,0 +1,59 @@\n+/*******************************************************************************\n+ * Copyright (c) 2015-2018 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+package org.deeplearning4j.util;\n+\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.linalg.util.ArrayUtil;\n+\n+/**\n+ * Utilities for CapsNet Layers\n+ * @see org.deeplearning4j.nn.conf.layers.CapsuleLayer\n+ * @see org.deeplearning4j.nn.conf.layers.PrimaryCapsules\n+ * @see org.deeplearning4j.nn.conf.layers.CapsuleStrengthLayer\n+ *\n+ * @author Ryan Nett\n+ */\n+public class CapsuleUtils {\n+\n+    /**\n+     *  Compute the squash operation used in CapsNet\n+     *  The formula is (||s||^2 / (1 + ||s||^2)) * (s / ||s||).\n+     *  Canceling one ||s|| gives ||s||*s/((1 + ||s||^2)\n+     *\n+     * @param SD The SameDiff environment\n+     * @param x The variable to squash\n+     * @return squash(x)\n+     */\n+    public static SDVariable squash(SameDiff SD, SDVariable x, int dim){\n+        SDVariable squaredNorm = SD.math.square(x).sum(true, dim);\n+        SDVariable scale = SD.math.sqrt(squaredNorm.plus(1e-5));\n+        return x.times(squaredNorm).div(squaredNorm.plus(1.0).times(scale));",
        "comment_created_at": "2019-04-04T18:41:59+00:00",
        "comment_author": "rnett",
        "comment_body": "That was because I saw it in every implementation I looked at and wasn't sure why, and so didn't want to remove it.\r\n\r\nFrom [here](https://github.com/tensorflow/tensorflow/issues/4914), it seems to be to keep the gradient stable if the squaredNorm is zero.  Is that an issue SameDiff will have?",
        "pr_file_module": null
      },
      {
        "comment_id": "272417866",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7391,
        "pr_file": "deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/util/CapsuleUtils.java",
        "discussion_id": "272031362",
        "commented_code": "@@ -0,0 +1,59 @@\n+/*******************************************************************************\n+ * Copyright (c) 2015-2018 Skymind, Inc.\n+ *\n+ * This program and the accompanying materials are made available under the\n+ * terms of the Apache License, Version 2.0 which is available at\n+ * https://www.apache.org/licenses/LICENSE-2.0.\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ ******************************************************************************/\n+\n+package org.deeplearning4j.util;\n+\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.linalg.util.ArrayUtil;\n+\n+/**\n+ * Utilities for CapsNet Layers\n+ * @see org.deeplearning4j.nn.conf.layers.CapsuleLayer\n+ * @see org.deeplearning4j.nn.conf.layers.PrimaryCapsules\n+ * @see org.deeplearning4j.nn.conf.layers.CapsuleStrengthLayer\n+ *\n+ * @author Ryan Nett\n+ */\n+public class CapsuleUtils {\n+\n+    /**\n+     *  Compute the squash operation used in CapsNet\n+     *  The formula is (||s||^2 / (1 + ||s||^2)) * (s / ||s||).\n+     *  Canceling one ||s|| gives ||s||*s/((1 + ||s||^2)\n+     *\n+     * @param SD The SameDiff environment\n+     * @param x The variable to squash\n+     * @return squash(x)\n+     */\n+    public static SDVariable squash(SameDiff SD, SDVariable x, int dim){\n+        SDVariable squaredNorm = SD.math.square(x).sum(true, dim);\n+        SDVariable scale = SD.math.sqrt(squaredNorm.plus(1e-5));\n+        return x.times(squaredNorm).div(squaredNorm.plus(1.0).times(scale));",
        "comment_created_at": "2019-04-05T01:05:22+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Ah, makes sense, let's leave it then.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "271830613",
    "pr_number": 7430,
    "pr_file": "deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/iterator/bert/BertMaskedLMMasker.java",
    "created_at": "2019-04-03T16:35:42+00:00",
    "commented_code": "+package org.deeplearning4j.iterator.bert;\n+\n+import org.nd4j.base.Preconditions;\n+import org.nd4j.linalg.primitives.Pair;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+/**\n+ * A standard/default {@link BertSequenceMasker}. Implements masking as per the BERT paper:\n+ * <a href=\"https://arxiv.org/abs/1810.04805\">https://arxiv.org/abs/1810.04805</a>\n+ * That is, each token is chosen to be masked independently with some probability \"maskProb\".\n+ * For tokens that are masked, 3 possibilities:<br>\n+ * 1. They are replaced with the mask token (such as \"[MASK]\") in the input, with probability \"maskTokenProb\"<br>\n+ * 2. They are replaced with a random word from the vocabulary, with probability \"randomTokenProb\"<br>\n+ * 3. They are are left unmodified with probability 1.0 - maskTokenProb - randomTokenProb<br>\n+ *\n+ * @author Alex Black\n+ */\n+public class BertMaskedLMMasker implements BertSequenceMasker {\n+    public static final double DEFAULT_MASK_PROB = 0.15;\n+    public static final double DEFAULT_MASK_TOKEN_PROB = 0.8;\n+    public static final double DEFAULT_RANDOM_WORD_PROB = 0.1;\n+\n+    protected final Random r;\n+    protected final double maskProb;\n+    protected final double maskTokenProb;\n+    protected final double randomTokenProb;\n+\n+    /**\n+     * Create a BertMaskedLMMasker with all default probabilities\n+     */\n+    public BertMaskedLMMasker(){\n+        this(new Random(), DEFAULT_MASK_PROB, DEFAULT_MASK_TOKEN_PROB, DEFAULT_RANDOM_WORD_PROB);\n+    }\n+\n+    /**\n+     * See: {@link BertMaskedLMMasker} for details.\n+     * @param r                 Random number generator\n+     * @param maskProb          Probability of masking each token\n+     * @param maskTokenProb     Probability of replacing a selected token with the mask token\n+     * @param randomTokenProb    Probability of replacing a selected token with a random token\n+     */\n+    public BertMaskedLMMasker(Random r, double maskProb, double maskTokenProb, double randomTokenProb){\n+        Preconditions.checkArgument(maskProb > 0 && maskProb < 1, \"Probability must be beteen 0 and 1, got %s\", maskProb);\n+        this.r = r;",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "271830613",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7430,
        "pr_file": "deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/iterator/bert/BertMaskedLMMasker.java",
        "discussion_id": "271830613",
        "commented_code": "@@ -0,0 +1,79 @@\n+package org.deeplearning4j.iterator.bert;\n+\n+import org.nd4j.base.Preconditions;\n+import org.nd4j.linalg.primitives.Pair;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+/**\n+ * A standard/default {@link BertSequenceMasker}. Implements masking as per the BERT paper:\n+ * <a href=\"https://arxiv.org/abs/1810.04805\">https://arxiv.org/abs/1810.04805</a>\n+ * That is, each token is chosen to be masked independently with some probability \"maskProb\".\n+ * For tokens that are masked, 3 possibilities:<br>\n+ * 1. They are replaced with the mask token (such as \"[MASK]\") in the input, with probability \"maskTokenProb\"<br>\n+ * 2. They are replaced with a random word from the vocabulary, with probability \"randomTokenProb\"<br>\n+ * 3. They are are left unmodified with probability 1.0 - maskTokenProb - randomTokenProb<br>\n+ *\n+ * @author Alex Black\n+ */\n+public class BertMaskedLMMasker implements BertSequenceMasker {\n+    public static final double DEFAULT_MASK_PROB = 0.15;\n+    public static final double DEFAULT_MASK_TOKEN_PROB = 0.8;\n+    public static final double DEFAULT_RANDOM_WORD_PROB = 0.1;\n+\n+    protected final Random r;\n+    protected final double maskProb;\n+    protected final double maskTokenProb;\n+    protected final double randomTokenProb;\n+\n+    /**\n+     * Create a BertMaskedLMMasker with all default probabilities\n+     */\n+    public BertMaskedLMMasker(){\n+        this(new Random(), DEFAULT_MASK_PROB, DEFAULT_MASK_TOKEN_PROB, DEFAULT_RANDOM_WORD_PROB);\n+    }\n+\n+    /**\n+     * See: {@link BertMaskedLMMasker} for details.\n+     * @param r                 Random number generator\n+     * @param maskProb          Probability of masking each token\n+     * @param maskTokenProb     Probability of replacing a selected token with the mask token\n+     * @param randomTokenProb    Probability of replacing a selected token with a random token\n+     */\n+    public BertMaskedLMMasker(Random r, double maskProb, double maskTokenProb, double randomTokenProb){\n+        Preconditions.checkArgument(maskProb > 0 && maskProb < 1, \"Probability must be beteen 0 and 1, got %s\", maskProb);\n+        this.r = r;",
        "comment_created_at": "2019-04-03T16:35:42+00:00",
        "comment_author": "treo",
        "comment_body": "maybe also check that 0 <= maskTokenProb + randomTokenProb <= 1?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "379996529",
    "pr_number": 8713,
    "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/lossfunctions/SameDiffLoss.java",
    "created_at": "2020-02-17T05:56:43+00:00",
    "commented_code": "+package org.nd4j.linalg.lossfunctions;\n+\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.base.Preconditions;\n+import org.nd4j.linalg.activations.IActivation;\n+import org.nd4j.linalg.api.buffer.DataType;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.primitives.Pair;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+\n+public abstract class SameDiffLoss implements ILossFunction {\n+\n+    SameDiff sd = SameDiff.create();\n+\n+\n+    protected SameDiffLoss() {\n+\n+\n+        SDVariable layerInput =  sd.placeHolder(\"layerInput\", DataType.FLOAT ,-1);\n+        SDVariable labels = sd.placeHolder(\"labels\", DataType.FLOAT ,-1);\n+        this.defineLoss(sd, layerInput, labels);\n+\n+\n+    }\n+\n+\n+    public abstract SDVariable defineLoss(SameDiff sd, SDVariable layerInput, SDVariable labels);\n+\n+    /**\n+     * Compute the score (loss function value) for the given inputs.\n+     * @param labels       Label/expected preOutput\n+     * @param preOutput    Output of the model (neural network)\n+     * @param activationFn Activation function that should be applied to preOutput\n+     * @param mask         Mask array; may be null\n+     * @param average      Whether the score should be averaged (divided by number of rows in labels/preOutput) or not   @return Loss function value\n+     */\n+    public double computeScore(INDArray labels, INDArray preOutput, IActivation activationFn, INDArray mask, boolean average) {\n+\n+            // The score overall consists of the\n+            // sum of the negative log likelihoods for each\n+            // of the individual labels.\n+\n+\n+\n+        INDArray scoreArr = computeScoreArray(labels, preOutput, activationFn, mask);\n+\n+        double score = scoreArr.sumNumber().doubleValue();\n+            if (average) {\n+                score /= scoreArr.size(0);\n+            }\n+            return score;\n+        }\n+\n+\n+\n+\n+\n+\n+    /**\n+     * Compute the score (loss function value) for each example individually.\n+     * For input [numExamples,nOut] returns scores as a column vector: [numExamples,1]\n+     * @param labels       Labels/expected output\n+     * @param preOutput    Output of the model (neural network)\n+     * @param activationFn Activation function that should be applied to preOutput\n+     * @param mask         @return Loss function value for each example; column vector\n+     */\n+    public INDArray computeScoreArray(INDArray labels, INDArray preOutput, IActivation activationFn, INDArray mask){\n+\n+        Preconditions.checkArgument((labels.size(1) != preOutput.size(1)),\"Labels array numColumns (size(1) = \" + labels.size(1) + \") does not match output layer\n\" +\n+                \"number of outputs (nOut = \" + preOutput.size(1) + \")\");\n+\n+\n+        INDArray scoreArr;\n+        INDArray output = activationFn.getActivation(preOutput.dup(), true);\n+        sd.var(\"out\",output);\n+        scoreArr = output.rsubi(labels).divi(labels);\n+        scoreArr.muli(100.0 / labels.size(1));",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "379996529",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 8713,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/lossfunctions/SameDiffLoss.java",
        "discussion_id": "379996529",
        "commented_code": "@@ -0,0 +1,146 @@\n+package org.nd4j.linalg.lossfunctions;\n+\n+import org.nd4j.autodiff.samediff.SDVariable;\n+import org.nd4j.autodiff.samediff.SameDiff;\n+import org.nd4j.base.Preconditions;\n+import org.nd4j.linalg.activations.IActivation;\n+import org.nd4j.linalg.api.buffer.DataType;\n+import org.nd4j.linalg.api.ndarray.INDArray;\n+import org.nd4j.linalg.primitives.Pair;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+\n+public abstract class SameDiffLoss implements ILossFunction {\n+\n+    SameDiff sd = SameDiff.create();\n+\n+\n+    protected SameDiffLoss() {\n+\n+\n+        SDVariable layerInput =  sd.placeHolder(\"layerInput\", DataType.FLOAT ,-1);\n+        SDVariable labels = sd.placeHolder(\"labels\", DataType.FLOAT ,-1);\n+        this.defineLoss(sd, layerInput, labels);\n+\n+\n+    }\n+\n+\n+    public abstract SDVariable defineLoss(SameDiff sd, SDVariable layerInput, SDVariable labels);\n+\n+    /**\n+     * Compute the score (loss function value) for the given inputs.\n+     * @param labels       Label/expected preOutput\n+     * @param preOutput    Output of the model (neural network)\n+     * @param activationFn Activation function that should be applied to preOutput\n+     * @param mask         Mask array; may be null\n+     * @param average      Whether the score should be averaged (divided by number of rows in labels/preOutput) or not   @return Loss function value\n+     */\n+    public double computeScore(INDArray labels, INDArray preOutput, IActivation activationFn, INDArray mask, boolean average) {\n+\n+            // The score overall consists of the\n+            // sum of the negative log likelihoods for each\n+            // of the individual labels.\n+\n+\n+\n+        INDArray scoreArr = computeScoreArray(labels, preOutput, activationFn, mask);\n+\n+        double score = scoreArr.sumNumber().doubleValue();\n+            if (average) {\n+                score /= scoreArr.size(0);\n+            }\n+            return score;\n+        }\n+\n+\n+\n+\n+\n+\n+    /**\n+     * Compute the score (loss function value) for each example individually.\n+     * For input [numExamples,nOut] returns scores as a column vector: [numExamples,1]\n+     * @param labels       Labels/expected output\n+     * @param preOutput    Output of the model (neural network)\n+     * @param activationFn Activation function that should be applied to preOutput\n+     * @param mask         @return Loss function value for each example; column vector\n+     */\n+    public INDArray computeScoreArray(INDArray labels, INDArray preOutput, IActivation activationFn, INDArray mask){\n+\n+        Preconditions.checkArgument((labels.size(1) != preOutput.size(1)),\"Labels array numColumns (size(1) = \" + labels.size(1) + \") does not match output layer\\n\" +\n+                \"number of outputs (nOut = \" + preOutput.size(1) + \")\");\n+\n+\n+        INDArray scoreArr;\n+        INDArray output = activationFn.getActivation(preOutput.dup(), true);\n+        sd.var(\"out\",output);\n+        scoreArr = output.rsubi(labels).divi(labels);\n+        scoreArr.muli(100.0 / labels.size(1));",
        "comment_created_at": "2020-02-17T05:56:43+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Again:\r\nBy definition, the score calculation _must_ use the SameDiff instance. The `output = activationFn.getActivation` part is ok, but after that just call SameDiff.output providing the placeholders instead.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "272787554",
    "pr_number": 7452,
    "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java",
    "created_at": "2019-04-06T07:43:38+00:00",
    "commented_code": "} else {\n                         inputsToOp = sameDiff.ops.get(df.getOwnName()).getInputsToOp();\n                         outputsOfOp = sameDiff.ops.get(df.getOwnName()).getOutputsOfOp();\n-                        numProcessed++;\n                     }\n \n \n                     //Get gradients for all output variables:\n                     List<SDVariable> grads = new ArrayList<>();\n                     for(String s : outputsOfOp){\n-                        SDVariable g = sameDiff.getVariable(s).gradient();\n-                        Preconditions.checkNotNull(g, \"Could not get gradient for variable %s as output of op %s\", g.getVarName(), df.getOwnName());\n-                        grads.add(g);\n+                        SDVariable v = sameDiff.getVariable(s);\n+                        SDVariable g = v.hasGradient() ? v.gradient() : null;\n+\n+                        if(g == null){\n+                            //If no gradient exists at this point, 3 possibilities:\n+                            // (a) we have a bug\n+                            // (b) output of this op isn't used in calculating the loss\n+                            // (c) output isn't a FP type\n+                            //In the FP case, we should create a zero variable to backprop, because we can't perform backprop\n+                            // for this op otherwise...\n+                            if(!v.dataType().isFPType()){\n+                                grads.add(null);\n+                            } else {\n+                                SDVariable gTemp = sameDiff.zerosLike(v);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "272787554",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7452,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java",
        "discussion_id": "272787554",
        "commented_code": "@@ -3198,48 +3512,120 @@ public void createGradFunction() {\n                     } else {\n                         inputsToOp = sameDiff.ops.get(df.getOwnName()).getInputsToOp();\n                         outputsOfOp = sameDiff.ops.get(df.getOwnName()).getOutputsOfOp();\n-                        numProcessed++;\n                     }\n \n \n                     //Get gradients for all output variables:\n                     List<SDVariable> grads = new ArrayList<>();\n                     for(String s : outputsOfOp){\n-                        SDVariable g = sameDiff.getVariable(s).gradient();\n-                        Preconditions.checkNotNull(g, \"Could not get gradient for variable %s as output of op %s\", g.getVarName(), df.getOwnName());\n-                        grads.add(g);\n+                        SDVariable v = sameDiff.getVariable(s);\n+                        SDVariable g = v.hasGradient() ? v.gradient() : null;\n+\n+                        if(g == null){\n+                            //If no gradient exists at this point, 3 possibilities:\n+                            // (a) we have a bug\n+                            // (b) output of this op isn't used in calculating the loss\n+                            // (c) output isn't a FP type\n+                            //In the FP case, we should create a zero variable to backprop, because we can't perform backprop\n+                            // for this op otherwise...\n+                            if(!v.dataType().isFPType()){\n+                                grads.add(null);\n+                            } else {\n+                                SDVariable gTemp = sameDiff.zerosLike(v);",
        "comment_created_at": "2019-04-06T07:43:38+00:00",
        "comment_author": "treo",
        "comment_body": "Should this even happen? Calculating with a zero gradient, should basically be \"don't do any change at all\" and therefore we shouldn't be performing backprop at all in that case?",
        "pr_file_module": null
      },
      {
        "comment_id": "272788263",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7452,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java",
        "discussion_id": "272787554",
        "commented_code": "@@ -3198,48 +3512,120 @@ public void createGradFunction() {\n                     } else {\n                         inputsToOp = sameDiff.ops.get(df.getOwnName()).getInputsToOp();\n                         outputsOfOp = sameDiff.ops.get(df.getOwnName()).getOutputsOfOp();\n-                        numProcessed++;\n                     }\n \n \n                     //Get gradients for all output variables:\n                     List<SDVariable> grads = new ArrayList<>();\n                     for(String s : outputsOfOp){\n-                        SDVariable g = sameDiff.getVariable(s).gradient();\n-                        Preconditions.checkNotNull(g, \"Could not get gradient for variable %s as output of op %s\", g.getVarName(), df.getOwnName());\n-                        grads.add(g);\n+                        SDVariable v = sameDiff.getVariable(s);\n+                        SDVariable g = v.hasGradient() ? v.gradient() : null;\n+\n+                        if(g == null){\n+                            //If no gradient exists at this point, 3 possibilities:\n+                            // (a) we have a bug\n+                            // (b) output of this op isn't used in calculating the loss\n+                            // (c) output isn't a FP type\n+                            //In the FP case, we should create a zero variable to backprop, because we can't perform backprop\n+                            // for this op otherwise...\n+                            if(!v.dataType().isFPType()){\n+                                grads.add(null);\n+                            } else {\n+                                SDVariable gTemp = sameDiff.zerosLike(v);",
        "comment_created_at": "2019-04-06T08:14:19+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "This is a little unintuitive, but it's definitely intended (and required).\r\nConsider the following graph:\r\n```\r\ninput -> split -> (A,B)\r\nB -> lossFn\r\n```\r\nHere, A is unused (in that it doesn't contribute to the loss function). But to do split op backprop, we need gradient variables/arrays for both outputs (A and B).\r\nWe know the shape and type of dL/dA must be exactly the same as A; we also know that the loss function doesn't depend on A. Hence, dL/dA == zerosLike(A)",
        "pr_file_module": null
      },
      {
        "comment_id": "272788308",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7452,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java",
        "discussion_id": "272787554",
        "commented_code": "@@ -3198,48 +3512,120 @@ public void createGradFunction() {\n                     } else {\n                         inputsToOp = sameDiff.ops.get(df.getOwnName()).getInputsToOp();\n                         outputsOfOp = sameDiff.ops.get(df.getOwnName()).getOutputsOfOp();\n-                        numProcessed++;\n                     }\n \n \n                     //Get gradients for all output variables:\n                     List<SDVariable> grads = new ArrayList<>();\n                     for(String s : outputsOfOp){\n-                        SDVariable g = sameDiff.getVariable(s).gradient();\n-                        Preconditions.checkNotNull(g, \"Could not get gradient for variable %s as output of op %s\", g.getVarName(), df.getOwnName());\n-                        grads.add(g);\n+                        SDVariable v = sameDiff.getVariable(s);\n+                        SDVariable g = v.hasGradient() ? v.gradient() : null;\n+\n+                        if(g == null){\n+                            //If no gradient exists at this point, 3 possibilities:\n+                            // (a) we have a bug\n+                            // (b) output of this op isn't used in calculating the loss\n+                            // (c) output isn't a FP type\n+                            //In the FP case, we should create a zero variable to backprop, because we can't perform backprop\n+                            // for this op otherwise...\n+                            if(!v.dataType().isFPType()){\n+                                grads.add(null);\n+                            } else {\n+                                SDVariable gTemp = sameDiff.zerosLike(v);",
        "comment_created_at": "2019-04-06T08:16:11+00:00",
        "comment_author": "treo",
        "comment_body": "got it. Maybe add the explanation to the comment on top?",
        "pr_file_module": null
      },
      {
        "comment_id": "272788622",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 7452,
        "pr_file": "nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java",
        "discussion_id": "272787554",
        "commented_code": "@@ -3198,48 +3512,120 @@ public void createGradFunction() {\n                     } else {\n                         inputsToOp = sameDiff.ops.get(df.getOwnName()).getInputsToOp();\n                         outputsOfOp = sameDiff.ops.get(df.getOwnName()).getOutputsOfOp();\n-                        numProcessed++;\n                     }\n \n \n                     //Get gradients for all output variables:\n                     List<SDVariable> grads = new ArrayList<>();\n                     for(String s : outputsOfOp){\n-                        SDVariable g = sameDiff.getVariable(s).gradient();\n-                        Preconditions.checkNotNull(g, \"Could not get gradient for variable %s as output of op %s\", g.getVarName(), df.getOwnName());\n-                        grads.add(g);\n+                        SDVariable v = sameDiff.getVariable(s);\n+                        SDVariable g = v.hasGradient() ? v.gradient() : null;\n+\n+                        if(g == null){\n+                            //If no gradient exists at this point, 3 possibilities:\n+                            // (a) we have a bug\n+                            // (b) output of this op isn't used in calculating the loss\n+                            // (c) output isn't a FP type\n+                            //In the FP case, we should create a zero variable to backprop, because we can't perform backprop\n+                            // for this op otherwise...\n+                            if(!v.dataType().isFPType()){\n+                                grads.add(null);\n+                            } else {\n+                                SDVariable gTemp = sameDiff.zerosLike(v);",
        "comment_created_at": "2019-04-06T08:32:02+00:00",
        "comment_author": "AlexDBlack",
        "comment_body": "Done.",
        "pr_file_module": null
      }
    ]
  }
]