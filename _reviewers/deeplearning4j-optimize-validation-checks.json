[
  {
    "discussion_id": "702354863",
    "pr_number": 9442,
    "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/ctcloss.cu",
    "created_at": "2021-09-05T02:43:56+00:00",
    "commented_code": "auto outputGrads = OUTPUT_VARIABLE(0);\n         int blankIndex = INT_ARG(0);\n \n-        auto dTypeInput = logitInput->dataType();\n-        auto intType = targetLabelLengths->dataType();\n-        auto dTypeOutput = outputGrads->dataType();\n+        Requirements req(\"CUDNN CTC_LOSS_GRAD OP\");",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "702354863",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9442,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/ctcloss.cu",
        "discussion_id": "702354863",
        "commented_code": "@@ -189,14 +192,18 @@ namespace platforms {\n         auto outputGrads = OUTPUT_VARIABLE(0);\n         int blankIndex = INT_ARG(0);\n \n-        auto dTypeInput = logitInput->dataType();\n-        auto intType = targetLabelLengths->dataType();\n-        auto dTypeOutput = outputGrads->dataType();\n+        Requirements req(\"CUDNN CTC_LOSS_GRAD OP\");",
        "comment_created_at": "2021-09-05T02:43:56+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "For performance reasons, I would imagine us wanting all of this behind an if debug. Do you mind implementing this?",
        "pr_file_module": null
      },
      {
        "comment_id": "702355652",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9442,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/ctcloss.cu",
        "discussion_id": "702354863",
        "commented_code": "@@ -189,14 +192,18 @@ namespace platforms {\n         auto outputGrads = OUTPUT_VARIABLE(0);\n         int blankIndex = INT_ARG(0);\n \n-        auto dTypeInput = logitInput->dataType();\n-        auto intType = targetLabelLengths->dataType();\n-        auto dTypeOutput = outputGrads->dataType();\n+        Requirements req(\"CUDNN CTC_LOSS_GRAD OP\");",
        "comment_created_at": "2021-09-05T02:52:51+00:00",
        "comment_author": "quickwritereader",
        "comment_body": "those checks are mandatory. it will just print additional info when debug is enabled ( which may affect performance unnoticeably little as it combine strings to output meaningful messages).",
        "pr_file_module": null
      },
      {
        "comment_id": "702356143",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9442,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/ctcloss.cu",
        "discussion_id": "702354863",
        "commented_code": "@@ -189,14 +192,18 @@ namespace platforms {\n         auto outputGrads = OUTPUT_VARIABLE(0);\n         int blankIndex = INT_ARG(0);\n \n-        auto dTypeInput = logitInput->dataType();\n-        auto intType = targetLabelLengths->dataType();\n-        auto dTypeOutput = outputGrads->dataType();\n+        Requirements req(\"CUDNN CTC_LOSS_GRAD OP\");",
        "comment_created_at": "2021-09-05T02:58:13+00:00",
        "comment_author": "quickwritereader",
        "comment_body": "I wrote it to replace not only platform checks with meaningful messages but also REQUIRE_TRUE macros (which throws invalid op error and log the error detail). it can do the same. I could even make throwing meaningful errors as well instead of invalid op.\r\nhttps://godbolt.org/z/M6b3hdnhK\r\n- it is implicitly bool so it can benefit from short circuit &&. And also work inside if statements and return bool expressions\r\n- it can accept lambdas to lazily evaluate values or expressions as well\r\nmaybe it will not be as fast as REQUIRE_TRUE macros on some compilers but those checks are not used in inner loops to worry about",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "702354915",
    "pr_number": 9442,
    "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/batchnorm.cu",
    "created_at": "2021-09-05T02:44:47+00:00",
    "commented_code": "else\n         axes.push_back(xRank-1);               // default dimension to reduce along is last dimension\n \n-    if(axes.size() != 1 && axes.size() != 3 && axes.size() != 4)\n-        return false;\n-\n-    // *********************************** //\n-    bool allParamsHaveSameShapeAndStrides = shape::haveSameShapeAndStrides(mean->shapeInfo(), variance->shapeInfo());\n-    if(gamma)\n-        allParamsHaveSameShapeAndStrides &= shape::haveSameShapeAndStrides(mean->shapeInfo(), gamma->shapeInfo());\n-    if(gradG)\n-        allParamsHaveSameShapeAndStrides &= shape::haveSameShapeAndStrides(mean->shapeInfo(), gradG->shapeInfo());\n-    if(gradB)\n-        allParamsHaveSameShapeAndStrides &= shape::haveSameShapeAndStrides(mean->shapeInfo(), gradB->shapeInfo());\n-\n-    if(!allParamsHaveSameShapeAndStrides)\n-        return false;\n-\n-    // *********************************** //\n-    bool isFormatGood = false;\n-    if(axes.size() == 1)\n-        isFormatGood = mean->lengthOf() == input->sizeAt(1) || mean->lengthOf() == input->sizeAt(-1);   // mean [C]\n-    else {\n+    Requirements req(\"CUDNN BATCHNORM_BP OP\");\n+    req.expectIn(makeInfoVariable(xRank, RANK_MSG_INPUT0), {4, 5}) &&\n+    req.expectIn(makeInfoVariable(input->dataType(), TYPE_MSG_INPUT0), {DataType::HALF, DataType::FLOAT32, DataType::DOUBLE}) &&\n+    req.expectIn(makeInfoVariable(axes.size(), \"axes.size()\"), {1, 3, 4}) &&\n+    req.expect(makeShapeInfoVariable(mean, SHAPE_MSG_INPUT1), makeShapeInfoVariable(variance, SHAPE_MSG_INPUT2),\n+    [](const decltype(mean)& l, const decltype(variance)& r){\n+        return shape::haveSameShapeAndStrides(l->shapeInfo(), r->shapeInfo());\n+    }\n+    , EXPECTED_EQ_MSG);\n+    if(gamma){\n+        req.expect(makeShapeInfoVariable(gamma, SHAPE_MSG_INPUT_ \"#gamma\"), makeShapeInfoVariable(mean, SHAPE_MSG_INPUT1),\n+        [](const decltype(gamma)& l, const decltype(mean)& r){\n+            return shape::haveSameShapeAndStrides(l->shapeInfo(), r->shapeInfo());\n+        }\n+        , EXPECTED_EQ_MSG);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "702354915",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9442,
        "pr_file": "libnd4j/include/ops/declarable/platform/cudnn/batchnorm.cu",
        "discussion_id": "702354915",
        "commented_code": "@@ -504,34 +497,51 @@ PLATFORM_CHECK(batchnorm_bp, ENGINE_CUDA) {\n     else\n         axes.push_back(xRank-1);               // default dimension to reduce along is last dimension\n \n-    if(axes.size() != 1 && axes.size() != 3 && axes.size() != 4)\n-        return false;\n-\n-    // *********************************** //\n-    bool allParamsHaveSameShapeAndStrides = shape::haveSameShapeAndStrides(mean->shapeInfo(), variance->shapeInfo());\n-    if(gamma)\n-        allParamsHaveSameShapeAndStrides &= shape::haveSameShapeAndStrides(mean->shapeInfo(), gamma->shapeInfo());\n-    if(gradG)\n-        allParamsHaveSameShapeAndStrides &= shape::haveSameShapeAndStrides(mean->shapeInfo(), gradG->shapeInfo());\n-    if(gradB)\n-        allParamsHaveSameShapeAndStrides &= shape::haveSameShapeAndStrides(mean->shapeInfo(), gradB->shapeInfo());\n-\n-    if(!allParamsHaveSameShapeAndStrides)\n-        return false;\n-\n-    // *********************************** //\n-    bool isFormatGood = false;\n-    if(axes.size() == 1)\n-        isFormatGood = mean->lengthOf() == input->sizeAt(1) || mean->lengthOf() == input->sizeAt(-1);   // mean [C]\n-    else {\n+    Requirements req(\"CUDNN BATCHNORM_BP OP\");\n+    req.expectIn(makeInfoVariable(xRank, RANK_MSG_INPUT0), {4, 5}) &&\n+    req.expectIn(makeInfoVariable(input->dataType(), TYPE_MSG_INPUT0), {DataType::HALF, DataType::FLOAT32, DataType::DOUBLE}) &&\n+    req.expectIn(makeInfoVariable(axes.size(), \"axes.size()\"), {1, 3, 4}) &&\n+    req.expect(makeShapeInfoVariable(mean, SHAPE_MSG_INPUT1), makeShapeInfoVariable(variance, SHAPE_MSG_INPUT2),\n+    [](const decltype(mean)& l, const decltype(variance)& r){\n+        return shape::haveSameShapeAndStrides(l->shapeInfo(), r->shapeInfo());\n+    }\n+    , EXPECTED_EQ_MSG);\n+    if(gamma){\n+        req.expect(makeShapeInfoVariable(gamma, SHAPE_MSG_INPUT_ \"#gamma\"), makeShapeInfoVariable(mean, SHAPE_MSG_INPUT1),\n+        [](const decltype(gamma)& l, const decltype(mean)& r){\n+            return shape::haveSameShapeAndStrides(l->shapeInfo(), r->shapeInfo());\n+        }\n+        , EXPECTED_EQ_MSG);",
        "comment_created_at": "2021-09-05T02:44:47+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "For performance reasons, I would imagine us wanting all of this behind an if debug. Do you mind implementing this?",
        "pr_file_module": null
      }
    ]
  }
]