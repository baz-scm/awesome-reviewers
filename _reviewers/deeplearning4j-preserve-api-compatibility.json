[
  {
    "discussion_id": "683245075",
    "pr_number": 9408,
    "pr_file": "contrib/codegen-tools/codegen/src/main/ops/org/nd4j/codegen/mixins/Mixins.kt",
    "created_at": "2021-08-05T08:32:36+00:00",
    "commented_code": "val reduce = Mixin(\"reduce\"){\n     legacy = true\n     Input(DataType.NUMERIC, \"in\") { description = \"Input variable\" }\n+    Arg(DataType.BOOL,\"keepDims\"){\"Whether to keep the original  dimensions or produce a shrunk array with less dimensions\"}",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "683245075",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9408,
        "pr_file": "contrib/codegen-tools/codegen/src/main/ops/org/nd4j/codegen/mixins/Mixins.kt",
        "discussion_id": "683245075",
        "commented_code": "@@ -94,31 +94,67 @@ val scalar = Mixin(\"scalar\"){\n val reduce = Mixin(\"reduce\"){\n     legacy = true\n     Input(DataType.NUMERIC, \"in\") { description = \"Input variable\" }\n+    Arg(DataType.BOOL,\"keepDims\"){\"Whether to keep the original  dimensions or produce a shrunk array with less dimensions\"}",
        "comment_created_at": "2021-08-05T08:32:36+00:00",
        "comment_author": "treo",
        "comment_body": "When adding new arguments, I think it makes sense to make them optional in order to keep backwards compatibility with existing code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "683246211",
    "pr_number": 9408,
    "pr_file": "contrib/codegen-tools/codegen/src/main/ops/org/nd4j/codegen/mixins/Mixins.kt",
    "created_at": "2021-08-05T08:34:07+00:00",
    "commented_code": "val reduce = Mixin(\"reduce\"){\n     legacy = true\n     Input(DataType.NUMERIC, \"in\") { description = \"Input variable\" }\n+    Arg(DataType.BOOL,\"keepDims\"){\"Whether to keep the original  dimensions or produce a shrunk array with less dimensions\"}\n     Arg(DataType.INT, \"dimensions\"){ count = AtLeast(0); description = \"Dimensions to reduce over. If dimensions are not specified, full array reduction is performed\" }\n     Output(DataType.NUMERIC, \"output\"){ description = \"Reduced array of rank (input rank - num dimensions)\" }\n }\n \n+val reduceVariableDimensions = Mixin(\"reduceVariable\") {\n+    legacy = true\n+    Input(DataType.NUMERIC, \"in\") { description = \"Input variable\" }\n+    Input(DataType.NUMERIC, name = \"dimensions\"){description = \"Dimensions to reduce along\"; defaultValue = null }\n+    Arg(DataType.BOOL,\"keepDims\"){\"Whether to keep the original  dimensions or produce a shrunk array with less dimensions\"}\n+    Output(DataType.NUMERIC, \"output\"){ description = \"Reduced array of rank (input rank - num dimensions)\" }\n+}\n+\n val reduceFloating = Mixin(\"reduceFloating\"){\n     useMixin(reduce)\n-    javaPackage = \"org.nd4j.linalg.api.ops.impl.reduce.floating\"\n+    javaPackage = \"org.nd4j.linalg.api.ops.impl.reduce.floating.custom\"",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "683246211",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9408,
        "pr_file": "contrib/codegen-tools/codegen/src/main/ops/org/nd4j/codegen/mixins/Mixins.kt",
        "discussion_id": "683246211",
        "commented_code": "@@ -94,31 +94,67 @@ val scalar = Mixin(\"scalar\"){\n val reduce = Mixin(\"reduce\"){\n     legacy = true\n     Input(DataType.NUMERIC, \"in\") { description = \"Input variable\" }\n+    Arg(DataType.BOOL,\"keepDims\"){\"Whether to keep the original  dimensions or produce a shrunk array with less dimensions\"}\n     Arg(DataType.INT, \"dimensions\"){ count = AtLeast(0); description = \"Dimensions to reduce over. If dimensions are not specified, full array reduction is performed\" }\n     Output(DataType.NUMERIC, \"output\"){ description = \"Reduced array of rank (input rank - num dimensions)\" }\n }\n \n+val reduceVariableDimensions = Mixin(\"reduceVariable\") {\n+    legacy = true\n+    Input(DataType.NUMERIC, \"in\") { description = \"Input variable\" }\n+    Input(DataType.NUMERIC, name = \"dimensions\"){description = \"Dimensions to reduce along\"; defaultValue = null }\n+    Arg(DataType.BOOL,\"keepDims\"){\"Whether to keep the original  dimensions or produce a shrunk array with less dimensions\"}\n+    Output(DataType.NUMERIC, \"output\"){ description = \"Reduced array of rank (input rank - num dimensions)\" }\n+}\n+\n val reduceFloating = Mixin(\"reduceFloating\"){\n     useMixin(reduce)\n-    javaPackage = \"org.nd4j.linalg.api.ops.impl.reduce.floating\"\n+    javaPackage = \"org.nd4j.linalg.api.ops.impl.reduce.floating.custom\"",
        "comment_created_at": "2021-08-05T08:34:07+00:00",
        "comment_author": "treo",
        "comment_body": "This will probably break some backwards compatibility - at least for people that are using Op objects directly.\r\n\r\nI think we can probably accept that breakage, as most people will be using the factory methods and for them it should be transparent.",
        "pr_file_module": null
      }
    ]
  }
]