[
  {
    "discussion_id": "1274761074",
    "pr_number": 10015,
    "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/segment.cpp",
    "created_at": "2023-07-26T10:32:57+00:00",
    "commented_code": "std::vector<sd::LongType> zeroVec = {0};\n     std::vector<sd::LongType> *restDims = ShapeUtils::evalDimsToExclude(input->rankOf(), 1,zeroVec.data());\n     ResultSet listOfBPTensors = tempRes.allTensorsAlongDimension(*restDims);\n+\n     ResultSet listOfGradOuts = gradOut->allTensorsAlongDimension(*restDims);\n+    gradOut->printAllTensorsAlongDimension(*restDims);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1274761074",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 10015,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/segment.cpp",
        "discussion_id": "1274761074",
        "commented_code": "@@ -614,11 +615,14 @@ sd::Status segmentMaxFunctorBP_(sd::LaunchContext* context, NDArray* input, NDAr\n     std::vector<sd::LongType> zeroVec = {0};\n     std::vector<sd::LongType> *restDims = ShapeUtils::evalDimsToExclude(input->rankOf(), 1,zeroVec.data());\n     ResultSet listOfBPTensors = tempRes.allTensorsAlongDimension(*restDims);\n+\n     ResultSet listOfGradOuts = gradOut->allTensorsAlongDimension(*restDims);\n+    gradOut->printAllTensorsAlongDimension(*restDims);",
        "comment_created_at": "2023-07-26T10:32:57+00:00",
        "comment_author": "treo",
        "comment_body": "Still contains a print.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1233658345",
    "pr_number": 10006,
    "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/triangular_solve.cpp",
    "created_at": "2023-06-19T07:48:37+00:00",
    "commented_code": "}\n BUILD_SINGLE_TEMPLATE(template void triangularSolve2D,\n                       (sd::LaunchContext * context, NDArray const& leftInput, NDArray const& rightInput,\n-                       bool const lower, bool const unitsOnDiag, NDArray& output),\n+                          bool const lower, bool const unitsOnDiag, NDArray& output),\n                       SD_FLOAT_TYPES);\n \n template <typename T>\n static sd::Status triangularSolveFunctor_(sd::LaunchContext* context, NDArray* leftInput, NDArray* rightInput,\n                                           bool lower, bool adjoint, NDArray* output) {\n+  sd_print(\"triangularSolveFunctor_\n\");\n+  leftInput->printIndexedBuffer(\"leftInput\");\n+  rightInput->printIndexedBuffer(\"rightInput\");\n+  output->printIndexedBuffer(\"output\");\n+",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1233658345",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 10006,
        "pr_file": "libnd4j/include/ops/declarable/helpers/cpu/triangular_solve.cpp",
        "discussion_id": "1233658345",
        "commented_code": "@@ -110,18 +109,24 @@ void triangularSolve2D(sd::LaunchContext* context, NDArray const& leftInput, NDA\n }\n BUILD_SINGLE_TEMPLATE(template void triangularSolve2D,\n                       (sd::LaunchContext * context, NDArray const& leftInput, NDArray const& rightInput,\n-                       bool const lower, bool const unitsOnDiag, NDArray& output),\n+                          bool const lower, bool const unitsOnDiag, NDArray& output),\n                       SD_FLOAT_TYPES);\n \n template <typename T>\n static sd::Status triangularSolveFunctor_(sd::LaunchContext* context, NDArray* leftInput, NDArray* rightInput,\n                                           bool lower, bool adjoint, NDArray* output) {\n+  sd_print(\"triangularSolveFunctor_\\n\");\n+  leftInput->printIndexedBuffer(\"leftInput\");\n+  rightInput->printIndexedBuffer(\"rightInput\");\n+  output->printIndexedBuffer(\"output\");\n+",
        "comment_created_at": "2023-06-19T07:48:37+00:00",
        "comment_author": "treo",
        "comment_body": "left over debug printing?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1112993635",
    "pr_number": 9919,
    "pr_file": "libnd4j/include/ops/declarable/generic/blas/batched_gemm.cpp",
    "created_at": "2023-02-21T12:35:45+00:00",
    "commented_code": "namespace ops {\n \n CUSTOM_OP_IMPL(batched_gemm, -1, -1, false, 0, 9) {\n+  sd_printf(\"Before op execution \n\",0);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1112993635",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9919,
        "pr_file": "libnd4j/include/ops/declarable/generic/blas/batched_gemm.cpp",
        "discussion_id": "1112993635",
        "commented_code": "@@ -31,6 +31,7 @@ namespace sd {\n namespace ops {\n \n CUSTOM_OP_IMPL(batched_gemm, -1, -1, false, 0, 9) {\n+  sd_printf(\"Before op execution \\n\",0);",
        "comment_created_at": "2023-02-21T12:35:45+00:00",
        "comment_author": "treo",
        "comment_body": "printf outside of error condition.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1112992159",
    "pr_number": 9918,
    "pr_file": "libnd4j/include/ops/declarable/generic/nn/recurrent/gru.cpp",
    "created_at": "2023-02-21T12:34:07+00:00",
    "commented_code": "const int bS = x->sizeAt(1);\n   const int nIn = x->sizeAt(2);\n   const int nOut = hI->sizeAt(1);\n-\n+  sd_printf(\"Batch size %d nIn %d nOut %d\n\",bS,nIn,nOut);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1112992159",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9918,
        "pr_file": "libnd4j/include/ops/declarable/generic/nn/recurrent/gru.cpp",
        "discussion_id": "1112992159",
        "commented_code": "@@ -85,12 +88,12 @@ DECLARE_SHAPE_FN(gru) {\n   const int bS = x->sizeAt(1);\n   const int nIn = x->sizeAt(2);\n   const int nOut = hI->sizeAt(1);\n-\n+  sd_printf(\"Batch size %d nIn %d nOut %d\\n\",bS,nIn,nOut);",
        "comment_created_at": "2023-02-21T12:34:07+00:00",
        "comment_author": "treo",
        "comment_body": "printf outside of error condition.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1099878153",
    "pr_number": 9907,
    "pr_file": "libnd4j/include/array/impl/NDArrayFactory.cpp",
    "created_at": "2023-02-08T09:35:48+00:00",
    "commented_code": "template <typename T>\n NDArray NDArrayFactory::create(T* buffer, const char order, const std::initializer_list<sd::LongType>& shape,\n                                sd::LaunchContext* context) {\n+  sd_printf(\"In arrLength usage at  NDArrayFactory::create(T* buffer, const char order, const std::initializer_list<sd::LongType>& shape,\n\"",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1099878153",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9907,
        "pr_file": "libnd4j/include/array/impl/NDArrayFactory.cpp",
        "discussion_id": "1099878153",
        "commented_code": "@@ -635,17 +644,19 @@ NDArray* NDArrayFactory::create_(const char order, const std::vector<sd::LongTyp\n template <typename T>\n NDArray NDArrayFactory::create(T* buffer, const char order, const std::initializer_list<sd::LongType>& shape,\n                                sd::LaunchContext* context) {\n+  sd_printf(\"In arrLength usage at  NDArrayFactory::create(T* buffer, const char order, const std::initializer_list<sd::LongType>& shape,\\n\"",
        "comment_created_at": "2023-02-08T09:35:48+00:00",
        "comment_author": "treo",
        "comment_body": "printf outside of error condition.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1092999018",
    "pr_number": 9900,
    "pr_file": "libnd4j/include/ops/declarable/generic/nlp/cbow.cpp",
    "created_at": "2023-02-01T09:56:50+00:00",
    "commented_code": "const std::vector<sd::LongType> *contextShape = contextSize;\n \n   std::vector<sd::LongType> *lockedWordsSize = new std::vector<sd::LongType>();\n-  contextSize->push_back(lockedWords->size());\n+  lockedWordsSize->push_back(lockedWords->size());\n   const std::vector<sd::LongType> *lockedWordsShape = lockedWordsSize;\n \n+  //sd_printf(\"Before converting args to ndarrays\n\",0);\n \n-  auto indicesArrOne = NDArrayFactory::create('c',*indicesShape,*indicesVec);\n+  auto indicesArrOne = indicesVec->size() > 0 ? NDArrayFactory::create('c',*indicesShape,*indicesVec) : NDArrayFactory::empty<sd::LongType>();\n   auto indicesArr = new NDArray(indicesArrOne);\n+  //sd_printf(\"After converting args to ndarrays indices\n\",0);\n \n-  auto codesArrOne = NDArrayFactory::create('c',*codesShape,*codesVec);\n+  auto codesArrOne = codesVec->size() > 0 ?  NDArrayFactory::create('c',*codesShape,*codesVec) :  NDArrayFactory::empty<sd::LongType>();\n   auto codesArr = new NDArray(codesArrOne);\n+  //sd_printf(\"After converting args to ndarrays codes\n\",0);\n \n-  auto contextArrOne = NDArrayFactory::create('c',*contextShape,*contextVec);\n+  //sd_printf(\"Context arr size %d Context shape entry is %d\n\",context->size(),contextShape->at(0));\n+  //sd_printf(\"Context: elements: \",0);\n+  for(int i = 0; i < context->size(); i++) {\n+    //sd_printf(\" %d \",context->at(i));\n+  }\n+  //sd_printf(\"\n\",0);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1092999018",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9900,
        "pr_file": "libnd4j/include/ops/declarable/generic/nlp/cbow.cpp",
        "discussion_id": "1092999018",
        "commented_code": "@@ -89,30 +89,42 @@ CONFIGURABLE_OP_IMPL(cbow_inference, 6, 6, true, -2, -2) {\n   const std::vector<sd::LongType> *contextShape = contextSize;\n \n   std::vector<sd::LongType> *lockedWordsSize = new std::vector<sd::LongType>();\n-  contextSize->push_back(lockedWords->size());\n+  lockedWordsSize->push_back(lockedWords->size());\n   const std::vector<sd::LongType> *lockedWordsShape = lockedWordsSize;\n \n+  //sd_printf(\"Before converting args to ndarrays\\n\",0);\n \n-  auto indicesArrOne = NDArrayFactory::create('c',*indicesShape,*indicesVec);\n+  auto indicesArrOne = indicesVec->size() > 0 ? NDArrayFactory::create('c',*indicesShape,*indicesVec) : NDArrayFactory::empty<sd::LongType>();\n   auto indicesArr = new NDArray(indicesArrOne);\n+  //sd_printf(\"After converting args to ndarrays indices\\n\",0);\n \n-  auto codesArrOne = NDArrayFactory::create('c',*codesShape,*codesVec);\n+  auto codesArrOne = codesVec->size() > 0 ?  NDArrayFactory::create('c',*codesShape,*codesVec) :  NDArrayFactory::empty<sd::LongType>();\n   auto codesArr = new NDArray(codesArrOne);\n+  //sd_printf(\"After converting args to ndarrays codes\\n\",0);\n \n-  auto contextArrOne = NDArrayFactory::create('c',*contextShape,*contextVec);\n+  //sd_printf(\"Context arr size %d Context shape entry is %d\\n\",context->size(),contextShape->at(0));\n+  //sd_printf(\"Context: elements: \",0);\n+  for(int i = 0; i < context->size(); i++) {\n+    //sd_printf(\" %d \",context->at(i));\n+  }\n+  //sd_printf(\"\\n\",0);",
        "comment_created_at": "2023-02-01T09:56:50+00:00",
        "comment_author": "treo",
        "comment_body": "Let's not have commented out code like that. And in particular not have a for loop doing nothing. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1093011042",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9900,
        "pr_file": "libnd4j/include/ops/declarable/generic/nlp/cbow.cpp",
        "discussion_id": "1092999018",
        "commented_code": "@@ -89,30 +89,42 @@ CONFIGURABLE_OP_IMPL(cbow_inference, 6, 6, true, -2, -2) {\n   const std::vector<sd::LongType> *contextShape = contextSize;\n \n   std::vector<sd::LongType> *lockedWordsSize = new std::vector<sd::LongType>();\n-  contextSize->push_back(lockedWords->size());\n+  lockedWordsSize->push_back(lockedWords->size());\n   const std::vector<sd::LongType> *lockedWordsShape = lockedWordsSize;\n \n+  //sd_printf(\"Before converting args to ndarrays\\n\",0);\n \n-  auto indicesArrOne = NDArrayFactory::create('c',*indicesShape,*indicesVec);\n+  auto indicesArrOne = indicesVec->size() > 0 ? NDArrayFactory::create('c',*indicesShape,*indicesVec) : NDArrayFactory::empty<sd::LongType>();\n   auto indicesArr = new NDArray(indicesArrOne);\n+  //sd_printf(\"After converting args to ndarrays indices\\n\",0);\n \n-  auto codesArrOne = NDArrayFactory::create('c',*codesShape,*codesVec);\n+  auto codesArrOne = codesVec->size() > 0 ?  NDArrayFactory::create('c',*codesShape,*codesVec) :  NDArrayFactory::empty<sd::LongType>();\n   auto codesArr = new NDArray(codesArrOne);\n+  //sd_printf(\"After converting args to ndarrays codes\\n\",0);\n \n-  auto contextArrOne = NDArrayFactory::create('c',*contextShape,*contextVec);\n+  //sd_printf(\"Context arr size %d Context shape entry is %d\\n\",context->size(),contextShape->at(0));\n+  //sd_printf(\"Context: elements: \",0);\n+  for(int i = 0; i < context->size(); i++) {\n+    //sd_printf(\" %d \",context->at(i));\n+  }\n+  //sd_printf(\"\\n\",0);",
        "comment_created_at": "2023-02-01T10:07:29+00:00",
        "comment_author": "agibsonccc",
        "comment_body": "Fixed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1083258618",
    "pr_number": 9892,
    "pr_file": "libnd4j/include/legacy/impl/cnpy.cpp",
    "created_at": "2023-01-21T07:48:15+00:00",
    "commented_code": "const int st = 10;\n   const int ti = 22;\n   const int si = 23;\n-\n+  printf(\"Reading datatype from header %s\n\",data);",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "1083258618",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9892,
        "pr_file": "libnd4j/include/legacy/impl/cnpy.cpp",
        "discussion_id": "1083258618",
        "commented_code": "@@ -111,7 +111,7 @@ sd::DataType cnpy::dataTypeFromHeader(char *data) {\n   const int st = 10;\n   const int ti = 22;\n   const int si = 23;\n-\n+  printf(\"Reading datatype from header %s\\n\",data);",
        "comment_created_at": "2023-01-21T07:48:15+00:00",
        "comment_author": "treo",
        "comment_body": "Unconditional printf. I guess a leftover from debugging?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "744551758",
    "pr_number": 9514,
    "pr_file": "libnd4j/include/ops/declarable/generic/transforms/scatter_add.cpp",
    "created_at": "2021-11-08T09:41:30+00:00",
    "commented_code": "namespace ops {\n \n OP_IMPL(scatter_add, 3, 1, true) {\n-  auto input = INPUT_VARIABLE(0);\n-  auto indices = INPUT_VARIABLE(1);\n-  auto updates = INPUT_VARIABLE(2);\n-\n-  auto output = OUTPUT_VARIABLE(0);\n-\n-  if (!block.isInplace()) output->assign(input);\n-\n-  const bool lock = block.getBArguments()->empty() ? false : B_ARG(0);\n-  const bool checkIndices = block.getBArguments()->size() <= 1 ? false : B_ARG(1);\n-\n-  const int inRank = input->rankOf();\n-  const int indRank = indices->rankOf();\n-  const int updRank = updates->rankOf();\n-  const sd::LongType indLen = indices->lengthOf();\n-\n-  REQUIRE_TRUE(inRank > 0, 0, \"SCATTER_ADD OP: input should not be scalar !\");\n-\n-  if (inRank == 1) {\n-    REQUIRE_TRUE(indices->isSameShape(updates), 0,\n-                 \"SCATTER_ADD OP: when input array has rank = 1 then indices and updates must have the same shapes, \"\n-                 \"but got %s and %s correspondingly !\",\n-                 ShapeUtils::shapeAsString(indices).c_str(), ShapeUtils::shapeAsString(updates).c_str());\n-  } else if (inRank == updRank && indices->isVector()) {\n-    std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n-    std::vector<sd::LongType> inShape = input->getShapeAsVector();\n-    std::vector<sd::LongType> expectedUpdShape = {indices->lengthOf()};\n-    expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + 1, inShape.end());\n-\n-    REQUIRE_TRUE(expectedUpdShape == updShape, 0,\n-                 \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\",\n-                 ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n-  } else {\n-    REQUIRE_TRUE(updRank == indRank + inRank - 1, 0,\n-                 \"SCATTER_ADD OP: wrong rank of updates array, expected is %i, but got %i instead !\",\n-                 indRank + inRank - 1, updRank);\n-\n-    std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n-    std::vector<sd::LongType> inShape = input->getShapeAsVector();\n-    std::vector<sd::LongType> expectedUpdShape = indices->getShapeAsVector();\n-    expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + sd::LongType(1L), inShape.end());\n-\n-    REQUIRE_TRUE(expectedUpdShape == updShape, 0,\n-                 \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\",\n-                 ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n-  }\n-\n-  if (!indices->isEmpty()) {\n-    if (checkIndices) {\n-      const sd::LongType numOfBadIndx = helpers::checkIndices(block.launchContext(), *indices, *output, 0);\n-      REQUIRE_TRUE(numOfBadIndx == 0, 0,\n-                   \"SCATTER_ADD OP: please check elements of indices-array, total number of wrong elements is %lld!\",\n-                   numOfBadIndx);\n+    auto input = INPUT_VARIABLE(0);\n+    auto indices = INPUT_VARIABLE(1);\n+    auto updates = INPUT_VARIABLE(2);\n+\n+    auto output = OUTPUT_VARIABLE(0);\n+\n+    if (!block.isInplace())\n+        output->assign(input);\n+\n+    const bool lock = block.getBArguments()->empty() ? false : B_ARG(0);\n+    const bool checkIndices = block.getBArguments()->size() <= 1 ? false : B_ARG(1);\n+\n+    const int inRank  = input->rankOf();\n+    const int indRank = indices->rankOf();\n+    const int updRank = updates->rankOf();\n+    const sd::LongType indLen = indices->lengthOf();\n+\n+    REQUIRE_TRUE(inRank > 0, 0, \"SCATTER_ADD OP: input should not be scalar !\");\n+\n+    if(inRank == 1) {\n+        REQUIRE_TRUE(indices->isSameShape(updates), 0, \"SCATTER_ADD OP: when input array has rank = 1 then indices and updates must have the same shapes, but got %s and %s correspondingly !\", ShapeUtils::shapeAsString(indices).c_str(), ShapeUtils::shapeAsString(updates).c_str());\n+    }\n+    else if (inRank == updRank && indices->isVector()) {\n+        std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n+        std::vector<sd::LongType> inShape  = input->getShapeAsVector();\n+        std::vector<sd::LongType> expectedUpdShape = {indices->lengthOf()};\n+        expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin()+1, inShape.end());\n+\n+        //REQUIRE_TRUE(expectedUpdShape == updShape, 0, \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\", ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "744551758",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9514,
        "pr_file": "libnd4j/include/ops/declarable/generic/transforms/scatter_add.cpp",
        "discussion_id": "744551758",
        "commented_code": "@@ -31,78 +31,72 @@ namespace sd {\n namespace ops {\n \n OP_IMPL(scatter_add, 3, 1, true) {\n-  auto input = INPUT_VARIABLE(0);\n-  auto indices = INPUT_VARIABLE(1);\n-  auto updates = INPUT_VARIABLE(2);\n-\n-  auto output = OUTPUT_VARIABLE(0);\n-\n-  if (!block.isInplace()) output->assign(input);\n-\n-  const bool lock = block.getBArguments()->empty() ? false : B_ARG(0);\n-  const bool checkIndices = block.getBArguments()->size() <= 1 ? false : B_ARG(1);\n-\n-  const int inRank = input->rankOf();\n-  const int indRank = indices->rankOf();\n-  const int updRank = updates->rankOf();\n-  const sd::LongType indLen = indices->lengthOf();\n-\n-  REQUIRE_TRUE(inRank > 0, 0, \"SCATTER_ADD OP: input should not be scalar !\");\n-\n-  if (inRank == 1) {\n-    REQUIRE_TRUE(indices->isSameShape(updates), 0,\n-                 \"SCATTER_ADD OP: when input array has rank = 1 then indices and updates must have the same shapes, \"\n-                 \"but got %s and %s correspondingly !\",\n-                 ShapeUtils::shapeAsString(indices).c_str(), ShapeUtils::shapeAsString(updates).c_str());\n-  } else if (inRank == updRank && indices->isVector()) {\n-    std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n-    std::vector<sd::LongType> inShape = input->getShapeAsVector();\n-    std::vector<sd::LongType> expectedUpdShape = {indices->lengthOf()};\n-    expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + 1, inShape.end());\n-\n-    REQUIRE_TRUE(expectedUpdShape == updShape, 0,\n-                 \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\",\n-                 ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n-  } else {\n-    REQUIRE_TRUE(updRank == indRank + inRank - 1, 0,\n-                 \"SCATTER_ADD OP: wrong rank of updates array, expected is %i, but got %i instead !\",\n-                 indRank + inRank - 1, updRank);\n-\n-    std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n-    std::vector<sd::LongType> inShape = input->getShapeAsVector();\n-    std::vector<sd::LongType> expectedUpdShape = indices->getShapeAsVector();\n-    expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + sd::LongType(1L), inShape.end());\n-\n-    REQUIRE_TRUE(expectedUpdShape == updShape, 0,\n-                 \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\",\n-                 ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n-  }\n-\n-  if (!indices->isEmpty()) {\n-    if (checkIndices) {\n-      const sd::LongType numOfBadIndx = helpers::checkIndices(block.launchContext(), *indices, *output, 0);\n-      REQUIRE_TRUE(numOfBadIndx == 0, 0,\n-                   \"SCATTER_ADD OP: please check elements of indices-array, total number of wrong elements is %lld!\",\n-                   numOfBadIndx);\n+    auto input = INPUT_VARIABLE(0);\n+    auto indices = INPUT_VARIABLE(1);\n+    auto updates = INPUT_VARIABLE(2);\n+\n+    auto output = OUTPUT_VARIABLE(0);\n+\n+    if (!block.isInplace())\n+        output->assign(input);\n+\n+    const bool lock = block.getBArguments()->empty() ? false : B_ARG(0);\n+    const bool checkIndices = block.getBArguments()->size() <= 1 ? false : B_ARG(1);\n+\n+    const int inRank  = input->rankOf();\n+    const int indRank = indices->rankOf();\n+    const int updRank = updates->rankOf();\n+    const sd::LongType indLen = indices->lengthOf();\n+\n+    REQUIRE_TRUE(inRank > 0, 0, \"SCATTER_ADD OP: input should not be scalar !\");\n+\n+    if(inRank == 1) {\n+        REQUIRE_TRUE(indices->isSameShape(updates), 0, \"SCATTER_ADD OP: when input array has rank = 1 then indices and updates must have the same shapes, but got %s and %s correspondingly !\", ShapeUtils::shapeAsString(indices).c_str(), ShapeUtils::shapeAsString(updates).c_str());\n+    }\n+    else if (inRank == updRank && indices->isVector()) {\n+        std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n+        std::vector<sd::LongType> inShape  = input->getShapeAsVector();\n+        std::vector<sd::LongType> expectedUpdShape = {indices->lengthOf()};\n+        expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin()+1, inShape.end());\n+\n+        //REQUIRE_TRUE(expectedUpdShape == updShape, 0, \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\", ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());",
        "comment_created_at": "2021-11-08T09:41:30+00:00",
        "comment_author": "treo",
        "comment_body": "Please don't leave any commented out code around. Either remove it or keep it, but just commenting things out without any indication for why that is, shouldn't make it into a merged PR.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "744552073",
    "pr_number": 9514,
    "pr_file": "libnd4j/include/ops/declarable/generic/transforms/scatter_add.cpp",
    "created_at": "2021-11-08T09:41:50+00:00",
    "commented_code": "namespace ops {\n \n OP_IMPL(scatter_add, 3, 1, true) {\n-  auto input = INPUT_VARIABLE(0);\n-  auto indices = INPUT_VARIABLE(1);\n-  auto updates = INPUT_VARIABLE(2);\n-\n-  auto output = OUTPUT_VARIABLE(0);\n-\n-  if (!block.isInplace()) output->assign(input);\n-\n-  const bool lock = block.getBArguments()->empty() ? false : B_ARG(0);\n-  const bool checkIndices = block.getBArguments()->size() <= 1 ? false : B_ARG(1);\n-\n-  const int inRank = input->rankOf();\n-  const int indRank = indices->rankOf();\n-  const int updRank = updates->rankOf();\n-  const sd::LongType indLen = indices->lengthOf();\n-\n-  REQUIRE_TRUE(inRank > 0, 0, \"SCATTER_ADD OP: input should not be scalar !\");\n-\n-  if (inRank == 1) {\n-    REQUIRE_TRUE(indices->isSameShape(updates), 0,\n-                 \"SCATTER_ADD OP: when input array has rank = 1 then indices and updates must have the same shapes, \"\n-                 \"but got %s and %s correspondingly !\",\n-                 ShapeUtils::shapeAsString(indices).c_str(), ShapeUtils::shapeAsString(updates).c_str());\n-  } else if (inRank == updRank && indices->isVector()) {\n-    std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n-    std::vector<sd::LongType> inShape = input->getShapeAsVector();\n-    std::vector<sd::LongType> expectedUpdShape = {indices->lengthOf()};\n-    expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + 1, inShape.end());\n-\n-    REQUIRE_TRUE(expectedUpdShape == updShape, 0,\n-                 \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\",\n-                 ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n-  } else {\n-    REQUIRE_TRUE(updRank == indRank + inRank - 1, 0,\n-                 \"SCATTER_ADD OP: wrong rank of updates array, expected is %i, but got %i instead !\",\n-                 indRank + inRank - 1, updRank);\n-\n-    std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n-    std::vector<sd::LongType> inShape = input->getShapeAsVector();\n-    std::vector<sd::LongType> expectedUpdShape = indices->getShapeAsVector();\n-    expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + sd::LongType(1L), inShape.end());\n-\n-    REQUIRE_TRUE(expectedUpdShape == updShape, 0,\n-                 \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\",\n-                 ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n-  }\n-\n-  if (!indices->isEmpty()) {\n-    if (checkIndices) {\n-      const sd::LongType numOfBadIndx = helpers::checkIndices(block.launchContext(), *indices, *output, 0);\n-      REQUIRE_TRUE(numOfBadIndx == 0, 0,\n-                   \"SCATTER_ADD OP: please check elements of indices-array, total number of wrong elements is %lld!\",\n-                   numOfBadIndx);\n+    auto input = INPUT_VARIABLE(0);\n+    auto indices = INPUT_VARIABLE(1);\n+    auto updates = INPUT_VARIABLE(2);\n+\n+    auto output = OUTPUT_VARIABLE(0);\n+\n+    if (!block.isInplace())\n+        output->assign(input);\n+\n+    const bool lock = block.getBArguments()->empty() ? false : B_ARG(0);\n+    const bool checkIndices = block.getBArguments()->size() <= 1 ? false : B_ARG(1);\n+\n+    const int inRank  = input->rankOf();\n+    const int indRank = indices->rankOf();\n+    const int updRank = updates->rankOf();\n+    const sd::LongType indLen = indices->lengthOf();\n+\n+    REQUIRE_TRUE(inRank > 0, 0, \"SCATTER_ADD OP: input should not be scalar !\");\n+\n+    if(inRank == 1) {\n+        REQUIRE_TRUE(indices->isSameShape(updates), 0, \"SCATTER_ADD OP: when input array has rank = 1 then indices and updates must have the same shapes, but got %s and %s correspondingly !\", ShapeUtils::shapeAsString(indices).c_str(), ShapeUtils::shapeAsString(updates).c_str());\n+    }\n+    else if (inRank == updRank && indices->isVector()) {\n+        std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n+        std::vector<sd::LongType> inShape  = input->getShapeAsVector();\n+        std::vector<sd::LongType> expectedUpdShape = {indices->lengthOf()};\n+        expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin()+1, inShape.end());\n+\n+        //REQUIRE_TRUE(expectedUpdShape == updShape, 0, \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\", ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n     }\n+    else {\n+\n+        REQUIRE_TRUE(updRank == indRank + inRank - 1, 0, \"SCATTER_ADD OP: wrong rank of updates array, expected is %i, but got %i instead !\", indRank + inRank - 1 , updRank);\n \n-    helpers::scatter(block.launchContext(), pairwise::Add, *indices, *updates, *output, lock);\n-  }\n+        std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n+        std::vector<sd::LongType> inShape  = input->getShapeAsVector();\n+        std::vector<sd::LongType> expectedUpdShape = indices->getShapeAsVector();\n+        expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + sd::LongType(1L), inShape.end());\n \n-  return sd::Status::OK;\n+        //REQUIRE_TRUE(expectedUpdShape == updShape, 0, \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\", ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());",
    "repo_full_name": "deeplearning4j/deeplearning4j",
    "discussion_comments": [
      {
        "comment_id": "744552073",
        "repo_full_name": "deeplearning4j/deeplearning4j",
        "pr_number": 9514,
        "pr_file": "libnd4j/include/ops/declarable/generic/transforms/scatter_add.cpp",
        "discussion_id": "744552073",
        "commented_code": "@@ -31,78 +31,72 @@ namespace sd {\n namespace ops {\n \n OP_IMPL(scatter_add, 3, 1, true) {\n-  auto input = INPUT_VARIABLE(0);\n-  auto indices = INPUT_VARIABLE(1);\n-  auto updates = INPUT_VARIABLE(2);\n-\n-  auto output = OUTPUT_VARIABLE(0);\n-\n-  if (!block.isInplace()) output->assign(input);\n-\n-  const bool lock = block.getBArguments()->empty() ? false : B_ARG(0);\n-  const bool checkIndices = block.getBArguments()->size() <= 1 ? false : B_ARG(1);\n-\n-  const int inRank = input->rankOf();\n-  const int indRank = indices->rankOf();\n-  const int updRank = updates->rankOf();\n-  const sd::LongType indLen = indices->lengthOf();\n-\n-  REQUIRE_TRUE(inRank > 0, 0, \"SCATTER_ADD OP: input should not be scalar !\");\n-\n-  if (inRank == 1) {\n-    REQUIRE_TRUE(indices->isSameShape(updates), 0,\n-                 \"SCATTER_ADD OP: when input array has rank = 1 then indices and updates must have the same shapes, \"\n-                 \"but got %s and %s correspondingly !\",\n-                 ShapeUtils::shapeAsString(indices).c_str(), ShapeUtils::shapeAsString(updates).c_str());\n-  } else if (inRank == updRank && indices->isVector()) {\n-    std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n-    std::vector<sd::LongType> inShape = input->getShapeAsVector();\n-    std::vector<sd::LongType> expectedUpdShape = {indices->lengthOf()};\n-    expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + 1, inShape.end());\n-\n-    REQUIRE_TRUE(expectedUpdShape == updShape, 0,\n-                 \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\",\n-                 ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n-  } else {\n-    REQUIRE_TRUE(updRank == indRank + inRank - 1, 0,\n-                 \"SCATTER_ADD OP: wrong rank of updates array, expected is %i, but got %i instead !\",\n-                 indRank + inRank - 1, updRank);\n-\n-    std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n-    std::vector<sd::LongType> inShape = input->getShapeAsVector();\n-    std::vector<sd::LongType> expectedUpdShape = indices->getShapeAsVector();\n-    expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + sd::LongType(1L), inShape.end());\n-\n-    REQUIRE_TRUE(expectedUpdShape == updShape, 0,\n-                 \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\",\n-                 ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n-  }\n-\n-  if (!indices->isEmpty()) {\n-    if (checkIndices) {\n-      const sd::LongType numOfBadIndx = helpers::checkIndices(block.launchContext(), *indices, *output, 0);\n-      REQUIRE_TRUE(numOfBadIndx == 0, 0,\n-                   \"SCATTER_ADD OP: please check elements of indices-array, total number of wrong elements is %lld!\",\n-                   numOfBadIndx);\n+    auto input = INPUT_VARIABLE(0);\n+    auto indices = INPUT_VARIABLE(1);\n+    auto updates = INPUT_VARIABLE(2);\n+\n+    auto output = OUTPUT_VARIABLE(0);\n+\n+    if (!block.isInplace())\n+        output->assign(input);\n+\n+    const bool lock = block.getBArguments()->empty() ? false : B_ARG(0);\n+    const bool checkIndices = block.getBArguments()->size() <= 1 ? false : B_ARG(1);\n+\n+    const int inRank  = input->rankOf();\n+    const int indRank = indices->rankOf();\n+    const int updRank = updates->rankOf();\n+    const sd::LongType indLen = indices->lengthOf();\n+\n+    REQUIRE_TRUE(inRank > 0, 0, \"SCATTER_ADD OP: input should not be scalar !\");\n+\n+    if(inRank == 1) {\n+        REQUIRE_TRUE(indices->isSameShape(updates), 0, \"SCATTER_ADD OP: when input array has rank = 1 then indices and updates must have the same shapes, but got %s and %s correspondingly !\", ShapeUtils::shapeAsString(indices).c_str(), ShapeUtils::shapeAsString(updates).c_str());\n+    }\n+    else if (inRank == updRank && indices->isVector()) {\n+        std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n+        std::vector<sd::LongType> inShape  = input->getShapeAsVector();\n+        std::vector<sd::LongType> expectedUpdShape = {indices->lengthOf()};\n+        expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin()+1, inShape.end());\n+\n+        //REQUIRE_TRUE(expectedUpdShape == updShape, 0, \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\", ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());\n     }\n+    else {\n+\n+        REQUIRE_TRUE(updRank == indRank + inRank - 1, 0, \"SCATTER_ADD OP: wrong rank of updates array, expected is %i, but got %i instead !\", indRank + inRank - 1 , updRank);\n \n-    helpers::scatter(block.launchContext(), pairwise::Add, *indices, *updates, *output, lock);\n-  }\n+        std::vector<sd::LongType> updShape = updates->getShapeAsVector();\n+        std::vector<sd::LongType> inShape  = input->getShapeAsVector();\n+        std::vector<sd::LongType> expectedUpdShape = indices->getShapeAsVector();\n+        expectedUpdShape.insert(expectedUpdShape.end(), inShape.begin() + sd::LongType(1L), inShape.end());\n \n-  return sd::Status::OK;\n+        //REQUIRE_TRUE(expectedUpdShape == updShape, 0, \"SCATTER_ADD OP: wrong shape of updates array, expected is %s, but got %s instead !\", ShapeUtils::shapeAsString(expectedUpdShape).c_str(), ShapeUtils::shapeAsString(updShape).c_str());",
        "comment_created_at": "2021-11-08T09:41:50+00:00",
        "comment_author": "treo",
        "comment_body": "Please don't leave any commented out code around",
        "pr_file_module": null
      }
    ]
  }
]