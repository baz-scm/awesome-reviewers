[
  {
    "discussion_id": "2273701279",
    "pr_number": 501,
    "pr_file": "src/config/configuration.py",
    "created_at": "2025-08-13T14:44:52+00:00",
    "commented_code": "mcp_settings: dict = None  # MCP settings, including dynamic loaded tools\n     report_style: str = ReportStyle.ACADEMIC.value  # Report style\n     enable_deep_thinking: bool = False  # Whether to enable deep thinking\n+    mcp_planner_integration: bool = True  # Whether to enable MCP tool integration in planner",
    "repo_full_name": "bytedance/deer-flow",
    "discussion_comments": [
      {
        "comment_id": "2273701279",
        "repo_full_name": "bytedance/deer-flow",
        "pr_number": 501,
        "pr_file": "src/config/configuration.py",
        "discussion_id": "2273701279",
        "commented_code": "@@ -58,6 +58,32 @@ class Configuration:\n     mcp_settings: dict = None  # MCP settings, including dynamic loaded tools\n     report_style: str = ReportStyle.ACADEMIC.value  # Report style\n     enable_deep_thinking: bool = False  # Whether to enable deep thinking\n+    mcp_planner_integration: bool = True  # Whether to enable MCP tool integration in planner",
        "comment_created_at": "2025-08-13T14:44:52+00:00",
        "comment_author": "WillemJiang",
        "comment_body": "We disable the MCP feature by default, the feature can be enable from environment configuration.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089989840",
    "pr_number": 138,
    "pr_file": "src/graph/nodes.py",
    "created_at": "2025-05-15T01:17:43+00:00",
    "commented_code": ")\n \n     # Invoke the agent\n-    result = await agent.ainvoke(input=agent_input)\n+    try:\n+        recursion_limit = int(os.getenv(\"AGENT_RECURSION_LIMIT\", \"25\"))\n+        print(\"Recursion limit is set:\",recursion_limit)",
    "repo_full_name": "bytedance/deer-flow",
    "discussion_comments": [
      {
        "comment_id": "2089989840",
        "repo_full_name": "bytedance/deer-flow",
        "pr_number": 138,
        "pr_file": "src/graph/nodes.py",
        "discussion_id": "2089989840",
        "commented_code": "@@ -333,7 +334,17 @@ async def _execute_agent_step(\n         )\n \n     # Invoke the agent\n-    result = await agent.ainvoke(input=agent_input)\n+    try:\n+        recursion_limit = int(os.getenv(\"AGENT_RECURSION_LIMIT\", \"25\"))\n+        print(\"Recursion limit is set:\",recursion_limit)",
        "comment_created_at": "2025-05-15T01:17:43+00:00",
        "comment_author": "WillemJiang",
        "comment_body": "Please use logger infor to print the setting message, and we need to check if the recession_limit within a reasonalbe range (greater than zero).\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2090047521",
        "repo_full_name": "bytedance/deer-flow",
        "pr_number": 138,
        "pr_file": "src/graph/nodes.py",
        "discussion_id": "2089989840",
        "commented_code": "@@ -333,7 +334,17 @@ async def _execute_agent_step(\n         )\n \n     # Invoke the agent\n-    result = await agent.ainvoke(input=agent_input)\n+    try:\n+        recursion_limit = int(os.getenv(\"AGENT_RECURSION_LIMIT\", \"25\"))\n+        print(\"Recursion limit is set:\",recursion_limit)",
        "comment_created_at": "2025-05-15T01:48:50+00:00",
        "comment_author": "changqingla",
        "comment_body": "Good point.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2092308236",
    "pr_number": 107,
    "pr_file": "src/llms/llm.py",
    "created_at": "2025-05-16T04:41:12+00:00",
    "commented_code": "from pathlib import Path\n from typing import Any, Dict\n \n-from langchain_openai import ChatOpenAI\n+from langchain_core.language_models import BaseChatModel\n+from langchain_openai import ChatOpenAI, AzureChatOpenAI\n \n from src.config import load_yaml_config\n from src.config.agents import LLMType\n+import os\n \n # Cache for LLM instances\n-_llm_cache: dict[LLMType, ChatOpenAI] = {}\n+_llm_cache: Dict[str, BaseChatModel] = {}\n \n \n-def _create_llm_use_conf(llm_type: LLMType, conf: Dict[str, Any]) -> ChatOpenAI:\n-    llm_type_map = {\n+def _create_llm_use_conf(llm_type: LLMType, conf: Dict[str, Any]) -> BaseChatModel:\n+    basic = conf.get(\"BASIC_MODEL\")\n+    openai_basic = basic and basic.get(\"OPENAI\")\n+    azure_basic = basic and basic.get(\"AZURE\")\n+\n+    llm_config = {\n         \"reasoning\": conf.get(\"REASONING_MODEL\"),\n-        \"basic\": conf.get(\"BASIC_MODEL\"),\n+        \"basic\": openai_basic or azure_basic,\n         \"vision\": conf.get(\"VISION_MODEL\"),\n-    }\n-    llm_conf = llm_type_map.get(llm_type)\n-    if not llm_conf:\n+    }.get(llm_type)\n+\n+    if not llm_config:\n         raise ValueError(f\"Unknown LLM type: {llm_type}\")\n-    if not isinstance(llm_conf, dict):\n+    if not isinstance(llm_config, dict):\n         raise ValueError(f\"Invalid LLM Conf: {llm_type}\")\n-    return ChatOpenAI(**llm_conf)\n+\n+    if openai_basic is not None:\n+        return ChatOpenAI(**llm_config)\n+\n+    os.environ.update({\n+        \"OPENAI_API_VERSION\": llm_config.get(\"api_version\"),\n+        \"AZURE_OPENAI_API_KEY\": llm_config.get(\"api_key\"),\n+        \"AZURE_OPENAI_ENDPOINT\": llm_config.get(\"base_url\")\n+    })",
    "repo_full_name": "bytedance/deer-flow",
    "discussion_comments": [
      {
        "comment_id": "2092308236",
        "repo_full_name": "bytedance/deer-flow",
        "pr_number": 107,
        "pr_file": "src/llms/llm.py",
        "discussion_id": "2092308236",
        "commented_code": "@@ -4,32 +4,47 @@\n from pathlib import Path\n from typing import Any, Dict\n \n-from langchain_openai import ChatOpenAI\n+from langchain_core.language_models import BaseChatModel\n+from langchain_openai import ChatOpenAI, AzureChatOpenAI\n \n from src.config import load_yaml_config\n from src.config.agents import LLMType\n+import os\n \n # Cache for LLM instances\n-_llm_cache: dict[LLMType, ChatOpenAI] = {}\n+_llm_cache: Dict[str, BaseChatModel] = {}\n \n \n-def _create_llm_use_conf(llm_type: LLMType, conf: Dict[str, Any]) -> ChatOpenAI:\n-    llm_type_map = {\n+def _create_llm_use_conf(llm_type: LLMType, conf: Dict[str, Any]) -> BaseChatModel:\n+    basic = conf.get(\"BASIC_MODEL\")\n+    openai_basic = basic and basic.get(\"OPENAI\")\n+    azure_basic = basic and basic.get(\"AZURE\")\n+\n+    llm_config = {\n         \"reasoning\": conf.get(\"REASONING_MODEL\"),\n-        \"basic\": conf.get(\"BASIC_MODEL\"),\n+        \"basic\": openai_basic or azure_basic,\n         \"vision\": conf.get(\"VISION_MODEL\"),\n-    }\n-    llm_conf = llm_type_map.get(llm_type)\n-    if not llm_conf:\n+    }.get(llm_type)\n+\n+    if not llm_config:\n         raise ValueError(f\"Unknown LLM type: {llm_type}\")\n-    if not isinstance(llm_conf, dict):\n+    if not isinstance(llm_config, dict):\n         raise ValueError(f\"Invalid LLM Conf: {llm_type}\")\n-    return ChatOpenAI(**llm_conf)\n+\n+    if openai_basic is not None:\n+        return ChatOpenAI(**llm_config)\n+\n+    os.environ.update({\n+        \"OPENAI_API_VERSION\": llm_config.get(\"api_version\"),\n+        \"AZURE_OPENAI_API_KEY\": llm_config.get(\"api_key\"),\n+        \"AZURE_OPENAI_ENDPOINT\": llm_config.get(\"base_url\")\n+    })",
        "comment_created_at": "2025-05-16T04:41:12+00:00",
        "comment_author": "JounQin",
        "comment_body": "They can be passed as params https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html#langchain_openai.chat_models.azure.AzureChatOpenAI.azure_ad_token\r\n\r\nENV overriding is not a good practice IMO.",
        "pr_file_module": null
      }
    ]
  }
]