[
  {
    "discussion_id": "2312974002",
    "pr_number": 24428,
    "pr_file": "api/schedule/workflow_schedule_task.py",
    "created_at": "2025-09-01T06:09:21+00:00",
    "commented_code": "+import logging\n+from datetime import UTC, datetime\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.schedule_service import ScheduleService\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 3-step flow:\n+    1. Get rate-limited tenants from Redis\n+    2. Fetch due schedules excluding rate-limited tenants\n+    3. Process and dispatch valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        rate_limited_tenants = _get_rate_limited_tenants(session)\n+        due_schedules = _fetch_due_schedules(session, exclude_tenants=rate_limited_tenants)\n+\n+        if due_schedules:\n+            processed = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d tenants rate-limited)\",\n+                processed,\n+                len(due_schedules),\n+                len(rate_limited_tenants),\n+            )\n+\n+\n+def _get_rate_limited_tenants(session: Session) -> set[str]:\n+    \"\"\"Get tenants that have reached their daily rate limit.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    tenant_ids = session.scalars(\n+        select(WorkflowSchedulePlan.tenant_id)\n+        .distinct()\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+        )\n+    ).all()\n+\n+    if not tenant_ids:\n+        return set()\n+\n+    dispatcher_manager = QueueDispatcherManager()\n+    return {\n+        tenant_id\n+        for tenant_id in tenant_ids\n+        if not dispatcher_manager.get_dispatcher(tenant_id).check_daily_quota(tenant_id)\n+    }\n+\n+\n+def _fetch_due_schedules(session: Session, exclude_tenants: set[str]) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution, excluding rate-limited tenants.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    query = (\n+        select(WorkflowSchedulePlan)\n+        .join(\n+            AppTrigger,\n+            and_(\n+                AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                AppTrigger.trigger_type == \"trigger-schedule\",\n+            ),\n+        )\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+            AppTrigger.status == AppTriggerStatus.ENABLED,\n+        )\n+    )\n+\n+    if exclude_tenants:\n+        query = query.where(WorkflowSchedulePlan.tenant_id.notin_(exclude_tenants))\n+\n+    return list(\n+        session.scalars(query.with_for_update(skip_locked=True).limit(dify_config.WORKFLOW_SCHEDULE_POLLER_BATCH_SIZE))\n+    )\n+\n+\n+def _process_schedules(session: Session, schedules: list[WorkflowSchedulePlan]) -> int:\n+    \"\"\"Process schedules: update next run time and dispatch to Celery.\"\"\"\n+    if not schedules:\n+        return 0\n+\n+    dispatched = 0\n+\n+    for schedule in schedules:\n+        next_run_at = ScheduleService.calculate_next_run_at(\n+            schedule.cron_expression,\n+            schedule.timezone,\n+        )\n+\n+        if next_run_at:\n+            schedule.next_run_at = next_run_at",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2312974002",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24428,
        "pr_file": "api/schedule/workflow_schedule_task.py",
        "discussion_id": "2312974002",
        "commented_code": "@@ -0,0 +1,117 @@\n+import logging\n+from datetime import UTC, datetime\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.schedule_service import ScheduleService\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 3-step flow:\n+    1. Get rate-limited tenants from Redis\n+    2. Fetch due schedules excluding rate-limited tenants\n+    3. Process and dispatch valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        rate_limited_tenants = _get_rate_limited_tenants(session)\n+        due_schedules = _fetch_due_schedules(session, exclude_tenants=rate_limited_tenants)\n+\n+        if due_schedules:\n+            processed = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d tenants rate-limited)\",\n+                processed,\n+                len(due_schedules),\n+                len(rate_limited_tenants),\n+            )\n+\n+\n+def _get_rate_limited_tenants(session: Session) -> set[str]:\n+    \"\"\"Get tenants that have reached their daily rate limit.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    tenant_ids = session.scalars(\n+        select(WorkflowSchedulePlan.tenant_id)\n+        .distinct()\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+        )\n+    ).all()\n+\n+    if not tenant_ids:\n+        return set()\n+\n+    dispatcher_manager = QueueDispatcherManager()\n+    return {\n+        tenant_id\n+        for tenant_id in tenant_ids\n+        if not dispatcher_manager.get_dispatcher(tenant_id).check_daily_quota(tenant_id)\n+    }\n+\n+\n+def _fetch_due_schedules(session: Session, exclude_tenants: set[str]) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution, excluding rate-limited tenants.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    query = (\n+        select(WorkflowSchedulePlan)\n+        .join(\n+            AppTrigger,\n+            and_(\n+                AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                AppTrigger.trigger_type == \"trigger-schedule\",\n+            ),\n+        )\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+            AppTrigger.status == AppTriggerStatus.ENABLED,\n+        )\n+    )\n+\n+    if exclude_tenants:\n+        query = query.where(WorkflowSchedulePlan.tenant_id.notin_(exclude_tenants))\n+\n+    return list(\n+        session.scalars(query.with_for_update(skip_locked=True).limit(dify_config.WORKFLOW_SCHEDULE_POLLER_BATCH_SIZE))\n+    )\n+\n+\n+def _process_schedules(session: Session, schedules: list[WorkflowSchedulePlan]) -> int:\n+    \"\"\"Process schedules: update next run time and dispatch to Celery.\"\"\"\n+    if not schedules:\n+        return 0\n+\n+    dispatched = 0\n+\n+    for schedule in schedules:\n+        next_run_at = ScheduleService.calculate_next_run_at(\n+            schedule.cron_expression,\n+            schedule.timezone,\n+        )\n+\n+        if next_run_at:\n+            schedule.next_run_at = next_run_at",
        "comment_created_at": "2025-09-01T06:09:21+00:00",
        "comment_author": "vincentx11",
        "comment_body": "What if we take this operation after the execution of the scheduled async task?\r\n\r\nConsider the following scenario for reference:\r\n1. The scheduled poller runs per 1min.\r\n2. The scheduled trigger of a certain workflow is set to every 5min.\r\n3. Due to some non-declarative reasons, the workflow task takes 15 minutes to complete within a certain period.\r\n\r\nThere will be two different outcomes depending on where this code is placed:\r\n1. If we reset next_run_at before the task execution, the task will run at least 3 times within 15 minutes, with 1 run completing successfully and the other 2 remaining in a running state.\r\n2. If we reset next_run_at after the task execution, the task will only run once within 15 minutes.\r\n\r\nIn combination with the business scenario, I recommend adopting the second outcome. By the way, the above case isn't the worst-case scenario. The workflow task might also hang for an extended period. If this happens, the more times the task runs, the higher load of the service will be, which could eventually lead to the whole service unhealthy.",
        "pr_file_module": null
      },
      {
        "comment_id": "2317705044",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24428,
        "pr_file": "api/schedule/workflow_schedule_task.py",
        "discussion_id": "2312974002",
        "commented_code": "@@ -0,0 +1,117 @@\n+import logging\n+from datetime import UTC, datetime\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.schedule_service import ScheduleService\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 3-step flow:\n+    1. Get rate-limited tenants from Redis\n+    2. Fetch due schedules excluding rate-limited tenants\n+    3. Process and dispatch valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        rate_limited_tenants = _get_rate_limited_tenants(session)\n+        due_schedules = _fetch_due_schedules(session, exclude_tenants=rate_limited_tenants)\n+\n+        if due_schedules:\n+            processed = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d tenants rate-limited)\",\n+                processed,\n+                len(due_schedules),\n+                len(rate_limited_tenants),\n+            )\n+\n+\n+def _get_rate_limited_tenants(session: Session) -> set[str]:\n+    \"\"\"Get tenants that have reached their daily rate limit.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    tenant_ids = session.scalars(\n+        select(WorkflowSchedulePlan.tenant_id)\n+        .distinct()\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+        )\n+    ).all()\n+\n+    if not tenant_ids:\n+        return set()\n+\n+    dispatcher_manager = QueueDispatcherManager()\n+    return {\n+        tenant_id\n+        for tenant_id in tenant_ids\n+        if not dispatcher_manager.get_dispatcher(tenant_id).check_daily_quota(tenant_id)\n+    }\n+\n+\n+def _fetch_due_schedules(session: Session, exclude_tenants: set[str]) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution, excluding rate-limited tenants.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    query = (\n+        select(WorkflowSchedulePlan)\n+        .join(\n+            AppTrigger,\n+            and_(\n+                AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                AppTrigger.trigger_type == \"trigger-schedule\",\n+            ),\n+        )\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+            AppTrigger.status == AppTriggerStatus.ENABLED,\n+        )\n+    )\n+\n+    if exclude_tenants:\n+        query = query.where(WorkflowSchedulePlan.tenant_id.notin_(exclude_tenants))\n+\n+    return list(\n+        session.scalars(query.with_for_update(skip_locked=True).limit(dify_config.WORKFLOW_SCHEDULE_POLLER_BATCH_SIZE))\n+    )\n+\n+\n+def _process_schedules(session: Session, schedules: list[WorkflowSchedulePlan]) -> int:\n+    \"\"\"Process schedules: update next run time and dispatch to Celery.\"\"\"\n+    if not schedules:\n+        return 0\n+\n+    dispatched = 0\n+\n+    for schedule in schedules:\n+        next_run_at = ScheduleService.calculate_next_run_at(\n+            schedule.cron_expression,\n+            schedule.timezone,\n+        )\n+\n+        if next_run_at:\n+            schedule.next_run_at = next_run_at",
        "comment_created_at": "2025-09-03T04:08:26+00:00",
        "comment_author": "ACAne0320",
        "comment_body": "Thank you for raising this question.\r\n\r\nUnder the current architecture, regardless of where next_run_at is updated, the task will execute 3 times within 15 minutes.\r\nThe reason for this is that the execution of `trigger_workflow_async()` is non-blocking.\r\n\r\nOur scheduling system is intentionally designed to be fully decoupled:\r\n\r\n1. Scheduler: Responsible for triggering tasks based on cron expressions\r\n2. Executor: Handles workflow execution asyn via [Unified Trigger Entry](https://github.com/langgenius/dify/issues/23995)\r\n3. Non-blocking: `trigger_workflow_async()` immediately returns after successfully entering the queue without waiting for completion\r\n\r\nWhile your concern about multiple executions is valid from a scheduling perspective, our architecture has a critical safety mechanism that prevents the system overload you're worried about: the workflow execution queue.\r\n\r\nWhen `trigger_workflow_async()` is called, it doesn't immediately execute the workflow. Instead:\r\n\r\n1. Creates a trigger log entry with status QUEUED\r\n2. Dispatches to tier-specific queues (professional/team/sandbox)\r\n3. Returns immediately without waiting\r\n\r\nThe actual workflow execution happens in dedicated worker pools with concurrency limits per workspace.\r\n\r\nlike this:\r\nSchedule Trigger \u2192 Unified Trigger Entry \u2192 Workflow Queue \u2192 [Concurrency Control] \u2192 Actual Execution",
        "pr_file_module": null
      },
      {
        "comment_id": "2324252320",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24428,
        "pr_file": "api/schedule/workflow_schedule_task.py",
        "discussion_id": "2312974002",
        "commented_code": "@@ -0,0 +1,117 @@\n+import logging\n+from datetime import UTC, datetime\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.schedule_service import ScheduleService\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 3-step flow:\n+    1. Get rate-limited tenants from Redis\n+    2. Fetch due schedules excluding rate-limited tenants\n+    3. Process and dispatch valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        rate_limited_tenants = _get_rate_limited_tenants(session)\n+        due_schedules = _fetch_due_schedules(session, exclude_tenants=rate_limited_tenants)\n+\n+        if due_schedules:\n+            processed = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d tenants rate-limited)\",\n+                processed,\n+                len(due_schedules),\n+                len(rate_limited_tenants),\n+            )\n+\n+\n+def _get_rate_limited_tenants(session: Session) -> set[str]:\n+    \"\"\"Get tenants that have reached their daily rate limit.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    tenant_ids = session.scalars(\n+        select(WorkflowSchedulePlan.tenant_id)\n+        .distinct()\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+        )\n+    ).all()\n+\n+    if not tenant_ids:\n+        return set()\n+\n+    dispatcher_manager = QueueDispatcherManager()\n+    return {\n+        tenant_id\n+        for tenant_id in tenant_ids\n+        if not dispatcher_manager.get_dispatcher(tenant_id).check_daily_quota(tenant_id)\n+    }\n+\n+\n+def _fetch_due_schedules(session: Session, exclude_tenants: set[str]) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution, excluding rate-limited tenants.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    query = (\n+        select(WorkflowSchedulePlan)\n+        .join(\n+            AppTrigger,\n+            and_(\n+                AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                AppTrigger.trigger_type == \"trigger-schedule\",\n+            ),\n+        )\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+            AppTrigger.status == AppTriggerStatus.ENABLED,\n+        )\n+    )\n+\n+    if exclude_tenants:\n+        query = query.where(WorkflowSchedulePlan.tenant_id.notin_(exclude_tenants))\n+\n+    return list(\n+        session.scalars(query.with_for_update(skip_locked=True).limit(dify_config.WORKFLOW_SCHEDULE_POLLER_BATCH_SIZE))\n+    )\n+\n+\n+def _process_schedules(session: Session, schedules: list[WorkflowSchedulePlan]) -> int:\n+    \"\"\"Process schedules: update next run time and dispatch to Celery.\"\"\"\n+    if not schedules:\n+        return 0\n+\n+    dispatched = 0\n+\n+    for schedule in schedules:\n+        next_run_at = ScheduleService.calculate_next_run_at(\n+            schedule.cron_expression,\n+            schedule.timezone,\n+        )\n+\n+        if next_run_at:\n+            schedule.next_run_at = next_run_at",
        "comment_created_at": "2025-09-05T06:48:21+00:00",
        "comment_author": "vincentx11",
        "comment_body": "That sounds interesting. With this solution, whether tasks of a specified workflow can run in parallel (with multiple instances over a period) is controlled by the workflow itself\u2014specifically through the \"Parallel Setting\" in the trigger, which wasn't mentioned in this feature scenario.\r\n\r\nThis is acceptable for now. However, looking ahead, I strongly recommend adding a `Parallel Setting` to the trigger node with three modes:\r\n\r\n1. `Parallel` \u2013 this is the current solution\r\n2. `Serial Waiting` \u2013 only one task instance exists per workflow; new tasks wait if another is already running\r\n3. `Serial Rejection` \u2013 new tasks get rejected if one is already running, ensuring only one instance at a time\r\n\r\nFollowing the design approach you described, if we considered this solution, this check could be implemented before `trigger_workflow_async` within `run_schedule_trigger`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2331088085",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24428,
        "pr_file": "api/schedule/workflow_schedule_task.py",
        "discussion_id": "2312974002",
        "commented_code": "@@ -0,0 +1,117 @@\n+import logging\n+from datetime import UTC, datetime\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.schedule_service import ScheduleService\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 3-step flow:\n+    1. Get rate-limited tenants from Redis\n+    2. Fetch due schedules excluding rate-limited tenants\n+    3. Process and dispatch valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        rate_limited_tenants = _get_rate_limited_tenants(session)\n+        due_schedules = _fetch_due_schedules(session, exclude_tenants=rate_limited_tenants)\n+\n+        if due_schedules:\n+            processed = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d tenants rate-limited)\",\n+                processed,\n+                len(due_schedules),\n+                len(rate_limited_tenants),\n+            )\n+\n+\n+def _get_rate_limited_tenants(session: Session) -> set[str]:\n+    \"\"\"Get tenants that have reached their daily rate limit.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    tenant_ids = session.scalars(\n+        select(WorkflowSchedulePlan.tenant_id)\n+        .distinct()\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+        )\n+    ).all()\n+\n+    if not tenant_ids:\n+        return set()\n+\n+    dispatcher_manager = QueueDispatcherManager()\n+    return {\n+        tenant_id\n+        for tenant_id in tenant_ids\n+        if not dispatcher_manager.get_dispatcher(tenant_id).check_daily_quota(tenant_id)\n+    }\n+\n+\n+def _fetch_due_schedules(session: Session, exclude_tenants: set[str]) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution, excluding rate-limited tenants.\"\"\"\n+    now = datetime.now(UTC)\n+\n+    query = (\n+        select(WorkflowSchedulePlan)\n+        .join(\n+            AppTrigger,\n+            and_(\n+                AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                AppTrigger.trigger_type == \"trigger-schedule\",\n+            ),\n+        )\n+        .where(\n+            WorkflowSchedulePlan.next_run_at <= now,\n+            WorkflowSchedulePlan.next_run_at.isnot(None),\n+            AppTrigger.status == AppTriggerStatus.ENABLED,\n+        )\n+    )\n+\n+    if exclude_tenants:\n+        query = query.where(WorkflowSchedulePlan.tenant_id.notin_(exclude_tenants))\n+\n+    return list(\n+        session.scalars(query.with_for_update(skip_locked=True).limit(dify_config.WORKFLOW_SCHEDULE_POLLER_BATCH_SIZE))\n+    )\n+\n+\n+def _process_schedules(session: Session, schedules: list[WorkflowSchedulePlan]) -> int:\n+    \"\"\"Process schedules: update next run time and dispatch to Celery.\"\"\"\n+    if not schedules:\n+        return 0\n+\n+    dispatched = 0\n+\n+    for schedule in schedules:\n+        next_run_at = ScheduleService.calculate_next_run_at(\n+            schedule.cron_expression,\n+            schedule.timezone,\n+        )\n+\n+        if next_run_at:\n+            schedule.next_run_at = next_run_at",
        "comment_created_at": "2025-09-08T18:48:56+00:00",
        "comment_author": "ACAne0320",
        "comment_body": "I'm glad you were able to provide this solution.\r\nMaybe we need to discuss with the Dify team whether we should add this feature to the schedule trigger.\r\nI'll get back to you if there's any result.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2324060871",
    "pr_number": 24428,
    "pr_file": "api/schedule/workflow_schedule_task.py",
    "created_at": "2025-09-05T04:26:29+00:00",
    "commented_code": "+import logging\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from libs.datetime_utils import naive_utc_now\n+from libs.schedule_utils import calculate_next_run_at\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 2-step flow:\n+    1. Fetch due schedules\n+    2. Process valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        due_schedules = _fetch_due_schedules(session)\n+\n+        if due_schedules:\n+            dispatched_count, rate_limited_count = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d skipped due to rate limit)\",\n+                dispatched_count,\n+                len(due_schedules),\n+                rate_limited_count,\n+            )\n+\n+\n+def _fetch_due_schedules(session: Session) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution.\"\"\"\n+    now = naive_utc_now()\n+\n+    due_schedules = session.scalars(\n+        (\n+            select(WorkflowSchedulePlan)\n+            .join(\n+                AppTrigger,\n+                and_(\n+                    AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                    AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                    AppTrigger.trigger_type == \"trigger-schedule\",\n+                ),\n+            )\n+            .where(\n+                WorkflowSchedulePlan.next_run_at <= now,",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2324060871",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24428,
        "pr_file": "api/schedule/workflow_schedule_task.py",
        "discussion_id": "2324060871",
        "commented_code": "@@ -0,0 +1,98 @@\n+import logging\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from libs.datetime_utils import naive_utc_now\n+from libs.schedule_utils import calculate_next_run_at\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 2-step flow:\n+    1. Fetch due schedules\n+    2. Process valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        due_schedules = _fetch_due_schedules(session)\n+\n+        if due_schedules:\n+            dispatched_count, rate_limited_count = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d skipped due to rate limit)\",\n+                dispatched_count,\n+                len(due_schedules),\n+                rate_limited_count,\n+            )\n+\n+\n+def _fetch_due_schedules(session: Session) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution.\"\"\"\n+    now = naive_utc_now()\n+\n+    due_schedules = session.scalars(\n+        (\n+            select(WorkflowSchedulePlan)\n+            .join(\n+                AppTrigger,\n+                and_(\n+                    AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                    AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                    AppTrigger.trigger_type == \"trigger-schedule\",\n+                ),\n+            )\n+            .where(\n+                WorkflowSchedulePlan.next_run_at <= now,",
        "comment_created_at": "2025-09-05T04:26:29+00:00",
        "comment_author": "Yeuoly",
        "comment_body": "Is there a possible for the schedules to be triggered twice?\r\n1. _fetch_due_schedules was invoked but `next_run_at` was not updated, since _process_schedules may take some times to process all the plans, this would lead delays.\r\n2. when the next cycle occurs, if schedule plan already been fetched before but not been updated will been triggered again.",
        "pr_file_module": null
      },
      {
        "comment_id": "2331132714",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24428,
        "pr_file": "api/schedule/workflow_schedule_task.py",
        "discussion_id": "2324060871",
        "commented_code": "@@ -0,0 +1,98 @@\n+import logging\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from libs.datetime_utils import naive_utc_now\n+from libs.schedule_utils import calculate_next_run_at\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 2-step flow:\n+    1. Fetch due schedules\n+    2. Process valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        due_schedules = _fetch_due_schedules(session)\n+\n+        if due_schedules:\n+            dispatched_count, rate_limited_count = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d skipped due to rate limit)\",\n+                dispatched_count,\n+                len(due_schedules),\n+                rate_limited_count,\n+            )\n+\n+\n+def _fetch_due_schedules(session: Session) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution.\"\"\"\n+    now = naive_utc_now()\n+\n+    due_schedules = session.scalars(\n+        (\n+            select(WorkflowSchedulePlan)\n+            .join(\n+                AppTrigger,\n+                and_(\n+                    AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                    AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                    AppTrigger.trigger_type == \"trigger-schedule\",\n+                ),\n+            )\n+            .where(\n+                WorkflowSchedulePlan.next_run_at <= now,",
        "comment_created_at": "2025-09-08T19:08:27+00:00",
        "comment_author": "ACAne0320",
        "comment_body": "Under the current implementation, scheduling is not normally double-triggered.\r\n1. The scheduled plan is obtained via a `SELECT ... FOR UPDATE SKIP LOCKED` statement, preventing the same row from being reselected during processing. \r\n2. Within the same transaction, we calculate and set `next_run_at`, schedule the task, and then commit. After commit, the plan no longer expires and will not be reselected.",
        "pr_file_module": null
      },
      {
        "comment_id": "2335585070",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24428,
        "pr_file": "api/schedule/workflow_schedule_task.py",
        "discussion_id": "2324060871",
        "commented_code": "@@ -0,0 +1,98 @@\n+import logging\n+\n+from celery import shared_task\n+from sqlalchemy import and_, select\n+from sqlalchemy.orm import Session, sessionmaker\n+\n+from configs import dify_config\n+from extensions.ext_database import db\n+from libs.datetime_utils import naive_utc_now\n+from libs.schedule_utils import calculate_next_run_at\n+from models.workflow import AppTrigger, AppTriggerStatus, WorkflowSchedulePlan\n+from services.workflow.queue_dispatcher import QueueDispatcherManager\n+from tasks.workflow_schedule_tasks import run_schedule_trigger\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")\n+def poll_workflow_schedules() -> None:\n+    \"\"\"\n+    Poll and process due workflow schedules.\n+\n+    Simple 2-step flow:\n+    1. Fetch due schedules\n+    2. Process valid schedules\n+    \"\"\"\n+    session_factory = sessionmaker(bind=db.engine, expire_on_commit=False)\n+\n+    with session_factory() as session:\n+        due_schedules = _fetch_due_schedules(session)\n+\n+        if due_schedules:\n+            dispatched_count, rate_limited_count = _process_schedules(session, due_schedules)\n+            logger.info(\n+                \"Processed %d/%d schedules (%d skipped due to rate limit)\",\n+                dispatched_count,\n+                len(due_schedules),\n+                rate_limited_count,\n+            )\n+\n+\n+def _fetch_due_schedules(session: Session) -> list[WorkflowSchedulePlan]:\n+    \"\"\"Fetch all schedules that are due for execution.\"\"\"\n+    now = naive_utc_now()\n+\n+    due_schedules = session.scalars(\n+        (\n+            select(WorkflowSchedulePlan)\n+            .join(\n+                AppTrigger,\n+                and_(\n+                    AppTrigger.app_id == WorkflowSchedulePlan.app_id,\n+                    AppTrigger.node_id == WorkflowSchedulePlan.node_id,\n+                    AppTrigger.trigger_type == \"trigger-schedule\",\n+                ),\n+            )\n+            .where(\n+                WorkflowSchedulePlan.next_run_at <= now,",
        "comment_created_at": "2025-09-10T05:20:11+00:00",
        "comment_author": "Yeuoly",
        "comment_body": "Great!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2324065147",
    "pr_number": 24428,
    "pr_file": "api/tasks/workflow_schedule_tasks.py",
    "created_at": "2025-09-05T04:30:55+00:00",
    "commented_code": "+import logging\n+from datetime import UTC, datetime\n+from zoneinfo import ZoneInfo\n+\n+from celery import shared_task\n+from sqlalchemy.orm import sessionmaker\n+\n+from core.workflow.nodes.trigger_schedule.exc import (\n+    ScheduleExecutionError,\n+    ScheduleNotFoundError,\n+    TenantOwnerNotFoundError,\n+)\n+from extensions.ext_database import db\n+from models.enums import WorkflowRunTriggeredFrom\n+from models.workflow import WorkflowSchedulePlan\n+from services.async_workflow_service import AsyncWorkflowService\n+from services.schedule_service import ScheduleService\n+from services.workflow.entities import TriggerData\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2324065147",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24428,
        "pr_file": "api/tasks/workflow_schedule_tasks.py",
        "discussion_id": "2324065147",
        "commented_code": "@@ -0,0 +1,69 @@\n+import logging\n+from datetime import UTC, datetime\n+from zoneinfo import ZoneInfo\n+\n+from celery import shared_task\n+from sqlalchemy.orm import sessionmaker\n+\n+from core.workflow.nodes.trigger_schedule.exc import (\n+    ScheduleExecutionError,\n+    ScheduleNotFoundError,\n+    TenantOwnerNotFoundError,\n+)\n+from extensions.ext_database import db\n+from models.enums import WorkflowRunTriggeredFrom\n+from models.workflow import WorkflowSchedulePlan\n+from services.async_workflow_service import AsyncWorkflowService\n+from services.schedule_service import ScheduleService\n+from services.workflow.entities import TriggerData\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@shared_task(queue=\"schedule\")",
        "comment_created_at": "2025-09-05T04:30:55+00:00",
        "comment_author": "Yeuoly",
        "comment_body": "It may be beneficial for `run_schedule_trigger` and `poll_workflow_schedules` to operate on separate queues to prevent them from blocking one another.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2314092821",
    "pr_number": 24786,
    "pr_file": "api/events/event_handlers/update_provider_when_message_created.py",
    "created_at": "2025-09-01T14:18:19+00:00",
    "commented_code": "# Prepare values dict for SQLAlchemy update\n             update_values = {}\n-            # updateing to `last_used` is removed due to performance reason.\n-            # ref: https://github.com/langgenius/dify/issues/24526\n+\n+            # Time-window based update for last_used to avoid hot row contention\n+            if values.last_used is not None:\n+                cache_key = _get_provider_cache_key(filters.tenant_id, filters.provider_name)\n+                now = datetime_utils.naive_utc_now()\n+                last_update = _get_last_update_timestamp(cache_key)\n+\n+                if last_update is None or (now - last_update).total_seconds() > LAST_USED_UPDATE_WINDOW_SECONDS:\n+                    update_values[\"last_used\"] = values.last_used",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2314092821",
        "repo_full_name": "langgenius/dify",
        "pr_number": 24786,
        "pr_file": "api/events/event_handlers/update_provider_when_message_created.py",
        "discussion_id": "2314092821",
        "commented_code": "@@ -215,8 +249,17 @@ def _execute_provider_updates(updates_to_perform: list[_ProviderUpdateOperation]\n \n             # Prepare values dict for SQLAlchemy update\n             update_values = {}\n-            # updateing to `last_used` is removed due to performance reason.\n-            # ref: https://github.com/langgenius/dify/issues/24526\n+\n+            # Time-window based update for last_used to avoid hot row contention\n+            if values.last_used is not None:\n+                cache_key = _get_provider_cache_key(filters.tenant_id, filters.provider_name)\n+                now = datetime_utils.naive_utc_now()\n+                last_update = _get_last_update_timestamp(cache_key)\n+\n+                if last_update is None or (now - last_update).total_seconds() > LAST_USED_UPDATE_WINDOW_SECONDS:\n+                    update_values[\"last_used\"] = values.last_used",
        "comment_created_at": "2025-09-01T14:18:19+00:00",
        "comment_author": "QuantumGhost",
        "comment_body": "For frequently used providers, the current updating logic may experience race conditions or update contention under high load.\n\n(The current implementation is acceptable, but I suggest documenting the limitation for future optimization considerations.)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2204308591",
    "pr_number": 22169,
    "pr_file": "api/core/mcp/session/base_session.py",
    "created_at": "2025-07-14T09:09:07+00:00",
    "commented_code": "self._session_read_timeout_seconds = read_timeout_seconds\n         self._in_flight = {}\n         self._exit_stack = ExitStack()\n+        # Initialize executor and future to None for proper cleanup checks\n+        self._executor: ThreadPoolExecutor | None = None\n+        self._receiver_future: Future | None = None\n \n     def __enter__(self) -> Self:\n         self._executor = ThreadPoolExecutor()",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2204308591",
        "repo_full_name": "langgenius/dify",
        "pr_number": 22169,
        "pr_file": "api/core/mcp/session/base_session.py",
        "discussion_id": "2204308591",
        "commented_code": "@@ -171,23 +171,37 @@ def __init__(\n         self._session_read_timeout_seconds = read_timeout_seconds\n         self._in_flight = {}\n         self._exit_stack = ExitStack()\n+        # Initialize executor and future to None for proper cleanup checks\n+        self._executor: ThreadPoolExecutor | None = None\n+        self._receiver_future: Future | None = None\n \n     def __enter__(self) -> Self:\n         self._executor = ThreadPoolExecutor()",
        "comment_created_at": "2025-07-14T09:09:07+00:00",
        "comment_author": "QuantumGhost",
        "comment_body": "IMO, we should set `max_workers=1` for the executor.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1952040063",
    "pr_number": 13543,
    "pr_file": "api/core/rag/datasource/retrieval_service.py",
    "created_at": "2025-02-12T06:16:34+00:00",
    "commented_code": "):\n         if not query:\n             return []\n-        dataset = db.session.query(Dataset).filter(Dataset.id == dataset_id).first()\n-        if not dataset:\n-            return []\n-\n+        dataset = cls._get_dataset(dataset_id)\n         if not dataset or dataset.available_document_count == 0 or dataset.available_segment_count == 0:\n             return []\n+\n         all_documents: list[Document] = []\n-        threads: list[threading.Thread] = []\n         exceptions: list[str] = []\n-        # retrieval_model source with keyword\n-        if retrieval_method == \"keyword_search\":\n-            keyword_thread = threading.Thread(\n-                target=RetrievalService.keyword_search,\n-                kwargs={\n-                    \"flask_app\": current_app._get_current_object(),  # type: ignore\n-                    \"dataset_id\": dataset_id,\n-                    \"query\": query,\n-                    \"top_k\": top_k,\n-                    \"all_documents\": all_documents,\n-                    \"exceptions\": exceptions,\n-                },\n-            )\n-            threads.append(keyword_thread)\n-            keyword_thread.start()\n-        # retrieval_model source with semantic\n-        if RetrievalMethod.is_support_semantic_search(retrieval_method):\n-            embedding_thread = threading.Thread(\n-                target=RetrievalService.embedding_search,\n-                kwargs={\n-                    \"flask_app\": current_app._get_current_object(),  # type: ignore\n-                    \"dataset_id\": dataset_id,\n-                    \"query\": query,\n-                    \"top_k\": top_k,\n-                    \"score_threshold\": score_threshold,\n-                    \"reranking_model\": reranking_model,\n-                    \"all_documents\": all_documents,\n-                    \"retrieval_method\": retrieval_method,\n-                    \"exceptions\": exceptions,\n-                },\n-            )\n-            threads.append(embedding_thread)\n-            embedding_thread.start()\n-\n-        # retrieval source with full text\n-        if RetrievalMethod.is_support_fulltext_search(retrieval_method):\n-            full_text_index_thread = threading.Thread(\n-                target=RetrievalService.full_text_index_search,\n-                kwargs={\n-                    \"flask_app\": current_app._get_current_object(),  # type: ignore\n-                    \"dataset_id\": dataset_id,\n-                    \"query\": query,\n-                    \"retrieval_method\": retrieval_method,\n-                    \"score_threshold\": score_threshold,\n-                    \"top_k\": top_k,\n-                    \"reranking_model\": reranking_model,\n-                    \"all_documents\": all_documents,\n-                    \"exceptions\": exceptions,\n-                },\n-            )\n-            threads.append(full_text_index_thread)\n-            full_text_index_thread.start()\n \n-        for thread in threads:\n-            thread.join()\n+        # Optimize multithreading with thread pools\n+        futures = []\n+        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n+            if retrieval_method == \"keyword_search\":\n+                futures.append(\n+                    executor.submit(\n+                        cls.keyword_search,\n+                        flask_app=current_app._get_current_object(),  # type: ignore\n+                        dataset_id=dataset_id,\n+                        query=query,\n+                        top_k=top_k,\n+                        all_documents=all_documents,\n+                        exceptions=exceptions,\n+                    )\n+                )\n+            if RetrievalMethod.is_support_semantic_search(retrieval_method):\n+                futures.append(\n+                    executor.submit(\n+                        cls.embedding_search,\n+                        flask_app=current_app._get_current_object(),  # type: ignore\n+                        dataset_id=dataset_id,\n+                        query=query,\n+                        top_k=top_k,\n+                        score_threshold=score_threshold,\n+                        reranking_model=reranking_model,\n+                        all_documents=all_documents,\n+                        retrieval_method=retrieval_method,\n+                        exceptions=exceptions,\n+                    )\n+                )\n+            if RetrievalMethod.is_support_fulltext_search(retrieval_method):\n+                futures.append(\n+                    executor.submit(\n+                        cls.full_text_index_search,\n+                        flask_app=current_app._get_current_object(),  # type: ignore\n+                        dataset_id=dataset_id,\n+                        query=query,\n+                        top_k=top_k,\n+                        score_threshold=score_threshold,\n+                        reranking_model=reranking_model,\n+                        all_documents=all_documents,\n+                        retrieval_method=retrieval_method,\n+                        exceptions=exceptions,\n+                    )\n+                )\n+            concurrent.futures.wait(futures)",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "1952040063",
        "repo_full_name": "langgenius/dify",
        "pr_number": 13543,
        "pr_file": "api/core/rag/datasource/retrieval_service.py",
        "discussion_id": "1952040063",
        "commented_code": "@@ -41,74 +43,62 @@ def retrieve(\n     ):\n         if not query:\n             return []\n-        dataset = db.session.query(Dataset).filter(Dataset.id == dataset_id).first()\n-        if not dataset:\n-            return []\n-\n+        dataset = cls._get_dataset(dataset_id)\n         if not dataset or dataset.available_document_count == 0 or dataset.available_segment_count == 0:\n             return []\n+\n         all_documents: list[Document] = []\n-        threads: list[threading.Thread] = []\n         exceptions: list[str] = []\n-        # retrieval_model source with keyword\n-        if retrieval_method == \"keyword_search\":\n-            keyword_thread = threading.Thread(\n-                target=RetrievalService.keyword_search,\n-                kwargs={\n-                    \"flask_app\": current_app._get_current_object(),  # type: ignore\n-                    \"dataset_id\": dataset_id,\n-                    \"query\": query,\n-                    \"top_k\": top_k,\n-                    \"all_documents\": all_documents,\n-                    \"exceptions\": exceptions,\n-                },\n-            )\n-            threads.append(keyword_thread)\n-            keyword_thread.start()\n-        # retrieval_model source with semantic\n-        if RetrievalMethod.is_support_semantic_search(retrieval_method):\n-            embedding_thread = threading.Thread(\n-                target=RetrievalService.embedding_search,\n-                kwargs={\n-                    \"flask_app\": current_app._get_current_object(),  # type: ignore\n-                    \"dataset_id\": dataset_id,\n-                    \"query\": query,\n-                    \"top_k\": top_k,\n-                    \"score_threshold\": score_threshold,\n-                    \"reranking_model\": reranking_model,\n-                    \"all_documents\": all_documents,\n-                    \"retrieval_method\": retrieval_method,\n-                    \"exceptions\": exceptions,\n-                },\n-            )\n-            threads.append(embedding_thread)\n-            embedding_thread.start()\n-\n-        # retrieval source with full text\n-        if RetrievalMethod.is_support_fulltext_search(retrieval_method):\n-            full_text_index_thread = threading.Thread(\n-                target=RetrievalService.full_text_index_search,\n-                kwargs={\n-                    \"flask_app\": current_app._get_current_object(),  # type: ignore\n-                    \"dataset_id\": dataset_id,\n-                    \"query\": query,\n-                    \"retrieval_method\": retrieval_method,\n-                    \"score_threshold\": score_threshold,\n-                    \"top_k\": top_k,\n-                    \"reranking_model\": reranking_model,\n-                    \"all_documents\": all_documents,\n-                    \"exceptions\": exceptions,\n-                },\n-            )\n-            threads.append(full_text_index_thread)\n-            full_text_index_thread.start()\n \n-        for thread in threads:\n-            thread.join()\n+        # Optimize multithreading with thread pools\n+        futures = []\n+        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n+            if retrieval_method == \"keyword_search\":\n+                futures.append(\n+                    executor.submit(\n+                        cls.keyword_search,\n+                        flask_app=current_app._get_current_object(),  # type: ignore\n+                        dataset_id=dataset_id,\n+                        query=query,\n+                        top_k=top_k,\n+                        all_documents=all_documents,\n+                        exceptions=exceptions,\n+                    )\n+                )\n+            if RetrievalMethod.is_support_semantic_search(retrieval_method):\n+                futures.append(\n+                    executor.submit(\n+                        cls.embedding_search,\n+                        flask_app=current_app._get_current_object(),  # type: ignore\n+                        dataset_id=dataset_id,\n+                        query=query,\n+                        top_k=top_k,\n+                        score_threshold=score_threshold,\n+                        reranking_model=reranking_model,\n+                        all_documents=all_documents,\n+                        retrieval_method=retrieval_method,\n+                        exceptions=exceptions,\n+                    )\n+                )\n+            if RetrievalMethod.is_support_fulltext_search(retrieval_method):\n+                futures.append(\n+                    executor.submit(\n+                        cls.full_text_index_search,\n+                        flask_app=current_app._get_current_object(),  # type: ignore\n+                        dataset_id=dataset_id,\n+                        query=query,\n+                        top_k=top_k,\n+                        score_threshold=score_threshold,\n+                        reranking_model=reranking_model,\n+                        all_documents=all_documents,\n+                        retrieval_method=retrieval_method,\n+                        exceptions=exceptions,\n+                    )\n+                )\n+            concurrent.futures.wait(futures)",
        "comment_created_at": "2025-02-12T06:16:34+00:00",
        "comment_author": "bowenliang123",
        "comment_body": "Consider setting a timeout for waiting futures.\r\n`concurrent.futures.wait(fs, timeout=None, return_when=ALL_COMPLETED`\r\n(https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.wait)\r\n",
        "pr_file_module": null
      }
    ]
  }
]