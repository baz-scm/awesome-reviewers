[
  {
    "discussion_id": "2048436595",
    "pr_number": 17877,
    "pr_file": "api/core/llm_generator/llm_generator.py",
    "created_at": "2025-04-17T07:53:37+00:00",
    "commented_code": "answer = cast(str, response.message.content)\n         return answer.strip()\n+\n+    @classmethod\n+    def generate_structured_output(cls, tenant_id: str, instruction: str, model_config: dict, max_tokens: int):\n+        prompt_template = PromptTemplateParser(STRUCTURED_OUTPUT_GENERATE_TEMPLATE)\n+\n+        prompt = prompt_template.format(\n+            inputs={\n+                \"INSTRUCTION\": instruction,\n+            },\n+            remove_template_variables=False,\n+        )\n+\n+        model_manager = ModelManager()\n+        model_instance = model_manager.get_model_instance(\n+            tenant_id=tenant_id,\n+            model_type=ModelType.LLM,\n+            provider=model_config.get(\"provider\", \"\"),\n+            model=model_config.get(\"name\", \"\"),\n+        )\n+\n+        prompt_messages = [UserPromptMessage(content=prompt)]",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2048436595",
        "repo_full_name": "langgenius/dify",
        "pr_number": 17877,
        "pr_file": "api/core/llm_generator/llm_generator.py",
        "discussion_id": "2048436595",
        "commented_code": "@@ -340,3 +341,43 @@ def generate_qa_document(cls, tenant_id: str, query, document_language: str):\n \n         answer = cast(str, response.message.content)\n         return answer.strip()\n+\n+    @classmethod\n+    def generate_structured_output(cls, tenant_id: str, instruction: str, model_config: dict, max_tokens: int):\n+        prompt_template = PromptTemplateParser(STRUCTURED_OUTPUT_GENERATE_TEMPLATE)\n+\n+        prompt = prompt_template.format(\n+            inputs={\n+                \"INSTRUCTION\": instruction,\n+            },\n+            remove_template_variables=False,\n+        )\n+\n+        model_manager = ModelManager()\n+        model_instance = model_manager.get_model_instance(\n+            tenant_id=tenant_id,\n+            model_type=ModelType.LLM,\n+            provider=model_config.get(\"provider\", \"\"),\n+            model=model_config.get(\"name\", \"\"),\n+        )\n+\n+        prompt_messages = [UserPromptMessage(content=prompt)]",
        "comment_created_at": "2025-04-17T07:53:37+00:00",
        "comment_author": "QuantumGhost",
        "comment_body": "I suggest using the system role to provide instructions for JSON schema generation, and the user role to deliver the actual user input. Avoid embedding user input directly into the model's instructions through templating.\n\nThis approach helps mitigate prompt injection vulnerabilities.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2050168363",
    "pr_number": 17877,
    "pr_file": "api/core/workflow/nodes/llm/node.py",
    "created_at": "2025-04-18T06:47:44+00:00",
    "commented_code": "return prompt_messages\n \n+    def _handle_native_json_schema(self, model_parameters: dict, rules: list[ParameterRule]) -> dict:\n+        \"\"\"\n+        Handle structured output for models with native JSON schema support.\n+\n+        :param model_parameters: Model parameters to update\n+        :param rules: Model parameter rules\n+        :return: Updated model parameters with JSON schema configuration\n+        \"\"\"\n+        # Process schema according to model requirements\n+        schema = self._fetch_structured_output_schema()\n+        schema_json = self._prepare_schema_for_model(schema)\n+\n+        # Set JSON schema in parameters\n+        model_parameters[\"json_schema\"] = json.dumps(schema_json, ensure_ascii=False)\n+\n+        # Set appropriate response format if required by the model\n+        for rule in rules:\n+            if rule.name == \"response_format\" and ResponseFormat.JSON_SCHEMA.value in rule.options:\n+                model_parameters[\"response_format\"] = ResponseFormat.JSON_SCHEMA.value\n+\n+        return model_parameters\n+\n+    def _handle_prompt_based_schema(self, prompt_messages: Sequence[PromptMessage]) -> list[PromptMessage]:\n+        \"\"\"\n+        Handle structured output for models without native JSON schema support.\n+        This function modifies the prompt messages to include schema-based output requirements.\n+\n+        Args:\n+            prompt_messages: Original sequence of prompt messages\n+\n+        Returns:\n+            list[PromptMessage]: Updated prompt messages with structured output requirements\n+        \"\"\"\n+        # Convert schema to string format\n+        schema_str = json.dumps(self._fetch_structured_output_schema(), ensure_ascii=False)\n+\n+        # Find existing system prompt with schema placeholder\n+        system_prompt = next(\n+            (prompt for prompt in prompt_messages if isinstance(prompt, SystemPromptMessage)),\n+            None,\n+        )\n+        structured_output_prompt = STRUCTURED_OUTPUT_PROMPT.replace(\"{{schema}}\", schema_str)\n+        # Prepare system prompt content\n+        system_prompt_content = (\n+            structured_output_prompt + \"\n\n\" + system_prompt.content\n+            if system_prompt and isinstance(system_prompt.content, str)\n+            else structured_output_prompt\n+        )\n+        system_prompt = SystemPromptMessage(content=system_prompt_content)\n+\n+        # Extract content from the last user message\n+\n+        filtered_prompts = [prompt for prompt in prompt_messages if not isinstance(prompt, SystemPromptMessage)]",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2050168363",
        "repo_full_name": "langgenius/dify",
        "pr_number": 17877,
        "pr_file": "api/core/workflow/nodes/llm/node.py",
        "discussion_id": "2050168363",
        "commented_code": "@@ -926,6 +967,165 @@ def _handle_list_messages(\n \n         return prompt_messages\n \n+    def _handle_native_json_schema(self, model_parameters: dict, rules: list[ParameterRule]) -> dict:\n+        \"\"\"\n+        Handle structured output for models with native JSON schema support.\n+\n+        :param model_parameters: Model parameters to update\n+        :param rules: Model parameter rules\n+        :return: Updated model parameters with JSON schema configuration\n+        \"\"\"\n+        # Process schema according to model requirements\n+        schema = self._fetch_structured_output_schema()\n+        schema_json = self._prepare_schema_for_model(schema)\n+\n+        # Set JSON schema in parameters\n+        model_parameters[\"json_schema\"] = json.dumps(schema_json, ensure_ascii=False)\n+\n+        # Set appropriate response format if required by the model\n+        for rule in rules:\n+            if rule.name == \"response_format\" and ResponseFormat.JSON_SCHEMA.value in rule.options:\n+                model_parameters[\"response_format\"] = ResponseFormat.JSON_SCHEMA.value\n+\n+        return model_parameters\n+\n+    def _handle_prompt_based_schema(self, prompt_messages: Sequence[PromptMessage]) -> list[PromptMessage]:\n+        \"\"\"\n+        Handle structured output for models without native JSON schema support.\n+        This function modifies the prompt messages to include schema-based output requirements.\n+\n+        Args:\n+            prompt_messages: Original sequence of prompt messages\n+\n+        Returns:\n+            list[PromptMessage]: Updated prompt messages with structured output requirements\n+        \"\"\"\n+        # Convert schema to string format\n+        schema_str = json.dumps(self._fetch_structured_output_schema(), ensure_ascii=False)\n+\n+        # Find existing system prompt with schema placeholder\n+        system_prompt = next(\n+            (prompt for prompt in prompt_messages if isinstance(prompt, SystemPromptMessage)),\n+            None,\n+        )\n+        structured_output_prompt = STRUCTURED_OUTPUT_PROMPT.replace(\"{{schema}}\", schema_str)\n+        # Prepare system prompt content\n+        system_prompt_content = (\n+            structured_output_prompt + \"\\n\\n\" + system_prompt.content\n+            if system_prompt and isinstance(system_prompt.content, str)\n+            else structured_output_prompt\n+        )\n+        system_prompt = SystemPromptMessage(content=system_prompt_content)\n+\n+        # Extract content from the last user message\n+\n+        filtered_prompts = [prompt for prompt in prompt_messages if not isinstance(prompt, SystemPromptMessage)]",
        "comment_created_at": "2025-04-18T06:47:44+00:00",
        "comment_author": "QuantumGhost",
        "comment_body": "It seems that if structured output is enabled and the model doesn't natively support `json_schema`, the system prompts provided by the user are silently discarded. Should we consider explicitly highlighting this behavior in the relevant documentation?",
        "pr_file_module": null
      },
      {
        "comment_id": "2050202923",
        "repo_full_name": "langgenius/dify",
        "pr_number": 17877,
        "pr_file": "api/core/workflow/nodes/llm/node.py",
        "discussion_id": "2050168363",
        "commented_code": "@@ -926,6 +967,165 @@ def _handle_list_messages(\n \n         return prompt_messages\n \n+    def _handle_native_json_schema(self, model_parameters: dict, rules: list[ParameterRule]) -> dict:\n+        \"\"\"\n+        Handle structured output for models with native JSON schema support.\n+\n+        :param model_parameters: Model parameters to update\n+        :param rules: Model parameter rules\n+        :return: Updated model parameters with JSON schema configuration\n+        \"\"\"\n+        # Process schema according to model requirements\n+        schema = self._fetch_structured_output_schema()\n+        schema_json = self._prepare_schema_for_model(schema)\n+\n+        # Set JSON schema in parameters\n+        model_parameters[\"json_schema\"] = json.dumps(schema_json, ensure_ascii=False)\n+\n+        # Set appropriate response format if required by the model\n+        for rule in rules:\n+            if rule.name == \"response_format\" and ResponseFormat.JSON_SCHEMA.value in rule.options:\n+                model_parameters[\"response_format\"] = ResponseFormat.JSON_SCHEMA.value\n+\n+        return model_parameters\n+\n+    def _handle_prompt_based_schema(self, prompt_messages: Sequence[PromptMessage]) -> list[PromptMessage]:\n+        \"\"\"\n+        Handle structured output for models without native JSON schema support.\n+        This function modifies the prompt messages to include schema-based output requirements.\n+\n+        Args:\n+            prompt_messages: Original sequence of prompt messages\n+\n+        Returns:\n+            list[PromptMessage]: Updated prompt messages with structured output requirements\n+        \"\"\"\n+        # Convert schema to string format\n+        schema_str = json.dumps(self._fetch_structured_output_schema(), ensure_ascii=False)\n+\n+        # Find existing system prompt with schema placeholder\n+        system_prompt = next(\n+            (prompt for prompt in prompt_messages if isinstance(prompt, SystemPromptMessage)),\n+            None,\n+        )\n+        structured_output_prompt = STRUCTURED_OUTPUT_PROMPT.replace(\"{{schema}}\", schema_str)\n+        # Prepare system prompt content\n+        system_prompt_content = (\n+            structured_output_prompt + \"\\n\\n\" + system_prompt.content\n+            if system_prompt and isinstance(system_prompt.content, str)\n+            else structured_output_prompt\n+        )\n+        system_prompt = SystemPromptMessage(content=system_prompt_content)\n+\n+        # Extract content from the last user message\n+\n+        filtered_prompts = [prompt for prompt in prompt_messages if not isinstance(prompt, SystemPromptMessage)]",
        "comment_created_at": "2025-04-18T07:09:56+00:00",
        "comment_author": "Nov1c444",
        "comment_body": "You can see the notification in the interface.\r\n<img width=\"428\" alt=\"image\" src=\"https://github.com/user-attachments/assets/85e65604-77ec-4b14-a64d-d16eb934b2e9\" />\r\n",
        "pr_file_module": null
      }
    ]
  }
]