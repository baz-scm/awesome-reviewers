[
  {
    "discussion_id": "2277468618",
    "pr_number": 23827,
    "pr_file": "api/services/workflow_alias_service.py",
    "created_at": "2025-08-14T18:49:47+00:00",
    "commented_code": "+import logging\n+from typing import TYPE_CHECKING, Optional, Union\n+from uuid import uuid4\n+\n+from sqlalchemy import and_, func, select\n+from sqlalchemy.orm import Session\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import scoped_session\n+\n+from models import Workflow, WorkflowAlias\n+from models.workflow_alias import AliasType\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class WorkflowAliasService:\n+    def create_alias(\n+        self,\n+        session: Union[Session, \"scoped_session\"],\n+        tenant_id: str,\n+        app_id: str,\n+        workflow_id: str,\n+        alias_name: str,\n+        alias_type: str = AliasType.CUSTOM.value,\n+        created_by: str | None = None,\n+    ) -> WorkflowAlias:\n+        workflow = session.get(Workflow, workflow_id)\n+        if not workflow:\n+            raise ValueError(f\"Workflow {workflow_id} not found\")\n+\n+        if workflow.version == Workflow.VERSION_DRAFT:\n+            raise ValueError(\"Cannot create or transfer aliases for draft workflows\")\n+\n+        existing_alias = session.execute(\n+            select(WorkflowAlias).where(and_(WorkflowAlias.app_id == app_id, WorkflowAlias.alias_name == alias_name))\n+        ).scalar_one_or_none()\n+\n+        if existing_alias:\n+            old_workflow_id = existing_alias.workflow_id\n+            existing_alias.workflow_id = workflow_id\n+            existing_alias.updated_at = func.current_timestamp()\n+\n+            existing_alias._is_transferred = True\n+            existing_alias._old_workflow_id = old_workflow_id\n+\n+            logger.info(",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2277468618",
        "repo_full_name": "langgenius/dify",
        "pr_number": 23827,
        "pr_file": "api/services/workflow_alias_service.py",
        "discussion_id": "2277468618",
        "commented_code": "@@ -0,0 +1,119 @@\n+import logging\n+from typing import TYPE_CHECKING, Optional, Union\n+from uuid import uuid4\n+\n+from sqlalchemy import and_, func, select\n+from sqlalchemy.orm import Session\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import scoped_session\n+\n+from models import Workflow, WorkflowAlias\n+from models.workflow_alias import AliasType\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class WorkflowAliasService:\n+    def create_alias(\n+        self,\n+        session: Union[Session, \"scoped_session\"],\n+        tenant_id: str,\n+        app_id: str,\n+        workflow_id: str,\n+        alias_name: str,\n+        alias_type: str = AliasType.CUSTOM.value,\n+        created_by: str | None = None,\n+    ) -> WorkflowAlias:\n+        workflow = session.get(Workflow, workflow_id)\n+        if not workflow:\n+            raise ValueError(f\"Workflow {workflow_id} not found\")\n+\n+        if workflow.version == Workflow.VERSION_DRAFT:\n+            raise ValueError(\"Cannot create or transfer aliases for draft workflows\")\n+\n+        existing_alias = session.execute(\n+            select(WorkflowAlias).where(and_(WorkflowAlias.app_id == app_id, WorkflowAlias.alias_name == alias_name))\n+        ).scalar_one_or_none()\n+\n+        if existing_alias:\n+            old_workflow_id = existing_alias.workflow_id\n+            existing_alias.workflow_id = workflow_id\n+            existing_alias.updated_at = func.current_timestamp()\n+\n+            existing_alias._is_transferred = True\n+            existing_alias._old_workflow_id = old_workflow_id\n+\n+            logger.info(",
        "comment_created_at": "2025-08-14T18:49:47+00:00",
        "comment_author": "laipz8200",
        "comment_body": "This log is not meaningful for us; consider removing it or using the DEBUG level.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2277469170",
    "pr_number": 23827,
    "pr_file": "api/services/workflow_alias_service.py",
    "created_at": "2025-08-14T18:50:03+00:00",
    "commented_code": "+import logging\n+from typing import TYPE_CHECKING, Optional, Union\n+from uuid import uuid4\n+\n+from sqlalchemy import and_, func, select\n+from sqlalchemy.orm import Session\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import scoped_session\n+\n+from models import Workflow, WorkflowAlias\n+from models.workflow_alias import AliasType\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class WorkflowAliasService:\n+    def create_alias(\n+        self,\n+        session: Union[Session, \"scoped_session\"],\n+        tenant_id: str,\n+        app_id: str,\n+        workflow_id: str,\n+        alias_name: str,\n+        alias_type: str = AliasType.CUSTOM.value,\n+        created_by: str | None = None,\n+    ) -> WorkflowAlias:\n+        workflow = session.get(Workflow, workflow_id)\n+        if not workflow:\n+            raise ValueError(f\"Workflow {workflow_id} not found\")\n+\n+        if workflow.version == Workflow.VERSION_DRAFT:\n+            raise ValueError(\"Cannot create or transfer aliases for draft workflows\")\n+\n+        existing_alias = session.execute(\n+            select(WorkflowAlias).where(and_(WorkflowAlias.app_id == app_id, WorkflowAlias.alias_name == alias_name))\n+        ).scalar_one_or_none()\n+\n+        if existing_alias:\n+            old_workflow_id = existing_alias.workflow_id\n+            existing_alias.workflow_id = workflow_id\n+            existing_alias.updated_at = func.current_timestamp()\n+\n+            existing_alias._is_transferred = True\n+            existing_alias._old_workflow_id = old_workflow_id\n+\n+            logger.info(\n+                \"Transferred alias '%s' from workflow %s to workflow %s\", alias_name, old_workflow_id, workflow_id\n+            )\n+            return existing_alias\n+\n+        alias = WorkflowAlias(\n+            id=str(uuid4()),\n+            tenant_id=tenant_id,\n+            app_id=app_id,\n+            workflow_id=workflow_id,\n+            alias_name=alias_name,\n+            alias_type=alias_type,\n+            created_by=created_by,\n+        )\n+\n+        session.add(alias)\n+        session.flush()\n+\n+        logger.info(\"Created workflow alias: %s for workflow %s\", alias_name, workflow_id)",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2277469170",
        "repo_full_name": "langgenius/dify",
        "pr_number": 23827,
        "pr_file": "api/services/workflow_alias_service.py",
        "discussion_id": "2277469170",
        "commented_code": "@@ -0,0 +1,119 @@\n+import logging\n+from typing import TYPE_CHECKING, Optional, Union\n+from uuid import uuid4\n+\n+from sqlalchemy import and_, func, select\n+from sqlalchemy.orm import Session\n+\n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import scoped_session\n+\n+from models import Workflow, WorkflowAlias\n+from models.workflow_alias import AliasType\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class WorkflowAliasService:\n+    def create_alias(\n+        self,\n+        session: Union[Session, \"scoped_session\"],\n+        tenant_id: str,\n+        app_id: str,\n+        workflow_id: str,\n+        alias_name: str,\n+        alias_type: str = AliasType.CUSTOM.value,\n+        created_by: str | None = None,\n+    ) -> WorkflowAlias:\n+        workflow = session.get(Workflow, workflow_id)\n+        if not workflow:\n+            raise ValueError(f\"Workflow {workflow_id} not found\")\n+\n+        if workflow.version == Workflow.VERSION_DRAFT:\n+            raise ValueError(\"Cannot create or transfer aliases for draft workflows\")\n+\n+        existing_alias = session.execute(\n+            select(WorkflowAlias).where(and_(WorkflowAlias.app_id == app_id, WorkflowAlias.alias_name == alias_name))\n+        ).scalar_one_or_none()\n+\n+        if existing_alias:\n+            old_workflow_id = existing_alias.workflow_id\n+            existing_alias.workflow_id = workflow_id\n+            existing_alias.updated_at = func.current_timestamp()\n+\n+            existing_alias._is_transferred = True\n+            existing_alias._old_workflow_id = old_workflow_id\n+\n+            logger.info(\n+                \"Transferred alias '%s' from workflow %s to workflow %s\", alias_name, old_workflow_id, workflow_id\n+            )\n+            return existing_alias\n+\n+        alias = WorkflowAlias(\n+            id=str(uuid4()),\n+            tenant_id=tenant_id,\n+            app_id=app_id,\n+            workflow_id=workflow_id,\n+            alias_name=alias_name,\n+            alias_type=alias_type,\n+            created_by=created_by,\n+        )\n+\n+        session.add(alias)\n+        session.flush()\n+\n+        logger.info(\"Created workflow alias: %s for workflow %s\", alias_name, workflow_id)",
        "comment_created_at": "2025-08-14T18:50:03+00:00",
        "comment_author": "laipz8200",
        "comment_body": "This log is not meaningful for us; consider removing it or using the DEBUG level.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2084988849",
    "pr_number": 19551,
    "pr_file": "api/core/callback_handler/index_tool_callback_handler.py",
    "created_at": "2025-05-12T15:52:40+00:00",
    "commented_code": "\"\"\"Handle tool end.\"\"\"\n         for document in documents:\n             if document.metadata is not None:\n-                dataset_document = DatasetDocument.query.filter(\n-                    DatasetDocument.id == document.metadata[\"document_id\"]\n-                ).first()\n+                dataset_document = (\n+                    db.session.query(DatasetDocument)\n+                    .filter(DatasetDocument.id == document.metadata[\"document_id\"])\n+                    .first()\n+                )\n+                if not dataset_document:\n+                    continue",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "2084988849",
        "repo_full_name": "langgenius/dify",
        "pr_number": 19551,
        "pr_file": "api/core/callback_handler/index_tool_callback_handler.py",
        "discussion_id": "2084988849",
        "commented_code": "@@ -42,9 +42,13 @@ def on_tool_end(self, documents: list[Document]) -> None:\n         \"\"\"Handle tool end.\"\"\"\n         for document in documents:\n             if document.metadata is not None:\n-                dataset_document = DatasetDocument.query.filter(\n-                    DatasetDocument.id == document.metadata[\"document_id\"]\n-                ).first()\n+                dataset_document = (\n+                    db.session.query(DatasetDocument)\n+                    .filter(DatasetDocument.id == document.metadata[\"document_id\"])\n+                    .first()\n+                )\n+                if not dataset_document:\n+                    continue",
        "comment_created_at": "2025-05-12T15:52:40+00:00",
        "comment_author": "QuantumGhost",
        "comment_body": "I recommend adding an `warning` level log here to inform the user that the specified document does not exist.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1992875500",
    "pr_number": 15517,
    "pr_file": "api/core/rag/datasource/vdb/pgvector/pgvector.py",
    "created_at": "2025-03-13T06:39:02+00:00",
    "commented_code": "if not ids:\n             return\n         with self._get_cursor() as cur:\n-            cur.execute(f\"DELETE FROM {self.table_name} WHERE id IN %s\", (tuple(ids),))\n+            try:\n+                cur.execute(f\"DELETE FROM {self.table_name} WHERE id IN %s\", (tuple(ids),))\n+            except psycopg2.errors.UndefinedTable:\n+                # table not exists",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "1992875500",
        "repo_full_name": "langgenius/dify",
        "pr_number": 15517,
        "pr_file": "api/core/rag/datasource/vdb/pgvector/pgvector.py",
        "discussion_id": "1992875500",
        "commented_code": "@@ -140,7 +141,13 @@ def delete_by_ids(self, ids: list[str]) -> None:\n         if not ids:\n             return\n         with self._get_cursor() as cur:\n-            cur.execute(f\"DELETE FROM {self.table_name} WHERE id IN %s\", (tuple(ids),))\n+            try:\n+                cur.execute(f\"DELETE FROM {self.table_name} WHERE id IN %s\", (tuple(ids),))\n+            except psycopg2.errors.UndefinedTable:\n+                # table not exists",
        "comment_created_at": "2025-03-13T06:39:02+00:00",
        "comment_author": "laipz8200",
        "comment_body": "It is advisable to log this operation under the warning level.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1797642524",
    "pr_number": 9264,
    "pr_file": "api/core/embedding/cached_embedding.py",
    "created_at": "2024-10-12T09:32:51+00:00",
    "commented_code": "embedding_results = embedding_result.embeddings[0]\n             embedding_results = (embedding_results / np.linalg.norm(embedding_results)).tolist()\n         except Exception as ex:\n+            logging.exception(f\"Failed to embed query text: {ex}\")",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "1797642524",
        "repo_full_name": "langgenius/dify",
        "pr_number": 9264,
        "pr_file": "api/core/embedding/cached_embedding.py",
        "discussion_id": "1797642524",
        "commented_code": "@@ -110,6 +110,7 @@ def embed_query(self, text: str) -> list[float]:\n             embedding_results = embedding_result.embeddings[0]\n             embedding_results = (embedding_results / np.linalg.norm(embedding_results)).tolist()\n         except Exception as ex:\n+            logging.exception(f\"Failed to embed query text: {ex}\")",
        "comment_created_at": "2024-10-12T09:32:51+00:00",
        "comment_author": "crazywoola",
        "comment_body": "For new exceptions, please add a `DEBUG` check. Eg. if DEBUG then log something.",
        "pr_file_module": null
      },
      {
        "comment_id": "1797701624",
        "repo_full_name": "langgenius/dify",
        "pr_number": 9264,
        "pr_file": "api/core/embedding/cached_embedding.py",
        "discussion_id": "1797642524",
        "commented_code": "@@ -110,6 +110,7 @@ def embed_query(self, text: str) -> list[float]:\n             embedding_results = embedding_result.embeddings[0]\n             embedding_results = (embedding_results / np.linalg.norm(embedding_results)).tolist()\n         except Exception as ex:\n+            logging.exception(f\"Failed to embed query text: {ex}\")",
        "comment_created_at": "2024-10-12T13:24:20+00:00",
        "comment_author": "hwzhuhao",
        "comment_body": "@crazywoola done, pls help me to review again.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1798790754",
    "pr_number": 9300,
    "pr_file": "api/controllers/service_api/dataset/hit_testing.py",
    "created_at": "2024-10-14T06:00:55+00:00",
    "commented_code": "+import logging\n+\n+from flask_login import current_user\n+from flask_restful import marshal, reqparse\n+from werkzeug.exceptions import Forbidden, InternalServerError, NotFound\n+\n+import services.dataset_service\n+from controllers.console.app.error import (\n+    CompletionRequestError,\n+    ProviderModelCurrentlyNotSupportError,\n+    ProviderNotInitializeError,\n+    ProviderQuotaExceededError,\n+)\n+from controllers.console.datasets.error import DatasetNotInitializedError\n+from controllers.service_api import api\n+from controllers.service_api.wraps import DatasetApiResource\n+from core.errors.error import (\n+    LLMBadRequestError,\n+    ModelCurrentlyNotSupportError,\n+    ProviderTokenNotInitError,\n+    QuotaExceededError,\n+)\n+from core.model_runtime.errors.invoke import InvokeError\n+from fields.hit_testing_fields import hit_testing_record_fields\n+from services.dataset_service import DatasetService\n+from services.hit_testing_service import HitTestingService\n+\n+\n+class HitTestingApi(DatasetApiResource):\n+    def post(self, tenant_id, dataset_id):\n+        dataset_id_str = str(dataset_id)\n+        logging.error(f\"{dataset_id_str}\")",
    "repo_full_name": "langgenius/dify",
    "discussion_comments": [
      {
        "comment_id": "1798790754",
        "repo_full_name": "langgenius/dify",
        "pr_number": 9300,
        "pr_file": "api/controllers/service_api/dataset/hit_testing.py",
        "discussion_id": "1798790754",
        "commented_code": "@@ -0,0 +1,85 @@\n+import logging\n+\n+from flask_login import current_user\n+from flask_restful import marshal, reqparse\n+from werkzeug.exceptions import Forbidden, InternalServerError, NotFound\n+\n+import services.dataset_service\n+from controllers.console.app.error import (\n+    CompletionRequestError,\n+    ProviderModelCurrentlyNotSupportError,\n+    ProviderNotInitializeError,\n+    ProviderQuotaExceededError,\n+)\n+from controllers.console.datasets.error import DatasetNotInitializedError\n+from controllers.service_api import api\n+from controllers.service_api.wraps import DatasetApiResource\n+from core.errors.error import (\n+    LLMBadRequestError,\n+    ModelCurrentlyNotSupportError,\n+    ProviderTokenNotInitError,\n+    QuotaExceededError,\n+)\n+from core.model_runtime.errors.invoke import InvokeError\n+from fields.hit_testing_fields import hit_testing_record_fields\n+from services.dataset_service import DatasetService\n+from services.hit_testing_service import HitTestingService\n+\n+\n+class HitTestingApi(DatasetApiResource):\n+    def post(self, tenant_id, dataset_id):\n+        dataset_id_str = str(dataset_id)\n+        logging.error(f\"{dataset_id_str}\")",
        "comment_created_at": "2024-10-14T06:00:55+00:00",
        "comment_author": "hwzhuhao",
        "comment_body": "This is debug print information, right? It can be removed.",
        "pr_file_module": null
      },
      {
        "comment_id": "1798791543",
        "repo_full_name": "langgenius/dify",
        "pr_number": 9300,
        "pr_file": "api/controllers/service_api/dataset/hit_testing.py",
        "discussion_id": "1798790754",
        "commented_code": "@@ -0,0 +1,85 @@\n+import logging\n+\n+from flask_login import current_user\n+from flask_restful import marshal, reqparse\n+from werkzeug.exceptions import Forbidden, InternalServerError, NotFound\n+\n+import services.dataset_service\n+from controllers.console.app.error import (\n+    CompletionRequestError,\n+    ProviderModelCurrentlyNotSupportError,\n+    ProviderNotInitializeError,\n+    ProviderQuotaExceededError,\n+)\n+from controllers.console.datasets.error import DatasetNotInitializedError\n+from controllers.service_api import api\n+from controllers.service_api.wraps import DatasetApiResource\n+from core.errors.error import (\n+    LLMBadRequestError,\n+    ModelCurrentlyNotSupportError,\n+    ProviderTokenNotInitError,\n+    QuotaExceededError,\n+)\n+from core.model_runtime.errors.invoke import InvokeError\n+from fields.hit_testing_fields import hit_testing_record_fields\n+from services.dataset_service import DatasetService\n+from services.hit_testing_service import HitTestingService\n+\n+\n+class HitTestingApi(DatasetApiResource):\n+    def post(self, tenant_id, dataset_id):\n+        dataset_id_str = str(dataset_id)\n+        logging.error(f\"{dataset_id_str}\")",
        "comment_created_at": "2024-10-14T06:01:41+00:00",
        "comment_author": "hwzhuhao",
        "comment_body": "If it's really needed, change the log level to info.",
        "pr_file_module": null
      }
    ]
  }
]