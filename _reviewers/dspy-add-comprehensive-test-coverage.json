[
  {
    "discussion_id": "1868394192",
    "pr_number": 1874,
    "pr_file": "docs/docs/tutorials/deployment/index.md",
    "created_at": "2024-12-03T21:32:26+00:00",
    "commented_code": "}\n     except Exception as e:\n         raise HTTPException(status_code=500, detail=str(e))\n+\n+# Define an endpoint (streaming)\n+from fastapi.responses import StreamingResponse\n+\n+@app.post(\"/predict/stream\")\n+async def stream(question: Question):\n+    async def generate():\n+        async for value in streaming_dspy_program(question=question.text):\n+            if isinstance(value, dspy.Prediction):\n+                data = {\"prediction\": value.labels().toDict()}\n+            elif isinstance(value, litellm.ModelResponse):\n+                data = {\"chunk\": value.json()}\n+            yield f\"data: {ujson.dumps(data)}\n\n\"\n+        yield \"data: [DONE]\n\n\"\n+\n+    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n+\n+# Since you're often going to want to stream the result of a DSPy program as server-sent events,\n+# we've included a helper function for that, which is equivalent to the code above.\n+\n+from dspy.utils.streaming import streaming_response\n+\n+@app.post(\"/predict/stream\")\n+async def stream(question: Question):\n+    stream = streaming_dspy_program(question=question.text)\n+    return StreamingResponse(streaming_response(stream), media_type=\"text/event-stream\")",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1868394192",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "docs/docs/tutorials/deployment/index.md",
        "discussion_id": "1868394192",
        "commented_code": "@@ -54,14 +56,45 @@ async def predict(question: Question):\n         }\n     except Exception as e:\n         raise HTTPException(status_code=500, detail=str(e))\n+\n+# Define an endpoint (streaming)\n+from fastapi.responses import StreamingResponse\n+\n+@app.post(\"/predict/stream\")\n+async def stream(question: Question):\n+    async def generate():\n+        async for value in streaming_dspy_program(question=question.text):\n+            if isinstance(value, dspy.Prediction):\n+                data = {\"prediction\": value.labels().toDict()}\n+            elif isinstance(value, litellm.ModelResponse):\n+                data = {\"chunk\": value.json()}\n+            yield f\"data: {ujson.dumps(data)}\\n\\n\"\n+        yield \"data: [DONE]\\n\\n\"\n+\n+    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n+\n+# Since you're often going to want to stream the result of a DSPy program as server-sent events,\n+# we've included a helper function for that, which is equivalent to the code above.\n+\n+from dspy.utils.streaming import streaming_response\n+\n+@app.post(\"/predict/stream\")\n+async def stream(question: Question):\n+    stream = streaming_dspy_program(question=question.text)\n+    return StreamingResponse(streaming_response(stream), media_type=\"text/event-stream\")",
        "comment_created_at": "2024-12-03T21:32:26+00:00",
        "comment_author": "dbczumar",
        "comment_body": "Awesome example! Not sure if you have bandwidth, but it would be great to introduce test coverage like this as well by:\r\n\r\n1. Extending https://github.com/stanfordnlp/dspy/blob/main/tests/test_utils/server/litellm_server.py to introduce a streaming endpoint\r\n\r\n2. Writing a test case that uses `litellm_test_server` + streaming, similar to https://github.com/stanfordnlp/dspy/blob/97ca14cdae61b14bb1c2d520ba559e710d2cb7f6/tests/clients/test_lm.py#L9 (instead of calling the LM directly, pass it to `dspy.configure` and run a program that uses `streamify()`). Perhaps we can place the test in a file called `tests/streaming.py` \r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1870409341",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "docs/docs/tutorials/deployment/index.md",
        "discussion_id": "1868394192",
        "commented_code": "@@ -54,14 +56,45 @@ async def predict(question: Question):\n         }\n     except Exception as e:\n         raise HTTPException(status_code=500, detail=str(e))\n+\n+# Define an endpoint (streaming)\n+from fastapi.responses import StreamingResponse\n+\n+@app.post(\"/predict/stream\")\n+async def stream(question: Question):\n+    async def generate():\n+        async for value in streaming_dspy_program(question=question.text):\n+            if isinstance(value, dspy.Prediction):\n+                data = {\"prediction\": value.labels().toDict()}\n+            elif isinstance(value, litellm.ModelResponse):\n+                data = {\"chunk\": value.json()}\n+            yield f\"data: {ujson.dumps(data)}\\n\\n\"\n+        yield \"data: [DONE]\\n\\n\"\n+\n+    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n+\n+# Since you're often going to want to stream the result of a DSPy program as server-sent events,\n+# we've included a helper function for that, which is equivalent to the code above.\n+\n+from dspy.utils.streaming import streaming_response\n+\n+@app.post(\"/predict/stream\")\n+async def stream(question: Question):\n+    stream = streaming_dspy_program(question=question.text)\n+    return StreamingResponse(streaming_response(stream), media_type=\"text/event-stream\")",
        "comment_created_at": "2024-12-04T23:24:13+00:00",
        "comment_author": "CyrusNuevoDia",
        "comment_body": "@dbczumar would love your help on this, I'm stuck on the `miprov2-knn` branch for benchmarking atm",
        "pr_file_module": null
      },
      {
        "comment_id": "1870861590",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "docs/docs/tutorials/deployment/index.md",
        "discussion_id": "1868394192",
        "commented_code": "@@ -54,14 +56,45 @@ async def predict(question: Question):\n         }\n     except Exception as e:\n         raise HTTPException(status_code=500, detail=str(e))\n+\n+# Define an endpoint (streaming)\n+from fastapi.responses import StreamingResponse\n+\n+@app.post(\"/predict/stream\")\n+async def stream(question: Question):\n+    async def generate():\n+        async for value in streaming_dspy_program(question=question.text):\n+            if isinstance(value, dspy.Prediction):\n+                data = {\"prediction\": value.labels().toDict()}\n+            elif isinstance(value, litellm.ModelResponse):\n+                data = {\"chunk\": value.json()}\n+            yield f\"data: {ujson.dumps(data)}\\n\\n\"\n+        yield \"data: [DONE]\\n\\n\"\n+\n+    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n+\n+# Since you're often going to want to stream the result of a DSPy program as server-sent events,\n+# we've included a helper function for that, which is equivalent to the code above.\n+\n+from dspy.utils.streaming import streaming_response\n+\n+@app.post(\"/predict/stream\")\n+async def stream(question: Question):\n+    stream = streaming_dspy_program(question=question.text)\n+    return StreamingResponse(streaming_response(stream), media_type=\"text/event-stream\")",
        "comment_created_at": "2024-12-05T08:07:35+00:00",
        "comment_author": "dbczumar",
        "comment_body": "Done :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1833550957",
    "pr_number": 1773,
    "pr_file": "tests/quality/README.md",
    "created_at": "2024-11-08T00:45:28+00:00",
    "commented_code": "+# DSPy Quality Tests\n+\n+This directory contains quality tests for DSPy programs. The purpose of these tests is to verify that DSPy programs produce high-quality outputs across multiple large language models (LLMs), regardless of model size or capability. These tests are designed to ensure that DSPy programs maintain robustness and accuracy across diverse LLM configurations.\n+\n+### Overview\n+\n+Each test in this directory executes a DSPy program using various LLMs. By running the same tests across different models, these tests help validate that DSPy programs handle a wide range of inputs effectively and produce reliable outputs, even in cases where the model might struggle with the input or task.\n+\n+### Key Features\n+\n+- **Diverse LLMs**: Each DSPy program is tested with multiple LLMs, ranging from smaller models to more advanced, high-performance models. This approach allows us to assess the consistency and generality of DSPy program outputs across different model capabilities.\n+- **Challenging and Adversarial Tests**: Some of the tests are intentionally challenging or adversarial, crafted to push the boundaries of DSPy. These challenging cases allow us to gauge the robustness of DSPy and identify areas for potential improvement.\n+- **Cross-Model Compatibility**: By testing with different LLMs, we aim to ensure that DSPy programs perform well across model types and configurations, reducing model-specific edge cases and enhancing program versatility.\n+\n+### Running the Tests\n+\n+- First, populate the configuration file `quality_tests_conf.yaml` (located in this directory) with the necessary LiteLLM model/provider names and access credentials for 1. each LLM you want to test and 2. the LLM judge that you want to use for assessing the correctness of outputs in certain test cases. These should be placed in the `litellm_params` section for each model in the defined `model_list`. You can also use `litellm_params` to specify values for LLM hyperparameters like `temperature`. Any model that lacks configured `litellm_params` in the configuration file will be ignored during testing.\n+\n+  The configuration must also specify a DSPy adapter to use when testing, e.g. `\"chat\"` (for `dspy.ChatAdapter`) or `\"json\"` (for `dspy.JSONAdapter`)\n+\n+  An example of `quality_tests_conf.yaml`:\n+\n+      ```yaml\n+      adapter: chat\n+      model_list:\n+        # The model to use for judging the correctness of program\n+        # outputs throughout quality test suites. We recommend using\n+        # a high quality model as the judge, such as OpenAI GPT-4o\n+        - model_name: \"judge\"\n+          litellm_params:\n+            model: \"openai/gpt-4o\"\n+            api_key: \"<my_openai_api_key>\"\n+        - model_name: \"gpt-4o\"\n+          litellm_params:\n+            model: \"openai/gpt-4o\"\n+            api_key: \"<my_openai_api_key>\"\n+        - model_name: \"claude-3.5-sonnet\"\n+          litellm_params:\n+            model: \"anthropic/claude-3.5\"\n+            api_key: \"<my_anthropic_api_key>\"\n+\n+- Second, to run the tests, run the following command from this directory:\n+\n+  ```bash\n+      pytest .\n+  ```\n+\n+  This will execute all tests for the configured models and display detailed results for each model configuration. Tests are set up to mark expected failures for known challenging cases where a specific model might struggle, while actual (unexpected) DSPy quality issues are flagged as failures (see below).",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1833550957",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1773,
        "pr_file": "tests/quality/README.md",
        "discussion_id": "1833550957",
        "commented_code": "@@ -0,0 +1,58 @@\n+# DSPy Quality Tests\n+\n+This directory contains quality tests for DSPy programs. The purpose of these tests is to verify that DSPy programs produce high-quality outputs across multiple large language models (LLMs), regardless of model size or capability. These tests are designed to ensure that DSPy programs maintain robustness and accuracy across diverse LLM configurations.\n+\n+### Overview\n+\n+Each test in this directory executes a DSPy program using various LLMs. By running the same tests across different models, these tests help validate that DSPy programs handle a wide range of inputs effectively and produce reliable outputs, even in cases where the model might struggle with the input or task.\n+\n+### Key Features\n+\n+- **Diverse LLMs**: Each DSPy program is tested with multiple LLMs, ranging from smaller models to more advanced, high-performance models. This approach allows us to assess the consistency and generality of DSPy program outputs across different model capabilities.\n+- **Challenging and Adversarial Tests**: Some of the tests are intentionally challenging or adversarial, crafted to push the boundaries of DSPy. These challenging cases allow us to gauge the robustness of DSPy and identify areas for potential improvement.\n+- **Cross-Model Compatibility**: By testing with different LLMs, we aim to ensure that DSPy programs perform well across model types and configurations, reducing model-specific edge cases and enhancing program versatility.\n+\n+### Running the Tests\n+\n+- First, populate the configuration file `quality_tests_conf.yaml` (located in this directory) with the necessary LiteLLM model/provider names and access credentials for 1. each LLM you want to test and 2. the LLM judge that you want to use for assessing the correctness of outputs in certain test cases. These should be placed in the `litellm_params` section for each model in the defined `model_list`. You can also use `litellm_params` to specify values for LLM hyperparameters like `temperature`. Any model that lacks configured `litellm_params` in the configuration file will be ignored during testing.\n+\n+  The configuration must also specify a DSPy adapter to use when testing, e.g. `\"chat\"` (for `dspy.ChatAdapter`) or `\"json\"` (for `dspy.JSONAdapter`)\n+\n+  An example of `quality_tests_conf.yaml`:\n+\n+      ```yaml\n+      adapter: chat\n+      model_list:\n+        # The model to use for judging the correctness of program\n+        # outputs throughout quality test suites. We recommend using\n+        # a high quality model as the judge, such as OpenAI GPT-4o\n+        - model_name: \"judge\"\n+          litellm_params:\n+            model: \"openai/gpt-4o\"\n+            api_key: \"<my_openai_api_key>\"\n+        - model_name: \"gpt-4o\"\n+          litellm_params:\n+            model: \"openai/gpt-4o\"\n+            api_key: \"<my_openai_api_key>\"\n+        - model_name: \"claude-3.5-sonnet\"\n+          litellm_params:\n+            model: \"anthropic/claude-3.5\"\n+            api_key: \"<my_anthropic_api_key>\"\n+\n+- Second, to run the tests, run the following command from this directory:\n+\n+  ```bash\n+      pytest .\n+  ```\n+\n+  This will execute all tests for the configured models and display detailed results for each model configuration. Tests are set up to mark expected failures for known challenging cases where a specific model might struggle, while actual (unexpected) DSPy quality issues are flagged as failures (see below).",
        "comment_created_at": "2024-11-08T00:45:28+00:00",
        "comment_author": "dbczumar",
        "comment_body": "As a follow-up, I'll write a pytest post suite hook that produces a report summarizing the quality results by model. Eventually, we can publish this report as we make DSPy releases",
        "pr_file_module": null
      }
    ]
  }
]