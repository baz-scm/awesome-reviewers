[
  {
    "discussion_id": "2269228904",
    "pr_number": 8562,
    "pr_file": "docs/README.md",
    "created_at": "2025-08-12T09:16:31+00:00",
    "commented_code": "2. **Push your new changes on a new branch**: Feel free to add or edit existing documentation and open a PR for your changes. Once your PR is reviewed and approved, the changes will be ready to merge into main. \n \n-3. **Updating the website**: Once your changes are merged to main, the changes would be reflected on live websites usually in 5-15 mins.\n\\ No newline at end of file\n+3. **Updating the website**: Once your changes are merged to main, the changes would be reflected on live websites usually in 5-15 mins.\n+\n+## LLMs.txt Files\n+\n+DSPy documentation automatically generates [llmstxt.org](https://llmstxt.org/) compliant files to help LLMs understand and work with DSPy:\n+\n+- **`/llms.txt`** - A structured overview with links to key documentation sections",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2269228904",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8562,
        "pr_file": "docs/README.md",
        "discussion_id": "2269228904",
        "commented_code": "@@ -72,4 +72,14 @@ This guide is for contributors looking to make changes to the documentation in t\n \n 2. **Push your new changes on a new branch**: Feel free to add or edit existing documentation and open a PR for your changes. Once your PR is reviewed and approved, the changes will be ready to merge into main. \n \n-3. **Updating the website**: Once your changes are merged to main, the changes would be reflected on live websites usually in 5-15 mins.\n\\ No newline at end of file\n+3. **Updating the website**: Once your changes are merged to main, the changes would be reflected on live websites usually in 5-15 mins.\n+\n+## LLMs.txt Files\n+\n+DSPy documentation automatically generates [llmstxt.org](https://llmstxt.org/) compliant files to help LLMs understand and work with DSPy:\n+\n+- **`/llms.txt`** - A structured overview with links to key documentation sections",
        "comment_created_at": "2025-08-12T09:16:31+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Can we start only with `llms.txt`? `llms-full.txt` might be overwhelming",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1844580979",
    "pr_number": 1804,
    "pr_file": "docs/docs/tutorials/deployment/deploy_dspy_model.md",
    "created_at": "2024-11-15T22:10:14+00:00",
    "commented_code": "+# Deploy DSPy Model\n+\n+This guide demonstrates two popular approaches to deploy your DSPy models in production:\n+\n+1. FastAPI - For simple, lightweight deployments with direct model access\n+2. MLflow - For production-grade deployments with model versioning and management\n+\n+## Deploying with FastAPI\n+\n+FastAPI offers a quick and straightforward way to serve your DSPy model as a REST API. This approach is ideal when you have direct access to your model code and need a lightweight deployment solution.\n+\n+Before we get started, let's install the required libraries:\n+\n+```bash\n+pip install fastapi uvicorn\n+```\n+\n+And remember to set your OpenAI API key which is used by our example:\n+\n+```bash\n+export OPENAI_API_KEY=\"your-openai-api-key\"\n+```\n+\n+Here's a minimal example of a DSPy model:\n+\n+```python\n+import dspy\n+\n+lm = dspy.LM(\"openai/gpt-4o-mini\")\n+dspy.settings.configure(lm=lm)\n+dspy_model = dspy.ChainOfThought(\"question -> answer\")\n+```\n+\n+Create a FastAPI application to serve this model:\n+\n+```python\n+from fastapi import FastAPI, HTTPException\n+from pydantic import BaseModel\n+\n+import dspy\n+import asyncio\n+\n+app = FastAPI(\n+    title=\"DSPy Model API\",\n+    description=\"A simple API serving a DSPy Chain of Thought model\",\n+    version=\"1.0.0\"\n+)\n+\n+# Define request model for better documentation and validation\n+class Question(BaseModel):\n+    text: str\n+\n+# Configure your language model and Chain of Thought\n+lm = dspy.LM(\"openai/gpt-4o-mini\")\n+dspy.settings.configure(lm=lm)\n+dspy_model = dspy.ChainOfThought(\"question -> answer\")",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1844580979",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1804,
        "pr_file": "docs/docs/tutorials/deployment/deploy_dspy_model.md",
        "discussion_id": "1844580979",
        "commented_code": "@@ -0,0 +1,185 @@\n+# Deploy DSPy Model\n+\n+This guide demonstrates two popular approaches to deploy your DSPy models in production:\n+\n+1. FastAPI - For simple, lightweight deployments with direct model access\n+2. MLflow - For production-grade deployments with model versioning and management\n+\n+## Deploying with FastAPI\n+\n+FastAPI offers a quick and straightforward way to serve your DSPy model as a REST API. This approach is ideal when you have direct access to your model code and need a lightweight deployment solution.\n+\n+Before we get started, let's install the required libraries:\n+\n+```bash\n+pip install fastapi uvicorn\n+```\n+\n+And remember to set your OpenAI API key which is used by our example:\n+\n+```bash\n+export OPENAI_API_KEY=\"your-openai-api-key\"\n+```\n+\n+Here's a minimal example of a DSPy model:\n+\n+```python\n+import dspy\n+\n+lm = dspy.LM(\"openai/gpt-4o-mini\")\n+dspy.settings.configure(lm=lm)\n+dspy_model = dspy.ChainOfThought(\"question -> answer\")\n+```\n+\n+Create a FastAPI application to serve this model:\n+\n+```python\n+from fastapi import FastAPI, HTTPException\n+from pydantic import BaseModel\n+\n+import dspy\n+import asyncio\n+\n+app = FastAPI(\n+    title=\"DSPy Model API\",\n+    description=\"A simple API serving a DSPy Chain of Thought model\",\n+    version=\"1.0.0\"\n+)\n+\n+# Define request model for better documentation and validation\n+class Question(BaseModel):\n+    text: str\n+\n+# Configure your language model and Chain of Thought\n+lm = dspy.LM(\"openai/gpt-4o-mini\")\n+dspy.settings.configure(lm=lm)\n+dspy_model = dspy.ChainOfThought(\"question -> answer\")",
        "comment_created_at": "2024-11-15T22:10:14+00:00",
        "comment_author": "CyrusNuevoDia",
        "comment_body": "Can you merge in #1806 and then update this to something like\r\n\r\n```python\r\ndspy.settings.configure(lm=lm, async_capacity=16) # max 16 concurrent DSPy programs (default 8)\r\ndspy_model = dspy.asyncify(dspy.ChainOfThought(\"question -> answer\"))\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1833565466",
    "pr_number": 1773,
    "pr_file": "tests/quality/README.md",
    "created_at": "2024-11-08T01:06:14+00:00",
    "commented_code": "+# DSPy Quality Tests\n+\n+This directory contains quality tests for DSPy programs. The purpose of these tests is to verify that DSPy programs produce high-quality outputs across multiple large language models (LLMs), regardless of model size or capability. These tests are designed to ensure that DSPy programs maintain robustness and accuracy across diverse LLM configurations.\n+\n+### Overview\n+\n+Each test in this directory executes a DSPy program using various LLMs. By running the same tests across different models, these tests help validate that DSPy programs handle a wide range of inputs effectively and produce reliable outputs, even in cases where the model might struggle with the input or task.\n+\n+### Key Features\n+\n+- **Diverse LLMs**: Each DSPy program is tested with multiple LLMs, ranging from smaller models to more advanced, high-performance models. This approach allows us to assess the consistency and generality of DSPy program outputs across different model capabilities.\n+- **Challenging and Adversarial Tests**: Some of the tests are intentionally challenging or adversarial, crafted to push the boundaries of DSPy. These challenging cases allow us to gauge the robustness of DSPy and identify areas for potential improvement.\n+- **Cross-Model Compatibility**: By testing with different LLMs, we aim to ensure that DSPy programs perform well across model types and configurations, reducing model-specific edge cases and enhancing program versatility.\n+\n+### Running the Tests\n+\n+- First, populate the configuration file `quality_tests_conf.yaml` (located in this directory) with the necessary LiteLLM model/provider names and access credentials for 1. each LLM you want to test and 2. the LLM judge that you want to use for assessing the correctness of outputs in certain test cases. These should be placed in the `litellm_params` section for each model in the defined `model_list`. You can also use `litellm_params` to specify values for LLM hyperparameters like `temperature`. Any model that lacks configured `litellm_params` in the configuration file will be ignored during testing.",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1833565466",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1773,
        "pr_file": "tests/quality/README.md",
        "discussion_id": "1833565466",
        "commented_code": "@@ -0,0 +1,58 @@\n+# DSPy Quality Tests\n+\n+This directory contains quality tests for DSPy programs. The purpose of these tests is to verify that DSPy programs produce high-quality outputs across multiple large language models (LLMs), regardless of model size or capability. These tests are designed to ensure that DSPy programs maintain robustness and accuracy across diverse LLM configurations.\n+\n+### Overview\n+\n+Each test in this directory executes a DSPy program using various LLMs. By running the same tests across different models, these tests help validate that DSPy programs handle a wide range of inputs effectively and produce reliable outputs, even in cases where the model might struggle with the input or task.\n+\n+### Key Features\n+\n+- **Diverse LLMs**: Each DSPy program is tested with multiple LLMs, ranging from smaller models to more advanced, high-performance models. This approach allows us to assess the consistency and generality of DSPy program outputs across different model capabilities.\n+- **Challenging and Adversarial Tests**: Some of the tests are intentionally challenging or adversarial, crafted to push the boundaries of DSPy. These challenging cases allow us to gauge the robustness of DSPy and identify areas for potential improvement.\n+- **Cross-Model Compatibility**: By testing with different LLMs, we aim to ensure that DSPy programs perform well across model types and configurations, reducing model-specific edge cases and enhancing program versatility.\n+\n+### Running the Tests\n+\n+- First, populate the configuration file `quality_tests_conf.yaml` (located in this directory) with the necessary LiteLLM model/provider names and access credentials for 1. each LLM you want to test and 2. the LLM judge that you want to use for assessing the correctness of outputs in certain test cases. These should be placed in the `litellm_params` section for each model in the defined `model_list`. You can also use `litellm_params` to specify values for LLM hyperparameters like `temperature`. Any model that lacks configured `litellm_params` in the configuration file will be ignored during testing.",
        "comment_created_at": "2024-11-08T01:06:14+00:00",
        "comment_author": "dbczumar",
        "comment_body": "At some point, it seems plausible for us to define recommended hyperparameter configurations for specific LMs that are known to produce better performance (e.g. temperature)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1590347282",
    "pr_number": 951,
    "pr_file": "docs/docs/building-blocks/6-optimizers.md",
    "created_at": "2024-05-05T15:59:40+00:00",
    "commented_code": "#### Automatic Few-Shot Learning\n \n-These optimizers extend the signature by adding labeled examples to the prompt that will be submitted to the model, automatically implementing few-shot learning.\n+These optimizers extend the signature by automatically generating and including **optimized** examples within the prompt sent to the model, implementing few-shot learning.\n \n-1. **`LabeledFewShot`**: Simply constructs few-shot examples (demos) from provided labeled Q/A pairs.  Takes as parameter, `k`, the number of examples to be added to the prompt and chooses `k` examples randomly from the `trainset` parameter.\n+1. **`LabeledFewShot`**: Simply constructs few-shot examples (demos) from provided labeled input and output data points.  Requires `k` (number of examples for the prompt) and `trainset` to randomly select `k` examples from.\n \n-2. **`BootstrapFewShot`**: Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program in addition to the labeled examples in `trainset`. This is primarily useful if there are potentially many different ways to phrase the answer to a query, and you do not have a training set that covers the alternative phrasings. In this case a language model can be used to \"bootstrap,\" to generate additional answers to the questions in the training set.  Takes as parameters `max_labeled_demos` (the number of demonstrations to choose randomly from the `trainset`), and `max_bootstrapped_demos` (the number of additional examples to be generated by the `teacher`).  The bootstrapping process uses the `metric` to determine whether to accept or reject bootstrapped answers.  Will simply use the generated demonstrations (if they pass the metric) without any further optimization; i.e., does not check to see if the program's performance is improved by the compilation. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.\n+2. **`BootstrapFewShot`**: Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in `trainset`. Parameters include `max_labeled_demos` (the number of demonstrations randomly selected from the `trainset`) and `max_bootstrapped_demos` (the number of additional examples generated by the `teacher`). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the \"compiled\" prompt. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.\n \n-3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program.  Takes as parameter (in addition to the ones for `BootstrapFewShot`) `num_candidate_programs`: the number of random programs, each generated through `BootstrapFewShot`.  Picks a single best based on scores over the `valset` (which defaults to being the same as the `trainset`).\n+3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of `BootstrapFewShot`, with the addition of `num_candidate_programs`, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, `LabeledFewShot` optimized program, `BootstrapFewShot` compiled program with unshuffled examples and `num_candidate_programs` of `BootstrapFewShot` compiled programs with randomized example sets. \n \n-4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics. As before, chooses the best of a candidate set of demonstration examples, but this time using Optuna to make the choice. <!--- TBQH, I don't understand how Optuna does this. As far as I can tell it simply chooses best based on multiple evaluations, rather than a single one, and mention of \"hyperparameters\" seems to be a red herring. ---> \n+4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics and selecting the best demonstrations.",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1590347282",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 951,
        "pr_file": "docs/docs/building-blocks/6-optimizers.md",
        "discussion_id": "1590347282",
        "commented_code": "@@ -38,36 +38,32 @@ All of these can be accessed via `from dspy.teleprompt import *`.\n \n #### Automatic Few-Shot Learning\n \n-These optimizers extend the signature by adding labeled examples to the prompt that will be submitted to the model, automatically implementing few-shot learning.\n+These optimizers extend the signature by automatically generating and including **optimized** examples within the prompt sent to the model, implementing few-shot learning.\n \n-1. **`LabeledFewShot`**: Simply constructs few-shot examples (demos) from provided labeled Q/A pairs.  Takes as parameter, `k`, the number of examples to be added to the prompt and chooses `k` examples randomly from the `trainset` parameter.\n+1. **`LabeledFewShot`**: Simply constructs few-shot examples (demos) from provided labeled input and output data points.  Requires `k` (number of examples for the prompt) and `trainset` to randomly select `k` examples from.\n \n-2. **`BootstrapFewShot`**: Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program in addition to the labeled examples in `trainset`. This is primarily useful if there are potentially many different ways to phrase the answer to a query, and you do not have a training set that covers the alternative phrasings. In this case a language model can be used to \"bootstrap,\" to generate additional answers to the questions in the training set.  Takes as parameters `max_labeled_demos` (the number of demonstrations to choose randomly from the `trainset`), and `max_bootstrapped_demos` (the number of additional examples to be generated by the `teacher`).  The bootstrapping process uses the `metric` to determine whether to accept or reject bootstrapped answers.  Will simply use the generated demonstrations (if they pass the metric) without any further optimization; i.e., does not check to see if the program's performance is improved by the compilation. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.\n+2. **`BootstrapFewShot`**: Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in `trainset`. Parameters include `max_labeled_demos` (the number of demonstrations randomly selected from the `trainset`) and `max_bootstrapped_demos` (the number of additional examples generated by the `teacher`). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the \"compiled\" prompt. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.\n \n-3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program.  Takes as parameter (in addition to the ones for `BootstrapFewShot`) `num_candidate_programs`: the number of random programs, each generated through `BootstrapFewShot`.  Picks a single best based on scores over the `valset` (which defaults to being the same as the `trainset`).\n+3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of `BootstrapFewShot`, with the addition of `num_candidate_programs`, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, `LabeledFewShot` optimized program, `BootstrapFewShot` compiled program with unshuffled examples and `num_candidate_programs` of `BootstrapFewShot` compiled programs with randomized example sets. \n \n-4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics. As before, chooses the best of a candidate set of demonstration examples, but this time using Optuna to make the choice. <!--- TBQH, I don't understand how Optuna does this. As far as I can tell it simply chooses best based on multiple evaluations, rather than a single one, and mention of \"hyperparameters\" seems to be a red herring. ---> \n+4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics and selecting the best demonstrations. ",
        "comment_created_at": "2024-05-05T15:59:40+00:00",
        "comment_author": "rpgoldman",
        "comment_body": "@arnavsinghvi11 The thing that confuses me about this is the use of the term \"hyperparameter optimization.\" I don't _think_ there are any hyperparameters (the number of examples to include in the prompt, the number of samples to take from the trainset and the number of samples to generate) actually optimized here, are there? Doesn't this just optimize the choice of examples to maximize the metric? In that case, we are just optimizing _parameters_, not hyperparameters. That's why I said that the mention of hyperparameters was misleading.\r\n\r\nOr am I missing something?  It's hard to tell from just looking at the code in dspy what Optuna is doing, and I didn't have time to read over Optuna.",
        "pr_file_module": null
      },
      {
        "comment_id": "1590437696",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 951,
        "pr_file": "docs/docs/building-blocks/6-optimizers.md",
        "discussion_id": "1590347282",
        "commented_code": "@@ -38,36 +38,32 @@ All of these can be accessed via `from dspy.teleprompt import *`.\n \n #### Automatic Few-Shot Learning\n \n-These optimizers extend the signature by adding labeled examples to the prompt that will be submitted to the model, automatically implementing few-shot learning.\n+These optimizers extend the signature by automatically generating and including **optimized** examples within the prompt sent to the model, implementing few-shot learning.\n \n-1. **`LabeledFewShot`**: Simply constructs few-shot examples (demos) from provided labeled Q/A pairs.  Takes as parameter, `k`, the number of examples to be added to the prompt and chooses `k` examples randomly from the `trainset` parameter.\n+1. **`LabeledFewShot`**: Simply constructs few-shot examples (demos) from provided labeled input and output data points.  Requires `k` (number of examples for the prompt) and `trainset` to randomly select `k` examples from.\n \n-2. **`BootstrapFewShot`**: Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program in addition to the labeled examples in `trainset`. This is primarily useful if there are potentially many different ways to phrase the answer to a query, and you do not have a training set that covers the alternative phrasings. In this case a language model can be used to \"bootstrap,\" to generate additional answers to the questions in the training set.  Takes as parameters `max_labeled_demos` (the number of demonstrations to choose randomly from the `trainset`), and `max_bootstrapped_demos` (the number of additional examples to be generated by the `teacher`).  The bootstrapping process uses the `metric` to determine whether to accept or reject bootstrapped answers.  Will simply use the generated demonstrations (if they pass the metric) without any further optimization; i.e., does not check to see if the program's performance is improved by the compilation. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.\n+2. **`BootstrapFewShot`**: Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in `trainset`. Parameters include `max_labeled_demos` (the number of demonstrations randomly selected from the `trainset`) and `max_bootstrapped_demos` (the number of additional examples generated by the `teacher`). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the \"compiled\" prompt. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.\n \n-3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program.  Takes as parameter (in addition to the ones for `BootstrapFewShot`) `num_candidate_programs`: the number of random programs, each generated through `BootstrapFewShot`.  Picks a single best based on scores over the `valset` (which defaults to being the same as the `trainset`).\n+3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of `BootstrapFewShot`, with the addition of `num_candidate_programs`, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, `LabeledFewShot` optimized program, `BootstrapFewShot` compiled program with unshuffled examples and `num_candidate_programs` of `BootstrapFewShot` compiled programs with randomized example sets. \n \n-4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics. As before, chooses the best of a candidate set of demonstration examples, but this time using Optuna to make the choice. <!--- TBQH, I don't understand how Optuna does this. As far as I can tell it simply chooses best based on multiple evaluations, rather than a single one, and mention of \"hyperparameters\" seems to be a red herring. ---> \n+4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics and selecting the best demonstrations. ",
        "comment_created_at": "2024-05-05T22:56:37+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "seems like this is a matter of semantics but I think hyperparameter is fine since each demonstration is considered and optimized by Optuna which searches over the different indices to select the combination of demonstrations that maximizes the metric. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1591052736",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 951,
        "pr_file": "docs/docs/building-blocks/6-optimizers.md",
        "discussion_id": "1590347282",
        "commented_code": "@@ -38,36 +38,32 @@ All of these can be accessed via `from dspy.teleprompt import *`.\n \n #### Automatic Few-Shot Learning\n \n-These optimizers extend the signature by adding labeled examples to the prompt that will be submitted to the model, automatically implementing few-shot learning.\n+These optimizers extend the signature by automatically generating and including **optimized** examples within the prompt sent to the model, implementing few-shot learning.\n \n-1. **`LabeledFewShot`**: Simply constructs few-shot examples (demos) from provided labeled Q/A pairs.  Takes as parameter, `k`, the number of examples to be added to the prompt and chooses `k` examples randomly from the `trainset` parameter.\n+1. **`LabeledFewShot`**: Simply constructs few-shot examples (demos) from provided labeled input and output data points.  Requires `k` (number of examples for the prompt) and `trainset` to randomly select `k` examples from.\n \n-2. **`BootstrapFewShot`**: Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program in addition to the labeled examples in `trainset`. This is primarily useful if there are potentially many different ways to phrase the answer to a query, and you do not have a training set that covers the alternative phrasings. In this case a language model can be used to \"bootstrap,\" to generate additional answers to the questions in the training set.  Takes as parameters `max_labeled_demos` (the number of demonstrations to choose randomly from the `trainset`), and `max_bootstrapped_demos` (the number of additional examples to be generated by the `teacher`).  The bootstrapping process uses the `metric` to determine whether to accept or reject bootstrapped answers.  Will simply use the generated demonstrations (if they pass the metric) without any further optimization; i.e., does not check to see if the program's performance is improved by the compilation. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.\n+2. **`BootstrapFewShot`**: Uses a `teacher` module (which defaults to your program) to generate complete demonstrations for every stage of your program, along with labeled examples in `trainset`. Parameters include `max_labeled_demos` (the number of demonstrations randomly selected from the `trainset`) and `max_bootstrapped_demos` (the number of additional examples generated by the `teacher`). The bootstrapping process employs the metric to validate demonstrations, including only those that pass the metric in the \"compiled\" prompt. Advanced: Supports using a `teacher` program that is a *different* DSPy program that has compatible structure, for harder tasks.\n \n-3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program.  Takes as parameter (in addition to the ones for `BootstrapFewShot`) `num_candidate_programs`: the number of random programs, each generated through `BootstrapFewShot`.  Picks a single best based on scores over the `valset` (which defaults to being the same as the `trainset`).\n+3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program over the optimization. Parameters mirror those of `BootstrapFewShot`, with the addition of `num_candidate_programs`, which specifies the number of random programs evaluated over the optimization, including candidates of the uncompiled program, `LabeledFewShot` optimized program, `BootstrapFewShot` compiled program with unshuffled examples and `num_candidate_programs` of `BootstrapFewShot` compiled programs with randomized example sets. \n \n-4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics. As before, chooses the best of a candidate set of demonstration examples, but this time using Optuna to make the choice. <!--- TBQH, I don't understand how Optuna does this. As far as I can tell it simply chooses best based on multiple evaluations, rather than a single one, and mention of \"hyperparameters\" seems to be a red herring. ---> \n+4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics and selecting the best demonstrations. ",
        "comment_created_at": "2024-05-06T13:56:10+00:00",
        "comment_author": "rpgoldman",
        "comment_body": "This isn't just semantics, because it's actively misleading.  If it's just semantics, why isn't it ok to drop the word \"hyperparameter\" and just have \"Optuna optimization\"?\r\n\r\nThe reason I am so intent on this change is that the term \"hyperparameter\" applies not to model parameters, but to parameters _that control the learning process_ . So in this case things like `max_labeled_demos`, `max_bootstrapped_demos`, `max_errors`, `rounds` are the hyperparameters: the set of demos are the things that are learned, the parameters.\r\n\r\nPutting \"hyperparameter\" in here misleads the reader, who is led to expect optimization of the learning _process_ not optimization of the learning _product_.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1597487201",
    "pr_number": 859,
    "pr_file": "docs/api/language_model_clients/Snowflake.md",
    "created_at": "2024-05-11T18:08:45+00:00",
    "commented_code": "+---\n+sidebar_position: \n+---\n+\n+# dspy.Snowflake\n+\n+### Usage\n+\n+```python\n+lm = dspy.Snowflake(model=\"mixtral-8x7b\",credentials=connection_parameters)",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1597487201",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 859,
        "pr_file": "docs/api/language_model_clients/Snowflake.md",
        "discussion_id": "1597487201",
        "commented_code": "@@ -0,0 +1,32 @@\n+---\n+sidebar_position: \n+---\n+\n+# dspy.Snowflake\n+\n+### Usage\n+\n+```python\n+lm = dspy.Snowflake(model=\"mixtral-8x7b\",credentials=connection_parameters)",
        "comment_created_at": "2024-05-11T18:08:45+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "i think it would be useful to define some required `connection_parameters` from Snowflake based on the [documentation](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/api/snowflake.snowpark.Session) so users know what to expect when configuring `dspy.Snowflake`.",
        "pr_file_module": null
      },
      {
        "comment_id": "1597494595",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 859,
        "pr_file": "docs/api/language_model_clients/Snowflake.md",
        "discussion_id": "1597487201",
        "commented_code": "@@ -0,0 +1,32 @@\n+---\n+sidebar_position: \n+---\n+\n+# dspy.Snowflake\n+\n+### Usage\n+\n+```python\n+lm = dspy.Snowflake(model=\"mixtral-8x7b\",credentials=connection_parameters)",
        "comment_created_at": "2024-05-11T19:11:01+00:00",
        "comment_author": "sfc-gh-alherrera",
        "comment_body": "Done. added the same as in the RM docs",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1511750801",
    "pr_number": 548,
    "pr_file": "docs/api/retrieval_model_clients/Neo4jRM.md",
    "created_at": "2024-03-04T20:25:47+00:00",
    "commented_code": "+\n+# retrieve.neo4j_rm\n+\n+### Constructor\n+\n+Initialize an instance of the `Neo4jRM` class.\n+\n+```python\n+Neo4jRM(\n+    index_name: str,\n+    text_node_property: str,\n+    k: int = 5,\n+    retrieval_query: str = None,\n+    embedding_provider: str = \"openai\",\n+    embedding_model: str = \"text-embedding-ada-002\",\n+)\n+```\n+\n+**Environment Variables:**\n+\n+You need to define the credentials as environment variables:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1511750801",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 548,
        "pr_file": "docs/api/retrieval_model_clients/Neo4jRM.md",
        "discussion_id": "1511750801",
        "commented_code": "@@ -0,0 +1,80 @@\n+\n+# retrieve.neo4j_rm\n+\n+### Constructor\n+\n+Initialize an instance of the `Neo4jRM` class.\n+\n+```python\n+Neo4jRM(\n+    index_name: str,\n+    text_node_property: str,\n+    k: int = 5,\n+    retrieval_query: str = None,\n+    embedding_provider: str = \"openai\",\n+    embedding_model: str = \"text-embedding-ada-002\",\n+)\n+```\n+\n+**Environment Variables:**\n+\n+You need to define the credentials as environment variables:",
        "comment_created_at": "2024-03-04T20:25:47+00:00",
        "comment_author": "jexp",
        "comment_body": "add OPENAI_API_KEY as required env variable in docs + example",
        "pr_file_module": null
      }
    ]
  }
]