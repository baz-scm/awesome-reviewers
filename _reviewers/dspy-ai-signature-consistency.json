[
  {
    "discussion_id": "1856090538",
    "pr_number": 1801,
    "pr_file": "examples/vlm/mmmu.ipynb",
    "created_at": "2024-11-25T08:25:27+00:00",
    "commented_code": "\"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n+    \"import dspy\n\",\n+    \"from typing import List\n\",\n     \"class ColorSignature(dspy.Signature):\n\",\n     \"    \\\"\\\"\\\"Output the color of the designated image.\\\"\\\"\\\"\n\",\n-    \"    image_1: dspy.Image = dspy.InputField(desc=\\\"An image\\\")\n\",\n-    \"    image_2: dspy.Image = dspy.InputField(desc=\\\"An image\\\")\n\",\n+    \"    images: List[dspy.Image] = dspy.InputField(desc=\\\"An image\\\")\n\",",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1856090538",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1801,
        "pr_file": "examples/vlm/mmmu.ipynb",
        "discussion_id": "1856090538",
        "commented_code": "@@ -611,13 +627,25 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n+    \"import dspy\\n\",\n+    \"from typing import List\\n\",\n     \"class ColorSignature(dspy.Signature):\\n\",\n     \"    \\\"\\\"\\\"Output the color of the designated image.\\\"\\\"\\\"\\n\",\n-    \"    image_1: dspy.Image = dspy.InputField(desc=\\\"An image\\\")\\n\",\n-    \"    image_2: dspy.Image = dspy.InputField(desc=\\\"An image\\\")\\n\",\n+    \"    images: List[dspy.Image] = dspy.InputField(desc=\\\"An image\\\")\\n\",",
        "comment_created_at": "2024-11-25T08:25:27+00:00",
        "comment_author": "JehandadK",
        "comment_body": "```suggestion\r\n    \"    images: List[dspy.Image] = dspy.InputField(desc=\\\"A list of images\\\")\\n\",\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1699378329",
    "pr_number": 1340,
    "pr_file": "examples/multi-input-output/beginner-multi-input-output.ipynb",
    "created_at": "2024-08-01T04:14:51+00:00",
    "commented_code": "+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<img src=\\\"../../docs/images/DSPy8.png\\\" alt=\\\"DSPy7 Image\\\" height=\\\"150\\\"/>\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# DSPy Tutorial: Building a Code Plagiarism Detector\n\",\n+    \"\n\",\n+    \"If you've ever felt intimidated by DSPy, don't worry\u2014it might look complex at first glance, but it's actually quite approachable. This tutorial will walk you through the process of building a  project, providing a clear, step-by-step approach to understanding and implementing DSPy concepts.\n\",\n+    \"\n\",\n+    \"## TLDR \ud83d\ude80\n\",\n+    \"\n\",\n+    \"We're going to build a system for code plagiarism detection. Our goal is to compare two input code files, determine if plagiarism has occurred, and provide an explanation for the result. \n\",\n+    \"\n\",\n+    \"This project will showcase:\n\",\n+    \"\n\",\n+    \"- Multiple inputs and outputs\n\",\n+    \"- Double validation techniques\n\",\n+    \"\n\",\n+    \"I strongly recommend to read the [DSPy Cheatsheet](https://dspy-docs.vercel.app/docs/cheatsheet) it will help you with quick start.\n\",\n+    \"\n\",\n+    \"## How to Start?\n\",\n+    \"\n\",\n+    \"A highly effective practice I've found to be game-changing when starting any DSPy project is to answer [these 8 key questions](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task). This exercise helps you develop a clear vision for your project before diving into the code.\n\",\n+    \"\n\",\n+    \"Here's an example of how your answers might look:\n\",\n+    \"1. **Define your task**\n\",\n+    \"   - Expected input: Two input code files (strings containing plain code) to be compared.\n\",\n+    \"   - Expected output:\n\",\n+    \"     - Plagiarism detection result (Yes/No)\n\",\n+    \"     - Explanation/justification of the result\n\",\n+    \"   - Quality and Cost Specifications: Cost is not a concern; quality is the main priority. We want to try different models.\n\",\n+    \"\n\",\n+    \"2. **Define your pipeline**\n\",\n+    \"   - We don't need any external tools or document retrieval. It will be a simple chain-of-thought step, as we want to evaluate LLM capabilities for plagiarism detection.\n\",\n+    \"\n\",\n+    \"3. **Explore a few examples**\n\",\n+    \"   - We explored LLM capabilities for plagiarism detection using a few examples with ChatGPT and Claude, yielding promising results.\n\",\n+    \"\n\",\n+    \"4. **Define your data**\n\",\n+    \"   - We are working with a dataset from the publication: [Source Code Plagiarism Detection in Academia with Information Retrieval: Dataset and the Observation](https://github.com/oscarkarnalim/sourcecodeplagiarismdataset/blob/master/IR-Plag-Dataset.zip)\n\",\n+    \"   - We selected a subset and manually labeled the dataset with our output labels. This dataset should be used for training and testing, while the rest of the original dataset should be used for evaluation.\n\",\n+    \"   - Dataset: [train.csv](/data/train.tsv) (65 samples)\n\",\n+    \"   - When you don't hae labeled dataset, it is good idea to try hand-labeling a few examples to get a sense of the task. It will help you to understand the task better and also increse the quality of program.\n\",\n+    \"\n\",\n+    \"5. **Define your metric**\n\",\n+    \"   - We are dealing with a **classification problem**, so we will use accuracy as our main metric. \n\",\n+    \"   - Our metric will be simple: if pred_label == true_label then 1 else 0.\n\",\n+    \"   - As second evaluation we will be evaluating the quality of the explanation via secondary LLM.\n\",\n+    \"\n\",\n+    \"6. **Collect preliminary \\\"zero-shot\\\" evaluations**\n\",\n+    \"   - Done in code.\n\",\n+    \"\n\",\n+    \"7. **Compile with a DSPy optimizer**\n\",\n+    \"   - We don't want to update weights of the LLM, so we are looking at optimizers such as:\n\",\n+    \"     - BootstrapFewShot\n\",\n+    \"     - BootstrapFewShotWithRandomSearch\n\",\n+    \"     - MIPRO\n\",\n+    \"     - ...\n\",\n+    \"\n\",\n+    \"8. **Iterate**\n\",\n+    \"    - Regroup and attack again!\n\",\n+    \"\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import os\n\",\n+    \"import re\n\",\n+    \"\n\",\n+    \"import dspy\n\",\n+    \"import pandas as pd\n\",\n+    \"from dotenv import load_dotenv\n\",\n+    \"from dspy.evaluate import Evaluate\n\",\n+    \"from dspy.teleprompt import (\n\",\n+    \"    BootstrapFewShot,\n\",\n+    \"    BootstrapFewShotWithRandomSearch,\n\",\n+    \"    KNNFewShot,\n\",\n+    \"    MIPROv2,\n\",\n+    \")\n\",\n+    \"\n\",\n+    \"# load your environment variables from .env file\n\",\n+    \"load_dotenv()\n\",\n+    \"\n\",\n+    \"# azure-openai model deployment\n\",\n+    \"AZURE_OPENAI_KEY = os.getenv(\\\"AZURE_OPENAI_KEY\\\")\n\",\n+    \"AZURE_OPENAI_ENDPOINT = os.getenv(\\\"AZURE_OPENAI_ENDPOINT\\\")\n\",\n+    \"AZURE_OPENAI_DEPLOYMENT = os.getenv(\\\"AZURE_OPENAI_DEPLOYMENT\\\")\n\",\n+    \"AZURE_OPENAI_VERSION = os.getenv(\\\"AZURE_OPENAI_VERSION\\\")\n\",\n+    \"\n\",\n+    \"# openai model deployment\n\",\n+    \"\n\",\n+    \"OPENAI_MODEL = os.getenv(\\\"OPENAI_MODEL\\\")\n\",\n+    \"OPENAI_API_KEY = os.getenv(\\\"OPENAI_API_KEY\\\")\n\",\n+    \"\n\",\n+    \"# ollama deployment\n\",\n+    \"OLLAMA_URL = os.getenv(\\\"OLLAMA_URL\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 0. Load dataset\n\",\n+    \"\n\",\n+    \"Our first task is to load our dataset. Each entry in our dataset will consist of the following components:\n\",\n+    \"\n\",\n+    \"* `sample_1`: The first code sample to be analyzed\n\",\n+    \"* `sample_2`: The second code sample to be compared against the first\n\",\n+    \"* `plagiarized`: A boolean value (True if plagiarism is detected, False otherwise)\n\",\n+    \"* `reason`: A detailed explanation of the plagiarism detection result\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"df = pd.read_csv(\\\"train.csv\\\", sep=\\\"\\\\t\\\")\n\",\n+    \"df.head()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 1. Prepare dataset\n\",\n+    \"\n\",\n+    \"DSPy utilizes special objects called [`Example`](https://dspy-docs.vercel.app/docs/deep-dive/data-handling/examples#creating-an-example) to structure and process data. Our next task is to convert our raw dataset into a collection of these `Example` objects.\n\",\n+    \"\n\",\n+    \"## Creating Custom Example Objects\n\",\n+    \"\n\",\n+    \"For our plagiarism detection system, we'll create `Example` objects with the following attributes:\n\",\n+    \"- `code_sample_1`: The first code sample to analyze\n\",\n+    \"- `code_sample_2`: The second code sample to compare\n\",\n+    \"- `plagiarized`: Boolean indicating whether plagiarism was detected\n\",\n+    \"- `explanation`: Detailed reasoning for the plagiarism decision\n\",\n+    \"\n\",\n+    \"## Specifying Inputs\n\",\n+    \"\n\",\n+    \"It's crucial to inform DSPy which attributes serve as inputs for our model. We accomplish this using the `.with_inputs()` method. In our case, we'll specify:\n\",\n+    \"\n\",\n+    \"```python\n\",\n+    \".with_inputs(\\\"code_sample_1\\\", \\\"code_sample_2\\\")\n\",\n+    \"```\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def create_example(row: pd.Series) -> dspy.Example:\n\",\n+    \"    return dspy.Example(\n\",\n+    \"        code_sample_1=row[\\\"sample_1\\\"],\n\",\n+    \"        code_sample_2=row[\\\"sample_2\\\"],\n\",\n+    \"        plagiarized=\\\"Yes\\\" if row[\\\"plagiarized\\\"] else \\\"No\\\",\n\",\n+    \"        explanation=row[\\\"reason\\\"],\n\",\n+    \"    ).with_inputs(\\\"code_sample_1\\\", \\\"code_sample_2\\\")\n\",\n+    \"\n\",\n+    \"\n\",\n+    \"examples = []\n\",\n+    \"for _, row in df.iterrows():\n\",\n+    \"    example = create_example(row)\n\",\n+    \"    examples.append(example)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 2. DSPy setup\n\",\n+    \"\n\",\n+    \"DSPy is designed to be compatible with a variety of language models and their respective clients. For this tutorial, we will primarily utilize GPT-4 through the [Azure client](https://github.com/stanfordnlp/dspy/blob/main/dsp/modules/azure_openai.py). However, to demonstrate DSPy's flexibility, we will also provide configuration examples for other popular options:\n\",\n+    \"\n\",\n+    \"1. [OpenAI client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/remote_models/OpenAI)\n\",\n+    \"2. Local models via [Ollama](https://github.com/stanfordnlp/dspy/blob/main/dsp/modules/ollama.py)\n\",\n+    \"\n\",\n+    \"## Custom Client Implementation\n\",\n+    \"\n\",\n+    \"If your preferred language model client is not natively supported by DSPy, you have the option to implement a custom client. For detailed instructions on creating a custom client, please refer to this [comprehensive guide - custom-lm-client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client).\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# client for AzureOpenAI\n\",\n+    \"lm = dspy.AzureOpenAI(\n\",\n+    \"    api_base=AZURE_OPENAI_ENDPOINT,\n\",\n+    \"    api_version=AZURE_OPENAI_VERSION,\n\",\n+    \"    deployment_id=AZURE_OPENAI_DEPLOYMENT,\n\",\n+    \"    api_key=AZURE_OPENAI_KEY,\n\",\n+    \")\n\",\n+    \"\n\",\n+    \"# client for OpenAI\n\",\n+    \"# lm = dspy.OpenAI(\n\",\n+    \"#     api_key=OPENAI_API_KEY,\n\",\n+    \"#     model=OPENAI_MODEL,\n\",\n+    \"# )\n\",\n+    \"\n\",\n+    \"# client for Ollama\n\",\n+    \"# model_name = \\\"llama3.1\\\"\n\",\n+    \"# lm = dspy.OllamaLocal(base_url=OLLAMA_URL, model=model_name)\n\",\n+    \"\n\",\n+    \"dspy.settings.configure(lm=lm)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 3. Setting up signature and module\n\",\n+    \"\n\",\n+    \"\n\",\n+    \"## Core DSPy Components\n\",\n+    \"\n\",\n+    \"[dspy.Signature](https://dspy-docs.vercel.app/docs/building-blocks/signatures) and [dspy.Module](https://dspy-docs.vercel.app/docs/building-blocks/modules) are fundamental building blocks for DSPy programs:\n\",\n+    \"\n\",\n+    \"- **Signature**: A declarative specification of the input/output behavior of a DSPy module.\n\",\n+    \"- **Module**: A building block for programs that leverage Language Models (LMs).\n\",\n+    \"\n\",\n+    \"## Types of DSPy Modules\n\",\n+    \"\n\",\n+    \"DSPy offers various module types, each serving different purposes:\n\",\n+    \"\n\",\n+    \"1. [dspy.Predict](https://dspy-docs.vercel.app/api/modules/Predict)\n\",\n+    \"   - Basic predictor\n\",\n+    \"   - Maintains the original signature\n\",\n+    \"   - Handles key forms of learning (storing instructions, demonstrations, and LM updates)\n\",\n+    \"   - Most similar to direct LM usage\n\",\n+    \"\n\",\n+    \"2. [dspy.ChainOfThought](https://dspy-docs.vercel.app/api/modules/ChainOfThought)\n\",\n+    \"   - Enhances the LM to think step-by-step before producing the final response\n\",\n+    \"   - Modifies the signature to incorporate intermediate reasoning steps\n\",\n+    \"\n\",\n+    \"3. [Additional Advanced Modules](https://dspy-docs.vercel.app/api/category/modules)\n\",\n+    \"   - DSPy library offers a range of more specialized modules for complex tasks\n\",\n+    \"\n\",\n+    \"### Recommendation for starting\n\",\n+    \"\n\",\n+    \"For those new to DSPy, it's advisable to start with `dspy.Predict`. Its simplicity makes it ideal for understanding the basics of DSPy operation. Once you've successfully implemented your program using `dspy.Predict`, you can explore more advanced modules like `dspy.ChainOfThought` to potentially enhance your model's performance.\n\",\n+    \"\n\",\n+    \"For an overview of other prompting techniques beyond zero-shot learning, refer to the [Prompting Guide](https://www.promptingguide.ai/techniques). This resource covers various methods that can enhance your DSPy applications as you progress.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"class PlagiarismSignature(dspy.Signature):\n\",\n+    \"    # Clarify something about the nature of the task (expressed below as a docstring)! \n\",\n+    \"    \\\"\\\"\\\"Detect if two code samples are plagiarized. In plagiarized field answer only : Yes if the code samples are plagiarized, No otherwise. In explenation field add the reason why the code samples are/ are not plagiarized.\\\"\\\"\\\"\n\",\n+    \"\n\",\n+    \"    # Supply hints on the nature of an input field, expressed as a desc keyword argument for dspy.InputField.\n\",\n+    \"    code_sample_1 = dspy.InputField(desc=\\\"The first code sample to compare\\\")\n\",\n+    \"    code_sample_2 = dspy.InputField(desc=\\\"The second code sample to compare\\\")\n\",\n+    \"\n\",\n+    \"    # Supply constraints on an output field, expressed as a desc keyword argument for dspy.OutputField.\n\",\n+    \"    explanation = dspy.OutputField(\n\",\n+    \"        desc=\\\"Explanation or reason why the code samples are/ are not plagiarized\\\"\n\",\n+    \"    )\n\",\n+    \"    plagiarized = dspy.OutputField(\n\",\n+    \"        desc=\\\"Yes/No indicating if code samples are plagiarized\\\"\n\",\n+    \"    )\n\",\n+    \"\n\",\n+    \"\n\",\n+    \"class PlagiarismCoT(dspy.Module):\n\",\n+    \"    def __init__(self) -> None:\n\",\n+    \"        super().__init__()\n\",\n+    \"        # Define the program as a ChainOfThought object\n\",\n+    \"        # self.prog = dspy.PredictiveText(PlagiarismSignature)\n\",\n+    \"        self.prog = dspy.ChainOfThought(PlagiarismSignature)\n\",",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1699378329",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1340,
        "pr_file": "examples/multi-input-output/beginner-multi-input-output.ipynb",
        "discussion_id": "1699378329",
        "commented_code": "@@ -0,0 +1,829 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<img src=\\\"../../docs/images/DSPy8.png\\\" alt=\\\"DSPy7 Image\\\" height=\\\"150\\\"/>\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# DSPy Tutorial: Building a Code Plagiarism Detector\\n\",\n+    \"\\n\",\n+    \"If you've ever felt intimidated by DSPy, don't worry\u2014it might look complex at first glance, but it's actually quite approachable. This tutorial will walk you through the process of building a  project, providing a clear, step-by-step approach to understanding and implementing DSPy concepts.\\n\",\n+    \"\\n\",\n+    \"## TLDR \ud83d\ude80\\n\",\n+    \"\\n\",\n+    \"We're going to build a system for code plagiarism detection. Our goal is to compare two input code files, determine if plagiarism has occurred, and provide an explanation for the result. \\n\",\n+    \"\\n\",\n+    \"This project will showcase:\\n\",\n+    \"\\n\",\n+    \"- Multiple inputs and outputs\\n\",\n+    \"- Double validation techniques\\n\",\n+    \"\\n\",\n+    \"I strongly recommend to read the [DSPy Cheatsheet](https://dspy-docs.vercel.app/docs/cheatsheet) it will help you with quick start.\\n\",\n+    \"\\n\",\n+    \"## How to Start?\\n\",\n+    \"\\n\",\n+    \"A highly effective practice I've found to be game-changing when starting any DSPy project is to answer [these 8 key questions](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task). This exercise helps you develop a clear vision for your project before diving into the code.\\n\",\n+    \"\\n\",\n+    \"Here's an example of how your answers might look:\\n\",\n+    \"1. **Define your task**\\n\",\n+    \"   - Expected input: Two input code files (strings containing plain code) to be compared.\\n\",\n+    \"   - Expected output:\\n\",\n+    \"     - Plagiarism detection result (Yes/No)\\n\",\n+    \"     - Explanation/justification of the result\\n\",\n+    \"   - Quality and Cost Specifications: Cost is not a concern; quality is the main priority. We want to try different models.\\n\",\n+    \"\\n\",\n+    \"2. **Define your pipeline**\\n\",\n+    \"   - We don't need any external tools or document retrieval. It will be a simple chain-of-thought step, as we want to evaluate LLM capabilities for plagiarism detection.\\n\",\n+    \"\\n\",\n+    \"3. **Explore a few examples**\\n\",\n+    \"   - We explored LLM capabilities for plagiarism detection using a few examples with ChatGPT and Claude, yielding promising results.\\n\",\n+    \"\\n\",\n+    \"4. **Define your data**\\n\",\n+    \"   - We are working with a dataset from the publication: [Source Code Plagiarism Detection in Academia with Information Retrieval: Dataset and the Observation](https://github.com/oscarkarnalim/sourcecodeplagiarismdataset/blob/master/IR-Plag-Dataset.zip)\\n\",\n+    \"   - We selected a subset and manually labeled the dataset with our output labels. This dataset should be used for training and testing, while the rest of the original dataset should be used for evaluation.\\n\",\n+    \"   - Dataset: [train.csv](/data/train.tsv) (65 samples)\\n\",\n+    \"   - When you don't hae labeled dataset, it is good idea to try hand-labeling a few examples to get a sense of the task. It will help you to understand the task better and also increse the quality of program.\\n\",\n+    \"\\n\",\n+    \"5. **Define your metric**\\n\",\n+    \"   - We are dealing with a **classification problem**, so we will use accuracy as our main metric. \\n\",\n+    \"   - Our metric will be simple: if pred_label == true_label then 1 else 0.\\n\",\n+    \"   - As second evaluation we will be evaluating the quality of the explanation via secondary LLM.\\n\",\n+    \"\\n\",\n+    \"6. **Collect preliminary \\\"zero-shot\\\" evaluations**\\n\",\n+    \"   - Done in code.\\n\",\n+    \"\\n\",\n+    \"7. **Compile with a DSPy optimizer**\\n\",\n+    \"   - We don't want to update weights of the LLM, so we are looking at optimizers such as:\\n\",\n+    \"     - BootstrapFewShot\\n\",\n+    \"     - BootstrapFewShotWithRandomSearch\\n\",\n+    \"     - MIPRO\\n\",\n+    \"     - ...\\n\",\n+    \"\\n\",\n+    \"8. **Iterate**\\n\",\n+    \"    - Regroup and attack again!\\n\",\n+    \"\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import os\\n\",\n+    \"import re\\n\",\n+    \"\\n\",\n+    \"import dspy\\n\",\n+    \"import pandas as pd\\n\",\n+    \"from dotenv import load_dotenv\\n\",\n+    \"from dspy.evaluate import Evaluate\\n\",\n+    \"from dspy.teleprompt import (\\n\",\n+    \"    BootstrapFewShot,\\n\",\n+    \"    BootstrapFewShotWithRandomSearch,\\n\",\n+    \"    KNNFewShot,\\n\",\n+    \"    MIPROv2,\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# load your environment variables from .env file\\n\",\n+    \"load_dotenv()\\n\",\n+    \"\\n\",\n+    \"# azure-openai model deployment\\n\",\n+    \"AZURE_OPENAI_KEY = os.getenv(\\\"AZURE_OPENAI_KEY\\\")\\n\",\n+    \"AZURE_OPENAI_ENDPOINT = os.getenv(\\\"AZURE_OPENAI_ENDPOINT\\\")\\n\",\n+    \"AZURE_OPENAI_DEPLOYMENT = os.getenv(\\\"AZURE_OPENAI_DEPLOYMENT\\\")\\n\",\n+    \"AZURE_OPENAI_VERSION = os.getenv(\\\"AZURE_OPENAI_VERSION\\\")\\n\",\n+    \"\\n\",\n+    \"# openai model deployment\\n\",\n+    \"\\n\",\n+    \"OPENAI_MODEL = os.getenv(\\\"OPENAI_MODEL\\\")\\n\",\n+    \"OPENAI_API_KEY = os.getenv(\\\"OPENAI_API_KEY\\\")\\n\",\n+    \"\\n\",\n+    \"# ollama deployment\\n\",\n+    \"OLLAMA_URL = os.getenv(\\\"OLLAMA_URL\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 0. Load dataset\\n\",\n+    \"\\n\",\n+    \"Our first task is to load our dataset. Each entry in our dataset will consist of the following components:\\n\",\n+    \"\\n\",\n+    \"* `sample_1`: The first code sample to be analyzed\\n\",\n+    \"* `sample_2`: The second code sample to be compared against the first\\n\",\n+    \"* `plagiarized`: A boolean value (True if plagiarism is detected, False otherwise)\\n\",\n+    \"* `reason`: A detailed explanation of the plagiarism detection result\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"df = pd.read_csv(\\\"train.csv\\\", sep=\\\"\\\\t\\\")\\n\",\n+    \"df.head()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 1. Prepare dataset\\n\",\n+    \"\\n\",\n+    \"DSPy utilizes special objects called [`Example`](https://dspy-docs.vercel.app/docs/deep-dive/data-handling/examples#creating-an-example) to structure and process data. Our next task is to convert our raw dataset into a collection of these `Example` objects.\\n\",\n+    \"\\n\",\n+    \"## Creating Custom Example Objects\\n\",\n+    \"\\n\",\n+    \"For our plagiarism detection system, we'll create `Example` objects with the following attributes:\\n\",\n+    \"- `code_sample_1`: The first code sample to analyze\\n\",\n+    \"- `code_sample_2`: The second code sample to compare\\n\",\n+    \"- `plagiarized`: Boolean indicating whether plagiarism was detected\\n\",\n+    \"- `explanation`: Detailed reasoning for the plagiarism decision\\n\",\n+    \"\\n\",\n+    \"## Specifying Inputs\\n\",\n+    \"\\n\",\n+    \"It's crucial to inform DSPy which attributes serve as inputs for our model. We accomplish this using the `.with_inputs()` method. In our case, we'll specify:\\n\",\n+    \"\\n\",\n+    \"```python\\n\",\n+    \".with_inputs(\\\"code_sample_1\\\", \\\"code_sample_2\\\")\\n\",\n+    \"```\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def create_example(row: pd.Series) -> dspy.Example:\\n\",\n+    \"    return dspy.Example(\\n\",\n+    \"        code_sample_1=row[\\\"sample_1\\\"],\\n\",\n+    \"        code_sample_2=row[\\\"sample_2\\\"],\\n\",\n+    \"        plagiarized=\\\"Yes\\\" if row[\\\"plagiarized\\\"] else \\\"No\\\",\\n\",\n+    \"        explanation=row[\\\"reason\\\"],\\n\",\n+    \"    ).with_inputs(\\\"code_sample_1\\\", \\\"code_sample_2\\\")\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"examples = []\\n\",\n+    \"for _, row in df.iterrows():\\n\",\n+    \"    example = create_example(row)\\n\",\n+    \"    examples.append(example)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 2. DSPy setup\\n\",\n+    \"\\n\",\n+    \"DSPy is designed to be compatible with a variety of language models and their respective clients. For this tutorial, we will primarily utilize GPT-4 through the [Azure client](https://github.com/stanfordnlp/dspy/blob/main/dsp/modules/azure_openai.py). However, to demonstrate DSPy's flexibility, we will also provide configuration examples for other popular options:\\n\",\n+    \"\\n\",\n+    \"1. [OpenAI client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/remote_models/OpenAI)\\n\",\n+    \"2. Local models via [Ollama](https://github.com/stanfordnlp/dspy/blob/main/dsp/modules/ollama.py)\\n\",\n+    \"\\n\",\n+    \"## Custom Client Implementation\\n\",\n+    \"\\n\",\n+    \"If your preferred language model client is not natively supported by DSPy, you have the option to implement a custom client. For detailed instructions on creating a custom client, please refer to this [comprehensive guide - custom-lm-client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client).\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# client for AzureOpenAI\\n\",\n+    \"lm = dspy.AzureOpenAI(\\n\",\n+    \"    api_base=AZURE_OPENAI_ENDPOINT,\\n\",\n+    \"    api_version=AZURE_OPENAI_VERSION,\\n\",\n+    \"    deployment_id=AZURE_OPENAI_DEPLOYMENT,\\n\",\n+    \"    api_key=AZURE_OPENAI_KEY,\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# client for OpenAI\\n\",\n+    \"# lm = dspy.OpenAI(\\n\",\n+    \"#     api_key=OPENAI_API_KEY,\\n\",\n+    \"#     model=OPENAI_MODEL,\\n\",\n+    \"# )\\n\",\n+    \"\\n\",\n+    \"# client for Ollama\\n\",\n+    \"# model_name = \\\"llama3.1\\\"\\n\",\n+    \"# lm = dspy.OllamaLocal(base_url=OLLAMA_URL, model=model_name)\\n\",\n+    \"\\n\",\n+    \"dspy.settings.configure(lm=lm)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 3. Setting up signature and module\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"## Core DSPy Components\\n\",\n+    \"\\n\",\n+    \"[dspy.Signature](https://dspy-docs.vercel.app/docs/building-blocks/signatures) and [dspy.Module](https://dspy-docs.vercel.app/docs/building-blocks/modules) are fundamental building blocks for DSPy programs:\\n\",\n+    \"\\n\",\n+    \"- **Signature**: A declarative specification of the input/output behavior of a DSPy module.\\n\",\n+    \"- **Module**: A building block for programs that leverage Language Models (LMs).\\n\",\n+    \"\\n\",\n+    \"## Types of DSPy Modules\\n\",\n+    \"\\n\",\n+    \"DSPy offers various module types, each serving different purposes:\\n\",\n+    \"\\n\",\n+    \"1. [dspy.Predict](https://dspy-docs.vercel.app/api/modules/Predict)\\n\",\n+    \"   - Basic predictor\\n\",\n+    \"   - Maintains the original signature\\n\",\n+    \"   - Handles key forms of learning (storing instructions, demonstrations, and LM updates)\\n\",\n+    \"   - Most similar to direct LM usage\\n\",\n+    \"\\n\",\n+    \"2. [dspy.ChainOfThought](https://dspy-docs.vercel.app/api/modules/ChainOfThought)\\n\",\n+    \"   - Enhances the LM to think step-by-step before producing the final response\\n\",\n+    \"   - Modifies the signature to incorporate intermediate reasoning steps\\n\",\n+    \"\\n\",\n+    \"3. [Additional Advanced Modules](https://dspy-docs.vercel.app/api/category/modules)\\n\",\n+    \"   - DSPy library offers a range of more specialized modules for complex tasks\\n\",\n+    \"\\n\",\n+    \"### Recommendation for starting\\n\",\n+    \"\\n\",\n+    \"For those new to DSPy, it's advisable to start with `dspy.Predict`. Its simplicity makes it ideal for understanding the basics of DSPy operation. Once you've successfully implemented your program using `dspy.Predict`, you can explore more advanced modules like `dspy.ChainOfThought` to potentially enhance your model's performance.\\n\",\n+    \"\\n\",\n+    \"For an overview of other prompting techniques beyond zero-shot learning, refer to the [Prompting Guide](https://www.promptingguide.ai/techniques). This resource covers various methods that can enhance your DSPy applications as you progress.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"class PlagiarismSignature(dspy.Signature):\\n\",\n+    \"    # Clarify something about the nature of the task (expressed below as a docstring)! \\n\",\n+    \"    \\\"\\\"\\\"Detect if two code samples are plagiarized. In plagiarized field answer only : Yes if the code samples are plagiarized, No otherwise. In explenation field add the reason why the code samples are/ are not plagiarized.\\\"\\\"\\\"\\n\",\n+    \"\\n\",\n+    \"    # Supply hints on the nature of an input field, expressed as a desc keyword argument for dspy.InputField.\\n\",\n+    \"    code_sample_1 = dspy.InputField(desc=\\\"The first code sample to compare\\\")\\n\",\n+    \"    code_sample_2 = dspy.InputField(desc=\\\"The second code sample to compare\\\")\\n\",\n+    \"\\n\",\n+    \"    # Supply constraints on an output field, expressed as a desc keyword argument for dspy.OutputField.\\n\",\n+    \"    explanation = dspy.OutputField(\\n\",\n+    \"        desc=\\\"Explanation or reason why the code samples are/ are not plagiarized\\\"\\n\",\n+    \"    )\\n\",\n+    \"    plagiarized = dspy.OutputField(\\n\",\n+    \"        desc=\\\"Yes/No indicating if code samples are plagiarized\\\"\\n\",\n+    \"    )\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"class PlagiarismCoT(dspy.Module):\\n\",\n+    \"    def __init__(self) -> None:\\n\",\n+    \"        super().__init__()\\n\",\n+    \"        # Define the program as a ChainOfThought object\\n\",\n+    \"        # self.prog = dspy.PredictiveText(PlagiarismSignature)\\n\",\n+    \"        self.prog = dspy.ChainOfThought(PlagiarismSignature)\\n\",",
        "comment_created_at": "2024-08-01T04:14:51+00:00",
        "comment_author": "isaacbmiller",
        "comment_body": "The chain of thought here is actually unnecessary, because you have the explanation field inside of PlagarismSignature",
        "pr_file_module": null
      },
      {
        "comment_id": "1699510679",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1340,
        "pr_file": "examples/multi-input-output/beginner-multi-input-output.ipynb",
        "discussion_id": "1699378329",
        "commented_code": "@@ -0,0 +1,829 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"<img src=\\\"../../docs/images/DSPy8.png\\\" alt=\\\"DSPy7 Image\\\" height=\\\"150\\\"/>\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# DSPy Tutorial: Building a Code Plagiarism Detector\\n\",\n+    \"\\n\",\n+    \"If you've ever felt intimidated by DSPy, don't worry\u2014it might look complex at first glance, but it's actually quite approachable. This tutorial will walk you through the process of building a  project, providing a clear, step-by-step approach to understanding and implementing DSPy concepts.\\n\",\n+    \"\\n\",\n+    \"## TLDR \ud83d\ude80\\n\",\n+    \"\\n\",\n+    \"We're going to build a system for code plagiarism detection. Our goal is to compare two input code files, determine if plagiarism has occurred, and provide an explanation for the result. \\n\",\n+    \"\\n\",\n+    \"This project will showcase:\\n\",\n+    \"\\n\",\n+    \"- Multiple inputs and outputs\\n\",\n+    \"- Double validation techniques\\n\",\n+    \"\\n\",\n+    \"I strongly recommend to read the [DSPy Cheatsheet](https://dspy-docs.vercel.app/docs/cheatsheet) it will help you with quick start.\\n\",\n+    \"\\n\",\n+    \"## How to Start?\\n\",\n+    \"\\n\",\n+    \"A highly effective practice I've found to be game-changing when starting any DSPy project is to answer [these 8 key questions](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task). This exercise helps you develop a clear vision for your project before diving into the code.\\n\",\n+    \"\\n\",\n+    \"Here's an example of how your answers might look:\\n\",\n+    \"1. **Define your task**\\n\",\n+    \"   - Expected input: Two input code files (strings containing plain code) to be compared.\\n\",\n+    \"   - Expected output:\\n\",\n+    \"     - Plagiarism detection result (Yes/No)\\n\",\n+    \"     - Explanation/justification of the result\\n\",\n+    \"   - Quality and Cost Specifications: Cost is not a concern; quality is the main priority. We want to try different models.\\n\",\n+    \"\\n\",\n+    \"2. **Define your pipeline**\\n\",\n+    \"   - We don't need any external tools or document retrieval. It will be a simple chain-of-thought step, as we want to evaluate LLM capabilities for plagiarism detection.\\n\",\n+    \"\\n\",\n+    \"3. **Explore a few examples**\\n\",\n+    \"   - We explored LLM capabilities for plagiarism detection using a few examples with ChatGPT and Claude, yielding promising results.\\n\",\n+    \"\\n\",\n+    \"4. **Define your data**\\n\",\n+    \"   - We are working with a dataset from the publication: [Source Code Plagiarism Detection in Academia with Information Retrieval: Dataset and the Observation](https://github.com/oscarkarnalim/sourcecodeplagiarismdataset/blob/master/IR-Plag-Dataset.zip)\\n\",\n+    \"   - We selected a subset and manually labeled the dataset with our output labels. This dataset should be used for training and testing, while the rest of the original dataset should be used for evaluation.\\n\",\n+    \"   - Dataset: [train.csv](/data/train.tsv) (65 samples)\\n\",\n+    \"   - When you don't hae labeled dataset, it is good idea to try hand-labeling a few examples to get a sense of the task. It will help you to understand the task better and also increse the quality of program.\\n\",\n+    \"\\n\",\n+    \"5. **Define your metric**\\n\",\n+    \"   - We are dealing with a **classification problem**, so we will use accuracy as our main metric. \\n\",\n+    \"   - Our metric will be simple: if pred_label == true_label then 1 else 0.\\n\",\n+    \"   - As second evaluation we will be evaluating the quality of the explanation via secondary LLM.\\n\",\n+    \"\\n\",\n+    \"6. **Collect preliminary \\\"zero-shot\\\" evaluations**\\n\",\n+    \"   - Done in code.\\n\",\n+    \"\\n\",\n+    \"7. **Compile with a DSPy optimizer**\\n\",\n+    \"   - We don't want to update weights of the LLM, so we are looking at optimizers such as:\\n\",\n+    \"     - BootstrapFewShot\\n\",\n+    \"     - BootstrapFewShotWithRandomSearch\\n\",\n+    \"     - MIPRO\\n\",\n+    \"     - ...\\n\",\n+    \"\\n\",\n+    \"8. **Iterate**\\n\",\n+    \"    - Regroup and attack again!\\n\",\n+    \"\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import os\\n\",\n+    \"import re\\n\",\n+    \"\\n\",\n+    \"import dspy\\n\",\n+    \"import pandas as pd\\n\",\n+    \"from dotenv import load_dotenv\\n\",\n+    \"from dspy.evaluate import Evaluate\\n\",\n+    \"from dspy.teleprompt import (\\n\",\n+    \"    BootstrapFewShot,\\n\",\n+    \"    BootstrapFewShotWithRandomSearch,\\n\",\n+    \"    KNNFewShot,\\n\",\n+    \"    MIPROv2,\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# load your environment variables from .env file\\n\",\n+    \"load_dotenv()\\n\",\n+    \"\\n\",\n+    \"# azure-openai model deployment\\n\",\n+    \"AZURE_OPENAI_KEY = os.getenv(\\\"AZURE_OPENAI_KEY\\\")\\n\",\n+    \"AZURE_OPENAI_ENDPOINT = os.getenv(\\\"AZURE_OPENAI_ENDPOINT\\\")\\n\",\n+    \"AZURE_OPENAI_DEPLOYMENT = os.getenv(\\\"AZURE_OPENAI_DEPLOYMENT\\\")\\n\",\n+    \"AZURE_OPENAI_VERSION = os.getenv(\\\"AZURE_OPENAI_VERSION\\\")\\n\",\n+    \"\\n\",\n+    \"# openai model deployment\\n\",\n+    \"\\n\",\n+    \"OPENAI_MODEL = os.getenv(\\\"OPENAI_MODEL\\\")\\n\",\n+    \"OPENAI_API_KEY = os.getenv(\\\"OPENAI_API_KEY\\\")\\n\",\n+    \"\\n\",\n+    \"# ollama deployment\\n\",\n+    \"OLLAMA_URL = os.getenv(\\\"OLLAMA_URL\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 0. Load dataset\\n\",\n+    \"\\n\",\n+    \"Our first task is to load our dataset. Each entry in our dataset will consist of the following components:\\n\",\n+    \"\\n\",\n+    \"* `sample_1`: The first code sample to be analyzed\\n\",\n+    \"* `sample_2`: The second code sample to be compared against the first\\n\",\n+    \"* `plagiarized`: A boolean value (True if plagiarism is detected, False otherwise)\\n\",\n+    \"* `reason`: A detailed explanation of the plagiarism detection result\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"df = pd.read_csv(\\\"train.csv\\\", sep=\\\"\\\\t\\\")\\n\",\n+    \"df.head()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 1. Prepare dataset\\n\",\n+    \"\\n\",\n+    \"DSPy utilizes special objects called [`Example`](https://dspy-docs.vercel.app/docs/deep-dive/data-handling/examples#creating-an-example) to structure and process data. Our next task is to convert our raw dataset into a collection of these `Example` objects.\\n\",\n+    \"\\n\",\n+    \"## Creating Custom Example Objects\\n\",\n+    \"\\n\",\n+    \"For our plagiarism detection system, we'll create `Example` objects with the following attributes:\\n\",\n+    \"- `code_sample_1`: The first code sample to analyze\\n\",\n+    \"- `code_sample_2`: The second code sample to compare\\n\",\n+    \"- `plagiarized`: Boolean indicating whether plagiarism was detected\\n\",\n+    \"- `explanation`: Detailed reasoning for the plagiarism decision\\n\",\n+    \"\\n\",\n+    \"## Specifying Inputs\\n\",\n+    \"\\n\",\n+    \"It's crucial to inform DSPy which attributes serve as inputs for our model. We accomplish this using the `.with_inputs()` method. In our case, we'll specify:\\n\",\n+    \"\\n\",\n+    \"```python\\n\",\n+    \".with_inputs(\\\"code_sample_1\\\", \\\"code_sample_2\\\")\\n\",\n+    \"```\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def create_example(row: pd.Series) -> dspy.Example:\\n\",\n+    \"    return dspy.Example(\\n\",\n+    \"        code_sample_1=row[\\\"sample_1\\\"],\\n\",\n+    \"        code_sample_2=row[\\\"sample_2\\\"],\\n\",\n+    \"        plagiarized=\\\"Yes\\\" if row[\\\"plagiarized\\\"] else \\\"No\\\",\\n\",\n+    \"        explanation=row[\\\"reason\\\"],\\n\",\n+    \"    ).with_inputs(\\\"code_sample_1\\\", \\\"code_sample_2\\\")\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"examples = []\\n\",\n+    \"for _, row in df.iterrows():\\n\",\n+    \"    example = create_example(row)\\n\",\n+    \"    examples.append(example)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 2. DSPy setup\\n\",\n+    \"\\n\",\n+    \"DSPy is designed to be compatible with a variety of language models and their respective clients. For this tutorial, we will primarily utilize GPT-4 through the [Azure client](https://github.com/stanfordnlp/dspy/blob/main/dsp/modules/azure_openai.py). However, to demonstrate DSPy's flexibility, we will also provide configuration examples for other popular options:\\n\",\n+    \"\\n\",\n+    \"1. [OpenAI client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/remote_models/OpenAI)\\n\",\n+    \"2. Local models via [Ollama](https://github.com/stanfordnlp/dspy/blob/main/dsp/modules/ollama.py)\\n\",\n+    \"\\n\",\n+    \"## Custom Client Implementation\\n\",\n+    \"\\n\",\n+    \"If your preferred language model client is not natively supported by DSPy, you have the option to implement a custom client. For detailed instructions on creating a custom client, please refer to this [comprehensive guide - custom-lm-client](https://dspy-docs.vercel.app/docs/deep-dive/language_model_clients/custom-lm-client).\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# client for AzureOpenAI\\n\",\n+    \"lm = dspy.AzureOpenAI(\\n\",\n+    \"    api_base=AZURE_OPENAI_ENDPOINT,\\n\",\n+    \"    api_version=AZURE_OPENAI_VERSION,\\n\",\n+    \"    deployment_id=AZURE_OPENAI_DEPLOYMENT,\\n\",\n+    \"    api_key=AZURE_OPENAI_KEY,\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# client for OpenAI\\n\",\n+    \"# lm = dspy.OpenAI(\\n\",\n+    \"#     api_key=OPENAI_API_KEY,\\n\",\n+    \"#     model=OPENAI_MODEL,\\n\",\n+    \"# )\\n\",\n+    \"\\n\",\n+    \"# client for Ollama\\n\",\n+    \"# model_name = \\\"llama3.1\\\"\\n\",\n+    \"# lm = dspy.OllamaLocal(base_url=OLLAMA_URL, model=model_name)\\n\",\n+    \"\\n\",\n+    \"dspy.settings.configure(lm=lm)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# 3. Setting up signature and module\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"## Core DSPy Components\\n\",\n+    \"\\n\",\n+    \"[dspy.Signature](https://dspy-docs.vercel.app/docs/building-blocks/signatures) and [dspy.Module](https://dspy-docs.vercel.app/docs/building-blocks/modules) are fundamental building blocks for DSPy programs:\\n\",\n+    \"\\n\",\n+    \"- **Signature**: A declarative specification of the input/output behavior of a DSPy module.\\n\",\n+    \"- **Module**: A building block for programs that leverage Language Models (LMs).\\n\",\n+    \"\\n\",\n+    \"## Types of DSPy Modules\\n\",\n+    \"\\n\",\n+    \"DSPy offers various module types, each serving different purposes:\\n\",\n+    \"\\n\",\n+    \"1. [dspy.Predict](https://dspy-docs.vercel.app/api/modules/Predict)\\n\",\n+    \"   - Basic predictor\\n\",\n+    \"   - Maintains the original signature\\n\",\n+    \"   - Handles key forms of learning (storing instructions, demonstrations, and LM updates)\\n\",\n+    \"   - Most similar to direct LM usage\\n\",\n+    \"\\n\",\n+    \"2. [dspy.ChainOfThought](https://dspy-docs.vercel.app/api/modules/ChainOfThought)\\n\",\n+    \"   - Enhances the LM to think step-by-step before producing the final response\\n\",\n+    \"   - Modifies the signature to incorporate intermediate reasoning steps\\n\",\n+    \"\\n\",\n+    \"3. [Additional Advanced Modules](https://dspy-docs.vercel.app/api/category/modules)\\n\",\n+    \"   - DSPy library offers a range of more specialized modules for complex tasks\\n\",\n+    \"\\n\",\n+    \"### Recommendation for starting\\n\",\n+    \"\\n\",\n+    \"For those new to DSPy, it's advisable to start with `dspy.Predict`. Its simplicity makes it ideal for understanding the basics of DSPy operation. Once you've successfully implemented your program using `dspy.Predict`, you can explore more advanced modules like `dspy.ChainOfThought` to potentially enhance your model's performance.\\n\",\n+    \"\\n\",\n+    \"For an overview of other prompting techniques beyond zero-shot learning, refer to the [Prompting Guide](https://www.promptingguide.ai/techniques). This resource covers various methods that can enhance your DSPy applications as you progress.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"class PlagiarismSignature(dspy.Signature):\\n\",\n+    \"    # Clarify something about the nature of the task (expressed below as a docstring)! \\n\",\n+    \"    \\\"\\\"\\\"Detect if two code samples are plagiarized. In plagiarized field answer only : Yes if the code samples are plagiarized, No otherwise. In explenation field add the reason why the code samples are/ are not plagiarized.\\\"\\\"\\\"\\n\",\n+    \"\\n\",\n+    \"    # Supply hints on the nature of an input field, expressed as a desc keyword argument for dspy.InputField.\\n\",\n+    \"    code_sample_1 = dspy.InputField(desc=\\\"The first code sample to compare\\\")\\n\",\n+    \"    code_sample_2 = dspy.InputField(desc=\\\"The second code sample to compare\\\")\\n\",\n+    \"\\n\",\n+    \"    # Supply constraints on an output field, expressed as a desc keyword argument for dspy.OutputField.\\n\",\n+    \"    explanation = dspy.OutputField(\\n\",\n+    \"        desc=\\\"Explanation or reason why the code samples are/ are not plagiarized\\\"\\n\",\n+    \"    )\\n\",\n+    \"    plagiarized = dspy.OutputField(\\n\",\n+    \"        desc=\\\"Yes/No indicating if code samples are plagiarized\\\"\\n\",\n+    \"    )\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"class PlagiarismCoT(dspy.Module):\\n\",\n+    \"    def __init__(self) -> None:\\n\",\n+    \"        super().__init__()\\n\",\n+    \"        # Define the program as a ChainOfThought object\\n\",\n+    \"        # self.prog = dspy.PredictiveText(PlagiarismSignature)\\n\",\n+    \"        self.prog = dspy.ChainOfThought(PlagiarismSignature)\\n\",",
        "comment_created_at": "2024-08-01T06:28:58+00:00",
        "comment_author": "williambrach",
        "comment_body": "I changed the main module in the example to dspy.Predict.",
        "pr_file_module": null
      }
    ]
  }
]