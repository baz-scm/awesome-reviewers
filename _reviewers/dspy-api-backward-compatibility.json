[
  {
    "discussion_id": "2217875075",
    "pr_number": 8552,
    "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
    "created_at": "2025-07-20T16:13:48+00:00",
    "commented_code": "view_data_batch_size: int = 10,\n         tip_aware_proposer: bool = True,\n         fewshot_aware_proposer: bool = True,\n-        requires_permission_to_run: bool = True,\n+        requires_permission_to_run: bool = True, # deprecated",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2217875075",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8552,
        "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
        "discussion_id": "2217875075",
        "commented_code": "@@ -112,7 +109,7 @@ def compile(\n         view_data_batch_size: int = 10,\n         tip_aware_proposer: bool = True,\n         fewshot_aware_proposer: bool = True,\n-        requires_permission_to_run: bool = True,\n+        requires_permission_to_run: bool = True, # deprecated",
        "comment_created_at": "2025-07-20T16:13:48+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Unfortunately, removing this argument will result in breaking the users' code that passes `requires_permission_to_run=False`. Do we think this breaking change is acceptable in DSPy 3? We could add `**kwargs` instead, but it would lead to a loss of undefined argument detection.",
        "pr_file_module": null
      },
      {
        "comment_id": "2220697359",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8552,
        "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
        "discussion_id": "2217875075",
        "commented_code": "@@ -112,7 +109,7 @@ def compile(\n         view_data_batch_size: int = 10,\n         tip_aware_proposer: bool = True,\n         fewshot_aware_proposer: bool = True,\n-        requires_permission_to_run: bool = True,\n+        requires_permission_to_run: bool = True, # deprecated",
        "comment_created_at": "2025-07-22T00:53:31+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "We can introduce the wildcard `kwargs` and move the deprecated args there, and print a clear warning that it's deprecated.",
        "pr_file_module": null
      },
      {
        "comment_id": "2220961640",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8552,
        "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
        "discussion_id": "2217875075",
        "commented_code": "@@ -112,7 +109,7 @@ def compile(\n         view_data_batch_size: int = 10,\n         tip_aware_proposer: bool = True,\n         fewshot_aware_proposer: bool = True,\n-        requires_permission_to_run: bool = True,\n+        requires_permission_to_run: bool = True, # deprecated",
        "comment_created_at": "2025-07-22T03:38:42+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Yeah, that's feasible. But I would be cautious about introducing **kwarg as users won't be able to know when they mistakenly pass an undefined argument (or typo). What do you think?",
        "pr_file_module": null
      },
      {
        "comment_id": "2229918482",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8552,
        "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
        "discussion_id": "2217875075",
        "commented_code": "@@ -112,7 +109,7 @@ def compile(\n         view_data_batch_size: int = 10,\n         tip_aware_proposer: bool = True,\n         fewshot_aware_proposer: bool = True,\n-        requires_permission_to_run: bool = True,\n+        requires_permission_to_run: bool = True, # deprecated",
        "comment_created_at": "2025-07-25T01:24:17+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "that's a solid concern! IMO it's a tradeoff between safety and unexpected user misbehavior. We do have wild card `kwargs` for a few methods now, e.g., https://github.com/stanfordnlp/dspy/blob/2e60022f9a524dfd406edc04444114f835d017a0/dspy/predict/predict.py#L82, so I won't worry much about it. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2230243670",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8552,
        "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
        "discussion_id": "2217875075",
        "commented_code": "@@ -112,7 +109,7 @@ def compile(\n         view_data_batch_size: int = 10,\n         tip_aware_proposer: bool = True,\n         fewshot_aware_proposer: bool = True,\n-        requires_permission_to_run: bool = True,\n+        requires_permission_to_run: bool = True, # deprecated",
        "comment_created_at": "2025-07-25T06:14:48+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Yeah, we have **kwargs in many modules, shall we remove the requires_permission_to_run parameter and add kwargs then? cc: @klopsahlong @omkar-sh ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1847138892",
    "pr_number": 1739,
    "pr_file": "dspy/clients/embedding.py",
    "created_at": "2024-11-18T19:30:29+00:00",
    "commented_code": "```python\n         import dspy\n+        import numpy as np\n \n         def my_embedder(texts):\n             return np.random.rand(len(texts), 10)\n \n-        embedder = dspy.Embedding(my_embedder)\n+        embedder = dspy.Embedding(embedding_function=my_embedder)\n         embeddings = embedder([\"hello\", \"world\"])\n \n         assert embeddings.shape == (2, 10)\n         ```\n     \"\"\"\n \n-    def __init__(self, model):\n-        self.model = model\n+    def __init__(self, embedding_model: Union[str, Callable[[List[str]], List[List[float]]]] = 'text-embedding-ada-002', embedding_function: Optional[Callable[[List[str]], List[List[float]]]] = None):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1847138892",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1739,
        "pr_file": "dspy/clients/embedding.py",
        "discussion_id": "1847138892",
        "commented_code": "@@ -41,21 +42,27 @@ class Embedding:\n \n         ```python\n         import dspy\n+        import numpy as np\n \n         def my_embedder(texts):\n             return np.random.rand(len(texts), 10)\n \n-        embedder = dspy.Embedding(my_embedder)\n+        embedder = dspy.Embedding(embedding_function=my_embedder)\n         embeddings = embedder([\"hello\", \"world\"])\n \n         assert embeddings.shape == (2, 10)\n         ```\n     \"\"\"\n \n-    def __init__(self, model):\n-        self.model = model\n+    def __init__(self, embedding_model: Union[str, Callable[[List[str]], List[List[float]]]] = 'text-embedding-ada-002', embedding_function: Optional[Callable[[List[str]], List[List[float]]]] = None):",
        "comment_created_at": "2024-11-18T19:30:29+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "Why do we need this change? It's a common pattern to have model be string for hosted model, and callable for custom model, as most model instances are callable themselves. \r\n\r\nBtw, if we add type annotation, we shouldn't assume the arg type of the callable, which could be easily violated by users' custom function. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2153566543",
    "pr_number": 8394,
    "pr_file": "dspy/clients/cache.py",
    "created_at": "2025-06-18T03:32:43+00:00",
    "commented_code": "params = {k: transform_value(v) for k, v in request.items() if k not in ignored_args_for_cache_key}\n         return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()\n \n-    def get(self, request: Dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> Any:\n-        try:\n-            key = self.cache_key(request, ignored_args_for_cache_key)\n-        except Exception:\n-            logger.debug(f\"Failed to generate cache key for request: {request}\")\n-            return None\n-\n+    def get(self, key: str) -> Any:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2153566543",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8394,
        "pr_file": "dspy/clients/cache.py",
        "discussion_id": "2153566543",
        "commented_code": "@@ -96,13 +96,7 @@ def transform_value(value):\n         params = {k: transform_value(v) for k, v in request.items() if k not in ignored_args_for_cache_key}\n         return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()\n \n-    def get(self, request: Dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> Any:\n-        try:\n-            key = self.cache_key(request, ignored_args_for_cache_key)\n-        except Exception:\n-            logger.debug(f\"Failed to generate cache key for request: {request}\")\n-            return None\n-\n+    def get(self, key: str) -> Any:",
        "comment_created_at": "2025-06-18T03:32:43+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "This signature cannot be changed, we do need to handle the case where users directly call `get`. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2154783655",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8394,
        "pr_file": "dspy/clients/cache.py",
        "discussion_id": "2153566543",
        "commented_code": "@@ -96,13 +96,7 @@ def transform_value(value):\n         params = {k: transform_value(v) for k, v in request.items() if k not in ignored_args_for_cache_key}\n         return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()\n \n-    def get(self, request: Dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> Any:\n-        try:\n-            key = self.cache_key(request, ignored_args_for_cache_key)\n-        except Exception:\n-            logger.debug(f\"Failed to generate cache key for request: {request}\")\n-            return None\n-\n+    def get(self, key: str) -> Any:",
        "comment_created_at": "2025-06-18T14:37:05+00:00",
        "comment_author": "poudro",
        "comment_body": "I understand and that's totally valid, I've updated the PR to make a copy of request before calling litellm instead",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1986058159",
    "pr_number": 7922,
    "pr_file": "dspy/adapters/conversation_feedback_adapter.py",
    "created_at": "2025-03-08T11:14:44+00:00",
    "commented_code": "+from typing import List, Dict, Any\n+from dspy.signatures.signature import Signature\n+from dspy.adapters import ChatAdapter\n+\n+\n+class ConversationalFeedbackAdapter(ChatAdapter):\n+    \"\"\"\n+    Adapter that injects feedback as conversation turns into the model inputs.\n+    Directly inherits from ChatAdapter.\n+    \"\"\"\n+\n+    def __init__(self,\n+        feedback_history: List[str] = None,\n+        attempt_outputs: List[str] = None,\n+        **kwargs\n+    ):\n+        \"\"\"\n+        Initialize the adapter with feedback history and attempt outputs.\n+\n+        Args:\n+            feedback_history: List of feedback strings from previous attempts\n+            attempt_outputs: List of prediction outputs from previous attempts\n+            **kwargs: Additional arguments to pass to ChatAdapter\n+        \"\"\"\n+        super().__init__(**kwargs)\n+        self.feedback_history = feedback_history or []\n+        self.attempt_outputs = attempt_outputs or []",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1986058159",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 7922,
        "pr_file": "dspy/adapters/conversation_feedback_adapter.py",
        "discussion_id": "1986058159",
        "commented_code": "@@ -0,0 +1,93 @@\n+from typing import List, Dict, Any\n+from dspy.signatures.signature import Signature\n+from dspy.adapters import ChatAdapter\n+\n+\n+class ConversationalFeedbackAdapter(ChatAdapter):\n+    \"\"\"\n+    Adapter that injects feedback as conversation turns into the model inputs.\n+    Directly inherits from ChatAdapter.\n+    \"\"\"\n+\n+    def __init__(self,\n+        feedback_history: List[str] = None,\n+        attempt_outputs: List[str] = None,\n+        **kwargs\n+    ):\n+        \"\"\"\n+        Initialize the adapter with feedback history and attempt outputs.\n+\n+        Args:\n+            feedback_history: List of feedback strings from previous attempts\n+            attempt_outputs: List of prediction outputs from previous attempts\n+            **kwargs: Additional arguments to pass to ChatAdapter\n+        \"\"\"\n+        super().__init__(**kwargs)\n+        self.feedback_history = feedback_history or []\n+        self.attempt_outputs = attempt_outputs or []",
        "comment_created_at": "2025-03-08T11:14:44+00:00",
        "comment_author": "zbambergerNLP",
        "comment_body": "Rather than providing feedback history and attempted outputs via the initializer for this adapter, incorporate them into the signature that the adapter accepts during `format`. In this way, you can initialize one adapter, and it will operate smoothly across different inputs (with differing feedback histories and attempted outputs). ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1791174107",
    "pr_number": 1604,
    "pr_file": "dspy/clients/lm.py",
    "created_at": "2024-10-08T04:40:34+00:00",
    "commented_code": "-import os\n-import uuid \n-import ujson\n import functools\n-from pathlib import Path\n+import os\n+import uuid\n from datetime import datetime\n+from pathlib import Path\n \n-try:\n-    import warnings\n-    with warnings.catch_warnings():\n-        warnings.filterwarnings(\"ignore\", category=UserWarning)\n-        if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n-             os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n-        import litellm  \n-        litellm.telemetry = False\n+import litellm\n+import ujson\n+from litellm.caching import Cache\n \n-    from litellm.caching import Cache\n-    disk_cache_dir = os.environ.get('DSPY_CACHEDIR') or os.path.join(Path.home(), '.dspy_cache')\n-    litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+disk_cache_dir = os.environ.get(\"DSPY_CACHEDIR\") or os.path.join(Path.home(), \".dspy_cache\")\n+litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+litellm.telemetry = False\n \n-except ImportError:\n-    class LitellmPlaceholder:\n-        def __getattr__(self, _): raise ImportError(\"The LiteLLM package is not installed. Run `pip install litellm`.\")\n+if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n+    os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n \n-    litellm = LitellmPlaceholder()\n \n class LM:\n-    def __init__(self, model, model_type='chat', temperature=0.0, max_tokens=1000, cache=True, **kwargs):\n+    def __init__(self, model, model_type=\"chat\", temperature=0.0, max_tokens=1000, cache=True):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1791174107",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1604,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1791174107",
        "commented_code": "@@ -1,51 +1,46 @@\n-import os\n-import uuid \n-import ujson\n import functools\n-from pathlib import Path\n+import os\n+import uuid\n from datetime import datetime\n+from pathlib import Path\n \n-try:\n-    import warnings\n-    with warnings.catch_warnings():\n-        warnings.filterwarnings(\"ignore\", category=UserWarning)\n-        if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n-             os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n-        import litellm  \n-        litellm.telemetry = False\n+import litellm\n+import ujson\n+from litellm.caching import Cache\n \n-    from litellm.caching import Cache\n-    disk_cache_dir = os.environ.get('DSPY_CACHEDIR') or os.path.join(Path.home(), '.dspy_cache')\n-    litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+disk_cache_dir = os.environ.get(\"DSPY_CACHEDIR\") or os.path.join(Path.home(), \".dspy_cache\")\n+litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+litellm.telemetry = False\n \n-except ImportError:\n-    class LitellmPlaceholder:\n-        def __getattr__(self, _): raise ImportError(\"The LiteLLM package is not installed. Run `pip install litellm`.\")\n+if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n+    os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n \n-    litellm = LitellmPlaceholder()\n \n class LM:\n-    def __init__(self, model, model_type='chat', temperature=0.0, max_tokens=1000, cache=True, **kwargs):\n+    def __init__(self, model, model_type=\"chat\", temperature=0.0, max_tokens=1000, cache=True):",
        "comment_created_at": "2024-10-08T04:40:34+00:00",
        "comment_author": "okhat",
        "comment_body": "Thanks Chen! Why remove kwargs? The kwargs here are important. They're at the level of the LM object. The kwargs in`__call__` are at the level of the individual call. Both are needed.",
        "pr_file_module": null
      },
      {
        "comment_id": "1791178770",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1604,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1791174107",
        "commented_code": "@@ -1,51 +1,46 @@\n-import os\n-import uuid \n-import ujson\n import functools\n-from pathlib import Path\n+import os\n+import uuid\n from datetime import datetime\n+from pathlib import Path\n \n-try:\n-    import warnings\n-    with warnings.catch_warnings():\n-        warnings.filterwarnings(\"ignore\", category=UserWarning)\n-        if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n-             os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n-        import litellm  \n-        litellm.telemetry = False\n+import litellm\n+import ujson\n+from litellm.caching import Cache\n \n-    from litellm.caching import Cache\n-    disk_cache_dir = os.environ.get('DSPY_CACHEDIR') or os.path.join(Path.home(), '.dspy_cache')\n-    litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+disk_cache_dir = os.environ.get(\"DSPY_CACHEDIR\") or os.path.join(Path.home(), \".dspy_cache\")\n+litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+litellm.telemetry = False\n \n-except ImportError:\n-    class LitellmPlaceholder:\n-        def __getattr__(self, _): raise ImportError(\"The LiteLLM package is not installed. Run `pip install litellm`.\")\n+if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n+    os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n \n-    litellm = LitellmPlaceholder()\n \n class LM:\n-    def __init__(self, model, model_type='chat', temperature=0.0, max_tokens=1000, cache=True, **kwargs):\n+    def __init__(self, model, model_type=\"chat\", temperature=0.0, max_tokens=1000, cache=True):",
        "comment_created_at": "2024-10-08T04:46:42+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "ohh the `kwargs` is not really used in `__init__` method, and the only place it is used is inside `__call__` method, where we join it with `kwargs` of `__call__` method. This is not ideal because we are not drawing an explicit boundary between these two `kwargs`, so users don't know which one they should use. We might want to explicitly list out model args like `temperature` and `max_tokens` for clarity.",
        "pr_file_module": null
      },
      {
        "comment_id": "1791195822",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1604,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1791174107",
        "commented_code": "@@ -1,51 +1,46 @@\n-import os\n-import uuid \n-import ujson\n import functools\n-from pathlib import Path\n+import os\n+import uuid\n from datetime import datetime\n+from pathlib import Path\n \n-try:\n-    import warnings\n-    with warnings.catch_warnings():\n-        warnings.filterwarnings(\"ignore\", category=UserWarning)\n-        if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n-             os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n-        import litellm  \n-        litellm.telemetry = False\n+import litellm\n+import ujson\n+from litellm.caching import Cache\n \n-    from litellm.caching import Cache\n-    disk_cache_dir = os.environ.get('DSPY_CACHEDIR') or os.path.join(Path.home(), '.dspy_cache')\n-    litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+disk_cache_dir = os.environ.get(\"DSPY_CACHEDIR\") or os.path.join(Path.home(), \".dspy_cache\")\n+litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+litellm.telemetry = False\n \n-except ImportError:\n-    class LitellmPlaceholder:\n-        def __getattr__(self, _): raise ImportError(\"The LiteLLM package is not installed. Run `pip install litellm`.\")\n+if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n+    os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n \n-    litellm = LitellmPlaceholder()\n \n class LM:\n-    def __init__(self, model, model_type='chat', temperature=0.0, max_tokens=1000, cache=True, **kwargs):\n+    def __init__(self, model, model_type=\"chat\", temperature=0.0, max_tokens=1000, cache=True):",
        "comment_created_at": "2024-10-08T05:01:47+00:00",
        "comment_author": "okhat",
        "comment_body": "Thank you @chenmoneygithub ! It's important that users can set kwargs in `__init__`, which become defaults for `__call__`. The `kwargs` passed to `__call__` overwrite any defaults from `__init__`. It was well-defined and commonly used. We should add it back IMO.",
        "pr_file_module": null
      },
      {
        "comment_id": "1791210532",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1604,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1791174107",
        "commented_code": "@@ -1,51 +1,46 @@\n-import os\n-import uuid \n-import ujson\n import functools\n-from pathlib import Path\n+import os\n+import uuid\n from datetime import datetime\n+from pathlib import Path\n \n-try:\n-    import warnings\n-    with warnings.catch_warnings():\n-        warnings.filterwarnings(\"ignore\", category=UserWarning)\n-        if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n-             os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n-        import litellm  \n-        litellm.telemetry = False\n+import litellm\n+import ujson\n+from litellm.caching import Cache\n \n-    from litellm.caching import Cache\n-    disk_cache_dir = os.environ.get('DSPY_CACHEDIR') or os.path.join(Path.home(), '.dspy_cache')\n-    litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+disk_cache_dir = os.environ.get(\"DSPY_CACHEDIR\") or os.path.join(Path.home(), \".dspy_cache\")\n+litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type=\"disk\")\n+litellm.telemetry = False\n \n-except ImportError:\n-    class LitellmPlaceholder:\n-        def __getattr__(self, _): raise ImportError(\"The LiteLLM package is not installed. Run `pip install litellm`.\")\n+if \"LITELLM_LOCAL_MODEL_COST_MAP\" not in os.environ:\n+    os.environ[\"LITELLM_LOCAL_MODEL_COST_MAP\"] = \"True\"\n \n-    litellm = LitellmPlaceholder()\n \n class LM:\n-    def __init__(self, model, model_type='chat', temperature=0.0, max_tokens=1000, cache=True, **kwargs):\n+    def __init__(self, model, model_type=\"chat\", temperature=0.0, max_tokens=1000, cache=True):",
        "comment_created_at": "2024-10-08T05:23:19+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "sure thing! Let me add it back.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1821499888",
    "pr_number": 1698,
    "pr_file": "dspy/clients/provider.py",
    "created_at": "2024-10-29T20:36:46+00:00",
    "commented_code": "+from concurrent.futures import Future\n+from abc import abstractmethod\n+from threading import Thread\n+from typing import Any, Dict, List, Optional\n+\n+from dspy.clients.utils_finetune import DataFormat\n+from dspy.utils.logging import logger\n+\n+\n+class TrainingJob(Future):\n+    def __init__(\n+        self,\n+        thread: Thread,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ):\n+        self.thread = thread\n+        self.model = model\n+        self.train_data = train_data\n+        self.train_kwargs = train_kwargs or {}\n+        self.data_format = data_format\n+        super().__init__()\n+\n+    # Subclasses should override the cancel method to cancel the job; then call\n+    # the super's cancel method so that the future can be cancelled.\n+    def cancel(self):\n+        super().cancel()\n+\n+    @abstractmethod\n+    def status(self):\n+        raise NotImplementedError\n+\n+\n+class Provider:\n+    \n+    def __init__(self):\n+        self.finetunable = False\n+        self.TrainingJob = TrainingJob\n+\n+    @staticmethod\n+    def is_provider_model(model: str) -> bool:\n+        # Subclasses should actually check whether a model is supported if they\n+        # want to have the model provider auto-discovered.\n+        return False\n+\n+    @staticmethod\n+    def launch(model: str, launch_kwargs: Optional[Dict[str, Any]]=None):\n+        msg = f\"`launch()` is called for the auto-launched model {model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+    \n+    @staticmethod\n+    def kill(model: str, launch_kwargs: Optional[Dict[str, Any]]=None):\n+        msg = f\"`kill()` is called for the auto-launched model {model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+    \n+    @staticmethod\n+    def finetune(\n+        job: TrainingJob,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ) -> str:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1821499888",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1698,
        "pr_file": "dspy/clients/provider.py",
        "discussion_id": "1821499888",
        "commented_code": "@@ -0,0 +1,68 @@\n+from concurrent.futures import Future\n+from abc import abstractmethod\n+from threading import Thread\n+from typing import Any, Dict, List, Optional\n+\n+from dspy.clients.utils_finetune import DataFormat\n+from dspy.utils.logging import logger\n+\n+\n+class TrainingJob(Future):\n+    def __init__(\n+        self,\n+        thread: Thread,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ):\n+        self.thread = thread\n+        self.model = model\n+        self.train_data = train_data\n+        self.train_kwargs = train_kwargs or {}\n+        self.data_format = data_format\n+        super().__init__()\n+\n+    # Subclasses should override the cancel method to cancel the job; then call\n+    # the super's cancel method so that the future can be cancelled.\n+    def cancel(self):\n+        super().cancel()\n+\n+    @abstractmethod\n+    def status(self):\n+        raise NotImplementedError\n+\n+\n+class Provider:\n+    \n+    def __init__(self):\n+        self.finetunable = False\n+        self.TrainingJob = TrainingJob\n+\n+    @staticmethod\n+    def is_provider_model(model: str) -> bool:\n+        # Subclasses should actually check whether a model is supported if they\n+        # want to have the model provider auto-discovered.\n+        return False\n+\n+    @staticmethod\n+    def launch(model: str, launch_kwargs: Optional[Dict[str, Any]]=None):\n+        msg = f\"`launch()` is called for the auto-launched model {model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+    \n+    @staticmethod\n+    def kill(model: str, launch_kwargs: Optional[Dict[str, Any]]=None):\n+        msg = f\"`kill()` is called for the auto-launched model {model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+    \n+    @staticmethod\n+    def finetune(\n+        job: TrainingJob,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ) -> str:",
        "comment_created_at": "2024-10-29T20:36:46+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "This method doesn't necessarily need to be implemented IMO, or we can rename `Provider` to `FinetuningProvider`. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1821576983",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1698,
        "pr_file": "dspy/clients/provider.py",
        "discussion_id": "1821499888",
        "commented_code": "@@ -0,0 +1,68 @@\n+from concurrent.futures import Future\n+from abc import abstractmethod\n+from threading import Thread\n+from typing import Any, Dict, List, Optional\n+\n+from dspy.clients.utils_finetune import DataFormat\n+from dspy.utils.logging import logger\n+\n+\n+class TrainingJob(Future):\n+    def __init__(\n+        self,\n+        thread: Thread,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ):\n+        self.thread = thread\n+        self.model = model\n+        self.train_data = train_data\n+        self.train_kwargs = train_kwargs or {}\n+        self.data_format = data_format\n+        super().__init__()\n+\n+    # Subclasses should override the cancel method to cancel the job; then call\n+    # the super's cancel method so that the future can be cancelled.\n+    def cancel(self):\n+        super().cancel()\n+\n+    @abstractmethod\n+    def status(self):\n+        raise NotImplementedError\n+\n+\n+class Provider:\n+    \n+    def __init__(self):\n+        self.finetunable = False\n+        self.TrainingJob = TrainingJob\n+\n+    @staticmethod\n+    def is_provider_model(model: str) -> bool:\n+        # Subclasses should actually check whether a model is supported if they\n+        # want to have the model provider auto-discovered.\n+        return False\n+\n+    @staticmethod\n+    def launch(model: str, launch_kwargs: Optional[Dict[str, Any]]=None):\n+        msg = f\"`launch()` is called for the auto-launched model {model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+    \n+    @staticmethod\n+    def kill(model: str, launch_kwargs: Optional[Dict[str, Any]]=None):\n+        msg = f\"`kill()` is called for the auto-launched model {model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+    \n+    @staticmethod\n+    def finetune(\n+        job: TrainingJob,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ) -> str:",
        "comment_created_at": "2024-10-29T21:53:21+00:00",
        "comment_author": "okhat",
        "comment_body": "Yes rename the class to FinetuningProvider",
        "pr_file_module": null
      },
      {
        "comment_id": "1821677670",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1698,
        "pr_file": "dspy/clients/provider.py",
        "discussion_id": "1821499888",
        "commented_code": "@@ -0,0 +1,68 @@\n+from concurrent.futures import Future\n+from abc import abstractmethod\n+from threading import Thread\n+from typing import Any, Dict, List, Optional\n+\n+from dspy.clients.utils_finetune import DataFormat\n+from dspy.utils.logging import logger\n+\n+\n+class TrainingJob(Future):\n+    def __init__(\n+        self,\n+        thread: Thread,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ):\n+        self.thread = thread\n+        self.model = model\n+        self.train_data = train_data\n+        self.train_kwargs = train_kwargs or {}\n+        self.data_format = data_format\n+        super().__init__()\n+\n+    # Subclasses should override the cancel method to cancel the job; then call\n+    # the super's cancel method so that the future can be cancelled.\n+    def cancel(self):\n+        super().cancel()\n+\n+    @abstractmethod\n+    def status(self):\n+        raise NotImplementedError\n+\n+\n+class Provider:\n+    \n+    def __init__(self):\n+        self.finetunable = False\n+        self.TrainingJob = TrainingJob\n+\n+    @staticmethod\n+    def is_provider_model(model: str) -> bool:\n+        # Subclasses should actually check whether a model is supported if they\n+        # want to have the model provider auto-discovered.\n+        return False\n+\n+    @staticmethod\n+    def launch(model: str, launch_kwargs: Optional[Dict[str, Any]]=None):\n+        msg = f\"`launch()` is called for the auto-launched model {model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+    \n+    @staticmethod\n+    def kill(model: str, launch_kwargs: Optional[Dict[str, Any]]=None):\n+        msg = f\"`kill()` is called for the auto-launched model {model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+    \n+    @staticmethod\n+    def finetune(\n+        job: TrainingJob,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ) -> str:",
        "comment_created_at": "2024-10-30T00:02:09+00:00",
        "comment_author": "dilarasoylu",
        "comment_body": "More descriptive name sounds better, sounds good to me.\r\n\r\nAs one more data point, this class encompasses more than fine-tuning as @chenmoneygithub pointed out -- it also specifies a way to launch/kill. It can be used without fine-tuning (say, if we want to add support for launching TGI/SGLang)",
        "pr_file_module": null
      }
    ]
  }
]