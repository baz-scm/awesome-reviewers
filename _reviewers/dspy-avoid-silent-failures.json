[
  {
    "discussion_id": "1569731292",
    "pr_number": 424,
    "pr_file": "dspy/evaluate/loss.py",
    "created_at": "2024-04-18T00:41:53+00:00",
    "commented_code": "+from dataclasses import dataclass, field\n+from datetime import UTC, datetime\n+from typing import Callable\n+\n+from snoop import snoop",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1569731292",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 424,
        "pr_file": "dspy/evaluate/loss.py",
        "discussion_id": "1569731292",
        "commented_code": "@@ -0,0 +1,60 @@\n+from dataclasses import dataclass, field\n+from datetime import UTC, datetime\n+from typing import Callable\n+\n+from snoop import snoop",
        "comment_created_at": "2024-04-18T00:41:53+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "can we make this an optional dependency and wrap in try-except?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2087641223",
    "pr_number": 8171,
    "pr_file": "dspy/adapters/json_adapter.py",
    "created_at": "2025-05-13T21:12:04+00:00",
    "commented_code": "lm_kwargs[\"response_format\"] = structured_output_model\n             return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n         except Exception as e:\n-            logger.warning(f\"Failed to use structured output format. Falling back to JSON mode. Error: {e}\")\n+            if dspy.settings.log_json_fallbacks:\n+                logger.warning(f\"Failed to use structured output format. Falling back to JSON mode. Error: {e}\")\n             try:\n                 lm_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n                 return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n+            except ValueError as ve:\n+                raise ve",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2087641223",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8171,
        "pr_file": "dspy/adapters/json_adapter.py",
        "discussion_id": "2087641223",
        "commented_code": "@@ -62,10 +63,13 @@ def __call__(\n             lm_kwargs[\"response_format\"] = structured_output_model\n             return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n         except Exception as e:\n-            logger.warning(f\"Failed to use structured output format. Falling back to JSON mode. Error: {e}\")\n+            if dspy.settings.log_json_fallbacks:\n+                logger.warning(f\"Failed to use structured output format. Falling back to JSON mode. Error: {e}\")\n             try:\n                 lm_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n                 return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n+            except ValueError as ve:\n+                raise ve",
        "comment_created_at": "2025-05-13T21:12:04+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "same here",
        "pr_file_module": null
      },
      {
        "comment_id": "2087674720",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8171,
        "pr_file": "dspy/adapters/json_adapter.py",
        "discussion_id": "2087641223",
        "commented_code": "@@ -62,10 +63,13 @@ def __call__(\n             lm_kwargs[\"response_format\"] = structured_output_model\n             return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n         except Exception as e:\n-            logger.warning(f\"Failed to use structured output format. Falling back to JSON mode. Error: {e}\")\n+            if dspy.settings.log_json_fallbacks:\n+                logger.warning(f\"Failed to use structured output format. Falling back to JSON mode. Error: {e}\")\n             try:\n                 lm_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n                 return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n+            except ValueError as ve:\n+                raise ve",
        "comment_created_at": "2025-05-13T21:39:50+00:00",
        "comment_author": "LakshyAAAgrawal",
        "comment_body": "This is necessary, since the other except block captures all errors, and instead raises a RuntimeError. We need to pass in the ValueError as it is to transfer all the args.",
        "pr_file_module": null
      },
      {
        "comment_id": "2088132076",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8171,
        "pr_file": "dspy/adapters/json_adapter.py",
        "discussion_id": "2087641223",
        "commented_code": "@@ -62,10 +63,13 @@ def __call__(\n             lm_kwargs[\"response_format\"] = structured_output_model\n             return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n         except Exception as e:\n-            logger.warning(f\"Failed to use structured output format. Falling back to JSON mode. Error: {e}\")\n+            if dspy.settings.log_json_fallbacks:\n+                logger.warning(f\"Failed to use structured output format. Falling back to JSON mode. Error: {e}\")\n             try:\n                 lm_kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n                 return super().__call__(lm, lm_kwargs, signature, demos, inputs)\n+            except ValueError as ve:\n+                raise ve",
        "comment_created_at": "2025-05-14T06:17:47+00:00",
        "comment_author": "LakshyAAAgrawal",
        "comment_body": "Now that there is a parseerror, this should be changed to capture and passthrough the parse error",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1590472750",
    "pr_number": 849,
    "pr_file": "dspy/teleprompt/bootstrap.py",
    "created_at": "2024-05-06T01:38:34+00:00",
    "commented_code": "import random\n+\n+random.seed(42) # meaning of life reproducibility.\n import threading\n \n import tqdm\n+import wandb  # Maybe an idea to refactor into a config to isolate the dependency, but it doesn't add too many LOC to customize the observability to the compiler.",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1590472750",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 849,
        "pr_file": "dspy/teleprompt/bootstrap.py",
        "discussion_id": "1590472750",
        "commented_code": "@@ -1,7 +1,10 @@\n import random\n+\n+random.seed(42) # meaning of life reproducibility.\n import threading\n \n import tqdm\n+import wandb  # Maybe an idea to refactor into a config to isolate the dependency, but it doesn't add too many LOC to customize the observability to the compiler.",
        "comment_created_at": "2024-05-06T01:38:34+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "this can be wrapped in a try/except block similar to [handling import dependencies like here](https://github.com/stanfordnlp/dspy/blob/d7dace6b51e1c0899fd798c575262310939f6deb/dsp/modules/cohere.py#L8). This will also fix the failing test\r\n\r\nalso can remove comment",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1821471446",
    "pr_number": 1698,
    "pr_file": "dspy/clients/openai.py",
    "created_at": "2024-10-29T20:13:58+00:00",
    "commented_code": "import re\n import time\n-from collections import defaultdict\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional\n \n import openai\n \n-from dspy.clients.finetune import (\n-    FinetuneJob,\n-    TrainingMethod,\n-    TrainingStatus,\n-    save_data,\n-    validate_finetune_data,\n-)\n+from dspy.clients.provider import TrainingJob, Provider\n+from dspy.clients.utils_finetune import DataFormat, TrainingStatus, save_data\n from dspy.utils.logging import logger\n \n-# Provider name\n-PROVIDER_OPENAI = \"openai\"\n \n-\n-def is_openai_model(model: str) -> bool:\n-    \"\"\"Check if the model is an OpenAI model.\"\"\"\n-    # Filter the provider_prefix, if exists\n-    provider_prefix = f\"{PROVIDER_OPENAI}/\"\n-    if model.startswith(provider_prefix):\n-        model = model[len(provider_prefix) :]\n-\n-    client = openai.OpenAI()\n-    valid_model_names = [model.id for model in client.models.list().data]\n-    # Check if the model is a base OpenAI model\n-    if model in valid_model_names:\n-        return True\n-\n-    # Check if the model is a fine-tuned OpneAI model. Fine-tuned OpenAI models\n-    # have the prefix \"ft:<BASE_MODEL_NAME>:\", followed by a string specifying\n-    # the fine-tuned model. The following RegEx pattern is used to match the\n-    # base model name.\n-    # TODO: This part can be updated to match the actual fine-tuned model names\n-    # by making a call to the OpenAI API to be more exact, but this might\n-    # require an API key with the right permissions.\n-    match = re.match(r\"ft:([^:]+):\", model)\n-    if match and match.group(1) in valid_model_names:\n-        return True\n-\n-    return False\n+_OPENAI_MODELS = [\n+  'gpt-4-turbo',\n+  'gpt-4-turbo-2024-04-09',\n+  'tts-1',\n+  'tts-1-1106',\n+  'chatgpt-4o-latest',\n+  'dall-e-2',\n+  'whisper-1',\n+  'gpt-3.5-turbo-instruct',\n+  'gpt-3.5-turbo',\n+  'gpt-3.5-turbo-0125',\n+  'babbage-002',\n+  'davinci-002',\n+  'gpt-4o-mini-2024-07-18',\n+  'gpt-4o',\n+  'dall-e-3',\n+  'gpt-4o-mini',\n+  'gpt-4o-2024-08-06',\n+  'gpt-4o-2024-05-13',\n+  'o1-preview',\n+  'gpt-4o-audio-preview-2024-10-01',\n+  'o1-mini-2024-09-12',\n+  'gpt-4o-audio-preview',\n+  'tts-1-hd',\n+  'tts-1-hd-1106',\n+  'o1-preview-2024-09-12',\n+  'o1-mini',\n+  'gpt-4-1106-preview',\n+  'text-embedding-ada-002',\n+  'gpt-3.5-turbo-16k',\n+  'text-embedding-3-small',\n+  'text-embedding-3-large',\n+  'gpt-4o-realtime-preview-2024-10-01',\n+  'gpt-4o-realtime-preview',\n+  'gpt-3.5-turbo-1106',\n+  'gpt-4-0613',\n+  'gpt-4-turbo-preview',\n+  'gpt-4-0125-preview',\n+  'gpt-4',\n+  'gpt-3.5-turbo-instruct-0914'\n+]\n \n \n-class FinetuneJobOpenAI(FinetuneJob):\n+class TrainingJobOpenAI(TrainingJob):\n     def __init__(self, *args, **kwargs):\n-        self.provider_file_id = None  # TODO: Can we get this using the job_id?\n-        self.provider_job_id = None\n         super().__init__(*args, **kwargs)\n+        self.provider_file_id = None\n+        self.provider_job_id = None\n \n     def cancel(self):\n         # Cancel the provider job\n-        if _does_job_exist(self.provider_job_id):\n-            status = _get_training_status(self.provider_job_id)\n-            if _is_terminal_training_status(status):\n+        if OpenAIProvider.does_job_exist(self.provider_job_id):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1821471446",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1698,
        "pr_file": "dspy/clients/openai.py",
        "discussion_id": "1821471446",
        "commented_code": "@@ -1,358 +1,271 @@\n import re\n import time\n-from collections import defaultdict\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional\n \n import openai\n \n-from dspy.clients.finetune import (\n-    FinetuneJob,\n-    TrainingMethod,\n-    TrainingStatus,\n-    save_data,\n-    validate_finetune_data,\n-)\n+from dspy.clients.provider import TrainingJob, Provider\n+from dspy.clients.utils_finetune import DataFormat, TrainingStatus, save_data\n from dspy.utils.logging import logger\n \n-# Provider name\n-PROVIDER_OPENAI = \"openai\"\n \n-\n-def is_openai_model(model: str) -> bool:\n-    \"\"\"Check if the model is an OpenAI model.\"\"\"\n-    # Filter the provider_prefix, if exists\n-    provider_prefix = f\"{PROVIDER_OPENAI}/\"\n-    if model.startswith(provider_prefix):\n-        model = model[len(provider_prefix) :]\n-\n-    client = openai.OpenAI()\n-    valid_model_names = [model.id for model in client.models.list().data]\n-    # Check if the model is a base OpenAI model\n-    if model in valid_model_names:\n-        return True\n-\n-    # Check if the model is a fine-tuned OpneAI model. Fine-tuned OpenAI models\n-    # have the prefix \"ft:<BASE_MODEL_NAME>:\", followed by a string specifying\n-    # the fine-tuned model. The following RegEx pattern is used to match the\n-    # base model name.\n-    # TODO: This part can be updated to match the actual fine-tuned model names\n-    # by making a call to the OpenAI API to be more exact, but this might\n-    # require an API key with the right permissions.\n-    match = re.match(r\"ft:([^:]+):\", model)\n-    if match and match.group(1) in valid_model_names:\n-        return True\n-\n-    return False\n+_OPENAI_MODELS = [\n+  'gpt-4-turbo',\n+  'gpt-4-turbo-2024-04-09',\n+  'tts-1',\n+  'tts-1-1106',\n+  'chatgpt-4o-latest',\n+  'dall-e-2',\n+  'whisper-1',\n+  'gpt-3.5-turbo-instruct',\n+  'gpt-3.5-turbo',\n+  'gpt-3.5-turbo-0125',\n+  'babbage-002',\n+  'davinci-002',\n+  'gpt-4o-mini-2024-07-18',\n+  'gpt-4o',\n+  'dall-e-3',\n+  'gpt-4o-mini',\n+  'gpt-4o-2024-08-06',\n+  'gpt-4o-2024-05-13',\n+  'o1-preview',\n+  'gpt-4o-audio-preview-2024-10-01',\n+  'o1-mini-2024-09-12',\n+  'gpt-4o-audio-preview',\n+  'tts-1-hd',\n+  'tts-1-hd-1106',\n+  'o1-preview-2024-09-12',\n+  'o1-mini',\n+  'gpt-4-1106-preview',\n+  'text-embedding-ada-002',\n+  'gpt-3.5-turbo-16k',\n+  'text-embedding-3-small',\n+  'text-embedding-3-large',\n+  'gpt-4o-realtime-preview-2024-10-01',\n+  'gpt-4o-realtime-preview',\n+  'gpt-3.5-turbo-1106',\n+  'gpt-4-0613',\n+  'gpt-4-turbo-preview',\n+  'gpt-4-0125-preview',\n+  'gpt-4',\n+  'gpt-3.5-turbo-instruct-0914'\n+]\n \n \n-class FinetuneJobOpenAI(FinetuneJob):\n+class TrainingJobOpenAI(TrainingJob):\n     def __init__(self, *args, **kwargs):\n-        self.provider_file_id = None  # TODO: Can we get this using the job_id?\n-        self.provider_job_id = None\n         super().__init__(*args, **kwargs)\n+        self.provider_file_id = None\n+        self.provider_job_id = None\n \n     def cancel(self):\n         # Cancel the provider job\n-        if _does_job_exist(self.provider_job_id):\n-            status = _get_training_status(self.provider_job_id)\n-            if _is_terminal_training_status(status):\n+        if OpenAIProvider.does_job_exist(self.provider_job_id):",
        "comment_created_at": "2024-10-29T20:13:58+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "Let's print out a warning if job doesn't exist, otherwise the method will be a silent noop.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1563444729",
    "pr_number": 797,
    "pr_file": "dsp/modules/colbertv2.py",
    "created_at": "2024-04-13T00:51:53+00:00",
    "commented_code": "colbertv2_post_request = colbertv2_post_request_v2_wrapped\n+os.environ['COLBERT_LOAD_TORCH_EXTENSION_VERBOSE'] = \"True\"\n+\n+class ColBERTv2RetrieverLocal:\n+    def __init__(self,passages:List[str],colbert_config=None,load_only:bool=False,index_name:str=\"colbert_rm\",checkpoint:str='colbert-ir/colbertv2.0'):\n+        \"\"\"Colbertv2 retriever module\n+\n+        Args:\n+            passages (List[str]): list of passages\n+            load_only (bool, optional): whether to load the index or . Defaults to False.\n+            index_name (str, optional): name of the index. Defaults to \"colbert_rm\".\n+            checkpoint (str, optional): checkpoint for generating embeddings. Defaults to 'colbert-ir/colbertv2.0'.\n+            colbert_config (ColBERTConfig, optional): colbert config for building and searching. Defaults to ColBERTConfig().\n+        \"\"\"\n+        self.checkpoint = checkpoint\n+        self.colbert_config = colbert_config\n+        self.colbert_config.index_name = index_name\n+        self.checkpoint = checkpoint\n+        self.colbert_config.checkpoint = checkpoint\n+        self.passages = passages\n+\n+        if not load_only:\n+            print(f\"Building the index for experiment {self.colbert_config.experiment} with index name {self.colbert_config.index_name}\")\n+            self.build_index()\n+        \n+        print(f\"Loading the index for experiment {self.colbert_config.experiment} with index name {self.colbert_config.index_name}\")\n+        self.searcher = self.get_index()\n+\n+    def build_index(self):\n+\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+\n+        from colbert import Indexer\n+        from colbert.infra import Run, RunConfig\n+        with Run().context(RunConfig(nranks=self.colbert_config.nranks, experiment=self.colbert_config.experiment)):  \n+            indexer = Indexer(checkpoint=self.checkpoint, config=self.colbert_config)\n+            indexer.index(name=self.colbert_config.index_name, collection=self.passages, overwrite=True)\n+\n+    def get_index(self):\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+\n+        from colbert import Searcher\n+        from colbert.infra import Run, RunConfig\n+        \n+        with Run().context(RunConfig(experiment=self.colbert_config.experiment)):\n+            searcher = Searcher(index=self.colbert_config.index_name, collection=self.passages)\n+        return searcher\n+    \n+    def __call__(self,query:str,k:int=7,**kwargs):\n+        import torch\n+        \n+        if kwargs.get(\"filtered_pids\"):\n+            filtered_pids = kwargs.get(\"filtered_pids\")\n+            assert type(filtered_pids) == List[int], \"The filtered pids should be a list of integers\"\n+            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+            results = self.searcher.search(\n+                query,\n+                #Number of passages to receive\n+                k=k, \n+                #Passing the filter function of relevant \n+                filter_fn=lambda pids: torch.tensor(\n+                    [pid for pid in pids if pid in filtered_pids],dtype=torch.int32).to(device))\n+        else:\n+            searcher_results = self.searcher.search(query, k=k)\n+        results = []\n+        for pid,rank,score in zip(*searcher_results):\n+            results.append(dotdict({'long_text':self.searcher.collection[pid],'score':score,'pid':pid}))\n+        return results\n+\n+class ColBERTv2RerankerLocal:\n+    \n+    def __init__(self,colbert_config=None,checkpoint:str='bert-base-uncased'):\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+        \"\"\"_summary_\n+\n+        Args:\n+            checkpoint_name (str, optional): checkpoint for embeddings. Defaults to 'bert-base-uncased'.\n+            colbert_config (ColBERTConfig, optional): Colbert config. Defaults to ColBERTConfig().\n+        \"\"\"\n+        self.colbert_config = colbert_config\n+        self.checkpoint_name = checkpoint\n+        self.colbert_config.checkpoint = checkpoint\n+\n+    # def __call__(self, *args: Any, **kwargs: Any) -> Any:\n+    #     return self.forward(*args, **kwargs)\n+\n+    def __call__(self,query:str,passages:List[str]=[]):\n+        import numpy as np\n+        from colbert.modeling.colbert import ColBERT",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1563444729",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 797,
        "pr_file": "dsp/modules/colbertv2.py",
        "discussion_id": "1563444729",
        "commented_code": "@@ -74,3 +75,118 @@ def colbertv2_post_request_v2_wrapped(*args, **kwargs):\n \n \n colbertv2_post_request = colbertv2_post_request_v2_wrapped\n+os.environ['COLBERT_LOAD_TORCH_EXTENSION_VERBOSE'] = \"True\"\n+\n+class ColBERTv2RetrieverLocal:\n+    def __init__(self,passages:List[str],colbert_config=None,load_only:bool=False,index_name:str=\"colbert_rm\",checkpoint:str='colbert-ir/colbertv2.0'):\n+        \"\"\"Colbertv2 retriever module\n+\n+        Args:\n+            passages (List[str]): list of passages\n+            load_only (bool, optional): whether to load the index or . Defaults to False.\n+            index_name (str, optional): name of the index. Defaults to \"colbert_rm\".\n+            checkpoint (str, optional): checkpoint for generating embeddings. Defaults to 'colbert-ir/colbertv2.0'.\n+            colbert_config (ColBERTConfig, optional): colbert config for building and searching. Defaults to ColBERTConfig().\n+        \"\"\"\n+        self.checkpoint = checkpoint\n+        self.colbert_config = colbert_config\n+        self.colbert_config.index_name = index_name\n+        self.checkpoint = checkpoint\n+        self.colbert_config.checkpoint = checkpoint\n+        self.passages = passages\n+\n+        if not load_only:\n+            print(f\"Building the index for experiment {self.colbert_config.experiment} with index name {self.colbert_config.index_name}\")\n+            self.build_index()\n+        \n+        print(f\"Loading the index for experiment {self.colbert_config.experiment} with index name {self.colbert_config.index_name}\")\n+        self.searcher = self.get_index()\n+\n+    def build_index(self):\n+\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+\n+        from colbert import Indexer\n+        from colbert.infra import Run, RunConfig\n+        with Run().context(RunConfig(nranks=self.colbert_config.nranks, experiment=self.colbert_config.experiment)):  \n+            indexer = Indexer(checkpoint=self.checkpoint, config=self.colbert_config)\n+            indexer.index(name=self.colbert_config.index_name, collection=self.passages, overwrite=True)\n+\n+    def get_index(self):\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+\n+        from colbert import Searcher\n+        from colbert.infra import Run, RunConfig\n+        \n+        with Run().context(RunConfig(experiment=self.colbert_config.experiment)):\n+            searcher = Searcher(index=self.colbert_config.index_name, collection=self.passages)\n+        return searcher\n+    \n+    def __call__(self,query:str,k:int=7,**kwargs):\n+        import torch\n+        \n+        if kwargs.get(\"filtered_pids\"):\n+            filtered_pids = kwargs.get(\"filtered_pids\")\n+            assert type(filtered_pids) == List[int], \"The filtered pids should be a list of integers\"\n+            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+            results = self.searcher.search(\n+                query,\n+                #Number of passages to receive\n+                k=k, \n+                #Passing the filter function of relevant \n+                filter_fn=lambda pids: torch.tensor(\n+                    [pid for pid in pids if pid in filtered_pids],dtype=torch.int32).to(device))\n+        else:\n+            searcher_results = self.searcher.search(query, k=k)\n+        results = []\n+        for pid,rank,score in zip(*searcher_results):\n+            results.append(dotdict({'long_text':self.searcher.collection[pid],'score':score,'pid':pid}))\n+        return results\n+\n+class ColBERTv2RerankerLocal:\n+    \n+    def __init__(self,colbert_config=None,checkpoint:str='bert-base-uncased'):\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+        \"\"\"_summary_\n+\n+        Args:\n+            checkpoint_name (str, optional): checkpoint for embeddings. Defaults to 'bert-base-uncased'.\n+            colbert_config (ColBERTConfig, optional): Colbert config. Defaults to ColBERTConfig().\n+        \"\"\"\n+        self.colbert_config = colbert_config\n+        self.checkpoint_name = checkpoint\n+        self.colbert_config.checkpoint = checkpoint\n+\n+    # def __call__(self, *args: Any, **kwargs: Any) -> Any:\n+    #     return self.forward(*args, **kwargs)\n+\n+    def __call__(self,query:str,passages:List[str]=[]):\n+        import numpy as np\n+        from colbert.modeling.colbert import ColBERT",
        "comment_created_at": "2024-04-13T00:51:53+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "does this require try-except block for checking colbert import here too?",
        "pr_file_module": null
      },
      {
        "comment_id": "1564226097",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 797,
        "pr_file": "dsp/modules/colbertv2.py",
        "discussion_id": "1563444729",
        "commented_code": "@@ -74,3 +75,118 @@ def colbertv2_post_request_v2_wrapped(*args, **kwargs):\n \n \n colbertv2_post_request = colbertv2_post_request_v2_wrapped\n+os.environ['COLBERT_LOAD_TORCH_EXTENSION_VERBOSE'] = \"True\"\n+\n+class ColBERTv2RetrieverLocal:\n+    def __init__(self,passages:List[str],colbert_config=None,load_only:bool=False,index_name:str=\"colbert_rm\",checkpoint:str='colbert-ir/colbertv2.0'):\n+        \"\"\"Colbertv2 retriever module\n+\n+        Args:\n+            passages (List[str]): list of passages\n+            load_only (bool, optional): whether to load the index or . Defaults to False.\n+            index_name (str, optional): name of the index. Defaults to \"colbert_rm\".\n+            checkpoint (str, optional): checkpoint for generating embeddings. Defaults to 'colbert-ir/colbertv2.0'.\n+            colbert_config (ColBERTConfig, optional): colbert config for building and searching. Defaults to ColBERTConfig().\n+        \"\"\"\n+        self.checkpoint = checkpoint\n+        self.colbert_config = colbert_config\n+        self.colbert_config.index_name = index_name\n+        self.checkpoint = checkpoint\n+        self.colbert_config.checkpoint = checkpoint\n+        self.passages = passages\n+\n+        if not load_only:\n+            print(f\"Building the index for experiment {self.colbert_config.experiment} with index name {self.colbert_config.index_name}\")\n+            self.build_index()\n+        \n+        print(f\"Loading the index for experiment {self.colbert_config.experiment} with index name {self.colbert_config.index_name}\")\n+        self.searcher = self.get_index()\n+\n+    def build_index(self):\n+\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+\n+        from colbert import Indexer\n+        from colbert.infra import Run, RunConfig\n+        with Run().context(RunConfig(nranks=self.colbert_config.nranks, experiment=self.colbert_config.experiment)):  \n+            indexer = Indexer(checkpoint=self.checkpoint, config=self.colbert_config)\n+            indexer.index(name=self.colbert_config.index_name, collection=self.passages, overwrite=True)\n+\n+    def get_index(self):\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+\n+        from colbert import Searcher\n+        from colbert.infra import Run, RunConfig\n+        \n+        with Run().context(RunConfig(experiment=self.colbert_config.experiment)):\n+            searcher = Searcher(index=self.colbert_config.index_name, collection=self.passages)\n+        return searcher\n+    \n+    def __call__(self,query:str,k:int=7,**kwargs):\n+        import torch\n+        \n+        if kwargs.get(\"filtered_pids\"):\n+            filtered_pids = kwargs.get(\"filtered_pids\")\n+            assert type(filtered_pids) == List[int], \"The filtered pids should be a list of integers\"\n+            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+            results = self.searcher.search(\n+                query,\n+                #Number of passages to receive\n+                k=k, \n+                #Passing the filter function of relevant \n+                filter_fn=lambda pids: torch.tensor(\n+                    [pid for pid in pids if pid in filtered_pids],dtype=torch.int32).to(device))\n+        else:\n+            searcher_results = self.searcher.search(query, k=k)\n+        results = []\n+        for pid,rank,score in zip(*searcher_results):\n+            results.append(dotdict({'long_text':self.searcher.collection[pid],'score':score,'pid':pid}))\n+        return results\n+\n+class ColBERTv2RerankerLocal:\n+    \n+    def __init__(self,colbert_config=None,checkpoint:str='bert-base-uncased'):\n+        try:\n+            import colbert\n+        except ImportError:\n+            print(\"Colbert not found. Please check your installation or install the module using pip install colbert-ai[faiss-gpu,torch].\")\n+        \"\"\"_summary_\n+\n+        Args:\n+            checkpoint_name (str, optional): checkpoint for embeddings. Defaults to 'bert-base-uncased'.\n+            colbert_config (ColBERTConfig, optional): Colbert config. Defaults to ColBERTConfig().\n+        \"\"\"\n+        self.colbert_config = colbert_config\n+        self.checkpoint_name = checkpoint\n+        self.colbert_config.checkpoint = checkpoint\n+\n+    # def __call__(self, *args: Any, **kwargs: Any) -> Any:\n+    #     return self.forward(*args, **kwargs)\n+\n+    def __call__(self,query:str,passages:List[str]=[]):\n+        import numpy as np\n+        from colbert.modeling.colbert import ColBERT",
        "comment_created_at": "2024-04-13T19:59:50+00:00",
        "comment_author": "Athe-kunal",
        "comment_body": "I think users may use Colbert as a retriever and as a reranker separately, hence it would be helpful that we check the import errors separately. Please let me know if I am right in my assumption.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1543452234",
    "pr_number": 717,
    "pr_file": "dspy/modeling/templates/json.py",
    "created_at": "2024-03-28T18:34:38+00:00",
    "commented_code": "+\n+import json\n+\n+from dspy import Example, Signature\n+from dspy.primitives.prompt import Prompt\n+\n+from .base import BaseTemplate\n+\n+\n+class JSONTemplate(BaseTemplate):\n+    def generate(self, signature: Signature, example: Example) -> Prompt:\n+\n+        prompt_spans = []\n+\n+        # Start by getting the instructions\n+        prompt_spans.append(signature.instructions)\n+\n+        # Generate the Guidelines\n+        prompt_spans.append(self._guidelines(signature, example))\n+\n+        # Generate spans for all the demos\n+        for demo in example.demos:\n+            prompt_spans.append(self._example_span(signature, demo))\n+\n+        # Generate span for the active example\n+        prompt_spans.append(self._example_span(signature, example))\n+\n+        content = \"\n\n--\n\n\".join(prompt_spans)\n+        return Prompt(content=content, messages=[{\"role\": \"user\", \"content\": content}])\n+\n+    def extract(self, signature: Signature, example: Example, raw_pred: str) -> Example:\n+\n+        example = example.copy()\n+\n+        try:\n+            pred = json.loads(raw_pred)\n+            for k, v in pred.items():\n+                k = k.lower()\n+                if k in signature.fields:\n+                    example[k] = v\n+        except:\n+            pass",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1543452234",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 717,
        "pr_file": "dspy/modeling/templates/json.py",
        "discussion_id": "1543452234",
        "commented_code": "@@ -0,0 +1,87 @@\n+\n+import json\n+\n+from dspy import Example, Signature\n+from dspy.primitives.prompt import Prompt\n+\n+from .base import BaseTemplate\n+\n+\n+class JSONTemplate(BaseTemplate):\n+    def generate(self, signature: Signature, example: Example) -> Prompt:\n+\n+        prompt_spans = []\n+\n+        # Start by getting the instructions\n+        prompt_spans.append(signature.instructions)\n+\n+        # Generate the Guidelines\n+        prompt_spans.append(self._guidelines(signature, example))\n+\n+        # Generate spans for all the demos\n+        for demo in example.demos:\n+            prompt_spans.append(self._example_span(signature, demo))\n+\n+        # Generate span for the active example\n+        prompt_spans.append(self._example_span(signature, example))\n+\n+        content = \"\\n\\n--\\n\\n\".join(prompt_spans)\n+        return Prompt(content=content, messages=[{\"role\": \"user\", \"content\": content}])\n+\n+    def extract(self, signature: Signature, example: Example, raw_pred: str) -> Example:\n+\n+        example = example.copy()\n+\n+        try:\n+            pred = json.loads(raw_pred)\n+            for k, v in pred.items():\n+                k = k.lower()\n+                if k in signature.fields:\n+                    example[k] = v\n+        except:\n+            pass",
        "comment_created_at": "2024-03-28T18:34:38+00:00",
        "comment_author": "thomasahle",
        "comment_body": "What error are we hiding here?\r\nJson doesn't parse?\r\nShould we at least give a warning or something?",
        "pr_file_module": null
      },
      {
        "comment_id": "1543481575",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 717,
        "pr_file": "dspy/modeling/templates/json.py",
        "discussion_id": "1543452234",
        "commented_code": "@@ -0,0 +1,87 @@\n+\n+import json\n+\n+from dspy import Example, Signature\n+from dspy.primitives.prompt import Prompt\n+\n+from .base import BaseTemplate\n+\n+\n+class JSONTemplate(BaseTemplate):\n+    def generate(self, signature: Signature, example: Example) -> Prompt:\n+\n+        prompt_spans = []\n+\n+        # Start by getting the instructions\n+        prompt_spans.append(signature.instructions)\n+\n+        # Generate the Guidelines\n+        prompt_spans.append(self._guidelines(signature, example))\n+\n+        # Generate spans for all the demos\n+        for demo in example.demos:\n+            prompt_spans.append(self._example_span(signature, demo))\n+\n+        # Generate span for the active example\n+        prompt_spans.append(self._example_span(signature, example))\n+\n+        content = \"\\n\\n--\\n\\n\".join(prompt_spans)\n+        return Prompt(content=content, messages=[{\"role\": \"user\", \"content\": content}])\n+\n+    def extract(self, signature: Signature, example: Example, raw_pred: str) -> Example:\n+\n+        example = example.copy()\n+\n+        try:\n+            pred = json.loads(raw_pred)\n+            for k, v in pred.items():\n+                k = k.lower()\n+                if k in signature.fields:\n+                    example[k] = v\n+        except:\n+            pass",
        "comment_created_at": "2024-03-28T18:47:34+00:00",
        "comment_author": "KCaverly",
        "comment_body": "There are scenarios in which json does not parse. I think you are right in that we should provide a warning. I was planning on going through once this hits main, and updating all the print/todos surrounding logging at once.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1602249913",
    "pr_number": 1007,
    "pr_file": "dsp/modules/premai.py",
    "created_at": "2024-05-15T21:03:27+00:00",
    "commented_code": "+import os\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:\n+    import premai\n+except ImportError as err:\n+    raise ImportError(",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1602249913",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1007,
        "pr_file": "dsp/modules/premai.py",
        "discussion_id": "1602249913",
        "commented_code": "@@ -0,0 +1,159 @@\n+import os\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:\n+    import premai\n+except ImportError as err:\n+    raise ImportError(",
        "comment_created_at": "2024-05-15T21:03:27+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "note that this error shouldn't be raised here. this causes the failing test. \r\n\r\nInstead, you should just define the error and should be triggered only when someone uses dspy.PremAI and doesn't have it installed.\r\n\r\nRefer to [other LM providers](https://github.com/stanfordnlp/dspy/blob/0e595a70cd9d14b0db25252320aad11eae08fd72/dsp/modules/cohere.py#L10) on how to resolve this,",
        "pr_file_module": null
      },
      {
        "comment_id": "1602773992",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1007,
        "pr_file": "dsp/modules/premai.py",
        "discussion_id": "1602249913",
        "commented_code": "@@ -0,0 +1,159 @@\n+import os\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:\n+    import premai\n+except ImportError as err:\n+    raise ImportError(",
        "comment_created_at": "2024-05-16T07:35:55+00:00",
        "comment_author": "Anindyadeep",
        "comment_body": "I see, yeah missed resolving this one, thanks for pointing out. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1603031830",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1007,
        "pr_file": "dsp/modules/premai.py",
        "discussion_id": "1602249913",
        "commented_code": "@@ -0,0 +1,159 @@\n+import os\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:\n+    import premai\n+except ImportError as err:\n+    raise ImportError(",
        "comment_created_at": "2024-05-16T09:58:26+00:00",
        "comment_author": "Anindyadeep",
        "comment_body": "Resolved, now all tests are running. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1556206365",
    "pr_number": 772,
    "pr_file": "dsp/modules/aws_models.py",
    "created_at": "2024-04-08T17:54:31+00:00",
    "commented_code": "+\"\"\"AWS models for LMs.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import logging\n+from abc import abstractmethod\n+from typing import Any\n+\n+from dsp.modules.lm import LM\n+from shared.src.models.utils.aws_providers import AWSProvider, Bedrock, Sagemaker",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1556206365",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 772,
        "pr_file": "dsp/modules/aws_models.py",
        "discussion_id": "1556206365",
        "commented_code": "@@ -0,0 +1,274 @@\n+\"\"\"AWS models for LMs.\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import logging\n+from abc import abstractmethod\n+from typing import Any\n+\n+from dsp.modules.lm import LM\n+from shared.src.models.utils.aws_providers import AWSProvider, Bedrock, Sagemaker",
        "comment_created_at": "2024-04-08T17:54:31+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "where is this import coming from? can you wrap any external imports in a try/except block as done [here](https://github.com/stanfordnlp/dspy/blob/890ff91a6d075553b08ddafa991537bae6b0526c/dsp/modules/anthropic.py#L10)? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1556218971",
    "pr_number": 772,
    "pr_file": "dsp/modules/aws_providers.py",
    "created_at": "2024-04-08T17:59:58+00:00",
    "commented_code": "+\"\"\"AWS providers for LMs.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+from typing import Any, Optional\n+\n+\n+class AWSProvider(ABC):\n+    \"\"\"This abstract class adds support for AWS model providers such as Bedrock and SageMaker.\"\"\"\n+\n+    def __init__(\n+        self,\n+        region_name: str,\n+        service_name: str,\n+        profile_name: Optional[str] = None,\n+        batch_n_enabled: bool = True\n+    ) -> None:\n+        \"\"\"_summary_.\n+\n+        Args:\n+            region_name (str, optional): The AWS region where this LM is hosted.\n+            service_name (str): Used in context of invoking the boto3 API.\n+            profile_name (str, optional): boto3 credentials profile.\n+            batch_n_enabled (bool): If False, call the LM N times rather than batching.\n+        \"\"\"\n+        import boto3  # pylint: disable=import-outside-toplevel",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1556218971",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 772,
        "pr_file": "dsp/modules/aws_providers.py",
        "discussion_id": "1556218971",
        "commented_code": "@@ -0,0 +1,111 @@\n+\"\"\"AWS providers for LMs.\"\"\"\n+\n+from abc import ABC, abstractmethod\n+from typing import Any, Optional\n+\n+\n+class AWSProvider(ABC):\n+    \"\"\"This abstract class adds support for AWS model providers such as Bedrock and SageMaker.\"\"\"\n+\n+    def __init__(\n+        self,\n+        region_name: str,\n+        service_name: str,\n+        profile_name: Optional[str] = None,\n+        batch_n_enabled: bool = True\n+    ) -> None:\n+        \"\"\"_summary_.\n+\n+        Args:\n+            region_name (str, optional): The AWS region where this LM is hosted.\n+            service_name (str): Used in context of invoking the boto3 API.\n+            profile_name (str, optional): boto3 credentials profile.\n+            batch_n_enabled (bool): If False, call the LM N times rather than batching.\n+        \"\"\"\n+        import boto3  # pylint: disable=import-outside-toplevel",
        "comment_created_at": "2024-04-08T17:59:58+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "same comment as above - wrap external imports in a try/except block as done [here](https://github.com/stanfordnlp/dspy/blob/890ff91a6d075553b08ddafa991537bae6b0526c/dsp/modules/anthropic.py#L10)? ",
        "pr_file_module": null
      }
    ]
  }
]