[
  {
    "discussion_id": "1868321335",
    "pr_number": 1874,
    "pr_file": "dspy/utils/caching.py",
    "created_at": "2024-12-03T20:24:09+00:00",
    "commented_code": "subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1868321335",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "dspy/utils/caching.py",
        "discussion_id": "1868321335",
        "commented_code": "@@ -12,3 +18,58 @@ def create_subdir_in_cachedir(subdir: str) -> str:\n     subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
        "comment_created_at": "2024-12-03T20:24:09+00:00",
        "comment_author": "dbczumar",
        "comment_body": "@CyrusNuevoDia @bahtman Can we handle pydantic models here as well, by converting them to JSON objects using `.schema()`? This will be necessary for supporting structured outputs - https://github.com/stanfordnlp/dspy/pull/1881/files#diff-ee21f597435b8e775fd162c84aaa2c7a544120407baa26c12b3ff26d0d31a7dbR43-R44",
        "pr_file_module": null
      },
      {
        "comment_id": "1868361154",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "dspy/utils/caching.py",
        "discussion_id": "1868321335",
        "commented_code": "@@ -12,3 +18,58 @@ def create_subdir_in_cachedir(subdir: str) -> str:\n     subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
        "comment_created_at": "2024-12-03T21:00:07+00:00",
        "comment_author": "bahtman",
        "comment_body": "Would the pydantic model be a part of the dictionary request object or the entirety of it? Do we want to cache it or just avoid it like the callable parameters? (Above implementation ignores pydantic models because of the callable check)",
        "pr_file_module": null
      },
      {
        "comment_id": "1868370833",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "dspy/utils/caching.py",
        "discussion_id": "1868321335",
        "commented_code": "@@ -12,3 +18,58 @@ def create_subdir_in_cachedir(subdir: str) -> str:\n     subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
        "comment_created_at": "2024-12-03T21:09:44+00:00",
        "comment_author": "dbczumar",
        "comment_body": "Great question! I think we should cache it, I.e. it should be included in `params`. It will be part of the dictionary request object, e.g. an entry like `params[\"response_format\"] = <Pydantic object>`",
        "pr_file_module": null
      },
      {
        "comment_id": "1868377891",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "dspy/utils/caching.py",
        "discussion_id": "1868321335",
        "commented_code": "@@ -12,3 +18,58 @@ def create_subdir_in_cachedir(subdir: str) -> str:\n     subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
        "comment_created_at": "2024-12-03T21:16:40+00:00",
        "comment_author": "bahtman",
        "comment_body": "Can we assume its always \"response_format\" and we just check for that and convert it to dict?",
        "pr_file_module": null
      },
      {
        "comment_id": "1868388637",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "dspy/utils/caching.py",
        "discussion_id": "1868321335",
        "commented_code": "@@ -12,3 +18,58 @@ def create_subdir_in_cachedir(subdir: str) -> str:\n     subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
        "comment_created_at": "2024-12-03T21:26:51+00:00",
        "comment_author": "dbczumar",
        "comment_body": "I think it'd be a bit more future proof (and also totally sufficient) to check for `isinstance(v, pydantic.BaseModel)` and, if the `isinstance ` check passes, set the value to `v.schema()` in the dictionary",
        "pr_file_module": null
      },
      {
        "comment_id": "1868420040",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "dspy/utils/caching.py",
        "discussion_id": "1868321335",
        "commented_code": "@@ -12,3 +18,58 @@ def create_subdir_in_cachedir(subdir: str) -> str:\n     subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
        "comment_created_at": "2024-12-03T21:58:35+00:00",
        "comment_author": "bahtman",
        "comment_body": "To avoid another passthrough I would prefer a staticmethod `convert_pydantic` that check for `isinstance(v, pydantic.BaseModel)` and if true returns `v.schema()` else `v`. Then we can just call that while we do the dict comprehension.",
        "pr_file_module": null
      },
      {
        "comment_id": "1868420965",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "dspy/utils/caching.py",
        "discussion_id": "1868321335",
        "commented_code": "@@ -12,3 +18,58 @@ def create_subdir_in_cachedir(subdir: str) -> str:\n     subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
        "comment_created_at": "2024-12-03T21:59:32+00:00",
        "comment_author": "bahtman",
        "comment_body": "`params = {k: self.convert_pydantic(v) for k, v in request.items() if not callable(v)}`",
        "pr_file_module": null
      },
      {
        "comment_id": "1868422052",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1874,
        "pr_file": "dspy/utils/caching.py",
        "discussion_id": "1868321335",
        "commented_code": "@@ -12,3 +18,58 @@ def create_subdir_in_cachedir(subdir: str) -> str:\n     subdir = os.path.abspath(subdir)\n     os.makedirs(subdir, exist_ok=True)\n     return subdir\n+\n+\n+class LRUCache(OrderedDict):\n+    maxsize: int\n+\n+    def __init__(self, iterable, maxsize: int):\n+        super().__init__(iterable)\n+        self.maxsize = maxsize\n+\n+    @classmethod\n+    def load(cls, file, maxsize: int) -> \"LRUCache\":\n+        return cls(pickle.load(file), maxsize)\n+\n+    @staticmethod\n+    def dump(obj, file) -> None:\n+        pickle.dump([[k, v] for k, v in obj.items()], file)\n+\n+    def __setitem__(self, request: dict, value):\n+        key = self.cache_key(request)\n+\n+        if key in self:\n+            self.move_to_end(key)\n+            return\n+\n+        if len(self) == self.maxsize:\n+            self.popitem(last=False)\n+\n+        super().__setitem__(key, value)\n+\n+    def __getitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__getitem__(key)\n+\n+    def __contains__(self, request: dict):\n+        key = self.cache_key(request)\n+        return super().__contains__(key)\n+\n+    def get(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().get(key, default)\n+\n+    def __delitem__(self, request: dict):\n+        key = self.cache_key(request)\n+        super().__delitem__(key)\n+\n+    def pop(self, request: dict, default=None):\n+        key = self.cache_key(request)\n+        return super().pop(key, default)\n+\n+    @staticmethod\n+    def cache_key(request: Union[dict, str]) -> str:\n+        params = request\n+        if isinstance(request, dict):\n+            params = {k: v for k, v in request.items() if not callable(v)}\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()",
        "comment_created_at": "2024-12-03T22:00:39+00:00",
        "comment_author": "bahtman",
        "comment_body": "Ahh. This might give us issues, since v is callable when a pydantic model. Either we build an or statement or we just do 2 passthroughs (Later is probably more clean at this point)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1872893797",
    "pr_number": 1896,
    "pr_file": "dspy/clients/lm.py",
    "created_at": "2024-12-06T09:08:44+00:00",
    "commented_code": "return new_instance\n \n \n-@functools.lru_cache(maxsize=None)\n-def cached_litellm_completion(request, num_retries: int):\n+def request_cache(maxsize: Optional[int] = None):\n+    \"\"\"\n+    A threadsafe decorator to create an in-memory LRU cache for LM inference functions that accept\n+    a dictionary-like LM request. An in-memory cache for LM calls is critical for ensuring\n+    good performance when optimizing and evaluating DSPy LMs (disk caching alone is too slow).\n+\n+    Args:\n+        maxsize: The maximum size of the cache.\n+\n+    Returns:\n+        A decorator that wraps the target function with caching.\n+    \"\"\"\n+\n+    def cache_key(request: Dict[str, Any]) -> str:\n+        # Transform Pydantic models into JSON-convertible format and exclude unhashable objects\n+        params = {\n+            k: (v.dict() if isinstance(v, pydantic.BaseModel) else v) for k, v in request.items() if not callable(v)\n+        }\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()\n+\n+    def decorator(func):\n+        @cached(\n+            # NB: cachetools doesn't support maxsize=None; it recommends using float(\"inf\") instead\n+            cache=LRUCache(maxsize=maxsize or float(\"inf\")),\n+            key=lambda request, *args, **kwargs: cache_key(request),",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1872893797",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1896,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1872893797",
        "commented_code": "@@ -212,47 +219,82 @@ def copy(self, **kwargs):\n         return new_instance\n \n \n-@functools.lru_cache(maxsize=None)\n-def cached_litellm_completion(request, num_retries: int):\n+def request_cache(maxsize: Optional[int] = None):\n+    \"\"\"\n+    A threadsafe decorator to create an in-memory LRU cache for LM inference functions that accept\n+    a dictionary-like LM request. An in-memory cache for LM calls is critical for ensuring\n+    good performance when optimizing and evaluating DSPy LMs (disk caching alone is too slow).\n+\n+    Args:\n+        maxsize: The maximum size of the cache.\n+\n+    Returns:\n+        A decorator that wraps the target function with caching.\n+    \"\"\"\n+\n+    def cache_key(request: Dict[str, Any]) -> str:\n+        # Transform Pydantic models into JSON-convertible format and exclude unhashable objects\n+        params = {\n+            k: (v.dict() if isinstance(v, pydantic.BaseModel) else v) for k, v in request.items() if not callable(v)\n+        }\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()\n+\n+    def decorator(func):\n+        @cached(\n+            # NB: cachetools doesn't support maxsize=None; it recommends using float(\"inf\") instead\n+            cache=LRUCache(maxsize=maxsize or float(\"inf\")),\n+            key=lambda request, *args, **kwargs: cache_key(request),",
        "comment_created_at": "2024-12-06T09:08:44+00:00",
        "comment_author": "dbczumar",
        "comment_body": "This is the key advantage of cachetools. Unlike `lru_cache`, it allows us to define a cache key by applying a custom function to one or more arguments, rather than forcing all arguments to be hashed / JSON-encoded, passed to the function, and then decoded afterwards. Encoding / decoding is infeasible for callables.",
        "pr_file_module": null
      },
      {
        "comment_id": "1874114525",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1896,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1872893797",
        "commented_code": "@@ -212,47 +219,82 @@ def copy(self, **kwargs):\n         return new_instance\n \n \n-@functools.lru_cache(maxsize=None)\n-def cached_litellm_completion(request, num_retries: int):\n+def request_cache(maxsize: Optional[int] = None):\n+    \"\"\"\n+    A threadsafe decorator to create an in-memory LRU cache for LM inference functions that accept\n+    a dictionary-like LM request. An in-memory cache for LM calls is critical for ensuring\n+    good performance when optimizing and evaluating DSPy LMs (disk caching alone is too slow).\n+\n+    Args:\n+        maxsize: The maximum size of the cache.\n+\n+    Returns:\n+        A decorator that wraps the target function with caching.\n+    \"\"\"\n+\n+    def cache_key(request: Dict[str, Any]) -> str:\n+        # Transform Pydantic models into JSON-convertible format and exclude unhashable objects\n+        params = {\n+            k: (v.dict() if isinstance(v, pydantic.BaseModel) else v) for k, v in request.items() if not callable(v)\n+        }\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()\n+\n+    def decorator(func):\n+        @cached(\n+            # NB: cachetools doesn't support maxsize=None; it recommends using float(\"inf\") instead\n+            cache=LRUCache(maxsize=maxsize or float(\"inf\")),\n+            key=lambda request, *args, **kwargs: cache_key(request),",
        "comment_created_at": "2024-12-06T23:21:31+00:00",
        "comment_author": "CyrusNuevoDia",
        "comment_body": "Can you do a global `dspy.settings.request_cache default = LRUCache(maxsize=10_000_000)` and then have this function pull from that?",
        "pr_file_module": null
      },
      {
        "comment_id": "1874148912",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1896,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1872893797",
        "commented_code": "@@ -212,47 +219,82 @@ def copy(self, **kwargs):\n         return new_instance\n \n \n-@functools.lru_cache(maxsize=None)\n-def cached_litellm_completion(request, num_retries: int):\n+def request_cache(maxsize: Optional[int] = None):\n+    \"\"\"\n+    A threadsafe decorator to create an in-memory LRU cache for LM inference functions that accept\n+    a dictionary-like LM request. An in-memory cache for LM calls is critical for ensuring\n+    good performance when optimizing and evaluating DSPy LMs (disk caching alone is too slow).\n+\n+    Args:\n+        maxsize: The maximum size of the cache.\n+\n+    Returns:\n+        A decorator that wraps the target function with caching.\n+    \"\"\"\n+\n+    def cache_key(request: Dict[str, Any]) -> str:\n+        # Transform Pydantic models into JSON-convertible format and exclude unhashable objects\n+        params = {\n+            k: (v.dict() if isinstance(v, pydantic.BaseModel) else v) for k, v in request.items() if not callable(v)\n+        }\n+        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()\n+\n+    def decorator(func):\n+        @cached(\n+            # NB: cachetools doesn't support maxsize=None; it recommends using float(\"inf\") instead\n+            cache=LRUCache(maxsize=maxsize or float(\"inf\")),\n+            key=lambda request, *args, **kwargs: cache_key(request),",
        "comment_created_at": "2024-12-06T23:50:30+00:00",
        "comment_author": "dbczumar",
        "comment_body": "With this naming, it could be confused with the disk cache though, right? It seems like we'd want some unified way to refer to both caches, or more distinctive naming. Thoughts?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1872897223",
    "pr_number": 1896,
    "pr_file": "tests/caching/test_caching.py",
    "created_at": "2024-12-06T09:10:46+00:00",
    "commented_code": "request_logs = read_litellm_test_server_request_logs(server_log_file_path)\n     assert len(request_logs) == 0\n+\n+\n+def test_lm_calls_are_cached_in_memory_when_expected(litellm_test_server, temporary_blank_cache_dir):\n+    api_base, server_log_file_path = litellm_test_server\n+\n+    lm1 = dspy.LM(\n+        model=\"openai/dspy-test-model\",\n+        api_base=api_base,\n+        api_key=\"fakekey\",\n+    )\n+    lm1(\"Example query\")\n+    # Remove the disk cache, after which the LM must rely on in-memory caching\n+    shutil.rmtree(temporary_blank_cache_dir)\n+    lm1(\"Example query2\")\n+    lm1(\"Example query2\")\n+    lm1(\"Example query2\")\n+    lm1(\"Example query2\")\n+\n+    request_logs = read_litellm_test_server_request_logs(server_log_file_path)\n+    assert len(request_logs) == 2\n+\n+\n+def test_lm_calls_support_unhashable_types(litellm_test_server, temporary_blank_cache_dir):\n+    api_base, server_log_file_path = litellm_test_server\n+\n+    lm_with_unhashable_callable = dspy.LM(\n+        model=\"openai/dspy-test-model\",\n+        api_base=api_base,\n+        api_key=\"fakekey\",\n+        # Define a callable kwarg for the LM to use during inference\n+        azure_ad_token_provider=lambda *args, **kwargs: None,\n+    )\n+    lm_with_unhashable_callable(\"Query\")\n+\n+\n+def test_lm_calls_support_pydantic_models(litellm_test_server, temporary_blank_cache_dir):\n+    api_base, server_log_file_path = litellm_test_server\n+\n+    class ResponseFormat(pydantic.BaseModel):\n+        response: str\n+\n+    lm = dspy.LM(\n+        model=\"openai/dspy-test-model\",\n+        api_base=api_base,\n+        api_key=\"fakekey\",\n+        response_format=ResponseFormat,\n+    )\n+    lm(\"Query\")",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1872897223",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1896,
        "pr_file": "tests/caching/test_caching.py",
        "discussion_id": "1872897223",
        "commented_code": "@@ -88,3 +89,51 @@ def test_lm_calls_are_cached_across_interpreter_sessions(litellm_test_server, te\n \n     request_logs = read_litellm_test_server_request_logs(server_log_file_path)\n     assert len(request_logs) == 0\n+\n+\n+def test_lm_calls_are_cached_in_memory_when_expected(litellm_test_server, temporary_blank_cache_dir):\n+    api_base, server_log_file_path = litellm_test_server\n+\n+    lm1 = dspy.LM(\n+        model=\"openai/dspy-test-model\",\n+        api_base=api_base,\n+        api_key=\"fakekey\",\n+    )\n+    lm1(\"Example query\")\n+    # Remove the disk cache, after which the LM must rely on in-memory caching\n+    shutil.rmtree(temporary_blank_cache_dir)\n+    lm1(\"Example query2\")\n+    lm1(\"Example query2\")\n+    lm1(\"Example query2\")\n+    lm1(\"Example query2\")\n+\n+    request_logs = read_litellm_test_server_request_logs(server_log_file_path)\n+    assert len(request_logs) == 2\n+\n+\n+def test_lm_calls_support_unhashable_types(litellm_test_server, temporary_blank_cache_dir):\n+    api_base, server_log_file_path = litellm_test_server\n+\n+    lm_with_unhashable_callable = dspy.LM(\n+        model=\"openai/dspy-test-model\",\n+        api_base=api_base,\n+        api_key=\"fakekey\",\n+        # Define a callable kwarg for the LM to use during inference\n+        azure_ad_token_provider=lambda *args, **kwargs: None,\n+    )\n+    lm_with_unhashable_callable(\"Query\")\n+\n+\n+def test_lm_calls_support_pydantic_models(litellm_test_server, temporary_blank_cache_dir):\n+    api_base, server_log_file_path = litellm_test_server\n+\n+    class ResponseFormat(pydantic.BaseModel):\n+        response: str\n+\n+    lm = dspy.LM(\n+        model=\"openai/dspy-test-model\",\n+        api_base=api_base,\n+        api_key=\"fakekey\",\n+        response_format=ResponseFormat,\n+    )\n+    lm(\"Query\")",
        "comment_created_at": "2024-12-06T09:10:46+00:00",
        "comment_author": "dbczumar",
        "comment_body": "Fails on `main` with:\r\n\r\n```\r\nTypeError: <class 'tests.caching.test_caching.test_lm_calls_support_pydantic_models.<locals>.ResponseFormat'> is not JSON serializable\r\n```",
        "pr_file_module": null
      }
    ]
  }
]