[
  {
    "discussion_id": "1569751758",
    "pr_number": 424,
    "pr_file": "dspy/modeling/backends/text.py",
    "created_at": "2024-04-18T01:06:15+00:00",
    "commented_code": "+import logging\n+import typing as t\n+\n+import regex\n+from pydantic import Field\n+\n+from dspy.modeling.backends.base import BaseBackend\n+from dspy.primitives import Completions, Example\n+from dspy.signatures.signature import Signature, SignatureMeta\n+\n+logger = logging.getLogger(__name__)\n+\n+litellm_logger = logging.getLogger(\"LiteLLM\")\n+litellm_logger.setLevel(logging.WARNING)\n+\n+try:\n+    from litellm import completion\n+\n+    _missing_litellm = False\n+except ImportError:\n+    _missing_litellm = True\n+\n+\n+def passages_to_text(passages: t.Iterable[str]) -> str:\n+    passages = list(passages)\n+    if len(passages) == 0:\n+        raise ValueError(\"Empty passages passed to format handler.\")\n+\n+    if len(passages) == 1:\n+        return passages[0]\n+\n+    return \"\n\".join(\n+        [f\"[{idx + 1}] <<{text}>>\" for idx, text in enumerate(passages)],\n+    )\n+\n+\n+def format_answers(answers: t.Iterable[str]) -> str:\n+    answers = list(answers)\n+    if len(answers) == 0:\n+        raise ValueError(\"Empty answers passed to format handler.\")\n+\n+    return answers[0].strip()\n+\n+\n+def default_format_handler(x: str) -> str:\n+    if not isinstance(x, str):\n+        raise ValueError(f\"Wrong type passed to format handlers: {type(x)} should be a str\")\n+\n+    return \" \".join(x.split())\n+\n+\n+DEFAULT_FORMAT_HANDLERS = {\n+    \"context\": passages_to_text,\n+    \"passages\": passages_to_text,\n+    \"answers\": format_answers,\n+}\n+\n+\n+class TextBackend(BaseBackend):\n+    \"\"\"TextBackend takes a signature, its params, and predicts structured outputs leveraging LiteLLM.\"\"\"\n+\n+    STANDARD_PARAMS: dict[str, t.Any] = {\n+        \"temperature\": 0.5,\n+        \"max_tokens\": 500,\n+        \"top_p\": 1,\n+        \"frequency_penalty\": 0,\n+        \"presence_penalty\": 0,\n+        \"num_retries\": 3,\n+    }\n+\n+    model: str\n+    default_params: dict[str, t.Any] = Field(default_factory=dict)\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        if _missing_litellm:\n+            raise ImportError(",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1569751758",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 424,
        "pr_file": "dspy/modeling/backends/text.py",
        "discussion_id": "1569751758",
        "commented_code": "@@ -0,0 +1,226 @@\n+import logging\n+import typing as t\n+\n+import regex\n+from pydantic import Field\n+\n+from dspy.modeling.backends.base import BaseBackend\n+from dspy.primitives import Completions, Example\n+from dspy.signatures.signature import Signature, SignatureMeta\n+\n+logger = logging.getLogger(__name__)\n+\n+litellm_logger = logging.getLogger(\"LiteLLM\")\n+litellm_logger.setLevel(logging.WARNING)\n+\n+try:\n+    from litellm import completion\n+\n+    _missing_litellm = False\n+except ImportError:\n+    _missing_litellm = True\n+\n+\n+def passages_to_text(passages: t.Iterable[str]) -> str:\n+    passages = list(passages)\n+    if len(passages) == 0:\n+        raise ValueError(\"Empty passages passed to format handler.\")\n+\n+    if len(passages) == 1:\n+        return passages[0]\n+\n+    return \"\\n\".join(\n+        [f\"[{idx + 1}] <<{text}>>\" for idx, text in enumerate(passages)],\n+    )\n+\n+\n+def format_answers(answers: t.Iterable[str]) -> str:\n+    answers = list(answers)\n+    if len(answers) == 0:\n+        raise ValueError(\"Empty answers passed to format handler.\")\n+\n+    return answers[0].strip()\n+\n+\n+def default_format_handler(x: str) -> str:\n+    if not isinstance(x, str):\n+        raise ValueError(f\"Wrong type passed to format handlers: {type(x)} should be a str\")\n+\n+    return \" \".join(x.split())\n+\n+\n+DEFAULT_FORMAT_HANDLERS = {\n+    \"context\": passages_to_text,\n+    \"passages\": passages_to_text,\n+    \"answers\": format_answers,\n+}\n+\n+\n+class TextBackend(BaseBackend):\n+    \"\"\"TextBackend takes a signature, its params, and predicts structured outputs leveraging LiteLLM.\"\"\"\n+\n+    STANDARD_PARAMS: dict[str, t.Any] = {\n+        \"temperature\": 0.5,\n+        \"max_tokens\": 500,\n+        \"top_p\": 1,\n+        \"frequency_penalty\": 0,\n+        \"presence_penalty\": 0,\n+        \"num_retries\": 3,\n+    }\n+\n+    model: str\n+    default_params: dict[str, t.Any] = Field(default_factory=dict)\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        if _missing_litellm:\n+            raise ImportError(",
        "comment_created_at": "2024-04-18T01:06:15+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "i think we can add this to the try/except block like down for other LMs?",
        "pr_file_module": null
      },
      {
        "comment_id": "1569765666",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 424,
        "pr_file": "dspy/modeling/backends/text.py",
        "discussion_id": "1569751758",
        "commented_code": "@@ -0,0 +1,226 @@\n+import logging\n+import typing as t\n+\n+import regex\n+from pydantic import Field\n+\n+from dspy.modeling.backends.base import BaseBackend\n+from dspy.primitives import Completions, Example\n+from dspy.signatures.signature import Signature, SignatureMeta\n+\n+logger = logging.getLogger(__name__)\n+\n+litellm_logger = logging.getLogger(\"LiteLLM\")\n+litellm_logger.setLevel(logging.WARNING)\n+\n+try:\n+    from litellm import completion\n+\n+    _missing_litellm = False\n+except ImportError:\n+    _missing_litellm = True\n+\n+\n+def passages_to_text(passages: t.Iterable[str]) -> str:\n+    passages = list(passages)\n+    if len(passages) == 0:\n+        raise ValueError(\"Empty passages passed to format handler.\")\n+\n+    if len(passages) == 1:\n+        return passages[0]\n+\n+    return \"\\n\".join(\n+        [f\"[{idx + 1}] <<{text}>>\" for idx, text in enumerate(passages)],\n+    )\n+\n+\n+def format_answers(answers: t.Iterable[str]) -> str:\n+    answers = list(answers)\n+    if len(answers) == 0:\n+        raise ValueError(\"Empty answers passed to format handler.\")\n+\n+    return answers[0].strip()\n+\n+\n+def default_format_handler(x: str) -> str:\n+    if not isinstance(x, str):\n+        raise ValueError(f\"Wrong type passed to format handlers: {type(x)} should be a str\")\n+\n+    return \" \".join(x.split())\n+\n+\n+DEFAULT_FORMAT_HANDLERS = {\n+    \"context\": passages_to_text,\n+    \"passages\": passages_to_text,\n+    \"answers\": format_answers,\n+}\n+\n+\n+class TextBackend(BaseBackend):\n+    \"\"\"TextBackend takes a signature, its params, and predicts structured outputs leveraging LiteLLM.\"\"\"\n+\n+    STANDARD_PARAMS: dict[str, t.Any] = {\n+        \"temperature\": 0.5,\n+        \"max_tokens\": 500,\n+        \"top_p\": 1,\n+        \"frequency_penalty\": 0,\n+        \"presence_penalty\": 0,\n+        \"num_retries\": 3,\n+    }\n+\n+    model: str\n+    default_params: dict[str, t.Any] = Field(default_factory=dict)\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        if _missing_litellm:\n+            raise ImportError(",
        "comment_created_at": "2024-04-18T01:23:28+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "and mention `pip install dspy-ai[litellm]` since it's an extra",
        "pr_file_module": null
      },
      {
        "comment_id": "1571139068",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 424,
        "pr_file": "dspy/modeling/backends/text.py",
        "discussion_id": "1569751758",
        "commented_code": "@@ -0,0 +1,226 @@\n+import logging\n+import typing as t\n+\n+import regex\n+from pydantic import Field\n+\n+from dspy.modeling.backends.base import BaseBackend\n+from dspy.primitives import Completions, Example\n+from dspy.signatures.signature import Signature, SignatureMeta\n+\n+logger = logging.getLogger(__name__)\n+\n+litellm_logger = logging.getLogger(\"LiteLLM\")\n+litellm_logger.setLevel(logging.WARNING)\n+\n+try:\n+    from litellm import completion\n+\n+    _missing_litellm = False\n+except ImportError:\n+    _missing_litellm = True\n+\n+\n+def passages_to_text(passages: t.Iterable[str]) -> str:\n+    passages = list(passages)\n+    if len(passages) == 0:\n+        raise ValueError(\"Empty passages passed to format handler.\")\n+\n+    if len(passages) == 1:\n+        return passages[0]\n+\n+    return \"\\n\".join(\n+        [f\"[{idx + 1}] <<{text}>>\" for idx, text in enumerate(passages)],\n+    )\n+\n+\n+def format_answers(answers: t.Iterable[str]) -> str:\n+    answers = list(answers)\n+    if len(answers) == 0:\n+        raise ValueError(\"Empty answers passed to format handler.\")\n+\n+    return answers[0].strip()\n+\n+\n+def default_format_handler(x: str) -> str:\n+    if not isinstance(x, str):\n+        raise ValueError(f\"Wrong type passed to format handlers: {type(x)} should be a str\")\n+\n+    return \" \".join(x.split())\n+\n+\n+DEFAULT_FORMAT_HANDLERS = {\n+    \"context\": passages_to_text,\n+    \"passages\": passages_to_text,\n+    \"answers\": format_answers,\n+}\n+\n+\n+class TextBackend(BaseBackend):\n+    \"\"\"TextBackend takes a signature, its params, and predicts structured outputs leveraging LiteLLM.\"\"\"\n+\n+    STANDARD_PARAMS: dict[str, t.Any] = {\n+        \"temperature\": 0.5,\n+        \"max_tokens\": 500,\n+        \"top_p\": 1,\n+        \"frequency_penalty\": 0,\n+        \"presence_penalty\": 0,\n+        \"num_retries\": 3,\n+    }\n+\n+    model: str\n+    default_params: dict[str, t.Any] = Field(default_factory=dict)\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        if _missing_litellm:\n+            raise ImportError(",
        "comment_created_at": "2024-04-18T17:35:44+00:00",
        "comment_author": "CyrusNuevoDia",
        "comment_body": "@arnavsinghvi11 @KCaverly we should make litellm a required dep probably",
        "pr_file_module": null
      },
      {
        "comment_id": "1582370395",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 424,
        "pr_file": "dspy/modeling/backends/text.py",
        "discussion_id": "1569751758",
        "commented_code": "@@ -0,0 +1,226 @@\n+import logging\n+import typing as t\n+\n+import regex\n+from pydantic import Field\n+\n+from dspy.modeling.backends.base import BaseBackend\n+from dspy.primitives import Completions, Example\n+from dspy.signatures.signature import Signature, SignatureMeta\n+\n+logger = logging.getLogger(__name__)\n+\n+litellm_logger = logging.getLogger(\"LiteLLM\")\n+litellm_logger.setLevel(logging.WARNING)\n+\n+try:\n+    from litellm import completion\n+\n+    _missing_litellm = False\n+except ImportError:\n+    _missing_litellm = True\n+\n+\n+def passages_to_text(passages: t.Iterable[str]) -> str:\n+    passages = list(passages)\n+    if len(passages) == 0:\n+        raise ValueError(\"Empty passages passed to format handler.\")\n+\n+    if len(passages) == 1:\n+        return passages[0]\n+\n+    return \"\\n\".join(\n+        [f\"[{idx + 1}] <<{text}>>\" for idx, text in enumerate(passages)],\n+    )\n+\n+\n+def format_answers(answers: t.Iterable[str]) -> str:\n+    answers = list(answers)\n+    if len(answers) == 0:\n+        raise ValueError(\"Empty answers passed to format handler.\")\n+\n+    return answers[0].strip()\n+\n+\n+def default_format_handler(x: str) -> str:\n+    if not isinstance(x, str):\n+        raise ValueError(f\"Wrong type passed to format handlers: {type(x)} should be a str\")\n+\n+    return \" \".join(x.split())\n+\n+\n+DEFAULT_FORMAT_HANDLERS = {\n+    \"context\": passages_to_text,\n+    \"passages\": passages_to_text,\n+    \"answers\": format_answers,\n+}\n+\n+\n+class TextBackend(BaseBackend):\n+    \"\"\"TextBackend takes a signature, its params, and predicts structured outputs leveraging LiteLLM.\"\"\"\n+\n+    STANDARD_PARAMS: dict[str, t.Any] = {\n+        \"temperature\": 0.5,\n+        \"max_tokens\": 500,\n+        \"top_p\": 1,\n+        \"frequency_penalty\": 0,\n+        \"presence_penalty\": 0,\n+        \"num_retries\": 3,\n+    }\n+\n+    model: str\n+    default_params: dict[str, t.Any] = Field(default_factory=dict)\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        if _missing_litellm:\n+            raise ImportError(",
        "comment_created_at": "2024-04-28T20:29:36+00:00",
        "comment_author": "okhat",
        "comment_body": "what does litellm involve/add? how heavy?",
        "pr_file_module": null
      },
      {
        "comment_id": "1583491422",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 424,
        "pr_file": "dspy/modeling/backends/text.py",
        "discussion_id": "1569751758",
        "commented_code": "@@ -0,0 +1,226 @@\n+import logging\n+import typing as t\n+\n+import regex\n+from pydantic import Field\n+\n+from dspy.modeling.backends.base import BaseBackend\n+from dspy.primitives import Completions, Example\n+from dspy.signatures.signature import Signature, SignatureMeta\n+\n+logger = logging.getLogger(__name__)\n+\n+litellm_logger = logging.getLogger(\"LiteLLM\")\n+litellm_logger.setLevel(logging.WARNING)\n+\n+try:\n+    from litellm import completion\n+\n+    _missing_litellm = False\n+except ImportError:\n+    _missing_litellm = True\n+\n+\n+def passages_to_text(passages: t.Iterable[str]) -> str:\n+    passages = list(passages)\n+    if len(passages) == 0:\n+        raise ValueError(\"Empty passages passed to format handler.\")\n+\n+    if len(passages) == 1:\n+        return passages[0]\n+\n+    return \"\\n\".join(\n+        [f\"[{idx + 1}] <<{text}>>\" for idx, text in enumerate(passages)],\n+    )\n+\n+\n+def format_answers(answers: t.Iterable[str]) -> str:\n+    answers = list(answers)\n+    if len(answers) == 0:\n+        raise ValueError(\"Empty answers passed to format handler.\")\n+\n+    return answers[0].strip()\n+\n+\n+def default_format_handler(x: str) -> str:\n+    if not isinstance(x, str):\n+        raise ValueError(f\"Wrong type passed to format handlers: {type(x)} should be a str\")\n+\n+    return \" \".join(x.split())\n+\n+\n+DEFAULT_FORMAT_HANDLERS = {\n+    \"context\": passages_to_text,\n+    \"passages\": passages_to_text,\n+    \"answers\": format_answers,\n+}\n+\n+\n+class TextBackend(BaseBackend):\n+    \"\"\"TextBackend takes a signature, its params, and predicts structured outputs leveraging LiteLLM.\"\"\"\n+\n+    STANDARD_PARAMS: dict[str, t.Any] = {\n+        \"temperature\": 0.5,\n+        \"max_tokens\": 500,\n+        \"top_p\": 1,\n+        \"frequency_penalty\": 0,\n+        \"presence_penalty\": 0,\n+        \"num_retries\": 3,\n+    }\n+\n+    model: str\n+    default_params: dict[str, t.Any] = Field(default_factory=dict)\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        if _missing_litellm:\n+            raise ImportError(",
        "comment_created_at": "2024-04-29T17:56:30+00:00",
        "comment_author": "CyrusNuevoDia",
        "comment_body": "@okhat it's the backend that lets us have access to all the models from OpenAI, Anthropic, Huggingface, Cohere, etc. without having to build individual adapters for each one",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1650077388",
    "pr_number": 1186,
    "pr_file": "dsp/modules/ollama.py",
    "created_at": "2024-06-23T13:33:45+00:00",
    "commented_code": "import datetime\n import hashlib\n+import uuid\n from typing import Any, Literal\n \n import requests\n \n from dsp.modules.lm import LM\n \n+try:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1650077388",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1186,
        "pr_file": "dsp/modules/ollama.py",
        "discussion_id": "1650077388",
        "commented_code": "@@ -1,11 +1,19 @@\n import datetime\n import hashlib\n+import uuid\n from typing import Any, Literal\n \n import requests\n \n from dsp.modules.lm import LM\n \n+try:",
        "comment_created_at": "2024-06-23T13:33:45+00:00",
        "comment_author": "denisergashbaev",
        "comment_body": "i believe we should not have a tight coupling to langfuse in DSPy. It should be some sort of a hadler that is passed to DSPy: similar to how langfuse integrates with LangChain",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1571514717",
    "pr_number": 831,
    "pr_file": "tests/modules/vectorizer/integration_test_fastembed.py",
    "created_at": "2024-04-18T23:39:37+00:00",
    "commented_code": "+from dsp.modules.sentence_vectorizer import FastEmbedVectorizer\n+import pytest\n+\n+from dspy.primitives.example import Example\n+\n+# Skip the test if the 'fastembed' package is not installed\n+pytest.importorskip(\"fastembed\", reason=\"'fastembed' is not installed. Use `pip install fastembed` to install it.\")\n+\n+\n+@pytest.mark.parametrize(\n+    \"n_dims,model_name\", [(384, \"BAAI/bge-small-en-v1.5\"), (512, \"jinaai/jina-embeddings-v2-small-en\")]\n+)\n+def test_fastembed_with_examples(n_dims, model_name):\n+    vectorizer = FastEmbedVectorizer(model_name)",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1571514717",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 831,
        "pr_file": "tests/modules/vectorizer/integration_test_fastembed.py",
        "discussion_id": "1571514717",
        "commented_code": "@@ -0,0 +1,43 @@\n+from dsp.modules.sentence_vectorizer import FastEmbedVectorizer\n+import pytest\n+\n+from dspy.primitives.example import Example\n+\n+# Skip the test if the 'fastembed' package is not installed\n+pytest.importorskip(\"fastembed\", reason=\"'fastembed' is not installed. Use `pip install fastembed` to install it.\")\n+\n+\n+@pytest.mark.parametrize(\n+    \"n_dims,model_name\", [(384, \"BAAI/bge-small-en-v1.5\"), (512, \"jinaai/jina-embeddings-v2-small-en\")]\n+)\n+def test_fastembed_with_examples(n_dims, model_name):\n+    vectorizer = FastEmbedVectorizer(model_name)",
        "comment_created_at": "2024-04-18T23:39:37+00:00",
        "comment_author": "ammirsm",
        "comment_body": "what do you think about mocking the `embed` function and keep it in the unit test?\r\n\r\nthe other question is does it require downloading of the models? if yeah I would say that would be good to be able to have it mocked and run it in the test in the CI without downloading.",
        "pr_file_module": null
      },
      {
        "comment_id": "1571907924",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 831,
        "pr_file": "tests/modules/vectorizer/integration_test_fastembed.py",
        "discussion_id": "1571514717",
        "commented_code": "@@ -0,0 +1,43 @@\n+from dsp.modules.sentence_vectorizer import FastEmbedVectorizer\n+import pytest\n+\n+from dspy.primitives.example import Example\n+\n+# Skip the test if the 'fastembed' package is not installed\n+pytest.importorskip(\"fastembed\", reason=\"'fastembed' is not installed. Use `pip install fastembed` to install it.\")\n+\n+\n+@pytest.mark.parametrize(\n+    \"n_dims,model_name\", [(384, \"BAAI/bge-small-en-v1.5\"), (512, \"jinaai/jina-embeddings-v2-small-en\")]\n+)\n+def test_fastembed_with_examples(n_dims, model_name):\n+    vectorizer = FastEmbedVectorizer(model_name)",
        "comment_created_at": "2024-04-19T06:46:58+00:00",
        "comment_author": "Anush008",
        "comment_body": "Yes, the models are downloaded.\r\n\r\nI had planned mock tests. However, since the implementation is straightforward, the risk of failure comes from changes in the FastEmbed library, so ended up with these integration tests(Skipped if FastEmbed is not installed).",
        "pr_file_module": null
      },
      {
        "comment_id": "1572741784",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 831,
        "pr_file": "tests/modules/vectorizer/integration_test_fastembed.py",
        "discussion_id": "1571514717",
        "commented_code": "@@ -0,0 +1,43 @@\n+from dsp.modules.sentence_vectorizer import FastEmbedVectorizer\n+import pytest\n+\n+from dspy.primitives.example import Example\n+\n+# Skip the test if the 'fastembed' package is not installed\n+pytest.importorskip(\"fastembed\", reason=\"'fastembed' is not installed. Use `pip install fastembed` to install it.\")\n+\n+\n+@pytest.mark.parametrize(\n+    \"n_dims,model_name\", [(384, \"BAAI/bge-small-en-v1.5\"), (512, \"jinaai/jina-embeddings-v2-small-en\")]\n+)\n+def test_fastembed_with_examples(n_dims, model_name):\n+    vectorizer = FastEmbedVectorizer(model_name)",
        "comment_created_at": "2024-04-19T18:11:58+00:00",
        "comment_author": "ammirsm",
        "comment_body": "So I think you don't want to include them in CI, am I right?",
        "pr_file_module": null
      },
      {
        "comment_id": "1573174517",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 831,
        "pr_file": "tests/modules/vectorizer/integration_test_fastembed.py",
        "discussion_id": "1571514717",
        "commented_code": "@@ -0,0 +1,43 @@\n+from dsp.modules.sentence_vectorizer import FastEmbedVectorizer\n+import pytest\n+\n+from dspy.primitives.example import Example\n+\n+# Skip the test if the 'fastembed' package is not installed\n+pytest.importorskip(\"fastembed\", reason=\"'fastembed' is not installed. Use `pip install fastembed` to install it.\")\n+\n+\n+@pytest.mark.parametrize(\n+    \"n_dims,model_name\", [(384, \"BAAI/bge-small-en-v1.5\"), (512, \"jinaai/jina-embeddings-v2-small-en\")]\n+)\n+def test_fastembed_with_examples(n_dims, model_name):\n+    vectorizer = FastEmbedVectorizer(model_name)",
        "comment_created_at": "2024-04-20T06:18:02+00:00",
        "comment_author": "Anush008",
        "comment_body": "I've added them to the CI in https://github.com/stanfordnlp/dspy/pull/831/commits/9b04c68f8eed9da0af6d7ac19b2a5e657f2f4d26.",
        "pr_file_module": null
      },
      {
        "comment_id": "1577005532",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 831,
        "pr_file": "tests/modules/vectorizer/integration_test_fastembed.py",
        "discussion_id": "1571514717",
        "commented_code": "@@ -0,0 +1,43 @@\n+from dsp.modules.sentence_vectorizer import FastEmbedVectorizer\n+import pytest\n+\n+from dspy.primitives.example import Example\n+\n+# Skip the test if the 'fastembed' package is not installed\n+pytest.importorskip(\"fastembed\", reason=\"'fastembed' is not installed. Use `pip install fastembed` to install it.\")\n+\n+\n+@pytest.mark.parametrize(\n+    \"n_dims,model_name\", [(384, \"BAAI/bge-small-en-v1.5\"), (512, \"jinaai/jina-embeddings-v2-small-en\")]\n+)\n+def test_fastembed_with_examples(n_dims, model_name):\n+    vectorizer = FastEmbedVectorizer(model_name)",
        "comment_created_at": "2024-04-23T23:12:25+00:00",
        "comment_author": "ammirsm",
        "comment_body": "sorry for the delay in reply! \r\n\r\nI didn't meant to add them to the CI as they would take so long and I think they are not built for there to download each time we run the CI, just wanted to double check them for there with you! \r\n\r\nsorry if I made a confusion for you.",
        "pr_file_module": null
      },
      {
        "comment_id": "1577167758",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 831,
        "pr_file": "tests/modules/vectorizer/integration_test_fastembed.py",
        "discussion_id": "1571514717",
        "commented_code": "@@ -0,0 +1,43 @@\n+from dsp.modules.sentence_vectorizer import FastEmbedVectorizer\n+import pytest\n+\n+from dspy.primitives.example import Example\n+\n+# Skip the test if the 'fastembed' package is not installed\n+pytest.importorskip(\"fastembed\", reason=\"'fastembed' is not installed. Use `pip install fastembed` to install it.\")\n+\n+\n+@pytest.mark.parametrize(\n+    \"n_dims,model_name\", [(384, \"BAAI/bge-small-en-v1.5\"), (512, \"jinaai/jina-embeddings-v2-small-en\")]\n+)\n+def test_fastembed_with_examples(n_dims, model_name):\n+    vectorizer = FastEmbedVectorizer(model_name)",
        "comment_created_at": "2024-04-24T02:51:21+00:00",
        "comment_author": "Anush008",
        "comment_body": "Alright. I've removed it from the CI.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1518643118",
    "pr_number": 608,
    "pr_file": "dsp/modules/mistral.py",
    "created_at": "2024-03-09T18:43:00+00:00",
    "commented_code": "+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1518643118",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-09T18:43:00+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "add all related imports here",
        "pr_file_module": null
      },
      {
        "comment_id": "1518648829",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-09T19:30:05+00:00",
        "comment_author": "fsndzomga",
        "comment_body": "not sure I understand, I just followed the same canvas as for other models (google.py, cohere.py etc)",
        "pr_file_module": null
      },
      {
        "comment_id": "1518649898",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-09T19:38:34+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "Imports should be included locally - [import cohere](https://github.com/stanfordnlp/dspy/blob/a51aad7dcbe257e01d677025cad96acba7c13e90/dsp/modules/cohere.py#L9), [import google](https://github.com/stanfordnlp/dspy/blob/a51aad7dcbe257e01d677025cad96acba7c13e90/dsp/modules/google.py)",
        "pr_file_module": null
      },
      {
        "comment_id": "1518651861",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-09T19:53:46+00:00",
        "comment_author": "fsndzomga",
        "comment_body": "Got it, working on it now.",
        "pr_file_module": null
      },
      {
        "comment_id": "1530372647",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-19T13:30:37+00:00",
        "comment_author": "okhat",
        "comment_body": "Thanks so much @fsndzomga ! I'm happy to merge pretty much once the imports do not affect people who don't need this module.\r\n\r\nBasically: no errors/warnings (\"Not loading Mistral AI...\" is a confusing error if I don't need Mistral) if I don't try to use this class",
        "pr_file_module": null
      },
      {
        "comment_id": "1530549236",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-19T14:55:26+00:00",
        "comment_author": "fsndzomga",
        "comment_body": "Yes I did that already, moved to local imports as per @arnavsinghvi11 advice.",
        "pr_file_module": null
      },
      {
        "comment_id": "1530611367",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-19T15:26:17+00:00",
        "comment_author": "okhat",
        "comment_body": "Hmm I don't think it's local imports now. Isn't importing outside any methods? Also it's going to print \"Not loading Mistral AI...\"  for everyone?",
        "pr_file_module": null
      },
      {
        "comment_id": "1530888861",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-19T18:27:02+00:00",
        "comment_author": "fsndzomga",
        "comment_body": "Ok got it, you want me to do it exactly like the cohere implementation and comment or remove the print statement.\r\nOn it !\r\n<img width=\"533\" alt=\"Screenshot 2024-03-19 at 19 25 50\" src=\"https://github.com/stanfordnlp/dspy/assets/101533724/842a564b-6103-4782-8794-04e3cede71e7\">\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1530944706",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-19T19:07:46+00:00",
        "comment_author": "fsndzomga",
        "comment_body": "@okhat done ! I now raise an import error inside the init of the Mistral class. \r\nAlso added some docstrings to synthetic_data_generator since I was already working on it and plan to add tests later this week.\r\n<img width=\"957\" alt=\"Screenshot 2024-03-19 at 20 05 46\" src=\"https://github.com/stanfordnlp/dspy/assets/101533724/1eebfc2c-3bd4-4a18-b7ef-229ac0e9f58d\">\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1532574034",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-20T18:10:06+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "Thanks @fsndzomga ! Could you actually include the synthetic data generations in a separate PR as it is still WIP. Will merge this PR following that!",
        "pr_file_module": null
      },
      {
        "comment_id": "1532589516",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-20T18:23:38+00:00",
        "comment_author": "fsndzomga",
        "comment_body": "Yes sure !",
        "pr_file_module": null
      },
      {
        "comment_id": "1532634539",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 608,
        "pr_file": "dsp/modules/mistral.py",
        "discussion_id": "1518643118",
        "commented_code": "@@ -0,0 +1,124 @@\n+from typing import Any, Optional\n+\n+import backoff\n+\n+from dsp.modules.lm import LM\n+\n+try:",
        "comment_created_at": "2024-03-20T18:52:52+00:00",
        "comment_author": "fsndzomga",
        "comment_body": "Done ! @arnavsinghvi11 ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1669243734",
    "pr_number": 1225,
    "pr_file": "dsp/modules/llamaindex.py",
    "created_at": "2024-07-08T20:12:50+00:00",
    "commented_code": "+\"\"\"\n+this code implements a wrapper around the llama_index library to emulate a dspy llm\n+\n+this allows the llama_index library to be used in the dspy framework since dspy has limited support for LLMs\n+\n+This code is a slightly modified copy of dspy/dsp/modules/azure_openai.py\n+\n+The way this works is simply by creating a dummy openai client that wraps around any llama_index LLM object and implements .complete and .chat\n+\n+tested with python 3.12\n+\n+dspy==0.1.4\n+dspy-ai==2.4.9\n+llama-index==0.10.35\n+llama-index-llms-openai==0.1.18\n+\n+\"\"\"\n+\n+import json\n+import logging\n+from typing import Any, Literal\n+\n+from easydict import EasyDict\n+from llama_index.core.base.llms.types import ChatMessage\n+from llama_index.core.llms import LLM\n+\n+\n+def LlamaIndexOpenAIClientWrapper(llm: LLM):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1669243734",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1225,
        "pr_file": "dsp/modules/llamaindex.py",
        "discussion_id": "1669243734",
        "commented_code": "@@ -0,0 +1,335 @@\n+\"\"\"\n+this code implements a wrapper around the llama_index library to emulate a dspy llm\n+\n+this allows the llama_index library to be used in the dspy framework since dspy has limited support for LLMs\n+\n+This code is a slightly modified copy of dspy/dsp/modules/azure_openai.py\n+\n+The way this works is simply by creating a dummy openai client that wraps around any llama_index LLM object and implements .complete and .chat\n+\n+tested with python 3.12\n+\n+dspy==0.1.4\n+dspy-ai==2.4.9\n+llama-index==0.10.35\n+llama-index-llms-openai==0.1.18\n+\n+\"\"\"\n+\n+import json\n+import logging\n+from typing import Any, Literal\n+\n+from easydict import EasyDict\n+from llama_index.core.base.llms.types import ChatMessage\n+from llama_index.core.llms import LLM\n+\n+\n+def LlamaIndexOpenAIClientWrapper(llm: LLM):",
        "comment_created_at": "2024-07-08T20:12:50+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "To clarify, does this support compatibility with all Llamaindex-LLM integrations? It seems to me like the definition here is only for OpenAI clients, which would leave this change redundant to just using DSPy.OpenAI. If it is not just isolated to OpenAI, lets refactor this function to LlamaIndexLLMWrapper",
        "pr_file_module": null
      },
      {
        "comment_id": "1669351535",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1225,
        "pr_file": "dsp/modules/llamaindex.py",
        "discussion_id": "1669243734",
        "commented_code": "@@ -0,0 +1,335 @@\n+\"\"\"\n+this code implements a wrapper around the llama_index library to emulate a dspy llm\n+\n+this allows the llama_index library to be used in the dspy framework since dspy has limited support for LLMs\n+\n+This code is a slightly modified copy of dspy/dsp/modules/azure_openai.py\n+\n+The way this works is simply by creating a dummy openai client that wraps around any llama_index LLM object and implements .complete and .chat\n+\n+tested with python 3.12\n+\n+dspy==0.1.4\n+dspy-ai==2.4.9\n+llama-index==0.10.35\n+llama-index-llms-openai==0.1.18\n+\n+\"\"\"\n+\n+import json\n+import logging\n+from typing import Any, Literal\n+\n+from easydict import EasyDict\n+from llama_index.core.base.llms.types import ChatMessage\n+from llama_index.core.llms import LLM\n+\n+\n+def LlamaIndexOpenAIClientWrapper(llm: LLM):",
        "comment_created_at": "2024-07-08T21:27:56+00:00",
        "comment_author": "FarisHijazi",
        "comment_body": "This is supposed to support all llamaindex llms. However the way i use this is by wrapping the llamaindex llm to look like an `openai` client object, then i pass this to a dspy azure openai llm.\nLlamaindex llm.chat and llm.complete are used for the actual calls",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1820434260",
    "pr_number": 1714,
    "pr_file": "dsp/trackers/langfuse_tracker.py",
    "created_at": "2024-10-29T09:32:31+00:00",
    "commented_code": "-from typing import Optional, Union, List, Any\n+from dataclasses import dataclass\n+from typing import Optional, List, Any, NamedTuple\n import httpx\n import logging\n import os\n-from langfuse.client import Langfuse, StatefulTraceClient, StatefulSpanClient, StateType\n from dsp.trackers.base import BaseTracker\n \n+try:\n+    from langfuse.client import Langfuse\n+    from langfuse.decorators import observe\n+except ImportError:\n+    def observe():",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1820434260",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1714,
        "pr_file": "dsp/trackers/langfuse_tracker.py",
        "discussion_id": "1820434260",
        "commented_code": "@@ -1,18 +1,26 @@\n-from typing import Optional, Union, List, Any\n+from dataclasses import dataclass\n+from typing import Optional, List, Any, NamedTuple\n import httpx\n import logging\n import os\n-from langfuse.client import Langfuse, StatefulTraceClient, StatefulSpanClient, StateType\n from dsp.trackers.base import BaseTracker\n \n+try:\n+    from langfuse.client import Langfuse\n+    from langfuse.decorators import observe\n+except ImportError:\n+    def observe():",
        "comment_created_at": "2024-10-29T09:32:31+00:00",
        "comment_author": "marcklingen",
        "comment_body": "Great idea to switch to the decorator to track any LLM used via dspy instead of relying on the langfuse openai intgeration. I'd suggest to capture token counts and model parameters as these otherwise get lost when making this change: https://langfuse.com/docs/sdk/python/decorators#log-any-llm-call",
        "pr_file_module": null
      },
      {
        "comment_id": "1820721976",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1714,
        "pr_file": "dsp/trackers/langfuse_tracker.py",
        "discussion_id": "1820434260",
        "commented_code": "@@ -1,18 +1,26 @@\n-from typing import Optional, Union, List, Any\n+from dataclasses import dataclass\n+from typing import Optional, List, Any, NamedTuple\n import httpx\n import logging\n import os\n-from langfuse.client import Langfuse, StatefulTraceClient, StatefulSpanClient, StateType\n from dsp.trackers.base import BaseTracker\n \n+try:\n+    from langfuse.client import Langfuse\n+    from langfuse.decorators import observe\n+except ImportError:\n+    def observe():",
        "comment_created_at": "2024-10-29T12:42:28+00:00",
        "comment_author": "xucailiang",
        "comment_body": "Hit the mark!\r\nIf an exception occurs here, it means that the langfuse package is not currently installed. So is it unnecessary for me to obtain the indicator data here? And users can also obtain indicator data through the `history` parameter without installing any tracking tools. \r\n\r\nSo I am a little unsure whether to collect indicators here.\r\n\r\nIs there anything I haven't considered? Please point it out to me, thank you!",
        "pr_file_module": null
      },
      {
        "comment_id": "1820736896",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1714,
        "pr_file": "dsp/trackers/langfuse_tracker.py",
        "discussion_id": "1820434260",
        "commented_code": "@@ -1,18 +1,26 @@\n-from typing import Optional, Union, List, Any\n+from dataclasses import dataclass\n+from typing import Optional, List, Any, NamedTuple\n import httpx\n import logging\n import os\n-from langfuse.client import Langfuse, StatefulTraceClient, StatefulSpanClient, StateType\n from dsp.trackers.base import BaseTracker\n \n+try:\n+    from langfuse.client import Langfuse\n+    from langfuse.decorators import observe\n+except ImportError:\n+    def observe():",
        "comment_created_at": "2024-10-29T12:52:11+00:00",
        "comment_author": "marcklingen",
        "comment_body": "Sorry for the confusion. I added the feedback to this line because I noticed you switched to the `observe` decorator instead of the OpenAI integration while reading this.\r\n\r\nI wasn't suggesting that any change to this line is necessary; rather, I meant that you should log additional LLM call information, as shown in the documentation, to maintain these metrics.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1505605003",
    "pr_number": 319,
    "pr_file": "dspy/retrieve/chromadb_rm.py",
    "created_at": "2024-02-28T09:11:06+00:00",
    "commented_code": "model_name=openai_embed_model,\n         )\n \n+        if local_embed_model is not None:\n+            self._local_embed_model = AutoModel.from_pretrained(local_embed_model)\n+            self._local_tokenizer = AutoTokenizer.from_pretrained(local_embed_model)\n+            self.use_local_model = True\n+            self.device = torch.device(\n+                \"cuda:0\"\n+                if torch.cuda.is_available()\n+                else \"mps\"\n+                if torch.backends.mps.is_available()\n+                else \"cpu\"\n+            )",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1505605003",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 319,
        "pr_file": "dspy/retrieve/chromadb_rm.py",
        "discussion_id": "1505605003",
        "commented_code": "@@ -85,6 +97,20 @@ def __init__(\n             model_name=openai_embed_model,\n         )\n \n+        if local_embed_model is not None:\n+            self._local_embed_model = AutoModel.from_pretrained(local_embed_model)\n+            self._local_tokenizer = AutoTokenizer.from_pretrained(local_embed_model)\n+            self.use_local_model = True\n+            self.device = torch.device(\n+                \"cuda:0\"\n+                if torch.cuda.is_available()\n+                else \"mps\"\n+                if torch.backends.mps.is_available()\n+                else \"cpu\"\n+            )",
        "comment_created_at": "2024-02-28T09:11:06+00:00",
        "comment_author": "tomaarsen",
        "comment_body": "```suggestion\r\n            try:\r\n                from sentence_transformers import SentenceTransformer\r\n            except ImportError as e:\r\n                raise ImportError(\r\n                    \"You need to install sentence_transformers library to use pretrained embedders. \"\r\n                    \"Please check the official doc https://www.sbert.net/ \"\r\n                    \"or simply run `pip install sentence-transformers\"\r\n                )\r\n            self._local_embed_model = SentenceTransformer(local_embed_model)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1505607122",
    "pr_number": 319,
    "pr_file": "dspy/retrieve/chromadb_rm.py",
    "created_at": "2024-02-28T09:12:37+00:00",
    "commented_code": "List[List[float]]: List of embeddings corresponding to each query.\n         \"\"\"\n \n-        embedding = self.openai_ef._client.create(\n-            input=queries, model=self._openai_embed_model\n+        if not self.use_local_model:\n+            # Using OpenAI's embedding model\n+            embedding = openai.Embedding.create(\n+                input=queries, model=self._openai_embed_model\n+            )\n+            return [embedding.embedding for embedding in embedding.data]\n+\n+        # Use local model for embeddings\n+        encoded_input = self._local_tokenizer(\n+            queries, padding=True, truncation=True, return_tensors=\"pt\"\n+        ).to(self.device)\n+        with torch.no_grad():\n+            model_output = self._local_embed_model(**encoded_input.to(self.device))\n+\n+        embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"])\n+        normalized_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n+        return normalized_embeddings.cpu().numpy().tolist()\n+\n+    def _mean_pooling(self, model_output, attention_mask):\n+        token_embeddings = model_output[\n+            0\n+        ]  # First element of model_output contains all token embeddings\n+        input_mask_expanded = (\n+            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n+        )\n+        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n+            input_mask_expanded.sum(1), min=1e-9\n         )",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1505607122",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 319,
        "pr_file": "dspy/retrieve/chromadb_rm.py",
        "discussion_id": "1505607122",
        "commented_code": "@@ -127,10 +153,34 @@ def _get_embeddings(self, queries: List[str]) -> List[List[float]]:\n             List[List[float]]: List of embeddings corresponding to each query.\n         \"\"\"\n \n-        embedding = self.openai_ef._client.create(\n-            input=queries, model=self._openai_embed_model\n+        if not self.use_local_model:\n+            # Using OpenAI's embedding model\n+            embedding = openai.Embedding.create(\n+                input=queries, model=self._openai_embed_model\n+            )\n+            return [embedding.embedding for embedding in embedding.data]\n+\n+        # Use local model for embeddings\n+        encoded_input = self._local_tokenizer(\n+            queries, padding=True, truncation=True, return_tensors=\"pt\"\n+        ).to(self.device)\n+        with torch.no_grad():\n+            model_output = self._local_embed_model(**encoded_input.to(self.device))\n+\n+        embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"])\n+        normalized_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n+        return normalized_embeddings.cpu().numpy().tolist()\n+\n+    def _mean_pooling(self, model_output, attention_mask):\n+        token_embeddings = model_output[\n+            0\n+        ]  # First element of model_output contains all token embeddings\n+        input_mask_expanded = (\n+            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n+        )\n+        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n+            input_mask_expanded.sum(1), min=1e-9\n         )",
        "comment_created_at": "2024-02-28T09:12:37+00:00",
        "comment_author": "tomaarsen",
        "comment_body": "```suggestion\r\n        return self._local_embed_model.encode(queries).tolist()\r\n```",
        "pr_file_module": null
      }
    ]
  }
]