[
  {
    "discussion_id": "2089935187",
    "pr_number": 8190,
    "pr_file": "dspy/predict/react.py",
    "created_at": "2025-05-15T00:19:31+00:00",
    "commented_code": "trajectory.pop(key)\n \n         return trajectory\n+    \n+    def _copy_tools(self, tools):\n+        try:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2089935187",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8190,
        "pr_file": "dspy/predict/react.py",
        "discussion_id": "2089935187",
        "commented_code": "@@ -162,6 +165,12 @@ def truncate_trajectory(self, trajectory):\n             trajectory.pop(key)\n \n         return trajectory\n+    \n+    def _copy_tools(self, tools):\n+        try:",
        "comment_created_at": "2025-05-15T00:19:31+00:00",
        "comment_author": "okhat",
        "comment_body": "I don't think this is deal. Now if just one tool can't be deep copied, you get silent corruption.\r\n\r\nFirst improvement would be to return a _list_ of deepcopied/copied items, with per-item try/except.\r\n\r\nSecond improvement would be to only try to deep copy tools that are not functions but are Callable objects. And then print a warning if it's not deepcopy-able.\r\n\r\nThird improvement is to change the API slightly: NO deep copy in the first place at all. Instead, _instantiate_ / construct objects if the tool is a *Class* with `__call__`. I think this is the only way to ensure that semantics aren't mixed up. If the user wants the `forward` function to create a copy, it should just be by giving it the _class_ and it can create an object by `Class()`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2089971379",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8190,
        "pr_file": "dspy/predict/react.py",
        "discussion_id": "2089935187",
        "commented_code": "@@ -162,6 +165,12 @@ def truncate_trajectory(self, trajectory):\n             trajectory.pop(key)\n \n         return trajectory\n+    \n+    def _copy_tools(self, tools):\n+        try:",
        "comment_created_at": "2025-05-15T00:56:15+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Good call for minimizing the scope of copying, let me change the logic to copy tools one by one. For the third point, how do you think we can determine the correct arguments and keyword arguments for instantiation?",
        "pr_file_module": null
      },
      {
        "comment_id": "2089991637",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8190,
        "pr_file": "dspy/predict/react.py",
        "discussion_id": "2089935187",
        "commented_code": "@@ -162,6 +165,12 @@ def truncate_trajectory(self, trajectory):\n             trajectory.pop(key)\n \n         return trajectory\n+    \n+    def _copy_tools(self, tools):\n+        try:",
        "comment_created_at": "2025-05-15T01:20:17+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "The 1st and 2nd points are addressed!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2072455596",
    "pr_number": 8166,
    "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
    "created_at": "2025-05-03T19:51:30+00:00",
    "commented_code": "self.rng = random.Random(seed)\n         np.random.seed(seed)\n \n+    def _set_num_trials_from_num_candidates(self, program, zeroshot_opt, num_candidates):\n+        num_vars = len(program.predictors())\n+        if not zeroshot_opt:\n+            num_vars *= 2  # Account for few-shot examples + instruction variables\n+        # Trials = MAX(c*M*log(N), c=2, 3/2*N)\n+        num_trials = int(max(2 * num_vars * np.log2(num_candidates), 1.5 * num_candidates))\n+",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2072455596",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8166,
        "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
        "discussion_id": "2072455596",
        "commented_code": "@@ -204,6 +219,15 @@ def _set_random_seeds(self, seed):\n         self.rng = random.Random(seed)\n         np.random.seed(seed)\n \n+    def _set_num_trials_from_num_candidates(self, program, zeroshot_opt, num_candidates):\n+        num_vars = len(program.predictors())\n+        if not zeroshot_opt:\n+            num_vars *= 2  # Account for few-shot examples + instruction variables\n+        # Trials = MAX(c*M*log(N), c=2, 3/2*N)\n+        num_trials = int(max(2 * num_vars * np.log2(num_candidates), 1.5 * num_candidates))\n+",
        "comment_created_at": "2025-05-03T19:51:30+00:00",
        "comment_author": "klopsahlong",
        "comment_body": "Note: this both refactors the num_trials calculator as a helper function, so that we can use it in the error message as well, AND changes `np.log` --> `np.log2`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2062974847",
    "pr_number": 8128,
    "pr_file": "dspy/utils/MCPTools.py",
    "created_at": "2025-04-28T06:34:56+00:00",
    "commented_code": "+from typing import Any, Dict, List, Optional, Tuple, Type\n+import json\n+import logging\n+import anyio\n+from dspy.primitives.tool import Tool\n+\n+logger = logging.getLogger(__name__)\n+\n+def map_json_schema_to_tool_args(schema: Optional[Dict[str, Any]]) -> Tuple[Dict[str, Any], Dict[str, Type], Dict[str, str]]:\n+    \"\"\"Maps a JSON schema to tool arguments compatible with DSPy Tool.\n+    \n+    Args:\n+        schema: A JSON schema describing the tool's input parameters\n+        \n+    Returns:\n+        A tuple of (args, arg_types, arg_desc) dictionaries for DSPy Tool initialization\n+    \"\"\"\n+    args, arg_types, arg_desc = {}, {}, {}\n+    if not schema or \"properties\" not in schema:\n+        return args, arg_types, arg_desc\n+        \n+    type_mapping = {\"string\": str, \"integer\": int, \"number\": float, \"boolean\": bool, \"array\": list, \"object\": dict}\n+    required = schema.get(\"required\", [])\n+    \n+    for name, prop in schema[\"properties\"].items():\n+        args[name] = prop\n+        arg_types[name] = type_mapping.get(prop.get(\"type\", \"string\"), Any)\n+        arg_desc[name] = prop.get(\"description\", \"No description provided.\")\n+        if name in required:\n+            arg_desc[name] += \" (Required)\"\n+\n+    return args, arg_types, arg_desc\n+\n+class MCPTool(Tool):\n+    \"\"\"Wrapper for an MCP tool, compatible with DSPy agents.\n+    \n+    This class wraps a Model Context Protocol tool and makes it compatible with\n+    DSPy's ReAct and other agent frameworks. It handles the translation between\n+    DSPy's tool interface and MCP's JSON-RPC interface.\n+    \"\"\"\n+\n+    def __init__(self, tool_info: Any, session: Any):\n+        \"\"\"Create a DSPy Tool from an MCP tool description.\n+        \n+        Args:\n+            tool_info: Tool information from MCP (object, dict, or JSON string)\n+            session: MCP client session for making tool calls\n+        \"\"\"\n+        self.session = session\n+        self._raw_tool_info = tool_info\n+\n+        name, desc, input_schema = self._extract_tool_info(tool_info)\n+        self.name = name\n+        args, arg_types, arg_desc = map_json_schema_to_tool_args(input_schema)\n+\n+        super().__init__(\n+            func=self.call_tool_async,\n+            name=name,\n+            desc=desc,\n+            args=args,\n+            arg_types=arg_types,\n+            arg_desc=arg_desc\n+        )\n+\n+    def _extract_tool_info(self, tool_info: Any) -> Tuple[str, str, Optional[Dict[str, Any]]]:\n+        \"\"\"Extract name, description and input schema from tool info.\n+        \n+        Args:\n+            tool_info: Tool information in various formats (object, dict, JSON string)\n+            \n+        Returns:\n+            A tuple of (name, description, input_schema)\n+        \"\"\"\n+        # Try object attributes\n+        if hasattr(tool_info, 'name') and hasattr(tool_info, 'description'):\n+            return (\n+                tool_info.name,\n+                tool_info.description,\n+                getattr(tool_info, 'inputSchema', None)\n+            )\n+            \n+        # Try dict format\n+        if isinstance(tool_info, dict):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2062974847",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8128,
        "pr_file": "dspy/utils/MCPTools.py",
        "discussion_id": "2062974847",
        "commented_code": "@@ -0,0 +1,215 @@\n+from typing import Any, Dict, List, Optional, Tuple, Type\n+import json\n+import logging\n+import anyio\n+from dspy.primitives.tool import Tool\n+\n+logger = logging.getLogger(__name__)\n+\n+def map_json_schema_to_tool_args(schema: Optional[Dict[str, Any]]) -> Tuple[Dict[str, Any], Dict[str, Type], Dict[str, str]]:\n+    \"\"\"Maps a JSON schema to tool arguments compatible with DSPy Tool.\n+    \n+    Args:\n+        schema: A JSON schema describing the tool's input parameters\n+        \n+    Returns:\n+        A tuple of (args, arg_types, arg_desc) dictionaries for DSPy Tool initialization\n+    \"\"\"\n+    args, arg_types, arg_desc = {}, {}, {}\n+    if not schema or \"properties\" not in schema:\n+        return args, arg_types, arg_desc\n+        \n+    type_mapping = {\"string\": str, \"integer\": int, \"number\": float, \"boolean\": bool, \"array\": list, \"object\": dict}\n+    required = schema.get(\"required\", [])\n+    \n+    for name, prop in schema[\"properties\"].items():\n+        args[name] = prop\n+        arg_types[name] = type_mapping.get(prop.get(\"type\", \"string\"), Any)\n+        arg_desc[name] = prop.get(\"description\", \"No description provided.\")\n+        if name in required:\n+            arg_desc[name] += \" (Required)\"\n+\n+    return args, arg_types, arg_desc\n+\n+class MCPTool(Tool):\n+    \"\"\"Wrapper for an MCP tool, compatible with DSPy agents.\n+    \n+    This class wraps a Model Context Protocol tool and makes it compatible with\n+    DSPy's ReAct and other agent frameworks. It handles the translation between\n+    DSPy's tool interface and MCP's JSON-RPC interface.\n+    \"\"\"\n+\n+    def __init__(self, tool_info: Any, session: Any):\n+        \"\"\"Create a DSPy Tool from an MCP tool description.\n+        \n+        Args:\n+            tool_info: Tool information from MCP (object, dict, or JSON string)\n+            session: MCP client session for making tool calls\n+        \"\"\"\n+        self.session = session\n+        self._raw_tool_info = tool_info\n+\n+        name, desc, input_schema = self._extract_tool_info(tool_info)\n+        self.name = name\n+        args, arg_types, arg_desc = map_json_schema_to_tool_args(input_schema)\n+\n+        super().__init__(\n+            func=self.call_tool_async,\n+            name=name,\n+            desc=desc,\n+            args=args,\n+            arg_types=arg_types,\n+            arg_desc=arg_desc\n+        )\n+\n+    def _extract_tool_info(self, tool_info: Any) -> Tuple[str, str, Optional[Dict[str, Any]]]:\n+        \"\"\"Extract name, description and input schema from tool info.\n+        \n+        Args:\n+            tool_info: Tool information in various formats (object, dict, JSON string)\n+            \n+        Returns:\n+            A tuple of (name, description, input_schema)\n+        \"\"\"\n+        # Try object attributes\n+        if hasattr(tool_info, 'name') and hasattr(tool_info, 'description'):\n+            return (\n+                tool_info.name,\n+                tool_info.description,\n+                getattr(tool_info, 'inputSchema', None)\n+            )\n+            \n+        # Try dict format\n+        if isinstance(tool_info, dict):",
        "comment_created_at": "2025-04-28T06:34:56+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Why do we need to support types other than `mcp.types.Tool`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2063147565",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8128,
        "pr_file": "dspy/utils/MCPTools.py",
        "discussion_id": "2062974847",
        "commented_code": "@@ -0,0 +1,215 @@\n+from typing import Any, Dict, List, Optional, Tuple, Type\n+import json\n+import logging\n+import anyio\n+from dspy.primitives.tool import Tool\n+\n+logger = logging.getLogger(__name__)\n+\n+def map_json_schema_to_tool_args(schema: Optional[Dict[str, Any]]) -> Tuple[Dict[str, Any], Dict[str, Type], Dict[str, str]]:\n+    \"\"\"Maps a JSON schema to tool arguments compatible with DSPy Tool.\n+    \n+    Args:\n+        schema: A JSON schema describing the tool's input parameters\n+        \n+    Returns:\n+        A tuple of (args, arg_types, arg_desc) dictionaries for DSPy Tool initialization\n+    \"\"\"\n+    args, arg_types, arg_desc = {}, {}, {}\n+    if not schema or \"properties\" not in schema:\n+        return args, arg_types, arg_desc\n+        \n+    type_mapping = {\"string\": str, \"integer\": int, \"number\": float, \"boolean\": bool, \"array\": list, \"object\": dict}\n+    required = schema.get(\"required\", [])\n+    \n+    for name, prop in schema[\"properties\"].items():\n+        args[name] = prop\n+        arg_types[name] = type_mapping.get(prop.get(\"type\", \"string\"), Any)\n+        arg_desc[name] = prop.get(\"description\", \"No description provided.\")\n+        if name in required:\n+            arg_desc[name] += \" (Required)\"\n+\n+    return args, arg_types, arg_desc\n+\n+class MCPTool(Tool):\n+    \"\"\"Wrapper for an MCP tool, compatible with DSPy agents.\n+    \n+    This class wraps a Model Context Protocol tool and makes it compatible with\n+    DSPy's ReAct and other agent frameworks. It handles the translation between\n+    DSPy's tool interface and MCP's JSON-RPC interface.\n+    \"\"\"\n+\n+    def __init__(self, tool_info: Any, session: Any):\n+        \"\"\"Create a DSPy Tool from an MCP tool description.\n+        \n+        Args:\n+            tool_info: Tool information from MCP (object, dict, or JSON string)\n+            session: MCP client session for making tool calls\n+        \"\"\"\n+        self.session = session\n+        self._raw_tool_info = tool_info\n+\n+        name, desc, input_schema = self._extract_tool_info(tool_info)\n+        self.name = name\n+        args, arg_types, arg_desc = map_json_schema_to_tool_args(input_schema)\n+\n+        super().__init__(\n+            func=self.call_tool_async,\n+            name=name,\n+            desc=desc,\n+            args=args,\n+            arg_types=arg_types,\n+            arg_desc=arg_desc\n+        )\n+\n+    def _extract_tool_info(self, tool_info: Any) -> Tuple[str, str, Optional[Dict[str, Any]]]:\n+        \"\"\"Extract name, description and input schema from tool info.\n+        \n+        Args:\n+            tool_info: Tool information in various formats (object, dict, JSON string)\n+            \n+        Returns:\n+            A tuple of (name, description, input_schema)\n+        \"\"\"\n+        # Try object attributes\n+        if hasattr(tool_info, 'name') and hasattr(tool_info, 'description'):\n+            return (\n+                tool_info.name,\n+                tool_info.description,\n+                getattr(tool_info, 'inputSchema', None)\n+            )\n+            \n+        # Try dict format\n+        if isinstance(tool_info, dict):",
        "comment_created_at": "2025-04-28T08:17:36+00:00",
        "comment_author": "ThanabordeeN",
        "comment_body": "MCP has to be set as an optional package because it requires Python > 3.10. Otherwise, importing MCP might lead to package errors; another choice is to create independent types in DSPy.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2063401911",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8128,
        "pr_file": "dspy/utils/MCPTools.py",
        "discussion_id": "2062974847",
        "commented_code": "@@ -0,0 +1,215 @@\n+from typing import Any, Dict, List, Optional, Tuple, Type\n+import json\n+import logging\n+import anyio\n+from dspy.primitives.tool import Tool\n+\n+logger = logging.getLogger(__name__)\n+\n+def map_json_schema_to_tool_args(schema: Optional[Dict[str, Any]]) -> Tuple[Dict[str, Any], Dict[str, Type], Dict[str, str]]:\n+    \"\"\"Maps a JSON schema to tool arguments compatible with DSPy Tool.\n+    \n+    Args:\n+        schema: A JSON schema describing the tool's input parameters\n+        \n+    Returns:\n+        A tuple of (args, arg_types, arg_desc) dictionaries for DSPy Tool initialization\n+    \"\"\"\n+    args, arg_types, arg_desc = {}, {}, {}\n+    if not schema or \"properties\" not in schema:\n+        return args, arg_types, arg_desc\n+        \n+    type_mapping = {\"string\": str, \"integer\": int, \"number\": float, \"boolean\": bool, \"array\": list, \"object\": dict}\n+    required = schema.get(\"required\", [])\n+    \n+    for name, prop in schema[\"properties\"].items():\n+        args[name] = prop\n+        arg_types[name] = type_mapping.get(prop.get(\"type\", \"string\"), Any)\n+        arg_desc[name] = prop.get(\"description\", \"No description provided.\")\n+        if name in required:\n+            arg_desc[name] += \" (Required)\"\n+\n+    return args, arg_types, arg_desc\n+\n+class MCPTool(Tool):\n+    \"\"\"Wrapper for an MCP tool, compatible with DSPy agents.\n+    \n+    This class wraps a Model Context Protocol tool and makes it compatible with\n+    DSPy's ReAct and other agent frameworks. It handles the translation between\n+    DSPy's tool interface and MCP's JSON-RPC interface.\n+    \"\"\"\n+\n+    def __init__(self, tool_info: Any, session: Any):\n+        \"\"\"Create a DSPy Tool from an MCP tool description.\n+        \n+        Args:\n+            tool_info: Tool information from MCP (object, dict, or JSON string)\n+            session: MCP client session for making tool calls\n+        \"\"\"\n+        self.session = session\n+        self._raw_tool_info = tool_info\n+\n+        name, desc, input_schema = self._extract_tool_info(tool_info)\n+        self.name = name\n+        args, arg_types, arg_desc = map_json_schema_to_tool_args(input_schema)\n+\n+        super().__init__(\n+            func=self.call_tool_async,\n+            name=name,\n+            desc=desc,\n+            args=args,\n+            arg_types=arg_types,\n+            arg_desc=arg_desc\n+        )\n+\n+    def _extract_tool_info(self, tool_info: Any) -> Tuple[str, str, Optional[Dict[str, Any]]]:\n+        \"\"\"Extract name, description and input schema from tool info.\n+        \n+        Args:\n+            tool_info: Tool information in various formats (object, dict, JSON string)\n+            \n+        Returns:\n+            A tuple of (name, description, input_schema)\n+        \"\"\"\n+        # Try object attributes\n+        if hasattr(tool_info, 'name') and hasattr(tool_info, 'description'):\n+            return (\n+                tool_info.name,\n+                tool_info.description,\n+                getattr(tool_info, 'inputSchema', None)\n+            )\n+            \n+        # Try dict format\n+        if isinstance(tool_info, dict):",
        "comment_created_at": "2025-04-28T10:44:58+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "I got that point, I meant when we receive MCP tool info as dict or string?",
        "pr_file_module": null
      },
      {
        "comment_id": "2063466031",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8128,
        "pr_file": "dspy/utils/MCPTools.py",
        "discussion_id": "2062974847",
        "commented_code": "@@ -0,0 +1,215 @@\n+from typing import Any, Dict, List, Optional, Tuple, Type\n+import json\n+import logging\n+import anyio\n+from dspy.primitives.tool import Tool\n+\n+logger = logging.getLogger(__name__)\n+\n+def map_json_schema_to_tool_args(schema: Optional[Dict[str, Any]]) -> Tuple[Dict[str, Any], Dict[str, Type], Dict[str, str]]:\n+    \"\"\"Maps a JSON schema to tool arguments compatible with DSPy Tool.\n+    \n+    Args:\n+        schema: A JSON schema describing the tool's input parameters\n+        \n+    Returns:\n+        A tuple of (args, arg_types, arg_desc) dictionaries for DSPy Tool initialization\n+    \"\"\"\n+    args, arg_types, arg_desc = {}, {}, {}\n+    if not schema or \"properties\" not in schema:\n+        return args, arg_types, arg_desc\n+        \n+    type_mapping = {\"string\": str, \"integer\": int, \"number\": float, \"boolean\": bool, \"array\": list, \"object\": dict}\n+    required = schema.get(\"required\", [])\n+    \n+    for name, prop in schema[\"properties\"].items():\n+        args[name] = prop\n+        arg_types[name] = type_mapping.get(prop.get(\"type\", \"string\"), Any)\n+        arg_desc[name] = prop.get(\"description\", \"No description provided.\")\n+        if name in required:\n+            arg_desc[name] += \" (Required)\"\n+\n+    return args, arg_types, arg_desc\n+\n+class MCPTool(Tool):\n+    \"\"\"Wrapper for an MCP tool, compatible with DSPy agents.\n+    \n+    This class wraps a Model Context Protocol tool and makes it compatible with\n+    DSPy's ReAct and other agent frameworks. It handles the translation between\n+    DSPy's tool interface and MCP's JSON-RPC interface.\n+    \"\"\"\n+\n+    def __init__(self, tool_info: Any, session: Any):\n+        \"\"\"Create a DSPy Tool from an MCP tool description.\n+        \n+        Args:\n+            tool_info: Tool information from MCP (object, dict, or JSON string)\n+            session: MCP client session for making tool calls\n+        \"\"\"\n+        self.session = session\n+        self._raw_tool_info = tool_info\n+\n+        name, desc, input_schema = self._extract_tool_info(tool_info)\n+        self.name = name\n+        args, arg_types, arg_desc = map_json_schema_to_tool_args(input_schema)\n+\n+        super().__init__(\n+            func=self.call_tool_async,\n+            name=name,\n+            desc=desc,\n+            args=args,\n+            arg_types=arg_types,\n+            arg_desc=arg_desc\n+        )\n+\n+    def _extract_tool_info(self, tool_info: Any) -> Tuple[str, str, Optional[Dict[str, Any]]]:\n+        \"\"\"Extract name, description and input schema from tool info.\n+        \n+        Args:\n+            tool_info: Tool information in various formats (object, dict, JSON string)\n+            \n+        Returns:\n+            A tuple of (name, description, input_schema)\n+        \"\"\"\n+        # Try object attributes\n+        if hasattr(tool_info, 'name') and hasattr(tool_info, 'description'):\n+            return (\n+                tool_info.name,\n+                tool_info.description,\n+                getattr(tool_info, 'inputSchema', None)\n+            )\n+            \n+        # Try dict format\n+        if isinstance(tool_info, dict):",
        "comment_created_at": "2025-04-28T11:27:09+00:00",
        "comment_author": "ThanabordeeN",
        "comment_body": "Got it. The check over there is unnecessary, I'm going to remove it ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1856172929",
    "pr_number": 1853,
    "pr_file": "dspy/adapters/chat_adapter.py",
    "created_at": "2024-11-25T08:59:37+00:00",
    "commented_code": "parsed_value = value\n \n     if isinstance(annotation, enum.EnumMeta):\n-        parsed_value = annotation[value]",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1856172929",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1853,
        "pr_file": "dspy/adapters/chat_adapter.py",
        "discussion_id": "1856172929",
        "commented_code": "@@ -231,7 +151,7 @@ def parse_value(value, annotation):\n     parsed_value = value\n \n     if isinstance(annotation, enum.EnumMeta):\n-        parsed_value = annotation[value]",
        "comment_created_at": "2024-11-25T08:59:37+00:00",
        "comment_author": "dbczumar",
        "comment_body": "This enum handling logic was incorrect.\r\n\r\nGiven an enum like\r\n\r\n```\r\nfrom enum import Enum\r\n\r\nclass Status(Enum):\r\n    PENDING = \"pending\"\r\n    IN_PROGRESS = \"in_progress\"\r\n    COMPLETED = \"completed\"\r\n```\r\n\r\nSerializing this enum to JSON with Pydantic produces `in_progress` (an enum field *value*), not `IN_PROGRESS` (an enum field *name). Status('in_progress') is the correct way to restore the enum field from its value, while Status['in_progress'] throws:\r\n\r\n```\r\npython3.10/enum.py:440, in EnumMeta.__getitem__(cls, name)\r\n    439 def __getitem__(cls, name):\r\n--> 440     return cls._member_map_[name]\r\n\r\nKeyError: 'in_progress'\r\n```\r\n\r\nI've added test coverage to confirm that the new behavior is correct (see `test_predict` and `reliability/complex_types/generated/test_many_types_1`)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1788925957",
    "pr_number": 1585,
    "pr_file": "dspy/adapters/chat_adapter.py",
    "created_at": "2024-10-06T06:44:43+00:00",
    "commented_code": "v = v if not isinstance(v, list) else format_list(v)\n         output.append(f\"[[ ## {k} ## ]]\n{v}\")\n \n-    return '\n\n'.join(output).strip()\n-        \n+    return \"\n\n\".join(output).strip()\n+\n \n def parse_value(value, annotation):\n-    if annotation is str: return str(value)\n+    if annotation is str:\n+        return str(value)\n     parsed_value = value\n     if isinstance(value, str):\n-        try: parsed_value = json.loads(value)\n+        try:\n+            parsed_value = json.loads(value)\n         except json.JSONDecodeError:\n-            try: parsed_value = ast.literal_eval(value)\n-            except (ValueError, SyntaxError): parsed_value = value\n+            try:\n+                parsed_value = ast.literal_eval(value)\n+            except (ValueError, SyntaxError):\n+                parsed_value = value\n     return TypeAdapter(annotation).validate_python(parsed_value)\n \n \n-def format_turn(signature, values, role, incomplete=False):       \n+def format_turn(signature, values, role, incomplete=False):\n     content = []\n \n     if role == \"user\":\n         field_names = signature.input_fields.keys()\n         if incomplete:\n             content.append(\"This is an example of the task, though some input or output fields are not supplied.\")\n     else:\n-        field_names, values = list(signature.output_fields.keys()) + ['completed'], {**values, 'completed': ''}\n+        field_names, values = list(signature.output_fields.keys()) + [\"completed\"], {**values, \"completed\": \"\"}\n \n     if not incomplete:\n         if not set(values).issuperset(set(field_names)):\n             raise ValueError(f\"Expected {field_names} but got {values.keys()}\")\n-    \n+\n     content.append(format_fields({k: values.get(k, \"Not supplied for this particular example.\") for k in field_names}))\n \n     if role == \"user\":\n-        content.append(\"Respond with the corresponding output fields, starting with the field \" +\n-                       \", then \".join(f\"`{f}`\" for f in signature.output_fields) +\n-                       \", and then ending with the marker for `completed`.\")\n+        content.append(\n+            \"Respond with the corresponding output fields, starting with the field \"\n+            + \", then \".join(f\"`{f}`\" for f in signature.output_fields)\n+            + \", and then ending with the marker for `completed`.\"\n+        )\n \n-    return {\"role\": role, \"content\": '\n\n'.join(content).strip()}\n+    return {\"role\": role, \"content\": \"\n\n\".join(content).strip()}\n \n \n def get_annotation_name(annotation):\n     origin = get_origin(annotation)\n     args = get_args(annotation)\n     if origin is None:\n-        if hasattr(annotation, '__name__'):\n+        if hasattr(annotation, \"__name__\"):\n             return annotation.__name__\n         else:\n             return str(annotation)\n     else:\n-        args_str = ', '.join(get_annotation_name(arg) for arg in args)\n-        return f\"{origin.__name__}[{args_str}]\"\n+        args_str = \", \".join(get_annotation_name(arg) for arg in args)\n+        return f\"{get_annotation_name(origin)}[{args_str}]\"",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1788925957",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1585,
        "pr_file": "dspy/adapters/chat_adapter.py",
        "discussion_id": "1788925957",
        "commented_code": "@@ -79,82 +90,90 @@ def format_fields(fields):\n         v = v if not isinstance(v, list) else format_list(v)\n         output.append(f\"[[ ## {k} ## ]]\\n{v}\")\n \n-    return '\\n\\n'.join(output).strip()\n-        \n+    return \"\\n\\n\".join(output).strip()\n+\n \n def parse_value(value, annotation):\n-    if annotation is str: return str(value)\n+    if annotation is str:\n+        return str(value)\n     parsed_value = value\n     if isinstance(value, str):\n-        try: parsed_value = json.loads(value)\n+        try:\n+            parsed_value = json.loads(value)\n         except json.JSONDecodeError:\n-            try: parsed_value = ast.literal_eval(value)\n-            except (ValueError, SyntaxError): parsed_value = value\n+            try:\n+                parsed_value = ast.literal_eval(value)\n+            except (ValueError, SyntaxError):\n+                parsed_value = value\n     return TypeAdapter(annotation).validate_python(parsed_value)\n \n \n-def format_turn(signature, values, role, incomplete=False):       \n+def format_turn(signature, values, role, incomplete=False):\n     content = []\n \n     if role == \"user\":\n         field_names = signature.input_fields.keys()\n         if incomplete:\n             content.append(\"This is an example of the task, though some input or output fields are not supplied.\")\n     else:\n-        field_names, values = list(signature.output_fields.keys()) + ['completed'], {**values, 'completed': ''}\n+        field_names, values = list(signature.output_fields.keys()) + [\"completed\"], {**values, \"completed\": \"\"}\n \n     if not incomplete:\n         if not set(values).issuperset(set(field_names)):\n             raise ValueError(f\"Expected {field_names} but got {values.keys()}\")\n-    \n+\n     content.append(format_fields({k: values.get(k, \"Not supplied for this particular example.\") for k in field_names}))\n \n     if role == \"user\":\n-        content.append(\"Respond with the corresponding output fields, starting with the field \" +\n-                       \", then \".join(f\"`{f}`\" for f in signature.output_fields) +\n-                       \", and then ending with the marker for `completed`.\")\n+        content.append(\n+            \"Respond with the corresponding output fields, starting with the field \"\n+            + \", then \".join(f\"`{f}`\" for f in signature.output_fields)\n+            + \", and then ending with the marker for `completed`.\"\n+        )\n \n-    return {\"role\": role, \"content\": '\\n\\n'.join(content).strip()}\n+    return {\"role\": role, \"content\": \"\\n\\n\".join(content).strip()}\n \n \n def get_annotation_name(annotation):\n     origin = get_origin(annotation)\n     args = get_args(annotation)\n     if origin is None:\n-        if hasattr(annotation, '__name__'):\n+        if hasattr(annotation, \"__name__\"):\n             return annotation.__name__\n         else:\n             return str(annotation)\n     else:\n-        args_str = ', '.join(get_annotation_name(arg) for arg in args)\n-        return f\"{origin.__name__}[{args_str}]\"\n+        args_str = \", \".join(get_annotation_name(arg) for arg in args)\n+        return f\"{get_annotation_name(origin)}[{args_str}]\"",
        "comment_created_at": "2024-10-06T06:44:43+00:00",
        "comment_author": "mikeedjones",
        "comment_body": "Change to account for the origin of `Literal` not having a `__name__` attribute.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1511751457",
    "pr_number": 548,
    "pr_file": "dspy/retrieve/neo4j_rm.py",
    "created_at": "2024-03-04T20:26:18+00:00",
    "commented_code": "+import os\n+from typing import Any, List, Optional, Union\n+\n+import backoff\n+from openai import (\n+    APITimeoutError,\n+    InternalServerError,\n+    OpenAI,\n+    RateLimitError,\n+    UnprocessableEntityError,\n+)\n+\n+import dspy\n+from dsp.utils import dotdict\n+\n+try:\n+    from neo4j import GraphDatabase\n+    from neo4j.exceptions import (\n+        AuthError,\n+        ServiceUnavailable,\n+    )\n+except ImportError:\n+    raise ImportError(\n+        \"Please install the neo4j package by running `pip install dspy-ai[neo4j]`\",\n+    )\n+\n+\n+class Embedder:\n+    def __init__(self, provider: str, model: str):\n+        if provider == \"openai\":\n+            api_key = os.getenv(\"OPENAI_API_KEY\")\n+            if not api_key:\n+                raise ValueError(\"Environment variable OPENAI_API_KEY must be set\")\n+            self.client = OpenAI()\n+            self.model = model\n+\n+    @backoff.on_exception(\n+        backoff.expo,\n+        (\n+            APITimeoutError,\n+            InternalServerError,\n+            RateLimitError,\n+            UnprocessableEntityError,\n+        ),\n+        max_time=15,\n+    )\n+    def __call__(self, queries) -> Any:\n+        embedding = self.client.embeddings.create(input=queries, model=self.model)\n+        return [result.embedding for result in embedding.data]\n+\n+\n+DEFAULT_INDEX_QUERY = \"CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score \"\n+\n+\n+class Neo4jRM(dspy.Retrieve):\n+    def __init__(\n+        self,\n+        index_name: str,\n+        text_node_property: str = None,\n+        k: int = 5,\n+        retrieval_query: str = None,\n+        embedding_provider: str = \"openai\",\n+        embedding_model: str = \"text-embedding-ada-002\",\n+    ):\n+        super().__init__(k=k)\n+        self.index_name = index_name\n+        self.username = os.getenv(\"NEO4J_USERNAME\")\n+        self.password = os.getenv(\"NEO4J_PASSWORD\")\n+        self.uri = os.getenv(\"NEO4J_URI\")\n+        self.database = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n+        self.k = k\n+        self.retrieval_query = retrieval_query\n+        self.text_node_property = text_node_property\n+        if not self.username:\n+            raise ValueError(\"Environment variable NEO4J_USERNAME must be set\")\n+        if not self.password:\n+            raise ValueError(\"Environment variable NEO4J_PASSWORD must be set\")\n+        if not self.uri:\n+            raise ValueError(\"Environment variable NEO4J_URI must be set\")\n+        if not self.text_node_property and not self.retrieval_query:\n+            raise ValueError(\"Either `text_node_property` or `retrieval_query` parameters must be defined\")\n+        try:\n+            self.driver = GraphDatabase.driver(self.uri, auth=(self.username, self.password))\n+            self.driver.verify_connectivity()\n+\n+        except (\n+            ServiceUnavailable,\n+            AuthError,\n+        ) as e:\n+            raise ConnectionError(\"Failed to connect to Neo4j database\") from e\n+\n+        self.embedder = Embedder(provider=embedding_provider, model=embedding_model)\n+\n+    def forward(self, query_or_queries: Union[str, List[str]], k: Optional[int]) -> dspy.Prediction:\n+        if not isinstance(query_or_queries, list):\n+            query_or_queries = [query_or_queries]\n+        query_vectors = self.embedder(query_or_queries)\n+        contents = []\n+        retrieval_query = self.retrieval_query or f\"RETURN node.{self.text_node_property} AS text\"\n+        for vector in query_vectors:\n+            records, _, _ = self.driver.execute_query(\n+                DEFAULT_INDEX_QUERY + retrieval_query,\n+                {\"embedding\": vector, \"index\": self.index_name, \"k\": k or self.k},\n+                database_=self.database,\n+            )\n+            contents.extend([dotdict({\"long_text\": r[\"text\"]}) for r in records])\n+        return contents",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1511751457",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 548,
        "pr_file": "dspy/retrieve/neo4j_rm.py",
        "discussion_id": "1511751457",
        "commented_code": "@@ -0,0 +1,107 @@\n+import os\n+from typing import Any, List, Optional, Union\n+\n+import backoff\n+from openai import (\n+    APITimeoutError,\n+    InternalServerError,\n+    OpenAI,\n+    RateLimitError,\n+    UnprocessableEntityError,\n+)\n+\n+import dspy\n+from dsp.utils import dotdict\n+\n+try:\n+    from neo4j import GraphDatabase\n+    from neo4j.exceptions import (\n+        AuthError,\n+        ServiceUnavailable,\n+    )\n+except ImportError:\n+    raise ImportError(\n+        \"Please install the neo4j package by running `pip install dspy-ai[neo4j]`\",\n+    )\n+\n+\n+class Embedder:\n+    def __init__(self, provider: str, model: str):\n+        if provider == \"openai\":\n+            api_key = os.getenv(\"OPENAI_API_KEY\")\n+            if not api_key:\n+                raise ValueError(\"Environment variable OPENAI_API_KEY must be set\")\n+            self.client = OpenAI()\n+            self.model = model\n+\n+    @backoff.on_exception(\n+        backoff.expo,\n+        (\n+            APITimeoutError,\n+            InternalServerError,\n+            RateLimitError,\n+            UnprocessableEntityError,\n+        ),\n+        max_time=15,\n+    )\n+    def __call__(self, queries) -> Any:\n+        embedding = self.client.embeddings.create(input=queries, model=self.model)\n+        return [result.embedding for result in embedding.data]\n+\n+\n+DEFAULT_INDEX_QUERY = \"CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score \"\n+\n+\n+class Neo4jRM(dspy.Retrieve):\n+    def __init__(\n+        self,\n+        index_name: str,\n+        text_node_property: str = None,\n+        k: int = 5,\n+        retrieval_query: str = None,\n+        embedding_provider: str = \"openai\",\n+        embedding_model: str = \"text-embedding-ada-002\",\n+    ):\n+        super().__init__(k=k)\n+        self.index_name = index_name\n+        self.username = os.getenv(\"NEO4J_USERNAME\")\n+        self.password = os.getenv(\"NEO4J_PASSWORD\")\n+        self.uri = os.getenv(\"NEO4J_URI\")\n+        self.database = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n+        self.k = k\n+        self.retrieval_query = retrieval_query\n+        self.text_node_property = text_node_property\n+        if not self.username:\n+            raise ValueError(\"Environment variable NEO4J_USERNAME must be set\")\n+        if not self.password:\n+            raise ValueError(\"Environment variable NEO4J_PASSWORD must be set\")\n+        if not self.uri:\n+            raise ValueError(\"Environment variable NEO4J_URI must be set\")\n+        if not self.text_node_property and not self.retrieval_query:\n+            raise ValueError(\"Either `text_node_property` or `retrieval_query` parameters must be defined\")\n+        try:\n+            self.driver = GraphDatabase.driver(self.uri, auth=(self.username, self.password))\n+            self.driver.verify_connectivity()\n+\n+        except (\n+            ServiceUnavailable,\n+            AuthError,\n+        ) as e:\n+            raise ConnectionError(\"Failed to connect to Neo4j database\") from e\n+\n+        self.embedder = Embedder(provider=embedding_provider, model=embedding_model)\n+\n+    def forward(self, query_or_queries: Union[str, List[str]], k: Optional[int]) -> dspy.Prediction:\n+        if not isinstance(query_or_queries, list):\n+            query_or_queries = [query_or_queries]\n+        query_vectors = self.embedder(query_or_queries)\n+        contents = []\n+        retrieval_query = self.retrieval_query or f\"RETURN node.{self.text_node_property} AS text\"\n+        for vector in query_vectors:\n+            records, _, _ = self.driver.execute_query(\n+                DEFAULT_INDEX_QUERY + retrieval_query,\n+                {\"embedding\": vector, \"index\": self.index_name, \"k\": k or self.k},\n+                database_=self.database,\n+            )\n+            contents.extend([dotdict({\"long_text\": r[\"text\"]}) for r in records])\n+        return contents",
        "comment_created_at": "2024-03-04T20:26:18+00:00",
        "comment_author": "jexp",
        "comment_body": "sort by score and take top-k results",
        "pr_file_module": null
      },
      {
        "comment_id": "1511802124",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 548,
        "pr_file": "dspy/retrieve/neo4j_rm.py",
        "discussion_id": "1511751457",
        "commented_code": "@@ -0,0 +1,107 @@\n+import os\n+from typing import Any, List, Optional, Union\n+\n+import backoff\n+from openai import (\n+    APITimeoutError,\n+    InternalServerError,\n+    OpenAI,\n+    RateLimitError,\n+    UnprocessableEntityError,\n+)\n+\n+import dspy\n+from dsp.utils import dotdict\n+\n+try:\n+    from neo4j import GraphDatabase\n+    from neo4j.exceptions import (\n+        AuthError,\n+        ServiceUnavailable,\n+    )\n+except ImportError:\n+    raise ImportError(\n+        \"Please install the neo4j package by running `pip install dspy-ai[neo4j]`\",\n+    )\n+\n+\n+class Embedder:\n+    def __init__(self, provider: str, model: str):\n+        if provider == \"openai\":\n+            api_key = os.getenv(\"OPENAI_API_KEY\")\n+            if not api_key:\n+                raise ValueError(\"Environment variable OPENAI_API_KEY must be set\")\n+            self.client = OpenAI()\n+            self.model = model\n+\n+    @backoff.on_exception(\n+        backoff.expo,\n+        (\n+            APITimeoutError,\n+            InternalServerError,\n+            RateLimitError,\n+            UnprocessableEntityError,\n+        ),\n+        max_time=15,\n+    )\n+    def __call__(self, queries) -> Any:\n+        embedding = self.client.embeddings.create(input=queries, model=self.model)\n+        return [result.embedding for result in embedding.data]\n+\n+\n+DEFAULT_INDEX_QUERY = \"CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score \"\n+\n+\n+class Neo4jRM(dspy.Retrieve):\n+    def __init__(\n+        self,\n+        index_name: str,\n+        text_node_property: str = None,\n+        k: int = 5,\n+        retrieval_query: str = None,\n+        embedding_provider: str = \"openai\",\n+        embedding_model: str = \"text-embedding-ada-002\",\n+    ):\n+        super().__init__(k=k)\n+        self.index_name = index_name\n+        self.username = os.getenv(\"NEO4J_USERNAME\")\n+        self.password = os.getenv(\"NEO4J_PASSWORD\")\n+        self.uri = os.getenv(\"NEO4J_URI\")\n+        self.database = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n+        self.k = k\n+        self.retrieval_query = retrieval_query\n+        self.text_node_property = text_node_property\n+        if not self.username:\n+            raise ValueError(\"Environment variable NEO4J_USERNAME must be set\")\n+        if not self.password:\n+            raise ValueError(\"Environment variable NEO4J_PASSWORD must be set\")\n+        if not self.uri:\n+            raise ValueError(\"Environment variable NEO4J_URI must be set\")\n+        if not self.text_node_property and not self.retrieval_query:\n+            raise ValueError(\"Either `text_node_property` or `retrieval_query` parameters must be defined\")\n+        try:\n+            self.driver = GraphDatabase.driver(self.uri, auth=(self.username, self.password))\n+            self.driver.verify_connectivity()\n+\n+        except (\n+            ServiceUnavailable,\n+            AuthError,\n+        ) as e:\n+            raise ConnectionError(\"Failed to connect to Neo4j database\") from e\n+\n+        self.embedder = Embedder(provider=embedding_provider, model=embedding_model)\n+\n+    def forward(self, query_or_queries: Union[str, List[str]], k: Optional[int]) -> dspy.Prediction:\n+        if not isinstance(query_or_queries, list):\n+            query_or_queries = [query_or_queries]\n+        query_vectors = self.embedder(query_or_queries)\n+        contents = []\n+        retrieval_query = self.retrieval_query or f\"RETURN node.{self.text_node_property} AS text\"\n+        for vector in query_vectors:\n+            records, _, _ = self.driver.execute_query(\n+                DEFAULT_INDEX_QUERY + retrieval_query,\n+                {\"embedding\": vector, \"index\": self.index_name, \"k\": k or self.k},\n+                database_=self.database,\n+            )\n+            contents.extend([dotdict({\"long_text\": r[\"text\"]}) for r in records])\n+        return contents",
        "comment_created_at": "2024-03-04T21:13:03+00:00",
        "comment_author": "tomasonjo",
        "comment_body": "Fixed",
        "pr_file_module": null
      }
    ]
  }
]