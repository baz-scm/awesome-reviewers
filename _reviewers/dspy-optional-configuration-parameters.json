[
  {
    "discussion_id": "2049103385",
    "pr_number": 8077,
    "pr_file": "dspy/clients/lm.py",
    "created_at": "2025-04-17T14:37:40+00:00",
    "commented_code": "self,\n         model: str,\n         model_type: Literal[\"chat\", \"text\"] = \"chat\",\n-        temperature: float = 0.0,\n-        max_tokens: int = 1000,\n+        temperature: Optional[float] = None,\n+        max_tokens: Optional[int] = None,",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2049103385",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8077,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "2049103385",
        "commented_code": "@@ -35,8 +35,8 @@ def __init__(\n         self,\n         model: str,\n         model_type: Literal[\"chat\", \"text\"] = \"chat\",\n-        temperature: float = 0.0,\n-        max_tokens: int = 1000,\n+        temperature: Optional[float] = None,\n+        max_tokens: Optional[int] = None,",
        "comment_created_at": "2025-04-17T14:37:40+00:00",
        "comment_author": "klopsahlong",
        "comment_body": "Updating defaults, which were throwing errors for reasoning models. Now, instead of defaulting to a temp of 0.0 and max_tokens 1000 (and erroring out automatically for o3mini), we are setting temp and max_tokens based on whether the model is a reasoning model or not. If the user has intentionally set one of the values to something the reasoning model can't handle (i.e. temperature=0.7), then we will still throw an error.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2314691134",
    "pr_number": 8737,
    "pr_file": "dspy/teleprompt/gepa/gepa_utils.py",
    "created_at": "2025-09-02T00:34:36+00:00",
    "commented_code": "self.custom_instruction_proposer = custom_instruction_proposer\n \n         if self.custom_instruction_proposer is not None:\n+            assert self.reflection_lm is not None, \"reflection_lm must be provided when using custom_instruction_proposer\"\n             # We are only overriding the propose_new_texts method when a custom\n             # instruction proposer is provided. Otherwise, we use the GEPA\n             # default propose_new_texts.\n+\n             def custom_propose_new_texts(\n                 candidate: dict[str, str],\n                 reflective_dataset: dict[str, list[dict[str, Any]]],\n                 components_to_update: list[str]\n             ) -> dict[str, str]:\n-\n-                proposer = self.custom_instruction_proposer\n-\n-                new_texts: dict[str, str] = {}\n-                for name in components_to_update:\n-                    current_instruction = candidate[name]\n-                    dataset_with_feedback = reflective_dataset[name]\n-\n-                    with dspy.context(lm=self.reflection_lm):\n-                        new_texts[name] = proposer(\n-                            current_instruction=current_instruction,\n-                            reflective_dataset=dataset_with_feedback\n-                        )\n-\n-                return new_texts\n+                with dspy.context(lm=self.reflection_lm):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2314691134",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8737,
        "pr_file": "dspy/teleprompt/gepa/gepa_utils.py",
        "discussion_id": "2314691134",
        "commented_code": "@@ -77,29 +75,22 @@ def __init__(\n         self.custom_instruction_proposer = custom_instruction_proposer\n \n         if self.custom_instruction_proposer is not None:\n+            assert self.reflection_lm is not None, \"reflection_lm must be provided when using custom_instruction_proposer\"\n             # We are only overriding the propose_new_texts method when a custom\n             # instruction proposer is provided. Otherwise, we use the GEPA\n             # default propose_new_texts.\n+\n             def custom_propose_new_texts(\n                 candidate: dict[str, str],\n                 reflective_dataset: dict[str, list[dict[str, Any]]],\n                 components_to_update: list[str]\n             ) -> dict[str, str]:\n-\n-                proposer = self.custom_instruction_proposer\n-\n-                new_texts: dict[str, str] = {}\n-                for name in components_to_update:\n-                    current_instruction = candidate[name]\n-                    dataset_with_feedback = reflective_dataset[name]\n-\n-                    with dspy.context(lm=self.reflection_lm):\n-                        new_texts[name] = proposer(\n-                            current_instruction=current_instruction,\n-                            reflective_dataset=dataset_with_feedback\n-                        )\n-\n-                return new_texts\n+                with dspy.context(lm=self.reflection_lm):",
        "comment_created_at": "2025-09-02T00:34:36+00:00",
        "comment_author": "LakshyAAAgrawal",
        "comment_body": "Another flexibility we can provide here is, instead of mandating that the reflection_lm be provided and using `dspy.context` here, which essentially forces me to pick one reflection_lm and stick with it through optimization,\r\n\r\nwe can just let the user provide a custom_instruction_proposer, and call it. The custom_instruction_proposer takes care of what the reflection_lm is. If reflection_lm is provided in addition, then we use it, otherwise it is not mandatory.",
        "pr_file_module": null
      },
      {
        "comment_id": "2315016186",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8737,
        "pr_file": "dspy/teleprompt/gepa/gepa_utils.py",
        "discussion_id": "2314691134",
        "commented_code": "@@ -77,29 +75,22 @@ def __init__(\n         self.custom_instruction_proposer = custom_instruction_proposer\n \n         if self.custom_instruction_proposer is not None:\n+            assert self.reflection_lm is not None, \"reflection_lm must be provided when using custom_instruction_proposer\"\n             # We are only overriding the propose_new_texts method when a custom\n             # instruction proposer is provided. Otherwise, we use the GEPA\n             # default propose_new_texts.\n+\n             def custom_propose_new_texts(\n                 candidate: dict[str, str],\n                 reflective_dataset: dict[str, list[dict[str, Any]]],\n                 components_to_update: list[str]\n             ) -> dict[str, str]:\n-\n-                proposer = self.custom_instruction_proposer\n-\n-                new_texts: dict[str, str] = {}\n-                for name in components_to_update:\n-                    current_instruction = candidate[name]\n-                    dataset_with_feedback = reflective_dataset[name]\n-\n-                    with dspy.context(lm=self.reflection_lm):\n-                        new_texts[name] = proposer(\n-                            current_instruction=current_instruction,\n-                            reflective_dataset=dataset_with_feedback\n-                        )\n-\n-                return new_texts\n+                with dspy.context(lm=self.reflection_lm):",
        "comment_created_at": "2025-09-02T06:13:44+00:00",
        "comment_author": "andressrg",
        "comment_body": "I like the idea!\r\n\r\nJust made this change:\r\n\r\n```\r\n        if self.custom_instruction_proposer is not None:\r\n            # We are only overriding the propose_new_texts method when a custom\r\n            # instruction proposer is provided. Otherwise, we use the GEPA\r\n            # default propose_new_texts.\r\n\r\n            def custom_propose_new_texts(\r\n                candidate: dict[str, str],\r\n                reflective_dataset: dict[str, list[dict[str, Any]]],\r\n                components_to_update: list[str]\r\n            ) -> dict[str, str]:\r\n                if self.reflection_lm is not None:\r\n                    with dspy.context(lm=self.reflection_lm):\r\n                        return self.custom_instruction_proposer(\r\n                            candidate=candidate,\r\n                            reflective_dataset=reflective_dataset,\r\n                            components_to_update=components_to_update\r\n                        )\r\n                else:\r\n                    return self.custom_instruction_proposer(\r\n                        candidate=candidate,\r\n                        reflective_dataset=reflective_dataset,\r\n                        components_to_update=components_to_update\r\n                    )\r\n\r\n            self.propose_new_texts = custom_propose_new_texts\r\n```\r\n\r\nNow, the core `gepa/appi.py` does require the user to provide a reflection_lm:\r\n\r\nhttps://github.com/gepa-ai/gepa/blob/2cf10c79125533af345ffbd48497005dd1f68ee9/src/gepa/api.py#L127\r\n\r\nWhich means that in order to have true flexibility, the core gepa library needs to be updated as well I believe.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2317187575",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8737,
        "pr_file": "dspy/teleprompt/gepa/gepa_utils.py",
        "discussion_id": "2314691134",
        "commented_code": "@@ -77,29 +75,22 @@ def __init__(\n         self.custom_instruction_proposer = custom_instruction_proposer\n \n         if self.custom_instruction_proposer is not None:\n+            assert self.reflection_lm is not None, \"reflection_lm must be provided when using custom_instruction_proposer\"\n             # We are only overriding the propose_new_texts method when a custom\n             # instruction proposer is provided. Otherwise, we use the GEPA\n             # default propose_new_texts.\n+\n             def custom_propose_new_texts(\n                 candidate: dict[str, str],\n                 reflective_dataset: dict[str, list[dict[str, Any]]],\n                 components_to_update: list[str]\n             ) -> dict[str, str]:\n-\n-                proposer = self.custom_instruction_proposer\n-\n-                new_texts: dict[str, str] = {}\n-                for name in components_to_update:\n-                    current_instruction = candidate[name]\n-                    dataset_with_feedback = reflective_dataset[name]\n-\n-                    with dspy.context(lm=self.reflection_lm):\n-                        new_texts[name] = proposer(\n-                            current_instruction=current_instruction,\n-                            reflective_dataset=dataset_with_feedback\n-                        )\n-\n-                return new_texts\n+                with dspy.context(lm=self.reflection_lm):",
        "comment_created_at": "2025-09-02T21:13:23+00:00",
        "comment_author": "LakshyAAAgrawal",
        "comment_body": "Good catch. Let me update that!",
        "pr_file_module": null
      },
      {
        "comment_id": "2317200647",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8737,
        "pr_file": "dspy/teleprompt/gepa/gepa_utils.py",
        "discussion_id": "2314691134",
        "commented_code": "@@ -77,29 +75,22 @@ def __init__(\n         self.custom_instruction_proposer = custom_instruction_proposer\n \n         if self.custom_instruction_proposer is not None:\n+            assert self.reflection_lm is not None, \"reflection_lm must be provided when using custom_instruction_proposer\"\n             # We are only overriding the propose_new_texts method when a custom\n             # instruction proposer is provided. Otherwise, we use the GEPA\n             # default propose_new_texts.\n+\n             def custom_propose_new_texts(\n                 candidate: dict[str, str],\n                 reflective_dataset: dict[str, list[dict[str, Any]]],\n                 components_to_update: list[str]\n             ) -> dict[str, str]:\n-\n-                proposer = self.custom_instruction_proposer\n-\n-                new_texts: dict[str, str] = {}\n-                for name in components_to_update:\n-                    current_instruction = candidate[name]\n-                    dataset_with_feedback = reflective_dataset[name]\n-\n-                    with dspy.context(lm=self.reflection_lm):\n-                        new_texts[name] = proposer(\n-                            current_instruction=current_instruction,\n-                            reflective_dataset=dataset_with_feedback\n-                        )\n-\n-                return new_texts\n+                with dspy.context(lm=self.reflection_lm):",
        "comment_created_at": "2025-09-02T21:21:51+00:00",
        "comment_author": "LakshyAAAgrawal",
        "comment_body": "Can you take a look at: https://github.com/gepa-ai/gepa/pull/50\r\n\r\nI think this should fix it. Can you check?",
        "pr_file_module": null
      },
      {
        "comment_id": "2317533778",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8737,
        "pr_file": "dspy/teleprompt/gepa/gepa_utils.py",
        "discussion_id": "2314691134",
        "commented_code": "@@ -77,29 +75,22 @@ def __init__(\n         self.custom_instruction_proposer = custom_instruction_proposer\n \n         if self.custom_instruction_proposer is not None:\n+            assert self.reflection_lm is not None, \"reflection_lm must be provided when using custom_instruction_proposer\"\n             # We are only overriding the propose_new_texts method when a custom\n             # instruction proposer is provided. Otherwise, we use the GEPA\n             # default propose_new_texts.\n+\n             def custom_propose_new_texts(\n                 candidate: dict[str, str],\n                 reflective_dataset: dict[str, list[dict[str, Any]]],\n                 components_to_update: list[str]\n             ) -> dict[str, str]:\n-\n-                proposer = self.custom_instruction_proposer\n-\n-                new_texts: dict[str, str] = {}\n-                for name in components_to_update:\n-                    current_instruction = candidate[name]\n-                    dataset_with_feedback = reflective_dataset[name]\n-\n-                    with dspy.context(lm=self.reflection_lm):\n-                        new_texts[name] = proposer(\n-                            current_instruction=current_instruction,\n-                            reflective_dataset=dataset_with_feedback\n-                        )\n-\n-                return new_texts\n+                with dspy.context(lm=self.reflection_lm):",
        "comment_created_at": "2025-09-03T01:20:23+00:00",
        "comment_author": "andressrg",
        "comment_body": "Nice!. Checking now",
        "pr_file_module": null
      },
      {
        "comment_id": "2317565269",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8737,
        "pr_file": "dspy/teleprompt/gepa/gepa_utils.py",
        "discussion_id": "2314691134",
        "commented_code": "@@ -77,29 +75,22 @@ def __init__(\n         self.custom_instruction_proposer = custom_instruction_proposer\n \n         if self.custom_instruction_proposer is not None:\n+            assert self.reflection_lm is not None, \"reflection_lm must be provided when using custom_instruction_proposer\"\n             # We are only overriding the propose_new_texts method when a custom\n             # instruction proposer is provided. Otherwise, we use the GEPA\n             # default propose_new_texts.\n+\n             def custom_propose_new_texts(\n                 candidate: dict[str, str],\n                 reflective_dataset: dict[str, list[dict[str, Any]]],\n                 components_to_update: list[str]\n             ) -> dict[str, str]:\n-\n-                proposer = self.custom_instruction_proposer\n-\n-                new_texts: dict[str, str] = {}\n-                for name in components_to_update:\n-                    current_instruction = candidate[name]\n-                    dataset_with_feedback = reflective_dataset[name]\n-\n-                    with dspy.context(lm=self.reflection_lm):\n-                        new_texts[name] = proposer(\n-                            current_instruction=current_instruction,\n-                            reflective_dataset=dataset_with_feedback\n-                        )\n-\n-                return new_texts\n+                with dspy.context(lm=self.reflection_lm):",
        "comment_created_at": "2025-09-03T01:51:42+00:00",
        "comment_author": "andressrg",
        "comment_body": "It does fix it \ud83d\ude80 \r\n\r\nNow dspy.GEPA can support reflection_lm=None",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2275001492",
    "pr_number": 8635,
    "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
    "created_at": "2025-08-14T01:14:23+00:00",
    "commented_code": "requires_permission_to_run: bool = True, # deprecated\n         provide_traceback: bool | None = None,\n     ) -> Any:\n+        if requires_permission_to_run:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2275001492",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8635,
        "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
        "discussion_id": "2275001492",
        "commented_code": "@@ -112,6 +112,10 @@ def compile(\n         requires_permission_to_run: bool = True, # deprecated\n         provide_traceback: bool | None = None,\n     ) -> Any:\n+        if requires_permission_to_run:",
        "comment_created_at": "2025-08-14T01:14:23+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "this will result in warning in the default path, shall we flip the default value to False? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2275203119",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8635,
        "pr_file": "dspy/teleprompt/mipro_optimizer_v2.py",
        "discussion_id": "2275001492",
        "commented_code": "@@ -112,6 +112,10 @@ def compile(\n         requires_permission_to_run: bool = True, # deprecated\n         provide_traceback: bool | None = None,\n     ) -> Any:\n+        if requires_permission_to_run:",
        "comment_created_at": "2025-08-14T02:52:57+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Good call, let's change the default too.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2117161086",
    "pr_number": 8296,
    "pr_file": "dspy/primitives/python_interpreter.py",
    "created_at": "2025-05-31T03:40:42+00:00",
    "commented_code": "```\n     \"\"\"\n \n-    def __init__(self, deno_command: Optional[List[str]] = None) -> None:\n+    def __init__(self, deno_command: Optional[List[str]] = None, enable_read: Iterable[Union[PathLike, str]] = None, enable_env_vars: Iterable[str] = None, enable_network_access: Iterable[str] = None, enable_write: Iterable[Union[PathLike, str]] = None) -> None:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2117161086",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8296,
        "pr_file": "dspy/primitives/python_interpreter.py",
        "discussion_id": "2117161086",
        "commented_code": "@@ -24,16 +25,79 @@ class PythonInterpreter:\n     ```\n     \"\"\"\n \n-    def __init__(self, deno_command: Optional[List[str]] = None) -> None:\n+    def __init__(self, deno_command: Optional[List[str]] = None, enable_read: Iterable[Union[PathLike, str]] = None, enable_env_vars: Iterable[str] = None, enable_network_access: Iterable[str] = None, enable_write: Iterable[Union[PathLike, str]] = None) -> None:",
        "comment_created_at": "2025-05-31T03:40:42+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Every new configuration should be `Optional[Iterable[...]]`.\r\nAlso, shall we change the parameter names so that they expect a list of paths?",
        "pr_file_module": null
      },
      {
        "comment_id": "2122184009",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8296,
        "pr_file": "dspy/primitives/python_interpreter.py",
        "discussion_id": "2117161086",
        "commented_code": "@@ -24,16 +25,79 @@ class PythonInterpreter:\n     ```\n     \"\"\"\n \n-    def __init__(self, deno_command: Optional[List[str]] = None) -> None:\n+    def __init__(self, deno_command: Optional[List[str]] = None, enable_read: Iterable[Union[PathLike, str]] = None, enable_env_vars: Iterable[str] = None, enable_network_access: Iterable[str] = None, enable_write: Iterable[Union[PathLike, str]] = None) -> None:",
        "comment_created_at": "2025-06-02T21:23:37+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "This is marked as resolved, but seems unresolved?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2042895524",
    "pr_number": 8057,
    "pr_file": "dspy/clients/base_lm.py",
    "created_at": "2025-04-14T20:39:53+00:00",
    "commented_code": "from dspy.dsp.utils import settings\n from dspy.utils.callback import with_callbacks\n \n+MAX_HISTORY_SIZE = 100",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2042895524",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8057,
        "pr_file": "dspy/clients/base_lm.py",
        "discussion_id": "2042895524",
        "commented_code": "@@ -5,6 +5,7 @@\n from dspy.dsp.utils import settings\n from dspy.utils.callback import with_callbacks\n \n+MAX_HISTORY_SIZE = 100",
        "comment_created_at": "2025-04-14T20:39:53+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "Shall we make this configurable for users? 100 is usually sufficient for me, but there is one time that I need to maintain a long history for trajectory lookup when building `dspy.Refine`. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2042959093",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8057,
        "pr_file": "dspy/clients/base_lm.py",
        "discussion_id": "2042895524",
        "commented_code": "@@ -5,6 +5,7 @@\n from dspy.dsp.utils import settings\n from dspy.utils.callback import with_callbacks\n \n+MAX_HISTORY_SIZE = 100",
        "comment_created_at": "2025-04-14T21:20:24+00:00",
        "comment_author": "okhat",
        "comment_body": "I agree, but will merge now so we don't accumulate too many PRs. Making this configurable is a P2 or P3...",
        "pr_file_module": null
      },
      {
        "comment_id": "2042960148",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8057,
        "pr_file": "dspy/clients/base_lm.py",
        "discussion_id": "2042895524",
        "commented_code": "@@ -5,6 +5,7 @@\n from dspy.dsp.utils import settings\n from dspy.utils.callback import with_callbacks\n \n+MAX_HISTORY_SIZE = 100",
        "comment_created_at": "2025-04-14T21:20:56+00:00",
        "comment_author": "okhat",
        "comment_body": "I also increased the default to 10_000 so that request (configurable count) is much less urgent now",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1816503177",
    "pr_number": 1186,
    "pr_file": "dsp/modules/azure_openai.py",
    "created_at": "2024-10-25T11:18:55+00:00",
    "commented_code": "from typing import Any, Callable, Literal, Optional, cast\n \n import backoff\n-import openai\n+\n+try:\n+    \"\"\"\n+    If there is any error in the langfuse configuration, it will turn to request the real address(openai or azure endpoint)\n+    \"\"\"\n+    import langfuse\n+    from langfuse.openai import openai",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1816503177",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1186,
        "pr_file": "dsp/modules/azure_openai.py",
        "discussion_id": "1816503177",
        "commented_code": "@@ -4,7 +4,16 @@\n from typing import Any, Callable, Literal, Optional, cast\n \n import backoff\n-import openai\n+\n+try:\n+    \"\"\"\n+    If there is any error in the langfuse configuration, it will turn to request the real address(openai or azure endpoint)\n+    \"\"\"\n+    import langfuse\n+    from langfuse.openai import openai",
        "comment_created_at": "2024-10-25T11:18:55+00:00",
        "comment_author": "bettlebrox",
        "comment_body": "I believe this causes dspy to use the langfuse openai wrapper if langfuse is simply installed, even if env variables are not configured. \r\n\r\nConsidering langfuse also have a [decorator integration method](https://langfuse.com/docs/sdk/python/decorators) I think the wrapper should only be used if langfuse is configured via env variables or offer some other flag to switch of the use of the langfuse openai wrapper",
        "pr_file_module": null
      }
    ]
  }
]