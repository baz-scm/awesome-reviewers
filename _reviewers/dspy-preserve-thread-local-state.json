[
  {
    "discussion_id": "1800399650",
    "pr_number": 1622,
    "pr_file": "dspy/teleprompt/utils.py",
    "created_at": "2024-10-15T03:30:09+00:00",
    "commented_code": "student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1800399650",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1622,
        "pr_file": "dspy/teleprompt/utils.py",
        "discussion_id": "1800399650",
        "commented_code": "@@ -352,8 +353,44 @@ def create_n_fewshot_demo_sets(\n                 student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
        "comment_created_at": "2024-10-15T03:30:09+00:00",
        "comment_author": "okhat",
        "comment_body": "this code is probably wrong for a very very subtle reason that probably has zero effect for @XenonMolecule 's usecase but will have for some other people\r\n\r\nLaunching threads in DSPy is tricky, because (like dspy.Evaluate does) you need to handle dspy.settings.stack_by_tid or whatever it's called ",
        "pr_file_module": null
      },
      {
        "comment_id": "1800399869",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1622,
        "pr_file": "dspy/teleprompt/utils.py",
        "discussion_id": "1800399650",
        "commented_code": "@@ -352,8 +353,44 @@ def create_n_fewshot_demo_sets(\n                 student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
        "comment_created_at": "2024-10-15T03:30:34+00:00",
        "comment_author": "okhat",
        "comment_body": "We do have a good oppty here to refactor dspy.Evaluate's thread launcher into its own reuseable parallelizer for dspy and then use it here too",
        "pr_file_module": null
      },
      {
        "comment_id": "1800406044",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1622,
        "pr_file": "dspy/teleprompt/utils.py",
        "discussion_id": "1800399650",
        "commented_code": "@@ -352,8 +353,44 @@ def create_n_fewshot_demo_sets(\n                 student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
        "comment_created_at": "2024-10-15T03:43:34+00:00",
        "comment_author": "XenonMolecule",
        "comment_body": "This probably wasn't sufficient but I used ctrl-f on the repo and used GitHub search and both times the only use of `dspy.settings.stack_by_thread` was in Evaluate.py?  Might've missed it somewhere else though",
        "pr_file_module": null
      },
      {
        "comment_id": "1801030986",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1622,
        "pr_file": "dspy/teleprompt/utils.py",
        "discussion_id": "1800399650",
        "commented_code": "@@ -352,8 +353,44 @@ def create_n_fewshot_demo_sets(\n                 student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
        "comment_created_at": "2024-10-15T12:00:40+00:00",
        "comment_author": "okhat",
        "comment_body": "Hey @XenonMolecule , I don't get the response. I'm saying we cannot launch threads with ThreadProolExecutor plainly. It's missing the important steps that dspy.Evaluate does when launching a thread. Not sure if you addressed this comment above (I think no).",
        "pr_file_module": null
      },
      {
        "comment_id": "1801707219",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1622,
        "pr_file": "dspy/teleprompt/utils.py",
        "discussion_id": "1800399650",
        "commented_code": "@@ -352,8 +353,44 @@ def create_n_fewshot_demo_sets(\n                 student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
        "comment_created_at": "2024-10-15T18:23:05+00:00",
        "comment_author": "XenonMolecule",
        "comment_body": "No I don't think I addressed this comment fully, my point was that I wasn't sure where else `dspy.settings.stack_by_thread` was being used, so I was unsure of the expected behavior for this setting.  It looked like it was only used in Evaluate.py and nowhere else in the repo.  Maybe we can chat offline about how `dspy.settings.stack_by_thread` needs to be handled.",
        "pr_file_module": null
      },
      {
        "comment_id": "1801838122",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1622,
        "pr_file": "dspy/teleprompt/utils.py",
        "discussion_id": "1800399650",
        "commented_code": "@@ -352,8 +353,44 @@ def create_n_fewshot_demo_sets(\n                 student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
        "comment_created_at": "2024-10-15T20:03:09+00:00",
        "comment_author": "isaacbmiller",
        "comment_body": "This will very subtly ruin strange things if not handled correctly. Speaking from experience but don't let this one fall to the wayside.",
        "pr_file_module": null
      },
      {
        "comment_id": "1803380151",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1622,
        "pr_file": "dspy/teleprompt/utils.py",
        "discussion_id": "1800399650",
        "commented_code": "@@ -352,8 +353,44 @@ def create_n_fewshot_demo_sets(\n                 student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
        "comment_created_at": "2024-10-16T15:48:18+00:00",
        "comment_author": "okhat",
        "comment_body": "Yes, let's make sure we handle threading correctly.",
        "pr_file_module": null
      },
      {
        "comment_id": "1823882013",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1622,
        "pr_file": "dspy/teleprompt/utils.py",
        "discussion_id": "1800399650",
        "commented_code": "@@ -352,8 +353,44 @@ def create_n_fewshot_demo_sets(\n                 student, teacher=teacher, trainset=trainset_copy,\n             )\n \n+        # Collect the demos for each predictor\n+        predictor_demos = {}\n         for i, _ in enumerate(student.predictors()):\n-            demo_candidates[i].append(program2.predictors()[i].demos)\n+            predictor_demos[i] = program2.predictors()[i].demos\n+\n+        return seed, predictor_demos\n+\n+    if parallel_bootstrapping:\n+        # Use ThreadPoolExecutor to parallelize the creation of candidate sets\n+        num_threads = total_candidate_sets  # As per your request\n+\n+        # Dictionary to store the results in order\n+        seed_to_result = {}\n+\n+        with ThreadPoolExecutor(max_workers=num_threads) as executor:",
        "comment_created_at": "2024-10-31T06:50:19+00:00",
        "comment_author": "dilarasoylu",
        "comment_body": "> We do have a good oppty here to refactor dspy.Evaluate's thread launcher into its own reuseable parallelizer for dspy and then use it here too\r\n\r\nThis would be nice (not sure if it should be a part of this PR) Moreover, it would be even nicer if all the logic for handling various control signals can be handled by this parallelizer, ensuring that we don't deal with them every time we make use of parallelism in DSPy.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1912465391",
    "pr_number": 2031,
    "pr_file": "dspy/utils/parallelizer.py",
    "created_at": "2025-01-12T14:33:29+00:00",
    "commented_code": "self.compare_results = compare_results\n \n         self.error_count = 0\n-        self.error_lock = threading.Lock()\n-        self.cancel_jobs = threading.Event()\n+        self._lock = threading.Lock()\n \n     def execute(self, function, data):\n         wrapped_function = self._wrap_function(function)\n-        if self.num_threads == 1:\n-            return self._execute_isolated_single_thread(wrapped_function, data)\n-        else:\n-            return self._execute_multi_thread(wrapped_function, data)\n+        exec_type = \"multi\" if self.num_threads != 1 else \"single\"\n+        executor = getattr(self, f\"_execute_{exec_type}_thread\")\n+        return executor(wrapped_function, data)\n \n     def _wrap_function(self, function):\n+        from dspy.dsp.utils.settings import thread_local_overrides\n+\n         # Wrap the function with error handling\n         def wrapped(item):\n-            if self.cancel_jobs.is_set():\n-                return None\n+            original_overrides = thread_local_overrides.overrides\n+            thread_local_overrides.overrides = thread_local_overrides.overrides.copy()\n             try:\n                 return function(item)\n             except Exception as e:\n-                with self.error_lock:\n-                    self.error_count += 1\n-                    current_error_count = self.error_count\n-                if current_error_count >= self.max_errors:\n-                    self.cancel_jobs.set()\n-                    raise e\n                 if self.provide_traceback:\n-                    logger.error(\n-                        f\"Error processing item {item}: {e}\nStack trace:\n{traceback.format_exc()}\"\n-                    )\n+                    logger.error(f\"Error processing item {item}: {e}\nStack trace:\n{traceback.format_exc()}\")\n                 else:\n                     logger.error(\n                         f\"Error processing item {item}: {e}. Set `provide_traceback=True` to see the stack trace.\"\n                     )\n+                with self._lock:\n+                    self.error_count += 1\n+                    if self.error_count >= self.max_errors:\n+                        raise e\n                 return None\n+            finally:\n+                thread_local_overrides.overrides = original_overrides\n+\n         return wrapped\n \n-    def _execute_isolated_single_thread(self, function, data):\n-        results = []\n-        pbar = tqdm.tqdm(\n-            total=len(data),\n-            dynamic_ncols=True,\n-            disable=self.disable_progress_bar,\n-            file=sys.stdout\n-        )\n+    def _create_pbar(self, data: list):\n+        return tqdm(total=len(data), dynamic_ncols=True, disable=self.disable_progress_bar, file=sys.stdout)\n \n-        from dspy.dsp.utils.settings import thread_local_overrides\n-        original_overrides = thread_local_overrides.overrides\n-\n-        for item in data:\n-            with logging_redirect_tqdm():\n-                if self.cancel_jobs.is_set():\n-                    break\n+    def _update_pbar(self, pbar: tqdm, nresults, ntotal):\n+        if self.compare_results:\n+            percentage = round(100 * nresults / ntotal, 1) if ntotal > 0 else 0\n+            description = f\"Average Metric: {nresults:.2f} / {ntotal} ({percentage}%)\"\n+        else:\n+            description = f\"Processed {nresults} / {ntotal} examples\"\n+        pbar.set_description(description, refresh=True)\n \n-                # Create an isolated context for each task by copying current overrides\n-                # This way, even if an iteration modifies the overrides, it won't affect subsequent iterations\n-                thread_local_overrides.overrides = original_overrides.copy()\n+    def _execute_single_thread(self, function, data):\n+        pbar = self._create_pbar(data)\n+        total_score = 0\n+        total_processed = 0\n \n-                try:\n-                    result = function(item)\n-                    results.append(result)\n-                finally:\n-                    thread_local_overrides.overrides = original_overrides\n+        def function_with_progress(item):\n+            result = function(item)\n \n+            with self._lock:\n+                nonlocal total_score, total_processed, pbar\n+                total_processed += 1\n                 if self.compare_results:\n-                    # Assumes score is the last element of the result tuple\n-                    self._update_progress(\n-                        pbar,\n-                        sum([r[-1] for r in results if r is not None]),\n-                        len([r for r in data if r is not None]),\n-                    )\n+                    if result is not None:\n+                        total_score += result[-1]\n+                    self._update_pbar(pbar, total_score, total_processed)\n                 else:\n-                    self._update_progress(pbar, len(results), len(data))\n+                    self._update_pbar(pbar, total_processed, len(data))\n \n-        pbar.close()\n-\n-        if self.cancel_jobs.is_set():\n-            logger.warning(\"Execution was cancelled due to errors.\")\n-            raise Exception(\"Execution was cancelled due to errors.\")\n-\n-        return results\n-\n-    def _update_progress(self, pbar, nresults, ntotal):\n-        if self.compare_results:\n-            percentage = round(100 * nresults / ntotal, 1) if ntotal > 0 else 0\n-            pbar.set_description(f\"Average Metric: {nresults:.2f} / {ntotal} ({percentage}%)\")\n-        else:\n-            pbar.set_description(f\"Processed {nresults} / {ntotal} examples\")\n-\n-        pbar.update()\n-\n-    def _execute_multi_thread(self, function, data):\n-        results = [None] * len(data)  # Pre-allocate results list to maintain order\n-        job_cancelled = \"cancelled\"\n-\n-        @contextlib.contextmanager\n-        def interrupt_handler_manager():\n-            \"\"\"Sets the cancel_jobs event when a SIGINT is received, only in the main thread.\"\"\"\n-\n-            # TODO: Is this check conducive to nested usage of ParallelExecutor?\n-            if threading.current_thread() is threading.main_thread():\n-                default_handler = signal.getsignal(signal.SIGINT)\n-\n-                def interrupt_handler(sig, frame):\n-                    self.cancel_jobs.set()\n-                    logger.warning(\"Received SIGINT. Cancelling execution.\")\n-                    # Re-raise the signal to allow default behavior\n-                    default_handler(sig, frame)\n-\n-                signal.signal(signal.SIGINT, interrupt_handler)\n-                try:\n-                    yield\n-                finally:\n-                    signal.signal(signal.SIGINT, default_handler)\n-            else:\n-                # If not in the main thread, skip setting signal handlers\n-                yield\n-\n-        def cancellable_function(parent_overrides, index_item):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1912465391",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 2031,
        "pr_file": "dspy/utils/parallelizer.py",
        "discussion_id": "1912465391",
        "commented_code": "@@ -28,180 +29,102 @@ def __init__(\n         self.compare_results = compare_results\n \n         self.error_count = 0\n-        self.error_lock = threading.Lock()\n-        self.cancel_jobs = threading.Event()\n+        self._lock = threading.Lock()\n \n     def execute(self, function, data):\n         wrapped_function = self._wrap_function(function)\n-        if self.num_threads == 1:\n-            return self._execute_isolated_single_thread(wrapped_function, data)\n-        else:\n-            return self._execute_multi_thread(wrapped_function, data)\n+        exec_type = \"multi\" if self.num_threads != 1 else \"single\"\n+        executor = getattr(self, f\"_execute_{exec_type}_thread\")\n+        return executor(wrapped_function, data)\n \n     def _wrap_function(self, function):\n+        from dspy.dsp.utils.settings import thread_local_overrides\n+\n         # Wrap the function with error handling\n         def wrapped(item):\n-            if self.cancel_jobs.is_set():\n-                return None\n+            original_overrides = thread_local_overrides.overrides\n+            thread_local_overrides.overrides = thread_local_overrides.overrides.copy()\n             try:\n                 return function(item)\n             except Exception as e:\n-                with self.error_lock:\n-                    self.error_count += 1\n-                    current_error_count = self.error_count\n-                if current_error_count >= self.max_errors:\n-                    self.cancel_jobs.set()\n-                    raise e\n                 if self.provide_traceback:\n-                    logger.error(\n-                        f\"Error processing item {item}: {e}\\nStack trace:\\n{traceback.format_exc()}\"\n-                    )\n+                    logger.error(f\"Error processing item {item}: {e}\\nStack trace:\\n{traceback.format_exc()}\")\n                 else:\n                     logger.error(\n                         f\"Error processing item {item}: {e}. Set `provide_traceback=True` to see the stack trace.\"\n                     )\n+                with self._lock:\n+                    self.error_count += 1\n+                    if self.error_count >= self.max_errors:\n+                        raise e\n                 return None\n+            finally:\n+                thread_local_overrides.overrides = original_overrides\n+\n         return wrapped\n \n-    def _execute_isolated_single_thread(self, function, data):\n-        results = []\n-        pbar = tqdm.tqdm(\n-            total=len(data),\n-            dynamic_ncols=True,\n-            disable=self.disable_progress_bar,\n-            file=sys.stdout\n-        )\n+    def _create_pbar(self, data: list):\n+        return tqdm(total=len(data), dynamic_ncols=True, disable=self.disable_progress_bar, file=sys.stdout)\n \n-        from dspy.dsp.utils.settings import thread_local_overrides\n-        original_overrides = thread_local_overrides.overrides\n-\n-        for item in data:\n-            with logging_redirect_tqdm():\n-                if self.cancel_jobs.is_set():\n-                    break\n+    def _update_pbar(self, pbar: tqdm, nresults, ntotal):\n+        if self.compare_results:\n+            percentage = round(100 * nresults / ntotal, 1) if ntotal > 0 else 0\n+            description = f\"Average Metric: {nresults:.2f} / {ntotal} ({percentage}%)\"\n+        else:\n+            description = f\"Processed {nresults} / {ntotal} examples\"\n+        pbar.set_description(description, refresh=True)\n \n-                # Create an isolated context for each task by copying current overrides\n-                # This way, even if an iteration modifies the overrides, it won't affect subsequent iterations\n-                thread_local_overrides.overrides = original_overrides.copy()\n+    def _execute_single_thread(self, function, data):\n+        pbar = self._create_pbar(data)\n+        total_score = 0\n+        total_processed = 0\n \n-                try:\n-                    result = function(item)\n-                    results.append(result)\n-                finally:\n-                    thread_local_overrides.overrides = original_overrides\n+        def function_with_progress(item):\n+            result = function(item)\n \n+            with self._lock:\n+                nonlocal total_score, total_processed, pbar\n+                total_processed += 1\n                 if self.compare_results:\n-                    # Assumes score is the last element of the result tuple\n-                    self._update_progress(\n-                        pbar,\n-                        sum([r[-1] for r in results if r is not None]),\n-                        len([r for r in data if r is not None]),\n-                    )\n+                    if result is not None:\n+                        total_score += result[-1]\n+                    self._update_pbar(pbar, total_score, total_processed)\n                 else:\n-                    self._update_progress(pbar, len(results), len(data))\n+                    self._update_pbar(pbar, total_processed, len(data))\n \n-        pbar.close()\n-\n-        if self.cancel_jobs.is_set():\n-            logger.warning(\"Execution was cancelled due to errors.\")\n-            raise Exception(\"Execution was cancelled due to errors.\")\n-\n-        return results\n-\n-    def _update_progress(self, pbar, nresults, ntotal):\n-        if self.compare_results:\n-            percentage = round(100 * nresults / ntotal, 1) if ntotal > 0 else 0\n-            pbar.set_description(f\"Average Metric: {nresults:.2f} / {ntotal} ({percentage}%)\")\n-        else:\n-            pbar.set_description(f\"Processed {nresults} / {ntotal} examples\")\n-\n-        pbar.update()\n-\n-    def _execute_multi_thread(self, function, data):\n-        results = [None] * len(data)  # Pre-allocate results list to maintain order\n-        job_cancelled = \"cancelled\"\n-\n-        @contextlib.contextmanager\n-        def interrupt_handler_manager():\n-            \"\"\"Sets the cancel_jobs event when a SIGINT is received, only in the main thread.\"\"\"\n-\n-            # TODO: Is this check conducive to nested usage of ParallelExecutor?\n-            if threading.current_thread() is threading.main_thread():\n-                default_handler = signal.getsignal(signal.SIGINT)\n-\n-                def interrupt_handler(sig, frame):\n-                    self.cancel_jobs.set()\n-                    logger.warning(\"Received SIGINT. Cancelling execution.\")\n-                    # Re-raise the signal to allow default behavior\n-                    default_handler(sig, frame)\n-\n-                signal.signal(signal.SIGINT, interrupt_handler)\n-                try:\n-                    yield\n-                finally:\n-                    signal.signal(signal.SIGINT, default_handler)\n-            else:\n-                # If not in the main thread, skip setting signal handlers\n-                yield\n-\n-        def cancellable_function(parent_overrides, index_item):",
        "comment_created_at": "2025-01-12T14:33:29+00:00",
        "comment_author": "okhat",
        "comment_body": "If I recall correctly, this was really important. It seems to have gotten lost in the (otherwise extremely neat) refactor.\r\n\r\nWhen launching multiple threads, we want each thread to inherit the *parent* thread's local overrides.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1912466320",
    "pr_number": 2031,
    "pr_file": "dspy/utils/parallelizer.py",
    "created_at": "2025-01-12T14:38:31+00:00",
    "commented_code": "self.compare_results = compare_results\n \n         self.error_count = 0\n-        self.error_lock = threading.Lock()\n-        self.cancel_jobs = threading.Event()\n+        self._lock = threading.Lock()\n \n     def execute(self, function, data):\n         wrapped_function = self._wrap_function(function)\n-        if self.num_threads == 1:\n-            return self._execute_isolated_single_thread(wrapped_function, data)\n-        else:\n-            return self._execute_multi_thread(wrapped_function, data)\n+        exec_type = \"multi\" if self.num_threads != 1 else \"single\"\n+        executor = getattr(self, f\"_execute_{exec_type}_thread\")\n+        return executor(wrapped_function, data)\n \n     def _wrap_function(self, function):\n+        from dspy.dsp.utils.settings import thread_local_overrides\n+\n         # Wrap the function with error handling\n         def wrapped(item):\n-            if self.cancel_jobs.is_set():\n-                return None\n+            original_overrides = thread_local_overrides.overrides\n+            thread_local_overrides.overrides = thread_local_overrides.overrides.copy()\n             try:\n                 return function(item)\n             except Exception as e:\n-                with self.error_lock:\n-                    self.error_count += 1\n-                    current_error_count = self.error_count\n-                if current_error_count >= self.max_errors:\n-                    self.cancel_jobs.set()\n-                    raise e\n                 if self.provide_traceback:\n-                    logger.error(\n-                        f\"Error processing item {item}: {e}\nStack trace:\n{traceback.format_exc()}\"\n-                    )\n+                    logger.error(f\"Error processing item {item}: {e}\nStack trace:\n{traceback.format_exc()}\")\n                 else:\n                     logger.error(\n                         f\"Error processing item {item}: {e}. Set `provide_traceback=True` to see the stack trace.\"\n                     )\n+                with self._lock:\n+                    self.error_count += 1\n+                    if self.error_count >= self.max_errors:\n+                        raise e\n                 return None\n+            finally:\n+                thread_local_overrides.overrides = original_overrides\n+\n         return wrapped\n \n-    def _execute_isolated_single_thread(self, function, data):\n-        results = []\n-        pbar = tqdm.tqdm(\n-            total=len(data),\n-            dynamic_ncols=True,\n-            disable=self.disable_progress_bar,\n-            file=sys.stdout\n-        )\n+    def _create_pbar(self, data: list):\n+        return tqdm(total=len(data), dynamic_ncols=True, disable=self.disable_progress_bar, file=sys.stdout)\n \n-        from dspy.dsp.utils.settings import thread_local_overrides\n-        original_overrides = thread_local_overrides.overrides\n-\n-        for item in data:\n-            with logging_redirect_tqdm():\n-                if self.cancel_jobs.is_set():\n-                    break\n+    def _update_pbar(self, pbar: tqdm, nresults, ntotal):\n+        if self.compare_results:\n+            percentage = round(100 * nresults / ntotal, 1) if ntotal > 0 else 0\n+            description = f\"Average Metric: {nresults:.2f} / {ntotal} ({percentage}%)\"\n+        else:\n+            description = f\"Processed {nresults} / {ntotal} examples\"\n+        pbar.set_description(description, refresh=True)\n \n-                # Create an isolated context for each task by copying current overrides\n-                # This way, even if an iteration modifies the overrides, it won't affect subsequent iterations\n-                thread_local_overrides.overrides = original_overrides.copy()\n+    def _execute_single_thread(self, function, data):\n+        pbar = self._create_pbar(data)\n+        total_score = 0\n+        total_processed = 0\n \n-                try:\n-                    result = function(item)\n-                    results.append(result)\n-                finally:\n-                    thread_local_overrides.overrides = original_overrides\n+        def function_with_progress(item):\n+            result = function(item)\n \n+            with self._lock:\n+                nonlocal total_score, total_processed, pbar\n+                total_processed += 1\n                 if self.compare_results:\n-                    # Assumes score is the last element of the result tuple\n-                    self._update_progress(\n-                        pbar,\n-                        sum([r[-1] for r in results if r is not None]),\n-                        len([r for r in data if r is not None]),\n-                    )\n+                    if result is not None:\n+                        total_score += result[-1]\n+                    self._update_pbar(pbar, total_score, total_processed)\n                 else:\n-                    self._update_progress(pbar, len(results), len(data))\n+                    self._update_pbar(pbar, total_processed, len(data))\n \n-        pbar.close()\n-\n-        if self.cancel_jobs.is_set():\n-            logger.warning(\"Execution was cancelled due to errors.\")\n-            raise Exception(\"Execution was cancelled due to errors.\")\n-\n-        return results\n-\n-    def _update_progress(self, pbar, nresults, ntotal):\n-        if self.compare_results:\n-            percentage = round(100 * nresults / ntotal, 1) if ntotal > 0 else 0\n-            pbar.set_description(f\"Average Metric: {nresults:.2f} / {ntotal} ({percentage}%)\")\n-        else:\n-            pbar.set_description(f\"Processed {nresults} / {ntotal} examples\")\n-\n-        pbar.update()\n-\n-    def _execute_multi_thread(self, function, data):\n-        results = [None] * len(data)  # Pre-allocate results list to maintain order\n-        job_cancelled = \"cancelled\"\n-\n-        @contextlib.contextmanager\n-        def interrupt_handler_manager():\n-            \"\"\"Sets the cancel_jobs event when a SIGINT is received, only in the main thread.\"\"\"\n-\n-            # TODO: Is this check conducive to nested usage of ParallelExecutor?\n-            if threading.current_thread() is threading.main_thread():\n-                default_handler = signal.getsignal(signal.SIGINT)\n-\n-                def interrupt_handler(sig, frame):\n-                    self.cancel_jobs.set()\n-                    logger.warning(\"Received SIGINT. Cancelling execution.\")\n-                    # Re-raise the signal to allow default behavior\n-                    default_handler(sig, frame)\n-\n-                signal.signal(signal.SIGINT, interrupt_handler)\n-                try:\n-                    yield\n-                finally:\n-                    signal.signal(signal.SIGINT, default_handler)\n-            else:\n-                # If not in the main thread, skip setting signal handlers\n-                yield\n-\n-        def cancellable_function(parent_overrides, index_item):\n-            index, item = index_item\n-            if self.cancel_jobs.is_set():\n-                return index, job_cancelled\n-\n-            # Create an isolated context for each task by copying parent's overrides\n-            from dspy.dsp.utils.settings import thread_local_overrides\n-            original_overrides = thread_local_overrides.overrides\n-            thread_local_overrides.overrides = parent_overrides.copy()\n+            return result\n \n+        with logging_redirect_tqdm():\n             try:\n-                return index, function(item)\n+                return list(map(function_with_progress, data))\n             finally:\n-                thread_local_overrides.overrides = original_overrides\n+                pbar.close()\n \n-        with ThreadPoolExecutor(max_workers=self.num_threads) as executor, interrupt_handler_manager():\n-            from dspy.dsp.utils.settings import thread_local_overrides\n-            parent_overrides = thread_local_overrides.overrides.copy()\n-\n-            futures = {}\n-            for pair in enumerate(data):\n-                # Pass the parent thread's overrides to each thread\n-                future = executor.submit(cancellable_function, parent_overrides, pair)\n-                futures[future] = pair\n-\n-            pbar = tqdm.tqdm(\n-                total=len(data),\n-                dynamic_ncols=True,\n-                disable=self.disable_progress_bar,\n-                file=sys.stdout\n-            )\n-\n-            for future in as_completed(futures):\n-                index, result = future.result()\n-\n-                if result is job_cancelled:\n-                    continue\n+    def _execute_multi_thread(self, function, data):\n+        pbar = self._create_pbar(data)\n+        total_score = 0\n+        total_processed = 0",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1912466320",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 2031,
        "pr_file": "dspy/utils/parallelizer.py",
        "discussion_id": "1912466320",
        "commented_code": "@@ -28,180 +29,102 @@ def __init__(\n         self.compare_results = compare_results\n \n         self.error_count = 0\n-        self.error_lock = threading.Lock()\n-        self.cancel_jobs = threading.Event()\n+        self._lock = threading.Lock()\n \n     def execute(self, function, data):\n         wrapped_function = self._wrap_function(function)\n-        if self.num_threads == 1:\n-            return self._execute_isolated_single_thread(wrapped_function, data)\n-        else:\n-            return self._execute_multi_thread(wrapped_function, data)\n+        exec_type = \"multi\" if self.num_threads != 1 else \"single\"\n+        executor = getattr(self, f\"_execute_{exec_type}_thread\")\n+        return executor(wrapped_function, data)\n \n     def _wrap_function(self, function):\n+        from dspy.dsp.utils.settings import thread_local_overrides\n+\n         # Wrap the function with error handling\n         def wrapped(item):\n-            if self.cancel_jobs.is_set():\n-                return None\n+            original_overrides = thread_local_overrides.overrides\n+            thread_local_overrides.overrides = thread_local_overrides.overrides.copy()\n             try:\n                 return function(item)\n             except Exception as e:\n-                with self.error_lock:\n-                    self.error_count += 1\n-                    current_error_count = self.error_count\n-                if current_error_count >= self.max_errors:\n-                    self.cancel_jobs.set()\n-                    raise e\n                 if self.provide_traceback:\n-                    logger.error(\n-                        f\"Error processing item {item}: {e}\\nStack trace:\\n{traceback.format_exc()}\"\n-                    )\n+                    logger.error(f\"Error processing item {item}: {e}\\nStack trace:\\n{traceback.format_exc()}\")\n                 else:\n                     logger.error(\n                         f\"Error processing item {item}: {e}. Set `provide_traceback=True` to see the stack trace.\"\n                     )\n+                with self._lock:\n+                    self.error_count += 1\n+                    if self.error_count >= self.max_errors:\n+                        raise e\n                 return None\n+            finally:\n+                thread_local_overrides.overrides = original_overrides\n+\n         return wrapped\n \n-    def _execute_isolated_single_thread(self, function, data):\n-        results = []\n-        pbar = tqdm.tqdm(\n-            total=len(data),\n-            dynamic_ncols=True,\n-            disable=self.disable_progress_bar,\n-            file=sys.stdout\n-        )\n+    def _create_pbar(self, data: list):\n+        return tqdm(total=len(data), dynamic_ncols=True, disable=self.disable_progress_bar, file=sys.stdout)\n \n-        from dspy.dsp.utils.settings import thread_local_overrides\n-        original_overrides = thread_local_overrides.overrides\n-\n-        for item in data:\n-            with logging_redirect_tqdm():\n-                if self.cancel_jobs.is_set():\n-                    break\n+    def _update_pbar(self, pbar: tqdm, nresults, ntotal):\n+        if self.compare_results:\n+            percentage = round(100 * nresults / ntotal, 1) if ntotal > 0 else 0\n+            description = f\"Average Metric: {nresults:.2f} / {ntotal} ({percentage}%)\"\n+        else:\n+            description = f\"Processed {nresults} / {ntotal} examples\"\n+        pbar.set_description(description, refresh=True)\n \n-                # Create an isolated context for each task by copying current overrides\n-                # This way, even if an iteration modifies the overrides, it won't affect subsequent iterations\n-                thread_local_overrides.overrides = original_overrides.copy()\n+    def _execute_single_thread(self, function, data):\n+        pbar = self._create_pbar(data)\n+        total_score = 0\n+        total_processed = 0\n \n-                try:\n-                    result = function(item)\n-                    results.append(result)\n-                finally:\n-                    thread_local_overrides.overrides = original_overrides\n+        def function_with_progress(item):\n+            result = function(item)\n \n+            with self._lock:\n+                nonlocal total_score, total_processed, pbar\n+                total_processed += 1\n                 if self.compare_results:\n-                    # Assumes score is the last element of the result tuple\n-                    self._update_progress(\n-                        pbar,\n-                        sum([r[-1] for r in results if r is not None]),\n-                        len([r for r in data if r is not None]),\n-                    )\n+                    if result is not None:\n+                        total_score += result[-1]\n+                    self._update_pbar(pbar, total_score, total_processed)\n                 else:\n-                    self._update_progress(pbar, len(results), len(data))\n+                    self._update_pbar(pbar, total_processed, len(data))\n \n-        pbar.close()\n-\n-        if self.cancel_jobs.is_set():\n-            logger.warning(\"Execution was cancelled due to errors.\")\n-            raise Exception(\"Execution was cancelled due to errors.\")\n-\n-        return results\n-\n-    def _update_progress(self, pbar, nresults, ntotal):\n-        if self.compare_results:\n-            percentage = round(100 * nresults / ntotal, 1) if ntotal > 0 else 0\n-            pbar.set_description(f\"Average Metric: {nresults:.2f} / {ntotal} ({percentage}%)\")\n-        else:\n-            pbar.set_description(f\"Processed {nresults} / {ntotal} examples\")\n-\n-        pbar.update()\n-\n-    def _execute_multi_thread(self, function, data):\n-        results = [None] * len(data)  # Pre-allocate results list to maintain order\n-        job_cancelled = \"cancelled\"\n-\n-        @contextlib.contextmanager\n-        def interrupt_handler_manager():\n-            \"\"\"Sets the cancel_jobs event when a SIGINT is received, only in the main thread.\"\"\"\n-\n-            # TODO: Is this check conducive to nested usage of ParallelExecutor?\n-            if threading.current_thread() is threading.main_thread():\n-                default_handler = signal.getsignal(signal.SIGINT)\n-\n-                def interrupt_handler(sig, frame):\n-                    self.cancel_jobs.set()\n-                    logger.warning(\"Received SIGINT. Cancelling execution.\")\n-                    # Re-raise the signal to allow default behavior\n-                    default_handler(sig, frame)\n-\n-                signal.signal(signal.SIGINT, interrupt_handler)\n-                try:\n-                    yield\n-                finally:\n-                    signal.signal(signal.SIGINT, default_handler)\n-            else:\n-                # If not in the main thread, skip setting signal handlers\n-                yield\n-\n-        def cancellable_function(parent_overrides, index_item):\n-            index, item = index_item\n-            if self.cancel_jobs.is_set():\n-                return index, job_cancelled\n-\n-            # Create an isolated context for each task by copying parent's overrides\n-            from dspy.dsp.utils.settings import thread_local_overrides\n-            original_overrides = thread_local_overrides.overrides\n-            thread_local_overrides.overrides = parent_overrides.copy()\n+            return result\n \n+        with logging_redirect_tqdm():\n             try:\n-                return index, function(item)\n+                return list(map(function_with_progress, data))\n             finally:\n-                thread_local_overrides.overrides = original_overrides\n+                pbar.close()\n \n-        with ThreadPoolExecutor(max_workers=self.num_threads) as executor, interrupt_handler_manager():\n-            from dspy.dsp.utils.settings import thread_local_overrides\n-            parent_overrides = thread_local_overrides.overrides.copy()\n-\n-            futures = {}\n-            for pair in enumerate(data):\n-                # Pass the parent thread's overrides to each thread\n-                future = executor.submit(cancellable_function, parent_overrides, pair)\n-                futures[future] = pair\n-\n-            pbar = tqdm.tqdm(\n-                total=len(data),\n-                dynamic_ncols=True,\n-                disable=self.disable_progress_bar,\n-                file=sys.stdout\n-            )\n-\n-            for future in as_completed(futures):\n-                index, result = future.result()\n-\n-                if result is job_cancelled:\n-                    continue\n+    def _execute_multi_thread(self, function, data):\n+        pbar = self._create_pbar(data)\n+        total_score = 0\n+        total_processed = 0",
        "comment_created_at": "2025-01-12T14:38:31+00:00",
        "comment_author": "okhat",
        "comment_body": "Somewhere here, we'd need to have something like:\r\n\r\n```python\r\nfrom dspy.dsp.utils.settings import thread_local_overrides\r\nparent_overrides = thread_local_overrides.overrides.copy()\r\n```\r\n\r\nand then we should pass `parent_overrides` in `data`, so that `wrapped(item)` can handle using the *parent* thread's overrides, not the new child's overrides.",
        "pr_file_module": null
      }
    ]
  }
]