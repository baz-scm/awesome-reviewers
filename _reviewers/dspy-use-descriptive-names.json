[
  {
    "discussion_id": "2038224270",
    "pr_number": 8050,
    "pr_file": "dspy/dsp/utils/settings.py",
    "created_at": "2025-04-10T20:00:03+00:00",
    "commented_code": "disable_history=False,\n     track_usage=False,\n     usage_tracker=None,\n+    adapter_retry_count=0, # number of times to retry the adapter call",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2038224270",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8050,
        "pr_file": "dspy/dsp/utils/settings.py",
        "discussion_id": "2038224270",
        "commented_code": "@@ -22,6 +22,7 @@\n     disable_history=False,\n     track_usage=False,\n     usage_tracker=None,\n+    adapter_retry_count=0, # number of times to retry the adapter call",
        "comment_created_at": "2025-04-10T20:00:03+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "For thinking: \r\n\r\n- Shall we just call this `max_retry`? `adapter_retry_count` could be mysterious for DSPy beginners. \r\n- Shall we default to 1 instead of 0? It would be helpful to collect data on the success ratio of 1 retry, 2 retries when the first call fails. A small model like llama-3b might provide useful data for us. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2038740344",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8050,
        "pr_file": "dspy/dsp/utils/settings.py",
        "discussion_id": "2038224270",
        "commented_code": "@@ -22,6 +22,7 @@\n     disable_history=False,\n     track_usage=False,\n     usage_tracker=None,\n+    adapter_retry_count=0, # number of times to retry the adapter call",
        "comment_created_at": "2025-04-11T03:24:26+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "We have num_retries in LM. So I think it's confusing if we can use retry without any prefix. Maybe `max_parse_retries`?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2045302370",
    "pr_number": 8050,
    "pr_file": "dspy/adapters/retry_adapter.py",
    "created_at": "2025-04-15T19:20:13+00:00",
    "commented_code": "+\n+from typing import TYPE_CHECKING, Any, Optional, Type\n+import logging\n+\n+from dspy.adapters.base import Adapter\n+from dspy.signatures.signature import Signature\n+from dspy.adapters.utils import create_signature_for_retry\n+\n+if TYPE_CHECKING:\n+    from dspy.clients.lm import LM\n+\n+logger = logging.getLogger(__name__)\n+\n+class RetryAdapter(Adapter):\n+    \"\"\"\n+    RetryAdapter is an adapter that retries the execution of another adapter for\n+    a specified number of times if it fails to parse completion outputs.\n+    \"\"\"\n+\n+    def __init__(self, main_adapter: Adapter, fallback_adapter: Optional[Adapter] = None, max_retries: int = 3):\n+        \"\"\"\n+        Initializes the RetryAdapter.\n+\n+        Args:\n+            main_adapter (Adapter): The main adapter to use.\n+            fallback_adapter (Optional[Adapter]): The fallback adapter to use if the main adapter fails.\n+            max_retries (int): The maximum number of retries. Defaults to 3.",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2045302370",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8050,
        "pr_file": "dspy/adapters/retry_adapter.py",
        "discussion_id": "2045302370",
        "commented_code": "@@ -0,0 +1,145 @@\n+\n+from typing import TYPE_CHECKING, Any, Optional, Type\n+import logging\n+\n+from dspy.adapters.base import Adapter\n+from dspy.signatures.signature import Signature\n+from dspy.adapters.utils import create_signature_for_retry\n+\n+if TYPE_CHECKING:\n+    from dspy.clients.lm import LM\n+\n+logger = logging.getLogger(__name__)\n+\n+class RetryAdapter(Adapter):\n+    \"\"\"\n+    RetryAdapter is an adapter that retries the execution of another adapter for\n+    a specified number of times if it fails to parse completion outputs.\n+    \"\"\"\n+\n+    def __init__(self, main_adapter: Adapter, fallback_adapter: Optional[Adapter] = None, max_retries: int = 3):\n+        \"\"\"\n+        Initializes the RetryAdapter.\n+\n+        Args:\n+            main_adapter (Adapter): The main adapter to use.\n+            fallback_adapter (Optional[Adapter]): The fallback adapter to use if the main adapter fails.\n+            max_retries (int): The maximum number of retries. Defaults to 3.",
        "comment_created_at": "2025-04-15T19:20:13+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "shall we name it to `main_adapter_max_retries` for clarity? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2176079676",
    "pr_number": 8475,
    "pr_file": "tests/signatures/test_signature.py",
    "created_at": "2025-06-30T22:30:49+00:00",
    "commented_code": "path_obj = Path(\"/test/path\")\n     pred = dspy.Predict(test_signature)(input=path_obj)\n     assert pred.output == \"/test/path\"\n+\n+\n+def test_pep604_union_type_inline():\n+    sig = Signature(\n+        \"input_opt_right: str | None, input_opt_left: None | int -> output_union: int | str\"",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2176079676",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8475,
        "pr_file": "tests/signatures/test_signature.py",
        "discussion_id": "2176079676",
        "commented_code": "@@ -436,3 +437,139 @@ def test_custom_type_from_different_module():\n     path_obj = Path(\"/test/path\")\n     pred = dspy.Predict(test_signature)(input=path_obj)\n     assert pred.output == \"/test/path\"\n+\n+\n+def test_pep604_union_type_inline():\n+    sig = Signature(\n+        \"input_opt_right: str | None, input_opt_left: None | int -> output_union: int | str\"",
        "comment_created_at": "2025-06-30T22:30:49+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "the variable name `input_opt_right` and `input_opt_left` get me confused for a while, I would suggest using the simple `x1, x2` or `input1, input2` and add comment at assert statement to clarify what we are testing here. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2176142170",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8475,
        "pr_file": "tests/signatures/test_signature.py",
        "discussion_id": "2176079676",
        "commented_code": "@@ -436,3 +437,139 @@ def test_custom_type_from_different_module():\n     path_obj = Path(\"/test/path\")\n     pred = dspy.Predict(test_signature)(input=path_obj)\n     assert pred.output == \"/test/path\"\n+\n+\n+def test_pep604_union_type_inline():\n+    sig = Signature(\n+        \"input_opt_right: str | None, input_opt_left: None | int -> output_union: int | str\"",
        "comment_created_at": "2025-06-30T23:33:42+00:00",
        "comment_author": "erandeutsch",
        "comment_body": "Thanks, I renamed the variables to input1 and input2, and added a comment to clarify.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2080532523",
    "pr_number": 8168,
    "pr_file": "tests/adapters/test_chat_adapter.py",
    "created_at": "2025-05-08T22:11:15+00:00",
    "commented_code": "lm = dspy.utils.DummyLM([{\"answer\": \"Paris\"}])\n     result = await adapter.acall(lm, {}, signature, [], {\"question\": \"What is the capital of France?\"})\n     assert result == [{\"answer\": \"Paris\"}]\n+\n+\n+def test_chat_adapter_with_pydantic_models():\n+    class NestedField(pydantic.BaseModel):\n+        subsubfield1: str = pydantic.Field(description=\"String subsubfield\")\n+        subsubfield2: int = pydantic.Field(description=\"Integer subsubfield\", ge=0, le=10)\n+\n+    class InputField(pydantic.BaseModel):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2080532523",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8168,
        "pr_file": "tests/adapters/test_chat_adapter.py",
        "discussion_id": "2080532523",
        "commented_code": "@@ -94,3 +95,89 @@ async def test_chat_adapter_async_call():\n     lm = dspy.utils.DummyLM([{\"answer\": \"Paris\"}])\n     result = await adapter.acall(lm, {}, signature, [], {\"question\": \"What is the capital of France?\"})\n     assert result == [{\"answer\": \"Paris\"}]\n+\n+\n+def test_chat_adapter_with_pydantic_models():\n+    class NestedField(pydantic.BaseModel):\n+        subsubfield1: str = pydantic.Field(description=\"String subsubfield\")\n+        subsubfield2: int = pydantic.Field(description=\"Integer subsubfield\", ge=0, le=10)\n+\n+    class InputField(pydantic.BaseModel):",
        "comment_created_at": "2025-05-08T22:11:15+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "We should avoid defining class of name `InputField`. Although this is just unit test, our users could use it as sample code to follow. \r\n\r\nWould you mind refactoring the test case following the json adapter example? https://github.com/stanfordnlp/dspy/blob/ffb415f042baf51d731e203d08dc1f35005977c4/tests/adapters/test_json_adapter.py#L110",
        "pr_file_module": null
      },
      {
        "comment_id": "2083293028",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8168,
        "pr_file": "tests/adapters/test_chat_adapter.py",
        "discussion_id": "2080532523",
        "commented_code": "@@ -94,3 +95,89 @@ async def test_chat_adapter_async_call():\n     lm = dspy.utils.DummyLM([{\"answer\": \"Paris\"}])\n     result = await adapter.acall(lm, {}, signature, [], {\"question\": \"What is the capital of France?\"})\n     assert result == [{\"answer\": \"Paris\"}]\n+\n+\n+def test_chat_adapter_with_pydantic_models():\n+    class NestedField(pydantic.BaseModel):\n+        subsubfield1: str = pydantic.Field(description=\"String subsubfield\")\n+        subsubfield2: int = pydantic.Field(description=\"Integer subsubfield\", ge=0, le=10)\n+\n+    class InputField(pydantic.BaseModel):",
        "comment_created_at": "2025-05-10T21:25:36+00:00",
        "comment_author": "JHMuir",
        "comment_body": "Pushed some small changes, lmk how they look! \r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070663140",
    "pr_number": 8131,
    "pr_file": "tests/adapters/test_json_adapter.py",
    "created_at": "2025-05-01T18:58:53+00:00",
    "commented_code": "lm = dspy.utils.DummyLM([{\"answer\": \"Paris\"}])\n     result = await adapter.acall(lm, {}, signature, [], {\"question\": \"What is the capital of France?\"})\n     assert result == [{\"answer\": \"Paris\"}]\n+\n+\n+def test_json_adapter_passes_different_input_and_output_fields():\n+    class InputField1SubField3(pydantic.BaseModel):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2070663140",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8131,
        "pr_file": "tests/adapters/test_json_adapter.py",
        "discussion_id": "2070663140",
        "commented_code": "@@ -105,3 +107,114 @@ async def test_json_adapter_async_call():\n     lm = dspy.utils.DummyLM([{\"answer\": \"Paris\"}])\n     result = await adapter.acall(lm, {}, signature, [], {\"question\": \"What is the capital of France?\"})\n     assert result == [{\"answer\": \"Paris\"}]\n+\n+\n+def test_json_adapter_passes_different_input_and_output_fields():\n+    class InputField1SubField3(pydantic.BaseModel):",
        "comment_created_at": "2025-05-01T18:58:53+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "The class name is a bit confusing, can we create classes that can represent a real use case? For example:\r\n\r\n```\r\nclass UserProfile(BaseModel):\r\n    user_id: str\r\n    name: str\r\n    email: str\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2038155891",
    "pr_number": 8011,
    "pr_file": "dspy/adapters/utils.py",
    "created_at": "2025-04-10T19:28:24+00:00",
    "commented_code": "else:\n         # Neither => enclose in single quotes\n         return f\"'{s}'\"\n+    \n+def enumerate_fields(fields: dict) -> str:",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "2038155891",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8011,
        "pr_file": "dspy/adapters/utils.py",
        "discussion_id": "2038155891",
        "commented_code": "@@ -237,3 +237,14 @@ def _quoted_string_for_literal_type_annotation(s: str) -> str:\n     else:\n         # Neither => enclose in single quotes\n         return f\"'{s}'\"\n+    \n+def enumerate_fields(fields: dict) -> str:",
        "comment_created_at": "2025-04-10T19:28:24+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "this method has been renamed to `get_field_description_string` in the same file, can we use it? \r\n\r\nI did the renaming because `enumerate_fields` is a bit vague. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2041546686",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 8011,
        "pr_file": "dspy/adapters/utils.py",
        "discussion_id": "2038155891",
        "commented_code": "@@ -237,3 +237,14 @@ def _quoted_string_for_literal_type_annotation(s: str) -> str:\n     else:\n         # Neither => enclose in single quotes\n         return f\"'{s}'\"\n+    \n+def enumerate_fields(fields: dict) -> str:",
        "comment_created_at": "2025-04-14T07:15:09+00:00",
        "comment_author": "TomeHirata",
        "comment_body": "Good catch, let me use that instead!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1966628033",
    "pr_number": 7841,
    "pr_file": "docs/scripts/generate_api_docs.py",
    "created_at": "2025-02-22T23:54:24+00:00",
    "commented_code": "],\n }\n \n+def is_documented_method(obj):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1966628033",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 7841,
        "pr_file": "docs/scripts/generate_api_docs.py",
        "discussion_id": "1966628033",
        "commented_code": "@@ -76,6 +76,19 @@\n     ],\n }\n \n+def is_documented_method(obj):",
        "comment_created_at": "2025-02-22T23:54:24+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "`is_documented_method` sounds like we are checking if the method has a docstring, probably go with `should_document_method`? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1821423074",
    "pr_number": 1698,
    "pr_file": "dspy/clients/openai.py",
    "created_at": "2024-10-29T19:31:27+00:00",
    "commented_code": "import re\n import time\n-from collections import defaultdict\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional\n \n import openai\n \n-from dspy.clients.finetune import (\n-    FinetuneJob,\n-    TrainingMethod,\n-    TrainingStatus,\n-    save_data,\n-    validate_finetune_data,\n-)\n+from dspy.clients.provider import TrainingJob, Provider\n+from dspy.clients.utils_finetune import DataFormat, TrainingStatus, save_data\n from dspy.utils.logging import logger\n \n-# Provider name\n-PROVIDER_OPENAI = \"openai\"\n \n-\n-def is_openai_model(model: str) -> bool:\n-    \"\"\"Check if the model is an OpenAI model.\"\"\"\n-    # Filter the provider_prefix, if exists\n-    provider_prefix = f\"{PROVIDER_OPENAI}/\"\n-    if model.startswith(provider_prefix):\n-        model = model[len(provider_prefix) :]\n-\n-    client = openai.OpenAI()\n-    valid_model_names = [model.id for model in client.models.list().data]\n-    # Check if the model is a base OpenAI model\n-    if model in valid_model_names:\n-        return True\n-\n-    # Check if the model is a fine-tuned OpneAI model. Fine-tuned OpenAI models\n-    # have the prefix \"ft:<BASE_MODEL_NAME>:\", followed by a string specifying\n-    # the fine-tuned model. The following RegEx pattern is used to match the\n-    # base model name.\n-    # TODO: This part can be updated to match the actual fine-tuned model names\n-    # by making a call to the OpenAI API to be more exact, but this might\n-    # require an API key with the right permissions.\n-    match = re.match(r\"ft:([^:]+):\", model)\n-    if match and match.group(1) in valid_model_names:\n-        return True\n-\n-    return False\n+_OPENAI_MODELS = [\n+  'gpt-4-turbo',\n+  'gpt-4-turbo-2024-04-09',\n+  'tts-1',\n+  'tts-1-1106',\n+  'chatgpt-4o-latest',\n+  'dall-e-2',\n+  'whisper-1',\n+  'gpt-3.5-turbo-instruct',\n+  'gpt-3.5-turbo',\n+  'gpt-3.5-turbo-0125',\n+  'babbage-002',\n+  'davinci-002',\n+  'gpt-4o-mini-2024-07-18',\n+  'gpt-4o',\n+  'dall-e-3',\n+  'gpt-4o-mini',\n+  'gpt-4o-2024-08-06',\n+  'gpt-4o-2024-05-13',\n+  'o1-preview',\n+  'gpt-4o-audio-preview-2024-10-01',\n+  'o1-mini-2024-09-12',\n+  'gpt-4o-audio-preview',\n+  'tts-1-hd',\n+  'tts-1-hd-1106',\n+  'o1-preview-2024-09-12',\n+  'o1-mini',\n+  'gpt-4-1106-preview',\n+  'text-embedding-ada-002',\n+  'gpt-3.5-turbo-16k',\n+  'text-embedding-3-small',\n+  'text-embedding-3-large',\n+  'gpt-4o-realtime-preview-2024-10-01',\n+  'gpt-4o-realtime-preview',\n+  'gpt-3.5-turbo-1106',\n+  'gpt-4-0613',\n+  'gpt-4-turbo-preview',\n+  'gpt-4-0125-preview',\n+  'gpt-4',\n+  'gpt-3.5-turbo-instruct-0914'\n+]\n \n \n-class FinetuneJobOpenAI(FinetuneJob):\n+class TrainingJobOpenAI(TrainingJob):\n     def __init__(self, *args, **kwargs):\n-        self.provider_file_id = None  # TODO: Can we get this using the job_id?\n-        self.provider_job_id = None\n         super().__init__(*args, **kwargs)\n+        self.provider_file_id = None\n+        self.provider_job_id = None\n \n     def cancel(self):\n         # Cancel the provider job\n-        if _does_job_exist(self.provider_job_id):\n-            status = _get_training_status(self.provider_job_id)\n-            if _is_terminal_training_status(status):\n+        if OpenAIProvider.does_job_exist(self.provider_job_id):\n+            status = self.status()\n+            if OpenAIProvider.is_terminal_training_status(status):\n                 err_msg = \"Jobs that are complete cannot be canceled.\"\n                 err_msg += f\" Job with ID {self.provider_job_id} is done.\"\n                 raise Exception(err_msg)\n             openai.fine_tuning.jobs.cancel(self.provider_job_id)\n             self.provider_job_id = None\n \n         # Delete the provider file\n-        # TODO: Should there be a separate clean method?\n         if self.provider_file_id is not None:\n-            if _does_file_exist(self.provider_file_id):\n+            if OpenAIProvider.does_file_exist(self.provider_file_id):\n                 openai.files.delete(self.provider_file_id)\n             self.provider_file_id = None\n \n         # Call the super's cancel method after the custom cancellation logic\n         super().cancel()\n \n     def status(self) -> TrainingStatus:\n-        status = _get_training_status(self.provider_job_id)\n+        status = OpenAIProvider.get_training_status(self.provider_job_id)\n         return status\n \n \n-def finetune_openai(\n-    job: FinetuneJobOpenAI,\n-    model: str,\n-    train_data: List[Dict[str, Any]],\n-    train_kwargs: Optional[Dict[str, Any]] = None,\n-    train_method: TrainingMethod = TrainingMethod.SFT,\n-) -> str:\n-    train_kwargs = train_kwargs or {}\n-    train_method = TrainingMethod.SFT  # Note: This could be an argument; ignoring method\n-\n-    # Validate train data and method\n-    logger.info(\"[Finetune] Validating the formatting of the data\")\n-    _validate_data(train_data, train_method)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    # Convert to the OpenAI format\n-    logger.info(\"[Finetune] Converting the data to the OpenAI format\")\n-    # TODO: Should we use the system prompt?\n-    train_data = _convert_data(train_data)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    # Save to a file\n-    logger.info(\"[Finetune] Saving the data to a file\")\n-    data_path = save_data(train_data, provider_name=PROVIDER_OPENAI)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    # Upload the data to the cloud\n-    logger.info(\"[Finetune] Uploading the data to the provider\")\n-    provider_file_id = _upload_data(data_path)\n-    job.provider_file_id = provider_file_id\n-    logger.info(\"[Finetune] Done!\")\n-\n-    logger.info(\"[Finetune] Start remote training\")\n-    # We utilize model and train_kwargs here\n-    provider_job_id = _start_remote_training(\n-        train_file_id=job.provider_file_id,\n-        model=model,\n-        train_kwargs=train_kwargs,\n-    )\n-    job.provider_job_id = provider_job_id\n-    # job.provider_job_id = \"ftjob-ZdEL1mUDk0dwdDuZJQOng8Vv\"\n-    logger.info(\"[Finetune] Done!\")\n-\n-    logger.info(\"[Finetune] Wait for training to complete\")\n-    # TODO: Would it be possible to stream the logs?\n-    _wait_for_job(job)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    logger.info(\"[Finetune] Get trained model if the run was a success\")\n-    model = _get_trained_model(job)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    return model\n-\n-\n-_SUPPORTED_TRAINING_METHODS = [\n-    TrainingMethod.SFT,\n-]\n-\n-\n-def _get_training_status(job_id: str) -> Union[TrainingStatus, Exception]:\n-    # TODO: Should this type be shared across all fine-tune clients?\n-    provider_status_to_training_status = {\n-        \"validating_files\": TrainingStatus.pending,\n-        \"queued\": TrainingStatus.pending,\n-        \"running\": TrainingStatus.running,\n-        \"succeeded\": TrainingStatus.succeeded,\n-        \"failed\": TrainingStatus.failed,\n-        \"cancelled\": TrainingStatus.cancelled,\n-    }\n-\n-    # Check if there is an active job\n-    if job_id is None:\n-        logger.info(\"There is no active job.\")\n-        return TrainingStatus.not_started\n-\n-    err_msg = f\"Job with ID {job_id} does not exist.\"\n-    assert _does_job_exist(job_id), err_msg\n-\n-    # Retrieve the provider's job and report the status\n-    provider_job = openai.fine_tuning.jobs.retrieve(job_id)\n-    provider_status = provider_job.status\n-    status = provider_status_to_training_status[provider_status]\n-\n-    return status\n-\n-\n-def _does_job_exist(job_id: str) -> bool:\n-    try:\n-        # TODO: Error handling is vague\n-        openai.fine_tuning.jobs.retrieve(job_id)\n-        return True\n-    except Exception:\n-        return False\n-\n+class OpenAIProvider(Provider):\n+    \n+    def __init__(self):\n+        super().__init__()\n+        self.finetunable = True\n+        self.TrainingJob = TrainingJobOpenAI\n+\n+    @staticmethod\n+    def is_provider_model(model: str) -> bool:\n+        # Filter the provider_prefix, if exists\n+        provider_prefix = \"openai/\"\n+        if model.startswith(provider_prefix):\n+            model = model[len(provider_prefix):]\n+\n+        # Check if the model is a base OpenAI model\n+        # TODO(enhance) The following list can be replaced with\n+        # openai.models.list(), but doing so might require a key. Is there a\n+        # way to get the list of models without a key?\n+        valid_model_names = _OPENAI_MODELS\n+        if model in valid_model_names:\n+            return True\n+\n+        # Check if the model is a fine-tuned OpneAI model. Fine-tuned OpenAI\n+        # models have the prefix \"ft:<BASE_MODEL_NAME>:\", followed by a string\n+        # specifying the fine-tuned model. The following RegEx pattern is used\n+        # to match the base model name.\n+        # TODO(enhance): This part can be updated to match the actual fine-tuned\n+        # model names by making a call to the OpenAI API to be more exact, but\n+        # this might require an API key with the right permissions.\n+        match = re.match(r\"ft:([^:]+):\", model)\n+        if match and match.group(1) in valid_model_names:\n+            return True\n \n-def _does_file_exist(file_id: str) -> bool:\n-    try:\n-        # TODO: Error handling is vague\n-        openai.files.retrieve(file_id)\n-        return True\n-    except Exception:\n         return False\n \n-\n-def _is_terminal_training_status(status: TrainingStatus) -> bool:\n-    return status in [\n-        TrainingStatus.succeeded,\n-        TrainingStatus.failed,\n-        TrainingStatus.cancelled,\n-    ]\n-\n-\n-def _validate_data(data: Dict[str, str], train_method: TrainingMethod) -> Optional[Exception]:\n-    # Check if this train method is supported\n-    if train_method not in _SUPPORTED_TRAINING_METHODS:\n-        err_msg = f\"OpenAI does not support the training method {train_method}.\"\n-        raise ValueError(err_msg)\n-\n-    validate_finetune_data(data, train_method)\n-\n-\n-def _convert_data(\n-    data: List[Dict[str, str]],\n-    system_prompt: Optional[str] = None,\n-) -> Union[List[Dict[str, Any]], Exception]:\n-    # Item-wise conversion function\n-    def _row_converter(d):\n-        messages = [{\"role\": \"user\", \"content\": d[\"prompt\"]}, {\"role\": \"assistant\", \"content\": d[\"completion\"]}]\n-        if system_prompt:\n-            messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n-        messages_dict = {\"messages\": messages}\n-        return messages_dict\n-\n-    # Convert the data to the OpenAI format; validate the converted data\n-    converted_data = list(map(_row_converter, data))\n-    openai_data_validation(converted_data)\n-    return converted_data\n-\n-\n-def _upload_data(data_path: str) -> str:\n-    # Upload the data to the provider\n-    provider_file = openai.files.create(\n-        file=open(data_path, \"rb\"),\n-        purpose=\"fine-tune\",\n-    )\n-    return provider_file.id\n-\n-\n-def _start_remote_training(train_file_id: str, model: id, train_kwargs: Optional[Dict[str, Any]] = None) -> str:\n-    train_kwargs = train_kwargs or {}\n-    provider_job = openai.fine_tuning.jobs.create(\n-        model=model,\n-        training_file=train_file_id,\n-        hyperparameters=train_kwargs,\n-    )\n-    return provider_job.id\n-\n-\n-def _wait_for_job(\n-    job: FinetuneJobOpenAI,\n-    poll_frequency: int = 60,\n-):\n-    while not _is_terminal_training_status(job.status()):\n-        time.sleep(poll_frequency)\n-\n-\n-def _get_trained_model(job):\n-    status = job.status()\n-    if status != TrainingStatus.succeeded:\n-        err_msg = f\"Job status is {status}.\"\n-        err_msg += f\" Must be {TrainingStatus.succeeded} to retrieve the model.\"\n-        logger.error(err_msg)\n-        raise Exception(err_msg)\n-\n-    provider_job = openai.fine_tuning.jobs.retrieve(job.provider_job_id)\n-    finetuned_model = provider_job.fine_tuned_model\n-    return finetuned_model\n-\n-\n-# Adapted from https://cookbook.openai.com/examples/chat_finetuning_data_prep\n-def openai_data_validation(dataset: List[dict[str, Any]]):\n-    format_errors = defaultdict(int)\n-    for ex in dataset:\n-        if not isinstance(ex, dict):\n-            format_errors[\"data_type\"] += 1\n-            continue\n-\n-        messages = ex.get(\"messages\", None)\n-        if not messages:\n-            format_errors[\"missing_messages_list\"] += 1\n-            continue\n-\n-        for message in messages:\n-            if \"role\" not in message or \"content\" not in message:\n-                format_errors[\"message_missing_key\"] += 1\n-\n-            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n-                format_errors[\"message_unrecognized_key\"] += 1\n-\n-            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n-                format_errors[\"unrecognized_role\"] += 1\n-\n-            content = message.get(\"content\", None)\n-            function_call = message.get(\"function_call\", None)\n-\n-            if (not content and not function_call) or not isinstance(content, str):\n-                format_errors[\"missing_content\"] += 1\n-\n-        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n-            format_errors[\"example_missing_assistant_message\"] += 1\n-\n-    # Raise an error if there are any format errors\n-    if format_errors:\n-        err_msg = \"Found errors in the dataset format using the OpenAI API.\"\n-        err_msg += \" Here are the number of datapoints for each error type:\"\n-        for k, v in format_errors.items():\n-            err_msg += \"\n    {k}: {v}\"\n-        raise ValueError(err_msg)\n-\n-\n-def check_message_lengths(dataset: List[dict[str, Any]]) -> list[int]:\n-    n_missing_system = 0\n-    n_missing_user = 0\n-    n_messages = []\n-    convo_lens = []\n-    assistant_message_lens = []\n-\n-    for ex in dataset:\n-        messages = ex[\"messages\"]\n-        if not any(message[\"role\"] == \"system\" for message in messages):\n-            n_missing_system += 1\n-        if not any(message[\"role\"] == \"user\" for message in messages):\n-            n_missing_user += 1\n-        n_messages.append(len(messages))\n-        convo_lens.append(num_tokens_from_messages(messages))\n-        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n-    n_too_long = sum([length > 16385 for length in convo_lens])\n-\n-    if n_too_long > 0:\n-        logger.info(\n-            f\"There are {n_too_long} examples that may be over the 16,385 token limit, they will be truncated during fine-tuning.\"\n+    @staticmethod\n+    def finetune(\n+        job: TrainingJobOpenAI,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ) -> str:\n+        logger.info(\"[Finetune] Validating the data format\")\n+        OpenAIProvider.validate_data_format(data_format)",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1821423074",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1698,
        "pr_file": "dspy/clients/openai.py",
        "discussion_id": "1821423074",
        "commented_code": "@@ -1,358 +1,271 @@\n import re\n import time\n-from collections import defaultdict\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional\n \n import openai\n \n-from dspy.clients.finetune import (\n-    FinetuneJob,\n-    TrainingMethod,\n-    TrainingStatus,\n-    save_data,\n-    validate_finetune_data,\n-)\n+from dspy.clients.provider import TrainingJob, Provider\n+from dspy.clients.utils_finetune import DataFormat, TrainingStatus, save_data\n from dspy.utils.logging import logger\n \n-# Provider name\n-PROVIDER_OPENAI = \"openai\"\n \n-\n-def is_openai_model(model: str) -> bool:\n-    \"\"\"Check if the model is an OpenAI model.\"\"\"\n-    # Filter the provider_prefix, if exists\n-    provider_prefix = f\"{PROVIDER_OPENAI}/\"\n-    if model.startswith(provider_prefix):\n-        model = model[len(provider_prefix) :]\n-\n-    client = openai.OpenAI()\n-    valid_model_names = [model.id for model in client.models.list().data]\n-    # Check if the model is a base OpenAI model\n-    if model in valid_model_names:\n-        return True\n-\n-    # Check if the model is a fine-tuned OpneAI model. Fine-tuned OpenAI models\n-    # have the prefix \"ft:<BASE_MODEL_NAME>:\", followed by a string specifying\n-    # the fine-tuned model. The following RegEx pattern is used to match the\n-    # base model name.\n-    # TODO: This part can be updated to match the actual fine-tuned model names\n-    # by making a call to the OpenAI API to be more exact, but this might\n-    # require an API key with the right permissions.\n-    match = re.match(r\"ft:([^:]+):\", model)\n-    if match and match.group(1) in valid_model_names:\n-        return True\n-\n-    return False\n+_OPENAI_MODELS = [\n+  'gpt-4-turbo',\n+  'gpt-4-turbo-2024-04-09',\n+  'tts-1',\n+  'tts-1-1106',\n+  'chatgpt-4o-latest',\n+  'dall-e-2',\n+  'whisper-1',\n+  'gpt-3.5-turbo-instruct',\n+  'gpt-3.5-turbo',\n+  'gpt-3.5-turbo-0125',\n+  'babbage-002',\n+  'davinci-002',\n+  'gpt-4o-mini-2024-07-18',\n+  'gpt-4o',\n+  'dall-e-3',\n+  'gpt-4o-mini',\n+  'gpt-4o-2024-08-06',\n+  'gpt-4o-2024-05-13',\n+  'o1-preview',\n+  'gpt-4o-audio-preview-2024-10-01',\n+  'o1-mini-2024-09-12',\n+  'gpt-4o-audio-preview',\n+  'tts-1-hd',\n+  'tts-1-hd-1106',\n+  'o1-preview-2024-09-12',\n+  'o1-mini',\n+  'gpt-4-1106-preview',\n+  'text-embedding-ada-002',\n+  'gpt-3.5-turbo-16k',\n+  'text-embedding-3-small',\n+  'text-embedding-3-large',\n+  'gpt-4o-realtime-preview-2024-10-01',\n+  'gpt-4o-realtime-preview',\n+  'gpt-3.5-turbo-1106',\n+  'gpt-4-0613',\n+  'gpt-4-turbo-preview',\n+  'gpt-4-0125-preview',\n+  'gpt-4',\n+  'gpt-3.5-turbo-instruct-0914'\n+]\n \n \n-class FinetuneJobOpenAI(FinetuneJob):\n+class TrainingJobOpenAI(TrainingJob):\n     def __init__(self, *args, **kwargs):\n-        self.provider_file_id = None  # TODO: Can we get this using the job_id?\n-        self.provider_job_id = None\n         super().__init__(*args, **kwargs)\n+        self.provider_file_id = None\n+        self.provider_job_id = None\n \n     def cancel(self):\n         # Cancel the provider job\n-        if _does_job_exist(self.provider_job_id):\n-            status = _get_training_status(self.provider_job_id)\n-            if _is_terminal_training_status(status):\n+        if OpenAIProvider.does_job_exist(self.provider_job_id):\n+            status = self.status()\n+            if OpenAIProvider.is_terminal_training_status(status):\n                 err_msg = \"Jobs that are complete cannot be canceled.\"\n                 err_msg += f\" Job with ID {self.provider_job_id} is done.\"\n                 raise Exception(err_msg)\n             openai.fine_tuning.jobs.cancel(self.provider_job_id)\n             self.provider_job_id = None\n \n         # Delete the provider file\n-        # TODO: Should there be a separate clean method?\n         if self.provider_file_id is not None:\n-            if _does_file_exist(self.provider_file_id):\n+            if OpenAIProvider.does_file_exist(self.provider_file_id):\n                 openai.files.delete(self.provider_file_id)\n             self.provider_file_id = None\n \n         # Call the super's cancel method after the custom cancellation logic\n         super().cancel()\n \n     def status(self) -> TrainingStatus:\n-        status = _get_training_status(self.provider_job_id)\n+        status = OpenAIProvider.get_training_status(self.provider_job_id)\n         return status\n \n \n-def finetune_openai(\n-    job: FinetuneJobOpenAI,\n-    model: str,\n-    train_data: List[Dict[str, Any]],\n-    train_kwargs: Optional[Dict[str, Any]] = None,\n-    train_method: TrainingMethod = TrainingMethod.SFT,\n-) -> str:\n-    train_kwargs = train_kwargs or {}\n-    train_method = TrainingMethod.SFT  # Note: This could be an argument; ignoring method\n-\n-    # Validate train data and method\n-    logger.info(\"[Finetune] Validating the formatting of the data\")\n-    _validate_data(train_data, train_method)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    # Convert to the OpenAI format\n-    logger.info(\"[Finetune] Converting the data to the OpenAI format\")\n-    # TODO: Should we use the system prompt?\n-    train_data = _convert_data(train_data)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    # Save to a file\n-    logger.info(\"[Finetune] Saving the data to a file\")\n-    data_path = save_data(train_data, provider_name=PROVIDER_OPENAI)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    # Upload the data to the cloud\n-    logger.info(\"[Finetune] Uploading the data to the provider\")\n-    provider_file_id = _upload_data(data_path)\n-    job.provider_file_id = provider_file_id\n-    logger.info(\"[Finetune] Done!\")\n-\n-    logger.info(\"[Finetune] Start remote training\")\n-    # We utilize model and train_kwargs here\n-    provider_job_id = _start_remote_training(\n-        train_file_id=job.provider_file_id,\n-        model=model,\n-        train_kwargs=train_kwargs,\n-    )\n-    job.provider_job_id = provider_job_id\n-    # job.provider_job_id = \"ftjob-ZdEL1mUDk0dwdDuZJQOng8Vv\"\n-    logger.info(\"[Finetune] Done!\")\n-\n-    logger.info(\"[Finetune] Wait for training to complete\")\n-    # TODO: Would it be possible to stream the logs?\n-    _wait_for_job(job)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    logger.info(\"[Finetune] Get trained model if the run was a success\")\n-    model = _get_trained_model(job)\n-    logger.info(\"[Finetune] Done!\")\n-\n-    return model\n-\n-\n-_SUPPORTED_TRAINING_METHODS = [\n-    TrainingMethod.SFT,\n-]\n-\n-\n-def _get_training_status(job_id: str) -> Union[TrainingStatus, Exception]:\n-    # TODO: Should this type be shared across all fine-tune clients?\n-    provider_status_to_training_status = {\n-        \"validating_files\": TrainingStatus.pending,\n-        \"queued\": TrainingStatus.pending,\n-        \"running\": TrainingStatus.running,\n-        \"succeeded\": TrainingStatus.succeeded,\n-        \"failed\": TrainingStatus.failed,\n-        \"cancelled\": TrainingStatus.cancelled,\n-    }\n-\n-    # Check if there is an active job\n-    if job_id is None:\n-        logger.info(\"There is no active job.\")\n-        return TrainingStatus.not_started\n-\n-    err_msg = f\"Job with ID {job_id} does not exist.\"\n-    assert _does_job_exist(job_id), err_msg\n-\n-    # Retrieve the provider's job and report the status\n-    provider_job = openai.fine_tuning.jobs.retrieve(job_id)\n-    provider_status = provider_job.status\n-    status = provider_status_to_training_status[provider_status]\n-\n-    return status\n-\n-\n-def _does_job_exist(job_id: str) -> bool:\n-    try:\n-        # TODO: Error handling is vague\n-        openai.fine_tuning.jobs.retrieve(job_id)\n-        return True\n-    except Exception:\n-        return False\n-\n+class OpenAIProvider(Provider):\n+    \n+    def __init__(self):\n+        super().__init__()\n+        self.finetunable = True\n+        self.TrainingJob = TrainingJobOpenAI\n+\n+    @staticmethod\n+    def is_provider_model(model: str) -> bool:\n+        # Filter the provider_prefix, if exists\n+        provider_prefix = \"openai/\"\n+        if model.startswith(provider_prefix):\n+            model = model[len(provider_prefix):]\n+\n+        # Check if the model is a base OpenAI model\n+        # TODO(enhance) The following list can be replaced with\n+        # openai.models.list(), but doing so might require a key. Is there a\n+        # way to get the list of models without a key?\n+        valid_model_names = _OPENAI_MODELS\n+        if model in valid_model_names:\n+            return True\n+\n+        # Check if the model is a fine-tuned OpneAI model. Fine-tuned OpenAI\n+        # models have the prefix \"ft:<BASE_MODEL_NAME>:\", followed by a string\n+        # specifying the fine-tuned model. The following RegEx pattern is used\n+        # to match the base model name.\n+        # TODO(enhance): This part can be updated to match the actual fine-tuned\n+        # model names by making a call to the OpenAI API to be more exact, but\n+        # this might require an API key with the right permissions.\n+        match = re.match(r\"ft:([^:]+):\", model)\n+        if match and match.group(1) in valid_model_names:\n+            return True\n \n-def _does_file_exist(file_id: str) -> bool:\n-    try:\n-        # TODO: Error handling is vague\n-        openai.files.retrieve(file_id)\n-        return True\n-    except Exception:\n         return False\n \n-\n-def _is_terminal_training_status(status: TrainingStatus) -> bool:\n-    return status in [\n-        TrainingStatus.succeeded,\n-        TrainingStatus.failed,\n-        TrainingStatus.cancelled,\n-    ]\n-\n-\n-def _validate_data(data: Dict[str, str], train_method: TrainingMethod) -> Optional[Exception]:\n-    # Check if this train method is supported\n-    if train_method not in _SUPPORTED_TRAINING_METHODS:\n-        err_msg = f\"OpenAI does not support the training method {train_method}.\"\n-        raise ValueError(err_msg)\n-\n-    validate_finetune_data(data, train_method)\n-\n-\n-def _convert_data(\n-    data: List[Dict[str, str]],\n-    system_prompt: Optional[str] = None,\n-) -> Union[List[Dict[str, Any]], Exception]:\n-    # Item-wise conversion function\n-    def _row_converter(d):\n-        messages = [{\"role\": \"user\", \"content\": d[\"prompt\"]}, {\"role\": \"assistant\", \"content\": d[\"completion\"]}]\n-        if system_prompt:\n-            messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n-        messages_dict = {\"messages\": messages}\n-        return messages_dict\n-\n-    # Convert the data to the OpenAI format; validate the converted data\n-    converted_data = list(map(_row_converter, data))\n-    openai_data_validation(converted_data)\n-    return converted_data\n-\n-\n-def _upload_data(data_path: str) -> str:\n-    # Upload the data to the provider\n-    provider_file = openai.files.create(\n-        file=open(data_path, \"rb\"),\n-        purpose=\"fine-tune\",\n-    )\n-    return provider_file.id\n-\n-\n-def _start_remote_training(train_file_id: str, model: id, train_kwargs: Optional[Dict[str, Any]] = None) -> str:\n-    train_kwargs = train_kwargs or {}\n-    provider_job = openai.fine_tuning.jobs.create(\n-        model=model,\n-        training_file=train_file_id,\n-        hyperparameters=train_kwargs,\n-    )\n-    return provider_job.id\n-\n-\n-def _wait_for_job(\n-    job: FinetuneJobOpenAI,\n-    poll_frequency: int = 60,\n-):\n-    while not _is_terminal_training_status(job.status()):\n-        time.sleep(poll_frequency)\n-\n-\n-def _get_trained_model(job):\n-    status = job.status()\n-    if status != TrainingStatus.succeeded:\n-        err_msg = f\"Job status is {status}.\"\n-        err_msg += f\" Must be {TrainingStatus.succeeded} to retrieve the model.\"\n-        logger.error(err_msg)\n-        raise Exception(err_msg)\n-\n-    provider_job = openai.fine_tuning.jobs.retrieve(job.provider_job_id)\n-    finetuned_model = provider_job.fine_tuned_model\n-    return finetuned_model\n-\n-\n-# Adapted from https://cookbook.openai.com/examples/chat_finetuning_data_prep\n-def openai_data_validation(dataset: List[dict[str, Any]]):\n-    format_errors = defaultdict(int)\n-    for ex in dataset:\n-        if not isinstance(ex, dict):\n-            format_errors[\"data_type\"] += 1\n-            continue\n-\n-        messages = ex.get(\"messages\", None)\n-        if not messages:\n-            format_errors[\"missing_messages_list\"] += 1\n-            continue\n-\n-        for message in messages:\n-            if \"role\" not in message or \"content\" not in message:\n-                format_errors[\"message_missing_key\"] += 1\n-\n-            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n-                format_errors[\"message_unrecognized_key\"] += 1\n-\n-            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n-                format_errors[\"unrecognized_role\"] += 1\n-\n-            content = message.get(\"content\", None)\n-            function_call = message.get(\"function_call\", None)\n-\n-            if (not content and not function_call) or not isinstance(content, str):\n-                format_errors[\"missing_content\"] += 1\n-\n-        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n-            format_errors[\"example_missing_assistant_message\"] += 1\n-\n-    # Raise an error if there are any format errors\n-    if format_errors:\n-        err_msg = \"Found errors in the dataset format using the OpenAI API.\"\n-        err_msg += \" Here are the number of datapoints for each error type:\"\n-        for k, v in format_errors.items():\n-            err_msg += \"\\n    {k}: {v}\"\n-        raise ValueError(err_msg)\n-\n-\n-def check_message_lengths(dataset: List[dict[str, Any]]) -> list[int]:\n-    n_missing_system = 0\n-    n_missing_user = 0\n-    n_messages = []\n-    convo_lens = []\n-    assistant_message_lens = []\n-\n-    for ex in dataset:\n-        messages = ex[\"messages\"]\n-        if not any(message[\"role\"] == \"system\" for message in messages):\n-            n_missing_system += 1\n-        if not any(message[\"role\"] == \"user\" for message in messages):\n-            n_missing_user += 1\n-        n_messages.append(len(messages))\n-        convo_lens.append(num_tokens_from_messages(messages))\n-        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n-    n_too_long = sum([length > 16385 for length in convo_lens])\n-\n-    if n_too_long > 0:\n-        logger.info(\n-            f\"There are {n_too_long} examples that may be over the 16,385 token limit, they will be truncated during fine-tuning.\"\n+    @staticmethod\n+    def finetune(\n+        job: TrainingJobOpenAI,\n+        model: str,\n+        train_data: List[Dict[str, Any]],\n+        train_kwargs: Optional[Dict[str, Any]] = None,\n+        data_format: Optional[DataFormat] = None,\n+    ) -> str:\n+        logger.info(\"[Finetune] Validating the data format\")\n+        OpenAIProvider.validate_data_format(data_format)",
        "comment_created_at": "2024-10-29T19:31:27+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "Should this be `validate_finetuning_data`, or have `validate_data_format` take an additional arg to represent the job type? `validate_data_format` is a generic name.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1821497801",
    "pr_number": 1698,
    "pr_file": "dspy/clients/provider.py",
    "created_at": "2024-10-29T20:34:49+00:00",
    "commented_code": "+from concurrent.futures import Future\n+from abc import abstractmethod\n+from threading import Thread\n+from typing import Any, Dict, List, Optional\n+\n+from dspy.clients.utils_finetune import DataFormat\n+from dspy.utils.logging import logger\n+\n+\n+class TrainingJob(Future):",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1821497801",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1698,
        "pr_file": "dspy/clients/provider.py",
        "discussion_id": "1821497801",
        "commented_code": "@@ -0,0 +1,68 @@\n+from concurrent.futures import Future\n+from abc import abstractmethod\n+from threading import Thread\n+from typing import Any, Dict, List, Optional\n+\n+from dspy.clients.utils_finetune import DataFormat\n+from dspy.utils.logging import logger\n+\n+\n+class TrainingJob(Future):",
        "comment_created_at": "2024-10-29T20:34:49+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "Do we want to use the same interface to support other training jobs than finetuning? e.g., pretraining, traditional training. If not, I would prefer name `Finetuning` over `Training` for clarity ",
        "pr_file_module": null
      },
      {
        "comment_id": "1821575047",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1698,
        "pr_file": "dspy/clients/provider.py",
        "discussion_id": "1821497801",
        "commented_code": "@@ -0,0 +1,68 @@\n+from concurrent.futures import Future\n+from abc import abstractmethod\n+from threading import Thread\n+from typing import Any, Dict, List, Optional\n+\n+from dspy.clients.utils_finetune import DataFormat\n+from dspy.utils.logging import logger\n+\n+\n+class TrainingJob(Future):",
        "comment_created_at": "2024-10-29T21:51:00+00:00",
        "comment_author": "okhat",
        "comment_body": "We can improve the name later",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1805228305",
    "pr_number": 1594,
    "pr_file": "dspy/clients/lm.py",
    "created_at": "2024-10-17T18:17:32+00:00",
    "commented_code": "return outputs\n \n+    def copy(self, **kwargs):\n+        \"\"\"Create a copy of the current LM with updated kwargs.\"\"\"\n+        kwargs = {\n+            \"model\": self.model,\n+            \"model_type\": self.model_type,\n+            \"cache\": self.cache,\n+            \"launch_kwargs\": self.launch_kwargs,\n+            **self.kwargs,\n+            **kwargs,\n+        }\n+        return self.__class__(**kwargs)\n+    \n     def inspect_history(self, n: int = 1):\n         _inspect_history(self, n)\n \n+    def launch(self):\n+        \"\"\"Send a request to the provider to launch the model, if needed.\"\"\"\n+        if is_self_hosted_model(self.model):\n+            self_hosted_model_launch(self.model, self.launch_kwargs)\n+        elif is_anyscale_model(self.model):\n+            anyscale_model_launch(self.model, self.launch_kwargs)\n+        msg = f\"`launch()` is called for the auto-launched model {self.model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+\n+    def kill(self):\n+        \"\"\"Send a request to the provider to kill the model, if needed.\"\"\"\n+        if is_self_hosted_model(self.model):\n+            self_hosted_model_kill(self.model, self.launch_kwargs)\n+        elif is_anyscale_model(self.model):\n+            anyscale_model_kill(self.model, self.launch_kwargs)\n+        msg = f\"`kill()` is called for the auto-launched model {self.model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+\n+    def finetune(self,\n+            train_method: TrainingMethod,\n+            train_data: List[Dict[str, Any]],\n+            train_kwargs: Optional[Dict[str, Any]]=None,\n+            cache_finetune: bool = True,\n+            provider: str = \"openai\",\n+        ) -> FinetuneJob:\n+        \"\"\"Start model fine-tuning, if supported.\"\"\"\n+        # Fine-tuning is experimental and requires the experimental flag\n+        from dspy import settings as settings\n+        err = \"Fine-tuning is an experimental feature.\"\n+        err += \" Set `dspy.settings.experimental` to `True` to use it.\"\n+        assert settings.experimental, err\n+\n+        # Initialize the finetune job\n+        FinetuneJobClass = get_provider_finetune_job_class(provider=provider)\n+        finetune_job = FinetuneJobClass(",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1805228305",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1594,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1805228305",
        "commented_code": "@@ -60,9 +87,84 @@ def __call__(self, prompt=None, messages=None, **kwargs):\n         \n         return outputs\n \n+    def copy(self, **kwargs):\n+        \"\"\"Create a copy of the current LM with updated kwargs.\"\"\"\n+        kwargs = {\n+            \"model\": self.model,\n+            \"model_type\": self.model_type,\n+            \"cache\": self.cache,\n+            \"launch_kwargs\": self.launch_kwargs,\n+            **self.kwargs,\n+            **kwargs,\n+        }\n+        return self.__class__(**kwargs)\n+    \n     def inspect_history(self, n: int = 1):\n         _inspect_history(self, n)\n \n+    def launch(self):\n+        \"\"\"Send a request to the provider to launch the model, if needed.\"\"\"\n+        if is_self_hosted_model(self.model):\n+            self_hosted_model_launch(self.model, self.launch_kwargs)\n+        elif is_anyscale_model(self.model):\n+            anyscale_model_launch(self.model, self.launch_kwargs)\n+        msg = f\"`launch()` is called for the auto-launched model {self.model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+\n+    def kill(self):\n+        \"\"\"Send a request to the provider to kill the model, if needed.\"\"\"\n+        if is_self_hosted_model(self.model):\n+            self_hosted_model_kill(self.model, self.launch_kwargs)\n+        elif is_anyscale_model(self.model):\n+            anyscale_model_kill(self.model, self.launch_kwargs)\n+        msg = f\"`kill()` is called for the auto-launched model {self.model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+\n+    def finetune(self,\n+            train_method: TrainingMethod,\n+            train_data: List[Dict[str, Any]],\n+            train_kwargs: Optional[Dict[str, Any]]=None,\n+            cache_finetune: bool = True,\n+            provider: str = \"openai\",\n+        ) -> FinetuneJob:\n+        \"\"\"Start model fine-tuning, if supported.\"\"\"\n+        # Fine-tuning is experimental and requires the experimental flag\n+        from dspy import settings as settings\n+        err = \"Fine-tuning is an experimental feature.\"\n+        err += \" Set `dspy.settings.experimental` to `True` to use it.\"\n+        assert settings.experimental, err\n+\n+        # Initialize the finetune job\n+        FinetuneJobClass = get_provider_finetune_job_class(provider=provider)\n+        finetune_job = FinetuneJobClass(",
        "comment_created_at": "2024-10-17T18:17:32+00:00",
        "comment_author": "dilarasoylu",
        "comment_body": "Should we also pass `train_method` here? nit. Let's pick between `method` vs. `train_method` and change all the functions to use the same name ",
        "pr_file_module": null
      },
      {
        "comment_id": "1805233046",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1594,
        "pr_file": "dspy/clients/lm.py",
        "discussion_id": "1805228305",
        "commented_code": "@@ -60,9 +87,84 @@ def __call__(self, prompt=None, messages=None, **kwargs):\n         \n         return outputs\n \n+    def copy(self, **kwargs):\n+        \"\"\"Create a copy of the current LM with updated kwargs.\"\"\"\n+        kwargs = {\n+            \"model\": self.model,\n+            \"model_type\": self.model_type,\n+            \"cache\": self.cache,\n+            \"launch_kwargs\": self.launch_kwargs,\n+            **self.kwargs,\n+            **kwargs,\n+        }\n+        return self.__class__(**kwargs)\n+    \n     def inspect_history(self, n: int = 1):\n         _inspect_history(self, n)\n \n+    def launch(self):\n+        \"\"\"Send a request to the provider to launch the model, if needed.\"\"\"\n+        if is_self_hosted_model(self.model):\n+            self_hosted_model_launch(self.model, self.launch_kwargs)\n+        elif is_anyscale_model(self.model):\n+            anyscale_model_launch(self.model, self.launch_kwargs)\n+        msg = f\"`launch()` is called for the auto-launched model {self.model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+\n+    def kill(self):\n+        \"\"\"Send a request to the provider to kill the model, if needed.\"\"\"\n+        if is_self_hosted_model(self.model):\n+            self_hosted_model_kill(self.model, self.launch_kwargs)\n+        elif is_anyscale_model(self.model):\n+            anyscale_model_kill(self.model, self.launch_kwargs)\n+        msg = f\"`kill()` is called for the auto-launched model {self.model}\"\n+        msg += \" -- no action is taken!\"\n+        logger.info(msg)\n+\n+    def finetune(self,\n+            train_method: TrainingMethod,\n+            train_data: List[Dict[str, Any]],\n+            train_kwargs: Optional[Dict[str, Any]]=None,\n+            cache_finetune: bool = True,\n+            provider: str = \"openai\",\n+        ) -> FinetuneJob:\n+        \"\"\"Start model fine-tuning, if supported.\"\"\"\n+        # Fine-tuning is experimental and requires the experimental flag\n+        from dspy import settings as settings\n+        err = \"Fine-tuning is an experimental feature.\"\n+        err += \" Set `dspy.settings.experimental` to `True` to use it.\"\n+        assert settings.experimental, err\n+\n+        # Initialize the finetune job\n+        FinetuneJobClass = get_provider_finetune_job_class(provider=provider)\n+        finetune_job = FinetuneJobClass(",
        "comment_created_at": "2024-10-17T18:21:43+00:00",
        "comment_author": "isaacbmiller",
        "comment_body": "Lets use train method",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1537980601",
    "pr_number": 616,
    "pr_file": "dsp/modules/googlevertexai.py",
    "created_at": "2024-03-25T17:39:55+00:00",
    "commented_code": "+\"\"\"Module for interacting with Google Vertex AI.\"\"\"\n+from typing import Any, Dict\n+\n+import backoff\n+from pydantic_core import PydanticCustomError\n+\n+from dsp.modules.lm import LM\n+\n+try:\n+    import vertexai  # type: ignore[import-untyped]\n+    from vertexai.language_models import CodeGenerationModel, TextGenerationModel\n+    from vertexai.preview.generative_models import GenerativeModel\n+except ImportError:\n+    pass\n+\n+\n+def backoff_hdlr(details):\n+    \"\"\"Handler from https://pypi.org/project/backoff/\"\"\"\n+    print(f\"Backing off {details['wait']:0.1f} seconds after {details['tries']} tries \"\n+          f\"calling function {details['target']} with kwargs \"\n+          f\"{details['kwargs']}\")\n+\n+\n+def giveup_hdlr(details):\n+    \"\"\"wrapper function that decides when to give up on retry\"\"\"\n+    if \"rate limits\" in details.message:\n+        return False\n+    return True\n+\n+class GoogleVertexAI(LM):\n+    \"\"\"Wrapper around GoogleVertexAI's API.\n+\n+    Currently supported models include `gemini-pro-1.0`.\n+    \"\"\"\n+\n+    def __init__(\n+        self, model_name: str = \"text-bison@002\", **kwargs,\n+    ):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        model : str\n+            Which pre-trained model from Google to use?\n+            Choices are [`text-bison@002`]",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1537980601",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 616,
        "pr_file": "dsp/modules/googlevertexai.py",
        "discussion_id": "1537980601",
        "commented_code": "@@ -0,0 +1,174 @@\n+\"\"\"Module for interacting with Google Vertex AI.\"\"\"\n+from typing import Any, Dict\n+\n+import backoff\n+from pydantic_core import PydanticCustomError\n+\n+from dsp.modules.lm import LM\n+\n+try:\n+    import vertexai  # type: ignore[import-untyped]\n+    from vertexai.language_models import CodeGenerationModel, TextGenerationModel\n+    from vertexai.preview.generative_models import GenerativeModel\n+except ImportError:\n+    pass\n+\n+\n+def backoff_hdlr(details):\n+    \"\"\"Handler from https://pypi.org/project/backoff/\"\"\"\n+    print(f\"Backing off {details['wait']:0.1f} seconds after {details['tries']} tries \"\n+          f\"calling function {details['target']} with kwargs \"\n+          f\"{details['kwargs']}\")\n+\n+\n+def giveup_hdlr(details):\n+    \"\"\"wrapper function that decides when to give up on retry\"\"\"\n+    if \"rate limits\" in details.message:\n+        return False\n+    return True\n+\n+class GoogleVertexAI(LM):\n+    \"\"\"Wrapper around GoogleVertexAI's API.\n+\n+    Currently supported models include `gemini-pro-1.0`.\n+    \"\"\"\n+\n+    def __init__(\n+        self, model_name: str = \"text-bison@002\", **kwargs,\n+    ):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        model : str\n+            Which pre-trained model from Google to use?\n+            Choices are [`text-bison@002`]",
        "comment_created_at": "2024-03-25T17:39:55+00:00",
        "comment_author": "jderry-lucem",
        "comment_body": "can you clarify what the supported models are? Additionally, `model` is not a parameter. `model_name` is. This was confusing when I was using this code for a small demo project.",
        "pr_file_module": null
      }
    ]
  }
]