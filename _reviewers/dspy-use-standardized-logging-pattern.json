[
  {
    "discussion_id": "1806865857",
    "pr_number": 1107,
    "pr_file": "dspy/retrieve/websearch.py",
    "created_at": "2024-10-18T18:20:13+00:00",
    "commented_code": "+import functools\n+import os\n+from collections import Counter\n+from typing import Any, Dict, List, Optional\n+\n+import requests\n+import torch.nn.functional as F\n+from sentence_transformers import SentenceTransformer\n+\n+import dspy\n+from dsp.modules.cache_utils import CacheMemory, cache_turn_on\n+from dspy.primitives.prediction import Prediction\n+\n+\n+class BingSearch(dspy.Retrieve):\n+    EMBEDDING_MODEL = \"avsolatorio/GIST-small-Embedding-v0\"\n+    MAX_EMB_SEQ_LEN = 512\n+    DEFAULT_SEARCH_COUNT = 10\n+\n+    def __init__(\n+        self, api_key: Optional[str] = None, endpoint: Optional[str] = None,\n+    ) -> None:\n+        if api_key is None:\n+            api_key = os.environ.get(\"BING_SEARCH_V7_SUBSCRIPTION_KEY\")\n+            if api_key is None:\n+                raise ValueError(\"BING_SEARCH_V7_SUBSCRIPTION_KEY is not set\")\n+        if endpoint is None:\n+            endpoint = os.environ.get(\"BING_SEARCH_V7_ENDPOINT\")\n+            if endpoint is None:\n+                raise ValueError(\"BING_SEARCH_V7_ENDPOINT is not set\")\n+\n+        self.api_key = api_key\n+        self.endpoint = endpoint\n+        self.cache_path = \"bing_cache.db\"\n+        self.model = SentenceTransformer(self.EMBEDDING_MODEL)\n+\n+    def forward(self, query: str, count: int = 10) -> Optional[Dict[str, Any]]:\n+        headers = {\"Ocp-Apim-Subscription-Key\": self.api_key}\n+        params = {\"q\": query, \"count\": self.DEFAULT_SEARCH_COUNT, \"mkt\": \"en-us\"}\n+\n+        endpoint = self.endpoint + \"/v7.0/search\"\n+\n+        try:\n+            response = requests.get(endpoint, headers=headers, params=params)\n+        except Exception as e:\n+            print(e)",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1806865857",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1107,
        "pr_file": "dspy/retrieve/websearch.py",
        "discussion_id": "1806865857",
        "commented_code": "@@ -0,0 +1,197 @@\n+import functools\n+import os\n+from collections import Counter\n+from typing import Any, Dict, List, Optional\n+\n+import requests\n+import torch.nn.functional as F\n+from sentence_transformers import SentenceTransformer\n+\n+import dspy\n+from dsp.modules.cache_utils import CacheMemory, cache_turn_on\n+from dspy.primitives.prediction import Prediction\n+\n+\n+class BingSearch(dspy.Retrieve):\n+    EMBEDDING_MODEL = \"avsolatorio/GIST-small-Embedding-v0\"\n+    MAX_EMB_SEQ_LEN = 512\n+    DEFAULT_SEARCH_COUNT = 10\n+\n+    def __init__(\n+        self, api_key: Optional[str] = None, endpoint: Optional[str] = None,\n+    ) -> None:\n+        if api_key is None:\n+            api_key = os.environ.get(\"BING_SEARCH_V7_SUBSCRIPTION_KEY\")\n+            if api_key is None:\n+                raise ValueError(\"BING_SEARCH_V7_SUBSCRIPTION_KEY is not set\")\n+        if endpoint is None:\n+            endpoint = os.environ.get(\"BING_SEARCH_V7_ENDPOINT\")\n+            if endpoint is None:\n+                raise ValueError(\"BING_SEARCH_V7_ENDPOINT is not set\")\n+\n+        self.api_key = api_key\n+        self.endpoint = endpoint\n+        self.cache_path = \"bing_cache.db\"\n+        self.model = SentenceTransformer(self.EMBEDDING_MODEL)\n+\n+    def forward(self, query: str, count: int = 10) -> Optional[Dict[str, Any]]:\n+        headers = {\"Ocp-Apim-Subscription-Key\": self.api_key}\n+        params = {\"q\": query, \"count\": self.DEFAULT_SEARCH_COUNT, \"mkt\": \"en-us\"}\n+\n+        endpoint = self.endpoint + \"/v7.0/search\"\n+\n+        try:\n+            response = requests.get(endpoint, headers=headers, params=params)\n+        except Exception as e:\n+            print(e)",
        "comment_created_at": "2024-10-18T18:20:13+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "better to use dspy.logger here (and all corresponding errors)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1847142971",
    "pr_number": 1739,
    "pr_file": "dspy/retrieve/retrieve.py",
    "created_at": "2024-11-18T19:33:54+00:00",
    "commented_code": "passages_dict[\"passages\"] = passages_dict.pop(\"long_text\")\n     return Prediction(**passages_dict)\n \n+@lru_cache(maxsize=None)\n+def warn_once(msg: str):\n+    logging.warning(msg)",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1847142971",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1739,
        "pr_file": "dspy/retrieve/retrieve.py",
        "discussion_id": "1847142971",
        "commented_code": "@@ -16,13 +18,20 @@ def single_query_passage(passages):\n         passages_dict[\"passages\"] = passages_dict.pop(\"long_text\")\n     return Prediction(**passages_dict)\n \n+@lru_cache(maxsize=None)\n+def warn_once(msg: str):\n+    logging.warning(msg)",
        "comment_created_at": "2024-11-18T19:33:54+00:00",
        "comment_author": "chenmoneygithub",
        "comment_body": "We have standardized the logging pattern to be: \r\n\r\n```\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\nlogger.info(...)\r\n```\r\n\r\nFor example: https://github.com/stanfordnlp/dspy/blob/943ee02d4cf9a630c89136ff6dc6b9f6ebf805dd/dspy/utils/callback.py#L12",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1645454786",
    "pr_number": 1169,
    "pr_file": "dspy/teleprompt/hard_bootstrap.py",
    "created_at": "2024-06-19T06:07:03+00:00",
    "commented_code": "+import random\n+import threading\n+\n+import tqdm\n+\n+import dsp\n+from dspy.predict.retry import Retry\n+from dspy.primitives import Example\n+\n+from .teleprompt import Teleprompter\n+from .vanilla import LabeledFewShot\n+\n+# TODO: metrics should return an object with __bool__ basically, but fine if they're more complex.\n+# They can also be sortable.\n+\n+# TODO: Switch here from dsp.Example to dspy.Example. Right now, it's okay because it's internal only (predictors).\n+# NOTE: Notice the places where we don't shuffle examples. I do like that this one doesn't shuffle.\n+# Other ones that consider options may want to use both unshuffled and then shuffle a few times, when considering candidates.\n+\n+# TODO: the max_rounds via branch_idx to get past the cache, not just temperature.\n+# In principle, we can also sample multiple outputs from the final generation step\n+# (or even each step, in case the validation function just wants *one* thing that works, but nah)\n+# and try them all. Having a pretty solid guess on the \"final step\" of each example isn't hard by the second round,\n+# in the sense that we have the trace from the first round. (Yes it may change but that's an edge case that\n+# won't hurt our \"best effort\" guarantees.)\n+\n+# TODO: When this bootstraps for another teleprompter like finetune, we want all demos we gather.\n+# But when it's for direct use we may want to sample ONE demo per predictor--example pair. This is important for \"multi-use\" modules.\n+\n+# TODO: Add baselines=[...]\n+\n+\n+class BootstrapHardFewShot(Teleprompter):\n+    def __init__(self, metric=None, teacher_settings={}, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=5, temperature=0.7, min_failures=2):\n+        self.metric = metric\n+        self.teacher_settings = teacher_settings\n+\n+        self.max_bootstrapped_demos = max_bootstrapped_demos\n+        self.max_labeled_demos = max_labeled_demos\n+        self.temperature = temperature\n+        self.max_rounds = max_rounds\n+        self.max_errors= max_errors\n+        self.error_count = 0\n+        self.min_failures = min_failures\n+        self.error_lock = threading.Lock()\n+\n+    def compile(self, student, *, teacher=None, trainset, valset=None):\n+        self.trainset = trainset\n+        self.valset = valset\n+\n+        raise Exception(\"NOTE THAT THIS HAS BEEN EXPENSIVE TO RUN. CHECK IN WITH KRISTAOO BEFORE DOING A FULL RUN.\")\n+\n+        self._prepare_student_and_teacher(student, teacher)\n+        self._prepare_predictor_mappings()\n+        # self._bootstrap()\n+        #breakpoint()\n+        self._bootstrap_hard_examples()\n+\n+        self.student = self._train()\n+        self.student._compiled = True\n+\n+        return self.student\n+    \n+    def _prepare_student_and_teacher(self, student, teacher):\n+        self.student = student.reset_copy()\n+        self.teacher = teacher.deepcopy() if teacher is not None else student.reset_copy()\n+\n+        assert self.student._compiled is False, \"Student must be uncompiled.\"\n+\n+        if self.max_labeled_demos and self.teacher._compiled is False:\n+            teleprompter = LabeledFewShot(k=self.max_labeled_demos)\n+            self.teacher = teleprompter.compile(self.teacher.reset_copy(), trainset=self.trainset)\n+\n+    def _prepare_predictor_mappings(self):\n+        name2predictor, predictor2name = {}, {}\n+        student, teacher = self.student, self.teacher\n+\n+        assert len(student.predictors()) == len(teacher.predictors()), \"Student and teacher must have the same number of predictors.\"\n+\n+        for (name1, predictor1), (name2, predictor2) in zip(student.named_predictors(), teacher.named_predictors()):\n+            assert name1 == name2, \"Student and teacher must have the same program structure.\"\n+            assert predictor1.signature == predictor2.signature, f\"Student and teacher must have the same signatures. {type(predictor1.signature)} != {type(predictor2.signature)}\"\n+            assert id(predictor1) != id(predictor2), \"Student and teacher must be different objects.\"\n+\n+            name2predictor[name1] = None # dict(student=predictor1, teacher=predictor2)\n+            predictor2name[id(predictor1)] = name1\n+\n+            # FIXME(shangyint): This is an ugly hack to bind traces of\n+            # retry.module to retry\n+            if isinstance(predictor1, Retry):\n+                predictor2name[id(predictor1.module)] = name1\n+\n+            predictor2name[id(predictor2)] = name2            \n+\n+        self.name2predictor = name2predictor\n+        self.predictor2name = predictor2name\n+    \n+    def _bootstrap_hard_examples(self, *, max_bootsraps=None):\n+        max_bootsraps = max_bootsraps or self.max_bootstrapped_demos\n+\n+        bootstrapped = {}\n+        self.name2traces = {name: [] for name in self.name2predictor}\n+\n+        hard_examples = []\n+        # start by getting our pool of hard examples. we'll define these as examples that fail >= 3/5 times\n+        total_attempts = 5\n+        min_failed_attempts = self.min_failures\n+\n+        for round_idx in range(self.max_rounds):\n+            for example_idx, example in enumerate(tqdm.tqdm(self.trainset)):\n+                total_successes = 0\n+                successful_attempt = None\n+                add_example_to_traces = True\n+\n+                # Try # attempt times to see what % of attempts we get correct\n+                for i in range(total_attempts):\n+                    if total_successes > 0:\n+                        add_example_to_traces = False # We only need one successful demo for this example \n+                    print(f\"attempting bootstrap for {i+1}th time.\")\n+                    success, predictor_name, demo = self._bootstrap_one_example(example, i)\n+                    print(f\"success of bootstrap: {success}\")\n+                    if success:\n+                        successful_attempt = success \n+                        total_successes += int(success)\n+                    if total_successes > (total_attempts - min_failed_attempts):\n+                        break\n+                # if this example failed the right number of times and is therefore 'hard', let's add it as one of our demo examples\n+                # print(f\"Total Successes: {total_successes} | Total Failures: {total_attempts-total_successes}\")",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1645454786",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 1169,
        "pr_file": "dspy/teleprompt/hard_bootstrap.py",
        "discussion_id": "1645454786",
        "commented_code": "@@ -0,0 +1,239 @@\n+import random\n+import threading\n+\n+import tqdm\n+\n+import dsp\n+from dspy.predict.retry import Retry\n+from dspy.primitives import Example\n+\n+from .teleprompt import Teleprompter\n+from .vanilla import LabeledFewShot\n+\n+# TODO: metrics should return an object with __bool__ basically, but fine if they're more complex.\n+# They can also be sortable.\n+\n+# TODO: Switch here from dsp.Example to dspy.Example. Right now, it's okay because it's internal only (predictors).\n+# NOTE: Notice the places where we don't shuffle examples. I do like that this one doesn't shuffle.\n+# Other ones that consider options may want to use both unshuffled and then shuffle a few times, when considering candidates.\n+\n+# TODO: the max_rounds via branch_idx to get past the cache, not just temperature.\n+# In principle, we can also sample multiple outputs from the final generation step\n+# (or even each step, in case the validation function just wants *one* thing that works, but nah)\n+# and try them all. Having a pretty solid guess on the \"final step\" of each example isn't hard by the second round,\n+# in the sense that we have the trace from the first round. (Yes it may change but that's an edge case that\n+# won't hurt our \"best effort\" guarantees.)\n+\n+# TODO: When this bootstraps for another teleprompter like finetune, we want all demos we gather.\n+# But when it's for direct use we may want to sample ONE demo per predictor--example pair. This is important for \"multi-use\" modules.\n+\n+# TODO: Add baselines=[...]\n+\n+\n+class BootstrapHardFewShot(Teleprompter):\n+    def __init__(self, metric=None, teacher_settings={}, max_bootstrapped_demos=4, max_labeled_demos=16, max_rounds=1, max_errors=5, temperature=0.7, min_failures=2):\n+        self.metric = metric\n+        self.teacher_settings = teacher_settings\n+\n+        self.max_bootstrapped_demos = max_bootstrapped_demos\n+        self.max_labeled_demos = max_labeled_demos\n+        self.temperature = temperature\n+        self.max_rounds = max_rounds\n+        self.max_errors= max_errors\n+        self.error_count = 0\n+        self.min_failures = min_failures\n+        self.error_lock = threading.Lock()\n+\n+    def compile(self, student, *, teacher=None, trainset, valset=None):\n+        self.trainset = trainset\n+        self.valset = valset\n+\n+        raise Exception(\"NOTE THAT THIS HAS BEEN EXPENSIVE TO RUN. CHECK IN WITH KRISTAOO BEFORE DOING A FULL RUN.\")\n+\n+        self._prepare_student_and_teacher(student, teacher)\n+        self._prepare_predictor_mappings()\n+        # self._bootstrap()\n+        #breakpoint()\n+        self._bootstrap_hard_examples()\n+\n+        self.student = self._train()\n+        self.student._compiled = True\n+\n+        return self.student\n+    \n+    def _prepare_student_and_teacher(self, student, teacher):\n+        self.student = student.reset_copy()\n+        self.teacher = teacher.deepcopy() if teacher is not None else student.reset_copy()\n+\n+        assert self.student._compiled is False, \"Student must be uncompiled.\"\n+\n+        if self.max_labeled_demos and self.teacher._compiled is False:\n+            teleprompter = LabeledFewShot(k=self.max_labeled_demos)\n+            self.teacher = teleprompter.compile(self.teacher.reset_copy(), trainset=self.trainset)\n+\n+    def _prepare_predictor_mappings(self):\n+        name2predictor, predictor2name = {}, {}\n+        student, teacher = self.student, self.teacher\n+\n+        assert len(student.predictors()) == len(teacher.predictors()), \"Student and teacher must have the same number of predictors.\"\n+\n+        for (name1, predictor1), (name2, predictor2) in zip(student.named_predictors(), teacher.named_predictors()):\n+            assert name1 == name2, \"Student and teacher must have the same program structure.\"\n+            assert predictor1.signature == predictor2.signature, f\"Student and teacher must have the same signatures. {type(predictor1.signature)} != {type(predictor2.signature)}\"\n+            assert id(predictor1) != id(predictor2), \"Student and teacher must be different objects.\"\n+\n+            name2predictor[name1] = None # dict(student=predictor1, teacher=predictor2)\n+            predictor2name[id(predictor1)] = name1\n+\n+            # FIXME(shangyint): This is an ugly hack to bind traces of\n+            # retry.module to retry\n+            if isinstance(predictor1, Retry):\n+                predictor2name[id(predictor1.module)] = name1\n+\n+            predictor2name[id(predictor2)] = name2            \n+\n+        self.name2predictor = name2predictor\n+        self.predictor2name = predictor2name\n+    \n+    def _bootstrap_hard_examples(self, *, max_bootsraps=None):\n+        max_bootsraps = max_bootsraps or self.max_bootstrapped_demos\n+\n+        bootstrapped = {}\n+        self.name2traces = {name: [] for name in self.name2predictor}\n+\n+        hard_examples = []\n+        # start by getting our pool of hard examples. we'll define these as examples that fail >= 3/5 times\n+        total_attempts = 5\n+        min_failed_attempts = self.min_failures\n+\n+        for round_idx in range(self.max_rounds):\n+            for example_idx, example in enumerate(tqdm.tqdm(self.trainset)):\n+                total_successes = 0\n+                successful_attempt = None\n+                add_example_to_traces = True\n+\n+                # Try # attempt times to see what % of attempts we get correct\n+                for i in range(total_attempts):\n+                    if total_successes > 0:\n+                        add_example_to_traces = False # We only need one successful demo for this example \n+                    print(f\"attempting bootstrap for {i+1}th time.\")\n+                    success, predictor_name, demo = self._bootstrap_one_example(example, i)\n+                    print(f\"success of bootstrap: {success}\")\n+                    if success:\n+                        successful_attempt = success \n+                        total_successes += int(success)\n+                    if total_successes > (total_attempts - min_failed_attempts):\n+                        break\n+                # if this example failed the right number of times and is therefore 'hard', let's add it as one of our demo examples\n+                # print(f\"Total Successes: {total_successes} | Total Failures: {total_attempts-total_successes}\")",
        "comment_created_at": "2024-06-19T06:07:03+00:00",
        "comment_author": "mikeedjones",
        "comment_body": "I thought dspy had moved from print statements to logging?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1563447821",
    "pr_number": 797,
    "pr_file": "dsp/primitives/search.py",
    "created_at": "2024-04-13T00:52:37+00:00",
    "commented_code": "\"\"\"Retrieves passages from the RM for the query and returns the top k passages.\"\"\"\n     if not dsp.settings.rm:\n         raise AssertionError(\"No RM is loaded.\")\n+    if not dsp.settings.reranker:\n+        warnings.warn(\"If you want to use the Reranker, please use dspy.RetrieveThenRerank\")",
    "repo_full_name": "stanfordnlp/dspy",
    "discussion_comments": [
      {
        "comment_id": "1563447821",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 797,
        "pr_file": "dsp/primitives/search.py",
        "discussion_id": "1563447821",
        "commented_code": "@@ -9,17 +10,21 @@ def retrieve(query: str, k: int, **kwargs) -> list[str]:\n     \"\"\"Retrieves passages from the RM for the query and returns the top k passages.\"\"\"\n     if not dsp.settings.rm:\n         raise AssertionError(\"No RM is loaded.\")\n+    if not dsp.settings.reranker:\n+        warnings.warn(\"If you want to use the Reranker, please use dspy.RetrieveThenRerank\")",
        "comment_created_at": "2024-04-13T00:52:37+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "can we instead handle this through logging and through a Depecration message?",
        "pr_file_module": null
      },
      {
        "comment_id": "1564231142",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 797,
        "pr_file": "dsp/primitives/search.py",
        "discussion_id": "1563447821",
        "commented_code": "@@ -9,17 +10,21 @@ def retrieve(query: str, k: int, **kwargs) -> list[str]:\n     \"\"\"Retrieves passages from the RM for the query and returns the top k passages.\"\"\"\n     if not dsp.settings.rm:\n         raise AssertionError(\"No RM is loaded.\")\n+    if not dsp.settings.reranker:\n+        warnings.warn(\"If you want to use the Reranker, please use dspy.RetrieveThenRerank\")",
        "comment_created_at": "2024-04-13T20:10:06+00:00",
        "comment_author": "Athe-kunal",
        "comment_body": "@arnavsinghvi11, can you explain what you mean by logging?\r\nDo I have to create a python logging object file and then log these? Sorry if this is a trivial question\r\n\r\nI have added the deprecation warning for now",
        "pr_file_module": null
      },
      {
        "comment_id": "1582254206",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 797,
        "pr_file": "dsp/primitives/search.py",
        "discussion_id": "1563447821",
        "commented_code": "@@ -9,17 +10,21 @@ def retrieve(query: str, k: int, **kwargs) -> list[str]:\n     \"\"\"Retrieves passages from the RM for the query and returns the top k passages.\"\"\"\n     if not dsp.settings.rm:\n         raise AssertionError(\"No RM is loaded.\")\n+    if not dsp.settings.reranker:\n+        warnings.warn(\"If you want to use the Reranker, please use dspy.RetrieveThenRerank\")",
        "comment_created_at": "2024-04-28T17:46:28+00:00",
        "comment_author": "arnavsinghvi11",
        "comment_body": "https://github.com/stanfordnlp/dspy/blob/d09d984ecaf17f7262294d50fe46fd8105fbf291/dspy/evaluate/evaluate.py#L56 - feel free to reference this",
        "pr_file_module": null
      },
      {
        "comment_id": "1582593521",
        "repo_full_name": "stanfordnlp/dspy",
        "pr_number": 797,
        "pr_file": "dsp/primitives/search.py",
        "discussion_id": "1563447821",
        "commented_code": "@@ -9,17 +10,21 @@ def retrieve(query: str, k: int, **kwargs) -> list[str]:\n     \"\"\"Retrieves passages from the RM for the query and returns the top k passages.\"\"\"\n     if not dsp.settings.rm:\n         raise AssertionError(\"No RM is loaded.\")\n+    if not dsp.settings.reranker:\n+        warnings.warn(\"If you want to use the Reranker, please use dspy.RetrieveThenRerank\")",
        "comment_created_at": "2024-04-29T06:32:12+00:00",
        "comment_author": "Athe-kunal",
        "comment_body": "The dspy logger object is not available in the dsp folder, hence I followed logging as done [here](https://github.com/stanfordnlp/dspy/blob/6ef5e37d75e3555d91905401bfc374e45ab68282/dsp/modules/anthropic.py#L16) for anthropic LM. Is there a better way to log this?",
        "pr_file_module": null
      }
    ]
  }
]