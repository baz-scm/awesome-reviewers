[
  {
    "discussion_id": "2095073098",
    "pr_number": 17507,
    "pr_file": "scripts/regression/test_runner.py",
    "created_at": "2025-05-19T07:51:50+00:00",
    "commented_code": "other_results.append(BenchmarkResult(benchmark, old_res, new_res))\n     benchmark_list = [res.benchmark for res in regression_list]\n \n+\n+time_old = geomean(old_runner.complete_timings)\n+time_new = geomean(new_runner.complete_timings)\n+\n exit_code = 0\n+allowed_regressions: List[BenchmarkResult] = []\n+regressions_to_report: List[BenchmarkResult] = []\n regression_list.extend(error_list)\n summary = []\n+\n if len(regression_list) > 0:\n-    exit_code = 1\n-    print(\n-        '''====================================================\n+    print(\"HERE\")\n+    # filter regression_list to allowed and regressions to report\n+    for regression in regression_list:\n+        if isinstance(regression.old_result, (int, float)) or isinstance(regression.new_result, (int, float)):\n+            individual_regression_diff_perc = (\n+                (regression.new_result - regression.old_result) / regression.old_result\n+            ) * 100\n+            if isinstance(time_old, float) and isinstance(time_new, float):",
    "repo_full_name": "duckdb/duckdb",
    "discussion_comments": [
      {
        "comment_id": "2095073098",
        "repo_full_name": "duckdb/duckdb",
        "pr_number": 17507,
        "pr_file": "scripts/regression/test_runner.py",
        "discussion_id": "2095073098",
        "commented_code": "@@ -136,29 +136,86 @@ class BenchmarkResult:\n             other_results.append(BenchmarkResult(benchmark, old_res, new_res))\n     benchmark_list = [res.benchmark for res in regression_list]\n \n+\n+time_old = geomean(old_runner.complete_timings)\n+time_new = geomean(new_runner.complete_timings)\n+\n exit_code = 0\n+allowed_regressions: List[BenchmarkResult] = []\n+regressions_to_report: List[BenchmarkResult] = []\n regression_list.extend(error_list)\n summary = []\n+\n if len(regression_list) > 0:\n-    exit_code = 1\n-    print(\n-        '''====================================================\n+    print(\"HERE\")\n+    # filter regression_list to allowed and regressions to report\n+    for regression in regression_list:\n+        if isinstance(regression.old_result, (int, float)) or isinstance(regression.new_result, (int, float)):\n+            individual_regression_diff_perc = (\n+                (regression.new_result - regression.old_result) / regression.old_result\n+            ) * 100\n+            if isinstance(time_old, float) and isinstance(time_new, float):",
        "comment_created_at": "2025-05-19T07:51:50+00:00",
        "comment_author": "Tmonster",
        "comment_body": "I think you can run this check once outside of the for loop\r\n\r\nSo outside the for loop something like this\r\n```\r\nif isinstance(time_old, float) and isinstance(time_new, float):\r\n    gemean_time_slower = time_new <= time_old\r\n```\r\n\r\nThen inside\r\n\r\n```\r\nif gemean_time_slower and individual_regression_diff_perc <= 10.0:\r\n```\r\n\r\nAnd maybe the 10% can be a value we pass to the script with a default of 10. You can name it something like `MAX_ALLOWED_REGRESS_PERCENTAGE`\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2095147050",
        "repo_full_name": "duckdb/duckdb",
        "pr_number": 17507,
        "pr_file": "scripts/regression/test_runner.py",
        "discussion_id": "2095073098",
        "commented_code": "@@ -136,29 +136,86 @@ class BenchmarkResult:\n             other_results.append(BenchmarkResult(benchmark, old_res, new_res))\n     benchmark_list = [res.benchmark for res in regression_list]\n \n+\n+time_old = geomean(old_runner.complete_timings)\n+time_new = geomean(new_runner.complete_timings)\n+\n exit_code = 0\n+allowed_regressions: List[BenchmarkResult] = []\n+regressions_to_report: List[BenchmarkResult] = []\n regression_list.extend(error_list)\n summary = []\n+\n if len(regression_list) > 0:\n-    exit_code = 1\n-    print(\n-        '''====================================================\n+    print(\"HERE\")\n+    # filter regression_list to allowed and regressions to report\n+    for regression in regression_list:\n+        if isinstance(regression.old_result, (int, float)) or isinstance(regression.new_result, (int, float)):\n+            individual_regression_diff_perc = (\n+                (regression.new_result - regression.old_result) / regression.old_result\n+            ) * 100\n+            if isinstance(time_old, float) and isinstance(time_new, float):",
        "comment_created_at": "2025-05-19T08:28:42+00:00",
        "comment_author": "hmeriann",
        "comment_body": "oh yeah, this looks much cleaner and easier to read!\r\nI'll add the MAX_ALLOWED_REGRESS_PERCENTAGE as an argument with no default value, because we don't set this boundary for the faster benchmarks to let them be considered as regression when they fail individually",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2096011953",
    "pr_number": 17507,
    "pr_file": "scripts/regression/test_runner.py",
    "created_at": "2025-05-19T15:36:53+00:00",
    "commented_code": "time_new = geomean(new_runner.complete_timings)\n \n exit_code = 0\n-allowed_regressions: List[BenchmarkResult] = []\n-regressions_to_report: List[BenchmarkResult] = []\n regression_list.extend(error_list)\n summary = []\n \n if len(regression_list) > 0:\n-    print(\"HERE\")\n-    # filter regression_list to allowed and regressions to report\n-    for regression in regression_list:\n-        if isinstance(regression.old_result, (int, float)) or isinstance(regression.new_result, (int, float)):\n-            individual_regression_diff_perc = (\n-                (regression.new_result - regression.old_result) / regression.old_result\n-            ) * 100\n-            if isinstance(time_old, float) and isinstance(time_new, float):\n-                if time_new <= time_old and individual_regression_diff_perc <= 10.0:\n-                    # allow individual regressions less than 10% when overall geomean had improved or hadn't change\n-                    allowed_regressions.append(regression)\n-                else:\n-                    regressions_to_report.append(regression)\n-\n-    exit_code = 1 if len(regressions_to_report) > 0 else 0\n-\n-    if len(allowed_regressions) > 0:\n+    # regression_list already consists of the benchmarks regressed of more that 10%\n+    regression_percentage = int((time_new - time_old) * 100.0 / time_new)\n+    if isinstance(MAX_ALLOWED_REGRESS_PERCENTAGE, int) and regression_percentage < MAX_ALLOWED_REGRESS_PERCENTAGE:",
    "repo_full_name": "duckdb/duckdb",
    "discussion_comments": [
      {
        "comment_id": "2096011953",
        "repo_full_name": "duckdb/duckdb",
        "pr_number": 17507,
        "pr_file": "scripts/regression/test_runner.py",
        "discussion_id": "2096011953",
        "commented_code": "@@ -141,59 +150,28 @@ class BenchmarkResult:\n time_new = geomean(new_runner.complete_timings)\n \n exit_code = 0\n-allowed_regressions: List[BenchmarkResult] = []\n-regressions_to_report: List[BenchmarkResult] = []\n regression_list.extend(error_list)\n summary = []\n \n if len(regression_list) > 0:\n-    print(\"HERE\")\n-    # filter regression_list to allowed and regressions to report\n-    for regression in regression_list:\n-        if isinstance(regression.old_result, (int, float)) or isinstance(regression.new_result, (int, float)):\n-            individual_regression_diff_perc = (\n-                (regression.new_result - regression.old_result) / regression.old_result\n-            ) * 100\n-            if isinstance(time_old, float) and isinstance(time_new, float):\n-                if time_new <= time_old and individual_regression_diff_perc <= 10.0:\n-                    # allow individual regressions less than 10% when overall geomean had improved or hadn't change\n-                    allowed_regressions.append(regression)\n-                else:\n-                    regressions_to_report.append(regression)\n-\n-    exit_code = 1 if len(regressions_to_report) > 0 else 0\n-\n-    if len(allowed_regressions) > 0:\n+    # regression_list already consists of the benchmarks regressed of more that 10%\n+    regression_percentage = int((time_new - time_old) * 100.0 / time_new)\n+    if isinstance(MAX_ALLOWED_REGRESS_PERCENTAGE, int) and regression_percentage < MAX_ALLOWED_REGRESS_PERCENTAGE:",
        "comment_created_at": "2025-05-19T15:36:53+00:00",
        "comment_author": "Tmonster",
        "comment_body": "why are we no longer looping through all regressions? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2096106914",
        "repo_full_name": "duckdb/duckdb",
        "pr_number": 17507,
        "pr_file": "scripts/regression/test_runner.py",
        "discussion_id": "2096011953",
        "commented_code": "@@ -141,59 +150,28 @@ class BenchmarkResult:\n time_new = geomean(new_runner.complete_timings)\n \n exit_code = 0\n-allowed_regressions: List[BenchmarkResult] = []\n-regressions_to_report: List[BenchmarkResult] = []\n regression_list.extend(error_list)\n summary = []\n \n if len(regression_list) > 0:\n-    print(\"HERE\")\n-    # filter regression_list to allowed and regressions to report\n-    for regression in regression_list:\n-        if isinstance(regression.old_result, (int, float)) or isinstance(regression.new_result, (int, float)):\n-            individual_regression_diff_perc = (\n-                (regression.new_result - regression.old_result) / regression.old_result\n-            ) * 100\n-            if isinstance(time_old, float) and isinstance(time_new, float):\n-                if time_new <= time_old and individual_regression_diff_perc <= 10.0:\n-                    # allow individual regressions less than 10% when overall geomean had improved or hadn't change\n-                    allowed_regressions.append(regression)\n-                else:\n-                    regressions_to_report.append(regression)\n-\n-    exit_code = 1 if len(regressions_to_report) > 0 else 0\n-\n-    if len(allowed_regressions) > 0:\n+    # regression_list already consists of the benchmarks regressed of more that 10%\n+    regression_percentage = int((time_new - time_old) * 100.0 / time_new)\n+    if isinstance(MAX_ALLOWED_REGRESS_PERCENTAGE, int) and regression_percentage < MAX_ALLOWED_REGRESS_PERCENTAGE:",
        "comment_created_at": "2025-05-19T16:27:53+00:00",
        "comment_author": "hmeriann",
        "comment_body": "To create a regression list we iterate over all the benchmarks and fall to three cases: \r\n- the benchmark run result is a string => it goes to the `error_list`\r\n- the benchmark result on `new` build is slower => `regression_list`\r\n- all other cases => `other_results`\r\n\r\nThe decision is `new` run's complete timing had regressed is made by the [calculations](https://github.com/duckdb/duckdb/blob/6e13b87b52176e2b598ea04d5b1044279e742c21/scripts/regression/test_runner.py#L132) in test runner. It basically does the same as we tried to do comparing each individual benchmark result with `MAX_ALLOWED_REGRESS_PERCENTAGE`. \r\n\r\nSo, once a benchmark is already classified into the `regression_list` using that rule, I don't think we need to check again each individual entry against `MAX_ALLOWED_REGRESS_PERCENTAGE`. Instead, we can just check whether the overall geomean exceeds `MAX_ALLOWED_REGRESS_PERCENTAGE` and report all regressions if that\u2019s the case.",
        "pr_file_module": null
      }
    ]
  }
]