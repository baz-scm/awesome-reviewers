[
  {
    "discussion_id": "2191898797",
    "pr_number": 18172,
    "pr_file": "extension/parquet/parquet_reader.cpp",
    "created_at": "2025-07-08T08:56:17+00:00",
    "commented_code": "}\n \n \t\tauto &group = GetGroup(state);\n+\t\tif (state.op) {\n+\t\t\tconst auto event = state.offset_in_group == (idx_t)group.num_rows ? \"SkipRowGroup\" : \"ReadRowGroup\";",
    "repo_full_name": "duckdb/duckdb",
    "discussion_comments": [
      {
        "comment_id": "2191898797",
        "repo_full_name": "duckdb/duckdb",
        "pr_number": 18172,
        "pr_file": "extension/parquet/parquet_reader.cpp",
        "discussion_id": "2191898797",
        "commented_code": "@@ -1190,6 +1190,12 @@ bool ParquetReader::ScanInternal(ClientContext &context, ParquetReaderScanState\n \t\t}\n \n \t\tauto &group = GetGroup(state);\n+\t\tif (state.op) {\n+\t\t\tconst auto event = state.offset_in_group == (idx_t)group.num_rows ? \"SkipRowGroup\" : \"ReadRowGroup\";",
        "comment_created_at": "2025-07-08T08:56:17+00:00",
        "comment_author": "samansmink",
        "comment_body": "kind of a nit in this case because it's either super cheap or even optimized out by the compiler when logging is disabled, but we should strive to not execute any logging related code unless we are actually writing the log\r\n\r\nThe solution here would be to invoke the logger like:\r\n\r\n```C++\r\nDUCKDB_LOG(context, \r\n    PhysicalOperatorLogType, \r\n    *state.op, \r\n    \"ParquetReader\", \r\n    state.offset_in_group == (idx_t)group.num_rows ? \"SkipRowGroup\" : \"ReadRowGroup\",\r\n    {{\"file\", file.path}, {\"row_group_id\", to_string(state.group_idx_list[state.current_group])}}\r\n);\r\n```\r\n\r\nI think I will look into improving the logging macros in a follow up PR so we can make this a bit cleaner by allowing splitting up the SHOULD_LOG and WRITE_LOG calls:\r\n```C++\r\nif (DUCKDB_SHOULD_LOG(context, PhysicalOperatorLogType)) {\r\n    const auto event = state.offset_in_group == (idx_t)group.num_rows ? \"SkipRowGroup\" : \"ReadRowGroup\";\r\n    vector<pair<string, string>> log_payload = {\r\n\t    {\"file\", file.path},\r\n\t    {\"row_group_id\", to_string(state.group_idx_list[state.current_group])}\r\n    };\r\n    DUCKDB_WRITE_LOG(context, PhysicalOperatorLogType, *state.op, \"ParquetReader\", event, log_payload);\r\n}\r\n\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2025499440",
    "pr_number": 16833,
    "pr_file": "test/sqlite/sqllogic_test_logger.cpp",
    "created_at": "2025-04-02T19:58:55+00:00",
    "commented_code": "std::cerr << \"Expected \" << termcolor::bold << expected_column_count << termcolor::reset << \" columns, but got \"\n \t          << termcolor::bold << split_count << termcolor::reset << \" columns\" << std::endl;\n \tstd::cerr << \"Does the result contain tab values? In that case, place every value on a single row.\" << std::endl;\n+\tGetSummary() << \"Expected \" + to_string(expected_column_count) + \" columns, but got \" + to_string(split_count) +",
    "repo_full_name": "duckdb/duckdb",
    "discussion_comments": [
      {
        "comment_id": "2025499440",
        "repo_full_name": "duckdb/duckdb",
        "pr_number": 16833,
        "pr_file": "test/sqlite/sqllogic_test_logger.cpp",
        "discussion_id": "2025499440",
        "commented_code": "@@ -238,6 +274,9 @@ void SQLLogicTestLogger::SplitMismatch(idx_t row_number, idx_t expected_column_c\n \tstd::cerr << \"Expected \" << termcolor::bold << expected_column_count << termcolor::reset << \" columns, but got \"\n \t          << termcolor::bold << split_count << termcolor::reset << \" columns\" << std::endl;\n \tstd::cerr << \"Does the result contain tab values? In that case, place every value on a single row.\" << std::endl;\n+\tGetSummary() << \"Expected \" + to_string(expected_column_count) + \" columns, but got \" + to_string(split_count) +",
        "comment_created_at": "2025-04-02T19:58:55+00:00",
        "comment_author": "Tmonster",
        "comment_body": "It still looks like we are still creating the string twice and then adding it to two different string streams. Let's take a look at this on Friday",
        "pr_file_module": null
      },
      {
        "comment_id": "2026846989",
        "repo_full_name": "duckdb/duckdb",
        "pr_number": 16833,
        "pr_file": "test/sqlite/sqllogic_test_logger.cpp",
        "discussion_id": "2025499440",
        "commented_code": "@@ -238,6 +274,9 @@ void SQLLogicTestLogger::SplitMismatch(idx_t row_number, idx_t expected_column_c\n \tstd::cerr << \"Expected \" << termcolor::bold << expected_column_count << termcolor::reset << \" columns, but got \"\n \t          << termcolor::bold << split_count << termcolor::reset << \" columns\" << std::endl;\n \tstd::cerr << \"Does the result contain tab values? In that case, place every value on a single row.\" << std::endl;\n+\tGetSummary() << \"Expected \" + to_string(expected_column_count) + \" columns, but got \" + to_string(split_count) +",
        "comment_created_at": "2025-04-03T11:57:40+00:00",
        "comment_author": "hmeriann",
        "comment_body": "I've updated the logger where it looked possible to use a `log_str` variable to build a string and output it to `std::cerr` or to `GetSummary()` afterwards. \r\nIt would be great to look together at the places where the `termcolor` is used.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2256882020",
    "pr_number": 17692,
    "pr_file": "src/logging/log_storage.cpp",
    "created_at": "2025-08-06T11:40:42+00:00",
    "commented_code": "throw NotImplementedException(\"Not implemented for this LogStorage: TruncateLogStorage\");\n }\n \n-StdOutLogStorage::StdOutLogStorage() {\n+void LogStorage::UpdateConfig(DatabaseInstance &db, case_insensitive_map_t<Value> &config) {\n+\tif (config.size() > 1) {\n+\t\tthrow InvalidInputException(\"LogStorage does not support passing configuration\");\n+\t}\n+}\n+\n+unique_ptr<TableRef> LogStorage::BindReplaceEntries(ClientContext &context, TableFunctionBindInput &input) {\n+\treturn nullptr;\n+}\n+\n+unique_ptr<TableRef> LogStorage::BindReplaceContexts(ClientContext &context, TableFunctionBindInput &input) {\n+\treturn nullptr;\n+}\n+\n+CSVLogStorage::~CSVLogStorage() {\n+}\n+\n+CSVLogStorage::CSVLogStorage(DatabaseInstance &db) : BufferingLogStorage(db) {\n+\tResetCastChunk();\n+}\n+\n+void CSVLogStorage::UpdateConfig(DatabaseInstance &db, case_insensitive_map_t<Value> &config) {\n+\tlock_guard<mutex> lck(lock);\n+\treturn UpdateConfigInternal(db, config);\n+}\n+\n+void CSVLogStorage::ExecuteCast() {\n+\tlog_entries_cast_buffer->Reset();\n+\tlog_contexts_cast_buffer->Reset();\n+\n+\tbool success = true;\n+\n+\tCastParameters cast_params;\n+\n+\tif (normalize_contexts) {\n+\t\t// -- Cast Log Entries\n+\t\t// context_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[0], log_entries_cast_buffer->data[0], log_entries_buffer->size(), cast_params);\n+\t\t// timestamp: LogicalType::TIMESTAMP\n+\t\tsuccess &= VectorCastHelpers::StringCast<timestamp_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[1], log_entries_cast_buffer->data[1], log_entries_buffer->size(), cast_params);\n+\t\t// log_type: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[2].Reference(log_entries_buffer->data[2]);\n+\t\t// level: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[3].Reference(log_entries_buffer->data[3]);\n+\t\t// message: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[4].Reference(log_entries_buffer->data[4]);\n+\n+\t\tlog_entries_cast_buffer->SetCardinality(log_entries_buffer->size());\n+\n+\t\t// -- Cast Log Contexts\n+\t\t// context_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[0], log_contexts_cast_buffer->data[0], log_contexts_buffer->size(), cast_params);\n+\t\t// scope: LogicalType::VARCHAR (no cast)\n+\t\tlog_contexts_cast_buffer->data[1].Reference(log_contexts_buffer->data[1]);\n+\t\t// connection_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[2], log_contexts_cast_buffer->data[2], log_contexts_buffer->size(), cast_params);\n+\t\t// transaction_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[3], log_contexts_cast_buffer->data[3], log_contexts_buffer->size(), cast_params);\n+\t\t// query_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[4], log_contexts_cast_buffer->data[4], log_contexts_buffer->size(), cast_params);\n+\t\t// thread: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[5], log_contexts_cast_buffer->data[5], log_contexts_buffer->size(), cast_params);\n+\t\t// scope is already string so doesn't need casting\n+\n+\t\tlog_contexts_cast_buffer->SetCardinality(log_contexts_buffer->size());\n+\t} else {\n+\t\t// -- Cast Log Entries\n+\n+\t\t// context_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[0], log_entries_cast_buffer->data[0], log_entries_buffer->size(), cast_params);\n+\t\t// scope: LogicalType::VARCHAR (no cast)\n+\t\tlog_entries_cast_buffer->data[1].Reference(log_entries_buffer->data[1]);\n+\t\t// connection_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[2], log_entries_cast_buffer->data[2], log_entries_buffer->size(), cast_params);\n+\t\t// transaction_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[3], log_entries_cast_buffer->data[3], log_entries_buffer->size(), cast_params);\n+\t\t// query_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[4], log_entries_cast_buffer->data[4], log_entries_buffer->size(), cast_params);\n+\t\t// thread: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[5], log_entries_cast_buffer->data[5], log_entries_buffer->size(), cast_params);\n+\t\t// timestamp: LogicalType::TIMESTAMP\n+\t\tsuccess &= VectorCastHelpers::StringCast<timestamp_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[6], log_entries_cast_buffer->data[6], log_entries_buffer->size(), cast_params);\n+\t\t// log_type: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[7].Reference(log_entries_buffer->data[7]);\n+\t\t// level: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[8].Reference(log_entries_buffer->data[8]);\n+\t\t// message: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[9].Reference(log_entries_buffer->data[9]);\n+\n+\t\tlog_entries_cast_buffer->SetCardinality(log_entries_buffer->size());\n+\t}\n+\n+\tif (!success) {\n+\t\tthrow InvalidInputException(\"Failed to cast log entries\");\n+\t}\n+}\n+\n+void CSVLogStorage::ResetAllBuffers() {\n+\tBufferingLogStorage::ResetAllBuffers();\n+\tResetCastChunk();\n+}\n+\n+void CSVLogStorage::ResetCastChunk() {\n+\tlog_entries_cast_buffer = make_uniq<DataChunk>();\n+\tlog_contexts_cast_buffer = make_uniq<DataChunk>();\n+\n+\t// Initialize the Cast chunks for casting everything to strings\n+\tvector<LogicalType> types;\n+\ttypes.resize(log_entries_buffer->ColumnCount(), LogicalType::VARCHAR);\n+\tlog_entries_cast_buffer->Initialize(Allocator::DefaultAllocator(), types);\n+\n+\ttypes.resize(log_contexts_buffer->ColumnCount(), LogicalType::VARCHAR);\n+\tlog_contexts_cast_buffer->Initialize(Allocator::DefaultAllocator(), types);\n+}\n+\n+void CSVLogStorage::SetWriterConfigs(CSVWriter &writer, vector<string> column_names) {\n+\twriter.options.dialect_options.state_machine_options.escape = '\\\"';\n+\twriter.options.dialect_options.state_machine_options.quote = '\\\"';\n+\twriter.options.dialect_options.state_machine_options.delimiter = CSVOption<string>(\"\\t\");\n+\twriter.options.name_list = column_names;\n+\n+\twriter.options.force_quote = vector<bool>(column_names.size(), false);\n+}\n+\n+void CSVLogStorage::FlushInternal() {\n+\t// Execute the cast\n+\tExecuteCast();\n+\n+\t// Write the cast data to sCSV\n+\tif (log_entries_cast_buffer->size() > 0) {\n+\t\tlog_entries_writer->WriteChunk(*log_entries_cast_buffer);\n+\t\tlog_entries_writer->Flush();\n+\t\tlog_entries_buffer->Reset();\n+\t}\n+\n+\tif (log_contexts_cast_buffer->size() > 0) {\n+\t\tlog_contexts_writer->WriteChunk(*log_contexts_cast_buffer);\n+\t\tlog_contexts_writer->Flush();\n+\t\tlog_contexts_buffer->Reset();\n+\t}\n+}\n+\n+void CSVLogStorage::UpdateConfigInternal(DatabaseInstance &db, case_insensitive_map_t<Value> &config) {\n+\tfor (const auto &it : config) {\n+\t\tif (StringUtil::Lower(it.first) == \"buffer_size\") {\n+\t\t\tbuffer_limit = it.second.GetValue<uint64_t>();\n+\t\t} else {\n+\t\t\tthrow InvalidInputException(\"Unrecognized log storage config option: '%s'\", it.first);\n+\t\t}\n+\t}\n+}\n+\n+StdOutLogStorage::StdOutLogStorage(DatabaseInstance &db) : CSVLogStorage(db) {\n+\tlog_entries_stream = make_uniq<MemoryStream>();\n+\tlog_contexts_stream = make_uniq<MemoryStream>();\n+\tlog_entries_writer = make_uniq<CSVWriter>(*log_contexts_stream, GetEntriesColumnNames(true), false);\n+\tlog_contexts_writer = make_uniq<CSVWriter>(*log_contexts_stream, GetContextsColumnNames(), false);\n+\n+\tSetWriterConfigs(*log_entries_writer, GetEntriesColumnNames(normalize_contexts));\n+\tSetWriterConfigs(*log_contexts_writer, GetContextsColumnNames());\n }\n \n StdOutLogStorage::~StdOutLogStorage() {\n }\n \n-void StdOutLogStorage::WriteLogEntry(timestamp_t timestamp, LogLevel level, const string &log_type,\n-                                     const string &log_message, const RegisteredLoggingContext &context) {\n-\tstd::cout << StringUtil::Format(\n-\t    \"[LOG] %s, %s, %s, %s, %s, %s, %s, %s\n\", Value::TIMESTAMP(timestamp).ToString(), log_type,\n-\t    EnumUtil::ToString(level), log_message, EnumUtil::ToString(context.context.scope),\n-\t    context.context.connection_id.IsValid() ? to_string(context.context.connection_id.GetIndex()) : \"NULL\",\n-\t    context.context.transaction_id.IsValid() ? to_string(context.context.transaction_id.GetIndex()) : \"NULL\",\n-\t    context.context.thread_id.IsValid() ? to_string(context.context.thread_id.GetIndex()) : \"NULL\");\n+void StdOutLogStorage::FlushInternal() {\n+\t// Flush CSV buffer into stream\n+\tCSVLogStorage::FlushInternal();\n+\n+\t// Write stream to stdout\n+\tstd::cout.write(const_char_ptr_cast(log_entries_stream->GetData()),",
    "repo_full_name": "duckdb/duckdb",
    "discussion_comments": [
      {
        "comment_id": "2256882020",
        "repo_full_name": "duckdb/duckdb",
        "pr_number": 17692,
        "pr_file": "src/logging/log_storage.cpp",
        "discussion_id": "2256882020",
        "commented_code": "@@ -29,175 +42,640 @@ void LogStorage::Truncate() {\n \tthrow NotImplementedException(\"Not implemented for this LogStorage: TruncateLogStorage\");\n }\n \n-StdOutLogStorage::StdOutLogStorage() {\n+void LogStorage::UpdateConfig(DatabaseInstance &db, case_insensitive_map_t<Value> &config) {\n+\tif (config.size() > 1) {\n+\t\tthrow InvalidInputException(\"LogStorage does not support passing configuration\");\n+\t}\n+}\n+\n+unique_ptr<TableRef> LogStorage::BindReplaceEntries(ClientContext &context, TableFunctionBindInput &input) {\n+\treturn nullptr;\n+}\n+\n+unique_ptr<TableRef> LogStorage::BindReplaceContexts(ClientContext &context, TableFunctionBindInput &input) {\n+\treturn nullptr;\n+}\n+\n+CSVLogStorage::~CSVLogStorage() {\n+}\n+\n+CSVLogStorage::CSVLogStorage(DatabaseInstance &db) : BufferingLogStorage(db) {\n+\tResetCastChunk();\n+}\n+\n+void CSVLogStorage::UpdateConfig(DatabaseInstance &db, case_insensitive_map_t<Value> &config) {\n+\tlock_guard<mutex> lck(lock);\n+\treturn UpdateConfigInternal(db, config);\n+}\n+\n+void CSVLogStorage::ExecuteCast() {\n+\tlog_entries_cast_buffer->Reset();\n+\tlog_contexts_cast_buffer->Reset();\n+\n+\tbool success = true;\n+\n+\tCastParameters cast_params;\n+\n+\tif (normalize_contexts) {\n+\t\t// -- Cast Log Entries\n+\t\t// context_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[0], log_entries_cast_buffer->data[0], log_entries_buffer->size(), cast_params);\n+\t\t// timestamp: LogicalType::TIMESTAMP\n+\t\tsuccess &= VectorCastHelpers::StringCast<timestamp_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[1], log_entries_cast_buffer->data[1], log_entries_buffer->size(), cast_params);\n+\t\t// log_type: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[2].Reference(log_entries_buffer->data[2]);\n+\t\t// level: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[3].Reference(log_entries_buffer->data[3]);\n+\t\t// message: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[4].Reference(log_entries_buffer->data[4]);\n+\n+\t\tlog_entries_cast_buffer->SetCardinality(log_entries_buffer->size());\n+\n+\t\t// -- Cast Log Contexts\n+\t\t// context_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[0], log_contexts_cast_buffer->data[0], log_contexts_buffer->size(), cast_params);\n+\t\t// scope: LogicalType::VARCHAR (no cast)\n+\t\tlog_contexts_cast_buffer->data[1].Reference(log_contexts_buffer->data[1]);\n+\t\t// connection_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[2], log_contexts_cast_buffer->data[2], log_contexts_buffer->size(), cast_params);\n+\t\t// transaction_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[3], log_contexts_cast_buffer->data[3], log_contexts_buffer->size(), cast_params);\n+\t\t// query_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[4], log_contexts_cast_buffer->data[4], log_contexts_buffer->size(), cast_params);\n+\t\t// thread: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_contexts_buffer->data[5], log_contexts_cast_buffer->data[5], log_contexts_buffer->size(), cast_params);\n+\t\t// scope is already string so doesn't need casting\n+\n+\t\tlog_contexts_cast_buffer->SetCardinality(log_contexts_buffer->size());\n+\t} else {\n+\t\t// -- Cast Log Entries\n+\n+\t\t// context_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[0], log_entries_cast_buffer->data[0], log_entries_buffer->size(), cast_params);\n+\t\t// scope: LogicalType::VARCHAR (no cast)\n+\t\tlog_entries_cast_buffer->data[1].Reference(log_entries_buffer->data[1]);\n+\t\t// connection_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[2], log_entries_cast_buffer->data[2], log_entries_buffer->size(), cast_params);\n+\t\t// transaction_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[3], log_entries_cast_buffer->data[3], log_entries_buffer->size(), cast_params);\n+\t\t// query_id: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[4], log_entries_cast_buffer->data[4], log_entries_buffer->size(), cast_params);\n+\t\t// thread: LogicalType::UBIGINT\n+\t\tsuccess &= VectorCastHelpers::StringCast<idx_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[5], log_entries_cast_buffer->data[5], log_entries_buffer->size(), cast_params);\n+\t\t// timestamp: LogicalType::TIMESTAMP\n+\t\tsuccess &= VectorCastHelpers::StringCast<timestamp_t, duckdb::StringCast>(\n+\t\t    log_entries_buffer->data[6], log_entries_cast_buffer->data[6], log_entries_buffer->size(), cast_params);\n+\t\t// log_type: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[7].Reference(log_entries_buffer->data[7]);\n+\t\t// level: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[8].Reference(log_entries_buffer->data[8]);\n+\t\t// message: LogicalType::VARCHAR  (no cast)\n+\t\tlog_entries_cast_buffer->data[9].Reference(log_entries_buffer->data[9]);\n+\n+\t\tlog_entries_cast_buffer->SetCardinality(log_entries_buffer->size());\n+\t}\n+\n+\tif (!success) {\n+\t\tthrow InvalidInputException(\"Failed to cast log entries\");\n+\t}\n+}\n+\n+void CSVLogStorage::ResetAllBuffers() {\n+\tBufferingLogStorage::ResetAllBuffers();\n+\tResetCastChunk();\n+}\n+\n+void CSVLogStorage::ResetCastChunk() {\n+\tlog_entries_cast_buffer = make_uniq<DataChunk>();\n+\tlog_contexts_cast_buffer = make_uniq<DataChunk>();\n+\n+\t// Initialize the Cast chunks for casting everything to strings\n+\tvector<LogicalType> types;\n+\ttypes.resize(log_entries_buffer->ColumnCount(), LogicalType::VARCHAR);\n+\tlog_entries_cast_buffer->Initialize(Allocator::DefaultAllocator(), types);\n+\n+\ttypes.resize(log_contexts_buffer->ColumnCount(), LogicalType::VARCHAR);\n+\tlog_contexts_cast_buffer->Initialize(Allocator::DefaultAllocator(), types);\n+}\n+\n+void CSVLogStorage::SetWriterConfigs(CSVWriter &writer, vector<string> column_names) {\n+\twriter.options.dialect_options.state_machine_options.escape = '\\\"';\n+\twriter.options.dialect_options.state_machine_options.quote = '\\\"';\n+\twriter.options.dialect_options.state_machine_options.delimiter = CSVOption<string>(\"\\t\");\n+\twriter.options.name_list = column_names;\n+\n+\twriter.options.force_quote = vector<bool>(column_names.size(), false);\n+}\n+\n+void CSVLogStorage::FlushInternal() {\n+\t// Execute the cast\n+\tExecuteCast();\n+\n+\t// Write the cast data to sCSV\n+\tif (log_entries_cast_buffer->size() > 0) {\n+\t\tlog_entries_writer->WriteChunk(*log_entries_cast_buffer);\n+\t\tlog_entries_writer->Flush();\n+\t\tlog_entries_buffer->Reset();\n+\t}\n+\n+\tif (log_contexts_cast_buffer->size() > 0) {\n+\t\tlog_contexts_writer->WriteChunk(*log_contexts_cast_buffer);\n+\t\tlog_contexts_writer->Flush();\n+\t\tlog_contexts_buffer->Reset();\n+\t}\n+}\n+\n+void CSVLogStorage::UpdateConfigInternal(DatabaseInstance &db, case_insensitive_map_t<Value> &config) {\n+\tfor (const auto &it : config) {\n+\t\tif (StringUtil::Lower(it.first) == \"buffer_size\") {\n+\t\t\tbuffer_limit = it.second.GetValue<uint64_t>();\n+\t\t} else {\n+\t\t\tthrow InvalidInputException(\"Unrecognized log storage config option: '%s'\", it.first);\n+\t\t}\n+\t}\n+}\n+\n+StdOutLogStorage::StdOutLogStorage(DatabaseInstance &db) : CSVLogStorage(db) {\n+\tlog_entries_stream = make_uniq<MemoryStream>();\n+\tlog_contexts_stream = make_uniq<MemoryStream>();\n+\tlog_entries_writer = make_uniq<CSVWriter>(*log_contexts_stream, GetEntriesColumnNames(true), false);\n+\tlog_contexts_writer = make_uniq<CSVWriter>(*log_contexts_stream, GetContextsColumnNames(), false);\n+\n+\tSetWriterConfigs(*log_entries_writer, GetEntriesColumnNames(normalize_contexts));\n+\tSetWriterConfigs(*log_contexts_writer, GetContextsColumnNames());\n }\n \n StdOutLogStorage::~StdOutLogStorage() {\n }\n \n-void StdOutLogStorage::WriteLogEntry(timestamp_t timestamp, LogLevel level, const string &log_type,\n-                                     const string &log_message, const RegisteredLoggingContext &context) {\n-\tstd::cout << StringUtil::Format(\n-\t    \"[LOG] %s, %s, %s, %s, %s, %s, %s, %s\\n\", Value::TIMESTAMP(timestamp).ToString(), log_type,\n-\t    EnumUtil::ToString(level), log_message, EnumUtil::ToString(context.context.scope),\n-\t    context.context.connection_id.IsValid() ? to_string(context.context.connection_id.GetIndex()) : \"NULL\",\n-\t    context.context.transaction_id.IsValid() ? to_string(context.context.transaction_id.GetIndex()) : \"NULL\",\n-\t    context.context.thread_id.IsValid() ? to_string(context.context.thread_id.GetIndex()) : \"NULL\");\n+void StdOutLogStorage::FlushInternal() {\n+\t// Flush CSV buffer into stream\n+\tCSVLogStorage::FlushInternal();\n+\n+\t// Write stream to stdout\n+\tstd::cout.write(const_char_ptr_cast(log_entries_stream->GetData()),",
        "comment_created_at": "2025-08-06T11:40:42+00:00",
        "comment_author": "Mytherin",
        "comment_body": "StdoutLogStorage should not buffer by default and should flush after every row (this can be configurable of course)",
        "pr_file_module": null
      }
    ]
  }
]