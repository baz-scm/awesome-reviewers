---
title: Optimize before implementing
description: 'Before implementing algorithms, evaluate their efficiency implications,
  especially for operations that may be executed frequently or with large datasets.
  Consider:'
repository: elastic/elasticsearch
label: Algorithms
language: Java
comments_count: 8
repository_stars: 73104
---

Before implementing algorithms, evaluate their efficiency implications, especially for operations that may be executed frequently or with large datasets. Consider:

- Time and space complexity in typical and worst-case scenarios
- Memory allocation patterns and potential object churn
- Opportunities to avoid redundant computations or unnecessary iterations
- Impact on existing optimization pipelines and future optimization opportunities

Document algorithmic decisions and assumptions with clear comments. Flag potential future optimizations with TODO comments that explain the intended improvement.

For iterators and query processing, implement early termination strategies where possible:

```java
// Consider if we can skip entire ranges when we know no matches are possible
if (value < minTimestamp && allValuesInCurrentPrimarySort) {
    // We know we will not match any more documents in this primary sort
    approximation.match = Match.NO_AND_SKIP;
    return false;
}
```

For data structures, leverage existing infrastructure that handles memory tracking efficiently:

```java
// Use BlockHash to efficiently manage memory and circuit breaking
BlockHash blockHash = BlockHash.build(keys, blockFactory, breaker);
ObjectArray<Queue> queues = new ObjectArray<>(blockHash.capacity(), breaker);
```

When selecting nodes or distributing work, prefer approaches that minimize network traffic and object creation:

```java
// Iterate over eligible nodes (probably fewer) rather than all candidate nodes
return nodeIds.stream()
    .filter(nodeId -> candidateNodeIds.contains(nodeId))
    .collect(Collectors.toSet());
```


[
  {
    "discussion_id": "2170953937",
    "pr_number": 128917,
    "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java",
    "created_at": "2025-06-27T06:59:47+00:00",
    "commented_code": "values.add(List.of(\"coordinator\", \"optimizedPhysicalPlan\", physicalPlanString));\n            var blocks = BlockUtils.fromList(PlannerUtils.NON_BREAKING_BLOCK_FACTORY, values);\n            physicalPlan = new LocalSourceExec(Source.EMPTY, fields, LocalSupplier.of(blocks));\n            planRunner.run(physicalPlan, listener);\n        } else {\n            // TODO: this could be snuck into the underlying listener\n            EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n            // execute any potential subplans\n            executeSubPlans(optimizedPlan, planRunner, executionInfo, request, listener);\n        }\n        // TODO: this could be snuck into the underlying listener\n        EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n        // execute any potential subplans\n        executeSubPlans(physicalPlan, planRunner, executionInfo, request, listener);\n    }\n\n    private record PlanTuple(PhysicalPlan physical, LogicalPlan logical) {}\n    private record LogicalPlanTuple(LogicalPlan nonStubbedSubPlan, LogicalPlan originalSubPlan) {}\n\n    private void executeSubPlans(\n        PhysicalPlan physicalPlan,\n        LogicalPlan optimizedPlan,\n        PlanRunner runner,\n        EsqlExecutionInfo executionInfo,\n        EsqlQueryRequest request,\n        ActionListener<Result> listener\n    ) {\n        List<PlanTuple> subplans = new ArrayList<>();\n\n        // Currently the inlinestats are limited and supported as streaming operators, thus present inside the fragment as logical plans\n        // Below they get collected, translated into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n        physicalPlan.forEachUp(FragmentExec.class, f -> {\n            f.fragment().forEachUp(InlineJoin.class, ij -> {\n                // extract the right side of the plan and replace its source\n                LogicalPlan subplan = InlineJoin.replaceStub(ij.left(), ij.right());\n                // mark the new root node as optimized\n                subplan.setOptimized();\n                PhysicalPlan subqueryPlan = logicalPlanToPhysicalPlan(subplan, request);\n                subplans.add(new PlanTuple(subqueryPlan, ij.right()));\n            });\n        });\n\n        Iterator<PlanTuple> iterator = subplans.iterator();\n        var subPlan = firstSubPlan(optimizedPlan);\n\n        // TODO: merge into one method\n        if (subplans.size() > 0) {\n        if (subPlan != null) {\n            // code-path to execute subplans\n            executeSubPlan(new DriverCompletionInfo.Accumulator(), physicalPlan, iterator, executionInfo, runner, listener);\n            executeSubPlan(new DriverCompletionInfo.Accumulator(), optimizedPlan, subPlan, executionInfo, runner, request, listener);\n        } else {\n            PhysicalPlan physicalPlan = logicalPlanToPhysicalPlan(optimizedPlan, request);\n            // execute main plan\n            runner.run(physicalPlan, listener);\n        }\n    }\n\n    private LogicalPlanTuple firstSubPlan(LogicalPlan optimizedPlan) {\n        Holder<LogicalPlanTuple> subPlan = new Holder<>();\n        // Collect the first inlinejoin (bottom up in the tree)\n        optimizedPlan.forEachUp(InlineJoin.class, ij -> {\n            // extract the right side of the plan and replace its source\n            if (subPlan.get() == null && ij.right().anyMatch(p -> p instanceof StubRelation)) {\n                var p = InlineJoin.replaceStub(ij.left(), ij.right());\n                p.setOptimized();\n                subPlan.set(new LogicalPlanTuple(p, ij.right()));\n            }\n        });\n        return subPlan.get();\n    }\n\n    private void executeSubPlan(\n        DriverCompletionInfo.Accumulator completionInfoAccumulator,\n        PhysicalPlan plan,\n        Iterator<PlanTuple> subPlanIterator,\n        LogicalPlan optimizedPlan,\n        LogicalPlanTuple subPlans,\n        EsqlExecutionInfo executionInfo,\n        PlanRunner runner,\n        EsqlQueryRequest request,\n        ActionListener<Result> listener\n    ) {\n        PlanTuple tuple = subPlanIterator.next();\n        // Create a physical plan out of the logical sub-plan\n        var physicalSubPlan = logicalPlanToPhysicalPlan(subPlans.nonStubbedSubPlan, request);\n\n        runner.run(tuple.physical, listener.delegateFailureAndWrap((next, result) -> {\n        runner.run(physicalSubPlan, listener.delegateFailureAndWrap((next, result) -> {\n            try {\n                // Translate the subquery into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n                completionInfoAccumulator.accumulate(result.completionInfo());\n                LocalRelation resultWrapper = resultToPlan(tuple.logical, result);\n                LocalRelation resultWrapper = resultToPlan(subPlans.nonStubbedSubPlan, result);\n\n                // replace the original logical plan with the backing result\n                PhysicalPlan newPlan = plan.transformUp(FragmentExec.class, f -> {\n                    LogicalPlan frag = f.fragment();\n                    return f.withFragment(\n                        frag.transformUp(\n                            InlineJoin.class,\n                            ij -> ij.right() == tuple.logical ? InlineJoin.inlineData(ij, resultWrapper) : ij\n                        )\n                    );\n                });\n                LogicalPlan newLogicalPlan = optimizedPlan.transformUp(\n                    InlineJoin.class,\n                    ij -> ij.right() == subPlans.originalSubPlan ? InlineJoin.inlineData(ij, resultWrapper) : ij\n                );\n                newLogicalPlan.setOptimized();",
    "repo_full_name": "elastic/elasticsearch",
    "discussion_comments": [
      {
        "comment_id": "2170953937",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128917,
        "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java",
        "discussion_id": "2170953937",
        "commented_code": "@@ -231,85 +231,88 @@ public void executeOptimizedPlan(\n             values.add(List.of(\"coordinator\", \"optimizedPhysicalPlan\", physicalPlanString));\n             var blocks = BlockUtils.fromList(PlannerUtils.NON_BREAKING_BLOCK_FACTORY, values);\n             physicalPlan = new LocalSourceExec(Source.EMPTY, fields, LocalSupplier.of(blocks));\n+            planRunner.run(physicalPlan, listener);\n+        } else {\n+            // TODO: this could be snuck into the underlying listener\n+            EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n+            // execute any potential subplans\n+            executeSubPlans(optimizedPlan, planRunner, executionInfo, request, listener);\n         }\n-        // TODO: this could be snuck into the underlying listener\n-        EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n-        // execute any potential subplans\n-        executeSubPlans(physicalPlan, planRunner, executionInfo, request, listener);\n     }\n \n-    private record PlanTuple(PhysicalPlan physical, LogicalPlan logical) {}\n+    private record LogicalPlanTuple(LogicalPlan nonStubbedSubPlan, LogicalPlan originalSubPlan) {}\n \n     private void executeSubPlans(\n-        PhysicalPlan physicalPlan,\n+        LogicalPlan optimizedPlan,\n         PlanRunner runner,\n         EsqlExecutionInfo executionInfo,\n         EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        List<PlanTuple> subplans = new ArrayList<>();\n-\n-        // Currently the inlinestats are limited and supported as streaming operators, thus present inside the fragment as logical plans\n-        // Below they get collected, translated into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n-        physicalPlan.forEachUp(FragmentExec.class, f -> {\n-            f.fragment().forEachUp(InlineJoin.class, ij -> {\n-                // extract the right side of the plan and replace its source\n-                LogicalPlan subplan = InlineJoin.replaceStub(ij.left(), ij.right());\n-                // mark the new root node as optimized\n-                subplan.setOptimized();\n-                PhysicalPlan subqueryPlan = logicalPlanToPhysicalPlan(subplan, request);\n-                subplans.add(new PlanTuple(subqueryPlan, ij.right()));\n-            });\n-        });\n-\n-        Iterator<PlanTuple> iterator = subplans.iterator();\n+        var subPlan = firstSubPlan(optimizedPlan);\n \n         // TODO: merge into one method\n-        if (subplans.size() > 0) {\n+        if (subPlan != null) {\n             // code-path to execute subplans\n-            executeSubPlan(new DriverCompletionInfo.Accumulator(), physicalPlan, iterator, executionInfo, runner, listener);\n+            executeSubPlan(new DriverCompletionInfo.Accumulator(), optimizedPlan, subPlan, executionInfo, runner, request, listener);\n         } else {\n+            PhysicalPlan physicalPlan = logicalPlanToPhysicalPlan(optimizedPlan, request);\n             // execute main plan\n             runner.run(physicalPlan, listener);\n         }\n     }\n \n+    private LogicalPlanTuple firstSubPlan(LogicalPlan optimizedPlan) {\n+        Holder<LogicalPlanTuple> subPlan = new Holder<>();\n+        // Collect the first inlinejoin (bottom up in the tree)\n+        optimizedPlan.forEachUp(InlineJoin.class, ij -> {\n+            // extract the right side of the plan and replace its source\n+            if (subPlan.get() == null && ij.right().anyMatch(p -> p instanceof StubRelation)) {\n+                var p = InlineJoin.replaceStub(ij.left(), ij.right());\n+                p.setOptimized();\n+                subPlan.set(new LogicalPlanTuple(p, ij.right()));\n+            }\n+        });\n+        return subPlan.get();\n+    }\n+\n     private void executeSubPlan(\n         DriverCompletionInfo.Accumulator completionInfoAccumulator,\n-        PhysicalPlan plan,\n-        Iterator<PlanTuple> subPlanIterator,\n+        LogicalPlan optimizedPlan,\n+        LogicalPlanTuple subPlans,\n         EsqlExecutionInfo executionInfo,\n         PlanRunner runner,\n+        EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        PlanTuple tuple = subPlanIterator.next();\n+        // Create a physical plan out of the logical sub-plan\n+        var physicalSubPlan = logicalPlanToPhysicalPlan(subPlans.nonStubbedSubPlan, request);\n \n-        runner.run(tuple.physical, listener.delegateFailureAndWrap((next, result) -> {\n+        runner.run(physicalSubPlan, listener.delegateFailureAndWrap((next, result) -> {\n             try {\n+                // Translate the subquery into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n                 completionInfoAccumulator.accumulate(result.completionInfo());\n-                LocalRelation resultWrapper = resultToPlan(tuple.logical, result);\n+                LocalRelation resultWrapper = resultToPlan(subPlans.nonStubbedSubPlan, result);\n \n                 // replace the original logical plan with the backing result\n-                PhysicalPlan newPlan = plan.transformUp(FragmentExec.class, f -> {\n-                    LogicalPlan frag = f.fragment();\n-                    return f.withFragment(\n-                        frag.transformUp(\n-                            InlineJoin.class,\n-                            ij -> ij.right() == tuple.logical ? InlineJoin.inlineData(ij, resultWrapper) : ij\n-                        )\n-                    );\n-                });\n+                LogicalPlan newLogicalPlan = optimizedPlan.transformUp(\n+                    InlineJoin.class,\n+                    ij -> ij.right() == subPlans.originalSubPlan ? InlineJoin.inlineData(ij, resultWrapper) : ij\n+                );\n+                newLogicalPlan.setOptimized();",
        "comment_created_at": "2025-06-27T06:59:47+00:00",
        "comment_author": "alex-spies",
        "comment_body": "This is surprising. Shouldn't we re-optimize the plan now that we were able to replace the stub with an actual result?\n\nAfter the stub was replaced, we can actually do more stuff, like push down limits (which before that would be wrong as it would have affected the stats).\n\nIf I understand correctly, this is something we might want to improve later, right? If so, let's leave a comment; maybe a `TODO` to make the intention clear.",
        "pr_file_module": null
      },
      {
        "comment_id": "2170963284",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128917,
        "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java",
        "discussion_id": "2170953937",
        "commented_code": "@@ -231,85 +231,88 @@ public void executeOptimizedPlan(\n             values.add(List.of(\"coordinator\", \"optimizedPhysicalPlan\", physicalPlanString));\n             var blocks = BlockUtils.fromList(PlannerUtils.NON_BREAKING_BLOCK_FACTORY, values);\n             physicalPlan = new LocalSourceExec(Source.EMPTY, fields, LocalSupplier.of(blocks));\n+            planRunner.run(physicalPlan, listener);\n+        } else {\n+            // TODO: this could be snuck into the underlying listener\n+            EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n+            // execute any potential subplans\n+            executeSubPlans(optimizedPlan, planRunner, executionInfo, request, listener);\n         }\n-        // TODO: this could be snuck into the underlying listener\n-        EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n-        // execute any potential subplans\n-        executeSubPlans(physicalPlan, planRunner, executionInfo, request, listener);\n     }\n \n-    private record PlanTuple(PhysicalPlan physical, LogicalPlan logical) {}\n+    private record LogicalPlanTuple(LogicalPlan nonStubbedSubPlan, LogicalPlan originalSubPlan) {}\n \n     private void executeSubPlans(\n-        PhysicalPlan physicalPlan,\n+        LogicalPlan optimizedPlan,\n         PlanRunner runner,\n         EsqlExecutionInfo executionInfo,\n         EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        List<PlanTuple> subplans = new ArrayList<>();\n-\n-        // Currently the inlinestats are limited and supported as streaming operators, thus present inside the fragment as logical plans\n-        // Below they get collected, translated into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n-        physicalPlan.forEachUp(FragmentExec.class, f -> {\n-            f.fragment().forEachUp(InlineJoin.class, ij -> {\n-                // extract the right side of the plan and replace its source\n-                LogicalPlan subplan = InlineJoin.replaceStub(ij.left(), ij.right());\n-                // mark the new root node as optimized\n-                subplan.setOptimized();\n-                PhysicalPlan subqueryPlan = logicalPlanToPhysicalPlan(subplan, request);\n-                subplans.add(new PlanTuple(subqueryPlan, ij.right()));\n-            });\n-        });\n-\n-        Iterator<PlanTuple> iterator = subplans.iterator();\n+        var subPlan = firstSubPlan(optimizedPlan);\n \n         // TODO: merge into one method\n-        if (subplans.size() > 0) {\n+        if (subPlan != null) {\n             // code-path to execute subplans\n-            executeSubPlan(new DriverCompletionInfo.Accumulator(), physicalPlan, iterator, executionInfo, runner, listener);\n+            executeSubPlan(new DriverCompletionInfo.Accumulator(), optimizedPlan, subPlan, executionInfo, runner, request, listener);\n         } else {\n+            PhysicalPlan physicalPlan = logicalPlanToPhysicalPlan(optimizedPlan, request);\n             // execute main plan\n             runner.run(physicalPlan, listener);\n         }\n     }\n \n+    private LogicalPlanTuple firstSubPlan(LogicalPlan optimizedPlan) {\n+        Holder<LogicalPlanTuple> subPlan = new Holder<>();\n+        // Collect the first inlinejoin (bottom up in the tree)\n+        optimizedPlan.forEachUp(InlineJoin.class, ij -> {\n+            // extract the right side of the plan and replace its source\n+            if (subPlan.get() == null && ij.right().anyMatch(p -> p instanceof StubRelation)) {\n+                var p = InlineJoin.replaceStub(ij.left(), ij.right());\n+                p.setOptimized();\n+                subPlan.set(new LogicalPlanTuple(p, ij.right()));\n+            }\n+        });\n+        return subPlan.get();\n+    }\n+\n     private void executeSubPlan(\n         DriverCompletionInfo.Accumulator completionInfoAccumulator,\n-        PhysicalPlan plan,\n-        Iterator<PlanTuple> subPlanIterator,\n+        LogicalPlan optimizedPlan,\n+        LogicalPlanTuple subPlans,\n         EsqlExecutionInfo executionInfo,\n         PlanRunner runner,\n+        EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        PlanTuple tuple = subPlanIterator.next();\n+        // Create a physical plan out of the logical sub-plan\n+        var physicalSubPlan = logicalPlanToPhysicalPlan(subPlans.nonStubbedSubPlan, request);\n \n-        runner.run(tuple.physical, listener.delegateFailureAndWrap((next, result) -> {\n+        runner.run(physicalSubPlan, listener.delegateFailureAndWrap((next, result) -> {\n             try {\n+                // Translate the subquery into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n                 completionInfoAccumulator.accumulate(result.completionInfo());\n-                LocalRelation resultWrapper = resultToPlan(tuple.logical, result);\n+                LocalRelation resultWrapper = resultToPlan(subPlans.nonStubbedSubPlan, result);\n \n                 // replace the original logical plan with the backing result\n-                PhysicalPlan newPlan = plan.transformUp(FragmentExec.class, f -> {\n-                    LogicalPlan frag = f.fragment();\n-                    return f.withFragment(\n-                        frag.transformUp(\n-                            InlineJoin.class,\n-                            ij -> ij.right() == tuple.logical ? InlineJoin.inlineData(ij, resultWrapper) : ij\n-                        )\n-                    );\n-                });\n+                LogicalPlan newLogicalPlan = optimizedPlan.transformUp(\n+                    InlineJoin.class,\n+                    ij -> ij.right() == subPlans.originalSubPlan ? InlineJoin.inlineData(ij, resultWrapper) : ij\n+                );\n+                newLogicalPlan.setOptimized();",
        "comment_created_at": "2025-06-27T07:04:15+00:00",
        "comment_author": "astefan",
        "comment_body": "Yeah, a TODO is right.\r\n\r\nI had my doubts with this thing. I feel like there are things here that were left hanging (things do still seem they can be optimized further), but at this stage of the `inlinestats` progress, I think it's worth ignoring it. But TODO is needed, because it's something we need to think about a little.",
        "pr_file_module": null
      },
      {
        "comment_id": "2171129944",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128917,
        "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java",
        "discussion_id": "2170953937",
        "commented_code": "@@ -231,85 +231,88 @@ public void executeOptimizedPlan(\n             values.add(List.of(\"coordinator\", \"optimizedPhysicalPlan\", physicalPlanString));\n             var blocks = BlockUtils.fromList(PlannerUtils.NON_BREAKING_BLOCK_FACTORY, values);\n             physicalPlan = new LocalSourceExec(Source.EMPTY, fields, LocalSupplier.of(blocks));\n+            planRunner.run(physicalPlan, listener);\n+        } else {\n+            // TODO: this could be snuck into the underlying listener\n+            EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n+            // execute any potential subplans\n+            executeSubPlans(optimizedPlan, planRunner, executionInfo, request, listener);\n         }\n-        // TODO: this could be snuck into the underlying listener\n-        EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n-        // execute any potential subplans\n-        executeSubPlans(physicalPlan, planRunner, executionInfo, request, listener);\n     }\n \n-    private record PlanTuple(PhysicalPlan physical, LogicalPlan logical) {}\n+    private record LogicalPlanTuple(LogicalPlan nonStubbedSubPlan, LogicalPlan originalSubPlan) {}\n \n     private void executeSubPlans(\n-        PhysicalPlan physicalPlan,\n+        LogicalPlan optimizedPlan,\n         PlanRunner runner,\n         EsqlExecutionInfo executionInfo,\n         EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        List<PlanTuple> subplans = new ArrayList<>();\n-\n-        // Currently the inlinestats are limited and supported as streaming operators, thus present inside the fragment as logical plans\n-        // Below they get collected, translated into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n-        physicalPlan.forEachUp(FragmentExec.class, f -> {\n-            f.fragment().forEachUp(InlineJoin.class, ij -> {\n-                // extract the right side of the plan and replace its source\n-                LogicalPlan subplan = InlineJoin.replaceStub(ij.left(), ij.right());\n-                // mark the new root node as optimized\n-                subplan.setOptimized();\n-                PhysicalPlan subqueryPlan = logicalPlanToPhysicalPlan(subplan, request);\n-                subplans.add(new PlanTuple(subqueryPlan, ij.right()));\n-            });\n-        });\n-\n-        Iterator<PlanTuple> iterator = subplans.iterator();\n+        var subPlan = firstSubPlan(optimizedPlan);\n \n         // TODO: merge into one method\n-        if (subplans.size() > 0) {\n+        if (subPlan != null) {\n             // code-path to execute subplans\n-            executeSubPlan(new DriverCompletionInfo.Accumulator(), physicalPlan, iterator, executionInfo, runner, listener);\n+            executeSubPlan(new DriverCompletionInfo.Accumulator(), optimizedPlan, subPlan, executionInfo, runner, request, listener);\n         } else {\n+            PhysicalPlan physicalPlan = logicalPlanToPhysicalPlan(optimizedPlan, request);\n             // execute main plan\n             runner.run(physicalPlan, listener);\n         }\n     }\n \n+    private LogicalPlanTuple firstSubPlan(LogicalPlan optimizedPlan) {\n+        Holder<LogicalPlanTuple> subPlan = new Holder<>();\n+        // Collect the first inlinejoin (bottom up in the tree)\n+        optimizedPlan.forEachUp(InlineJoin.class, ij -> {\n+            // extract the right side of the plan and replace its source\n+            if (subPlan.get() == null && ij.right().anyMatch(p -> p instanceof StubRelation)) {\n+                var p = InlineJoin.replaceStub(ij.left(), ij.right());\n+                p.setOptimized();\n+                subPlan.set(new LogicalPlanTuple(p, ij.right()));\n+            }\n+        });\n+        return subPlan.get();\n+    }\n+\n     private void executeSubPlan(\n         DriverCompletionInfo.Accumulator completionInfoAccumulator,\n-        PhysicalPlan plan,\n-        Iterator<PlanTuple> subPlanIterator,\n+        LogicalPlan optimizedPlan,\n+        LogicalPlanTuple subPlans,\n         EsqlExecutionInfo executionInfo,\n         PlanRunner runner,\n+        EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        PlanTuple tuple = subPlanIterator.next();\n+        // Create a physical plan out of the logical sub-plan\n+        var physicalSubPlan = logicalPlanToPhysicalPlan(subPlans.nonStubbedSubPlan, request);\n \n-        runner.run(tuple.physical, listener.delegateFailureAndWrap((next, result) -> {\n+        runner.run(physicalSubPlan, listener.delegateFailureAndWrap((next, result) -> {\n             try {\n+                // Translate the subquery into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n                 completionInfoAccumulator.accumulate(result.completionInfo());\n-                LocalRelation resultWrapper = resultToPlan(tuple.logical, result);\n+                LocalRelation resultWrapper = resultToPlan(subPlans.nonStubbedSubPlan, result);\n \n                 // replace the original logical plan with the backing result\n-                PhysicalPlan newPlan = plan.transformUp(FragmentExec.class, f -> {\n-                    LogicalPlan frag = f.fragment();\n-                    return f.withFragment(\n-                        frag.transformUp(\n-                            InlineJoin.class,\n-                            ij -> ij.right() == tuple.logical ? InlineJoin.inlineData(ij, resultWrapper) : ij\n-                        )\n-                    );\n-                });\n+                LogicalPlan newLogicalPlan = optimizedPlan.transformUp(\n+                    InlineJoin.class,\n+                    ij -> ij.right() == subPlans.originalSubPlan ? InlineJoin.inlineData(ij, resultWrapper) : ij\n+                );\n+                newLogicalPlan.setOptimized();",
        "comment_created_at": "2025-06-27T08:02:42+00:00",
        "comment_author": "bpintea",
        "comment_body": "Hmm, I thought this _was_ sufficient. The approach basically gradually executes righthand sides of (inline) joins, always resulting in a `LocalRelation`. Ending up with a join with whatever was before on the left and this local relation on the right. Not sure if we can further optimise this (and haven't done it before - the LIMIT makes it past the InlineJoin into the lefthand side already, since InlineJoin preserves the rows count).\r\nBut a TODO can re-eval things later, \ud83d\udc4d .",
        "pr_file_module": null
      },
      {
        "comment_id": "2174975919",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128917,
        "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java",
        "discussion_id": "2170953937",
        "commented_code": "@@ -231,85 +231,88 @@ public void executeOptimizedPlan(\n             values.add(List.of(\"coordinator\", \"optimizedPhysicalPlan\", physicalPlanString));\n             var blocks = BlockUtils.fromList(PlannerUtils.NON_BREAKING_BLOCK_FACTORY, values);\n             physicalPlan = new LocalSourceExec(Source.EMPTY, fields, LocalSupplier.of(blocks));\n+            planRunner.run(physicalPlan, listener);\n+        } else {\n+            // TODO: this could be snuck into the underlying listener\n+            EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n+            // execute any potential subplans\n+            executeSubPlans(optimizedPlan, planRunner, executionInfo, request, listener);\n         }\n-        // TODO: this could be snuck into the underlying listener\n-        EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n-        // execute any potential subplans\n-        executeSubPlans(physicalPlan, planRunner, executionInfo, request, listener);\n     }\n \n-    private record PlanTuple(PhysicalPlan physical, LogicalPlan logical) {}\n+    private record LogicalPlanTuple(LogicalPlan nonStubbedSubPlan, LogicalPlan originalSubPlan) {}\n \n     private void executeSubPlans(\n-        PhysicalPlan physicalPlan,\n+        LogicalPlan optimizedPlan,\n         PlanRunner runner,\n         EsqlExecutionInfo executionInfo,\n         EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        List<PlanTuple> subplans = new ArrayList<>();\n-\n-        // Currently the inlinestats are limited and supported as streaming operators, thus present inside the fragment as logical plans\n-        // Below they get collected, translated into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n-        physicalPlan.forEachUp(FragmentExec.class, f -> {\n-            f.fragment().forEachUp(InlineJoin.class, ij -> {\n-                // extract the right side of the plan and replace its source\n-                LogicalPlan subplan = InlineJoin.replaceStub(ij.left(), ij.right());\n-                // mark the new root node as optimized\n-                subplan.setOptimized();\n-                PhysicalPlan subqueryPlan = logicalPlanToPhysicalPlan(subplan, request);\n-                subplans.add(new PlanTuple(subqueryPlan, ij.right()));\n-            });\n-        });\n-\n-        Iterator<PlanTuple> iterator = subplans.iterator();\n+        var subPlan = firstSubPlan(optimizedPlan);\n \n         // TODO: merge into one method\n-        if (subplans.size() > 0) {\n+        if (subPlan != null) {\n             // code-path to execute subplans\n-            executeSubPlan(new DriverCompletionInfo.Accumulator(), physicalPlan, iterator, executionInfo, runner, listener);\n+            executeSubPlan(new DriverCompletionInfo.Accumulator(), optimizedPlan, subPlan, executionInfo, runner, request, listener);\n         } else {\n+            PhysicalPlan physicalPlan = logicalPlanToPhysicalPlan(optimizedPlan, request);\n             // execute main plan\n             runner.run(physicalPlan, listener);\n         }\n     }\n \n+    private LogicalPlanTuple firstSubPlan(LogicalPlan optimizedPlan) {\n+        Holder<LogicalPlanTuple> subPlan = new Holder<>();\n+        // Collect the first inlinejoin (bottom up in the tree)\n+        optimizedPlan.forEachUp(InlineJoin.class, ij -> {\n+            // extract the right side of the plan and replace its source\n+            if (subPlan.get() == null && ij.right().anyMatch(p -> p instanceof StubRelation)) {\n+                var p = InlineJoin.replaceStub(ij.left(), ij.right());\n+                p.setOptimized();\n+                subPlan.set(new LogicalPlanTuple(p, ij.right()));\n+            }\n+        });\n+        return subPlan.get();\n+    }\n+\n     private void executeSubPlan(\n         DriverCompletionInfo.Accumulator completionInfoAccumulator,\n-        PhysicalPlan plan,\n-        Iterator<PlanTuple> subPlanIterator,\n+        LogicalPlan optimizedPlan,\n+        LogicalPlanTuple subPlans,\n         EsqlExecutionInfo executionInfo,\n         PlanRunner runner,\n+        EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        PlanTuple tuple = subPlanIterator.next();\n+        // Create a physical plan out of the logical sub-plan\n+        var physicalSubPlan = logicalPlanToPhysicalPlan(subPlans.nonStubbedSubPlan, request);\n \n-        runner.run(tuple.physical, listener.delegateFailureAndWrap((next, result) -> {\n+        runner.run(physicalSubPlan, listener.delegateFailureAndWrap((next, result) -> {\n             try {\n+                // Translate the subquery into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n                 completionInfoAccumulator.accumulate(result.completionInfo());\n-                LocalRelation resultWrapper = resultToPlan(tuple.logical, result);\n+                LocalRelation resultWrapper = resultToPlan(subPlans.nonStubbedSubPlan, result);\n \n                 // replace the original logical plan with the backing result\n-                PhysicalPlan newPlan = plan.transformUp(FragmentExec.class, f -> {\n-                    LogicalPlan frag = f.fragment();\n-                    return f.withFragment(\n-                        frag.transformUp(\n-                            InlineJoin.class,\n-                            ij -> ij.right() == tuple.logical ? InlineJoin.inlineData(ij, resultWrapper) : ij\n-                        )\n-                    );\n-                });\n+                LogicalPlan newLogicalPlan = optimizedPlan.transformUp(\n+                    InlineJoin.class,\n+                    ij -> ij.right() == subPlans.originalSubPlan ? InlineJoin.inlineData(ij, resultWrapper) : ij\n+                );\n+                newLogicalPlan.setOptimized();",
        "comment_created_at": "2025-06-30T12:38:14+00:00",
        "comment_author": "alex-spies",
        "comment_body": "Oh, we can further optimize it alright :)\r\n\r\nThe limit being pushed/copied down into the left hand side is a bug - that should only happen in subsequent passes. See my comment [here](https://github.com/elastic/elasticsearch/pull/128917#discussion_r2174955992).\r\n\r\nAlso, if the `INLINESTATS` has no `BY` clause, it can be turned into an `EVAL` with literals in subsequent phases, which can trigger more optimizations (like constant folding, filter pushdown, optimizing away checks against constant keyword fields). Some of this could probably be somehow hacked into the first optimization pass, but currently I think it's more natural (or at least easier) to just have another optimizer pass per query phase.",
        "pr_file_module": null
      },
      {
        "comment_id": "2175521950",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128917,
        "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java",
        "discussion_id": "2170953937",
        "commented_code": "@@ -231,85 +231,88 @@ public void executeOptimizedPlan(\n             values.add(List.of(\"coordinator\", \"optimizedPhysicalPlan\", physicalPlanString));\n             var blocks = BlockUtils.fromList(PlannerUtils.NON_BREAKING_BLOCK_FACTORY, values);\n             physicalPlan = new LocalSourceExec(Source.EMPTY, fields, LocalSupplier.of(blocks));\n+            planRunner.run(physicalPlan, listener);\n+        } else {\n+            // TODO: this could be snuck into the underlying listener\n+            EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n+            // execute any potential subplans\n+            executeSubPlans(optimizedPlan, planRunner, executionInfo, request, listener);\n         }\n-        // TODO: this could be snuck into the underlying listener\n-        EsqlCCSUtils.updateExecutionInfoAtEndOfPlanning(executionInfo);\n-        // execute any potential subplans\n-        executeSubPlans(physicalPlan, planRunner, executionInfo, request, listener);\n     }\n \n-    private record PlanTuple(PhysicalPlan physical, LogicalPlan logical) {}\n+    private record LogicalPlanTuple(LogicalPlan nonStubbedSubPlan, LogicalPlan originalSubPlan) {}\n \n     private void executeSubPlans(\n-        PhysicalPlan physicalPlan,\n+        LogicalPlan optimizedPlan,\n         PlanRunner runner,\n         EsqlExecutionInfo executionInfo,\n         EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        List<PlanTuple> subplans = new ArrayList<>();\n-\n-        // Currently the inlinestats are limited and supported as streaming operators, thus present inside the fragment as logical plans\n-        // Below they get collected, translated into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n-        physicalPlan.forEachUp(FragmentExec.class, f -> {\n-            f.fragment().forEachUp(InlineJoin.class, ij -> {\n-                // extract the right side of the plan and replace its source\n-                LogicalPlan subplan = InlineJoin.replaceStub(ij.left(), ij.right());\n-                // mark the new root node as optimized\n-                subplan.setOptimized();\n-                PhysicalPlan subqueryPlan = logicalPlanToPhysicalPlan(subplan, request);\n-                subplans.add(new PlanTuple(subqueryPlan, ij.right()));\n-            });\n-        });\n-\n-        Iterator<PlanTuple> iterator = subplans.iterator();\n+        var subPlan = firstSubPlan(optimizedPlan);\n \n         // TODO: merge into one method\n-        if (subplans.size() > 0) {\n+        if (subPlan != null) {\n             // code-path to execute subplans\n-            executeSubPlan(new DriverCompletionInfo.Accumulator(), physicalPlan, iterator, executionInfo, runner, listener);\n+            executeSubPlan(new DriverCompletionInfo.Accumulator(), optimizedPlan, subPlan, executionInfo, runner, request, listener);\n         } else {\n+            PhysicalPlan physicalPlan = logicalPlanToPhysicalPlan(optimizedPlan, request);\n             // execute main plan\n             runner.run(physicalPlan, listener);\n         }\n     }\n \n+    private LogicalPlanTuple firstSubPlan(LogicalPlan optimizedPlan) {\n+        Holder<LogicalPlanTuple> subPlan = new Holder<>();\n+        // Collect the first inlinejoin (bottom up in the tree)\n+        optimizedPlan.forEachUp(InlineJoin.class, ij -> {\n+            // extract the right side of the plan and replace its source\n+            if (subPlan.get() == null && ij.right().anyMatch(p -> p instanceof StubRelation)) {\n+                var p = InlineJoin.replaceStub(ij.left(), ij.right());\n+                p.setOptimized();\n+                subPlan.set(new LogicalPlanTuple(p, ij.right()));\n+            }\n+        });\n+        return subPlan.get();\n+    }\n+\n     private void executeSubPlan(\n         DriverCompletionInfo.Accumulator completionInfoAccumulator,\n-        PhysicalPlan plan,\n-        Iterator<PlanTuple> subPlanIterator,\n+        LogicalPlan optimizedPlan,\n+        LogicalPlanTuple subPlans,\n         EsqlExecutionInfo executionInfo,\n         PlanRunner runner,\n+        EsqlQueryRequest request,\n         ActionListener<Result> listener\n     ) {\n-        PlanTuple tuple = subPlanIterator.next();\n+        // Create a physical plan out of the logical sub-plan\n+        var physicalSubPlan = logicalPlanToPhysicalPlan(subPlans.nonStubbedSubPlan, request);\n \n-        runner.run(tuple.physical, listener.delegateFailureAndWrap((next, result) -> {\n+        runner.run(physicalSubPlan, listener.delegateFailureAndWrap((next, result) -> {\n             try {\n+                // Translate the subquery into a separate, coordinator based plan and the results 'broadcasted' as a local relation\n                 completionInfoAccumulator.accumulate(result.completionInfo());\n-                LocalRelation resultWrapper = resultToPlan(tuple.logical, result);\n+                LocalRelation resultWrapper = resultToPlan(subPlans.nonStubbedSubPlan, result);\n \n                 // replace the original logical plan with the backing result\n-                PhysicalPlan newPlan = plan.transformUp(FragmentExec.class, f -> {\n-                    LogicalPlan frag = f.fragment();\n-                    return f.withFragment(\n-                        frag.transformUp(\n-                            InlineJoin.class,\n-                            ij -> ij.right() == tuple.logical ? InlineJoin.inlineData(ij, resultWrapper) : ij\n-                        )\n-                    );\n-                });\n+                LogicalPlan newLogicalPlan = optimizedPlan.transformUp(\n+                    InlineJoin.class,\n+                    ij -> ij.right() == subPlans.originalSubPlan ? InlineJoin.inlineData(ij, resultWrapper) : ij\n+                );\n+                newLogicalPlan.setOptimized();",
        "comment_created_at": "2025-06-30T16:57:00+00:00",
        "comment_author": "bpintea",
        "comment_body": "> The limit being pushed/copied down into the left hand side is a bug - that should only happen in subsequent passes.\r\n\r\nActually, very true!\r\n\r\n> Also, if the INLINESTATS has no BY clause, it can be turned into an EVAL with literals in subsequent phases\r\n\r\nI see. Yes, this could be done, but not with the current shape of the planning (i.e. rerunning the optimiser as it is now won't replan). But yes, you're right, this could still be evolved.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175016623",
    "pr_number": 127223,
    "pr_file": "server/src/main/java/org/elasticsearch/index/mapper/vectors/DenseVectorFieldMapper.java",
    "created_at": "2025-06-30T12:59:04+00:00",
    "commented_code": "return knnQuery;\n        }\n\n        private Query maybeWrapPatience(Query knnQuery) {\n            Query finalQuery = knnQuery;\n            if (knnQuery instanceof KnnByteVectorQuery knnByteVectorQuery) {\n                finalQuery = maybeWrapPatienceByte(knnByteVectorQuery, Math.max(7, (int) (knnByteVectorQuery.getK() * 0.3)));\n            } else if (knnQuery instanceof KnnFloatVectorQuery knnFloatVectorQuery) {\n                finalQuery = maybeWrapPatienceFloat(knnFloatVectorQuery, Math.max(7, (int) (knnFloatVectorQuery.getK() * 0.3)));\n            }\n            return finalQuery;\n        }\n\n        private Query maybeWrapPatienceByte(KnnByteVectorQuery knnQuery, int patience) {\n            Query returnedQuery = knnQuery;\n            if (indexOptions instanceof HnswIndexOptions) {\n                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n            } else if (indexOptions instanceof Int8HnswIndexOptions) {\n                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n            } else if (indexOptions instanceof Int4HnswIndexOptions) {\n                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n            } else if (indexOptions instanceof BBQHnswIndexOptions) {\n                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n            }\n            return returnedQuery;\n        }\n\n        private Query maybeWrapPatienceFloat(KnnFloatVectorQuery knnQuery, int patience) {\n            Query returnedQuery = knnQuery;\n            if (indexOptions instanceof HnswIndexOptions hnswIndexOptions) {\n                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n            } else if (indexOptions instanceof Int8HnswIndexOptions hnswIndexOptions) {\n                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n            } else if (indexOptions instanceof Int4HnswIndexOptions hnswIndexOptions) {\n                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n            } else if (indexOptions instanceof BBQHnswIndexOptions hnswIndexOptions) {\n                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n            }\n            return returnedQuery;",
    "repo_full_name": "elastic/elasticsearch",
    "discussion_comments": [
      {
        "comment_id": "2175016623",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127223,
        "pr_file": "server/src/main/java/org/elasticsearch/index/mapper/vectors/DenseVectorFieldMapper.java",
        "discussion_id": "2175016623",
        "commented_code": "@@ -2526,6 +2561,44 @@ private Query createKnnByteQuery(\n             return knnQuery;\n         }\n \n+        private Query maybeWrapPatience(Query knnQuery) {\n+            Query finalQuery = knnQuery;\n+            if (knnQuery instanceof KnnByteVectorQuery knnByteVectorQuery) {\n+                finalQuery = maybeWrapPatienceByte(knnByteVectorQuery, Math.max(7, (int) (knnByteVectorQuery.getK() * 0.3)));\n+            } else if (knnQuery instanceof KnnFloatVectorQuery knnFloatVectorQuery) {\n+                finalQuery = maybeWrapPatienceFloat(knnFloatVectorQuery, Math.max(7, (int) (knnFloatVectorQuery.getK() * 0.3)));\n+            }\n+            return finalQuery;\n+        }\n+\n+        private Query maybeWrapPatienceByte(KnnByteVectorQuery knnQuery, int patience) {\n+            Query returnedQuery = knnQuery;\n+            if (indexOptions instanceof HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int8HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int4HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof BBQHnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            }\n+            return returnedQuery;\n+        }\n+\n+        private Query maybeWrapPatienceFloat(KnnFloatVectorQuery knnQuery, int patience) {\n+            Query returnedQuery = knnQuery;\n+            if (indexOptions instanceof HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int8HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int4HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof BBQHnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            }\n+            return returnedQuery;",
        "comment_created_at": "2025-06-30T12:59:04+00:00",
        "comment_author": "benwtrent",
        "comment_body": "Why do we have this big predicate branch? \r\n\r\nAlso, it seems that this `0.995` is just the default in Lucene. \r\n\r\nWhy can't we simplify all this and return `PatienceKnnVectorQuery.fromFloatQuery(knnQuery)` ?\r\n\r\nSimilar question to the byte query.",
        "pr_file_module": null
      },
      {
        "comment_id": "2175126375",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127223,
        "pr_file": "server/src/main/java/org/elasticsearch/index/mapper/vectors/DenseVectorFieldMapper.java",
        "discussion_id": "2175016623",
        "commented_code": "@@ -2526,6 +2561,44 @@ private Query createKnnByteQuery(\n             return knnQuery;\n         }\n \n+        private Query maybeWrapPatience(Query knnQuery) {\n+            Query finalQuery = knnQuery;\n+            if (knnQuery instanceof KnnByteVectorQuery knnByteVectorQuery) {\n+                finalQuery = maybeWrapPatienceByte(knnByteVectorQuery, Math.max(7, (int) (knnByteVectorQuery.getK() * 0.3)));\n+            } else if (knnQuery instanceof KnnFloatVectorQuery knnFloatVectorQuery) {\n+                finalQuery = maybeWrapPatienceFloat(knnFloatVectorQuery, Math.max(7, (int) (knnFloatVectorQuery.getK() * 0.3)));\n+            }\n+            return finalQuery;\n+        }\n+\n+        private Query maybeWrapPatienceByte(KnnByteVectorQuery knnQuery, int patience) {\n+            Query returnedQuery = knnQuery;\n+            if (indexOptions instanceof HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int8HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int4HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof BBQHnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            }\n+            return returnedQuery;\n+        }\n+\n+        private Query maybeWrapPatienceFloat(KnnFloatVectorQuery knnQuery, int patience) {\n+            Query returnedQuery = knnQuery;\n+            if (indexOptions instanceof HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int8HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int4HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof BBQHnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            }\n+            return returnedQuery;",
        "comment_created_at": "2025-06-30T13:47:17+00:00",
        "comment_author": "tteofili",
        "comment_body": "++ on the defaults.\r\nre the branches, it doesn't make sense to wrap the knn query with non-HNSW index types because, currently, it only works with them.",
        "pr_file_module": null
      },
      {
        "comment_id": "2175182641",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127223,
        "pr_file": "server/src/main/java/org/elasticsearch/index/mapper/vectors/DenseVectorFieldMapper.java",
        "discussion_id": "2175016623",
        "commented_code": "@@ -2526,6 +2561,44 @@ private Query createKnnByteQuery(\n             return knnQuery;\n         }\n \n+        private Query maybeWrapPatience(Query knnQuery) {\n+            Query finalQuery = knnQuery;\n+            if (knnQuery instanceof KnnByteVectorQuery knnByteVectorQuery) {\n+                finalQuery = maybeWrapPatienceByte(knnByteVectorQuery, Math.max(7, (int) (knnByteVectorQuery.getK() * 0.3)));\n+            } else if (knnQuery instanceof KnnFloatVectorQuery knnFloatVectorQuery) {\n+                finalQuery = maybeWrapPatienceFloat(knnFloatVectorQuery, Math.max(7, (int) (knnFloatVectorQuery.getK() * 0.3)));\n+            }\n+            return finalQuery;\n+        }\n+\n+        private Query maybeWrapPatienceByte(KnnByteVectorQuery knnQuery, int patience) {\n+            Query returnedQuery = knnQuery;\n+            if (indexOptions instanceof HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int8HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int4HnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof BBQHnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromByteQuery(knnQuery, 0.995, patience);\n+            }\n+            return returnedQuery;\n+        }\n+\n+        private Query maybeWrapPatienceFloat(KnnFloatVectorQuery knnQuery, int patience) {\n+            Query returnedQuery = knnQuery;\n+            if (indexOptions instanceof HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int8HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof Int4HnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            } else if (indexOptions instanceof BBQHnswIndexOptions hnswIndexOptions) {\n+                returnedQuery = PatienceKnnVectorQuery.fromFloatQuery(knnQuery, 0.995, patience);\n+            }\n+            return returnedQuery;",
        "comment_created_at": "2025-06-30T14:13:20+00:00",
        "comment_author": "benwtrent",
        "comment_body": "@tteofili OK, I understand that, I think maybe we can then collapse into predicate then :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2170926461",
    "pr_number": 130160,
    "pr_file": "x-pack/plugin/downsample/src/main/java/org/elasticsearch/xpack/downsample/DownsampleShardPersistentTaskExecutor.java",
    "created_at": "2025-06-27T06:44:02+00:00",
    "commented_code": ".orElse(NO_NODE_FOUND);\n    }\n\n    /**\n     * An eligible node to run the downsampling task for a shard is a node that holds\n     * a searchable version of this shard.\n     * In stateless deployment we choose only nodes that hold search shards.\n     * Otherwise, we choose the node that holds the primary shard.\n     * Visible for testing.\n     * @param indexShardRouting the routing of the shard to be downsampled\n     * @return the set of candidate nodes downsampling can run on.\n     */\n    Set<String> getEligibleNodes(IndexShardRoutingTable indexShardRouting) {",
    "repo_full_name": "elastic/elasticsearch",
    "discussion_comments": [
      {
        "comment_id": "2170926461",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 130160,
        "pr_file": "x-pack/plugin/downsample/src/main/java/org/elasticsearch/xpack/downsample/DownsampleShardPersistentTaskExecutor.java",
        "discussion_id": "2170926461",
        "commented_code": "@@ -159,6 +164,29 @@ public PersistentTasksCustomMetadata.Assignment getAssignment(\n             .orElse(NO_NODE_FOUND);\n     }\n \n+    /**\n+     * An eligible node to run the downsampling task for a shard is a node that holds\n+     * a searchable version of this shard.\n+     * In stateless deployment we choose only nodes that hold search shards.\n+     * Otherwise, we choose the node that holds the primary shard.\n+     * Visible for testing.\n+     * @param indexShardRouting the routing of the shard to be downsampled\n+     * @return the set of candidate nodes downsampling can run on.\n+     */\n+    Set<String> getEligibleNodes(IndexShardRoutingTable indexShardRouting) {",
        "comment_created_at": "2025-06-27T06:44:02+00:00",
        "comment_author": "martijnvg",
        "comment_body": "Maybe rewrite this method like this:\r\n\r\n```\r\nPredicate<DiscoveryNode> getEligibleNodes(IndexShardRoutingTable indexShardRouting) {\r\n        if (isStateless) {\r\n            return candidateNode -> {\r\n                for (var shardRouting : indexShardRouting.replicaShards()) {\r\n                    if (shardRouting.started()) {\r\n                        return shardRouting.currentNodeId().equals(candidateNode.getId());\r\n                    }\r\n                }\r\n                return false;\r\n            };\r\n        } else if (indexShardRouting.primaryShard().started()) {\r\n            return candidateNode -> indexShardRouting.primaryShard().currentNodeId().equals(candidateNode.getId());\r\n        } else {\r\n            return null;\r\n        }\r\n    }\r\n```\r\n\r\nThat way the logic is better contained and no intermediate set is created?",
        "pr_file_module": null
      },
      {
        "comment_id": "2171212580",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 130160,
        "pr_file": "x-pack/plugin/downsample/src/main/java/org/elasticsearch/xpack/downsample/DownsampleShardPersistentTaskExecutor.java",
        "discussion_id": "2170926461",
        "commented_code": "@@ -159,6 +164,29 @@ public PersistentTasksCustomMetadata.Assignment getAssignment(\n             .orElse(NO_NODE_FOUND);\n     }\n \n+    /**\n+     * An eligible node to run the downsampling task for a shard is a node that holds\n+     * a searchable version of this shard.\n+     * In stateless deployment we choose only nodes that hold search shards.\n+     * Otherwise, we choose the node that holds the primary shard.\n+     * Visible for testing.\n+     * @param indexShardRouting the routing of the shard to be downsampled\n+     * @return the set of candidate nodes downsampling can run on.\n+     */\n+    Set<String> getEligibleNodes(IndexShardRoutingTable indexShardRouting) {",
        "comment_created_at": "2025-06-27T08:29:16+00:00",
        "comment_author": "gmarouli",
        "comment_body": "I have been thinking about this, and I think both my current implementation and this can be a bit inefficient in big clusters because we iterate over all the candidate nodes that could be a lot. \r\n\r\nThinking about this some more, I am considering iterating over the eligible nodes which are probably way less than the total candidates and check if they are a candidate node.\r\n\r\nWhat do you think?\r\n\r\nAbout the code you shared, it does read nicely, but I am concerned that for every candidate node we are going to be building a new iterator over the replica shards ending up with more object churn than with the previous solution. Right?\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2171650282",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 130160,
        "pr_file": "x-pack/plugin/downsample/src/main/java/org/elasticsearch/xpack/downsample/DownsampleShardPersistentTaskExecutor.java",
        "discussion_id": "2170926461",
        "commented_code": "@@ -159,6 +164,29 @@ public PersistentTasksCustomMetadata.Assignment getAssignment(\n             .orElse(NO_NODE_FOUND);\n     }\n \n+    /**\n+     * An eligible node to run the downsampling task for a shard is a node that holds\n+     * a searchable version of this shard.\n+     * In stateless deployment we choose only nodes that hold search shards.\n+     * Otherwise, we choose the node that holds the primary shard.\n+     * Visible for testing.\n+     * @param indexShardRouting the routing of the shard to be downsampled\n+     * @return the set of candidate nodes downsampling can run on.\n+     */\n+    Set<String> getEligibleNodes(IndexShardRoutingTable indexShardRouting) {",
        "comment_created_at": "2025-06-27T11:07:12+00:00",
        "comment_author": "gmarouli",
        "comment_body": "@martijnvg I refactored it, I think it looks a bit cleaner now. Let me know what you think.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1977324330",
    "pr_number": 123322,
    "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/optimizer/QueryProgressFragmentOptimizer.java",
    "created_at": "2025-03-03T11:09:38+00:00",
    "commented_code": "/*\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n * or more contributor license agreements. Licensed under the Elastic License\n * 2.0; you may not use this file except in compliance with the Elastic License\n * 2.0.\n */\n\npackage org.elasticsearch.xpack.esql.optimizer;\n\nimport org.elasticsearch.core.Nullable;\nimport org.elasticsearch.xpack.esql.core.expression.FoldContext;\nimport org.elasticsearch.xpack.esql.core.expression.Literal;\nimport org.elasticsearch.xpack.esql.plan.logical.Limit;\nimport org.elasticsearch.xpack.esql.plan.physical.FragmentExec;\nimport org.elasticsearch.xpack.esql.plan.physical.PhysicalPlan;\nimport org.elasticsearch.xpack.esql.planner.mapper.Mapper;\n\nimport java.util.function.IntSupplier;\n\n/**\n * Attempts to rewrite the fragment enclosed in a data-node plan based on the progress of the query. For example:\n * `FROM x | LIMIT 100` can be safely rewritten to `FROM x | LIMIT 40` if 60 rows have already been collected.\n * TODO: Rewrite TopN to filters\n */\npublic final class QueryProgressFragmentOptimizer {\n    private final PhysicalVerifier verifier = PhysicalVerifier.INSTANCE;\n    private final IntSupplier alreadyCollectedLimit;\n\n    /**\n     * @param alreadyCollectedLimit the supplier to get the number of rows already collected\n     */\n    public QueryProgressFragmentOptimizer(@Nullable IntSupplier alreadyCollectedLimit) {\n        this.alreadyCollectedLimit = alreadyCollectedLimit;\n    }\n\n    /**\n     * Attempts to optimize the fragment enclosed in a data-node plan based on the progress of the query.\n     * @param plan the input plan\n     * @return the optimized plan. If this returns null, the query can be early terminated.\n     */\n    public PhysicalPlan optimizeFragment(PhysicalPlan plan) {\n        if (alreadyCollectedLimit == null) {\n            return plan;\n        }\n        final var fragments = plan.collectFirstChildren(p -> p instanceof FragmentExec);\n        if (fragments.size() != 1) {\n            return plan;\n        }\n        final FragmentExec fragment = (FragmentExec) fragments.getFirst();\n        final var pipelineBreakers = fragment.fragment().collectFirstChildren(Mapper::isPipelineBreaker);\n        if (pipelineBreakers.isEmpty()) {\n            return plan;\n        }\n        // Rewrite LIMIT\n        if (pipelineBreakers.getFirst() instanceof Limit firstLimit) {",
    "repo_full_name": "elastic/elasticsearch",
    "discussion_comments": [
      {
        "comment_id": "1977324330",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 123322,
        "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/optimizer/QueryProgressFragmentOptimizer.java",
        "discussion_id": "1977324330",
        "commented_code": "@@ -0,0 +1,84 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+\n+package org.elasticsearch.xpack.esql.optimizer;\n+\n+import org.elasticsearch.core.Nullable;\n+import org.elasticsearch.xpack.esql.core.expression.FoldContext;\n+import org.elasticsearch.xpack.esql.core.expression.Literal;\n+import org.elasticsearch.xpack.esql.plan.logical.Limit;\n+import org.elasticsearch.xpack.esql.plan.physical.FragmentExec;\n+import org.elasticsearch.xpack.esql.plan.physical.PhysicalPlan;\n+import org.elasticsearch.xpack.esql.planner.mapper.Mapper;\n+\n+import java.util.function.IntSupplier;\n+\n+/**\n+ * Attempts to rewrite the fragment enclosed in a data-node plan based on the progress of the query. For example:\n+ * `FROM x | LIMIT 100` can be safely rewritten to `FROM x | LIMIT 40` if 60 rows have already been collected.\n+ * TODO: Rewrite TopN to filters\n+ */\n+public final class QueryProgressFragmentOptimizer {\n+    private final PhysicalVerifier verifier = PhysicalVerifier.INSTANCE;\n+    private final IntSupplier alreadyCollectedLimit;\n+\n+    /**\n+     * @param alreadyCollectedLimit the supplier to get the number of rows already collected\n+     */\n+    public QueryProgressFragmentOptimizer(@Nullable IntSupplier alreadyCollectedLimit) {\n+        this.alreadyCollectedLimit = alreadyCollectedLimit;\n+    }\n+\n+    /**\n+     * Attempts to optimize the fragment enclosed in a data-node plan based on the progress of the query.\n+     * @param plan the input plan\n+     * @return the optimized plan. If this returns null, the query can be early terminated.\n+     */\n+    public PhysicalPlan optimizeFragment(PhysicalPlan plan) {\n+        if (alreadyCollectedLimit == null) {\n+            return plan;\n+        }\n+        final var fragments = plan.collectFirstChildren(p -> p instanceof FragmentExec);\n+        if (fragments.size() != 1) {\n+            return plan;\n+        }\n+        final FragmentExec fragment = (FragmentExec) fragments.getFirst();\n+        final var pipelineBreakers = fragment.fragment().collectFirstChildren(Mapper::isPipelineBreaker);\n+        if (pipelineBreakers.isEmpty()) {\n+            return plan;\n+        }\n+        // Rewrite LIMIT\n+        if (pipelineBreakers.getFirst() instanceof Limit firstLimit) {",
        "comment_created_at": "2025-03-03T11:09:38+00:00",
        "comment_author": "alex-spies",
        "comment_body": "The approach here assumes that there are no query plan nodes after the `LIMIT` which may rely on the number of rows that they see. I believe this is correct now (as there just shouldn't be anything after the `LIMIT`), but we cannot guarantee that this will always be the case, esp. as different teams add new commands to ES|QL.\r\n\r\nCouldn't we limit the scope of this optimizer rule to the case where the _last_ plan node in the fragment is a `LIMIT`? That's more easily provable to be correct and will break less likely when we add new query plan nodes.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1977694495",
    "pr_number": 123322,
    "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/optimizer/QueryProgressFragmentOptimizer.java",
    "created_at": "2025-03-03T15:14:00+00:00",
    "commented_code": "/*\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n * or more contributor license agreements. Licensed under the Elastic License\n * 2.0; you may not use this file except in compliance with the Elastic License\n * 2.0.\n */\n\npackage org.elasticsearch.xpack.esql.optimizer;\n\nimport org.elasticsearch.core.Nullable;\nimport org.elasticsearch.xpack.esql.core.expression.FoldContext;\nimport org.elasticsearch.xpack.esql.core.expression.Literal;\nimport org.elasticsearch.xpack.esql.plan.logical.Limit;\nimport org.elasticsearch.xpack.esql.plan.physical.FragmentExec;\nimport org.elasticsearch.xpack.esql.plan.physical.PhysicalPlan;\nimport org.elasticsearch.xpack.esql.planner.mapper.Mapper;\n\nimport java.util.function.IntSupplier;\n\n/**\n * Attempts to rewrite the fragment enclosed in a data-node plan based on the progress of the query. For example:\n * `FROM x | LIMIT 100` can be safely rewritten to `FROM x | LIMIT 40` if 60 rows have already been collected.\n * TODO: Rewrite TopN to filters\n */\npublic final class QueryProgressFragmentOptimizer {",
    "repo_full_name": "elastic/elasticsearch",
    "discussion_comments": [
      {
        "comment_id": "1977694495",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 123322,
        "pr_file": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/optimizer/QueryProgressFragmentOptimizer.java",
        "discussion_id": "1977694495",
        "commented_code": "@@ -0,0 +1,84 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+\n+package org.elasticsearch.xpack.esql.optimizer;\n+\n+import org.elasticsearch.core.Nullable;\n+import org.elasticsearch.xpack.esql.core.expression.FoldContext;\n+import org.elasticsearch.xpack.esql.core.expression.Literal;\n+import org.elasticsearch.xpack.esql.plan.logical.Limit;\n+import org.elasticsearch.xpack.esql.plan.physical.FragmentExec;\n+import org.elasticsearch.xpack.esql.plan.physical.PhysicalPlan;\n+import org.elasticsearch.xpack.esql.planner.mapper.Mapper;\n+\n+import java.util.function.IntSupplier;\n+\n+/**\n+ * Attempts to rewrite the fragment enclosed in a data-node plan based on the progress of the query. For example:\n+ * `FROM x | LIMIT 100` can be safely rewritten to `FROM x | LIMIT 40` if 60 rows have already been collected.\n+ * TODO: Rewrite TopN to filters\n+ */\n+public final class QueryProgressFragmentOptimizer {",
        "comment_created_at": "2025-03-03T15:14:00+00:00",
        "comment_author": "astefan",
        "comment_body": "I would look into integrating this step into existing optimization infrastructure. Even though this is a data-node level optimization that depends on how many rows have been returned, in essence I believe it should be a **logical plan level optimization**. There is a recent change in our code base that duplicates a `limit` in other parts of the query in certain situations which means the original plan can have a `limit 50` somewhere in the middle and now, instead of a `limit 50` somewhere at the end of the query, it has a `limit 10` or similar. Probably it doesn't affect the overall execution, but we don't know how this code evolves in the future.\r\n\r\nInitially, I thought that this could be put somewhere in `ComputeService:379` in the physical plan optimizer, meaning here\r\n```\r\nplan = PlannerUtils.localPlan(context.searchExecutionContexts(), context.configuration(), context.foldCtx(), plan);\r\n```\r\n\r\nAnd this method could receive the new value for the limit. That value should then be passed to the **physical plan optimizer**. **Maybe this _could_ be a better start for this feature.**\r\n\r\nI think this is a really great idea for optimizing simple queries to be faster, but a less fragile implementation should integrate this step in the usual optimization processes, since this _could_ interfere with other optimization steps. Having it there instead can also be a signal for future development that this optimization exists. The code in the ES|QL distributed code is quite separated from the one in the optimization plan code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2100298832",
    "pr_number": 128135,
    "pr_file": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/topn/TopNOperator.java",
    "created_at": "2025-05-21T13:26:36+00:00",
    "commented_code": "private final BlockFactory blockFactory;\n    private final CircuitBreaker breaker;\n    private final Queue inputQueue;\n    private final Map<String, Queue> inputQueues;",
    "repo_full_name": "elastic/elasticsearch",
    "discussion_comments": [
      {
        "comment_id": "2100298832",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128135,
        "pr_file": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/topn/TopNOperator.java",
        "discussion_id": "2100298832",
        "commented_code": "@@ -264,12 +282,14 @@ public String describe() {\n \n     private final BlockFactory blockFactory;\n     private final CircuitBreaker breaker;\n-    private final Queue inputQueue;\n+    private final Map<String, Queue> inputQueues;",
        "comment_created_at": "2025-05-21T13:26:36+00:00",
        "comment_author": "nik9000",
        "comment_body": "I'm a bit worried about memory tracking for the keys.\r\n\r\nGenerally we use `ObjectArray` to keep things like the `Queue`s and we use `BytesRefHash` to assign the ids.\r\n\r\nIf we did that it'd read very very very similarly to an aggregation. In aggs-land we separate the hashing implementation from the \"collecting\" one. I wonder if we'd be better off using the aggs infrastructure in this case. No idea how we migrate from one to the other, but we have a `TopBytesRefAggregator`. And we could try to use that *somehow*. If we did that we'd get nice handling for the hashing for free. We're quite good at those hash keys.",
        "pr_file_module": null
      },
      {
        "comment_id": "2102502245",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128135,
        "pr_file": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/topn/TopNOperator.java",
        "discussion_id": "2100298832",
        "commented_code": "@@ -264,12 +282,14 @@ public String describe() {\n \n     private final BlockFactory blockFactory;\n     private final CircuitBreaker breaker;\n-    private final Queue inputQueue;\n+    private final Map<String, Queue> inputQueues;",
        "comment_created_at": "2025-05-22T12:59:38+00:00",
        "comment_author": "przemekwitek",
        "comment_body": "> Generally we use ObjectArray to keep things like the Queues and we use BytesRefHash to assign the ids.\r\n\r\nOk, that's something I'll take a look into.\r\n\r\n> And we could try to use that somehow.\r\n\r\nDo you think it would be beneficial to do it in this PR? To me it sounds like quite a complex refactoring.",
        "pr_file_module": null
      },
      {
        "comment_id": "2102526209",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128135,
        "pr_file": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/topn/TopNOperator.java",
        "discussion_id": "2100298832",
        "commented_code": "@@ -264,12 +282,14 @@ public String describe() {\n \n     private final BlockFactory blockFactory;\n     private final CircuitBreaker breaker;\n-    private final Queue inputQueue;\n+    private final Map<String, Queue> inputQueues;",
        "comment_created_at": "2025-05-22T13:11:25+00:00",
        "comment_author": "nik9000",
        "comment_body": "It is. It has the advantage of applying all of the grouping code we worked hard on for aggs so you don't have to hand build something on your side. Maybe you'd be better off just using the `BlockHash.build` infrastructure here. It'd be quite compatible with the ObjectArray stuff. I'm just worried that if you did it you'd find yourself building most of the `HashAggregatorOperator`.\r\n\r\nI can have a little look around this morning though.",
        "pr_file_module": null
      },
      {
        "comment_id": "2102580378",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 128135,
        "pr_file": "x-pack/plugin/esql/compute/src/main/java/org/elasticsearch/compute/operator/topn/TopNOperator.java",
        "discussion_id": "2100298832",
        "commented_code": "@@ -264,12 +282,14 @@ public String describe() {\n \n     private final BlockFactory blockFactory;\n     private final CircuitBreaker breaker;\n-    private final Queue inputQueue;\n+    private final Map<String, Queue> inputQueues;",
        "comment_created_at": "2025-05-22T13:34:28+00:00",
        "comment_author": "nik9000",
        "comment_body": "I had a look and think it's probably not worth trying to migrate the whole thing to an agg. Aggs encode their intermediate representation as a single dense column. TopN builds the result columns. They aren't incompatible, but it'd be tricky to migrate.\r\n\r\nI suppose what I suggest instead is to use `BlockHash` to turn the grouping keys into an integer and then `ObjectArray` to contain the heaps. In that case `BlockHash` is going to do a good job managing memory, breaking if it uses too much, and cleaning up. It also has support for non-string keys and complex sets of keys. And will absorb any optimizations we make in the future.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2099445576",
    "pr_number": 127260,
    "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
    "created_at": "2025-05-21T06:17:28+00:00",
    "commented_code": "/*\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n * or more contributor license agreements. Licensed under the \"Elastic License\n * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n * Public License v 1\"; you may not use this file except in compliance with, at\n * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n * License v3.0 only\", or the \"Server Side Public License, v 1\".\n */\npackage org.elasticsearch.lucene.queries;\n\nimport org.apache.lucene.index.DocValuesSkipper;\nimport org.apache.lucene.index.NumericDocValues;\nimport org.apache.lucene.search.DocIdSetIterator;\nimport org.apache.lucene.search.TwoPhaseIterator;\n\nimport java.io.IOException;\n\n/**\n * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n * <p>\n * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n * <p>\n * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n * the current value is lower than minTimestamp.\n */\nfinal class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n\n    private final RangeNoGapsApproximation approximation;\n    private final NumericDocValues timestamps;\n\n    private final long minTimestamp;\n    private final long maxTimestamp;\n\n    TimestampTwoPhaseIterator(\n        NumericDocValues timestamps,\n        DocValuesSkipper timestampSkipper,\n        DocValuesSkipper primaryFieldSkipper,\n        long minTimestamp,\n        long maxTimestamp\n    ) {\n        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n        this.approximation = (RangeNoGapsApproximation) approximation();\n        this.timestamps = timestamps;\n        this.minTimestamp = minTimestamp;\n        this.maxTimestamp = maxTimestamp;\n    }\n\n    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n\n        private final DocIdSetIterator innerApproximation;\n\n        final DocValuesSkipper timestampSkipper;\n        final DocValuesSkipper primaryFieldSkipper;\n        final long minTimestamp;\n        final long maxTimestamp;\n\n        private int doc = -1;\n\n        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n        Match match = Match.MAYBE;\n        int upTo = -1;\n        int primaryFieldUpTo = -1;\n\n        RangeNoGapsApproximation(\n            DocIdSetIterator innerApproximation,\n            DocValuesSkipper timestampSkipper,\n            DocValuesSkipper primaryFieldSkipper,\n            long minTimestamp,\n            long maxTimestamp\n        ) {\n            this.innerApproximation = innerApproximation;\n            this.timestampSkipper = timestampSkipper;\n            this.primaryFieldSkipper = primaryFieldSkipper;\n            this.minTimestamp = minTimestamp;\n            this.maxTimestamp = maxTimestamp;\n        }\n\n        @Override\n        public int docID() {\n            return doc;\n        }\n\n        @Override\n        public int nextDoc() throws IOException {\n            return advance(docID() + 1);\n        }\n\n        @Override\n        public int advance(int target) throws IOException {\n            while (true) {\n                if (target > upTo) {\n                    timestampSkipper.advance(target);\n                    upTo = timestampSkipper.maxDocID(0);\n                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n                    }\n                    match = match(0);\n\n                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n                    // level (= on a wider range of doc IDs)\n                    int nextLevel = 1;\n                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n                        upTo = timestampSkipper.maxDocID(nextLevel);\n                        nextLevel++;\n                    }\n                }\n                switch (match) {\n                    case YES:\n                        return doc = target;\n                    case MAYBE:\n                        if (target > innerApproximation.docID()) {\n                            target = innerApproximation.advance(target);",
    "repo_full_name": "elastic/elasticsearch",
    "discussion_comments": [
      {
        "comment_id": "2099445576",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127260,
        "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
        "discussion_id": "2099445576",
        "commented_code": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+package org.elasticsearch.lucene.queries;\n+\n+import org.apache.lucene.index.DocValuesSkipper;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.search.TwoPhaseIterator;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n+ * <p>\n+ * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n+ * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n+ * <p>\n+ * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n+ * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n+ * the current value is lower than minTimestamp.\n+ */\n+final class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n+\n+    private final RangeNoGapsApproximation approximation;\n+    private final NumericDocValues timestamps;\n+\n+    private final long minTimestamp;\n+    private final long maxTimestamp;\n+\n+    TimestampTwoPhaseIterator(\n+        NumericDocValues timestamps,\n+        DocValuesSkipper timestampSkipper,\n+        DocValuesSkipper primaryFieldSkipper,\n+        long minTimestamp,\n+        long maxTimestamp\n+    ) {\n+        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n+        this.approximation = (RangeNoGapsApproximation) approximation();\n+        this.timestamps = timestamps;\n+        this.minTimestamp = minTimestamp;\n+        this.maxTimestamp = maxTimestamp;\n+    }\n+\n+    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n+\n+        private final DocIdSetIterator innerApproximation;\n+\n+        final DocValuesSkipper timestampSkipper;\n+        final DocValuesSkipper primaryFieldSkipper;\n+        final long minTimestamp;\n+        final long maxTimestamp;\n+\n+        private int doc = -1;\n+\n+        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n+        Match match = Match.MAYBE;\n+        int upTo = -1;\n+        int primaryFieldUpTo = -1;\n+\n+        RangeNoGapsApproximation(\n+            DocIdSetIterator innerApproximation,\n+            DocValuesSkipper timestampSkipper,\n+            DocValuesSkipper primaryFieldSkipper,\n+            long minTimestamp,\n+            long maxTimestamp\n+        ) {\n+            this.innerApproximation = innerApproximation;\n+            this.timestampSkipper = timestampSkipper;\n+            this.primaryFieldSkipper = primaryFieldSkipper;\n+            this.minTimestamp = minTimestamp;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return doc;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            return advance(docID() + 1);\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            while (true) {\n+                if (target > upTo) {\n+                    timestampSkipper.advance(target);\n+                    upTo = timestampSkipper.maxDocID(0);\n+                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n+                    }\n+                    match = match(0);\n+\n+                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n+                    // level (= on a wider range of doc IDs)\n+                    int nextLevel = 1;\n+                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n+                        upTo = timestampSkipper.maxDocID(nextLevel);\n+                        nextLevel++;\n+                    }\n+                }\n+                switch (match) {\n+                    case YES:\n+                        return doc = target;\n+                    case MAYBE:\n+                        if (target > innerApproximation.docID()) {\n+                            target = innerApproximation.advance(target);",
        "comment_created_at": "2025-05-21T06:17:28+00:00",
        "comment_author": "iverase",
        "comment_body": "I would expect to advance here the timestamps iterator instead of the innerApproximation. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2099525131",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127260,
        "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
        "discussion_id": "2099445576",
        "commented_code": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+package org.elasticsearch.lucene.queries;\n+\n+import org.apache.lucene.index.DocValuesSkipper;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.search.TwoPhaseIterator;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n+ * <p>\n+ * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n+ * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n+ * <p>\n+ * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n+ * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n+ * the current value is lower than minTimestamp.\n+ */\n+final class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n+\n+    private final RangeNoGapsApproximation approximation;\n+    private final NumericDocValues timestamps;\n+\n+    private final long minTimestamp;\n+    private final long maxTimestamp;\n+\n+    TimestampTwoPhaseIterator(\n+        NumericDocValues timestamps,\n+        DocValuesSkipper timestampSkipper,\n+        DocValuesSkipper primaryFieldSkipper,\n+        long minTimestamp,\n+        long maxTimestamp\n+    ) {\n+        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n+        this.approximation = (RangeNoGapsApproximation) approximation();\n+        this.timestamps = timestamps;\n+        this.minTimestamp = minTimestamp;\n+        this.maxTimestamp = maxTimestamp;\n+    }\n+\n+    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n+\n+        private final DocIdSetIterator innerApproximation;\n+\n+        final DocValuesSkipper timestampSkipper;\n+        final DocValuesSkipper primaryFieldSkipper;\n+        final long minTimestamp;\n+        final long maxTimestamp;\n+\n+        private int doc = -1;\n+\n+        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n+        Match match = Match.MAYBE;\n+        int upTo = -1;\n+        int primaryFieldUpTo = -1;\n+\n+        RangeNoGapsApproximation(\n+            DocIdSetIterator innerApproximation,\n+            DocValuesSkipper timestampSkipper,\n+            DocValuesSkipper primaryFieldSkipper,\n+            long minTimestamp,\n+            long maxTimestamp\n+        ) {\n+            this.innerApproximation = innerApproximation;\n+            this.timestampSkipper = timestampSkipper;\n+            this.primaryFieldSkipper = primaryFieldSkipper;\n+            this.minTimestamp = minTimestamp;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return doc;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            return advance(docID() + 1);\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            while (true) {\n+                if (target > upTo) {\n+                    timestampSkipper.advance(target);\n+                    upTo = timestampSkipper.maxDocID(0);\n+                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n+                    }\n+                    match = match(0);\n+\n+                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n+                    // level (= on a wider range of doc IDs)\n+                    int nextLevel = 1;\n+                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n+                        upTo = timestampSkipper.maxDocID(nextLevel);\n+                        nextLevel++;\n+                    }\n+                }\n+                switch (match) {\n+                    case YES:\n+                        return doc = target;\n+                    case MAYBE:\n+                        if (target > innerApproximation.docID()) {\n+                            target = innerApproximation.advance(target);",
        "comment_created_at": "2025-05-21T07:05:52+00:00",
        "comment_author": "iverase",
        "comment_body": "I see now that it is equivalent. Would it make sense to make innerApproximation a NumericDocValues and remove the reference from the parent class? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2111161746",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127260,
        "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
        "discussion_id": "2099445576",
        "commented_code": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+package org.elasticsearch.lucene.queries;\n+\n+import org.apache.lucene.index.DocValuesSkipper;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.search.TwoPhaseIterator;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n+ * <p>\n+ * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n+ * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n+ * <p>\n+ * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n+ * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n+ * the current value is lower than minTimestamp.\n+ */\n+final class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n+\n+    private final RangeNoGapsApproximation approximation;\n+    private final NumericDocValues timestamps;\n+\n+    private final long minTimestamp;\n+    private final long maxTimestamp;\n+\n+    TimestampTwoPhaseIterator(\n+        NumericDocValues timestamps,\n+        DocValuesSkipper timestampSkipper,\n+        DocValuesSkipper primaryFieldSkipper,\n+        long minTimestamp,\n+        long maxTimestamp\n+    ) {\n+        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n+        this.approximation = (RangeNoGapsApproximation) approximation();\n+        this.timestamps = timestamps;\n+        this.minTimestamp = minTimestamp;\n+        this.maxTimestamp = maxTimestamp;\n+    }\n+\n+    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n+\n+        private final DocIdSetIterator innerApproximation;\n+\n+        final DocValuesSkipper timestampSkipper;\n+        final DocValuesSkipper primaryFieldSkipper;\n+        final long minTimestamp;\n+        final long maxTimestamp;\n+\n+        private int doc = -1;\n+\n+        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n+        Match match = Match.MAYBE;\n+        int upTo = -1;\n+        int primaryFieldUpTo = -1;\n+\n+        RangeNoGapsApproximation(\n+            DocIdSetIterator innerApproximation,\n+            DocValuesSkipper timestampSkipper,\n+            DocValuesSkipper primaryFieldSkipper,\n+            long minTimestamp,\n+            long maxTimestamp\n+        ) {\n+            this.innerApproximation = innerApproximation;\n+            this.timestampSkipper = timestampSkipper;\n+            this.primaryFieldSkipper = primaryFieldSkipper;\n+            this.minTimestamp = minTimestamp;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return doc;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            return advance(docID() + 1);\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            while (true) {\n+                if (target > upTo) {\n+                    timestampSkipper.advance(target);\n+                    upTo = timestampSkipper.maxDocID(0);\n+                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n+                    }\n+                    match = match(0);\n+\n+                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n+                    // level (= on a wider range of doc IDs)\n+                    int nextLevel = 1;\n+                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n+                        upTo = timestampSkipper.maxDocID(nextLevel);\n+                        nextLevel++;\n+                    }\n+                }\n+                switch (match) {\n+                    case YES:\n+                        return doc = target;\n+                    case MAYBE:\n+                        if (target > innerApproximation.docID()) {\n+                            target = innerApproximation.advance(target);",
        "comment_created_at": "2025-05-28T07:37:57+00:00",
        "comment_author": "martijnvg",
        "comment_body": "done: 24819c4929b7591fdb9795333cf4d1e3329cdfc3",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2099475391",
    "pr_number": 127260,
    "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
    "created_at": "2025-05-21T06:38:24+00:00",
    "commented_code": "/*\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n * or more contributor license agreements. Licensed under the \"Elastic License\n * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n * Public License v 1\"; you may not use this file except in compliance with, at\n * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n * License v3.0 only\", or the \"Server Side Public License, v 1\".\n */\npackage org.elasticsearch.lucene.queries;\n\nimport org.apache.lucene.index.DocValuesSkipper;\nimport org.apache.lucene.index.NumericDocValues;\nimport org.apache.lucene.search.DocIdSetIterator;\nimport org.apache.lucene.search.TwoPhaseIterator;\n\nimport java.io.IOException;\n\n/**\n * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n * <p>\n * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n * <p>\n * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n * the current value is lower than minTimestamp.\n */\nfinal class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n\n    private final RangeNoGapsApproximation approximation;\n    private final NumericDocValues timestamps;\n\n    private final long minTimestamp;\n    private final long maxTimestamp;\n\n    TimestampTwoPhaseIterator(\n        NumericDocValues timestamps,\n        DocValuesSkipper timestampSkipper,\n        DocValuesSkipper primaryFieldSkipper,\n        long minTimestamp,\n        long maxTimestamp\n    ) {\n        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n        this.approximation = (RangeNoGapsApproximation) approximation();\n        this.timestamps = timestamps;\n        this.minTimestamp = minTimestamp;\n        this.maxTimestamp = maxTimestamp;\n    }\n\n    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n\n        private final DocIdSetIterator innerApproximation;\n\n        final DocValuesSkipper timestampSkipper;\n        final DocValuesSkipper primaryFieldSkipper;\n        final long minTimestamp;\n        final long maxTimestamp;\n\n        private int doc = -1;\n\n        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n        Match match = Match.MAYBE;\n        int upTo = -1;\n        int primaryFieldUpTo = -1;\n\n        RangeNoGapsApproximation(\n            DocIdSetIterator innerApproximation,\n            DocValuesSkipper timestampSkipper,\n            DocValuesSkipper primaryFieldSkipper,\n            long minTimestamp,\n            long maxTimestamp\n        ) {\n            this.innerApproximation = innerApproximation;\n            this.timestampSkipper = timestampSkipper;\n            this.primaryFieldSkipper = primaryFieldSkipper;\n            this.minTimestamp = minTimestamp;\n            this.maxTimestamp = maxTimestamp;\n        }\n\n        @Override\n        public int docID() {\n            return doc;\n        }\n\n        @Override\n        public int nextDoc() throws IOException {\n            return advance(docID() + 1);\n        }\n\n        @Override\n        public int advance(int target) throws IOException {\n            while (true) {\n                if (target > upTo) {\n                    timestampSkipper.advance(target);\n                    upTo = timestampSkipper.maxDocID(0);\n                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n                    }\n                    match = match(0);\n\n                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n                    // level (= on a wider range of doc IDs)\n                    int nextLevel = 1;\n                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n                        upTo = timestampSkipper.maxDocID(nextLevel);\n                        nextLevel++;\n                    }\n                }\n                switch (match) {\n                    case YES:\n                        return doc = target;\n                    case MAYBE:\n                        if (target > innerApproximation.docID()) {\n                            target = innerApproximation.advance(target);\n                        }\n                        if (target <= upTo) {\n                            return doc = target;\n                        }\n                        break;\n                    case NO_AND_SKIP:\n                        if (target > primaryFieldUpTo) {\n                            primaryFieldSkipper.advance(target);\n                            for (int level = 0; level < primaryFieldSkipper.numLevels(); level++) {\n                                if (primaryFieldSkipper.minValue(level) == primaryFieldSkipper.maxValue(level)) {\n                                    primaryFieldUpTo = primaryFieldSkipper.maxDocID(level);\n                                } else {\n                                    break;\n                                }\n                            }\n                            if (primaryFieldUpTo > upTo) {\n                                upTo = primaryFieldUpTo;\n                            }\n                        }\n                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n                            return doc = NO_MORE_DOCS;\n                        }\n                        target = upTo + 1;\n                        break;\n                    case NO:\n                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n                            return doc = NO_MORE_DOCS;\n                        }\n                        target = upTo + 1;\n                        break;\n                    default:\n                        throw new AssertionError(\"Unknown enum constant: \" + match);\n                }\n            }\n        }\n\n        @Override\n        public long cost() {\n            return innerApproximation.cost();\n        }\n\n        Match match(int level) {\n            long minValue = timestampSkipper.minValue(level);\n            long maxValue = timestampSkipper.maxValue(level);\n            if (minValue > maxTimestamp) {\n                return Match.NO;\n            } else if (maxValue < minTimestamp) {\n                return Match.NO_AND_SKIP;\n            } else if (minValue >= minTimestamp && maxValue <= maxTimestamp) {\n                return Match.YES;\n            } else {\n                return Match.MAYBE;\n            }\n        }\n\n    }\n\n    @Override\n    public boolean matches() throws IOException {\n        return switch (approximation.match) {\n            case YES -> true;\n            case MAYBE -> {\n                final long value = timestamps.longValue();\n                yield value >= minTimestamp && value <= maxTimestamp;",
    "repo_full_name": "elastic/elasticsearch",
    "discussion_comments": [
      {
        "comment_id": "2099475391",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127260,
        "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
        "discussion_id": "2099475391",
        "commented_code": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+package org.elasticsearch.lucene.queries;\n+\n+import org.apache.lucene.index.DocValuesSkipper;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.search.TwoPhaseIterator;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n+ * <p>\n+ * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n+ * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n+ * <p>\n+ * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n+ * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n+ * the current value is lower than minTimestamp.\n+ */\n+final class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n+\n+    private final RangeNoGapsApproximation approximation;\n+    private final NumericDocValues timestamps;\n+\n+    private final long minTimestamp;\n+    private final long maxTimestamp;\n+\n+    TimestampTwoPhaseIterator(\n+        NumericDocValues timestamps,\n+        DocValuesSkipper timestampSkipper,\n+        DocValuesSkipper primaryFieldSkipper,\n+        long minTimestamp,\n+        long maxTimestamp\n+    ) {\n+        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n+        this.approximation = (RangeNoGapsApproximation) approximation();\n+        this.timestamps = timestamps;\n+        this.minTimestamp = minTimestamp;\n+        this.maxTimestamp = maxTimestamp;\n+    }\n+\n+    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n+\n+        private final DocIdSetIterator innerApproximation;\n+\n+        final DocValuesSkipper timestampSkipper;\n+        final DocValuesSkipper primaryFieldSkipper;\n+        final long minTimestamp;\n+        final long maxTimestamp;\n+\n+        private int doc = -1;\n+\n+        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n+        Match match = Match.MAYBE;\n+        int upTo = -1;\n+        int primaryFieldUpTo = -1;\n+\n+        RangeNoGapsApproximation(\n+            DocIdSetIterator innerApproximation,\n+            DocValuesSkipper timestampSkipper,\n+            DocValuesSkipper primaryFieldSkipper,\n+            long minTimestamp,\n+            long maxTimestamp\n+        ) {\n+            this.innerApproximation = innerApproximation;\n+            this.timestampSkipper = timestampSkipper;\n+            this.primaryFieldSkipper = primaryFieldSkipper;\n+            this.minTimestamp = minTimestamp;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return doc;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            return advance(docID() + 1);\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            while (true) {\n+                if (target > upTo) {\n+                    timestampSkipper.advance(target);\n+                    upTo = timestampSkipper.maxDocID(0);\n+                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n+                    }\n+                    match = match(0);\n+\n+                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n+                    // level (= on a wider range of doc IDs)\n+                    int nextLevel = 1;\n+                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n+                        upTo = timestampSkipper.maxDocID(nextLevel);\n+                        nextLevel++;\n+                    }\n+                }\n+                switch (match) {\n+                    case YES:\n+                        return doc = target;\n+                    case MAYBE:\n+                        if (target > innerApproximation.docID()) {\n+                            target = innerApproximation.advance(target);\n+                        }\n+                        if (target <= upTo) {\n+                            return doc = target;\n+                        }\n+                        break;\n+                    case NO_AND_SKIP:\n+                        if (target > primaryFieldUpTo) {\n+                            primaryFieldSkipper.advance(target);\n+                            for (int level = 0; level < primaryFieldSkipper.numLevels(); level++) {\n+                                if (primaryFieldSkipper.minValue(level) == primaryFieldSkipper.maxValue(level)) {\n+                                    primaryFieldUpTo = primaryFieldSkipper.maxDocID(level);\n+                                } else {\n+                                    break;\n+                                }\n+                            }\n+                            if (primaryFieldUpTo > upTo) {\n+                                upTo = primaryFieldUpTo;\n+                            }\n+                        }\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    case NO:\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    default:\n+                        throw new AssertionError(\"Unknown enum constant: \" + match);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public long cost() {\n+            return innerApproximation.cost();\n+        }\n+\n+        Match match(int level) {\n+            long minValue = timestampSkipper.minValue(level);\n+            long maxValue = timestampSkipper.maxValue(level);\n+            if (minValue > maxTimestamp) {\n+                return Match.NO;\n+            } else if (maxValue < minTimestamp) {\n+                return Match.NO_AND_SKIP;\n+            } else if (minValue >= minTimestamp && maxValue <= maxTimestamp) {\n+                return Match.YES;\n+            } else {\n+                return Match.MAYBE;\n+            }\n+        }\n+\n+    }\n+\n+    @Override\n+    public boolean matches() throws IOException {\n+        return switch (approximation.match) {\n+            case YES -> true;\n+            case MAYBE -> {\n+                final long value = timestamps.longValue();\n+                yield value >= minTimestamp && value <= maxTimestamp;",
        "comment_created_at": "2025-05-21T06:38:24+00:00",
        "comment_author": "iverase",
        "comment_body": "if value < minTimestamp, we know we will not be matching any more documents, maybe we can move approximation.match to NO?",
        "pr_file_module": null
      },
      {
        "comment_id": "2099478400",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127260,
        "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
        "discussion_id": "2099475391",
        "commented_code": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+package org.elasticsearch.lucene.queries;\n+\n+import org.apache.lucene.index.DocValuesSkipper;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.search.TwoPhaseIterator;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n+ * <p>\n+ * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n+ * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n+ * <p>\n+ * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n+ * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n+ * the current value is lower than minTimestamp.\n+ */\n+final class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n+\n+    private final RangeNoGapsApproximation approximation;\n+    private final NumericDocValues timestamps;\n+\n+    private final long minTimestamp;\n+    private final long maxTimestamp;\n+\n+    TimestampTwoPhaseIterator(\n+        NumericDocValues timestamps,\n+        DocValuesSkipper timestampSkipper,\n+        DocValuesSkipper primaryFieldSkipper,\n+        long minTimestamp,\n+        long maxTimestamp\n+    ) {\n+        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n+        this.approximation = (RangeNoGapsApproximation) approximation();\n+        this.timestamps = timestamps;\n+        this.minTimestamp = minTimestamp;\n+        this.maxTimestamp = maxTimestamp;\n+    }\n+\n+    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n+\n+        private final DocIdSetIterator innerApproximation;\n+\n+        final DocValuesSkipper timestampSkipper;\n+        final DocValuesSkipper primaryFieldSkipper;\n+        final long minTimestamp;\n+        final long maxTimestamp;\n+\n+        private int doc = -1;\n+\n+        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n+        Match match = Match.MAYBE;\n+        int upTo = -1;\n+        int primaryFieldUpTo = -1;\n+\n+        RangeNoGapsApproximation(\n+            DocIdSetIterator innerApproximation,\n+            DocValuesSkipper timestampSkipper,\n+            DocValuesSkipper primaryFieldSkipper,\n+            long minTimestamp,\n+            long maxTimestamp\n+        ) {\n+            this.innerApproximation = innerApproximation;\n+            this.timestampSkipper = timestampSkipper;\n+            this.primaryFieldSkipper = primaryFieldSkipper;\n+            this.minTimestamp = minTimestamp;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return doc;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            return advance(docID() + 1);\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            while (true) {\n+                if (target > upTo) {\n+                    timestampSkipper.advance(target);\n+                    upTo = timestampSkipper.maxDocID(0);\n+                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n+                    }\n+                    match = match(0);\n+\n+                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n+                    // level (= on a wider range of doc IDs)\n+                    int nextLevel = 1;\n+                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n+                        upTo = timestampSkipper.maxDocID(nextLevel);\n+                        nextLevel++;\n+                    }\n+                }\n+                switch (match) {\n+                    case YES:\n+                        return doc = target;\n+                    case MAYBE:\n+                        if (target > innerApproximation.docID()) {\n+                            target = innerApproximation.advance(target);\n+                        }\n+                        if (target <= upTo) {\n+                            return doc = target;\n+                        }\n+                        break;\n+                    case NO_AND_SKIP:\n+                        if (target > primaryFieldUpTo) {\n+                            primaryFieldSkipper.advance(target);\n+                            for (int level = 0; level < primaryFieldSkipper.numLevels(); level++) {\n+                                if (primaryFieldSkipper.minValue(level) == primaryFieldSkipper.maxValue(level)) {\n+                                    primaryFieldUpTo = primaryFieldSkipper.maxDocID(level);\n+                                } else {\n+                                    break;\n+                                }\n+                            }\n+                            if (primaryFieldUpTo > upTo) {\n+                                upTo = primaryFieldUpTo;\n+                            }\n+                        }\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    case NO:\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    default:\n+                        throw new AssertionError(\"Unknown enum constant: \" + match);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public long cost() {\n+            return innerApproximation.cost();\n+        }\n+\n+        Match match(int level) {\n+            long minValue = timestampSkipper.minValue(level);\n+            long maxValue = timestampSkipper.maxValue(level);\n+            if (minValue > maxTimestamp) {\n+                return Match.NO;\n+            } else if (maxValue < minTimestamp) {\n+                return Match.NO_AND_SKIP;\n+            } else if (minValue >= minTimestamp && maxValue <= maxTimestamp) {\n+                return Match.YES;\n+            } else {\n+                return Match.MAYBE;\n+            }\n+        }\n+\n+    }\n+\n+    @Override\n+    public boolean matches() throws IOException {\n+        return switch (approximation.match) {\n+            case YES -> true;\n+            case MAYBE -> {\n+                final long value = timestamps.longValue();\n+                yield value >= minTimestamp && value <= maxTimestamp;",
        "comment_created_at": "2025-05-21T06:40:02+00:00",
        "comment_author": "iverase",
        "comment_body": "Although this is only true if we are always in the same primary sort.",
        "pr_file_module": null
      },
      {
        "comment_id": "2099497432",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127260,
        "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
        "discussion_id": "2099475391",
        "commented_code": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+package org.elasticsearch.lucene.queries;\n+\n+import org.apache.lucene.index.DocValuesSkipper;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.search.TwoPhaseIterator;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n+ * <p>\n+ * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n+ * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n+ * <p>\n+ * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n+ * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n+ * the current value is lower than minTimestamp.\n+ */\n+final class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n+\n+    private final RangeNoGapsApproximation approximation;\n+    private final NumericDocValues timestamps;\n+\n+    private final long minTimestamp;\n+    private final long maxTimestamp;\n+\n+    TimestampTwoPhaseIterator(\n+        NumericDocValues timestamps,\n+        DocValuesSkipper timestampSkipper,\n+        DocValuesSkipper primaryFieldSkipper,\n+        long minTimestamp,\n+        long maxTimestamp\n+    ) {\n+        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n+        this.approximation = (RangeNoGapsApproximation) approximation();\n+        this.timestamps = timestamps;\n+        this.minTimestamp = minTimestamp;\n+        this.maxTimestamp = maxTimestamp;\n+    }\n+\n+    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n+\n+        private final DocIdSetIterator innerApproximation;\n+\n+        final DocValuesSkipper timestampSkipper;\n+        final DocValuesSkipper primaryFieldSkipper;\n+        final long minTimestamp;\n+        final long maxTimestamp;\n+\n+        private int doc = -1;\n+\n+        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n+        Match match = Match.MAYBE;\n+        int upTo = -1;\n+        int primaryFieldUpTo = -1;\n+\n+        RangeNoGapsApproximation(\n+            DocIdSetIterator innerApproximation,\n+            DocValuesSkipper timestampSkipper,\n+            DocValuesSkipper primaryFieldSkipper,\n+            long minTimestamp,\n+            long maxTimestamp\n+        ) {\n+            this.innerApproximation = innerApproximation;\n+            this.timestampSkipper = timestampSkipper;\n+            this.primaryFieldSkipper = primaryFieldSkipper;\n+            this.minTimestamp = minTimestamp;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return doc;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            return advance(docID() + 1);\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            while (true) {\n+                if (target > upTo) {\n+                    timestampSkipper.advance(target);\n+                    upTo = timestampSkipper.maxDocID(0);\n+                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n+                    }\n+                    match = match(0);\n+\n+                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n+                    // level (= on a wider range of doc IDs)\n+                    int nextLevel = 1;\n+                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n+                        upTo = timestampSkipper.maxDocID(nextLevel);\n+                        nextLevel++;\n+                    }\n+                }\n+                switch (match) {\n+                    case YES:\n+                        return doc = target;\n+                    case MAYBE:\n+                        if (target > innerApproximation.docID()) {\n+                            target = innerApproximation.advance(target);\n+                        }\n+                        if (target <= upTo) {\n+                            return doc = target;\n+                        }\n+                        break;\n+                    case NO_AND_SKIP:\n+                        if (target > primaryFieldUpTo) {\n+                            primaryFieldSkipper.advance(target);\n+                            for (int level = 0; level < primaryFieldSkipper.numLevels(); level++) {\n+                                if (primaryFieldSkipper.minValue(level) == primaryFieldSkipper.maxValue(level)) {\n+                                    primaryFieldUpTo = primaryFieldSkipper.maxDocID(level);\n+                                } else {\n+                                    break;\n+                                }\n+                            }\n+                            if (primaryFieldUpTo > upTo) {\n+                                upTo = primaryFieldUpTo;\n+                            }\n+                        }\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    case NO:\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    default:\n+                        throw new AssertionError(\"Unknown enum constant: \" + match);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public long cost() {\n+            return innerApproximation.cost();\n+        }\n+\n+        Match match(int level) {\n+            long minValue = timestampSkipper.minValue(level);\n+            long maxValue = timestampSkipper.maxValue(level);\n+            if (minValue > maxTimestamp) {\n+                return Match.NO;\n+            } else if (maxValue < minTimestamp) {\n+                return Match.NO_AND_SKIP;\n+            } else if (minValue >= minTimestamp && maxValue <= maxTimestamp) {\n+                return Match.YES;\n+            } else {\n+                return Match.MAYBE;\n+            }\n+        }\n+\n+    }\n+\n+    @Override\n+    public boolean matches() throws IOException {\n+        return switch (approximation.match) {\n+            case YES -> true;\n+            case MAYBE -> {\n+                final long value = timestamps.longValue();\n+                yield value >= minTimestamp && value <= maxTimestamp;",
        "comment_created_at": "2025-05-21T06:51:10+00:00",
        "comment_author": "iverase",
        "comment_body": "I wonder if we should have two MAYBE options, one that we know we are always in the same primary sort and another one with mixed primary sort. In that case we can apply the optimization  in the first case (which should be the common case).",
        "pr_file_module": null
      },
      {
        "comment_id": "2099593498",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127260,
        "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
        "discussion_id": "2099475391",
        "commented_code": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+package org.elasticsearch.lucene.queries;\n+\n+import org.apache.lucene.index.DocValuesSkipper;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.search.TwoPhaseIterator;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n+ * <p>\n+ * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n+ * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n+ * <p>\n+ * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n+ * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n+ * the current value is lower than minTimestamp.\n+ */\n+final class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n+\n+    private final RangeNoGapsApproximation approximation;\n+    private final NumericDocValues timestamps;\n+\n+    private final long minTimestamp;\n+    private final long maxTimestamp;\n+\n+    TimestampTwoPhaseIterator(\n+        NumericDocValues timestamps,\n+        DocValuesSkipper timestampSkipper,\n+        DocValuesSkipper primaryFieldSkipper,\n+        long minTimestamp,\n+        long maxTimestamp\n+    ) {\n+        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n+        this.approximation = (RangeNoGapsApproximation) approximation();\n+        this.timestamps = timestamps;\n+        this.minTimestamp = minTimestamp;\n+        this.maxTimestamp = maxTimestamp;\n+    }\n+\n+    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n+\n+        private final DocIdSetIterator innerApproximation;\n+\n+        final DocValuesSkipper timestampSkipper;\n+        final DocValuesSkipper primaryFieldSkipper;\n+        final long minTimestamp;\n+        final long maxTimestamp;\n+\n+        private int doc = -1;\n+\n+        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n+        Match match = Match.MAYBE;\n+        int upTo = -1;\n+        int primaryFieldUpTo = -1;\n+\n+        RangeNoGapsApproximation(\n+            DocIdSetIterator innerApproximation,\n+            DocValuesSkipper timestampSkipper,\n+            DocValuesSkipper primaryFieldSkipper,\n+            long minTimestamp,\n+            long maxTimestamp\n+        ) {\n+            this.innerApproximation = innerApproximation;\n+            this.timestampSkipper = timestampSkipper;\n+            this.primaryFieldSkipper = primaryFieldSkipper;\n+            this.minTimestamp = minTimestamp;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return doc;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            return advance(docID() + 1);\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            while (true) {\n+                if (target > upTo) {\n+                    timestampSkipper.advance(target);\n+                    upTo = timestampSkipper.maxDocID(0);\n+                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n+                    }\n+                    match = match(0);\n+\n+                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n+                    // level (= on a wider range of doc IDs)\n+                    int nextLevel = 1;\n+                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n+                        upTo = timestampSkipper.maxDocID(nextLevel);\n+                        nextLevel++;\n+                    }\n+                }\n+                switch (match) {\n+                    case YES:\n+                        return doc = target;\n+                    case MAYBE:\n+                        if (target > innerApproximation.docID()) {\n+                            target = innerApproximation.advance(target);\n+                        }\n+                        if (target <= upTo) {\n+                            return doc = target;\n+                        }\n+                        break;\n+                    case NO_AND_SKIP:\n+                        if (target > primaryFieldUpTo) {\n+                            primaryFieldSkipper.advance(target);\n+                            for (int level = 0; level < primaryFieldSkipper.numLevels(); level++) {\n+                                if (primaryFieldSkipper.minValue(level) == primaryFieldSkipper.maxValue(level)) {\n+                                    primaryFieldUpTo = primaryFieldSkipper.maxDocID(level);\n+                                } else {\n+                                    break;\n+                                }\n+                            }\n+                            if (primaryFieldUpTo > upTo) {\n+                                upTo = primaryFieldUpTo;\n+                            }\n+                        }\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    case NO:\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    default:\n+                        throw new AssertionError(\"Unknown enum constant: \" + match);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public long cost() {\n+            return innerApproximation.cost();\n+        }\n+\n+        Match match(int level) {\n+            long minValue = timestampSkipper.minValue(level);\n+            long maxValue = timestampSkipper.maxValue(level);\n+            if (minValue > maxTimestamp) {\n+                return Match.NO;\n+            } else if (maxValue < minTimestamp) {\n+                return Match.NO_AND_SKIP;\n+            } else if (minValue >= minTimestamp && maxValue <= maxTimestamp) {\n+                return Match.YES;\n+            } else {\n+                return Match.MAYBE;\n+            }\n+        }\n+\n+    }\n+\n+    @Override\n+    public boolean matches() throws IOException {\n+        return switch (approximation.match) {\n+            case YES -> true;\n+            case MAYBE -> {\n+                final long value = timestamps.longValue();\n+                yield value >= minTimestamp && value <= maxTimestamp;",
        "comment_created_at": "2025-05-21T07:42:10+00:00",
        "comment_author": "martijnvg",
        "comment_body": "Interesting, let me take a look at this.",
        "pr_file_module": null
      },
      {
        "comment_id": "2126409027",
        "repo_full_name": "elastic/elasticsearch",
        "pr_number": 127260,
        "pr_file": "server/src/main/java/org/elasticsearch/lucene/queries/TimestampTwoPhaseIterator.java",
        "discussion_id": "2099475391",
        "commented_code": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+package org.elasticsearch.lucene.queries;\n+\n+import org.apache.lucene.index.DocValuesSkipper;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.search.TwoPhaseIterator;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Based on {@link org.apache.lucene.search.DocValuesRangeIterator} but modified for time series and logsdb use cases.\n+ * <p>\n+ * The @timestamp field always has exactly one value and all documents always have a @timestamp value.\n+ * Additionally, the @timestamp field is always the secondary index sort field and sort order is always descending.\n+ * <p>\n+ * This makes the doc value skipper on @timestamp field less effective, and so the doc value skipper for the first index sort field is\n+ * also used to skip to the next primary sort value if for the current skipper represents one value (min and max value are the same) and\n+ * the current value is lower than minTimestamp.\n+ */\n+final class TimestampTwoPhaseIterator extends TwoPhaseIterator {\n+\n+    private final RangeNoGapsApproximation approximation;\n+    private final NumericDocValues timestamps;\n+\n+    private final long minTimestamp;\n+    private final long maxTimestamp;\n+\n+    TimestampTwoPhaseIterator(\n+        NumericDocValues timestamps,\n+        DocValuesSkipper timestampSkipper,\n+        DocValuesSkipper primaryFieldSkipper,\n+        long minTimestamp,\n+        long maxTimestamp\n+    ) {\n+        super(new RangeNoGapsApproximation(timestamps, timestampSkipper, primaryFieldSkipper, minTimestamp, maxTimestamp));\n+        this.approximation = (RangeNoGapsApproximation) approximation();\n+        this.timestamps = timestamps;\n+        this.minTimestamp = minTimestamp;\n+        this.maxTimestamp = maxTimestamp;\n+    }\n+\n+    static final class RangeNoGapsApproximation extends DocIdSetIterator {\n+\n+        private final DocIdSetIterator innerApproximation;\n+\n+        final DocValuesSkipper timestampSkipper;\n+        final DocValuesSkipper primaryFieldSkipper;\n+        final long minTimestamp;\n+        final long maxTimestamp;\n+\n+        private int doc = -1;\n+\n+        // Track a decision for all doc IDs between the current doc ID and upTo inclusive.\n+        Match match = Match.MAYBE;\n+        int upTo = -1;\n+        int primaryFieldUpTo = -1;\n+\n+        RangeNoGapsApproximation(\n+            DocIdSetIterator innerApproximation,\n+            DocValuesSkipper timestampSkipper,\n+            DocValuesSkipper primaryFieldSkipper,\n+            long minTimestamp,\n+            long maxTimestamp\n+        ) {\n+            this.innerApproximation = innerApproximation;\n+            this.timestampSkipper = timestampSkipper;\n+            this.primaryFieldSkipper = primaryFieldSkipper;\n+            this.minTimestamp = minTimestamp;\n+            this.maxTimestamp = maxTimestamp;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return doc;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            return advance(docID() + 1);\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            while (true) {\n+                if (target > upTo) {\n+                    timestampSkipper.advance(target);\n+                    upTo = timestampSkipper.maxDocID(0);\n+                    if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                        return doc = DocIdSetIterator.NO_MORE_DOCS;\n+                    }\n+                    match = match(0);\n+\n+                    // If we have a YES or NO decision, see if we still have the same decision on a higher\n+                    // level (= on a wider range of doc IDs)\n+                    int nextLevel = 1;\n+                    while (match != Match.MAYBE && nextLevel < timestampSkipper.numLevels() && match == match(nextLevel)) {\n+                        upTo = timestampSkipper.maxDocID(nextLevel);\n+                        nextLevel++;\n+                    }\n+                }\n+                switch (match) {\n+                    case YES:\n+                        return doc = target;\n+                    case MAYBE:\n+                        if (target > innerApproximation.docID()) {\n+                            target = innerApproximation.advance(target);\n+                        }\n+                        if (target <= upTo) {\n+                            return doc = target;\n+                        }\n+                        break;\n+                    case NO_AND_SKIP:\n+                        if (target > primaryFieldUpTo) {\n+                            primaryFieldSkipper.advance(target);\n+                            for (int level = 0; level < primaryFieldSkipper.numLevels(); level++) {\n+                                if (primaryFieldSkipper.minValue(level) == primaryFieldSkipper.maxValue(level)) {\n+                                    primaryFieldUpTo = primaryFieldSkipper.maxDocID(level);\n+                                } else {\n+                                    break;\n+                                }\n+                            }\n+                            if (primaryFieldUpTo > upTo) {\n+                                upTo = primaryFieldUpTo;\n+                            }\n+                        }\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    case NO:\n+                        if (upTo == DocIdSetIterator.NO_MORE_DOCS) {\n+                            return doc = NO_MORE_DOCS;\n+                        }\n+                        target = upTo + 1;\n+                        break;\n+                    default:\n+                        throw new AssertionError(\"Unknown enum constant: \" + match);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public long cost() {\n+            return innerApproximation.cost();\n+        }\n+\n+        Match match(int level) {\n+            long minValue = timestampSkipper.minValue(level);\n+            long maxValue = timestampSkipper.maxValue(level);\n+            if (minValue > maxTimestamp) {\n+                return Match.NO;\n+            } else if (maxValue < minTimestamp) {\n+                return Match.NO_AND_SKIP;\n+            } else if (minValue >= minTimestamp && maxValue <= maxTimestamp) {\n+                return Match.YES;\n+            } else {\n+                return Match.MAYBE;\n+            }\n+        }\n+\n+    }\n+\n+    @Override\n+    public boolean matches() throws IOException {\n+        return switch (approximation.match) {\n+            case YES -> true;\n+            case MAYBE -> {\n+                final long value = timestamps.longValue();\n+                yield value >= minTimestamp && value <= maxTimestamp;",
        "comment_created_at": "2025-06-04T11:49:14+00:00",
        "comment_author": "martijnvg",
        "comment_body": "done: 57395650b39aa637db395b059c1740b8cdf82bda",
        "pr_file_module": null
      }
    ]
  }
]
