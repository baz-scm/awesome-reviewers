[
  {
    "discussion_id": "1173673076",
    "pr_number": 5139,
    "pr_file": "lib/response.js",
    "created_at": "2023-04-21T11:43:53+00:00",
    "commented_code": "this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      var response = this;\n+      chunk.arrayBuffer().then(function (result) {",
    "repo_full_name": "expressjs/express",
    "discussion_comments": [
      {
        "comment_id": "1173673076",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1173673076",
        "commented_code": "@@ -229,7 +233,16 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      var response = this;\n+      chunk.arrayBuffer().then(function (result) {",
        "comment_created_at": "2023-04-21T11:43:53+00:00",
        "comment_author": "dougwilson",
        "comment_body": "Should we add a code path here to use https://nodejs.org/api/buffer.html#blobstream when supported? Not sure how usable that is, but seems Node.js is adding the ability to make a Blob from a file which could be huge.",
        "pr_file_module": null
      },
      {
        "comment_id": "1174903577",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1173673076",
        "commented_code": "@@ -229,7 +233,16 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      var response = this;\n+      chunk.arrayBuffer().then(function (result) {",
        "comment_created_at": "2023-04-24T07:44:24+00:00",
        "comment_author": "debadutta98",
        "comment_body": "yes @dougwilson, you are right here for a high volume of data I used [`Writablestream`](https://nodejs.org/dist/latest-v20.x/docs/api/webstreams.html#class-writablestream) to achieve better control over this  \r\n```javascript \r\nvar WritableStream = require('stream/web').WritableStream;\r\n        blob.stream().pipeTo(new WritableStream({\r\n          write: function (chunk) {\r\n            _this.write(chunk);\r\n          },\r\n          close: function () {\r\n            _this.end();\r\n          }\r\n        }))\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1184570940",
    "pr_number": 5139,
    "pr_file": "lib/response.js",
    "created_at": "2023-05-04T05:57:56+00:00",
    "commented_code": "this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var _this = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      try {\n+        var WritableStream = require('stream/web').WritableStream;\n+        blob.stream().pipeTo(new WritableStream({\n+          write: function (chunk) {\n+            _this.write(chunk);",
    "repo_full_name": "expressjs/express",
    "discussion_comments": [
      {
        "comment_id": "1184570940",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1184570940",
        "commented_code": "@@ -229,7 +236,39 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var _this = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      try {\n+        var WritableStream = require('stream/web').WritableStream;\n+        blob.stream().pipeTo(new WritableStream({\n+          write: function (chunk) {\n+            _this.write(chunk);",
        "comment_created_at": "2023-05-04T05:57:56+00:00",
        "comment_author": "dougwilson",
        "comment_body": "I'm not sure why a custom write stream needs to be added, but it is missing backpressure handling, which is very important in http servers like this because the clients can stop reading if they know this is a large response, leading to a dos vector. But I would think you don't need to implement that at all, as you should be able to use `.toWeb()`/`.fromWeb()`, right? I ask because I'm not familiar with the new APIs for web streams (yet). So either we should use that or this custom stream needs to have backpressure handling added",
        "pr_file_module": null
      },
      {
        "comment_id": "1186721183",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1184570940",
        "commented_code": "@@ -229,7 +236,39 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var _this = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      try {\n+        var WritableStream = require('stream/web').WritableStream;\n+        blob.stream().pipeTo(new WritableStream({\n+          write: function (chunk) {\n+            _this.write(chunk);",
        "comment_created_at": "2023-05-06T16:56:30+00:00",
        "comment_author": "debadutta98",
        "comment_body": "Hi, @dougwilson, you are right here. I just modify the code in such a way that it can now handle backpressure.",
        "pr_file_module": null
      },
      {
        "comment_id": "1186721897",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1184570940",
        "commented_code": "@@ -229,7 +236,39 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var _this = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      try {\n+        var WritableStream = require('stream/web').WritableStream;\n+        blob.stream().pipeTo(new WritableStream({\n+          write: function (chunk) {\n+            _this.write(chunk);",
        "comment_created_at": "2023-05-06T17:03:03+00:00",
        "comment_author": "debadutta98",
        "comment_body": "`.fromWeb()` is not yet stable so I think [stream.Readable.from(iterable[, options])](https://nodejs.org/api/stream.html#streamreadablefromiterable-options) is better to be used here.",
        "pr_file_module": null
      },
      {
        "comment_id": "1209563402",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1184570940",
        "commented_code": "@@ -229,7 +236,39 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var _this = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      try {\n+        var WritableStream = require('stream/web').WritableStream;\n+        blob.stream().pipeTo(new WritableStream({\n+          write: function (chunk) {\n+            _this.write(chunk);",
        "comment_created_at": "2023-05-29T21:29:23+00:00",
        "comment_author": "jimmywarting",
        "comment_body": "If i would have came here earlier to review this PR then i would have suggested stream.Readable.from(iterable) as well.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1209544421",
    "pr_number": 5139,
    "pr_file": "lib/response.js",
    "created_at": "2023-05-29T20:27:40+00:00",
    "commented_code": "this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
    "repo_full_name": "expressjs/express",
    "discussion_comments": [
      {
        "comment_id": "1209544421",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-29T20:27:40+00:00",
        "comment_author": "jimmywarting",
        "comment_body": "I think this could just end up using:\r\n```js\r\n        blob.arrayBuffer()\r\n        .then(function(arrayBuffer){\r\n          nodeStream.end(new Uint8Array(arrayBuffer));\r\n```\r\nthere is really no need to wrap the buffer in a readable stream with extra overhead ?",
        "pr_file_module": null
      },
      {
        "comment_id": "1209712421",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T05:10:14+00:00",
        "comment_author": "debadutta98",
        "comment_body": "Hi @jimmywarting, what if the blob size is so large? I think it will unnecessarily create backpressure on the writable stream. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1210277650",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T13:30:57+00:00",
        "comment_author": "jimmywarting",
        "comment_body": "At this point in time when you have already done `blob.arrayBuffer()` then it won't matter so much cuz you have already allocated the hole blob's content into the memory.\r\nWhat you are doing is just enqueue'ing the buffer into a readable stream that will only read two chunks\r\n\r\n1. The full buffer\r\n2. null\r\n\r\nThe readable stream will not magically split up the hole Buffer from having multiple smaller chunks and pipe small amount of data at a time and have some form of backpressure\r\n\r\nthe hole point of having stream is to be able to pipe data from a external location, like\r\n- From the disk\r\n- From a network request\r\n- or to generate little data at a time (like the form of a fibonacci generator function that never ends)\r\n\r\nif you at some point already have the hole data in memory. then there is little to no reason to even create a stream at all.\r\n\r\n---\r\n\r\nif you do wish to better magically support backpressure from a blob that don't have `.stream()` \r\nthen i think the correct approach would be to slice up the blob into smaller chunks and pull the arraybuffer from each of every chunk one at a time. \r\n\r\n```js\r\nfor (let start = 0; start < blob.size; start += chunkSize) {\r\n  const chunk = file.slice(start, start + chunkSize + 1)\r\n  const ab = await chunk.arrayBuffer()\r\n  const uint8 = new Uint8Array(ab)\r\n  yield uint8\r\n}\r\n\r\n// readable.from(the_above_iterable_generator)\r\n```\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1210278322",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T13:31:25+00:00",
        "comment_author": "jimmywarting",
        "comment_body": "And beside nobody is gona create a in memory Blob that have that much data allocated into memory. cuz blob's have mostly been pretty useless upon till v20 when it for the first time was able to create blob's backed up by the filesystem\r\n...Thanks to `fs.openAsBlob(path, { type })` in NodeJS v20\r\nbefore then Blob's have been a pretty useless thing to have. One could instead have used ArrayBuffer or any typed array instead. cuz they would not bring any other usefulness in to the ecosystem otherwise. (but i'm lying a tiny bit. there is still a few small tiny reason but not something that is so significant)\r\n\r\nI honestly think the hole `res.send(blob)` feature is only going to be used by folks if they are able to get a blob/file that are backed up by the filesystem and if they are able to generate a stream out of it. \r\n\r\n---\r\n\r\nThen you would have something that is more \"backpressure friendly\"\r\nbut honestly i would probably not try to bother with supporting blob chunks that do not have `Blob.prototype.stream()`\r\n\r\ni would instead have made a blob detection that checks wether or not it's has `symbol.toString` and a stream function. \r\n\r\n```js\r\n  function isSpecdBlob() {\r\n    return (\r\n      typeof object === 'object' &&\r\n      object && // null check\r\n      typeof object.stream === 'function' && \r\n      /^(Blob|File)$/.test(object[Symbol.toStringTag])\r\n    )\r\n  }\r\n```\r\nif it quacks like a duck and walk like a duck then it can float / swim\r\nblob's without stream will just sink. cuz they are no useful to have. then you might as well use `ArrayBuffer` or `TypedArray`. cuz there is no way to construct a blob if it can't be backed up by the filesystem if they must live in the memory. It would be better to just recommend ppl to only use this `res.send(blob)` feature only if they are using NodeJS v20+ or if they are instead using `fetch-blob`",
        "pr_file_module": null
      },
      {
        "comment_id": "1210491955",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T15:54:35+00:00",
        "comment_author": "debadutta98",
        "comment_body": "> At this point in time when you have already done `blob.arrayBuffer()` then it won't matter so much cuz you have already allocated the hole blob's content into the memory. What you are doing is just enqueue'ing the buffer into a readable stream that will only read two chunks\r\n> \r\n> 1. The full buffer\r\n> 2. null\r\n> \r\n> The readable stream will not magically split up the hole Buffer from having multiple smaller chunks and pipe small amount of data at a time and have some form of backpressure\r\n> \r\n> the hole point of having stream is to be able to pipe data from a external location, like\r\n> \r\n> * From the disk\r\n> * From a network request\r\n> * or to generate little data at a time (like the form of a fibonacci generator function that never ends)\r\n> \r\n> if you at some point already have the hole data in memory. then there is little to no reason to even create a stream at all.\r\n> \r\n> if you do wish to better magically support backpressure from a blob that don't have `.stream()` then i think the correct approach would be to slice up the blob into smaller chunks and pull the arraybuffer from each of every chunk one at a time.\r\n> \r\n> ```js\r\n> for (let start = 0; start < blob.size; start += chunkSize) {\r\n>   const chunk = file.slice(start, start + chunkSize + 1)\r\n>   const ab = await chunk.arrayBuffer()\r\n>   const uint8 = new Uint8Array(ab)\r\n>   yield uint8\r\n> }\r\n> \r\n> // readable.from(the_above_iterable_generator)\r\n> ```\r\n\r\n@jimmywarting As you know that express is written in such a way that it can be compatible with each node version i.e. `node >= 0.10.0`  so I think we can't include the es6 script as it is not supported for some of the versions and Generator functions are coming under es6 script.",
        "pr_file_module": null
      },
      {
        "comment_id": "1210521607",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T16:18:00+00:00",
        "comment_author": "dougwilson",
        "comment_body": "Just to note, if it doesn't work or make sense to land in the 4.x line due to back compat, that's no problem, as we have the 5.x line thay doesn't have that concern and as soon as I finish getting out a sec path for a diff module, the last 5.x pre release will be published so we can publish the final.",
        "pr_file_module": null
      },
      {
        "comment_id": "1210547546",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T16:41:16+00:00",
        "comment_author": "jimmywarting",
        "comment_body": "that solution was just to give you a rough estimate on what you actually have to do logic wise - reading part of a blob and return something iterable that keeps reading little by little.  not how to actually write a complete example in old school code. i left out a hole lot in that code example like the `function` signature, where it gets the blob from how it uses `stream.Readable.from()`\r\n\r\nit's 100% compatible to write a old-style iterable function that simply just returns a object and having a single Symbol.iterator symbol.\r\n\r\nhere is a mdn example of creating a iterable. just had to make a sligh adjustment to say that it should return a object with a symbol.iterator instead\r\n```js\r\nfunction makeRangeIterator(start = 0, end = Infinity, step = 1) {\r\n  let nextIndex = start;\r\n  let iterationCount = 0;\r\n\r\n  const rangeIterator = {\r\n    next() {\r\n      let result;\r\n      if (nextIndex < end) {\r\n        result = { value: nextIndex, done: false };\r\n        nextIndex += step;\r\n        iterationCount++;\r\n        return result;\r\n      }\r\n      return { value: iterationCount, done: true };\r\n    },\r\n  }\r\n  const obj = {}\r\n  obj[Symbol.iterator] = function() {\r\n    return rangeIterator\r\n  }\r\n  return obj;\r\n}\r\n\r\n[...makeRangeIterator(0, 10)] // 0, 1, 2 ... 10\r\n```\r\nAs you can see this dose not use any generator or `yield` stuff. it's enough that you have a object that has a `symbol.toIterator` that returns a simple object that has `{ next() { ... }}` which in upon calling should return a `{ value, done }` object for it to be acceptable by `stream.Readable.from(iterable)` the only thing left is change things to `var` instead.\r\n\r\ni just didn't feel like writing a hole such example of reading blobs little by little using a own custom iterable function would be necessary, cuz i don't think it should be included anyway. but if you like me too do it then i could. \r\n\r\nanyway. i'm going to mark this message as of topic. as v5.x don't seem to have the same restriction anymore",
        "pr_file_module": null
      },
      {
        "comment_id": "1210564667",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T16:56:58+00:00",
        "comment_author": "jimmywarting",
        "comment_body": "now you actually made me want to show of using older syntax just for the sake of it ðŸ˜œ \r\n\r\n```js\r\n/**\r\n * Read a blob little by little using .arrayBuffer() and slice()\r\n * @param {Blob} blob \r\n * @param {number} chunkSize\r\n * @returns and iterable object\r\n */\r\nfunction streamBlob(blob, chunkSize = 1024) {\r\n  var offset = 0\r\n\r\n  var iterator = {\r\n    next() {\r\n      return new Promise(function (resolve, reject) {\r\n        var chunk = blob.slice(offset, offset + chunkSize)\r\n        \r\n        return chunk.text().then(function text) {\r\n          resolve({\r\n            value: text,\r\n            done: offset >= blob.size\r\n          })\r\n\r\n          offset += chunkSize\r\n        })\r\n      })\r\n    }\r\n  }\r\n  \r\n  var obj = {}\r\n  obj[Symbol.asyncIterator] = function () {\r\n    return iterator\r\n  }\r\n\r\n  return obj\r\n}\r\n\r\n// To show that it's now iterable:\r\nconst blob = new Blob(['hello world'])\r\nconst iterable = streamBlob(blob, 2)\r\n\r\nfor await (const chunk of iterable) {\r\n  console.log(chunk)\r\n}\r\n```\r\nmight just still have to get rid of the optional chunkSize argument and change it to resolve with a Uint8Array instead of a string (text) just wanted to use it cuz it's more for demo purpose",
        "pr_file_module": null
      },
      {
        "comment_id": "1210629382",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T17:56:21+00:00",
        "comment_author": "debadutta98",
        "comment_body": "Thanks @dougwilson for your suggestions.",
        "pr_file_module": null
      },
      {
        "comment_id": "1210637459",
        "repo_full_name": "expressjs/express",
        "pr_number": 5139,
        "pr_file": "lib/response.js",
        "discussion_id": "1209544421",
        "commented_code": "@@ -229,7 +236,34 @@ res.send = function send(body) {\n     this.end();\n   } else {\n     // respond\n-    this.end(chunk, encoding);\n+    if (isChunkBlob) {\n+      /**\n+       * @type {http.ServerResponse}\n+       */\n+      var res = this;\n+      /**\n+       * @type {Blob}\n+       */\n+      var blob = chunk;\n+      var sendBlob = function (iterable) {\n+        var Readable = require('stream').Readable;\n+        var nodeStream = Readable.from(iterable);\n+        nodeStream.on('error', req.next)\n+        res.on('error', req.next)\n+        nodeStream.pipe(res);\n+      }\n+      if(typeof blob.stream === 'function') {\n+        sendBlob(blob.stream());\n+      } else {\n+        blob.arrayBuffer()\n+        .then(function(arrayBuffer){\n+          sendBlob(Buffer.from(arrayBuffer));",
        "comment_created_at": "2023-05-30T18:02:46+00:00",
        "comment_author": "debadutta98",
        "comment_body": "@dougwilson i think there is no issues with 4.x as of now but we can write the code more optimally in 5.x if es6 supports there.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "854262020",
    "pr_number": 4893,
    "pr_file": "lib/response.js",
    "created_at": "2022-04-20T15:16:01+00:00",
    "commented_code": "// Respond\n   this.statusCode = status;\n-  this.set('Content-Length', Buffer.byteLength(body));\n+\n+  // Because Content-Length and Transfer-Encoding can't be present in the response headers together,\n+  // Content-Length should be added only if there is no Transfer-Encoding header\n+  if (!this.getHeader(\"transfer-encoding\")) {",
    "repo_full_name": "expressjs/express",
    "discussion_comments": [
      {
        "comment_id": "854262020",
        "repo_full_name": "expressjs/express",
        "pr_number": 4893,
        "pr_file": "lib/response.js",
        "discussion_id": "854262020",
        "commented_code": "@@ -948,7 +950,12 @@ res.redirect = function redirect(url) {\n \n   // Respond\n   this.statusCode = status;\n-  this.set('Content-Length', Buffer.byteLength(body));\n+\n+  // Because Content-Length and Transfer-Encoding can't be present in the response headers together,\n+  // Content-Length should be added only if there is no Transfer-Encoding header\n+  if (!this.getHeader(\"transfer-encoding\")) {",
        "comment_created_at": "2022-04-20T15:16:01+00:00",
        "comment_author": "dougwilson",
        "comment_body": "`this.getHeader(\"transfer-encoding\")` should be `this.get(\"Transfer-Encoding\")`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "16327369",
    "pr_number": 2300,
    "pr_file": "lib/response.js",
    "created_at": "2014-08-17T00:04:55+00:00",
    "commented_code": "throw new TypeError('path must be absolute or specify root to res.sendFile');\n   }\n \n-  // socket errors\n-  req.socket.on('error', onerror);\n+  // @todo find a good way to test if request was aborted\n+  if (!req.socket.writable) onabort();\n+  req.on('aborted', onabort);",
    "repo_full_name": "expressjs/express",
    "discussion_comments": [
      {
        "comment_id": "16327369",
        "repo_full_name": "expressjs/express",
        "pr_number": 2300,
        "pr_file": "lib/response.js",
        "discussion_id": "16327369",
        "commented_code": "@@ -364,17 +364,20 @@ res.sendFile = function sendFile(path, options, fn) {\n     throw new TypeError('path must be absolute or specify root to res.sendFile');\n   }\n \n-  // socket errors\n-  req.socket.on('error', onerror);\n+  // @todo find a good way to test if request was aborted\n+  if (!req.socket.writable) onabort();\n+  req.on('aborted', onabort);",
        "comment_created_at": "2014-08-17T00:04:55+00:00",
        "comment_author": "dougwilson",
        "comment_body": "remove your aborted listener when done\n",
        "pr_file_module": null
      }
    ]
  }
]