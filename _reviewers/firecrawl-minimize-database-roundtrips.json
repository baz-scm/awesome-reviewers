[
  {
    "discussion_id": "2207484551",
    "pr_number": 1799,
    "pr_file": "apps/api/src/controllers/v1/crawl-status.ts",
    "created_at": "2025-07-15T13:49:52+00:00",
    "commented_code": "),\n     );\n \n+    if (jobStatuses.filter((x) => x[1] === \"unknown\").length > 0 && process.env.USE_DB_AUTHENTICATION === \"true\") {\n+      for (let rangeStart = 0;; rangeStart += 1000) {",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "2207542184",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 1799,
        "pr_file": "apps/api/src/controllers/v1/crawl-status.ts",
        "discussion_id": "2207484551",
        "commented_code": "@@ -162,6 +162,42 @@ export async function crawlStatusController(\n       ),\n     );\n \n+    if (jobStatuses.filter((x) => x[1] === \"unknown\").length > 0 && process.env.USE_DB_AUTHENTICATION === \"true\") {\n+      for (let rangeStart = 0;; rangeStart += 1000) {",
        "comment_created_at": "2025-07-15T13:49:52+00:00",
        "comment_author": "mogery",
        "comment_body": "@cubic-dev-ai I know, but it's fine in this case. We want to keep going while there's data coming from the DB. And we can't get it all in one chunk due to Supabase limits.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1905185456",
    "pr_number": 1044,
    "pr_file": "apps/api/src/controllers/v1/extract-status.ts",
    "created_at": "2025-01-07T09:48:22+00:00",
    "commented_code": "+import { Response } from \"express\";\n+import {\n+  supabaseGetJobByIdOnlyData,\n+  supabaseGetJobsById,\n+} from \"../../lib/supabase-jobs\";\n+import { scrapeStatusRateLimiter } from \"../../services/rate-limiter\";\n+import { RequestWithAuth } from \"./types\";\n+\n+export async function extractStatusController(\n+  req: RequestWithAuth<{ jobId: string }, any, any>,\n+  res: Response,\n+) {\n+  try {\n+    const rateLimiter = scrapeStatusRateLimiter;\n+    const incomingIP = (req.headers[\"x-forwarded-for\"] ||\n+      req.socket.remoteAddress) as string;\n+    const iptoken = incomingIP;\n+    await rateLimiter.consume(iptoken);\n+\n+    const job = await supabaseGetJobByIdOnlyData(req.params.jobId);\n+    if (!job || job.team_id !== req.auth.team_id) {\n+      return res.status(403).json({\n+        success: false,\n+        error: \"You are not allowed to access this resource.\",\n+      });\n+    }\n+\n+    const jobData = await supabaseGetJobsById([req.params.jobId]);\n+    if (!jobData || jobData.length === 0) {\n+      return res.status(404).json({\n+        success: false,\n+        error: \"Job not found\",\n+      });\n+    }",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "1905185456",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 1044,
        "pr_file": "apps/api/src/controllers/v1/extract-status.ts",
        "discussion_id": "1905185456",
        "commented_code": "@@ -0,0 +1,53 @@\n+import { Response } from \"express\";\n+import {\n+  supabaseGetJobByIdOnlyData,\n+  supabaseGetJobsById,\n+} from \"../../lib/supabase-jobs\";\n+import { scrapeStatusRateLimiter } from \"../../services/rate-limiter\";\n+import { RequestWithAuth } from \"./types\";\n+\n+export async function extractStatusController(\n+  req: RequestWithAuth<{ jobId: string }, any, any>,\n+  res: Response,\n+) {\n+  try {\n+    const rateLimiter = scrapeStatusRateLimiter;\n+    const incomingIP = (req.headers[\"x-forwarded-for\"] ||\n+      req.socket.remoteAddress) as string;\n+    const iptoken = incomingIP;\n+    await rateLimiter.consume(iptoken);\n+\n+    const job = await supabaseGetJobByIdOnlyData(req.params.jobId);\n+    if (!job || job.team_id !== req.auth.team_id) {\n+      return res.status(403).json({\n+        success: false,\n+        error: \"You are not allowed to access this resource.\",\n+      });\n+    }\n+\n+    const jobData = await supabaseGetJobsById([req.params.jobId]);\n+    if (!jobData || jobData.length === 0) {\n+      return res.status(404).json({\n+        success: false,\n+        error: \"Job not found\",\n+      });\n+    }",
        "comment_created_at": "2025-01-07T09:48:22+00:00",
        "comment_author": "mogery",
        "comment_body": "Why request the DB twice?",
        "pr_file_module": null
      }
    ]
  }
]