[
  {
    "discussion_id": "2136528038",
    "pr_number": 1653,
    "pr_file": "apps/api/src/controllers/v1/types.ts",
    "created_at": "2025-06-09T21:18:48+00:00",
    "commented_code": ".refine(\n         (val) => {\n           if (!val) return true; // Allow undefined schema\n+          \n+          // Check if this is an unconverted Zod schema\n+          if (val && typeof val === 'object' && val._def) {\n+            console.error('Schema validation error: Detected unconverted Zod schema with _def property:', {\n+              type: val._def?.typeName,\n+              keys: Object.keys(val),\n+              hasZodProperties: !!(val._def || val.parse || val.safeParse)\n+            });\n+            return false;\n+          }\n+          \n           try {\n             const validate = ajv.compile(val);\n             return typeof validate === \"function\";\n           } catch (e) {\n+            console.error('AJV schema compilation failed:', {\n+              error: e.message,\n+              schema: val,\n+              schemaKeys: val && typeof val === 'object' ? Object.keys(val) : 'not_object'\n+            });",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "2136528038",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 1653,
        "pr_file": "apps/api/src/controllers/v1/types.ts",
        "discussion_id": "2136528038",
        "commented_code": "@@ -467,15 +471,31 @@ export const extractV1Options = z\n       .refine(\n         (val) => {\n           if (!val) return true; // Allow undefined schema\n+          \n+          // Check if this is an unconverted Zod schema\n+          if (val && typeof val === 'object' && val._def) {\n+            console.error('Schema validation error: Detected unconverted Zod schema with _def property:', {\n+              type: val._def?.typeName,\n+              keys: Object.keys(val),\n+              hasZodProperties: !!(val._def || val.parse || val.safeParse)\n+            });\n+            return false;\n+          }\n+          \n           try {\n             const validate = ajv.compile(val);\n             return typeof validate === \"function\";\n           } catch (e) {\n+            console.error('AJV schema compilation failed:', {\n+              error: e.message,\n+              schema: val,\n+              schemaKeys: val && typeof val === 'object' ? Object.keys(val) : 'not_object'\n+            });",
        "comment_created_at": "2025-06-09T21:18:48+00:00",
        "comment_author": "mogery",
        "comment_body": "`console.error` instead of `logger.error` is not allowed, logging should not be done here as it will lead to log flooding on production.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2136528185",
    "pr_number": 1653,
    "pr_file": "apps/api/src/controllers/v1/types.ts",
    "created_at": "2025-06-09T21:18:54+00:00",
    "commented_code": ".refine(\n         (val) => {\n           if (!val) return true; // Allow undefined schema\n+          \n+          // Check if this is an unconverted Zod schema\n+          if (val && typeof val === 'object' && val._def) {\n+            console.error('Schema validation error: Detected unconverted Zod schema with _def property:', {\n+              type: val._def?.typeName,\n+              keys: Object.keys(val),\n+              hasZodProperties: !!(val._def || val.parse || val.safeParse)\n+            });",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "2136528185",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 1653,
        "pr_file": "apps/api/src/controllers/v1/types.ts",
        "discussion_id": "2136528185",
        "commented_code": "@@ -467,15 +471,31 @@ export const extractV1Options = z\n       .refine(\n         (val) => {\n           if (!val) return true; // Allow undefined schema\n+          \n+          // Check if this is an unconverted Zod schema\n+          if (val && typeof val === 'object' && val._def) {\n+            console.error('Schema validation error: Detected unconverted Zod schema with _def property:', {\n+              type: val._def?.typeName,\n+              keys: Object.keys(val),\n+              hasZodProperties: !!(val._def || val.parse || val.safeParse)\n+            });",
        "comment_created_at": "2025-06-09T21:18:54+00:00",
        "comment_author": "mogery",
        "comment_body": "`console.error` instead of `logger.error` is not allowed, logging should not be done here as it will lead to log flooding on production.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1961946185",
    "pr_number": 1201,
    "pr_file": "apps/api/src/lib/generate-llmstxt/generate-llmstxt-service.ts",
    "created_at": "2025-02-19T15:54:02+00:00",
    "commented_code": "+import { logger as _logger } from \"../logger\";\n+import { updateGeneratedLlmsTxt } from \"./generate-llmstxt-redis\";\n+import { mapController } from \"../../controllers/v1/map\";\n+import { scrapeController } from \"../../controllers/v1/scrape\";\n+import { MapResponse, ScrapeResponse, Document } from \"../../controllers/v1/types\";\n+import { Response } from \"express\";\n+import OpenAI from \"openai\";\n+import { zodResponseFormat } from \"openai/helpers/zod\";\n+import { z } from \"zod\";\n+\n+interface GenerateLLMsTextServiceOptions {\n+  generationId: string;\n+  teamId: string;\n+  plan: string;\n+  url: string;\n+  maxUrls: number;\n+  showFullText: boolean;\n+}\n+\n+function createExpressResponse<T>(): Response {\n+  const res = {} as Response;\n+  res.status = () => res;\n+  res.json = (data: T) => data as any;\n+  res.send = () => res;\n+  res.sendStatus = () => res;\n+  res.links = () => res;\n+  res.jsonp = () => res;\n+  res.sendFile = () => res;\n+  return res;\n+}\n+\n+const DescriptionSchema = z.object({\n+  description: z.string(),\n+  title: z.string(),\n+});\n+\n+export async function performGenerateLlmsTxt(options: GenerateLLMsTextServiceOptions) {\n+  const openai = new OpenAI();\n+  const { generationId, teamId, plan, url, maxUrls, showFullText } = options;\n+  \n+  const logger = _logger.child({\n+    module: \"generate-llmstxt\",\n+    method: \"performGenerateLlmsTxt\",\n+    generationId,\n+  });\n+\n+  try {\n+    // First, get all URLs from the map controller\n+    const mockMapReq = {\n+      auth: { team_id: teamId, plan },\n+      body: { url, limit: maxUrls, includeSubdomains: false },\n+    };\n+\n+   \n+    const mapResult = await mapController(mockMapReq as any, createExpressResponse<MapResponse>());\n+    const mapResponse = mapResult as unknown as MapResponse;\n+\n+    if (!mapResponse.success) {\n+      throw new Error(`Failed to map URLs: ${mapResponse.error}`);\n+    }\n+\n+    _logger.debug(\"Mapping URLs\", mapResponse.links);\n+\n+    const urls = mapResponse.links;\n+    let llmstxt = `# ${url} llms.txt\n\n`;\n+    let llmsFulltxt = `# ${url} llms-full.txt\n\n`;\n+\n+\n+    // Scrape each URL\n+    for (const url of urls) {\n+      const mockScrapeReq = {\n+        auth: { team_id: teamId, plan },\n+        body: {\n+          url,\n+          formats: [\"markdown\"],\n+          onlyMainContent: true,\n+        },\n+      };\n+\n+      _logger.debug(`Scraping URL: ${url}`);\n+      const scrapeResult = await scrapeController(mockScrapeReq as any, createExpressResponse<ScrapeResponse>());\n+      const scrapeResponse = scrapeResult as unknown as ScrapeResponse;\n+\n+      if (!scrapeResponse.success) {\n+        logger.error(`Failed to scrape URL ${url}: ${scrapeResponse.error}`);\n+        continue;\n+      }\n+\n+      // Process scraped result\n+      const document = Array.isArray(scrapeResponse.data) ? scrapeResponse.data[0] : scrapeResponse.data;\n+      \n+      if (!document.markdown) continue;\n+\n+      _logger.debug(`Generating description for ${document.metadata?.url}`);\n+      \n+      const completion = await openai.beta.chat.completions.parse({\n+        model: \"gpt-4o-mini\",\n+        messages: [\n+          {\n+            role: \"user\", \n+            content: `Generate a 9-10 word description and a 3-4 word title of the entire page based on ALL the content one will find on the page for this url: ${document.metadata?.url}. This will help in a user finding the page for its intended purpose. Here is the content: ${document.markdown}`\n+          }\n+        ],\n+        response_format: zodResponseFormat(DescriptionSchema, \"description\")\n+      });\n+\n+      try {\n+        const parsedResponse = completion.choices[0].message.parsed;\n+        const description = parsedResponse!.description;\n+        const title = parsedResponse!.title;\n+        \n+        llmstxt += `- [${title}](${document.metadata?.url}): ${description}\n`;\n+        llmsFulltxt += `## ${title}\n${document.markdown}\n\n`;\n+\n+        // Update progress with both generated text and full text\n+        await updateGeneratedLlmsTxt(generationId, {\n+          status: \"processing\", \n+          generatedText: llmstxt,\n+          fullText: llmsFulltxt,\n+        });\n+      } catch (error) {\n+        logger.error(`Failed to parse LLM response for ${document.metadata?.url}:`, error);",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "1961946185",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 1201,
        "pr_file": "apps/api/src/lib/generate-llmstxt/generate-llmstxt-service.ts",
        "discussion_id": "1961946185",
        "commented_code": "@@ -0,0 +1,154 @@\n+import { logger as _logger } from \"../logger\";\n+import { updateGeneratedLlmsTxt } from \"./generate-llmstxt-redis\";\n+import { mapController } from \"../../controllers/v1/map\";\n+import { scrapeController } from \"../../controllers/v1/scrape\";\n+import { MapResponse, ScrapeResponse, Document } from \"../../controllers/v1/types\";\n+import { Response } from \"express\";\n+import OpenAI from \"openai\";\n+import { zodResponseFormat } from \"openai/helpers/zod\";\n+import { z } from \"zod\";\n+\n+interface GenerateLLMsTextServiceOptions {\n+  generationId: string;\n+  teamId: string;\n+  plan: string;\n+  url: string;\n+  maxUrls: number;\n+  showFullText: boolean;\n+}\n+\n+function createExpressResponse<T>(): Response {\n+  const res = {} as Response;\n+  res.status = () => res;\n+  res.json = (data: T) => data as any;\n+  res.send = () => res;\n+  res.sendStatus = () => res;\n+  res.links = () => res;\n+  res.jsonp = () => res;\n+  res.sendFile = () => res;\n+  return res;\n+}\n+\n+const DescriptionSchema = z.object({\n+  description: z.string(),\n+  title: z.string(),\n+});\n+\n+export async function performGenerateLlmsTxt(options: GenerateLLMsTextServiceOptions) {\n+  const openai = new OpenAI();\n+  const { generationId, teamId, plan, url, maxUrls, showFullText } = options;\n+  \n+  const logger = _logger.child({\n+    module: \"generate-llmstxt\",\n+    method: \"performGenerateLlmsTxt\",\n+    generationId,\n+  });\n+\n+  try {\n+    // First, get all URLs from the map controller\n+    const mockMapReq = {\n+      auth: { team_id: teamId, plan },\n+      body: { url, limit: maxUrls, includeSubdomains: false },\n+    };\n+\n+   \n+    const mapResult = await mapController(mockMapReq as any, createExpressResponse<MapResponse>());\n+    const mapResponse = mapResult as unknown as MapResponse;\n+\n+    if (!mapResponse.success) {\n+      throw new Error(`Failed to map URLs: ${mapResponse.error}`);\n+    }\n+\n+    _logger.debug(\"Mapping URLs\", mapResponse.links);\n+\n+    const urls = mapResponse.links;\n+    let llmstxt = `# ${url} llms.txt\\n\\n`;\n+    let llmsFulltxt = `# ${url} llms-full.txt\\n\\n`;\n+\n+\n+    // Scrape each URL\n+    for (const url of urls) {\n+      const mockScrapeReq = {\n+        auth: { team_id: teamId, plan },\n+        body: {\n+          url,\n+          formats: [\"markdown\"],\n+          onlyMainContent: true,\n+        },\n+      };\n+\n+      _logger.debug(`Scraping URL: ${url}`);\n+      const scrapeResult = await scrapeController(mockScrapeReq as any, createExpressResponse<ScrapeResponse>());\n+      const scrapeResponse = scrapeResult as unknown as ScrapeResponse;\n+\n+      if (!scrapeResponse.success) {\n+        logger.error(`Failed to scrape URL ${url}: ${scrapeResponse.error}`);\n+        continue;\n+      }\n+\n+      // Process scraped result\n+      const document = Array.isArray(scrapeResponse.data) ? scrapeResponse.data[0] : scrapeResponse.data;\n+      \n+      if (!document.markdown) continue;\n+\n+      _logger.debug(`Generating description for ${document.metadata?.url}`);\n+      \n+      const completion = await openai.beta.chat.completions.parse({\n+        model: \"gpt-4o-mini\",\n+        messages: [\n+          {\n+            role: \"user\", \n+            content: `Generate a 9-10 word description and a 3-4 word title of the entire page based on ALL the content one will find on the page for this url: ${document.metadata?.url}. This will help in a user finding the page for its intended purpose. Here is the content: ${document.markdown}`\n+          }\n+        ],\n+        response_format: zodResponseFormat(DescriptionSchema, \"description\")\n+      });\n+\n+      try {\n+        const parsedResponse = completion.choices[0].message.parsed;\n+        const description = parsedResponse!.description;\n+        const title = parsedResponse!.title;\n+        \n+        llmstxt += `- [${title}](${document.metadata?.url}): ${description}\\n`;\n+        llmsFulltxt += `## ${title}\\n${document.markdown}\\n\\n`;\n+\n+        // Update progress with both generated text and full text\n+        await updateGeneratedLlmsTxt(generationId, {\n+          status: \"processing\", \n+          generatedText: llmstxt,\n+          fullText: llmsFulltxt,\n+        });\n+      } catch (error) {\n+        logger.error(`Failed to parse LLM response for ${document.metadata?.url}:`, error);",
        "comment_created_at": "2025-02-19T15:54:02+00:00",
        "comment_author": "mogery",
        "comment_body": "```suggestion\r\n        logger.error(`Failed to parse LLM response for ${document.metadata?.url}`, { error });\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1961946633",
    "pr_number": 1201,
    "pr_file": "apps/api/src/lib/generate-llmstxt/generate-llmstxt-service.ts",
    "created_at": "2025-02-19T15:54:17+00:00",
    "commented_code": "+import { logger as _logger } from \"../logger\";\n+import { updateGeneratedLlmsTxt } from \"./generate-llmstxt-redis\";\n+import { mapController } from \"../../controllers/v1/map\";\n+import { scrapeController } from \"../../controllers/v1/scrape\";\n+import { MapResponse, ScrapeResponse, Document } from \"../../controllers/v1/types\";\n+import { Response } from \"express\";\n+import OpenAI from \"openai\";\n+import { zodResponseFormat } from \"openai/helpers/zod\";\n+import { z } from \"zod\";\n+\n+interface GenerateLLMsTextServiceOptions {\n+  generationId: string;\n+  teamId: string;\n+  plan: string;\n+  url: string;\n+  maxUrls: number;\n+  showFullText: boolean;\n+}\n+\n+function createExpressResponse<T>(): Response {\n+  const res = {} as Response;\n+  res.status = () => res;\n+  res.json = (data: T) => data as any;\n+  res.send = () => res;\n+  res.sendStatus = () => res;\n+  res.links = () => res;\n+  res.jsonp = () => res;\n+  res.sendFile = () => res;\n+  return res;\n+}\n+\n+const DescriptionSchema = z.object({\n+  description: z.string(),\n+  title: z.string(),\n+});\n+\n+export async function performGenerateLlmsTxt(options: GenerateLLMsTextServiceOptions) {\n+  const openai = new OpenAI();\n+  const { generationId, teamId, plan, url, maxUrls, showFullText } = options;\n+  \n+  const logger = _logger.child({\n+    module: \"generate-llmstxt\",\n+    method: \"performGenerateLlmsTxt\",\n+    generationId,\n+  });\n+\n+  try {\n+    // First, get all URLs from the map controller\n+    const mockMapReq = {\n+      auth: { team_id: teamId, plan },\n+      body: { url, limit: maxUrls, includeSubdomains: false },\n+    };\n+\n+   \n+    const mapResult = await mapController(mockMapReq as any, createExpressResponse<MapResponse>());\n+    const mapResponse = mapResult as unknown as MapResponse;\n+\n+    if (!mapResponse.success) {\n+      throw new Error(`Failed to map URLs: ${mapResponse.error}`);\n+    }\n+\n+    _logger.debug(\"Mapping URLs\", mapResponse.links);\n+\n+    const urls = mapResponse.links;\n+    let llmstxt = `# ${url} llms.txt\n\n`;\n+    let llmsFulltxt = `# ${url} llms-full.txt\n\n`;\n+\n+\n+    // Scrape each URL\n+    for (const url of urls) {\n+      const mockScrapeReq = {\n+        auth: { team_id: teamId, plan },\n+        body: {\n+          url,\n+          formats: [\"markdown\"],\n+          onlyMainContent: true,\n+        },\n+      };\n+\n+      _logger.debug(`Scraping URL: ${url}`);\n+      const scrapeResult = await scrapeController(mockScrapeReq as any, createExpressResponse<ScrapeResponse>());\n+      const scrapeResponse = scrapeResult as unknown as ScrapeResponse;\n+\n+      if (!scrapeResponse.success) {\n+        logger.error(`Failed to scrape URL ${url}: ${scrapeResponse.error}`);",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "1961946633",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 1201,
        "pr_file": "apps/api/src/lib/generate-llmstxt/generate-llmstxt-service.ts",
        "discussion_id": "1961946633",
        "commented_code": "@@ -0,0 +1,154 @@\n+import { logger as _logger } from \"../logger\";\n+import { updateGeneratedLlmsTxt } from \"./generate-llmstxt-redis\";\n+import { mapController } from \"../../controllers/v1/map\";\n+import { scrapeController } from \"../../controllers/v1/scrape\";\n+import { MapResponse, ScrapeResponse, Document } from \"../../controllers/v1/types\";\n+import { Response } from \"express\";\n+import OpenAI from \"openai\";\n+import { zodResponseFormat } from \"openai/helpers/zod\";\n+import { z } from \"zod\";\n+\n+interface GenerateLLMsTextServiceOptions {\n+  generationId: string;\n+  teamId: string;\n+  plan: string;\n+  url: string;\n+  maxUrls: number;\n+  showFullText: boolean;\n+}\n+\n+function createExpressResponse<T>(): Response {\n+  const res = {} as Response;\n+  res.status = () => res;\n+  res.json = (data: T) => data as any;\n+  res.send = () => res;\n+  res.sendStatus = () => res;\n+  res.links = () => res;\n+  res.jsonp = () => res;\n+  res.sendFile = () => res;\n+  return res;\n+}\n+\n+const DescriptionSchema = z.object({\n+  description: z.string(),\n+  title: z.string(),\n+});\n+\n+export async function performGenerateLlmsTxt(options: GenerateLLMsTextServiceOptions) {\n+  const openai = new OpenAI();\n+  const { generationId, teamId, plan, url, maxUrls, showFullText } = options;\n+  \n+  const logger = _logger.child({\n+    module: \"generate-llmstxt\",\n+    method: \"performGenerateLlmsTxt\",\n+    generationId,\n+  });\n+\n+  try {\n+    // First, get all URLs from the map controller\n+    const mockMapReq = {\n+      auth: { team_id: teamId, plan },\n+      body: { url, limit: maxUrls, includeSubdomains: false },\n+    };\n+\n+   \n+    const mapResult = await mapController(mockMapReq as any, createExpressResponse<MapResponse>());\n+    const mapResponse = mapResult as unknown as MapResponse;\n+\n+    if (!mapResponse.success) {\n+      throw new Error(`Failed to map URLs: ${mapResponse.error}`);\n+    }\n+\n+    _logger.debug(\"Mapping URLs\", mapResponse.links);\n+\n+    const urls = mapResponse.links;\n+    let llmstxt = `# ${url} llms.txt\\n\\n`;\n+    let llmsFulltxt = `# ${url} llms-full.txt\\n\\n`;\n+\n+\n+    // Scrape each URL\n+    for (const url of urls) {\n+      const mockScrapeReq = {\n+        auth: { team_id: teamId, plan },\n+        body: {\n+          url,\n+          formats: [\"markdown\"],\n+          onlyMainContent: true,\n+        },\n+      };\n+\n+      _logger.debug(`Scraping URL: ${url}`);\n+      const scrapeResult = await scrapeController(mockScrapeReq as any, createExpressResponse<ScrapeResponse>());\n+      const scrapeResponse = scrapeResult as unknown as ScrapeResponse;\n+\n+      if (!scrapeResponse.success) {\n+        logger.error(`Failed to scrape URL ${url}: ${scrapeResponse.error}`);",
        "comment_created_at": "2025-02-19T15:54:17+00:00",
        "comment_author": "mogery",
        "comment_body": "```suggestion\r\n        logger.error(`Failed to scrape URL ${url}`, { error: scrapeResponse.error });\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1961912905",
    "pr_number": 1202,
    "pr_file": "apps/api/src/lib/deep-research/deep-research-service.ts",
    "created_at": "2025-02-19T15:37:28+00:00",
    "commented_code": "+import { logger as _logger } from \"../logger\";\n+import { updateDeepResearch } from \"./deep-research-redis\";\n+import { PlanType } from \"../../types\";\n+import { searchAndScrapeSearchResult } from \"../../controllers/v1/search\";\n+import { ResearchLLMService, ResearchStateManager } from \"./research-manager\";\n+import { logJob } from \"../../services/logging/log_job\";\n+import { updateExtract } from \"../extract/extract-redis\";\n+import { billTeam } from \"../../services/billing/credit_billing\";\n+\n+interface DeepResearchServiceOptions {\n+  researchId: string;\n+  teamId: string;\n+  plan: string;\n+  topic: string;\n+  maxDepth: number;\n+  timeLimit: number;\n+  subId?: string;\n+}\n+\n+export async function performDeepResearch(options: DeepResearchServiceOptions) {\n+  const { researchId, teamId, plan, timeLimit, subId } = options;\n+  const startTime = Date.now();\n+  let currentTopic = options.topic;\n+\n+  const logger = _logger.child({\n+    module: \"deep-research\",\n+    method: \"performDeepResearch\",\n+    researchId,\n+  });\n+\n+  logger.debug(\"[Deep Research] Starting research with options:\", options);\n+\n+  const state = new ResearchStateManager(\n+    researchId,\n+    teamId,\n+    plan,\n+    options.maxDepth,\n+    logger,\n+    options.topic,\n+  );\n+  const llmService = new ResearchLLMService(logger);\n+\n+  try {\n+    while (!state.hasReachedMaxDepth()) {\n+      logger.debug(\"[Deep Research] Current depth:\", state.getCurrentDepth());\n+      const timeElapsed = Date.now() - startTime;\n+      if (timeElapsed >= timeLimit * 1000) {\n+        logger.debug(\"[Deep Research] Time limit reached, stopping research\");\n+        break;\n+      }\n+\n+      await state.incrementDepth();\n+\n+      // Search phase\n+      await state.addActivity({\n+        type: \"search\",\n+        status: \"processing\",\n+        message: `Generating search queries for \"${currentTopic}\"`,\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      const nextSearchTopic = state.getNextSearchTopic();\n+      logger.debug(\"[Deep Research] Next search topic:\", nextSearchTopic);\n+\n+      const searchQueries = (\n+        await llmService.generateSearchQueries(\n+          nextSearchTopic,\n+          state.getFindings(),\n+        )\n+      ).slice(0, 3);\n+\n+      logger.debug(\"[Deep Research] Generated search queries:\", searchQueries);\n+\n+      await state.addActivity({\n+        type: \"search\",\n+        status: \"processing\",\n+        message: `Starting ${searchQueries.length} parallel searches for \"${currentTopic}\"`,\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      // Run all searches in parallel\n+      const searchPromises = searchQueries.map(async (searchQuery) => {\n+        await state.addActivity({\n+          type: \"search\",\n+          status: \"processing\",\n+          message: `Searching for \"${searchQuery.query}\" - Goal: ${searchQuery.researchGoal}`,\n+          timestamp: new Date().toISOString(),\n+          depth: state.getCurrentDepth(),\n+        });\n+\n+        const response = await searchAndScrapeSearchResult(searchQuery.query, {\n+          teamId: options.teamId,\n+          plan: options.plan as PlanType,\n+          origin: \"deep-research\",\n+          timeout: 15000,\n+          scrapeOptions: {\n+            formats: [\"markdown\"],\n+            onlyMainContent: true,\n+            waitFor: 0,\n+            mobile: false,\n+            parsePDF: false,\n+            useMock: \"none\",\n+            skipTlsVerification: false,\n+            removeBase64Images: false,\n+            fastMode: false,\n+            blockAds: false,\n+          },\n+        });\n+        return response.length > 0 ? response : [];\n+      });\n+\n+      const searchResultsArrays = await Promise.all(searchPromises);\n+      const searchResults = searchResultsArrays.flat();\n+\n+      logger.debug(\n+        \"[Deep Research] Search results count:\",\n+        searchResults.length,\n+      );\n+\n+      if (!searchResults || searchResults.length === 0) {\n+        logger.debug(\n+          \"[Deep Research] No results found for topic:\",\n+          currentTopic,\n+        );\n+        await state.addActivity({\n+          type: \"search\",\n+          status: \"error\",\n+          message: `No results found for any queries about \"${currentTopic}\"`,\n+          timestamp: new Date().toISOString(),\n+          depth: state.getCurrentDepth(),\n+        });\n+        continue;\n+      }\n+\n+      // Filter out already seen URLs and track new ones\n+      const newSearchResults = searchResults.filter((result) => {\n+        if (!result.url || state.hasSeenUrl(result.url)) {\n+          return false;\n+        }\n+        state.addSeenUrl(result.url);\n+        return true;\n+      });\n+\n+      logger.debug(\n+        \"[Deep Research] New unique results count:\",\n+        newSearchResults.length,\n+      );\n+\n+      if (newSearchResults.length === 0) {\n+        logger.debug(\n+          \"[Deep Research] No new unique results found for topic:\",\n+          currentTopic,\n+        );\n+        await state.addActivity({\n+          type: \"search\",\n+          status: \"error\",\n+          message: `Found ${searchResults.length} results but all URLs were already processed for \"${currentTopic}\"`,\n+          timestamp: new Date().toISOString(),\n+          depth: state.getCurrentDepth(),\n+        });\n+        continue;\n+      }\n+\n+      await state.addActivity({\n+        type: \"search\",\n+        status: \"complete\",\n+        message: `Found ${newSearchResults.length} new relevant results across ${searchQueries.length} parallel queries`,\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      await state.addFindings(\n+        newSearchResults.map((result) => ({\n+          text: result.markdown ?? \"\",\n+          source: result.url ?? \"\",\n+        })),\n+      );\n+\n+      // Analysis phase\n+      await state.addActivity({\n+        type: \"analyze\",\n+        status: \"processing\",\n+        message: \"Analyzing findings\",\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      const timeRemaining = timeLimit * 1000 - (Date.now() - startTime);\n+      logger.debug(\"[Deep Research] Time remaining (ms):\", timeRemaining);\n+\n+      const analysis = await llmService.analyzeAndPlan(\n+        state.getFindings(),\n+        currentTopic,\n+        timeRemaining,\n+      );\n+\n+      if (!analysis) {\n+        logger.debug(\"[Deep Research] Analysis failed\");\n+        await state.addActivity({\n+          type: \"analyze\",\n+          status: \"error\",\n+          message: \"Failed to analyze findings\",\n+          timestamp: new Date().toISOString(),\n+          depth: state.getCurrentDepth(),\n+        });\n+\n+        state.incrementFailedAttempts();\n+        if (state.hasReachedMaxFailedAttempts()) {\n+          logger.debug(\"[Deep Research] Max failed attempts reached\");\n+          break;\n+        }\n+        continue;\n+      }\n+\n+      logger.debug(\"[Deep Research] Analysis result:\", {\n+        nextTopic: analysis.nextSearchTopic,\n+        shouldContinue: analysis.shouldContinue,\n+        gapsCount: analysis.gaps.length,\n+      });\n+\n+      state.setNextSearchTopic(analysis.nextSearchTopic || \"\");\n+\n+      await state.addActivity({\n+        type: \"analyze\",\n+        status: \"complete\",\n+        message: \"Analyzed findings\",\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      if (!analysis.shouldContinue || analysis.gaps.length === 0) {\n+        logger.debug(\"[Deep Research] No more gaps to research, ending search\");\n+        break;\n+      }\n+\n+      currentTopic = analysis.gaps[0] || currentTopic;\n+      logger.debug(\"[Deep Research] Next topic to research:\", currentTopic);\n+    }\n+\n+    // Final synthesis\n+    logger.debug(\"[Deep Research] Starting final synthesis\");\n+    await state.addActivity({\n+      type: \"synthesis\",\n+      status: \"processing\",\n+      message: \"Preparing final analysis\",\n+      timestamp: new Date().toISOString(),\n+      depth: state.getCurrentDepth(),\n+    });\n+\n+    const finalAnalysis = await llmService.generateFinalAnalysis(\n+      options.topic,\n+      state.getFindings(),\n+      state.getSummaries(),\n+    );\n+\n+    await state.addActivity({\n+      type: \"synthesis\",\n+      status: \"complete\",\n+      message: \"Research completed\",\n+      timestamp: new Date().toISOString(),\n+      depth: state.getCurrentDepth(),\n+    });\n+\n+    const progress = state.getProgress();\n+    logger.debug(\"[Deep Research] Research completed successfully\");\n+\n+    // Log job with token usage and sources\n+    await logJob({\n+      job_id: researchId,\n+      success: true,\n+      message: \"Research completed\",\n+      num_docs: 1,\n+      docs: [{ finalAnalysis: finalAnalysis }],\n+      time_taken: (Date.now() - startTime) / 1000,\n+      team_id: teamId,\n+      mode: \"deep-research\",\n+      url: options.topic,\n+      scrapeOptions: options,\n+      origin: \"api\",\n+      num_tokens: 0,\n+      tokens_billed: 0,\n+      sources: {},\n+    });\n+    await updateDeepResearch(researchId, {\n+      status: \"completed\",\n+      finalAnalysis: finalAnalysis,\n+    });\n+    // Bill team for usage\n+    billTeam(teamId, subId, state.getFindings().length, logger).catch(\n+      (error) => {\n+        logger.error(\n+          `Failed to bill team ${teamId} for ${state.getFindings().length} findings: ${error}`,\n+        );\n+      },\n+    );\n+    return {\n+      success: true,\n+      data: {\n+        finalAnalysis: finalAnalysis,\n+      },\n+    };\n+  } catch (error: any) {\n+    console.error(\"[Deep Research] Error:\", error);\n+    logger.error(\"Deep research error:\", error);",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "1961912905",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 1202,
        "pr_file": "apps/api/src/lib/deep-research/deep-research-service.ts",
        "discussion_id": "1961912905",
        "commented_code": "@@ -0,0 +1,313 @@\n+import { logger as _logger } from \"../logger\";\n+import { updateDeepResearch } from \"./deep-research-redis\";\n+import { PlanType } from \"../../types\";\n+import { searchAndScrapeSearchResult } from \"../../controllers/v1/search\";\n+import { ResearchLLMService, ResearchStateManager } from \"./research-manager\";\n+import { logJob } from \"../../services/logging/log_job\";\n+import { updateExtract } from \"../extract/extract-redis\";\n+import { billTeam } from \"../../services/billing/credit_billing\";\n+\n+interface DeepResearchServiceOptions {\n+  researchId: string;\n+  teamId: string;\n+  plan: string;\n+  topic: string;\n+  maxDepth: number;\n+  timeLimit: number;\n+  subId?: string;\n+}\n+\n+export async function performDeepResearch(options: DeepResearchServiceOptions) {\n+  const { researchId, teamId, plan, timeLimit, subId } = options;\n+  const startTime = Date.now();\n+  let currentTopic = options.topic;\n+\n+  const logger = _logger.child({\n+    module: \"deep-research\",\n+    method: \"performDeepResearch\",\n+    researchId,\n+  });\n+\n+  logger.debug(\"[Deep Research] Starting research with options:\", options);\n+\n+  const state = new ResearchStateManager(\n+    researchId,\n+    teamId,\n+    plan,\n+    options.maxDepth,\n+    logger,\n+    options.topic,\n+  );\n+  const llmService = new ResearchLLMService(logger);\n+\n+  try {\n+    while (!state.hasReachedMaxDepth()) {\n+      logger.debug(\"[Deep Research] Current depth:\", state.getCurrentDepth());\n+      const timeElapsed = Date.now() - startTime;\n+      if (timeElapsed >= timeLimit * 1000) {\n+        logger.debug(\"[Deep Research] Time limit reached, stopping research\");\n+        break;\n+      }\n+\n+      await state.incrementDepth();\n+\n+      // Search phase\n+      await state.addActivity({\n+        type: \"search\",\n+        status: \"processing\",\n+        message: `Generating search queries for \"${currentTopic}\"`,\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      const nextSearchTopic = state.getNextSearchTopic();\n+      logger.debug(\"[Deep Research] Next search topic:\", nextSearchTopic);\n+\n+      const searchQueries = (\n+        await llmService.generateSearchQueries(\n+          nextSearchTopic,\n+          state.getFindings(),\n+        )\n+      ).slice(0, 3);\n+\n+      logger.debug(\"[Deep Research] Generated search queries:\", searchQueries);\n+\n+      await state.addActivity({\n+        type: \"search\",\n+        status: \"processing\",\n+        message: `Starting ${searchQueries.length} parallel searches for \"${currentTopic}\"`,\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      // Run all searches in parallel\n+      const searchPromises = searchQueries.map(async (searchQuery) => {\n+        await state.addActivity({\n+          type: \"search\",\n+          status: \"processing\",\n+          message: `Searching for \"${searchQuery.query}\" - Goal: ${searchQuery.researchGoal}`,\n+          timestamp: new Date().toISOString(),\n+          depth: state.getCurrentDepth(),\n+        });\n+\n+        const response = await searchAndScrapeSearchResult(searchQuery.query, {\n+          teamId: options.teamId,\n+          plan: options.plan as PlanType,\n+          origin: \"deep-research\",\n+          timeout: 15000,\n+          scrapeOptions: {\n+            formats: [\"markdown\"],\n+            onlyMainContent: true,\n+            waitFor: 0,\n+            mobile: false,\n+            parsePDF: false,\n+            useMock: \"none\",\n+            skipTlsVerification: false,\n+            removeBase64Images: false,\n+            fastMode: false,\n+            blockAds: false,\n+          },\n+        });\n+        return response.length > 0 ? response : [];\n+      });\n+\n+      const searchResultsArrays = await Promise.all(searchPromises);\n+      const searchResults = searchResultsArrays.flat();\n+\n+      logger.debug(\n+        \"[Deep Research] Search results count:\",\n+        searchResults.length,\n+      );\n+\n+      if (!searchResults || searchResults.length === 0) {\n+        logger.debug(\n+          \"[Deep Research] No results found for topic:\",\n+          currentTopic,\n+        );\n+        await state.addActivity({\n+          type: \"search\",\n+          status: \"error\",\n+          message: `No results found for any queries about \"${currentTopic}\"`,\n+          timestamp: new Date().toISOString(),\n+          depth: state.getCurrentDepth(),\n+        });\n+        continue;\n+      }\n+\n+      // Filter out already seen URLs and track new ones\n+      const newSearchResults = searchResults.filter((result) => {\n+        if (!result.url || state.hasSeenUrl(result.url)) {\n+          return false;\n+        }\n+        state.addSeenUrl(result.url);\n+        return true;\n+      });\n+\n+      logger.debug(\n+        \"[Deep Research] New unique results count:\",\n+        newSearchResults.length,\n+      );\n+\n+      if (newSearchResults.length === 0) {\n+        logger.debug(\n+          \"[Deep Research] No new unique results found for topic:\",\n+          currentTopic,\n+        );\n+        await state.addActivity({\n+          type: \"search\",\n+          status: \"error\",\n+          message: `Found ${searchResults.length} results but all URLs were already processed for \"${currentTopic}\"`,\n+          timestamp: new Date().toISOString(),\n+          depth: state.getCurrentDepth(),\n+        });\n+        continue;\n+      }\n+\n+      await state.addActivity({\n+        type: \"search\",\n+        status: \"complete\",\n+        message: `Found ${newSearchResults.length} new relevant results across ${searchQueries.length} parallel queries`,\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      await state.addFindings(\n+        newSearchResults.map((result) => ({\n+          text: result.markdown ?? \"\",\n+          source: result.url ?? \"\",\n+        })),\n+      );\n+\n+      // Analysis phase\n+      await state.addActivity({\n+        type: \"analyze\",\n+        status: \"processing\",\n+        message: \"Analyzing findings\",\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      const timeRemaining = timeLimit * 1000 - (Date.now() - startTime);\n+      logger.debug(\"[Deep Research] Time remaining (ms):\", timeRemaining);\n+\n+      const analysis = await llmService.analyzeAndPlan(\n+        state.getFindings(),\n+        currentTopic,\n+        timeRemaining,\n+      );\n+\n+      if (!analysis) {\n+        logger.debug(\"[Deep Research] Analysis failed\");\n+        await state.addActivity({\n+          type: \"analyze\",\n+          status: \"error\",\n+          message: \"Failed to analyze findings\",\n+          timestamp: new Date().toISOString(),\n+          depth: state.getCurrentDepth(),\n+        });\n+\n+        state.incrementFailedAttempts();\n+        if (state.hasReachedMaxFailedAttempts()) {\n+          logger.debug(\"[Deep Research] Max failed attempts reached\");\n+          break;\n+        }\n+        continue;\n+      }\n+\n+      logger.debug(\"[Deep Research] Analysis result:\", {\n+        nextTopic: analysis.nextSearchTopic,\n+        shouldContinue: analysis.shouldContinue,\n+        gapsCount: analysis.gaps.length,\n+      });\n+\n+      state.setNextSearchTopic(analysis.nextSearchTopic || \"\");\n+\n+      await state.addActivity({\n+        type: \"analyze\",\n+        status: \"complete\",\n+        message: \"Analyzed findings\",\n+        timestamp: new Date().toISOString(),\n+        depth: state.getCurrentDepth(),\n+      });\n+\n+      if (!analysis.shouldContinue || analysis.gaps.length === 0) {\n+        logger.debug(\"[Deep Research] No more gaps to research, ending search\");\n+        break;\n+      }\n+\n+      currentTopic = analysis.gaps[0] || currentTopic;\n+      logger.debug(\"[Deep Research] Next topic to research:\", currentTopic);\n+    }\n+\n+    // Final synthesis\n+    logger.debug(\"[Deep Research] Starting final synthesis\");\n+    await state.addActivity({\n+      type: \"synthesis\",\n+      status: \"processing\",\n+      message: \"Preparing final analysis\",\n+      timestamp: new Date().toISOString(),\n+      depth: state.getCurrentDepth(),\n+    });\n+\n+    const finalAnalysis = await llmService.generateFinalAnalysis(\n+      options.topic,\n+      state.getFindings(),\n+      state.getSummaries(),\n+    );\n+\n+    await state.addActivity({\n+      type: \"synthesis\",\n+      status: \"complete\",\n+      message: \"Research completed\",\n+      timestamp: new Date().toISOString(),\n+      depth: state.getCurrentDepth(),\n+    });\n+\n+    const progress = state.getProgress();\n+    logger.debug(\"[Deep Research] Research completed successfully\");\n+\n+    // Log job with token usage and sources\n+    await logJob({\n+      job_id: researchId,\n+      success: true,\n+      message: \"Research completed\",\n+      num_docs: 1,\n+      docs: [{ finalAnalysis: finalAnalysis }],\n+      time_taken: (Date.now() - startTime) / 1000,\n+      team_id: teamId,\n+      mode: \"deep-research\",\n+      url: options.topic,\n+      scrapeOptions: options,\n+      origin: \"api\",\n+      num_tokens: 0,\n+      tokens_billed: 0,\n+      sources: {},\n+    });\n+    await updateDeepResearch(researchId, {\n+      status: \"completed\",\n+      finalAnalysis: finalAnalysis,\n+    });\n+    // Bill team for usage\n+    billTeam(teamId, subId, state.getFindings().length, logger).catch(\n+      (error) => {\n+        logger.error(\n+          `Failed to bill team ${teamId} for ${state.getFindings().length} findings: ${error}`,\n+        );\n+      },\n+    );\n+    return {\n+      success: true,\n+      data: {\n+        finalAnalysis: finalAnalysis,\n+      },\n+    };\n+  } catch (error: any) {\n+    console.error(\"[Deep Research] Error:\", error);\n+    logger.error(\"Deep research error:\", error);",
        "comment_created_at": "2025-02-19T15:37:28+00:00",
        "comment_author": "mogery",
        "comment_body": "```suggestion\r\n    logger.error(\"Deep research error\", { error });\r\n```",
        "pr_file_module": null
      }
    ]
  }
]