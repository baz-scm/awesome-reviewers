[
  {
    "discussion_id": "1963113533",
    "pr_number": 1193,
    "pr_file": "apps/api/src/search/searxng.ts",
    "created_at": "2025-02-20T08:56:47+00:00",
    "commented_code": "+import axios from \"axios\";\n+import dotenv from \"dotenv\";\n+import { SearchResult } from \"../../src/lib/entities\";\n+\n+dotenv.config();\n+\n+interface SearchOptions {\n+  tbs?: string;\n+  filter?: string;\n+  lang?: string;\n+  country?: string;\n+  location?: string;\n+  num_results: number;\n+  page?: number;\n+}\n+\n+export async function searxng_search(\n+  q: string,\n+  options: SearchOptions,\n+): Promise<SearchResult[]> {\n+  const params = {\n+    q: q,\n+    language: options.lang,\n+    // gl: options.country, //not possible with SearXNG\n+    // location: options.location, //not possible with SearXNG\n+    // num: options.num_results, //not possible with SearXNG\n+    engines: process.env.SEARXNG_ENGINES || \"\",\n+    categories: process.env.SEARXNG_CATEGORIES || \"general\",\n+    pageno: options.page ?? 1,\n+    format: \"json\"\n+  };\n+\n+  const url = process.env.SEARXNG_ENDPOINT as string;\n+  if (!url) {\n+    console.error(`SEARXNG_ENDPOINT environment variable is not set`);\n+  }",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "1963113533",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 1193,
        "pr_file": "apps/api/src/search/searxng.ts",
        "discussion_id": "1963113533",
        "commented_code": "@@ -0,0 +1,66 @@\n+import axios from \"axios\";\n+import dotenv from \"dotenv\";\n+import { SearchResult } from \"../../src/lib/entities\";\n+\n+dotenv.config();\n+\n+interface SearchOptions {\n+  tbs?: string;\n+  filter?: string;\n+  lang?: string;\n+  country?: string;\n+  location?: string;\n+  num_results: number;\n+  page?: number;\n+}\n+\n+export async function searxng_search(\n+  q: string,\n+  options: SearchOptions,\n+): Promise<SearchResult[]> {\n+  const params = {\n+    q: q,\n+    language: options.lang,\n+    // gl: options.country, //not possible with SearXNG\n+    // location: options.location, //not possible with SearXNG\n+    // num: options.num_results, //not possible with SearXNG\n+    engines: process.env.SEARXNG_ENGINES || \"\",\n+    categories: process.env.SEARXNG_CATEGORIES || \"general\",\n+    pageno: options.page ?? 1,\n+    format: \"json\"\n+  };\n+\n+  const url = process.env.SEARXNG_ENDPOINT as string;\n+  if (!url) {\n+    console.error(`SEARXNG_ENDPOINT environment variable is not set`);\n+  }",
        "comment_created_at": "2025-02-20T08:56:47+00:00",
        "comment_author": "mogery",
        "comment_body": "Not required since it's only called if the endpoint is defined\r\n```suggestion\r\n  const url = process.env.SEARXNG_ENDPOINT!;\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1854762115",
    "pr_number": 915,
    "pr_file": "apps/api/src/controllers/v1/extract.ts",
    "created_at": "2024-11-22T21:47:07+00:00",
    "commented_code": "+import { Request, Response } from \"express\";\n+import {\n+  // Document,\n+  RequestWithAuth,\n+  ExtractRequest,\n+  extractRequestSchema,\n+  ExtractResponse,\n+  MapDocument,\n+  scrapeOptions,\n+} from \"./types\";\n+import { Document } from \"../../lib/entities\";\n+import Redis from \"ioredis\";\n+import { configDotenv } from \"dotenv\";\n+import { performRanking } from \"../../lib/ranker\";\n+import { billTeam } from \"../../services/billing/credit_billing\";\n+import { logJob } from \"../../services/logging/log_job\";\n+import { logger } from \"../../lib/logger\";\n+import { getScrapeQueue } from \"../../services/queue-service\";\n+import { waitForJob } from \"../../services/queue-jobs\";\n+import { addScrapeJob } from \"../../services/queue-jobs\";\n+import { PlanType } from \"../../types\";\n+import { getJobPriority } from \"../../lib/job-priority\";\n+import { generateOpenAICompletions } from \"../../scraper/scrapeURL/transformers/llmExtract\";\n+import { isUrlBlocked } from \"../../scraper/WebScraper/utils/blocklist\";\n+import { getMapResults } from \"./map\";\n+import { buildDocument } from \"../../lib/extract/build-document\";\n+\n+configDotenv();\n+const redis = new Redis(process.env.REDIS_URL!);\n+\n+const MAX_EXTRACT_LIMIT = 100;\n+const MAX_RANKING_LIMIT = 10;\n+const SCORE_THRESHOLD = 0.75;\n+\n+/**\n+ * Extracts data from the provided URLs based on the request parameters.\n+ * Currently in beta.\n+ * @param req - The request object containing authentication and extraction details.\n+ * @param res - The response object to send the extraction results.\n+ * @returns A promise that resolves when the extraction process is complete.\n+ */\n+export async function extractController(\n+  req: RequestWithAuth<{}, ExtractResponse, ExtractRequest>,\n+  res: Response<ExtractResponse>\n+) {\n+  const selfHosted = process.env.USE_DB_AUTHENTICATION !== \"true\";\n+  \n+  req.body = extractRequestSchema.parse(req.body);\n+\n+  const id = crypto.randomUUID();\n+  let links: string[] = [];\n+  let docs: Document[] = [];\n+  const earlyReturn = false;\n+\n+  // Process all URLs in parallel\n+  const urlPromises = req.body.urls.map(async (url) => {\n+    if (url.includes('/*') || req.body.allowExternalLinks) {\n+      // Handle glob pattern URLs\n+      const baseUrl = url.replace('/*', '');\n+      // const pathPrefix = baseUrl.split('/').slice(3).join('/'); // Get path after domain if any\n+\n+      const allowExternalLinks = req.body.allowExternalLinks ?? true;\n+      let urlWithoutWww = baseUrl.replace(\"www.\", \"\");\n+      let mapUrl = req.body.prompt && allowExternalLinks\n+        ? `${req.body.prompt} ${urlWithoutWww}`\n+        : req.body.prompt ? `${req.body.prompt} site:${urlWithoutWww}`\n+        : `site:${urlWithoutWww}`;\n+\n+      const mapResults = await getMapResults({\n+        url: baseUrl,\n+        search: req.body.prompt,\n+        teamId: req.auth.team_id,\n+        plan: req.auth.plan,\n+        allowExternalLinks,\n+        origin: req.body.origin,\n+        limit: req.body.limit,\n+        // If we're self-hosted, we don't want to ignore the sitemap, due to our fire-engine mapping\n+        ignoreSitemap: !selfHosted ? true : false,\n+        includeMetadata: true,\n+        includeSubdomains: req.body.includeSubdomains,\n+      });\n+\n+      let mappedLinks = mapResults.links as MapDocument[];\n+      // Limit number of links to MAX_EXTRACT_LIMIT\n+      mappedLinks = mappedLinks.slice(0, MAX_EXTRACT_LIMIT);\n+\n+      let mappedLinksRerank = mappedLinks.map(x => `url: ${x.url}, title: ${x.title}, description: ${x.description}`);\n+      \n+      // Filter by path prefix if present\n+      // wrong\n+      // if (pathPrefix) {\n+      //   mappedLinks = mappedLinks.filter(x => x.url && x.url.includes(`/${pathPrefix}/`));\n+      // }\n+\n+      if (req.body.prompt) {\n+        const linksAndScores : { link: string, linkWithContext: string, score: number, originalIndex: number }[] = await performRanking(mappedLinksRerank, mappedLinks.map(l => l.url), mapUrl);\n+        mappedLinks = linksAndScores\n+          .filter(x => x.score > SCORE_THRESHOLD)\n+          .map(x => mappedLinks.find(link => link.url === x.link))\n+          .filter((x): x is MapDocument => x !== undefined && x.url !== undefined && !isUrlBlocked(x.url))\n+          .slice(0, MAX_RANKING_LIMIT);\n+        // console.log(\"linksAndScores\", linksAndScores);\n+        // console.log(\"linksAndScores\", linksAndScores.length);\n+      }\n+\n+      return mappedLinks.map(x => x.url) as string[];\n+\n+    } else {\n+      // Handle direct URLs without glob pattern\n+      if (!isUrlBlocked(url)) {\n+        return [url];\n+      }\n+      return [];\n+    }\n+  });\n+\n+  // Wait for all URL processing to complete and flatten results\n+  const processedUrls = await Promise.all(urlPromises);\n+  links.push(...processedUrls.flat());\n+\n+  // console.log(\"links\", links.length);\n+  // Scrape all links in parallel\n+  const scrapePromises = links.map(async (url) => {\n+    const origin = req.body.origin || \"api\";\n+    const timeout = req.body.timeout ?? 30000;",
    "repo_full_name": "firecrawl/firecrawl",
    "discussion_comments": [
      {
        "comment_id": "1854762115",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 915,
        "pr_file": "apps/api/src/controllers/v1/extract.ts",
        "discussion_id": "1854762115",
        "commented_code": "@@ -0,0 +1,239 @@\n+import { Request, Response } from \"express\";\n+import {\n+  // Document,\n+  RequestWithAuth,\n+  ExtractRequest,\n+  extractRequestSchema,\n+  ExtractResponse,\n+  MapDocument,\n+  scrapeOptions,\n+} from \"./types\";\n+import { Document } from \"../../lib/entities\";\n+import Redis from \"ioredis\";\n+import { configDotenv } from \"dotenv\";\n+import { performRanking } from \"../../lib/ranker\";\n+import { billTeam } from \"../../services/billing/credit_billing\";\n+import { logJob } from \"../../services/logging/log_job\";\n+import { logger } from \"../../lib/logger\";\n+import { getScrapeQueue } from \"../../services/queue-service\";\n+import { waitForJob } from \"../../services/queue-jobs\";\n+import { addScrapeJob } from \"../../services/queue-jobs\";\n+import { PlanType } from \"../../types\";\n+import { getJobPriority } from \"../../lib/job-priority\";\n+import { generateOpenAICompletions } from \"../../scraper/scrapeURL/transformers/llmExtract\";\n+import { isUrlBlocked } from \"../../scraper/WebScraper/utils/blocklist\";\n+import { getMapResults } from \"./map\";\n+import { buildDocument } from \"../../lib/extract/build-document\";\n+\n+configDotenv();\n+const redis = new Redis(process.env.REDIS_URL!);\n+\n+const MAX_EXTRACT_LIMIT = 100;\n+const MAX_RANKING_LIMIT = 10;\n+const SCORE_THRESHOLD = 0.75;\n+\n+/**\n+ * Extracts data from the provided URLs based on the request parameters.\n+ * Currently in beta.\n+ * @param req - The request object containing authentication and extraction details.\n+ * @param res - The response object to send the extraction results.\n+ * @returns A promise that resolves when the extraction process is complete.\n+ */\n+export async function extractController(\n+  req: RequestWithAuth<{}, ExtractResponse, ExtractRequest>,\n+  res: Response<ExtractResponse>\n+) {\n+  const selfHosted = process.env.USE_DB_AUTHENTICATION !== \"true\";\n+  \n+  req.body = extractRequestSchema.parse(req.body);\n+\n+  const id = crypto.randomUUID();\n+  let links: string[] = [];\n+  let docs: Document[] = [];\n+  const earlyReturn = false;\n+\n+  // Process all URLs in parallel\n+  const urlPromises = req.body.urls.map(async (url) => {\n+    if (url.includes('/*') || req.body.allowExternalLinks) {\n+      // Handle glob pattern URLs\n+      const baseUrl = url.replace('/*', '');\n+      // const pathPrefix = baseUrl.split('/').slice(3).join('/'); // Get path after domain if any\n+\n+      const allowExternalLinks = req.body.allowExternalLinks ?? true;\n+      let urlWithoutWww = baseUrl.replace(\"www.\", \"\");\n+      let mapUrl = req.body.prompt && allowExternalLinks\n+        ? `${req.body.prompt} ${urlWithoutWww}`\n+        : req.body.prompt ? `${req.body.prompt} site:${urlWithoutWww}`\n+        : `site:${urlWithoutWww}`;\n+\n+      const mapResults = await getMapResults({\n+        url: baseUrl,\n+        search: req.body.prompt,\n+        teamId: req.auth.team_id,\n+        plan: req.auth.plan,\n+        allowExternalLinks,\n+        origin: req.body.origin,\n+        limit: req.body.limit,\n+        // If we're self-hosted, we don't want to ignore the sitemap, due to our fire-engine mapping\n+        ignoreSitemap: !selfHosted ? true : false,\n+        includeMetadata: true,\n+        includeSubdomains: req.body.includeSubdomains,\n+      });\n+\n+      let mappedLinks = mapResults.links as MapDocument[];\n+      // Limit number of links to MAX_EXTRACT_LIMIT\n+      mappedLinks = mappedLinks.slice(0, MAX_EXTRACT_LIMIT);\n+\n+      let mappedLinksRerank = mappedLinks.map(x => `url: ${x.url}, title: ${x.title}, description: ${x.description}`);\n+      \n+      // Filter by path prefix if present\n+      // wrong\n+      // if (pathPrefix) {\n+      //   mappedLinks = mappedLinks.filter(x => x.url && x.url.includes(`/${pathPrefix}/`));\n+      // }\n+\n+      if (req.body.prompt) {\n+        const linksAndScores : { link: string, linkWithContext: string, score: number, originalIndex: number }[] = await performRanking(mappedLinksRerank, mappedLinks.map(l => l.url), mapUrl);\n+        mappedLinks = linksAndScores\n+          .filter(x => x.score > SCORE_THRESHOLD)\n+          .map(x => mappedLinks.find(link => link.url === x.link))\n+          .filter((x): x is MapDocument => x !== undefined && x.url !== undefined && !isUrlBlocked(x.url))\n+          .slice(0, MAX_RANKING_LIMIT);\n+        // console.log(\"linksAndScores\", linksAndScores);\n+        // console.log(\"linksAndScores\", linksAndScores.length);\n+      }\n+\n+      return mappedLinks.map(x => x.url) as string[];\n+\n+    } else {\n+      // Handle direct URLs without glob pattern\n+      if (!isUrlBlocked(url)) {\n+        return [url];\n+      }\n+      return [];\n+    }\n+  });\n+\n+  // Wait for all URL processing to complete and flatten results\n+  const processedUrls = await Promise.all(urlPromises);\n+  links.push(...processedUrls.flat());\n+\n+  // console.log(\"links\", links.length);\n+  // Scrape all links in parallel\n+  const scrapePromises = links.map(async (url) => {\n+    const origin = req.body.origin || \"api\";\n+    const timeout = req.body.timeout ?? 30000;",
        "comment_created_at": "2024-11-22T21:47:07+00:00",
        "comment_author": "nickscamara",
        "comment_body": "Maybe rename this timeout to scrapeTimeout?",
        "pr_file_module": null
      },
      {
        "comment_id": "1855728853",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 915,
        "pr_file": "apps/api/src/controllers/v1/extract.ts",
        "discussion_id": "1854762115",
        "commented_code": "@@ -0,0 +1,239 @@\n+import { Request, Response } from \"express\";\n+import {\n+  // Document,\n+  RequestWithAuth,\n+  ExtractRequest,\n+  extractRequestSchema,\n+  ExtractResponse,\n+  MapDocument,\n+  scrapeOptions,\n+} from \"./types\";\n+import { Document } from \"../../lib/entities\";\n+import Redis from \"ioredis\";\n+import { configDotenv } from \"dotenv\";\n+import { performRanking } from \"../../lib/ranker\";\n+import { billTeam } from \"../../services/billing/credit_billing\";\n+import { logJob } from \"../../services/logging/log_job\";\n+import { logger } from \"../../lib/logger\";\n+import { getScrapeQueue } from \"../../services/queue-service\";\n+import { waitForJob } from \"../../services/queue-jobs\";\n+import { addScrapeJob } from \"../../services/queue-jobs\";\n+import { PlanType } from \"../../types\";\n+import { getJobPriority } from \"../../lib/job-priority\";\n+import { generateOpenAICompletions } from \"../../scraper/scrapeURL/transformers/llmExtract\";\n+import { isUrlBlocked } from \"../../scraper/WebScraper/utils/blocklist\";\n+import { getMapResults } from \"./map\";\n+import { buildDocument } from \"../../lib/extract/build-document\";\n+\n+configDotenv();\n+const redis = new Redis(process.env.REDIS_URL!);\n+\n+const MAX_EXTRACT_LIMIT = 100;\n+const MAX_RANKING_LIMIT = 10;\n+const SCORE_THRESHOLD = 0.75;\n+\n+/**\n+ * Extracts data from the provided URLs based on the request parameters.\n+ * Currently in beta.\n+ * @param req - The request object containing authentication and extraction details.\n+ * @param res - The response object to send the extraction results.\n+ * @returns A promise that resolves when the extraction process is complete.\n+ */\n+export async function extractController(\n+  req: RequestWithAuth<{}, ExtractResponse, ExtractRequest>,\n+  res: Response<ExtractResponse>\n+) {\n+  const selfHosted = process.env.USE_DB_AUTHENTICATION !== \"true\";\n+  \n+  req.body = extractRequestSchema.parse(req.body);\n+\n+  const id = crypto.randomUUID();\n+  let links: string[] = [];\n+  let docs: Document[] = [];\n+  const earlyReturn = false;\n+\n+  // Process all URLs in parallel\n+  const urlPromises = req.body.urls.map(async (url) => {\n+    if (url.includes('/*') || req.body.allowExternalLinks) {\n+      // Handle glob pattern URLs\n+      const baseUrl = url.replace('/*', '');\n+      // const pathPrefix = baseUrl.split('/').slice(3).join('/'); // Get path after domain if any\n+\n+      const allowExternalLinks = req.body.allowExternalLinks ?? true;\n+      let urlWithoutWww = baseUrl.replace(\"www.\", \"\");\n+      let mapUrl = req.body.prompt && allowExternalLinks\n+        ? `${req.body.prompt} ${urlWithoutWww}`\n+        : req.body.prompt ? `${req.body.prompt} site:${urlWithoutWww}`\n+        : `site:${urlWithoutWww}`;\n+\n+      const mapResults = await getMapResults({\n+        url: baseUrl,\n+        search: req.body.prompt,\n+        teamId: req.auth.team_id,\n+        plan: req.auth.plan,\n+        allowExternalLinks,\n+        origin: req.body.origin,\n+        limit: req.body.limit,\n+        // If we're self-hosted, we don't want to ignore the sitemap, due to our fire-engine mapping\n+        ignoreSitemap: !selfHosted ? true : false,\n+        includeMetadata: true,\n+        includeSubdomains: req.body.includeSubdomains,\n+      });\n+\n+      let mappedLinks = mapResults.links as MapDocument[];\n+      // Limit number of links to MAX_EXTRACT_LIMIT\n+      mappedLinks = mappedLinks.slice(0, MAX_EXTRACT_LIMIT);\n+\n+      let mappedLinksRerank = mappedLinks.map(x => `url: ${x.url}, title: ${x.title}, description: ${x.description}`);\n+      \n+      // Filter by path prefix if present\n+      // wrong\n+      // if (pathPrefix) {\n+      //   mappedLinks = mappedLinks.filter(x => x.url && x.url.includes(`/${pathPrefix}/`));\n+      // }\n+\n+      if (req.body.prompt) {\n+        const linksAndScores : { link: string, linkWithContext: string, score: number, originalIndex: number }[] = await performRanking(mappedLinksRerank, mappedLinks.map(l => l.url), mapUrl);\n+        mappedLinks = linksAndScores\n+          .filter(x => x.score > SCORE_THRESHOLD)\n+          .map(x => mappedLinks.find(link => link.url === x.link))\n+          .filter((x): x is MapDocument => x !== undefined && x.url !== undefined && !isUrlBlocked(x.url))\n+          .slice(0, MAX_RANKING_LIMIT);\n+        // console.log(\"linksAndScores\", linksAndScores);\n+        // console.log(\"linksAndScores\", linksAndScores.length);\n+      }\n+\n+      return mappedLinks.map(x => x.url) as string[];\n+\n+    } else {\n+      // Handle direct URLs without glob pattern\n+      if (!isUrlBlocked(url)) {\n+        return [url];\n+      }\n+      return [];\n+    }\n+  });\n+\n+  // Wait for all URL processing to complete and flatten results\n+  const processedUrls = await Promise.all(urlPromises);\n+  links.push(...processedUrls.flat());\n+\n+  // console.log(\"links\", links.length);\n+  // Scrape all links in parallel\n+  const scrapePromises = links.map(async (url) => {\n+    const origin = req.body.origin || \"api\";\n+    const timeout = req.body.timeout ?? 30000;",
        "comment_created_at": "2024-11-25T03:40:24+00:00",
        "comment_author": "nickscamara",
        "comment_body": "Lets not expose this timeout and instead just make a % of the total request timeout.",
        "pr_file_module": null
      },
      {
        "comment_id": "1855729077",
        "repo_full_name": "firecrawl/firecrawl",
        "pr_number": 915,
        "pr_file": "apps/api/src/controllers/v1/extract.ts",
        "discussion_id": "1854762115",
        "commented_code": "@@ -0,0 +1,239 @@\n+import { Request, Response } from \"express\";\n+import {\n+  // Document,\n+  RequestWithAuth,\n+  ExtractRequest,\n+  extractRequestSchema,\n+  ExtractResponse,\n+  MapDocument,\n+  scrapeOptions,\n+} from \"./types\";\n+import { Document } from \"../../lib/entities\";\n+import Redis from \"ioredis\";\n+import { configDotenv } from \"dotenv\";\n+import { performRanking } from \"../../lib/ranker\";\n+import { billTeam } from \"../../services/billing/credit_billing\";\n+import { logJob } from \"../../services/logging/log_job\";\n+import { logger } from \"../../lib/logger\";\n+import { getScrapeQueue } from \"../../services/queue-service\";\n+import { waitForJob } from \"../../services/queue-jobs\";\n+import { addScrapeJob } from \"../../services/queue-jobs\";\n+import { PlanType } from \"../../types\";\n+import { getJobPriority } from \"../../lib/job-priority\";\n+import { generateOpenAICompletions } from \"../../scraper/scrapeURL/transformers/llmExtract\";\n+import { isUrlBlocked } from \"../../scraper/WebScraper/utils/blocklist\";\n+import { getMapResults } from \"./map\";\n+import { buildDocument } from \"../../lib/extract/build-document\";\n+\n+configDotenv();\n+const redis = new Redis(process.env.REDIS_URL!);\n+\n+const MAX_EXTRACT_LIMIT = 100;\n+const MAX_RANKING_LIMIT = 10;\n+const SCORE_THRESHOLD = 0.75;\n+\n+/**\n+ * Extracts data from the provided URLs based on the request parameters.\n+ * Currently in beta.\n+ * @param req - The request object containing authentication and extraction details.\n+ * @param res - The response object to send the extraction results.\n+ * @returns A promise that resolves when the extraction process is complete.\n+ */\n+export async function extractController(\n+  req: RequestWithAuth<{}, ExtractResponse, ExtractRequest>,\n+  res: Response<ExtractResponse>\n+) {\n+  const selfHosted = process.env.USE_DB_AUTHENTICATION !== \"true\";\n+  \n+  req.body = extractRequestSchema.parse(req.body);\n+\n+  const id = crypto.randomUUID();\n+  let links: string[] = [];\n+  let docs: Document[] = [];\n+  const earlyReturn = false;\n+\n+  // Process all URLs in parallel\n+  const urlPromises = req.body.urls.map(async (url) => {\n+    if (url.includes('/*') || req.body.allowExternalLinks) {\n+      // Handle glob pattern URLs\n+      const baseUrl = url.replace('/*', '');\n+      // const pathPrefix = baseUrl.split('/').slice(3).join('/'); // Get path after domain if any\n+\n+      const allowExternalLinks = req.body.allowExternalLinks ?? true;\n+      let urlWithoutWww = baseUrl.replace(\"www.\", \"\");\n+      let mapUrl = req.body.prompt && allowExternalLinks\n+        ? `${req.body.prompt} ${urlWithoutWww}`\n+        : req.body.prompt ? `${req.body.prompt} site:${urlWithoutWww}`\n+        : `site:${urlWithoutWww}`;\n+\n+      const mapResults = await getMapResults({\n+        url: baseUrl,\n+        search: req.body.prompt,\n+        teamId: req.auth.team_id,\n+        plan: req.auth.plan,\n+        allowExternalLinks,\n+        origin: req.body.origin,\n+        limit: req.body.limit,\n+        // If we're self-hosted, we don't want to ignore the sitemap, due to our fire-engine mapping\n+        ignoreSitemap: !selfHosted ? true : false,\n+        includeMetadata: true,\n+        includeSubdomains: req.body.includeSubdomains,\n+      });\n+\n+      let mappedLinks = mapResults.links as MapDocument[];\n+      // Limit number of links to MAX_EXTRACT_LIMIT\n+      mappedLinks = mappedLinks.slice(0, MAX_EXTRACT_LIMIT);\n+\n+      let mappedLinksRerank = mappedLinks.map(x => `url: ${x.url}, title: ${x.title}, description: ${x.description}`);\n+      \n+      // Filter by path prefix if present\n+      // wrong\n+      // if (pathPrefix) {\n+      //   mappedLinks = mappedLinks.filter(x => x.url && x.url.includes(`/${pathPrefix}/`));\n+      // }\n+\n+      if (req.body.prompt) {\n+        const linksAndScores : { link: string, linkWithContext: string, score: number, originalIndex: number }[] = await performRanking(mappedLinksRerank, mappedLinks.map(l => l.url), mapUrl);\n+        mappedLinks = linksAndScores\n+          .filter(x => x.score > SCORE_THRESHOLD)\n+          .map(x => mappedLinks.find(link => link.url === x.link))\n+          .filter((x): x is MapDocument => x !== undefined && x.url !== undefined && !isUrlBlocked(x.url))\n+          .slice(0, MAX_RANKING_LIMIT);\n+        // console.log(\"linksAndScores\", linksAndScores);\n+        // console.log(\"linksAndScores\", linksAndScores.length);\n+      }\n+\n+      return mappedLinks.map(x => x.url) as string[];\n+\n+    } else {\n+      // Handle direct URLs without glob pattern\n+      if (!isUrlBlocked(url)) {\n+        return [url];\n+      }\n+      return [];\n+    }\n+  });\n+\n+  // Wait for all URL processing to complete and flatten results\n+  const processedUrls = await Promise.all(urlPromises);\n+  links.push(...processedUrls.flat());\n+\n+  // console.log(\"links\", links.length);\n+  // Scrape all links in parallel\n+  const scrapePromises = links.map(async (url) => {\n+    const origin = req.body.origin || \"api\";\n+    const timeout = req.body.timeout ?? 30000;",
        "comment_created_at": "2024-11-25T03:40:49+00:00",
        "comment_author": "nickscamara",
        "comment_body": "Done!",
        "pr_file_module": null
      }
    ]
  }
]