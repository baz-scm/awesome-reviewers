[
  {
    "discussion_id": "2298528223",
    "pr_number": 4316,
    "pr_file": "crates/goose/src/providers/bedrock.rs",
    "created_at": "2025-08-25T16:12:19+00:00",
    "commented_code": "pub const BEDROCK_KNOWN_MODELS: &[&str] = &[\n     \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n     \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n+    \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n+    \"anthropic.claude-sonnet-4-20250514-v1:0\",\n+    \"anthropic.claude-sonnet-4-20250514-v1:0\",",
    "repo_full_name": "block/goose",
    "discussion_comments": [
      {
        "comment_id": "2298528223",
        "repo_full_name": "block/goose",
        "pr_number": 4316,
        "pr_file": "crates/goose/src/providers/bedrock.rs",
        "discussion_id": "2298528223",
        "commented_code": "@@ -27,13 +27,23 @@ pub const BEDROCK_DEFAULT_MODEL: &str = \"anthropic.claude-3-5-sonnet-20240620-v1\n pub const BEDROCK_KNOWN_MODELS: &[&str] = &[\n     \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n     \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n+    \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n+    \"anthropic.claude-sonnet-4-20250514-v1:0\",\n+    \"anthropic.claude-sonnet-4-20250514-v1:0\",",
        "comment_created_at": "2025-08-25T16:12:19+00:00",
        "comment_author": "DOsinga",
        "comment_body": "maybe change the default model too?",
        "pr_file_module": null
      },
      {
        "comment_id": "2298558314",
        "repo_full_name": "block/goose",
        "pr_number": 4316,
        "pr_file": "crates/goose/src/providers/bedrock.rs",
        "discussion_id": "2298528223",
        "commented_code": "@@ -27,13 +27,23 @@ pub const BEDROCK_DEFAULT_MODEL: &str = \"anthropic.claude-3-5-sonnet-20240620-v1\n pub const BEDROCK_KNOWN_MODELS: &[&str] = &[\n     \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n     \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n+    \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n+    \"anthropic.claude-sonnet-4-20250514-v1:0\",\n+    \"anthropic.claude-sonnet-4-20250514-v1:0\",",
        "comment_created_at": "2025-08-25T16:27:25+00:00",
        "comment_author": "Dan-Wuensch",
        "comment_body": "Does the default Goose install script use latest instead of version pinning? \r\n\r\nIf so, I worry about disruption to existing users by changing the model since their prompts may work worse (or better!) on a new model. Could require tuning prompt(s) and running evals to adopt in some cases\r\n\r\nThat upgrade concern probably only applies to pipeline / automatic use cases not desktop CLI/GUI though. Idk what percentage of adoption that represents, and then of course Bedrock would only be a subset of that\r\n\r\nWhat do you think about me logging an Issue for the model update instead, to have that be a separate more visible change?",
        "pr_file_module": null
      },
      {
        "comment_id": "2298564824",
        "repo_full_name": "block/goose",
        "pr_number": 4316,
        "pr_file": "crates/goose/src/providers/bedrock.rs",
        "discussion_id": "2298528223",
        "commented_code": "@@ -27,13 +27,23 @@ pub const BEDROCK_DEFAULT_MODEL: &str = \"anthropic.claude-3-5-sonnet-20240620-v1\n pub const BEDROCK_KNOWN_MODELS: &[&str] = &[\n     \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n     \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n+    \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n+    \"anthropic.claude-sonnet-4-20250514-v1:0\",\n+    \"anthropic.claude-sonnet-4-20250514-v1:0\",",
        "comment_created_at": "2025-08-25T16:30:39+00:00",
        "comment_author": "DOsinga",
        "comment_body": "yeah, we're working on that. right now it is a bit of a mess which model a conversation will use. we're working on making this a real property of the session so that if you change the global settings, it won't change it for existing conversations.\r\n\r\neither way, changing the default here, should only change which model is suggested when you select the provider",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2289426129",
    "pr_number": 4228,
    "pr_file": "crates/goose/src/providers/anthropic.rs",
    "created_at": "2025-08-20T22:19:25+00:00",
    "commented_code": "impl_provider_default!(AnthropicProvider);\n \n impl AnthropicProvider {\n-    pub fn from_env(model: ModelConfig) -> Result<Self> {\n+    pub fn from_env(mut model: ModelConfig) -> Result<Self> {\n+        // Set the default fast model for Anthropic\n+        model.fast_model = Some(\"claude-3-5-haiku-latest\".to_string());",
    "repo_full_name": "block/goose",
    "discussion_comments": [
      {
        "comment_id": "2289426129",
        "repo_full_name": "block/goose",
        "pr_number": 4228,
        "pr_file": "crates/goose/src/providers/anthropic.rs",
        "discussion_id": "2289426129",
        "commented_code": "@@ -49,7 +49,10 @@ pub struct AnthropicProvider {\n impl_provider_default!(AnthropicProvider);\n \n impl AnthropicProvider {\n-    pub fn from_env(model: ModelConfig) -> Result<Self> {\n+    pub fn from_env(mut model: ModelConfig) -> Result<Self> {\n+        // Set the default fast model for Anthropic\n+        model.fast_model = Some(\"claude-3-5-haiku-latest\".to_string());",
        "comment_created_at": "2025-08-20T22:19:25+00:00",
        "comment_author": "DOsinga",
        "comment_body": "make that into a constant on top - ideally we'd do this in a more elegant way, but let's do that later.  don't make this mutable though, but have something like model = model.with_fast(CONSTANT) - possibly also check whether that is already set and don't overwrite it",
        "pr_file_module": null
      },
      {
        "comment_id": "2289794003",
        "repo_full_name": "block/goose",
        "pr_number": 4228,
        "pr_file": "crates/goose/src/providers/anthropic.rs",
        "discussion_id": "2289426129",
        "commented_code": "@@ -49,7 +49,10 @@ pub struct AnthropicProvider {\n impl_provider_default!(AnthropicProvider);\n \n impl AnthropicProvider {\n-    pub fn from_env(model: ModelConfig) -> Result<Self> {\n+    pub fn from_env(mut model: ModelConfig) -> Result<Self> {\n+        // Set the default fast model for Anthropic\n+        model.fast_model = Some(\"claude-3-5-haiku-latest\".to_string());",
        "comment_created_at": "2025-08-21T03:59:55+00:00",
        "comment_author": "katzdave",
        "comment_body": "done + applied to openAI/google",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2241218311",
    "pr_number": 3718,
    "pr_file": "crates/goose/src/model.rs",
    "created_at": "2025-07-29T23:27:00+00:00",
    "commented_code": "use once_cell::sync::Lazy;\n use serde::{Deserialize, Serialize};\n-use std::collections::HashMap;\n+use thiserror::Error;\n \n const DEFAULT_CONTEXT_LIMIT: usize = 128_000;\n \n-// Define the model limits as a static HashMap for reuse\n-static MODEL_SPECIFIC_LIMITS: Lazy<HashMap<&'static str, usize>> = Lazy::new(|| {\n-    let mut map = HashMap::new();\n-    // OpenAI models, https://platform.openai.com/docs/models#models-overview\n-    map.insert(\"gpt-4o\", 128_000);\n-    map.insert(\"gpt-4-turbo\", 128_000);\n-    map.insert(\"o3\", 200_000);\n-    map.insert(\"o3-mini\", 200_000);\n-    map.insert(\"o4-mini\", 200_000);\n-    map.insert(\"gpt-4.1\", 1_000_000);\n-    map.insert(\"gpt-4-1\", 1_000_000);\n-\n-    // Anthropic models, https://docs.anthropic.com/en/docs/about-claude/models\n-    map.insert(\"claude\", 200_000);\n-\n-    // Google models, https://ai.google/get-started/our-models/\n-    map.insert(\"gemini-2.5\", 1_000_000);\n-    map.insert(\"gemini-2-5\", 1_000_000);\n-\n-    // Meta Llama models, https://github.com/meta-llama/llama-models/tree/main?tab=readme-ov-file#llama-models-1\n-    map.insert(\"llama3.2\", 128_000);\n-    map.insert(\"llama3.3\", 128_000);\n-\n-    // x.ai Grok models, https://docs.x.ai/docs/overview\n-    map.insert(\"grok\", 131_072);\n-\n-    // Groq models, https://console.groq.com/docs/models\n-    map.insert(\"gemma2-9b\", 8_192);\n-    map.insert(\"kimi-k2\", 131_072);\n-    map.insert(\"qwen3-32b\", 131_072);\n-    map.insert(\"grok-3\", 131_072);\n-    map.insert(\"grok-4\", 256_000); // 256K\n-    map.insert(\"qwen3-coder\", 262_144); // 262K\n-\n-    map\n+#[derive(Error, Debug)]\n+pub enum ConfigError {\n+    #[error(\"Environment variable '{0}' not found\")]\n+    EnvVarMissing(String),\n+    #[error(\"Invalid value for '{0}': '{1}' - {2}\")]\n+    InvalidValue(String, String, String),\n+    #[error(\"Value for '{0}' is out of valid range: {1}\")]\n+    InvalidRange(String, String),\n+}\n+\n+static MODEL_SPECIFIC_LIMITS: Lazy<Vec<(&'static str, usize)>> = Lazy::new(|| {",
    "repo_full_name": "block/goose",
    "discussion_comments": [
      {
        "comment_id": "2241218311",
        "repo_full_name": "block/goose",
        "pr_number": 3718,
        "pr_file": "crates/goose/src/model.rs",
        "discussion_id": "2241218311",
        "commented_code": "@@ -1,323 +1,346 @@\n use once_cell::sync::Lazy;\n use serde::{Deserialize, Serialize};\n-use std::collections::HashMap;\n+use thiserror::Error;\n \n const DEFAULT_CONTEXT_LIMIT: usize = 128_000;\n \n-// Define the model limits as a static HashMap for reuse\n-static MODEL_SPECIFIC_LIMITS: Lazy<HashMap<&'static str, usize>> = Lazy::new(|| {\n-    let mut map = HashMap::new();\n-    // OpenAI models, https://platform.openai.com/docs/models#models-overview\n-    map.insert(\"gpt-4o\", 128_000);\n-    map.insert(\"gpt-4-turbo\", 128_000);\n-    map.insert(\"o3\", 200_000);\n-    map.insert(\"o3-mini\", 200_000);\n-    map.insert(\"o4-mini\", 200_000);\n-    map.insert(\"gpt-4.1\", 1_000_000);\n-    map.insert(\"gpt-4-1\", 1_000_000);\n-\n-    // Anthropic models, https://docs.anthropic.com/en/docs/about-claude/models\n-    map.insert(\"claude\", 200_000);\n-\n-    // Google models, https://ai.google/get-started/our-models/\n-    map.insert(\"gemini-2.5\", 1_000_000);\n-    map.insert(\"gemini-2-5\", 1_000_000);\n-\n-    // Meta Llama models, https://github.com/meta-llama/llama-models/tree/main?tab=readme-ov-file#llama-models-1\n-    map.insert(\"llama3.2\", 128_000);\n-    map.insert(\"llama3.3\", 128_000);\n-\n-    // x.ai Grok models, https://docs.x.ai/docs/overview\n-    map.insert(\"grok\", 131_072);\n-\n-    // Groq models, https://console.groq.com/docs/models\n-    map.insert(\"gemma2-9b\", 8_192);\n-    map.insert(\"kimi-k2\", 131_072);\n-    map.insert(\"qwen3-32b\", 131_072);\n-    map.insert(\"grok-3\", 131_072);\n-    map.insert(\"grok-4\", 256_000); // 256K\n-    map.insert(\"qwen3-coder\", 262_144); // 262K\n-\n-    map\n+#[derive(Error, Debug)]\n+pub enum ConfigError {\n+    #[error(\"Environment variable '{0}' not found\")]\n+    EnvVarMissing(String),\n+    #[error(\"Invalid value for '{0}': '{1}' - {2}\")]\n+    InvalidValue(String, String, String),\n+    #[error(\"Value for '{0}' is out of valid range: {1}\")]\n+    InvalidRange(String, String),\n+}\n+\n+static MODEL_SPECIFIC_LIMITS: Lazy<Vec<(&'static str, usize)>> = Lazy::new(|| {",
        "comment_created_at": "2025-07-29T23:27:00+00:00",
        "comment_author": "michaelneale",
        "comment_body": "are we likely to replace this one day with something provider driven? If not I wonder if we should put this in token_limits.rs so it is easy to find, but doesn't really matter either way. \r\n(some providers can furnish the limits - like openrouter can, but not sure about the general case)",
        "pr_file_module": null
      },
      {
        "comment_id": "2242053412",
        "repo_full_name": "block/goose",
        "pr_number": 3718,
        "pr_file": "crates/goose/src/model.rs",
        "discussion_id": "2241218311",
        "commented_code": "@@ -1,323 +1,346 @@\n use once_cell::sync::Lazy;\n use serde::{Deserialize, Serialize};\n-use std::collections::HashMap;\n+use thiserror::Error;\n \n const DEFAULT_CONTEXT_LIMIT: usize = 128_000;\n \n-// Define the model limits as a static HashMap for reuse\n-static MODEL_SPECIFIC_LIMITS: Lazy<HashMap<&'static str, usize>> = Lazy::new(|| {\n-    let mut map = HashMap::new();\n-    // OpenAI models, https://platform.openai.com/docs/models#models-overview\n-    map.insert(\"gpt-4o\", 128_000);\n-    map.insert(\"gpt-4-turbo\", 128_000);\n-    map.insert(\"o3\", 200_000);\n-    map.insert(\"o3-mini\", 200_000);\n-    map.insert(\"o4-mini\", 200_000);\n-    map.insert(\"gpt-4.1\", 1_000_000);\n-    map.insert(\"gpt-4-1\", 1_000_000);\n-\n-    // Anthropic models, https://docs.anthropic.com/en/docs/about-claude/models\n-    map.insert(\"claude\", 200_000);\n-\n-    // Google models, https://ai.google/get-started/our-models/\n-    map.insert(\"gemini-2.5\", 1_000_000);\n-    map.insert(\"gemini-2-5\", 1_000_000);\n-\n-    // Meta Llama models, https://github.com/meta-llama/llama-models/tree/main?tab=readme-ov-file#llama-models-1\n-    map.insert(\"llama3.2\", 128_000);\n-    map.insert(\"llama3.3\", 128_000);\n-\n-    // x.ai Grok models, https://docs.x.ai/docs/overview\n-    map.insert(\"grok\", 131_072);\n-\n-    // Groq models, https://console.groq.com/docs/models\n-    map.insert(\"gemma2-9b\", 8_192);\n-    map.insert(\"kimi-k2\", 131_072);\n-    map.insert(\"qwen3-32b\", 131_072);\n-    map.insert(\"grok-3\", 131_072);\n-    map.insert(\"grok-4\", 256_000); // 256K\n-    map.insert(\"qwen3-coder\", 262_144); // 262K\n-\n-    map\n+#[derive(Error, Debug)]\n+pub enum ConfigError {\n+    #[error(\"Environment variable '{0}' not found\")]\n+    EnvVarMissing(String),\n+    #[error(\"Invalid value for '{0}': '{1}' - {2}\")]\n+    InvalidValue(String, String, String),\n+    #[error(\"Value for '{0}' is out of valid range: {1}\")]\n+    InvalidRange(String, String),\n+}\n+\n+static MODEL_SPECIFIC_LIMITS: Lazy<Vec<(&'static str, usize)>> = Lazy::new(|| {",
        "comment_created_at": "2025-07-30T09:25:55+00:00",
        "comment_author": "DOsinga",
        "comment_body": "yeah, I think this goes back to the whole thinking around models <-> providers. in some ideal world I would say we have a models folder which defines all the models and their properties and then providers indicate which of those models they support. llama3-50b-9f has the same properties independent of the provider. I think",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2237139271",
    "pr_number": 3676,
    "pr_file": "crates/goose/src/providers/groq.rs",
    "created_at": "2025-07-28T16:16:30+00:00",
    "commented_code": "use url::Url;\n \n pub const GROQ_API_HOST: &str = \"https://api.groq.com\";\n-pub const GROQ_DEFAULT_MODEL: &str = \"llama-3.3-70b-versatile\";\n-pub const GROQ_KNOWN_MODELS: &[&str] = &[\"gemma2-9b-it\", \"llama-3.3-70b-versatile\"];\n+pub const GROQ_DEFAULT_MODEL: &str = \"moonshotai/kimi-k2-instruct\";\n+pub const GROQ_KNOWN_MODELS: &[&str] = &[\n+    \"gemma2-9b-it\",\n+    \"llama-3.3-70b-versatile\",\n+    \"moonshotai/kimi-k2-instruct\",\n+    \"qwen/qwen3-32b\",\n+];",
    "repo_full_name": "block/goose",
    "discussion_comments": [
      {
        "comment_id": "2237139271",
        "repo_full_name": "block/goose",
        "pr_number": 3676,
        "pr_file": "crates/goose/src/providers/groq.rs",
        "discussion_id": "2237139271",
        "commented_code": "@@ -13,8 +13,13 @@ use std::time::Duration;\n use url::Url;\n \n pub const GROQ_API_HOST: &str = \"https://api.groq.com\";\n-pub const GROQ_DEFAULT_MODEL: &str = \"llama-3.3-70b-versatile\";\n-pub const GROQ_KNOWN_MODELS: &[&str] = &[\"gemma2-9b-it\", \"llama-3.3-70b-versatile\"];\n+pub const GROQ_DEFAULT_MODEL: &str = \"moonshotai/kimi-k2-instruct\";\n+pub const GROQ_KNOWN_MODELS: &[&str] = &[\n+    \"gemma2-9b-it\",\n+    \"llama-3.3-70b-versatile\",\n+    \"moonshotai/kimi-k2-instruct\",\n+    \"qwen/qwen3-32b\",\n+];",
        "comment_created_at": "2025-07-28T16:16:30+00:00",
        "comment_author": "DOsinga",
        "comment_body": "make sure these models also have their token limits defined in models.rs",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2237157984",
    "pr_number": 3675,
    "pr_file": "crates/goose-cli/src/session/output.rs",
    "created_at": "2025-07-28T16:21:26+00:00",
    "commented_code": "input_tokens: usize,\n     output_tokens: usize,\n ) -> Option<f64> {\n+    // For OpenRouter, parse the model name to extract real provider/model\n+    let openrouter_data = if provider == \"openrouter\" {\n+        parse_model_id(model)\n+    } else {\n+        None\n+    };\n+\n+    let (provider_to_use, model_to_use) = match &openrouter_data {\n+        Some((real_provider, real_model)) => (real_provider.as_str(), real_model.as_str()),\n+        None => (provider, model),\n+    };",
    "repo_full_name": "block/goose",
    "discussion_comments": [
      {
        "comment_id": "2237157984",
        "repo_full_name": "block/goose",
        "pr_number": 3675,
        "pr_file": "crates/goose-cli/src/session/output.rs",
        "discussion_id": "2237157984",
        "commented_code": "@@ -732,9 +733,21 @@ async fn estimate_cost_usd(\n     input_tokens: usize,\n     output_tokens: usize,\n ) -> Option<f64> {\n+    // For OpenRouter, parse the model name to extract real provider/model\n+    let openrouter_data = if provider == \"openrouter\" {\n+        parse_model_id(model)\n+    } else {\n+        None\n+    };\n+\n+    let (provider_to_use, model_to_use) = match &openrouter_data {\n+        Some((real_provider, real_model)) => (real_provider.as_str(), real_model.as_str()),\n+        None => (provider, model),\n+    };",
        "comment_created_at": "2025-07-28T16:21:26+00:00",
        "comment_author": "DOsinga",
        "comment_body": "how about:\r\n\r\n```\r\n    let (provider_to_use, model_to_use) = if provider == \"openrouter\" {\r\n        parse_model_id(model)\r\n            .map(|(real_provider, real_model)| (real_provider.as_str(), real_model.as_str()))\r\n            .unwrap_or((provider, model))\r\n    } else {\r\n        (provider, model)\r\n    };\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2237956353",
        "repo_full_name": "block/goose",
        "pr_number": 3675,
        "pr_file": "crates/goose-cli/src/session/output.rs",
        "discussion_id": "2237157984",
        "commented_code": "@@ -732,9 +733,21 @@ async fn estimate_cost_usd(\n     input_tokens: usize,\n     output_tokens: usize,\n ) -> Option<f64> {\n+    // For OpenRouter, parse the model name to extract real provider/model\n+    let openrouter_data = if provider == \"openrouter\" {\n+        parse_model_id(model)\n+    } else {\n+        None\n+    };\n+\n+    let (provider_to_use, model_to_use) = match &openrouter_data {\n+        Some((real_provider, real_model)) => (real_provider.as_str(), real_model.as_str()),\n+        None => (provider, model),\n+    };",
        "comment_created_at": "2025-07-28T22:17:00+00:00",
        "comment_author": "praneeth-ovckd",
        "comment_body": "Won't this lead to a dangling reference?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160061939",
    "pr_number": 3021,
    "pr_file": "crates/goose/src/providers/openrouter.rs",
    "created_at": "2025-06-21T15:13:15+00:00",
    "commented_code": "// OpenRouter can run many models, we suggest the default\n pub const OPENROUTER_KNOWN_MODELS: &[&str] = &[\n-    \"anthropic/claude-3.5-sonnet\",\n-    \"anthropic/claude-3.7-sonnet\",\n-    \"google/gemini-2.5-pro-exp-03-25:free\",\n-    \"deepseek/deepseek-r1\",\n+    \"anthropic/claude-sonnet-4\",\n+    \"google/gemini-2.5-pro-preview\"\n+    \"google/gemini-2.0-flash-001\",\n+    \"deepseek/deepseek-r1-0528:free\",\n ];",
    "repo_full_name": "block/goose",
    "discussion_comments": [
      {
        "comment_id": "2160061939",
        "repo_full_name": "block/goose",
        "pr_number": 3021,
        "pr_file": "crates/goose/src/providers/openrouter.rs",
        "discussion_id": "2160061939",
        "commented_code": "@@ -21,10 +21,10 @@ pub const OPENROUTER_MODEL_PREFIX_ANTHROPIC: &str = \"anthropic\";\n \n // OpenRouter can run many models, we suggest the default\n pub const OPENROUTER_KNOWN_MODELS: &[&str] = &[\n-    \"anthropic/claude-3.5-sonnet\",\n-    \"anthropic/claude-3.7-sonnet\",\n-    \"google/gemini-2.5-pro-exp-03-25:free\",\n-    \"deepseek/deepseek-r1\",\n+    \"anthropic/claude-sonnet-4\",\n+    \"google/gemini-2.5-pro-preview\"\n+    \"google/gemini-2.0-flash-001\",\n+    \"deepseek/deepseek-r1-0528:free\",\n ];",
        "comment_created_at": "2025-06-21T15:13:15+00:00",
        "comment_author": "blackgirlbytes",
        "comment_body": "thanks for updating! I'm happy about adding some models, but some of these we dont need to remove. For example, yes let's remove \"google/gemini-2.5-pro-exp-03-25:free\"\r\n\r\nBut, we actually use anthropic/claude-sonnet-3.5 and anthropic/claude-sonnet-3.7 in our meetups and hackathons, so I would prefer to keep those included. You can still add claude-sonnet-4.\r\n\r\n",
        "pr_file_module": null
      }
    ]
  }
]