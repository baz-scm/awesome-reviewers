[
  {
    "discussion_id": "2177535538",
    "pr_number": 107305,
    "pr_file": "pkg/storage/unified/resource/storage_backend.go",
    "created_at": "2025-07-01T12:54:49+00:00",
    "commented_code": "+package resource\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"encoding/json\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"math/rand/v2\"\n+\t\"net/http\"\n+\t\"sort\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/bwmarrin/snowflake\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/utils\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+)\n+\n+const (\n+\tdefaultListBufferSize = 100\n+)\n+\n+// Unified storage backend based on KV storage.\n+type kvStorageBackend struct {\n+\tsnowflake  *snowflake.Node\n+\tkv         KV\n+\tdataStore  *dataStore\n+\tmetaStore  *metadataStore\n+\teventStore *eventStore\n+\tnotifier   *notifier\n+\tbuilder    DocumentBuilder\n+}\n+\n+var _ StorageBackend = &kvStorageBackend{}\n+\n+func NewkvStorageBackend(kv KV) *kvStorageBackend {\n+\ts, err := snowflake.NewNode(rand.Int64N(1024))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\teventStore := newEventStore(kv)\n+\treturn &kvStorageBackend{\n+\t\tkv:         kv,\n+\t\tdataStore:  newDataStore(kv),\n+\t\tmetaStore:  newMetadataStore(kv),\n+\t\teventStore: eventStore,\n+\t\tnotifier:   newNotifier(eventStore, notifierOptions{}),\n+\t\tsnowflake:  s,\n+\t\tbuilder:    StandardDocumentBuilder(), // For now we use the standard document builder.\n+\t}\n+}\n+\n+// WriteEvent writes a resource event (create/update/delete) to the storage backend.\n+func (k *kvStorageBackend) WriteEvent(ctx context.Context, event WriteEvent) (int64, error) {\n+\tif err := event.Validate(); err != nil {\n+\t\treturn 0, fmt.Errorf(\"invalid event: %w\", err)\n+\t}\n+\trv := k.snowflake.Generate().Int64()\n+\n+\t// Write data.\n+\tvar action DataAction\n+\tswitch event.Type {\n+\tcase resourcepb.WatchEvent_ADDED:\n+\t\taction = DataActionCreated\n+\t\t// Check if resource already exists for create operations\n+\t\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\t\tNamespace: event.Key.Namespace,\n+\t\t\tGroup:     event.Key.Group,\n+\t\t\tResource:  event.Key.Resource,\n+\t\t\tName:      event.Key.Name,\n+\t\t})\n+\t\tif err == nil {\n+\t\t\t// Resource exists, return already exists error\n+\t\t\treturn 0, ErrResourceAlreadyExists\n+\t\t}\n+\t\tif !errors.Is(err, ErrNotFound) {\n+\t\t\t// Some other error occurred\n+\t\t\treturn 0, fmt.Errorf(\"failed to check if resource exists: %w\", err)\n+\t\t}\n+\tcase resourcepb.WatchEvent_MODIFIED:\n+\t\taction = DataActionUpdated\n+\tcase resourcepb.WatchEvent_DELETED:\n+\t\taction = DataActionDeleted\n+\tdefault:\n+\t\treturn 0, fmt.Errorf(\"invalid event type: %d\", event.Type)\n+\t}\n+\n+\t// Build the search document\n+\tdoc, err := k.builder.BuildDocument(ctx, event.Key, rv, event.Value)\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to build document: %w\", err)\n+\t}\n+\n+\t// Write the data\n+\terr = k.dataStore.Save(ctx, DataKey{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t}, io.NopCloser(bytes.NewReader(event.Value)))\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write data: %w\", err)\n+\t}\n+\n+\t// Write metadata\n+\terr = k.metaStore.Save(ctx, MetaDataObj{\n+\t\tKey: MetaDataKey{\n+\t\t\tNamespace:       event.Key.Namespace,\n+\t\t\tGroup:           event.Key.Group,\n+\t\t\tResource:        event.Key.Resource,\n+\t\t\tName:            event.Key.Name,\n+\t\t\tResourceVersion: rv,\n+\t\t\tAction:          action,\n+\t\t\tFolder:          event.Object.GetFolder(),\n+\t\t},\n+\t\tValue: MetaData{\n+\t\t\tIndexableDocument: *doc,\n+\t\t},\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write metadata: %w\", err)\n+\t}\n+\n+\t// Write event\n+\terr = k.eventStore.Save(ctx, Event{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t\tFolder:          event.Object.GetFolder(),\n+\t\tPreviousRV:      event.PreviousRV,\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to save event: %w\", err)\n+\t}\n+\treturn rv, nil\n+}\n+\n+func (k *kvStorageBackend) ReadResource(ctx context.Context, req *resourcepb.ReadRequest) *BackendReadResponse {\n+\tif req.Key == nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusBadRequest, Message: \"missing key\"}}\n+\t}\n+\tmeta, err := k.metaStore.GetResourceKeyAtRevision(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Key.Namespace,\n+\t\tGroup:     req.Key.Group,\n+\t\tResource:  req.Key.Resource,\n+\t\tName:      req.Key.Name,\n+\t}, req.ResourceVersion)\n+\tif errors.Is(err, ErrNotFound) {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusNotFound, Message: \"not found\"}}\n+\t} else if err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tdata, err := k.dataStore.Get(ctx, DataKey{\n+\t\tNamespace:       req.Key.Namespace,\n+\t\tGroup:           req.Key.Group,\n+\t\tResource:        req.Key.Resource,\n+\t\tName:            req.Key.Name,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tAction:          meta.Action,\n+\t})\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tvalue, err := io.ReadAll(data)\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\treturn &BackendReadResponse{\n+\t\tKey:             req.Key,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tValue:           value,\n+\t\tFolder:          meta.Folder,\n+\t}\n+}\n+\n+// // ListIterator returns an iterator for listing resources.\n+func (k *kvStorageBackend) ListIterator(ctx context.Context, req *resourcepb.ListRequest, cb func(ListIterator) error) (int64, error) {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn 0, fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\t// Parse continue token if provided\n+\toffset := int64(0)\n+\tresourceVersion := req.ResourceVersion\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\toffset = token.StartOffset\n+\t\tresourceVersion = token.ResourceVersion\n+\t}\n+\n+\t// We set the listRV to the current time.\n+\tlistRV := k.snowflake.Generate().Int64()\n+\tif resourceVersion > 0 {\n+\t\tlistRV = resourceVersion\n+\t}\n+\n+\t// Fetch the latest objects\n+\tkeys := make([]MetaDataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\tidx := 0\n+\tfor metaKey, err := range k.metaStore.ListResourceKeysAtRevision(ctx, MetaListRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t}, resourceVersion) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\t// Skip the first offset items. This is not efficient, but it's a simple way to implement it for now.\n+\t\tif idx < int(offset) {\n+\t\t\tidx++\n+\t\t\tcontinue\n+\t\t}\n+\t\tkeys = append(keys, metaKey)\n+\t\t// Only fetch the first limit items + 1 to get the next token.\n+\t\tif len(keys) >= int(req.Limit+1) {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\titer := kvListIterator{\n+\t\tkeys:         keys,\n+\t\tcurrentIndex: -1,\n+\t\tctx:          ctx,\n+\t\tlistRV:       listRV,\n+\t\toffset:       offset,\n+\t\tdataStore:    k.dataStore,\n+\t}\n+\terr := cb(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvListIterator implements ListIterator for KV storage\n+type kvListIterator struct {\n+\tctx          context.Context\n+\tkeys         []MetaDataKey\n+\tcurrentIndex int\n+\tdataStore    *dataStore\n+\tlistRV       int64\n+\toffset       int64\n+\n+\t// current\n+\trv    int64\n+\terr   error\n+\tvalue []byte\n+}\n+\n+func (i *kvListIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\ti.rv, i.err = i.keys[i.currentIndex].ResourceVersion, nil\n+\n+\tdata, err := i.dataStore.Get(i.ctx, DataKey{\n+\t\tNamespace:       i.keys[i.currentIndex].Namespace,\n+\t\tGroup:           i.keys[i.currentIndex].Group,\n+\t\tResource:        i.keys[i.currentIndex].Resource,\n+\t\tName:            i.keys[i.currentIndex].Name,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tAction:          i.keys[i.currentIndex].Action,\n+\t})\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\ti.value, i.err = io.ReadAll(data)\n+\tif i.err != nil {\n+\t\treturn false\n+\t}\n+\n+\t// increment the offset\n+\ti.offset++\n+\n+\treturn true\n+}\n+\n+func (i *kvListIterator) Error() error {\n+\treturn nil\n+}\n+\n+func (i *kvListIterator) ContinueToken() string {\n+\treturn ContinueToken{\n+\t\tStartOffset:     i.offset,\n+\t\tResourceVersion: i.listRV,\n+\t}.String()\n+}\n+\n+func (i *kvListIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvListIterator) Namespace() string {\n+\treturn i.keys[i.currentIndex].Namespace\n+}\n+\n+func (i *kvListIterator) Name() string {\n+\treturn i.keys[i.currentIndex].Name\n+}\n+\n+func (i *kvListIterator) Folder() string {\n+\treturn i.keys[i.currentIndex].Folder\n+}\n+\n+func (i *kvListIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+func validateListHistoryRequest(req *resourcepb.ListRequest) error {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\tkey := req.Options.Key\n+\tif key.Group == \"\" {\n+\t\treturn fmt.Errorf(\"group is required\")\n+\t}\n+\tif key.Resource == \"\" {\n+\t\treturn fmt.Errorf(\"resource is required\")\n+\t}\n+\tif key.Namespace == \"\" {\n+\t\treturn fmt.Errorf(\"namespace is required\")\n+\t}\n+\tif key.Name == \"\" {\n+\t\treturn fmt.Errorf(\"name is required\")\n+\t}\n+\treturn nil\n+}\n+\n+// filterHistoryKeysByVersion filters history keys based on version match criteria\n+func filterHistoryKeysByVersion(historyKeys []DataKey, req *resourcepb.ListRequest) ([]DataKey, error) {\n+\tswitch req.GetVersionMatchV2() {\n+\tcase resourcepb.ResourceVersionMatchV2_Exact:\n+\t\tif req.ResourceVersion <= 0 {\n+\t\t\treturn nil, fmt.Errorf(\"expecting an explicit resource version query when using Exact matching\")\n+\t\t}\n+\t\tvar exactKeys []DataKey\n+\t\tfor _, key := range historyKeys {\n+\t\t\tif key.ResourceVersion == req.ResourceVersion {\n+\t\t\t\texactKeys = append(exactKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn exactKeys, nil\n+\tcase resourcepb.ResourceVersionMatchV2_NotOlderThan:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion >= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\tdefault:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion <= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\t}\n+\treturn historyKeys, nil\n+}\n+\n+// applyLiveHistoryFilter applies \"live\" history logic by ignoring events before the last delete\n+func applyLiveHistoryFilter(filteredKeys []DataKey, req *resourcepb.ListRequest) []DataKey {\n+\tuseLatestDeletionAsMinRV := req.ResourceVersion == 0 && req.Source != resourcepb.ListRequest_TRASH && req.GetVersionMatchV2() != resourcepb.ResourceVersionMatchV2_Exact\n+\tif !useLatestDeletionAsMinRV {\n+\t\treturn filteredKeys\n+\t}\n+\n+\tlatestDeleteRV := int64(0)\n+\tfor _, key := range filteredKeys {\n+\t\tif key.Action == DataActionDeleted && key.ResourceVersion > latestDeleteRV {\n+\t\t\tlatestDeleteRV = key.ResourceVersion\n+\t\t}\n+\t}\n+\tif latestDeleteRV > 0 {\n+\t\tvar liveKeys []DataKey\n+\t\tfor _, key := range filteredKeys {\n+\t\t\tif key.ResourceVersion > latestDeleteRV {\n+\t\t\t\tliveKeys = append(liveKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn liveKeys\n+\t}\n+\treturn filteredKeys\n+}\n+\n+// sortHistoryKeys sorts the history keys based on the sortAscending flag\n+func sortHistoryKeys(filteredKeys []DataKey, sortAscending bool) {\n+\tif sortAscending {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion < filteredKeys[j].ResourceVersion\n+\t\t})\n+\t} else {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion > filteredKeys[j].ResourceVersion\n+\t\t})\n+\t}\n+}\n+\n+// applyPagination filters keys based on pagination parameters\n+func applyPagination(keys []DataKey, lastSeenRV int64, sortAscending bool) []DataKey {\n+\tif lastSeenRV == 0 {\n+\t\treturn keys\n+\t}\n+\n+\tvar pagedKeys []DataKey\n+\tfor _, key := range keys {\n+\t\tif sortAscending && key.ResourceVersion > lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t} else if !sortAscending && key.ResourceVersion < lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t}\n+\t}\n+\treturn pagedKeys\n+}\n+\n+// ListHistory is like ListIterator, but it returns the history of a resource.\n+func (k *kvStorageBackend) ListHistory(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error) (int64, error) {\n+\tif err := validateListHistoryRequest(req); err != nil {\n+\t\treturn 0, err\n+\t}\n+\tkey := req.Options.Key\n+\t// Parse continue token if provided\n+\tlastSeenRV := int64(0)\n+\tsortAscending := req.GetVersionMatchV2() == resourcepb.ResourceVersionMatchV2_NotOlderThan\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\tlastSeenRV = token.ResourceVersion\n+\t\tsortAscending = token.SortAscending\n+\t}\n+\n+\t// Generate a new resource version for the list\n+\tlistRV := k.snowflake.Generate().Int64()\n+\n+\t// Get all history entries by iterating through datastore keys\n+\thistoryKeys := make([]DataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\n+\t// Use datastore.Keys to get all data keys for this specific resource\n+\tfor dataKey, err := range k.dataStore.Keys(ctx, ListRequestKey{\n+\t\tNamespace: key.Namespace,\n+\t\tGroup:     key.Group,\n+\t\tResource:  key.Resource,\n+\t\tName:      key.Name,\n+\t}) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\thistoryKeys = append(historyKeys, dataKey)\n+\t}\n+\n+\t// Check if context has been cancelled\n+\tif ctx.Err() != nil {\n+\t\treturn 0, ctx.Err()\n+\t}\n+\n+\t// Handle trash differently from regular history\n+\tif req.Source == resourcepb.ListRequest_TRASH {\n+\t\treturn k.processTrashEntries(ctx, req, fn, historyKeys, lastSeenRV, sortAscending, listRV)\n+\t}\n+\n+\t// Apply filtering based on version match\n+\tfilteredKeys, filterErr := filterHistoryKeysByVersion(historyKeys, req)\n+\tif filterErr != nil {\n+\t\treturn 0, filterErr\n+\t}\n+\n+\t// Apply \"live\" history logic: ignore events before the last delete\n+\tfilteredKeys = applyLiveHistoryFilter(filteredKeys, req)\n+\n+\t// Sort the entries if not already sorted correctly\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr := fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// processTrashEntries handles the special case of listing deleted items (trash)\n+func (k *kvStorageBackend) processTrashEntries(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error, historyKeys []DataKey, lastSeenRV int64, sortAscending bool, listRV int64) (int64, error) {\n+\t// Filter to only deleted entries\n+\tvar deletedKeys []DataKey\n+\tfor _, key := range historyKeys {\n+\t\tif key.Action == DataActionDeleted {\n+\t\t\tdeletedKeys = append(deletedKeys, key)\n+\t\t}\n+\t}\n+\n+\t// Check if the resource currently exists (is live)\n+\t// If it exists, don't return any trash entries\n+\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t})\n+\n+\tvar trashKeys []DataKey\n+\tif errors.Is(err, ErrNotFound) {\n+\t\t// Resource doesn't exist currently, so we can return the latest delete\n+\t\t// Find the latest delete event\n+\t\tvar latestDelete *DataKey\n+\t\tfor _, key := range deletedKeys {\n+\t\t\tif latestDelete == nil || key.ResourceVersion > latestDelete.ResourceVersion {\n+\t\t\t\tlatestDelete = &key\n+\t\t\t}\n+\t\t}\n+\t\tif latestDelete != nil {\n+\t\t\ttrashKeys = append(trashKeys, *latestDelete)\n+\t\t}\n+\t}\n+\t// If err != ErrNotFound, the resource exists, so no trash entries should be returned\n+\n+\t// Apply version filtering\n+\tfilteredKeys, err := filterHistoryKeysByVersion(trashKeys, req)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\t// Sort the entries\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr = fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvHistoryIterator implements ListIterator for KV storage history\n+type kvHistoryIterator struct {\n+\tctx           context.Context\n+\tkeys          []DataKey\n+\tcurrentIndex  int\n+\tlistRV        int64\n+\tsortAscending bool\n+\tdataStore     *dataStore\n+\n+\t// current\n+\trv     int64\n+\terr    error\n+\tvalue  []byte\n+\tfolder string\n+}\n+\n+func (i *kvHistoryIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\tkey := i.keys[i.currentIndex]\n+\ti.rv = key.ResourceVersion\n+\n+\t// Read the value from the ReadCloser\n+\tdata, err := i.dataStore.Get(i.ctx, key)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.value, err = io.ReadAll(data)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\t// Extract the folder from the meta data\n+\tpartial := &metav1.PartialObjectMetadata{}\n+\terr = json.Unmarshal(i.value, partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\tmeta, err := utils.MetaAccessor(partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.folder = meta.GetFolder()\n+\ti.err = nil\n+\n+\treturn true\n+}\n+\n+func (i *kvHistoryIterator) Error() error {\n+\treturn i.err\n+}\n+\n+func (i *kvHistoryIterator) ContinueToken() string {\n+\tif i.currentIndex < 0 || i.currentIndex >= len(i.keys) {\n+\t\treturn \"\"\n+\t}\n+\ttoken := ContinueToken{\n+\t\tStartOffset:     i.rv,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tSortAscending:   i.sortAscending,\n+\t}\n+\treturn token.String()\n+}\n+\n+func (i *kvHistoryIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvHistoryIterator) Namespace() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Namespace\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Name() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Name\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Folder() string {\n+\treturn i.folder\n+}\n+\n+func (i *kvHistoryIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+// WatchWriteEvents returns a channel that receives write events.\n+func (k *kvStorageBackend) WatchWriteEvents(ctx context.Context) (<-chan *WrittenEvent, error) {\n+\t// Create a channel to receive events\n+\tevents := make(chan *WrittenEvent, 10000) // TODO: make this configurable\n+\n+\tnotifierEvents := k.notifier.Watch(ctx, defaultWatchOptions())\n+\tgo func() {\n+\t\tfor event := range notifierEvents {\n+\t\t\t// fetch the data\n+\t\t\tdataReader, err := k.dataStore.Get(ctx, DataKey{\n+\t\t\t\tNamespace:       event.Namespace,\n+\t\t\t\tGroup:           event.Group,\n+\t\t\t\tResource:        event.Resource,\n+\t\t\t\tName:            event.Name,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tAction:          event.Action,\n+\t\t\t})\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tdata, err := io.ReadAll(dataReader)\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tvar t resourcepb.WatchEvent_Type\n+\t\t\tswitch event.Action {\n+\t\t\tcase DataActionCreated:\n+\t\t\t\tt = resourcepb.WatchEvent_ADDED\n+\t\t\tcase DataActionUpdated:\n+\t\t\t\tt = resourcepb.WatchEvent_MODIFIED\n+\t\t\tcase DataActionDeleted:\n+\t\t\t\tt = resourcepb.WatchEvent_DELETED\n+\t\t\t}\n+\n+\t\t\tevents <- &WrittenEvent{\n+\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\tNamespace: event.Namespace,\n+\t\t\t\t\tGroup:     event.Group,\n+\t\t\t\t\tResource:  event.Resource,\n+\t\t\t\t\tName:      event.Name,\n+\t\t\t\t},\n+\t\t\t\tType:            t,\n+\t\t\t\tFolder:          event.Folder,\n+\t\t\t\tValue:           data,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tPreviousRV:      event.PreviousRV,\n+\t\t\t\tTimestamp:       event.ResourceVersion / time.Second.Nanoseconds(), // convert to seconds\n+\t\t\t}\n+\t\t}\n+\t\tclose(events)\n+\t}()\n+\treturn events, nil\n+}\n+\n+// GetResourceStats returns resource stats within the storage backend.\n+// TODO: this isn't very efficient, we should use a more efficient algorithm.\n+func (k *kvStorageBackend) GetResourceStats(ctx context.Context, namespace string, minCount int) ([]ResourceStats, error) {\n+\tstats := make([]ResourceStats, 0)\n+\tres := make(map[string]map[string]bool)\n+\trvs := make(map[string]int64)\n+\n+\t// Use datastore.Keys to get all data keys for the namespace\n+\tfor dataKey, err := range k.dataStore.Keys(ctx, ListRequestKey{Namespace: namespace}) {\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tkey := fmt.Sprintf(\"%s/%s/%s\", dataKey.Namespace, dataKey.Group, dataKey.Resource)\n+\t\tif _, ok := res[key]; !ok {\n+\t\t\tres[key] = make(map[string]bool)\n+\t\t\trvs[key] = 1\n+\t\t}\n+\t\tres[key][dataKey.Name] = dataKey.Action != DataActionDeleted",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "2177535538",
        "repo_full_name": "grafana/grafana",
        "pr_number": 107305,
        "pr_file": "pkg/storage/unified/resource/storage_backend.go",
        "discussion_id": "2177535538",
        "commented_code": "@@ -0,0 +1,777 @@\n+package resource\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"encoding/json\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"math/rand/v2\"\n+\t\"net/http\"\n+\t\"sort\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/bwmarrin/snowflake\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/utils\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+)\n+\n+const (\n+\tdefaultListBufferSize = 100\n+)\n+\n+// Unified storage backend based on KV storage.\n+type kvStorageBackend struct {\n+\tsnowflake  *snowflake.Node\n+\tkv         KV\n+\tdataStore  *dataStore\n+\tmetaStore  *metadataStore\n+\teventStore *eventStore\n+\tnotifier   *notifier\n+\tbuilder    DocumentBuilder\n+}\n+\n+var _ StorageBackend = &kvStorageBackend{}\n+\n+func NewkvStorageBackend(kv KV) *kvStorageBackend {\n+\ts, err := snowflake.NewNode(rand.Int64N(1024))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\teventStore := newEventStore(kv)\n+\treturn &kvStorageBackend{\n+\t\tkv:         kv,\n+\t\tdataStore:  newDataStore(kv),\n+\t\tmetaStore:  newMetadataStore(kv),\n+\t\teventStore: eventStore,\n+\t\tnotifier:   newNotifier(eventStore, notifierOptions{}),\n+\t\tsnowflake:  s,\n+\t\tbuilder:    StandardDocumentBuilder(), // For now we use the standard document builder.\n+\t}\n+}\n+\n+// WriteEvent writes a resource event (create/update/delete) to the storage backend.\n+func (k *kvStorageBackend) WriteEvent(ctx context.Context, event WriteEvent) (int64, error) {\n+\tif err := event.Validate(); err != nil {\n+\t\treturn 0, fmt.Errorf(\"invalid event: %w\", err)\n+\t}\n+\trv := k.snowflake.Generate().Int64()\n+\n+\t// Write data.\n+\tvar action DataAction\n+\tswitch event.Type {\n+\tcase resourcepb.WatchEvent_ADDED:\n+\t\taction = DataActionCreated\n+\t\t// Check if resource already exists for create operations\n+\t\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\t\tNamespace: event.Key.Namespace,\n+\t\t\tGroup:     event.Key.Group,\n+\t\t\tResource:  event.Key.Resource,\n+\t\t\tName:      event.Key.Name,\n+\t\t})\n+\t\tif err == nil {\n+\t\t\t// Resource exists, return already exists error\n+\t\t\treturn 0, ErrResourceAlreadyExists\n+\t\t}\n+\t\tif !errors.Is(err, ErrNotFound) {\n+\t\t\t// Some other error occurred\n+\t\t\treturn 0, fmt.Errorf(\"failed to check if resource exists: %w\", err)\n+\t\t}\n+\tcase resourcepb.WatchEvent_MODIFIED:\n+\t\taction = DataActionUpdated\n+\tcase resourcepb.WatchEvent_DELETED:\n+\t\taction = DataActionDeleted\n+\tdefault:\n+\t\treturn 0, fmt.Errorf(\"invalid event type: %d\", event.Type)\n+\t}\n+\n+\t// Build the search document\n+\tdoc, err := k.builder.BuildDocument(ctx, event.Key, rv, event.Value)\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to build document: %w\", err)\n+\t}\n+\n+\t// Write the data\n+\terr = k.dataStore.Save(ctx, DataKey{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t}, io.NopCloser(bytes.NewReader(event.Value)))\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write data: %w\", err)\n+\t}\n+\n+\t// Write metadata\n+\terr = k.metaStore.Save(ctx, MetaDataObj{\n+\t\tKey: MetaDataKey{\n+\t\t\tNamespace:       event.Key.Namespace,\n+\t\t\tGroup:           event.Key.Group,\n+\t\t\tResource:        event.Key.Resource,\n+\t\t\tName:            event.Key.Name,\n+\t\t\tResourceVersion: rv,\n+\t\t\tAction:          action,\n+\t\t\tFolder:          event.Object.GetFolder(),\n+\t\t},\n+\t\tValue: MetaData{\n+\t\t\tIndexableDocument: *doc,\n+\t\t},\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write metadata: %w\", err)\n+\t}\n+\n+\t// Write event\n+\terr = k.eventStore.Save(ctx, Event{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t\tFolder:          event.Object.GetFolder(),\n+\t\tPreviousRV:      event.PreviousRV,\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to save event: %w\", err)\n+\t}\n+\treturn rv, nil\n+}\n+\n+func (k *kvStorageBackend) ReadResource(ctx context.Context, req *resourcepb.ReadRequest) *BackendReadResponse {\n+\tif req.Key == nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusBadRequest, Message: \"missing key\"}}\n+\t}\n+\tmeta, err := k.metaStore.GetResourceKeyAtRevision(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Key.Namespace,\n+\t\tGroup:     req.Key.Group,\n+\t\tResource:  req.Key.Resource,\n+\t\tName:      req.Key.Name,\n+\t}, req.ResourceVersion)\n+\tif errors.Is(err, ErrNotFound) {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusNotFound, Message: \"not found\"}}\n+\t} else if err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tdata, err := k.dataStore.Get(ctx, DataKey{\n+\t\tNamespace:       req.Key.Namespace,\n+\t\tGroup:           req.Key.Group,\n+\t\tResource:        req.Key.Resource,\n+\t\tName:            req.Key.Name,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tAction:          meta.Action,\n+\t})\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tvalue, err := io.ReadAll(data)\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\treturn &BackendReadResponse{\n+\t\tKey:             req.Key,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tValue:           value,\n+\t\tFolder:          meta.Folder,\n+\t}\n+}\n+\n+// // ListIterator returns an iterator for listing resources.\n+func (k *kvStorageBackend) ListIterator(ctx context.Context, req *resourcepb.ListRequest, cb func(ListIterator) error) (int64, error) {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn 0, fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\t// Parse continue token if provided\n+\toffset := int64(0)\n+\tresourceVersion := req.ResourceVersion\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\toffset = token.StartOffset\n+\t\tresourceVersion = token.ResourceVersion\n+\t}\n+\n+\t// We set the listRV to the current time.\n+\tlistRV := k.snowflake.Generate().Int64()\n+\tif resourceVersion > 0 {\n+\t\tlistRV = resourceVersion\n+\t}\n+\n+\t// Fetch the latest objects\n+\tkeys := make([]MetaDataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\tidx := 0\n+\tfor metaKey, err := range k.metaStore.ListResourceKeysAtRevision(ctx, MetaListRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t}, resourceVersion) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\t// Skip the first offset items. This is not efficient, but it's a simple way to implement it for now.\n+\t\tif idx < int(offset) {\n+\t\t\tidx++\n+\t\t\tcontinue\n+\t\t}\n+\t\tkeys = append(keys, metaKey)\n+\t\t// Only fetch the first limit items + 1 to get the next token.\n+\t\tif len(keys) >= int(req.Limit+1) {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\titer := kvListIterator{\n+\t\tkeys:         keys,\n+\t\tcurrentIndex: -1,\n+\t\tctx:          ctx,\n+\t\tlistRV:       listRV,\n+\t\toffset:       offset,\n+\t\tdataStore:    k.dataStore,\n+\t}\n+\terr := cb(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvListIterator implements ListIterator for KV storage\n+type kvListIterator struct {\n+\tctx          context.Context\n+\tkeys         []MetaDataKey\n+\tcurrentIndex int\n+\tdataStore    *dataStore\n+\tlistRV       int64\n+\toffset       int64\n+\n+\t// current\n+\trv    int64\n+\terr   error\n+\tvalue []byte\n+}\n+\n+func (i *kvListIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\ti.rv, i.err = i.keys[i.currentIndex].ResourceVersion, nil\n+\n+\tdata, err := i.dataStore.Get(i.ctx, DataKey{\n+\t\tNamespace:       i.keys[i.currentIndex].Namespace,\n+\t\tGroup:           i.keys[i.currentIndex].Group,\n+\t\tResource:        i.keys[i.currentIndex].Resource,\n+\t\tName:            i.keys[i.currentIndex].Name,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tAction:          i.keys[i.currentIndex].Action,\n+\t})\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\ti.value, i.err = io.ReadAll(data)\n+\tif i.err != nil {\n+\t\treturn false\n+\t}\n+\n+\t// increment the offset\n+\ti.offset++\n+\n+\treturn true\n+}\n+\n+func (i *kvListIterator) Error() error {\n+\treturn nil\n+}\n+\n+func (i *kvListIterator) ContinueToken() string {\n+\treturn ContinueToken{\n+\t\tStartOffset:     i.offset,\n+\t\tResourceVersion: i.listRV,\n+\t}.String()\n+}\n+\n+func (i *kvListIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvListIterator) Namespace() string {\n+\treturn i.keys[i.currentIndex].Namespace\n+}\n+\n+func (i *kvListIterator) Name() string {\n+\treturn i.keys[i.currentIndex].Name\n+}\n+\n+func (i *kvListIterator) Folder() string {\n+\treturn i.keys[i.currentIndex].Folder\n+}\n+\n+func (i *kvListIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+func validateListHistoryRequest(req *resourcepb.ListRequest) error {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\tkey := req.Options.Key\n+\tif key.Group == \"\" {\n+\t\treturn fmt.Errorf(\"group is required\")\n+\t}\n+\tif key.Resource == \"\" {\n+\t\treturn fmt.Errorf(\"resource is required\")\n+\t}\n+\tif key.Namespace == \"\" {\n+\t\treturn fmt.Errorf(\"namespace is required\")\n+\t}\n+\tif key.Name == \"\" {\n+\t\treturn fmt.Errorf(\"name is required\")\n+\t}\n+\treturn nil\n+}\n+\n+// filterHistoryKeysByVersion filters history keys based on version match criteria\n+func filterHistoryKeysByVersion(historyKeys []DataKey, req *resourcepb.ListRequest) ([]DataKey, error) {\n+\tswitch req.GetVersionMatchV2() {\n+\tcase resourcepb.ResourceVersionMatchV2_Exact:\n+\t\tif req.ResourceVersion <= 0 {\n+\t\t\treturn nil, fmt.Errorf(\"expecting an explicit resource version query when using Exact matching\")\n+\t\t}\n+\t\tvar exactKeys []DataKey\n+\t\tfor _, key := range historyKeys {\n+\t\t\tif key.ResourceVersion == req.ResourceVersion {\n+\t\t\t\texactKeys = append(exactKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn exactKeys, nil\n+\tcase resourcepb.ResourceVersionMatchV2_NotOlderThan:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion >= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\tdefault:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion <= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\t}\n+\treturn historyKeys, nil\n+}\n+\n+// applyLiveHistoryFilter applies \"live\" history logic by ignoring events before the last delete\n+func applyLiveHistoryFilter(filteredKeys []DataKey, req *resourcepb.ListRequest) []DataKey {\n+\tuseLatestDeletionAsMinRV := req.ResourceVersion == 0 && req.Source != resourcepb.ListRequest_TRASH && req.GetVersionMatchV2() != resourcepb.ResourceVersionMatchV2_Exact\n+\tif !useLatestDeletionAsMinRV {\n+\t\treturn filteredKeys\n+\t}\n+\n+\tlatestDeleteRV := int64(0)\n+\tfor _, key := range filteredKeys {\n+\t\tif key.Action == DataActionDeleted && key.ResourceVersion > latestDeleteRV {\n+\t\t\tlatestDeleteRV = key.ResourceVersion\n+\t\t}\n+\t}\n+\tif latestDeleteRV > 0 {\n+\t\tvar liveKeys []DataKey\n+\t\tfor _, key := range filteredKeys {\n+\t\t\tif key.ResourceVersion > latestDeleteRV {\n+\t\t\t\tliveKeys = append(liveKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn liveKeys\n+\t}\n+\treturn filteredKeys\n+}\n+\n+// sortHistoryKeys sorts the history keys based on the sortAscending flag\n+func sortHistoryKeys(filteredKeys []DataKey, sortAscending bool) {\n+\tif sortAscending {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion < filteredKeys[j].ResourceVersion\n+\t\t})\n+\t} else {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion > filteredKeys[j].ResourceVersion\n+\t\t})\n+\t}\n+}\n+\n+// applyPagination filters keys based on pagination parameters\n+func applyPagination(keys []DataKey, lastSeenRV int64, sortAscending bool) []DataKey {\n+\tif lastSeenRV == 0 {\n+\t\treturn keys\n+\t}\n+\n+\tvar pagedKeys []DataKey\n+\tfor _, key := range keys {\n+\t\tif sortAscending && key.ResourceVersion > lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t} else if !sortAscending && key.ResourceVersion < lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t}\n+\t}\n+\treturn pagedKeys\n+}\n+\n+// ListHistory is like ListIterator, but it returns the history of a resource.\n+func (k *kvStorageBackend) ListHistory(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error) (int64, error) {\n+\tif err := validateListHistoryRequest(req); err != nil {\n+\t\treturn 0, err\n+\t}\n+\tkey := req.Options.Key\n+\t// Parse continue token if provided\n+\tlastSeenRV := int64(0)\n+\tsortAscending := req.GetVersionMatchV2() == resourcepb.ResourceVersionMatchV2_NotOlderThan\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\tlastSeenRV = token.ResourceVersion\n+\t\tsortAscending = token.SortAscending\n+\t}\n+\n+\t// Generate a new resource version for the list\n+\tlistRV := k.snowflake.Generate().Int64()\n+\n+\t// Get all history entries by iterating through datastore keys\n+\thistoryKeys := make([]DataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\n+\t// Use datastore.Keys to get all data keys for this specific resource\n+\tfor dataKey, err := range k.dataStore.Keys(ctx, ListRequestKey{\n+\t\tNamespace: key.Namespace,\n+\t\tGroup:     key.Group,\n+\t\tResource:  key.Resource,\n+\t\tName:      key.Name,\n+\t}) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\thistoryKeys = append(historyKeys, dataKey)\n+\t}\n+\n+\t// Check if context has been cancelled\n+\tif ctx.Err() != nil {\n+\t\treturn 0, ctx.Err()\n+\t}\n+\n+\t// Handle trash differently from regular history\n+\tif req.Source == resourcepb.ListRequest_TRASH {\n+\t\treturn k.processTrashEntries(ctx, req, fn, historyKeys, lastSeenRV, sortAscending, listRV)\n+\t}\n+\n+\t// Apply filtering based on version match\n+\tfilteredKeys, filterErr := filterHistoryKeysByVersion(historyKeys, req)\n+\tif filterErr != nil {\n+\t\treturn 0, filterErr\n+\t}\n+\n+\t// Apply \"live\" history logic: ignore events before the last delete\n+\tfilteredKeys = applyLiveHistoryFilter(filteredKeys, req)\n+\n+\t// Sort the entries if not already sorted correctly\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr := fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// processTrashEntries handles the special case of listing deleted items (trash)\n+func (k *kvStorageBackend) processTrashEntries(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error, historyKeys []DataKey, lastSeenRV int64, sortAscending bool, listRV int64) (int64, error) {\n+\t// Filter to only deleted entries\n+\tvar deletedKeys []DataKey\n+\tfor _, key := range historyKeys {\n+\t\tif key.Action == DataActionDeleted {\n+\t\t\tdeletedKeys = append(deletedKeys, key)\n+\t\t}\n+\t}\n+\n+\t// Check if the resource currently exists (is live)\n+\t// If it exists, don't return any trash entries\n+\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t})\n+\n+\tvar trashKeys []DataKey\n+\tif errors.Is(err, ErrNotFound) {\n+\t\t// Resource doesn't exist currently, so we can return the latest delete\n+\t\t// Find the latest delete event\n+\t\tvar latestDelete *DataKey\n+\t\tfor _, key := range deletedKeys {\n+\t\t\tif latestDelete == nil || key.ResourceVersion > latestDelete.ResourceVersion {\n+\t\t\t\tlatestDelete = &key\n+\t\t\t}\n+\t\t}\n+\t\tif latestDelete != nil {\n+\t\t\ttrashKeys = append(trashKeys, *latestDelete)\n+\t\t}\n+\t}\n+\t// If err != ErrNotFound, the resource exists, so no trash entries should be returned\n+\n+\t// Apply version filtering\n+\tfilteredKeys, err := filterHistoryKeysByVersion(trashKeys, req)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\t// Sort the entries\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr = fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvHistoryIterator implements ListIterator for KV storage history\n+type kvHistoryIterator struct {\n+\tctx           context.Context\n+\tkeys          []DataKey\n+\tcurrentIndex  int\n+\tlistRV        int64\n+\tsortAscending bool\n+\tdataStore     *dataStore\n+\n+\t// current\n+\trv     int64\n+\terr    error\n+\tvalue  []byte\n+\tfolder string\n+}\n+\n+func (i *kvHistoryIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\tkey := i.keys[i.currentIndex]\n+\ti.rv = key.ResourceVersion\n+\n+\t// Read the value from the ReadCloser\n+\tdata, err := i.dataStore.Get(i.ctx, key)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.value, err = io.ReadAll(data)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\t// Extract the folder from the meta data\n+\tpartial := &metav1.PartialObjectMetadata{}\n+\terr = json.Unmarshal(i.value, partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\tmeta, err := utils.MetaAccessor(partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.folder = meta.GetFolder()\n+\ti.err = nil\n+\n+\treturn true\n+}\n+\n+func (i *kvHistoryIterator) Error() error {\n+\treturn i.err\n+}\n+\n+func (i *kvHistoryIterator) ContinueToken() string {\n+\tif i.currentIndex < 0 || i.currentIndex >= len(i.keys) {\n+\t\treturn \"\"\n+\t}\n+\ttoken := ContinueToken{\n+\t\tStartOffset:     i.rv,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tSortAscending:   i.sortAscending,\n+\t}\n+\treturn token.String()\n+}\n+\n+func (i *kvHistoryIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvHistoryIterator) Namespace() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Namespace\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Name() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Name\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Folder() string {\n+\treturn i.folder\n+}\n+\n+func (i *kvHistoryIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+// WatchWriteEvents returns a channel that receives write events.\n+func (k *kvStorageBackend) WatchWriteEvents(ctx context.Context) (<-chan *WrittenEvent, error) {\n+\t// Create a channel to receive events\n+\tevents := make(chan *WrittenEvent, 10000) // TODO: make this configurable\n+\n+\tnotifierEvents := k.notifier.Watch(ctx, defaultWatchOptions())\n+\tgo func() {\n+\t\tfor event := range notifierEvents {\n+\t\t\t// fetch the data\n+\t\t\tdataReader, err := k.dataStore.Get(ctx, DataKey{\n+\t\t\t\tNamespace:       event.Namespace,\n+\t\t\t\tGroup:           event.Group,\n+\t\t\t\tResource:        event.Resource,\n+\t\t\t\tName:            event.Name,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tAction:          event.Action,\n+\t\t\t})\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tdata, err := io.ReadAll(dataReader)\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tvar t resourcepb.WatchEvent_Type\n+\t\t\tswitch event.Action {\n+\t\t\tcase DataActionCreated:\n+\t\t\t\tt = resourcepb.WatchEvent_ADDED\n+\t\t\tcase DataActionUpdated:\n+\t\t\t\tt = resourcepb.WatchEvent_MODIFIED\n+\t\t\tcase DataActionDeleted:\n+\t\t\t\tt = resourcepb.WatchEvent_DELETED\n+\t\t\t}\n+\n+\t\t\tevents <- &WrittenEvent{\n+\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\tNamespace: event.Namespace,\n+\t\t\t\t\tGroup:     event.Group,\n+\t\t\t\t\tResource:  event.Resource,\n+\t\t\t\t\tName:      event.Name,\n+\t\t\t\t},\n+\t\t\t\tType:            t,\n+\t\t\t\tFolder:          event.Folder,\n+\t\t\t\tValue:           data,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tPreviousRV:      event.PreviousRV,\n+\t\t\t\tTimestamp:       event.ResourceVersion / time.Second.Nanoseconds(), // convert to seconds\n+\t\t\t}\n+\t\t}\n+\t\tclose(events)\n+\t}()\n+\treturn events, nil\n+}\n+\n+// GetResourceStats returns resource stats within the storage backend.\n+// TODO: this isn't very efficient, we should use a more efficient algorithm.\n+func (k *kvStorageBackend) GetResourceStats(ctx context.Context, namespace string, minCount int) ([]ResourceStats, error) {\n+\tstats := make([]ResourceStats, 0)\n+\tres := make(map[string]map[string]bool)\n+\trvs := make(map[string]int64)\n+\n+\t// Use datastore.Keys to get all data keys for the namespace\n+\tfor dataKey, err := range k.dataStore.Keys(ctx, ListRequestKey{Namespace: namespace}) {\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tkey := fmt.Sprintf(\"%s/%s/%s\", dataKey.Namespace, dataKey.Group, dataKey.Resource)\n+\t\tif _, ok := res[key]; !ok {\n+\t\t\tres[key] = make(map[string]bool)\n+\t\t\trvs[key] = 1\n+\t\t}\n+\t\tres[key][dataKey.Name] = dataKey.Action != DataActionDeleted",
        "comment_created_at": "2025-07-01T12:54:49+00:00",
        "comment_author": "pstibrany",
        "comment_body": "Now I see what you mean by \"this isn't very efficient\" part. I would expect we don't need to store all Names in memory -- since data keys are sorted, we can easily find last revision for each resource, or whether it's deleted. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2179686572",
        "repo_full_name": "grafana/grafana",
        "pr_number": 107305,
        "pr_file": "pkg/storage/unified/resource/storage_backend.go",
        "discussion_id": "2177535538",
        "commented_code": "@@ -0,0 +1,777 @@\n+package resource\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"encoding/json\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"math/rand/v2\"\n+\t\"net/http\"\n+\t\"sort\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/bwmarrin/snowflake\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/utils\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+)\n+\n+const (\n+\tdefaultListBufferSize = 100\n+)\n+\n+// Unified storage backend based on KV storage.\n+type kvStorageBackend struct {\n+\tsnowflake  *snowflake.Node\n+\tkv         KV\n+\tdataStore  *dataStore\n+\tmetaStore  *metadataStore\n+\teventStore *eventStore\n+\tnotifier   *notifier\n+\tbuilder    DocumentBuilder\n+}\n+\n+var _ StorageBackend = &kvStorageBackend{}\n+\n+func NewkvStorageBackend(kv KV) *kvStorageBackend {\n+\ts, err := snowflake.NewNode(rand.Int64N(1024))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\teventStore := newEventStore(kv)\n+\treturn &kvStorageBackend{\n+\t\tkv:         kv,\n+\t\tdataStore:  newDataStore(kv),\n+\t\tmetaStore:  newMetadataStore(kv),\n+\t\teventStore: eventStore,\n+\t\tnotifier:   newNotifier(eventStore, notifierOptions{}),\n+\t\tsnowflake:  s,\n+\t\tbuilder:    StandardDocumentBuilder(), // For now we use the standard document builder.\n+\t}\n+}\n+\n+// WriteEvent writes a resource event (create/update/delete) to the storage backend.\n+func (k *kvStorageBackend) WriteEvent(ctx context.Context, event WriteEvent) (int64, error) {\n+\tif err := event.Validate(); err != nil {\n+\t\treturn 0, fmt.Errorf(\"invalid event: %w\", err)\n+\t}\n+\trv := k.snowflake.Generate().Int64()\n+\n+\t// Write data.\n+\tvar action DataAction\n+\tswitch event.Type {\n+\tcase resourcepb.WatchEvent_ADDED:\n+\t\taction = DataActionCreated\n+\t\t// Check if resource already exists for create operations\n+\t\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\t\tNamespace: event.Key.Namespace,\n+\t\t\tGroup:     event.Key.Group,\n+\t\t\tResource:  event.Key.Resource,\n+\t\t\tName:      event.Key.Name,\n+\t\t})\n+\t\tif err == nil {\n+\t\t\t// Resource exists, return already exists error\n+\t\t\treturn 0, ErrResourceAlreadyExists\n+\t\t}\n+\t\tif !errors.Is(err, ErrNotFound) {\n+\t\t\t// Some other error occurred\n+\t\t\treturn 0, fmt.Errorf(\"failed to check if resource exists: %w\", err)\n+\t\t}\n+\tcase resourcepb.WatchEvent_MODIFIED:\n+\t\taction = DataActionUpdated\n+\tcase resourcepb.WatchEvent_DELETED:\n+\t\taction = DataActionDeleted\n+\tdefault:\n+\t\treturn 0, fmt.Errorf(\"invalid event type: %d\", event.Type)\n+\t}\n+\n+\t// Build the search document\n+\tdoc, err := k.builder.BuildDocument(ctx, event.Key, rv, event.Value)\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to build document: %w\", err)\n+\t}\n+\n+\t// Write the data\n+\terr = k.dataStore.Save(ctx, DataKey{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t}, io.NopCloser(bytes.NewReader(event.Value)))\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write data: %w\", err)\n+\t}\n+\n+\t// Write metadata\n+\terr = k.metaStore.Save(ctx, MetaDataObj{\n+\t\tKey: MetaDataKey{\n+\t\t\tNamespace:       event.Key.Namespace,\n+\t\t\tGroup:           event.Key.Group,\n+\t\t\tResource:        event.Key.Resource,\n+\t\t\tName:            event.Key.Name,\n+\t\t\tResourceVersion: rv,\n+\t\t\tAction:          action,\n+\t\t\tFolder:          event.Object.GetFolder(),\n+\t\t},\n+\t\tValue: MetaData{\n+\t\t\tIndexableDocument: *doc,\n+\t\t},\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write metadata: %w\", err)\n+\t}\n+\n+\t// Write event\n+\terr = k.eventStore.Save(ctx, Event{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t\tFolder:          event.Object.GetFolder(),\n+\t\tPreviousRV:      event.PreviousRV,\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to save event: %w\", err)\n+\t}\n+\treturn rv, nil\n+}\n+\n+func (k *kvStorageBackend) ReadResource(ctx context.Context, req *resourcepb.ReadRequest) *BackendReadResponse {\n+\tif req.Key == nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusBadRequest, Message: \"missing key\"}}\n+\t}\n+\tmeta, err := k.metaStore.GetResourceKeyAtRevision(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Key.Namespace,\n+\t\tGroup:     req.Key.Group,\n+\t\tResource:  req.Key.Resource,\n+\t\tName:      req.Key.Name,\n+\t}, req.ResourceVersion)\n+\tif errors.Is(err, ErrNotFound) {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusNotFound, Message: \"not found\"}}\n+\t} else if err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tdata, err := k.dataStore.Get(ctx, DataKey{\n+\t\tNamespace:       req.Key.Namespace,\n+\t\tGroup:           req.Key.Group,\n+\t\tResource:        req.Key.Resource,\n+\t\tName:            req.Key.Name,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tAction:          meta.Action,\n+\t})\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tvalue, err := io.ReadAll(data)\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\treturn &BackendReadResponse{\n+\t\tKey:             req.Key,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tValue:           value,\n+\t\tFolder:          meta.Folder,\n+\t}\n+}\n+\n+// // ListIterator returns an iterator for listing resources.\n+func (k *kvStorageBackend) ListIterator(ctx context.Context, req *resourcepb.ListRequest, cb func(ListIterator) error) (int64, error) {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn 0, fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\t// Parse continue token if provided\n+\toffset := int64(0)\n+\tresourceVersion := req.ResourceVersion\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\toffset = token.StartOffset\n+\t\tresourceVersion = token.ResourceVersion\n+\t}\n+\n+\t// We set the listRV to the current time.\n+\tlistRV := k.snowflake.Generate().Int64()\n+\tif resourceVersion > 0 {\n+\t\tlistRV = resourceVersion\n+\t}\n+\n+\t// Fetch the latest objects\n+\tkeys := make([]MetaDataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\tidx := 0\n+\tfor metaKey, err := range k.metaStore.ListResourceKeysAtRevision(ctx, MetaListRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t}, resourceVersion) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\t// Skip the first offset items. This is not efficient, but it's a simple way to implement it for now.\n+\t\tif idx < int(offset) {\n+\t\t\tidx++\n+\t\t\tcontinue\n+\t\t}\n+\t\tkeys = append(keys, metaKey)\n+\t\t// Only fetch the first limit items + 1 to get the next token.\n+\t\tif len(keys) >= int(req.Limit+1) {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\titer := kvListIterator{\n+\t\tkeys:         keys,\n+\t\tcurrentIndex: -1,\n+\t\tctx:          ctx,\n+\t\tlistRV:       listRV,\n+\t\toffset:       offset,\n+\t\tdataStore:    k.dataStore,\n+\t}\n+\terr := cb(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvListIterator implements ListIterator for KV storage\n+type kvListIterator struct {\n+\tctx          context.Context\n+\tkeys         []MetaDataKey\n+\tcurrentIndex int\n+\tdataStore    *dataStore\n+\tlistRV       int64\n+\toffset       int64\n+\n+\t// current\n+\trv    int64\n+\terr   error\n+\tvalue []byte\n+}\n+\n+func (i *kvListIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\ti.rv, i.err = i.keys[i.currentIndex].ResourceVersion, nil\n+\n+\tdata, err := i.dataStore.Get(i.ctx, DataKey{\n+\t\tNamespace:       i.keys[i.currentIndex].Namespace,\n+\t\tGroup:           i.keys[i.currentIndex].Group,\n+\t\tResource:        i.keys[i.currentIndex].Resource,\n+\t\tName:            i.keys[i.currentIndex].Name,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tAction:          i.keys[i.currentIndex].Action,\n+\t})\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\ti.value, i.err = io.ReadAll(data)\n+\tif i.err != nil {\n+\t\treturn false\n+\t}\n+\n+\t// increment the offset\n+\ti.offset++\n+\n+\treturn true\n+}\n+\n+func (i *kvListIterator) Error() error {\n+\treturn nil\n+}\n+\n+func (i *kvListIterator) ContinueToken() string {\n+\treturn ContinueToken{\n+\t\tStartOffset:     i.offset,\n+\t\tResourceVersion: i.listRV,\n+\t}.String()\n+}\n+\n+func (i *kvListIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvListIterator) Namespace() string {\n+\treturn i.keys[i.currentIndex].Namespace\n+}\n+\n+func (i *kvListIterator) Name() string {\n+\treturn i.keys[i.currentIndex].Name\n+}\n+\n+func (i *kvListIterator) Folder() string {\n+\treturn i.keys[i.currentIndex].Folder\n+}\n+\n+func (i *kvListIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+func validateListHistoryRequest(req *resourcepb.ListRequest) error {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\tkey := req.Options.Key\n+\tif key.Group == \"\" {\n+\t\treturn fmt.Errorf(\"group is required\")\n+\t}\n+\tif key.Resource == \"\" {\n+\t\treturn fmt.Errorf(\"resource is required\")\n+\t}\n+\tif key.Namespace == \"\" {\n+\t\treturn fmt.Errorf(\"namespace is required\")\n+\t}\n+\tif key.Name == \"\" {\n+\t\treturn fmt.Errorf(\"name is required\")\n+\t}\n+\treturn nil\n+}\n+\n+// filterHistoryKeysByVersion filters history keys based on version match criteria\n+func filterHistoryKeysByVersion(historyKeys []DataKey, req *resourcepb.ListRequest) ([]DataKey, error) {\n+\tswitch req.GetVersionMatchV2() {\n+\tcase resourcepb.ResourceVersionMatchV2_Exact:\n+\t\tif req.ResourceVersion <= 0 {\n+\t\t\treturn nil, fmt.Errorf(\"expecting an explicit resource version query when using Exact matching\")\n+\t\t}\n+\t\tvar exactKeys []DataKey\n+\t\tfor _, key := range historyKeys {\n+\t\t\tif key.ResourceVersion == req.ResourceVersion {\n+\t\t\t\texactKeys = append(exactKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn exactKeys, nil\n+\tcase resourcepb.ResourceVersionMatchV2_NotOlderThan:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion >= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\tdefault:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion <= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\t}\n+\treturn historyKeys, nil\n+}\n+\n+// applyLiveHistoryFilter applies \"live\" history logic by ignoring events before the last delete\n+func applyLiveHistoryFilter(filteredKeys []DataKey, req *resourcepb.ListRequest) []DataKey {\n+\tuseLatestDeletionAsMinRV := req.ResourceVersion == 0 && req.Source != resourcepb.ListRequest_TRASH && req.GetVersionMatchV2() != resourcepb.ResourceVersionMatchV2_Exact\n+\tif !useLatestDeletionAsMinRV {\n+\t\treturn filteredKeys\n+\t}\n+\n+\tlatestDeleteRV := int64(0)\n+\tfor _, key := range filteredKeys {\n+\t\tif key.Action == DataActionDeleted && key.ResourceVersion > latestDeleteRV {\n+\t\t\tlatestDeleteRV = key.ResourceVersion\n+\t\t}\n+\t}\n+\tif latestDeleteRV > 0 {\n+\t\tvar liveKeys []DataKey\n+\t\tfor _, key := range filteredKeys {\n+\t\t\tif key.ResourceVersion > latestDeleteRV {\n+\t\t\t\tliveKeys = append(liveKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn liveKeys\n+\t}\n+\treturn filteredKeys\n+}\n+\n+// sortHistoryKeys sorts the history keys based on the sortAscending flag\n+func sortHistoryKeys(filteredKeys []DataKey, sortAscending bool) {\n+\tif sortAscending {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion < filteredKeys[j].ResourceVersion\n+\t\t})\n+\t} else {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion > filteredKeys[j].ResourceVersion\n+\t\t})\n+\t}\n+}\n+\n+// applyPagination filters keys based on pagination parameters\n+func applyPagination(keys []DataKey, lastSeenRV int64, sortAscending bool) []DataKey {\n+\tif lastSeenRV == 0 {\n+\t\treturn keys\n+\t}\n+\n+\tvar pagedKeys []DataKey\n+\tfor _, key := range keys {\n+\t\tif sortAscending && key.ResourceVersion > lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t} else if !sortAscending && key.ResourceVersion < lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t}\n+\t}\n+\treturn pagedKeys\n+}\n+\n+// ListHistory is like ListIterator, but it returns the history of a resource.\n+func (k *kvStorageBackend) ListHistory(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error) (int64, error) {\n+\tif err := validateListHistoryRequest(req); err != nil {\n+\t\treturn 0, err\n+\t}\n+\tkey := req.Options.Key\n+\t// Parse continue token if provided\n+\tlastSeenRV := int64(0)\n+\tsortAscending := req.GetVersionMatchV2() == resourcepb.ResourceVersionMatchV2_NotOlderThan\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\tlastSeenRV = token.ResourceVersion\n+\t\tsortAscending = token.SortAscending\n+\t}\n+\n+\t// Generate a new resource version for the list\n+\tlistRV := k.snowflake.Generate().Int64()\n+\n+\t// Get all history entries by iterating through datastore keys\n+\thistoryKeys := make([]DataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\n+\t// Use datastore.Keys to get all data keys for this specific resource\n+\tfor dataKey, err := range k.dataStore.Keys(ctx, ListRequestKey{\n+\t\tNamespace: key.Namespace,\n+\t\tGroup:     key.Group,\n+\t\tResource:  key.Resource,\n+\t\tName:      key.Name,\n+\t}) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\thistoryKeys = append(historyKeys, dataKey)\n+\t}\n+\n+\t// Check if context has been cancelled\n+\tif ctx.Err() != nil {\n+\t\treturn 0, ctx.Err()\n+\t}\n+\n+\t// Handle trash differently from regular history\n+\tif req.Source == resourcepb.ListRequest_TRASH {\n+\t\treturn k.processTrashEntries(ctx, req, fn, historyKeys, lastSeenRV, sortAscending, listRV)\n+\t}\n+\n+\t// Apply filtering based on version match\n+\tfilteredKeys, filterErr := filterHistoryKeysByVersion(historyKeys, req)\n+\tif filterErr != nil {\n+\t\treturn 0, filterErr\n+\t}\n+\n+\t// Apply \"live\" history logic: ignore events before the last delete\n+\tfilteredKeys = applyLiveHistoryFilter(filteredKeys, req)\n+\n+\t// Sort the entries if not already sorted correctly\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr := fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// processTrashEntries handles the special case of listing deleted items (trash)\n+func (k *kvStorageBackend) processTrashEntries(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error, historyKeys []DataKey, lastSeenRV int64, sortAscending bool, listRV int64) (int64, error) {\n+\t// Filter to only deleted entries\n+\tvar deletedKeys []DataKey\n+\tfor _, key := range historyKeys {\n+\t\tif key.Action == DataActionDeleted {\n+\t\t\tdeletedKeys = append(deletedKeys, key)\n+\t\t}\n+\t}\n+\n+\t// Check if the resource currently exists (is live)\n+\t// If it exists, don't return any trash entries\n+\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t})\n+\n+\tvar trashKeys []DataKey\n+\tif errors.Is(err, ErrNotFound) {\n+\t\t// Resource doesn't exist currently, so we can return the latest delete\n+\t\t// Find the latest delete event\n+\t\tvar latestDelete *DataKey\n+\t\tfor _, key := range deletedKeys {\n+\t\t\tif latestDelete == nil || key.ResourceVersion > latestDelete.ResourceVersion {\n+\t\t\t\tlatestDelete = &key\n+\t\t\t}\n+\t\t}\n+\t\tif latestDelete != nil {\n+\t\t\ttrashKeys = append(trashKeys, *latestDelete)\n+\t\t}\n+\t}\n+\t// If err != ErrNotFound, the resource exists, so no trash entries should be returned\n+\n+\t// Apply version filtering\n+\tfilteredKeys, err := filterHistoryKeysByVersion(trashKeys, req)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\t// Sort the entries\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr = fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvHistoryIterator implements ListIterator for KV storage history\n+type kvHistoryIterator struct {\n+\tctx           context.Context\n+\tkeys          []DataKey\n+\tcurrentIndex  int\n+\tlistRV        int64\n+\tsortAscending bool\n+\tdataStore     *dataStore\n+\n+\t// current\n+\trv     int64\n+\terr    error\n+\tvalue  []byte\n+\tfolder string\n+}\n+\n+func (i *kvHistoryIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\tkey := i.keys[i.currentIndex]\n+\ti.rv = key.ResourceVersion\n+\n+\t// Read the value from the ReadCloser\n+\tdata, err := i.dataStore.Get(i.ctx, key)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.value, err = io.ReadAll(data)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\t// Extract the folder from the meta data\n+\tpartial := &metav1.PartialObjectMetadata{}\n+\terr = json.Unmarshal(i.value, partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\tmeta, err := utils.MetaAccessor(partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.folder = meta.GetFolder()\n+\ti.err = nil\n+\n+\treturn true\n+}\n+\n+func (i *kvHistoryIterator) Error() error {\n+\treturn i.err\n+}\n+\n+func (i *kvHistoryIterator) ContinueToken() string {\n+\tif i.currentIndex < 0 || i.currentIndex >= len(i.keys) {\n+\t\treturn \"\"\n+\t}\n+\ttoken := ContinueToken{\n+\t\tStartOffset:     i.rv,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tSortAscending:   i.sortAscending,\n+\t}\n+\treturn token.String()\n+}\n+\n+func (i *kvHistoryIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvHistoryIterator) Namespace() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Namespace\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Name() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Name\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Folder() string {\n+\treturn i.folder\n+}\n+\n+func (i *kvHistoryIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+// WatchWriteEvents returns a channel that receives write events.\n+func (k *kvStorageBackend) WatchWriteEvents(ctx context.Context) (<-chan *WrittenEvent, error) {\n+\t// Create a channel to receive events\n+\tevents := make(chan *WrittenEvent, 10000) // TODO: make this configurable\n+\n+\tnotifierEvents := k.notifier.Watch(ctx, defaultWatchOptions())\n+\tgo func() {\n+\t\tfor event := range notifierEvents {\n+\t\t\t// fetch the data\n+\t\t\tdataReader, err := k.dataStore.Get(ctx, DataKey{\n+\t\t\t\tNamespace:       event.Namespace,\n+\t\t\t\tGroup:           event.Group,\n+\t\t\t\tResource:        event.Resource,\n+\t\t\t\tName:            event.Name,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tAction:          event.Action,\n+\t\t\t})\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tdata, err := io.ReadAll(dataReader)\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tvar t resourcepb.WatchEvent_Type\n+\t\t\tswitch event.Action {\n+\t\t\tcase DataActionCreated:\n+\t\t\t\tt = resourcepb.WatchEvent_ADDED\n+\t\t\tcase DataActionUpdated:\n+\t\t\t\tt = resourcepb.WatchEvent_MODIFIED\n+\t\t\tcase DataActionDeleted:\n+\t\t\t\tt = resourcepb.WatchEvent_DELETED\n+\t\t\t}\n+\n+\t\t\tevents <- &WrittenEvent{\n+\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\tNamespace: event.Namespace,\n+\t\t\t\t\tGroup:     event.Group,\n+\t\t\t\t\tResource:  event.Resource,\n+\t\t\t\t\tName:      event.Name,\n+\t\t\t\t},\n+\t\t\t\tType:            t,\n+\t\t\t\tFolder:          event.Folder,\n+\t\t\t\tValue:           data,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tPreviousRV:      event.PreviousRV,\n+\t\t\t\tTimestamp:       event.ResourceVersion / time.Second.Nanoseconds(), // convert to seconds\n+\t\t\t}\n+\t\t}\n+\t\tclose(events)\n+\t}()\n+\treturn events, nil\n+}\n+\n+// GetResourceStats returns resource stats within the storage backend.\n+// TODO: this isn't very efficient, we should use a more efficient algorithm.\n+func (k *kvStorageBackend) GetResourceStats(ctx context.Context, namespace string, minCount int) ([]ResourceStats, error) {\n+\tstats := make([]ResourceStats, 0)\n+\tres := make(map[string]map[string]bool)\n+\trvs := make(map[string]int64)\n+\n+\t// Use datastore.Keys to get all data keys for the namespace\n+\tfor dataKey, err := range k.dataStore.Keys(ctx, ListRequestKey{Namespace: namespace}) {\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t\tkey := fmt.Sprintf(\"%s/%s/%s\", dataKey.Namespace, dataKey.Group, dataKey.Resource)\n+\t\tif _, ok := res[key]; !ok {\n+\t\t\tres[key] = make(map[string]bool)\n+\t\t\trvs[key] = 1\n+\t\t}\n+\t\tres[key][dataKey.Name] = dataKey.Action != DataActionDeleted",
        "comment_created_at": "2025-07-02T10:19:23+00:00",
        "comment_author": "chaudyg",
        "comment_body": "We need to add a StatsStore. This will be coming next.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2044182623",
    "pr_number": 104024,
    "pr_file": "pkg/expr/convert_to_full_long_ts_test.go",
    "created_at": "2025-04-15T10:14:51+00:00",
    "commented_code": "}\n \t})\n }\n+\n+func BenchmarkConvertToFullLong(b *testing.B) {\n+\t// Create time-series data with 5,000 rows\n+\ttimes := make([]time.Time, 5000)\n+\tfor i := range times {\n+\t\ttimes[i] = time.Unix(int64(i), 0)\n+\t}\n+\tvalues := make([]float64, 5000)\n+\tfor i := range values {\n+\t\tvalues[i] = float64(i)\n+\t}\n+\n+\t// Create a second series with 5,000 rows\n+\ttimes2 := make([]time.Time, 5000)\n+\tfor i := range times2 {\n+\t\ttimes2[i] = time.Unix(int64(i), 0)\n+\t}\n+\tvalues2 := make([]float64, 5000)\n+\tfor i := range values2 {\n+\t\tvalues2[i] = float64(i)\n+\t}\n+\n+\t// Create a frame with the times\n+\tframe := data.NewFrame(\"cpu\", data.NewField(\"time\", nil, times), data.NewField(\"cpu\", nil, values))\n+\tframe.Meta = &data.FrameMeta{Type: data.FrameTypeTimeSeriesMulti}\n+\n+\t// Create a second frame with the times\n+\tframe2 := data.NewFrame(\"cpu\", data.NewField(\"time\", nil, times2), data.NewField(\"cpu\", nil, values2))\n+\tframe2.Meta = &data.FrameMeta{Type: data.FrameTypeTimeSeriesMulti}\n+\n+\t// Run the conversion",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "2044182623",
        "repo_full_name": "grafana/grafana",
        "pr_number": 104024,
        "pr_file": "pkg/expr/convert_to_full_long_ts_test.go",
        "discussion_id": "2044182623",
        "commented_code": "@@ -371,3 +371,38 @@ func TestConvertTimeSeriesMultiToFullLongWithDisplayName(t *testing.T) {\n \t\t}\n \t})\n }\n+\n+func BenchmarkConvertToFullLong(b *testing.B) {\n+\t// Create time-series data with 5,000 rows\n+\ttimes := make([]time.Time, 5000)\n+\tfor i := range times {\n+\t\ttimes[i] = time.Unix(int64(i), 0)\n+\t}\n+\tvalues := make([]float64, 5000)\n+\tfor i := range values {\n+\t\tvalues[i] = float64(i)\n+\t}\n+\n+\t// Create a second series with 5,000 rows\n+\ttimes2 := make([]time.Time, 5000)\n+\tfor i := range times2 {\n+\t\ttimes2[i] = time.Unix(int64(i), 0)\n+\t}\n+\tvalues2 := make([]float64, 5000)\n+\tfor i := range values2 {\n+\t\tvalues2[i] = float64(i)\n+\t}\n+\n+\t// Create a frame with the times\n+\tframe := data.NewFrame(\"cpu\", data.NewField(\"time\", nil, times), data.NewField(\"cpu\", nil, values))\n+\tframe.Meta = &data.FrameMeta{Type: data.FrameTypeTimeSeriesMulti}\n+\n+\t// Create a second frame with the times\n+\tframe2 := data.NewFrame(\"cpu\", data.NewField(\"time\", nil, times2), data.NewField(\"cpu\", nil, values2))\n+\tframe2.Meta = &data.FrameMeta{Type: data.FrameTypeTimeSeriesMulti}\n+\n+\t// Run the conversion",
        "comment_created_at": "2025-04-15T10:14:51+00:00",
        "comment_author": "Copilot",
        "comment_body": "Consider adding b.ResetTimer() before the loop to ensure that the benchmark measurement excludes the setup time.\n```suggestion\n\t// Run the conversion\n\tb.ResetTimer()\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2044182639",
    "pr_number": 104024,
    "pr_file": "pkg/expr/convert_to_full_long.go",
    "created_at": "2025-04-15T10:14:52+00:00",
    "commented_code": "if timeField == nil {\n \t\t\treturn nil, fmt.Errorf(\"missing time field\")\n \t\t}\n+\t}\n+\n+\t// Calculate total row count to pre-allocate\n+\tvar totalRows int\n+\tfor _, frame := range frames {\n+\t\t// Directly count numeric fields since we already validated time fields\n+\t\tfor _, f := range frame.Fields {\n+\t\t\tif f.Type().Numeric() {\n+\t\t\t\ttotalRows += f.Len()\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\trows := make([]row, 0, totalRows)\n+\tlabelKeysSet := map[string]struct{}{}\n+\thasDisplayCol := false\n+\n+\t// Process all frames to build rows\n+\tfor _, frame := range frames {\n+\t\tvar timeField *data.Field\n+\t\tfor _, f := range frame.Fields {\n+\t\t\tif f.Type() == data.FieldTypeTime {\n+\t\t\t\ttimeField = f\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n \t\tfor _, f := range frame.Fields {",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "2044182639",
        "repo_full_name": "grafana/grafana",
        "pr_number": 104024,
        "pr_file": "pkg/expr/convert_to_full_long.go",
        "discussion_id": "2044182639",
        "commented_code": "@@ -160,6 +157,32 @@ func convertTimeSeriesMultiToFullLong(frames data.Frames) (data.Frames, error) {\n \t\tif timeField == nil {\n \t\t\treturn nil, fmt.Errorf(\"missing time field\")\n \t\t}\n+\t}\n+\n+\t// Calculate total row count to pre-allocate\n+\tvar totalRows int\n+\tfor _, frame := range frames {\n+\t\t// Directly count numeric fields since we already validated time fields\n+\t\tfor _, f := range frame.Fields {\n+\t\t\tif f.Type().Numeric() {\n+\t\t\t\ttotalRows += f.Len()\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\trows := make([]row, 0, totalRows)\n+\tlabelKeysSet := map[string]struct{}{}\n+\thasDisplayCol := false\n+\n+\t// Process all frames to build rows\n+\tfor _, frame := range frames {\n+\t\tvar timeField *data.Field\n+\t\tfor _, f := range frame.Fields {\n+\t\t\tif f.Type() == data.FieldTypeTime {\n+\t\t\t\ttimeField = f\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n \t\tfor _, f := range frame.Fields {",
        "comment_created_at": "2025-04-15T10:14:52+00:00",
        "comment_author": "Copilot",
        "comment_body": "[nitpick] The time field is searched for twice in each frame (once during validation and again during processing). Storing the time field from the validation loop could reduce redundant iteration.\n```suggestion\n\t\ttimeField := timeFields[frame]\n\t\tfor _, f := range frame.Fields {\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1333587584",
    "pr_number": 74998,
    "pr_file": "pkg/services/ngalert/store/instance_database.go",
    "created_at": "2023-09-21T20:50:47+00:00",
    "commented_code": "return result, err\n }\n \n+func (st DBstore) FullSync(ctx context.Context, instances []models.AlertInstance) error {\n+\tif len(instances) == 0 {\n+\t\treturn nil\n+\t}\n+\treturn st.SQLStore.WithTransactionalDbSession(ctx, func(sess *sqlstore.DBSession) error {\n+\t\t// First we delete all records from the table\n+\t\tif _, err := sess.Exec(\"DELETE FROM alert_instance\"); err != nil {\n+\t\t\treturn fmt.Errorf(\"failed to delete alert_instance table: %w\", err)\n+\t\t}\n+\t\tfor _, alertInstance := range instances {\n+\t\t\tif err := models.ValidateAlertInstance(alertInstance); err != nil {\n+\t\t\t\tst.Logger.Warn(\"Failed to validate alert instance, skipping\", \"err\", err, \"rule_uid\", alertInstance.RuleUID)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tlabelTupleJSON, err := alertInstance.Labels.StringKey()\n+\t\t\tif err != nil {\n+\t\t\t\tst.Logger.Warn(\"Failed to generate alert instance labels key, skipping\", \"err\", err, \"rule_uid\", alertInstance.RuleUID)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tparams := append(make([]any, 0), alertInstance.RuleOrgID, alertInstance.RuleUID, labelTupleJSON, alertInstance.LabelsHash, alertInstance.CurrentState, alertInstance.CurrentReason, alertInstance.CurrentStateSince.Unix(), alertInstance.CurrentStateEnd.Unix(), alertInstance.LastEvalTime.Unix())\n+\n+\t\t\tupsertSQL := st.SQLStore.GetDialect().UpsertSQL(",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "1333587584",
        "repo_full_name": "grafana/grafana",
        "pr_number": 74998,
        "pr_file": "pkg/services/ngalert/store/instance_database.go",
        "discussion_id": "1333587584",
        "commented_code": "@@ -43,6 +44,46 @@ func (st DBstore) ListAlertInstances(ctx context.Context, cmd *models.ListAlertI\n \treturn result, err\n }\n \n+func (st DBstore) FullSync(ctx context.Context, instances []models.AlertInstance) error {\n+\tif len(instances) == 0 {\n+\t\treturn nil\n+\t}\n+\treturn st.SQLStore.WithTransactionalDbSession(ctx, func(sess *sqlstore.DBSession) error {\n+\t\t// First we delete all records from the table\n+\t\tif _, err := sess.Exec(\"DELETE FROM alert_instance\"); err != nil {\n+\t\t\treturn fmt.Errorf(\"failed to delete alert_instance table: %w\", err)\n+\t\t}\n+\t\tfor _, alertInstance := range instances {\n+\t\t\tif err := models.ValidateAlertInstance(alertInstance); err != nil {\n+\t\t\t\tst.Logger.Warn(\"Failed to validate alert instance, skipping\", \"err\", err, \"rule_uid\", alertInstance.RuleUID)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tlabelTupleJSON, err := alertInstance.Labels.StringKey()\n+\t\t\tif err != nil {\n+\t\t\t\tst.Logger.Warn(\"Failed to generate alert instance labels key, skipping\", \"err\", err, \"rule_uid\", alertInstance.RuleUID)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tparams := append(make([]any, 0), alertInstance.RuleOrgID, alertInstance.RuleUID, labelTupleJSON, alertInstance.LabelsHash, alertInstance.CurrentState, alertInstance.CurrentReason, alertInstance.CurrentStateSince.Unix(), alertInstance.CurrentStateEnd.Unix(), alertInstance.LastEvalTime.Unix())\n+\n+\t\t\tupsertSQL := st.SQLStore.GetDialect().UpsertSQL(",
        "comment_created_at": "2023-09-21T20:50:47+00:00",
        "comment_author": "grobinson-grafana",
        "comment_body": "Do we need to do an upsert if we've truncated the table? Can we do a regular insert? Would it be faster?",
        "pr_file_module": null
      },
      {
        "comment_id": "1333621074",
        "repo_full_name": "grafana/grafana",
        "pr_number": 74998,
        "pr_file": "pkg/services/ngalert/store/instance_database.go",
        "discussion_id": "1333587584",
        "commented_code": "@@ -43,6 +44,46 @@ func (st DBstore) ListAlertInstances(ctx context.Context, cmd *models.ListAlertI\n \treturn result, err\n }\n \n+func (st DBstore) FullSync(ctx context.Context, instances []models.AlertInstance) error {\n+\tif len(instances) == 0 {\n+\t\treturn nil\n+\t}\n+\treturn st.SQLStore.WithTransactionalDbSession(ctx, func(sess *sqlstore.DBSession) error {\n+\t\t// First we delete all records from the table\n+\t\tif _, err := sess.Exec(\"DELETE FROM alert_instance\"); err != nil {\n+\t\t\treturn fmt.Errorf(\"failed to delete alert_instance table: %w\", err)\n+\t\t}\n+\t\tfor _, alertInstance := range instances {\n+\t\t\tif err := models.ValidateAlertInstance(alertInstance); err != nil {\n+\t\t\t\tst.Logger.Warn(\"Failed to validate alert instance, skipping\", \"err\", err, \"rule_uid\", alertInstance.RuleUID)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tlabelTupleJSON, err := alertInstance.Labels.StringKey()\n+\t\t\tif err != nil {\n+\t\t\t\tst.Logger.Warn(\"Failed to generate alert instance labels key, skipping\", \"err\", err, \"rule_uid\", alertInstance.RuleUID)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tparams := append(make([]any, 0), alertInstance.RuleOrgID, alertInstance.RuleUID, labelTupleJSON, alertInstance.LabelsHash, alertInstance.CurrentState, alertInstance.CurrentReason, alertInstance.CurrentStateSince.Unix(), alertInstance.CurrentStateEnd.Unix(), alertInstance.LastEvalTime.Unix())\n+\n+\t\t\tupsertSQL := st.SQLStore.GetDialect().UpsertSQL(",
        "comment_created_at": "2023-09-21T21:27:58+00:00",
        "comment_author": "JohnnyQQQQ",
        "comment_body": "Good catch. I just copied it, will test it tomorrow, it should be faster with inserts.",
        "pr_file_module": null
      },
      {
        "comment_id": "1333663679",
        "repo_full_name": "grafana/grafana",
        "pr_number": 74998,
        "pr_file": "pkg/services/ngalert/store/instance_database.go",
        "discussion_id": "1333587584",
        "commented_code": "@@ -43,6 +44,46 @@ func (st DBstore) ListAlertInstances(ctx context.Context, cmd *models.ListAlertI\n \treturn result, err\n }\n \n+func (st DBstore) FullSync(ctx context.Context, instances []models.AlertInstance) error {\n+\tif len(instances) == 0 {\n+\t\treturn nil\n+\t}\n+\treturn st.SQLStore.WithTransactionalDbSession(ctx, func(sess *sqlstore.DBSession) error {\n+\t\t// First we delete all records from the table\n+\t\tif _, err := sess.Exec(\"DELETE FROM alert_instance\"); err != nil {\n+\t\t\treturn fmt.Errorf(\"failed to delete alert_instance table: %w\", err)\n+\t\t}\n+\t\tfor _, alertInstance := range instances {\n+\t\t\tif err := models.ValidateAlertInstance(alertInstance); err != nil {\n+\t\t\t\tst.Logger.Warn(\"Failed to validate alert instance, skipping\", \"err\", err, \"rule_uid\", alertInstance.RuleUID)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tlabelTupleJSON, err := alertInstance.Labels.StringKey()\n+\t\t\tif err != nil {\n+\t\t\t\tst.Logger.Warn(\"Failed to generate alert instance labels key, skipping\", \"err\", err, \"rule_uid\", alertInstance.RuleUID)\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tparams := append(make([]any, 0), alertInstance.RuleOrgID, alertInstance.RuleUID, labelTupleJSON, alertInstance.LabelsHash, alertInstance.CurrentState, alertInstance.CurrentReason, alertInstance.CurrentStateSince.Unix(), alertInstance.CurrentStateEnd.Unix(), alertInstance.LastEvalTime.Unix())\n+\n+\t\t\tupsertSQL := st.SQLStore.GetDialect().UpsertSQL(",
        "comment_created_at": "2023-09-21T22:34:22+00:00",
        "comment_author": "JohnnyQQQQ",
        "comment_body": "Changed it in https://github.com/grafana/grafana/pull/74998/commits/b09fb7010cea0a5128dab384c889bf721cb9be90, seems to be 50% faster in SQLite.",
        "pr_file_module": null
      }
    ]
  }
]