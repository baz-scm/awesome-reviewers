[
  {
    "discussion_id": "2184688299",
    "pr_number": 107581,
    "pr_file": "pkg/apimachinery/identity/context.go",
    "created_at": "2025-07-04T08:06:40+00:00",
    "commented_code": "serviceNameForProvisioning = \"provisioning\"\n )\n \n-func newInternalIdentity(name string, namespace string, orgID int64) Requester {\n-\treturn &StaticRequester{\n+type IdentityOpts func(*StaticRequester)\n+\n+// WithServiceIdentityName sets the `StaticRequester.AccessTokenClaims.Rest.ServiceIdentity` field to the provided name.\n+// This is so far only used by Secrets Manager to identify and gate the service decrypting a secret.\n+func WithServiceIdentityName(name string) IdentityOpts {\n+\treturn func(r *StaticRequester) {\n+\t\tr.AccessTokenClaims.Rest.ServiceIdentity = name\n+\t}\n+}\n+\n+func newInternalIdentity(name string, namespace string, orgID int64, opts ...IdentityOpts) Requester {\n+\t// Create a copy of the ServiceIdentityClaims to avoid modifying the global one.\n+\t// Some of the options might mutate it.\n+\tclaimsCopy := *ServiceIdentityClaims",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "2184688299",
        "repo_full_name": "grafana/grafana",
        "pr_number": 107581,
        "pr_file": "pkg/apimachinery/identity/context.go",
        "discussion_id": "2184688299",
        "commented_code": "@@ -36,8 +36,22 @@ const (\n \tserviceNameForProvisioning = \"provisioning\"\n )\n \n-func newInternalIdentity(name string, namespace string, orgID int64) Requester {\n-\treturn &StaticRequester{\n+type IdentityOpts func(*StaticRequester)\n+\n+// WithServiceIdentityName sets the `StaticRequester.AccessTokenClaims.Rest.ServiceIdentity` field to the provided name.\n+// This is so far only used by Secrets Manager to identify and gate the service decrypting a secret.\n+func WithServiceIdentityName(name string) IdentityOpts {\n+\treturn func(r *StaticRequester) {\n+\t\tr.AccessTokenClaims.Rest.ServiceIdentity = name\n+\t}\n+}\n+\n+func newInternalIdentity(name string, namespace string, orgID int64, opts ...IdentityOpts) Requester {\n+\t// Create a copy of the ServiceIdentityClaims to avoid modifying the global one.\n+\t// Some of the options might mutate it.\n+\tclaimsCopy := *ServiceIdentityClaims",
        "comment_created_at": "2025-07-04T08:06:40+00:00",
        "comment_author": "macabu",
        "comment_body": "Previously this was taking a pointer to a global variable, and if we mutate that with the options, well, it mutates the global variable and we don't want that ðŸ˜… ",
        "pr_file_module": null
      },
      {
        "comment_id": "2185633394",
        "repo_full_name": "grafana/grafana",
        "pr_number": 107581,
        "pr_file": "pkg/apimachinery/identity/context.go",
        "discussion_id": "2184688299",
        "commented_code": "@@ -36,8 +36,22 @@ const (\n \tserviceNameForProvisioning = \"provisioning\"\n )\n \n-func newInternalIdentity(name string, namespace string, orgID int64) Requester {\n-\treturn &StaticRequester{\n+type IdentityOpts func(*StaticRequester)\n+\n+// WithServiceIdentityName sets the `StaticRequester.AccessTokenClaims.Rest.ServiceIdentity` field to the provided name.\n+// This is so far only used by Secrets Manager to identify and gate the service decrypting a secret.\n+func WithServiceIdentityName(name string) IdentityOpts {\n+\treturn func(r *StaticRequester) {\n+\t\tr.AccessTokenClaims.Rest.ServiceIdentity = name\n+\t}\n+}\n+\n+func newInternalIdentity(name string, namespace string, orgID int64, opts ...IdentityOpts) Requester {\n+\t// Create a copy of the ServiceIdentityClaims to avoid modifying the global one.\n+\t// Some of the options might mutate it.\n+\tclaimsCopy := *ServiceIdentityClaims",
        "comment_created_at": "2025-07-04T15:17:10+00:00",
        "comment_author": "gamab",
        "comment_body": "Good catch",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2177521218",
    "pr_number": 107305,
    "pr_file": "pkg/storage/unified/resource/storage_backend.go",
    "created_at": "2025-07-01T12:47:39+00:00",
    "commented_code": "+package resource\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"encoding/json\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"math/rand/v2\"\n+\t\"net/http\"\n+\t\"sort\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/bwmarrin/snowflake\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/utils\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+)\n+\n+const (\n+\tdefaultListBufferSize = 100\n+)\n+\n+// Unified storage backend based on KV storage.\n+type kvStorageBackend struct {\n+\tsnowflake  *snowflake.Node\n+\tkv         KV\n+\tdataStore  *dataStore\n+\tmetaStore  *metadataStore\n+\teventStore *eventStore\n+\tnotifier   *notifier\n+\tbuilder    DocumentBuilder\n+}\n+\n+var _ StorageBackend = &kvStorageBackend{}\n+\n+func NewkvStorageBackend(kv KV) *kvStorageBackend {\n+\ts, err := snowflake.NewNode(rand.Int64N(1024))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\teventStore := newEventStore(kv)\n+\treturn &kvStorageBackend{\n+\t\tkv:         kv,\n+\t\tdataStore:  newDataStore(kv),\n+\t\tmetaStore:  newMetadataStore(kv),\n+\t\teventStore: eventStore,\n+\t\tnotifier:   newNotifier(eventStore, notifierOptions{}),\n+\t\tsnowflake:  s,\n+\t\tbuilder:    StandardDocumentBuilder(), // For now we use the standard document builder.\n+\t}\n+}\n+\n+// WriteEvent writes a resource event (create/update/delete) to the storage backend.\n+func (k *kvStorageBackend) WriteEvent(ctx context.Context, event WriteEvent) (int64, error) {\n+\tif err := event.Validate(); err != nil {\n+\t\treturn 0, fmt.Errorf(\"invalid event: %w\", err)\n+\t}\n+\trv := k.snowflake.Generate().Int64()\n+\n+\t// Write data.\n+\tvar action DataAction\n+\tswitch event.Type {\n+\tcase resourcepb.WatchEvent_ADDED:\n+\t\taction = DataActionCreated\n+\t\t// Check if resource already exists for create operations\n+\t\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\t\tNamespace: event.Key.Namespace,\n+\t\t\tGroup:     event.Key.Group,\n+\t\t\tResource:  event.Key.Resource,\n+\t\t\tName:      event.Key.Name,\n+\t\t})\n+\t\tif err == nil {\n+\t\t\t// Resource exists, return already exists error\n+\t\t\treturn 0, ErrResourceAlreadyExists\n+\t\t}\n+\t\tif !errors.Is(err, ErrNotFound) {\n+\t\t\t// Some other error occurred\n+\t\t\treturn 0, fmt.Errorf(\"failed to check if resource exists: %w\", err)\n+\t\t}\n+\tcase resourcepb.WatchEvent_MODIFIED:\n+\t\taction = DataActionUpdated\n+\tcase resourcepb.WatchEvent_DELETED:\n+\t\taction = DataActionDeleted\n+\tdefault:\n+\t\treturn 0, fmt.Errorf(\"invalid event type: %d\", event.Type)\n+\t}\n+\n+\t// Build the search document\n+\tdoc, err := k.builder.BuildDocument(ctx, event.Key, rv, event.Value)\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to build document: %w\", err)\n+\t}\n+\n+\t// Write the data\n+\terr = k.dataStore.Save(ctx, DataKey{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t}, io.NopCloser(bytes.NewReader(event.Value)))\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write data: %w\", err)\n+\t}\n+\n+\t// Write metadata\n+\terr = k.metaStore.Save(ctx, MetaDataObj{\n+\t\tKey: MetaDataKey{\n+\t\t\tNamespace:       event.Key.Namespace,\n+\t\t\tGroup:           event.Key.Group,\n+\t\t\tResource:        event.Key.Resource,\n+\t\t\tName:            event.Key.Name,\n+\t\t\tResourceVersion: rv,\n+\t\t\tAction:          action,\n+\t\t\tFolder:          event.Object.GetFolder(),\n+\t\t},\n+\t\tValue: MetaData{\n+\t\t\tIndexableDocument: *doc,\n+\t\t},\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write metadata: %w\", err)\n+\t}\n+\n+\t// Write event\n+\terr = k.eventStore.Save(ctx, Event{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t\tFolder:          event.Object.GetFolder(),\n+\t\tPreviousRV:      event.PreviousRV,\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to save event: %w\", err)\n+\t}\n+\treturn rv, nil\n+}\n+\n+func (k *kvStorageBackend) ReadResource(ctx context.Context, req *resourcepb.ReadRequest) *BackendReadResponse {\n+\tif req.Key == nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusBadRequest, Message: \"missing key\"}}\n+\t}\n+\tmeta, err := k.metaStore.GetResourceKeyAtRevision(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Key.Namespace,\n+\t\tGroup:     req.Key.Group,\n+\t\tResource:  req.Key.Resource,\n+\t\tName:      req.Key.Name,\n+\t}, req.ResourceVersion)\n+\tif errors.Is(err, ErrNotFound) {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusNotFound, Message: \"not found\"}}\n+\t} else if err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tdata, err := k.dataStore.Get(ctx, DataKey{\n+\t\tNamespace:       req.Key.Namespace,\n+\t\tGroup:           req.Key.Group,\n+\t\tResource:        req.Key.Resource,\n+\t\tName:            req.Key.Name,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tAction:          meta.Action,\n+\t})\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tvalue, err := io.ReadAll(data)\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\treturn &BackendReadResponse{\n+\t\tKey:             req.Key,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tValue:           value,\n+\t\tFolder:          meta.Folder,\n+\t}\n+}\n+\n+// // ListIterator returns an iterator for listing resources.\n+func (k *kvStorageBackend) ListIterator(ctx context.Context, req *resourcepb.ListRequest, cb func(ListIterator) error) (int64, error) {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn 0, fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\t// Parse continue token if provided\n+\toffset := int64(0)\n+\tresourceVersion := req.ResourceVersion\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\toffset = token.StartOffset\n+\t\tresourceVersion = token.ResourceVersion\n+\t}\n+\n+\t// We set the listRV to the current time.\n+\tlistRV := k.snowflake.Generate().Int64()\n+\tif resourceVersion > 0 {\n+\t\tlistRV = resourceVersion\n+\t}\n+\n+\t// Fetch the latest objects\n+\tkeys := make([]MetaDataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\tidx := 0\n+\tfor metaKey, err := range k.metaStore.ListResourceKeysAtRevision(ctx, MetaListRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t}, resourceVersion) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\t// Skip the first offset items. This is not efficient, but it's a simple way to implement it for now.\n+\t\tif idx < int(offset) {\n+\t\t\tidx++\n+\t\t\tcontinue\n+\t\t}\n+\t\tkeys = append(keys, metaKey)\n+\t\t// Only fetch the first limit items + 1 to get the next token.\n+\t\tif len(keys) >= int(req.Limit+1) {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\titer := kvListIterator{\n+\t\tkeys:         keys,\n+\t\tcurrentIndex: -1,\n+\t\tctx:          ctx,\n+\t\tlistRV:       listRV,\n+\t\toffset:       offset,\n+\t\tdataStore:    k.dataStore,\n+\t}\n+\terr := cb(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvListIterator implements ListIterator for KV storage\n+type kvListIterator struct {\n+\tctx          context.Context\n+\tkeys         []MetaDataKey\n+\tcurrentIndex int\n+\tdataStore    *dataStore\n+\tlistRV       int64\n+\toffset       int64\n+\n+\t// current\n+\trv    int64\n+\terr   error\n+\tvalue []byte\n+}\n+\n+func (i *kvListIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\ti.rv, i.err = i.keys[i.currentIndex].ResourceVersion, nil\n+\n+\tdata, err := i.dataStore.Get(i.ctx, DataKey{\n+\t\tNamespace:       i.keys[i.currentIndex].Namespace,\n+\t\tGroup:           i.keys[i.currentIndex].Group,\n+\t\tResource:        i.keys[i.currentIndex].Resource,\n+\t\tName:            i.keys[i.currentIndex].Name,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tAction:          i.keys[i.currentIndex].Action,\n+\t})\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\ti.value, i.err = io.ReadAll(data)\n+\tif i.err != nil {\n+\t\treturn false\n+\t}\n+\n+\t// increment the offset\n+\ti.offset++\n+\n+\treturn true\n+}\n+\n+func (i *kvListIterator) Error() error {\n+\treturn nil\n+}\n+\n+func (i *kvListIterator) ContinueToken() string {\n+\treturn ContinueToken{\n+\t\tStartOffset:     i.offset,\n+\t\tResourceVersion: i.listRV,\n+\t}.String()\n+}\n+\n+func (i *kvListIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvListIterator) Namespace() string {\n+\treturn i.keys[i.currentIndex].Namespace\n+}\n+\n+func (i *kvListIterator) Name() string {\n+\treturn i.keys[i.currentIndex].Name\n+}\n+\n+func (i *kvListIterator) Folder() string {\n+\treturn i.keys[i.currentIndex].Folder\n+}\n+\n+func (i *kvListIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+func validateListHistoryRequest(req *resourcepb.ListRequest) error {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\tkey := req.Options.Key\n+\tif key.Group == \"\" {\n+\t\treturn fmt.Errorf(\"group is required\")\n+\t}\n+\tif key.Resource == \"\" {\n+\t\treturn fmt.Errorf(\"resource is required\")\n+\t}\n+\tif key.Namespace == \"\" {\n+\t\treturn fmt.Errorf(\"namespace is required\")\n+\t}\n+\tif key.Name == \"\" {\n+\t\treturn fmt.Errorf(\"name is required\")\n+\t}\n+\treturn nil\n+}\n+\n+// filterHistoryKeysByVersion filters history keys based on version match criteria\n+func filterHistoryKeysByVersion(historyKeys []DataKey, req *resourcepb.ListRequest) ([]DataKey, error) {\n+\tswitch req.GetVersionMatchV2() {\n+\tcase resourcepb.ResourceVersionMatchV2_Exact:\n+\t\tif req.ResourceVersion <= 0 {\n+\t\t\treturn nil, fmt.Errorf(\"expecting an explicit resource version query when using Exact matching\")\n+\t\t}\n+\t\tvar exactKeys []DataKey\n+\t\tfor _, key := range historyKeys {\n+\t\t\tif key.ResourceVersion == req.ResourceVersion {\n+\t\t\t\texactKeys = append(exactKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn exactKeys, nil\n+\tcase resourcepb.ResourceVersionMatchV2_NotOlderThan:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion >= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\tdefault:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion <= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\t}\n+\treturn historyKeys, nil\n+}\n+\n+// applyLiveHistoryFilter applies \"live\" history logic by ignoring events before the last delete\n+func applyLiveHistoryFilter(filteredKeys []DataKey, req *resourcepb.ListRequest) []DataKey {\n+\tuseLatestDeletionAsMinRV := req.ResourceVersion == 0 && req.Source != resourcepb.ListRequest_TRASH && req.GetVersionMatchV2() != resourcepb.ResourceVersionMatchV2_Exact\n+\tif !useLatestDeletionAsMinRV {\n+\t\treturn filteredKeys\n+\t}\n+\n+\tlatestDeleteRV := int64(0)\n+\tfor _, key := range filteredKeys {\n+\t\tif key.Action == DataActionDeleted && key.ResourceVersion > latestDeleteRV {\n+\t\t\tlatestDeleteRV = key.ResourceVersion\n+\t\t}\n+\t}\n+\tif latestDeleteRV > 0 {\n+\t\tvar liveKeys []DataKey\n+\t\tfor _, key := range filteredKeys {\n+\t\t\tif key.ResourceVersion > latestDeleteRV {\n+\t\t\t\tliveKeys = append(liveKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn liveKeys\n+\t}\n+\treturn filteredKeys\n+}\n+\n+// sortHistoryKeys sorts the history keys based on the sortAscending flag\n+func sortHistoryKeys(filteredKeys []DataKey, sortAscending bool) {\n+\tif sortAscending {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion < filteredKeys[j].ResourceVersion\n+\t\t})\n+\t} else {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion > filteredKeys[j].ResourceVersion\n+\t\t})\n+\t}\n+}\n+\n+// applyPagination filters keys based on pagination parameters\n+func applyPagination(keys []DataKey, lastSeenRV int64, sortAscending bool) []DataKey {\n+\tif lastSeenRV == 0 {\n+\t\treturn keys\n+\t}\n+\n+\tvar pagedKeys []DataKey\n+\tfor _, key := range keys {\n+\t\tif sortAscending && key.ResourceVersion > lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t} else if !sortAscending && key.ResourceVersion < lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t}\n+\t}\n+\treturn pagedKeys\n+}\n+\n+// ListHistory is like ListIterator, but it returns the history of a resource.\n+func (k *kvStorageBackend) ListHistory(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error) (int64, error) {\n+\tif err := validateListHistoryRequest(req); err != nil {\n+\t\treturn 0, err\n+\t}\n+\tkey := req.Options.Key\n+\t// Parse continue token if provided\n+\tlastSeenRV := int64(0)\n+\tsortAscending := req.GetVersionMatchV2() == resourcepb.ResourceVersionMatchV2_NotOlderThan\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\tlastSeenRV = token.ResourceVersion\n+\t\tsortAscending = token.SortAscending\n+\t}\n+\n+\t// Generate a new resource version for the list\n+\tlistRV := k.snowflake.Generate().Int64()\n+\n+\t// Get all history entries by iterating through datastore keys\n+\thistoryKeys := make([]DataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\n+\t// Use datastore.Keys to get all data keys for this specific resource\n+\tfor dataKey, err := range k.dataStore.Keys(ctx, ListRequestKey{\n+\t\tNamespace: key.Namespace,\n+\t\tGroup:     key.Group,\n+\t\tResource:  key.Resource,\n+\t\tName:      key.Name,\n+\t}) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\thistoryKeys = append(historyKeys, dataKey)\n+\t}\n+\n+\t// Check if context has been cancelled\n+\tif ctx.Err() != nil {\n+\t\treturn 0, ctx.Err()\n+\t}\n+\n+\t// Handle trash differently from regular history\n+\tif req.Source == resourcepb.ListRequest_TRASH {\n+\t\treturn k.processTrashEntries(ctx, req, fn, historyKeys, lastSeenRV, sortAscending, listRV)\n+\t}\n+\n+\t// Apply filtering based on version match\n+\tfilteredKeys, filterErr := filterHistoryKeysByVersion(historyKeys, req)\n+\tif filterErr != nil {\n+\t\treturn 0, filterErr\n+\t}\n+\n+\t// Apply \"live\" history logic: ignore events before the last delete\n+\tfilteredKeys = applyLiveHistoryFilter(filteredKeys, req)\n+\n+\t// Sort the entries if not already sorted correctly\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr := fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// processTrashEntries handles the special case of listing deleted items (trash)\n+func (k *kvStorageBackend) processTrashEntries(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error, historyKeys []DataKey, lastSeenRV int64, sortAscending bool, listRV int64) (int64, error) {\n+\t// Filter to only deleted entries\n+\tvar deletedKeys []DataKey\n+\tfor _, key := range historyKeys {\n+\t\tif key.Action == DataActionDeleted {\n+\t\t\tdeletedKeys = append(deletedKeys, key)\n+\t\t}\n+\t}\n+\n+\t// Check if the resource currently exists (is live)\n+\t// If it exists, don't return any trash entries\n+\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t})\n+\n+\tvar trashKeys []DataKey\n+\tif errors.Is(err, ErrNotFound) {\n+\t\t// Resource doesn't exist currently, so we can return the latest delete\n+\t\t// Find the latest delete event\n+\t\tvar latestDelete *DataKey\n+\t\tfor _, key := range deletedKeys {\n+\t\t\tif latestDelete == nil || key.ResourceVersion > latestDelete.ResourceVersion {\n+\t\t\t\tlatestDelete = &key\n+\t\t\t}\n+\t\t}\n+\t\tif latestDelete != nil {\n+\t\t\ttrashKeys = append(trashKeys, *latestDelete)\n+\t\t}\n+\t}\n+\t// If err != ErrNotFound, the resource exists, so no trash entries should be returned\n+\n+\t// Apply version filtering\n+\tfilteredKeys, err := filterHistoryKeysByVersion(trashKeys, req)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\t// Sort the entries\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr = fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvHistoryIterator implements ListIterator for KV storage history\n+type kvHistoryIterator struct {\n+\tctx           context.Context\n+\tkeys          []DataKey\n+\tcurrentIndex  int\n+\tlistRV        int64\n+\tsortAscending bool\n+\tdataStore     *dataStore\n+\n+\t// current\n+\trv     int64\n+\terr    error\n+\tvalue  []byte\n+\tfolder string\n+}\n+\n+func (i *kvHistoryIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\tkey := i.keys[i.currentIndex]\n+\ti.rv = key.ResourceVersion\n+\n+\t// Read the value from the ReadCloser\n+\tdata, err := i.dataStore.Get(i.ctx, key)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.value, err = io.ReadAll(data)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\t// Extract the folder from the meta data\n+\tpartial := &metav1.PartialObjectMetadata{}\n+\terr = json.Unmarshal(i.value, partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\tmeta, err := utils.MetaAccessor(partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.folder = meta.GetFolder()\n+\ti.err = nil\n+\n+\treturn true\n+}\n+\n+func (i *kvHistoryIterator) Error() error {\n+\treturn i.err\n+}\n+\n+func (i *kvHistoryIterator) ContinueToken() string {\n+\tif i.currentIndex < 0 || i.currentIndex >= len(i.keys) {\n+\t\treturn \"\"\n+\t}\n+\ttoken := ContinueToken{\n+\t\tStartOffset:     i.rv,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tSortAscending:   i.sortAscending,\n+\t}\n+\treturn token.String()\n+}\n+\n+func (i *kvHistoryIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvHistoryIterator) Namespace() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Namespace\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Name() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Name\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Folder() string {\n+\treturn i.folder\n+}\n+\n+func (i *kvHistoryIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+// WatchWriteEvents returns a channel that receives write events.\n+func (k *kvStorageBackend) WatchWriteEvents(ctx context.Context) (<-chan *WrittenEvent, error) {\n+\t// Create a channel to receive events\n+\tevents := make(chan *WrittenEvent, 10000) // TODO: make this configurable\n+\n+\tnotifierEvents := k.notifier.Watch(ctx, defaultWatchOptions())\n+\tgo func() {\n+\t\tfor event := range notifierEvents {\n+\t\t\t// fetch the data\n+\t\t\tdataReader, err := k.dataStore.Get(ctx, DataKey{\n+\t\t\t\tNamespace:       event.Namespace,\n+\t\t\t\tGroup:           event.Group,\n+\t\t\t\tResource:        event.Resource,\n+\t\t\t\tName:            event.Name,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tAction:          event.Action,\n+\t\t\t})\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tdata, err := io.ReadAll(dataReader)\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tvar t resourcepb.WatchEvent_Type\n+\t\t\tswitch event.Action {\n+\t\t\tcase DataActionCreated:\n+\t\t\t\tt = resourcepb.WatchEvent_ADDED\n+\t\t\tcase DataActionUpdated:\n+\t\t\t\tt = resourcepb.WatchEvent_MODIFIED\n+\t\t\tcase DataActionDeleted:\n+\t\t\t\tt = resourcepb.WatchEvent_DELETED\n+\t\t\t}\n+\n+\t\t\tevents <- &WrittenEvent{\n+\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\tNamespace: event.Namespace,\n+\t\t\t\t\tGroup:     event.Group,\n+\t\t\t\t\tResource:  event.Resource,\n+\t\t\t\t\tName:      event.Name,\n+\t\t\t\t},\n+\t\t\t\tType:            t,\n+\t\t\t\tFolder:          event.Folder,\n+\t\t\t\tValue:           data,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tPreviousRV:      event.PreviousRV,\n+\t\t\t\tTimestamp:       event.ResourceVersion / time.Second.Nanoseconds(), // convert to seconds\n+\t\t\t}\n+\t\t}\n+\t\tclose(events)",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "2177521218",
        "repo_full_name": "grafana/grafana",
        "pr_number": 107305,
        "pr_file": "pkg/storage/unified/resource/storage_backend.go",
        "discussion_id": "2177521218",
        "commented_code": "@@ -0,0 +1,777 @@\n+package resource\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"encoding/json\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"math/rand/v2\"\n+\t\"net/http\"\n+\t\"sort\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/bwmarrin/snowflake\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/utils\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+)\n+\n+const (\n+\tdefaultListBufferSize = 100\n+)\n+\n+// Unified storage backend based on KV storage.\n+type kvStorageBackend struct {\n+\tsnowflake  *snowflake.Node\n+\tkv         KV\n+\tdataStore  *dataStore\n+\tmetaStore  *metadataStore\n+\teventStore *eventStore\n+\tnotifier   *notifier\n+\tbuilder    DocumentBuilder\n+}\n+\n+var _ StorageBackend = &kvStorageBackend{}\n+\n+func NewkvStorageBackend(kv KV) *kvStorageBackend {\n+\ts, err := snowflake.NewNode(rand.Int64N(1024))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\teventStore := newEventStore(kv)\n+\treturn &kvStorageBackend{\n+\t\tkv:         kv,\n+\t\tdataStore:  newDataStore(kv),\n+\t\tmetaStore:  newMetadataStore(kv),\n+\t\teventStore: eventStore,\n+\t\tnotifier:   newNotifier(eventStore, notifierOptions{}),\n+\t\tsnowflake:  s,\n+\t\tbuilder:    StandardDocumentBuilder(), // For now we use the standard document builder.\n+\t}\n+}\n+\n+// WriteEvent writes a resource event (create/update/delete) to the storage backend.\n+func (k *kvStorageBackend) WriteEvent(ctx context.Context, event WriteEvent) (int64, error) {\n+\tif err := event.Validate(); err != nil {\n+\t\treturn 0, fmt.Errorf(\"invalid event: %w\", err)\n+\t}\n+\trv := k.snowflake.Generate().Int64()\n+\n+\t// Write data.\n+\tvar action DataAction\n+\tswitch event.Type {\n+\tcase resourcepb.WatchEvent_ADDED:\n+\t\taction = DataActionCreated\n+\t\t// Check if resource already exists for create operations\n+\t\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\t\tNamespace: event.Key.Namespace,\n+\t\t\tGroup:     event.Key.Group,\n+\t\t\tResource:  event.Key.Resource,\n+\t\t\tName:      event.Key.Name,\n+\t\t})\n+\t\tif err == nil {\n+\t\t\t// Resource exists, return already exists error\n+\t\t\treturn 0, ErrResourceAlreadyExists\n+\t\t}\n+\t\tif !errors.Is(err, ErrNotFound) {\n+\t\t\t// Some other error occurred\n+\t\t\treturn 0, fmt.Errorf(\"failed to check if resource exists: %w\", err)\n+\t\t}\n+\tcase resourcepb.WatchEvent_MODIFIED:\n+\t\taction = DataActionUpdated\n+\tcase resourcepb.WatchEvent_DELETED:\n+\t\taction = DataActionDeleted\n+\tdefault:\n+\t\treturn 0, fmt.Errorf(\"invalid event type: %d\", event.Type)\n+\t}\n+\n+\t// Build the search document\n+\tdoc, err := k.builder.BuildDocument(ctx, event.Key, rv, event.Value)\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to build document: %w\", err)\n+\t}\n+\n+\t// Write the data\n+\terr = k.dataStore.Save(ctx, DataKey{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t}, io.NopCloser(bytes.NewReader(event.Value)))\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write data: %w\", err)\n+\t}\n+\n+\t// Write metadata\n+\terr = k.metaStore.Save(ctx, MetaDataObj{\n+\t\tKey: MetaDataKey{\n+\t\t\tNamespace:       event.Key.Namespace,\n+\t\t\tGroup:           event.Key.Group,\n+\t\t\tResource:        event.Key.Resource,\n+\t\t\tName:            event.Key.Name,\n+\t\t\tResourceVersion: rv,\n+\t\t\tAction:          action,\n+\t\t\tFolder:          event.Object.GetFolder(),\n+\t\t},\n+\t\tValue: MetaData{\n+\t\t\tIndexableDocument: *doc,\n+\t\t},\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to write metadata: %w\", err)\n+\t}\n+\n+\t// Write event\n+\terr = k.eventStore.Save(ctx, Event{\n+\t\tNamespace:       event.Key.Namespace,\n+\t\tGroup:           event.Key.Group,\n+\t\tResource:        event.Key.Resource,\n+\t\tName:            event.Key.Name,\n+\t\tResourceVersion: rv,\n+\t\tAction:          action,\n+\t\tFolder:          event.Object.GetFolder(),\n+\t\tPreviousRV:      event.PreviousRV,\n+\t})\n+\tif err != nil {\n+\t\treturn 0, fmt.Errorf(\"failed to save event: %w\", err)\n+\t}\n+\treturn rv, nil\n+}\n+\n+func (k *kvStorageBackend) ReadResource(ctx context.Context, req *resourcepb.ReadRequest) *BackendReadResponse {\n+\tif req.Key == nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusBadRequest, Message: \"missing key\"}}\n+\t}\n+\tmeta, err := k.metaStore.GetResourceKeyAtRevision(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Key.Namespace,\n+\t\tGroup:     req.Key.Group,\n+\t\tResource:  req.Key.Resource,\n+\t\tName:      req.Key.Name,\n+\t}, req.ResourceVersion)\n+\tif errors.Is(err, ErrNotFound) {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusNotFound, Message: \"not found\"}}\n+\t} else if err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tdata, err := k.dataStore.Get(ctx, DataKey{\n+\t\tNamespace:       req.Key.Namespace,\n+\t\tGroup:           req.Key.Group,\n+\t\tResource:        req.Key.Resource,\n+\t\tName:            req.Key.Name,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tAction:          meta.Action,\n+\t})\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\tvalue, err := io.ReadAll(data)\n+\tif err != nil {\n+\t\treturn &BackendReadResponse{Error: &resourcepb.ErrorResult{Code: http.StatusInternalServerError, Message: err.Error()}}\n+\t}\n+\treturn &BackendReadResponse{\n+\t\tKey:             req.Key,\n+\t\tResourceVersion: meta.ResourceVersion,\n+\t\tValue:           value,\n+\t\tFolder:          meta.Folder,\n+\t}\n+}\n+\n+// // ListIterator returns an iterator for listing resources.\n+func (k *kvStorageBackend) ListIterator(ctx context.Context, req *resourcepb.ListRequest, cb func(ListIterator) error) (int64, error) {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn 0, fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\t// Parse continue token if provided\n+\toffset := int64(0)\n+\tresourceVersion := req.ResourceVersion\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\toffset = token.StartOffset\n+\t\tresourceVersion = token.ResourceVersion\n+\t}\n+\n+\t// We set the listRV to the current time.\n+\tlistRV := k.snowflake.Generate().Int64()\n+\tif resourceVersion > 0 {\n+\t\tlistRV = resourceVersion\n+\t}\n+\n+\t// Fetch the latest objects\n+\tkeys := make([]MetaDataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\tidx := 0\n+\tfor metaKey, err := range k.metaStore.ListResourceKeysAtRevision(ctx, MetaListRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t}, resourceVersion) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\t// Skip the first offset items. This is not efficient, but it's a simple way to implement it for now.\n+\t\tif idx < int(offset) {\n+\t\t\tidx++\n+\t\t\tcontinue\n+\t\t}\n+\t\tkeys = append(keys, metaKey)\n+\t\t// Only fetch the first limit items + 1 to get the next token.\n+\t\tif len(keys) >= int(req.Limit+1) {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\titer := kvListIterator{\n+\t\tkeys:         keys,\n+\t\tcurrentIndex: -1,\n+\t\tctx:          ctx,\n+\t\tlistRV:       listRV,\n+\t\toffset:       offset,\n+\t\tdataStore:    k.dataStore,\n+\t}\n+\terr := cb(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvListIterator implements ListIterator for KV storage\n+type kvListIterator struct {\n+\tctx          context.Context\n+\tkeys         []MetaDataKey\n+\tcurrentIndex int\n+\tdataStore    *dataStore\n+\tlistRV       int64\n+\toffset       int64\n+\n+\t// current\n+\trv    int64\n+\terr   error\n+\tvalue []byte\n+}\n+\n+func (i *kvListIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\ti.rv, i.err = i.keys[i.currentIndex].ResourceVersion, nil\n+\n+\tdata, err := i.dataStore.Get(i.ctx, DataKey{\n+\t\tNamespace:       i.keys[i.currentIndex].Namespace,\n+\t\tGroup:           i.keys[i.currentIndex].Group,\n+\t\tResource:        i.keys[i.currentIndex].Resource,\n+\t\tName:            i.keys[i.currentIndex].Name,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tAction:          i.keys[i.currentIndex].Action,\n+\t})\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\ti.value, i.err = io.ReadAll(data)\n+\tif i.err != nil {\n+\t\treturn false\n+\t}\n+\n+\t// increment the offset\n+\ti.offset++\n+\n+\treturn true\n+}\n+\n+func (i *kvListIterator) Error() error {\n+\treturn nil\n+}\n+\n+func (i *kvListIterator) ContinueToken() string {\n+\treturn ContinueToken{\n+\t\tStartOffset:     i.offset,\n+\t\tResourceVersion: i.listRV,\n+\t}.String()\n+}\n+\n+func (i *kvListIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvListIterator) Namespace() string {\n+\treturn i.keys[i.currentIndex].Namespace\n+}\n+\n+func (i *kvListIterator) Name() string {\n+\treturn i.keys[i.currentIndex].Name\n+}\n+\n+func (i *kvListIterator) Folder() string {\n+\treturn i.keys[i.currentIndex].Folder\n+}\n+\n+func (i *kvListIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+func validateListHistoryRequest(req *resourcepb.ListRequest) error {\n+\tif req.Options == nil || req.Options.Key == nil {\n+\t\treturn fmt.Errorf(\"missing options or key in ListRequest\")\n+\t}\n+\tkey := req.Options.Key\n+\tif key.Group == \"\" {\n+\t\treturn fmt.Errorf(\"group is required\")\n+\t}\n+\tif key.Resource == \"\" {\n+\t\treturn fmt.Errorf(\"resource is required\")\n+\t}\n+\tif key.Namespace == \"\" {\n+\t\treturn fmt.Errorf(\"namespace is required\")\n+\t}\n+\tif key.Name == \"\" {\n+\t\treturn fmt.Errorf(\"name is required\")\n+\t}\n+\treturn nil\n+}\n+\n+// filterHistoryKeysByVersion filters history keys based on version match criteria\n+func filterHistoryKeysByVersion(historyKeys []DataKey, req *resourcepb.ListRequest) ([]DataKey, error) {\n+\tswitch req.GetVersionMatchV2() {\n+\tcase resourcepb.ResourceVersionMatchV2_Exact:\n+\t\tif req.ResourceVersion <= 0 {\n+\t\t\treturn nil, fmt.Errorf(\"expecting an explicit resource version query when using Exact matching\")\n+\t\t}\n+\t\tvar exactKeys []DataKey\n+\t\tfor _, key := range historyKeys {\n+\t\t\tif key.ResourceVersion == req.ResourceVersion {\n+\t\t\t\texactKeys = append(exactKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn exactKeys, nil\n+\tcase resourcepb.ResourceVersionMatchV2_NotOlderThan:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion >= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\tdefault:\n+\t\tif req.ResourceVersion > 0 {\n+\t\t\tvar filteredKeys []DataKey\n+\t\t\tfor _, key := range historyKeys {\n+\t\t\t\tif key.ResourceVersion <= req.ResourceVersion {\n+\t\t\t\t\tfilteredKeys = append(filteredKeys, key)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn filteredKeys, nil\n+\t\t}\n+\t}\n+\treturn historyKeys, nil\n+}\n+\n+// applyLiveHistoryFilter applies \"live\" history logic by ignoring events before the last delete\n+func applyLiveHistoryFilter(filteredKeys []DataKey, req *resourcepb.ListRequest) []DataKey {\n+\tuseLatestDeletionAsMinRV := req.ResourceVersion == 0 && req.Source != resourcepb.ListRequest_TRASH && req.GetVersionMatchV2() != resourcepb.ResourceVersionMatchV2_Exact\n+\tif !useLatestDeletionAsMinRV {\n+\t\treturn filteredKeys\n+\t}\n+\n+\tlatestDeleteRV := int64(0)\n+\tfor _, key := range filteredKeys {\n+\t\tif key.Action == DataActionDeleted && key.ResourceVersion > latestDeleteRV {\n+\t\t\tlatestDeleteRV = key.ResourceVersion\n+\t\t}\n+\t}\n+\tif latestDeleteRV > 0 {\n+\t\tvar liveKeys []DataKey\n+\t\tfor _, key := range filteredKeys {\n+\t\t\tif key.ResourceVersion > latestDeleteRV {\n+\t\t\t\tliveKeys = append(liveKeys, key)\n+\t\t\t}\n+\t\t}\n+\t\treturn liveKeys\n+\t}\n+\treturn filteredKeys\n+}\n+\n+// sortHistoryKeys sorts the history keys based on the sortAscending flag\n+func sortHistoryKeys(filteredKeys []DataKey, sortAscending bool) {\n+\tif sortAscending {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion < filteredKeys[j].ResourceVersion\n+\t\t})\n+\t} else {\n+\t\tsort.Slice(filteredKeys, func(i, j int) bool {\n+\t\t\treturn filteredKeys[i].ResourceVersion > filteredKeys[j].ResourceVersion\n+\t\t})\n+\t}\n+}\n+\n+// applyPagination filters keys based on pagination parameters\n+func applyPagination(keys []DataKey, lastSeenRV int64, sortAscending bool) []DataKey {\n+\tif lastSeenRV == 0 {\n+\t\treturn keys\n+\t}\n+\n+\tvar pagedKeys []DataKey\n+\tfor _, key := range keys {\n+\t\tif sortAscending && key.ResourceVersion > lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t} else if !sortAscending && key.ResourceVersion < lastSeenRV {\n+\t\t\tpagedKeys = append(pagedKeys, key)\n+\t\t}\n+\t}\n+\treturn pagedKeys\n+}\n+\n+// ListHistory is like ListIterator, but it returns the history of a resource.\n+func (k *kvStorageBackend) ListHistory(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error) (int64, error) {\n+\tif err := validateListHistoryRequest(req); err != nil {\n+\t\treturn 0, err\n+\t}\n+\tkey := req.Options.Key\n+\t// Parse continue token if provided\n+\tlastSeenRV := int64(0)\n+\tsortAscending := req.GetVersionMatchV2() == resourcepb.ResourceVersionMatchV2_NotOlderThan\n+\tif req.NextPageToken != \"\" {\n+\t\ttoken, err := GetContinueToken(req.NextPageToken)\n+\t\tif err != nil {\n+\t\t\treturn 0, fmt.Errorf(\"invalid continue token: %w\", err)\n+\t\t}\n+\t\tlastSeenRV = token.ResourceVersion\n+\t\tsortAscending = token.SortAscending\n+\t}\n+\n+\t// Generate a new resource version for the list\n+\tlistRV := k.snowflake.Generate().Int64()\n+\n+\t// Get all history entries by iterating through datastore keys\n+\thistoryKeys := make([]DataKey, 0, min(defaultListBufferSize, req.Limit+1))\n+\n+\t// Use datastore.Keys to get all data keys for this specific resource\n+\tfor dataKey, err := range k.dataStore.Keys(ctx, ListRequestKey{\n+\t\tNamespace: key.Namespace,\n+\t\tGroup:     key.Group,\n+\t\tResource:  key.Resource,\n+\t\tName:      key.Name,\n+\t}) {\n+\t\tif err != nil {\n+\t\t\treturn 0, err\n+\t\t}\n+\t\thistoryKeys = append(historyKeys, dataKey)\n+\t}\n+\n+\t// Check if context has been cancelled\n+\tif ctx.Err() != nil {\n+\t\treturn 0, ctx.Err()\n+\t}\n+\n+\t// Handle trash differently from regular history\n+\tif req.Source == resourcepb.ListRequest_TRASH {\n+\t\treturn k.processTrashEntries(ctx, req, fn, historyKeys, lastSeenRV, sortAscending, listRV)\n+\t}\n+\n+\t// Apply filtering based on version match\n+\tfilteredKeys, filterErr := filterHistoryKeysByVersion(historyKeys, req)\n+\tif filterErr != nil {\n+\t\treturn 0, filterErr\n+\t}\n+\n+\t// Apply \"live\" history logic: ignore events before the last delete\n+\tfilteredKeys = applyLiveHistoryFilter(filteredKeys, req)\n+\n+\t// Sort the entries if not already sorted correctly\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr := fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// processTrashEntries handles the special case of listing deleted items (trash)\n+func (k *kvStorageBackend) processTrashEntries(ctx context.Context, req *resourcepb.ListRequest, fn func(ListIterator) error, historyKeys []DataKey, lastSeenRV int64, sortAscending bool, listRV int64) (int64, error) {\n+\t// Filter to only deleted entries\n+\tvar deletedKeys []DataKey\n+\tfor _, key := range historyKeys {\n+\t\tif key.Action == DataActionDeleted {\n+\t\t\tdeletedKeys = append(deletedKeys, key)\n+\t\t}\n+\t}\n+\n+\t// Check if the resource currently exists (is live)\n+\t// If it exists, don't return any trash entries\n+\t_, err := k.metaStore.GetLatestResourceKey(ctx, MetaGetRequestKey{\n+\t\tNamespace: req.Options.Key.Namespace,\n+\t\tGroup:     req.Options.Key.Group,\n+\t\tResource:  req.Options.Key.Resource,\n+\t\tName:      req.Options.Key.Name,\n+\t})\n+\n+\tvar trashKeys []DataKey\n+\tif errors.Is(err, ErrNotFound) {\n+\t\t// Resource doesn't exist currently, so we can return the latest delete\n+\t\t// Find the latest delete event\n+\t\tvar latestDelete *DataKey\n+\t\tfor _, key := range deletedKeys {\n+\t\t\tif latestDelete == nil || key.ResourceVersion > latestDelete.ResourceVersion {\n+\t\t\t\tlatestDelete = &key\n+\t\t\t}\n+\t\t}\n+\t\tif latestDelete != nil {\n+\t\t\ttrashKeys = append(trashKeys, *latestDelete)\n+\t\t}\n+\t}\n+\t// If err != ErrNotFound, the resource exists, so no trash entries should be returned\n+\n+\t// Apply version filtering\n+\tfilteredKeys, err := filterHistoryKeysByVersion(trashKeys, req)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\t// Sort the entries\n+\tsortHistoryKeys(filteredKeys, sortAscending)\n+\n+\t// Pagination: filter out items up to and including lastSeenRV\n+\tpagedKeys := applyPagination(filteredKeys, lastSeenRV, sortAscending)\n+\n+\titer := kvHistoryIterator{\n+\t\tkeys:          pagedKeys,\n+\t\tcurrentIndex:  -1,\n+\t\tctx:           ctx,\n+\t\tlistRV:        listRV,\n+\t\tsortAscending: sortAscending,\n+\t\tdataStore:     k.dataStore,\n+\t}\n+\n+\terr = fn(&iter)\n+\tif err != nil {\n+\t\treturn 0, err\n+\t}\n+\n+\treturn listRV, nil\n+}\n+\n+// kvHistoryIterator implements ListIterator for KV storage history\n+type kvHistoryIterator struct {\n+\tctx           context.Context\n+\tkeys          []DataKey\n+\tcurrentIndex  int\n+\tlistRV        int64\n+\tsortAscending bool\n+\tdataStore     *dataStore\n+\n+\t// current\n+\trv     int64\n+\terr    error\n+\tvalue  []byte\n+\tfolder string\n+}\n+\n+func (i *kvHistoryIterator) Next() bool {\n+\ti.currentIndex++\n+\n+\tif i.currentIndex >= len(i.keys) {\n+\t\treturn false\n+\t}\n+\n+\tkey := i.keys[i.currentIndex]\n+\ti.rv = key.ResourceVersion\n+\n+\t// Read the value from the ReadCloser\n+\tdata, err := i.dataStore.Get(i.ctx, key)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.value, err = io.ReadAll(data)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\t// Extract the folder from the meta data\n+\tpartial := &metav1.PartialObjectMetadata{}\n+\terr = json.Unmarshal(i.value, partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\n+\tmeta, err := utils.MetaAccessor(partial)\n+\tif err != nil {\n+\t\ti.err = err\n+\t\treturn false\n+\t}\n+\ti.folder = meta.GetFolder()\n+\ti.err = nil\n+\n+\treturn true\n+}\n+\n+func (i *kvHistoryIterator) Error() error {\n+\treturn i.err\n+}\n+\n+func (i *kvHistoryIterator) ContinueToken() string {\n+\tif i.currentIndex < 0 || i.currentIndex >= len(i.keys) {\n+\t\treturn \"\"\n+\t}\n+\ttoken := ContinueToken{\n+\t\tStartOffset:     i.rv,\n+\t\tResourceVersion: i.keys[i.currentIndex].ResourceVersion,\n+\t\tSortAscending:   i.sortAscending,\n+\t}\n+\treturn token.String()\n+}\n+\n+func (i *kvHistoryIterator) ResourceVersion() int64 {\n+\treturn i.rv\n+}\n+\n+func (i *kvHistoryIterator) Namespace() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Namespace\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Name() string {\n+\tif i.currentIndex >= 0 && i.currentIndex < len(i.keys) {\n+\t\treturn i.keys[i.currentIndex].Name\n+\t}\n+\treturn \"\"\n+}\n+\n+func (i *kvHistoryIterator) Folder() string {\n+\treturn i.folder\n+}\n+\n+func (i *kvHistoryIterator) Value() []byte {\n+\treturn i.value\n+}\n+\n+// WatchWriteEvents returns a channel that receives write events.\n+func (k *kvStorageBackend) WatchWriteEvents(ctx context.Context) (<-chan *WrittenEvent, error) {\n+\t// Create a channel to receive events\n+\tevents := make(chan *WrittenEvent, 10000) // TODO: make this configurable\n+\n+\tnotifierEvents := k.notifier.Watch(ctx, defaultWatchOptions())\n+\tgo func() {\n+\t\tfor event := range notifierEvents {\n+\t\t\t// fetch the data\n+\t\t\tdataReader, err := k.dataStore.Get(ctx, DataKey{\n+\t\t\t\tNamespace:       event.Namespace,\n+\t\t\t\tGroup:           event.Group,\n+\t\t\t\tResource:        event.Resource,\n+\t\t\t\tName:            event.Name,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tAction:          event.Action,\n+\t\t\t})\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tdata, err := io.ReadAll(dataReader)\n+\t\t\tif err != nil {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tvar t resourcepb.WatchEvent_Type\n+\t\t\tswitch event.Action {\n+\t\t\tcase DataActionCreated:\n+\t\t\t\tt = resourcepb.WatchEvent_ADDED\n+\t\t\tcase DataActionUpdated:\n+\t\t\t\tt = resourcepb.WatchEvent_MODIFIED\n+\t\t\tcase DataActionDeleted:\n+\t\t\t\tt = resourcepb.WatchEvent_DELETED\n+\t\t\t}\n+\n+\t\t\tevents <- &WrittenEvent{\n+\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\tNamespace: event.Namespace,\n+\t\t\t\t\tGroup:     event.Group,\n+\t\t\t\t\tResource:  event.Resource,\n+\t\t\t\t\tName:      event.Name,\n+\t\t\t\t},\n+\t\t\t\tType:            t,\n+\t\t\t\tFolder:          event.Folder,\n+\t\t\t\tValue:           data,\n+\t\t\t\tResourceVersion: event.ResourceVersion,\n+\t\t\t\tPreviousRV:      event.PreviousRV,\n+\t\t\t\tTimestamp:       event.ResourceVersion / time.Second.Nanoseconds(), // convert to seconds\n+\t\t\t}\n+\t\t}\n+\t\tclose(events)",
        "comment_created_at": "2025-07-01T12:47:39+00:00",
        "comment_author": "pstibrany",
        "comment_body": "Call via `defer`? There are some `return` statements (in case of errors) that will cause that `events` doesn't get closed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2171500383",
    "pr_number": 105771,
    "pr_file": "pkg/server/distributor_test.go",
    "created_at": "2025-06-27T10:11:20+00:00",
    "commented_code": "+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "2171500383",
        "repo_full_name": "grafana/grafana",
        "pr_number": 105771,
        "pr_file": "pkg/server/distributor_test.go",
        "discussion_id": "2171500383",
        "commented_code": "@@ -0,0 +1,475 @@\n+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err",
        "comment_created_at": "2025-06-27T10:11:20+00:00",
        "comment_author": "pstibrany",
        "comment_body": "We're accessing `runErrs` from two goroutines here without synchronization.\n\nInstead of collecting errors, shall we \n1) use `require.NoError(t, err)`\n2) register `t.Cleanup` function to stop the server eventually?\n\nSame suggestion applies below (no collecting of errors).",
        "pr_file_module": null
      },
      {
        "comment_id": "2171991759",
        "repo_full_name": "grafana/grafana",
        "pr_number": 105771,
        "pr_file": "pkg/server/distributor_test.go",
        "discussion_id": "2171500383",
        "commented_code": "@@ -0,0 +1,475 @@\n+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err",
        "comment_created_at": "2025-06-27T12:55:09+00:00",
        "comment_author": "gassiss",
        "comment_body": "three goroutines actually ðŸ™ˆ (distributor + 2 test servers)\r\n\r\nunfortunately we can't `require.NoError(t, err)` in a goroutine I don't think - I tried failing tests in goroutines and go complains about it. I had synchronization at some point but I removed it since I stop one server at a time, but I think they could still fail for another reason without me stopping it. \r\n\r\nWDYT? Add a lock before accessing runerrs then?",
        "pr_file_module": null
      },
      {
        "comment_id": "2172047722",
        "repo_full_name": "grafana/grafana",
        "pr_number": 105771,
        "pr_file": "pkg/server/distributor_test.go",
        "discussion_id": "2171500383",
        "commented_code": "@@ -0,0 +1,475 @@\n+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err",
        "comment_created_at": "2025-06-27T13:26:47+00:00",
        "comment_author": "pstibrany",
        "comment_body": "`assert.NoError` doesn't have this restriction, as it doesn't fail the test immediately. However we DO want to test fail immediately.\r\n\r\nIdeally `testServer.server.Run()` would not block, but be observable (like dskit's `services.Service`), but that's outside of the scope of this PR.\r\n\r\nAdding locks looks like the simplest solution for now.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172396592",
        "repo_full_name": "grafana/grafana",
        "pr_number": 105771,
        "pr_file": "pkg/server/distributor_test.go",
        "discussion_id": "2171500383",
        "commented_code": "@@ -0,0 +1,475 @@\n+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err",
        "comment_created_at": "2025-06-27T16:26:08+00:00",
        "comment_author": "gassiss",
        "comment_body": "I added the no error thing, seems to be working fine ðŸ‘ ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1631526725",
    "pr_number": 88886,
    "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
    "created_at": "2024-06-07T17:52:15+00:00",
    "commented_code": "}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "1631526725",
        "repo_full_name": "grafana/grafana",
        "pr_number": 88886,
        "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
        "discussion_id": "1631526725",
        "commented_code": "@@ -49,15 +52,20 @@ func (d *DualWriterMode1) Create(ctx context.Context, original runtime.Object, c\n \t}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)",
        "comment_created_at": "2024-06-07T17:52:15+00:00",
        "comment_author": "diegommm",
        "comment_body": "If we don't use `WithoutCancel`, the context can either be canceled because of the timeout we set below with `WithTimeoutCause` or because the parent was canceled, which we don't want.",
        "pr_file_module": null
      },
      {
        "comment_id": "1631658373",
        "repo_full_name": "grafana/grafana",
        "pr_number": 88886,
        "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
        "discussion_id": "1631526725",
        "commented_code": "@@ -49,15 +52,20 @@ func (d *DualWriterMode1) Create(ctx context.Context, original runtime.Object, c\n \t}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)",
        "comment_created_at": "2024-06-07T20:28:54+00:00",
        "comment_author": "hairyhenderson",
        "comment_body": "> or because the parent was canceled, which we don't want.\r\n\r\nare you _sure?_ it seems like calling `Create` with a cancelled context is a very bad idea that will lead to bugs...",
        "pr_file_module": null
      },
      {
        "comment_id": "1631706948",
        "repo_full_name": "grafana/grafana",
        "pr_number": 88886,
        "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
        "discussion_id": "1631526725",
        "commented_code": "@@ -49,15 +52,20 @@ func (d *DualWriterMode1) Create(ctx context.Context, original runtime.Object, c\n \t}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)",
        "comment_created_at": "2024-06-07T21:21:56+00:00",
        "comment_author": "diegommm",
        "comment_body": "The parent context may be canceled at any time. For instance, once the request is served the server may opt to cancel it. Consider the following example server:\r\n```go\r\nfunc (s *server) serve(ctx context.Context, l net.Listener) error {\r\n    ctx, cancel := context.WithCancel(ctx)\r\n    defer cancel()\r\n    for {\r\n        conn, err := l.Accept()\r\n        // handle err\r\n        go s.handleConn(ctx, conn)\r\n    }\r\n    return nil\r\n}\r\n\r\nfunc (s *server) handleConn(ctx context.Context, conn *net.TCPConn) {\r\n    ctx, cancel := context.WithCancel(ctx)\r\n    defer cancel()\r\n    req, err := parseRequest(conn)\r\n    // handle err\r\n    s.handleRequest(ctx, req)\r\n}\r\n```\r\nThis means that as soon as the request is served, the context derived and passed to the handler is canceled. This ensures that all handlers are bound by the server context, and none of them leak. In reality, the server typically does more work after serving the request and before canceling the derived context passed to the handler, and the goroutine we created to make the background `Create` will fail doing any context-aware work, like writing to a database. While it is true that using `context.WithoutCancel` allows to circumventing the server providing the transport operation to prevent handler leaks, we provide our own server that is allowed to do work outside of the transport server (i.e., the same as a server that serves over HTTP but also has \"cron\"-like jobs that are non-HTTP).\r\n\r\nThe concern about values in the context that may be tied to a single request, and thus may produce unpredictable behaviour if the context is canceled is valid. For instance, if an observability decorator flushes to the network the spans created during the execution of the request and then releases some pointers, a subsequent method call to create a new span may panic. But how do we track the work that needs to branch from a request, like something that needs retrying in the background, etc.? This lead to this [discussion](https://github.com/golang/go/issues/40221) and the creation of `context.WithoutCancel`, and the recommendation is that decorators are aware and gracefully handle these conditions. For example, in OpenTracing, we can create a span with `FollowsFrom` that allows to represent this detachment from a request (I don't recall the name for OpenTelementry, but it's similar).\r\n\r\nSo yeah, we need to account for branching operations with context in our decorators.",
        "pr_file_module": null
      },
      {
        "comment_id": "1632441953",
        "repo_full_name": "grafana/grafana",
        "pr_number": 88886,
        "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
        "discussion_id": "1631526725",
        "commented_code": "@@ -49,15 +52,20 @@ func (d *DualWriterMode1) Create(ctx context.Context, original runtime.Object, c\n \t}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)",
        "comment_created_at": "2024-06-10T00:26:06+00:00",
        "comment_author": "hairyhenderson",
        "comment_body": "I'm aware of the usefulness of `context.WithoutCancel`, but it needs to be used sparingly and only in cases where it's safe to do so.\r\n\r\nMaybe I should rephrase my question:\r\n\r\nWhy is it desirable to continue creating resources if the context has been cancelled?\r\n\r\nA potential source of data integrity bugs involves performing operations while the server is shutting down. If the context has been cancelled because the server is shutting down and we blindly ignore that cancellation and continue trying to do I/O, we may end up persisting inconsistent or corrupted data.",
        "pr_file_module": null
      },
      {
        "comment_id": "1632761001",
        "repo_full_name": "grafana/grafana",
        "pr_number": 88886,
        "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
        "discussion_id": "1631526725",
        "commented_code": "@@ -49,15 +52,20 @@ func (d *DualWriterMode1) Create(ctx context.Context, original runtime.Object, c\n \t}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)",
        "comment_created_at": "2024-06-10T07:54:35+00:00",
        "comment_author": "leonorfmartins",
        "comment_body": "I would agree that we would want to cancel if the parent context got cancelled. In mode1, the only thing we need to guarantee is that we write to legacy store. Writing to unified store works as a best effort. And actually, one thing we want to avoid is succeeding writing to unified storage when _for some reason_ writing to legacy fails (because the context was cancelled, for example).",
        "pr_file_module": null
      },
      {
        "comment_id": "1633110137",
        "repo_full_name": "grafana/grafana",
        "pr_number": 88886,
        "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
        "discussion_id": "1631526725",
        "commented_code": "@@ -49,15 +52,20 @@ func (d *DualWriterMode1) Create(ctx context.Context, original runtime.Object, c\n \t}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)",
        "comment_created_at": "2024-06-10T11:43:20+00:00",
        "comment_author": "diegommm",
        "comment_body": "> Why is it desirable to continue creating resources if the context has been cancelled?\r\n\r\nI think the best way to phrase the general intention would be: try to do some work without blocking the response to the user to have the least impact on them. But past the moment of returning from a handler makes it very likely that the context is cancelled very shortly, and the background goroutine has just started and it still needs to make some I/O *(i.e. call the db), so it's very likely that it will just become a noisy nop just failing most of the time.\r\n\r\n> A potential source of data integrity bugs involves performing operations while the server is shutting down.\r\n\r\nTotally agree. That's why I wrote [this](https://github.com/grafana/grafana/pull/88886/files#r1631525475) comment mentioning that we should consider server lifecycle, and the comments on the code mention that this is a temporary workaround that needs to be fixed since past the execution of the main loop the status of the storage is undefined.",
        "pr_file_module": null
      },
      {
        "comment_id": "1633112410",
        "repo_full_name": "grafana/grafana",
        "pr_number": 88886,
        "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
        "discussion_id": "1631526725",
        "commented_code": "@@ -49,15 +52,20 @@ func (d *DualWriterMode1) Create(ctx context.Context, original runtime.Object, c\n \t}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)",
        "comment_created_at": "2024-06-10T11:45:13+00:00",
        "comment_author": "diegommm",
        "comment_body": "But I do agree with you that introducing `WihtoutCancel` here without adding the correct mechanism for handling the server lifecycle would in itself be introducing a bug. So I think this should be taken away, thanks for pointing that out!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1631528389",
    "pr_number": 88886,
    "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
    "created_at": "2024-06-07T17:54:10+00:00",
    "commented_code": "}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)\n \t\tctx, cancel := context.WithTimeoutCause(ctx, time.Second*10, errors.New(\"storage create timeout\"))\n+\t\tdefer cancel()\n+\t\tcreated := created.DeepCopyObject()",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "1631528389",
        "repo_full_name": "grafana/grafana",
        "pr_number": 88886,
        "pr_file": "pkg/apiserver/rest/dualwriter_mode1.go",
        "discussion_id": "1631528389",
        "commented_code": "@@ -49,15 +52,20 @@ func (d *DualWriterMode1) Create(ctx context.Context, original runtime.Object, c\n \t}\n \td.recordLegacyDuration(false, mode1Str, options.Kind, method, startLegacy)\n \n+\td.pendingActions.Add(1)\n \tgo func() {\n+\t\tdefer d.pendingActions.Done()\n+\t\tctx := context.WithoutCancel(ctx)\n \t\tctx, cancel := context.WithTimeoutCause(ctx, time.Second*10, errors.New(\"storage create timeout\"))\n+\t\tdefer cancel()\n+\t\tcreated := created.DeepCopyObject()",
        "comment_created_at": "2024-06-07T17:54:10+00:00",
        "comment_author": "diegommm",
        "comment_body": "If we don't make a deep copy, we will be modifying the returned object instead when we call `enrichLegacyObject` below. And this would be done in a different goroutine, which means that what is sent to the client is undefined, since we don't know if it will be the response from legacy, the version enriched or something in the middle since it may be in the process of being written.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1520138120",
    "pr_number": 84129,
    "pr_file": "pkg/services/store/entity/sqlstash/watch_pg_notify_listen.go",
    "created_at": "2024-03-11T17:30:50+00:00",
    "commented_code": "+package sqlstash\n+\n+import (\n+\t\"context\"\n+\t\"database/sql/driver\"\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\t\"strconv\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/lib/pq\"\n+\n+\t\"github.com/grafana/grafana/pkg/services/store/entity\"\n+)\n+\n+type pgNotifyPayload struct {\n+\tResourceVersion string `json:\"resource_version\"`\n+\tKey             string `json:\"key\"`\n+}\n+\n+// pgWatcher is like poller but uses the native postgres LISTEN/NOTIFY SQL commands\n+func (s *sqlEntityServer) pgWatcher(stream chan *entity.Entity) {\n+\tctx := context.Background()\n+\n+\tengine, err := s.db.GetEngine()\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting engine: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tconn, err := engine.DB().DB.Conn(ctx)\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting db connection: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tdefer func() { _ = conn.Close() }()\n+\n+\terr = conn.Raw(func(driverConn any) error {\n+\t\tdc, ok := driverConn.(driver.Conn)\n+\t\tif !ok {\n+\t\t\treturn fmt.Errorf(\"cannot convert driverConn from %T to driver.Conn\", driverConn)\n+\t\t}\n+\n+\t\tpq.SetNotificationHandler(dc, func(n *pq.Notification) {\n+\t\t\tif n == nil {\n+\t\t\t\ts.log.Debug(\"received postgres notification\", \"nil\", true)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\ts.log.Debug(\"received postgres notification\", \"channel\", n.Channel, \"payload\", n.Extra)\n+\n+\t\t\tgo func() {",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "1520138120",
        "repo_full_name": "grafana/grafana",
        "pr_number": 84129,
        "pr_file": "pkg/services/store/entity/sqlstash/watch_pg_notify_listen.go",
        "discussion_id": "1520138120",
        "commented_code": "@@ -0,0 +1,125 @@\n+package sqlstash\n+\n+import (\n+\t\"context\"\n+\t\"database/sql/driver\"\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\t\"strconv\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/lib/pq\"\n+\n+\t\"github.com/grafana/grafana/pkg/services/store/entity\"\n+)\n+\n+type pgNotifyPayload struct {\n+\tResourceVersion string `json:\"resource_version\"`\n+\tKey             string `json:\"key\"`\n+}\n+\n+// pgWatcher is like poller but uses the native postgres LISTEN/NOTIFY SQL commands\n+func (s *sqlEntityServer) pgWatcher(stream chan *entity.Entity) {\n+\tctx := context.Background()\n+\n+\tengine, err := s.db.GetEngine()\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting engine: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tconn, err := engine.DB().DB.Conn(ctx)\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting db connection: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tdefer func() { _ = conn.Close() }()\n+\n+\terr = conn.Raw(func(driverConn any) error {\n+\t\tdc, ok := driverConn.(driver.Conn)\n+\t\tif !ok {\n+\t\t\treturn fmt.Errorf(\"cannot convert driverConn from %T to driver.Conn\", driverConn)\n+\t\t}\n+\n+\t\tpq.SetNotificationHandler(dc, func(n *pq.Notification) {\n+\t\t\tif n == nil {\n+\t\t\t\ts.log.Debug(\"received postgres notification\", \"nil\", true)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\ts.log.Debug(\"received postgres notification\", \"channel\", n.Channel, \"payload\", n.Extra)\n+\n+\t\t\tgo func() {",
        "comment_created_at": "2024-03-11T17:30:50+00:00",
        "comment_author": "diegommm",
        "comment_body": "We can't send to the channel in a different goroutine. If the channel is blocked and we receive +1 consecutive notifications for the same watched entity, then when the channel is ready to receive again the goroutines will race to send to the channel. This would cause notifications to arrive out of sequence.\r\nNotifications should be queued until the channel is ready to receive again, that's the purpose of `makeUnboundedQueuedChans` in https://github.com/grafana/grafana/pull/83772\r\n\r\nI believe we should do the same in `(*broadcaster[T]).stream` in `broadcaster.go` instead of dropping the notifications. Specifically for consumers, we could use a ring buffer to allow some slowness but at the same time cap the buffer length (although I would suggest the limit is high and grows progressively).\r\n\r\nI can envision a situation of spiky workload like an ops team (from a client) creating a new cluster in the cloud with many pods, services, etc., and that process creating and updating thousands of different entities (dashboards, panels, alerts, other entities from third-party Grafana App Plugins, etc.). This would cause a surge in writes to Unified Storage, which translates to flooding the stream of events to be processed for watchers. In that scenario, there would also be a particularly high number of watchers. Some clients watching for changes may lag due to racing for resources while trying to perform all of their complex tasks, and we should provide some queuing for them until they are ready to accept them.\r\n\r\nThis case may also be one of the strongest use cases of watch, or at least the noisiest in terms of support.",
        "pr_file_module": null
      },
      {
        "comment_id": "1520318171",
        "repo_full_name": "grafana/grafana",
        "pr_number": 84129,
        "pr_file": "pkg/services/store/entity/sqlstash/watch_pg_notify_listen.go",
        "discussion_id": "1520138120",
        "commented_code": "@@ -0,0 +1,125 @@\n+package sqlstash\n+\n+import (\n+\t\"context\"\n+\t\"database/sql/driver\"\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\t\"strconv\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/lib/pq\"\n+\n+\t\"github.com/grafana/grafana/pkg/services/store/entity\"\n+)\n+\n+type pgNotifyPayload struct {\n+\tResourceVersion string `json:\"resource_version\"`\n+\tKey             string `json:\"key\"`\n+}\n+\n+// pgWatcher is like poller but uses the native postgres LISTEN/NOTIFY SQL commands\n+func (s *sqlEntityServer) pgWatcher(stream chan *entity.Entity) {\n+\tctx := context.Background()\n+\n+\tengine, err := s.db.GetEngine()\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting engine: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tconn, err := engine.DB().DB.Conn(ctx)\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting db connection: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tdefer func() { _ = conn.Close() }()\n+\n+\terr = conn.Raw(func(driverConn any) error {\n+\t\tdc, ok := driverConn.(driver.Conn)\n+\t\tif !ok {\n+\t\t\treturn fmt.Errorf(\"cannot convert driverConn from %T to driver.Conn\", driverConn)\n+\t\t}\n+\n+\t\tpq.SetNotificationHandler(dc, func(n *pq.Notification) {\n+\t\t\tif n == nil {\n+\t\t\t\ts.log.Debug(\"received postgres notification\", \"nil\", true)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\ts.log.Debug(\"received postgres notification\", \"channel\", n.Channel, \"payload\", n.Extra)\n+\n+\t\t\tgo func() {",
        "comment_created_at": "2024-03-11T19:29:10+00:00",
        "comment_author": "DanCech",
        "comment_body": "stream is already a buffered channel, currently hardcoded to 100 but we could increase that and/or make it configurable.",
        "pr_file_module": null
      },
      {
        "comment_id": "1520466113",
        "repo_full_name": "grafana/grafana",
        "pr_number": 84129,
        "pr_file": "pkg/services/store/entity/sqlstash/watch_pg_notify_listen.go",
        "discussion_id": "1520138120",
        "commented_code": "@@ -0,0 +1,125 @@\n+package sqlstash\n+\n+import (\n+\t\"context\"\n+\t\"database/sql/driver\"\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\t\"strconv\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/lib/pq\"\n+\n+\t\"github.com/grafana/grafana/pkg/services/store/entity\"\n+)\n+\n+type pgNotifyPayload struct {\n+\tResourceVersion string `json:\"resource_version\"`\n+\tKey             string `json:\"key\"`\n+}\n+\n+// pgWatcher is like poller but uses the native postgres LISTEN/NOTIFY SQL commands\n+func (s *sqlEntityServer) pgWatcher(stream chan *entity.Entity) {\n+\tctx := context.Background()\n+\n+\tengine, err := s.db.GetEngine()\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting engine: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tconn, err := engine.DB().DB.Conn(ctx)\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting db connection: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tdefer func() { _ = conn.Close() }()\n+\n+\terr = conn.Raw(func(driverConn any) error {\n+\t\tdc, ok := driverConn.(driver.Conn)\n+\t\tif !ok {\n+\t\t\treturn fmt.Errorf(\"cannot convert driverConn from %T to driver.Conn\", driverConn)\n+\t\t}\n+\n+\t\tpq.SetNotificationHandler(dc, func(n *pq.Notification) {\n+\t\t\tif n == nil {\n+\t\t\t\ts.log.Debug(\"received postgres notification\", \"nil\", true)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\ts.log.Debug(\"received postgres notification\", \"channel\", n.Channel, \"payload\", n.Extra)\n+\n+\t\t\tgo func() {",
        "comment_created_at": "2024-03-11T21:43:45+00:00",
        "comment_author": "owensmallwood",
        "comment_body": "Somewhat related question: Does this need to be in a goroutine? Were not offloading much extra work; just doing a read from the db.",
        "pr_file_module": null
      },
      {
        "comment_id": "1520478874",
        "repo_full_name": "grafana/grafana",
        "pr_number": 84129,
        "pr_file": "pkg/services/store/entity/sqlstash/watch_pg_notify_listen.go",
        "discussion_id": "1520138120",
        "commented_code": "@@ -0,0 +1,125 @@\n+package sqlstash\n+\n+import (\n+\t\"context\"\n+\t\"database/sql/driver\"\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\t\"strconv\"\n+\t\"strings\"\n+\t\"time\"\n+\n+\t\"github.com/lib/pq\"\n+\n+\t\"github.com/grafana/grafana/pkg/services/store/entity\"\n+)\n+\n+type pgNotifyPayload struct {\n+\tResourceVersion string `json:\"resource_version\"`\n+\tKey             string `json:\"key\"`\n+}\n+\n+// pgWatcher is like poller but uses the native postgres LISTEN/NOTIFY SQL commands\n+func (s *sqlEntityServer) pgWatcher(stream chan *entity.Entity) {\n+\tctx := context.Background()\n+\n+\tengine, err := s.db.GetEngine()\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting engine: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tconn, err := engine.DB().DB.Conn(ctx)\n+\tif err != nil {\n+\t\ts.log.Error(\"error getting db connection: %w\", err)\n+\t\treturn\n+\t}\n+\n+\tdefer func() { _ = conn.Close() }()\n+\n+\terr = conn.Raw(func(driverConn any) error {\n+\t\tdc, ok := driverConn.(driver.Conn)\n+\t\tif !ok {\n+\t\t\treturn fmt.Errorf(\"cannot convert driverConn from %T to driver.Conn\", driverConn)\n+\t\t}\n+\n+\t\tpq.SetNotificationHandler(dc, func(n *pq.Notification) {\n+\t\t\tif n == nil {\n+\t\t\t\ts.log.Debug(\"received postgres notification\", \"nil\", true)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\ts.log.Debug(\"received postgres notification\", \"channel\", n.Channel, \"payload\", n.Extra)\n+\n+\t\t\tgo func() {",
        "comment_created_at": "2024-03-11T22:00:01+00:00",
        "comment_author": "DanCech",
        "comment_body": "the initial idea was to release the listen connection as quickly as possible, but it may not be necessary",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1333589286",
    "pr_number": 74998,
    "pr_file": "pkg/services/ngalert/state/manager.go",
    "created_at": "2023-09-21T20:51:53+00:00",
    "commented_code": "case <-ctx.Done():\n \t\t\tst.log.Debug(\"Stopping\")\n \t\t\tticker.Stop()\n+\t\t\tif st.saveStateAsync {\n+\t\t\t\tst.stateRunnerShutdown <- struct{}{}",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "1333589286",
        "repo_full_name": "grafana/grafana",
        "pr_number": 74998,
        "pr_file": "pkg/services/ngalert/state/manager.go",
        "discussion_id": "1333589286",
        "commented_code": "@@ -99,11 +110,71 @@ func (st *Manager) Run(ctx context.Context) error {\n \t\tcase <-ctx.Done():\n \t\t\tst.log.Debug(\"Stopping\")\n \t\t\tticker.Stop()\n+\t\t\tif st.saveStateAsync {\n+\t\t\t\tst.stateRunnerShutdown <- struct{}{}",
        "comment_created_at": "2023-09-21T20:51:53+00:00",
        "comment_author": "grobinson-grafana",
        "comment_body": "```suggestion\r\n\t\t\t\tclose(st.stateRunnerShutdown)\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1333620520",
        "repo_full_name": "grafana/grafana",
        "pr_number": 74998,
        "pr_file": "pkg/services/ngalert/state/manager.go",
        "discussion_id": "1333589286",
        "commented_code": "@@ -99,11 +110,71 @@ func (st *Manager) Run(ctx context.Context) error {\n \t\tcase <-ctx.Done():\n \t\t\tst.log.Debug(\"Stopping\")\n \t\t\tticker.Stop()\n+\t\t\tif st.saveStateAsync {\n+\t\t\t\tst.stateRunnerShutdown <- struct{}{}",
        "comment_created_at": "2023-09-21T21:27:09+00:00",
        "comment_author": "JohnnyQQQQ",
        "comment_body": "Good catch, changed ðŸ‘ ",
        "pr_file_module": null
      }
    ]
  }
]