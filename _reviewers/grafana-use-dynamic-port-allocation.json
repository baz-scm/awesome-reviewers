[
  {
    "discussion_id": "2171526763",
    "pr_number": 105771,
    "pr_file": "pkg/server/distributor_test.go",
    "created_at": "2025-06-27T10:19:33+00:00",
    "commented_code": "+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err\n+\t\t}\n+\t}()\n+\n+\tdeadline := time.Now().Add(20 * time.Second)\n+\tfor {\n+\t\trequire.NoError(t, runErrs[testServer.id], \"failed to start \"+testServer.id)\n+\n+\t\tconn, err := net.DialTimeout(\"tcp\", testServer.grpcAddress, 1*time.Second)\n+\t\tif err == nil {\n+\t\t\t_ = conn.Close()\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tif time.Now().After(deadline) {\n+\t\t\tt.Fatal(\"server failed to become ready: \", testServer.id)\n+\t\t}\n+\n+\t\ttime.Sleep(1 * time.Second)\n+\t}\n+\n+\tres, err := testServer.healthClient.Check(context.Background(), &grpc_health_v1.HealthCheckRequest{})\n+\trequire.NoError(t, err)\n+\trequire.Equal(t, res.Status, grpc_health_v1.HealthCheckResponse_SERVING)\n+}\n+\n+type testModuleServer struct {\n+\tserver         *ModuleServer\n+\thealthClient   grpc_health_v1.HealthClient\n+\tresourceClient resource.ResourceClient\n+\tid             string\n+\tgrpcAddress    string\n+}\n+\n+func initDistributorServerForTest(t *testing.T) testModuleServer {\n+\tcfg := setting.NewCfg()\n+\tcfg.HTTPPort = \"13000\"\n+\tcfg.GRPCServer.Network = \"tcp\"\n+\tcfg.GRPCServer.Address = \"127.0.0.1:20000\"\n+\tcfg.EnableSharding = true\n+\tcfg.MemberlistBindAddr = \"127.0.0.1\"\n+\tcfg.MemberlistJoinMember = \"127.0.0.1:17946\"\n+\tcfg.MemberlistAdvertiseAddr = \"127.0.0.1\"\n+\tcfg.MemberlistAdvertisePort = 17946\n+\tcfg.Target = []string{modules.Distributor}\n+\tcfg.InstanceID = \"distributor\" // does nothing for the distributor but may be useful to debug tests\n+\n+\tconn, err := grpc.NewClient(cfg.GRPCServer.Address,\n+\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n+\t)\n+\trequire.NoError(t, err)\n+\tclient := resource.NewLegacyResourceClient(conn)\n+\n+\tserver := initModuleServerForTest(t, cfg, Options{}, api.ServerOptions{})\n+\n+\tserver.resourceClient = client\n+\n+\treturn server\n+}\n+\n+func createStorageServerApi(t *testing.T, instanceId int, dbType, dbConnStr string) testModuleServer {\n+\tcfg := setting.NewCfg()\n+\tsection, err := cfg.Raw.NewSection(\"database\")\n+\trequire.NoError(t, err)\n+\n+\t_, err = section.NewKey(\"type\", dbType)\n+\trequire.NoError(t, err)\n+\t_, err = section.NewKey(\"connection_string\", dbConnStr)\n+\trequire.NoError(t, err)\n+\n+\tcfg.HTTPPort = \"1300\" + strconv.Itoa(instanceId) // 13001 for instance-1, 13002 for instance-2 etc",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "2171526763",
        "repo_full_name": "grafana/grafana",
        "pr_number": 105771,
        "pr_file": "pkg/server/distributor_test.go",
        "discussion_id": "2171526763",
        "commented_code": "@@ -0,0 +1,475 @@\n+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err\n+\t\t}\n+\t}()\n+\n+\tdeadline := time.Now().Add(20 * time.Second)\n+\tfor {\n+\t\trequire.NoError(t, runErrs[testServer.id], \"failed to start \"+testServer.id)\n+\n+\t\tconn, err := net.DialTimeout(\"tcp\", testServer.grpcAddress, 1*time.Second)\n+\t\tif err == nil {\n+\t\t\t_ = conn.Close()\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tif time.Now().After(deadline) {\n+\t\t\tt.Fatal(\"server failed to become ready: \", testServer.id)\n+\t\t}\n+\n+\t\ttime.Sleep(1 * time.Second)\n+\t}\n+\n+\tres, err := testServer.healthClient.Check(context.Background(), &grpc_health_v1.HealthCheckRequest{})\n+\trequire.NoError(t, err)\n+\trequire.Equal(t, res.Status, grpc_health_v1.HealthCheckResponse_SERVING)\n+}\n+\n+type testModuleServer struct {\n+\tserver         *ModuleServer\n+\thealthClient   grpc_health_v1.HealthClient\n+\tresourceClient resource.ResourceClient\n+\tid             string\n+\tgrpcAddress    string\n+}\n+\n+func initDistributorServerForTest(t *testing.T) testModuleServer {\n+\tcfg := setting.NewCfg()\n+\tcfg.HTTPPort = \"13000\"\n+\tcfg.GRPCServer.Network = \"tcp\"\n+\tcfg.GRPCServer.Address = \"127.0.0.1:20000\"\n+\tcfg.EnableSharding = true\n+\tcfg.MemberlistBindAddr = \"127.0.0.1\"\n+\tcfg.MemberlistJoinMember = \"127.0.0.1:17946\"\n+\tcfg.MemberlistAdvertiseAddr = \"127.0.0.1\"\n+\tcfg.MemberlistAdvertisePort = 17946\n+\tcfg.Target = []string{modules.Distributor}\n+\tcfg.InstanceID = \"distributor\" // does nothing for the distributor but may be useful to debug tests\n+\n+\tconn, err := grpc.NewClient(cfg.GRPCServer.Address,\n+\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n+\t)\n+\trequire.NoError(t, err)\n+\tclient := resource.NewLegacyResourceClient(conn)\n+\n+\tserver := initModuleServerForTest(t, cfg, Options{}, api.ServerOptions{})\n+\n+\tserver.resourceClient = client\n+\n+\treturn server\n+}\n+\n+func createStorageServerApi(t *testing.T, instanceId int, dbType, dbConnStr string) testModuleServer {\n+\tcfg := setting.NewCfg()\n+\tsection, err := cfg.Raw.NewSection(\"database\")\n+\trequire.NoError(t, err)\n+\n+\t_, err = section.NewKey(\"type\", dbType)\n+\trequire.NoError(t, err)\n+\t_, err = section.NewKey(\"connection_string\", dbConnStr)\n+\trequire.NoError(t, err)\n+\n+\tcfg.HTTPPort = \"1300\" + strconv.Itoa(instanceId) // 13001 for instance-1, 13002 for instance-2 etc",
        "comment_created_at": "2025-06-27T10:19:33+00:00",
        "comment_author": "pstibrany",
        "comment_body": "It would be safer to use 0, and let system assign the port to avoid conflicts. Same for gRPC and memberlist ports. We will then need to find assigned ports by checking for listener ports.",
        "pr_file_module": null
      },
      {
        "comment_id": "2171992290",
        "repo_full_name": "grafana/grafana",
        "pr_number": 105771,
        "pr_file": "pkg/server/distributor_test.go",
        "discussion_id": "2171526763",
        "commented_code": "@@ -0,0 +1,475 @@\n+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err\n+\t\t}\n+\t}()\n+\n+\tdeadline := time.Now().Add(20 * time.Second)\n+\tfor {\n+\t\trequire.NoError(t, runErrs[testServer.id], \"failed to start \"+testServer.id)\n+\n+\t\tconn, err := net.DialTimeout(\"tcp\", testServer.grpcAddress, 1*time.Second)\n+\t\tif err == nil {\n+\t\t\t_ = conn.Close()\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tif time.Now().After(deadline) {\n+\t\t\tt.Fatal(\"server failed to become ready: \", testServer.id)\n+\t\t}\n+\n+\t\ttime.Sleep(1 * time.Second)\n+\t}\n+\n+\tres, err := testServer.healthClient.Check(context.Background(), &grpc_health_v1.HealthCheckRequest{})\n+\trequire.NoError(t, err)\n+\trequire.Equal(t, res.Status, grpc_health_v1.HealthCheckResponse_SERVING)\n+}\n+\n+type testModuleServer struct {\n+\tserver         *ModuleServer\n+\thealthClient   grpc_health_v1.HealthClient\n+\tresourceClient resource.ResourceClient\n+\tid             string\n+\tgrpcAddress    string\n+}\n+\n+func initDistributorServerForTest(t *testing.T) testModuleServer {\n+\tcfg := setting.NewCfg()\n+\tcfg.HTTPPort = \"13000\"\n+\tcfg.GRPCServer.Network = \"tcp\"\n+\tcfg.GRPCServer.Address = \"127.0.0.1:20000\"\n+\tcfg.EnableSharding = true\n+\tcfg.MemberlistBindAddr = \"127.0.0.1\"\n+\tcfg.MemberlistJoinMember = \"127.0.0.1:17946\"\n+\tcfg.MemberlistAdvertiseAddr = \"127.0.0.1\"\n+\tcfg.MemberlistAdvertisePort = 17946\n+\tcfg.Target = []string{modules.Distributor}\n+\tcfg.InstanceID = \"distributor\" // does nothing for the distributor but may be useful to debug tests\n+\n+\tconn, err := grpc.NewClient(cfg.GRPCServer.Address,\n+\t\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\n+\t)\n+\trequire.NoError(t, err)\n+\tclient := resource.NewLegacyResourceClient(conn)\n+\n+\tserver := initModuleServerForTest(t, cfg, Options{}, api.ServerOptions{})\n+\n+\tserver.resourceClient = client\n+\n+\treturn server\n+}\n+\n+func createStorageServerApi(t *testing.T, instanceId int, dbType, dbConnStr string) testModuleServer {\n+\tcfg := setting.NewCfg()\n+\tsection, err := cfg.Raw.NewSection(\"database\")\n+\trequire.NoError(t, err)\n+\n+\t_, err = section.NewKey(\"type\", dbType)\n+\trequire.NoError(t, err)\n+\t_, err = section.NewKey(\"connection_string\", dbConnStr)\n+\trequire.NoError(t, err)\n+\n+\tcfg.HTTPPort = \"1300\" + strconv.Itoa(instanceId) // 13001 for instance-1, 13002 for instance-2 etc",
        "comment_created_at": "2025-06-27T12:55:26+00:00",
        "comment_author": "gassiss",
        "comment_body": "good point, let me try and get that to work",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2171533745",
    "pr_number": 105771,
    "pr_file": "pkg/server/distributor_test.go",
    "created_at": "2025-06-27T10:21:35+00:00",
    "commented_code": "+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err\n+\t\t}\n+\t}()\n+\n+\tdeadline := time.Now().Add(20 * time.Second)\n+\tfor {\n+\t\trequire.NoError(t, runErrs[testServer.id], \"failed to start \"+testServer.id)\n+\n+\t\tconn, err := net.DialTimeout(\"tcp\", testServer.grpcAddress, 1*time.Second)\n+\t\tif err == nil {\n+\t\t\t_ = conn.Close()\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tif time.Now().After(deadline) {\n+\t\t\tt.Fatal(\"server failed to become ready: \", testServer.id)\n+\t\t}\n+\n+\t\ttime.Sleep(1 * time.Second)\n+\t}\n+\n+\tres, err := testServer.healthClient.Check(context.Background(), &grpc_health_v1.HealthCheckRequest{})\n+\trequire.NoError(t, err)\n+\trequire.Equal(t, res.Status, grpc_health_v1.HealthCheckResponse_SERVING)\n+}\n+\n+type testModuleServer struct {\n+\tserver         *ModuleServer\n+\thealthClient   grpc_health_v1.HealthClient\n+\tresourceClient resource.ResourceClient\n+\tid             string\n+\tgrpcAddress    string\n+}\n+\n+func initDistributorServerForTest(t *testing.T) testModuleServer {\n+\tcfg := setting.NewCfg()\n+\tcfg.HTTPPort = \"13000\"\n+\tcfg.GRPCServer.Network = \"tcp\"\n+\tcfg.GRPCServer.Address = \"127.0.0.1:20000\"",
    "repo_full_name": "grafana/grafana",
    "discussion_comments": [
      {
        "comment_id": "2171533745",
        "repo_full_name": "grafana/grafana",
        "pr_number": 105771,
        "pr_file": "pkg/server/distributor_test.go",
        "discussion_id": "2171533745",
        "commented_code": "@@ -0,0 +1,475 @@\n+package server\n+\n+import (\n+\t\"context\"\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"math/rand\"\n+\t\"net\"\n+\t\"net/http\"\n+\t\"strconv\"\n+\t\"sync\"\n+\t\"testing\"\n+\t\"time\"\n+\n+\tclaims \"github.com/grafana/authlib/types\"\n+\t\"github.com/grafana/grafana/pkg/api\"\n+\t\"github.com/grafana/grafana/pkg/apimachinery/identity\"\n+\t\"github.com/grafana/grafana/pkg/modules\"\n+\t\"github.com/grafana/grafana/pkg/services/featuremgmt\"\n+\t\"github.com/grafana/grafana/pkg/services/sqlstore/sqlutil\"\n+\t\"github.com/grafana/grafana/pkg/setting\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resource\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/resourcepb\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/search\"\n+\t\"github.com/grafana/grafana/pkg/storage/unified/sql\"\n+\t\"github.com/prometheus/client_golang/prometheus\"\n+\t\"github.com/stretchr/testify/require\"\n+\t\"go.opentelemetry.io/otel/trace/noop\"\n+\t\"google.golang.org/grpc\"\n+\t\"google.golang.org/grpc/credentials/insecure\"\n+\t\"google.golang.org/grpc/health/grpc_health_v1\"\n+\t\"google.golang.org/grpc/metadata\"\n+\t\"k8s.io/component-base/metrics/legacyregistry\"\n+)\n+\n+var (\n+\ttestIndexFileThreshold  = 200 // just needs to be bigger than max playlist number, so the indexer don't use the filesystem\n+\tnamespaceCount          = 250 // how many stacks we're simulating\n+\tmaxPlaylistPerNamespace = 50  // upper bound on how many playlists we will seed to each stack.\n+)\n+\n+//nolint:gocyclo\n+func TestIntegrationDistributor(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"skipping integration test\")\n+\t}\n+\n+\tdbType := sqlutil.GetTestDBType()\n+\tif dbType != \"mysql\" {\n+\t\tt.Skip()\n+\t}\n+\n+\t// this next line is to avoid double registration when registering sprinkles metrics\n+\tlegacyregistry.Registerer = func() prometheus.Registerer { return prometheus.NewRegistry() }\n+\n+\tdb, err := sqlutil.GetTestDB(dbType)\n+\trequire.NoError(t, err)\n+\n+\ttestNamespaces := make([]string, 0, namespaceCount)\n+\tfor i := range namespaceCount {\n+\t\ttestNamespaces = append(testNamespaces, \"stacks-\"+strconv.Itoa(i))\n+\t}\n+\n+\tbaselineServer := createBaselineServer(t, dbType, db.ConnStr, testNamespaces)\n+\n+\trunErrs := make(map[string]error)\n+\ttestServers := make([]testModuleServer, 0, 2)\n+\tdistributorServer := initDistributorServerForTest(t)\n+\ttestServers = append(testServers, createStorageServerApi(t, 1, dbType, db.ConnStr))\n+\ttestServers = append(testServers, createStorageServerApi(t, 2, dbType, db.ConnStr))\n+\n+\tstartAndWaitHealthy(t, distributorServer, runErrs)\n+\n+\tfor _, testServer := range testServers {\n+\t\tstartAndWaitHealthy(t, testServer, runErrs)\n+\t}\n+\n+\tt.Run(\"should expose ring endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/ring\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"should expose memberlist endpoint\", func(t *testing.T) {\n+\t\tclient := http.Client{}\n+\t\tres, err := client.Get(\"http://localhost:13000/memberlist\")\n+\t\trequire.NoError(t, err)\n+\n+\t\trequire.Equal(t, res.StatusCode, http.StatusOK)\n+\t\t_ = res.Body.Close()\n+\t})\n+\n+\tt.Run(\"GetStats\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.GetStats(ctx, &resourcepb.ResourceStatsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"CountManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.CountManagedObjects(ctx, &resourcepb.CountManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"ListManagedObjects\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.ListManagedObjects(ctx, &resourcepb.ListManagedObjectsRequest{\n+\t\t\t\tNamespace: ns,\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tt.Run(\"Search\", func(t *testing.T) {\n+\t\tinstanceResponseCount := make(map[string]int)\n+\n+\t\tfor _, ns := range testNamespaces {\n+\t\t\tctx := context.Background()\n+\t\t\tbaselineRes, err := baselineServer.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t})\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\tctx = identity.WithServiceIdentityContext(context.Background(), 1)\n+\t\t\tvar header metadata.MD\n+\t\t\tres, err := distributorServer.resourceClient.Search(ctx, &resourcepb.ResourceSearchRequest{\n+\t\t\t\tOptions: &resourcepb.ListOptions{\n+\t\t\t\t\tKey: &resourcepb.ResourceKey{\n+\t\t\t\t\t\tGroup:     \"playlist.grafana.app\",\n+\t\t\t\t\t\tResource:  \"aoeuaeou\",\n+\t\t\t\t\t\tNamespace: ns,\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t}, grpc.Header(&header))\n+\t\t\trequire.NoError(t, err)\n+\n+\t\t\t// sometimes the querycost is different between the two. Happens randomly and we don't have control over it\n+\t\t\t// as it comes from bleve. Since we are not testing search functionality we hard-set this to 0 to avoid\n+\t\t\t// flaky tests\n+\t\t\tres.QueryCost = 0\n+\t\t\tbaselineRes.QueryCost = 0\n+\n+\t\t\trequire.Equal(t, baselineRes.String(), res.String())\n+\n+\t\t\tinstance := header.Get(\"proxied-instance-id\")\n+\t\t\tif len(instance) != 1 {\n+\t\t\t\tt.Fatal(\"received invalid proxied-instance-id header\", instance)\n+\t\t\t}\n+\n+\t\t\tinstanceResponseCount[instance[0]] += 1\n+\t\t}\n+\n+\t\tfor instance, count := range instanceResponseCount {\n+\t\t\trequire.GreaterOrEqual(t, count, 1, \"instance did not get any traffic: \"+instance)\n+\t\t}\n+\t})\n+\n+\tstopErrs := make(map[string]error)\n+\tstopServers := func(done chan error) {\n+\t\tvar wg sync.WaitGroup\n+\t\tfor _, testServer := range testServers {\n+\t\t\twg.Add(1)\n+\t\t\tgo func(s testModuleServer) {\n+\t\t\t\tdefer wg.Done()\n+\t\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\t\t\tdefer cancel()\n+\t\t\t\tif err := testServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\t\t\tstopErrs[testServer.id] = err\n+\t\t\t\t}\n+\t\t\t}(testServer)\n+\t\t}\n+\t\twg.Wait()\n+\n+\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n+\t\tdefer cancel()\n+\t\tif err := distributorServer.server.Shutdown(ctx, \"tests are done\"); err != nil {\n+\t\t\tstopErrs[distributorServer.id] = err\n+\t\t}\n+\t\tdone <- nil\n+\t}\n+\n+\tdone := make(chan error, 1)\n+\tgo stopServers(done)\n+\tselect {\n+\tcase <-done:\n+\tcase <-time.After(30 * time.Second):\n+\t\tt.Fatal(\"timeout waiting for servers to shutdown\")\n+\t}\n+\n+\tfor server, runErr := range runErrs {\n+\t\tt.Fatalf(\"unexpected run error from module server %s: %v\", server, runErr)\n+\t}\n+\n+\tfor server, stopErr := range stopErrs {\n+\t\tt.Fatalf(\"unexpected stop error from module server %s: %v\", server, stopErr)\n+\t}\n+}\n+\n+func startAndWaitHealthy(t *testing.T, testServer testModuleServer, runErrs map[string]error) {\n+\tgo func() {\n+\t\t// this next line is to avoid double registration, as both InitializeDocumentBuilders as well as ProvideUnifiedStorageGrpcService\n+\t\t// are hard-coded to use prometheus.DefaultRegisterer\n+\t\t// the alternative would be to get the registry from wire, in which case the tests would receive a new\n+\t\t// registry automatically, but that _may_ change metric names\n+\t\t// We can remove this once that's fixed\n+\t\tprometheus.DefaultRegisterer = prometheus.NewRegistry()\n+\t\tif err := testServer.server.Run(); err != nil && !errors.Is(err, context.Canceled) {\n+\t\t\trunErrs[testServer.id] = err\n+\t\t}\n+\t}()\n+\n+\tdeadline := time.Now().Add(20 * time.Second)\n+\tfor {\n+\t\trequire.NoError(t, runErrs[testServer.id], \"failed to start \"+testServer.id)\n+\n+\t\tconn, err := net.DialTimeout(\"tcp\", testServer.grpcAddress, 1*time.Second)\n+\t\tif err == nil {\n+\t\t\t_ = conn.Close()\n+\t\t\tbreak\n+\t\t}\n+\n+\t\tif time.Now().After(deadline) {\n+\t\t\tt.Fatal(\"server failed to become ready: \", testServer.id)\n+\t\t}\n+\n+\t\ttime.Sleep(1 * time.Second)\n+\t}\n+\n+\tres, err := testServer.healthClient.Check(context.Background(), &grpc_health_v1.HealthCheckRequest{})\n+\trequire.NoError(t, err)\n+\trequire.Equal(t, res.Status, grpc_health_v1.HealthCheckResponse_SERVING)\n+}\n+\n+type testModuleServer struct {\n+\tserver         *ModuleServer\n+\thealthClient   grpc_health_v1.HealthClient\n+\tresourceClient resource.ResourceClient\n+\tid             string\n+\tgrpcAddress    string\n+}\n+\n+func initDistributorServerForTest(t *testing.T) testModuleServer {\n+\tcfg := setting.NewCfg()\n+\tcfg.HTTPPort = \"13000\"\n+\tcfg.GRPCServer.Network = \"tcp\"\n+\tcfg.GRPCServer.Address = \"127.0.0.1:20000\"",
        "comment_created_at": "2025-06-27T10:21:35+00:00",
        "comment_author": "pstibrany",
        "comment_body": "Same comment about port assignment -- we should let the system assign the random unique ports, and then find out what it used.",
        "pr_file_module": null
      }
    ]
  }
]