[
  {
    "discussion_id": "2131504567",
    "pr_number": 478,
    "pr_file": "apps/web/utils/actions/settings.ts",
    "created_at": "2025-06-06T04:57:45+00:00",
    "commented_code": "ctx: { userId },\n       parsedInput: { aiProvider, aiModel, aiApiKey },\n     }) => {\n-      await prisma.user.update({\n-        where: { id: userId },\n-        data:\n-          aiProvider === DEFAULT_PROVIDER\n-            ? { aiProvider: null, aiModel: null, aiApiKey: null }\n-            : { aiProvider, aiModel, aiApiKey },\n-      });\n+      try {\n+        await prisma.user.update({\n+          where: { id: userId },\n+          data: {\n+            aiProvider: aiProvider || DEFAULT_PROVIDER,\n+            aiModel,\n+            aiApiKey,\n+          },\n+        });\n+\n+        return { success: true };\n+      } catch (error) {\n+        console.error(\"Error updating AI settings:\", error);\n+        return { serverError: \"Failed to update settings\" };\n+      }\n+    },\n+  );\n+\n+export const updateDigestFrequencyAction = actionClient\n+  .metadata({ name: \"updateDigestFrequency\" })\n+  .schema(saveDigestFrequencyBody)\n+  .action(\n+    async ({\n+      ctx: { emailAccountId },\n+      parsedInput: { digestEmailFrequency },\n+    }) => {\n+      try {\n+        const emailAccount = await prisma.emailAccount.findUnique({\n+          where: { id: emailAccountId },\n+          include: { digestFrequency: true },\n+        });\n+\n+        if (!emailAccount) {\n+          return { serverError: \"Email account not found\" };\n+        }\n+\n+        // Handle digest frequency\n+        const userFrequency = frequencyToUserFrequency(digestEmailFrequency);\n+        if (userFrequency) {\n+          // Create or update the UserFrequency\n+          const userFrequencyRecord = await prisma.userFrequency.upsert({\n+            where: {\n+              emailAccountId,\n+            },\n+            create: {\n+              ...userFrequency,\n+              emailAccountId,\n+              lastOccurrenceAt: new Date(),\n+              nextOccurrenceAt: calculateNextDigestDate(userFrequency),\n+            },\n+            update: {\n+              ...userFrequency,\n+              lastOccurrenceAt: new Date(),\n+              nextOccurrenceAt: calculateNextDigestDate(userFrequency),\n+            },\n+          });\n+\n+          // Update the email account with the new digest frequency ID\n+          await prisma.emailAccount.update({\n+            where: { id: emailAccountId },\n+            data: {\n+              digestFrequencyId: userFrequencyRecord.id,\n+            },\n+          });\n+        } else if (emailAccount.digestFrequencyId) {\n+          // If frequency is set to NEVER, delete the UserFrequency\n+          await prisma.userFrequency.delete({\n+            where: {\n+              id: emailAccount.digestFrequencyId,\n+            },\n+          });\n+\n+          // Update the email account to remove the digest frequency ID\n+          await prisma.emailAccount.update({\n+            where: { id: emailAccountId },\n+            data: {\n+              digestFrequencyId: null,\n+            },\n+          });\n+        }",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "2131504567",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 478,
        "pr_file": "apps/web/utils/actions/settings.ts",
        "discussion_id": "2131504567",
        "commented_code": "@@ -34,12 +43,91 @@ export const updateAiSettingsAction = actionClientUser\n       ctx: { userId },\n       parsedInput: { aiProvider, aiModel, aiApiKey },\n     }) => {\n-      await prisma.user.update({\n-        where: { id: userId },\n-        data:\n-          aiProvider === DEFAULT_PROVIDER\n-            ? { aiProvider: null, aiModel: null, aiApiKey: null }\n-            : { aiProvider, aiModel, aiApiKey },\n-      });\n+      try {\n+        await prisma.user.update({\n+          where: { id: userId },\n+          data: {\n+            aiProvider: aiProvider || DEFAULT_PROVIDER,\n+            aiModel,\n+            aiApiKey,\n+          },\n+        });\n+\n+        return { success: true };\n+      } catch (error) {\n+        console.error(\"Error updating AI settings:\", error);\n+        return { serverError: \"Failed to update settings\" };\n+      }\n+    },\n+  );\n+\n+export const updateDigestFrequencyAction = actionClient\n+  .metadata({ name: \"updateDigestFrequency\" })\n+  .schema(saveDigestFrequencyBody)\n+  .action(\n+    async ({\n+      ctx: { emailAccountId },\n+      parsedInput: { digestEmailFrequency },\n+    }) => {\n+      try {\n+        const emailAccount = await prisma.emailAccount.findUnique({\n+          where: { id: emailAccountId },\n+          include: { digestFrequency: true },\n+        });\n+\n+        if (!emailAccount) {\n+          return { serverError: \"Email account not found\" };\n+        }\n+\n+        // Handle digest frequency\n+        const userFrequency = frequencyToUserFrequency(digestEmailFrequency);\n+        if (userFrequency) {\n+          // Create or update the UserFrequency\n+          const userFrequencyRecord = await prisma.userFrequency.upsert({\n+            where: {\n+              emailAccountId,\n+            },\n+            create: {\n+              ...userFrequency,\n+              emailAccountId,\n+              lastOccurrenceAt: new Date(),\n+              nextOccurrenceAt: calculateNextDigestDate(userFrequency),\n+            },\n+            update: {\n+              ...userFrequency,\n+              lastOccurrenceAt: new Date(),\n+              nextOccurrenceAt: calculateNextDigestDate(userFrequency),\n+            },\n+          });\n+\n+          // Update the email account with the new digest frequency ID\n+          await prisma.emailAccount.update({\n+            where: { id: emailAccountId },\n+            data: {\n+              digestFrequencyId: userFrequencyRecord.id,\n+            },\n+          });\n+        } else if (emailAccount.digestFrequencyId) {\n+          // If frequency is set to NEVER, delete the UserFrequency\n+          await prisma.userFrequency.delete({\n+            where: {\n+              id: emailAccount.digestFrequencyId,\n+            },\n+          });\n+\n+          // Update the email account to remove the digest frequency ID\n+          await prisma.emailAccount.update({\n+            where: { id: emailAccountId },\n+            data: {\n+              digestFrequencyId: null,\n+            },\n+          });\n+        }",
        "comment_created_at": "2025-06-06T04:57:45+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "_\ud83d\udee0\ufe0f Refactor suggestion_\n\n**Consider using database transactions for atomic updates.**\n\nThe digest frequency update involves multiple database operations that should be atomic to prevent inconsistent state if any operation fails.\n\n\nConsider wrapping the operations in a transaction:\n\n```diff\n         // Handle digest frequency\n         const userFrequency = frequencyToUserFrequency(digestEmailFrequency);\n-        if (userFrequency) {\n-          // Create or update the UserFrequency\n-          const userFrequencyRecord = await prisma.userFrequency.upsert({\n-            where: {\n-              emailAccountId,\n-            },\n-            create: {\n-              ...userFrequency,\n-              emailAccountId,\n-              lastOccurrenceAt: new Date(),\n-              nextOccurrenceAt: calculateNextDigestDate(userFrequency),\n-            },\n-            update: {\n-              ...userFrequency,\n-              lastOccurrenceAt: new Date(),\n-              nextOccurrenceAt: calculateNextDigestDate(userFrequency),\n-            },\n-          });\n-\n-          // Update the email account with the new digest frequency ID\n-          await prisma.emailAccount.update({\n-            where: { id: emailAccountId },\n-            data: {\n-              digestFrequencyId: userFrequencyRecord.id,\n-            },\n-          });\n-        } else if (emailAccount.digestFrequencyId) {\n-          // If frequency is set to NEVER, delete the UserFrequency\n-          await prisma.userFrequency.delete({\n-            where: {\n-              id: emailAccount.digestFrequencyId,\n-            },\n-          });\n-\n-          // Update the email account to remove the digest frequency ID\n-          await prisma.emailAccount.update({\n-            where: { id: emailAccountId },\n-            data: {\n-              digestFrequencyId: null,\n-            },\n-          });\n-        }\n+        await prisma.$transaction(async (tx) => {\n+          if (userFrequency) {\n+            // Create or update the UserFrequency\n+            const userFrequencyRecord = await tx.userFrequency.upsert({\n+              where: {\n+                emailAccountId,\n+              },\n+              create: {\n+                ...userFrequency,\n+                emailAccountId,\n+                lastOccurrenceAt: new Date(),\n+                nextOccurrenceAt: calculateNextDigestDate(userFrequency),\n+              },\n+              update: {\n+                ...userFrequency,\n+                lastOccurrenceAt: new Date(),\n+                nextOccurrenceAt: calculateNextDigestDate(userFrequency),\n+              },\n+            });\n+\n+            // Update the email account with the new digest frequency ID\n+            await tx.emailAccount.update({\n+              where: { id: emailAccountId },\n+              data: {\n+                digestFrequencyId: userFrequencyRecord.id,\n+              },\n+            });\n+          } else if (emailAccount.digestFrequencyId) {\n+            // If frequency is set to NEVER, delete the UserFrequency\n+            await tx.userFrequency.delete({\n+              where: {\n+                id: emailAccount.digestFrequencyId,\n+              },\n+            });\n+\n+            // Update the email account to remove the digest frequency ID\n+            await tx.emailAccount.update({\n+              where: { id: emailAccountId },\n+              data: {\n+                digestFrequencyId: null,\n+              },\n+            });\n+          }\n+        });\n```\n\n<details>\n<summary>\ud83e\udd16 Prompt for AI Agents</summary>\n\n```\nIn apps/web/utils/actions/settings.ts around lines 84 to 125, the code performs\nmultiple related database operations to update the digest frequency, which\nshould be atomic to avoid inconsistent state if one operation fails. Refactor\nthe code to wrap the upsert and update or the delete and update calls inside a\nsingle Prisma transaction, ensuring all operations succeed or fail together.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n\u2705 Addressed in commits 86d074c to bd385a6",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2141672661",
    "pr_number": 478,
    "pr_file": "apps/web/utils/digest/index.ts",
    "created_at": "2025-06-12T04:07:17+00:00",
    "commented_code": "+import { env } from \"@/env\";\n+import { publishToQstashQueue } from \"@/utils/upstash\";\n+import { getCronSecretHeader } from \"@/utils/cron\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+import prisma from \"@/utils/prisma\";\n+import type { DigestBody } from \"@/app/api/ai/digest/validation\";\n+import { DigestStatus } from \"@prisma/client\";\n+import type { DigestEmailSummarySchema } from \"@/app/api/resend/digest/validation\";\n+import type { EmailForAction } from \"@/utils/ai/types\";\n+\n+const logger = createScopedLogger(\"digest\");\n+\n+export async function enqueueDigestItem({\n+  email,\n+  emailAccountId,\n+  actionId,\n+  coldEmailId,\n+}: {\n+  email: EmailForAction;\n+  emailAccountId: string;\n+  actionId?: string;\n+  coldEmailId?: string;\n+}) {\n+  const url = `${env.NEXT_PUBLIC_BASE_URL}/api/ai/digest`;\n+  try {\n+    await publishToQstashQueue<DigestBody>({\n+      queueName: \"digest-item-summarize\",\n+      parallelism: 3, // Allow up to 3 concurrent jobs from this queue\n+      url,\n+      body: {\n+        emailAccountId,\n+        actionId,\n+        coldEmailId,\n+        message: {\n+          id: email.id,\n+          threadId: email.threadId,\n+          from: email.headers.from,\n+          to: email.headers.to,\n+          subject: email.headers.subject,\n+          content: email.textPlain || \"\",\n+        },\n+      },\n+      headers: getCronSecretHeader(),\n+    });\n+  } catch (error) {\n+    logger.error(\"Failed to publish to Qstash\", {\n+      emailAccountId,\n+      error,\n+    });\n+  }\n+}\n+\n+async function findOldestUnsentDigest(emailAccountId: string) {\n+  return prisma.digest.findFirst({\n+    where: {\n+      emailAccountId,\n+      status: DigestStatus.PENDING,\n+    },\n+    orderBy: {\n+      createdAt: \"asc\",\n+    },\n+  });\n+}\n+\n+export async function upsertDigest({\n+  messageId,\n+  threadId,\n+  emailAccountId,\n+  actionId,\n+  coldEmailId,\n+  content,\n+}: {\n+  messageId: string;\n+  threadId: string;\n+  emailAccountId: string;\n+  actionId?: string;\n+  coldEmailId?: string;\n+  content: DigestEmailSummarySchema;\n+}) {\n+  const logger = createScopedLogger(\"upsert-digest\").with({\n+    messageId,\n+    threadId,\n+    emailAccountId,\n+    actionId,\n+    coldEmailId,\n+  });\n+\n+  try {\n+    await prisma.$transaction(async (tx) => {\n+      // Find or create the digest atomically\n+      const digest =\n+        (await tx.digest.findFirst({\n+          where: {\n+            emailAccountId,\n+            status: DigestStatus.PENDING,\n+          },\n+          orderBy: {\n+            createdAt: \"asc\",\n+          },\n+        })) ||\n+        (await tx.digest.create({\n+          data: {\n+            emailAccountId,\n+            status: DigestStatus.PENDING,\n+          },\n+        }));",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "2141672661",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 478,
        "pr_file": "apps/web/utils/digest/index.ts",
        "discussion_id": "2141672661",
        "commented_code": "@@ -0,0 +1,147 @@\n+import { env } from \"@/env\";\n+import { publishToQstashQueue } from \"@/utils/upstash\";\n+import { getCronSecretHeader } from \"@/utils/cron\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+import prisma from \"@/utils/prisma\";\n+import type { DigestBody } from \"@/app/api/ai/digest/validation\";\n+import { DigestStatus } from \"@prisma/client\";\n+import type { DigestEmailSummarySchema } from \"@/app/api/resend/digest/validation\";\n+import type { EmailForAction } from \"@/utils/ai/types\";\n+\n+const logger = createScopedLogger(\"digest\");\n+\n+export async function enqueueDigestItem({\n+  email,\n+  emailAccountId,\n+  actionId,\n+  coldEmailId,\n+}: {\n+  email: EmailForAction;\n+  emailAccountId: string;\n+  actionId?: string;\n+  coldEmailId?: string;\n+}) {\n+  const url = `${env.NEXT_PUBLIC_BASE_URL}/api/ai/digest`;\n+  try {\n+    await publishToQstashQueue<DigestBody>({\n+      queueName: \"digest-item-summarize\",\n+      parallelism: 3, // Allow up to 3 concurrent jobs from this queue\n+      url,\n+      body: {\n+        emailAccountId,\n+        actionId,\n+        coldEmailId,\n+        message: {\n+          id: email.id,\n+          threadId: email.threadId,\n+          from: email.headers.from,\n+          to: email.headers.to,\n+          subject: email.headers.subject,\n+          content: email.textPlain || \"\",\n+        },\n+      },\n+      headers: getCronSecretHeader(),\n+    });\n+  } catch (error) {\n+    logger.error(\"Failed to publish to Qstash\", {\n+      emailAccountId,\n+      error,\n+    });\n+  }\n+}\n+\n+async function findOldestUnsentDigest(emailAccountId: string) {\n+  return prisma.digest.findFirst({\n+    where: {\n+      emailAccountId,\n+      status: DigestStatus.PENDING,\n+    },\n+    orderBy: {\n+      createdAt: \"asc\",\n+    },\n+  });\n+}\n+\n+export async function upsertDigest({\n+  messageId,\n+  threadId,\n+  emailAccountId,\n+  actionId,\n+  coldEmailId,\n+  content,\n+}: {\n+  messageId: string;\n+  threadId: string;\n+  emailAccountId: string;\n+  actionId?: string;\n+  coldEmailId?: string;\n+  content: DigestEmailSummarySchema;\n+}) {\n+  const logger = createScopedLogger(\"upsert-digest\").with({\n+    messageId,\n+    threadId,\n+    emailAccountId,\n+    actionId,\n+    coldEmailId,\n+  });\n+\n+  try {\n+    await prisma.$transaction(async (tx) => {\n+      // Find or create the digest atomically\n+      const digest =\n+        (await tx.digest.findFirst({\n+          where: {\n+            emailAccountId,\n+            status: DigestStatus.PENDING,\n+          },\n+          orderBy: {\n+            createdAt: \"asc\",\n+          },\n+        })) ||\n+        (await tx.digest.create({\n+          data: {\n+            emailAccountId,\n+            status: DigestStatus.PENDING,\n+          },\n+        }));",
        "comment_created_at": "2025-06-12T04:07:17+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "_\u26a0\ufe0f Potential issue_\n\n**Race condition persists despite transaction**\n\nTwo concurrent transactions can still read \u201cno pending digest\u201d and both insert a new row because the `findFirst` query doesn\u2019t lock anything and there\u2019s no unique constraint on `(emailAccountId, status)`.  \nAdd a partial unique index (or use `connectOrCreate`) to guarantee at most one `PENDING` digest per account:\n\n```sql\n-- migration snippet\nCREATE UNIQUE INDEX one_pending_digest_per_account\n  ON \"Digest\"( \"emailAccountId\" )\n  WHERE status = 'PENDING';\n```\n\nThen rely on `tx.digest.upsert` / `connectOrCreate` and catch `UniqueViolation` for a clean, contention-free solution.\n\n<details>\n<summary>\ud83e\udd16 Prompt for AI Agents</summary>\n\n```\nIn apps/web/utils/digest/index.ts around lines 88 to 106, the current\ntransaction uses findFirst without locking and lacks a unique constraint on\n(emailAccountId, status), causing race conditions with concurrent inserts. Fix\nthis by adding a partial unique index on Digest(emailAccountId) where status is\n'PENDING' via a migration. Then replace the findFirst and create logic with a\nsingle upsert or connectOrCreate call inside the transaction, and handle\nUniqueViolation errors to ensure only one PENDING digest per account without\ncontention.\n```\n\n</details>\n\n<!-- This is an auto-generated reply by CodeRabbit -->",
        "pr_file_module": null
      },
      {
        "comment_id": "2141678988",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 478,
        "pr_file": "apps/web/utils/digest/index.ts",
        "discussion_id": "2141672661",
        "commented_code": "@@ -0,0 +1,147 @@\n+import { env } from \"@/env\";\n+import { publishToQstashQueue } from \"@/utils/upstash\";\n+import { getCronSecretHeader } from \"@/utils/cron\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+import prisma from \"@/utils/prisma\";\n+import type { DigestBody } from \"@/app/api/ai/digest/validation\";\n+import { DigestStatus } from \"@prisma/client\";\n+import type { DigestEmailSummarySchema } from \"@/app/api/resend/digest/validation\";\n+import type { EmailForAction } from \"@/utils/ai/types\";\n+\n+const logger = createScopedLogger(\"digest\");\n+\n+export async function enqueueDigestItem({\n+  email,\n+  emailAccountId,\n+  actionId,\n+  coldEmailId,\n+}: {\n+  email: EmailForAction;\n+  emailAccountId: string;\n+  actionId?: string;\n+  coldEmailId?: string;\n+}) {\n+  const url = `${env.NEXT_PUBLIC_BASE_URL}/api/ai/digest`;\n+  try {\n+    await publishToQstashQueue<DigestBody>({\n+      queueName: \"digest-item-summarize\",\n+      parallelism: 3, // Allow up to 3 concurrent jobs from this queue\n+      url,\n+      body: {\n+        emailAccountId,\n+        actionId,\n+        coldEmailId,\n+        message: {\n+          id: email.id,\n+          threadId: email.threadId,\n+          from: email.headers.from,\n+          to: email.headers.to,\n+          subject: email.headers.subject,\n+          content: email.textPlain || \"\",\n+        },\n+      },\n+      headers: getCronSecretHeader(),\n+    });\n+  } catch (error) {\n+    logger.error(\"Failed to publish to Qstash\", {\n+      emailAccountId,\n+      error,\n+    });\n+  }\n+}\n+\n+async function findOldestUnsentDigest(emailAccountId: string) {\n+  return prisma.digest.findFirst({\n+    where: {\n+      emailAccountId,\n+      status: DigestStatus.PENDING,\n+    },\n+    orderBy: {\n+      createdAt: \"asc\",\n+    },\n+  });\n+}\n+\n+export async function upsertDigest({\n+  messageId,\n+  threadId,\n+  emailAccountId,\n+  actionId,\n+  coldEmailId,\n+  content,\n+}: {\n+  messageId: string;\n+  threadId: string;\n+  emailAccountId: string;\n+  actionId?: string;\n+  coldEmailId?: string;\n+  content: DigestEmailSummarySchema;\n+}) {\n+  const logger = createScopedLogger(\"upsert-digest\").with({\n+    messageId,\n+    threadId,\n+    emailAccountId,\n+    actionId,\n+    coldEmailId,\n+  });\n+\n+  try {\n+    await prisma.$transaction(async (tx) => {\n+      // Find or create the digest atomically\n+      const digest =\n+        (await tx.digest.findFirst({\n+          where: {\n+            emailAccountId,\n+            status: DigestStatus.PENDING,\n+          },\n+          orderBy: {\n+            createdAt: \"asc\",\n+          },\n+        })) ||\n+        (await tx.digest.create({\n+          data: {\n+            emailAccountId,\n+            status: DigestStatus.PENDING,\n+          },\n+        }));",
        "comment_created_at": "2025-06-12T04:14:21+00:00",
        "comment_author": "edulelis",
        "comment_body": "Unsupported by Prisma",
        "pr_file_module": null
      },
      {
        "comment_id": "2141679866",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 478,
        "pr_file": "apps/web/utils/digest/index.ts",
        "discussion_id": "2141672661",
        "commented_code": "@@ -0,0 +1,147 @@\n+import { env } from \"@/env\";\n+import { publishToQstashQueue } from \"@/utils/upstash\";\n+import { getCronSecretHeader } from \"@/utils/cron\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+import prisma from \"@/utils/prisma\";\n+import type { DigestBody } from \"@/app/api/ai/digest/validation\";\n+import { DigestStatus } from \"@prisma/client\";\n+import type { DigestEmailSummarySchema } from \"@/app/api/resend/digest/validation\";\n+import type { EmailForAction } from \"@/utils/ai/types\";\n+\n+const logger = createScopedLogger(\"digest\");\n+\n+export async function enqueueDigestItem({\n+  email,\n+  emailAccountId,\n+  actionId,\n+  coldEmailId,\n+}: {\n+  email: EmailForAction;\n+  emailAccountId: string;\n+  actionId?: string;\n+  coldEmailId?: string;\n+}) {\n+  const url = `${env.NEXT_PUBLIC_BASE_URL}/api/ai/digest`;\n+  try {\n+    await publishToQstashQueue<DigestBody>({\n+      queueName: \"digest-item-summarize\",\n+      parallelism: 3, // Allow up to 3 concurrent jobs from this queue\n+      url,\n+      body: {\n+        emailAccountId,\n+        actionId,\n+        coldEmailId,\n+        message: {\n+          id: email.id,\n+          threadId: email.threadId,\n+          from: email.headers.from,\n+          to: email.headers.to,\n+          subject: email.headers.subject,\n+          content: email.textPlain || \"\",\n+        },\n+      },\n+      headers: getCronSecretHeader(),\n+    });\n+  } catch (error) {\n+    logger.error(\"Failed to publish to Qstash\", {\n+      emailAccountId,\n+      error,\n+    });\n+  }\n+}\n+\n+async function findOldestUnsentDigest(emailAccountId: string) {\n+  return prisma.digest.findFirst({\n+    where: {\n+      emailAccountId,\n+      status: DigestStatus.PENDING,\n+    },\n+    orderBy: {\n+      createdAt: \"asc\",\n+    },\n+  });\n+}\n+\n+export async function upsertDigest({\n+  messageId,\n+  threadId,\n+  emailAccountId,\n+  actionId,\n+  coldEmailId,\n+  content,\n+}: {\n+  messageId: string;\n+  threadId: string;\n+  emailAccountId: string;\n+  actionId?: string;\n+  coldEmailId?: string;\n+  content: DigestEmailSummarySchema;\n+}) {\n+  const logger = createScopedLogger(\"upsert-digest\").with({\n+    messageId,\n+    threadId,\n+    emailAccountId,\n+    actionId,\n+    coldEmailId,\n+  });\n+\n+  try {\n+    await prisma.$transaction(async (tx) => {\n+      // Find or create the digest atomically\n+      const digest =\n+        (await tx.digest.findFirst({\n+          where: {\n+            emailAccountId,\n+            status: DigestStatus.PENDING,\n+          },\n+          orderBy: {\n+            createdAt: \"asc\",\n+          },\n+        })) ||\n+        (await tx.digest.create({\n+          data: {\n+            emailAccountId,\n+            status: DigestStatus.PENDING,\n+          },\n+        }));",
        "comment_created_at": "2025-06-12T04:15:32+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "@edulelis you\u2019re right\u2014Prisma\u2019s schema DSL can\u2019t express a partial-unique index, so `@@unique`/`@id` alone can\u2019t enforce \u201conly one PENDING digest per account\u201d. We still have a couple of safe, Prisma-friendly options:\n\n1. Keep the partial index, but create it with a raw SQL migration  \n   Prisma happily co-exists with indexes it doesn\u2019t know about; it just ignores them on `migrate dev`.  \n   ```sql\n   -- prisma/migrations/<timestamp>_one_pending_digest/migration.sql\n   CREATE UNIQUE INDEX IF NOT EXISTS one_pending_digest_per_account\n   ON \"Digest\" (\"emailAccountId\")\n   WHERE status = 'PENDING';\n   ```\n   You then keep the existing `findFirst` / `create` logic, but wrap the `create` in a try/catch and fall back to `findFirst` on `P2002` (unique-violation) to cope with the rare clash:\n\n   ```ts\n   let digest;\n   try {\n     digest = await tx.digest.create({ data: { emailAccountId, status: DigestStatus.PENDING } });\n   } catch (e) {\n     if (e instanceof Prisma.PrismaClientKnownRequestError && e.code === \"P2002\") {\n       digest = await tx.digest.findFirst({ where: { emailAccountId, status: DigestStatus.PENDING } });\n     } else {\n       throw e;\n     }\n   }\n   ```\n\n2. Stay purely in Prisma by widening the uniqueness to every status  \n   Add `@@unique([emailAccountId, status])`.  \n   That guarantees one digest _per status_ per account, which means historical digests would need their status column cleared or replaced (e.g. `ARCHIVED`) once they\u2019re sent if you want more than one \u201ccompleted\u201d digest record. If that data-model tweak is acceptable, you can then replace the current pair of `findFirst`/`create` calls with a straight `upsert`:\n\n   ```ts\n   const digest = await tx.digest.upsert({\n     where: { emailAccountId_status: { emailAccountId, status: DigestStatus.PENDING } },\n     create: { emailAccountId, status: DigestStatus.PENDING },\n     update: {},\n   });\n   ```\n\n   (The compound unique name `emailAccountId_status` is automatically generated by Prisma.)\n\n---\n\nMost teams choose option 1\u2014raw SQL partial index\u2014because it leaves the table design untouched and enforces exactly the constraint we need, while keeping the Prisma schema clean. If you prefer to avoid raw SQL migrations entirely, option 2 works with a small data-model concession.\n\nLet me know which direction you\u2019d like to take and I can help draft the code/migration.\n\n\n<!-- <agentComponentType>chat-answer</agentComponentType> -->\n\n<!-- <rawResChunk><answer>@edulelis you\u2019re right\u2014Prisma\u2019s schema DSL can\u2019t express a partial-unique index, so `@@unique`/`@id` alone can\u2019t enforce \u201conly one PENDING digest per account\u201d. We still have a couple of safe, Prisma-friendly options:\n\n1. Keep the partial index, but create it with a raw SQL migration  \n   Prisma happily co-exists with indexes it doesn\u2019t know about; it just ignores them on `migrate dev`.  \n   ```sql\n   -- prisma/migrations/<timestamp>_one_pending_digest/migration.sql\n   CREATE UNIQUE INDEX IF NOT EXISTS one_pending_digest_per_account\n   ON \"Digest\" (\"emailAccountId\")\n   WHERE status = 'PENDING';\n   ```\n   You then keep the existing `findFirst` / `create` logic, but wrap the `create` in a try/catch and fall back to `findFirst` on `P2002` (unique-violation) to cope with the rare clash:\n\n   ```ts\n   let digest;\n   try {\n     digest = await tx.digest.create({ data: { emailAccountId, status: DigestStatus.PENDING } });\n   } catch (e) {\n     if (e instanceof Prisma.PrismaClientKnownRequestError && e.code === \"P2002\") {\n       digest = await tx.digest.findFirst({ where: { emailAccountId, status: DigestStatus.PENDING } });\n     } else {\n       throw e;\n     }\n   }\n   ```\n\n2. Stay purely in Prisma by widening the uniqueness to every status  \n   Add `@@unique([emailAccountId, status])`.  \n   That guarantees one digest _per status_ per account, which means historical digests would need their status column cleared or replaced (e.g. `ARCHIVED`) once they\u2019re sent if you want more than one \u201ccompleted\u201d digest record. If that data-model tweak is acceptable, you can then replace the current pair of `findFirst`/`create` calls with a straight `upsert`:\n\n   ```ts\n   const digest = await tx.digest.upsert({\n     where: { emailAccountId_status: { emailAccountId, status: DigestStatus.PENDING } },\n     create: { emailAccountId, status: DigestStatus.PENDING },\n     update: {},\n   });\n   ```\n\n   (The compound unique name `emailAccountId_status` is automatically generated by Prisma.)\n\n---\n\nMost teams choose option 1\u2014raw SQL partial index\u2014because it leaves the table design untouched and enforces exactly the constraint we need, while keeping the Prisma schema clean. If you prefer to avoid raw SQL migrations entirely, option 2 works with a small data-model concession.\n\nLet me know which direction you\u2019d like to take and I can help draft the code/migration.</answer></rawResChunk> -->\n\n<!-- This is an auto-generated reply by CodeRabbit -->",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2147632316",
    "pr_number": 478,
    "pr_file": "apps/web/utils/actions/settings.ts",
    "created_at": "2025-06-15T11:35:48+00:00",
    "commented_code": "ctx: { userId },\n       parsedInput: { aiProvider, aiModel, aiApiKey },\n     }) => {\n-      await prisma.user.update({\n-        where: { id: userId },\n-        data:\n-          aiProvider === DEFAULT_PROVIDER\n-            ? { aiProvider: null, aiModel: null, aiApiKey: null }\n-            : { aiProvider, aiModel, aiApiKey },\n-      });\n+      try {\n+        await prisma.user.update({\n+          where: { id: userId },\n+          data: {\n+            aiProvider: aiProvider || DEFAULT_PROVIDER,\n+            aiModel,\n+            aiApiKey,\n+          },\n+        });\n+\n+        return { success: true };\n+      } catch (error) {\n+        return { serverError: \"Failed to update settings\" };\n+      }\n+    },\n+  );\n+\n+export const updateDigestFrequencyAction = actionClient\n+  .metadata({ name: \"updateDigestFrequency\" })\n+  .schema(saveDigestFrequencyBody)\n+  .action(\n+    async ({ ctx: { emailAccountId }, parsedInput: { userFrequency } }) => {\n+      try {\n+        const emailAccount = await prisma.emailAccount.findUnique({\n+          where: { id: emailAccountId },\n+          include: { digestFrequency: true },\n+        });\n+\n+        if (!emailAccount) {\n+          return { serverError: \"Email account not found\" };\n+        }\n+\n+        await prisma.$transaction(async (tx) => {",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "2147632316",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 478,
        "pr_file": "apps/web/utils/actions/settings.ts",
        "discussion_id": "2147632316",
        "commented_code": "@@ -34,12 +37,86 @@ export const updateAiSettingsAction = actionClientUser\n       ctx: { userId },\n       parsedInput: { aiProvider, aiModel, aiApiKey },\n     }) => {\n-      await prisma.user.update({\n-        where: { id: userId },\n-        data:\n-          aiProvider === DEFAULT_PROVIDER\n-            ? { aiProvider: null, aiModel: null, aiApiKey: null }\n-            : { aiProvider, aiModel, aiApiKey },\n-      });\n+      try {\n+        await prisma.user.update({\n+          where: { id: userId },\n+          data: {\n+            aiProvider: aiProvider || DEFAULT_PROVIDER,\n+            aiModel,\n+            aiApiKey,\n+          },\n+        });\n+\n+        return { success: true };\n+      } catch (error) {\n+        return { serverError: \"Failed to update settings\" };\n+      }\n+    },\n+  );\n+\n+export const updateDigestFrequencyAction = actionClient\n+  .metadata({ name: \"updateDigestFrequency\" })\n+  .schema(saveDigestFrequencyBody)\n+  .action(\n+    async ({ ctx: { emailAccountId }, parsedInput: { userFrequency } }) => {\n+      try {\n+        const emailAccount = await prisma.emailAccount.findUnique({\n+          where: { id: emailAccountId },\n+          include: { digestFrequency: true },\n+        });\n+\n+        if (!emailAccount) {\n+          return { serverError: \"Email account not found\" };\n+        }\n+\n+        await prisma.$transaction(async (tx) => {",
        "comment_created_at": "2025-06-15T11:35:48+00:00",
        "comment_author": "elie222",
        "comment_body": "i prefer not to use this form of transaction unless absolutely needed. prefer to use array instead:\r\n\r\n```\r\n$transaction([...])\r\n```\r\n\r\n1. Warning against them here: https://www.prisma.io/docs/orm/prisma-client/queries/transactions#interactive-transactions\r\n2. Will force us to use Websockets if we want to use this: https://neon.com/docs/serverless/serverless-driver\r\n\r\nIn general we can almost always use non-interactive transactions so I'd default to that.",
        "pr_file_module": null
      },
      {
        "comment_id": "2150052278",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 478,
        "pr_file": "apps/web/utils/actions/settings.ts",
        "discussion_id": "2147632316",
        "commented_code": "@@ -34,12 +37,86 @@ export const updateAiSettingsAction = actionClientUser\n       ctx: { userId },\n       parsedInput: { aiProvider, aiModel, aiApiKey },\n     }) => {\n-      await prisma.user.update({\n-        where: { id: userId },\n-        data:\n-          aiProvider === DEFAULT_PROVIDER\n-            ? { aiProvider: null, aiModel: null, aiApiKey: null }\n-            : { aiProvider, aiModel, aiApiKey },\n-      });\n+      try {\n+        await prisma.user.update({\n+          where: { id: userId },\n+          data: {\n+            aiProvider: aiProvider || DEFAULT_PROVIDER,\n+            aiModel,\n+            aiApiKey,\n+          },\n+        });\n+\n+        return { success: true };\n+      } catch (error) {\n+        return { serverError: \"Failed to update settings\" };\n+      }\n+    },\n+  );\n+\n+export const updateDigestFrequencyAction = actionClient\n+  .metadata({ name: \"updateDigestFrequency\" })\n+  .schema(saveDigestFrequencyBody)\n+  .action(\n+    async ({ ctx: { emailAccountId }, parsedInput: { userFrequency } }) => {\n+      try {\n+        const emailAccount = await prisma.emailAccount.findUnique({\n+          where: { id: emailAccountId },\n+          include: { digestFrequency: true },\n+        });\n+\n+        if (!emailAccount) {\n+          return { serverError: \"Email account not found\" };\n+        }\n+\n+        await prisma.$transaction(async (tx) => {",
        "comment_created_at": "2025-06-16T13:45:05+00:00",
        "comment_author": "edulelis",
        "comment_body": "Most of these are not a must, so I removed them.\r\nThese were CodeRabbit suggestions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2151115255",
    "pr_number": 478,
    "pr_file": "apps/web/app/api/resend/digest/route.ts",
    "created_at": "2025-06-17T00:44:27+00:00",
    "commented_code": "+import { NextResponse } from \"next/server\";\n+import { sendDigestEmail } from \"@inboxzero/resend\";\n+import { withEmailAccount, withError } from \"@/utils/middleware\";\n+import { env } from \"@/env\";\n+import { hasCronSecret } from \"@/utils/cron\";\n+import { captureException, SafeError } from \"@/utils/error\";\n+import prisma from \"@/utils/prisma\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+import { createUnsubscribeToken } from \"@/utils/unsubscribe\";\n+import { camelCase } from \"lodash\";\n+import { calculateNextScheduleDate } from \"@/utils/frequency\";\n+import { getMessagesLargeBatch } from \"@/utils/gmail/message\";\n+import {\n+  digestCategorySchema,\n+  sendDigestEmailBody,\n+  type Digest,\n+  DigestEmailSummarySchema,\n+} from \"./validation\";\n+import { DigestStatus } from \"@prisma/client\";\n+import { extractNameFromEmail } from \"../../../../utils/email\";\n+import { RuleName } from \"@/utils/rule/consts\";\n+import { getEmailAccountWithAiAndTokens } from \"@/utils/user/get\";\n+import { getGmailClientWithRefresh } from \"@/utils/gmail/client\";\n+\n+export const maxDuration = 60;\n+\n+const logger = createScopedLogger(\"resend/digest\");\n+\n+type SendEmailResult = {\n+  success: boolean;\n+  message: string;\n+};\n+\n+async function sendEmail({\n+  emailAccountId,\n+  force,\n+}: {\n+  emailAccountId: string;\n+  force?: boolean;\n+}): Promise<SendEmailResult> {\n+  const loggerOptions = { emailAccountId, force };\n+  logger.info(\"Sending digest email\", loggerOptions);\n+\n+  const emailAccount = await getEmailAccountWithAiAndTokens({ emailAccountId });\n+\n+  if (!emailAccount) {\n+    throw new Error(\"Email account not found\");\n+  }\n+\n+  if (!emailAccount.tokens.access_token) {\n+    throw new Error(\"No access token available\");\n+  }\n+\n+  const gmail = await getGmailClientWithRefresh({\n+    accessToken: emailAccount.tokens.access_token,\n+    refreshToken: emailAccount.tokens.refresh_token,\n+    expiresAt: emailAccount.tokens.expires_at,\n+    emailAccountId,\n+  });\n+\n+  const pendingDigests = await prisma.digest.findMany({\n+    where: {\n+      emailAccountId,\n+      status: DigestStatus.PENDING,\n+    },\n+    select: {\n+      id: true,\n+      items: {\n+        select: {\n+          messageId: true,\n+          content: true,\n+          action: {\n+            select: {\n+              executedRule: {\n+                select: {\n+                  rule: {\n+                    select: {\n+                      name: true,\n+                    },\n+                  },\n+                },\n+              },\n+            },\n+          },\n+        },\n+      },\n+    },\n+  });\n+\n+  if (pendingDigests.length) {\n+    // Mark all found digests as processing\n+    await prisma.digest.updateMany({\n+      where: {\n+        id: {\n+          in: pendingDigests.map((d) => d.id),\n+        },\n+      },\n+      data: {\n+        status: DigestStatus.PROCESSING,\n+      },\n+    });\n+  }\n+\n+  try {\n+    // Return early if no digests were found, unless force is true\n+    if (pendingDigests.length === 0) {\n+      if (!force) {\n+        return { success: true, message: \"No digests to process\" };\n+      }\n+      // When force is true, send an empty digest to indicate the system is working\n+      logger.info(\"Force sending empty digest\", { emailAccountId });\n+    }\n+\n+    // Store the digest IDs for the final update\n+    const processedDigestIds = pendingDigests.map((d) => d.id);\n+\n+    const messageIds = pendingDigests.flatMap((digest) =>\n+      digest.items.map((item) => item.messageId),\n+    );\n+\n+    const messages = await getMessagesLargeBatch({\n+      gmail,\n+      messageIds,\n+    });\n+\n+    // Create a message lookup map for O(1) access\n+    const messageMap = new Map(messages.map((m) => [m.id, m]));\n+\n+    // Transform and group in a single pass\n+    const executedRulesByRule = pendingDigests.reduce((acc, digest) => {\n+      digest.items.forEach((item) => {\n+        const message = messageMap.get(item.messageId);\n+        if (!message) {\n+          logger.warn(\"Message not found, skipping digest item\", {\n+            messageId: item.messageId,\n+          });\n+          return;\n+        }\n+\n+        const ruleName = camelCase(\n+          item.action?.executedRule?.rule?.name || RuleName.ColdEmail,\n+        );\n+\n+        // Only include if it's one of our known categories\n+        const categoryResult = digestCategorySchema.safeParse(ruleName);\n+        if (categoryResult.success) {\n+          const category = categoryResult.data;\n+          if (!acc[category]) {\n+            acc[category] = [];\n+          }\n+\n+          let parsedContent: unknown;\n+          try {\n+            parsedContent = JSON.parse(item.content);\n+          } catch (error) {\n+            logger.warn(\"Failed to parse digest item content, skipping item\", {\n+              messageId: item.messageId,\n+              digestId: digest.id,\n+              error: error instanceof Error ? error.message : \"Unknown error\",\n+            });\n+            return; // Skip this item and continue with the next one\n+          }\n+\n+          const contentResult =\n+            DigestEmailSummarySchema.safeParse(parsedContent);\n+\n+          if (contentResult.success) {\n+            acc[category].push({\n+              content: {\n+                entries: contentResult.data?.entries || [],\n+                summary: contentResult.data?.summary,\n+              },\n+              from: extractNameFromEmail(message?.headers?.from || \"\"),\n+              subject: message?.headers?.subject || \"\",\n+            });\n+          }\n+        }\n+      });\n+      return acc;\n+    }, {} as Digest);\n+\n+    const token = await createUnsubscribeToken({ emailAccountId });\n+\n+    // First, send the digest email and wait for it to complete\n+    await sendDigestEmail({\n+      from: env.RESEND_FROM_EMAIL,\n+      to: emailAccount.email,\n+      emailProps: {\n+        ...executedRulesByRule,\n+        baseUrl: env.NEXT_PUBLIC_BASE_URL,\n+        unsubscribeToken: token,\n+        date: new Date(),\n+      },\n+    });\n+\n+    // Only update database if email sending succeeded\n+    await Promise.all([\n+      ...(emailAccount.digestScheduleId\n+        ? [\n+            prisma.schedule.update({\n+              where: {\n+                id: emailAccount.digestScheduleId,\n+                emailAccountId,\n+              },\n+              data: {\n+                lastOccurrenceAt: new Date(),\n+                nextOccurrenceAt: calculateNextScheduleDate(\n+                  emailAccount.digestSchedule!,\n+                ),\n+              },\n+            }),\n+          ]\n+        : []),\n+      // Mark only the processed digests as sent\n+      prisma.digest.updateMany({\n+        where: {\n+          id: {\n+            in: processedDigestIds,\n+          },\n+        },\n+        data: {\n+          status: DigestStatus.SENT,\n+          sentAt: new Date(),\n+        },\n+      }),\n+      // Redact all DigestItems for the processed digests\n+      prisma.digestItem.updateMany({\n+        data: { content: \"[REDACTED]\" },\n+        where: {\n+          digestId: {\n+            in: processedDigestIds,\n+          },\n+        },\n+      }),\n+    ]);",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "2151115255",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 478,
        "pr_file": "apps/web/app/api/resend/digest/route.ts",
        "discussion_id": "2151115255",
        "commented_code": "@@ -0,0 +1,298 @@\n+import { NextResponse } from \"next/server\";\n+import { sendDigestEmail } from \"@inboxzero/resend\";\n+import { withEmailAccount, withError } from \"@/utils/middleware\";\n+import { env } from \"@/env\";\n+import { hasCronSecret } from \"@/utils/cron\";\n+import { captureException, SafeError } from \"@/utils/error\";\n+import prisma from \"@/utils/prisma\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+import { createUnsubscribeToken } from \"@/utils/unsubscribe\";\n+import { camelCase } from \"lodash\";\n+import { calculateNextScheduleDate } from \"@/utils/frequency\";\n+import { getMessagesLargeBatch } from \"@/utils/gmail/message\";\n+import {\n+  digestCategorySchema,\n+  sendDigestEmailBody,\n+  type Digest,\n+  DigestEmailSummarySchema,\n+} from \"./validation\";\n+import { DigestStatus } from \"@prisma/client\";\n+import { extractNameFromEmail } from \"../../../../utils/email\";\n+import { RuleName } from \"@/utils/rule/consts\";\n+import { getEmailAccountWithAiAndTokens } from \"@/utils/user/get\";\n+import { getGmailClientWithRefresh } from \"@/utils/gmail/client\";\n+\n+export const maxDuration = 60;\n+\n+const logger = createScopedLogger(\"resend/digest\");\n+\n+type SendEmailResult = {\n+  success: boolean;\n+  message: string;\n+};\n+\n+async function sendEmail({\n+  emailAccountId,\n+  force,\n+}: {\n+  emailAccountId: string;\n+  force?: boolean;\n+}): Promise<SendEmailResult> {\n+  const loggerOptions = { emailAccountId, force };\n+  logger.info(\"Sending digest email\", loggerOptions);\n+\n+  const emailAccount = await getEmailAccountWithAiAndTokens({ emailAccountId });\n+\n+  if (!emailAccount) {\n+    throw new Error(\"Email account not found\");\n+  }\n+\n+  if (!emailAccount.tokens.access_token) {\n+    throw new Error(\"No access token available\");\n+  }\n+\n+  const gmail = await getGmailClientWithRefresh({\n+    accessToken: emailAccount.tokens.access_token,\n+    refreshToken: emailAccount.tokens.refresh_token,\n+    expiresAt: emailAccount.tokens.expires_at,\n+    emailAccountId,\n+  });\n+\n+  const pendingDigests = await prisma.digest.findMany({\n+    where: {\n+      emailAccountId,\n+      status: DigestStatus.PENDING,\n+    },\n+    select: {\n+      id: true,\n+      items: {\n+        select: {\n+          messageId: true,\n+          content: true,\n+          action: {\n+            select: {\n+              executedRule: {\n+                select: {\n+                  rule: {\n+                    select: {\n+                      name: true,\n+                    },\n+                  },\n+                },\n+              },\n+            },\n+          },\n+        },\n+      },\n+    },\n+  });\n+\n+  if (pendingDigests.length) {\n+    // Mark all found digests as processing\n+    await prisma.digest.updateMany({\n+      where: {\n+        id: {\n+          in: pendingDigests.map((d) => d.id),\n+        },\n+      },\n+      data: {\n+        status: DigestStatus.PROCESSING,\n+      },\n+    });\n+  }\n+\n+  try {\n+    // Return early if no digests were found, unless force is true\n+    if (pendingDigests.length === 0) {\n+      if (!force) {\n+        return { success: true, message: \"No digests to process\" };\n+      }\n+      // When force is true, send an empty digest to indicate the system is working\n+      logger.info(\"Force sending empty digest\", { emailAccountId });\n+    }\n+\n+    // Store the digest IDs for the final update\n+    const processedDigestIds = pendingDigests.map((d) => d.id);\n+\n+    const messageIds = pendingDigests.flatMap((digest) =>\n+      digest.items.map((item) => item.messageId),\n+    );\n+\n+    const messages = await getMessagesLargeBatch({\n+      gmail,\n+      messageIds,\n+    });\n+\n+    // Create a message lookup map for O(1) access\n+    const messageMap = new Map(messages.map((m) => [m.id, m]));\n+\n+    // Transform and group in a single pass\n+    const executedRulesByRule = pendingDigests.reduce((acc, digest) => {\n+      digest.items.forEach((item) => {\n+        const message = messageMap.get(item.messageId);\n+        if (!message) {\n+          logger.warn(\"Message not found, skipping digest item\", {\n+            messageId: item.messageId,\n+          });\n+          return;\n+        }\n+\n+        const ruleName = camelCase(\n+          item.action?.executedRule?.rule?.name || RuleName.ColdEmail,\n+        );\n+\n+        // Only include if it's one of our known categories\n+        const categoryResult = digestCategorySchema.safeParse(ruleName);\n+        if (categoryResult.success) {\n+          const category = categoryResult.data;\n+          if (!acc[category]) {\n+            acc[category] = [];\n+          }\n+\n+          let parsedContent: unknown;\n+          try {\n+            parsedContent = JSON.parse(item.content);\n+          } catch (error) {\n+            logger.warn(\"Failed to parse digest item content, skipping item\", {\n+              messageId: item.messageId,\n+              digestId: digest.id,\n+              error: error instanceof Error ? error.message : \"Unknown error\",\n+            });\n+            return; // Skip this item and continue with the next one\n+          }\n+\n+          const contentResult =\n+            DigestEmailSummarySchema.safeParse(parsedContent);\n+\n+          if (contentResult.success) {\n+            acc[category].push({\n+              content: {\n+                entries: contentResult.data?.entries || [],\n+                summary: contentResult.data?.summary,\n+              },\n+              from: extractNameFromEmail(message?.headers?.from || \"\"),\n+              subject: message?.headers?.subject || \"\",\n+            });\n+          }\n+        }\n+      });\n+      return acc;\n+    }, {} as Digest);\n+\n+    const token = await createUnsubscribeToken({ emailAccountId });\n+\n+    // First, send the digest email and wait for it to complete\n+    await sendDigestEmail({\n+      from: env.RESEND_FROM_EMAIL,\n+      to: emailAccount.email,\n+      emailProps: {\n+        ...executedRulesByRule,\n+        baseUrl: env.NEXT_PUBLIC_BASE_URL,\n+        unsubscribeToken: token,\n+        date: new Date(),\n+      },\n+    });\n+\n+    // Only update database if email sending succeeded\n+    await Promise.all([\n+      ...(emailAccount.digestScheduleId\n+        ? [\n+            prisma.schedule.update({\n+              where: {\n+                id: emailAccount.digestScheduleId,\n+                emailAccountId,\n+              },\n+              data: {\n+                lastOccurrenceAt: new Date(),\n+                nextOccurrenceAt: calculateNextScheduleDate(\n+                  emailAccount.digestSchedule!,\n+                ),\n+              },\n+            }),\n+          ]\n+        : []),\n+      // Mark only the processed digests as sent\n+      prisma.digest.updateMany({\n+        where: {\n+          id: {\n+            in: processedDigestIds,\n+          },\n+        },\n+        data: {\n+          status: DigestStatus.SENT,\n+          sentAt: new Date(),\n+        },\n+      }),\n+      // Redact all DigestItems for the processed digests\n+      prisma.digestItem.updateMany({\n+        data: { content: \"[REDACTED]\" },\n+        where: {\n+          digestId: {\n+            in: processedDigestIds,\n+          },\n+        },\n+      }),\n+    ]);",
        "comment_created_at": "2025-06-17T00:44:27+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "_\ud83d\udee0\ufe0f Refactor suggestion_\n\n**Database updates run in parallel without a transaction \u2013 risk of partial success**\n\n`Promise.all` fires three independent mutations.  \nIf any of them fails (e.g. schedule row locked) the others may already have committed, leaving the system in an inconsistent state (digests marked `SENT` while schedule wasn\u2019t advanced, or vice-versa).\n\nConsider:\n\n```diff\n-    await Promise.all([\n-      prisma.schedule.update({ \u2026 }),\n-      prisma.digest.updateMany({ \u2026 }),\n-      prisma.digestItem.updateMany({ \u2026 }),\n-    ]);\n+    await prisma.$transaction([\n+      prisma.schedule.update({ \u2026 }),\n+      prisma.digest.updateMany({ \u2026 }),\n+      prisma.digestItem.updateMany({ \u2026 }),\n+    ]);\n```\n\nThis keeps the write-set atomic while still avoiding interactive transactions in user code.  \n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>\ud83e\udd16 Prompt for AI Agents</summary>\n\n```\nIn apps/web/app/api/resend/digest/route.ts between lines 197 and 235, multiple\ndatabase updates are executed in parallel using Promise.all without a\ntransaction, risking partial commits and inconsistent state if one update fails.\nRefactor the code to run these updates inside a single Prisma transaction to\nensure atomicity, so either all updates succeed together or none are applied,\nmaintaining data consistency.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1969249398",
    "pr_number": 360,
    "pr_file": "apps/web/app/api/auth/account-linking-handler/route.ts",
    "created_at": "2025-02-25T08:28:50+00:00",
    "commented_code": "+import { NextResponse } from \"next/server\";\n+import { withError } from \"@/utils/middleware\";\n+import prisma from \"@/utils/prisma\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+\n+const logger = createScopedLogger(\"account-linking-handler\");\n+\n+/**\n+ * This route is called after a successful OAuth authentication in the linking flow\n+ * It links the newly created account to the original user and cleans up\n+ */\n+export const POST = withError(async (request: Request) => {\n+  try {\n+    // Extract the link account headers\n+    const linkAccount = request.headers.get(\"x-link-account\");\n+    const originalUserId = request.headers.get(\"x-original-user-id\");\n+\n+    if (linkAccount !== \"true\" || !originalUserId) {\n+      return NextResponse.json(\n+        { error: \"Invalid account linking request\" },\n+        { status: 400 },\n+      );\n+    }\n+\n+    // Get the payload that includes the OAuth account details\n+    const payload = await request.json();\n+    const { accountId, email, provider } = payload;\n+\n+    if (!accountId || !email || !provider) {\n+      return NextResponse.json(\n+        { error: \"Missing required account information\" },\n+        { status: 400 },\n+      );\n+    }\n+\n+    // Find the newly created user account (created by NextAuth)\n+    const newAccount = await prisma.account.findUnique({\n+      where: {\n+        id: accountId,\n+      },\n+      include: {\n+        user: true,\n+      },\n+    });\n+\n+    if (!newAccount) {\n+      logger.error(\"Account not found\", { accountId });\n+      return NextResponse.json({ error: \"Account not found\" }, { status: 404 });\n+    }\n+\n+    // Check if the new account is associated with a new user\n+    if (newAccount.userId === originalUserId) {\n+      logger.info(\"Account already linked to the original user\", {\n+        accountId,\n+        originalUserId,\n+      });\n+      return NextResponse.json({ success: true });\n+    }\n+\n+    // Get the temporary user that was created during OAuth\n+    const tempUser = newAccount.user;\n+\n+    // Update the account to be associated with the original user\n+    await prisma.account.update({\n+      where: {\n+        id: accountId,\n+      },\n+      data: {\n+        userId: originalUserId,\n+      },\n+    });\n+\n+    // Delete the temporary user (if it's not the original user)\n+    // But first check if there are any other accounts linked to it\n+    const otherAccounts = await prisma.account.findMany({\n+      where: {\n+        userId: tempUser.id,\n+      },\n+    });\n+\n+    if (otherAccounts.length === 0) {\n+      await prisma.user.delete({\n+        where: {\n+          id: tempUser.id,\n+        },\n+      });\n+\n+      logger.info(\"Deleted temporary user\", { userId: tempUser.id });\n+    }\n+",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "1969249398",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 360,
        "pr_file": "apps/web/app/api/auth/account-linking-handler/route.ts",
        "discussion_id": "1969249398",
        "commented_code": "@@ -0,0 +1,110 @@\n+import { NextResponse } from \"next/server\";\n+import { withError } from \"@/utils/middleware\";\n+import prisma from \"@/utils/prisma\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+\n+const logger = createScopedLogger(\"account-linking-handler\");\n+\n+/**\n+ * This route is called after a successful OAuth authentication in the linking flow\n+ * It links the newly created account to the original user and cleans up\n+ */\n+export const POST = withError(async (request: Request) => {\n+  try {\n+    // Extract the link account headers\n+    const linkAccount = request.headers.get(\"x-link-account\");\n+    const originalUserId = request.headers.get(\"x-original-user-id\");\n+\n+    if (linkAccount !== \"true\" || !originalUserId) {\n+      return NextResponse.json(\n+        { error: \"Invalid account linking request\" },\n+        { status: 400 },\n+      );\n+    }\n+\n+    // Get the payload that includes the OAuth account details\n+    const payload = await request.json();\n+    const { accountId, email, provider } = payload;\n+\n+    if (!accountId || !email || !provider) {\n+      return NextResponse.json(\n+        { error: \"Missing required account information\" },\n+        { status: 400 },\n+      );\n+    }\n+\n+    // Find the newly created user account (created by NextAuth)\n+    const newAccount = await prisma.account.findUnique({\n+      where: {\n+        id: accountId,\n+      },\n+      include: {\n+        user: true,\n+      },\n+    });\n+\n+    if (!newAccount) {\n+      logger.error(\"Account not found\", { accountId });\n+      return NextResponse.json({ error: \"Account not found\" }, { status: 404 });\n+    }\n+\n+    // Check if the new account is associated with a new user\n+    if (newAccount.userId === originalUserId) {\n+      logger.info(\"Account already linked to the original user\", {\n+        accountId,\n+        originalUserId,\n+      });\n+      return NextResponse.json({ success: true });\n+    }\n+\n+    // Get the temporary user that was created during OAuth\n+    const tempUser = newAccount.user;\n+\n+    // Update the account to be associated with the original user\n+    await prisma.account.update({\n+      where: {\n+        id: accountId,\n+      },\n+      data: {\n+        userId: originalUserId,\n+      },\n+    });\n+\n+    // Delete the temporary user (if it's not the original user)\n+    // But first check if there are any other accounts linked to it\n+    const otherAccounts = await prisma.account.findMany({\n+      where: {\n+        userId: tempUser.id,\n+      },\n+    });\n+\n+    if (otherAccounts.length === 0) {\n+      await prisma.user.delete({\n+        where: {\n+          id: tempUser.id,\n+        },\n+      });\n+\n+      logger.info(\"Deleted temporary user\", { userId: tempUser.id });\n+    }\n+",
        "comment_created_at": "2025-02-25T08:28:50+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Wrap account update and user deletion in a transaction.**\n\nThe update of the `newAccount` user and potential deletion of `tempUser` could create inconsistencies if one operation fails. Consider wrapping these operations within a database transaction to maintain atomicity.\n\n\n```diff\n// Example with Prisma transaction\n    await prisma.$transaction(async (prismaTx) => {\n      await prismaTx.account.update({\n        where: { id: accountId },\n        data: { userId: originalUserId },\n      });\n\n      const otherAccounts = await prismaTx.account.findMany({\n        where: { userId: tempUser.id },\n      });\n\n      if (otherAccounts.length === 0) {\n        await prismaTx.user.delete({\n          where: { id: tempUser.id },\n        });\n      }\n    });\n```\n\n\n<!-- suggestion_start -->\n\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\n    await prisma.$transaction(async (prismaTx) => {\n      await prismaTx.account.update({\n        where: { id: accountId },\n        data: { userId: originalUserId },\n      });\n\n      const otherAccounts = await prismaTx.account.findMany({\n        where: { userId: tempUser.id },\n      });\n\n      if (otherAccounts.length === 0) {\n        await prismaTx.user.delete({\n          where: { id: tempUser.id },\n        });\n      }\n    });\n`````\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2055861389",
    "pr_number": 412,
    "pr_file": "apps/web/app/api/google/labels/create/controller.ts",
    "created_at": "2025-04-23T11:39:50+00:00",
    "commented_code": "export type CreateLabelBody = z.infer<typeof createLabelBody>;\n export type CreateLabelResponse = Awaited<ReturnType<typeof createLabel>>;\n \n-export async function createLabel(body: CreateLabelBody) {\n-  const session = await auth();\n-  if (!session?.user.email) throw new SafeError(\"Not authenticated\");\n-  const gmail = getGmailClient(session);\n-  const label = await getOrCreateLabel({ gmail, name: body.name });\n+export async function createLabel({\n+  gmail,\n+  email,\n+  name,\n+  description,\n+}: {\n+  gmail: gmail_v1.Gmail;\n+  email: string;\n+  name: string;\n+  description?: string;\n+}) {\n+  const label = await getOrCreateLabel({ gmail, name });\n \n   if (!label.id) throw new SafeError(\"Failed to create label\");",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "2055861389",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 412,
        "pr_file": "apps/web/app/api/google/labels/create/controller.ts",
        "discussion_id": "2055861389",
        "commented_code": "@@ -13,34 +12,41 @@ export const createLabelBody = z.object({\n export type CreateLabelBody = z.infer<typeof createLabelBody>;\n export type CreateLabelResponse = Awaited<ReturnType<typeof createLabel>>;\n \n-export async function createLabel(body: CreateLabelBody) {\n-  const session = await auth();\n-  if (!session?.user.email) throw new SafeError(\"Not authenticated\");\n-  const gmail = getGmailClient(session);\n-  const label = await getOrCreateLabel({ gmail, name: body.name });\n+export async function createLabel({\n+  gmail,\n+  email,\n+  name,\n+  description,\n+}: {\n+  gmail: gmail_v1.Gmail;\n+  email: string;\n+  name: string;\n+  description?: string;\n+}) {\n+  const label = await getOrCreateLabel({ gmail, name });\n \n   if (!label.id) throw new SafeError(\"Failed to create label\");",
        "comment_created_at": "2025-04-23T11:39:50+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "_\u26a0\ufe0f Potential issue_\n\n**Avoid partial writes \u2013 wrap DB\u00a0\u2194\u00a0Redis operations in a single transactional flow**\n\n`Promise.all([dbPromise, redisPromise])` fires both writes concurrently.  \nIf `prisma.label.upsert` fails (e.g., unique\u2011constraint, network hiccup) but Redis succeeds, the cache will advertise a label that doesn\u2019t exist in the DB \u2013 a hard\u2011to\u2011debug inconsistency.\n\nEither:\n\n```diff\n-const dbPromise = prisma.label.upsert({ \u2026 })\n-const redisPromise = saveUserLabel({ \u2026 })\n-await Promise.all([dbPromise, redisPromise])\n+const dbResult = await prisma.label.upsert({ \u2026 }); // throws if it fails\n+await saveUserLabel({ \u2026 });                         // only executed when DB succeeded\n```\n\nor wrap both in a real transaction / multi\u2011exec pipeline (if available).\n\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2032832655",
    "pr_number": 395,
    "pr_file": "apps/web/app/api/ai/analyze-sender-pattern/route.ts",
    "created_at": "2025-04-08T09:54:28+00:00",
    "commented_code": "+import { NextResponse, after } from \"next/server\";\n+import { headers } from \"next/headers\";\n+import type { gmail_v1 } from \"@googleapis/gmail\";\n+import { z } from \"zod\";\n+import { getGmailClient } from \"@/utils/gmail/client\";\n+import { withError } from \"@/utils/middleware\";\n+import prisma from \"@/utils/prisma\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+import { aiDetectRecurringPattern } from \"@/utils/ai/choose-rule/ai-detect-recurring-pattern\";\n+import { isValidInternalApiKey } from \"@/utils/internal-api\";\n+import { GroupItemType } from \"@prisma/client\";\n+import { getThreadMessages, getThreads } from \"@/utils/gmail/thread\";\n+import { extractEmailAddress } from \"@/utils/email\";\n+import { getEmailForLLM } from \"@/utils/get-email-from-message\";\n+\n+export const maxDuration = 60;\n+\n+const THRESHOLD_EMAILS = 3;\n+const MAX_RESULTS = 10;\n+\n+const logger = createScopedLogger(\"api/ai/pattern-match\");\n+\n+const schema = z.object({\n+  userId: z.string(),\n+  from: z.string().email(\"Invalid sender email\"),\n+});\n+export type AnalyzeSenderPatternBody = z.infer<typeof schema>;\n+\n+/**\n+ * Main background process function that:\n+ * 1. Checks if sender has been analyzed before\n+ * 2. Gets threads from the sender\n+ * 3. Analyzes whether threads are one-way communications\n+ * 4. Detects patterns using AI\n+ * 5. Stores patterns in DB for future categorization\n+ */\n+async function process(request: Request) {\n+  if (!isValidInternalApiKey(await headers())) {\n+    logger.error(\"Invalid API key\");\n+    return NextResponse.json({ error: \"Invalid API key\" });\n+  }\n+\n+  const json = await request.json();\n+  const data = schema.parse(json);\n+  const { userId } = data;\n+  const from = extractEmailAddress(data.from);\n+\n+  try {\n+    // Check if we've already analyzed this sender\n+    const existingCheck = await prisma.newsletter.findUnique({\n+      where: { email_userId: { email: extractEmailAddress(from), userId } },\n+    });\n+\n+    if (existingCheck?.patternAnalyzed) {\n+      logger.info(\"Sender has already been analyzed\", { from, userId });\n+      return NextResponse.json({ success: true });\n+    }\n+\n+    const user = await getUserWithRules(userId);\n+\n+    if (!user) {\n+      logger.error(\"User not found\", { userId });\n+      return NextResponse.json({ success: false }, { status: 404 });\n+    }\n+\n+    const account = user.accounts[0];\n+\n+    if (!account.access_token || !account.refresh_token) {\n+      logger.error(\"No Gmail account found\", { userId });\n+      return NextResponse.json({ success: false }, { status: 404 });\n+    }\n+\n+    const gmail = getGmailClient({\n+      accessToken: account.access_token,\n+      refreshToken: account.refresh_token,\n+    });\n+\n+    // Get threads from this sender\n+    const threadsWithMessages = await getThreadsFromSender(\n+      gmail,\n+      from,\n+      MAX_RESULTS,\n+    );\n+\n+    // If no threads found or we've detected a conversation, return early\n+    if (threadsWithMessages.length === 0) {\n+      logger.info(\"No threads found from this sender\", {\n+        from,\n+        userId,\n+      });\n+\n+      // Don't record a check since we didn't run the AI analysis\n+      return NextResponse.json({ success: true });\n+    }\n+\n+    // Get all messages and check if we have enough for pattern detection\n+    const allMessages = threadsWithMessages.flatMap(\n+      (thread) => thread.messages,\n+    );\n+\n+    if (allMessages.length < THRESHOLD_EMAILS) {\n+      logger.info(\"Not enough emails found from this sender\", {\n+        from,\n+        userId,\n+        count: allMessages.length,\n+      });\n+\n+      // Don't record a check since we didn't run the AI analysis\n+      return NextResponse.json({ success: true });\n+    }\n+\n+    // Convert messages to EmailForLLM format\n+    const emails = allMessages.map((message) => getEmailForLLM(message));\n+\n+    // Detect pattern using AI\n+    const patternResult = await aiDetectRecurringPattern({\n+      emails,\n+      user: {\n+        id: user.id,\n+        email: user.email || \"\",\n+        about: user.about,\n+        aiProvider: user.aiProvider,\n+        aiModel: user.aiModel,\n+        aiApiKey: user.aiApiKey,\n+      },\n+      rules: user.rules.map((rule) => ({\n+        name: rule.name,\n+        instructions: rule.instructions || \"\",\n+      })),\n+    });\n+\n+    if (patternResult?.matchedRule) {\n+      // Save pattern to DB (adds sender to rule's group)\n+      await saveToDb({\n+        userId,\n+        from,\n+        ruleName: patternResult.matchedRule,\n+      });\n+    }\n+\n+    // Record the pattern analysis result\n+    await savePatternCheck(userId, from);",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "2032832655",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 395,
        "pr_file": "apps/web/app/api/ai/analyze-sender-pattern/route.ts",
        "discussion_id": "2032832655",
        "commented_code": "@@ -0,0 +1,309 @@\n+import { NextResponse, after } from \"next/server\";\n+import { headers } from \"next/headers\";\n+import type { gmail_v1 } from \"@googleapis/gmail\";\n+import { z } from \"zod\";\n+import { getGmailClient } from \"@/utils/gmail/client\";\n+import { withError } from \"@/utils/middleware\";\n+import prisma from \"@/utils/prisma\";\n+import { createScopedLogger } from \"@/utils/logger\";\n+import { aiDetectRecurringPattern } from \"@/utils/ai/choose-rule/ai-detect-recurring-pattern\";\n+import { isValidInternalApiKey } from \"@/utils/internal-api\";\n+import { GroupItemType } from \"@prisma/client\";\n+import { getThreadMessages, getThreads } from \"@/utils/gmail/thread\";\n+import { extractEmailAddress } from \"@/utils/email\";\n+import { getEmailForLLM } from \"@/utils/get-email-from-message\";\n+\n+export const maxDuration = 60;\n+\n+const THRESHOLD_EMAILS = 3;\n+const MAX_RESULTS = 10;\n+\n+const logger = createScopedLogger(\"api/ai/pattern-match\");\n+\n+const schema = z.object({\n+  userId: z.string(),\n+  from: z.string().email(\"Invalid sender email\"),\n+});\n+export type AnalyzeSenderPatternBody = z.infer<typeof schema>;\n+\n+/**\n+ * Main background process function that:\n+ * 1. Checks if sender has been analyzed before\n+ * 2. Gets threads from the sender\n+ * 3. Analyzes whether threads are one-way communications\n+ * 4. Detects patterns using AI\n+ * 5. Stores patterns in DB for future categorization\n+ */\n+async function process(request: Request) {\n+  if (!isValidInternalApiKey(await headers())) {\n+    logger.error(\"Invalid API key\");\n+    return NextResponse.json({ error: \"Invalid API key\" });\n+  }\n+\n+  const json = await request.json();\n+  const data = schema.parse(json);\n+  const { userId } = data;\n+  const from = extractEmailAddress(data.from);\n+\n+  try {\n+    // Check if we've already analyzed this sender\n+    const existingCheck = await prisma.newsletter.findUnique({\n+      where: { email_userId: { email: extractEmailAddress(from), userId } },\n+    });\n+\n+    if (existingCheck?.patternAnalyzed) {\n+      logger.info(\"Sender has already been analyzed\", { from, userId });\n+      return NextResponse.json({ success: true });\n+    }\n+\n+    const user = await getUserWithRules(userId);\n+\n+    if (!user) {\n+      logger.error(\"User not found\", { userId });\n+      return NextResponse.json({ success: false }, { status: 404 });\n+    }\n+\n+    const account = user.accounts[0];\n+\n+    if (!account.access_token || !account.refresh_token) {\n+      logger.error(\"No Gmail account found\", { userId });\n+      return NextResponse.json({ success: false }, { status: 404 });\n+    }\n+\n+    const gmail = getGmailClient({\n+      accessToken: account.access_token,\n+      refreshToken: account.refresh_token,\n+    });\n+\n+    // Get threads from this sender\n+    const threadsWithMessages = await getThreadsFromSender(\n+      gmail,\n+      from,\n+      MAX_RESULTS,\n+    );\n+\n+    // If no threads found or we've detected a conversation, return early\n+    if (threadsWithMessages.length === 0) {\n+      logger.info(\"No threads found from this sender\", {\n+        from,\n+        userId,\n+      });\n+\n+      // Don't record a check since we didn't run the AI analysis\n+      return NextResponse.json({ success: true });\n+    }\n+\n+    // Get all messages and check if we have enough for pattern detection\n+    const allMessages = threadsWithMessages.flatMap(\n+      (thread) => thread.messages,\n+    );\n+\n+    if (allMessages.length < THRESHOLD_EMAILS) {\n+      logger.info(\"Not enough emails found from this sender\", {\n+        from,\n+        userId,\n+        count: allMessages.length,\n+      });\n+\n+      // Don't record a check since we didn't run the AI analysis\n+      return NextResponse.json({ success: true });\n+    }\n+\n+    // Convert messages to EmailForLLM format\n+    const emails = allMessages.map((message) => getEmailForLLM(message));\n+\n+    // Detect pattern using AI\n+    const patternResult = await aiDetectRecurringPattern({\n+      emails,\n+      user: {\n+        id: user.id,\n+        email: user.email || \"\",\n+        about: user.about,\n+        aiProvider: user.aiProvider,\n+        aiModel: user.aiModel,\n+        aiApiKey: user.aiApiKey,\n+      },\n+      rules: user.rules.map((rule) => ({\n+        name: rule.name,\n+        instructions: rule.instructions || \"\",\n+      })),\n+    });\n+\n+    if (patternResult?.matchedRule) {\n+      // Save pattern to DB (adds sender to rule's group)\n+      await saveToDb({\n+        userId,\n+        from,\n+        ruleName: patternResult.matchedRule,\n+      });\n+    }\n+\n+    // Record the pattern analysis result\n+    await savePatternCheck(userId, from);",
        "comment_created_at": "2025-04-08T09:54:28+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "_\ud83d\udee0\ufe0f Refactor suggestion_\n\n**Wrap database operations in a transaction.**\n\nThe code performs multiple database operations (`saveToDb` and `savePatternCheck`) without using a transaction. If one operation succeeds but the other fails, it could leave the database in an inconsistent state.\n\n\nWrap the related database operations in a transaction:\n\n```diff\n  if (patternResult?.matchedRule) {\n+   await prisma.$transaction(async (tx) => {\n      // Save pattern to DB (adds sender to rule's group)\n-     await saveToDb({\n+     await saveToDb({\n+       prismaClient: tx,\n        userId,\n        from,\n        ruleName: patternResult.matchedRule,\n      });\n      \n+     // Record the pattern analysis result\n+     await savePatternCheck(userId, from, tx);\n+   });\n- }\n-\n- // Record the pattern analysis result\n- await savePatternCheck(userId, from);\n+ }\n+ else {\n+   // Record the pattern analysis result without a pattern match\n+   await savePatternCheck(userId, from);\n+ }\n```\n\nThis will require updating the function signatures of `saveToDb` and `savePatternCheck` to accept an optional transaction client.\n\n\n<!-- suggestion_start -->\n\n<details>\n<summary>\ud83d\udcdd Committable suggestion</summary>\n\n> \u203c\ufe0f **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n`````suggestion\nif (patternResult?.matchedRule) {\n  await prisma.$transaction(async (tx) => {\n    // Save pattern to DB (adds sender to rule's group)\n    await saveToDb({\n      prismaClient: tx,\n      userId,\n      from,\n      ruleName: patternResult.matchedRule,\n    });\n    \n    // Record the pattern analysis result\n    await savePatternCheck(userId, from, tx);\n  });\n} else {\n  // Record the pattern analysis result without a pattern match\n  await savePatternCheck(userId, from);\n}\n`````\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1940283444",
    "pr_number": 320,
    "pr_file": "apps/web/utils/reply-tracker/inbound.ts",
    "created_at": "2025-02-03T23:52:14+00:00",
    "commented_code": "+import prisma from \"@/utils/prisma\";\n+import { ThreadTrackerType } from \"@prisma/client\";\n+import type { gmail_v1 } from \"@googleapis/gmail\";\n+import {\n+  labelNeedsReply,\n+  removeAwaitingReplyLabel,\n+  getReplyTrackingLabels,\n+} from \"@/utils/reply-tracker/label\";\n+\n+export async function markNeedsReply(\n+  userId: string,\n+  threadId: string,\n+  messageId: string,\n+  sentAt: Date,\n+  gmail: gmail_v1.Gmail,\n+) {\n+  const { awaitingReplyLabelId, needsReplyLabelId } =\n+    await getReplyTrackingLabels(gmail);\n+\n+  // Resolve existing AWAITING trackers\n+  const updateDbPromise = prisma.threadTracker.updateMany({\n+    where: {\n+      userId,\n+      threadId,\n+      type: ThreadTrackerType.AWAITING,\n+    },\n+    data: {\n+      resolved: true,\n+    },\n+  });",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "1940283444",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 320,
        "pr_file": "apps/web/utils/reply-tracker/inbound.ts",
        "discussion_id": "1940283444",
        "commented_code": "@@ -0,0 +1,64 @@\n+import prisma from \"@/utils/prisma\";\n+import { ThreadTrackerType } from \"@prisma/client\";\n+import type { gmail_v1 } from \"@googleapis/gmail\";\n+import {\n+  labelNeedsReply,\n+  removeAwaitingReplyLabel,\n+  getReplyTrackingLabels,\n+} from \"@/utils/reply-tracker/label\";\n+\n+export async function markNeedsReply(\n+  userId: string,\n+  threadId: string,\n+  messageId: string,\n+  sentAt: Date,\n+  gmail: gmail_v1.Gmail,\n+) {\n+  const { awaitingReplyLabelId, needsReplyLabelId } =\n+    await getReplyTrackingLabels(gmail);\n+\n+  // Resolve existing AWAITING trackers\n+  const updateDbPromise = prisma.threadTracker.updateMany({\n+    where: {\n+      userId,\n+      threadId,\n+      type: ThreadTrackerType.AWAITING,\n+    },\n+    data: {\n+      resolved: true,\n+    },\n+  });",
        "comment_created_at": "2025-02-03T23:52:14+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Consider using a transaction for database operations.**\n\nThe database operations are related and should be atomic. Consider wrapping them in a transaction to ensure data consistency.\n\nApply this diff to use a transaction:\n\n```diff\n-  const updateDbPromise = prisma.threadTracker.updateMany({\n-    where: {\n-      userId,\n-      threadId,\n-      type: ThreadTrackerType.AWAITING,\n-    },\n-    data: {\n-      resolved: true,\n-    },\n-  });\n-\n-  const upsertDbPromise = prisma.threadTracker.upsert({\n+  const dbPromise = prisma.$transaction([\n+    prisma.threadTracker.updateMany({\n+      where: {\n+        userId,\n+        threadId,\n+        type: ThreadTrackerType.AWAITING,\n+      },\n+      data: {\n+        resolved: true,\n+      },\n+    }),\n+    prisma.threadTracker.upsert({\n```\n\n\nAlso applies to: 33-49\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1923312493",
    "pr_number": 313,
    "pr_file": "apps/web/utils/actions/rule.ts",
    "created_at": "2025-01-21T08:56:27+00:00",
    "commented_code": "},\n );\n \n+export async function updateRuleInstructionsAndPromptFile({\n+  userId,\n+  ruleId,\n+  instructions,\n+  currentRule,\n+}: {\n+  userId: string;\n+  ruleId: string;\n+  instructions: string;\n+  currentRule: RuleWithRelations | null;\n+}) {\n+  const updatedRule = await prisma.rule.update({\n+    where: { id: ruleId, userId },\n+    data: { instructions },\n+    include: { actions: true, categoryFilters: true, group: true },\n+  });\n+\n+  // update prompt file\n+  if (currentRule) {\n+    await updatePromptFileOnUpdate(userId, currentRule, updatedRule);\n+  } else {\n+    await updateUserPrompt(userId, instructions);\n+  }\n+}",
    "repo_full_name": "elie222/inbox-zero",
    "discussion_comments": [
      {
        "comment_id": "1923312493",
        "repo_full_name": "elie222/inbox-zero",
        "pr_number": 313,
        "pr_file": "apps/web/utils/actions/rule.ts",
        "discussion_id": "1923312493",
        "commented_code": "@@ -342,6 +340,31 @@ export const getRuleExamplesAction = withActionInstrumentation(\n   },\n );\n \n+export async function updateRuleInstructionsAndPromptFile({\n+  userId,\n+  ruleId,\n+  instructions,\n+  currentRule,\n+}: {\n+  userId: string;\n+  ruleId: string;\n+  instructions: string;\n+  currentRule: RuleWithRelations | null;\n+}) {\n+  const updatedRule = await prisma.rule.update({\n+    where: { id: ruleId, userId },\n+    data: { instructions },\n+    include: { actions: true, categoryFilters: true, group: true },\n+  });\n+\n+  // update prompt file\n+  if (currentRule) {\n+    await updatePromptFileOnUpdate(userId, currentRule, updatedRule);\n+  } else {\n+    await updateUserPrompt(userId, instructions);\n+  }\n+}",
        "comment_created_at": "2025-01-21T08:56:27+00:00",
        "comment_author": "coderabbitai[bot]",
        "comment_body": "_:hammer_and_wrench: Refactor suggestion_\n\n**Wrap database operations in a transaction.**\n\nThe function performs multiple database operations that should be atomic. If the prompt file update fails, the rule update should be rolled back.\n\n```diff\n export async function updateRuleInstructionsAndPromptFile({\n   userId,\n   ruleId,\n   instructions,\n   currentRule,\n }: {\n   userId: string;\n   ruleId: string;\n   instructions: string;\n   currentRule: RuleWithRelations | null;\n }) {\n+  try {\n+    return await prisma.$transaction(async (tx) => {\n       const updatedRule = await prisma.rule.update({\n         where: { id: ruleId, userId },\n         data: { instructions },\n         include: { actions: true, categoryFilters: true, group: true },\n       });\n\n       // update prompt file\n       if (currentRule) {\n         await updatePromptFileOnUpdate(userId, currentRule, updatedRule);\n       } else {\n         await updateUserPrompt(userId, instructions);\n       }\n+\n+      return updatedRule;\n+    });\n+  } catch (error) {\n+    console.error(\"Failed to update rule instructions:\", error);\n+    throw new SafeError(\"Failed to update rule instructions\");\n+  }\n }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
        "pr_file_module": null
      }
    ]
  }
]