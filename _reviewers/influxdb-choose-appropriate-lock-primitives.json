[
  {
    "discussion_id": "1772165602",
    "pr_number": 25377,
    "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
    "created_at": "2024-09-23T21:34:37+00:00",
    "commented_code": "+//! An in-memory cache of Parquet files that are persisted to object storage\n+use std::{\n+    fmt::Debug, hash::RandomState, num::NonZeroUsize, ops::Range, sync::Arc, time::Duration,\n+};\n+\n+use async_trait::async_trait;\n+use bytes::Bytes;\n+use clru::{CLruCache, CLruCacheConfig, WeightScale};\n+use futures::{StreamExt, TryStreamExt};\n+use futures_util::stream::BoxStream;\n+use object_store::{\n+    path::Path, Error, GetOptions, GetResult, GetResultPayload, ListResult, MultipartUpload,\n+    ObjectMeta, ObjectStore, PutMultipartOpts, PutOptions, PutPayload, PutResult,\n+    Result as ObjectStoreResult,\n+};\n+use observability_deps::tracing::{error, info};\n+use tokio::sync::{\n+    mpsc::{channel, Receiver, Sender},\n+    oneshot, Mutex,\n+};\n+\n+/// A request to fetch an item at the given `path` from an object store\n+///\n+/// Contains a notifier to notify the caller that registers the cache request when the item\n+/// has been cached successfully (or if the cache request failed in some way)\n+pub struct CacheRequest {\n+    path: Path,\n+    notifier: oneshot::Sender<()>,\n+}\n+\n+impl CacheRequest {\n+    /// Create a new [`CacheRequest`] along with a receiver to catch the notify message when\n+    /// the cache request has been fulfilled.\n+    pub fn create(path: Path) -> (Self, oneshot::Receiver<()>) {\n+        let (notifier, receiver) = oneshot::channel();\n+        (Self { path, notifier }, receiver)\n+    }\n+}\n+\n+/// An interface for interacting with a Parquet Cache by registering [`CacheRequest`]s to it.\n+pub trait ParquetCacheOracle: Send + Sync + Debug {\n+    /// Register a cache request with the oracle\n+    fn register(&self, cache_request: CacheRequest);\n+}\n+\n+/// Concrete implementation of the [`ParquetCacheOracle`]\n+///\n+/// This implementation sends all requests registered to be cached.\n+#[derive(Debug, Clone)]\n+pub struct MemCacheOracle {\n+    cache_request_tx: Sender<CacheRequest>,\n+}\n+\n+// TODO(trevor): make this configurable with reasonable default\n+const CACHE_REQUEST_BUFFER_SIZE: usize = 1_000_000;\n+\n+impl MemCacheOracle {\n+    /// Create a new [`MemCacheOracle`]\n+    ///\n+    /// This spawns two background tasks:\n+    /// * one to handle registered [`CacheRequest`]s\n+    /// * one to prune deleted and un-needed cache entries on an interval\n+    // TODO(trevor): this should be more configurable, e.g., channel size, prune interval\n+    fn new(mem_cached_store: Arc<MemCachedObjectStore>) -> Self {\n+        let (cache_request_tx, cache_request_rx) = channel(CACHE_REQUEST_BUFFER_SIZE);\n+        background_cache_request_handler(Arc::clone(&mem_cached_store), cache_request_rx);\n+        background_cache_pruner(mem_cached_store);\n+        Self { cache_request_tx }\n+    }\n+}\n+\n+impl ParquetCacheOracle for MemCacheOracle {\n+    fn register(&self, request: CacheRequest) {\n+        let tx = self.cache_request_tx.clone();\n+        tokio::spawn(async move {\n+            if let Err(error) = tx.send(request).await {\n+                error!(%error, \"error registering cache request\");\n+            };\n+        });\n+    }\n+}\n+\n+/// Helper function for creation of a [`MemCachedObjectStore`] and [`MemCacheOracle`]\n+/// that returns them as their `Arc<dyn _>` equivalent.\n+pub fn create_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+    cache_capacity: usize,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    let store = Arc::new(MemCachedObjectStore::new(object_store, cache_capacity));\n+    let oracle = Arc::new(MemCacheOracle::new(Arc::clone(&store)));\n+    (store, oracle)\n+}\n+\n+/// Create a test cached object store with a cache capacity of 1GB\n+pub fn test_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    create_cached_obj_store_and_oracle(object_store, 1024 * 1024 * 1024)\n+}\n+\n+/// An entry in the cache, containing the actual bytes as well as object store metadata\n+#[derive(Debug)]\n+struct CacheValue {\n+    data: Bytes,\n+    meta: ObjectMeta,\n+}\n+\n+impl CacheValue {\n+    /// Get the size of the cache value's memory footprint in bytes\n+    fn size(&self) -> usize {\n+        // TODO(trevor): could also calculate the size of the metadata...\n+        self.data.len()\n+    }\n+}\n+\n+/// The state of a cache entry\n+#[derive(Debug)]\n+enum CacheEntry {\n+    /// The cache entry is being fetched from object store\n+    Fetching,\n+    /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n+    Success(Arc<CacheValue>),\n+    /// The request to the object store failed\n+    Failed,\n+    /// The cache entry was deleted\n+    Deleted,\n+    /// The object is too large for the cache\n+    TooLarge,\n+}\n+\n+impl CacheEntry {\n+    /// Get the size of thje cache entry in bytes\n+    fn size(&self) -> usize {\n+        match self {\n+            CacheEntry::Fetching => 0,\n+            CacheEntry::Success(v) => v.size(),\n+            CacheEntry::Failed => 0,\n+            CacheEntry::Deleted => 0,\n+            CacheEntry::TooLarge => 0,\n+        }\n+    }\n+\n+    fn is_fetching(&self) -> bool {\n+        matches!(self, CacheEntry::Fetching)\n+    }\n+\n+    fn is_success(&self) -> bool {\n+        matches!(self, CacheEntry::Success(_))\n+    }\n+\n+    fn keep(&self) -> bool {\n+        self.is_fetching() || self.is_success()\n+    }\n+}\n+\n+/// Implements the [`WeightScale`] trait to determine a [`CacheEntry`]'s size on insertion to\n+/// the cache\n+#[derive(Debug)]\n+struct CacheEntryScale;\n+\n+impl WeightScale<Path, CacheEntry> for CacheEntryScale {\n+    fn weight(&self, key: &Path, value: &CacheEntry) -> usize {\n+        key.as_ref().len() + value.size()\n+    }\n+}\n+\n+/// Placeholder name for formatting datafusion errors\n+const STORE_NAME: &str = \"mem_cached_object_store\";\n+\n+/// An object store with an associated cache that can serve GET-style requests using the cache\n+///\n+/// The least-recently used (LRU) entries will be evicted when new entries are inserted, if the\n+/// new entry would exceed the cache's memory capacity\n+#[derive(Debug)]\n+pub struct MemCachedObjectStore {\n+    /// An inner object store for which items will be cached\n+    inner: Arc<dyn ObjectStore>,\n+    /// A weighted LRU cache for storing the objects associated with a given path in memory\n+    // NOTE(trevor): this uses a mutex as the CLruCache type needs &mut self for its get method, so",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1772165602",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25377,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1772165602",
        "commented_code": "@@ -0,0 +1,804 @@\n+//! An in-memory cache of Parquet files that are persisted to object storage\n+use std::{\n+    fmt::Debug, hash::RandomState, num::NonZeroUsize, ops::Range, sync::Arc, time::Duration,\n+};\n+\n+use async_trait::async_trait;\n+use bytes::Bytes;\n+use clru::{CLruCache, CLruCacheConfig, WeightScale};\n+use futures::{StreamExt, TryStreamExt};\n+use futures_util::stream::BoxStream;\n+use object_store::{\n+    path::Path, Error, GetOptions, GetResult, GetResultPayload, ListResult, MultipartUpload,\n+    ObjectMeta, ObjectStore, PutMultipartOpts, PutOptions, PutPayload, PutResult,\n+    Result as ObjectStoreResult,\n+};\n+use observability_deps::tracing::{error, info};\n+use tokio::sync::{\n+    mpsc::{channel, Receiver, Sender},\n+    oneshot, Mutex,\n+};\n+\n+/// A request to fetch an item at the given `path` from an object store\n+///\n+/// Contains a notifier to notify the caller that registers the cache request when the item\n+/// has been cached successfully (or if the cache request failed in some way)\n+pub struct CacheRequest {\n+    path: Path,\n+    notifier: oneshot::Sender<()>,\n+}\n+\n+impl CacheRequest {\n+    /// Create a new [`CacheRequest`] along with a receiver to catch the notify message when\n+    /// the cache request has been fulfilled.\n+    pub fn create(path: Path) -> (Self, oneshot::Receiver<()>) {\n+        let (notifier, receiver) = oneshot::channel();\n+        (Self { path, notifier }, receiver)\n+    }\n+}\n+\n+/// An interface for interacting with a Parquet Cache by registering [`CacheRequest`]s to it.\n+pub trait ParquetCacheOracle: Send + Sync + Debug {\n+    /// Register a cache request with the oracle\n+    fn register(&self, cache_request: CacheRequest);\n+}\n+\n+/// Concrete implementation of the [`ParquetCacheOracle`]\n+///\n+/// This implementation sends all requests registered to be cached.\n+#[derive(Debug, Clone)]\n+pub struct MemCacheOracle {\n+    cache_request_tx: Sender<CacheRequest>,\n+}\n+\n+// TODO(trevor): make this configurable with reasonable default\n+const CACHE_REQUEST_BUFFER_SIZE: usize = 1_000_000;\n+\n+impl MemCacheOracle {\n+    /// Create a new [`MemCacheOracle`]\n+    ///\n+    /// This spawns two background tasks:\n+    /// * one to handle registered [`CacheRequest`]s\n+    /// * one to prune deleted and un-needed cache entries on an interval\n+    // TODO(trevor): this should be more configurable, e.g., channel size, prune interval\n+    fn new(mem_cached_store: Arc<MemCachedObjectStore>) -> Self {\n+        let (cache_request_tx, cache_request_rx) = channel(CACHE_REQUEST_BUFFER_SIZE);\n+        background_cache_request_handler(Arc::clone(&mem_cached_store), cache_request_rx);\n+        background_cache_pruner(mem_cached_store);\n+        Self { cache_request_tx }\n+    }\n+}\n+\n+impl ParquetCacheOracle for MemCacheOracle {\n+    fn register(&self, request: CacheRequest) {\n+        let tx = self.cache_request_tx.clone();\n+        tokio::spawn(async move {\n+            if let Err(error) = tx.send(request).await {\n+                error!(%error, \"error registering cache request\");\n+            };\n+        });\n+    }\n+}\n+\n+/// Helper function for creation of a [`MemCachedObjectStore`] and [`MemCacheOracle`]\n+/// that returns them as their `Arc<dyn _>` equivalent.\n+pub fn create_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+    cache_capacity: usize,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    let store = Arc::new(MemCachedObjectStore::new(object_store, cache_capacity));\n+    let oracle = Arc::new(MemCacheOracle::new(Arc::clone(&store)));\n+    (store, oracle)\n+}\n+\n+/// Create a test cached object store with a cache capacity of 1GB\n+pub fn test_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    create_cached_obj_store_and_oracle(object_store, 1024 * 1024 * 1024)\n+}\n+\n+/// An entry in the cache, containing the actual bytes as well as object store metadata\n+#[derive(Debug)]\n+struct CacheValue {\n+    data: Bytes,\n+    meta: ObjectMeta,\n+}\n+\n+impl CacheValue {\n+    /// Get the size of the cache value's memory footprint in bytes\n+    fn size(&self) -> usize {\n+        // TODO(trevor): could also calculate the size of the metadata...\n+        self.data.len()\n+    }\n+}\n+\n+/// The state of a cache entry\n+#[derive(Debug)]\n+enum CacheEntry {\n+    /// The cache entry is being fetched from object store\n+    Fetching,\n+    /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n+    Success(Arc<CacheValue>),\n+    /// The request to the object store failed\n+    Failed,\n+    /// The cache entry was deleted\n+    Deleted,\n+    /// The object is too large for the cache\n+    TooLarge,\n+}\n+\n+impl CacheEntry {\n+    /// Get the size of thje cache entry in bytes\n+    fn size(&self) -> usize {\n+        match self {\n+            CacheEntry::Fetching => 0,\n+            CacheEntry::Success(v) => v.size(),\n+            CacheEntry::Failed => 0,\n+            CacheEntry::Deleted => 0,\n+            CacheEntry::TooLarge => 0,\n+        }\n+    }\n+\n+    fn is_fetching(&self) -> bool {\n+        matches!(self, CacheEntry::Fetching)\n+    }\n+\n+    fn is_success(&self) -> bool {\n+        matches!(self, CacheEntry::Success(_))\n+    }\n+\n+    fn keep(&self) -> bool {\n+        self.is_fetching() || self.is_success()\n+    }\n+}\n+\n+/// Implements the [`WeightScale`] trait to determine a [`CacheEntry`]'s size on insertion to\n+/// the cache\n+#[derive(Debug)]\n+struct CacheEntryScale;\n+\n+impl WeightScale<Path, CacheEntry> for CacheEntryScale {\n+    fn weight(&self, key: &Path, value: &CacheEntry) -> usize {\n+        key.as_ref().len() + value.size()\n+    }\n+}\n+\n+/// Placeholder name for formatting datafusion errors\n+const STORE_NAME: &str = \"mem_cached_object_store\";\n+\n+/// An object store with an associated cache that can serve GET-style requests using the cache\n+///\n+/// The least-recently used (LRU) entries will be evicted when new entries are inserted, if the\n+/// new entry would exceed the cache's memory capacity\n+#[derive(Debug)]\n+pub struct MemCachedObjectStore {\n+    /// An inner object store for which items will be cached\n+    inner: Arc<dyn ObjectStore>,\n+    /// A weighted LRU cache for storing the objects associated with a given path in memory\n+    // NOTE(trevor): this uses a mutex as the CLruCache type needs &mut self for its get method, so",
        "comment_created_at": "2024-09-23T21:34:37+00:00",
        "comment_author": "pauldix",
        "comment_body": "This definitely feels like it's going to be a problem. An RWLock would be very preferred. I guess we can wait and see though.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773413276",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25377,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1772165602",
        "commented_code": "@@ -0,0 +1,804 @@\n+//! An in-memory cache of Parquet files that are persisted to object storage\n+use std::{\n+    fmt::Debug, hash::RandomState, num::NonZeroUsize, ops::Range, sync::Arc, time::Duration,\n+};\n+\n+use async_trait::async_trait;\n+use bytes::Bytes;\n+use clru::{CLruCache, CLruCacheConfig, WeightScale};\n+use futures::{StreamExt, TryStreamExt};\n+use futures_util::stream::BoxStream;\n+use object_store::{\n+    path::Path, Error, GetOptions, GetResult, GetResultPayload, ListResult, MultipartUpload,\n+    ObjectMeta, ObjectStore, PutMultipartOpts, PutOptions, PutPayload, PutResult,\n+    Result as ObjectStoreResult,\n+};\n+use observability_deps::tracing::{error, info};\n+use tokio::sync::{\n+    mpsc::{channel, Receiver, Sender},\n+    oneshot, Mutex,\n+};\n+\n+/// A request to fetch an item at the given `path` from an object store\n+///\n+/// Contains a notifier to notify the caller that registers the cache request when the item\n+/// has been cached successfully (or if the cache request failed in some way)\n+pub struct CacheRequest {\n+    path: Path,\n+    notifier: oneshot::Sender<()>,\n+}\n+\n+impl CacheRequest {\n+    /// Create a new [`CacheRequest`] along with a receiver to catch the notify message when\n+    /// the cache request has been fulfilled.\n+    pub fn create(path: Path) -> (Self, oneshot::Receiver<()>) {\n+        let (notifier, receiver) = oneshot::channel();\n+        (Self { path, notifier }, receiver)\n+    }\n+}\n+\n+/// An interface for interacting with a Parquet Cache by registering [`CacheRequest`]s to it.\n+pub trait ParquetCacheOracle: Send + Sync + Debug {\n+    /// Register a cache request with the oracle\n+    fn register(&self, cache_request: CacheRequest);\n+}\n+\n+/// Concrete implementation of the [`ParquetCacheOracle`]\n+///\n+/// This implementation sends all requests registered to be cached.\n+#[derive(Debug, Clone)]\n+pub struct MemCacheOracle {\n+    cache_request_tx: Sender<CacheRequest>,\n+}\n+\n+// TODO(trevor): make this configurable with reasonable default\n+const CACHE_REQUEST_BUFFER_SIZE: usize = 1_000_000;\n+\n+impl MemCacheOracle {\n+    /// Create a new [`MemCacheOracle`]\n+    ///\n+    /// This spawns two background tasks:\n+    /// * one to handle registered [`CacheRequest`]s\n+    /// * one to prune deleted and un-needed cache entries on an interval\n+    // TODO(trevor): this should be more configurable, e.g., channel size, prune interval\n+    fn new(mem_cached_store: Arc<MemCachedObjectStore>) -> Self {\n+        let (cache_request_tx, cache_request_rx) = channel(CACHE_REQUEST_BUFFER_SIZE);\n+        background_cache_request_handler(Arc::clone(&mem_cached_store), cache_request_rx);\n+        background_cache_pruner(mem_cached_store);\n+        Self { cache_request_tx }\n+    }\n+}\n+\n+impl ParquetCacheOracle for MemCacheOracle {\n+    fn register(&self, request: CacheRequest) {\n+        let tx = self.cache_request_tx.clone();\n+        tokio::spawn(async move {\n+            if let Err(error) = tx.send(request).await {\n+                error!(%error, \"error registering cache request\");\n+            };\n+        });\n+    }\n+}\n+\n+/// Helper function for creation of a [`MemCachedObjectStore`] and [`MemCacheOracle`]\n+/// that returns them as their `Arc<dyn _>` equivalent.\n+pub fn create_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+    cache_capacity: usize,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    let store = Arc::new(MemCachedObjectStore::new(object_store, cache_capacity));\n+    let oracle = Arc::new(MemCacheOracle::new(Arc::clone(&store)));\n+    (store, oracle)\n+}\n+\n+/// Create a test cached object store with a cache capacity of 1GB\n+pub fn test_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    create_cached_obj_store_and_oracle(object_store, 1024 * 1024 * 1024)\n+}\n+\n+/// An entry in the cache, containing the actual bytes as well as object store metadata\n+#[derive(Debug)]\n+struct CacheValue {\n+    data: Bytes,\n+    meta: ObjectMeta,\n+}\n+\n+impl CacheValue {\n+    /// Get the size of the cache value's memory footprint in bytes\n+    fn size(&self) -> usize {\n+        // TODO(trevor): could also calculate the size of the metadata...\n+        self.data.len()\n+    }\n+}\n+\n+/// The state of a cache entry\n+#[derive(Debug)]\n+enum CacheEntry {\n+    /// The cache entry is being fetched from object store\n+    Fetching,\n+    /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n+    Success(Arc<CacheValue>),\n+    /// The request to the object store failed\n+    Failed,\n+    /// The cache entry was deleted\n+    Deleted,\n+    /// The object is too large for the cache\n+    TooLarge,\n+}\n+\n+impl CacheEntry {\n+    /// Get the size of thje cache entry in bytes\n+    fn size(&self) -> usize {\n+        match self {\n+            CacheEntry::Fetching => 0,\n+            CacheEntry::Success(v) => v.size(),\n+            CacheEntry::Failed => 0,\n+            CacheEntry::Deleted => 0,\n+            CacheEntry::TooLarge => 0,\n+        }\n+    }\n+\n+    fn is_fetching(&self) -> bool {\n+        matches!(self, CacheEntry::Fetching)\n+    }\n+\n+    fn is_success(&self) -> bool {\n+        matches!(self, CacheEntry::Success(_))\n+    }\n+\n+    fn keep(&self) -> bool {\n+        self.is_fetching() || self.is_success()\n+    }\n+}\n+\n+/// Implements the [`WeightScale`] trait to determine a [`CacheEntry`]'s size on insertion to\n+/// the cache\n+#[derive(Debug)]\n+struct CacheEntryScale;\n+\n+impl WeightScale<Path, CacheEntry> for CacheEntryScale {\n+    fn weight(&self, key: &Path, value: &CacheEntry) -> usize {\n+        key.as_ref().len() + value.size()\n+    }\n+}\n+\n+/// Placeholder name for formatting datafusion errors\n+const STORE_NAME: &str = \"mem_cached_object_store\";\n+\n+/// An object store with an associated cache that can serve GET-style requests using the cache\n+///\n+/// The least-recently used (LRU) entries will be evicted when new entries are inserted, if the\n+/// new entry would exceed the cache's memory capacity\n+#[derive(Debug)]\n+pub struct MemCachedObjectStore {\n+    /// An inner object store for which items will be cached\n+    inner: Arc<dyn ObjectStore>,\n+    /// A weighted LRU cache for storing the objects associated with a given path in memory\n+    // NOTE(trevor): this uses a mutex as the CLruCache type needs &mut self for its get method, so",
        "comment_created_at": "2024-09-24T13:58:14+00:00",
        "comment_author": "hiltontj",
        "comment_body": "I agree, I opened https://github.com/influxdata/influxdb/issues/25382 to look for alternatives.\r\n\r\nI think most straight-up LRU implementations will have this problem, given that they need to update the recency when getting an item (the popular [`lru` crate](https://crates.io/crates/lru/) that `clru` is based on is essentially the same API).",
        "pr_file_module": null
      },
      {
        "comment_id": "1773447327",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25377,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1772165602",
        "commented_code": "@@ -0,0 +1,804 @@\n+//! An in-memory cache of Parquet files that are persisted to object storage\n+use std::{\n+    fmt::Debug, hash::RandomState, num::NonZeroUsize, ops::Range, sync::Arc, time::Duration,\n+};\n+\n+use async_trait::async_trait;\n+use bytes::Bytes;\n+use clru::{CLruCache, CLruCacheConfig, WeightScale};\n+use futures::{StreamExt, TryStreamExt};\n+use futures_util::stream::BoxStream;\n+use object_store::{\n+    path::Path, Error, GetOptions, GetResult, GetResultPayload, ListResult, MultipartUpload,\n+    ObjectMeta, ObjectStore, PutMultipartOpts, PutOptions, PutPayload, PutResult,\n+    Result as ObjectStoreResult,\n+};\n+use observability_deps::tracing::{error, info};\n+use tokio::sync::{\n+    mpsc::{channel, Receiver, Sender},\n+    oneshot, Mutex,\n+};\n+\n+/// A request to fetch an item at the given `path` from an object store\n+///\n+/// Contains a notifier to notify the caller that registers the cache request when the item\n+/// has been cached successfully (or if the cache request failed in some way)\n+pub struct CacheRequest {\n+    path: Path,\n+    notifier: oneshot::Sender<()>,\n+}\n+\n+impl CacheRequest {\n+    /// Create a new [`CacheRequest`] along with a receiver to catch the notify message when\n+    /// the cache request has been fulfilled.\n+    pub fn create(path: Path) -> (Self, oneshot::Receiver<()>) {\n+        let (notifier, receiver) = oneshot::channel();\n+        (Self { path, notifier }, receiver)\n+    }\n+}\n+\n+/// An interface for interacting with a Parquet Cache by registering [`CacheRequest`]s to it.\n+pub trait ParquetCacheOracle: Send + Sync + Debug {\n+    /// Register a cache request with the oracle\n+    fn register(&self, cache_request: CacheRequest);\n+}\n+\n+/// Concrete implementation of the [`ParquetCacheOracle`]\n+///\n+/// This implementation sends all requests registered to be cached.\n+#[derive(Debug, Clone)]\n+pub struct MemCacheOracle {\n+    cache_request_tx: Sender<CacheRequest>,\n+}\n+\n+// TODO(trevor): make this configurable with reasonable default\n+const CACHE_REQUEST_BUFFER_SIZE: usize = 1_000_000;\n+\n+impl MemCacheOracle {\n+    /// Create a new [`MemCacheOracle`]\n+    ///\n+    /// This spawns two background tasks:\n+    /// * one to handle registered [`CacheRequest`]s\n+    /// * one to prune deleted and un-needed cache entries on an interval\n+    // TODO(trevor): this should be more configurable, e.g., channel size, prune interval\n+    fn new(mem_cached_store: Arc<MemCachedObjectStore>) -> Self {\n+        let (cache_request_tx, cache_request_rx) = channel(CACHE_REQUEST_BUFFER_SIZE);\n+        background_cache_request_handler(Arc::clone(&mem_cached_store), cache_request_rx);\n+        background_cache_pruner(mem_cached_store);\n+        Self { cache_request_tx }\n+    }\n+}\n+\n+impl ParquetCacheOracle for MemCacheOracle {\n+    fn register(&self, request: CacheRequest) {\n+        let tx = self.cache_request_tx.clone();\n+        tokio::spawn(async move {\n+            if let Err(error) = tx.send(request).await {\n+                error!(%error, \"error registering cache request\");\n+            };\n+        });\n+    }\n+}\n+\n+/// Helper function for creation of a [`MemCachedObjectStore`] and [`MemCacheOracle`]\n+/// that returns them as their `Arc<dyn _>` equivalent.\n+pub fn create_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+    cache_capacity: usize,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    let store = Arc::new(MemCachedObjectStore::new(object_store, cache_capacity));\n+    let oracle = Arc::new(MemCacheOracle::new(Arc::clone(&store)));\n+    (store, oracle)\n+}\n+\n+/// Create a test cached object store with a cache capacity of 1GB\n+pub fn test_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    create_cached_obj_store_and_oracle(object_store, 1024 * 1024 * 1024)\n+}\n+\n+/// An entry in the cache, containing the actual bytes as well as object store metadata\n+#[derive(Debug)]\n+struct CacheValue {\n+    data: Bytes,\n+    meta: ObjectMeta,\n+}\n+\n+impl CacheValue {\n+    /// Get the size of the cache value's memory footprint in bytes\n+    fn size(&self) -> usize {\n+        // TODO(trevor): could also calculate the size of the metadata...\n+        self.data.len()\n+    }\n+}\n+\n+/// The state of a cache entry\n+#[derive(Debug)]\n+enum CacheEntry {\n+    /// The cache entry is being fetched from object store\n+    Fetching,\n+    /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n+    Success(Arc<CacheValue>),\n+    /// The request to the object store failed\n+    Failed,\n+    /// The cache entry was deleted\n+    Deleted,\n+    /// The object is too large for the cache\n+    TooLarge,\n+}\n+\n+impl CacheEntry {\n+    /// Get the size of thje cache entry in bytes\n+    fn size(&self) -> usize {\n+        match self {\n+            CacheEntry::Fetching => 0,\n+            CacheEntry::Success(v) => v.size(),\n+            CacheEntry::Failed => 0,\n+            CacheEntry::Deleted => 0,\n+            CacheEntry::TooLarge => 0,\n+        }\n+    }\n+\n+    fn is_fetching(&self) -> bool {\n+        matches!(self, CacheEntry::Fetching)\n+    }\n+\n+    fn is_success(&self) -> bool {\n+        matches!(self, CacheEntry::Success(_))\n+    }\n+\n+    fn keep(&self) -> bool {\n+        self.is_fetching() || self.is_success()\n+    }\n+}\n+\n+/// Implements the [`WeightScale`] trait to determine a [`CacheEntry`]'s size on insertion to\n+/// the cache\n+#[derive(Debug)]\n+struct CacheEntryScale;\n+\n+impl WeightScale<Path, CacheEntry> for CacheEntryScale {\n+    fn weight(&self, key: &Path, value: &CacheEntry) -> usize {\n+        key.as_ref().len() + value.size()\n+    }\n+}\n+\n+/// Placeholder name for formatting datafusion errors\n+const STORE_NAME: &str = \"mem_cached_object_store\";\n+\n+/// An object store with an associated cache that can serve GET-style requests using the cache\n+///\n+/// The least-recently used (LRU) entries will be evicted when new entries are inserted, if the\n+/// new entry would exceed the cache's memory capacity\n+#[derive(Debug)]\n+pub struct MemCachedObjectStore {\n+    /// An inner object store for which items will be cached\n+    inner: Arc<dyn ObjectStore>,\n+    /// A weighted LRU cache for storing the objects associated with a given path in memory\n+    // NOTE(trevor): this uses a mutex as the CLruCache type needs &mut self for its get method, so",
        "comment_created_at": "2024-09-24T14:17:45+00:00",
        "comment_author": "pauldix",
        "comment_body": "Seems like you could separate out the recency tracking from the data map. Also, atomics!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1772304391",
    "pr_number": 25377,
    "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
    "created_at": "2024-09-23T22:34:35+00:00",
    "commented_code": "+//! An in-memory cache of Parquet files that are persisted to object storage\n+use std::{\n+    fmt::Debug, hash::RandomState, num::NonZeroUsize, ops::Range, sync::Arc, time::Duration,\n+};\n+\n+use async_trait::async_trait;\n+use bytes::Bytes;\n+use clru::{CLruCache, CLruCacheConfig, WeightScale};\n+use futures::{StreamExt, TryStreamExt};\n+use futures_util::stream::BoxStream;\n+use object_store::{\n+    path::Path, Error, GetOptions, GetResult, GetResultPayload, ListResult, MultipartUpload,\n+    ObjectMeta, ObjectStore, PutMultipartOpts, PutOptions, PutPayload, PutResult,\n+    Result as ObjectStoreResult,\n+};\n+use observability_deps::tracing::{error, info};\n+use tokio::sync::{\n+    mpsc::{channel, Receiver, Sender},\n+    oneshot, Mutex,\n+};\n+\n+/// A request to fetch an item at the given `path` from an object store\n+///\n+/// Contains a notifier to notify the caller that registers the cache request when the item\n+/// has been cached successfully (or if the cache request failed in some way)\n+pub struct CacheRequest {\n+    path: Path,\n+    notifier: oneshot::Sender<()>,\n+}\n+\n+impl CacheRequest {\n+    /// Create a new [`CacheRequest`] along with a receiver to catch the notify message when\n+    /// the cache request has been fulfilled.\n+    pub fn create(path: Path) -> (Self, oneshot::Receiver<()>) {\n+        let (notifier, receiver) = oneshot::channel();\n+        (Self { path, notifier }, receiver)\n+    }\n+}\n+\n+/// An interface for interacting with a Parquet Cache by registering [`CacheRequest`]s to it.\n+pub trait ParquetCacheOracle: Send + Sync + Debug {\n+    /// Register a cache request with the oracle\n+    fn register(&self, cache_request: CacheRequest);\n+}\n+\n+/// Concrete implementation of the [`ParquetCacheOracle`]\n+///\n+/// This implementation sends all requests registered to be cached.\n+#[derive(Debug, Clone)]\n+pub struct MemCacheOracle {\n+    cache_request_tx: Sender<CacheRequest>,\n+}\n+\n+// TODO(trevor): make this configurable with reasonable default\n+const CACHE_REQUEST_BUFFER_SIZE: usize = 1_000_000;\n+\n+impl MemCacheOracle {\n+    /// Create a new [`MemCacheOracle`]\n+    ///\n+    /// This spawns two background tasks:\n+    /// * one to handle registered [`CacheRequest`]s\n+    /// * one to prune deleted and un-needed cache entries on an interval\n+    // TODO(trevor): this should be more configurable, e.g., channel size, prune interval\n+    fn new(mem_cached_store: Arc<MemCachedObjectStore>) -> Self {\n+        let (cache_request_tx, cache_request_rx) = channel(CACHE_REQUEST_BUFFER_SIZE);\n+        background_cache_request_handler(Arc::clone(&mem_cached_store), cache_request_rx);\n+        background_cache_pruner(mem_cached_store);\n+        Self { cache_request_tx }\n+    }\n+}\n+\n+impl ParquetCacheOracle for MemCacheOracle {\n+    fn register(&self, request: CacheRequest) {\n+        let tx = self.cache_request_tx.clone();\n+        tokio::spawn(async move {\n+            if let Err(error) = tx.send(request).await {\n+                error!(%error, \"error registering cache request\");\n+            };\n+        });\n+    }\n+}\n+\n+/// Helper function for creation of a [`MemCachedObjectStore`] and [`MemCacheOracle`]\n+/// that returns them as their `Arc<dyn _>` equivalent.\n+pub fn create_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+    cache_capacity: usize,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    let store = Arc::new(MemCachedObjectStore::new(object_store, cache_capacity));\n+    let oracle = Arc::new(MemCacheOracle::new(Arc::clone(&store)));\n+    (store, oracle)\n+}\n+\n+/// Create a test cached object store with a cache capacity of 1GB\n+pub fn test_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    create_cached_obj_store_and_oracle(object_store, 1024 * 1024 * 1024)\n+}\n+\n+/// An entry in the cache, containing the actual bytes as well as object store metadata\n+#[derive(Debug)]\n+struct CacheValue {\n+    data: Bytes,\n+    meta: ObjectMeta,\n+}\n+\n+impl CacheValue {\n+    /// Get the size of the cache value's memory footprint in bytes\n+    fn size(&self) -> usize {\n+        // TODO(trevor): could also calculate the size of the metadata...\n+        self.data.len()\n+    }\n+}\n+\n+/// The state of a cache entry\n+#[derive(Debug)]\n+enum CacheEntry {\n+    /// The cache entry is being fetched from object store\n+    Fetching,\n+    /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n+    Success(Arc<CacheValue>),\n+    /// The request to the object store failed\n+    Failed,\n+    /// The cache entry was deleted\n+    Deleted,\n+    /// The object is too large for the cache\n+    TooLarge,\n+}\n+\n+impl CacheEntry {\n+    /// Get the size of thje cache entry in bytes\n+    fn size(&self) -> usize {\n+        match self {\n+            CacheEntry::Fetching => 0,\n+            CacheEntry::Success(v) => v.size(),\n+            CacheEntry::Failed => 0,\n+            CacheEntry::Deleted => 0,\n+            CacheEntry::TooLarge => 0,\n+        }\n+    }\n+\n+    fn is_fetching(&self) -> bool {\n+        matches!(self, CacheEntry::Fetching)\n+    }\n+\n+    fn is_success(&self) -> bool {\n+        matches!(self, CacheEntry::Success(_))\n+    }\n+\n+    fn keep(&self) -> bool {\n+        self.is_fetching() || self.is_success()\n+    }\n+}\n+\n+/// Implements the [`WeightScale`] trait to determine a [`CacheEntry`]'s size on insertion to\n+/// the cache\n+#[derive(Debug)]\n+struct CacheEntryScale;\n+\n+impl WeightScale<Path, CacheEntry> for CacheEntryScale {\n+    fn weight(&self, key: &Path, value: &CacheEntry) -> usize {\n+        key.as_ref().len() + value.size()\n+    }\n+}\n+\n+/// Placeholder name for formatting datafusion errors\n+const STORE_NAME: &str = \"mem_cached_object_store\";\n+\n+/// An object store with an associated cache that can serve GET-style requests using the cache\n+///\n+/// The least-recently used (LRU) entries will be evicted when new entries are inserted, if the\n+/// new entry would exceed the cache's memory capacity\n+#[derive(Debug)]\n+pub struct MemCachedObjectStore {\n+    /// An inner object store for which items will be cached\n+    inner: Arc<dyn ObjectStore>,\n+    /// A weighted LRU cache for storing the objects associated with a given path in memory\n+    // NOTE(trevor): this uses a mutex as the CLruCache type needs &mut self for its get method, so\n+    // we always need an exclusive lock on the cache. If this creates a performance bottleneck then\n+    // we will need to look for alternatives.\n+    //\n+    // A Tokio mutex is used to prevent blocking the thread while waiting for a lock, and so that\n+    // the lock can be held accross await points.\n+    cache: Arc<Mutex<CLruCache<Path, CacheEntry, RandomState, CacheEntryScale>>>,\n+}\n+\n+impl MemCachedObjectStore {\n+    /// Create a new [`MemCachedObjectStore`] with the given memory capacity\n+    fn new(inner: Arc<dyn ObjectStore>, memory_capacity: usize) -> Self {\n+        let cache = CLruCache::with_config(\n+            CLruCacheConfig::new(NonZeroUsize::new(memory_capacity).unwrap())\n+                .with_scale(CacheEntryScale),\n+        );\n+        Self {\n+            inner,\n+            cache: Arc::new(Mutex::new(cache)),\n+        }\n+    }\n+\n+    /// Get an entry in the cache if it contains a successful fetch result, or `None` otherwise\n+    ///\n+    /// This requires `&mut self` as the underlying method on the cache requires a mutable reference\n+    /// in order to update the recency of the entry in the cache\n+    async fn get_cache_value(&self, path: &Path) -> Option<Arc<CacheValue>> {\n+        self.cache\n+            .lock()\n+            .await\n+            .get(path)\n+            .and_then(|entry| match entry {\n+                CacheEntry::Fetching\n+                | CacheEntry::Failed\n+                | CacheEntry::Deleted\n+                | CacheEntry::TooLarge => None,\n+                CacheEntry::Success(v) => Some(Arc::clone(v)),\n+            })\n+    }\n+\n+    /// Set the state of a cache entry to `Deleted`, since we cannot remove elements from the\n+    /// cache directly.\n+    async fn delete_cache_value(&self, path: &Path) {\n+        let _ = self\n+            .cache\n+            .lock()\n+            .await\n+            .put_with_weight(path.clone(), CacheEntry::Deleted);\n+    }\n+}\n+\n+impl std::fmt::Display for MemCachedObjectStore {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"MemCachedObjectStore({})\", self.inner)\n+    }\n+}\n+\n+/// [`MemCachedObjectStore`] implements most [`ObjectStore`] methods as a pass-through, since\n+/// caching is decided externally. The exception is `delete`, which will have the entry removed\n+/// from the cache if the delete to the object store was successful.\n+///\n+/// GET-style methods will first check the cache for the object at the given path, before forwarding\n+/// to the inner [`ObjectStore`]. They do not, however, populate the cache after data has been fetched\n+/// from the inner store.\n+#[async_trait]\n+impl ObjectStore for MemCachedObjectStore {\n+    async fn put(&self, location: &Path, bytes: PutPayload) -> ObjectStoreResult<PutResult> {\n+        self.inner.put(location, bytes).await\n+    }\n+\n+    async fn put_opts(\n+        &self,\n+        location: &Path,\n+        bytes: PutPayload,\n+        opts: PutOptions,\n+    ) -> ObjectStoreResult<PutResult> {\n+        self.inner.put_opts(location, bytes, opts).await\n+    }\n+\n+    async fn put_multipart(&self, location: &Path) -> ObjectStoreResult<Box<dyn MultipartUpload>> {\n+        self.inner.put_multipart(location).await\n+    }\n+\n+    async fn put_multipart_opts(\n+        &self,\n+        location: &Path,\n+        opts: PutMultipartOpts,\n+    ) -> ObjectStoreResult<Box<dyn MultipartUpload>> {\n+        self.inner.put_multipart_opts(location, opts).await\n+    }\n+\n+    /// Get an object from the object store. If this object is cached, then it will not make a request\n+    /// to the inner object store.\n+    async fn get(&self, location: &Path) -> ObjectStoreResult<GetResult> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            Ok(GetResult {\n+                payload: GetResultPayload::Stream(\n+                    futures::stream::iter([Ok(v.data.clone())]).boxed(),\n+                ),\n+                meta: v.meta.clone(),\n+                range: 0..v.data.len(),\n+                attributes: Default::default(),\n+            })\n+        } else {\n+            self.inner.get(location).await\n+        }\n+    }\n+\n+    async fn get_opts(&self, location: &Path, options: GetOptions) -> ObjectStoreResult<GetResult> {\n+        // NOTE(trevor): this could probably be supported through the cache if we need it via the\n+        // ObjectMeta stored in the cache. For now this is conservative:\n+        self.inner.get_opts(location, options).await\n+    }\n+\n+    async fn get_range(&self, location: &Path, range: Range<usize>) -> ObjectStoreResult<Bytes> {\n+        Ok(self\n+            .get_ranges(location, &[range])\n+            .await?\n+            .into_iter()\n+            .next()\n+            .expect(\"requested one range\"))\n+    }\n+\n+    /// This request is used by DataFusion when requesting metadata for Parquet files, so we need\n+    /// to use the cache to prevent excess network calls during query planning.\n+    async fn get_ranges(\n+        &self,\n+        location: &Path,\n+        ranges: &[Range<usize>],\n+    ) -> ObjectStoreResult<Vec<Bytes>> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            ranges\n+                .iter()\n+                .map(|range| {\n+                    if range.end > v.data.len() {\n+                        return Err(Error::Generic {\n+                            store: STORE_NAME,\n+                            source: format!(\n+                                \"Range end ({}) out of bounds, object size is {}\",\n+                                range.end,\n+                                v.data.len()\n+                            )\n+                            .into(),\n+                        });\n+                    }\n+                    if range.start > range.end {\n+                        return Err(Error::Generic {\n+                            store: STORE_NAME,\n+                            source: format!(\n+                                \"Range end ({}) is before range start ({})\",\n+                                range.end, range.start\n+                            )\n+                            .into(),\n+                        });\n+                    }\n+                    Ok(v.data.slice(range.clone()))\n+                })\n+                .collect()\n+        } else {\n+            self.inner.get_ranges(location, ranges).await\n+        }\n+    }\n+\n+    async fn head(&self, location: &Path) -> ObjectStoreResult<ObjectMeta> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            Ok(v.meta.clone())\n+        } else {\n+            self.inner.head(location).await\n+        }\n+    }\n+\n+    /// Delete an object on object store, but also remove it from the cache.\n+    async fn delete(&self, location: &Path) -> ObjectStoreResult<()> {\n+        let result = self.inner.delete(location).await?;\n+        self.delete_cache_value(location).await;\n+        Ok(result)\n+    }\n+\n+    fn delete_stream<'a>(\n+        &'a self,\n+        locations: BoxStream<'a, ObjectStoreResult<Path>>,\n+    ) -> BoxStream<'a, ObjectStoreResult<Path>> {\n+        locations\n+            .and_then(|_| futures::future::err(Error::NotImplemented))\n+            .boxed()\n+    }\n+\n+    fn list(&self, prefix: Option<&Path>) -> BoxStream<'_, ObjectStoreResult<ObjectMeta>> {\n+        self.inner.list(prefix)\n+    }\n+\n+    fn list_with_offset(\n+        &self,\n+        prefix: Option<&Path>,\n+        offset: &Path,\n+    ) -> BoxStream<'_, ObjectStoreResult<ObjectMeta>> {\n+        self.inner.list_with_offset(prefix, offset)\n+    }\n+\n+    async fn list_with_delimiter(&self, prefix: Option<&Path>) -> ObjectStoreResult<ListResult> {\n+        self.inner.list_with_delimiter(prefix).await\n+    }\n+\n+    async fn copy(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.copy(from, to).await\n+    }\n+\n+    async fn rename(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.rename(from, to).await\n+    }\n+\n+    async fn copy_if_not_exists(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.copy_if_not_exists(from, to).await\n+    }\n+\n+    async fn rename_if_not_exists(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.rename_if_not_exists(from, to).await\n+    }\n+}\n+\n+/// Handle [`CacheRequest`]s in a background task\n+///\n+/// This waits on the given `Receiver` for new cache requests to be registered, i.e., via the oracle.\n+/// If a cache request is received for an entry that has not already been fetched successfully, or\n+/// one that is in the process of being fetched, then this will spin a separate background task to\n+/// fetch the object from object store and update the cache. This is so that cache requests can be\n+/// handled in parallel.\n+fn background_cache_request_handler(\n+    mem_store: Arc<MemCachedObjectStore>,\n+    mut rx: Receiver<CacheRequest>,\n+) -> tokio::task::JoinHandle<()> {\n+    tokio::spawn(async move {\n+        while let Some(CacheRequest { path, notifier }) = rx.recv().await {\n+            // Check that the cache does not already contain an entry for the provide path, or that\n+            // it is not already in the process of fetching the given path:\n+            let mut cache_lock = mem_store.cache.lock().await;\n+            if cache_lock\n+                .get(&path)\n+                .is_some_and(|entry| entry.is_fetching() || entry.is_success())\n+            {\n+                continue;\n+            }\n+            // Put a `Fetching` state in the entry to prevent concurrent requests to the same path:\n+            let _ = cache_lock.put_with_weight(path.clone(), CacheEntry::Fetching);",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1772304391",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25377,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1772304391",
        "commented_code": "@@ -0,0 +1,804 @@\n+//! An in-memory cache of Parquet files that are persisted to object storage\n+use std::{\n+    fmt::Debug, hash::RandomState, num::NonZeroUsize, ops::Range, sync::Arc, time::Duration,\n+};\n+\n+use async_trait::async_trait;\n+use bytes::Bytes;\n+use clru::{CLruCache, CLruCacheConfig, WeightScale};\n+use futures::{StreamExt, TryStreamExt};\n+use futures_util::stream::BoxStream;\n+use object_store::{\n+    path::Path, Error, GetOptions, GetResult, GetResultPayload, ListResult, MultipartUpload,\n+    ObjectMeta, ObjectStore, PutMultipartOpts, PutOptions, PutPayload, PutResult,\n+    Result as ObjectStoreResult,\n+};\n+use observability_deps::tracing::{error, info};\n+use tokio::sync::{\n+    mpsc::{channel, Receiver, Sender},\n+    oneshot, Mutex,\n+};\n+\n+/// A request to fetch an item at the given `path` from an object store\n+///\n+/// Contains a notifier to notify the caller that registers the cache request when the item\n+/// has been cached successfully (or if the cache request failed in some way)\n+pub struct CacheRequest {\n+    path: Path,\n+    notifier: oneshot::Sender<()>,\n+}\n+\n+impl CacheRequest {\n+    /// Create a new [`CacheRequest`] along with a receiver to catch the notify message when\n+    /// the cache request has been fulfilled.\n+    pub fn create(path: Path) -> (Self, oneshot::Receiver<()>) {\n+        let (notifier, receiver) = oneshot::channel();\n+        (Self { path, notifier }, receiver)\n+    }\n+}\n+\n+/// An interface for interacting with a Parquet Cache by registering [`CacheRequest`]s to it.\n+pub trait ParquetCacheOracle: Send + Sync + Debug {\n+    /// Register a cache request with the oracle\n+    fn register(&self, cache_request: CacheRequest);\n+}\n+\n+/// Concrete implementation of the [`ParquetCacheOracle`]\n+///\n+/// This implementation sends all requests registered to be cached.\n+#[derive(Debug, Clone)]\n+pub struct MemCacheOracle {\n+    cache_request_tx: Sender<CacheRequest>,\n+}\n+\n+// TODO(trevor): make this configurable with reasonable default\n+const CACHE_REQUEST_BUFFER_SIZE: usize = 1_000_000;\n+\n+impl MemCacheOracle {\n+    /// Create a new [`MemCacheOracle`]\n+    ///\n+    /// This spawns two background tasks:\n+    /// * one to handle registered [`CacheRequest`]s\n+    /// * one to prune deleted and un-needed cache entries on an interval\n+    // TODO(trevor): this should be more configurable, e.g., channel size, prune interval\n+    fn new(mem_cached_store: Arc<MemCachedObjectStore>) -> Self {\n+        let (cache_request_tx, cache_request_rx) = channel(CACHE_REQUEST_BUFFER_SIZE);\n+        background_cache_request_handler(Arc::clone(&mem_cached_store), cache_request_rx);\n+        background_cache_pruner(mem_cached_store);\n+        Self { cache_request_tx }\n+    }\n+}\n+\n+impl ParquetCacheOracle for MemCacheOracle {\n+    fn register(&self, request: CacheRequest) {\n+        let tx = self.cache_request_tx.clone();\n+        tokio::spawn(async move {\n+            if let Err(error) = tx.send(request).await {\n+                error!(%error, \"error registering cache request\");\n+            };\n+        });\n+    }\n+}\n+\n+/// Helper function for creation of a [`MemCachedObjectStore`] and [`MemCacheOracle`]\n+/// that returns them as their `Arc<dyn _>` equivalent.\n+pub fn create_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+    cache_capacity: usize,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    let store = Arc::new(MemCachedObjectStore::new(object_store, cache_capacity));\n+    let oracle = Arc::new(MemCacheOracle::new(Arc::clone(&store)));\n+    (store, oracle)\n+}\n+\n+/// Create a test cached object store with a cache capacity of 1GB\n+pub fn test_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    create_cached_obj_store_and_oracle(object_store, 1024 * 1024 * 1024)\n+}\n+\n+/// An entry in the cache, containing the actual bytes as well as object store metadata\n+#[derive(Debug)]\n+struct CacheValue {\n+    data: Bytes,\n+    meta: ObjectMeta,\n+}\n+\n+impl CacheValue {\n+    /// Get the size of the cache value's memory footprint in bytes\n+    fn size(&self) -> usize {\n+        // TODO(trevor): could also calculate the size of the metadata...\n+        self.data.len()\n+    }\n+}\n+\n+/// The state of a cache entry\n+#[derive(Debug)]\n+enum CacheEntry {\n+    /// The cache entry is being fetched from object store\n+    Fetching,\n+    /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n+    Success(Arc<CacheValue>),\n+    /// The request to the object store failed\n+    Failed,\n+    /// The cache entry was deleted\n+    Deleted,\n+    /// The object is too large for the cache\n+    TooLarge,\n+}\n+\n+impl CacheEntry {\n+    /// Get the size of thje cache entry in bytes\n+    fn size(&self) -> usize {\n+        match self {\n+            CacheEntry::Fetching => 0,\n+            CacheEntry::Success(v) => v.size(),\n+            CacheEntry::Failed => 0,\n+            CacheEntry::Deleted => 0,\n+            CacheEntry::TooLarge => 0,\n+        }\n+    }\n+\n+    fn is_fetching(&self) -> bool {\n+        matches!(self, CacheEntry::Fetching)\n+    }\n+\n+    fn is_success(&self) -> bool {\n+        matches!(self, CacheEntry::Success(_))\n+    }\n+\n+    fn keep(&self) -> bool {\n+        self.is_fetching() || self.is_success()\n+    }\n+}\n+\n+/// Implements the [`WeightScale`] trait to determine a [`CacheEntry`]'s size on insertion to\n+/// the cache\n+#[derive(Debug)]\n+struct CacheEntryScale;\n+\n+impl WeightScale<Path, CacheEntry> for CacheEntryScale {\n+    fn weight(&self, key: &Path, value: &CacheEntry) -> usize {\n+        key.as_ref().len() + value.size()\n+    }\n+}\n+\n+/// Placeholder name for formatting datafusion errors\n+const STORE_NAME: &str = \"mem_cached_object_store\";\n+\n+/// An object store with an associated cache that can serve GET-style requests using the cache\n+///\n+/// The least-recently used (LRU) entries will be evicted when new entries are inserted, if the\n+/// new entry would exceed the cache's memory capacity\n+#[derive(Debug)]\n+pub struct MemCachedObjectStore {\n+    /// An inner object store for which items will be cached\n+    inner: Arc<dyn ObjectStore>,\n+    /// A weighted LRU cache for storing the objects associated with a given path in memory\n+    // NOTE(trevor): this uses a mutex as the CLruCache type needs &mut self for its get method, so\n+    // we always need an exclusive lock on the cache. If this creates a performance bottleneck then\n+    // we will need to look for alternatives.\n+    //\n+    // A Tokio mutex is used to prevent blocking the thread while waiting for a lock, and so that\n+    // the lock can be held accross await points.\n+    cache: Arc<Mutex<CLruCache<Path, CacheEntry, RandomState, CacheEntryScale>>>,\n+}\n+\n+impl MemCachedObjectStore {\n+    /// Create a new [`MemCachedObjectStore`] with the given memory capacity\n+    fn new(inner: Arc<dyn ObjectStore>, memory_capacity: usize) -> Self {\n+        let cache = CLruCache::with_config(\n+            CLruCacheConfig::new(NonZeroUsize::new(memory_capacity).unwrap())\n+                .with_scale(CacheEntryScale),\n+        );\n+        Self {\n+            inner,\n+            cache: Arc::new(Mutex::new(cache)),\n+        }\n+    }\n+\n+    /// Get an entry in the cache if it contains a successful fetch result, or `None` otherwise\n+    ///\n+    /// This requires `&mut self` as the underlying method on the cache requires a mutable reference\n+    /// in order to update the recency of the entry in the cache\n+    async fn get_cache_value(&self, path: &Path) -> Option<Arc<CacheValue>> {\n+        self.cache\n+            .lock()\n+            .await\n+            .get(path)\n+            .and_then(|entry| match entry {\n+                CacheEntry::Fetching\n+                | CacheEntry::Failed\n+                | CacheEntry::Deleted\n+                | CacheEntry::TooLarge => None,\n+                CacheEntry::Success(v) => Some(Arc::clone(v)),\n+            })\n+    }\n+\n+    /// Set the state of a cache entry to `Deleted`, since we cannot remove elements from the\n+    /// cache directly.\n+    async fn delete_cache_value(&self, path: &Path) {\n+        let _ = self\n+            .cache\n+            .lock()\n+            .await\n+            .put_with_weight(path.clone(), CacheEntry::Deleted);\n+    }\n+}\n+\n+impl std::fmt::Display for MemCachedObjectStore {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"MemCachedObjectStore({})\", self.inner)\n+    }\n+}\n+\n+/// [`MemCachedObjectStore`] implements most [`ObjectStore`] methods as a pass-through, since\n+/// caching is decided externally. The exception is `delete`, which will have the entry removed\n+/// from the cache if the delete to the object store was successful.\n+///\n+/// GET-style methods will first check the cache for the object at the given path, before forwarding\n+/// to the inner [`ObjectStore`]. They do not, however, populate the cache after data has been fetched\n+/// from the inner store.\n+#[async_trait]\n+impl ObjectStore for MemCachedObjectStore {\n+    async fn put(&self, location: &Path, bytes: PutPayload) -> ObjectStoreResult<PutResult> {\n+        self.inner.put(location, bytes).await\n+    }\n+\n+    async fn put_opts(\n+        &self,\n+        location: &Path,\n+        bytes: PutPayload,\n+        opts: PutOptions,\n+    ) -> ObjectStoreResult<PutResult> {\n+        self.inner.put_opts(location, bytes, opts).await\n+    }\n+\n+    async fn put_multipart(&self, location: &Path) -> ObjectStoreResult<Box<dyn MultipartUpload>> {\n+        self.inner.put_multipart(location).await\n+    }\n+\n+    async fn put_multipart_opts(\n+        &self,\n+        location: &Path,\n+        opts: PutMultipartOpts,\n+    ) -> ObjectStoreResult<Box<dyn MultipartUpload>> {\n+        self.inner.put_multipart_opts(location, opts).await\n+    }\n+\n+    /// Get an object from the object store. If this object is cached, then it will not make a request\n+    /// to the inner object store.\n+    async fn get(&self, location: &Path) -> ObjectStoreResult<GetResult> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            Ok(GetResult {\n+                payload: GetResultPayload::Stream(\n+                    futures::stream::iter([Ok(v.data.clone())]).boxed(),\n+                ),\n+                meta: v.meta.clone(),\n+                range: 0..v.data.len(),\n+                attributes: Default::default(),\n+            })\n+        } else {\n+            self.inner.get(location).await\n+        }\n+    }\n+\n+    async fn get_opts(&self, location: &Path, options: GetOptions) -> ObjectStoreResult<GetResult> {\n+        // NOTE(trevor): this could probably be supported through the cache if we need it via the\n+        // ObjectMeta stored in the cache. For now this is conservative:\n+        self.inner.get_opts(location, options).await\n+    }\n+\n+    async fn get_range(&self, location: &Path, range: Range<usize>) -> ObjectStoreResult<Bytes> {\n+        Ok(self\n+            .get_ranges(location, &[range])\n+            .await?\n+            .into_iter()\n+            .next()\n+            .expect(\"requested one range\"))\n+    }\n+\n+    /// This request is used by DataFusion when requesting metadata for Parquet files, so we need\n+    /// to use the cache to prevent excess network calls during query planning.\n+    async fn get_ranges(\n+        &self,\n+        location: &Path,\n+        ranges: &[Range<usize>],\n+    ) -> ObjectStoreResult<Vec<Bytes>> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            ranges\n+                .iter()\n+                .map(|range| {\n+                    if range.end > v.data.len() {\n+                        return Err(Error::Generic {\n+                            store: STORE_NAME,\n+                            source: format!(\n+                                \"Range end ({}) out of bounds, object size is {}\",\n+                                range.end,\n+                                v.data.len()\n+                            )\n+                            .into(),\n+                        });\n+                    }\n+                    if range.start > range.end {\n+                        return Err(Error::Generic {\n+                            store: STORE_NAME,\n+                            source: format!(\n+                                \"Range end ({}) is before range start ({})\",\n+                                range.end, range.start\n+                            )\n+                            .into(),\n+                        });\n+                    }\n+                    Ok(v.data.slice(range.clone()))\n+                })\n+                .collect()\n+        } else {\n+            self.inner.get_ranges(location, ranges).await\n+        }\n+    }\n+\n+    async fn head(&self, location: &Path) -> ObjectStoreResult<ObjectMeta> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            Ok(v.meta.clone())\n+        } else {\n+            self.inner.head(location).await\n+        }\n+    }\n+\n+    /// Delete an object on object store, but also remove it from the cache.\n+    async fn delete(&self, location: &Path) -> ObjectStoreResult<()> {\n+        let result = self.inner.delete(location).await?;\n+        self.delete_cache_value(location).await;\n+        Ok(result)\n+    }\n+\n+    fn delete_stream<'a>(\n+        &'a self,\n+        locations: BoxStream<'a, ObjectStoreResult<Path>>,\n+    ) -> BoxStream<'a, ObjectStoreResult<Path>> {\n+        locations\n+            .and_then(|_| futures::future::err(Error::NotImplemented))\n+            .boxed()\n+    }\n+\n+    fn list(&self, prefix: Option<&Path>) -> BoxStream<'_, ObjectStoreResult<ObjectMeta>> {\n+        self.inner.list(prefix)\n+    }\n+\n+    fn list_with_offset(\n+        &self,\n+        prefix: Option<&Path>,\n+        offset: &Path,\n+    ) -> BoxStream<'_, ObjectStoreResult<ObjectMeta>> {\n+        self.inner.list_with_offset(prefix, offset)\n+    }\n+\n+    async fn list_with_delimiter(&self, prefix: Option<&Path>) -> ObjectStoreResult<ListResult> {\n+        self.inner.list_with_delimiter(prefix).await\n+    }\n+\n+    async fn copy(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.copy(from, to).await\n+    }\n+\n+    async fn rename(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.rename(from, to).await\n+    }\n+\n+    async fn copy_if_not_exists(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.copy_if_not_exists(from, to).await\n+    }\n+\n+    async fn rename_if_not_exists(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.rename_if_not_exists(from, to).await\n+    }\n+}\n+\n+/// Handle [`CacheRequest`]s in a background task\n+///\n+/// This waits on the given `Receiver` for new cache requests to be registered, i.e., via the oracle.\n+/// If a cache request is received for an entry that has not already been fetched successfully, or\n+/// one that is in the process of being fetched, then this will spin a separate background task to\n+/// fetch the object from object store and update the cache. This is so that cache requests can be\n+/// handled in parallel.\n+fn background_cache_request_handler(\n+    mem_store: Arc<MemCachedObjectStore>,\n+    mut rx: Receiver<CacheRequest>,\n+) -> tokio::task::JoinHandle<()> {\n+    tokio::spawn(async move {\n+        while let Some(CacheRequest { path, notifier }) = rx.recv().await {\n+            // Check that the cache does not already contain an entry for the provide path, or that\n+            // it is not already in the process of fetching the given path:\n+            let mut cache_lock = mem_store.cache.lock().await;\n+            if cache_lock\n+                .get(&path)\n+                .is_some_and(|entry| entry.is_fetching() || entry.is_success())\n+            {\n+                continue;\n+            }\n+            // Put a `Fetching` state in the entry to prevent concurrent requests to the same path:\n+            let _ = cache_lock.put_with_weight(path.clone(), CacheEntry::Fetching);",
        "comment_created_at": "2024-09-23T22:34:35+00:00",
        "comment_author": "pauldix",
        "comment_body": "small thing, but I'd clone the path before you acquire the lock.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773508863",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25377,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1772304391",
        "commented_code": "@@ -0,0 +1,804 @@\n+//! An in-memory cache of Parquet files that are persisted to object storage\n+use std::{\n+    fmt::Debug, hash::RandomState, num::NonZeroUsize, ops::Range, sync::Arc, time::Duration,\n+};\n+\n+use async_trait::async_trait;\n+use bytes::Bytes;\n+use clru::{CLruCache, CLruCacheConfig, WeightScale};\n+use futures::{StreamExt, TryStreamExt};\n+use futures_util::stream::BoxStream;\n+use object_store::{\n+    path::Path, Error, GetOptions, GetResult, GetResultPayload, ListResult, MultipartUpload,\n+    ObjectMeta, ObjectStore, PutMultipartOpts, PutOptions, PutPayload, PutResult,\n+    Result as ObjectStoreResult,\n+};\n+use observability_deps::tracing::{error, info};\n+use tokio::sync::{\n+    mpsc::{channel, Receiver, Sender},\n+    oneshot, Mutex,\n+};\n+\n+/// A request to fetch an item at the given `path` from an object store\n+///\n+/// Contains a notifier to notify the caller that registers the cache request when the item\n+/// has been cached successfully (or if the cache request failed in some way)\n+pub struct CacheRequest {\n+    path: Path,\n+    notifier: oneshot::Sender<()>,\n+}\n+\n+impl CacheRequest {\n+    /// Create a new [`CacheRequest`] along with a receiver to catch the notify message when\n+    /// the cache request has been fulfilled.\n+    pub fn create(path: Path) -> (Self, oneshot::Receiver<()>) {\n+        let (notifier, receiver) = oneshot::channel();\n+        (Self { path, notifier }, receiver)\n+    }\n+}\n+\n+/// An interface for interacting with a Parquet Cache by registering [`CacheRequest`]s to it.\n+pub trait ParquetCacheOracle: Send + Sync + Debug {\n+    /// Register a cache request with the oracle\n+    fn register(&self, cache_request: CacheRequest);\n+}\n+\n+/// Concrete implementation of the [`ParquetCacheOracle`]\n+///\n+/// This implementation sends all requests registered to be cached.\n+#[derive(Debug, Clone)]\n+pub struct MemCacheOracle {\n+    cache_request_tx: Sender<CacheRequest>,\n+}\n+\n+// TODO(trevor): make this configurable with reasonable default\n+const CACHE_REQUEST_BUFFER_SIZE: usize = 1_000_000;\n+\n+impl MemCacheOracle {\n+    /// Create a new [`MemCacheOracle`]\n+    ///\n+    /// This spawns two background tasks:\n+    /// * one to handle registered [`CacheRequest`]s\n+    /// * one to prune deleted and un-needed cache entries on an interval\n+    // TODO(trevor): this should be more configurable, e.g., channel size, prune interval\n+    fn new(mem_cached_store: Arc<MemCachedObjectStore>) -> Self {\n+        let (cache_request_tx, cache_request_rx) = channel(CACHE_REQUEST_BUFFER_SIZE);\n+        background_cache_request_handler(Arc::clone(&mem_cached_store), cache_request_rx);\n+        background_cache_pruner(mem_cached_store);\n+        Self { cache_request_tx }\n+    }\n+}\n+\n+impl ParquetCacheOracle for MemCacheOracle {\n+    fn register(&self, request: CacheRequest) {\n+        let tx = self.cache_request_tx.clone();\n+        tokio::spawn(async move {\n+            if let Err(error) = tx.send(request).await {\n+                error!(%error, \"error registering cache request\");\n+            };\n+        });\n+    }\n+}\n+\n+/// Helper function for creation of a [`MemCachedObjectStore`] and [`MemCacheOracle`]\n+/// that returns them as their `Arc<dyn _>` equivalent.\n+pub fn create_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+    cache_capacity: usize,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    let store = Arc::new(MemCachedObjectStore::new(object_store, cache_capacity));\n+    let oracle = Arc::new(MemCacheOracle::new(Arc::clone(&store)));\n+    (store, oracle)\n+}\n+\n+/// Create a test cached object store with a cache capacity of 1GB\n+pub fn test_cached_obj_store_and_oracle(\n+    object_store: Arc<dyn ObjectStore>,\n+) -> (Arc<dyn ObjectStore>, Arc<dyn ParquetCacheOracle>) {\n+    create_cached_obj_store_and_oracle(object_store, 1024 * 1024 * 1024)\n+}\n+\n+/// An entry in the cache, containing the actual bytes as well as object store metadata\n+#[derive(Debug)]\n+struct CacheValue {\n+    data: Bytes,\n+    meta: ObjectMeta,\n+}\n+\n+impl CacheValue {\n+    /// Get the size of the cache value's memory footprint in bytes\n+    fn size(&self) -> usize {\n+        // TODO(trevor): could also calculate the size of the metadata...\n+        self.data.len()\n+    }\n+}\n+\n+/// The state of a cache entry\n+#[derive(Debug)]\n+enum CacheEntry {\n+    /// The cache entry is being fetched from object store\n+    Fetching,\n+    /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n+    Success(Arc<CacheValue>),\n+    /// The request to the object store failed\n+    Failed,\n+    /// The cache entry was deleted\n+    Deleted,\n+    /// The object is too large for the cache\n+    TooLarge,\n+}\n+\n+impl CacheEntry {\n+    /// Get the size of thje cache entry in bytes\n+    fn size(&self) -> usize {\n+        match self {\n+            CacheEntry::Fetching => 0,\n+            CacheEntry::Success(v) => v.size(),\n+            CacheEntry::Failed => 0,\n+            CacheEntry::Deleted => 0,\n+            CacheEntry::TooLarge => 0,\n+        }\n+    }\n+\n+    fn is_fetching(&self) -> bool {\n+        matches!(self, CacheEntry::Fetching)\n+    }\n+\n+    fn is_success(&self) -> bool {\n+        matches!(self, CacheEntry::Success(_))\n+    }\n+\n+    fn keep(&self) -> bool {\n+        self.is_fetching() || self.is_success()\n+    }\n+}\n+\n+/// Implements the [`WeightScale`] trait to determine a [`CacheEntry`]'s size on insertion to\n+/// the cache\n+#[derive(Debug)]\n+struct CacheEntryScale;\n+\n+impl WeightScale<Path, CacheEntry> for CacheEntryScale {\n+    fn weight(&self, key: &Path, value: &CacheEntry) -> usize {\n+        key.as_ref().len() + value.size()\n+    }\n+}\n+\n+/// Placeholder name for formatting datafusion errors\n+const STORE_NAME: &str = \"mem_cached_object_store\";\n+\n+/// An object store with an associated cache that can serve GET-style requests using the cache\n+///\n+/// The least-recently used (LRU) entries will be evicted when new entries are inserted, if the\n+/// new entry would exceed the cache's memory capacity\n+#[derive(Debug)]\n+pub struct MemCachedObjectStore {\n+    /// An inner object store for which items will be cached\n+    inner: Arc<dyn ObjectStore>,\n+    /// A weighted LRU cache for storing the objects associated with a given path in memory\n+    // NOTE(trevor): this uses a mutex as the CLruCache type needs &mut self for its get method, so\n+    // we always need an exclusive lock on the cache. If this creates a performance bottleneck then\n+    // we will need to look for alternatives.\n+    //\n+    // A Tokio mutex is used to prevent blocking the thread while waiting for a lock, and so that\n+    // the lock can be held accross await points.\n+    cache: Arc<Mutex<CLruCache<Path, CacheEntry, RandomState, CacheEntryScale>>>,\n+}\n+\n+impl MemCachedObjectStore {\n+    /// Create a new [`MemCachedObjectStore`] with the given memory capacity\n+    fn new(inner: Arc<dyn ObjectStore>, memory_capacity: usize) -> Self {\n+        let cache = CLruCache::with_config(\n+            CLruCacheConfig::new(NonZeroUsize::new(memory_capacity).unwrap())\n+                .with_scale(CacheEntryScale),\n+        );\n+        Self {\n+            inner,\n+            cache: Arc::new(Mutex::new(cache)),\n+        }\n+    }\n+\n+    /// Get an entry in the cache if it contains a successful fetch result, or `None` otherwise\n+    ///\n+    /// This requires `&mut self` as the underlying method on the cache requires a mutable reference\n+    /// in order to update the recency of the entry in the cache\n+    async fn get_cache_value(&self, path: &Path) -> Option<Arc<CacheValue>> {\n+        self.cache\n+            .lock()\n+            .await\n+            .get(path)\n+            .and_then(|entry| match entry {\n+                CacheEntry::Fetching\n+                | CacheEntry::Failed\n+                | CacheEntry::Deleted\n+                | CacheEntry::TooLarge => None,\n+                CacheEntry::Success(v) => Some(Arc::clone(v)),\n+            })\n+    }\n+\n+    /// Set the state of a cache entry to `Deleted`, since we cannot remove elements from the\n+    /// cache directly.\n+    async fn delete_cache_value(&self, path: &Path) {\n+        let _ = self\n+            .cache\n+            .lock()\n+            .await\n+            .put_with_weight(path.clone(), CacheEntry::Deleted);\n+    }\n+}\n+\n+impl std::fmt::Display for MemCachedObjectStore {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"MemCachedObjectStore({})\", self.inner)\n+    }\n+}\n+\n+/// [`MemCachedObjectStore`] implements most [`ObjectStore`] methods as a pass-through, since\n+/// caching is decided externally. The exception is `delete`, which will have the entry removed\n+/// from the cache if the delete to the object store was successful.\n+///\n+/// GET-style methods will first check the cache for the object at the given path, before forwarding\n+/// to the inner [`ObjectStore`]. They do not, however, populate the cache after data has been fetched\n+/// from the inner store.\n+#[async_trait]\n+impl ObjectStore for MemCachedObjectStore {\n+    async fn put(&self, location: &Path, bytes: PutPayload) -> ObjectStoreResult<PutResult> {\n+        self.inner.put(location, bytes).await\n+    }\n+\n+    async fn put_opts(\n+        &self,\n+        location: &Path,\n+        bytes: PutPayload,\n+        opts: PutOptions,\n+    ) -> ObjectStoreResult<PutResult> {\n+        self.inner.put_opts(location, bytes, opts).await\n+    }\n+\n+    async fn put_multipart(&self, location: &Path) -> ObjectStoreResult<Box<dyn MultipartUpload>> {\n+        self.inner.put_multipart(location).await\n+    }\n+\n+    async fn put_multipart_opts(\n+        &self,\n+        location: &Path,\n+        opts: PutMultipartOpts,\n+    ) -> ObjectStoreResult<Box<dyn MultipartUpload>> {\n+        self.inner.put_multipart_opts(location, opts).await\n+    }\n+\n+    /// Get an object from the object store. If this object is cached, then it will not make a request\n+    /// to the inner object store.\n+    async fn get(&self, location: &Path) -> ObjectStoreResult<GetResult> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            Ok(GetResult {\n+                payload: GetResultPayload::Stream(\n+                    futures::stream::iter([Ok(v.data.clone())]).boxed(),\n+                ),\n+                meta: v.meta.clone(),\n+                range: 0..v.data.len(),\n+                attributes: Default::default(),\n+            })\n+        } else {\n+            self.inner.get(location).await\n+        }\n+    }\n+\n+    async fn get_opts(&self, location: &Path, options: GetOptions) -> ObjectStoreResult<GetResult> {\n+        // NOTE(trevor): this could probably be supported through the cache if we need it via the\n+        // ObjectMeta stored in the cache. For now this is conservative:\n+        self.inner.get_opts(location, options).await\n+    }\n+\n+    async fn get_range(&self, location: &Path, range: Range<usize>) -> ObjectStoreResult<Bytes> {\n+        Ok(self\n+            .get_ranges(location, &[range])\n+            .await?\n+            .into_iter()\n+            .next()\n+            .expect(\"requested one range\"))\n+    }\n+\n+    /// This request is used by DataFusion when requesting metadata for Parquet files, so we need\n+    /// to use the cache to prevent excess network calls during query planning.\n+    async fn get_ranges(\n+        &self,\n+        location: &Path,\n+        ranges: &[Range<usize>],\n+    ) -> ObjectStoreResult<Vec<Bytes>> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            ranges\n+                .iter()\n+                .map(|range| {\n+                    if range.end > v.data.len() {\n+                        return Err(Error::Generic {\n+                            store: STORE_NAME,\n+                            source: format!(\n+                                \"Range end ({}) out of bounds, object size is {}\",\n+                                range.end,\n+                                v.data.len()\n+                            )\n+                            .into(),\n+                        });\n+                    }\n+                    if range.start > range.end {\n+                        return Err(Error::Generic {\n+                            store: STORE_NAME,\n+                            source: format!(\n+                                \"Range end ({}) is before range start ({})\",\n+                                range.end, range.start\n+                            )\n+                            .into(),\n+                        });\n+                    }\n+                    Ok(v.data.slice(range.clone()))\n+                })\n+                .collect()\n+        } else {\n+            self.inner.get_ranges(location, ranges).await\n+        }\n+    }\n+\n+    async fn head(&self, location: &Path) -> ObjectStoreResult<ObjectMeta> {\n+        if let Some(v) = self.get_cache_value(location).await {\n+            Ok(v.meta.clone())\n+        } else {\n+            self.inner.head(location).await\n+        }\n+    }\n+\n+    /// Delete an object on object store, but also remove it from the cache.\n+    async fn delete(&self, location: &Path) -> ObjectStoreResult<()> {\n+        let result = self.inner.delete(location).await?;\n+        self.delete_cache_value(location).await;\n+        Ok(result)\n+    }\n+\n+    fn delete_stream<'a>(\n+        &'a self,\n+        locations: BoxStream<'a, ObjectStoreResult<Path>>,\n+    ) -> BoxStream<'a, ObjectStoreResult<Path>> {\n+        locations\n+            .and_then(|_| futures::future::err(Error::NotImplemented))\n+            .boxed()\n+    }\n+\n+    fn list(&self, prefix: Option<&Path>) -> BoxStream<'_, ObjectStoreResult<ObjectMeta>> {\n+        self.inner.list(prefix)\n+    }\n+\n+    fn list_with_offset(\n+        &self,\n+        prefix: Option<&Path>,\n+        offset: &Path,\n+    ) -> BoxStream<'_, ObjectStoreResult<ObjectMeta>> {\n+        self.inner.list_with_offset(prefix, offset)\n+    }\n+\n+    async fn list_with_delimiter(&self, prefix: Option<&Path>) -> ObjectStoreResult<ListResult> {\n+        self.inner.list_with_delimiter(prefix).await\n+    }\n+\n+    async fn copy(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.copy(from, to).await\n+    }\n+\n+    async fn rename(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.rename(from, to).await\n+    }\n+\n+    async fn copy_if_not_exists(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.copy_if_not_exists(from, to).await\n+    }\n+\n+    async fn rename_if_not_exists(&self, from: &Path, to: &Path) -> ObjectStoreResult<()> {\n+        self.inner.rename_if_not_exists(from, to).await\n+    }\n+}\n+\n+/// Handle [`CacheRequest`]s in a background task\n+///\n+/// This waits on the given `Receiver` for new cache requests to be registered, i.e., via the oracle.\n+/// If a cache request is received for an entry that has not already been fetched successfully, or\n+/// one that is in the process of being fetched, then this will spin a separate background task to\n+/// fetch the object from object store and update the cache. This is so that cache requests can be\n+/// handled in parallel.\n+fn background_cache_request_handler(\n+    mem_store: Arc<MemCachedObjectStore>,\n+    mut rx: Receiver<CacheRequest>,\n+) -> tokio::task::JoinHandle<()> {\n+    tokio::spawn(async move {\n+        while let Some(CacheRequest { path, notifier }) = rx.recv().await {\n+            // Check that the cache does not already contain an entry for the provide path, or that\n+            // it is not already in the process of fetching the given path:\n+            let mut cache_lock = mem_store.cache.lock().await;\n+            if cache_lock\n+                .get(&path)\n+                .is_some_and(|entry| entry.is_fetching() || entry.is_success())\n+            {\n+                continue;\n+            }\n+            // Put a `Fetching` state in the entry to prevent concurrent requests to the same path:\n+            let _ = cache_lock.put_with_weight(path.clone(), CacheEntry::Fetching);",
        "comment_created_at": "2024-09-24T14:40:27+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Addressed in https://github.com/influxdata/influxdb/pull/25377/commits/ac8d7d3ba9ce25cf0dd04b4471e955e8ae4e1790",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1785132763",
    "pr_number": 25421,
    "pr_file": "influxdb3_catalog/src/catalog.rs",
    "created_at": "2024-10-02T19:38:48+00:00",
    "commented_code": "pub fn new(host_id: Arc<str>, instance_id: Arc<str>) -> Self {\n         Self {\n             inner: RwLock::new(InnerCatalog::new(host_id, instance_id)),\n+            db_map: RwLock::new(BiHashMap::new()),",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1785132763",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25421,
        "pr_file": "influxdb3_catalog/src/catalog.rs",
        "discussion_id": "1785132763",
        "commented_code": "@@ -124,21 +127,70 @@ impl Catalog {\n     pub fn new(host_id: Arc<str>, instance_id: Arc<str>) -> Self {\n         Self {\n             inner: RwLock::new(InnerCatalog::new(host_id, instance_id)),\n+            db_map: RwLock::new(BiHashMap::new()),",
        "comment_created_at": "2024-10-02T19:38:48+00:00",
        "comment_author": "pauldix",
        "comment_body": "Shouldn't these just be in the `InnerCatalog` under its lock? Otherwise you won't be able to update these maps when inserting a new database or table in and ensure the mappings go in at the same time.",
        "pr_file_module": null
      },
      {
        "comment_id": "1786628962",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25421,
        "pr_file": "influxdb3_catalog/src/catalog.rs",
        "discussion_id": "1785132763",
        "commented_code": "@@ -124,21 +127,70 @@ impl Catalog {\n     pub fn new(host_id: Arc<str>, instance_id: Arc<str>) -> Self {\n         Self {\n             inner: RwLock::new(InnerCatalog::new(host_id, instance_id)),\n+            db_map: RwLock::new(BiHashMap::new()),",
        "comment_created_at": "2024-10-03T17:43:12+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "I put these into the InnerCatalog",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1774252155",
    "pr_number": 25389,
    "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
    "created_at": "2024-09-25T00:30:00+00:00",
    "commented_code": ") -> tokio::task::JoinHandle<()> {\n     tokio::spawn(async move {\n         while let Some(CacheRequest { path, notifier }) = rx.recv().await {\n-            // clone the path before acquiring the lock:\n-            let path_cloned = path.clone();\n-            // Check that the cache does not already contain an entry for the provide path, or that\n-            // it is not already in the process of fetching the given path:\n-            let mut cache_lock = mem_store.cache.lock().await;\n-            if cache_lock\n-                .get(&path)\n-                .is_some_and(|entry| entry.is_fetching() || entry.is_success())\n-            {\n+            // We assume that objects on object store are immutable, so we can skip objects that\n+            // we have already fetched:\n+            if mem_store.cache.path_already_fetched(&path).await {\n                 continue;\n             }\n             // Put a `Fetching` state in the entry to prevent concurrent requests to the same path:\n-            let _ = cache_lock.put_with_weight(path_cloned, CacheEntry::Fetching);\n-            // Drop the lock before spawning the task below\n-            drop(cache_lock);\n+            mem_store.cache.set_fetching(&path).await;\n             let mem_store_captured = Arc::clone(&mem_store);\n             tokio::spawn(async move {\n-                let cache_insertion_result = match mem_store_captured.inner.get(&path).await {\n+                match mem_store_captured.inner.get(&path).await {\n                     Ok(result) => {\n                         let meta = result.meta.clone();\n                         match result.bytes().await {\n-                            Ok(data) => mem_store_captured.cache.lock().await.put_with_weight(\n-                                path,\n-                                CacheEntry::Success(Arc::new(CacheValue { data, meta })),\n-                            ),\n-                            Err(error) => {\n-                                error!(%error, \"failed to retrieve payload from object store get result\");\n-                                mem_store_captured\n+                            Ok(data) => {\n+                                if let Err(error) = mem_store_captured\n                                     .cache\n-                                    .lock()\n+                                    .set_success(&path, CacheValue { data, meta })\n                                     .await\n-                                    .put_with_weight(path, CacheEntry::Failed)\n+                                {\n+                                    error!(%error, \"failed to set the success state on the cache\");\n+                                    mem_store_captured.cache.remove(&path).await;",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1774252155",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25389,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1774252155",
        "commented_code": "@@ -409,61 +526,40 @@ fn background_cache_request_handler(\n ) -> tokio::task::JoinHandle<()> {\n     tokio::spawn(async move {\n         while let Some(CacheRequest { path, notifier }) = rx.recv().await {\n-            // clone the path before acquiring the lock:\n-            let path_cloned = path.clone();\n-            // Check that the cache does not already contain an entry for the provide path, or that\n-            // it is not already in the process of fetching the given path:\n-            let mut cache_lock = mem_store.cache.lock().await;\n-            if cache_lock\n-                .get(&path)\n-                .is_some_and(|entry| entry.is_fetching() || entry.is_success())\n-            {\n+            // We assume that objects on object store are immutable, so we can skip objects that\n+            // we have already fetched:\n+            if mem_store.cache.path_already_fetched(&path).await {\n                 continue;\n             }\n             // Put a `Fetching` state in the entry to prevent concurrent requests to the same path:\n-            let _ = cache_lock.put_with_weight(path_cloned, CacheEntry::Fetching);\n-            // Drop the lock before spawning the task below\n-            drop(cache_lock);\n+            mem_store.cache.set_fetching(&path).await;\n             let mem_store_captured = Arc::clone(&mem_store);\n             tokio::spawn(async move {\n-                let cache_insertion_result = match mem_store_captured.inner.get(&path).await {\n+                match mem_store_captured.inner.get(&path).await {\n                     Ok(result) => {\n                         let meta = result.meta.clone();\n                         match result.bytes().await {\n-                            Ok(data) => mem_store_captured.cache.lock().await.put_with_weight(\n-                                path,\n-                                CacheEntry::Success(Arc::new(CacheValue { data, meta })),\n-                            ),\n-                            Err(error) => {\n-                                error!(%error, \"failed to retrieve payload from object store get result\");\n-                                mem_store_captured\n+                            Ok(data) => {\n+                                if let Err(error) = mem_store_captured\n                                     .cache\n-                                    .lock()\n+                                    .set_success(&path, CacheValue { data, meta })\n                                     .await\n-                                    .put_with_weight(path, CacheEntry::Failed)\n+                                {\n+                                    error!(%error, \"failed to set the success state on the cache\");\n+                                    mem_store_captured.cache.remove(&path).await;",
        "comment_created_at": "2024-09-25T00:30:00+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Might be better to do the remove in `set_success` directly, so that it does it all under one lock.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1775972129",
    "pr_number": 25389,
    "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
    "created_at": "2024-09-25T20:45:49+00:00",
    "commented_code": "impl CacheValue {\n     /// Get the size of the cache value's memory footprint in bytes\n     fn size(&self) -> usize {\n-        // TODO(trevor): could also calculate the size of the metadata...\n-        self.data.len()\n+        let Self { data, meta } = self;\n+        let ObjectMeta {\n+            location,\n+            last_modified: _,\n+            size: _,\n+            e_tag,\n+            version,\n+        } = meta;\n+\n+        data.len()\n+            + location.as_ref().len()\n+            + e_tag.as_ref().map(|s| s.capacity()).unwrap_or_default()\n+            + version.as_ref().map(|s| s.capacity()).unwrap_or_default()\n+    }\n+}\n+\n+#[derive(Debug)]\n+struct CacheEntry {\n+    state: CacheEntryState,\n+    /// A counter for tracking how many times this entry has been hit.\n+    ///\n+    /// When first created in the `Fetching` state, this will be set to -1, which will prevent\n+    /// the fetching entry from being evicted by a prune operation\n+    hits: AtomicI32,\n+}\n+\n+impl CacheEntry {\n+    /// Increment the used counter for this entry unless it is already at the maximum value\n+    fn increment_hits(&self) {\n+        let _ = self\n+            .hits\n+            .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |x| {\n+                if x == i32::MAX {\n+                    None\n+                } else {\n+                    Some(x + 1)\n+                }\n+            });\n+    }\n+\n+    /// Get the approximate memory footprint of this entry in bytes\n+    fn size(&self) -> usize {\n+        self.state.size() + std::mem::size_of::<AtomicI32>()\n     }\n }\n \n /// The state of a cache entry\n #[derive(Debug)]\n-enum CacheEntry {\n+enum CacheEntryState {\n     /// The cache entry is being fetched from object store\n-    Fetching,\n+    ///\n+    /// This holds a [`watch::Sender`] that is used to notify requests made to get this entry\n+    /// while it is being fetched by the cache oracle.\n+    Fetching(watch::Sender<Option<Arc<CacheValue>>>),\n     /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n     Success(Arc<CacheValue>),\n-    /// The request to the object store failed\n-    Failed,\n-    /// The cache entry was deleted\n-    Deleted,\n-    /// The object is too large for the cache\n-    TooLarge,\n }\n \n-impl CacheEntry {\n-    /// Get the size of thje cache entry in bytes\n+impl CacheEntryState {\n+    /// Get the approximate size of the cache entry in bytes\n     fn size(&self) -> usize {\n         match self {\n-            CacheEntry::Fetching => 0,\n-            CacheEntry::Success(v) => v.size(),\n-            CacheEntry::Failed => 0,\n-            CacheEntry::Deleted => 0,\n-            CacheEntry::TooLarge => 0,\n+            CacheEntryState::Fetching(tx) => std::mem::size_of_val(tx),\n+            CacheEntryState::Success(v) => v.size(),\n         }\n     }\n+}\n+\n+/// A cache for storing objects from object storage by their [`Path`]\n+///\n+/// This acts as a Least-Frequently-Used (LFU) cache that allows for concurrent reads. See the\n+/// [`Cache::prune`] method for implementation of how the cache entries are pruned. Pruning must\n+/// be invoked externally, e.g., on an interval.\n+#[derive(Debug)]\n+struct Cache {\n+    /// The maximum amount of memory this cache should occupy\n+    capacity: usize,\n+    /// The current amount of memory being used by the cache\n+    used: AtomicUsize,\n+    /// The maximum amount of memory to free during a prune operation\n+    max_free_amount: usize,\n+    /// The map storing cache entries\n+    ///\n+    /// This uses [`IndexMap`] to preserve insertion order, such that, when iterating over the map\n+    /// to prune entries, iteration occurs in the order that entries were inserted. This will have\n+    /// older entries removed before newer entries\n+    map: RwLock<IndexMap<Path, CacheEntry>>,",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1775972129",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25389,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1775972129",
        "commented_code": "@@ -108,59 +115,231 @@ struct CacheValue {\n impl CacheValue {\n     /// Get the size of the cache value's memory footprint in bytes\n     fn size(&self) -> usize {\n-        // TODO(trevor): could also calculate the size of the metadata...\n-        self.data.len()\n+        let Self { data, meta } = self;\n+        let ObjectMeta {\n+            location,\n+            last_modified: _,\n+            size: _,\n+            e_tag,\n+            version,\n+        } = meta;\n+\n+        data.len()\n+            + location.as_ref().len()\n+            + e_tag.as_ref().map(|s| s.capacity()).unwrap_or_default()\n+            + version.as_ref().map(|s| s.capacity()).unwrap_or_default()\n+    }\n+}\n+\n+#[derive(Debug)]\n+struct CacheEntry {\n+    state: CacheEntryState,\n+    /// A counter for tracking how many times this entry has been hit.\n+    ///\n+    /// When first created in the `Fetching` state, this will be set to -1, which will prevent\n+    /// the fetching entry from being evicted by a prune operation\n+    hits: AtomicI32,\n+}\n+\n+impl CacheEntry {\n+    /// Increment the used counter for this entry unless it is already at the maximum value\n+    fn increment_hits(&self) {\n+        let _ = self\n+            .hits\n+            .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |x| {\n+                if x == i32::MAX {\n+                    None\n+                } else {\n+                    Some(x + 1)\n+                }\n+            });\n+    }\n+\n+    /// Get the approximate memory footprint of this entry in bytes\n+    fn size(&self) -> usize {\n+        self.state.size() + std::mem::size_of::<AtomicI32>()\n     }\n }\n \n /// The state of a cache entry\n #[derive(Debug)]\n-enum CacheEntry {\n+enum CacheEntryState {\n     /// The cache entry is being fetched from object store\n-    Fetching,\n+    ///\n+    /// This holds a [`watch::Sender`] that is used to notify requests made to get this entry\n+    /// while it is being fetched by the cache oracle.\n+    Fetching(watch::Sender<Option<Arc<CacheValue>>>),\n     /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n     Success(Arc<CacheValue>),\n-    /// The request to the object store failed\n-    Failed,\n-    /// The cache entry was deleted\n-    Deleted,\n-    /// The object is too large for the cache\n-    TooLarge,\n }\n \n-impl CacheEntry {\n-    /// Get the size of thje cache entry in bytes\n+impl CacheEntryState {\n+    /// Get the approximate size of the cache entry in bytes\n     fn size(&self) -> usize {\n         match self {\n-            CacheEntry::Fetching => 0,\n-            CacheEntry::Success(v) => v.size(),\n-            CacheEntry::Failed => 0,\n-            CacheEntry::Deleted => 0,\n-            CacheEntry::TooLarge => 0,\n+            CacheEntryState::Fetching(tx) => std::mem::size_of_val(tx),\n+            CacheEntryState::Success(v) => v.size(),\n         }\n     }\n+}\n+\n+/// A cache for storing objects from object storage by their [`Path`]\n+///\n+/// This acts as a Least-Frequently-Used (LFU) cache that allows for concurrent reads. See the\n+/// [`Cache::prune`] method for implementation of how the cache entries are pruned. Pruning must\n+/// be invoked externally, e.g., on an interval.\n+#[derive(Debug)]\n+struct Cache {\n+    /// The maximum amount of memory this cache should occupy\n+    capacity: usize,\n+    /// The current amount of memory being used by the cache\n+    used: AtomicUsize,\n+    /// The maximum amount of memory to free during a prune operation\n+    max_free_amount: usize,\n+    /// The map storing cache entries\n+    ///\n+    /// This uses [`IndexMap`] to preserve insertion order, such that, when iterating over the map\n+    /// to prune entries, iteration occurs in the order that entries were inserted. This will have\n+    /// older entries removed before newer entries\n+    map: RwLock<IndexMap<Path, CacheEntry>>,",
        "comment_created_at": "2024-09-25T20:45:49+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "I wonder using something like [SkipMap](https://github.com/crossbeam-rs/crossbeam/blob/master/crossbeam-skiplist/src/lib.rs) (if order is relevant) or [DashMap](https://github.com/xacrimon/dashmap/) (if order is not relevant) sort of implementations will work better so that `Path` (key) lookup locks are distributed. Two paths that are not related might have lock contention in this implementation, which could possibly be avoided?\r\n\r\nI haven't looked at the call site, so I could be missing something but it'd be really nice if the contention for lock is scoped really to two identical paths.",
        "pr_file_module": null
      },
      {
        "comment_id": "1776113942",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25389,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1775972129",
        "commented_code": "@@ -108,59 +115,231 @@ struct CacheValue {\n impl CacheValue {\n     /// Get the size of the cache value's memory footprint in bytes\n     fn size(&self) -> usize {\n-        // TODO(trevor): could also calculate the size of the metadata...\n-        self.data.len()\n+        let Self { data, meta } = self;\n+        let ObjectMeta {\n+            location,\n+            last_modified: _,\n+            size: _,\n+            e_tag,\n+            version,\n+        } = meta;\n+\n+        data.len()\n+            + location.as_ref().len()\n+            + e_tag.as_ref().map(|s| s.capacity()).unwrap_or_default()\n+            + version.as_ref().map(|s| s.capacity()).unwrap_or_default()\n+    }\n+}\n+\n+#[derive(Debug)]\n+struct CacheEntry {\n+    state: CacheEntryState,\n+    /// A counter for tracking how many times this entry has been hit.\n+    ///\n+    /// When first created in the `Fetching` state, this will be set to -1, which will prevent\n+    /// the fetching entry from being evicted by a prune operation\n+    hits: AtomicI32,\n+}\n+\n+impl CacheEntry {\n+    /// Increment the used counter for this entry unless it is already at the maximum value\n+    fn increment_hits(&self) {\n+        let _ = self\n+            .hits\n+            .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |x| {\n+                if x == i32::MAX {\n+                    None\n+                } else {\n+                    Some(x + 1)\n+                }\n+            });\n+    }\n+\n+    /// Get the approximate memory footprint of this entry in bytes\n+    fn size(&self) -> usize {\n+        self.state.size() + std::mem::size_of::<AtomicI32>()\n     }\n }\n \n /// The state of a cache entry\n #[derive(Debug)]\n-enum CacheEntry {\n+enum CacheEntryState {\n     /// The cache entry is being fetched from object store\n-    Fetching,\n+    ///\n+    /// This holds a [`watch::Sender`] that is used to notify requests made to get this entry\n+    /// while it is being fetched by the cache oracle.\n+    Fetching(watch::Sender<Option<Arc<CacheValue>>>),\n     /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n     Success(Arc<CacheValue>),\n-    /// The request to the object store failed\n-    Failed,\n-    /// The cache entry was deleted\n-    Deleted,\n-    /// The object is too large for the cache\n-    TooLarge,\n }\n \n-impl CacheEntry {\n-    /// Get the size of thje cache entry in bytes\n+impl CacheEntryState {\n+    /// Get the approximate size of the cache entry in bytes\n     fn size(&self) -> usize {\n         match self {\n-            CacheEntry::Fetching => 0,\n-            CacheEntry::Success(v) => v.size(),\n-            CacheEntry::Failed => 0,\n-            CacheEntry::Deleted => 0,\n-            CacheEntry::TooLarge => 0,\n+            CacheEntryState::Fetching(tx) => std::mem::size_of_val(tx),\n+            CacheEntryState::Success(v) => v.size(),\n         }\n     }\n+}\n+\n+/// A cache for storing objects from object storage by their [`Path`]\n+///\n+/// This acts as a Least-Frequently-Used (LFU) cache that allows for concurrent reads. See the\n+/// [`Cache::prune`] method for implementation of how the cache entries are pruned. Pruning must\n+/// be invoked externally, e.g., on an interval.\n+#[derive(Debug)]\n+struct Cache {\n+    /// The maximum amount of memory this cache should occupy\n+    capacity: usize,\n+    /// The current amount of memory being used by the cache\n+    used: AtomicUsize,\n+    /// The maximum amount of memory to free during a prune operation\n+    max_free_amount: usize,\n+    /// The map storing cache entries\n+    ///\n+    /// This uses [`IndexMap`] to preserve insertion order, such that, when iterating over the map\n+    /// to prune entries, iteration occurs in the order that entries were inserted. This will have\n+    /// older entries removed before newer entries\n+    map: RwLock<IndexMap<Path, CacheEntry>>,",
        "comment_created_at": "2024-09-25T23:57:43+00:00",
        "comment_author": "hiltontj",
        "comment_body": "DashMap could definitely be useful here. I actually was using it at one point, so maybe I will re-introduce. I want to read more about how it works though.",
        "pr_file_module": null
      },
      {
        "comment_id": "1778681307",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25389,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1775972129",
        "commented_code": "@@ -108,59 +115,231 @@ struct CacheValue {\n impl CacheValue {\n     /// Get the size of the cache value's memory footprint in bytes\n     fn size(&self) -> usize {\n-        // TODO(trevor): could also calculate the size of the metadata...\n-        self.data.len()\n+        let Self { data, meta } = self;\n+        let ObjectMeta {\n+            location,\n+            last_modified: _,\n+            size: _,\n+            e_tag,\n+            version,\n+        } = meta;\n+\n+        data.len()\n+            + location.as_ref().len()\n+            + e_tag.as_ref().map(|s| s.capacity()).unwrap_or_default()\n+            + version.as_ref().map(|s| s.capacity()).unwrap_or_default()\n+    }\n+}\n+\n+#[derive(Debug)]\n+struct CacheEntry {\n+    state: CacheEntryState,\n+    /// A counter for tracking how many times this entry has been hit.\n+    ///\n+    /// When first created in the `Fetching` state, this will be set to -1, which will prevent\n+    /// the fetching entry from being evicted by a prune operation\n+    hits: AtomicI32,\n+}\n+\n+impl CacheEntry {\n+    /// Increment the used counter for this entry unless it is already at the maximum value\n+    fn increment_hits(&self) {\n+        let _ = self\n+            .hits\n+            .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |x| {\n+                if x == i32::MAX {\n+                    None\n+                } else {\n+                    Some(x + 1)\n+                }\n+            });\n+    }\n+\n+    /// Get the approximate memory footprint of this entry in bytes\n+    fn size(&self) -> usize {\n+        self.state.size() + std::mem::size_of::<AtomicI32>()\n     }\n }\n \n /// The state of a cache entry\n #[derive(Debug)]\n-enum CacheEntry {\n+enum CacheEntryState {\n     /// The cache entry is being fetched from object store\n-    Fetching,\n+    ///\n+    /// This holds a [`watch::Sender`] that is used to notify requests made to get this entry\n+    /// while it is being fetched by the cache oracle.\n+    Fetching(watch::Sender<Option<Arc<CacheValue>>>),\n     /// The cache entry was successfully fetched and is stored in the cache as a [`CacheValue`]\n     Success(Arc<CacheValue>),\n-    /// The request to the object store failed\n-    Failed,\n-    /// The cache entry was deleted\n-    Deleted,\n-    /// The object is too large for the cache\n-    TooLarge,\n }\n \n-impl CacheEntry {\n-    /// Get the size of thje cache entry in bytes\n+impl CacheEntryState {\n+    /// Get the approximate size of the cache entry in bytes\n     fn size(&self) -> usize {\n         match self {\n-            CacheEntry::Fetching => 0,\n-            CacheEntry::Success(v) => v.size(),\n-            CacheEntry::Failed => 0,\n-            CacheEntry::Deleted => 0,\n-            CacheEntry::TooLarge => 0,\n+            CacheEntryState::Fetching(tx) => std::mem::size_of_val(tx),\n+            CacheEntryState::Success(v) => v.size(),\n         }\n     }\n+}\n+\n+/// A cache for storing objects from object storage by their [`Path`]\n+///\n+/// This acts as a Least-Frequently-Used (LFU) cache that allows for concurrent reads. See the\n+/// [`Cache::prune`] method for implementation of how the cache entries are pruned. Pruning must\n+/// be invoked externally, e.g., on an interval.\n+#[derive(Debug)]\n+struct Cache {\n+    /// The maximum amount of memory this cache should occupy\n+    capacity: usize,\n+    /// The current amount of memory being used by the cache\n+    used: AtomicUsize,\n+    /// The maximum amount of memory to free during a prune operation\n+    max_free_amount: usize,\n+    /// The map storing cache entries\n+    ///\n+    /// This uses [`IndexMap`] to preserve insertion order, such that, when iterating over the map\n+    /// to prune entries, iteration occurs in the order that entries were inserted. This will have\n+    /// older entries removed before newer entries\n+    map: RwLock<IndexMap<Path, CacheEntry>>,",
        "comment_created_at": "2024-09-27T14:01:49+00:00",
        "comment_author": "hiltontj",
        "comment_body": "I switched over to using `DashMap` which hopefully will help reduce some lock contention.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1659349365",
    "pr_number": 25109,
    "pr_file": "influxdb3_write/src/last_cache.rs",
    "created_at": "2024-06-28T21:27:10+00:00",
    "commented_code": "+use std::{any::Any, collections::VecDeque, sync::Arc};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{DataType, GenericStringType, Int32Type, SchemaRef, TimeUnit},\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{Expr, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::TIME_COLUMN_NAME;\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+}\n+\n+/// A two level hashmap storing Database Name -> Table Name -> LastCache\n+///\n+/// There are two lock levels, one at the top and one at the bottom:\n+/// - Top: lock the entire cache for creating new entries\n+/// - Bottom: lock an individual cache for pushing in new data\n+type CacheMap = RwLock<HashMap<String, HashMap<String, RwLock<LastCache>>>>;",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1659349365",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25109,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1659349365",
        "commented_code": "@@ -0,0 +1,498 @@\n+use std::{any::Any, collections::VecDeque, sync::Arc};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{DataType, GenericStringType, Int32Type, SchemaRef, TimeUnit},\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{Expr, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::TIME_COLUMN_NAME;\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+}\n+\n+/// A two level hashmap storing Database Name -> Table Name -> LastCache\n+///\n+/// There are two lock levels, one at the top and one at the bottom:\n+/// - Top: lock the entire cache for creating new entries\n+/// - Bottom: lock an individual cache for pushing in new data\n+type CacheMap = RwLock<HashMap<String, HashMap<String, RwLock<LastCache>>>>;",
        "comment_created_at": "2024-06-28T21:27:10+00:00",
        "comment_author": "pauldix",
        "comment_body": "I think you'd be better off just having a single RwLock around this whole struct. New last caches are only added when a new table or database is created. When writes come in, they're always for multiple databases & tables, so you end up grabbing a bunch of individual locks. Also, the write lock is only obtained on the flush interval for the WAL (which for now is 10ms, but it's going to increase in a later refactoring I have planned).\r\n\r\nYou end up having to do a bunch more locking for writes and for reads with the double lock that won't really pay off.",
        "pr_file_module": null
      },
      {
        "comment_id": "1665991368",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25109,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1659349365",
        "commented_code": "@@ -0,0 +1,498 @@\n+use std::{any::Any, collections::VecDeque, sync::Arc};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{DataType, GenericStringType, Int32Type, SchemaRef, TimeUnit},\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{Expr, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::TIME_COLUMN_NAME;\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+}\n+\n+/// A two level hashmap storing Database Name -> Table Name -> LastCache\n+///\n+/// There are two lock levels, one at the top and one at the bottom:\n+/// - Top: lock the entire cache for creating new entries\n+/// - Bottom: lock an individual cache for pushing in new data\n+type CacheMap = RwLock<HashMap<String, HashMap<String, RwLock<LastCache>>>>;",
        "comment_created_at": "2024-07-04T18:27:31+00:00",
        "comment_author": "hiltontj",
        "comment_body": "I changed it so that there is only one top-level lock, and now it is a three-level hashmap, so that it also stores multiple caches per table.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1580125268",
    "pr_number": 24907,
    "pr_file": "influxdb3_write/src/cache.rs",
    "created_at": "2024-04-25T21:03:03+00:00",
    "commented_code": "+use crate::persister::serialize_to_parquet;\n+use crate::persister::Error;\n+use crate::ParquetFile;\n+use bytes::Bytes;\n+use datafusion::execution::memory_pool::MemoryPool;\n+use datafusion::physical_plan::SendableRecordBatchStream;\n+use object_store::memory::InMemory;\n+use object_store::path::Path as ObjPath;\n+use object_store::ObjectStore;\n+use parking_lot::Mutex;\n+use std::collections::HashMap;\n+use std::sync::Arc;\n+\n+type MetaData = Mutex<HashMap<String, HashMap<String, HashMap<String, ParquetFile>>>>;",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1580125268",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 24907,
        "pr_file": "influxdb3_write/src/cache.rs",
        "discussion_id": "1580125268",
        "commented_code": "@@ -0,0 +1,132 @@\n+use crate::persister::serialize_to_parquet;\n+use crate::persister::Error;\n+use crate::ParquetFile;\n+use bytes::Bytes;\n+use datafusion::execution::memory_pool::MemoryPool;\n+use datafusion::physical_plan::SendableRecordBatchStream;\n+use object_store::memory::InMemory;\n+use object_store::path::Path as ObjPath;\n+use object_store::ObjectStore;\n+use parking_lot::Mutex;\n+use std::collections::HashMap;\n+use std::sync::Arc;\n+\n+type MetaData = Mutex<HashMap<String, HashMap<String, HashMap<String, ParquetFile>>>>;",
        "comment_created_at": "2024-04-25T21:03:03+00:00",
        "comment_author": "pauldix",
        "comment_body": "I'd wrap this in a RWLock instead as queries will hit this a ton, but writes shouldn't be that often.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1581464933",
    "pr_number": 24907,
    "pr_file": "influxdb3_write/src/cache.rs",
    "created_at": "2024-04-26T19:40:20+00:00",
    "commented_code": "+use crate::persister::serialize_to_parquet;\n+use crate::persister::Error;\n+use crate::ParquetFile;\n+use bytes::Bytes;\n+use datafusion::execution::memory_pool::MemoryPool;\n+use datafusion::physical_plan::SendableRecordBatchStream;\n+use object_store::memory::InMemory;\n+use object_store::path::Path as ObjPath;\n+use object_store::ObjectStore;\n+use parking_lot::RwLock;\n+use std::collections::HashMap;\n+use std::sync::Arc;\n+\n+type MetaData = RwLock<HashMap<String, HashMap<String, HashMap<String, ParquetFile>>>>;\n+\n+#[derive(Debug)]\n+pub struct ParquetCache {\n+    object_store: Arc<dyn ObjectStore>,\n+    meta_data: MetaData,\n+    mem_pool: Arc<dyn MemoryPool>,\n+}\n+\n+impl ParquetCache {\n+    /// Create a new `ParquetCache`\n+    pub fn new(mem_pool: &Arc<dyn MemoryPool>) -> Self {\n+        Self {\n+            object_store: Arc::new(InMemory::new()),\n+            meta_data: RwLock::new(HashMap::new()),\n+            mem_pool: Arc::clone(mem_pool),\n+        }\n+    }\n+\n+    /// Get the parquet file metadata for a given database and table\n+    pub fn get_parquet_files(&self, database_name: &str, table_name: &str) -> Vec<ParquetFile> {\n+        self.meta_data\n+            .read()\n+            .get(database_name)\n+            .and_then(|db| db.get(table_name))\n+            .cloned()\n+            .unwrap_or_default()\n+            .into_values()\n+            .collect()\n+    }\n+\n+    /// Persist a new parquet file to the cache or pass an object store path to update a currently\n+    /// existing file in the cache\n+    // Note we want to hold across await points until everything is cleared\n+    // before letting other tasks access the data\n+    #[allow(clippy::await_holding_lock)]\n+    pub async fn persist_parquet_file(\n+        &self,\n+        db_name: &str,\n+        table_name: &str,\n+        min_time: i64,\n+        max_time: i64,\n+        record_batches: SendableRecordBatchStream,\n+        path: Option<ObjPath>,\n+    ) -> Result<(), Error> {\n+        // Lock the data structure until everything is written into the object\n+        // store and metadata\n+        let mut meta_data_lock = self.meta_data.write();",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1581464933",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 24907,
        "pr_file": "influxdb3_write/src/cache.rs",
        "discussion_id": "1581464933",
        "commented_code": "@@ -0,0 +1,393 @@\n+use crate::persister::serialize_to_parquet;\n+use crate::persister::Error;\n+use crate::ParquetFile;\n+use bytes::Bytes;\n+use datafusion::execution::memory_pool::MemoryPool;\n+use datafusion::physical_plan::SendableRecordBatchStream;\n+use object_store::memory::InMemory;\n+use object_store::path::Path as ObjPath;\n+use object_store::ObjectStore;\n+use parking_lot::RwLock;\n+use std::collections::HashMap;\n+use std::sync::Arc;\n+\n+type MetaData = RwLock<HashMap<String, HashMap<String, HashMap<String, ParquetFile>>>>;\n+\n+#[derive(Debug)]\n+pub struct ParquetCache {\n+    object_store: Arc<dyn ObjectStore>,\n+    meta_data: MetaData,\n+    mem_pool: Arc<dyn MemoryPool>,\n+}\n+\n+impl ParquetCache {\n+    /// Create a new `ParquetCache`\n+    pub fn new(mem_pool: &Arc<dyn MemoryPool>) -> Self {\n+        Self {\n+            object_store: Arc::new(InMemory::new()),\n+            meta_data: RwLock::new(HashMap::new()),\n+            mem_pool: Arc::clone(mem_pool),\n+        }\n+    }\n+\n+    /// Get the parquet file metadata for a given database and table\n+    pub fn get_parquet_files(&self, database_name: &str, table_name: &str) -> Vec<ParquetFile> {\n+        self.meta_data\n+            .read()\n+            .get(database_name)\n+            .and_then(|db| db.get(table_name))\n+            .cloned()\n+            .unwrap_or_default()\n+            .into_values()\n+            .collect()\n+    }\n+\n+    /// Persist a new parquet file to the cache or pass an object store path to update a currently\n+    /// existing file in the cache\n+    // Note we want to hold across await points until everything is cleared\n+    // before letting other tasks access the data\n+    #[allow(clippy::await_holding_lock)]\n+    pub async fn persist_parquet_file(\n+        &self,\n+        db_name: &str,\n+        table_name: &str,\n+        min_time: i64,\n+        max_time: i64,\n+        record_batches: SendableRecordBatchStream,\n+        path: Option<ObjPath>,\n+    ) -> Result<(), Error> {\n+        // Lock the data structure until everything is written into the object\n+        // store and metadata\n+        let mut meta_data_lock = self.meta_data.write();",
        "comment_created_at": "2024-04-26T19:40:20+00:00",
        "comment_author": "pauldix",
        "comment_body": "Shouldn't the taking of the metadata lock happen after we've serialized the data? That way, the work happens there without a lock and then we're only holding the lock to actually update the metadata and object store, rather than doing any work.",
        "pr_file_module": null
      },
      {
        "comment_id": "1583407233",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 24907,
        "pr_file": "influxdb3_write/src/cache.rs",
        "discussion_id": "1581464933",
        "commented_code": "@@ -0,0 +1,393 @@\n+use crate::persister::serialize_to_parquet;\n+use crate::persister::Error;\n+use crate::ParquetFile;\n+use bytes::Bytes;\n+use datafusion::execution::memory_pool::MemoryPool;\n+use datafusion::physical_plan::SendableRecordBatchStream;\n+use object_store::memory::InMemory;\n+use object_store::path::Path as ObjPath;\n+use object_store::ObjectStore;\n+use parking_lot::RwLock;\n+use std::collections::HashMap;\n+use std::sync::Arc;\n+\n+type MetaData = RwLock<HashMap<String, HashMap<String, HashMap<String, ParquetFile>>>>;\n+\n+#[derive(Debug)]\n+pub struct ParquetCache {\n+    object_store: Arc<dyn ObjectStore>,\n+    meta_data: MetaData,\n+    mem_pool: Arc<dyn MemoryPool>,\n+}\n+\n+impl ParquetCache {\n+    /// Create a new `ParquetCache`\n+    pub fn new(mem_pool: &Arc<dyn MemoryPool>) -> Self {\n+        Self {\n+            object_store: Arc::new(InMemory::new()),\n+            meta_data: RwLock::new(HashMap::new()),\n+            mem_pool: Arc::clone(mem_pool),\n+        }\n+    }\n+\n+    /// Get the parquet file metadata for a given database and table\n+    pub fn get_parquet_files(&self, database_name: &str, table_name: &str) -> Vec<ParquetFile> {\n+        self.meta_data\n+            .read()\n+            .get(database_name)\n+            .and_then(|db| db.get(table_name))\n+            .cloned()\n+            .unwrap_or_default()\n+            .into_values()\n+            .collect()\n+    }\n+\n+    /// Persist a new parquet file to the cache or pass an object store path to update a currently\n+    /// existing file in the cache\n+    // Note we want to hold across await points until everything is cleared\n+    // before letting other tasks access the data\n+    #[allow(clippy::await_holding_lock)]\n+    pub async fn persist_parquet_file(\n+        &self,\n+        db_name: &str,\n+        table_name: &str,\n+        min_time: i64,\n+        max_time: i64,\n+        record_batches: SendableRecordBatchStream,\n+        path: Option<ObjPath>,\n+    ) -> Result<(), Error> {\n+        // Lock the data structure until everything is written into the object\n+        // store and metadata\n+        let mut meta_data_lock = self.meta_data.write();",
        "comment_created_at": "2024-04-29T16:49:36+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Yeah I think we can afford that for the serializing to parquet, but not for the put and the metadata update. In fact in order to make sure we don't hold this lock across await points and accidentally causing a deadlock I should make the put be blocking",
        "pr_file_module": null
      }
    ]
  }
]