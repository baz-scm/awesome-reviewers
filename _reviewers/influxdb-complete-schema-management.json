[
  {
    "discussion_id": "1732104826",
    "pr_number": 25253,
    "pr_file": "cmd/influx_tools/parquet/exporter.go",
    "created_at": "2024-08-27T04:20:56+00:00",
    "commented_code": "+package parquet\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"sort\"\n+\t\"text/tabwriter\"\n+\t\"time\"\n+\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/internal/storage\"\n+\texport2 \"github.com/influxdata/influxdb/cmd/influx_tools/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/server\"\n+\t\"github.com/influxdata/influxdb/models\"\n+\t\"github.com/influxdata/influxdb/services/meta\"\n+\t\"github.com/influxdata/influxdb/tsdb\"\n+)\n+\n+type exporterConfig struct {\n+\tDatabase      string\n+\tRP            string\n+\tMeasurement   string\n+\tShardDuration time.Duration\n+\tMin, Max      uint64\n+}\n+\n+type exporter struct {\n+\tmetaClient server.MetaClient\n+\ttsdbStore  *tsdb.Store\n+\tstore      *storage.Store\n+\n+\tmin, max     uint64\n+\tdb, rp       string\n+\tm            string\n+\td            time.Duration\n+\tsourceGroups []meta.ShardGroupInfo\n+\ttargetGroups []meta.ShardGroupInfo\n+\n+\tmeasurements measurements\n+\texporter     *export2.Exporter\n+\n+\t// source data time range\n+\tstartDate time.Time\n+\tendDate   time.Time\n+}\n+\n+func newExporter(server server.Interface, cfg *exporterConfig) (*exporter, error) {\n+\tclient := server.MetaClient()\n+\n+\tdbi := client.Database(cfg.Database)\n+\tif dbi == nil {\n+\t\treturn nil, fmt.Errorf(\"database '%s' does not exist\", cfg.Database)\n+\t}\n+\n+\tif cfg.RP == \"\" {\n+\t\t// select default RP\n+\t\tcfg.RP = dbi.DefaultRetentionPolicy\n+\t}\n+\n+\trpi, err := client.RetentionPolicy(cfg.Database, cfg.RP)\n+\tif rpi == nil || err != nil {\n+\t\treturn nil, fmt.Errorf(\"retention policy '%s' does not exist\", cfg.RP)\n+\t}\n+\n+\tstore := tsdb.NewStore(server.TSDBConfig().Dir)\n+\tif server.Logger() != nil {\n+\t\tstore.WithLogger(server.Logger())\n+\t}\n+\tstore.EngineOptions.MonitorDisabled = true\n+\tstore.EngineOptions.CompactionDisabled = true\n+\tstore.EngineOptions.Config = server.TSDBConfig()\n+\tstore.EngineOptions.EngineVersion = server.TSDBConfig().Engine\n+\tstore.EngineOptions.IndexVersion = server.TSDBConfig().Index\n+\tstore.EngineOptions.DatabaseFilter = func(database string) bool {\n+\t\treturn database == cfg.Database\n+\t}\n+\tstore.EngineOptions.RetentionPolicyFilter = func(_, rp string) bool {\n+\t\treturn rp == cfg.RP\n+\t}\n+\tstore.EngineOptions.ShardFilter = func(_, _ string, _ uint64) bool {\n+\t\treturn false\n+\t}\n+\n+\treturn &exporter{\n+\t\tmetaClient: client,\n+\t\ttsdbStore:  store,\n+\t\tstore:      &storage.Store{TSDBStore: store},\n+\t\tmin:        cfg.Min,\n+\t\tmax:        cfg.Max,\n+\t\tdb:         cfg.Database,\n+\t\trp:         cfg.RP,\n+\t\tm:          cfg.Measurement,\n+\t\td:          cfg.ShardDuration,\n+\t}, nil\n+}\n+\n+func (e *exporter) Open() (err error) {\n+\terr = e.tsdbStore.Open()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\terr = e.loadShardGroups()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\te.targetGroups = planShardGroups(e.sourceGroups, e.startDate, e.endDate, e.d)\n+\tif e.max >= uint64(len(e.targetGroups)) {\n+\t\te.max = uint64(len(e.targetGroups) - 1)\n+\t}\n+\tif e.min > e.max {\n+\t\treturn fmt.Errorf(\"invalid shard group range %d to %d\", e.min, e.max)\n+\t}\n+\n+\te.targetGroups = e.targetGroups[e.min : e.max+1]\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) PrintPlan(w io.Writer) {\n+\tfmt.Fprintf(w, \"Source data from: %s -> %s\n\n\", e.startDate, e.endDate)\n+\tfmt.Fprintf(w, \"Converting source from %d shard group(s) to %d shard groups:\n\n\", len(e.sourceGroups), len(e.targetGroups))\n+\te.printShardGroups(w, 0, e.sourceGroups)\n+\tfmt.Fprintln(w)\n+\te.printShardGroups(w, int(e.min), e.targetGroups)\n+}\n+\n+func (e *exporter) printShardGroups(w io.Writer, base int, target []meta.ShardGroupInfo) {\n+\ttw := tabwriter.NewWriter(w, 10, 8, 1, '\\t', 0)\n+\tfmt.Fprintln(tw, \"Seq #\\tID\\tStart\\tEnd\")\n+\tfor i := 0; i < len(target); i++ {\n+\t\tg := target[i]\n+\t\tfmt.Fprintf(tw, \"%d\\t%d\\t%s\\t%s\n\", i+base, g.ID, g.StartTime, g.EndTime)\n+\t}\n+\ttw.Flush()\n+}\n+\n+func (e *exporter) SourceTimeRange() (time.Time, time.Time)  { return e.startDate, e.endDate }\n+func (e *exporter) SourceShardGroups() []meta.ShardGroupInfo { return e.sourceGroups }\n+func (e *exporter) TargetShardGroups() []meta.ShardGroupInfo { return e.targetGroups }\n+\n+func (e *exporter) loadShardGroups() error {\n+\tmin := time.Unix(0, models.MinNanoTime)\n+\tmax := time.Unix(0, models.MaxNanoTime)\n+\n+\tgroups, err := e.metaClient.NodeShardGroupsByTimeRange(e.db, e.rp, min, max)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tif len(groups) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\tsort.Sort(meta.ShardGroupInfos(groups))\n+\te.sourceGroups = groups\n+\te.startDate = groups[0].StartTime\n+\te.endDate = groups[len(groups)-1].EndTime\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) shardsGroupsByTimeRange(min, max time.Time) []meta.ShardGroupInfo {\n+\tgroups := make([]meta.ShardGroupInfo, 0, len(e.sourceGroups))\n+\tfor _, g := range e.sourceGroups {\n+\t\tif !g.Overlaps(min, max) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tgroups = append(groups, g)\n+\t}\n+\treturn groups\n+}\n+\n+func (e *exporter) GatherInfo() error {\n+\te.measurements = newMeasurements()\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\te.gatherSchema(min, max, e.m, rs)\n+\t\trs.Close()\n+\t}\n+\treturn nil\n+}\n+\n+func (e *exporter) WriteTo() error {\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif err := e.writeBucket(min, max, e.m, rs); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\trs.Close()\n+\t\tfmt.Println(\"..\")\n+\t}\n+\tfmt.Println(\".\")\n+\treturn nil\n+}\n+\n+// Read creates a ResultSet that reads all points with a timestamp ts, such that start \u2264 ts < end.\n+func (e *exporter) read(min, max time.Time) (*storage.ResultSet, error) {\n+\tshards, err := e.getShards(min, max)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treq := storage.ReadRequest{\n+\t\tDatabase: e.db,\n+\t\tRP:       e.rp,\n+\t\tShards:   shards,\n+\t\tStart:    min.UnixNano(),\n+\t\tEnd:      max.UnixNano(),\n+\t}\n+\n+\treturn e.store.Read(context.Background(), &req)\n+}\n+\n+func (e *exporter) Close() error {\n+\treturn e.tsdbStore.Close()\n+}\n+\n+func (e *exporter) getShards(min, max time.Time) ([]*tsdb.Shard, error) {\n+\tgroups := e.shardsGroupsByTimeRange(min, max)\n+\tvar ids []uint64\n+\tfor _, g := range groups {\n+\t\tfor _, s := range g.Shards {\n+\t\t\tids = append(ids, s.ID)\n+\t\t}\n+\t}\n+\n+\tshards := e.tsdbStore.Shards(ids)\n+\tif len(shards) == len(ids) {\n+\t\treturn shards, nil\n+\t}\n+\n+\treturn e.openStoreWithShardsIDs(ids)\n+}\n+\n+func (e *exporter) openStoreWithShardsIDs(ids []uint64) ([]*tsdb.Shard, error) {\n+\te.tsdbStore.Close()\n+\te.tsdbStore.EngineOptions.ShardFilter = func(_, _ string, id uint64) bool {\n+\t\tfor i := range ids {\n+\t\t\tif id == ids[i] {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t\treturn false\n+\t}\n+\tif err := e.tsdbStore.Open(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\treturn e.tsdbStore.Shards(ids), nil\n+}\n+\n+func (e *exporter) gatherSchema(start, end time.Time, measurement string, rs *storage.ResultSet) {\n+\tfmt.Printf(\"gather schema start: %s, end: %s\n\", start.Format(time.RFC3339), end.Format(time.RFC3339))\n+\n+\tfor rs.Next() {\n+\t\tmeasurementName := string(models.ParseName(rs.Name()))\n+\t\tif measurement != \"\" && measurement != measurementName {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tt := e.measurements.getTable(measurementName)\n+\t\tt.addTags(rs.Tags())\n+\t\tt.addField(rs.Field(), rs.FieldType())\n+\t}\n+}",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1732104826",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25253,
        "pr_file": "cmd/influx_tools/parquet/exporter.go",
        "discussion_id": "1732104826",
        "commented_code": "@@ -0,0 +1,359 @@\n+package parquet\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"sort\"\n+\t\"text/tabwriter\"\n+\t\"time\"\n+\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/internal/storage\"\n+\texport2 \"github.com/influxdata/influxdb/cmd/influx_tools/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/server\"\n+\t\"github.com/influxdata/influxdb/models\"\n+\t\"github.com/influxdata/influxdb/services/meta\"\n+\t\"github.com/influxdata/influxdb/tsdb\"\n+)\n+\n+type exporterConfig struct {\n+\tDatabase      string\n+\tRP            string\n+\tMeasurement   string\n+\tShardDuration time.Duration\n+\tMin, Max      uint64\n+}\n+\n+type exporter struct {\n+\tmetaClient server.MetaClient\n+\ttsdbStore  *tsdb.Store\n+\tstore      *storage.Store\n+\n+\tmin, max     uint64\n+\tdb, rp       string\n+\tm            string\n+\td            time.Duration\n+\tsourceGroups []meta.ShardGroupInfo\n+\ttargetGroups []meta.ShardGroupInfo\n+\n+\tmeasurements measurements\n+\texporter     *export2.Exporter\n+\n+\t// source data time range\n+\tstartDate time.Time\n+\tendDate   time.Time\n+}\n+\n+func newExporter(server server.Interface, cfg *exporterConfig) (*exporter, error) {\n+\tclient := server.MetaClient()\n+\n+\tdbi := client.Database(cfg.Database)\n+\tif dbi == nil {\n+\t\treturn nil, fmt.Errorf(\"database '%s' does not exist\", cfg.Database)\n+\t}\n+\n+\tif cfg.RP == \"\" {\n+\t\t// select default RP\n+\t\tcfg.RP = dbi.DefaultRetentionPolicy\n+\t}\n+\n+\trpi, err := client.RetentionPolicy(cfg.Database, cfg.RP)\n+\tif rpi == nil || err != nil {\n+\t\treturn nil, fmt.Errorf(\"retention policy '%s' does not exist\", cfg.RP)\n+\t}\n+\n+\tstore := tsdb.NewStore(server.TSDBConfig().Dir)\n+\tif server.Logger() != nil {\n+\t\tstore.WithLogger(server.Logger())\n+\t}\n+\tstore.EngineOptions.MonitorDisabled = true\n+\tstore.EngineOptions.CompactionDisabled = true\n+\tstore.EngineOptions.Config = server.TSDBConfig()\n+\tstore.EngineOptions.EngineVersion = server.TSDBConfig().Engine\n+\tstore.EngineOptions.IndexVersion = server.TSDBConfig().Index\n+\tstore.EngineOptions.DatabaseFilter = func(database string) bool {\n+\t\treturn database == cfg.Database\n+\t}\n+\tstore.EngineOptions.RetentionPolicyFilter = func(_, rp string) bool {\n+\t\treturn rp == cfg.RP\n+\t}\n+\tstore.EngineOptions.ShardFilter = func(_, _ string, _ uint64) bool {\n+\t\treturn false\n+\t}\n+\n+\treturn &exporter{\n+\t\tmetaClient: client,\n+\t\ttsdbStore:  store,\n+\t\tstore:      &storage.Store{TSDBStore: store},\n+\t\tmin:        cfg.Min,\n+\t\tmax:        cfg.Max,\n+\t\tdb:         cfg.Database,\n+\t\trp:         cfg.RP,\n+\t\tm:          cfg.Measurement,\n+\t\td:          cfg.ShardDuration,\n+\t}, nil\n+}\n+\n+func (e *exporter) Open() (err error) {\n+\terr = e.tsdbStore.Open()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\terr = e.loadShardGroups()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\te.targetGroups = planShardGroups(e.sourceGroups, e.startDate, e.endDate, e.d)\n+\tif e.max >= uint64(len(e.targetGroups)) {\n+\t\te.max = uint64(len(e.targetGroups) - 1)\n+\t}\n+\tif e.min > e.max {\n+\t\treturn fmt.Errorf(\"invalid shard group range %d to %d\", e.min, e.max)\n+\t}\n+\n+\te.targetGroups = e.targetGroups[e.min : e.max+1]\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) PrintPlan(w io.Writer) {\n+\tfmt.Fprintf(w, \"Source data from: %s -> %s\\n\\n\", e.startDate, e.endDate)\n+\tfmt.Fprintf(w, \"Converting source from %d shard group(s) to %d shard groups:\\n\\n\", len(e.sourceGroups), len(e.targetGroups))\n+\te.printShardGroups(w, 0, e.sourceGroups)\n+\tfmt.Fprintln(w)\n+\te.printShardGroups(w, int(e.min), e.targetGroups)\n+}\n+\n+func (e *exporter) printShardGroups(w io.Writer, base int, target []meta.ShardGroupInfo) {\n+\ttw := tabwriter.NewWriter(w, 10, 8, 1, '\\t', 0)\n+\tfmt.Fprintln(tw, \"Seq #\\tID\\tStart\\tEnd\")\n+\tfor i := 0; i < len(target); i++ {\n+\t\tg := target[i]\n+\t\tfmt.Fprintf(tw, \"%d\\t%d\\t%s\\t%s\\n\", i+base, g.ID, g.StartTime, g.EndTime)\n+\t}\n+\ttw.Flush()\n+}\n+\n+func (e *exporter) SourceTimeRange() (time.Time, time.Time)  { return e.startDate, e.endDate }\n+func (e *exporter) SourceShardGroups() []meta.ShardGroupInfo { return e.sourceGroups }\n+func (e *exporter) TargetShardGroups() []meta.ShardGroupInfo { return e.targetGroups }\n+\n+func (e *exporter) loadShardGroups() error {\n+\tmin := time.Unix(0, models.MinNanoTime)\n+\tmax := time.Unix(0, models.MaxNanoTime)\n+\n+\tgroups, err := e.metaClient.NodeShardGroupsByTimeRange(e.db, e.rp, min, max)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tif len(groups) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\tsort.Sort(meta.ShardGroupInfos(groups))\n+\te.sourceGroups = groups\n+\te.startDate = groups[0].StartTime\n+\te.endDate = groups[len(groups)-1].EndTime\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) shardsGroupsByTimeRange(min, max time.Time) []meta.ShardGroupInfo {\n+\tgroups := make([]meta.ShardGroupInfo, 0, len(e.sourceGroups))\n+\tfor _, g := range e.sourceGroups {\n+\t\tif !g.Overlaps(min, max) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tgroups = append(groups, g)\n+\t}\n+\treturn groups\n+}\n+\n+func (e *exporter) GatherInfo() error {\n+\te.measurements = newMeasurements()\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\te.gatherSchema(min, max, e.m, rs)\n+\t\trs.Close()\n+\t}\n+\treturn nil\n+}\n+\n+func (e *exporter) WriteTo() error {\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif err := e.writeBucket(min, max, e.m, rs); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\trs.Close()\n+\t\tfmt.Println(\"..\")\n+\t}\n+\tfmt.Println(\".\")\n+\treturn nil\n+}\n+\n+// Read creates a ResultSet that reads all points with a timestamp ts, such that start \u2264 ts < end.\n+func (e *exporter) read(min, max time.Time) (*storage.ResultSet, error) {\n+\tshards, err := e.getShards(min, max)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treq := storage.ReadRequest{\n+\t\tDatabase: e.db,\n+\t\tRP:       e.rp,\n+\t\tShards:   shards,\n+\t\tStart:    min.UnixNano(),\n+\t\tEnd:      max.UnixNano(),\n+\t}\n+\n+\treturn e.store.Read(context.Background(), &req)\n+}\n+\n+func (e *exporter) Close() error {\n+\treturn e.tsdbStore.Close()\n+}\n+\n+func (e *exporter) getShards(min, max time.Time) ([]*tsdb.Shard, error) {\n+\tgroups := e.shardsGroupsByTimeRange(min, max)\n+\tvar ids []uint64\n+\tfor _, g := range groups {\n+\t\tfor _, s := range g.Shards {\n+\t\t\tids = append(ids, s.ID)\n+\t\t}\n+\t}\n+\n+\tshards := e.tsdbStore.Shards(ids)\n+\tif len(shards) == len(ids) {\n+\t\treturn shards, nil\n+\t}\n+\n+\treturn e.openStoreWithShardsIDs(ids)\n+}\n+\n+func (e *exporter) openStoreWithShardsIDs(ids []uint64) ([]*tsdb.Shard, error) {\n+\te.tsdbStore.Close()\n+\te.tsdbStore.EngineOptions.ShardFilter = func(_, _ string, id uint64) bool {\n+\t\tfor i := range ids {\n+\t\t\tif id == ids[i] {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t\treturn false\n+\t}\n+\tif err := e.tsdbStore.Open(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\treturn e.tsdbStore.Shards(ids), nil\n+}\n+\n+func (e *exporter) gatherSchema(start, end time.Time, measurement string, rs *storage.ResultSet) {\n+\tfmt.Printf(\"gather schema start: %s, end: %s\\n\", start.Format(time.RFC3339), end.Format(time.RFC3339))\n+\n+\tfor rs.Next() {\n+\t\tmeasurementName := string(models.ParseName(rs.Name()))\n+\t\tif measurement != \"\" && measurement != measurementName {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tt := e.measurements.getTable(measurementName)\n+\t\tt.addTags(rs.Tags())\n+\t\tt.addField(rs.Field(), rs.FieldType())\n+\t}\n+}",
        "comment_created_at": "2024-08-27T04:20:56+00:00",
        "comment_author": "stuartcarnie",
        "comment_body": "I would recommend you use the following, more efficient approach to gather the schema in InfluxDB 1.x. As mentioned in [this comment](https://github.com/influxdata/influxdb/pull/25047#issuecomment-2264522992), I recommend you process and export each shard separately to 1 or more parquet field, as there can be no schema conflicts _within a single shard_.\r\n\r\nGiven that, you will also be able to gather the complete schema _very efficiently_ using existing indices. \r\n\r\nFor example, the shards returned by your `getShards` function returns a slice of `*tsdb.Shard`. Using that, you can get both the exact set of tag keys for that shard, and the set of fields:\r\n\r\n```go\r\n\tcond := influxql.MustParseExpr(\"_name = '<measurement name>'\")\r\n\tshard := shards[0]\r\n\ttagKeys, err := e.tsdbStore.TagKeys(context.Background(), query.OpenAuthorizer, []uint64{shard.ID()}, cond)\r\n\tfields := shard.MeasurementFields([]byte(\"<measurement name>\"))\r\n```\r\n\r\nYou could merge together the full set of tag keys across all shards to ensure the Parquet schema tag keys are consistent, and also perform a check that the field keys are all consistent data types. I would recommend generating a warning of field conflicts, but I would suggest you still export the individual shards, as the field types will be consistent within the shard.",
        "pr_file_module": null
      },
      {
        "comment_id": "1732667562",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25253,
        "pr_file": "cmd/influx_tools/parquet/exporter.go",
        "discussion_id": "1732104826",
        "commented_code": "@@ -0,0 +1,359 @@\n+package parquet\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"sort\"\n+\t\"text/tabwriter\"\n+\t\"time\"\n+\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/internal/storage\"\n+\texport2 \"github.com/influxdata/influxdb/cmd/influx_tools/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/server\"\n+\t\"github.com/influxdata/influxdb/models\"\n+\t\"github.com/influxdata/influxdb/services/meta\"\n+\t\"github.com/influxdata/influxdb/tsdb\"\n+)\n+\n+type exporterConfig struct {\n+\tDatabase      string\n+\tRP            string\n+\tMeasurement   string\n+\tShardDuration time.Duration\n+\tMin, Max      uint64\n+}\n+\n+type exporter struct {\n+\tmetaClient server.MetaClient\n+\ttsdbStore  *tsdb.Store\n+\tstore      *storage.Store\n+\n+\tmin, max     uint64\n+\tdb, rp       string\n+\tm            string\n+\td            time.Duration\n+\tsourceGroups []meta.ShardGroupInfo\n+\ttargetGroups []meta.ShardGroupInfo\n+\n+\tmeasurements measurements\n+\texporter     *export2.Exporter\n+\n+\t// source data time range\n+\tstartDate time.Time\n+\tendDate   time.Time\n+}\n+\n+func newExporter(server server.Interface, cfg *exporterConfig) (*exporter, error) {\n+\tclient := server.MetaClient()\n+\n+\tdbi := client.Database(cfg.Database)\n+\tif dbi == nil {\n+\t\treturn nil, fmt.Errorf(\"database '%s' does not exist\", cfg.Database)\n+\t}\n+\n+\tif cfg.RP == \"\" {\n+\t\t// select default RP\n+\t\tcfg.RP = dbi.DefaultRetentionPolicy\n+\t}\n+\n+\trpi, err := client.RetentionPolicy(cfg.Database, cfg.RP)\n+\tif rpi == nil || err != nil {\n+\t\treturn nil, fmt.Errorf(\"retention policy '%s' does not exist\", cfg.RP)\n+\t}\n+\n+\tstore := tsdb.NewStore(server.TSDBConfig().Dir)\n+\tif server.Logger() != nil {\n+\t\tstore.WithLogger(server.Logger())\n+\t}\n+\tstore.EngineOptions.MonitorDisabled = true\n+\tstore.EngineOptions.CompactionDisabled = true\n+\tstore.EngineOptions.Config = server.TSDBConfig()\n+\tstore.EngineOptions.EngineVersion = server.TSDBConfig().Engine\n+\tstore.EngineOptions.IndexVersion = server.TSDBConfig().Index\n+\tstore.EngineOptions.DatabaseFilter = func(database string) bool {\n+\t\treturn database == cfg.Database\n+\t}\n+\tstore.EngineOptions.RetentionPolicyFilter = func(_, rp string) bool {\n+\t\treturn rp == cfg.RP\n+\t}\n+\tstore.EngineOptions.ShardFilter = func(_, _ string, _ uint64) bool {\n+\t\treturn false\n+\t}\n+\n+\treturn &exporter{\n+\t\tmetaClient: client,\n+\t\ttsdbStore:  store,\n+\t\tstore:      &storage.Store{TSDBStore: store},\n+\t\tmin:        cfg.Min,\n+\t\tmax:        cfg.Max,\n+\t\tdb:         cfg.Database,\n+\t\trp:         cfg.RP,\n+\t\tm:          cfg.Measurement,\n+\t\td:          cfg.ShardDuration,\n+\t}, nil\n+}\n+\n+func (e *exporter) Open() (err error) {\n+\terr = e.tsdbStore.Open()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\terr = e.loadShardGroups()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\te.targetGroups = planShardGroups(e.sourceGroups, e.startDate, e.endDate, e.d)\n+\tif e.max >= uint64(len(e.targetGroups)) {\n+\t\te.max = uint64(len(e.targetGroups) - 1)\n+\t}\n+\tif e.min > e.max {\n+\t\treturn fmt.Errorf(\"invalid shard group range %d to %d\", e.min, e.max)\n+\t}\n+\n+\te.targetGroups = e.targetGroups[e.min : e.max+1]\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) PrintPlan(w io.Writer) {\n+\tfmt.Fprintf(w, \"Source data from: %s -> %s\\n\\n\", e.startDate, e.endDate)\n+\tfmt.Fprintf(w, \"Converting source from %d shard group(s) to %d shard groups:\\n\\n\", len(e.sourceGroups), len(e.targetGroups))\n+\te.printShardGroups(w, 0, e.sourceGroups)\n+\tfmt.Fprintln(w)\n+\te.printShardGroups(w, int(e.min), e.targetGroups)\n+}\n+\n+func (e *exporter) printShardGroups(w io.Writer, base int, target []meta.ShardGroupInfo) {\n+\ttw := tabwriter.NewWriter(w, 10, 8, 1, '\\t', 0)\n+\tfmt.Fprintln(tw, \"Seq #\\tID\\tStart\\tEnd\")\n+\tfor i := 0; i < len(target); i++ {\n+\t\tg := target[i]\n+\t\tfmt.Fprintf(tw, \"%d\\t%d\\t%s\\t%s\\n\", i+base, g.ID, g.StartTime, g.EndTime)\n+\t}\n+\ttw.Flush()\n+}\n+\n+func (e *exporter) SourceTimeRange() (time.Time, time.Time)  { return e.startDate, e.endDate }\n+func (e *exporter) SourceShardGroups() []meta.ShardGroupInfo { return e.sourceGroups }\n+func (e *exporter) TargetShardGroups() []meta.ShardGroupInfo { return e.targetGroups }\n+\n+func (e *exporter) loadShardGroups() error {\n+\tmin := time.Unix(0, models.MinNanoTime)\n+\tmax := time.Unix(0, models.MaxNanoTime)\n+\n+\tgroups, err := e.metaClient.NodeShardGroupsByTimeRange(e.db, e.rp, min, max)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tif len(groups) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\tsort.Sort(meta.ShardGroupInfos(groups))\n+\te.sourceGroups = groups\n+\te.startDate = groups[0].StartTime\n+\te.endDate = groups[len(groups)-1].EndTime\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) shardsGroupsByTimeRange(min, max time.Time) []meta.ShardGroupInfo {\n+\tgroups := make([]meta.ShardGroupInfo, 0, len(e.sourceGroups))\n+\tfor _, g := range e.sourceGroups {\n+\t\tif !g.Overlaps(min, max) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tgroups = append(groups, g)\n+\t}\n+\treturn groups\n+}\n+\n+func (e *exporter) GatherInfo() error {\n+\te.measurements = newMeasurements()\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\te.gatherSchema(min, max, e.m, rs)\n+\t\trs.Close()\n+\t}\n+\treturn nil\n+}\n+\n+func (e *exporter) WriteTo() error {\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif err := e.writeBucket(min, max, e.m, rs); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\trs.Close()\n+\t\tfmt.Println(\"..\")\n+\t}\n+\tfmt.Println(\".\")\n+\treturn nil\n+}\n+\n+// Read creates a ResultSet that reads all points with a timestamp ts, such that start \u2264 ts < end.\n+func (e *exporter) read(min, max time.Time) (*storage.ResultSet, error) {\n+\tshards, err := e.getShards(min, max)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treq := storage.ReadRequest{\n+\t\tDatabase: e.db,\n+\t\tRP:       e.rp,\n+\t\tShards:   shards,\n+\t\tStart:    min.UnixNano(),\n+\t\tEnd:      max.UnixNano(),\n+\t}\n+\n+\treturn e.store.Read(context.Background(), &req)\n+}\n+\n+func (e *exporter) Close() error {\n+\treturn e.tsdbStore.Close()\n+}\n+\n+func (e *exporter) getShards(min, max time.Time) ([]*tsdb.Shard, error) {\n+\tgroups := e.shardsGroupsByTimeRange(min, max)\n+\tvar ids []uint64\n+\tfor _, g := range groups {\n+\t\tfor _, s := range g.Shards {\n+\t\t\tids = append(ids, s.ID)\n+\t\t}\n+\t}\n+\n+\tshards := e.tsdbStore.Shards(ids)\n+\tif len(shards) == len(ids) {\n+\t\treturn shards, nil\n+\t}\n+\n+\treturn e.openStoreWithShardsIDs(ids)\n+}\n+\n+func (e *exporter) openStoreWithShardsIDs(ids []uint64) ([]*tsdb.Shard, error) {\n+\te.tsdbStore.Close()\n+\te.tsdbStore.EngineOptions.ShardFilter = func(_, _ string, id uint64) bool {\n+\t\tfor i := range ids {\n+\t\t\tif id == ids[i] {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t\treturn false\n+\t}\n+\tif err := e.tsdbStore.Open(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\treturn e.tsdbStore.Shards(ids), nil\n+}\n+\n+func (e *exporter) gatherSchema(start, end time.Time, measurement string, rs *storage.ResultSet) {\n+\tfmt.Printf(\"gather schema start: %s, end: %s\\n\", start.Format(time.RFC3339), end.Format(time.RFC3339))\n+\n+\tfor rs.Next() {\n+\t\tmeasurementName := string(models.ParseName(rs.Name()))\n+\t\tif measurement != \"\" && measurement != measurementName {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tt := e.measurements.getTable(measurementName)\n+\t\tt.addTags(rs.Tags())\n+\t\tt.addField(rs.Field(), rs.FieldType())\n+\t}\n+}",
        "comment_created_at": "2024-08-27T11:30:35+00:00",
        "comment_author": "alespour",
        "comment_body": "Thank you very much for the feedback. I have refactored the code per your suggestion, now individual shards are exported and schema is retrieved using the code above. The export is now single-pass op therefore.\r\nUnfortunately, I still get the same wrong result (incomplete output).",
        "pr_file_module": null
      },
      {
        "comment_id": "1744064479",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25253,
        "pr_file": "cmd/influx_tools/parquet/exporter.go",
        "discussion_id": "1732104826",
        "commented_code": "@@ -0,0 +1,359 @@\n+package parquet\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"sort\"\n+\t\"text/tabwriter\"\n+\t\"time\"\n+\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/internal/storage\"\n+\texport2 \"github.com/influxdata/influxdb/cmd/influx_tools/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/server\"\n+\t\"github.com/influxdata/influxdb/models\"\n+\t\"github.com/influxdata/influxdb/services/meta\"\n+\t\"github.com/influxdata/influxdb/tsdb\"\n+)\n+\n+type exporterConfig struct {\n+\tDatabase      string\n+\tRP            string\n+\tMeasurement   string\n+\tShardDuration time.Duration\n+\tMin, Max      uint64\n+}\n+\n+type exporter struct {\n+\tmetaClient server.MetaClient\n+\ttsdbStore  *tsdb.Store\n+\tstore      *storage.Store\n+\n+\tmin, max     uint64\n+\tdb, rp       string\n+\tm            string\n+\td            time.Duration\n+\tsourceGroups []meta.ShardGroupInfo\n+\ttargetGroups []meta.ShardGroupInfo\n+\n+\tmeasurements measurements\n+\texporter     *export2.Exporter\n+\n+\t// source data time range\n+\tstartDate time.Time\n+\tendDate   time.Time\n+}\n+\n+func newExporter(server server.Interface, cfg *exporterConfig) (*exporter, error) {\n+\tclient := server.MetaClient()\n+\n+\tdbi := client.Database(cfg.Database)\n+\tif dbi == nil {\n+\t\treturn nil, fmt.Errorf(\"database '%s' does not exist\", cfg.Database)\n+\t}\n+\n+\tif cfg.RP == \"\" {\n+\t\t// select default RP\n+\t\tcfg.RP = dbi.DefaultRetentionPolicy\n+\t}\n+\n+\trpi, err := client.RetentionPolicy(cfg.Database, cfg.RP)\n+\tif rpi == nil || err != nil {\n+\t\treturn nil, fmt.Errorf(\"retention policy '%s' does not exist\", cfg.RP)\n+\t}\n+\n+\tstore := tsdb.NewStore(server.TSDBConfig().Dir)\n+\tif server.Logger() != nil {\n+\t\tstore.WithLogger(server.Logger())\n+\t}\n+\tstore.EngineOptions.MonitorDisabled = true\n+\tstore.EngineOptions.CompactionDisabled = true\n+\tstore.EngineOptions.Config = server.TSDBConfig()\n+\tstore.EngineOptions.EngineVersion = server.TSDBConfig().Engine\n+\tstore.EngineOptions.IndexVersion = server.TSDBConfig().Index\n+\tstore.EngineOptions.DatabaseFilter = func(database string) bool {\n+\t\treturn database == cfg.Database\n+\t}\n+\tstore.EngineOptions.RetentionPolicyFilter = func(_, rp string) bool {\n+\t\treturn rp == cfg.RP\n+\t}\n+\tstore.EngineOptions.ShardFilter = func(_, _ string, _ uint64) bool {\n+\t\treturn false\n+\t}\n+\n+\treturn &exporter{\n+\t\tmetaClient: client,\n+\t\ttsdbStore:  store,\n+\t\tstore:      &storage.Store{TSDBStore: store},\n+\t\tmin:        cfg.Min,\n+\t\tmax:        cfg.Max,\n+\t\tdb:         cfg.Database,\n+\t\trp:         cfg.RP,\n+\t\tm:          cfg.Measurement,\n+\t\td:          cfg.ShardDuration,\n+\t}, nil\n+}\n+\n+func (e *exporter) Open() (err error) {\n+\terr = e.tsdbStore.Open()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\terr = e.loadShardGroups()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\te.targetGroups = planShardGroups(e.sourceGroups, e.startDate, e.endDate, e.d)\n+\tif e.max >= uint64(len(e.targetGroups)) {\n+\t\te.max = uint64(len(e.targetGroups) - 1)\n+\t}\n+\tif e.min > e.max {\n+\t\treturn fmt.Errorf(\"invalid shard group range %d to %d\", e.min, e.max)\n+\t}\n+\n+\te.targetGroups = e.targetGroups[e.min : e.max+1]\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) PrintPlan(w io.Writer) {\n+\tfmt.Fprintf(w, \"Source data from: %s -> %s\\n\\n\", e.startDate, e.endDate)\n+\tfmt.Fprintf(w, \"Converting source from %d shard group(s) to %d shard groups:\\n\\n\", len(e.sourceGroups), len(e.targetGroups))\n+\te.printShardGroups(w, 0, e.sourceGroups)\n+\tfmt.Fprintln(w)\n+\te.printShardGroups(w, int(e.min), e.targetGroups)\n+}\n+\n+func (e *exporter) printShardGroups(w io.Writer, base int, target []meta.ShardGroupInfo) {\n+\ttw := tabwriter.NewWriter(w, 10, 8, 1, '\\t', 0)\n+\tfmt.Fprintln(tw, \"Seq #\\tID\\tStart\\tEnd\")\n+\tfor i := 0; i < len(target); i++ {\n+\t\tg := target[i]\n+\t\tfmt.Fprintf(tw, \"%d\\t%d\\t%s\\t%s\\n\", i+base, g.ID, g.StartTime, g.EndTime)\n+\t}\n+\ttw.Flush()\n+}\n+\n+func (e *exporter) SourceTimeRange() (time.Time, time.Time)  { return e.startDate, e.endDate }\n+func (e *exporter) SourceShardGroups() []meta.ShardGroupInfo { return e.sourceGroups }\n+func (e *exporter) TargetShardGroups() []meta.ShardGroupInfo { return e.targetGroups }\n+\n+func (e *exporter) loadShardGroups() error {\n+\tmin := time.Unix(0, models.MinNanoTime)\n+\tmax := time.Unix(0, models.MaxNanoTime)\n+\n+\tgroups, err := e.metaClient.NodeShardGroupsByTimeRange(e.db, e.rp, min, max)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tif len(groups) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\tsort.Sort(meta.ShardGroupInfos(groups))\n+\te.sourceGroups = groups\n+\te.startDate = groups[0].StartTime\n+\te.endDate = groups[len(groups)-1].EndTime\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) shardsGroupsByTimeRange(min, max time.Time) []meta.ShardGroupInfo {\n+\tgroups := make([]meta.ShardGroupInfo, 0, len(e.sourceGroups))\n+\tfor _, g := range e.sourceGroups {\n+\t\tif !g.Overlaps(min, max) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tgroups = append(groups, g)\n+\t}\n+\treturn groups\n+}\n+\n+func (e *exporter) GatherInfo() error {\n+\te.measurements = newMeasurements()\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\te.gatherSchema(min, max, e.m, rs)\n+\t\trs.Close()\n+\t}\n+\treturn nil\n+}\n+\n+func (e *exporter) WriteTo() error {\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif err := e.writeBucket(min, max, e.m, rs); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\trs.Close()\n+\t\tfmt.Println(\"..\")\n+\t}\n+\tfmt.Println(\".\")\n+\treturn nil\n+}\n+\n+// Read creates a ResultSet that reads all points with a timestamp ts, such that start \u2264 ts < end.\n+func (e *exporter) read(min, max time.Time) (*storage.ResultSet, error) {\n+\tshards, err := e.getShards(min, max)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treq := storage.ReadRequest{\n+\t\tDatabase: e.db,\n+\t\tRP:       e.rp,\n+\t\tShards:   shards,\n+\t\tStart:    min.UnixNano(),\n+\t\tEnd:      max.UnixNano(),\n+\t}\n+\n+\treturn e.store.Read(context.Background(), &req)\n+}\n+\n+func (e *exporter) Close() error {\n+\treturn e.tsdbStore.Close()\n+}\n+\n+func (e *exporter) getShards(min, max time.Time) ([]*tsdb.Shard, error) {\n+\tgroups := e.shardsGroupsByTimeRange(min, max)\n+\tvar ids []uint64\n+\tfor _, g := range groups {\n+\t\tfor _, s := range g.Shards {\n+\t\t\tids = append(ids, s.ID)\n+\t\t}\n+\t}\n+\n+\tshards := e.tsdbStore.Shards(ids)\n+\tif len(shards) == len(ids) {\n+\t\treturn shards, nil\n+\t}\n+\n+\treturn e.openStoreWithShardsIDs(ids)\n+}\n+\n+func (e *exporter) openStoreWithShardsIDs(ids []uint64) ([]*tsdb.Shard, error) {\n+\te.tsdbStore.Close()\n+\te.tsdbStore.EngineOptions.ShardFilter = func(_, _ string, id uint64) bool {\n+\t\tfor i := range ids {\n+\t\t\tif id == ids[i] {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t\treturn false\n+\t}\n+\tif err := e.tsdbStore.Open(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\treturn e.tsdbStore.Shards(ids), nil\n+}\n+\n+func (e *exporter) gatherSchema(start, end time.Time, measurement string, rs *storage.ResultSet) {\n+\tfmt.Printf(\"gather schema start: %s, end: %s\\n\", start.Format(time.RFC3339), end.Format(time.RFC3339))\n+\n+\tfor rs.Next() {\n+\t\tmeasurementName := string(models.ParseName(rs.Name()))\n+\t\tif measurement != \"\" && measurement != measurementName {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tt := e.measurements.getTable(measurementName)\n+\t\tt.addTags(rs.Tags())\n+\t\tt.addField(rs.Field(), rs.FieldType())\n+\t}\n+}",
        "comment_created_at": "2024-09-04T16:05:08+00:00",
        "comment_author": "srebhan",
        "comment_body": "@alespour looking [at the code](https://github.com/influxdata/influxdb/blob/3697f752a422e96c38512c7fb36e391b65c58465/cmd/influx_tools/parquet/exporter.go#L186), it seems like you do not create a schema if a measurement has no tags. Even though this might be rare, it's perfectly valid to have a series without _any_ tag... Do you by chance miss data in the output from measurements without tags?",
        "pr_file_module": null
      },
      {
        "comment_id": "1745299527",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25253,
        "pr_file": "cmd/influx_tools/parquet/exporter.go",
        "discussion_id": "1732104826",
        "commented_code": "@@ -0,0 +1,359 @@\n+package parquet\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"sort\"\n+\t\"text/tabwriter\"\n+\t\"time\"\n+\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/internal/storage\"\n+\texport2 \"github.com/influxdata/influxdb/cmd/influx_tools/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/server\"\n+\t\"github.com/influxdata/influxdb/models\"\n+\t\"github.com/influxdata/influxdb/services/meta\"\n+\t\"github.com/influxdata/influxdb/tsdb\"\n+)\n+\n+type exporterConfig struct {\n+\tDatabase      string\n+\tRP            string\n+\tMeasurement   string\n+\tShardDuration time.Duration\n+\tMin, Max      uint64\n+}\n+\n+type exporter struct {\n+\tmetaClient server.MetaClient\n+\ttsdbStore  *tsdb.Store\n+\tstore      *storage.Store\n+\n+\tmin, max     uint64\n+\tdb, rp       string\n+\tm            string\n+\td            time.Duration\n+\tsourceGroups []meta.ShardGroupInfo\n+\ttargetGroups []meta.ShardGroupInfo\n+\n+\tmeasurements measurements\n+\texporter     *export2.Exporter\n+\n+\t// source data time range\n+\tstartDate time.Time\n+\tendDate   time.Time\n+}\n+\n+func newExporter(server server.Interface, cfg *exporterConfig) (*exporter, error) {\n+\tclient := server.MetaClient()\n+\n+\tdbi := client.Database(cfg.Database)\n+\tif dbi == nil {\n+\t\treturn nil, fmt.Errorf(\"database '%s' does not exist\", cfg.Database)\n+\t}\n+\n+\tif cfg.RP == \"\" {\n+\t\t// select default RP\n+\t\tcfg.RP = dbi.DefaultRetentionPolicy\n+\t}\n+\n+\trpi, err := client.RetentionPolicy(cfg.Database, cfg.RP)\n+\tif rpi == nil || err != nil {\n+\t\treturn nil, fmt.Errorf(\"retention policy '%s' does not exist\", cfg.RP)\n+\t}\n+\n+\tstore := tsdb.NewStore(server.TSDBConfig().Dir)\n+\tif server.Logger() != nil {\n+\t\tstore.WithLogger(server.Logger())\n+\t}\n+\tstore.EngineOptions.MonitorDisabled = true\n+\tstore.EngineOptions.CompactionDisabled = true\n+\tstore.EngineOptions.Config = server.TSDBConfig()\n+\tstore.EngineOptions.EngineVersion = server.TSDBConfig().Engine\n+\tstore.EngineOptions.IndexVersion = server.TSDBConfig().Index\n+\tstore.EngineOptions.DatabaseFilter = func(database string) bool {\n+\t\treturn database == cfg.Database\n+\t}\n+\tstore.EngineOptions.RetentionPolicyFilter = func(_, rp string) bool {\n+\t\treturn rp == cfg.RP\n+\t}\n+\tstore.EngineOptions.ShardFilter = func(_, _ string, _ uint64) bool {\n+\t\treturn false\n+\t}\n+\n+\treturn &exporter{\n+\t\tmetaClient: client,\n+\t\ttsdbStore:  store,\n+\t\tstore:      &storage.Store{TSDBStore: store},\n+\t\tmin:        cfg.Min,\n+\t\tmax:        cfg.Max,\n+\t\tdb:         cfg.Database,\n+\t\trp:         cfg.RP,\n+\t\tm:          cfg.Measurement,\n+\t\td:          cfg.ShardDuration,\n+\t}, nil\n+}\n+\n+func (e *exporter) Open() (err error) {\n+\terr = e.tsdbStore.Open()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\terr = e.loadShardGroups()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\te.targetGroups = planShardGroups(e.sourceGroups, e.startDate, e.endDate, e.d)\n+\tif e.max >= uint64(len(e.targetGroups)) {\n+\t\te.max = uint64(len(e.targetGroups) - 1)\n+\t}\n+\tif e.min > e.max {\n+\t\treturn fmt.Errorf(\"invalid shard group range %d to %d\", e.min, e.max)\n+\t}\n+\n+\te.targetGroups = e.targetGroups[e.min : e.max+1]\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) PrintPlan(w io.Writer) {\n+\tfmt.Fprintf(w, \"Source data from: %s -> %s\\n\\n\", e.startDate, e.endDate)\n+\tfmt.Fprintf(w, \"Converting source from %d shard group(s) to %d shard groups:\\n\\n\", len(e.sourceGroups), len(e.targetGroups))\n+\te.printShardGroups(w, 0, e.sourceGroups)\n+\tfmt.Fprintln(w)\n+\te.printShardGroups(w, int(e.min), e.targetGroups)\n+}\n+\n+func (e *exporter) printShardGroups(w io.Writer, base int, target []meta.ShardGroupInfo) {\n+\ttw := tabwriter.NewWriter(w, 10, 8, 1, '\\t', 0)\n+\tfmt.Fprintln(tw, \"Seq #\\tID\\tStart\\tEnd\")\n+\tfor i := 0; i < len(target); i++ {\n+\t\tg := target[i]\n+\t\tfmt.Fprintf(tw, \"%d\\t%d\\t%s\\t%s\\n\", i+base, g.ID, g.StartTime, g.EndTime)\n+\t}\n+\ttw.Flush()\n+}\n+\n+func (e *exporter) SourceTimeRange() (time.Time, time.Time)  { return e.startDate, e.endDate }\n+func (e *exporter) SourceShardGroups() []meta.ShardGroupInfo { return e.sourceGroups }\n+func (e *exporter) TargetShardGroups() []meta.ShardGroupInfo { return e.targetGroups }\n+\n+func (e *exporter) loadShardGroups() error {\n+\tmin := time.Unix(0, models.MinNanoTime)\n+\tmax := time.Unix(0, models.MaxNanoTime)\n+\n+\tgroups, err := e.metaClient.NodeShardGroupsByTimeRange(e.db, e.rp, min, max)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tif len(groups) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\tsort.Sort(meta.ShardGroupInfos(groups))\n+\te.sourceGroups = groups\n+\te.startDate = groups[0].StartTime\n+\te.endDate = groups[len(groups)-1].EndTime\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) shardsGroupsByTimeRange(min, max time.Time) []meta.ShardGroupInfo {\n+\tgroups := make([]meta.ShardGroupInfo, 0, len(e.sourceGroups))\n+\tfor _, g := range e.sourceGroups {\n+\t\tif !g.Overlaps(min, max) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tgroups = append(groups, g)\n+\t}\n+\treturn groups\n+}\n+\n+func (e *exporter) GatherInfo() error {\n+\te.measurements = newMeasurements()\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\te.gatherSchema(min, max, e.m, rs)\n+\t\trs.Close()\n+\t}\n+\treturn nil\n+}\n+\n+func (e *exporter) WriteTo() error {\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif err := e.writeBucket(min, max, e.m, rs); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\trs.Close()\n+\t\tfmt.Println(\"..\")\n+\t}\n+\tfmt.Println(\".\")\n+\treturn nil\n+}\n+\n+// Read creates a ResultSet that reads all points with a timestamp ts, such that start \u2264 ts < end.\n+func (e *exporter) read(min, max time.Time) (*storage.ResultSet, error) {\n+\tshards, err := e.getShards(min, max)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treq := storage.ReadRequest{\n+\t\tDatabase: e.db,\n+\t\tRP:       e.rp,\n+\t\tShards:   shards,\n+\t\tStart:    min.UnixNano(),\n+\t\tEnd:      max.UnixNano(),\n+\t}\n+\n+\treturn e.store.Read(context.Background(), &req)\n+}\n+\n+func (e *exporter) Close() error {\n+\treturn e.tsdbStore.Close()\n+}\n+\n+func (e *exporter) getShards(min, max time.Time) ([]*tsdb.Shard, error) {\n+\tgroups := e.shardsGroupsByTimeRange(min, max)\n+\tvar ids []uint64\n+\tfor _, g := range groups {\n+\t\tfor _, s := range g.Shards {\n+\t\t\tids = append(ids, s.ID)\n+\t\t}\n+\t}\n+\n+\tshards := e.tsdbStore.Shards(ids)\n+\tif len(shards) == len(ids) {\n+\t\treturn shards, nil\n+\t}\n+\n+\treturn e.openStoreWithShardsIDs(ids)\n+}\n+\n+func (e *exporter) openStoreWithShardsIDs(ids []uint64) ([]*tsdb.Shard, error) {\n+\te.tsdbStore.Close()\n+\te.tsdbStore.EngineOptions.ShardFilter = func(_, _ string, id uint64) bool {\n+\t\tfor i := range ids {\n+\t\t\tif id == ids[i] {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t\treturn false\n+\t}\n+\tif err := e.tsdbStore.Open(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\treturn e.tsdbStore.Shards(ids), nil\n+}\n+\n+func (e *exporter) gatherSchema(start, end time.Time, measurement string, rs *storage.ResultSet) {\n+\tfmt.Printf(\"gather schema start: %s, end: %s\\n\", start.Format(time.RFC3339), end.Format(time.RFC3339))\n+\n+\tfor rs.Next() {\n+\t\tmeasurementName := string(models.ParseName(rs.Name()))\n+\t\tif measurement != \"\" && measurement != measurementName {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tt := e.measurements.getTable(measurementName)\n+\t\tt.addTags(rs.Tags())\n+\t\tt.addField(rs.Field(), rs.FieldType())\n+\t}\n+}",
        "comment_created_at": "2024-09-05T11:33:34+00:00",
        "comment_author": "alespour",
        "comment_body": "@srebhan I believe the database and measurement (telegraf, cpu / disk etc) I 'm using for testing does not contain tag-less data.\r\nBut you are right, that needs to be fixed, thank you.",
        "pr_file_module": null
      },
      {
        "comment_id": "1745301880",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25253,
        "pr_file": "cmd/influx_tools/parquet/exporter.go",
        "discussion_id": "1732104826",
        "commented_code": "@@ -0,0 +1,359 @@\n+package parquet\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"sort\"\n+\t\"text/tabwriter\"\n+\t\"time\"\n+\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/internal/storage\"\n+\texport2 \"github.com/influxdata/influxdb/cmd/influx_tools/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_tools/server\"\n+\t\"github.com/influxdata/influxdb/models\"\n+\t\"github.com/influxdata/influxdb/services/meta\"\n+\t\"github.com/influxdata/influxdb/tsdb\"\n+)\n+\n+type exporterConfig struct {\n+\tDatabase      string\n+\tRP            string\n+\tMeasurement   string\n+\tShardDuration time.Duration\n+\tMin, Max      uint64\n+}\n+\n+type exporter struct {\n+\tmetaClient server.MetaClient\n+\ttsdbStore  *tsdb.Store\n+\tstore      *storage.Store\n+\n+\tmin, max     uint64\n+\tdb, rp       string\n+\tm            string\n+\td            time.Duration\n+\tsourceGroups []meta.ShardGroupInfo\n+\ttargetGroups []meta.ShardGroupInfo\n+\n+\tmeasurements measurements\n+\texporter     *export2.Exporter\n+\n+\t// source data time range\n+\tstartDate time.Time\n+\tendDate   time.Time\n+}\n+\n+func newExporter(server server.Interface, cfg *exporterConfig) (*exporter, error) {\n+\tclient := server.MetaClient()\n+\n+\tdbi := client.Database(cfg.Database)\n+\tif dbi == nil {\n+\t\treturn nil, fmt.Errorf(\"database '%s' does not exist\", cfg.Database)\n+\t}\n+\n+\tif cfg.RP == \"\" {\n+\t\t// select default RP\n+\t\tcfg.RP = dbi.DefaultRetentionPolicy\n+\t}\n+\n+\trpi, err := client.RetentionPolicy(cfg.Database, cfg.RP)\n+\tif rpi == nil || err != nil {\n+\t\treturn nil, fmt.Errorf(\"retention policy '%s' does not exist\", cfg.RP)\n+\t}\n+\n+\tstore := tsdb.NewStore(server.TSDBConfig().Dir)\n+\tif server.Logger() != nil {\n+\t\tstore.WithLogger(server.Logger())\n+\t}\n+\tstore.EngineOptions.MonitorDisabled = true\n+\tstore.EngineOptions.CompactionDisabled = true\n+\tstore.EngineOptions.Config = server.TSDBConfig()\n+\tstore.EngineOptions.EngineVersion = server.TSDBConfig().Engine\n+\tstore.EngineOptions.IndexVersion = server.TSDBConfig().Index\n+\tstore.EngineOptions.DatabaseFilter = func(database string) bool {\n+\t\treturn database == cfg.Database\n+\t}\n+\tstore.EngineOptions.RetentionPolicyFilter = func(_, rp string) bool {\n+\t\treturn rp == cfg.RP\n+\t}\n+\tstore.EngineOptions.ShardFilter = func(_, _ string, _ uint64) bool {\n+\t\treturn false\n+\t}\n+\n+\treturn &exporter{\n+\t\tmetaClient: client,\n+\t\ttsdbStore:  store,\n+\t\tstore:      &storage.Store{TSDBStore: store},\n+\t\tmin:        cfg.Min,\n+\t\tmax:        cfg.Max,\n+\t\tdb:         cfg.Database,\n+\t\trp:         cfg.RP,\n+\t\tm:          cfg.Measurement,\n+\t\td:          cfg.ShardDuration,\n+\t}, nil\n+}\n+\n+func (e *exporter) Open() (err error) {\n+\terr = e.tsdbStore.Open()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\terr = e.loadShardGroups()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\te.targetGroups = planShardGroups(e.sourceGroups, e.startDate, e.endDate, e.d)\n+\tif e.max >= uint64(len(e.targetGroups)) {\n+\t\te.max = uint64(len(e.targetGroups) - 1)\n+\t}\n+\tif e.min > e.max {\n+\t\treturn fmt.Errorf(\"invalid shard group range %d to %d\", e.min, e.max)\n+\t}\n+\n+\te.targetGroups = e.targetGroups[e.min : e.max+1]\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) PrintPlan(w io.Writer) {\n+\tfmt.Fprintf(w, \"Source data from: %s -> %s\\n\\n\", e.startDate, e.endDate)\n+\tfmt.Fprintf(w, \"Converting source from %d shard group(s) to %d shard groups:\\n\\n\", len(e.sourceGroups), len(e.targetGroups))\n+\te.printShardGroups(w, 0, e.sourceGroups)\n+\tfmt.Fprintln(w)\n+\te.printShardGroups(w, int(e.min), e.targetGroups)\n+}\n+\n+func (e *exporter) printShardGroups(w io.Writer, base int, target []meta.ShardGroupInfo) {\n+\ttw := tabwriter.NewWriter(w, 10, 8, 1, '\\t', 0)\n+\tfmt.Fprintln(tw, \"Seq #\\tID\\tStart\\tEnd\")\n+\tfor i := 0; i < len(target); i++ {\n+\t\tg := target[i]\n+\t\tfmt.Fprintf(tw, \"%d\\t%d\\t%s\\t%s\\n\", i+base, g.ID, g.StartTime, g.EndTime)\n+\t}\n+\ttw.Flush()\n+}\n+\n+func (e *exporter) SourceTimeRange() (time.Time, time.Time)  { return e.startDate, e.endDate }\n+func (e *exporter) SourceShardGroups() []meta.ShardGroupInfo { return e.sourceGroups }\n+func (e *exporter) TargetShardGroups() []meta.ShardGroupInfo { return e.targetGroups }\n+\n+func (e *exporter) loadShardGroups() error {\n+\tmin := time.Unix(0, models.MinNanoTime)\n+\tmax := time.Unix(0, models.MaxNanoTime)\n+\n+\tgroups, err := e.metaClient.NodeShardGroupsByTimeRange(e.db, e.rp, min, max)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tif len(groups) == 0 {\n+\t\treturn nil\n+\t}\n+\n+\tsort.Sort(meta.ShardGroupInfos(groups))\n+\te.sourceGroups = groups\n+\te.startDate = groups[0].StartTime\n+\te.endDate = groups[len(groups)-1].EndTime\n+\n+\treturn nil\n+}\n+\n+func (e *exporter) shardsGroupsByTimeRange(min, max time.Time) []meta.ShardGroupInfo {\n+\tgroups := make([]meta.ShardGroupInfo, 0, len(e.sourceGroups))\n+\tfor _, g := range e.sourceGroups {\n+\t\tif !g.Overlaps(min, max) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tgroups = append(groups, g)\n+\t}\n+\treturn groups\n+}\n+\n+func (e *exporter) GatherInfo() error {\n+\te.measurements = newMeasurements()\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\te.gatherSchema(min, max, e.m, rs)\n+\t\trs.Close()\n+\t}\n+\treturn nil\n+}\n+\n+func (e *exporter) WriteTo() error {\n+\tfor _, g := range e.targetGroups {\n+\t\tmin, max := g.StartTime, g.EndTime\n+\t\trs, err := e.read(min, max.Add(-1))\n+\t\tif err != nil || rs == nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif err := e.writeBucket(min, max, e.m, rs); err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\trs.Close()\n+\t\tfmt.Println(\"..\")\n+\t}\n+\tfmt.Println(\".\")\n+\treturn nil\n+}\n+\n+// Read creates a ResultSet that reads all points with a timestamp ts, such that start \u2264 ts < end.\n+func (e *exporter) read(min, max time.Time) (*storage.ResultSet, error) {\n+\tshards, err := e.getShards(min, max)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\treq := storage.ReadRequest{\n+\t\tDatabase: e.db,\n+\t\tRP:       e.rp,\n+\t\tShards:   shards,\n+\t\tStart:    min.UnixNano(),\n+\t\tEnd:      max.UnixNano(),\n+\t}\n+\n+\treturn e.store.Read(context.Background(), &req)\n+}\n+\n+func (e *exporter) Close() error {\n+\treturn e.tsdbStore.Close()\n+}\n+\n+func (e *exporter) getShards(min, max time.Time) ([]*tsdb.Shard, error) {\n+\tgroups := e.shardsGroupsByTimeRange(min, max)\n+\tvar ids []uint64\n+\tfor _, g := range groups {\n+\t\tfor _, s := range g.Shards {\n+\t\t\tids = append(ids, s.ID)\n+\t\t}\n+\t}\n+\n+\tshards := e.tsdbStore.Shards(ids)\n+\tif len(shards) == len(ids) {\n+\t\treturn shards, nil\n+\t}\n+\n+\treturn e.openStoreWithShardsIDs(ids)\n+}\n+\n+func (e *exporter) openStoreWithShardsIDs(ids []uint64) ([]*tsdb.Shard, error) {\n+\te.tsdbStore.Close()\n+\te.tsdbStore.EngineOptions.ShardFilter = func(_, _ string, id uint64) bool {\n+\t\tfor i := range ids {\n+\t\t\tif id == ids[i] {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t\treturn false\n+\t}\n+\tif err := e.tsdbStore.Open(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\treturn e.tsdbStore.Shards(ids), nil\n+}\n+\n+func (e *exporter) gatherSchema(start, end time.Time, measurement string, rs *storage.ResultSet) {\n+\tfmt.Printf(\"gather schema start: %s, end: %s\\n\", start.Format(time.RFC3339), end.Format(time.RFC3339))\n+\n+\tfor rs.Next() {\n+\t\tmeasurementName := string(models.ParseName(rs.Name()))\n+\t\tif measurement != \"\" && measurement != measurementName {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tt := e.measurements.getTable(measurementName)\n+\t\tt.addTags(rs.Tags())\n+\t\tt.addField(rs.Field(), rs.FieldType())\n+\t}\n+}",
        "comment_created_at": "2024-09-05T11:35:40+00:00",
        "comment_author": "alespour",
        "comment_body": "I'm using telegraf to check parquet output like this:\r\n`telegraf --config ./telegraf-parquet.conf --once`\r\n```\r\n[[inputs.file]]\r\n  files = [\"/tmp/parquet/table-*.parquet\"]\r\n  name_override = \"cpu\"\r\n  data_format = \"parquet\"\r\n  tag_columns = [\"datacenter\",\"hostname\",\"os\",\"rack\",\"region\",\"service\",\"team\"]\r\n  timestamp_column = \"time\"\r\n  timestamp_format = \"unix_ns\"\r\n\r\n[[outputs.file]]\r\n  ## Files to write to, \"stdout\" is a specially handled file.\r\n  files = [\"stdout\"]\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1689057927",
    "pr_number": 25047,
    "pr_file": "cmd/influx_inspect/export/export_parquet.go",
    "created_at": "2024-07-24T03:29:28+00:00",
    "commented_code": "+package export\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"os\"\n+\t\"path/filepath\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/apache/arrow/go/v16/parquet\"\n+\t\"github.com/apache/arrow/go/v16/parquet/compress\"\n+\tpqexport \"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/index\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/models\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/resultset\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/table\"\n+\ttsdb \"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/tsm1\"\n+\t\"github.com/influxdata/influxdb/tsdb/engine/tsm1\"\n+\t\"go.uber.org/zap\"\n+\t\"golang.org/x/exp/maps\"\n+)\n+\n+//\n+// Export to Parquet file(s) is done per each TSM file. The files are apparently not sorted.\n+// Therefore, neither are output files. So for example `table-00001.parquet` may contain older data\n+// than `table-00000.parquet`.\n+// If the data should be sorted then\n+//  1. writeValuesParquet must collect multiple chunks of field values, and they should be sorted later\n+//  2. exporter must call exportDone after all files were processed\n+//\n+\n+// sequence is used for the suffix of generated parquet files\n+var sequence int\n+\n+// vc is the key:field:values collection of exported values\n+var vc map[string]map[string][]tsm1.Value\n+\n+func (cmd *Command) writeValuesParquet(_ io.Writer, seriesKey []byte, field string, values []tsm1.Value) error {\n+\tif vc == nil {\n+\t\tvc = make(map[string]map[string][]tsm1.Value)\n+\t}\n+\n+\tkey := string(seriesKey)\n+\tfields, ok := vc[key]\n+\tif !ok {\n+\t\tfields = make(map[string][]tsm1.Value)\n+\t\tvc[key] = fields\n+\t}\n+\tfields[field] = values\n+\treturn nil\n+}\n+\n+func (cmd *Command) exportDoneParquet(_ string) error {\n+\tdefer func() {\n+\t\tvc = nil\n+\t}()\n+\n+\tvar schema *index.MeasurementSchema\n+\n+\tfor key, fields := range vc {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1689057927",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25047,
        "pr_file": "cmd/influx_inspect/export/export_parquet.go",
        "discussion_id": "1689057927",
        "commented_code": "@@ -0,0 +1,348 @@\n+package export\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"os\"\n+\t\"path/filepath\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/apache/arrow/go/v16/parquet\"\n+\t\"github.com/apache/arrow/go/v16/parquet/compress\"\n+\tpqexport \"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/index\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/models\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/resultset\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/table\"\n+\ttsdb \"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/tsm1\"\n+\t\"github.com/influxdata/influxdb/tsdb/engine/tsm1\"\n+\t\"go.uber.org/zap\"\n+\t\"golang.org/x/exp/maps\"\n+)\n+\n+//\n+// Export to Parquet file(s) is done per each TSM file. The files are apparently not sorted.\n+// Therefore, neither are output files. So for example `table-00001.parquet` may contain older data\n+// than `table-00000.parquet`.\n+// If the data should be sorted then\n+//  1. writeValuesParquet must collect multiple chunks of field values, and they should be sorted later\n+//  2. exporter must call exportDone after all files were processed\n+//\n+\n+// sequence is used for the suffix of generated parquet files\n+var sequence int\n+\n+// vc is the key:field:values collection of exported values\n+var vc map[string]map[string][]tsm1.Value\n+\n+func (cmd *Command) writeValuesParquet(_ io.Writer, seriesKey []byte, field string, values []tsm1.Value) error {\n+\tif vc == nil {\n+\t\tvc = make(map[string]map[string][]tsm1.Value)\n+\t}\n+\n+\tkey := string(seriesKey)\n+\tfields, ok := vc[key]\n+\tif !ok {\n+\t\tfields = make(map[string][]tsm1.Value)\n+\t\tvc[key] = fields\n+\t}\n+\tfields[field] = values\n+\treturn nil\n+}\n+\n+func (cmd *Command) exportDoneParquet(_ string) error {\n+\tdefer func() {\n+\t\tvc = nil\n+\t}()\n+\n+\tvar schema *index.MeasurementSchema\n+\n+\tfor key, fields := range vc {",
        "comment_created_at": "2024-07-24T03:29:28+00:00",
        "comment_author": "stuartcarnie",
        "comment_body": "This loop does not produce a valid schema, if the user inserts data with mixed tag sets.\r\n\r\nUsing `influx` to create some test data:\r\n\r\n```\r\n> create database mixed_schema\r\n> use mixed_schema\r\nUsing database mixed_schema\r\n> insert test,tag0=val0 fieldF=1.2\r\n> insert test,tag0=val0,tag1=val0 fieldF=3.1\r\n```\r\n\r\nAnd then running the command to export some data for the `test` measurement[^1]. We can view the data using `duckdb`:\r\n\r\n```sh\r\nduckdb -box -s \"from 'parquet/*.parquet' select *\"\r\n```\r\n```\r\ntime                        tag0  fieldF\r\n--------------------------  ----  ------\r\n2024-07-24 02:57:41.285306        1.2\r\n2024-07-24 02:57:45.308777        3.1\r\n```\r\n\r\n> [!NOTE]\r\n>\r\n> The `tag1` column is missing.\r\n>\r\n> We'll address the missing data in the `tag0` column separately.\r\n\r\nTo produce the correct schema, all series keys must be enumerated and the tag keys merged. An example implementation is the `KeyMerger` type:\r\n\r\nhttps://github.com/influxdata/influxdb/blob/636a27e77f00f0faeff186c8b6b005d79da7c424/storage/reads/keymerger.go#L10-L15\r\n\r\n[^1]: ranging over a map produces keys in a non-deterministic order, so you may need to run the export multiple times to achieve the result above",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1690549352",
    "pr_number": 25047,
    "pr_file": "cmd/influx_inspect/export/export_parquet.go",
    "created_at": "2024-07-24T22:59:27+00:00",
    "commented_code": "TagSet:   tagSet,\n \t\t\tFieldSet: fieldSet,\n \t\t}\n-\t\t// schema does not change in a table\n+\t\t// schema does not change in a table in one tsm file",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1690549352",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25047,
        "pr_file": "cmd/influx_inspect/export/export_parquet.go",
        "discussion_id": "1690549352",
        "commented_code": "@@ -87,7 +91,7 @@ func (cmd *Command) exportDoneParquet(_ string) error {\n \t\t\tTagSet:   tagSet,\n \t\t\tFieldSet: fieldSet,\n \t\t}\n-\t\t// schema does not change in a table\n+\t\t// schema does not change in a table in one tsm file",
        "comment_created_at": "2024-07-24T22:59:27+00:00",
        "comment_author": "stuartcarnie",
        "comment_body": "The tag set schema can change within a single TSM file from one series key to the next.\r\n\r\nIf a user writes the following point: \r\n\r\n```\r\nm0,tag0=val0 f0=1.3\r\n```\r\n\r\nThe schema is for the previous line is:\r\n\r\n| col | type |\r\n| :--- | :--- |\r\n| tag0 | string (tag) |\r\n| f0 | float (field) |\r\n\r\nIf the next write is:\r\n\r\n```\r\nm0,tag1=val0,tag2=val1 f1=false\r\n```\r\n\r\nThe schema for that line is:\r\n\r\n| col | type |\r\n| :--- | :--- |\r\n| tag1 | string (tag) |\r\n| tag2 | string (tag) |\r\n| f1 | bool (field) |\r\n\r\nTherefore, the schema must be the union of all series keys, resulting in a table schema of:\r\n\r\n| col | type |\r\n| :--- | :--- |\r\n| tag0 | string (tag) |\r\n| tag1 | string (tag) |\r\n| tag2 | string (tag) |\r\n| f0 | float (field) |\r\n| f1 | bool (field) |\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1691092184",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25047,
        "pr_file": "cmd/influx_inspect/export/export_parquet.go",
        "discussion_id": "1690549352",
        "commented_code": "@@ -87,7 +91,7 @@ func (cmd *Command) exportDoneParquet(_ string) error {\n \t\t\tTagSet:   tagSet,\n \t\t\tFieldSet: fieldSet,\n \t\t}\n-\t\t// schema does not change in a table\n+\t\t// schema does not change in a table in one tsm file",
        "comment_created_at": "2024-07-25T08:56:10+00:00",
        "comment_author": "alespour",
        "comment_body": "I assume then the export have to iterate over twice TSM files. In the first iteration, complete tables schema would be gathered, and in the seconds iteration the actual data exported, correct?",
        "pr_file_module": null
      }
    ]
  }
]