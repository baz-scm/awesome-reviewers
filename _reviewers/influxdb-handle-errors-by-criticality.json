[
  {
    "discussion_id": "1699083978",
    "pr_number": 25196,
    "pr_file": "influxdb3_wal/src/object_store.rs",
    "created_at": "2024-07-31T20:43:40+00:00",
    "commented_code": "+use crate::serialize::verify_file_type_and_deserialize;\n+use crate::snapshot_tracker::{SnapshotInfo, SnapshotTracker, WalPeriod};\n+use crate::{\n+    CatalogBatch, SnapshotDetails, Wal, WalConfig, WalContents, WalFileNotifier,\n+    WalFileSequenceNumber, WalOp, WriteBatch,\n+};\n+use bytes::Bytes;\n+use data_types::Timestamp;\n+use futures_util::stream::StreamExt;\n+use hashbrown::HashMap;\n+use object_store::path::Path;\n+use object_store::{ObjectStore, PutPayload};\n+use observability_deps::tracing::error;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::Mutex;\n+use tokio::sync::{oneshot, OwnedSemaphorePermit, Semaphore};\n+\n+#[derive(Debug)]\n+pub struct WalObjectStore {\n+    object_store: Arc<dyn ObjectStore>,\n+    file_notifier: Arc<dyn WalFileNotifier>,\n+    /// Buffered wal ops go in here along with the state to track when to snapshot\n+    flush_buffer: Mutex<FlushBuffer>,\n+}\n+\n+impl WalObjectStore {\n+    /// Creates a new WAL. Note that you will have to call replay and then start the flusher\n+    /// to fully initialize the WAL.\n+    pub fn new(\n+        object_store: Arc<dyn ObjectStore>,\n+        file_notifier: Arc<dyn WalFileNotifier>,\n+        config: WalConfig,\n+    ) -> Self {\n+        Self {\n+            object_store,\n+            file_notifier,\n+            flush_buffer: Mutex::new(FlushBuffer::new(\n+                WalBuffer {\n+                    is_shutdown: false,\n+                    wal_file_sequence_number: Default::default(),\n+                    op_limit: config.max_write_buffer_size,\n+                    op_count: 0,\n+                    database_to_write_batch: Default::default(),\n+                    catalog_batches: vec![],\n+                    write_op_responses: vec![],\n+                },\n+                SnapshotTracker::new(config.snapshot_size, config.level_0_duration),\n+            )),\n+        }\n+    }\n+\n+    /// Loads the WAL files in order from object store, calling the file notifier on each one and\n+    /// populating the snapshot tracker with the WAL periods.\n+    pub async fn replay(&self) -> crate::Result<()> {\n+        let paths = self.load_existing_wal_file_paths().await?;\n+\n+        for path in paths {\n+            let file_bytes = self.object_store.get(&path).await?.bytes().await?;\n+            let wal_contents = verify_file_type_and_deserialize(file_bytes)?;\n+\n+            // add this to the snapshot tracker so we know what to clear out when\n+            self.flush_buffer\n+                .lock()\n+                .await\n+                .snapshot_tracker\n+                .add_wal_period(WalPeriod::new(\n+                    wal_contents.wal_file_number,\n+                    Timestamp::new(wal_contents.min_timestamp_ns),\n+                    Timestamp::new(wal_contents.max_timestamp_ns),\n+                ));\n+\n+            match wal_contents.snapshot {\n+                None => self.file_notifier.notify(wal_contents),\n+                Some(snapshot_details) => {\n+                    let snapshot_info = {\n+                        let mut buffer = self.flush_buffer.lock().await;\n+\n+                        match buffer.snapshot_tracker.snapshot() {\n+                            None => None,\n+                            Some(info) => {\n+                                let semaphore = Arc::clone(&buffer.snapshot_semaphore);\n+                                let permit = semaphore.acquire_owned().await.unwrap();\n+\n+                                Some((info, permit))\n+                            }\n+                        }\n+                    };\n+\n+                    let snapshot_done = self\n+                        .file_notifier\n+                        .notify_and_snapshot(wal_contents, snapshot_details)\n+                        .await;\n+                    let details = snapshot_done.await.unwrap();\n+                    assert_eq!(snapshot_details, details);",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1699083978",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_wal/src/object_store.rs",
        "discussion_id": "1699083978",
        "commented_code": "@@ -0,0 +1,907 @@\n+use crate::serialize::verify_file_type_and_deserialize;\n+use crate::snapshot_tracker::{SnapshotInfo, SnapshotTracker, WalPeriod};\n+use crate::{\n+    CatalogBatch, SnapshotDetails, Wal, WalConfig, WalContents, WalFileNotifier,\n+    WalFileSequenceNumber, WalOp, WriteBatch,\n+};\n+use bytes::Bytes;\n+use data_types::Timestamp;\n+use futures_util::stream::StreamExt;\n+use hashbrown::HashMap;\n+use object_store::path::Path;\n+use object_store::{ObjectStore, PutPayload};\n+use observability_deps::tracing::error;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::Mutex;\n+use tokio::sync::{oneshot, OwnedSemaphorePermit, Semaphore};\n+\n+#[derive(Debug)]\n+pub struct WalObjectStore {\n+    object_store: Arc<dyn ObjectStore>,\n+    file_notifier: Arc<dyn WalFileNotifier>,\n+    /// Buffered wal ops go in here along with the state to track when to snapshot\n+    flush_buffer: Mutex<FlushBuffer>,\n+}\n+\n+impl WalObjectStore {\n+    /// Creates a new WAL. Note that you will have to call replay and then start the flusher\n+    /// to fully initialize the WAL.\n+    pub fn new(\n+        object_store: Arc<dyn ObjectStore>,\n+        file_notifier: Arc<dyn WalFileNotifier>,\n+        config: WalConfig,\n+    ) -> Self {\n+        Self {\n+            object_store,\n+            file_notifier,\n+            flush_buffer: Mutex::new(FlushBuffer::new(\n+                WalBuffer {\n+                    is_shutdown: false,\n+                    wal_file_sequence_number: Default::default(),\n+                    op_limit: config.max_write_buffer_size,\n+                    op_count: 0,\n+                    database_to_write_batch: Default::default(),\n+                    catalog_batches: vec![],\n+                    write_op_responses: vec![],\n+                },\n+                SnapshotTracker::new(config.snapshot_size, config.level_0_duration),\n+            )),\n+        }\n+    }\n+\n+    /// Loads the WAL files in order from object store, calling the file notifier on each one and\n+    /// populating the snapshot tracker with the WAL periods.\n+    pub async fn replay(&self) -> crate::Result<()> {\n+        let paths = self.load_existing_wal_file_paths().await?;\n+\n+        for path in paths {\n+            let file_bytes = self.object_store.get(&path).await?.bytes().await?;\n+            let wal_contents = verify_file_type_and_deserialize(file_bytes)?;\n+\n+            // add this to the snapshot tracker so we know what to clear out when\n+            self.flush_buffer\n+                .lock()\n+                .await\n+                .snapshot_tracker\n+                .add_wal_period(WalPeriod::new(\n+                    wal_contents.wal_file_number,\n+                    Timestamp::new(wal_contents.min_timestamp_ns),\n+                    Timestamp::new(wal_contents.max_timestamp_ns),\n+                ));\n+\n+            match wal_contents.snapshot {\n+                None => self.file_notifier.notify(wal_contents),\n+                Some(snapshot_details) => {\n+                    let snapshot_info = {\n+                        let mut buffer = self.flush_buffer.lock().await;\n+\n+                        match buffer.snapshot_tracker.snapshot() {\n+                            None => None,\n+                            Some(info) => {\n+                                let semaphore = Arc::clone(&buffer.snapshot_semaphore);\n+                                let permit = semaphore.acquire_owned().await.unwrap();\n+\n+                                Some((info, permit))\n+                            }\n+                        }\n+                    };\n+\n+                    let snapshot_done = self\n+                        .file_notifier\n+                        .notify_and_snapshot(wal_contents, snapshot_details)\n+                        .await;\n+                    let details = snapshot_done.await.unwrap();\n+                    assert_eq!(snapshot_details, details);",
        "comment_created_at": "2024-07-31T20:43:40+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Do we want this to be here to crash the system if they don't line up? If not then `debug_assert_eq` would be better.",
        "pr_file_module": null
      },
      {
        "comment_id": "1699160159",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_wal/src/object_store.rs",
        "discussion_id": "1699083978",
        "commented_code": "@@ -0,0 +1,907 @@\n+use crate::serialize::verify_file_type_and_deserialize;\n+use crate::snapshot_tracker::{SnapshotInfo, SnapshotTracker, WalPeriod};\n+use crate::{\n+    CatalogBatch, SnapshotDetails, Wal, WalConfig, WalContents, WalFileNotifier,\n+    WalFileSequenceNumber, WalOp, WriteBatch,\n+};\n+use bytes::Bytes;\n+use data_types::Timestamp;\n+use futures_util::stream::StreamExt;\n+use hashbrown::HashMap;\n+use object_store::path::Path;\n+use object_store::{ObjectStore, PutPayload};\n+use observability_deps::tracing::error;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::Mutex;\n+use tokio::sync::{oneshot, OwnedSemaphorePermit, Semaphore};\n+\n+#[derive(Debug)]\n+pub struct WalObjectStore {\n+    object_store: Arc<dyn ObjectStore>,\n+    file_notifier: Arc<dyn WalFileNotifier>,\n+    /// Buffered wal ops go in here along with the state to track when to snapshot\n+    flush_buffer: Mutex<FlushBuffer>,\n+}\n+\n+impl WalObjectStore {\n+    /// Creates a new WAL. Note that you will have to call replay and then start the flusher\n+    /// to fully initialize the WAL.\n+    pub fn new(\n+        object_store: Arc<dyn ObjectStore>,\n+        file_notifier: Arc<dyn WalFileNotifier>,\n+        config: WalConfig,\n+    ) -> Self {\n+        Self {\n+            object_store,\n+            file_notifier,\n+            flush_buffer: Mutex::new(FlushBuffer::new(\n+                WalBuffer {\n+                    is_shutdown: false,\n+                    wal_file_sequence_number: Default::default(),\n+                    op_limit: config.max_write_buffer_size,\n+                    op_count: 0,\n+                    database_to_write_batch: Default::default(),\n+                    catalog_batches: vec![],\n+                    write_op_responses: vec![],\n+                },\n+                SnapshotTracker::new(config.snapshot_size, config.level_0_duration),\n+            )),\n+        }\n+    }\n+\n+    /// Loads the WAL files in order from object store, calling the file notifier on each one and\n+    /// populating the snapshot tracker with the WAL periods.\n+    pub async fn replay(&self) -> crate::Result<()> {\n+        let paths = self.load_existing_wal_file_paths().await?;\n+\n+        for path in paths {\n+            let file_bytes = self.object_store.get(&path).await?.bytes().await?;\n+            let wal_contents = verify_file_type_and_deserialize(file_bytes)?;\n+\n+            // add this to the snapshot tracker so we know what to clear out when\n+            self.flush_buffer\n+                .lock()\n+                .await\n+                .snapshot_tracker\n+                .add_wal_period(WalPeriod::new(\n+                    wal_contents.wal_file_number,\n+                    Timestamp::new(wal_contents.min_timestamp_ns),\n+                    Timestamp::new(wal_contents.max_timestamp_ns),\n+                ));\n+\n+            match wal_contents.snapshot {\n+                None => self.file_notifier.notify(wal_contents),\n+                Some(snapshot_details) => {\n+                    let snapshot_info = {\n+                        let mut buffer = self.flush_buffer.lock().await;\n+\n+                        match buffer.snapshot_tracker.snapshot() {\n+                            None => None,\n+                            Some(info) => {\n+                                let semaphore = Arc::clone(&buffer.snapshot_semaphore);\n+                                let permit = semaphore.acquire_owned().await.unwrap();\n+\n+                                Some((info, permit))\n+                            }\n+                        }\n+                    };\n+\n+                    let snapshot_done = self\n+                        .file_notifier\n+                        .notify_and_snapshot(wal_contents, snapshot_details)\n+                        .await;\n+                    let details = snapshot_done.await.unwrap();\n+                    assert_eq!(snapshot_details, details);",
        "comment_created_at": "2024-07-31T22:12:34+00:00",
        "comment_author": "pauldix",
        "comment_body": "Good call to change to debug 👍 ",
        "pr_file_module": null
      },
      {
        "comment_id": "1699162576",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_wal/src/object_store.rs",
        "discussion_id": "1699083978",
        "commented_code": "@@ -0,0 +1,907 @@\n+use crate::serialize::verify_file_type_and_deserialize;\n+use crate::snapshot_tracker::{SnapshotInfo, SnapshotTracker, WalPeriod};\n+use crate::{\n+    CatalogBatch, SnapshotDetails, Wal, WalConfig, WalContents, WalFileNotifier,\n+    WalFileSequenceNumber, WalOp, WriteBatch,\n+};\n+use bytes::Bytes;\n+use data_types::Timestamp;\n+use futures_util::stream::StreamExt;\n+use hashbrown::HashMap;\n+use object_store::path::Path;\n+use object_store::{ObjectStore, PutPayload};\n+use observability_deps::tracing::error;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::Mutex;\n+use tokio::sync::{oneshot, OwnedSemaphorePermit, Semaphore};\n+\n+#[derive(Debug)]\n+pub struct WalObjectStore {\n+    object_store: Arc<dyn ObjectStore>,\n+    file_notifier: Arc<dyn WalFileNotifier>,\n+    /// Buffered wal ops go in here along with the state to track when to snapshot\n+    flush_buffer: Mutex<FlushBuffer>,\n+}\n+\n+impl WalObjectStore {\n+    /// Creates a new WAL. Note that you will have to call replay and then start the flusher\n+    /// to fully initialize the WAL.\n+    pub fn new(\n+        object_store: Arc<dyn ObjectStore>,\n+        file_notifier: Arc<dyn WalFileNotifier>,\n+        config: WalConfig,\n+    ) -> Self {\n+        Self {\n+            object_store,\n+            file_notifier,\n+            flush_buffer: Mutex::new(FlushBuffer::new(\n+                WalBuffer {\n+                    is_shutdown: false,\n+                    wal_file_sequence_number: Default::default(),\n+                    op_limit: config.max_write_buffer_size,\n+                    op_count: 0,\n+                    database_to_write_batch: Default::default(),\n+                    catalog_batches: vec![],\n+                    write_op_responses: vec![],\n+                },\n+                SnapshotTracker::new(config.snapshot_size, config.level_0_duration),\n+            )),\n+        }\n+    }\n+\n+    /// Loads the WAL files in order from object store, calling the file notifier on each one and\n+    /// populating the snapshot tracker with the WAL periods.\n+    pub async fn replay(&self) -> crate::Result<()> {\n+        let paths = self.load_existing_wal_file_paths().await?;\n+\n+        for path in paths {\n+            let file_bytes = self.object_store.get(&path).await?.bytes().await?;\n+            let wal_contents = verify_file_type_and_deserialize(file_bytes)?;\n+\n+            // add this to the snapshot tracker so we know what to clear out when\n+            self.flush_buffer\n+                .lock()\n+                .await\n+                .snapshot_tracker\n+                .add_wal_period(WalPeriod::new(\n+                    wal_contents.wal_file_number,\n+                    Timestamp::new(wal_contents.min_timestamp_ns),\n+                    Timestamp::new(wal_contents.max_timestamp_ns),\n+                ));\n+\n+            match wal_contents.snapshot {\n+                None => self.file_notifier.notify(wal_contents),\n+                Some(snapshot_details) => {\n+                    let snapshot_info = {\n+                        let mut buffer = self.flush_buffer.lock().await;\n+\n+                        match buffer.snapshot_tracker.snapshot() {\n+                            None => None,\n+                            Some(info) => {\n+                                let semaphore = Arc::clone(&buffer.snapshot_semaphore);\n+                                let permit = semaphore.acquire_owned().await.unwrap();\n+\n+                                Some((info, permit))\n+                            }\n+                        }\n+                    };\n+\n+                    let snapshot_done = self\n+                        .file_notifier\n+                        .notify_and_snapshot(wal_contents, snapshot_details)\n+                        .await;\n+                    let details = snapshot_done.await.unwrap();\n+                    assert_eq!(snapshot_details, details);",
        "comment_created_at": "2024-07-31T22:16:03+00:00",
        "comment_author": "pauldix",
        "comment_body": "Actually, changed my mind. I do want it to crash, because the next thing it does is delete WAL files. If for some reason the snapshot details we get back aren't expected, we want to crash rather than deleting WAL files, which may be unrecoverable. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1678255028",
    "pr_number": 25161,
    "pr_file": "influxdb3_write/src/last_cache/table_function.rs",
    "created_at": "2024-07-15T18:49:07+00:00",
    "commented_code": "filters: &[Expr],\n         _limit: Option<usize>,\n     ) -> Result<Arc<dyn ExecutionPlan>> {\n-        let predicates = self.convert_filter_exprs(filters);\n-        let batches = self.to_record_batches(&predicates)?;\n-        let mut exec = MemoryExec::try_new(&[batches], self.schema(), projection.cloned())?;\n+        let read = self.provider.cache_map.read();\n+        let cache = read\n+            .get(&self.db_name)\n+            .unwrap()\n+            .get(&self.table_name)\n+            .unwrap()\n+            .get(&self.cache_name)\n+            .unwrap();",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1678255028",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25161,
        "pr_file": "influxdb3_write/src/last_cache/table_function.rs",
        "discussion_id": "1678255028",
        "commented_code": "@@ -41,9 +57,18 @@ impl TableProvider for LastCache {\n         filters: &[Expr],\n         _limit: Option<usize>,\n     ) -> Result<Arc<dyn ExecutionPlan>> {\n-        let predicates = self.convert_filter_exprs(filters);\n-        let batches = self.to_record_batches(&predicates)?;\n-        let mut exec = MemoryExec::try_new(&[batches], self.schema(), projection.cloned())?;\n+        let read = self.provider.cache_map.read();\n+        let cache = read\n+            .get(&self.db_name)\n+            .unwrap()\n+            .get(&self.table_name)\n+            .unwrap()\n+            .get(&self.cache_name)\n+            .unwrap();",
        "comment_created_at": "2024-07-15T18:49:07+00:00",
        "comment_author": "hiltontj",
        "comment_body": "These unwraps can probably be changed to errors, but if any of these fails, it means that there is a race condition, so it might be that panicking is the right thing.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1699100590",
    "pr_number": 25196,
    "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
    "created_at": "2024-07-31T20:59:23+00:00",
    "commented_code": "+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    #[cfg(test)]\n+    #[allow(dead_code)]\n+    pub(crate) fn get_table_record_batches(\n+        &self,\n+        database: &str,\n+        table: &str,\n+        schema: SchemaRef,\n+    ) -> Vec<RecordBatch> {\n+        self.buffer\n+            .read()\n+            .db_to_table\n+            .get(database)\n+            .unwrap()\n+            .get(table)\n+            .unwrap()\n+            .record_batches(schema, &[])\n+            .unwrap()\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);\n+\n+        for op in write.ops {\n+            match op {\n+                WalOp::Write(write_batch) => buffer.add_write_batch(write_batch),\n+                WalOp::Catalog(catalog_batch) => buffer\n+                    .catalog\n+                    .apply_catalog_batch(&catalog_batch)\n+                    .expect(\"catalog batch should apply\"),\n+            }\n+        }\n+    }\n+\n+    /// Called when the wal has written a new file and is attempting to snapshot. Kicks off persistence of\n+    /// data that can be snapshot in the background after putting the data in the buffer.\n+    async fn buffer_contents_and_persist_snapshotted_data(\n+        &self,\n+        write: WalContents,\n+        snapshot_details: SnapshotDetails,\n+    ) -> Receiver<SnapshotDetails> {\n+        let persist_jobs = {\n+            let mut buffer = self.buffer.write();\n+\n+            for op in write.ops {\n+                match op {\n+                    WalOp::Write(write_batch) => buffer.add_write_batch(write_batch),\n+                    WalOp::Catalog(catalog_batch) => buffer\n+                        .catalog\n+                        .apply_catalog_batch(&catalog_batch)\n+                        .expect(\"catalog batch should apply\"),\n+                }\n+            }\n+\n+            let mut persisting_chunks = vec![];\n+            for (database_name, table_map) in buffer.db_to_table.iter_mut() {\n+                for (table_name, table_buffer) in table_map.iter_mut() {\n+                    let snapshot_chunks = table_buffer.snapshot(snapshot_details.end_time_marker);\n+\n+                    for chunk in snapshot_chunks {\n+                        let persist_job = PersistJob {\n+                            database_name: Arc::clone(database_name),\n+                            table_name: Arc::clone(table_name),\n+                            chunk_time: chunk.chunk_time,\n+                            path: ParquetFilePath::new_with_chunk_time(\n+                                database_name.as_ref(),\n+                                table_name.as_ref(),\n+                                chunk.chunk_time,\n+                                write.wal_file_number,\n+                            ),\n+                            batch: chunk.record_batch,\n+                            schema: chunk.schema,\n+                            timestamp_min_max: chunk.timestamp_min_max,\n+                            sort_key: table_buffer.sort_key.clone(),\n+                        };\n+\n+                        persisting_chunks.push(persist_job);\n+                    }\n+                }\n+            }\n+\n+            persisting_chunks\n+        };\n+\n+        let (sender, receiver) = oneshot::channel();\n+\n+        let persister = Arc::clone(&self.persister);\n+        let executor = Arc::clone(&self.executor);\n+        let persisted_files = Arc::clone(&self.persisted_files);\n+        let wal_file_number = write.wal_file_number;\n+        let buffer = Arc::clone(&self.buffer);\n+        let catalog = Arc::clone(&self.catalog);\n+\n+        tokio::spawn(async move {\n+            // persist the catalog\n+            loop {\n+                let catalog = catalog.clone_inner();\n+\n+                match persister\n+                    .persist_catalog(wal_file_number, Catalog::from_inner(catalog))\n+                    .await\n+                {\n+                    Ok(_) => break,\n+                    Err(e) => {\n+                        error!(%e, \"Error persisting catalog, sleeping and retrying...\");\n+                        tokio::time::sleep(Duration::from_secs(1)).await;\n+                    }\n+                }\n+            }\n+\n+            // persist the individual files, building the snapshot as we go\n+            let mut persisted_snapshot = PersistedSnapshot::new(wal_file_number);\n+            for persist_job in persist_jobs {\n+                let path = persist_job.path.to_string();\n+                let database_name = Arc::clone(&persist_job.database_name);\n+                let table_name = Arc::clone(&persist_job.table_name);\n+                let chunk_time = persist_job.chunk_time;\n+                let min_time = persist_job.timestamp_min_max.min;\n+                let max_time = persist_job.timestamp_min_max.max;\n+\n+                let (size_bytes, meta) =\n+                    sort_dedupe_persist(persist_job, Arc::clone(&persister), Arc::clone(&executor))\n+                        .await;\n+                persisted_snapshot.add_parquet_file(\n+                    database_name,\n+                    table_name,\n+                    ParquetFile {\n+                        path,\n+                        size_bytes,\n+                        row_count: meta.num_rows as u64,\n+                        chunk_time,\n+                        min_time,\n+                        max_time,\n+                    },\n+                )\n+            }\n+\n+            // persist the snapshot file\n+            loop {\n+                match persister.persist_snapshot(&persisted_snapshot).await {\n+                    Ok(_) => break,\n+                    Err(e) => {\n+                        error!(%e, \"Error persisting snapshot, sleeping and retrying...\");",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1699100590",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1699100590",
        "commented_code": "@@ -0,0 +1,470 @@\n+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    #[cfg(test)]\n+    #[allow(dead_code)]\n+    pub(crate) fn get_table_record_batches(\n+        &self,\n+        database: &str,\n+        table: &str,\n+        schema: SchemaRef,\n+    ) -> Vec<RecordBatch> {\n+        self.buffer\n+            .read()\n+            .db_to_table\n+            .get(database)\n+            .unwrap()\n+            .get(table)\n+            .unwrap()\n+            .record_batches(schema, &[])\n+            .unwrap()\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);\n+\n+        for op in write.ops {\n+            match op {\n+                WalOp::Write(write_batch) => buffer.add_write_batch(write_batch),\n+                WalOp::Catalog(catalog_batch) => buffer\n+                    .catalog\n+                    .apply_catalog_batch(&catalog_batch)\n+                    .expect(\"catalog batch should apply\"),\n+            }\n+        }\n+    }\n+\n+    /// Called when the wal has written a new file and is attempting to snapshot. Kicks off persistence of\n+    /// data that can be snapshot in the background after putting the data in the buffer.\n+    async fn buffer_contents_and_persist_snapshotted_data(\n+        &self,\n+        write: WalContents,\n+        snapshot_details: SnapshotDetails,\n+    ) -> Receiver<SnapshotDetails> {\n+        let persist_jobs = {\n+            let mut buffer = self.buffer.write();\n+\n+            for op in write.ops {\n+                match op {\n+                    WalOp::Write(write_batch) => buffer.add_write_batch(write_batch),\n+                    WalOp::Catalog(catalog_batch) => buffer\n+                        .catalog\n+                        .apply_catalog_batch(&catalog_batch)\n+                        .expect(\"catalog batch should apply\"),\n+                }\n+            }\n+\n+            let mut persisting_chunks = vec![];\n+            for (database_name, table_map) in buffer.db_to_table.iter_mut() {\n+                for (table_name, table_buffer) in table_map.iter_mut() {\n+                    let snapshot_chunks = table_buffer.snapshot(snapshot_details.end_time_marker);\n+\n+                    for chunk in snapshot_chunks {\n+                        let persist_job = PersistJob {\n+                            database_name: Arc::clone(database_name),\n+                            table_name: Arc::clone(table_name),\n+                            chunk_time: chunk.chunk_time,\n+                            path: ParquetFilePath::new_with_chunk_time(\n+                                database_name.as_ref(),\n+                                table_name.as_ref(),\n+                                chunk.chunk_time,\n+                                write.wal_file_number,\n+                            ),\n+                            batch: chunk.record_batch,\n+                            schema: chunk.schema,\n+                            timestamp_min_max: chunk.timestamp_min_max,\n+                            sort_key: table_buffer.sort_key.clone(),\n+                        };\n+\n+                        persisting_chunks.push(persist_job);\n+                    }\n+                }\n+            }\n+\n+            persisting_chunks\n+        };\n+\n+        let (sender, receiver) = oneshot::channel();\n+\n+        let persister = Arc::clone(&self.persister);\n+        let executor = Arc::clone(&self.executor);\n+        let persisted_files = Arc::clone(&self.persisted_files);\n+        let wal_file_number = write.wal_file_number;\n+        let buffer = Arc::clone(&self.buffer);\n+        let catalog = Arc::clone(&self.catalog);\n+\n+        tokio::spawn(async move {\n+            // persist the catalog\n+            loop {\n+                let catalog = catalog.clone_inner();\n+\n+                match persister\n+                    .persist_catalog(wal_file_number, Catalog::from_inner(catalog))\n+                    .await\n+                {\n+                    Ok(_) => break,\n+                    Err(e) => {\n+                        error!(%e, \"Error persisting catalog, sleeping and retrying...\");\n+                        tokio::time::sleep(Duration::from_secs(1)).await;\n+                    }\n+                }\n+            }\n+\n+            // persist the individual files, building the snapshot as we go\n+            let mut persisted_snapshot = PersistedSnapshot::new(wal_file_number);\n+            for persist_job in persist_jobs {\n+                let path = persist_job.path.to_string();\n+                let database_name = Arc::clone(&persist_job.database_name);\n+                let table_name = Arc::clone(&persist_job.table_name);\n+                let chunk_time = persist_job.chunk_time;\n+                let min_time = persist_job.timestamp_min_max.min;\n+                let max_time = persist_job.timestamp_min_max.max;\n+\n+                let (size_bytes, meta) =\n+                    sort_dedupe_persist(persist_job, Arc::clone(&persister), Arc::clone(&executor))\n+                        .await;\n+                persisted_snapshot.add_parquet_file(\n+                    database_name,\n+                    table_name,\n+                    ParquetFile {\n+                        path,\n+                        size_bytes,\n+                        row_count: meta.num_rows as u64,\n+                        chunk_time,\n+                        min_time,\n+                        max_time,\n+                    },\n+                )\n+            }\n+\n+            // persist the snapshot file\n+            loop {\n+                match persister.persist_snapshot(&persisted_snapshot).await {\n+                    Ok(_) => break,\n+                    Err(e) => {\n+                        error!(%e, \"Error persisting snapshot, sleeping and retrying...\");",
        "comment_created_at": "2024-07-31T20:59:23+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "This will happen forever possibly. Do we want to handle that?",
        "pr_file_module": null
      },
      {
        "comment_id": "1699183111",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1699100590",
        "commented_code": "@@ -0,0 +1,470 @@\n+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    #[cfg(test)]\n+    #[allow(dead_code)]\n+    pub(crate) fn get_table_record_batches(\n+        &self,\n+        database: &str,\n+        table: &str,\n+        schema: SchemaRef,\n+    ) -> Vec<RecordBatch> {\n+        self.buffer\n+            .read()\n+            .db_to_table\n+            .get(database)\n+            .unwrap()\n+            .get(table)\n+            .unwrap()\n+            .record_batches(schema, &[])\n+            .unwrap()\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);\n+\n+        for op in write.ops {\n+            match op {\n+                WalOp::Write(write_batch) => buffer.add_write_batch(write_batch),\n+                WalOp::Catalog(catalog_batch) => buffer\n+                    .catalog\n+                    .apply_catalog_batch(&catalog_batch)\n+                    .expect(\"catalog batch should apply\"),\n+            }\n+        }\n+    }\n+\n+    /// Called when the wal has written a new file and is attempting to snapshot. Kicks off persistence of\n+    /// data that can be snapshot in the background after putting the data in the buffer.\n+    async fn buffer_contents_and_persist_snapshotted_data(\n+        &self,\n+        write: WalContents,\n+        snapshot_details: SnapshotDetails,\n+    ) -> Receiver<SnapshotDetails> {\n+        let persist_jobs = {\n+            let mut buffer = self.buffer.write();\n+\n+            for op in write.ops {\n+                match op {\n+                    WalOp::Write(write_batch) => buffer.add_write_batch(write_batch),\n+                    WalOp::Catalog(catalog_batch) => buffer\n+                        .catalog\n+                        .apply_catalog_batch(&catalog_batch)\n+                        .expect(\"catalog batch should apply\"),\n+                }\n+            }\n+\n+            let mut persisting_chunks = vec![];\n+            for (database_name, table_map) in buffer.db_to_table.iter_mut() {\n+                for (table_name, table_buffer) in table_map.iter_mut() {\n+                    let snapshot_chunks = table_buffer.snapshot(snapshot_details.end_time_marker);\n+\n+                    for chunk in snapshot_chunks {\n+                        let persist_job = PersistJob {\n+                            database_name: Arc::clone(database_name),\n+                            table_name: Arc::clone(table_name),\n+                            chunk_time: chunk.chunk_time,\n+                            path: ParquetFilePath::new_with_chunk_time(\n+                                database_name.as_ref(),\n+                                table_name.as_ref(),\n+                                chunk.chunk_time,\n+                                write.wal_file_number,\n+                            ),\n+                            batch: chunk.record_batch,\n+                            schema: chunk.schema,\n+                            timestamp_min_max: chunk.timestamp_min_max,\n+                            sort_key: table_buffer.sort_key.clone(),\n+                        };\n+\n+                        persisting_chunks.push(persist_job);\n+                    }\n+                }\n+            }\n+\n+            persisting_chunks\n+        };\n+\n+        let (sender, receiver) = oneshot::channel();\n+\n+        let persister = Arc::clone(&self.persister);\n+        let executor = Arc::clone(&self.executor);\n+        let persisted_files = Arc::clone(&self.persisted_files);\n+        let wal_file_number = write.wal_file_number;\n+        let buffer = Arc::clone(&self.buffer);\n+        let catalog = Arc::clone(&self.catalog);\n+\n+        tokio::spawn(async move {\n+            // persist the catalog\n+            loop {\n+                let catalog = catalog.clone_inner();\n+\n+                match persister\n+                    .persist_catalog(wal_file_number, Catalog::from_inner(catalog))\n+                    .await\n+                {\n+                    Ok(_) => break,\n+                    Err(e) => {\n+                        error!(%e, \"Error persisting catalog, sleeping and retrying...\");\n+                        tokio::time::sleep(Duration::from_secs(1)).await;\n+                    }\n+                }\n+            }\n+\n+            // persist the individual files, building the snapshot as we go\n+            let mut persisted_snapshot = PersistedSnapshot::new(wal_file_number);\n+            for persist_job in persist_jobs {\n+                let path = persist_job.path.to_string();\n+                let database_name = Arc::clone(&persist_job.database_name);\n+                let table_name = Arc::clone(&persist_job.table_name);\n+                let chunk_time = persist_job.chunk_time;\n+                let min_time = persist_job.timestamp_min_max.min;\n+                let max_time = persist_job.timestamp_min_max.max;\n+\n+                let (size_bytes, meta) =\n+                    sort_dedupe_persist(persist_job, Arc::clone(&persister), Arc::clone(&executor))\n+                        .await;\n+                persisted_snapshot.add_parquet_file(\n+                    database_name,\n+                    table_name,\n+                    ParquetFile {\n+                        path,\n+                        size_bytes,\n+                        row_count: meta.num_rows as u64,\n+                        chunk_time,\n+                        min_time,\n+                        max_time,\n+                    },\n+                )\n+            }\n+\n+            // persist the snapshot file\n+            loop {\n+                match persister.persist_snapshot(&persisted_snapshot).await {\n+                    Ok(_) => break,\n+                    Err(e) => {\n+                        error!(%e, \"Error persisting snapshot, sleeping and retrying...\");",
        "comment_created_at": "2024-07-31T22:47:44+00:00",
        "comment_author": "pauldix",
        "comment_body": "At the moment I'm not sure what we can do other than keep trying. If we're unable to persist, it means the object store is unreachable, so writes will then return errors to clients because we'll be able to persist wal files to object storage. So clients attempting to write data will start receiving errors and errors will show up in the logs. It's the equivalent of losing the disk, so not much we can do about it. Unless we decide we want to exit the program entirely.",
        "pr_file_module": null
      },
      {
        "comment_id": "1700655917",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1699100590",
        "commented_code": "@@ -0,0 +1,470 @@\n+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    #[cfg(test)]\n+    #[allow(dead_code)]\n+    pub(crate) fn get_table_record_batches(\n+        &self,\n+        database: &str,\n+        table: &str,\n+        schema: SchemaRef,\n+    ) -> Vec<RecordBatch> {\n+        self.buffer\n+            .read()\n+            .db_to_table\n+            .get(database)\n+            .unwrap()\n+            .get(table)\n+            .unwrap()\n+            .record_batches(schema, &[])\n+            .unwrap()\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);\n+\n+        for op in write.ops {\n+            match op {\n+                WalOp::Write(write_batch) => buffer.add_write_batch(write_batch),\n+                WalOp::Catalog(catalog_batch) => buffer\n+                    .catalog\n+                    .apply_catalog_batch(&catalog_batch)\n+                    .expect(\"catalog batch should apply\"),\n+            }\n+        }\n+    }\n+\n+    /// Called when the wal has written a new file and is attempting to snapshot. Kicks off persistence of\n+    /// data that can be snapshot in the background after putting the data in the buffer.\n+    async fn buffer_contents_and_persist_snapshotted_data(\n+        &self,\n+        write: WalContents,\n+        snapshot_details: SnapshotDetails,\n+    ) -> Receiver<SnapshotDetails> {\n+        let persist_jobs = {\n+            let mut buffer = self.buffer.write();\n+\n+            for op in write.ops {\n+                match op {\n+                    WalOp::Write(write_batch) => buffer.add_write_batch(write_batch),\n+                    WalOp::Catalog(catalog_batch) => buffer\n+                        .catalog\n+                        .apply_catalog_batch(&catalog_batch)\n+                        .expect(\"catalog batch should apply\"),\n+                }\n+            }\n+\n+            let mut persisting_chunks = vec![];\n+            for (database_name, table_map) in buffer.db_to_table.iter_mut() {\n+                for (table_name, table_buffer) in table_map.iter_mut() {\n+                    let snapshot_chunks = table_buffer.snapshot(snapshot_details.end_time_marker);\n+\n+                    for chunk in snapshot_chunks {\n+                        let persist_job = PersistJob {\n+                            database_name: Arc::clone(database_name),\n+                            table_name: Arc::clone(table_name),\n+                            chunk_time: chunk.chunk_time,\n+                            path: ParquetFilePath::new_with_chunk_time(\n+                                database_name.as_ref(),\n+                                table_name.as_ref(),\n+                                chunk.chunk_time,\n+                                write.wal_file_number,\n+                            ),\n+                            batch: chunk.record_batch,\n+                            schema: chunk.schema,\n+                            timestamp_min_max: chunk.timestamp_min_max,\n+                            sort_key: table_buffer.sort_key.clone(),\n+                        };\n+\n+                        persisting_chunks.push(persist_job);\n+                    }\n+                }\n+            }\n+\n+            persisting_chunks\n+        };\n+\n+        let (sender, receiver) = oneshot::channel();\n+\n+        let persister = Arc::clone(&self.persister);\n+        let executor = Arc::clone(&self.executor);\n+        let persisted_files = Arc::clone(&self.persisted_files);\n+        let wal_file_number = write.wal_file_number;\n+        let buffer = Arc::clone(&self.buffer);\n+        let catalog = Arc::clone(&self.catalog);\n+\n+        tokio::spawn(async move {\n+            // persist the catalog\n+            loop {\n+                let catalog = catalog.clone_inner();\n+\n+                match persister\n+                    .persist_catalog(wal_file_number, Catalog::from_inner(catalog))\n+                    .await\n+                {\n+                    Ok(_) => break,\n+                    Err(e) => {\n+                        error!(%e, \"Error persisting catalog, sleeping and retrying...\");\n+                        tokio::time::sleep(Duration::from_secs(1)).await;\n+                    }\n+                }\n+            }\n+\n+            // persist the individual files, building the snapshot as we go\n+            let mut persisted_snapshot = PersistedSnapshot::new(wal_file_number);\n+            for persist_job in persist_jobs {\n+                let path = persist_job.path.to_string();\n+                let database_name = Arc::clone(&persist_job.database_name);\n+                let table_name = Arc::clone(&persist_job.table_name);\n+                let chunk_time = persist_job.chunk_time;\n+                let min_time = persist_job.timestamp_min_max.min;\n+                let max_time = persist_job.timestamp_min_max.max;\n+\n+                let (size_bytes, meta) =\n+                    sort_dedupe_persist(persist_job, Arc::clone(&persister), Arc::clone(&executor))\n+                        .await;\n+                persisted_snapshot.add_parquet_file(\n+                    database_name,\n+                    table_name,\n+                    ParquetFile {\n+                        path,\n+                        size_bytes,\n+                        row_count: meta.num_rows as u64,\n+                        chunk_time,\n+                        min_time,\n+                        max_time,\n+                    },\n+                )\n+            }\n+\n+            // persist the snapshot file\n+            loop {\n+                match persister.persist_snapshot(&persisted_snapshot).await {\n+                    Ok(_) => break,\n+                    Err(e) => {\n+                        error!(%e, \"Error persisting snapshot, sleeping and retrying...\");",
        "comment_created_at": "2024-08-01T18:43:08+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "As long as this is backing everything else up including new writes then I think this is fine to keep as is.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1920560008",
    "pr_number": 25852,
    "pr_file": "influxdb3_processing_engine/src/plugins.rs",
    "created_at": "2025-01-17T18:01:29+00:00",
    "commented_code": "}\n \n             if !db_writes.is_empty() {\n-                for (db_name, output_lines) in db_writes {\n-                    let ingest_time = SystemTime::now()\n-                        .duration_since(SystemTime::UNIX_EPOCH)\n-                        .unwrap();\n-                    self.write_buffer\n-                        .write_lp(\n-                            NamespaceName::new(db_name).unwrap(),\n-                            output_lines.join(\"\n\").as_str(),\n-                            Time::from_timestamp_nanos(ingest_time.as_nanos() as i64),\n-                            false,\n-                            Precision::Nanosecond,\n-                        )\n-                        .await?;\n-                }\n+                db_writes.execute(&self.write_buffer).await?;\n             }\n \n             Ok(false)\n         }\n     }\n+\n+    struct DatabaseWriteBuffer {\n+        writes: HashMap<String, Vec<String>>,\n+    }\n+    impl DatabaseWriteBuffer {\n+        fn new() -> Self {\n+            Self {\n+                writes: HashMap::new(),\n+            }\n+        }\n+\n+        fn add_lines(&mut self, db_name: &str, lines: Vec<String>) {\n+            self.writes.entry_ref(db_name).or_default().extend(lines);\n+        }\n+\n+        fn is_empty(&self) -> bool {\n+            self.writes.is_empty()\n+        }\n+\n+        async fn execute(self, write_buffer: &Arc<dyn WriteBuffer>) -> Result<(), Error> {\n+            for (db_name, output_lines) in self.writes {\n+                let ingest_time = SystemTime::now()\n+                    .duration_since(SystemTime::UNIX_EPOCH)\n+                    .unwrap();\n+                write_buffer\n+                    .write_lp(\n+                        NamespaceName::new(db_name).unwrap(),\n+                        output_lines.join(\"\n\").as_str(),\n+                        Time::from_timestamp_nanos(ingest_time.as_nanos() as i64),\n+                        false,\n+                        Precision::Nanosecond,\n+                    )\n+                    .await?;\n+            }\n+            Ok(())\n+        }\n+    }\n+\n+    struct CronTriggerRunner {\n+        schedule: OwnedScheduleIterator<Utc>,\n+        next_trigger_time: Option<DateTime<Utc>>,\n+    }\n+    impl CronTriggerRunner {\n+        fn new(cron_schedule: Schedule) -> Self {\n+            let mut schedule = cron_schedule.upcoming_owned(Utc);\n+            let next_trigger_time = schedule.next();\n+            Self {\n+                schedule,\n+                next_trigger_time,\n+            }\n+        }\n+\n+        async fn run_at_time(\n+            &mut self,\n+            plugin: &TriggerPlugin,\n+            db_schema: Arc<DatabaseSchema>,\n+        ) -> Result<(), Error> {\n+            let Some(trigger_time) = self.next_trigger_time else {\n+                return Err(anyhow!(\"running a cron trigger that is finished.\").into());\n+            };\n+            let result = execute_cron_trigger(\n+                &plugin.plugin_code,\n+                trigger_time,\n+                Arc::clone(&db_schema),\n+                Arc::clone(&plugin.query_executor),\n+                &plugin.trigger_definition.trigger_arguments,\n+            )\n+            .expect(\"should be able to run trigger\");",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1920560008",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25852,
        "pr_file": "influxdb3_processing_engine/src/plugins.rs",
        "discussion_id": "1920560008",
        "commented_code": "@@ -208,25 +286,109 @@ mod python_plugin {\n             }\n \n             if !db_writes.is_empty() {\n-                for (db_name, output_lines) in db_writes {\n-                    let ingest_time = SystemTime::now()\n-                        .duration_since(SystemTime::UNIX_EPOCH)\n-                        .unwrap();\n-                    self.write_buffer\n-                        .write_lp(\n-                            NamespaceName::new(db_name).unwrap(),\n-                            output_lines.join(\"\\n\").as_str(),\n-                            Time::from_timestamp_nanos(ingest_time.as_nanos() as i64),\n-                            false,\n-                            Precision::Nanosecond,\n-                        )\n-                        .await?;\n-                }\n+                db_writes.execute(&self.write_buffer).await?;\n             }\n \n             Ok(false)\n         }\n     }\n+\n+    struct DatabaseWriteBuffer {\n+        writes: HashMap<String, Vec<String>>,\n+    }\n+    impl DatabaseWriteBuffer {\n+        fn new() -> Self {\n+            Self {\n+                writes: HashMap::new(),\n+            }\n+        }\n+\n+        fn add_lines(&mut self, db_name: &str, lines: Vec<String>) {\n+            self.writes.entry_ref(db_name).or_default().extend(lines);\n+        }\n+\n+        fn is_empty(&self) -> bool {\n+            self.writes.is_empty()\n+        }\n+\n+        async fn execute(self, write_buffer: &Arc<dyn WriteBuffer>) -> Result<(), Error> {\n+            for (db_name, output_lines) in self.writes {\n+                let ingest_time = SystemTime::now()\n+                    .duration_since(SystemTime::UNIX_EPOCH)\n+                    .unwrap();\n+                write_buffer\n+                    .write_lp(\n+                        NamespaceName::new(db_name).unwrap(),\n+                        output_lines.join(\"\\n\").as_str(),\n+                        Time::from_timestamp_nanos(ingest_time.as_nanos() as i64),\n+                        false,\n+                        Precision::Nanosecond,\n+                    )\n+                    .await?;\n+            }\n+            Ok(())\n+        }\n+    }\n+\n+    struct CronTriggerRunner {\n+        schedule: OwnedScheduleIterator<Utc>,\n+        next_trigger_time: Option<DateTime<Utc>>,\n+    }\n+    impl CronTriggerRunner {\n+        fn new(cron_schedule: Schedule) -> Self {\n+            let mut schedule = cron_schedule.upcoming_owned(Utc);\n+            let next_trigger_time = schedule.next();\n+            Self {\n+                schedule,\n+                next_trigger_time,\n+            }\n+        }\n+\n+        async fn run_at_time(\n+            &mut self,\n+            plugin: &TriggerPlugin,\n+            db_schema: Arc<DatabaseSchema>,\n+        ) -> Result<(), Error> {\n+            let Some(trigger_time) = self.next_trigger_time else {\n+                return Err(anyhow!(\"running a cron trigger that is finished.\").into());\n+            };\n+            let result = execute_cron_trigger(\n+                &plugin.plugin_code,\n+                trigger_time,\n+                Arc::clone(&db_schema),\n+                Arc::clone(&plugin.query_executor),\n+                &plugin.trigger_definition.trigger_arguments,\n+            )\n+            .expect(\"should be able to run trigger\");",
        "comment_created_at": "2025-01-17T18:01:29+00:00",
        "comment_author": "pauldix",
        "comment_body": "any reason this might error, can we catch this and log it?",
        "pr_file_module": null
      },
      {
        "comment_id": "1920791314",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25852,
        "pr_file": "influxdb3_processing_engine/src/plugins.rs",
        "discussion_id": "1920560008",
        "commented_code": "@@ -208,25 +286,109 @@ mod python_plugin {\n             }\n \n             if !db_writes.is_empty() {\n-                for (db_name, output_lines) in db_writes {\n-                    let ingest_time = SystemTime::now()\n-                        .duration_since(SystemTime::UNIX_EPOCH)\n-                        .unwrap();\n-                    self.write_buffer\n-                        .write_lp(\n-                            NamespaceName::new(db_name).unwrap(),\n-                            output_lines.join(\"\\n\").as_str(),\n-                            Time::from_timestamp_nanos(ingest_time.as_nanos() as i64),\n-                            false,\n-                            Precision::Nanosecond,\n-                        )\n-                        .await?;\n-                }\n+                db_writes.execute(&self.write_buffer).await?;\n             }\n \n             Ok(false)\n         }\n     }\n+\n+    struct DatabaseWriteBuffer {\n+        writes: HashMap<String, Vec<String>>,\n+    }\n+    impl DatabaseWriteBuffer {\n+        fn new() -> Self {\n+            Self {\n+                writes: HashMap::new(),\n+            }\n+        }\n+\n+        fn add_lines(&mut self, db_name: &str, lines: Vec<String>) {\n+            self.writes.entry_ref(db_name).or_default().extend(lines);\n+        }\n+\n+        fn is_empty(&self) -> bool {\n+            self.writes.is_empty()\n+        }\n+\n+        async fn execute(self, write_buffer: &Arc<dyn WriteBuffer>) -> Result<(), Error> {\n+            for (db_name, output_lines) in self.writes {\n+                let ingest_time = SystemTime::now()\n+                    .duration_since(SystemTime::UNIX_EPOCH)\n+                    .unwrap();\n+                write_buffer\n+                    .write_lp(\n+                        NamespaceName::new(db_name).unwrap(),\n+                        output_lines.join(\"\\n\").as_str(),\n+                        Time::from_timestamp_nanos(ingest_time.as_nanos() as i64),\n+                        false,\n+                        Precision::Nanosecond,\n+                    )\n+                    .await?;\n+            }\n+            Ok(())\n+        }\n+    }\n+\n+    struct CronTriggerRunner {\n+        schedule: OwnedScheduleIterator<Utc>,\n+        next_trigger_time: Option<DateTime<Utc>>,\n+    }\n+    impl CronTriggerRunner {\n+        fn new(cron_schedule: Schedule) -> Self {\n+            let mut schedule = cron_schedule.upcoming_owned(Utc);\n+            let next_trigger_time = schedule.next();\n+            Self {\n+                schedule,\n+                next_trigger_time,\n+            }\n+        }\n+\n+        async fn run_at_time(\n+            &mut self,\n+            plugin: &TriggerPlugin,\n+            db_schema: Arc<DatabaseSchema>,\n+        ) -> Result<(), Error> {\n+            let Some(trigger_time) = self.next_trigger_time else {\n+                return Err(anyhow!(\"running a cron trigger that is finished.\").into());\n+            };\n+            let result = execute_cron_trigger(\n+                &plugin.plugin_code,\n+                trigger_time,\n+                Arc::clone(&db_schema),\n+                Arc::clone(&plugin.query_executor),\n+                &plugin.trigger_definition.trigger_arguments,\n+            )\n+            .expect(\"should be able to run trigger\");",
        "comment_created_at": "2025-01-17T21:36:06+00:00",
        "comment_author": "jacksonrnewhouse",
        "comment_body": "Yeah, any plugin error would trigger, switching to just have it exit with `?`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1791935286",
    "pr_number": 25432,
    "pr_file": "influxdb3_telemetry/src/sender.rs",
    "created_at": "2024-10-08T13:58:37+00:00",
    "commented_code": "duration_secs: Duration,\n ) -> tokio::task::JoinHandle<()> {\n     tokio::spawn(async move {\n-        let telem_sender = TelemetrySender::new(\n+        let mut telem_sender = TelemetrySender::new(\n             reqwest::Client::new(),\n-            \"https://telemetry.influxdata.foo.com\".to_owned(),\n+            \"https://telemetry.influxdata.foo.com\",\n         );\n         let mut interval = tokio::time::interval(duration_secs);\n         interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);\n \n         loop {\n             interval.tick().await;\n-            let telemetry = store.snapshot();\n-            if let Err(e) = telem_sender.try_sending(&telemetry).await {\n-                // TODO: change to error! - until endpoint is decided keep\n-                //       this as debug log\n-                debug!(error = ?e, \"Cannot send telemetry\");\n-            }\n-            // if we tried sending and failed, we currently still reset the\n-            // metrics, it is ok to miss few samples\n-            store.reset_metrics();\n+            send_telemetry(&store, &mut telem_sender).await;\n         }\n     })\n }\n+\n+async fn send_telemetry(store: &Arc<TelemetryStore>, telem_sender: &mut TelemetrySender) {\n+    let telemetry = store.snapshot();\n+    if let Err(e) = telem_sender.try_sending(&telemetry).await {\n+        // TODO: change to error! - until endpoint is decided keep\n+        //       this as debug log",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1791935286",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25432,
        "pr_file": "influxdb3_telemetry/src/sender.rs",
        "discussion_id": "1791935286",
        "commented_code": "@@ -77,24 +86,90 @@ pub(crate) async fn send_telemetry_in_background(\n     duration_secs: Duration,\n ) -> tokio::task::JoinHandle<()> {\n     tokio::spawn(async move {\n-        let telem_sender = TelemetrySender::new(\n+        let mut telem_sender = TelemetrySender::new(\n             reqwest::Client::new(),\n-            \"https://telemetry.influxdata.foo.com\".to_owned(),\n+            \"https://telemetry.influxdata.foo.com\",\n         );\n         let mut interval = tokio::time::interval(duration_secs);\n         interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);\n \n         loop {\n             interval.tick().await;\n-            let telemetry = store.snapshot();\n-            if let Err(e) = telem_sender.try_sending(&telemetry).await {\n-                // TODO: change to error! - until endpoint is decided keep\n-                //       this as debug log\n-                debug!(error = ?e, \"Cannot send telemetry\");\n-            }\n-            // if we tried sending and failed, we currently still reset the\n-            // metrics, it is ok to miss few samples\n-            store.reset_metrics();\n+            send_telemetry(&store, &mut telem_sender).await;\n         }\n     })\n }\n+\n+async fn send_telemetry(store: &Arc<TelemetryStore>, telem_sender: &mut TelemetrySender) {\n+    let telemetry = store.snapshot();\n+    if let Err(e) = telem_sender.try_sending(&telemetry).await {\n+        // TODO: change to error! - until endpoint is decided keep\n+        //       this as debug log",
        "comment_created_at": "2024-10-08T13:58:37+00:00",
        "comment_author": "hiltontj",
        "comment_body": "FWIW - I think at most it would be `warn!`, since telemetry not sending is not a critical, _wake up your engineers in the middle of the night_ kind of error 😆 \r\n\r\nI have been fairly loose with the use of `error!` myself, which probably needs to be revisited.",
        "pr_file_module": null
      },
      {
        "comment_id": "1792002794",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25432,
        "pr_file": "influxdb3_telemetry/src/sender.rs",
        "discussion_id": "1791935286",
        "commented_code": "@@ -77,24 +86,90 @@ pub(crate) async fn send_telemetry_in_background(\n     duration_secs: Duration,\n ) -> tokio::task::JoinHandle<()> {\n     tokio::spawn(async move {\n-        let telem_sender = TelemetrySender::new(\n+        let mut telem_sender = TelemetrySender::new(\n             reqwest::Client::new(),\n-            \"https://telemetry.influxdata.foo.com\".to_owned(),\n+            \"https://telemetry.influxdata.foo.com\",\n         );\n         let mut interval = tokio::time::interval(duration_secs);\n         interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);\n \n         loop {\n             interval.tick().await;\n-            let telemetry = store.snapshot();\n-            if let Err(e) = telem_sender.try_sending(&telemetry).await {\n-                // TODO: change to error! - until endpoint is decided keep\n-                //       this as debug log\n-                debug!(error = ?e, \"Cannot send telemetry\");\n-            }\n-            // if we tried sending and failed, we currently still reset the\n-            // metrics, it is ok to miss few samples\n-            store.reset_metrics();\n+            send_telemetry(&store, &mut telem_sender).await;\n         }\n     })\n }\n+\n+async fn send_telemetry(store: &Arc<TelemetryStore>, telem_sender: &mut TelemetrySender) {\n+    let telemetry = store.snapshot();\n+    if let Err(e) = telem_sender.try_sending(&telemetry).await {\n+        // TODO: change to error! - until endpoint is decided keep\n+        //       this as debug log",
        "comment_created_at": "2024-10-08T14:33:51+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "True - it'll stay as debug :smile: ",
        "pr_file_module": null
      }
    ]
  }
]