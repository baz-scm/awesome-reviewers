[
  {
    "discussion_id": "1994563821",
    "pr_number": 26135,
    "pr_file": "influxdb3_cache/src/last_cache/mod.rs",
    "created_at": "2025-03-14T02:10:39+00:00",
    "commented_code": "}\n     }\n \n-    #[tokio::test]\n-    async fn idempotent_cache_creation() {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1994563821",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26135,
        "pr_file": "influxdb3_cache/src/last_cache/mod.rs",
        "discussion_id": "1994563821",
        "commented_code": "@@ -1171,131 +1171,6 @@ mod tests {\n         }\n     }\n \n-    #[tokio::test]\n-    async fn idempotent_cache_creation() {",
        "comment_created_at": "2025-03-14T02:10:39+00:00",
        "comment_author": "hiltontj",
        "comment_body": "This test was removed because cache creation no longer behaves the same as before - previously if you made the same request to create a cache twice, the second one would succeed with a `204 NO CONTENT`, but have no effect; now, the second one fails with a `409 CONFLICT`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1781729109",
    "pr_number": 25415,
    "pr_file": "influxdb3/src/commands/serve.rs",
    "created_at": "2024-09-30T21:00:10+00:00",
    "commented_code": "/// for any hosts that share the same object store configuration, i.e., the same bucket.\n     #[clap(long = \"host-id\", env = \"INFLUXDB3_HOST_IDENTIFIER_PREFIX\", action)]\n     pub host_identifier_prefix: String,\n+\n+    /// The size of the in-memory Parquet cache in bytes.\n+    #[clap(\n+        long = \"parquet-mem-cache-size\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_SIZE\",\n+        default_value_t = 1024 * 1024 * 1024,\n+        action\n+    )]\n+    pub parquet_mem_cache_size: usize,\n+\n+    /// The percentage of entries to prune during a prune operation on the in-memory Parquet cache.\n+    ///\n+    /// This must be a number between 0 and 1.\n+    #[clap(\n+        long = \"parquet-mem-cache-prune-percentage\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_PRUNE_PERCENTAGE\",\n+        default_value = \"0.1\",\n+        action\n+    )]\n+    pub parquet_mem_cache_prune_percentage: PrunePercent,\n+\n+    /// The interval on which to check if the in-memory Parquet cache needs to be pruned.\n+    #[clap(\n+        long = \"parquet-mem-cache-prune-interval\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_PRUNE_INTERVAL\",\n+        default_value = \"10ms\",",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1781729109",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25415,
        "pr_file": "influxdb3/src/commands/serve.rs",
        "discussion_id": "1781729109",
        "commented_code": "@@ -216,6 +217,67 @@ pub struct Config {\n     /// for any hosts that share the same object store configuration, i.e., the same bucket.\n     #[clap(long = \"host-id\", env = \"INFLUXDB3_HOST_IDENTIFIER_PREFIX\", action)]\n     pub host_identifier_prefix: String,\n+\n+    /// The size of the in-memory Parquet cache in bytes.\n+    #[clap(\n+        long = \"parquet-mem-cache-size\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_SIZE\",\n+        default_value_t = 1024 * 1024 * 1024,\n+        action\n+    )]\n+    pub parquet_mem_cache_size: usize,\n+\n+    /// The percentage of entries to prune during a prune operation on the in-memory Parquet cache.\n+    ///\n+    /// This must be a number between 0 and 1.\n+    #[clap(\n+        long = \"parquet-mem-cache-prune-percentage\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_PRUNE_PERCENTAGE\",\n+        default_value = \"0.1\",\n+        action\n+    )]\n+    pub parquet_mem_cache_prune_percentage: PrunePercent,\n+\n+    /// The interval on which to check if the in-memory Parquet cache needs to be pruned.\n+    #[clap(\n+        long = \"parquet-mem-cache-prune-interval\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_PRUNE_INTERVAL\",\n+        default_value = \"10ms\",",
        "comment_created_at": "2024-09-30T21:00:10+00:00",
        "comment_author": "pauldix",
        "comment_body": "Does this just grab a read lock? Seems like we don't need to check this so frequently by default, once a second seems more than enough?",
        "pr_file_module": null
      },
      {
        "comment_id": "1781904600",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25415,
        "pr_file": "influxdb3/src/commands/serve.rs",
        "discussion_id": "1781729109",
        "commented_code": "@@ -216,6 +217,67 @@ pub struct Config {\n     /// for any hosts that share the same object store configuration, i.e., the same bucket.\n     #[clap(long = \"host-id\", env = \"INFLUXDB3_HOST_IDENTIFIER_PREFIX\", action)]\n     pub host_identifier_prefix: String,\n+\n+    /// The size of the in-memory Parquet cache in bytes.\n+    #[clap(\n+        long = \"parquet-mem-cache-size\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_SIZE\",\n+        default_value_t = 1024 * 1024 * 1024,\n+        action\n+    )]\n+    pub parquet_mem_cache_size: usize,\n+\n+    /// The percentage of entries to prune during a prune operation on the in-memory Parquet cache.\n+    ///\n+    /// This must be a number between 0 and 1.\n+    #[clap(\n+        long = \"parquet-mem-cache-prune-percentage\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_PRUNE_PERCENTAGE\",\n+        default_value = \"0.1\",\n+        action\n+    )]\n+    pub parquet_mem_cache_prune_percentage: PrunePercent,\n+\n+    /// The interval on which to check if the in-memory Parquet cache needs to be pruned.\n+    #[clap(\n+        long = \"parquet-mem-cache-prune-interval\",\n+        env = \"INFLUXDB3_PARQUET_MEM_CACHE_PRUNE_INTERVAL\",\n+        default_value = \"10ms\",",
        "comment_created_at": "2024-09-30T23:21:49+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Yeah, only if it needs to prune would lock the inner map, but I agree, this is a little over-zealous.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1707022323",
    "pr_number": 25196,
    "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
    "created_at": "2024-08-07T13:41:13+00:00",
    "commented_code": "+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1707022323",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1707022323",
        "commented_code": "@@ -0,0 +1,455 @@\n+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);",
        "comment_created_at": "2024-08-07T13:41:13+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Each of these functions acquires their own write lock. Originally, they were called from separate places, but if they always get called at the same time like this, it might be better to do the evict and write in the same call, under the same write lock.",
        "pr_file_module": null
      },
      {
        "comment_id": "1707050199",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1707022323",
        "commented_code": "@@ -0,0 +1,455 @@\n+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);",
        "comment_created_at": "2024-08-07T13:55:54+00:00",
        "comment_author": "pauldix",
        "comment_body": "I put it here for now, but I'm not sure we want to walk the last cache and evict on every single wal file flush. I think it should be changed to only do eviction on snapshot, that way we're not spending too much time on it.",
        "pr_file_module": null
      },
      {
        "comment_id": "1707054018",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1707022323",
        "commented_code": "@@ -0,0 +1,455 @@\n+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);",
        "comment_created_at": "2024-08-07T13:58:10+00:00",
        "comment_author": "pauldix",
        "comment_body": "Logged #25223 to track",
        "pr_file_module": null
      },
      {
        "comment_id": "1707060297",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1707022323",
        "commented_code": "@@ -0,0 +1,455 @@\n+use crate::catalog::{Catalog, DatabaseSchema};\n+use crate::chunk::BufferChunk;\n+use crate::last_cache::LastCacheProvider;\n+use crate::paths::ParquetFilePath;\n+use crate::persister::PersisterImpl;\n+use crate::write_buffer::parquet_chunk_from_file;\n+use crate::write_buffer::persisted_files::PersistedFiles;\n+use crate::write_buffer::table_buffer::TableBuffer;\n+use crate::{persister, write_buffer, ParquetFile, PersistedSnapshot, Persister};\n+use arrow::datatypes::SchemaRef;\n+use arrow::record_batch::RecordBatch;\n+use async_trait::async_trait;\n+use data_types::{\n+    ChunkId, ChunkOrder, PartitionKey, TableId, TimestampMinMax, TransitionPartitionId,\n+};\n+use datafusion::common::DataFusionError;\n+use datafusion::execution::context::SessionState;\n+use datafusion::logical_expr::Expr;\n+use datafusion_util::stream_from_batches;\n+use hashbrown::HashMap;\n+use influxdb3_wal::{SnapshotDetails, WalContents, WalFileNotifier, WalOp, WriteBatch};\n+use iox_query::chunk_statistics::{create_chunk_statistics, NoColumnRanges};\n+use iox_query::exec::Executor;\n+use iox_query::frontend::reorg::ReorgPlanner;\n+use iox_query::QueryChunk;\n+use observability_deps::tracing::{error, info};\n+use parking_lot::RwLock;\n+use parquet::format::FileMetaData;\n+use schema::sort::SortKey;\n+use schema::Schema;\n+use std::any::Any;\n+use std::sync::Arc;\n+use std::time::Duration;\n+use tokio::sync::oneshot;\n+use tokio::sync::oneshot::Receiver;\n+\n+#[derive(Debug)]\n+pub(crate) struct QueryableBuffer {\n+    executor: Arc<Executor>,\n+    catalog: Arc<Catalog>,\n+    last_cache_provider: Arc<LastCacheProvider>,\n+    persister: Arc<PersisterImpl>,\n+    persisted_files: Arc<PersistedFiles>,\n+    buffer: Arc<RwLock<BufferState>>,\n+}\n+\n+impl QueryableBuffer {\n+    pub(crate) fn new(\n+        executor: Arc<Executor>,\n+        catalog: Arc<Catalog>,\n+        persister: Arc<PersisterImpl>,\n+        last_cache_provider: Arc<LastCacheProvider>,\n+        persisted_files: Arc<PersistedFiles>,\n+    ) -> Self {\n+        let buffer = Arc::new(RwLock::new(BufferState::new(Arc::clone(&catalog))));\n+        Self {\n+            executor,\n+            catalog,\n+            last_cache_provider,\n+            persister,\n+            persisted_files,\n+            buffer,\n+        }\n+    }\n+\n+    pub(crate) fn get_table_chunks(\n+        &self,\n+        db_schema: Arc<DatabaseSchema>,\n+        table_name: &str,\n+        filters: &[Expr],\n+        projection: Option<&Vec<usize>>,\n+        _ctx: &SessionState,\n+    ) -> Result<Vec<Arc<dyn QueryChunk>>, DataFusionError> {\n+        let table = db_schema\n+            .tables\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let arrow_schema: SchemaRef = match projection {\n+            Some(projection) => Arc::new(table.schema.as_arrow().project(projection).unwrap()),\n+            None => table.schema.as_arrow(),\n+        };\n+\n+        let schema = schema::Schema::try_from(Arc::clone(&arrow_schema))\n+            .map_err(|e| DataFusionError::Execution(format!(\"schema error {}\", e)))?;\n+\n+        let mut chunks: Vec<Arc<dyn QueryChunk>> = vec![];\n+\n+        for parquet_file in self.persisted_files.get_files(&db_schema.name, table_name) {\n+            let parquet_chunk = parquet_chunk_from_file(\n+                &parquet_file,\n+                &schema,\n+                self.persister.object_store_url(),\n+                self.persister.object_store(),\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            );\n+\n+            chunks.push(Arc::new(parquet_chunk));\n+        }\n+\n+        let buffer = self.buffer.read();\n+\n+        let table_buffer = buffer\n+            .db_to_table\n+            .get(db_schema.name.as_ref())\n+            .ok_or_else(|| {\n+                DataFusionError::Execution(format!(\"database {} not found\", db_schema.name))\n+            })?\n+            .get(table_name)\n+            .ok_or_else(|| DataFusionError::Execution(format!(\"table {} not found\", table_name)))?;\n+\n+        let batches = table_buffer\n+            .record_batches(Arc::clone(&arrow_schema), filters)\n+            .map_err(|e| DataFusionError::Execution(format!(\"error getting batches {}\", e)))?;\n+\n+        let timestamp_min_max = table_buffer.timestamp_min_max();\n+\n+        let row_count = batches.iter().map(|batch| batch.num_rows()).sum::<usize>();\n+\n+        let chunk_stats = create_chunk_statistics(\n+            Some(row_count),\n+            &schema,\n+            Some(timestamp_min_max),\n+            &NoColumnRanges,\n+        );\n+\n+        chunks.push(Arc::new(BufferChunk {\n+            batches,\n+            schema: schema.clone(),\n+            stats: Arc::new(chunk_stats),\n+            partition_id: TransitionPartitionId::new(\n+                TableId::new(0),\n+                &PartitionKey::from(\"buffer_partition\"),\n+            ),\n+            sort_key: None,\n+            id: ChunkId::new(),\n+            chunk_order: ChunkOrder::new(\n+                chunks\n+                    .len()\n+                    .try_into()\n+                    .expect(\"should never have this many chunks\"),\n+            ),\n+        }));\n+\n+        Ok(chunks)\n+    }\n+\n+    /// Called when the wal has persisted a new file. Buffer the contents in memory and update the last cache so the data is queryable.\n+    fn buffer_contents(&self, write: WalContents) {\n+        let mut buffer = self.buffer.write();\n+        self.last_cache_provider.evict_expired_cache_entries();\n+        self.last_cache_provider.write_wal_contents_to_cache(&write);",
        "comment_created_at": "2024-08-07T14:01:16+00:00",
        "comment_author": "hiltontj",
        "comment_body": "> I think it should be changed to only do eviction on snapshot, that way we're not spending too much time on it.\r\n\r\nThat sounds reasonable.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1670686480",
    "pr_number": 25109,
    "pr_file": "influxdb3_write/src/last_cache.rs",
    "created_at": "2024-07-09T15:00:15+00:00",
    "commented_code": "+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1670686480",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25109,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1670686480",
        "commented_code": "@@ -0,0 +1,2225 @@\n+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name",
        "comment_created_at": "2024-07-09T15:00:15+00:00",
        "comment_author": "pauldix",
        "comment_body": "The behavior I like to see on creation of things is that if the user tries to create something with all the same arguments, it returns ok, if the arguments are different, then it returns with an error that it already exists. This might be handled higher up the stack in the API layer, but I'm not sure that this error would give the caller enough information to make the determination.\r\n\r\nThe reason I opt for this behavior is that it makes automation easier where they can call create as many times as they want and it'll always succeed as long as the settings are the same.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1670732610",
    "pr_number": 25109,
    "pr_file": "influxdb3_write/src/last_cache.rs",
    "created_at": "2024-07-09T15:26:35+00:00",
    "commented_code": "+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name\n+        if self\n+            .cache_map\n+            .read()\n+            .get(&db_name)\n+            .and_then(|db| db.get(&tbl_name))\n+            .is_some_and(|tbl| tbl.contains_key(&cache_name))\n+        {\n+            return Err(Error::CacheAlreadyExists);\n+        }\n+\n+        let value_columns = if let Some(mut vals) = value_columns {\n+            // if value columns are specified, check that they are present in the table schema\n+            for name in vals.iter() {\n+                if schema.field_by_name(name).is_none() {\n+                    return Err(Error::ValueColumnDoesNotExist {\n+                        column_name: name.into(),\n+                    });\n+                }\n+            }\n+            // double-check that time column is included\n+            let time_col = TIME_COLUMN_NAME.to_string();\n+            if !vals.contains(&time_col) {\n+                vals.push(time_col);\n+            }\n+            vals\n+        } else {\n+            // default to all non-key columns\n+            schema\n+                .iter()\n+                .filter_map(|(_, f)| {\n+                    if key_columns.contains(f.name()) {\n+                        None\n+                    } else {\n+                        Some(f.name().to_string())\n+                    }\n+                })\n+                .collect::<Vec<String>>()\n+        };\n+\n+        // build a schema that only holds the field columns\n+        let mut schema_builder = SchemaBuilder::new();\n+        for (t, name) in schema\n+            .iter()\n+            .filter(|&(_, f)| value_columns.contains(f.name()))\n+            .map(|(t, f)| (t, f.name()))\n+        {\n+            schema_builder.influx_column(name, t);\n+        }\n+\n+        // create the actual last cache:\n+        let last_cache = LastCache::new(\n+            count\n+                .unwrap_or(1)\n+                .try_into()\n+                .map_err(|_| Error::InvalidCacheSize)?,\n+            ttl.unwrap_or(DEFAULT_CACHE_TTL),\n+            key_columns,\n+            schema_builder.build()?,\n+        );\n+\n+        // get the write lock and insert:\n+        self.cache_map\n+            .write()\n+            .entry(db_name)\n+            .or_default()\n+            .entry(tbl_name)\n+            .or_default()\n+            .insert(cache_name, last_cache);\n+\n+        Ok(())\n+    }\n+\n+    /// Write a batch from the buffer into the cache by iterating over its database and table batches\n+    /// to find entries that belong in the cache.\n+    ///\n+    /// Only if rows are newer than the latest entry in the cache will they be entered.\n+    pub(crate) fn write_batch_to_cache(&self, write_batch: &WriteBatch) {\n+        let mut cache_map = self.cache_map.write();\n+        for (db_name, db_batch) in &write_batch.database_batches {\n+            if let Some(db_cache) = cache_map.get_mut(db_name.as_str()) {\n+                if db_cache.is_empty() {\n+                    continue;\n+                }\n+                for (tbl_name, tbl_batch) in &db_batch.table_batches {\n+                    if let Some(tbl_cache) = db_cache.get_mut(tbl_name) {\n+                        for (_, last_cache) in tbl_cache.iter_mut() {\n+                            for row in &tbl_batch.rows {\n+                                last_cache.push(row);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Recurse down the cache structure to evict expired cache entries, based on their respective\n+    /// time-to-live (TTL).\n+    pub(crate) fn evict_expired_cache_entries(&self) {\n+        let mut cache_map = self.cache_map.write();\n+        cache_map.iter_mut().for_each(|(_, db)| {\n+            db.iter_mut()\n+                .for_each(|(_, tbl)| tbl.iter_mut().for_each(|(_, lc)| lc.remove_expired()))\n+        });\n+    }\n+\n+    /// Output the records for a given cache as arrow [`RecordBatch`]es\n+    #[cfg(test)]\n+    fn get_cache_record_batches(\n+        &self,\n+        db_name: &str,\n+        tbl_name: &str,\n+        cache_name: Option<&str>,\n+        predicates: &[Predicate],\n+    ) -> Option<Result<Vec<RecordBatch>, ArrowError>> {\n+        self.cache_map\n+            .read()\n+            .get(db_name)\n+            .and_then(|db| db.get(tbl_name))\n+            .and_then(|tbl| {\n+                if let Some(name) = cache_name {\n+                    tbl.get(name)\n+                } else if tbl.len() == 1 {\n+                    tbl.iter().next().map(|(_, lc)| lc)\n+                } else {\n+                    None\n+                }\n+            })\n+            .map(|lc| lc.to_record_batches(predicates))\n+    }\n+}\n+\n+/// A Last-N-Values Cache\n+///\n+/// A hierarchical cache whose structure is determined by a set of `key_columns`, each of which\n+/// represents a level in the hierarchy. The lowest level of the hierarchy holds the last N values\n+/// for the field columns in the cache.\n+pub(crate) struct LastCache {\n+    /// The number of values to hold in the cache\n+    ///\n+    /// Once the cache reaches this size, old values will be evicted when new values are pushed in.\n+    count: LastCacheSize,\n+    /// The time-to-live (TTL) for values in the cache\n+    ///\n+    /// Once values have lived in the cache beyond this [`Duration`], they can be evicted using\n+    /// the [`remove_expired`][LastCache::remove_expired] method.\n+    ttl: Duration,\n+    /// The key columns for this cache\n+    key_columns: Vec<String>,\n+    /// The Influx Schema for the table that this cache is associated with\n+    schema: Schema,\n+    /// The internal state of the cache\n+    state: LastCacheState,\n+}\n+\n+impl LastCache {\n+    /// Create a new [`LastCache`]\n+    fn new(count: LastCacheSize, ttl: Duration, key_columns: Vec<String>, schema: Schema) -> Self {\n+        Self {\n+            count,\n+            ttl,\n+            key_columns,\n+            schema,\n+            state: LastCacheState::Init,\n+        }\n+    }\n+\n+    /// Push a [`Row`] from the write buffer into the cache\n+    ///\n+    /// If a key column is not present in the row, the row will be ignored.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This will panic if the internal cache state's keys are out-of-order with respect to the\n+    /// order of the `key_columns` on this [`LastCache`]\n+    pub(crate) fn push(&mut self, row: &Row) {\n+        let mut target = &mut self.state;\n+        let mut key_iter = self.key_columns.iter().peekable();\n+        while let (Some(key), peek) = (key_iter.next(), key_iter.peek()) {\n+            if target.is_init() {\n+                *target = LastCacheState::Key(LastCacheKey {\n+                    column_name: key.to_string(),\n+                    value_map: Default::default(),\n+                });\n+            }\n+            let Some(value) = row\n+                .fields\n+                .iter()\n+                .find(|f| f.name == *key)\n+                .map(|f| KeyValue::from(&f.value))\n+            else {\n+                // ignore the row if it does not contain all key columns\n+                return;\n+            };\n+            let cache_key = target.as_key_mut().unwrap();\n+            assert_eq!(\n+                &cache_key.column_name, key,\n+                \"key columns must match cache key order\"\n+            );\n+            target = cache_key.value_map.entry(value).or_insert_with(|| {\n+                if let Some(next_key) = peek {\n+                    LastCacheState::Key(LastCacheKey {\n+                        column_name: next_key.to_string(),\n+                        value_map: Default::default(),\n+                    })\n+                } else {\n+                    LastCacheState::Store(LastCacheStore::new(\n+                        self.count.into(),\n+                        self.ttl,\n+                        self.schema.clone(),\n+                    ))\n+                }\n+            });\n+        }\n+        // If there are no key columns we still need to initialize the state the first time:\n+        if target.is_init() {\n+            *target = LastCacheState::Store(LastCacheStore::new(\n+                self.count.into(),\n+                self.ttl,\n+                self.schema.clone(),\n+            ));\n+        }\n+        target\n+            .as_store_mut()\n+            .expect(\n+                \"cache target should be the actual store after iterating through all key columns\",\n+            )\n+            .push(row);\n+    }\n+\n+    /// Produce a set of [`RecordBatch`]es from the cache, using the given set of [`Predicate`]s\n+    fn to_record_batches(&self, predicates: &[Predicate]) -> Result<Vec<RecordBatch>, ArrowError> {\n+        // map the provided predicates on to the key columns\n+        // there may not be predicates provided for each key column, hence the Option\n+        let predicates: Vec<Option<Predicate>> = self\n+            .key_columns\n+            .iter()\n+            .map(|key| predicates.iter().find(|p| p.key == *key).cloned())\n+            .collect();\n+\n+        let mut caches = vec![ExtendedLastCacheState {\n+            state: &self.state,\n+            additional_columns: vec![],\n+        }];\n+\n+        for predicate in predicates {\n+            if caches.is_empty() {\n+                return Ok(vec![]);\n+            }\n+            let mut new_caches = vec![];\n+            'cache_loop: for c in caches {\n+                let cache_key = c.state.as_key().unwrap();\n+                if let Some(ref pred) = predicate {\n+                    let Some(next_state) = cache_key.evaluate_predicate(pred) else {\n+                        continue 'cache_loop;\n+                    };\n+                    new_caches.push(ExtendedLastCacheState {\n+                        state: next_state,\n+                        additional_columns: c.additional_columns.clone(),\n+                    });\n+                } else {\n+                    new_caches.extend(cache_key.value_map.iter().map(|(v, state)| {\n+                        let mut additional_columns = c.additional_columns.clone();\n+                        additional_columns.push((&cache_key.column_name, v));\n+                        ExtendedLastCacheState {\n+                            state,\n+                            additional_columns,\n+                        }\n+                    }));\n+                }\n+            }\n+            caches = new_caches;\n+        }\n+\n+        caches.into_iter().map(|c| c.to_record_batch()).collect()\n+    }\n+\n+    /// Convert a set of DataFusion filter [`Expr`]s into [`Predicate`]s\n+    ///\n+    /// This only handles binary expressions, e.g., `foo = 'bar'`, and will use the `key_columns`\n+    /// to filter out expressions that do not match key columns in the cache.\n+    fn convert_filter_exprs(&self, exprs: &[Expr]) -> Vec<Predicate> {\n+        exprs\n+            .iter()\n+            .filter_map(|expr| {\n+                if let Expr::BinaryExpr(BinaryExpr { left, op, right }) = expr {\n+                    if *op == Operator::Eq {\n+                        if let Expr::Column(c) = left.as_ref() {\n+                            let key = c.name.to_string();\n+                            if !self.key_columns.contains(&key) {\n+                                return None;\n+                            }\n+                            return match right.as_ref() {\n+                                Expr::Literal(ScalarValue::Utf8(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::String(v.to_owned()),\n+                                }),\n+                                Expr::Literal(ScalarValue::Boolean(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::Bool(*v),\n+                                }),\n+                                // TODO: handle integer types that can be casted up to i64/u64:\n+                                Expr::Literal(ScalarValue::Int64(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::Int(*v),\n+                                }),\n+                                Expr::Literal(ScalarValue::UInt64(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::UInt(*v),\n+                                }),\n+                                _ => None,\n+                            };\n+                        }\n+                    }\n+                }\n+                None\n+            })\n+            .collect()\n+    }\n+\n+    /// Remove expired values from the internal cache state\n+    fn remove_expired(&mut self) {\n+        self.state.remove_expired();\n+    }\n+}\n+\n+/// Extend a [`LastCacheState`] with additional columns\n+///\n+/// This is used for scenarios where key column values need to be produced in query outputs. Since\n+/// They are not stored in the terminal [`LastCacheStore`], we pass them down using this structure.\n+#[derive(Debug)]\n+struct ExtendedLastCacheState<'a> {\n+    state: &'a LastCacheState,\n+    additional_columns: Vec<(&'a String, &'a KeyValue)>,\n+}\n+\n+impl<'a> ExtendedLastCacheState<'a> {\n+    /// Produce a set of [`RecordBatch`]es from this extended state\n+    ///\n+    /// This converts any additional columns to arrow arrays which will extend the [`RecordBatch`]es\n+    /// produced by the inner [`LastCacheStore`]\n+    ///\n+    /// # Panics\n+    ///\n+    /// This assumes taht the `state` is a [`LastCacheStore`] and will panic otherwise.\n+    fn to_record_batch(&self) -> Result<RecordBatch, ArrowError> {\n+        let store = self\n+            .state\n+            .as_store()\n+            .expect(\"should only be calling to_record_batch when using a store\");\n+        let n = store.len();\n+        let extended: Option<(Vec<FieldRef>, Vec<ArrayRef>)> = if self.additional_columns.is_empty()\n+        {\n+            None\n+        } else {\n+            Some(\n+                self.additional_columns\n+                    .iter()\n+                    .map(|(name, value)| {\n+                        let field = Arc::new(value.as_arrow_field(*name));\n+                        match value {\n+                            KeyValue::String(v) => {\n+                                let mut builder = StringBuilder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::Int(v) => {\n+                                let mut builder = Int64Builder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::UInt(v) => {\n+                                let mut builder = UInt64Builder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::Bool(v) => {\n+                                let mut builder = BooleanBuilder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                        }\n+                    })\n+                    .collect(),\n+            )\n+        };\n+        store.to_record_batch(extended)\n+    }\n+}\n+\n+/// A predicate used for evaluating key column values in the cache on query\n+#[derive(Debug, Clone)]\n+struct Predicate {\n+    /// The left-hand-side of the predicate\n+    key: String,\n+    /// The right-hand-side of the predicate\n+    value: KeyValue,\n+}\n+\n+#[cfg(test)]\n+impl Predicate {\n+    fn new(key: impl Into<String>, value: KeyValue) -> Self {\n+        Self {\n+            key: key.into(),\n+            value,\n+        }\n+    }\n+}\n+\n+/// Represents the hierarchical last cache structure\n+#[derive(Debug)]\n+enum LastCacheState {\n+    /// An initialized state that is used for easy construction of the cache\n+    Init,\n+    /// Represents a branch node in the hierarchy of key columns for the cache\n+    Key(LastCacheKey),\n+    /// Represents a terminal node in the hierarchy, i.e., the cache of field values\n+    Store(LastCacheStore),\n+}\n+\n+impl LastCacheState {\n+    fn is_init(&self) -> bool {\n+        matches!(self, Self::Init)\n+    }\n+\n+    fn as_key(&self) -> Option<&LastCacheKey> {\n+        match self {\n+            LastCacheState::Key(key) => Some(key),\n+            LastCacheState::Store(_) | LastCacheState::Init => None,\n+        }\n+    }\n+\n+    fn as_store(&self) -> Option<&LastCacheStore> {\n+        match self {\n+            LastCacheState::Key(_) | LastCacheState::Init => None,\n+            LastCacheState::Store(store) => Some(store),\n+        }\n+    }\n+\n+    fn as_key_mut(&mut self) -> Option<&mut LastCacheKey> {\n+        match self {\n+            LastCacheState::Key(key) => Some(key),\n+            LastCacheState::Store(_) | LastCacheState::Init => None,\n+        }\n+    }\n+\n+    fn as_store_mut(&mut self) -> Option<&mut LastCacheStore> {\n+        match self {\n+            LastCacheState::Key(_) | LastCacheState::Init => None,\n+            LastCacheState::Store(store) => Some(store),\n+        }\n+    }\n+\n+    /// Remove expired values from this [`LastCacheState`]\n+    fn remove_expired(&mut self) {\n+        match self {\n+            LastCacheState::Key(k) => k.remove_expired(),\n+            LastCacheState::Store(s) => s.remove_expired(),\n+            LastCacheState::Init => (),\n+        }\n+    }\n+}\n+\n+/// Holds a node within a [`LastCache`] for a given key column\n+#[derive(Debug)]\n+struct LastCacheKey {\n+    /// The name of the key column\n+    column_name: String,\n+    /// A map of key column value to nested [`LastCacheState`]\n+    ///\n+    /// All values should point at either another key or a [`LastCacheStore`]\n+    value_map: HashMap<KeyValue, LastCacheState>,\n+}\n+\n+impl LastCacheKey {\n+    /// Evaluate the provided [`Predicate`] by using its value to lookup in this [`LastCacheKey`]'s\n+    /// value map.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This assumes that a predicate for this [`LastCacheKey`]'s column was provided, and will panic\n+    /// otherwise.\n+    fn evaluate_predicate(&self, predicate: &Predicate) -> Option<&LastCacheState> {\n+        if predicate.key != self.column_name {\n+            panic!(\n+                \"attempted to evaluate unexpected predicate with key {} for column named {}\",\n+                predicate.key, self.column_name\n+            );\n+        }\n+        self.value_map.get(&predicate.value)\n+    }\n+\n+    /// Remove expired values from any cache nested within this [`LastCacheKey`]\n+    fn remove_expired(&mut self) {\n+        self.value_map\n+            .iter_mut()\n+            .for_each(|(_, m)| m.remove_expired());",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1670732610",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25109,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1670732610",
        "commented_code": "@@ -0,0 +1,2225 @@\n+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name\n+        if self\n+            .cache_map\n+            .read()\n+            .get(&db_name)\n+            .and_then(|db| db.get(&tbl_name))\n+            .is_some_and(|tbl| tbl.contains_key(&cache_name))\n+        {\n+            return Err(Error::CacheAlreadyExists);\n+        }\n+\n+        let value_columns = if let Some(mut vals) = value_columns {\n+            // if value columns are specified, check that they are present in the table schema\n+            for name in vals.iter() {\n+                if schema.field_by_name(name).is_none() {\n+                    return Err(Error::ValueColumnDoesNotExist {\n+                        column_name: name.into(),\n+                    });\n+                }\n+            }\n+            // double-check that time column is included\n+            let time_col = TIME_COLUMN_NAME.to_string();\n+            if !vals.contains(&time_col) {\n+                vals.push(time_col);\n+            }\n+            vals\n+        } else {\n+            // default to all non-key columns\n+            schema\n+                .iter()\n+                .filter_map(|(_, f)| {\n+                    if key_columns.contains(f.name()) {\n+                        None\n+                    } else {\n+                        Some(f.name().to_string())\n+                    }\n+                })\n+                .collect::<Vec<String>>()\n+        };\n+\n+        // build a schema that only holds the field columns\n+        let mut schema_builder = SchemaBuilder::new();\n+        for (t, name) in schema\n+            .iter()\n+            .filter(|&(_, f)| value_columns.contains(f.name()))\n+            .map(|(t, f)| (t, f.name()))\n+        {\n+            schema_builder.influx_column(name, t);\n+        }\n+\n+        // create the actual last cache:\n+        let last_cache = LastCache::new(\n+            count\n+                .unwrap_or(1)\n+                .try_into()\n+                .map_err(|_| Error::InvalidCacheSize)?,\n+            ttl.unwrap_or(DEFAULT_CACHE_TTL),\n+            key_columns,\n+            schema_builder.build()?,\n+        );\n+\n+        // get the write lock and insert:\n+        self.cache_map\n+            .write()\n+            .entry(db_name)\n+            .or_default()\n+            .entry(tbl_name)\n+            .or_default()\n+            .insert(cache_name, last_cache);\n+\n+        Ok(())\n+    }\n+\n+    /// Write a batch from the buffer into the cache by iterating over its database and table batches\n+    /// to find entries that belong in the cache.\n+    ///\n+    /// Only if rows are newer than the latest entry in the cache will they be entered.\n+    pub(crate) fn write_batch_to_cache(&self, write_batch: &WriteBatch) {\n+        let mut cache_map = self.cache_map.write();\n+        for (db_name, db_batch) in &write_batch.database_batches {\n+            if let Some(db_cache) = cache_map.get_mut(db_name.as_str()) {\n+                if db_cache.is_empty() {\n+                    continue;\n+                }\n+                for (tbl_name, tbl_batch) in &db_batch.table_batches {\n+                    if let Some(tbl_cache) = db_cache.get_mut(tbl_name) {\n+                        for (_, last_cache) in tbl_cache.iter_mut() {\n+                            for row in &tbl_batch.rows {\n+                                last_cache.push(row);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Recurse down the cache structure to evict expired cache entries, based on their respective\n+    /// time-to-live (TTL).\n+    pub(crate) fn evict_expired_cache_entries(&self) {\n+        let mut cache_map = self.cache_map.write();\n+        cache_map.iter_mut().for_each(|(_, db)| {\n+            db.iter_mut()\n+                .for_each(|(_, tbl)| tbl.iter_mut().for_each(|(_, lc)| lc.remove_expired()))\n+        });\n+    }\n+\n+    /// Output the records for a given cache as arrow [`RecordBatch`]es\n+    #[cfg(test)]\n+    fn get_cache_record_batches(\n+        &self,\n+        db_name: &str,\n+        tbl_name: &str,\n+        cache_name: Option<&str>,\n+        predicates: &[Predicate],\n+    ) -> Option<Result<Vec<RecordBatch>, ArrowError>> {\n+        self.cache_map\n+            .read()\n+            .get(db_name)\n+            .and_then(|db| db.get(tbl_name))\n+            .and_then(|tbl| {\n+                if let Some(name) = cache_name {\n+                    tbl.get(name)\n+                } else if tbl.len() == 1 {\n+                    tbl.iter().next().map(|(_, lc)| lc)\n+                } else {\n+                    None\n+                }\n+            })\n+            .map(|lc| lc.to_record_batches(predicates))\n+    }\n+}\n+\n+/// A Last-N-Values Cache\n+///\n+/// A hierarchical cache whose structure is determined by a set of `key_columns`, each of which\n+/// represents a level in the hierarchy. The lowest level of the hierarchy holds the last N values\n+/// for the field columns in the cache.\n+pub(crate) struct LastCache {\n+    /// The number of values to hold in the cache\n+    ///\n+    /// Once the cache reaches this size, old values will be evicted when new values are pushed in.\n+    count: LastCacheSize,\n+    /// The time-to-live (TTL) for values in the cache\n+    ///\n+    /// Once values have lived in the cache beyond this [`Duration`], they can be evicted using\n+    /// the [`remove_expired`][LastCache::remove_expired] method.\n+    ttl: Duration,\n+    /// The key columns for this cache\n+    key_columns: Vec<String>,\n+    /// The Influx Schema for the table that this cache is associated with\n+    schema: Schema,\n+    /// The internal state of the cache\n+    state: LastCacheState,\n+}\n+\n+impl LastCache {\n+    /// Create a new [`LastCache`]\n+    fn new(count: LastCacheSize, ttl: Duration, key_columns: Vec<String>, schema: Schema) -> Self {\n+        Self {\n+            count,\n+            ttl,\n+            key_columns,\n+            schema,\n+            state: LastCacheState::Init,\n+        }\n+    }\n+\n+    /// Push a [`Row`] from the write buffer into the cache\n+    ///\n+    /// If a key column is not present in the row, the row will be ignored.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This will panic if the internal cache state's keys are out-of-order with respect to the\n+    /// order of the `key_columns` on this [`LastCache`]\n+    pub(crate) fn push(&mut self, row: &Row) {\n+        let mut target = &mut self.state;\n+        let mut key_iter = self.key_columns.iter().peekable();\n+        while let (Some(key), peek) = (key_iter.next(), key_iter.peek()) {\n+            if target.is_init() {\n+                *target = LastCacheState::Key(LastCacheKey {\n+                    column_name: key.to_string(),\n+                    value_map: Default::default(),\n+                });\n+            }\n+            let Some(value) = row\n+                .fields\n+                .iter()\n+                .find(|f| f.name == *key)\n+                .map(|f| KeyValue::from(&f.value))\n+            else {\n+                // ignore the row if it does not contain all key columns\n+                return;\n+            };\n+            let cache_key = target.as_key_mut().unwrap();\n+            assert_eq!(\n+                &cache_key.column_name, key,\n+                \"key columns must match cache key order\"\n+            );\n+            target = cache_key.value_map.entry(value).or_insert_with(|| {\n+                if let Some(next_key) = peek {\n+                    LastCacheState::Key(LastCacheKey {\n+                        column_name: next_key.to_string(),\n+                        value_map: Default::default(),\n+                    })\n+                } else {\n+                    LastCacheState::Store(LastCacheStore::new(\n+                        self.count.into(),\n+                        self.ttl,\n+                        self.schema.clone(),\n+                    ))\n+                }\n+            });\n+        }\n+        // If there are no key columns we still need to initialize the state the first time:\n+        if target.is_init() {\n+            *target = LastCacheState::Store(LastCacheStore::new(\n+                self.count.into(),\n+                self.ttl,\n+                self.schema.clone(),\n+            ));\n+        }\n+        target\n+            .as_store_mut()\n+            .expect(\n+                \"cache target should be the actual store after iterating through all key columns\",\n+            )\n+            .push(row);\n+    }\n+\n+    /// Produce a set of [`RecordBatch`]es from the cache, using the given set of [`Predicate`]s\n+    fn to_record_batches(&self, predicates: &[Predicate]) -> Result<Vec<RecordBatch>, ArrowError> {\n+        // map the provided predicates on to the key columns\n+        // there may not be predicates provided for each key column, hence the Option\n+        let predicates: Vec<Option<Predicate>> = self\n+            .key_columns\n+            .iter()\n+            .map(|key| predicates.iter().find(|p| p.key == *key).cloned())\n+            .collect();\n+\n+        let mut caches = vec![ExtendedLastCacheState {\n+            state: &self.state,\n+            additional_columns: vec![],\n+        }];\n+\n+        for predicate in predicates {\n+            if caches.is_empty() {\n+                return Ok(vec![]);\n+            }\n+            let mut new_caches = vec![];\n+            'cache_loop: for c in caches {\n+                let cache_key = c.state.as_key().unwrap();\n+                if let Some(ref pred) = predicate {\n+                    let Some(next_state) = cache_key.evaluate_predicate(pred) else {\n+                        continue 'cache_loop;\n+                    };\n+                    new_caches.push(ExtendedLastCacheState {\n+                        state: next_state,\n+                        additional_columns: c.additional_columns.clone(),\n+                    });\n+                } else {\n+                    new_caches.extend(cache_key.value_map.iter().map(|(v, state)| {\n+                        let mut additional_columns = c.additional_columns.clone();\n+                        additional_columns.push((&cache_key.column_name, v));\n+                        ExtendedLastCacheState {\n+                            state,\n+                            additional_columns,\n+                        }\n+                    }));\n+                }\n+            }\n+            caches = new_caches;\n+        }\n+\n+        caches.into_iter().map(|c| c.to_record_batch()).collect()\n+    }\n+\n+    /// Convert a set of DataFusion filter [`Expr`]s into [`Predicate`]s\n+    ///\n+    /// This only handles binary expressions, e.g., `foo = 'bar'`, and will use the `key_columns`\n+    /// to filter out expressions that do not match key columns in the cache.\n+    fn convert_filter_exprs(&self, exprs: &[Expr]) -> Vec<Predicate> {\n+        exprs\n+            .iter()\n+            .filter_map(|expr| {\n+                if let Expr::BinaryExpr(BinaryExpr { left, op, right }) = expr {\n+                    if *op == Operator::Eq {\n+                        if let Expr::Column(c) = left.as_ref() {\n+                            let key = c.name.to_string();\n+                            if !self.key_columns.contains(&key) {\n+                                return None;\n+                            }\n+                            return match right.as_ref() {\n+                                Expr::Literal(ScalarValue::Utf8(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::String(v.to_owned()),\n+                                }),\n+                                Expr::Literal(ScalarValue::Boolean(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::Bool(*v),\n+                                }),\n+                                // TODO: handle integer types that can be casted up to i64/u64:\n+                                Expr::Literal(ScalarValue::Int64(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::Int(*v),\n+                                }),\n+                                Expr::Literal(ScalarValue::UInt64(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::UInt(*v),\n+                                }),\n+                                _ => None,\n+                            };\n+                        }\n+                    }\n+                }\n+                None\n+            })\n+            .collect()\n+    }\n+\n+    /// Remove expired values from the internal cache state\n+    fn remove_expired(&mut self) {\n+        self.state.remove_expired();\n+    }\n+}\n+\n+/// Extend a [`LastCacheState`] with additional columns\n+///\n+/// This is used for scenarios where key column values need to be produced in query outputs. Since\n+/// They are not stored in the terminal [`LastCacheStore`], we pass them down using this structure.\n+#[derive(Debug)]\n+struct ExtendedLastCacheState<'a> {\n+    state: &'a LastCacheState,\n+    additional_columns: Vec<(&'a String, &'a KeyValue)>,\n+}\n+\n+impl<'a> ExtendedLastCacheState<'a> {\n+    /// Produce a set of [`RecordBatch`]es from this extended state\n+    ///\n+    /// This converts any additional columns to arrow arrays which will extend the [`RecordBatch`]es\n+    /// produced by the inner [`LastCacheStore`]\n+    ///\n+    /// # Panics\n+    ///\n+    /// This assumes taht the `state` is a [`LastCacheStore`] and will panic otherwise.\n+    fn to_record_batch(&self) -> Result<RecordBatch, ArrowError> {\n+        let store = self\n+            .state\n+            .as_store()\n+            .expect(\"should only be calling to_record_batch when using a store\");\n+        let n = store.len();\n+        let extended: Option<(Vec<FieldRef>, Vec<ArrayRef>)> = if self.additional_columns.is_empty()\n+        {\n+            None\n+        } else {\n+            Some(\n+                self.additional_columns\n+                    .iter()\n+                    .map(|(name, value)| {\n+                        let field = Arc::new(value.as_arrow_field(*name));\n+                        match value {\n+                            KeyValue::String(v) => {\n+                                let mut builder = StringBuilder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::Int(v) => {\n+                                let mut builder = Int64Builder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::UInt(v) => {\n+                                let mut builder = UInt64Builder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::Bool(v) => {\n+                                let mut builder = BooleanBuilder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                        }\n+                    })\n+                    .collect(),\n+            )\n+        };\n+        store.to_record_batch(extended)\n+    }\n+}\n+\n+/// A predicate used for evaluating key column values in the cache on query\n+#[derive(Debug, Clone)]\n+struct Predicate {\n+    /// The left-hand-side of the predicate\n+    key: String,\n+    /// The right-hand-side of the predicate\n+    value: KeyValue,\n+}\n+\n+#[cfg(test)]\n+impl Predicate {\n+    fn new(key: impl Into<String>, value: KeyValue) -> Self {\n+        Self {\n+            key: key.into(),\n+            value,\n+        }\n+    }\n+}\n+\n+/// Represents the hierarchical last cache structure\n+#[derive(Debug)]\n+enum LastCacheState {\n+    /// An initialized state that is used for easy construction of the cache\n+    Init,\n+    /// Represents a branch node in the hierarchy of key columns for the cache\n+    Key(LastCacheKey),\n+    /// Represents a terminal node in the hierarchy, i.e., the cache of field values\n+    Store(LastCacheStore),\n+}\n+\n+impl LastCacheState {\n+    fn is_init(&self) -> bool {\n+        matches!(self, Self::Init)\n+    }\n+\n+    fn as_key(&self) -> Option<&LastCacheKey> {\n+        match self {\n+            LastCacheState::Key(key) => Some(key),\n+            LastCacheState::Store(_) | LastCacheState::Init => None,\n+        }\n+    }\n+\n+    fn as_store(&self) -> Option<&LastCacheStore> {\n+        match self {\n+            LastCacheState::Key(_) | LastCacheState::Init => None,\n+            LastCacheState::Store(store) => Some(store),\n+        }\n+    }\n+\n+    fn as_key_mut(&mut self) -> Option<&mut LastCacheKey> {\n+        match self {\n+            LastCacheState::Key(key) => Some(key),\n+            LastCacheState::Store(_) | LastCacheState::Init => None,\n+        }\n+    }\n+\n+    fn as_store_mut(&mut self) -> Option<&mut LastCacheStore> {\n+        match self {\n+            LastCacheState::Key(_) | LastCacheState::Init => None,\n+            LastCacheState::Store(store) => Some(store),\n+        }\n+    }\n+\n+    /// Remove expired values from this [`LastCacheState`]\n+    fn remove_expired(&mut self) {\n+        match self {\n+            LastCacheState::Key(k) => k.remove_expired(),\n+            LastCacheState::Store(s) => s.remove_expired(),\n+            LastCacheState::Init => (),\n+        }\n+    }\n+}\n+\n+/// Holds a node within a [`LastCache`] for a given key column\n+#[derive(Debug)]\n+struct LastCacheKey {\n+    /// The name of the key column\n+    column_name: String,\n+    /// A map of key column value to nested [`LastCacheState`]\n+    ///\n+    /// All values should point at either another key or a [`LastCacheStore`]\n+    value_map: HashMap<KeyValue, LastCacheState>,\n+}\n+\n+impl LastCacheKey {\n+    /// Evaluate the provided [`Predicate`] by using its value to lookup in this [`LastCacheKey`]'s\n+    /// value map.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This assumes that a predicate for this [`LastCacheKey`]'s column was provided, and will panic\n+    /// otherwise.\n+    fn evaluate_predicate(&self, predicate: &Predicate) -> Option<&LastCacheState> {\n+        if predicate.key != self.column_name {\n+            panic!(\n+                \"attempted to evaluate unexpected predicate with key {} for column named {}\",\n+                predicate.key, self.column_name\n+            );\n+        }\n+        self.value_map.get(&predicate.value)\n+    }\n+\n+    /// Remove expired values from any cache nested within this [`LastCacheKey`]\n+    fn remove_expired(&mut self) {\n+        self.value_map\n+            .iter_mut()\n+            .for_each(|(_, m)| m.remove_expired());",
        "comment_created_at": "2024-07-09T15:26:35+00:00",
        "comment_author": "pauldix",
        "comment_body": "If all rows have been expired, this key should be removed from the map. Otherwise, key values that stop sending data (think ephemeral things like container id, etc) will blow up the size of the cache over time with a bunch of entry map entries. (I'm assuming I'm reading this correctly and only the values are getting removed).\r\n\r\nThis should be true walking up the tree. Each key/value should be removed from the map if all children are empty.",
        "pr_file_module": null
      },
      {
        "comment_id": "1670769672",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25109,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1670732610",
        "commented_code": "@@ -0,0 +1,2225 @@\n+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name\n+        if self\n+            .cache_map\n+            .read()\n+            .get(&db_name)\n+            .and_then(|db| db.get(&tbl_name))\n+            .is_some_and(|tbl| tbl.contains_key(&cache_name))\n+        {\n+            return Err(Error::CacheAlreadyExists);\n+        }\n+\n+        let value_columns = if let Some(mut vals) = value_columns {\n+            // if value columns are specified, check that they are present in the table schema\n+            for name in vals.iter() {\n+                if schema.field_by_name(name).is_none() {\n+                    return Err(Error::ValueColumnDoesNotExist {\n+                        column_name: name.into(),\n+                    });\n+                }\n+            }\n+            // double-check that time column is included\n+            let time_col = TIME_COLUMN_NAME.to_string();\n+            if !vals.contains(&time_col) {\n+                vals.push(time_col);\n+            }\n+            vals\n+        } else {\n+            // default to all non-key columns\n+            schema\n+                .iter()\n+                .filter_map(|(_, f)| {\n+                    if key_columns.contains(f.name()) {\n+                        None\n+                    } else {\n+                        Some(f.name().to_string())\n+                    }\n+                })\n+                .collect::<Vec<String>>()\n+        };\n+\n+        // build a schema that only holds the field columns\n+        let mut schema_builder = SchemaBuilder::new();\n+        for (t, name) in schema\n+            .iter()\n+            .filter(|&(_, f)| value_columns.contains(f.name()))\n+            .map(|(t, f)| (t, f.name()))\n+        {\n+            schema_builder.influx_column(name, t);\n+        }\n+\n+        // create the actual last cache:\n+        let last_cache = LastCache::new(\n+            count\n+                .unwrap_or(1)\n+                .try_into()\n+                .map_err(|_| Error::InvalidCacheSize)?,\n+            ttl.unwrap_or(DEFAULT_CACHE_TTL),\n+            key_columns,\n+            schema_builder.build()?,\n+        );\n+\n+        // get the write lock and insert:\n+        self.cache_map\n+            .write()\n+            .entry(db_name)\n+            .or_default()\n+            .entry(tbl_name)\n+            .or_default()\n+            .insert(cache_name, last_cache);\n+\n+        Ok(())\n+    }\n+\n+    /// Write a batch from the buffer into the cache by iterating over its database and table batches\n+    /// to find entries that belong in the cache.\n+    ///\n+    /// Only if rows are newer than the latest entry in the cache will they be entered.\n+    pub(crate) fn write_batch_to_cache(&self, write_batch: &WriteBatch) {\n+        let mut cache_map = self.cache_map.write();\n+        for (db_name, db_batch) in &write_batch.database_batches {\n+            if let Some(db_cache) = cache_map.get_mut(db_name.as_str()) {\n+                if db_cache.is_empty() {\n+                    continue;\n+                }\n+                for (tbl_name, tbl_batch) in &db_batch.table_batches {\n+                    if let Some(tbl_cache) = db_cache.get_mut(tbl_name) {\n+                        for (_, last_cache) in tbl_cache.iter_mut() {\n+                            for row in &tbl_batch.rows {\n+                                last_cache.push(row);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Recurse down the cache structure to evict expired cache entries, based on their respective\n+    /// time-to-live (TTL).\n+    pub(crate) fn evict_expired_cache_entries(&self) {\n+        let mut cache_map = self.cache_map.write();\n+        cache_map.iter_mut().for_each(|(_, db)| {\n+            db.iter_mut()\n+                .for_each(|(_, tbl)| tbl.iter_mut().for_each(|(_, lc)| lc.remove_expired()))\n+        });\n+    }\n+\n+    /// Output the records for a given cache as arrow [`RecordBatch`]es\n+    #[cfg(test)]\n+    fn get_cache_record_batches(\n+        &self,\n+        db_name: &str,\n+        tbl_name: &str,\n+        cache_name: Option<&str>,\n+        predicates: &[Predicate],\n+    ) -> Option<Result<Vec<RecordBatch>, ArrowError>> {\n+        self.cache_map\n+            .read()\n+            .get(db_name)\n+            .and_then(|db| db.get(tbl_name))\n+            .and_then(|tbl| {\n+                if let Some(name) = cache_name {\n+                    tbl.get(name)\n+                } else if tbl.len() == 1 {\n+                    tbl.iter().next().map(|(_, lc)| lc)\n+                } else {\n+                    None\n+                }\n+            })\n+            .map(|lc| lc.to_record_batches(predicates))\n+    }\n+}\n+\n+/// A Last-N-Values Cache\n+///\n+/// A hierarchical cache whose structure is determined by a set of `key_columns`, each of which\n+/// represents a level in the hierarchy. The lowest level of the hierarchy holds the last N values\n+/// for the field columns in the cache.\n+pub(crate) struct LastCache {\n+    /// The number of values to hold in the cache\n+    ///\n+    /// Once the cache reaches this size, old values will be evicted when new values are pushed in.\n+    count: LastCacheSize,\n+    /// The time-to-live (TTL) for values in the cache\n+    ///\n+    /// Once values have lived in the cache beyond this [`Duration`], they can be evicted using\n+    /// the [`remove_expired`][LastCache::remove_expired] method.\n+    ttl: Duration,\n+    /// The key columns for this cache\n+    key_columns: Vec<String>,\n+    /// The Influx Schema for the table that this cache is associated with\n+    schema: Schema,\n+    /// The internal state of the cache\n+    state: LastCacheState,\n+}\n+\n+impl LastCache {\n+    /// Create a new [`LastCache`]\n+    fn new(count: LastCacheSize, ttl: Duration, key_columns: Vec<String>, schema: Schema) -> Self {\n+        Self {\n+            count,\n+            ttl,\n+            key_columns,\n+            schema,\n+            state: LastCacheState::Init,\n+        }\n+    }\n+\n+    /// Push a [`Row`] from the write buffer into the cache\n+    ///\n+    /// If a key column is not present in the row, the row will be ignored.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This will panic if the internal cache state's keys are out-of-order with respect to the\n+    /// order of the `key_columns` on this [`LastCache`]\n+    pub(crate) fn push(&mut self, row: &Row) {\n+        let mut target = &mut self.state;\n+        let mut key_iter = self.key_columns.iter().peekable();\n+        while let (Some(key), peek) = (key_iter.next(), key_iter.peek()) {\n+            if target.is_init() {\n+                *target = LastCacheState::Key(LastCacheKey {\n+                    column_name: key.to_string(),\n+                    value_map: Default::default(),\n+                });\n+            }\n+            let Some(value) = row\n+                .fields\n+                .iter()\n+                .find(|f| f.name == *key)\n+                .map(|f| KeyValue::from(&f.value))\n+            else {\n+                // ignore the row if it does not contain all key columns\n+                return;\n+            };\n+            let cache_key = target.as_key_mut().unwrap();\n+            assert_eq!(\n+                &cache_key.column_name, key,\n+                \"key columns must match cache key order\"\n+            );\n+            target = cache_key.value_map.entry(value).or_insert_with(|| {\n+                if let Some(next_key) = peek {\n+                    LastCacheState::Key(LastCacheKey {\n+                        column_name: next_key.to_string(),\n+                        value_map: Default::default(),\n+                    })\n+                } else {\n+                    LastCacheState::Store(LastCacheStore::new(\n+                        self.count.into(),\n+                        self.ttl,\n+                        self.schema.clone(),\n+                    ))\n+                }\n+            });\n+        }\n+        // If there are no key columns we still need to initialize the state the first time:\n+        if target.is_init() {\n+            *target = LastCacheState::Store(LastCacheStore::new(\n+                self.count.into(),\n+                self.ttl,\n+                self.schema.clone(),\n+            ));\n+        }\n+        target\n+            .as_store_mut()\n+            .expect(\n+                \"cache target should be the actual store after iterating through all key columns\",\n+            )\n+            .push(row);\n+    }\n+\n+    /// Produce a set of [`RecordBatch`]es from the cache, using the given set of [`Predicate`]s\n+    fn to_record_batches(&self, predicates: &[Predicate]) -> Result<Vec<RecordBatch>, ArrowError> {\n+        // map the provided predicates on to the key columns\n+        // there may not be predicates provided for each key column, hence the Option\n+        let predicates: Vec<Option<Predicate>> = self\n+            .key_columns\n+            .iter()\n+            .map(|key| predicates.iter().find(|p| p.key == *key).cloned())\n+            .collect();\n+\n+        let mut caches = vec![ExtendedLastCacheState {\n+            state: &self.state,\n+            additional_columns: vec![],\n+        }];\n+\n+        for predicate in predicates {\n+            if caches.is_empty() {\n+                return Ok(vec![]);\n+            }\n+            let mut new_caches = vec![];\n+            'cache_loop: for c in caches {\n+                let cache_key = c.state.as_key().unwrap();\n+                if let Some(ref pred) = predicate {\n+                    let Some(next_state) = cache_key.evaluate_predicate(pred) else {\n+                        continue 'cache_loop;\n+                    };\n+                    new_caches.push(ExtendedLastCacheState {\n+                        state: next_state,\n+                        additional_columns: c.additional_columns.clone(),\n+                    });\n+                } else {\n+                    new_caches.extend(cache_key.value_map.iter().map(|(v, state)| {\n+                        let mut additional_columns = c.additional_columns.clone();\n+                        additional_columns.push((&cache_key.column_name, v));\n+                        ExtendedLastCacheState {\n+                            state,\n+                            additional_columns,\n+                        }\n+                    }));\n+                }\n+            }\n+            caches = new_caches;\n+        }\n+\n+        caches.into_iter().map(|c| c.to_record_batch()).collect()\n+    }\n+\n+    /// Convert a set of DataFusion filter [`Expr`]s into [`Predicate`]s\n+    ///\n+    /// This only handles binary expressions, e.g., `foo = 'bar'`, and will use the `key_columns`\n+    /// to filter out expressions that do not match key columns in the cache.\n+    fn convert_filter_exprs(&self, exprs: &[Expr]) -> Vec<Predicate> {\n+        exprs\n+            .iter()\n+            .filter_map(|expr| {\n+                if let Expr::BinaryExpr(BinaryExpr { left, op, right }) = expr {\n+                    if *op == Operator::Eq {\n+                        if let Expr::Column(c) = left.as_ref() {\n+                            let key = c.name.to_string();\n+                            if !self.key_columns.contains(&key) {\n+                                return None;\n+                            }\n+                            return match right.as_ref() {\n+                                Expr::Literal(ScalarValue::Utf8(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::String(v.to_owned()),\n+                                }),\n+                                Expr::Literal(ScalarValue::Boolean(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::Bool(*v),\n+                                }),\n+                                // TODO: handle integer types that can be casted up to i64/u64:\n+                                Expr::Literal(ScalarValue::Int64(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::Int(*v),\n+                                }),\n+                                Expr::Literal(ScalarValue::UInt64(Some(v))) => Some(Predicate {\n+                                    key,\n+                                    value: KeyValue::UInt(*v),\n+                                }),\n+                                _ => None,\n+                            };\n+                        }\n+                    }\n+                }\n+                None\n+            })\n+            .collect()\n+    }\n+\n+    /// Remove expired values from the internal cache state\n+    fn remove_expired(&mut self) {\n+        self.state.remove_expired();\n+    }\n+}\n+\n+/// Extend a [`LastCacheState`] with additional columns\n+///\n+/// This is used for scenarios where key column values need to be produced in query outputs. Since\n+/// They are not stored in the terminal [`LastCacheStore`], we pass them down using this structure.\n+#[derive(Debug)]\n+struct ExtendedLastCacheState<'a> {\n+    state: &'a LastCacheState,\n+    additional_columns: Vec<(&'a String, &'a KeyValue)>,\n+}\n+\n+impl<'a> ExtendedLastCacheState<'a> {\n+    /// Produce a set of [`RecordBatch`]es from this extended state\n+    ///\n+    /// This converts any additional columns to arrow arrays which will extend the [`RecordBatch`]es\n+    /// produced by the inner [`LastCacheStore`]\n+    ///\n+    /// # Panics\n+    ///\n+    /// This assumes taht the `state` is a [`LastCacheStore`] and will panic otherwise.\n+    fn to_record_batch(&self) -> Result<RecordBatch, ArrowError> {\n+        let store = self\n+            .state\n+            .as_store()\n+            .expect(\"should only be calling to_record_batch when using a store\");\n+        let n = store.len();\n+        let extended: Option<(Vec<FieldRef>, Vec<ArrayRef>)> = if self.additional_columns.is_empty()\n+        {\n+            None\n+        } else {\n+            Some(\n+                self.additional_columns\n+                    .iter()\n+                    .map(|(name, value)| {\n+                        let field = Arc::new(value.as_arrow_field(*name));\n+                        match value {\n+                            KeyValue::String(v) => {\n+                                let mut builder = StringBuilder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::Int(v) => {\n+                                let mut builder = Int64Builder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::UInt(v) => {\n+                                let mut builder = UInt64Builder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                            KeyValue::Bool(v) => {\n+                                let mut builder = BooleanBuilder::new();\n+                                for _ in 0..n {\n+                                    builder.append_value(*v);\n+                                }\n+                                (field, Arc::new(builder.finish()) as ArrayRef)\n+                            }\n+                        }\n+                    })\n+                    .collect(),\n+            )\n+        };\n+        store.to_record_batch(extended)\n+    }\n+}\n+\n+/// A predicate used for evaluating key column values in the cache on query\n+#[derive(Debug, Clone)]\n+struct Predicate {\n+    /// The left-hand-side of the predicate\n+    key: String,\n+    /// The right-hand-side of the predicate\n+    value: KeyValue,\n+}\n+\n+#[cfg(test)]\n+impl Predicate {\n+    fn new(key: impl Into<String>, value: KeyValue) -> Self {\n+        Self {\n+            key: key.into(),\n+            value,\n+        }\n+    }\n+}\n+\n+/// Represents the hierarchical last cache structure\n+#[derive(Debug)]\n+enum LastCacheState {\n+    /// An initialized state that is used for easy construction of the cache\n+    Init,\n+    /// Represents a branch node in the hierarchy of key columns for the cache\n+    Key(LastCacheKey),\n+    /// Represents a terminal node in the hierarchy, i.e., the cache of field values\n+    Store(LastCacheStore),\n+}\n+\n+impl LastCacheState {\n+    fn is_init(&self) -> bool {\n+        matches!(self, Self::Init)\n+    }\n+\n+    fn as_key(&self) -> Option<&LastCacheKey> {\n+        match self {\n+            LastCacheState::Key(key) => Some(key),\n+            LastCacheState::Store(_) | LastCacheState::Init => None,\n+        }\n+    }\n+\n+    fn as_store(&self) -> Option<&LastCacheStore> {\n+        match self {\n+            LastCacheState::Key(_) | LastCacheState::Init => None,\n+            LastCacheState::Store(store) => Some(store),\n+        }\n+    }\n+\n+    fn as_key_mut(&mut self) -> Option<&mut LastCacheKey> {\n+        match self {\n+            LastCacheState::Key(key) => Some(key),\n+            LastCacheState::Store(_) | LastCacheState::Init => None,\n+        }\n+    }\n+\n+    fn as_store_mut(&mut self) -> Option<&mut LastCacheStore> {\n+        match self {\n+            LastCacheState::Key(_) | LastCacheState::Init => None,\n+            LastCacheState::Store(store) => Some(store),\n+        }\n+    }\n+\n+    /// Remove expired values from this [`LastCacheState`]\n+    fn remove_expired(&mut self) {\n+        match self {\n+            LastCacheState::Key(k) => k.remove_expired(),\n+            LastCacheState::Store(s) => s.remove_expired(),\n+            LastCacheState::Init => (),\n+        }\n+    }\n+}\n+\n+/// Holds a node within a [`LastCache`] for a given key column\n+#[derive(Debug)]\n+struct LastCacheKey {\n+    /// The name of the key column\n+    column_name: String,\n+    /// A map of key column value to nested [`LastCacheState`]\n+    ///\n+    /// All values should point at either another key or a [`LastCacheStore`]\n+    value_map: HashMap<KeyValue, LastCacheState>,\n+}\n+\n+impl LastCacheKey {\n+    /// Evaluate the provided [`Predicate`] by using its value to lookup in this [`LastCacheKey`]'s\n+    /// value map.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This assumes that a predicate for this [`LastCacheKey`]'s column was provided, and will panic\n+    /// otherwise.\n+    fn evaluate_predicate(&self, predicate: &Predicate) -> Option<&LastCacheState> {\n+        if predicate.key != self.column_name {\n+            panic!(\n+                \"attempted to evaluate unexpected predicate with key {} for column named {}\",\n+                predicate.key, self.column_name\n+            );\n+        }\n+        self.value_map.get(&predicate.value)\n+    }\n+\n+    /// Remove expired values from any cache nested within this [`LastCacheKey`]\n+    fn remove_expired(&mut self) {\n+        self.value_map\n+            .iter_mut()\n+            .for_each(|(_, m)| m.remove_expired());",
        "comment_created_at": "2024-07-09T15:49:00+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Yes, good call, this only removes the values and is not walking up and cleaning up the maps. I'll address that with the other immediate issues in a follow-on PR.",
        "pr_file_module": null
      }
    ]
  }
]