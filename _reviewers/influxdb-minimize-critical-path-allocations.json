[
  {
    "discussion_id": "2106353960",
    "pr_number": 26456,
    "pr_file": "influxdb3_processing_engine/src/plugins.rs",
    "created_at": "2025-05-26T00:25:33+00:00",
    "commented_code": "}\n \n     enum Schedule {\n-        Cron(OwnedScheduleIterator<Utc>),\n+        Cron(Box<OwnedScheduleIterator<Utc>>),",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "2106353960",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26456,
        "pr_file": "influxdb3_processing_engine/src/plugins.rs",
        "discussion_id": "2106353960",
        "commented_code": "@@ -718,7 +718,7 @@ mod python_plugin {\n     }\n \n     enum Schedule {\n-        Cron(OwnedScheduleIterator<Utc>),\n+        Cron(Box<OwnedScheduleIterator<Utc>>),",
        "comment_created_at": "2025-05-26T00:25:33+00:00",
        "comment_author": "stuartcarnie",
        "comment_body": "clippy got smart about significant difference in size of variants and suggested boxing instead.\r\n\r\nIt said that `Cron(OwnedScheduleIterator<Utc>)` required 288 bytes, whereas `Every(Duration)` was 16 bytes. This reduces the enum to 16 bytes.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2106354155",
    "pr_number": 26456,
    "pr_file": "influxdb3_server/src/http.rs",
    "created_at": "2025-05-26T00:26:10+00:00",
    "commented_code": "let path = uri.path();\n     // admin token creation should be allowed without authentication\n     // and any endpoints that are disabled\n-    if path == all_paths::API_V3_CONFIGURE_ADMIN_TOKEN\n-        || paths_without_authz\n-            .iter()\n-            .any(|disabled_authz_path| *disabled_authz_path == path)\n-    {\n+    if path == all_paths::API_V3_CONFIGURE_ADMIN_TOKEN || paths_without_authz.contains(&path) {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "2106354155",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26456,
        "pr_file": "influxdb3_server/src/http.rs",
        "discussion_id": "2106354155",
        "commented_code": "@@ -1784,11 +1782,7 @@ pub(crate) async fn route_request(\n     let path = uri.path();\n     // admin token creation should be allowed without authentication\n     // and any endpoints that are disabled\n-    if path == all_paths::API_V3_CONFIGURE_ADMIN_TOKEN\n-        || paths_without_authz\n-            .iter()\n-            .any(|disabled_authz_path| *disabled_authz_path == path)\n-    {\n+    if path == all_paths::API_V3_CONFIGURE_ADMIN_TOKEN || paths_without_authz.contains(&path) {",
        "comment_created_at": "2025-05-26T00:26:10+00:00",
        "comment_author": "stuartcarnie",
        "comment_body": "clippy suggested that `contains` was more efficient than `iter().any()`, which makes sense.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1967416191",
    "pr_number": 26043,
    "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
    "created_at": "2025-02-24T11:00:22+00:00",
    "commented_code": "for (database_id, table_map) in buffer.db_to_table.iter_mut() {\n                 let db_schema = catalog.db_schema_by_id(database_id).expect(\"db exists\");\n                 for (table_id, table_buffer) in table_map.iter_mut() {\n+                    info!(db_name = ?db_schema.name, ?table_id, \">>> working on db, table\");\n                     let table_def = db_schema\n                         .table_definition_by_id(table_id)\n                         .expect(\"table exists\");\n-                    let snapshot_chunks =\n-                        table_buffer.snapshot(table_def, snapshot_details.end_time_marker);\n+                    let sort_key = table_buffer.sort_key.clone();\n+                    let all_keys_to_remove =\n+                        table_buffer.get_keys_to_remove(snapshot_details.end_time_marker);\n+                    info!(num_keys_to_remove = ?all_keys_to_remove.len(), \">>> num keys to remove\");\n+\n+                    let chunk_time_to_chunk = &mut table_buffer.chunk_time_to_chunks;\n+                    let snapshot_chunks = &mut table_buffer.snapshotting_chunks;\n+                    let snapshot_chunks_iter = SnaphotChunkIter {\n+                        keys_to_remove: all_keys_to_remove.iter(),\n+                        map: chunk_time_to_chunk,\n+                        table_def,\n+                    };\n \n-                    for chunk in snapshot_chunks {\n+                    for chunk in snapshot_chunks_iter {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1967416191",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26043,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1967416191",
        "commented_code": "@@ -188,17 +191,31 @@ impl QueryableBuffer {\n             for (database_id, table_map) in buffer.db_to_table.iter_mut() {\n                 let db_schema = catalog.db_schema_by_id(database_id).expect(\"db exists\");\n                 for (table_id, table_buffer) in table_map.iter_mut() {\n+                    info!(db_name = ?db_schema.name, ?table_id, \">>> working on db, table\");\n                     let table_def = db_schema\n                         .table_definition_by_id(table_id)\n                         .expect(\"table exists\");\n-                    let snapshot_chunks =\n-                        table_buffer.snapshot(table_def, snapshot_details.end_time_marker);\n+                    let sort_key = table_buffer.sort_key.clone();\n+                    let all_keys_to_remove =\n+                        table_buffer.get_keys_to_remove(snapshot_details.end_time_marker);\n+                    info!(num_keys_to_remove = ?all_keys_to_remove.len(), \">>> num keys to remove\");\n+\n+                    let chunk_time_to_chunk = &mut table_buffer.chunk_time_to_chunks;\n+                    let snapshot_chunks = &mut table_buffer.snapshotting_chunks;\n+                    let snapshot_chunks_iter = SnaphotChunkIter {\n+                        keys_to_remove: all_keys_to_remove.iter(),\n+                        map: chunk_time_to_chunk,\n+                        table_def,\n+                    };\n \n-                    for chunk in snapshot_chunks {\n+                    for chunk in snapshot_chunks_iter {",
        "comment_created_at": "2025-02-24T11:00:22+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "This `snapshot_chunks_iter` produces `SnapshotChunk` lazily, uses the chunk to create `PersistJob` and then moves it to `TableBuffer`'s `snapshotting_chunks`. Because there's a write lock on this buffer above, it is ok to remove the key and then add it back. Previously the `snapshotting_chunks` was cloned and this avoids the cloning. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1983984532",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26043,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1967416191",
        "commented_code": "@@ -188,17 +191,31 @@ impl QueryableBuffer {\n             for (database_id, table_map) in buffer.db_to_table.iter_mut() {\n                 let db_schema = catalog.db_schema_by_id(database_id).expect(\"db exists\");\n                 for (table_id, table_buffer) in table_map.iter_mut() {\n+                    info!(db_name = ?db_schema.name, ?table_id, \">>> working on db, table\");\n                     let table_def = db_schema\n                         .table_definition_by_id(table_id)\n                         .expect(\"table exists\");\n-                    let snapshot_chunks =\n-                        table_buffer.snapshot(table_def, snapshot_details.end_time_marker);\n+                    let sort_key = table_buffer.sort_key.clone();\n+                    let all_keys_to_remove =\n+                        table_buffer.get_keys_to_remove(snapshot_details.end_time_marker);\n+                    info!(num_keys_to_remove = ?all_keys_to_remove.len(), \">>> num keys to remove\");\n+\n+                    let chunk_time_to_chunk = &mut table_buffer.chunk_time_to_chunks;\n+                    let snapshot_chunks = &mut table_buffer.snapshotting_chunks;\n+                    let snapshot_chunks_iter = SnaphotChunkIter {\n+                        keys_to_remove: all_keys_to_remove.iter(),\n+                        map: chunk_time_to_chunk,\n+                        table_def,\n+                    };\n \n-                    for chunk in snapshot_chunks {\n+                    for chunk in snapshot_chunks_iter {",
        "comment_created_at": "2025-03-06T20:07:33+00:00",
        "comment_author": "waynr",
        "comment_body": "Hmm, you say it gets removed then added back, right? I see that it's removed from `TableBuffer.chunk_time_to_chunks` we iterate over `SnaphotChunkIter` but I don't see where it's added back to that map. I also see where the new `SnapshotChunk` you refer to in this comment is added to the `TableBuffer.snapshotting_chunks` but I don't see where anything is removed from that vec. When you say \"added back\" are you just referring to it being added to the snapshotting chunks?",
        "pr_file_module": null
      },
      {
        "comment_id": "1984624576",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26043,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1967416191",
        "commented_code": "@@ -188,17 +191,31 @@ impl QueryableBuffer {\n             for (database_id, table_map) in buffer.db_to_table.iter_mut() {\n                 let db_schema = catalog.db_schema_by_id(database_id).expect(\"db exists\");\n                 for (table_id, table_buffer) in table_map.iter_mut() {\n+                    info!(db_name = ?db_schema.name, ?table_id, \">>> working on db, table\");\n                     let table_def = db_schema\n                         .table_definition_by_id(table_id)\n                         .expect(\"table exists\");\n-                    let snapshot_chunks =\n-                        table_buffer.snapshot(table_def, snapshot_details.end_time_marker);\n+                    let sort_key = table_buffer.sort_key.clone();\n+                    let all_keys_to_remove =\n+                        table_buffer.get_keys_to_remove(snapshot_details.end_time_marker);\n+                    info!(num_keys_to_remove = ?all_keys_to_remove.len(), \">>> num keys to remove\");\n+\n+                    let chunk_time_to_chunk = &mut table_buffer.chunk_time_to_chunks;\n+                    let snapshot_chunks = &mut table_buffer.snapshotting_chunks;\n+                    let snapshot_chunks_iter = SnaphotChunkIter {\n+                        keys_to_remove: all_keys_to_remove.iter(),\n+                        map: chunk_time_to_chunk,\n+                        table_def,\n+                    };\n \n-                    for chunk in snapshot_chunks {\n+                    for chunk in snapshot_chunks_iter {",
        "comment_created_at": "2025-03-07T08:08:24+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "> are you just referring to it being added to the snapshotting chunks?\r\n\r\nYes - I can see how I've confused you, I should rephrase that. It is added (as opposed to added back) to the `snapshotting_chunks`. I was trying to imply it's added _back_ to `TableBuffer`s `snapshotting_chunks`. \r\n\r\nThe previous operation was \r\n- to remove those keys from the map\r\n- convert them to record batches\r\n- hold it in `snapshotting_chunks`, \r\n- clone it and return the copy back to create persist jobs out of it.\r\n\r\nNow, iterator holds all the keys to remove\r\n- each call to `next()` converts it to record batch and yields it\r\n- this loop creates persist job\r\n- then adds it to `snapshotting_chunks` at the end\r\n\r\nThe main thing is there's a write lock for this operation and hence both these operations should leave `TableBuffer` in same state.",
        "pr_file_module": null
      },
      {
        "comment_id": "1985216049",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26043,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1967416191",
        "commented_code": "@@ -188,17 +191,31 @@ impl QueryableBuffer {\n             for (database_id, table_map) in buffer.db_to_table.iter_mut() {\n                 let db_schema = catalog.db_schema_by_id(database_id).expect(\"db exists\");\n                 for (table_id, table_buffer) in table_map.iter_mut() {\n+                    info!(db_name = ?db_schema.name, ?table_id, \">>> working on db, table\");\n                     let table_def = db_schema\n                         .table_definition_by_id(table_id)\n                         .expect(\"table exists\");\n-                    let snapshot_chunks =\n-                        table_buffer.snapshot(table_def, snapshot_details.end_time_marker);\n+                    let sort_key = table_buffer.sort_key.clone();\n+                    let all_keys_to_remove =\n+                        table_buffer.get_keys_to_remove(snapshot_details.end_time_marker);\n+                    info!(num_keys_to_remove = ?all_keys_to_remove.len(), \">>> num keys to remove\");\n+\n+                    let chunk_time_to_chunk = &mut table_buffer.chunk_time_to_chunks;\n+                    let snapshot_chunks = &mut table_buffer.snapshotting_chunks;\n+                    let snapshot_chunks_iter = SnaphotChunkIter {\n+                        keys_to_remove: all_keys_to_remove.iter(),\n+                        map: chunk_time_to_chunk,\n+                        table_def,\n+                    };\n \n-                    for chunk in snapshot_chunks {\n+                    for chunk in snapshot_chunks_iter {",
        "comment_created_at": "2025-03-07T14:57:26+00:00",
        "comment_author": "pauldix",
        "comment_body": "Has this been tested with high query concurrency to verify that this isn't a performance regression?",
        "pr_file_module": null
      },
      {
        "comment_id": "1985319087",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26043,
        "pr_file": "influxdb3_write/src/write_buffer/queryable_buffer.rs",
        "discussion_id": "1967416191",
        "commented_code": "@@ -188,17 +191,31 @@ impl QueryableBuffer {\n             for (database_id, table_map) in buffer.db_to_table.iter_mut() {\n                 let db_schema = catalog.db_schema_by_id(database_id).expect(\"db exists\");\n                 for (table_id, table_buffer) in table_map.iter_mut() {\n+                    info!(db_name = ?db_schema.name, ?table_id, \">>> working on db, table\");\n                     let table_def = db_schema\n                         .table_definition_by_id(table_id)\n                         .expect(\"table exists\");\n-                    let snapshot_chunks =\n-                        table_buffer.snapshot(table_def, snapshot_details.end_time_marker);\n+                    let sort_key = table_buffer.sort_key.clone();\n+                    let all_keys_to_remove =\n+                        table_buffer.get_keys_to_remove(snapshot_details.end_time_marker);\n+                    info!(num_keys_to_remove = ?all_keys_to_remove.len(), \">>> num keys to remove\");\n+\n+                    let chunk_time_to_chunk = &mut table_buffer.chunk_time_to_chunks;\n+                    let snapshot_chunks = &mut table_buffer.snapshotting_chunks;\n+                    let snapshot_chunks_iter = SnaphotChunkIter {\n+                        keys_to_remove: all_keys_to_remove.iter(),\n+                        map: chunk_time_to_chunk,\n+                        table_def,\n+                    };\n \n-                    for chunk in snapshot_chunks {\n+                    for chunk in snapshot_chunks_iter {",
        "comment_created_at": "2025-03-07T16:02:42+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "Yes - it has been, if I'm making further changes I'd probably need this to be perf tested again.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1983927435",
    "pr_number": 26043,
    "pr_file": "influxdb3_write/src/write_buffer/table_buffer.rs",
    "created_at": "2025-03-06T19:19:55+00:00",
    "commented_code": "}\n \n     pub fn clear_snapshots(&mut self) {\n-        self.snapshotting_chunks.clear();\n+        // vec clear still holds the mem (capacity), so use take",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1983927435",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26043,
        "pr_file": "influxdb3_write/src/write_buffer/table_buffer.rs",
        "discussion_id": "1983927435",
        "commented_code": "@@ -179,7 +191,35 @@ impl TableBuffer {\n     }\n \n     pub fn clear_snapshots(&mut self) {\n-        self.snapshotting_chunks.clear();\n+        // vec clear still holds the mem (capacity), so use take",
        "comment_created_at": "2025-03-06T19:19:55+00:00",
        "comment_author": "waynr",
        "comment_body": "Is it a bad thing to hold the memory in this case? Wouldn't that mean fewer allocations when filling the vec on the next pass?",
        "pr_file_module": null
      },
      {
        "comment_id": "1984633034",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26043,
        "pr_file": "influxdb3_write/src/write_buffer/table_buffer.rs",
        "discussion_id": "1983927435",
        "commented_code": "@@ -179,7 +191,35 @@ impl TableBuffer {\n     }\n \n     pub fn clear_snapshots(&mut self) {\n-        self.snapshotting_chunks.clear();\n+        // vec clear still holds the mem (capacity), so use take",
        "comment_created_at": "2025-03-07T08:15:05+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "It is a good point, previously we were replacing the `snapshotting_chunks` each time - it was creating a vec each time to replace original one, so I changed it to reclaim the memory. But now that I'm looping through and adding them, I can reuse it. I didn't revisit this code after I made the change to use the iterator. I'll change it and run through the profiler.  ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1862683553",
    "pr_number": 25596,
    "pr_file": "influxdb3_catalog/src/catalog.rs",
    "created_at": "2024-11-28T21:01:44+00:00",
    "commented_code": ".get_by_right(&name.into())\n             .and_then(|id| self.columns.get(id).map(|def| (*id, def)))\n     }\n+\n+    pub fn series_key(&self) -> Vec<String> {\n+        self.series_key\n+            .iter()\n+            .map(|k| self.column_id_to_name_unchecked(k).to_string())\n+            .collect()\n+    }",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1862683553",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25596,
        "pr_file": "influxdb3_catalog/src/catalog.rs",
        "discussion_id": "1862683553",
        "commented_code": "@@ -1139,6 +1133,13 @@ impl TableDefinition {\n             .get_by_right(&name.into())\n             .and_then(|id| self.columns.get(id).map(|def| (*id, def)))\n     }\n+\n+    pub fn series_key(&self) -> Vec<String> {\n+        self.series_key\n+            .iter()\n+            .map(|k| self.column_id_to_name_unchecked(k).to_string())\n+            .collect()\n+    }",
        "comment_created_at": "2024-11-28T21:01:44+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Since this gets called in the write path for each line, might be worth returning a `Vec<&str>` or `Vec<Arc<str>>` to avoid the string copies. Or even a slice, if possible.\r\n\r\nFurthermore, having to do the lookup by ID for the name every time could also be avoided by holding the `Arc<str>` names around in the `TableDefinition` then just iterating over those directly.\r\n\r\nDepending on how far you take it, this could lead to a substantial change if you had to change the `TableDefinition` struct, so might be better for a follow-on if that's the case.",
        "pr_file_module": null
      },
      {
        "comment_id": "1866160492",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25596,
        "pr_file": "influxdb3_catalog/src/catalog.rs",
        "discussion_id": "1862683553",
        "commented_code": "@@ -1139,6 +1133,13 @@ impl TableDefinition {\n             .get_by_right(&name.into())\n             .and_then(|id| self.columns.get(id).map(|def| (*id, def)))\n     }\n+\n+    pub fn series_key(&self) -> Vec<String> {\n+        self.series_key\n+            .iter()\n+            .map(|k| self.column_id_to_name_unchecked(k).to_string())\n+            .collect()\n+    }",
        "comment_created_at": "2024-12-02T16:25:13+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Just from a cursory try I do think it'll be a bit more substantial, especially if we cache those keys in the `TableDefinition`. I've opened up an issue for future work https://github.com/influxdata/influxdb/issues/25606",
        "pr_file_module": null
      },
      {
        "comment_id": "1867969542",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25596,
        "pr_file": "influxdb3_catalog/src/catalog.rs",
        "discussion_id": "1862683553",
        "commented_code": "@@ -1139,6 +1133,13 @@ impl TableDefinition {\n             .get_by_right(&name.into())\n             .and_then(|id| self.columns.get(id).map(|def| (*id, def)))\n     }\n+\n+    pub fn series_key(&self) -> Vec<String> {\n+        self.series_key\n+            .iter()\n+            .map(|k| self.column_id_to_name_unchecked(k).to_string())\n+            .collect()\n+    }",
        "comment_created_at": "2024-12-03T15:44:44+00:00",
        "comment_author": "pauldix",
        "comment_body": "yeah, returning the series key should be as cheap as possible. Given that every table now has one, it should just be a reference that gets returned (or an Arc'd thing?).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1826017843",
    "pr_number": 25495,
    "pr_file": "influxdb3_write/src/last_cache/mod.rs",
    "created_at": "2024-11-01T16:22:49+00:00",
    "commented_code": "/// for the cache.\n     fn to_record_batch(",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1826017843",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25495,
        "pr_file": "influxdb3_write/src/last_cache/mod.rs",
        "discussion_id": "1826017843",
        "commented_code": "@@ -1278,35 +1412,33 @@ impl LastCacheStore {\n     /// for the cache.\n     fn to_record_batch(",
        "comment_created_at": "2024-11-01T16:22:49+00:00",
        "comment_author": "hiltontj",
        "comment_body": "This function no longer builds a new arrow schema on _every_ call. As a result, the arrow schema will only ever need to be rebuilt when new fields are added for caches that accept new fields, _or_ never for caches that have an explicit set of value columns. Furthermore, for the explicit case, it produces the value columns by iterating directly on the cache `IndexMap`, instead of iterating over the schema. The non-explicit case still needs to iterate over the schema and do a lookup to get column ID. Therefore, I suspect the explicit case will be considerably more performant.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1816897637",
    "pr_number": 25492,
    "pr_file": "influxdb3_catalog/src/catalog.rs",
    "created_at": "2024-10-25T15:12:31+00:00",
    "commented_code": "pub id: DbId,\n     pub name: Arc<str>,\n     /// The database is a map of tables\n-    pub tables: BTreeMap<TableId, TableDefinition>,\n+    pub tables: SerdeVecHashMap<TableId, TableDefinition>,",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1816897637",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25492,
        "pr_file": "influxdb3_catalog/src/catalog.rs",
        "discussion_id": "1816897637",
        "commented_code": "@@ -466,7 +420,7 @@ pub struct DatabaseSchema {\n     pub id: DbId,\n     pub name: Arc<str>,\n     /// The database is a map of tables\n-    pub tables: BTreeMap<TableId, TableDefinition>,\n+    pub tables: SerdeVecHashMap<TableId, TableDefinition>,",
        "comment_created_at": "2024-10-25T15:12:31+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Changing this to use a `HashMap` instead of `BTreeMap` for faster lookups.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1698958486",
    "pr_number": 25196,
    "pr_file": "influxdb3_write/src/write_buffer/mod.rs",
    "created_at": "2024-07-31T18:47:10+00:00",
    "commented_code": "object_store: Arc<dyn ObjectStore>,\n     chunk_order: i64,\n ) -> ParquetChunk {\n-    // TODO: update persisted segments to serialize their key to use here\n-    let partition_key = data_types::PartitionKey::from(parquet_file.path.clone());\n+    let partition_key = data_types::PartitionKey::from(format!(\"{}\", parquet_file.chunk_time));",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1698958486",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/mod.rs",
        "discussion_id": "1698958486",
        "commented_code": "@@ -443,8 +411,7 @@ pub(crate) fn parquet_chunk_from_file(\n     object_store: Arc<dyn ObjectStore>,\n     chunk_order: i64,\n ) -> ParquetChunk {\n-    // TODO: update persisted segments to serialize their key to use here\n-    let partition_key = data_types::PartitionKey::from(parquet_file.path.clone());\n+    let partition_key = data_types::PartitionKey::from(format!(\"{}\", parquet_file.chunk_time));",
        "comment_created_at": "2024-07-31T18:47:10+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "This helps avoid going through the slower `fmt` machinery if possible.\r\n```suggestion\r\n    let partition_key = data_types::PartitionKey::from(parquet_file.chunk_time.to_string());\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1698959295",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25196,
        "pr_file": "influxdb3_write/src/write_buffer/mod.rs",
        "discussion_id": "1698958486",
        "commented_code": "@@ -443,8 +411,7 @@ pub(crate) fn parquet_chunk_from_file(\n     object_store: Arc<dyn ObjectStore>,\n     chunk_order: i64,\n ) -> ParquetChunk {\n-    // TODO: update persisted segments to serialize their key to use here\n-    let partition_key = data_types::PartitionKey::from(parquet_file.path.clone());\n+    let partition_key = data_types::PartitionKey::from(format!(\"{}\", parquet_file.chunk_time));",
        "comment_created_at": "2024-07-31T18:47:55+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Ideally you only need `format!` if you need to put multiple things together, but it does have overhead",
        "pr_file_module": null
      }
    ]
  }
]