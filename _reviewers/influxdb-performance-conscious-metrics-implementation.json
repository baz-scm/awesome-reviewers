[
  {
    "discussion_id": "2121332633",
    "pr_number": 26455,
    "pr_file": "influxdb3/src/commands/serve.rs",
    "created_at": "2025-06-02T14:29:57+00:00",
    "commented_code": "let f = SendPanicsToTracing::new_with_metrics(&metrics);\n     std::mem::forget(f);\n \n+    // When you have extra executor, you need separate metrics registry! It is not clear what\n+    // the impact would be\n+    // TODO: confirm this is not going to mess up downstream metrics consumers\n+    let write_path_metrics = setup_metric_registry();",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "2121332633",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26455,
        "pr_file": "influxdb3/src/commands/serve.rs",
        "discussion_id": "2121332633",
        "commented_code": "@@ -546,6 +545,20 @@ pub async fn command(config: Config) -> Result<()> {\n     let f = SendPanicsToTracing::new_with_metrics(&metrics);\n     std::mem::forget(f);\n \n+    // When you have extra executor, you need separate metrics registry! It is not clear what\n+    // the impact would be\n+    // TODO: confirm this is not going to mess up downstream metrics consumers\n+    let write_path_metrics = setup_metric_registry();",
        "comment_created_at": "2025-06-02T14:29:57+00:00",
        "comment_author": "hiltontj",
        "comment_body": "This will likely be an issue, since the HTTP endpoint that serves prometheus (`/metrics`) assumes a single registry: https://github.com/influxdata/influxdb/blob/be25c6f52b046e57ec909b815e5471d4c6bb4f19/influxdb3_server/src/http.rs#L734-L740\r\n\r\nIs the issue that using the same registry for multiple executors causes them to overwrite each other, or contend for locks with each other?",
        "pr_file_module": null
      },
      {
        "comment_id": "2121394240",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26455,
        "pr_file": "influxdb3/src/commands/serve.rs",
        "discussion_id": "2121332633",
        "commented_code": "@@ -546,6 +545,20 @@ pub async fn command(config: Config) -> Result<()> {\n     let f = SendPanicsToTracing::new_with_metrics(&metrics);\n     std::mem::forget(f);\n \n+    // When you have extra executor, you need separate metrics registry! It is not clear what\n+    // the impact would be\n+    // TODO: confirm this is not going to mess up downstream metrics consumers\n+    let write_path_metrics = setup_metric_registry();",
        "comment_created_at": "2025-06-02T14:50:24+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "It runs into panic, \r\n```\r\n2025-06-02T14:49:39.205230Z ERROR panic_logging: Thread panic panic_type=\"unknown\" panic_message=\"More than one execution pool created: previously existing instrument\" panic_file=\"/home/praveen/.cargo/git/checkouts/influxdb3_core-2ede6fca005e1dcf/fd0e474/iox_query/src/exec.rs\" panic_line=281 panic_column=9\r\n\r\nthread 'main' panicked at /home/praveen/.cargo/git/checkouts/influxdb3_core-2ede6fca005e1dcf/fd0e474/iox_query/src/exec.rs:281:9:\r\nMore than one execution pool created: previously existing instrument\r\nstack backtrace:\r\n   0:     0x629f44b5e172 - <std::sys::backtrace::BacktraceLock::print::DisplayBacktrace as core::fmt::Display>::fmt::hc04c8f544ab24d66\r\n   1:     0x629f44b8eb63 - core::fmt::write::hfe57b7174b7d8eab\r\n   2:     0x629f44b595a3 - std::io::Write::write_fmt::h154385efa8565236\r\n   3:     0x629f44b5dfc2 - std::sys::backtrace::BacktraceLock::print::h0c8f24e22f5873a8\r\n   4:     0x629f44b5f24c - std::panicking::default_hook::{{closure}}::hd07d57e6a602c8e4\r\n   5:     0x629f44b5f04f - std::panicking::default_hook::h63d12f7d95bd91ed\r\n   6:     0x629f3fd807db - panic_logging::SendPanicsToTracing::new_inner::{{closure}}::h4f1478e3035af477\r\n   7:     0x629f44b5fd43 - std::panicking::rust_panic_with_hook::h33b18b24045abff4\r\n   8:     0x629f44b5f9c6 - std::panicking::begin_panic_handler::{{closure}}::hf8313cc2fd0126bc\r\n   9:     0x629f44b5e679 - std::sys::backtrace::__rust_end_short_backtrace::h57fe07c8aea5c98a\r\n  10:     0x629f44b5f68d - __rustc[95feac21a9532783]::rust_begin_unwind\r\n  11:     0x629f44b8bac0 - core::panicking::panic_fmt::hd54fb667be51beea\r\n  12:     0x629f414b6cb7 - iox_query::exec::Executor::new_with_config_and_executor::h3ef1059edcb25ade\r\n  13:     0x629f3f99fd9c - influxdb3::commands::serve::command::{{closure}}::h2cdf5ca9df83df25\r\n  14:     0x629f3f9b6fcb - influxdb3::main::{{closure}}::hc953cfc298ca6770\r\n  15:     0x629f3f987b39 - tokio::runtime::park::CachedParkThread::block_on::h51b18ac33f8a0e4d\r\n  16:     0x629f3fb2a7bf - tokio::runtime::runtime::Runtime::block_on::h9eb33b87acb6fa53\r\n  17:     0x629f3fc20d55 - influxdb3::main::h75a268e75e689bc6\r\n  18:     0x629f3fcbb256 - std::sys::backtrace::__rust_begin_short_backtrace::h5b4e77177edb3cca\r\n  19:     0x629f3fab4321 - std::rt::lang_start::{{closure}}::hc69eb1d94c6de306\r\n  20:     0x629f44b4e080 - std::rt::lang_start_internal::h418648f91f5be3a1\r\n  21:     0x629f3fc3b19d - main\r\n  22:     0x7ed71c33d488 - <unknown>\r\n  23:     0x7ed71c33d54c - __libc_start_main\r\n  24:     0x629f3f95b325 - _start\r\n  25:                0x0 - <unknown>\r\n2025-06-02T14:49:39.295724Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\r\n2025-06-02T14:49:39.296308Z  WARN executor: DedicatedExecutor dropped without calling shutdown()\r\n\r\n```\r\n\r\nI can look into the panic and see if I can address that in a different way if this is going to mess downstream consumers.",
        "pr_file_module": null
      },
      {
        "comment_id": "2121513637",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26455,
        "pr_file": "influxdb3/src/commands/serve.rs",
        "discussion_id": "2121332633",
        "commented_code": "@@ -546,6 +545,20 @@ pub async fn command(config: Config) -> Result<()> {\n     let f = SendPanicsToTracing::new_with_metrics(&metrics);\n     std::mem::forget(f);\n \n+    // When you have extra executor, you need separate metrics registry! It is not clear what\n+    // the impact would be\n+    // TODO: confirm this is not going to mess up downstream metrics consumers\n+    let write_path_metrics = setup_metric_registry();",
        "comment_created_at": "2025-06-02T15:30:42+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "I looked into the code that runs into panic \r\n\r\nhttps://github.com/influxdata/influxdb3_core/blob/fd0e474a6c0af5ba867399d753f5df18f59907cb/iox_query/src/exec.rs#L268-L284\r\n\r\nIt looks like there is an assumption that you violate single memory pool -> executor relationship if the \"datafusion_pool\" is already registered. Even though we don't break that relationship, i.e in this branch there are two executors and each has it's own memory pool so the relationship is still correct but because registry is shared it runs into this error. \r\n\r\nI need to spend a bit more time to see if I can create the executor outside without hooking it up to metrics to start with (or use a different name for instrument \"datafusion_write_pool\") and then experiment with how it's reporting.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1894050400",
    "pr_number": 25692,
    "pr_file": "influxdb3_write/src/write_buffer/mod.rs",
    "created_at": "2024-12-20T15:06:37+00:00",
    "commented_code": "assert!(result.is_ok());\n     }\n \n+    #[tokio::test]\n+    async fn write_metrics() {\n+        let object_store = Arc::new(InMemory::new());\n+        let (buf, metrics) = setup_with_metrics(\n+            Time::from_timestamp_nanos(0),\n+            object_store,\n+            WalConfig::test_config(),\n+        )\n+        .await;\n+        let lines_observer = metrics\n+            .get_instrument::<Metric<U64Counter>>(WRITE_LINES_TOTAL_NAME)\n+            .unwrap();\n+        let bytes_observer = metrics\n+            .get_instrument::<Metric<U64Counter>>(WRITE_BYTES_TOTAL_NAME)\n+            .unwrap();\n+\n+        let db_1 = \"foo\";\n+        let db_2 = \"bar\";\n+\n+        // do a write and check the metrics:\n+        let lp = \"\\\n+            cpu,region=us,host=a usage=10\n\\\n+            cpu,region=eu,host=b usage=10\n\\\n+            cpu,region=ca,host=c usage=10\n\\\n+            \";\n+        do_writes(\n+            db_1,\n+            buf.as_ref(),\n+            &[TestWrite {\n+                lp,\n+                time_seconds: 1,\n+            }],\n+        )\n+        .await;\n+        assert_eq!(\n+            3,\n+            lines_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+        let mut bytes: usize = lp.lines().map(|l| l.len()).sum();\n+        assert_eq!(\n+            bytes as u64,\n+            bytes_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+\n+        // do another write to that db and check again for updates:\n+        let lp = \"\\\n+            mem,region=us,host=a used=1,swap=4\n\\\n+            mem,region=eu,host=b used=1,swap=4\n\\\n+            mem,region=ca,host=c used=1,swap=4\n\\\n+            \";\n+        do_writes(\n+            db_1,\n+            buf.as_ref(),\n+            &[TestWrite {\n+                lp,\n+                time_seconds: 1,\n+            }],\n+        )\n+        .await;\n+        assert_eq!(\n+            6,\n+            lines_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+        bytes += lp.lines().map(|l| l.len()).sum::<usize>();\n+        assert_eq!(\n+            bytes as u64,\n+            bytes_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+\n+        // now do a write that will only be partially accepted to ensure that\n+        // the metrics are only calculated for writes that get accepted:\n+\n+        // the legume will not be accepted, because it contains a new tag,",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1894050400",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25692,
        "pr_file": "influxdb3_write/src/write_buffer/mod.rs",
        "discussion_id": "1894050400",
        "commented_code": "@@ -2350,6 +2369,125 @@ mod tests {\n         assert!(result.is_ok());\n     }\n \n+    #[tokio::test]\n+    async fn write_metrics() {\n+        let object_store = Arc::new(InMemory::new());\n+        let (buf, metrics) = setup_with_metrics(\n+            Time::from_timestamp_nanos(0),\n+            object_store,\n+            WalConfig::test_config(),\n+        )\n+        .await;\n+        let lines_observer = metrics\n+            .get_instrument::<Metric<U64Counter>>(WRITE_LINES_TOTAL_NAME)\n+            .unwrap();\n+        let bytes_observer = metrics\n+            .get_instrument::<Metric<U64Counter>>(WRITE_BYTES_TOTAL_NAME)\n+            .unwrap();\n+\n+        let db_1 = \"foo\";\n+        let db_2 = \"bar\";\n+\n+        // do a write and check the metrics:\n+        let lp = \"\\\n+            cpu,region=us,host=a usage=10\\n\\\n+            cpu,region=eu,host=b usage=10\\n\\\n+            cpu,region=ca,host=c usage=10\\n\\\n+            \";\n+        do_writes(\n+            db_1,\n+            buf.as_ref(),\n+            &[TestWrite {\n+                lp,\n+                time_seconds: 1,\n+            }],\n+        )\n+        .await;\n+        assert_eq!(\n+            3,\n+            lines_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+        let mut bytes: usize = lp.lines().map(|l| l.len()).sum();\n+        assert_eq!(\n+            bytes as u64,\n+            bytes_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+\n+        // do another write to that db and check again for updates:\n+        let lp = \"\\\n+            mem,region=us,host=a used=1,swap=4\\n\\\n+            mem,region=eu,host=b used=1,swap=4\\n\\\n+            mem,region=ca,host=c used=1,swap=4\\n\\\n+            \";\n+        do_writes(\n+            db_1,\n+            buf.as_ref(),\n+            &[TestWrite {\n+                lp,\n+                time_seconds: 1,\n+            }],\n+        )\n+        .await;\n+        assert_eq!(\n+            6,\n+            lines_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+        bytes += lp.lines().map(|l| l.len()).sum::<usize>();\n+        assert_eq!(\n+            bytes as u64,\n+            bytes_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+\n+        // now do a write that will only be partially accepted to ensure that\n+        // the metrics are only calculated for writes that get accepted:\n+\n+        // the legume will not be accepted, because it contains a new tag,",
        "comment_created_at": "2024-12-20T15:06:37+00:00",
        "comment_author": "pauldix",
        "comment_body": "As a follow up it would be great to add a `influxdb_write_lines_rejected_total` counter.",
        "pr_file_module": null
      },
      {
        "comment_id": "1894111853",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25692,
        "pr_file": "influxdb3_write/src/write_buffer/mod.rs",
        "discussion_id": "1894050400",
        "commented_code": "@@ -2350,6 +2369,125 @@ mod tests {\n         assert!(result.is_ok());\n     }\n \n+    #[tokio::test]\n+    async fn write_metrics() {\n+        let object_store = Arc::new(InMemory::new());\n+        let (buf, metrics) = setup_with_metrics(\n+            Time::from_timestamp_nanos(0),\n+            object_store,\n+            WalConfig::test_config(),\n+        )\n+        .await;\n+        let lines_observer = metrics\n+            .get_instrument::<Metric<U64Counter>>(WRITE_LINES_TOTAL_NAME)\n+            .unwrap();\n+        let bytes_observer = metrics\n+            .get_instrument::<Metric<U64Counter>>(WRITE_BYTES_TOTAL_NAME)\n+            .unwrap();\n+\n+        let db_1 = \"foo\";\n+        let db_2 = \"bar\";\n+\n+        // do a write and check the metrics:\n+        let lp = \"\\\n+            cpu,region=us,host=a usage=10\\n\\\n+            cpu,region=eu,host=b usage=10\\n\\\n+            cpu,region=ca,host=c usage=10\\n\\\n+            \";\n+        do_writes(\n+            db_1,\n+            buf.as_ref(),\n+            &[TestWrite {\n+                lp,\n+                time_seconds: 1,\n+            }],\n+        )\n+        .await;\n+        assert_eq!(\n+            3,\n+            lines_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+        let mut bytes: usize = lp.lines().map(|l| l.len()).sum();\n+        assert_eq!(\n+            bytes as u64,\n+            bytes_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+\n+        // do another write to that db and check again for updates:\n+        let lp = \"\\\n+            mem,region=us,host=a used=1,swap=4\\n\\\n+            mem,region=eu,host=b used=1,swap=4\\n\\\n+            mem,region=ca,host=c used=1,swap=4\\n\\\n+            \";\n+        do_writes(\n+            db_1,\n+            buf.as_ref(),\n+            &[TestWrite {\n+                lp,\n+                time_seconds: 1,\n+            }],\n+        )\n+        .await;\n+        assert_eq!(\n+            6,\n+            lines_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+        bytes += lp.lines().map(|l| l.len()).sum::<usize>();\n+        assert_eq!(\n+            bytes as u64,\n+            bytes_observer\n+                .get_observer(&Attributes::from(&[(\"db\", db_1)]))\n+                .unwrap()\n+                .fetch()\n+        );\n+\n+        // now do a write that will only be partially accepted to ensure that\n+        // the metrics are only calculated for writes that get accepted:\n+\n+        // the legume will not be accepted, because it contains a new tag,",
        "comment_created_at": "2024-12-20T16:02:25+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Opened https://github.com/influxdata/influxdb/issues/25696",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1783089185",
    "pr_number": 25409,
    "pr_file": "influxdb3_server/src/http.rs",
    "created_at": "2024-10-01T15:33:07+00:00",
    "commented_code": ".await?\n         };\n \n+        let num_lines = result.line_count;\n+        let payload_size = body.len();\n+        let telem_store = Arc::clone(&self.common_state.telemetry_store);\n+        tokio::spawn(async move {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1783089185",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25409,
        "pr_file": "influxdb3_server/src/http.rs",
        "discussion_id": "1783089185",
        "commented_code": "@@ -419,6 +419,13 @@ where\n                 .await?\n         };\n \n+        let num_lines = result.line_count;\n+        let payload_size = body.len();\n+        let telem_store = Arc::clone(&self.common_state.telemetry_store);\n+        tokio::spawn(async move {",
        "comment_created_at": "2024-10-01T15:33:07+00:00",
        "comment_author": "pauldix",
        "comment_body": "I don't think we want to spawn on every write call. We should be able to just call add_write_metrics on the store without this.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1768774763",
    "pr_number": 25374,
    "pr_file": "telemetry/src/lib.rs",
    "created_at": "2024-09-20T14:57:52+00:00",
    "commented_code": "+use std::{collections::VecDeque, sync, sync::Arc, time::Duration};\n+\n+use observability_deps::tracing::error;\n+use serde::Serialize;\n+use tokio::sync::mpsc;\n+\n+/// This is the core type exposed from this crate which can be\n+/// cloned in different places to send the `InternalTelemetryMessage`.\n+#[derive(Clone)]\n+pub struct TelemetryHandle {\n+    /// mpsc sender, safe to clone\n+    sender: mpsc::Sender<InternalTelemetryMessage>,\n+}\n+\n+impl TelemetryHandle {\n+    pub async fn new(\n+        instance_id: String,\n+        os: String,\n+        influx_version: String,\n+        storage_type: String,\n+        cores: u32,\n+    ) -> Self {\n+        let store = TelemetryStore::new(instance_id, os, influx_version, storage_type, cores);\n+        // TODO: decide the buffer size\n+        let (sender, receiver) = mpsc::channel(100);",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1768774763",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25374,
        "pr_file": "telemetry/src/lib.rs",
        "discussion_id": "1768774763",
        "commented_code": "@@ -0,0 +1,235 @@\n+use std::{collections::VecDeque, sync, sync::Arc, time::Duration};\n+\n+use observability_deps::tracing::error;\n+use serde::Serialize;\n+use tokio::sync::mpsc;\n+\n+/// This is the core type exposed from this crate which can be\n+/// cloned in different places to send the `InternalTelemetryMessage`.\n+#[derive(Clone)]\n+pub struct TelemetryHandle {\n+    /// mpsc sender, safe to clone\n+    sender: mpsc::Sender<InternalTelemetryMessage>,\n+}\n+\n+impl TelemetryHandle {\n+    pub async fn new(\n+        instance_id: String,\n+        os: String,\n+        influx_version: String,\n+        storage_type: String,\n+        cores: u32,\n+    ) -> Self {\n+        let store = TelemetryStore::new(instance_id, os, influx_version, storage_type, cores);\n+        // TODO: decide the buffer size\n+        let (sender, receiver) = mpsc::channel(100);",
        "comment_created_at": "2024-09-20T14:57:52+00:00",
        "comment_author": "pauldix",
        "comment_body": "Seems like we can have this be pretty big given they're small messages. 10k?",
        "pr_file_module": null
      },
      {
        "comment_id": "1768804534",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25374,
        "pr_file": "telemetry/src/lib.rs",
        "discussion_id": "1768774763",
        "commented_code": "@@ -0,0 +1,235 @@\n+use std::{collections::VecDeque, sync, sync::Arc, time::Duration};\n+\n+use observability_deps::tracing::error;\n+use serde::Serialize;\n+use tokio::sync::mpsc;\n+\n+/// This is the core type exposed from this crate which can be\n+/// cloned in different places to send the `InternalTelemetryMessage`.\n+#[derive(Clone)]\n+pub struct TelemetryHandle {\n+    /// mpsc sender, safe to clone\n+    sender: mpsc::Sender<InternalTelemetryMessage>,\n+}\n+\n+impl TelemetryHandle {\n+    pub async fn new(\n+        instance_id: String,\n+        os: String,\n+        influx_version: String,\n+        storage_type: String,\n+        cores: u32,\n+    ) -> Self {\n+        let store = TelemetryStore::new(instance_id, os, influx_version, storage_type, cores);\n+        // TODO: decide the buffer size\n+        let (sender, receiver) = mpsc::channel(100);",
        "comment_created_at": "2024-09-20T15:17:24+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "When I was at Fastly rewriting our entire metrics pipeline we had a bound of about 10k given the amount of reqs per second (thousands easily). It ended up being faster to just use a lock on the map to the stats and let the handle be able to update the stats rather than even using a channel.\r\n\r\nIf we're updating these after every single write for instance I fear the queue will fill up really quick and we'll start applying back pressure where we really don't want too, regardless of the queue size.",
        "pr_file_module": null
      },
      {
        "comment_id": "1768826727",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25374,
        "pr_file": "telemetry/src/lib.rs",
        "discussion_id": "1768774763",
        "commented_code": "@@ -0,0 +1,235 @@\n+use std::{collections::VecDeque, sync, sync::Arc, time::Duration};\n+\n+use observability_deps::tracing::error;\n+use serde::Serialize;\n+use tokio::sync::mpsc;\n+\n+/// This is the core type exposed from this crate which can be\n+/// cloned in different places to send the `InternalTelemetryMessage`.\n+#[derive(Clone)]\n+pub struct TelemetryHandle {\n+    /// mpsc sender, safe to clone\n+    sender: mpsc::Sender<InternalTelemetryMessage>,\n+}\n+\n+impl TelemetryHandle {\n+    pub async fn new(\n+        instance_id: String,\n+        os: String,\n+        influx_version: String,\n+        storage_type: String,\n+        cores: u32,\n+    ) -> Self {\n+        let store = TelemetryStore::new(instance_id, os, influx_version, storage_type, cores);\n+        // TODO: decide the buffer size\n+        let (sender, receiver) = mpsc::channel(100);",
        "comment_created_at": "2024-09-20T15:35:05+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "Yea - I've hard coded it to 10k for now",
        "pr_file_module": null
      },
      {
        "comment_id": "1768866606",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25374,
        "pr_file": "telemetry/src/lib.rs",
        "discussion_id": "1768774763",
        "commented_code": "@@ -0,0 +1,235 @@\n+use std::{collections::VecDeque, sync, sync::Arc, time::Duration};\n+\n+use observability_deps::tracing::error;\n+use serde::Serialize;\n+use tokio::sync::mpsc;\n+\n+/// This is the core type exposed from this crate which can be\n+/// cloned in different places to send the `InternalTelemetryMessage`.\n+#[derive(Clone)]\n+pub struct TelemetryHandle {\n+    /// mpsc sender, safe to clone\n+    sender: mpsc::Sender<InternalTelemetryMessage>,\n+}\n+\n+impl TelemetryHandle {\n+    pub async fn new(\n+        instance_id: String,\n+        os: String,\n+        influx_version: String,\n+        storage_type: String,\n+        cores: u32,\n+    ) -> Self {\n+        let store = TelemetryStore::new(instance_id, os, influx_version, storage_type, cores);\n+        // TODO: decide the buffer size\n+        let (sender, receiver) = mpsc::channel(100);",
        "comment_created_at": "2024-09-20T16:02:54+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "@mgattozzi - thanks, I was hoping the metrics gathering will get out of the critical path for any writes/reads when following the message passing model. If you think that having lock on the store directly will be quick enough anyway, happy to change the implementation to expose the `TelemetryStore` directly instead of going through the `TelemetryHandle`.",
        "pr_file_module": null
      }
    ]
  }
]