[
  {
    "discussion_id": "2122374182",
    "pr_number": 26479,
    "pr_file": "influxdb3_catalog/src/snapshot/versions/mod.rs",
    "created_at": "2025-06-03T00:25:01+00:00",
    "commented_code": "id: snap.id,\n             name: snap.name,\n             tables: Repository::from_snapshot(snap.tables),\n+            retention_period: snap\n+                .retention_period\n+                .map(Snapshot::from_snapshot)\n+                .unwrap_or(RetentionPeriod::Indefinite),",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "2122374182",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26479,
        "pr_file": "influxdb3_catalog/src/snapshot/versions/mod.rs",
        "discussion_id": "2122374182",
        "commented_code": "@@ -317,6 +335,10 @@ impl Snapshot for DatabaseSchema {\n             id: snap.id,\n             name: snap.name,\n             tables: Repository::from_snapshot(snap.tables),\n+            retention_period: snap\n+                .retention_period\n+                .map(Snapshot::from_snapshot)\n+                .unwrap_or(RetentionPeriod::Indefinite),",
        "comment_created_at": "2025-06-03T00:25:01+00:00",
        "comment_author": "stuartcarnie",
        "comment_body": "âœ… default to `Indefinite` is `snap.retention_period` is `None`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2063962088",
    "pr_number": 26333,
    "pr_file": "influxdb3_catalog/src/log/versions/v2.rs",
    "created_at": "2025-04-28T15:43:02+00:00",
    "commented_code": "AllNonKeyColumns,\n }\n \n-/// The maximum allowed size for a last cache\n-pub const LAST_CACHE_MAX_SIZE: usize = 10;\n-\n /// The size of the last cache\n ///\n-/// Must be between 1 and [`LAST_CACHE_MAX_SIZE`]\n+/// Must be greater than 0\n #[derive(Debug, Serialize, Eq, PartialEq, Clone, Copy)]\n pub struct LastCacheSize(pub(crate) usize);",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "2063962088",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26333,
        "pr_file": "influxdb3_catalog/src/log/versions/v2.rs",
        "discussion_id": "2063962088",
        "commented_code": "@@ -380,18 +380,15 @@ pub enum LastCacheValueColumnsDef {\n     AllNonKeyColumns,\n }\n \n-/// The maximum allowed size for a last cache\n-pub const LAST_CACHE_MAX_SIZE: usize = 10;\n-\n /// The size of the last cache\n ///\n-/// Must be between 1 and [`LAST_CACHE_MAX_SIZE`]\n+/// Must be greater than 0\n #[derive(Debug, Serialize, Eq, PartialEq, Clone, Copy)]\n pub struct LastCacheSize(pub(crate) usize);",
        "comment_created_at": "2025-04-28T15:43:02+00:00",
        "comment_author": "hiltontj",
        "comment_body": "This is a textbook use case for [`NonZeroUsize`](https://doc.rust-lang.org/std/num/type.NonZeroUsize.html), i.e.,\r\n```rust\r\n/// Must be greater than 0\r\n#[derive(Debug, Serialize, Eq, PartialEq, Clone, Copy)]\r\npub struct LastCacheSize(pub(crate) NonZeroUsize);",
        "pr_file_module": null
      },
      {
        "comment_id": "2064099373",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26333,
        "pr_file": "influxdb3_catalog/src/log/versions/v2.rs",
        "discussion_id": "2063962088",
        "commented_code": "@@ -380,18 +380,15 @@ pub enum LastCacheValueColumnsDef {\n     AllNonKeyColumns,\n }\n \n-/// The maximum allowed size for a last cache\n-pub const LAST_CACHE_MAX_SIZE: usize = 10;\n-\n /// The size of the last cache\n ///\n-/// Must be between 1 and [`LAST_CACHE_MAX_SIZE`]\n+/// Must be greater than 0\n #[derive(Debug, Serialize, Eq, PartialEq, Clone, Copy)]\n pub struct LastCacheSize(pub(crate) usize);",
        "comment_created_at": "2025-04-28T16:57:12+00:00",
        "comment_author": "peterbarnett03",
        "comment_body": "This looks like it'll be more involved given implementations across the DB will have to be updated; I can do that update, would want to do it in a separate chore though if that's alright. \r\n\r\nIf you've reconsidered and do think it should be blocking, let me know and I'll make the update. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2064267645",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26333,
        "pr_file": "influxdb3_catalog/src/log/versions/v2.rs",
        "discussion_id": "2063962088",
        "commented_code": "@@ -380,18 +380,15 @@ pub enum LastCacheValueColumnsDef {\n     AllNonKeyColumns,\n }\n \n-/// The maximum allowed size for a last cache\n-pub const LAST_CACHE_MAX_SIZE: usize = 10;\n-\n /// The size of the last cache\n ///\n-/// Must be between 1 and [`LAST_CACHE_MAX_SIZE`]\n+/// Must be greater than 0\n #[derive(Debug, Serialize, Eq, PartialEq, Clone, Copy)]\n pub struct LastCacheSize(pub(crate) usize);",
        "comment_created_at": "2025-04-28T18:32:35+00:00",
        "comment_author": "hiltontj",
        "comment_body": "No worries. We can refactor this later",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1965974959",
    "pr_number": 26042,
    "pr_file": "influxdb3_processing_engine/src/virtualenv.rs",
    "created_at": "2025-02-21T18:16:31+00:00",
    "commented_code": "CommandError(#[from] std::io::Error),\n }\n \n-fn get_python_version() -> Result<(u8, u8), std::io::Error> {\n+// Find the python installation location (not virtual env).\n+// XXX: use build flag?\n+fn find_python_install() -> PathBuf {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1965974959",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26042,
        "pr_file": "influxdb3_processing_engine/src/virtualenv.rs",
        "discussion_id": "1965974959",
        "commented_code": "@@ -17,13 +17,47 @@ pub enum VenvError {\n     CommandError(#[from] std::io::Error),\n }\n \n-fn get_python_version() -> Result<(u8, u8), std::io::Error> {\n+// Find the python installation location (not virtual env).\n+// XXX: use build flag?\n+fn find_python_install() -> PathBuf {",
        "comment_created_at": "2025-02-21T18:16:31+00:00",
        "comment_author": "jacksonrnewhouse",
        "comment_body": "Have this return an `Option<PathBuf>` rather than an empty PathBuf.",
        "pr_file_module": null
      },
      {
        "comment_id": "1970596608",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 26042,
        "pr_file": "influxdb3_processing_engine/src/virtualenv.rs",
        "discussion_id": "1965974959",
        "commented_code": "@@ -17,13 +17,47 @@ pub enum VenvError {\n     CommandError(#[from] std::io::Error),\n }\n \n-fn get_python_version() -> Result<(u8, u8), std::io::Error> {\n+// Find the python installation location (not virtual env).\n+// XXX: use build flag?\n+fn find_python_install() -> PathBuf {",
        "comment_created_at": "2025-02-25T21:53:35+00:00",
        "comment_author": "jdstrand",
        "comment_body": "Addressed in 6ce074be3e6",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1670699685",
    "pr_number": 25109,
    "pr_file": "influxdb3_write/src/last_cache.rs",
    "created_at": "2024-07-09T15:08:26+00:00",
    "commented_code": "+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name\n+        if self\n+            .cache_map\n+            .read()\n+            .get(&db_name)\n+            .and_then(|db| db.get(&tbl_name))\n+            .is_some_and(|tbl| tbl.contains_key(&cache_name))\n+        {\n+            return Err(Error::CacheAlreadyExists);\n+        }\n+\n+        let value_columns = if let Some(mut vals) = value_columns {\n+            // if value columns are specified, check that they are present in the table schema\n+            for name in vals.iter() {\n+                if schema.field_by_name(name).is_none() {\n+                    return Err(Error::ValueColumnDoesNotExist {\n+                        column_name: name.into(),\n+                    });\n+                }\n+            }\n+            // double-check that time column is included\n+            let time_col = TIME_COLUMN_NAME.to_string();\n+            if !vals.contains(&time_col) {\n+                vals.push(time_col);\n+            }\n+            vals\n+        } else {\n+            // default to all non-key columns\n+            schema\n+                .iter()\n+                .filter_map(|(_, f)| {\n+                    if key_columns.contains(f.name()) {\n+                        None\n+                    } else {\n+                        Some(f.name().to_string())\n+                    }\n+                })\n+                .collect::<Vec<String>>()\n+        };\n+\n+        // build a schema that only holds the field columns\n+        let mut schema_builder = SchemaBuilder::new();\n+        for (t, name) in schema\n+            .iter()\n+            .filter(|&(_, f)| value_columns.contains(f.name()))\n+            .map(|(t, f)| (t, f.name()))\n+        {\n+            schema_builder.influx_column(name, t);\n+        }\n+\n+        // create the actual last cache:\n+        let last_cache = LastCache::new(\n+            count\n+                .unwrap_or(1)\n+                .try_into()\n+                .map_err(|_| Error::InvalidCacheSize)?,\n+            ttl.unwrap_or(DEFAULT_CACHE_TTL),\n+            key_columns,\n+            schema_builder.build()?,\n+        );\n+\n+        // get the write lock and insert:\n+        self.cache_map\n+            .write()\n+            .entry(db_name)\n+            .or_default()\n+            .entry(tbl_name)\n+            .or_default()\n+            .insert(cache_name, last_cache);\n+\n+        Ok(())\n+    }\n+\n+    /// Write a batch from the buffer into the cache by iterating over its database and table batches\n+    /// to find entries that belong in the cache.\n+    ///\n+    /// Only if rows are newer than the latest entry in the cache will they be entered.\n+    pub(crate) fn write_batch_to_cache(&self, write_batch: &WriteBatch) {\n+        let mut cache_map = self.cache_map.write();\n+        for (db_name, db_batch) in &write_batch.database_batches {\n+            if let Some(db_cache) = cache_map.get_mut(db_name.as_str()) {\n+                if db_cache.is_empty() {\n+                    continue;\n+                }\n+                for (tbl_name, tbl_batch) in &db_batch.table_batches {\n+                    if let Some(tbl_cache) = db_cache.get_mut(tbl_name) {\n+                        for (_, last_cache) in tbl_cache.iter_mut() {\n+                            for row in &tbl_batch.rows {\n+                                last_cache.push(row);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Recurse down the cache structure to evict expired cache entries, based on their respective\n+    /// time-to-live (TTL).\n+    pub(crate) fn evict_expired_cache_entries(&self) {\n+        let mut cache_map = self.cache_map.write();\n+        cache_map.iter_mut().for_each(|(_, db)| {\n+            db.iter_mut()\n+                .for_each(|(_, tbl)| tbl.iter_mut().for_each(|(_, lc)| lc.remove_expired()))\n+        });\n+    }\n+\n+    /// Output the records for a given cache as arrow [`RecordBatch`]es\n+    #[cfg(test)]\n+    fn get_cache_record_batches(\n+        &self,\n+        db_name: &str,\n+        tbl_name: &str,\n+        cache_name: Option<&str>,\n+        predicates: &[Predicate],\n+    ) -> Option<Result<Vec<RecordBatch>, ArrowError>> {\n+        self.cache_map\n+            .read()\n+            .get(db_name)\n+            .and_then(|db| db.get(tbl_name))\n+            .and_then(|tbl| {\n+                if let Some(name) = cache_name {\n+                    tbl.get(name)\n+                } else if tbl.len() == 1 {\n+                    tbl.iter().next().map(|(_, lc)| lc)\n+                } else {\n+                    None\n+                }\n+            })\n+            .map(|lc| lc.to_record_batches(predicates))\n+    }\n+}\n+\n+/// A Last-N-Values Cache\n+///\n+/// A hierarchical cache whose structure is determined by a set of `key_columns`, each of which\n+/// represents a level in the hierarchy. The lowest level of the hierarchy holds the last N values\n+/// for the field columns in the cache.\n+pub(crate) struct LastCache {\n+    /// The number of values to hold in the cache\n+    ///\n+    /// Once the cache reaches this size, old values will be evicted when new values are pushed in.\n+    count: LastCacheSize,\n+    /// The time-to-live (TTL) for values in the cache\n+    ///\n+    /// Once values have lived in the cache beyond this [`Duration`], they can be evicted using\n+    /// the [`remove_expired`][LastCache::remove_expired] method.\n+    ttl: Duration,\n+    /// The key columns for this cache\n+    key_columns: Vec<String>,\n+    /// The Influx Schema for the table that this cache is associated with\n+    schema: Schema,\n+    /// The internal state of the cache\n+    state: LastCacheState,\n+}\n+\n+impl LastCache {\n+    /// Create a new [`LastCache`]\n+    fn new(count: LastCacheSize, ttl: Duration, key_columns: Vec<String>, schema: Schema) -> Self {\n+        Self {\n+            count,\n+            ttl,\n+            key_columns,\n+            schema,\n+            state: LastCacheState::Init,\n+        }\n+    }\n+\n+    /// Push a [`Row`] from the write buffer into the cache\n+    ///\n+    /// If a key column is not present in the row, the row will be ignored.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This will panic if the internal cache state's keys are out-of-order with respect to the\n+    /// order of the `key_columns` on this [`LastCache`]\n+    pub(crate) fn push(&mut self, row: &Row) {\n+        let mut target = &mut self.state;\n+        let mut key_iter = self.key_columns.iter().peekable();\n+        while let (Some(key), peek) = (key_iter.next(), key_iter.peek()) {\n+            if target.is_init() {\n+                *target = LastCacheState::Key(LastCacheKey {\n+                    column_name: key.to_string(),\n+                    value_map: Default::default(),\n+                });\n+            }\n+            let Some(value) = row\n+                .fields\n+                .iter()\n+                .find(|f| f.name == *key)\n+                .map(|f| KeyValue::from(&f.value))\n+            else {\n+                // ignore the row if it does not contain all key columns",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1670699685",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25109,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1670699685",
        "commented_code": "@@ -0,0 +1,2225 @@\n+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name\n+        if self\n+            .cache_map\n+            .read()\n+            .get(&db_name)\n+            .and_then(|db| db.get(&tbl_name))\n+            .is_some_and(|tbl| tbl.contains_key(&cache_name))\n+        {\n+            return Err(Error::CacheAlreadyExists);\n+        }\n+\n+        let value_columns = if let Some(mut vals) = value_columns {\n+            // if value columns are specified, check that they are present in the table schema\n+            for name in vals.iter() {\n+                if schema.field_by_name(name).is_none() {\n+                    return Err(Error::ValueColumnDoesNotExist {\n+                        column_name: name.into(),\n+                    });\n+                }\n+            }\n+            // double-check that time column is included\n+            let time_col = TIME_COLUMN_NAME.to_string();\n+            if !vals.contains(&time_col) {\n+                vals.push(time_col);\n+            }\n+            vals\n+        } else {\n+            // default to all non-key columns\n+            schema\n+                .iter()\n+                .filter_map(|(_, f)| {\n+                    if key_columns.contains(f.name()) {\n+                        None\n+                    } else {\n+                        Some(f.name().to_string())\n+                    }\n+                })\n+                .collect::<Vec<String>>()\n+        };\n+\n+        // build a schema that only holds the field columns\n+        let mut schema_builder = SchemaBuilder::new();\n+        for (t, name) in schema\n+            .iter()\n+            .filter(|&(_, f)| value_columns.contains(f.name()))\n+            .map(|(t, f)| (t, f.name()))\n+        {\n+            schema_builder.influx_column(name, t);\n+        }\n+\n+        // create the actual last cache:\n+        let last_cache = LastCache::new(\n+            count\n+                .unwrap_or(1)\n+                .try_into()\n+                .map_err(|_| Error::InvalidCacheSize)?,\n+            ttl.unwrap_or(DEFAULT_CACHE_TTL),\n+            key_columns,\n+            schema_builder.build()?,\n+        );\n+\n+        // get the write lock and insert:\n+        self.cache_map\n+            .write()\n+            .entry(db_name)\n+            .or_default()\n+            .entry(tbl_name)\n+            .or_default()\n+            .insert(cache_name, last_cache);\n+\n+        Ok(())\n+    }\n+\n+    /// Write a batch from the buffer into the cache by iterating over its database and table batches\n+    /// to find entries that belong in the cache.\n+    ///\n+    /// Only if rows are newer than the latest entry in the cache will they be entered.\n+    pub(crate) fn write_batch_to_cache(&self, write_batch: &WriteBatch) {\n+        let mut cache_map = self.cache_map.write();\n+        for (db_name, db_batch) in &write_batch.database_batches {\n+            if let Some(db_cache) = cache_map.get_mut(db_name.as_str()) {\n+                if db_cache.is_empty() {\n+                    continue;\n+                }\n+                for (tbl_name, tbl_batch) in &db_batch.table_batches {\n+                    if let Some(tbl_cache) = db_cache.get_mut(tbl_name) {\n+                        for (_, last_cache) in tbl_cache.iter_mut() {\n+                            for row in &tbl_batch.rows {\n+                                last_cache.push(row);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Recurse down the cache structure to evict expired cache entries, based on their respective\n+    /// time-to-live (TTL).\n+    pub(crate) fn evict_expired_cache_entries(&self) {\n+        let mut cache_map = self.cache_map.write();\n+        cache_map.iter_mut().for_each(|(_, db)| {\n+            db.iter_mut()\n+                .for_each(|(_, tbl)| tbl.iter_mut().for_each(|(_, lc)| lc.remove_expired()))\n+        });\n+    }\n+\n+    /// Output the records for a given cache as arrow [`RecordBatch`]es\n+    #[cfg(test)]\n+    fn get_cache_record_batches(\n+        &self,\n+        db_name: &str,\n+        tbl_name: &str,\n+        cache_name: Option<&str>,\n+        predicates: &[Predicate],\n+    ) -> Option<Result<Vec<RecordBatch>, ArrowError>> {\n+        self.cache_map\n+            .read()\n+            .get(db_name)\n+            .and_then(|db| db.get(tbl_name))\n+            .and_then(|tbl| {\n+                if let Some(name) = cache_name {\n+                    tbl.get(name)\n+                } else if tbl.len() == 1 {\n+                    tbl.iter().next().map(|(_, lc)| lc)\n+                } else {\n+                    None\n+                }\n+            })\n+            .map(|lc| lc.to_record_batches(predicates))\n+    }\n+}\n+\n+/// A Last-N-Values Cache\n+///\n+/// A hierarchical cache whose structure is determined by a set of `key_columns`, each of which\n+/// represents a level in the hierarchy. The lowest level of the hierarchy holds the last N values\n+/// for the field columns in the cache.\n+pub(crate) struct LastCache {\n+    /// The number of values to hold in the cache\n+    ///\n+    /// Once the cache reaches this size, old values will be evicted when new values are pushed in.\n+    count: LastCacheSize,\n+    /// The time-to-live (TTL) for values in the cache\n+    ///\n+    /// Once values have lived in the cache beyond this [`Duration`], they can be evicted using\n+    /// the [`remove_expired`][LastCache::remove_expired] method.\n+    ttl: Duration,\n+    /// The key columns for this cache\n+    key_columns: Vec<String>,\n+    /// The Influx Schema for the table that this cache is associated with\n+    schema: Schema,\n+    /// The internal state of the cache\n+    state: LastCacheState,\n+}\n+\n+impl LastCache {\n+    /// Create a new [`LastCache`]\n+    fn new(count: LastCacheSize, ttl: Duration, key_columns: Vec<String>, schema: Schema) -> Self {\n+        Self {\n+            count,\n+            ttl,\n+            key_columns,\n+            schema,\n+            state: LastCacheState::Init,\n+        }\n+    }\n+\n+    /// Push a [`Row`] from the write buffer into the cache\n+    ///\n+    /// If a key column is not present in the row, the row will be ignored.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This will panic if the internal cache state's keys are out-of-order with respect to the\n+    /// order of the `key_columns` on this [`LastCache`]\n+    pub(crate) fn push(&mut self, row: &Row) {\n+        let mut target = &mut self.state;\n+        let mut key_iter = self.key_columns.iter().peekable();\n+        while let (Some(key), peek) = (key_iter.next(), key_iter.peek()) {\n+            if target.is_init() {\n+                *target = LastCacheState::Key(LastCacheKey {\n+                    column_name: key.to_string(),\n+                    value_map: Default::default(),\n+                });\n+            }\n+            let Some(value) = row\n+                .fields\n+                .iter()\n+                .find(|f| f.name == *key)\n+                .map(|f| KeyValue::from(&f.value))\n+            else {\n+                // ignore the row if it does not contain all key columns",
        "comment_created_at": "2024-07-09T15:08:26+00:00",
        "comment_author": "pauldix",
        "comment_body": "We might want to represent null key column values in the structure? I'm not sure about this yet though. Something to think about and discuss.",
        "pr_file_module": null
      },
      {
        "comment_id": "1670751878",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25109,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1670699685",
        "commented_code": "@@ -0,0 +1,2225 @@\n+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name\n+        if self\n+            .cache_map\n+            .read()\n+            .get(&db_name)\n+            .and_then(|db| db.get(&tbl_name))\n+            .is_some_and(|tbl| tbl.contains_key(&cache_name))\n+        {\n+            return Err(Error::CacheAlreadyExists);\n+        }\n+\n+        let value_columns = if let Some(mut vals) = value_columns {\n+            // if value columns are specified, check that they are present in the table schema\n+            for name in vals.iter() {\n+                if schema.field_by_name(name).is_none() {\n+                    return Err(Error::ValueColumnDoesNotExist {\n+                        column_name: name.into(),\n+                    });\n+                }\n+            }\n+            // double-check that time column is included\n+            let time_col = TIME_COLUMN_NAME.to_string();\n+            if !vals.contains(&time_col) {\n+                vals.push(time_col);\n+            }\n+            vals\n+        } else {\n+            // default to all non-key columns\n+            schema\n+                .iter()\n+                .filter_map(|(_, f)| {\n+                    if key_columns.contains(f.name()) {\n+                        None\n+                    } else {\n+                        Some(f.name().to_string())\n+                    }\n+                })\n+                .collect::<Vec<String>>()\n+        };\n+\n+        // build a schema that only holds the field columns\n+        let mut schema_builder = SchemaBuilder::new();\n+        for (t, name) in schema\n+            .iter()\n+            .filter(|&(_, f)| value_columns.contains(f.name()))\n+            .map(|(t, f)| (t, f.name()))\n+        {\n+            schema_builder.influx_column(name, t);\n+        }\n+\n+        // create the actual last cache:\n+        let last_cache = LastCache::new(\n+            count\n+                .unwrap_or(1)\n+                .try_into()\n+                .map_err(|_| Error::InvalidCacheSize)?,\n+            ttl.unwrap_or(DEFAULT_CACHE_TTL),\n+            key_columns,\n+            schema_builder.build()?,\n+        );\n+\n+        // get the write lock and insert:\n+        self.cache_map\n+            .write()\n+            .entry(db_name)\n+            .or_default()\n+            .entry(tbl_name)\n+            .or_default()\n+            .insert(cache_name, last_cache);\n+\n+        Ok(())\n+    }\n+\n+    /// Write a batch from the buffer into the cache by iterating over its database and table batches\n+    /// to find entries that belong in the cache.\n+    ///\n+    /// Only if rows are newer than the latest entry in the cache will they be entered.\n+    pub(crate) fn write_batch_to_cache(&self, write_batch: &WriteBatch) {\n+        let mut cache_map = self.cache_map.write();\n+        for (db_name, db_batch) in &write_batch.database_batches {\n+            if let Some(db_cache) = cache_map.get_mut(db_name.as_str()) {\n+                if db_cache.is_empty() {\n+                    continue;\n+                }\n+                for (tbl_name, tbl_batch) in &db_batch.table_batches {\n+                    if let Some(tbl_cache) = db_cache.get_mut(tbl_name) {\n+                        for (_, last_cache) in tbl_cache.iter_mut() {\n+                            for row in &tbl_batch.rows {\n+                                last_cache.push(row);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Recurse down the cache structure to evict expired cache entries, based on their respective\n+    /// time-to-live (TTL).\n+    pub(crate) fn evict_expired_cache_entries(&self) {\n+        let mut cache_map = self.cache_map.write();\n+        cache_map.iter_mut().for_each(|(_, db)| {\n+            db.iter_mut()\n+                .for_each(|(_, tbl)| tbl.iter_mut().for_each(|(_, lc)| lc.remove_expired()))\n+        });\n+    }\n+\n+    /// Output the records for a given cache as arrow [`RecordBatch`]es\n+    #[cfg(test)]\n+    fn get_cache_record_batches(\n+        &self,\n+        db_name: &str,\n+        tbl_name: &str,\n+        cache_name: Option<&str>,\n+        predicates: &[Predicate],\n+    ) -> Option<Result<Vec<RecordBatch>, ArrowError>> {\n+        self.cache_map\n+            .read()\n+            .get(db_name)\n+            .and_then(|db| db.get(tbl_name))\n+            .and_then(|tbl| {\n+                if let Some(name) = cache_name {\n+                    tbl.get(name)\n+                } else if tbl.len() == 1 {\n+                    tbl.iter().next().map(|(_, lc)| lc)\n+                } else {\n+                    None\n+                }\n+            })\n+            .map(|lc| lc.to_record_batches(predicates))\n+    }\n+}\n+\n+/// A Last-N-Values Cache\n+///\n+/// A hierarchical cache whose structure is determined by a set of `key_columns`, each of which\n+/// represents a level in the hierarchy. The lowest level of the hierarchy holds the last N values\n+/// for the field columns in the cache.\n+pub(crate) struct LastCache {\n+    /// The number of values to hold in the cache\n+    ///\n+    /// Once the cache reaches this size, old values will be evicted when new values are pushed in.\n+    count: LastCacheSize,\n+    /// The time-to-live (TTL) for values in the cache\n+    ///\n+    /// Once values have lived in the cache beyond this [`Duration`], they can be evicted using\n+    /// the [`remove_expired`][LastCache::remove_expired] method.\n+    ttl: Duration,\n+    /// The key columns for this cache\n+    key_columns: Vec<String>,\n+    /// The Influx Schema for the table that this cache is associated with\n+    schema: Schema,\n+    /// The internal state of the cache\n+    state: LastCacheState,\n+}\n+\n+impl LastCache {\n+    /// Create a new [`LastCache`]\n+    fn new(count: LastCacheSize, ttl: Duration, key_columns: Vec<String>, schema: Schema) -> Self {\n+        Self {\n+            count,\n+            ttl,\n+            key_columns,\n+            schema,\n+            state: LastCacheState::Init,\n+        }\n+    }\n+\n+    /// Push a [`Row`] from the write buffer into the cache\n+    ///\n+    /// If a key column is not present in the row, the row will be ignored.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This will panic if the internal cache state's keys are out-of-order with respect to the\n+    /// order of the `key_columns` on this [`LastCache`]\n+    pub(crate) fn push(&mut self, row: &Row) {\n+        let mut target = &mut self.state;\n+        let mut key_iter = self.key_columns.iter().peekable();\n+        while let (Some(key), peek) = (key_iter.next(), key_iter.peek()) {\n+            if target.is_init() {\n+                *target = LastCacheState::Key(LastCacheKey {\n+                    column_name: key.to_string(),\n+                    value_map: Default::default(),\n+                });\n+            }\n+            let Some(value) = row\n+                .fields\n+                .iter()\n+                .find(|f| f.name == *key)\n+                .map(|f| KeyValue::from(&f.value))\n+            else {\n+                // ignore the row if it does not contain all key columns",
        "comment_created_at": "2024-07-09T15:37:15+00:00",
        "comment_author": "hiltontj",
        "comment_body": "When I first refactored this, I had the `LastCacheKey` hold a `HashMap<Option<KeyValue>, LastCacheState>` to handle null key values, but switched to this behaviour to simplify it.\r\n\r\nI think It just needs to do that, i.e., use `Option<KeyValue>` instead of `KeyValue`, and then store the datatype in the `LastCacheKey`, because 1) key columns will always have a fixed data type, and 2) you can't rely on the `KeyValue` alone to get the data type when it is `None` (for creating `RecordBatch`es).\r\n\r\nI can open up an issue for this.",
        "pr_file_module": null
      },
      {
        "comment_id": "1670753939",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25109,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1670699685",
        "commented_code": "@@ -0,0 +1,2225 @@\n+use std::{\n+    any::Any,\n+    collections::VecDeque,\n+    sync::Arc,\n+    time::{Duration, Instant},\n+};\n+\n+use arrow::{\n+    array::{\n+        ArrayRef, BooleanBuilder, Float64Builder, GenericByteDictionaryBuilder, Int64Builder,\n+        RecordBatch, StringBuilder, StringDictionaryBuilder, TimestampNanosecondBuilder,\n+        UInt64Builder,\n+    },\n+    datatypes::{\n+        DataType, Field as ArrowField, FieldRef, GenericStringType, Int32Type,\n+        SchemaBuilder as ArrowSchemaBuilder, SchemaRef as ArrowSchemaRef, TimeUnit,\n+    },\n+    error::ArrowError,\n+};\n+use async_trait::async_trait;\n+use datafusion::{\n+    common::Result as DFResult,\n+    datasource::{TableProvider, TableType},\n+    execution::context::SessionState,\n+    logical_expr::{BinaryExpr, Expr, Operator, TableProviderFilterPushDown},\n+    physical_plan::{memory::MemoryExec, ExecutionPlan},\n+    scalar::ScalarValue,\n+};\n+use hashbrown::{HashMap, HashSet};\n+use indexmap::IndexMap;\n+use iox_time::Time;\n+use parking_lot::RwLock;\n+use schema::{InfluxColumnType, InfluxFieldType, Schema, SchemaBuilder, TIME_COLUMN_NAME};\n+\n+use crate::{\n+    catalog::LastCacheSize,\n+    write_buffer::{buffer_segment::WriteBatch, FieldData, Row},\n+};\n+\n+#[derive(Debug, thiserror::Error)]\n+pub enum Error {\n+    #[error(\"invalid cache size\")]\n+    InvalidCacheSize,\n+    #[error(\"last cache already exists for database and table\")]\n+    CacheAlreadyExists,\n+    #[error(\"specified key column ({column_name}) does not exist in the table schema\")]\n+    KeyColumnDoesNotExist { column_name: String },\n+    #[error(\"key column must be string, int, uint, or bool types\")]\n+    InvalidKeyColumn,\n+    #[error(\"specified value column ({column_name}) does not exist in the table schema\")]\n+    ValueColumnDoesNotExist { column_name: String },\n+    #[error(\"schema builder error: {0}\")]\n+    SchemaBuilder(#[from] schema::builder::Error),\n+}\n+\n+/// A three level hashmap storing Database Name -> Table Name -> Cache Name -> LastCache\n+type CacheMap = RwLock<HashMap<String, HashMap<String, HashMap<String, LastCache>>>>;\n+\n+/// Provides all last-N-value caches for the entire database\n+pub struct LastCacheProvider {\n+    cache_map: CacheMap,\n+}\n+\n+impl std::fmt::Debug for LastCacheProvider {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"LastCacheProvider\")\n+    }\n+}\n+\n+/// The default cache time-to-live (TTL) is 4 hours\n+const DEFAULT_CACHE_TTL: Duration = Duration::from_secs(60 * 60 * 4);\n+\n+/// Arguments to the [`LastCacheProvider::create_cache`] method\n+pub(crate) struct CreateCacheArguments {\n+    /// The name of the database to create the cache for\n+    pub(crate) db_name: String,\n+    /// The name of the table in the database to create the cache for\n+    pub(crate) tbl_name: String,\n+    /// The Influx Schema of the table\n+    pub(crate) schema: Schema,\n+    /// An optional name for the cache\n+    ///\n+    /// The cache name will default to `<table_name>_<keys>_last_cache`\n+    pub(crate) cache_name: Option<String>,\n+    /// The number of values to hold in the created cache\n+    ///\n+    /// This will default to 1.\n+    pub(crate) count: Option<usize>,\n+    /// The time-to-live (TTL) for the created cache\n+    ///\n+    /// This will default to [`DEFAULT_CACHE_TTL`]\n+    pub(crate) ttl: Option<Duration>,\n+    /// The key column names to use in the cache hierarchy\n+    ///\n+    /// This will default to:\n+    /// - the series key columns for a v3 table\n+    /// - the lexicographically ordered tag set for a v1 table\n+    pub(crate) key_columns: Option<Vec<String>>,\n+    /// The value columns to use in the cache\n+    ///\n+    /// This will default to all non-key columns. The `time` column is always included.\n+    pub(crate) value_columns: Option<Vec<String>>,\n+}\n+\n+impl LastCacheProvider {\n+    /// Create a new [`LastCacheProvider`]\n+    pub(crate) fn new() -> Self {\n+        Self {\n+            cache_map: Default::default(),\n+        }\n+    }\n+\n+    /// Create a new entry in the last cache for a given database and table, along with the given\n+    /// parameters.\n+    pub(crate) fn create_cache(\n+        &self,\n+        CreateCacheArguments {\n+            db_name,\n+            tbl_name,\n+            schema,\n+            cache_name,\n+            count,\n+            ttl,\n+            key_columns,\n+            value_columns,\n+        }: CreateCacheArguments,\n+    ) -> Result<(), Error> {\n+        let key_columns = if let Some(keys) = key_columns {\n+            // validate the key columns specified to ensure correct type (string, int, unit, or bool)\n+            // and that they exist in the table's schema.\n+            for key in keys.iter() {\n+                use InfluxColumnType::*;\n+                use InfluxFieldType::*;\n+                match schema.field_by_name(key) {\n+                    Some((\n+                        Tag | Field(Integer) | Field(UInteger) | Field(String) | Field(Boolean),\n+                        _,\n+                    )) => (),\n+                    Some((_, _)) => return Err(Error::InvalidKeyColumn),\n+                    None => {\n+                        return Err(Error::KeyColumnDoesNotExist {\n+                            column_name: key.into(),\n+                        })\n+                    }\n+                }\n+            }\n+            keys\n+        } else {\n+            // use primary key, which defaults to series key if present, then lexicographically\n+            // ordered tags otherwise, there is no user-defined sort order in the schema, so if that\n+            // is introduced, we will need to make sure that is accommodated here.\n+            let mut keys = schema.primary_key();\n+            if let Some(&TIME_COLUMN_NAME) = keys.last() {\n+                keys.pop();\n+            }\n+            keys.iter().map(|s| s.to_string()).collect()\n+        };\n+\n+        // Generate the cache name if it was not provided\n+        let cache_name = cache_name.unwrap_or_else(|| {\n+            format!(\"{tbl_name}_{keys}_last_cache\", keys = key_columns.join(\"_\"))\n+        });\n+\n+        // reject creation if there is already a cache with specified database, table, and cache name\n+        if self\n+            .cache_map\n+            .read()\n+            .get(&db_name)\n+            .and_then(|db| db.get(&tbl_name))\n+            .is_some_and(|tbl| tbl.contains_key(&cache_name))\n+        {\n+            return Err(Error::CacheAlreadyExists);\n+        }\n+\n+        let value_columns = if let Some(mut vals) = value_columns {\n+            // if value columns are specified, check that they are present in the table schema\n+            for name in vals.iter() {\n+                if schema.field_by_name(name).is_none() {\n+                    return Err(Error::ValueColumnDoesNotExist {\n+                        column_name: name.into(),\n+                    });\n+                }\n+            }\n+            // double-check that time column is included\n+            let time_col = TIME_COLUMN_NAME.to_string();\n+            if !vals.contains(&time_col) {\n+                vals.push(time_col);\n+            }\n+            vals\n+        } else {\n+            // default to all non-key columns\n+            schema\n+                .iter()\n+                .filter_map(|(_, f)| {\n+                    if key_columns.contains(f.name()) {\n+                        None\n+                    } else {\n+                        Some(f.name().to_string())\n+                    }\n+                })\n+                .collect::<Vec<String>>()\n+        };\n+\n+        // build a schema that only holds the field columns\n+        let mut schema_builder = SchemaBuilder::new();\n+        for (t, name) in schema\n+            .iter()\n+            .filter(|&(_, f)| value_columns.contains(f.name()))\n+            .map(|(t, f)| (t, f.name()))\n+        {\n+            schema_builder.influx_column(name, t);\n+        }\n+\n+        // create the actual last cache:\n+        let last_cache = LastCache::new(\n+            count\n+                .unwrap_or(1)\n+                .try_into()\n+                .map_err(|_| Error::InvalidCacheSize)?,\n+            ttl.unwrap_or(DEFAULT_CACHE_TTL),\n+            key_columns,\n+            schema_builder.build()?,\n+        );\n+\n+        // get the write lock and insert:\n+        self.cache_map\n+            .write()\n+            .entry(db_name)\n+            .or_default()\n+            .entry(tbl_name)\n+            .or_default()\n+            .insert(cache_name, last_cache);\n+\n+        Ok(())\n+    }\n+\n+    /// Write a batch from the buffer into the cache by iterating over its database and table batches\n+    /// to find entries that belong in the cache.\n+    ///\n+    /// Only if rows are newer than the latest entry in the cache will they be entered.\n+    pub(crate) fn write_batch_to_cache(&self, write_batch: &WriteBatch) {\n+        let mut cache_map = self.cache_map.write();\n+        for (db_name, db_batch) in &write_batch.database_batches {\n+            if let Some(db_cache) = cache_map.get_mut(db_name.as_str()) {\n+                if db_cache.is_empty() {\n+                    continue;\n+                }\n+                for (tbl_name, tbl_batch) in &db_batch.table_batches {\n+                    if let Some(tbl_cache) = db_cache.get_mut(tbl_name) {\n+                        for (_, last_cache) in tbl_cache.iter_mut() {\n+                            for row in &tbl_batch.rows {\n+                                last_cache.push(row);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Recurse down the cache structure to evict expired cache entries, based on their respective\n+    /// time-to-live (TTL).\n+    pub(crate) fn evict_expired_cache_entries(&self) {\n+        let mut cache_map = self.cache_map.write();\n+        cache_map.iter_mut().for_each(|(_, db)| {\n+            db.iter_mut()\n+                .for_each(|(_, tbl)| tbl.iter_mut().for_each(|(_, lc)| lc.remove_expired()))\n+        });\n+    }\n+\n+    /// Output the records for a given cache as arrow [`RecordBatch`]es\n+    #[cfg(test)]\n+    fn get_cache_record_batches(\n+        &self,\n+        db_name: &str,\n+        tbl_name: &str,\n+        cache_name: Option<&str>,\n+        predicates: &[Predicate],\n+    ) -> Option<Result<Vec<RecordBatch>, ArrowError>> {\n+        self.cache_map\n+            .read()\n+            .get(db_name)\n+            .and_then(|db| db.get(tbl_name))\n+            .and_then(|tbl| {\n+                if let Some(name) = cache_name {\n+                    tbl.get(name)\n+                } else if tbl.len() == 1 {\n+                    tbl.iter().next().map(|(_, lc)| lc)\n+                } else {\n+                    None\n+                }\n+            })\n+            .map(|lc| lc.to_record_batches(predicates))\n+    }\n+}\n+\n+/// A Last-N-Values Cache\n+///\n+/// A hierarchical cache whose structure is determined by a set of `key_columns`, each of which\n+/// represents a level in the hierarchy. The lowest level of the hierarchy holds the last N values\n+/// for the field columns in the cache.\n+pub(crate) struct LastCache {\n+    /// The number of values to hold in the cache\n+    ///\n+    /// Once the cache reaches this size, old values will be evicted when new values are pushed in.\n+    count: LastCacheSize,\n+    /// The time-to-live (TTL) for values in the cache\n+    ///\n+    /// Once values have lived in the cache beyond this [`Duration`], they can be evicted using\n+    /// the [`remove_expired`][LastCache::remove_expired] method.\n+    ttl: Duration,\n+    /// The key columns for this cache\n+    key_columns: Vec<String>,\n+    /// The Influx Schema for the table that this cache is associated with\n+    schema: Schema,\n+    /// The internal state of the cache\n+    state: LastCacheState,\n+}\n+\n+impl LastCache {\n+    /// Create a new [`LastCache`]\n+    fn new(count: LastCacheSize, ttl: Duration, key_columns: Vec<String>, schema: Schema) -> Self {\n+        Self {\n+            count,\n+            ttl,\n+            key_columns,\n+            schema,\n+            state: LastCacheState::Init,\n+        }\n+    }\n+\n+    /// Push a [`Row`] from the write buffer into the cache\n+    ///\n+    /// If a key column is not present in the row, the row will be ignored.\n+    ///\n+    /// # Panics\n+    ///\n+    /// This will panic if the internal cache state's keys are out-of-order with respect to the\n+    /// order of the `key_columns` on this [`LastCache`]\n+    pub(crate) fn push(&mut self, row: &Row) {\n+        let mut target = &mut self.state;\n+        let mut key_iter = self.key_columns.iter().peekable();\n+        while let (Some(key), peek) = (key_iter.next(), key_iter.peek()) {\n+            if target.is_init() {\n+                *target = LastCacheState::Key(LastCacheKey {\n+                    column_name: key.to_string(),\n+                    value_map: Default::default(),\n+                });\n+            }\n+            let Some(value) = row\n+                .fields\n+                .iter()\n+                .find(|f| f.name == *key)\n+                .map(|f| KeyValue::from(&f.value))\n+            else {\n+                // ignore the row if it does not contain all key columns",
        "comment_created_at": "2024-07-09T15:38:35+00:00",
        "comment_author": "pauldix",
        "comment_body": "Yeah, still not totally sure we want to bother with this, so maybe log an issue to see if anyone cares. For now I'd leave it as is.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1652836844",
    "pr_number": 25085,
    "pr_file": "influxdb3_server/src/http/v1.rs",
    "created_at": "2024-06-25T13:34:35+00:00",
    "commented_code": "}))\n             .context(\"failed to cast batch time column with `epoch` parameter specified\")?;\n         }\n-        // See https://github.com/influxdata/influxdb/issues/24981\n-        #[allow(deprecated)]\n-        let json_rows = record_batches_to_json_rows(&[&batch])\n-            .context(\"failed to convert RecordBatch to JSON rows\")?;\n-        for json_row in json_rows {\n-            let mut row = vec![Value::Null; self.column_map.len()];\n-            for (k, v) in json_row {\n-                if k == INFLUXQL_MEASUREMENT_COLUMN_NAME\n-                    && (self.buffer.current_measurement_name().is_none()\n-                        || self\n-                            .buffer\n-                            .current_measurement_name()\n-                            .is_some_and(|n| *n != v))\n-                {\n-                    // we are on the \"iox::measurement\" column, which gives the name of the time series\n-                    // if we are on the first row, or if the measurement changes, we push into the\n-                    // buffer queue\n-                    self.buffer\n-                        .push_next_measurement(v.as_str().with_context(|| {\n-                            format!(\"{INFLUXQL_MEASUREMENT_COLUMN_NAME} value was not a string\")\n-                        })?);\n-                } else if k == INFLUXQL_MEASUREMENT_COLUMN_NAME {\n-                    // we are still working on the current measurement in the buffer, so ignore\n-                    continue;\n+        let column_map = &self.column_map;\n+        let columns = batch.columns();\n+        let schema = batch.schema();\n+\n+        for row_index in 0..batch.num_rows() {\n+            let mut row = vec![Value::Null; column_map.len()];\n+\n+            for (col_index, column) in columns.iter().enumerate() {\n+                let field = schema.field(col_index);\n+                let column_name = field.name();\n+\n+                let mut cell_value = if !column.is_valid(row_index) {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1652836844",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25085,
        "pr_file": "influxdb3_server/src/http/v1.rs",
        "discussion_id": "1652836844",
        "commented_code": "@@ -337,41 +341,46 @@ impl QueryResponseStream {\n             }))\n             .context(\"failed to cast batch time column with `epoch` parameter specified\")?;\n         }\n-        // See https://github.com/influxdata/influxdb/issues/24981\n-        #[allow(deprecated)]\n-        let json_rows = record_batches_to_json_rows(&[&batch])\n-            .context(\"failed to convert RecordBatch to JSON rows\")?;\n-        for json_row in json_rows {\n-            let mut row = vec![Value::Null; self.column_map.len()];\n-            for (k, v) in json_row {\n-                if k == INFLUXQL_MEASUREMENT_COLUMN_NAME\n-                    && (self.buffer.current_measurement_name().is_none()\n-                        || self\n-                            .buffer\n-                            .current_measurement_name()\n-                            .is_some_and(|n| *n != v))\n-                {\n-                    // we are on the \"iox::measurement\" column, which gives the name of the time series\n-                    // if we are on the first row, or if the measurement changes, we push into the\n-                    // buffer queue\n-                    self.buffer\n-                        .push_next_measurement(v.as_str().with_context(|| {\n-                            format!(\"{INFLUXQL_MEASUREMENT_COLUMN_NAME} value was not a string\")\n-                        })?);\n-                } else if k == INFLUXQL_MEASUREMENT_COLUMN_NAME {\n-                    // we are still working on the current measurement in the buffer, so ignore\n-                    continue;\n+        let column_map = &self.column_map;\n+        let columns = batch.columns();\n+        let schema = batch.schema();\n+\n+        for row_index in 0..batch.num_rows() {\n+            let mut row = vec![Value::Null; column_map.len()];\n+\n+            for (col_index, column) in columns.iter().enumerate() {\n+                let field = schema.field(col_index);\n+                let column_name = field.name();\n+\n+                let mut cell_value = if !column.is_valid(row_index) {",
        "comment_created_at": "2024-06-25T13:34:35+00:00",
        "comment_author": "hiltontj",
        "comment_body": "The [`is_valid` function](https://docs.rs/arrow/latest/arrow/array/trait.Array.html#method.is_valid) indicates whether the value at given index is null or not - the data is still _valid_, if that makes sense. So, instead of `bail!`ing here, I think you can just `continue;` in the inner loop, leaving the cell as `Value::Null`.",
        "pr_file_module": null
      },
      {
        "comment_id": "1655554005",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25085,
        "pr_file": "influxdb3_server/src/http/v1.rs",
        "discussion_id": "1652836844",
        "commented_code": "@@ -337,41 +341,46 @@ impl QueryResponseStream {\n             }))\n             .context(\"failed to cast batch time column with `epoch` parameter specified\")?;\n         }\n-        // See https://github.com/influxdata/influxdb/issues/24981\n-        #[allow(deprecated)]\n-        let json_rows = record_batches_to_json_rows(&[&batch])\n-            .context(\"failed to convert RecordBatch to JSON rows\")?;\n-        for json_row in json_rows {\n-            let mut row = vec![Value::Null; self.column_map.len()];\n-            for (k, v) in json_row {\n-                if k == INFLUXQL_MEASUREMENT_COLUMN_NAME\n-                    && (self.buffer.current_measurement_name().is_none()\n-                        || self\n-                            .buffer\n-                            .current_measurement_name()\n-                            .is_some_and(|n| *n != v))\n-                {\n-                    // we are on the \"iox::measurement\" column, which gives the name of the time series\n-                    // if we are on the first row, or if the measurement changes, we push into the\n-                    // buffer queue\n-                    self.buffer\n-                        .push_next_measurement(v.as_str().with_context(|| {\n-                            format!(\"{INFLUXQL_MEASUREMENT_COLUMN_NAME} value was not a string\")\n-                        })?);\n-                } else if k == INFLUXQL_MEASUREMENT_COLUMN_NAME {\n-                    // we are still working on the current measurement in the buffer, so ignore\n-                    continue;\n+        let column_map = &self.column_map;\n+        let columns = batch.columns();\n+        let schema = batch.schema();\n+\n+        for row_index in 0..batch.num_rows() {\n+            let mut row = vec![Value::Null; column_map.len()];\n+\n+            for (col_index, column) in columns.iter().enumerate() {\n+                let field = schema.field(col_index);\n+                let column_name = field.name();\n+\n+                let mut cell_value = if !column.is_valid(row_index) {",
        "comment_created_at": "2024-06-26T21:38:32+00:00",
        "comment_author": "JeanArhancet",
        "comment_body": "Fixed ",
        "pr_file_module": null
      }
    ]
  }
]