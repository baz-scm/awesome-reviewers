[
  {
    "discussion_id": "1975740868",
    "pr_number": 25997,
    "pr_file": "influxdb3/src/commands/show/system.rs",
    "created_at": "2025-02-28T17:03:12+00:00",
    "commented_code": "impl SystemCommandRunner {\n     async fn list(&self, config: TableListConfig) -> Result<()> {\n-        let bs = self\n+        let TableListConfig {\n+            output_format,\n+            output_file_path,\n+        } = &config;\n+\n+        let mut bs = self\n             .client\n             .api_v3_query_sql(self.db.as_str(), SYS_TABLES_QUERY)\n-            .format(config.output_format.into())\n+            .format(output_format.clone().into())\n             .send()\n             .await?;\n \n-        println!(\"{}\", String::from_utf8(bs.as_ref().to_vec()).unwrap());\n+        if let Some(path) = output_file_path {\n+            let mut f = OpenOptions::new()\n+                .write(true)\n+                .create(true)\n+                .truncate(true)\n+                .open(path)\n+                .await?;\n+            f.write_all_buf(&mut bs).await?;\n+        } else {\n+            if output_format.is_parquet() {\n+                Err(Error::NoOutputFileForParquet)?\n+            }\n+            println!(\"{}\", String::from_utf8(bs.as_ref().to_vec()).unwrap());\n+        }",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1975740868",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25997,
        "pr_file": "influxdb3/src/commands/show/system.rs",
        "discussion_id": "1975740868",
        "commented_code": "@@ -100,14 +115,32 @@ impl std::fmt::Display for SystemTableNotFound {\n \n impl SystemCommandRunner {\n     async fn list(&self, config: TableListConfig) -> Result<()> {\n-        let bs = self\n+        let TableListConfig {\n+            output_format,\n+            output_file_path,\n+        } = &config;\n+\n+        let mut bs = self\n             .client\n             .api_v3_query_sql(self.db.as_str(), SYS_TABLES_QUERY)\n-            .format(config.output_format.into())\n+            .format(output_format.clone().into())\n             .send()\n             .await?;\n \n-        println!(\"{}\", String::from_utf8(bs.as_ref().to_vec()).unwrap());\n+        if let Some(path) = output_file_path {\n+            let mut f = OpenOptions::new()\n+                .write(true)\n+                .create(true)\n+                .truncate(true)\n+                .open(path)\n+                .await?;\n+            f.write_all_buf(&mut bs).await?;\n+        } else {\n+            if output_format.is_parquet() {\n+                Err(Error::NoOutputFileForParquet)?\n+            }\n+            println!(\"{}\", String::from_utf8(bs.as_ref().to_vec()).unwrap());\n+        }",
        "comment_created_at": "2025-02-28T17:03:12+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Could you move this code into a helper function since it is re-used in several places.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1955255363",
    "pr_number": 25969,
    "pr_file": "influxdb3_processing_engine/src/virtualenv.rs",
    "created_at": "2025-02-13T21:44:35+00:00",
    "commented_code": "}\n \n fn get_python_version() -> Result<(u8, u8), std::io::Error> {\n-    let output = Command::new(\"python3\")\n+    // linux/osx have python3, but windows only has python. Use python since it is in all of them\n+    let python_exe = if cfg!(target_os = \"windows\") {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1955255363",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25969,
        "pr_file": "influxdb3_processing_engine/src/virtualenv.rs",
        "discussion_id": "1955255363",
        "commented_code": "@@ -17,7 +17,14 @@ pub enum VenvError {\n }\n \n fn get_python_version() -> Result<(u8, u8), std::io::Error> {\n-    let output = Command::new(\"python3\")\n+    // linux/osx have python3, but windows only has python. Use python since it is in all of them\n+    let python_exe = if cfg!(target_os = \"windows\") {",
        "comment_created_at": "2025-02-13T21:44:35+00:00",
        "comment_author": "jacksonrnewhouse",
        "comment_body": "What about factoring this out into a constant?",
        "pr_file_module": null
      },
      {
        "comment_id": "1955269424",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25969,
        "pr_file": "influxdb3_processing_engine/src/virtualenv.rs",
        "discussion_id": "1955255363",
        "commented_code": "@@ -17,7 +17,14 @@ pub enum VenvError {\n }\n \n fn get_python_version() -> Result<(u8, u8), std::io::Error> {\n-    let output = Command::new(\"python3\")\n+    // linux/osx have python3, but windows only has python. Use python since it is in all of them\n+    let python_exe = if cfg!(target_os = \"windows\") {",
        "comment_created_at": "2025-02-13T21:58:47+00:00",
        "comment_author": "jdstrand",
        "comment_body": "I'm fine with that but considering more needs to happen here (https://github.com/influxdata/influxdb/issues/26012, currently assigned to you), I wonder if that can be in a follow-up PR?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1901388502",
    "pr_number": 25727,
    "pr_file": "influxdb3_wal/src/lib.rs",
    "created_at": "2025-01-03T00:21:15+00:00",
    "commented_code": "/// Returns the last persisted wal file sequence number\n     async fn last_snapshot_sequence_number(&self) -> SnapshotSequenceNumber;\n \n+    /// Returns the snapshot info, if force snapshot is set it avoids checking\n+    /// certain cases and returns snapshot info leaving only the last wal period\n+    async fn snapshot_info(\n+        &self,\n+        force_snapshot: bool,\n+    ) -> Option<(SnapshotInfo, OwnedSemaphorePermit)>;\n+\n     /// Stop all writes to the WAL and flush the buffer to a WAL file.\n     async fn shutdown(&self);\n+\n+    async fn flush_buffer_and_cleanup_snapshot(self: Arc<Self>) {\n+        let cleanup_after_snapshot = self.flush_buffer().await;\n+        self.cleanup_after_snapshot(cleanup_after_snapshot).await;",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1901388502",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25727,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1901388502",
        "commented_code": "@@ -97,8 +97,44 @@ pub trait Wal: Debug + Send + Sync + 'static {\n     /// Returns the last persisted wal file sequence number\n     async fn last_snapshot_sequence_number(&self) -> SnapshotSequenceNumber;\n \n+    /// Returns the snapshot info, if force snapshot is set it avoids checking\n+    /// certain cases and returns snapshot info leaving only the last wal period\n+    async fn snapshot_info(\n+        &self,\n+        force_snapshot: bool,\n+    ) -> Option<(SnapshotInfo, OwnedSemaphorePermit)>;\n+\n     /// Stop all writes to the WAL and flush the buffer to a WAL file.\n     async fn shutdown(&self);\n+\n+    async fn flush_buffer_and_cleanup_snapshot(self: Arc<Self>) {\n+        let cleanup_after_snapshot = self.flush_buffer().await;\n+        self.cleanup_after_snapshot(cleanup_after_snapshot).await;",
        "comment_created_at": "2025-01-03T00:21:15+00:00",
        "comment_author": "pauldix",
        "comment_body": "I think this would be clearer if the `cleanup_after_snapshot` method took the separate arguments (i.e. not an Option of a tuple). That way you can put the check up here. Meaning, we show that what we're doing here is flushing the buffer. And if, the return from that is `Some(...)` then it means we have a snapshot to cleanup after.\r\n\r\nSo we call the `cleanup_after_snapshot` function with this signature: `self: Arc<Self>, snapshot_finished_reciever: oneshot::Receiver<SnapshotDetails>, snapshot_info: SnapshotInfo, snapshot_permit: OwnedSemaphorePermit`.\r\n\r\nThe separation and the argument names will make it a little more clear what's going on here.",
        "pr_file_module": null
      },
      {
        "comment_id": "1901737376",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25727,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1901388502",
        "commented_code": "@@ -97,8 +97,44 @@ pub trait Wal: Debug + Send + Sync + 'static {\n     /// Returns the last persisted wal file sequence number\n     async fn last_snapshot_sequence_number(&self) -> SnapshotSequenceNumber;\n \n+    /// Returns the snapshot info, if force snapshot is set it avoids checking\n+    /// certain cases and returns snapshot info leaving only the last wal period\n+    async fn snapshot_info(\n+        &self,\n+        force_snapshot: bool,\n+    ) -> Option<(SnapshotInfo, OwnedSemaphorePermit)>;\n+\n     /// Stop all writes to the WAL and flush the buffer to a WAL file.\n     async fn shutdown(&self);\n+\n+    async fn flush_buffer_and_cleanup_snapshot(self: Arc<Self>) {\n+        let cleanup_after_snapshot = self.flush_buffer().await;\n+        self.cleanup_after_snapshot(cleanup_after_snapshot).await;",
        "comment_created_at": "2025-01-03T12:23:21+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "Yes - good point, I'll follow that up. I was incrementally moving code around without breaking the tests, I can take it further and make it a clearer boundary between flushing WAL buffer and running a snapshot.",
        "pr_file_module": null
      },
      {
        "comment_id": "1901779051",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25727,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1901388502",
        "commented_code": "@@ -97,8 +97,44 @@ pub trait Wal: Debug + Send + Sync + 'static {\n     /// Returns the last persisted wal file sequence number\n     async fn last_snapshot_sequence_number(&self) -> SnapshotSequenceNumber;\n \n+    /// Returns the snapshot info, if force snapshot is set it avoids checking\n+    /// certain cases and returns snapshot info leaving only the last wal period\n+    async fn snapshot_info(\n+        &self,\n+        force_snapshot: bool,\n+    ) -> Option<(SnapshotInfo, OwnedSemaphorePermit)>;\n+\n     /// Stop all writes to the WAL and flush the buffer to a WAL file.\n     async fn shutdown(&self);\n+\n+    async fn flush_buffer_and_cleanup_snapshot(self: Arc<Self>) {\n+        let cleanup_after_snapshot = self.flush_buffer().await;\n+        self.cleanup_after_snapshot(cleanup_after_snapshot).await;",
        "comment_created_at": "2025-01-03T13:19:53+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "Addressed it in [6619e9e](https://github.com/influxdata/influxdb/pull/25727/commits/6619e9eccea9bd7f794864a2a4284a4705b2ef25)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1904438513",
    "pr_number": 25704,
    "pr_file": "influxdb3_write/src/write_buffer/plugins.rs",
    "created_at": "2025-01-06T17:30:12+00:00",
    "commented_code": "}\n     }\n }\n+\n+#[cfg(feature = \"system-py\")]\n+pub(crate) fn run_test_wal_plugin(\n+    now_time: iox_time::Time,\n+    catalog: Arc<influxdb3_catalog::catalog::Catalog>,\n+    code: String,\n+    request: WalPluginTestRequest,\n+) -> Result<WalPluginTestResponse, Error> {\n+    use crate::write_buffer::validator::WriteValidator;\n+    use crate::Precision;\n+    use data_types::NamespaceName;\n+    use influxdb3_wal::Gen1Duration;\n+\n+    // parse the lp into a write batch\n+    let namespace = NamespaceName::new(\"_testdb\").unwrap();",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1904438513",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25704,
        "pr_file": "influxdb3_write/src/write_buffer/plugins.rs",
        "discussion_id": "1904438513",
        "commented_code": "@@ -216,3 +225,288 @@ mod python_plugin {\n         }\n     }\n }\n+\n+#[cfg(feature = \"system-py\")]\n+pub(crate) fn run_test_wal_plugin(\n+    now_time: iox_time::Time,\n+    catalog: Arc<influxdb3_catalog::catalog::Catalog>,\n+    code: String,\n+    request: WalPluginTestRequest,\n+) -> Result<WalPluginTestResponse, Error> {\n+    use crate::write_buffer::validator::WriteValidator;\n+    use crate::Precision;\n+    use data_types::NamespaceName;\n+    use influxdb3_wal::Gen1Duration;\n+\n+    // parse the lp into a write batch\n+    let namespace = NamespaceName::new(\"_testdb\").unwrap();",
        "comment_created_at": "2025-01-06T17:30:12+00:00",
        "comment_author": "jacksonrnewhouse",
        "comment_body": "Let's factor \"_testdb\" out into a static variable.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1889292833",
    "pr_number": 25672,
    "pr_file": "influxdb3_write/src/write_buffer/mod.rs",
    "created_at": "2024-12-17T22:09:00+00:00",
    "commented_code": "last_cache,\n             persisted_files,\n             buffer: queryable_buffer,\n-        })\n+        });\n+        let write_buffer: Arc<dyn WriteBuffer> = result.clone();\n+        let triggers = result.catalog().inner().read().triggers();",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1889292833",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25672,
        "pr_file": "influxdb3_write/src/write_buffer/mod.rs",
        "discussion_id": "1889292833",
        "commented_code": "@@ -222,7 +231,15 @@ impl WriteBufferImpl {\n             last_cache,\n             persisted_files,\n             buffer: queryable_buffer,\n-        })\n+        });\n+        let write_buffer: Arc<dyn WriteBuffer> = result.clone();\n+        let triggers = result.catalog().inner().read().triggers();",
        "comment_created_at": "2024-12-17T22:09:00+00:00",
        "comment_author": "pauldix",
        "comment_body": "Better to make this a function on `Catalog` rather than going into its internals to get it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1853770590",
    "pr_number": 25566,
    "pr_file": "influxdb3_cache/src/meta_cache/table_function.rs",
    "created_at": "2024-11-22T11:30:42+00:00",
    "commented_code": "+use std::{any::Any, sync::Arc};\n+\n+use arrow::{array::RecordBatch, datatypes::SchemaRef};\n+use async_trait::async_trait;\n+use datafusion::{\n+    catalog::{Session, TableProvider},\n+    common::{internal_err, plan_err, DFSchema, Result},\n+    datasource::{function::TableFunctionImpl, TableType},\n+    execution::context::ExecutionProps,\n+    logical_expr::TableProviderFilterPushDown,\n+    physical_expr::{\n+        create_physical_expr,\n+        utils::{Guarantee, LiteralGuarantee},\n+    },\n+    physical_plan::{memory::MemoryExec, DisplayAs, DisplayFormatType, ExecutionPlan},\n+    prelude::Expr,\n+    scalar::ScalarValue,\n+};\n+use indexmap::IndexMap;\n+use influxdb3_catalog::catalog::TableDefinition;\n+use influxdb3_id::{ColumnId, DbId};\n+\n+use super::{cache::Predicate, MetaCacheProvider};\n+\n+/// The name used to call the metadata cache in SQL queries\n+pub const META_CACHE_UDTF_NAME: &str = \"meta_cache\";\n+\n+/// Implementor of the [`TableProvider`] trait that is produced a call to the [`MetaCacheFunction`]\n+struct MetaCacheFunctionProvider {\n+    /// Reference to the [`MetaCache`][super::cache::MetaCache] being queried's schema\n+    schema: SchemaRef,\n+    /// Forwarded ref to the [`MetaCacheProvider`] which is used to get the\n+    /// [`MetaCache`][super::cache::MetaCache] for the query, along with the `db_id` and\n+    /// `table_def`. This is done instead of passing forward a reference to the `MetaCache`\n+    /// directly because doing so is not easy or possible with the Rust borrow checker.\n+    provider: Arc<MetaCacheProvider>,\n+    /// The database ID that the called cache is related to\n+    db_id: DbId,\n+    /// The table definition that the called cache is related to\n+    table_def: Arc<TableDefinition>,\n+    /// The name of the cache, which is determined when calling the `meta_cache` function\n+    cache_name: Arc<str>,\n+}\n+\n+#[async_trait]\n+impl TableProvider for MetaCacheFunctionProvider {\n+    fn as_any(&self) -> &dyn Any {\n+        self as &dyn Any\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        Arc::clone(&self.schema)\n+    }\n+\n+    fn table_type(&self) -> TableType {\n+        TableType::Temporary\n+    }\n+\n+    fn supports_filters_pushdown(\n+        &self,\n+        filters: &[&Expr],\n+    ) -> Result<Vec<TableProviderFilterPushDown>> {\n+        Ok(vec![TableProviderFilterPushDown::Inexact; filters.len()])\n+    }\n+\n+    async fn scan(\n+        &self,\n+        ctx: &dyn Session,\n+        projection: Option<&Vec<usize>>,\n+        filters: &[Expr],\n+        _limit: Option<usize>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        let read = self.provider.cache_map.read();\n+        let (batches, predicates) = if let Some(cache) = read\n+            .get(&self.db_id)\n+            .and_then(|db| db.get(&self.table_def.table_id))\n+            .and_then(|tbl| tbl.get(&self.cache_name))\n+        {\n+            let predicates = convert_filter_exprs(&self.table_def, self.schema(), filters)?;\n+            (\n+                cache\n+                    .to_record_batch(&predicates)\n+                    .map(|batch| vec![batch])?,\n+                (!predicates.is_empty()).then_some(predicates),\n+            )\n+        } else {\n+            (vec![], None)\n+        };\n+        let mut exec =\n+            MetaCacheExec::try_new(predicates, &[batches], self.schema(), projection.cloned())?;\n+\n+        let show_sizes = ctx.config_options().explain.show_sizes;\n+        exec = exec.with_show_sizes(show_sizes);\n+\n+        Ok(Arc::new(exec))\n+    }\n+}\n+\n+/// Convert the given list of filter expressions to a map of [`ColumnId`] to [`Predicate`]\n+///\n+/// The resulting map uses [`IndexMap`] to ensure consistent ordering of the map. This makes testing\n+/// the filter conversion significantly easier using EXPLAIN queries.\n+fn convert_filter_exprs(\n+    table_def: &TableDefinition,\n+    cache_schema: SchemaRef,\n+    filters: &[Expr],\n+) -> Result<IndexMap<ColumnId, Predicate>> {\n+    let mut predicate_map: IndexMap<ColumnId, Option<Predicate>> = IndexMap::new();\n+\n+    // for create_physical_expr:\n+    let schema: DFSchema = cache_schema.try_into()?;\n+    let props = ExecutionProps::new();\n+\n+    // The set of `filters` that are passed in from DataFusion varies: 1) based on how they are\n+    // defined in the query, and 2) based on some decisions that DataFusion makes when parsing the\n+    // query into the `Expr` syntax tree. For example, the predicate:\n+    //\n+    // WHERE foo IN ('bar', 'baz')\n+    //\n+    // instead of being expressed as an `InList`, would be simplified to the following `Expr` tree:\n+    //\n+    // [\n+    //     BinaryExpr {\n+    //         left: BinaryExpr { left: \"foo\", op: Eq, right: \"bar\" },\n+    //         op: Or,\n+    //         right: BinaryExpr { left: \"foo\", op: Eq, right: \"baz\" }\n+    //     }\n+    // ]\n+    //\n+    // while the predicate:\n+    //\n+    // WHERE foo = 'bar' OR foo = 'baz' OR foo = 'bop' OR foo = 'bla'\n+    //\n+    // instead of being expressed as a tree of `BinaryExpr`s, is expressed as an `InList` with four\n+    // entries:\n+    //\n+    // [\n+    //     InList { col: \"foo\", values: [\"bar\", \"baz\", \"bop\", \"bla\"], negated: false }\n+    // ]\n+    //\n+    // Instead of handling all the combinations of `Expr`s that may be passed by the caller of\n+    // `TableProider::scan`, we can use the cache's schema to convert each `Expr` to a `PhysicalExpr`\n+    // and analyze it using DataFusion's `LiteralGuarantee`.\n+    //\n+    // This will distill the provided set of `Expr`s down to either an IN list, or a NOT IN list\n+    // which we can convert to the `Predicate` type for the metadata cache.\n+    //\n+    // The main caveat is that if for some reason there are multiple `Expr`s that apply predicates\n+    // on a given column, i.e., leading to multiple `LiteralGuarantee`s on a specific column, we\n+    // discard those predicates and have DataFusion handle the filtering.\n+    //\n+    // This is a conservative approach; it may be that we can combine multiple literal guarantees on\n+    // a single column, but thusfar, from testing in the parent module, this does not seem necessary.\n+\n+    for expr in filters {\n+        let physical_expr = create_physical_expr(expr, &schema, &props)?;\n+        let literal_guarantees = LiteralGuarantee::analyze(&physical_expr);\n+        for LiteralGuarantee {\n+            column,\n+            guarantee,\n+            literals,\n+        } in literal_guarantees\n+        {\n+            let Some(column_id) = table_def.column_name_to_id(column.name()) else {\n+                return plan_err!(\n+                    \"invalid column name in filter expression: {}\",\n+                    column.name()\n+                );\n+            };\n+            let value_iter = literals.into_iter().filter_map(|l| match l {\n+                ScalarValue::Utf8(Some(s)) | ScalarValue::Utf8View(Some(s)) => Some(s),\n+                _ => None,\n+            });\n+\n+            let predicate = match guarantee {\n+                Guarantee::In => Predicate::new_in(value_iter),\n+                Guarantee::NotIn => Predicate::new_not_in(value_iter),\n+            };\n+            predicate_map\n+                .entry(column_id)\n+                .and_modify(|e| {\n+                    // We do not currently support multiple literal guarantees per column.\n+                    //\n+                    // In this case we replace the predicate with None so that it does not filter\n+                    // any records from the cache downstream. Datafusion will still do filtering at\n+                    // a higher level, once _all_ records are produced from the cache.\n+                    e.take();\n+                })\n+                .or_insert_with(|| Some(predicate));\n+        }\n+    }\n+\n+    Ok(predicate_map\n+        .into_iter()\n+        .filter_map(|(column_id, predicate)| predicate.map(|predicate| (column_id, predicate)))\n+        .collect())\n+}\n+\n+/// Implementor of the [`TableFunctionImpl`] trait, to be registered as a user-defined table function\n+/// in the Datafusion `SessionContext`.\n+#[derive(Debug)]\n+pub struct MetaCacheFunction {\n+    db_id: DbId,\n+    provider: Arc<MetaCacheProvider>,\n+}\n+\n+impl MetaCacheFunction {\n+    pub fn new(db_id: DbId, provider: Arc<MetaCacheProvider>) -> Self {\n+        Self { db_id, provider }\n+    }\n+}\n+\n+impl TableFunctionImpl for MetaCacheFunction {\n+    fn call(&self, args: &[Expr]) -> Result<Arc<dyn TableProvider>> {\n+        let Some(Expr::Literal(ScalarValue::Utf8(Some(table_name)))) = args.first() else {\n+            return plan_err!(\"first argument must be the table name as a string\");\n+        };\n+        let cache_name = match args.get(1) {\n+            Some(Expr::Literal(ScalarValue::Utf8(Some(name)))) => Some(name),\n+            Some(_) => {\n+                return plan_err!(\"second argument, if passed, must be the cache name as a string\")\n+            }\n+            None => None,\n+        };\n+\n+        let Some(table_def) = self\n+            .provider\n+            .catalog\n+            .db_schema_by_id(&self.db_id)\n+            .and_then(|db| db.table_definition(table_name.as_str()))\n+        else {\n+            return plan_err!(\"provided table name ({}) is invalid\", table_name);\n+        };\n+        let Some((cache_name, schema)) = self.provider.get_cache_name_and_schema(\n+            self.db_id,\n+            table_def.table_id,\n+            cache_name.map(|n| n.as_str()),\n+        ) else {\n+            return plan_err!(\"could not find meta cache for the given arguments\");\n+        };\n+        Ok(Arc::new(MetaCacheFunctionProvider {\n+            schema,\n+            provider: Arc::clone(&self.provider),\n+            db_id: self.db_id,\n+            table_def,\n+            cache_name,\n+        }))\n+    }\n+}\n+\n+/// Custom implementor of the [`ExecutionPlan`] trait for use by the metadata cache\n+///\n+/// Wraps a [`MemoryExec`] from DataFusion, and mostly re-uses that. The special functionality\n+/// provided by this type is to track the predicates that are pushed down to the underlying cache\n+/// during query planning/execution.\n+///\n+/// # Example\n+///\n+/// For a query that does not provide any predicates, or one that does provide predicates, but they\n+/// do no get pushed down, the `EXPLAIN` for said query will contain a line for the `MetaCacheExec`\n+/// with no predicates, including what is emitted by the inner `MemoryExec`:\n+///\n+/// ```text\n+/// MetaCacheExec: inner=MemoryExec: partitions=1, partition_sizes=[1]\n+/// ```\n+///\n+/// For queries that do have predicates that get pushed down, the output will include them, e.g.:\n+///\n+/// ```text\n+/// MetaCacheExec: predicates=[[0 IN (us-east)], [1 IN (a,b)]] inner=MemoryExec: partitions=1, partition_sizes=[1]\n+/// ```\n+#[derive(Debug)]\n+struct MetaCacheExec {\n+    inner: MemoryExec,\n+    predicates: Option<IndexMap<ColumnId, Predicate>>,\n+}\n+\n+impl MetaCacheExec {\n+    fn try_new(\n+        predicates: Option<IndexMap<ColumnId, Predicate>>,\n+        partitions: &[Vec<RecordBatch>],\n+        schema: SchemaRef,\n+        projection: Option<Vec<usize>>,\n+    ) -> Result<Self> {\n+        Ok(Self {\n+            inner: MemoryExec::try_new(partitions, schema, projection)?,\n+            predicates,\n+        })\n+    }\n+\n+    fn with_show_sizes(self, show_sizes: bool) -> Self {\n+        Self {\n+            inner: self.inner.with_show_sizes(show_sizes),\n+            ..self\n+        }\n+    }\n+}\n+\n+impl DisplayAs for MetaCacheExec {\n+    fn fmt_as(&self, t: DisplayFormatType, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match t {\n+            DisplayFormatType::Default | DisplayFormatType::Verbose => {\n+                write!(f, \"MetaCacheExec:\")?;\n+                if let Some(predicates) = self.predicates.as_ref() {\n+                    write!(f, \" predicates=[\")?;\n+                    let mut p_iter = predicates.iter();\n+                    while let Some((col_id, predicate)) = p_iter.next() {\n+                        write!(f, \"[{col_id} {predicate}]\")?;",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1853770590",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25566,
        "pr_file": "influxdb3_cache/src/meta_cache/table_function.rs",
        "discussion_id": "1853770590",
        "commented_code": "@@ -0,0 +1,359 @@\n+use std::{any::Any, sync::Arc};\n+\n+use arrow::{array::RecordBatch, datatypes::SchemaRef};\n+use async_trait::async_trait;\n+use datafusion::{\n+    catalog::{Session, TableProvider},\n+    common::{internal_err, plan_err, DFSchema, Result},\n+    datasource::{function::TableFunctionImpl, TableType},\n+    execution::context::ExecutionProps,\n+    logical_expr::TableProviderFilterPushDown,\n+    physical_expr::{\n+        create_physical_expr,\n+        utils::{Guarantee, LiteralGuarantee},\n+    },\n+    physical_plan::{memory::MemoryExec, DisplayAs, DisplayFormatType, ExecutionPlan},\n+    prelude::Expr,\n+    scalar::ScalarValue,\n+};\n+use indexmap::IndexMap;\n+use influxdb3_catalog::catalog::TableDefinition;\n+use influxdb3_id::{ColumnId, DbId};\n+\n+use super::{cache::Predicate, MetaCacheProvider};\n+\n+/// The name used to call the metadata cache in SQL queries\n+pub const META_CACHE_UDTF_NAME: &str = \"meta_cache\";\n+\n+/// Implementor of the [`TableProvider`] trait that is produced a call to the [`MetaCacheFunction`]\n+struct MetaCacheFunctionProvider {\n+    /// Reference to the [`MetaCache`][super::cache::MetaCache] being queried's schema\n+    schema: SchemaRef,\n+    /// Forwarded ref to the [`MetaCacheProvider`] which is used to get the\n+    /// [`MetaCache`][super::cache::MetaCache] for the query, along with the `db_id` and\n+    /// `table_def`. This is done instead of passing forward a reference to the `MetaCache`\n+    /// directly because doing so is not easy or possible with the Rust borrow checker.\n+    provider: Arc<MetaCacheProvider>,\n+    /// The database ID that the called cache is related to\n+    db_id: DbId,\n+    /// The table definition that the called cache is related to\n+    table_def: Arc<TableDefinition>,\n+    /// The name of the cache, which is determined when calling the `meta_cache` function\n+    cache_name: Arc<str>,\n+}\n+\n+#[async_trait]\n+impl TableProvider for MetaCacheFunctionProvider {\n+    fn as_any(&self) -> &dyn Any {\n+        self as &dyn Any\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        Arc::clone(&self.schema)\n+    }\n+\n+    fn table_type(&self) -> TableType {\n+        TableType::Temporary\n+    }\n+\n+    fn supports_filters_pushdown(\n+        &self,\n+        filters: &[&Expr],\n+    ) -> Result<Vec<TableProviderFilterPushDown>> {\n+        Ok(vec![TableProviderFilterPushDown::Inexact; filters.len()])\n+    }\n+\n+    async fn scan(\n+        &self,\n+        ctx: &dyn Session,\n+        projection: Option<&Vec<usize>>,\n+        filters: &[Expr],\n+        _limit: Option<usize>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        let read = self.provider.cache_map.read();\n+        let (batches, predicates) = if let Some(cache) = read\n+            .get(&self.db_id)\n+            .and_then(|db| db.get(&self.table_def.table_id))\n+            .and_then(|tbl| tbl.get(&self.cache_name))\n+        {\n+            let predicates = convert_filter_exprs(&self.table_def, self.schema(), filters)?;\n+            (\n+                cache\n+                    .to_record_batch(&predicates)\n+                    .map(|batch| vec![batch])?,\n+                (!predicates.is_empty()).then_some(predicates),\n+            )\n+        } else {\n+            (vec![], None)\n+        };\n+        let mut exec =\n+            MetaCacheExec::try_new(predicates, &[batches], self.schema(), projection.cloned())?;\n+\n+        let show_sizes = ctx.config_options().explain.show_sizes;\n+        exec = exec.with_show_sizes(show_sizes);\n+\n+        Ok(Arc::new(exec))\n+    }\n+}\n+\n+/// Convert the given list of filter expressions to a map of [`ColumnId`] to [`Predicate`]\n+///\n+/// The resulting map uses [`IndexMap`] to ensure consistent ordering of the map. This makes testing\n+/// the filter conversion significantly easier using EXPLAIN queries.\n+fn convert_filter_exprs(\n+    table_def: &TableDefinition,\n+    cache_schema: SchemaRef,\n+    filters: &[Expr],\n+) -> Result<IndexMap<ColumnId, Predicate>> {\n+    let mut predicate_map: IndexMap<ColumnId, Option<Predicate>> = IndexMap::new();\n+\n+    // for create_physical_expr:\n+    let schema: DFSchema = cache_schema.try_into()?;\n+    let props = ExecutionProps::new();\n+\n+    // The set of `filters` that are passed in from DataFusion varies: 1) based on how they are\n+    // defined in the query, and 2) based on some decisions that DataFusion makes when parsing the\n+    // query into the `Expr` syntax tree. For example, the predicate:\n+    //\n+    // WHERE foo IN ('bar', 'baz')\n+    //\n+    // instead of being expressed as an `InList`, would be simplified to the following `Expr` tree:\n+    //\n+    // [\n+    //     BinaryExpr {\n+    //         left: BinaryExpr { left: \"foo\", op: Eq, right: \"bar\" },\n+    //         op: Or,\n+    //         right: BinaryExpr { left: \"foo\", op: Eq, right: \"baz\" }\n+    //     }\n+    // ]\n+    //\n+    // while the predicate:\n+    //\n+    // WHERE foo = 'bar' OR foo = 'baz' OR foo = 'bop' OR foo = 'bla'\n+    //\n+    // instead of being expressed as a tree of `BinaryExpr`s, is expressed as an `InList` with four\n+    // entries:\n+    //\n+    // [\n+    //     InList { col: \"foo\", values: [\"bar\", \"baz\", \"bop\", \"bla\"], negated: false }\n+    // ]\n+    //\n+    // Instead of handling all the combinations of `Expr`s that may be passed by the caller of\n+    // `TableProider::scan`, we can use the cache's schema to convert each `Expr` to a `PhysicalExpr`\n+    // and analyze it using DataFusion's `LiteralGuarantee`.\n+    //\n+    // This will distill the provided set of `Expr`s down to either an IN list, or a NOT IN list\n+    // which we can convert to the `Predicate` type for the metadata cache.\n+    //\n+    // The main caveat is that if for some reason there are multiple `Expr`s that apply predicates\n+    // on a given column, i.e., leading to multiple `LiteralGuarantee`s on a specific column, we\n+    // discard those predicates and have DataFusion handle the filtering.\n+    //\n+    // This is a conservative approach; it may be that we can combine multiple literal guarantees on\n+    // a single column, but thusfar, from testing in the parent module, this does not seem necessary.\n+\n+    for expr in filters {\n+        let physical_expr = create_physical_expr(expr, &schema, &props)?;\n+        let literal_guarantees = LiteralGuarantee::analyze(&physical_expr);\n+        for LiteralGuarantee {\n+            column,\n+            guarantee,\n+            literals,\n+        } in literal_guarantees\n+        {\n+            let Some(column_id) = table_def.column_name_to_id(column.name()) else {\n+                return plan_err!(\n+                    \"invalid column name in filter expression: {}\",\n+                    column.name()\n+                );\n+            };\n+            let value_iter = literals.into_iter().filter_map(|l| match l {\n+                ScalarValue::Utf8(Some(s)) | ScalarValue::Utf8View(Some(s)) => Some(s),\n+                _ => None,\n+            });\n+\n+            let predicate = match guarantee {\n+                Guarantee::In => Predicate::new_in(value_iter),\n+                Guarantee::NotIn => Predicate::new_not_in(value_iter),\n+            };\n+            predicate_map\n+                .entry(column_id)\n+                .and_modify(|e| {\n+                    // We do not currently support multiple literal guarantees per column.\n+                    //\n+                    // In this case we replace the predicate with None so that it does not filter\n+                    // any records from the cache downstream. Datafusion will still do filtering at\n+                    // a higher level, once _all_ records are produced from the cache.\n+                    e.take();\n+                })\n+                .or_insert_with(|| Some(predicate));\n+        }\n+    }\n+\n+    Ok(predicate_map\n+        .into_iter()\n+        .filter_map(|(column_id, predicate)| predicate.map(|predicate| (column_id, predicate)))\n+        .collect())\n+}\n+\n+/// Implementor of the [`TableFunctionImpl`] trait, to be registered as a user-defined table function\n+/// in the Datafusion `SessionContext`.\n+#[derive(Debug)]\n+pub struct MetaCacheFunction {\n+    db_id: DbId,\n+    provider: Arc<MetaCacheProvider>,\n+}\n+\n+impl MetaCacheFunction {\n+    pub fn new(db_id: DbId, provider: Arc<MetaCacheProvider>) -> Self {\n+        Self { db_id, provider }\n+    }\n+}\n+\n+impl TableFunctionImpl for MetaCacheFunction {\n+    fn call(&self, args: &[Expr]) -> Result<Arc<dyn TableProvider>> {\n+        let Some(Expr::Literal(ScalarValue::Utf8(Some(table_name)))) = args.first() else {\n+            return plan_err!(\"first argument must be the table name as a string\");\n+        };\n+        let cache_name = match args.get(1) {\n+            Some(Expr::Literal(ScalarValue::Utf8(Some(name)))) => Some(name),\n+            Some(_) => {\n+                return plan_err!(\"second argument, if passed, must be the cache name as a string\")\n+            }\n+            None => None,\n+        };\n+\n+        let Some(table_def) = self\n+            .provider\n+            .catalog\n+            .db_schema_by_id(&self.db_id)\n+            .and_then(|db| db.table_definition(table_name.as_str()))\n+        else {\n+            return plan_err!(\"provided table name ({}) is invalid\", table_name);\n+        };\n+        let Some((cache_name, schema)) = self.provider.get_cache_name_and_schema(\n+            self.db_id,\n+            table_def.table_id,\n+            cache_name.map(|n| n.as_str()),\n+        ) else {\n+            return plan_err!(\"could not find meta cache for the given arguments\");\n+        };\n+        Ok(Arc::new(MetaCacheFunctionProvider {\n+            schema,\n+            provider: Arc::clone(&self.provider),\n+            db_id: self.db_id,\n+            table_def,\n+            cache_name,\n+        }))\n+    }\n+}\n+\n+/// Custom implementor of the [`ExecutionPlan`] trait for use by the metadata cache\n+///\n+/// Wraps a [`MemoryExec`] from DataFusion, and mostly re-uses that. The special functionality\n+/// provided by this type is to track the predicates that are pushed down to the underlying cache\n+/// during query planning/execution.\n+///\n+/// # Example\n+///\n+/// For a query that does not provide any predicates, or one that does provide predicates, but they\n+/// do no get pushed down, the `EXPLAIN` for said query will contain a line for the `MetaCacheExec`\n+/// with no predicates, including what is emitted by the inner `MemoryExec`:\n+///\n+/// ```text\n+/// MetaCacheExec: inner=MemoryExec: partitions=1, partition_sizes=[1]\n+/// ```\n+///\n+/// For queries that do have predicates that get pushed down, the output will include them, e.g.:\n+///\n+/// ```text\n+/// MetaCacheExec: predicates=[[0 IN (us-east)], [1 IN (a,b)]] inner=MemoryExec: partitions=1, partition_sizes=[1]\n+/// ```\n+#[derive(Debug)]\n+struct MetaCacheExec {\n+    inner: MemoryExec,\n+    predicates: Option<IndexMap<ColumnId, Predicate>>,\n+}\n+\n+impl MetaCacheExec {\n+    fn try_new(\n+        predicates: Option<IndexMap<ColumnId, Predicate>>,\n+        partitions: &[Vec<RecordBatch>],\n+        schema: SchemaRef,\n+        projection: Option<Vec<usize>>,\n+    ) -> Result<Self> {\n+        Ok(Self {\n+            inner: MemoryExec::try_new(partitions, schema, projection)?,\n+            predicates,\n+        })\n+    }\n+\n+    fn with_show_sizes(self, show_sizes: bool) -> Self {\n+        Self {\n+            inner: self.inner.with_show_sizes(show_sizes),\n+            ..self\n+        }\n+    }\n+}\n+\n+impl DisplayAs for MetaCacheExec {\n+    fn fmt_as(&self, t: DisplayFormatType, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match t {\n+            DisplayFormatType::Default | DisplayFormatType::Verbose => {\n+                write!(f, \"MetaCacheExec:\")?;\n+                if let Some(predicates) = self.predicates.as_ref() {\n+                    write!(f, \" predicates=[\")?;\n+                    let mut p_iter = predicates.iter();\n+                    while let Some((col_id, predicate)) = p_iter.next() {\n+                        write!(f, \"[{col_id} {predicate}]\")?;",
        "comment_created_at": "2024-11-22T11:30:42+00:00",
        "comment_author": "praveen-influx",
        "comment_body": "I like we have a custom display to plug into `explain` output. Maybe we could swap column name for id here? \r\n\r\n```\r\npredicates=[[0 IN (us-east)], [1 IN (a,b)]]\r\n```\r\nprobably reads better (imo) when its\r\n\r\n```\r\npredicates=[[region IN (us-east)], [host IN (a,b)]]\r\n```\r\n\r\nJust an UX thing, when there's many columns involved this might be a little difficult to understand. If `col_id` is useful in `explain`, then we could _add_ that info here like `region$0` or some variation of it?",
        "pr_file_module": null
      },
      {
        "comment_id": "1854067622",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25566,
        "pr_file": "influxdb3_cache/src/meta_cache/table_function.rs",
        "discussion_id": "1853770590",
        "commented_code": "@@ -0,0 +1,359 @@\n+use std::{any::Any, sync::Arc};\n+\n+use arrow::{array::RecordBatch, datatypes::SchemaRef};\n+use async_trait::async_trait;\n+use datafusion::{\n+    catalog::{Session, TableProvider},\n+    common::{internal_err, plan_err, DFSchema, Result},\n+    datasource::{function::TableFunctionImpl, TableType},\n+    execution::context::ExecutionProps,\n+    logical_expr::TableProviderFilterPushDown,\n+    physical_expr::{\n+        create_physical_expr,\n+        utils::{Guarantee, LiteralGuarantee},\n+    },\n+    physical_plan::{memory::MemoryExec, DisplayAs, DisplayFormatType, ExecutionPlan},\n+    prelude::Expr,\n+    scalar::ScalarValue,\n+};\n+use indexmap::IndexMap;\n+use influxdb3_catalog::catalog::TableDefinition;\n+use influxdb3_id::{ColumnId, DbId};\n+\n+use super::{cache::Predicate, MetaCacheProvider};\n+\n+/// The name used to call the metadata cache in SQL queries\n+pub const META_CACHE_UDTF_NAME: &str = \"meta_cache\";\n+\n+/// Implementor of the [`TableProvider`] trait that is produced a call to the [`MetaCacheFunction`]\n+struct MetaCacheFunctionProvider {\n+    /// Reference to the [`MetaCache`][super::cache::MetaCache] being queried's schema\n+    schema: SchemaRef,\n+    /// Forwarded ref to the [`MetaCacheProvider`] which is used to get the\n+    /// [`MetaCache`][super::cache::MetaCache] for the query, along with the `db_id` and\n+    /// `table_def`. This is done instead of passing forward a reference to the `MetaCache`\n+    /// directly because doing so is not easy or possible with the Rust borrow checker.\n+    provider: Arc<MetaCacheProvider>,\n+    /// The database ID that the called cache is related to\n+    db_id: DbId,\n+    /// The table definition that the called cache is related to\n+    table_def: Arc<TableDefinition>,\n+    /// The name of the cache, which is determined when calling the `meta_cache` function\n+    cache_name: Arc<str>,\n+}\n+\n+#[async_trait]\n+impl TableProvider for MetaCacheFunctionProvider {\n+    fn as_any(&self) -> &dyn Any {\n+        self as &dyn Any\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        Arc::clone(&self.schema)\n+    }\n+\n+    fn table_type(&self) -> TableType {\n+        TableType::Temporary\n+    }\n+\n+    fn supports_filters_pushdown(\n+        &self,\n+        filters: &[&Expr],\n+    ) -> Result<Vec<TableProviderFilterPushDown>> {\n+        Ok(vec![TableProviderFilterPushDown::Inexact; filters.len()])\n+    }\n+\n+    async fn scan(\n+        &self,\n+        ctx: &dyn Session,\n+        projection: Option<&Vec<usize>>,\n+        filters: &[Expr],\n+        _limit: Option<usize>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        let read = self.provider.cache_map.read();\n+        let (batches, predicates) = if let Some(cache) = read\n+            .get(&self.db_id)\n+            .and_then(|db| db.get(&self.table_def.table_id))\n+            .and_then(|tbl| tbl.get(&self.cache_name))\n+        {\n+            let predicates = convert_filter_exprs(&self.table_def, self.schema(), filters)?;\n+            (\n+                cache\n+                    .to_record_batch(&predicates)\n+                    .map(|batch| vec![batch])?,\n+                (!predicates.is_empty()).then_some(predicates),\n+            )\n+        } else {\n+            (vec![], None)\n+        };\n+        let mut exec =\n+            MetaCacheExec::try_new(predicates, &[batches], self.schema(), projection.cloned())?;\n+\n+        let show_sizes = ctx.config_options().explain.show_sizes;\n+        exec = exec.with_show_sizes(show_sizes);\n+\n+        Ok(Arc::new(exec))\n+    }\n+}\n+\n+/// Convert the given list of filter expressions to a map of [`ColumnId`] to [`Predicate`]\n+///\n+/// The resulting map uses [`IndexMap`] to ensure consistent ordering of the map. This makes testing\n+/// the filter conversion significantly easier using EXPLAIN queries.\n+fn convert_filter_exprs(\n+    table_def: &TableDefinition,\n+    cache_schema: SchemaRef,\n+    filters: &[Expr],\n+) -> Result<IndexMap<ColumnId, Predicate>> {\n+    let mut predicate_map: IndexMap<ColumnId, Option<Predicate>> = IndexMap::new();\n+\n+    // for create_physical_expr:\n+    let schema: DFSchema = cache_schema.try_into()?;\n+    let props = ExecutionProps::new();\n+\n+    // The set of `filters` that are passed in from DataFusion varies: 1) based on how they are\n+    // defined in the query, and 2) based on some decisions that DataFusion makes when parsing the\n+    // query into the `Expr` syntax tree. For example, the predicate:\n+    //\n+    // WHERE foo IN ('bar', 'baz')\n+    //\n+    // instead of being expressed as an `InList`, would be simplified to the following `Expr` tree:\n+    //\n+    // [\n+    //     BinaryExpr {\n+    //         left: BinaryExpr { left: \"foo\", op: Eq, right: \"bar\" },\n+    //         op: Or,\n+    //         right: BinaryExpr { left: \"foo\", op: Eq, right: \"baz\" }\n+    //     }\n+    // ]\n+    //\n+    // while the predicate:\n+    //\n+    // WHERE foo = 'bar' OR foo = 'baz' OR foo = 'bop' OR foo = 'bla'\n+    //\n+    // instead of being expressed as a tree of `BinaryExpr`s, is expressed as an `InList` with four\n+    // entries:\n+    //\n+    // [\n+    //     InList { col: \"foo\", values: [\"bar\", \"baz\", \"bop\", \"bla\"], negated: false }\n+    // ]\n+    //\n+    // Instead of handling all the combinations of `Expr`s that may be passed by the caller of\n+    // `TableProider::scan`, we can use the cache's schema to convert each `Expr` to a `PhysicalExpr`\n+    // and analyze it using DataFusion's `LiteralGuarantee`.\n+    //\n+    // This will distill the provided set of `Expr`s down to either an IN list, or a NOT IN list\n+    // which we can convert to the `Predicate` type for the metadata cache.\n+    //\n+    // The main caveat is that if for some reason there are multiple `Expr`s that apply predicates\n+    // on a given column, i.e., leading to multiple `LiteralGuarantee`s on a specific column, we\n+    // discard those predicates and have DataFusion handle the filtering.\n+    //\n+    // This is a conservative approach; it may be that we can combine multiple literal guarantees on\n+    // a single column, but thusfar, from testing in the parent module, this does not seem necessary.\n+\n+    for expr in filters {\n+        let physical_expr = create_physical_expr(expr, &schema, &props)?;\n+        let literal_guarantees = LiteralGuarantee::analyze(&physical_expr);\n+        for LiteralGuarantee {\n+            column,\n+            guarantee,\n+            literals,\n+        } in literal_guarantees\n+        {\n+            let Some(column_id) = table_def.column_name_to_id(column.name()) else {\n+                return plan_err!(\n+                    \"invalid column name in filter expression: {}\",\n+                    column.name()\n+                );\n+            };\n+            let value_iter = literals.into_iter().filter_map(|l| match l {\n+                ScalarValue::Utf8(Some(s)) | ScalarValue::Utf8View(Some(s)) => Some(s),\n+                _ => None,\n+            });\n+\n+            let predicate = match guarantee {\n+                Guarantee::In => Predicate::new_in(value_iter),\n+                Guarantee::NotIn => Predicate::new_not_in(value_iter),\n+            };\n+            predicate_map\n+                .entry(column_id)\n+                .and_modify(|e| {\n+                    // We do not currently support multiple literal guarantees per column.\n+                    //\n+                    // In this case we replace the predicate with None so that it does not filter\n+                    // any records from the cache downstream. Datafusion will still do filtering at\n+                    // a higher level, once _all_ records are produced from the cache.\n+                    e.take();\n+                })\n+                .or_insert_with(|| Some(predicate));\n+        }\n+    }\n+\n+    Ok(predicate_map\n+        .into_iter()\n+        .filter_map(|(column_id, predicate)| predicate.map(|predicate| (column_id, predicate)))\n+        .collect())\n+}\n+\n+/// Implementor of the [`TableFunctionImpl`] trait, to be registered as a user-defined table function\n+/// in the Datafusion `SessionContext`.\n+#[derive(Debug)]\n+pub struct MetaCacheFunction {\n+    db_id: DbId,\n+    provider: Arc<MetaCacheProvider>,\n+}\n+\n+impl MetaCacheFunction {\n+    pub fn new(db_id: DbId, provider: Arc<MetaCacheProvider>) -> Self {\n+        Self { db_id, provider }\n+    }\n+}\n+\n+impl TableFunctionImpl for MetaCacheFunction {\n+    fn call(&self, args: &[Expr]) -> Result<Arc<dyn TableProvider>> {\n+        let Some(Expr::Literal(ScalarValue::Utf8(Some(table_name)))) = args.first() else {\n+            return plan_err!(\"first argument must be the table name as a string\");\n+        };\n+        let cache_name = match args.get(1) {\n+            Some(Expr::Literal(ScalarValue::Utf8(Some(name)))) => Some(name),\n+            Some(_) => {\n+                return plan_err!(\"second argument, if passed, must be the cache name as a string\")\n+            }\n+            None => None,\n+        };\n+\n+        let Some(table_def) = self\n+            .provider\n+            .catalog\n+            .db_schema_by_id(&self.db_id)\n+            .and_then(|db| db.table_definition(table_name.as_str()))\n+        else {\n+            return plan_err!(\"provided table name ({}) is invalid\", table_name);\n+        };\n+        let Some((cache_name, schema)) = self.provider.get_cache_name_and_schema(\n+            self.db_id,\n+            table_def.table_id,\n+            cache_name.map(|n| n.as_str()),\n+        ) else {\n+            return plan_err!(\"could not find meta cache for the given arguments\");\n+        };\n+        Ok(Arc::new(MetaCacheFunctionProvider {\n+            schema,\n+            provider: Arc::clone(&self.provider),\n+            db_id: self.db_id,\n+            table_def,\n+            cache_name,\n+        }))\n+    }\n+}\n+\n+/// Custom implementor of the [`ExecutionPlan`] trait for use by the metadata cache\n+///\n+/// Wraps a [`MemoryExec`] from DataFusion, and mostly re-uses that. The special functionality\n+/// provided by this type is to track the predicates that are pushed down to the underlying cache\n+/// during query planning/execution.\n+///\n+/// # Example\n+///\n+/// For a query that does not provide any predicates, or one that does provide predicates, but they\n+/// do no get pushed down, the `EXPLAIN` for said query will contain a line for the `MetaCacheExec`\n+/// with no predicates, including what is emitted by the inner `MemoryExec`:\n+///\n+/// ```text\n+/// MetaCacheExec: inner=MemoryExec: partitions=1, partition_sizes=[1]\n+/// ```\n+///\n+/// For queries that do have predicates that get pushed down, the output will include them, e.g.:\n+///\n+/// ```text\n+/// MetaCacheExec: predicates=[[0 IN (us-east)], [1 IN (a,b)]] inner=MemoryExec: partitions=1, partition_sizes=[1]\n+/// ```\n+#[derive(Debug)]\n+struct MetaCacheExec {\n+    inner: MemoryExec,\n+    predicates: Option<IndexMap<ColumnId, Predicate>>,\n+}\n+\n+impl MetaCacheExec {\n+    fn try_new(\n+        predicates: Option<IndexMap<ColumnId, Predicate>>,\n+        partitions: &[Vec<RecordBatch>],\n+        schema: SchemaRef,\n+        projection: Option<Vec<usize>>,\n+    ) -> Result<Self> {\n+        Ok(Self {\n+            inner: MemoryExec::try_new(partitions, schema, projection)?,\n+            predicates,\n+        })\n+    }\n+\n+    fn with_show_sizes(self, show_sizes: bool) -> Self {\n+        Self {\n+            inner: self.inner.with_show_sizes(show_sizes),\n+            ..self\n+        }\n+    }\n+}\n+\n+impl DisplayAs for MetaCacheExec {\n+    fn fmt_as(&self, t: DisplayFormatType, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match t {\n+            DisplayFormatType::Default | DisplayFormatType::Verbose => {\n+                write!(f, \"MetaCacheExec:\")?;\n+                if let Some(predicates) = self.predicates.as_ref() {\n+                    write!(f, \" predicates=[\")?;\n+                    let mut p_iter = predicates.iter();\n+                    while let Some((col_id, predicate)) = p_iter.next() {\n+                        write!(f, \"[{col_id} {predicate}]\")?;",
        "comment_created_at": "2024-11-22T15:11:50+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Good call, I think I could do this by passing forward the `Arc<TableDefinition>` to the `MetaCacheExec` so it only pulls the column names when needed, e.g., for the `EXPLAIN` output. The table definition is already there so should not be too tricky, and yeah - given that part of the motivation for this whole execution plan implementation is to make it readable to the operator so I will get this in.",
        "pr_file_module": null
      },
      {
        "comment_id": "1854170874",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25566,
        "pr_file": "influxdb3_cache/src/meta_cache/table_function.rs",
        "discussion_id": "1853770590",
        "commented_code": "@@ -0,0 +1,359 @@\n+use std::{any::Any, sync::Arc};\n+\n+use arrow::{array::RecordBatch, datatypes::SchemaRef};\n+use async_trait::async_trait;\n+use datafusion::{\n+    catalog::{Session, TableProvider},\n+    common::{internal_err, plan_err, DFSchema, Result},\n+    datasource::{function::TableFunctionImpl, TableType},\n+    execution::context::ExecutionProps,\n+    logical_expr::TableProviderFilterPushDown,\n+    physical_expr::{\n+        create_physical_expr,\n+        utils::{Guarantee, LiteralGuarantee},\n+    },\n+    physical_plan::{memory::MemoryExec, DisplayAs, DisplayFormatType, ExecutionPlan},\n+    prelude::Expr,\n+    scalar::ScalarValue,\n+};\n+use indexmap::IndexMap;\n+use influxdb3_catalog::catalog::TableDefinition;\n+use influxdb3_id::{ColumnId, DbId};\n+\n+use super::{cache::Predicate, MetaCacheProvider};\n+\n+/// The name used to call the metadata cache in SQL queries\n+pub const META_CACHE_UDTF_NAME: &str = \"meta_cache\";\n+\n+/// Implementor of the [`TableProvider`] trait that is produced a call to the [`MetaCacheFunction`]\n+struct MetaCacheFunctionProvider {\n+    /// Reference to the [`MetaCache`][super::cache::MetaCache] being queried's schema\n+    schema: SchemaRef,\n+    /// Forwarded ref to the [`MetaCacheProvider`] which is used to get the\n+    /// [`MetaCache`][super::cache::MetaCache] for the query, along with the `db_id` and\n+    /// `table_def`. This is done instead of passing forward a reference to the `MetaCache`\n+    /// directly because doing so is not easy or possible with the Rust borrow checker.\n+    provider: Arc<MetaCacheProvider>,\n+    /// The database ID that the called cache is related to\n+    db_id: DbId,\n+    /// The table definition that the called cache is related to\n+    table_def: Arc<TableDefinition>,\n+    /// The name of the cache, which is determined when calling the `meta_cache` function\n+    cache_name: Arc<str>,\n+}\n+\n+#[async_trait]\n+impl TableProvider for MetaCacheFunctionProvider {\n+    fn as_any(&self) -> &dyn Any {\n+        self as &dyn Any\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        Arc::clone(&self.schema)\n+    }\n+\n+    fn table_type(&self) -> TableType {\n+        TableType::Temporary\n+    }\n+\n+    fn supports_filters_pushdown(\n+        &self,\n+        filters: &[&Expr],\n+    ) -> Result<Vec<TableProviderFilterPushDown>> {\n+        Ok(vec![TableProviderFilterPushDown::Inexact; filters.len()])\n+    }\n+\n+    async fn scan(\n+        &self,\n+        ctx: &dyn Session,\n+        projection: Option<&Vec<usize>>,\n+        filters: &[Expr],\n+        _limit: Option<usize>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        let read = self.provider.cache_map.read();\n+        let (batches, predicates) = if let Some(cache) = read\n+            .get(&self.db_id)\n+            .and_then(|db| db.get(&self.table_def.table_id))\n+            .and_then(|tbl| tbl.get(&self.cache_name))\n+        {\n+            let predicates = convert_filter_exprs(&self.table_def, self.schema(), filters)?;\n+            (\n+                cache\n+                    .to_record_batch(&predicates)\n+                    .map(|batch| vec![batch])?,\n+                (!predicates.is_empty()).then_some(predicates),\n+            )\n+        } else {\n+            (vec![], None)\n+        };\n+        let mut exec =\n+            MetaCacheExec::try_new(predicates, &[batches], self.schema(), projection.cloned())?;\n+\n+        let show_sizes = ctx.config_options().explain.show_sizes;\n+        exec = exec.with_show_sizes(show_sizes);\n+\n+        Ok(Arc::new(exec))\n+    }\n+}\n+\n+/// Convert the given list of filter expressions to a map of [`ColumnId`] to [`Predicate`]\n+///\n+/// The resulting map uses [`IndexMap`] to ensure consistent ordering of the map. This makes testing\n+/// the filter conversion significantly easier using EXPLAIN queries.\n+fn convert_filter_exprs(\n+    table_def: &TableDefinition,\n+    cache_schema: SchemaRef,\n+    filters: &[Expr],\n+) -> Result<IndexMap<ColumnId, Predicate>> {\n+    let mut predicate_map: IndexMap<ColumnId, Option<Predicate>> = IndexMap::new();\n+\n+    // for create_physical_expr:\n+    let schema: DFSchema = cache_schema.try_into()?;\n+    let props = ExecutionProps::new();\n+\n+    // The set of `filters` that are passed in from DataFusion varies: 1) based on how they are\n+    // defined in the query, and 2) based on some decisions that DataFusion makes when parsing the\n+    // query into the `Expr` syntax tree. For example, the predicate:\n+    //\n+    // WHERE foo IN ('bar', 'baz')\n+    //\n+    // instead of being expressed as an `InList`, would be simplified to the following `Expr` tree:\n+    //\n+    // [\n+    //     BinaryExpr {\n+    //         left: BinaryExpr { left: \"foo\", op: Eq, right: \"bar\" },\n+    //         op: Or,\n+    //         right: BinaryExpr { left: \"foo\", op: Eq, right: \"baz\" }\n+    //     }\n+    // ]\n+    //\n+    // while the predicate:\n+    //\n+    // WHERE foo = 'bar' OR foo = 'baz' OR foo = 'bop' OR foo = 'bla'\n+    //\n+    // instead of being expressed as a tree of `BinaryExpr`s, is expressed as an `InList` with four\n+    // entries:\n+    //\n+    // [\n+    //     InList { col: \"foo\", values: [\"bar\", \"baz\", \"bop\", \"bla\"], negated: false }\n+    // ]\n+    //\n+    // Instead of handling all the combinations of `Expr`s that may be passed by the caller of\n+    // `TableProider::scan`, we can use the cache's schema to convert each `Expr` to a `PhysicalExpr`\n+    // and analyze it using DataFusion's `LiteralGuarantee`.\n+    //\n+    // This will distill the provided set of `Expr`s down to either an IN list, or a NOT IN list\n+    // which we can convert to the `Predicate` type for the metadata cache.\n+    //\n+    // The main caveat is that if for some reason there are multiple `Expr`s that apply predicates\n+    // on a given column, i.e., leading to multiple `LiteralGuarantee`s on a specific column, we\n+    // discard those predicates and have DataFusion handle the filtering.\n+    //\n+    // This is a conservative approach; it may be that we can combine multiple literal guarantees on\n+    // a single column, but thusfar, from testing in the parent module, this does not seem necessary.\n+\n+    for expr in filters {\n+        let physical_expr = create_physical_expr(expr, &schema, &props)?;\n+        let literal_guarantees = LiteralGuarantee::analyze(&physical_expr);\n+        for LiteralGuarantee {\n+            column,\n+            guarantee,\n+            literals,\n+        } in literal_guarantees\n+        {\n+            let Some(column_id) = table_def.column_name_to_id(column.name()) else {\n+                return plan_err!(\n+                    \"invalid column name in filter expression: {}\",\n+                    column.name()\n+                );\n+            };\n+            let value_iter = literals.into_iter().filter_map(|l| match l {\n+                ScalarValue::Utf8(Some(s)) | ScalarValue::Utf8View(Some(s)) => Some(s),\n+                _ => None,\n+            });\n+\n+            let predicate = match guarantee {\n+                Guarantee::In => Predicate::new_in(value_iter),\n+                Guarantee::NotIn => Predicate::new_not_in(value_iter),\n+            };\n+            predicate_map\n+                .entry(column_id)\n+                .and_modify(|e| {\n+                    // We do not currently support multiple literal guarantees per column.\n+                    //\n+                    // In this case we replace the predicate with None so that it does not filter\n+                    // any records from the cache downstream. Datafusion will still do filtering at\n+                    // a higher level, once _all_ records are produced from the cache.\n+                    e.take();\n+                })\n+                .or_insert_with(|| Some(predicate));\n+        }\n+    }\n+\n+    Ok(predicate_map\n+        .into_iter()\n+        .filter_map(|(column_id, predicate)| predicate.map(|predicate| (column_id, predicate)))\n+        .collect())\n+}\n+\n+/// Implementor of the [`TableFunctionImpl`] trait, to be registered as a user-defined table function\n+/// in the Datafusion `SessionContext`.\n+#[derive(Debug)]\n+pub struct MetaCacheFunction {\n+    db_id: DbId,\n+    provider: Arc<MetaCacheProvider>,\n+}\n+\n+impl MetaCacheFunction {\n+    pub fn new(db_id: DbId, provider: Arc<MetaCacheProvider>) -> Self {\n+        Self { db_id, provider }\n+    }\n+}\n+\n+impl TableFunctionImpl for MetaCacheFunction {\n+    fn call(&self, args: &[Expr]) -> Result<Arc<dyn TableProvider>> {\n+        let Some(Expr::Literal(ScalarValue::Utf8(Some(table_name)))) = args.first() else {\n+            return plan_err!(\"first argument must be the table name as a string\");\n+        };\n+        let cache_name = match args.get(1) {\n+            Some(Expr::Literal(ScalarValue::Utf8(Some(name)))) => Some(name),\n+            Some(_) => {\n+                return plan_err!(\"second argument, if passed, must be the cache name as a string\")\n+            }\n+            None => None,\n+        };\n+\n+        let Some(table_def) = self\n+            .provider\n+            .catalog\n+            .db_schema_by_id(&self.db_id)\n+            .and_then(|db| db.table_definition(table_name.as_str()))\n+        else {\n+            return plan_err!(\"provided table name ({}) is invalid\", table_name);\n+        };\n+        let Some((cache_name, schema)) = self.provider.get_cache_name_and_schema(\n+            self.db_id,\n+            table_def.table_id,\n+            cache_name.map(|n| n.as_str()),\n+        ) else {\n+            return plan_err!(\"could not find meta cache for the given arguments\");\n+        };\n+        Ok(Arc::new(MetaCacheFunctionProvider {\n+            schema,\n+            provider: Arc::clone(&self.provider),\n+            db_id: self.db_id,\n+            table_def,\n+            cache_name,\n+        }))\n+    }\n+}\n+\n+/// Custom implementor of the [`ExecutionPlan`] trait for use by the metadata cache\n+///\n+/// Wraps a [`MemoryExec`] from DataFusion, and mostly re-uses that. The special functionality\n+/// provided by this type is to track the predicates that are pushed down to the underlying cache\n+/// during query planning/execution.\n+///\n+/// # Example\n+///\n+/// For a query that does not provide any predicates, or one that does provide predicates, but they\n+/// do no get pushed down, the `EXPLAIN` for said query will contain a line for the `MetaCacheExec`\n+/// with no predicates, including what is emitted by the inner `MemoryExec`:\n+///\n+/// ```text\n+/// MetaCacheExec: inner=MemoryExec: partitions=1, partition_sizes=[1]\n+/// ```\n+///\n+/// For queries that do have predicates that get pushed down, the output will include them, e.g.:\n+///\n+/// ```text\n+/// MetaCacheExec: predicates=[[0 IN (us-east)], [1 IN (a,b)]] inner=MemoryExec: partitions=1, partition_sizes=[1]\n+/// ```\n+#[derive(Debug)]\n+struct MetaCacheExec {\n+    inner: MemoryExec,\n+    predicates: Option<IndexMap<ColumnId, Predicate>>,\n+}\n+\n+impl MetaCacheExec {\n+    fn try_new(\n+        predicates: Option<IndexMap<ColumnId, Predicate>>,\n+        partitions: &[Vec<RecordBatch>],\n+        schema: SchemaRef,\n+        projection: Option<Vec<usize>>,\n+    ) -> Result<Self> {\n+        Ok(Self {\n+            inner: MemoryExec::try_new(partitions, schema, projection)?,\n+            predicates,\n+        })\n+    }\n+\n+    fn with_show_sizes(self, show_sizes: bool) -> Self {\n+        Self {\n+            inner: self.inner.with_show_sizes(show_sizes),\n+            ..self\n+        }\n+    }\n+}\n+\n+impl DisplayAs for MetaCacheExec {\n+    fn fmt_as(&self, t: DisplayFormatType, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match t {\n+            DisplayFormatType::Default | DisplayFormatType::Verbose => {\n+                write!(f, \"MetaCacheExec:\")?;\n+                if let Some(predicates) = self.predicates.as_ref() {\n+                    write!(f, \" predicates=[\")?;\n+                    let mut p_iter = predicates.iter();\n+                    while let Some((col_id, predicate)) = p_iter.next() {\n+                        write!(f, \"[{col_id} {predicate}]\")?;",
        "comment_created_at": "2024-11-22T15:56:50+00:00",
        "comment_author": "hiltontj",
        "comment_body": "https://github.com/influxdata/influxdb/issues/25582",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1777234069",
    "pr_number": 25389,
    "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
    "created_at": "2024-09-26T14:50:03+00:00",
    "commented_code": "impl CacheValue {\n     /// Get the size of the cache value's memory footprint in bytes\n     fn size(&self) -> usize {\n-        // TODO(trevor): could also calculate the size of the metadata...\n-        self.data.len()\n+        let Self { data, meta } = self;\n+        let ObjectMeta {\n+            location,\n+            last_modified: _,\n+            size: _,\n+            e_tag,\n+            version,\n+        } = meta;",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1777234069",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25389,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1777234069",
        "commented_code": "@@ -108,59 +125,213 @@ struct CacheValue {\n impl CacheValue {\n     /// Get the size of the cache value's memory footprint in bytes\n     fn size(&self) -> usize {\n-        // TODO(trevor): could also calculate the size of the metadata...\n-        self.data.len()\n+        let Self { data, meta } = self;\n+        let ObjectMeta {\n+            location,\n+            last_modified: _,\n+            size: _,\n+            e_tag,\n+            version,\n+        } = meta;",
        "comment_created_at": "2024-09-26T14:50:03+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "```suggestion\r\n        let Self { \r\n            data, \r\n            meta: ObjectMeta {\r\n                location,\r\n                last_modified: _,\r\n                size: _,\r\n                e_tag,\r\n                version,\r\n            }\r\n        } = self;\r\n```\r\n\r\nI think you can just combine these into one destructure statement like this",
        "pr_file_module": null
      },
      {
        "comment_id": "1777309928",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25389,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1777234069",
        "commented_code": "@@ -108,59 +125,213 @@ struct CacheValue {\n impl CacheValue {\n     /// Get the size of the cache value's memory footprint in bytes\n     fn size(&self) -> usize {\n-        // TODO(trevor): could also calculate the size of the metadata...\n-        self.data.len()\n+        let Self { data, meta } = self;\n+        let ObjectMeta {\n+            location,\n+            last_modified: _,\n+            size: _,\n+            e_tag,\n+            version,\n+        } = meta;",
        "comment_created_at": "2024-09-26T15:22:53+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Yep, that works! I will apply it locally.",
        "pr_file_module": null
      },
      {
        "comment_id": "1778722252",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25389,
        "pr_file": "influxdb3_write/src/parquet_cache/mod.rs",
        "discussion_id": "1777234069",
        "commented_code": "@@ -108,59 +125,213 @@ struct CacheValue {\n impl CacheValue {\n     /// Get the size of the cache value's memory footprint in bytes\n     fn size(&self) -> usize {\n-        // TODO(trevor): could also calculate the size of the metadata...\n-        self.data.len()\n+        let Self { data, meta } = self;\n+        let ObjectMeta {\n+            location,\n+            last_modified: _,\n+            size: _,\n+            e_tag,\n+            version,\n+        } = meta;",
        "comment_created_at": "2024-09-27T14:31:35+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Done in https://github.com/influxdata/influxdb/pull/25389/commits/71daada9884066ec8a2f7b2a1ac33d90fbf4cd99",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1680094060",
    "pr_number": 25166,
    "pr_file": "influxdb3_server/src/query_executor.rs",
    "created_at": "2024-07-16T21:39:17+00:00",
    "commented_code": "let batch = RecordBatch::try_new(schema, columns)?;\n     Ok(batch)\n }\n+\n+struct LastCachesTable {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1680094060",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25166,
        "pr_file": "influxdb3_server/src/query_executor.rs",
        "discussion_id": "1680094060",
        "commented_code": "@@ -840,3 +854,117 @@ fn from_query_log_entries(\n     let batch = RecordBatch::try_new(schema, columns)?;\n     Ok(batch)\n }\n+\n+struct LastCachesTable {",
        "comment_created_at": "2024-07-16T21:39:17+00:00",
        "comment_author": "pauldix",
        "comment_body": "Might be good to pull this out of the query_executor and into its own file. Create a structure for different system tables to be defined. I imagine we'll have a number of these over time.",
        "pr_file_module": null
      },
      {
        "comment_id": "1681026831",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25166,
        "pr_file": "influxdb3_server/src/query_executor.rs",
        "discussion_id": "1680094060",
        "commented_code": "@@ -840,3 +854,117 @@ fn from_query_log_entries(\n     let batch = RecordBatch::try_new(schema, columns)?;\n     Ok(batch)\n }\n+\n+struct LastCachesTable {",
        "comment_created_at": "2024-07-17T13:11:22+00:00",
        "comment_author": "hiltontj",
        "comment_body": "I moved out the system table related code to its own module in https://github.com/influxdata/influxdb/pull/25166/commits/7c1f4db1eea45f8b12cc6a013260fc668c658fc9 -  definitely cleaner.\r\n\r\nNo code was changed and CI is  so I will get this merged.",
        "pr_file_module": null
      }
    ]
  }
]