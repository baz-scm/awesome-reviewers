[
  {
    "discussion_id": "1791973891",
    "pr_number": 25388,
    "pr_file": "influxdb3_catalog/src/serialize.rs",
    "created_at": "2024-10-08T14:18:32+00:00",
    "commented_code": "let cols = def\n             .schema()\n             .iter()\n-            .map(|(col_type, f)| {\n+            .enumerate()",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1791973891",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25388,
        "pr_file": "influxdb3_catalog/src/serialize.rs",
        "discussion_id": "1791973891",
        "commented_code": "@@ -135,10 +138,12 @@ impl<'a> From<&'a TableDefinition> for TableSnapshot<'a> {\n         let cols = def\n             .schema()\n             .iter()\n-            .map(|(col_type, f)| {\n+            .enumerate()",
        "comment_created_at": "2024-10-08T14:18:32+00:00",
        "comment_author": "hiltontj",
        "comment_body": "I don't know if using `enumerate` to determine the column ID is a good idea. I think that generally, columns are always appended, in which case, it is okay, but in the event that we allow for dropping columns, then this would change their order and mess up the IDs.\r\n\r\nWe probably need some way to generate the IDs, based on what was the largest already used ID for a given table, and then ensure that that ID remains fixed for the column it is applied to for all time.",
        "pr_file_module": null
      },
      {
        "comment_id": "1792013364",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25388,
        "pr_file": "influxdb3_catalog/src/serialize.rs",
        "discussion_id": "1791973891",
        "commented_code": "@@ -135,10 +138,12 @@ impl<'a> From<&'a TableDefinition> for TableSnapshot<'a> {\n         let cols = def\n             .schema()\n             .iter()\n-            .map(|(col_type, f)| {\n+            .enumerate()",
        "comment_created_at": "2024-10-08T14:39:56+00:00",
        "comment_author": "pauldix",
        "comment_body": "Ah yes, that's a good catch. They should have a well defined ID that remains static regardless of what schema changes later happen to the table.",
        "pr_file_module": null
      },
      {
        "comment_id": "1795673007",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25388,
        "pr_file": "influxdb3_catalog/src/serialize.rs",
        "discussion_id": "1791973891",
        "commented_code": "@@ -135,10 +138,12 @@ impl<'a> From<&'a TableDefinition> for TableSnapshot<'a> {\n         let cols = def\n             .schema()\n             .iter()\n-            .map(|(col_type, f)| {\n+            .enumerate()",
        "comment_created_at": "2024-10-10T15:24:51+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Okay I've taken a look at this more @pauldix and @hiltontj and I've come to the conclusion we either use enumerate or we don't even bother with a column id. Here's why:\r\n\r\n- [Here's where we define a new TableDefinition](https://github.com/influxdata/influxdb/blob/277a6e0a877d0b1068618bebbed61407c81e7abc/influxdb3_catalog/src/catalog.rs#L629-L635), it's what ends up in our Catalog when serialized\r\n- When we create a new one [here is where we see ](https://github.com/influxdata/influxdb/blob/277a6e0a877d0b1068618bebbed61407c81e7abc/influxdb3_catalog/src/catalog.rs#L644)what we use to make a new Schema\r\n- This becomes a [`Schema`](https://github.com/influxdata/influxdb3_core/blob/1eaa4ed5ea147bc24db98d9686e457c124dfd5b7/schema/src/lib.rs#L148-L155)\r\n- Which is just a wrapper around [arrow::datatypes::SchemaRef](https://arrow.apache.org/rust/arrow/datatypes/type.SchemaRef.html)\r\n- This has no way to add IDs for columns and the way most arrow stuff works is by indexing on a column essentially. There is no stable Id for a column.\r\n\r\nI don't think this is viable beyond doing enumerate or we'd have to upstream changes to arrow itself and that seems like it wouldn't be worth the trade off or something that would make sense upstream. I don't think this change is worth it given how everything in arrow works off indexing or column name not id.",
        "pr_file_module": null
      },
      {
        "comment_id": "1795707988",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25388,
        "pr_file": "influxdb3_catalog/src/serialize.rs",
        "discussion_id": "1791973891",
        "commented_code": "@@ -135,10 +138,12 @@ impl<'a> From<&'a TableDefinition> for TableSnapshot<'a> {\n         let cols = def\n             .schema()\n             .iter()\n-            .map(|(col_type, f)| {\n+            .enumerate()",
        "comment_created_at": "2024-10-10T15:47:10+00:00",
        "comment_author": "pauldix",
        "comment_body": "We could have our own `TableSchema` which includes the `SchemaRef` and also includes a map of column name to id. Then the `TableSchema` is what we serialize in the catalog. We want to have the ids and not the string identifiers in the WALContents because that's much cheaper to serialize and deserialize. Also cheaper to index when inserting the data into the `WriteBuffer`.\r\n\r\nSo we don't need to update arrow, we just need a wrapper around the arrow struct where we can add our own stuff.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1786749239",
    "pr_number": 25421,
    "pr_file": "influxdb3_write/src/write_buffer/validator.rs",
    "created_at": "2024-10-03T19:30:14+00:00",
    "commented_code": "/// This errors if the write is being performed against a v1 table, i.e., one that does not have\n /// a series key.\n fn validate_v3_line<'a>(\n+    catalog: &Catalog,\n     db_schema: &mut Cow<'_, DatabaseSchema>,\n     line_number: usize,\n     line: v3::ParsedLine<'a>,\n     raw_line: &str,\n ) -> Result<(v3::ParsedLine<'a>, Option<CatalogOp>), WriteLineError> {\n     let mut catalog_op = None;\n     let table_name = line.series.measurement.as_str();\n-    if let Some(table_def) = db_schema.get_table(table_name) {\n+    if let Some(table_def) = catalog\n+        .table_name_to_id(db_schema.id, table_name.into())",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1786749239",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25421,
        "pr_file": "influxdb3_write/src/write_buffer/validator.rs",
        "discussion_id": "1786749239",
        "commented_code": "@@ -216,14 +226,19 @@ impl WriteValidator<WithCatalog> {\n /// This errors if the write is being performed against a v1 table, i.e., one that does not have\n /// a series key.\n fn validate_v3_line<'a>(\n+    catalog: &Catalog,\n     db_schema: &mut Cow<'_, DatabaseSchema>,\n     line_number: usize,\n     line: v3::ParsedLine<'a>,\n     raw_line: &str,\n ) -> Result<(v3::ParsedLine<'a>, Option<CatalogOp>), WriteLineError> {\n     let mut catalog_op = None;\n     let table_name = line.series.measurement.as_str();\n-    if let Some(table_def) = db_schema.get_table(table_name) {\n+    if let Some(table_def) = catalog\n+        .table_name_to_id(db_schema.id, table_name.into())",
        "comment_created_at": "2024-10-03T19:30:14+00:00",
        "comment_author": "pauldix",
        "comment_body": "A thought I had is that the `DbSchema` should contain the mapping of the table name to id and back. That way when a write comes in, you grab a lock on the catalog once to get the schema. Then from that point on, assuming you're not creating any new schema, you won't need to grab a lock on the catalog to validate any of the lines.\r\n\r\nYou only end up having to lock again and replace the schema you're working with if they insert a new table.\r\n\r\nIt previously worked like this to minimize grabbing locks line by line.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1772312775",
    "pr_number": 25376,
    "pr_file": "influxdb3_wal/src/lib.rs",
    "created_at": "2024-09-23T22:47:55+00:00",
    "commented_code": "pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1772312775",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-23T22:47:55+00:00",
        "comment_author": "pauldix",
        "comment_body": "This feels like it's going to be problematic. I'd prefer to have the name and ID be separate fields. I think the better approach would be to have the key be the int table id and then have the name be a member on the value, which could have the table name and the data.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773356281",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T13:31:13+00:00",
        "comment_author": "hiltontj",
        "comment_body": "This could be done by flipping the hashmap to\r\n```rust\r\nHashMap<TableId, (Arc<str>, TableChunks)>\r\n```\r\nSerde would then `Serialize`/`Deserialize` it gracefully using derive.\r\n\r\nNot sure how gracefully that fits in to the broader change set but it would certainly be nice to not need the custom serialization code.\r\n\r\nIf you need to have both the name and ID in the hash key then you could define a new-type, e.g.,\r\n```rust\r\n#[derive(/* ... */, Serialize, Deserialize)]\r\nstruct WriteBatch {\r\n    /* ... */\r\n    table_chunks: HashMap<TableKey, TableChunks>,\r\n}\r\n\r\nstruct TableKey(Arc<str>, TableId);\r\n```\r\nThen, implement `Serialize`/`Deserialize` on `TableKey`, vs. having to do it for the whole `WriteBatch` type.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773556973",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T15:07:45+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Okay tried flipping them but that did not work. The main problem here is that serde will not serialize a tuple in a map to JSON. It just will not work, JSON has no tuple type. We could do:\r\n\r\nHashMap<Arc<str>, HashMap<TableId, TableChunks>>\r\n\r\nbut that seems like a lot. The original point was to add the TableId, but it's beginning to look like that's just not possible. @pauldix I think we can either use the name or the Id, but not both. We could punt on adding it to WriteBatch. It's currently in the catalog and when we have actual optimizations we want to do we can bring it in.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773579049",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T15:20:24+00:00",
        "comment_author": "hiltontj",
        "comment_body": "JSON doesn't have a tuple, but `serde_json` will serialize/deserialize tuples as lists (see [here](https://docs.rs/serde_json/latest/src/serde_json/ser.rs.html#303-305)). I think part of the issue would be using `TableId` as the key in the map, since JSON doesn't support integer map keys. For that we would either need to de/serialize `TableId`s as strings, or not use JSON.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773597895",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T15:32:01+00:00",
        "comment_author": "pauldix",
        "comment_body": "I was thinking that you want the TableId as the key in the map. Then the value will have the name (as a member). We want the map key to be an int anyway because it's way faster to lookup.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773600138",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T15:33:31+00:00",
        "comment_author": "pauldix",
        "comment_body": "So the `TableChunks` struct would have table name as part of it. Probably worth renaming TableChunks to be something more appropriate.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773704311",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T16:46:38+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Okay I can make these changes work then!\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1773722721",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T17:00:09+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Hmmm okay so this approach also is fraught with coming head to head with other design choices. TableChunks is meant to just contain those, adding the table name is proving hard to do. For instance we heavily rely on a Default impl and have no easy way to pass in table names, we'd have to do a lookup, but we don't really have access to the catalog. It's becoming increasingly clear that we have a lot of assumptions baked into using table names everywhere and where we have access into looking things up. I think we can't have both really as it's way too invasive. We either switch fully to Ids everywhere and let the names be stored in the catalog or just stick to names. I'm leaning towards the former.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773729175",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T17:04:53+00:00",
        "comment_author": "pauldix",
        "comment_body": "We should move to IDs everywhere, but anything that shows up in the WAL should also have the name in it somewhere. That is, an individual WAL file should be self-describing without having to pair it with a catalog.\r\n\r\nThe other thing about IDs everywhere is that for user facing things, it should be name. For our internal systems like lookup tables, etc, it should be ids.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773737042",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T17:11:20+00:00",
        "comment_author": "pauldix",
        "comment_body": "I guess we could drop the name from the WAL part if it proves too difficult. I'm just thinking about future recoverability & debugability. Easier if the name is there.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773761055",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T17:31:32+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Would a lookup table of name to Id also being in WriteBatch be a good idea?",
        "pr_file_module": null
      },
      {
        "comment_id": "1773776988",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_wal/src/lib.rs",
        "discussion_id": "1772312775",
        "commented_code": "@@ -424,20 +429,124 @@ pub struct LastCacheDelete {\n     pub name: String,\n }\n \n-#[derive(Debug, Clone, Eq, PartialEq, Serialize, Deserialize)]\n+#[derive(Debug, Clone, Eq, PartialEq)]\n pub struct WriteBatch {\n     pub database_id: DbId,\n     pub database_name: Arc<str>,\n-    pub table_chunks: HashMap<Arc<str>, TableChunks>,\n+    pub table_chunks: HashMap<(Arc<str>, TableId), TableChunks>,\n     pub min_time_ns: i64,\n     pub max_time_ns: i64,\n }\n \n+impl Serialize for WriteBatch {\n+    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n+    where\n+        S: Serializer,\n+    {\n+        let mut state = serializer.serialize_struct(\"WriteBatch\", 5)?;\n+        state.serialize_field(\"database_id\", &self.database_id)?;\n+        state.serialize_field(\"database_name\", &self.database_name)?;\n+        state.serialize_field(\"min_time_ns\", &self.min_time_ns)?;\n+        state.serialize_field(\"max_time_ns\", &self.max_time_ns)?;\n+        state.serialize_field(\n+            \"table_chunks\",\n+            &self\n+                .table_chunks\n+                .iter()\n+                .map(|((table_name, table_id), value)| {\n+                    (format!(\"{table_name}//{}\", table_id.as_u32()), value)\n+                })\n+                .collect::<HashMap<String, &TableChunks>>(),\n+        )?;\n+        state.end()\n+    }\n+}\n+\n+impl<'de> Deserialize<'de> for WriteBatch {\n+    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+    where\n+        D: Deserializer<'de>,\n+    {\n+        struct WriteBatchVisitor;\n+        impl<'de> Visitor<'de> for WriteBatchVisitor {\n+            type Value = WriteBatch;\n+            fn expecting(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {\n+                formatter.write_str(\"struct WriteBatch\")\n+            }\n+            fn visit_map<V>(self, mut map: V) -> Result<Self::Value, V::Error>\n+            where\n+                V: MapAccess<'de>,\n+            {\n+                let mut database_id = None;\n+                let mut database_name = None;\n+                let mut table_chunks = None;\n+                let mut min_time_ns = None;\n+                let mut max_time_ns = None;\n+                while let Some(key) = map.next_key()? {\n+                    match key {\n+                        \"database_id\" => database_id = Some(DbId::from(map.next_value::<u32>()?)),\n+                        \"database_name\" => database_name = Some(map.next_value::<&str>()?.into()),\n+                        \"min_time_ns\" => min_time_ns = Some(map.next_value::<i64>()?),\n+                        \"max_time_ns\" => max_time_ns = Some(map.next_value::<i64>()?),\n+                        \"table_chunks\" => {\n+                            table_chunks = Some(\n+                                map.next_value::<HashMap<String, TableChunks>>()?\n+                                    .into_iter()\n+                                    .map(|(key, value)| {\n+                                        let mut keys = key.split(\"//\");",
        "comment_created_at": "2024-09-24T17:43:52+00:00",
        "comment_author": "pauldix",
        "comment_body": "I would do it the other way. All the data stored in a map that's keyed off ID. Then have a lookup table of ID to name. Although I'm not sure we want to do that. We don't need it assuming that we have a Catalog (which in all current cases we do). So it's extra payload that increases size and decode time for downstream consumers. Maybe just leave the name out and rely only on the ID.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1773346271",
    "pr_number": 25376,
    "pr_file": "influxdb3_write/src/write_buffer/validator.rs",
    "created_at": "2024-09-24T13:25:27+00:00",
    "commented_code": "// Add the row into the correct chunk in the table\n     let chunk_time = gen1_duration.chunk_time_for_timestamp(Timestamp::new(time_value_nanos));\n     let table_name: Arc<str> = line.series.measurement.to_string().into();\n-    let table_chunks = table_chunk_map.entry(Arc::clone(&table_name)).or_default();\n+    let table_id = db_schema\n+        .get_table(&table_name)\n+        .map(|t| t.table_id)\n+        .unwrap_or_else(|| TableId::from(0));",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1773346271",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_write/src/write_buffer/validator.rs",
        "discussion_id": "1773346271",
        "commented_code": "@@ -637,7 +628,13 @@ fn convert_v3_parsed_line(\n     // Add the row into the correct chunk in the table\n     let chunk_time = gen1_duration.chunk_time_for_timestamp(Timestamp::new(time_value_nanos));\n     let table_name: Arc<str> = line.series.measurement.to_string().into();\n-    let table_chunks = table_chunk_map.entry(Arc::clone(&table_name)).or_default();\n+    let table_id = db_schema\n+        .get_table(&table_name)\n+        .map(|t| t.table_id)\n+        .unwrap_or_else(|| TableId::from(0));",
        "comment_created_at": "2024-09-24T13:25:27+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Should this take the next available table ID, instead of using 0?",
        "pr_file_module": null
      },
      {
        "comment_id": "1773367169",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_write/src/write_buffer/validator.rs",
        "discussion_id": "1773346271",
        "commented_code": "@@ -637,7 +628,13 @@ fn convert_v3_parsed_line(\n     // Add the row into the correct chunk in the table\n     let chunk_time = gen1_duration.chunk_time_for_timestamp(Timestamp::new(time_value_nanos));\n     let table_name: Arc<str> = line.series.measurement.to_string().into();\n-    let table_chunks = table_chunk_map.entry(Arc::clone(&table_name)).or_default();\n+    let table_id = db_schema\n+        .get_table(&table_name)\n+        .map(|t| t.table_id)\n+        .unwrap_or_else(|| TableId::from(0));",
        "comment_created_at": "2024-09-24T13:37:29+00:00",
        "comment_author": "hiltontj",
        "comment_body": "(the same would be said for the other parsing function)",
        "pr_file_module": null
      },
      {
        "comment_id": "1773559765",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_write/src/write_buffer/validator.rs",
        "discussion_id": "1773346271",
        "commented_code": "@@ -637,7 +628,13 @@ fn convert_v3_parsed_line(\n     // Add the row into the correct chunk in the table\n     let chunk_time = gen1_duration.chunk_time_for_timestamp(Timestamp::new(time_value_nanos));\n     let table_name: Arc<str> = line.series.measurement.to_string().into();\n-    let table_chunks = table_chunk_map.entry(Arc::clone(&table_name)).or_default();\n+    let table_id = db_schema\n+        .get_table(&table_name)\n+        .map(|t| t.table_id)\n+        .unwrap_or_else(|| TableId::from(0));",
        "comment_created_at": "2024-09-24T15:09:32+00:00",
        "comment_author": "mgattozzi",
        "comment_body": "Hmmmmm I was unsure. It's validating not creating a new table as far as I understood.",
        "pr_file_module": null
      },
      {
        "comment_id": "1773617609",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25376,
        "pr_file": "influxdb3_write/src/write_buffer/validator.rs",
        "discussion_id": "1773346271",
        "commented_code": "@@ -637,7 +628,13 @@ fn convert_v3_parsed_line(\n     // Add the row into the correct chunk in the table\n     let chunk_time = gen1_duration.chunk_time_for_timestamp(Timestamp::new(time_value_nanos));\n     let table_name: Arc<str> = line.series.measurement.to_string().into();\n-    let table_chunks = table_chunk_map.entry(Arc::clone(&table_name)).or_default();\n+    let table_id = db_schema\n+        .get_table(&table_name)\n+        .map(|t| t.table_id)\n+        .unwrap_or_else(|| TableId::from(0));",
        "comment_created_at": "2024-09-24T15:44:54+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Ah, I see, the code in validator is a bit confusing right now. For example, it used to update the in-memory catalog schema using a `Cow` to check that it had changes which is why this is here: https://github.com/influxdata/influxdb/blob/9c71b3ce251f32cf8f23db0a4f09873e04686c1a/influxdb3_write/src/write_buffer/validator.rs#L514-L521\r\n\r\nBut it doesn't look like it uses the cow anymore. It does still update the catalog in its state here, though, by applying the catalog batch: https://github.com/influxdata/influxdb/blob/9c71b3ce251f32cf8f23db0a4f09873e04686c1a/influxdb3_write/src/write_buffer/validator.rs#L182-L196\r\n\r\nAnd I believe that will result in the table being created, so I think it should probably be using the next ID instead of 0. Perhaps a test would help suss that out.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1668578321",
    "pr_number": 25125,
    "pr_file": "influxdb3_write/src/last_cache.rs",
    "created_at": "2024-07-08T12:53:21+00:00",
    "commented_code": "*target = LastCacheState::Store(LastCacheStore::new(\n                 self.count.into(),\n                 self.ttl,\n-                self.schema.clone(),\n+                Arc::clone(&self.schema),\n+                self.series_key.as_ref(),\n             ));\n         }\n-        target\n-            .as_store_mut()\n-            .expect(\n-                \"cache target should be the actual store after iterating through all key columns\",\n-            )\n-            .push(row);\n+        let store = target.as_store_mut().expect(\n+            \"cache target should be the actual store after iterating through all key columns\",\n+        );\n+        let Some(new_columns) = store.push(row, self.accept_new, &self.key_columns) else {\n+            // Unless new columns were added, and we need to update the schema, we are done.\n+            return;\n+        };\n+\n+        let mut sb = ArrowSchemaBuilder::new();\n+        for f in self.schema.fields().iter() {\n+            sb.push(Arc::clone(f));\n+        }\n+        for (name, data_type) in new_columns {\n+            sb.push(Arc::new(ArrowField::new(name, data_type, true)));\n+        }\n+        let new_schema = Arc::new(sb.finish());",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1668578321",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25125,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1668578321",
        "commented_code": "@@ -381,15 +405,28 @@ impl LastCache {\n             *target = LastCacheState::Store(LastCacheStore::new(\n                 self.count.into(),\n                 self.ttl,\n-                self.schema.clone(),\n+                Arc::clone(&self.schema),\n+                self.series_key.as_ref(),\n             ));\n         }\n-        target\n-            .as_store_mut()\n-            .expect(\n-                \"cache target should be the actual store after iterating through all key columns\",\n-            )\n-            .push(row);\n+        let store = target.as_store_mut().expect(\n+            \"cache target should be the actual store after iterating through all key columns\",\n+        );\n+        let Some(new_columns) = store.push(row, self.accept_new, &self.key_columns) else {\n+            // Unless new columns were added, and we need to update the schema, we are done.\n+            return;\n+        };\n+\n+        let mut sb = ArrowSchemaBuilder::new();\n+        for f in self.schema.fields().iter() {\n+            sb.push(Arc::clone(f));\n+        }\n+        for (name, data_type) in new_columns {\n+            sb.push(Arc::new(ArrowField::new(name, data_type, true)));\n+        }\n+        let new_schema = Arc::new(sb.finish());",
        "comment_created_at": "2024-07-08T12:53:21+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Need to handle for collisions here, i.e., to ensure there are no duplicate fields, and also check for ordering. I don't think that race conditions are a concern since this method is invoked from a single event loop, and is processing rows from the buffer sequentially.\r\n\r\nA good test would be to have a cache on the table `foo` with keys `[t1]` and values `[f1, time]`, and then write the following LP that would add `f2` and `f3` value columns:\r\n```\r\nfoo,t1=a f1=1,f2=2,f3=3\r\nfoo,t1=b f1=1,f3=3,f2=2\r\n```\r\nAssuming that `t1=a` and `t1=b` have already been written to, and therefore each have a cache associated; each cache will have the new value columns `f2` and `f3` added, but they will be added in a different order to each respective cache:\r\n```\r\nt1=a -> f2,f3\r\nt1=b -> f3,f2\r\n```\r\nThen check that `RecordBatch`es spanning both `t1` key values can be produced and combined.",
        "pr_file_module": null
      },
      {
        "comment_id": "1668614638",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25125,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1668578321",
        "commented_code": "@@ -381,15 +405,28 @@ impl LastCache {\n             *target = LastCacheState::Store(LastCacheStore::new(\n                 self.count.into(),\n                 self.ttl,\n-                self.schema.clone(),\n+                Arc::clone(&self.schema),\n+                self.series_key.as_ref(),\n             ));\n         }\n-        target\n-            .as_store_mut()\n-            .expect(\n-                \"cache target should be the actual store after iterating through all key columns\",\n-            )\n-            .push(row);\n+        let store = target.as_store_mut().expect(\n+            \"cache target should be the actual store after iterating through all key columns\",\n+        );\n+        let Some(new_columns) = store.push(row, self.accept_new, &self.key_columns) else {\n+            // Unless new columns were added, and we need to update the schema, we are done.\n+            return;\n+        };\n+\n+        let mut sb = ArrowSchemaBuilder::new();\n+        for f in self.schema.fields().iter() {\n+            sb.push(Arc::clone(f));\n+        }\n+        for (name, data_type) in new_columns {\n+            sb.push(Arc::new(ArrowField::new(name, data_type, true)));\n+        }\n+        let new_schema = Arc::new(sb.finish());",
        "comment_created_at": "2024-07-08T13:17:42+00:00",
        "comment_author": "hiltontj",
        "comment_body": "Something worth noting is that the write buffer is validating the incoming writes, so we shouldn't have to worry about new fields being written with incompatible types in subsequent lines of LP. I do think ensuring that ordering discrepancies like above should be handled.",
        "pr_file_module": null
      },
      {
        "comment_id": "1669166327",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25125,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1668578321",
        "commented_code": "@@ -381,15 +405,28 @@ impl LastCache {\n             *target = LastCacheState::Store(LastCacheStore::new(\n                 self.count.into(),\n                 self.ttl,\n-                self.schema.clone(),\n+                Arc::clone(&self.schema),\n+                self.series_key.as_ref(),\n             ));\n         }\n-        target\n-            .as_store_mut()\n-            .expect(\n-                \"cache target should be the actual store after iterating through all key columns\",\n-            )\n-            .push(row);\n+        let store = target.as_store_mut().expect(\n+            \"cache target should be the actual store after iterating through all key columns\",\n+        );\n+        let Some(new_columns) = store.push(row, self.accept_new, &self.key_columns) else {\n+            // Unless new columns were added, and we need to update the schema, we are done.\n+            return;\n+        };\n+\n+        let mut sb = ArrowSchemaBuilder::new();\n+        for f in self.schema.fields().iter() {\n+            sb.push(Arc::clone(f));\n+        }\n+        for (name, data_type) in new_columns {\n+            sb.push(Arc::new(ArrowField::new(name, data_type, true)));\n+        }\n+        let new_schema = Arc::new(sb.finish());",
        "comment_created_at": "2024-07-08T19:21:13+00:00",
        "comment_author": "hiltontj",
        "comment_body": "https://github.com/influxdata/influxdb/pull/25125/commits/a129d003397bbb859a112980aee3de6286d1adcf switched to using `SchemaBuilder::try_merge` to prevent conflicts, and with the added test case, the above scenario will not cause issues.\r\n\r\n_Edit_: that commit did not produce the correct behaviour \ud83e\udd26",
        "pr_file_module": null
      },
      {
        "comment_id": "1670580127",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25125,
        "pr_file": "influxdb3_write/src/last_cache.rs",
        "discussion_id": "1668578321",
        "commented_code": "@@ -381,15 +405,28 @@ impl LastCache {\n             *target = LastCacheState::Store(LastCacheStore::new(\n                 self.count.into(),\n                 self.ttl,\n-                self.schema.clone(),\n+                Arc::clone(&self.schema),\n+                self.series_key.as_ref(),\n             ));\n         }\n-        target\n-            .as_store_mut()\n-            .expect(\n-                \"cache target should be the actual store after iterating through all key columns\",\n-            )\n-            .push(row);\n+        let store = target.as_store_mut().expect(\n+            \"cache target should be the actual store after iterating through all key columns\",\n+        );\n+        let Some(new_columns) = store.push(row, self.accept_new, &self.key_columns) else {\n+            // Unless new columns were added, and we need to update the schema, we are done.\n+            return;\n+        };\n+\n+        let mut sb = ArrowSchemaBuilder::new();\n+        for f in self.schema.fields().iter() {\n+            sb.push(Arc::clone(f));\n+        }\n+        for (name, data_type) in new_columns {\n+            sb.push(Arc::new(ArrowField::new(name, data_type, true)));\n+        }\n+        let new_schema = Arc::new(sb.finish());",
        "comment_created_at": "2024-07-09T13:56:28+00:00",
        "comment_author": "hiltontj",
        "comment_body": "https://github.com/influxdata/influxdb/pull/25125/commits/2917fc149766ac349a4cb297f86ea7dfb6395797 Looks to have resolved this.",
        "pr_file_module": null
      }
    ]
  }
]