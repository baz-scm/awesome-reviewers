[
  {
    "discussion_id": "1914209553",
    "pr_number": 25594,
    "pr_file": "tsdb/engine/tsm1/compact_test.go",
    "created_at": "2025-01-14T04:00:03+00:00",
    "commented_code": "}\n }\n \n-// This test is added to acount for many TSM files within a group being over 2 GB\n-// we want to ensure that the shard will be planned.\n-func TestDefaultPlanner_PlanOptimize_LargeMultiGeneration(t *testing.T) {\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-05.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-06.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-07.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-08.tsm1\",\n-\t\t\tSize: 1048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-05.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-06.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-07.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-08.tsm1\",\n-\t\t\tSize: 1048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"03-04.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"03-05.tsm1\",\n-\t\t\tSize: 512 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n-\t\t\tPathsFn: func() []tsm1.FileStat {\n-\t\t\t\treturn data\n+func TestDefaultPlanner_PlanOptimize_Test(t *testing.T) {\n+\t// bc are the block counts for the tsm files\n+\t// bc can be an empty list\n+\ttype PlanOptimizeTests struct {\n+\t\tname                            string\n+\t\tfs                              []tsm1.FileStat\n+\t\tbc                              []int\n+\t\texpectedFullyCompactedReasonExp string\n+\t\texpectedgenerationCount         int64\n+\t}\n+\n+\tfurtherCompactedTests := []PlanOptimizeTests{\n+\t\t// Large multi generation group with files at and under 2GB\n+\t\t{\n+\t\t\t\"Large multi generation group with files at and under 2GB\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-05.tsm1\",\n+\t\t\t\t\tSize: 512 * 1024 * 1024,\n+\t\t\t\t},\n \t\t\t},\n-\t\t}, tsdb.DefaultCompactFullWriteColdDuration,\n-\t)\n-\n-\texpFiles := make([]tsm1.FileStat, 0)\n-\tfor _, file := range data {\n-\t\texpFiles = append(expFiles, file)\n-\t}\n-\n-\t_, cgLen := cp.PlanLevel(1)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\")\n-\t_, cgLen = cp.PlanLevel(2)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\")\n-\t_, cgLen = cp.PlanLevel(3)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\")\n-\n-\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n-\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\")\n-\trequire.Zero(t, pLenP, \"compaction group length; Plan()\")\n-\n-\ttsm, pLen, _ := cp.PlanOptimize()\n-\trequire.Equal(t, 1, len(tsm), \"compaction group\")\n-\trequire.Equal(t, int64(len(tsm)), pLen, \"compaction group length\")\n-\trequire.Equal(t, len(expFiles), len(tsm[0]), \"expected TSM files\")\n-}\n-\n-// This test is added to account for a single generation that has a group size\n-// under 2 GB so it should be further compacted to a single file.\n-func TestDefaultPlanner_PlanOptimize_SmallSingleGeneration(t *testing.T) {\n-\t// ~650 MB total group size\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-05.tsm1\",\n-\t\t\tSize: 300 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-06.tsm1\",\n-\t\t\tSize: 200 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-07.tsm1\",\n-\t\t\tSize: 100 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-08.tsm1\",\n-\t\t\tSize: 50 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n-\t\t\tPathsFn: func() []tsm1.FileStat {\n-\t\t\t\treturn data\n+\t\t\t[]int{},\n+\t\t\t\"not fully compacted and not idle because of more than one generation\",\n+\t\t\t3,\n+\t\t},\n+\t\t// ~650mb group size\n+\t\t{\n+\t\t\t\"Small group size with single generation\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n \t\t\t},\n-\t\t}, tsdb.DefaultCompactFullWriteColdDuration,\n-\t)\n-\n-\texpFiles := make([]tsm1.FileStat, 0)\n-\tfor _, file := range data {\n-\t\texpFiles = append(expFiles, file)\n-\t}\n-\n-\t_, cgLen := cp.PlanLevel(1)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\")\n-\t_, cgLen = cp.PlanLevel(2)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\")\n-\t_, cgLen = cp.PlanLevel(3)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\")\n-\n-\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n-\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\")\n-\trequire.Zero(t, pLenP, \"compaction group length; Plan()\")\n-\n-\ttsm, pLen, gLen := cp.PlanOptimize()\n-\trequire.Equal(t, 1, len(tsm), \"compaction group\")\n-\trequire.Equal(t, int64(len(tsm)), pLen, \"compaction group length\")\n-\trequire.Equal(t, int64(1), gLen, \"generation of TSM files\")\n-\trequire.Equal(t, len(expFiles), len(tsm[0]), \"expected TSM files\")\n-}\n-\n-// This test is added to account for a single generation that has a group size\n-// under 2 GB and has less then level 4 files it should be further compacted to a single file.\n-func TestDefaultPlanner_PlanOptimize_SmallSingleGenerationUnderLevel4(t *testing.T) {\n-\t// ~650 MB total group size\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-02.tsm1\",\n-\t\t\tSize: 300 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-03.tsm1\",\n-\t\t\tSize: 200 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-04.tsm1\",\n-\t\t\tSize: 100 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n+\t\t\t[]int{},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t// ~650 MB total group size with generations under 4\n+\t\t{\n+\t\t\t\"Small group size with single generation and levels under 4\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-02.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-03.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-04.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\t[]int{},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t\"Small group size with single generation all at DefaultMaxPointsPerBlock\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t// > 2 GB total group size\n+\t\t// 50% of files are at aggressive max block size\n+\t\t{\n+\t\t\t\"Small group size with single generation 50% at DefaultMaxPointsPerBlock and 50% at AggressiveMaxPointsPerBlock\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-09.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-10.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-11.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-12.tsm1\",\n+\t\t\t\t\tSize: 25 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\t[]int{\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t\"Group size over 2GB with single generation\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-13.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-14.tsm1\",\n+\t\t\t\t\tSize: 650 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-15.tsm1\",\n+\t\t\t\t\tSize: 450 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t// Last files are lower than first files generations\n+\t\t\t// Mix of less than 4 level files and > level 4 files\n+\t\t\t\"Generations with files under level 4\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 600 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-06.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{}, \"not fully compacted and not idle because of more than one generation\", 3,\n+\t\t},\n+\t\t{\n+\t\t\t// This test will mock a 'backfill' condition where we have a single\n+\t\t\t// shard with many generations. The initial generation should be fully\n+\t\t\t// compacted, but we have some new generations that are not. We need to ensure\n+\t\t\t// the optimize planner will pick these up and compact everything together.\n+\t\t\t\"Backfill mock condition\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-04.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-02.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"04-01.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"04-02.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t// Use some magic numbers but these are just small values for block counts\n+\t\t\t100,\n+\t\t\t10,\n+\t\t},\n+\t\t\t\"not fully compacted and not idle because of more than one generation\",\n+\t\t\t4,\n+\t\t},\n+\t}\n+\n+\texpectedNotFullyCompacted := func(cp *tsm1.DefaultPlanner, reasonExp string, generationCountExp int64, testName string) {\n+\t\tcompacted, reason := cp.FullyCompacted()\n+\t\trequire.Equal(t, reason, reasonExp, \"fullyCompacted reason\", testName)\n+\t\trequire.False(t, compacted, \"is fully compacted\", testName)\n+\n+\t\t_, cgLen := cp.PlanLevel(1)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\", testName)\n+\t\t_, cgLen = cp.PlanLevel(2)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\", testName)\n+\t\t_, cgLen = cp.PlanLevel(3)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\", testName)\n+\n+\t\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n+\t\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\", testName)\n+\t\trequire.Zero(t, pLenP, \"compaction group length; Plan()\", testName)\n+\n+\t\t_, cgLen, genLen := cp.PlanOptimize()\n+\t\trequire.Equal(t, int64(1), cgLen, \"compaction group length\", testName)\n+\t\trequire.Equal(t, generationCountExp, genLen, \"generation count\", testName)\n+\n+\t}\n+\n+\tfor _, test := range furtherCompactedTests {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1914209553",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25594,
        "pr_file": "tsdb/engine/tsm1/compact_test.go",
        "discussion_id": "1914209553",
        "commented_code": "@@ -2269,837 +2270,510 @@ func TestDefaultPlanner_PlanOptimize_NoLevel4(t *testing.T) {\n \t}\n }\n \n-// This test is added to acount for many TSM files within a group being over 2 GB\n-// we want to ensure that the shard will be planned.\n-func TestDefaultPlanner_PlanOptimize_LargeMultiGeneration(t *testing.T) {\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-05.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-06.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-07.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-08.tsm1\",\n-\t\t\tSize: 1048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-05.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-06.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-07.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-08.tsm1\",\n-\t\t\tSize: 1048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"03-04.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"03-05.tsm1\",\n-\t\t\tSize: 512 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n-\t\t\tPathsFn: func() []tsm1.FileStat {\n-\t\t\t\treturn data\n+func TestDefaultPlanner_PlanOptimize_Test(t *testing.T) {\n+\t// bc are the block counts for the tsm files\n+\t// bc can be an empty list\n+\ttype PlanOptimizeTests struct {\n+\t\tname                            string\n+\t\tfs                              []tsm1.FileStat\n+\t\tbc                              []int\n+\t\texpectedFullyCompactedReasonExp string\n+\t\texpectedgenerationCount         int64\n+\t}\n+\n+\tfurtherCompactedTests := []PlanOptimizeTests{\n+\t\t// Large multi generation group with files at and under 2GB\n+\t\t{\n+\t\t\t\"Large multi generation group with files at and under 2GB\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-05.tsm1\",\n+\t\t\t\t\tSize: 512 * 1024 * 1024,\n+\t\t\t\t},\n \t\t\t},\n-\t\t}, tsdb.DefaultCompactFullWriteColdDuration,\n-\t)\n-\n-\texpFiles := make([]tsm1.FileStat, 0)\n-\tfor _, file := range data {\n-\t\texpFiles = append(expFiles, file)\n-\t}\n-\n-\t_, cgLen := cp.PlanLevel(1)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\")\n-\t_, cgLen = cp.PlanLevel(2)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\")\n-\t_, cgLen = cp.PlanLevel(3)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\")\n-\n-\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n-\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\")\n-\trequire.Zero(t, pLenP, \"compaction group length; Plan()\")\n-\n-\ttsm, pLen, _ := cp.PlanOptimize()\n-\trequire.Equal(t, 1, len(tsm), \"compaction group\")\n-\trequire.Equal(t, int64(len(tsm)), pLen, \"compaction group length\")\n-\trequire.Equal(t, len(expFiles), len(tsm[0]), \"expected TSM files\")\n-}\n-\n-// This test is added to account for a single generation that has a group size\n-// under 2 GB so it should be further compacted to a single file.\n-func TestDefaultPlanner_PlanOptimize_SmallSingleGeneration(t *testing.T) {\n-\t// ~650 MB total group size\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-05.tsm1\",\n-\t\t\tSize: 300 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-06.tsm1\",\n-\t\t\tSize: 200 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-07.tsm1\",\n-\t\t\tSize: 100 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-08.tsm1\",\n-\t\t\tSize: 50 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n-\t\t\tPathsFn: func() []tsm1.FileStat {\n-\t\t\t\treturn data\n+\t\t\t[]int{},\n+\t\t\t\"not fully compacted and not idle because of more than one generation\",\n+\t\t\t3,\n+\t\t},\n+\t\t// ~650mb group size\n+\t\t{\n+\t\t\t\"Small group size with single generation\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n \t\t\t},\n-\t\t}, tsdb.DefaultCompactFullWriteColdDuration,\n-\t)\n-\n-\texpFiles := make([]tsm1.FileStat, 0)\n-\tfor _, file := range data {\n-\t\texpFiles = append(expFiles, file)\n-\t}\n-\n-\t_, cgLen := cp.PlanLevel(1)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\")\n-\t_, cgLen = cp.PlanLevel(2)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\")\n-\t_, cgLen = cp.PlanLevel(3)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\")\n-\n-\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n-\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\")\n-\trequire.Zero(t, pLenP, \"compaction group length; Plan()\")\n-\n-\ttsm, pLen, gLen := cp.PlanOptimize()\n-\trequire.Equal(t, 1, len(tsm), \"compaction group\")\n-\trequire.Equal(t, int64(len(tsm)), pLen, \"compaction group length\")\n-\trequire.Equal(t, int64(1), gLen, \"generation of TSM files\")\n-\trequire.Equal(t, len(expFiles), len(tsm[0]), \"expected TSM files\")\n-}\n-\n-// This test is added to account for a single generation that has a group size\n-// under 2 GB and has less then level 4 files it should be further compacted to a single file.\n-func TestDefaultPlanner_PlanOptimize_SmallSingleGenerationUnderLevel4(t *testing.T) {\n-\t// ~650 MB total group size\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-02.tsm1\",\n-\t\t\tSize: 300 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-03.tsm1\",\n-\t\t\tSize: 200 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-04.tsm1\",\n-\t\t\tSize: 100 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n+\t\t\t[]int{},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t// ~650 MB total group size with generations under 4\n+\t\t{\n+\t\t\t\"Small group size with single generation and levels under 4\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-02.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-03.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-04.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\t[]int{},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t\"Small group size with single generation all at DefaultMaxPointsPerBlock\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t// > 2 GB total group size\n+\t\t// 50% of files are at aggressive max block size\n+\t\t{\n+\t\t\t\"Small group size with single generation 50% at DefaultMaxPointsPerBlock and 50% at AggressiveMaxPointsPerBlock\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-09.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-10.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-11.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-12.tsm1\",\n+\t\t\t\t\tSize: 25 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\t[]int{\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t\"Group size over 2GB with single generation\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-13.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-14.tsm1\",\n+\t\t\t\t\tSize: 650 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-15.tsm1\",\n+\t\t\t\t\tSize: 450 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t// Last files are lower than first files generations\n+\t\t\t// Mix of less than 4 level files and > level 4 files\n+\t\t\t\"Generations with files under level 4\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 600 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-06.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{}, \"not fully compacted and not idle because of more than one generation\", 3,\n+\t\t},\n+\t\t{\n+\t\t\t// This test will mock a 'backfill' condition where we have a single\n+\t\t\t// shard with many generations. The initial generation should be fully\n+\t\t\t// compacted, but we have some new generations that are not. We need to ensure\n+\t\t\t// the optimize planner will pick these up and compact everything together.\n+\t\t\t\"Backfill mock condition\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-04.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-02.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"04-01.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"04-02.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t// Use some magic numbers but these are just small values for block counts\n+\t\t\t100,\n+\t\t\t10,\n+\t\t},\n+\t\t\t\"not fully compacted and not idle because of more than one generation\",\n+\t\t\t4,\n+\t\t},\n+\t}\n+\n+\texpectedNotFullyCompacted := func(cp *tsm1.DefaultPlanner, reasonExp string, generationCountExp int64, testName string) {\n+\t\tcompacted, reason := cp.FullyCompacted()\n+\t\trequire.Equal(t, reason, reasonExp, \"fullyCompacted reason\", testName)\n+\t\trequire.False(t, compacted, \"is fully compacted\", testName)\n+\n+\t\t_, cgLen := cp.PlanLevel(1)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\", testName)\n+\t\t_, cgLen = cp.PlanLevel(2)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\", testName)\n+\t\t_, cgLen = cp.PlanLevel(3)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\", testName)\n+\n+\t\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n+\t\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\", testName)\n+\t\trequire.Zero(t, pLenP, \"compaction group length; Plan()\", testName)\n+\n+\t\t_, cgLen, genLen := cp.PlanOptimize()\n+\t\trequire.Equal(t, int64(1), cgLen, \"compaction group length\", testName)\n+\t\trequire.Equal(t, generationCountExp, genLen, \"generation count\", testName)\n+\n+\t}\n+\n+\tfor _, test := range furtherCompactedTests {",
        "comment_created_at": "2025-01-14T04:00:03+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "Would [testing.T.Run](https://pkg.go.dev/testing#T.Run) be better here?  Sometimes it can complicate things, and if it doesn't help, there's no need to switch.\r\n\r\nInterestingly, you can see a minimal example of `testing.T.Run` in the `testify /require` [documentation](https://pkg.go.dev/github.com/stretchr/testify/require#example-BoolAssertionFunc), or various places in our code.",
        "pr_file_module": null
      },
      {
        "comment_id": "1914211069",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25594,
        "pr_file": "tsdb/engine/tsm1/compact_test.go",
        "discussion_id": "1914209553",
        "commented_code": "@@ -2269,837 +2270,510 @@ func TestDefaultPlanner_PlanOptimize_NoLevel4(t *testing.T) {\n \t}\n }\n \n-// This test is added to acount for many TSM files within a group being over 2 GB\n-// we want to ensure that the shard will be planned.\n-func TestDefaultPlanner_PlanOptimize_LargeMultiGeneration(t *testing.T) {\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-05.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-06.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-07.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-08.tsm1\",\n-\t\t\tSize: 1048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-05.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-06.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-07.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"02-08.tsm1\",\n-\t\t\tSize: 1048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"03-04.tsm1\",\n-\t\t\tSize: 2048 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"03-05.tsm1\",\n-\t\t\tSize: 512 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n-\t\t\tPathsFn: func() []tsm1.FileStat {\n-\t\t\t\treturn data\n+func TestDefaultPlanner_PlanOptimize_Test(t *testing.T) {\n+\t// bc are the block counts for the tsm files\n+\t// bc can be an empty list\n+\ttype PlanOptimizeTests struct {\n+\t\tname                            string\n+\t\tfs                              []tsm1.FileStat\n+\t\tbc                              []int\n+\t\texpectedFullyCompactedReasonExp string\n+\t\texpectedgenerationCount         int64\n+\t}\n+\n+\tfurtherCompactedTests := []PlanOptimizeTests{\n+\t\t// Large multi generation group with files at and under 2GB\n+\t\t{\n+\t\t\t\"Large multi generation group with files at and under 2GB\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-05.tsm1\",\n+\t\t\t\t\tSize: 512 * 1024 * 1024,\n+\t\t\t\t},\n \t\t\t},\n-\t\t}, tsdb.DefaultCompactFullWriteColdDuration,\n-\t)\n-\n-\texpFiles := make([]tsm1.FileStat, 0)\n-\tfor _, file := range data {\n-\t\texpFiles = append(expFiles, file)\n-\t}\n-\n-\t_, cgLen := cp.PlanLevel(1)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\")\n-\t_, cgLen = cp.PlanLevel(2)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\")\n-\t_, cgLen = cp.PlanLevel(3)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\")\n-\n-\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n-\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\")\n-\trequire.Zero(t, pLenP, \"compaction group length; Plan()\")\n-\n-\ttsm, pLen, _ := cp.PlanOptimize()\n-\trequire.Equal(t, 1, len(tsm), \"compaction group\")\n-\trequire.Equal(t, int64(len(tsm)), pLen, \"compaction group length\")\n-\trequire.Equal(t, len(expFiles), len(tsm[0]), \"expected TSM files\")\n-}\n-\n-// This test is added to account for a single generation that has a group size\n-// under 2 GB so it should be further compacted to a single file.\n-func TestDefaultPlanner_PlanOptimize_SmallSingleGeneration(t *testing.T) {\n-\t// ~650 MB total group size\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-05.tsm1\",\n-\t\t\tSize: 300 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-06.tsm1\",\n-\t\t\tSize: 200 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-07.tsm1\",\n-\t\t\tSize: 100 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-08.tsm1\",\n-\t\t\tSize: 50 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n-\t\t\tPathsFn: func() []tsm1.FileStat {\n-\t\t\t\treturn data\n+\t\t\t[]int{},\n+\t\t\t\"not fully compacted and not idle because of more than one generation\",\n+\t\t\t3,\n+\t\t},\n+\t\t// ~650mb group size\n+\t\t{\n+\t\t\t\"Small group size with single generation\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n \t\t\t},\n-\t\t}, tsdb.DefaultCompactFullWriteColdDuration,\n-\t)\n-\n-\texpFiles := make([]tsm1.FileStat, 0)\n-\tfor _, file := range data {\n-\t\texpFiles = append(expFiles, file)\n-\t}\n-\n-\t_, cgLen := cp.PlanLevel(1)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\")\n-\t_, cgLen = cp.PlanLevel(2)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\")\n-\t_, cgLen = cp.PlanLevel(3)\n-\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\")\n-\n-\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n-\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\")\n-\trequire.Zero(t, pLenP, \"compaction group length; Plan()\")\n-\n-\ttsm, pLen, gLen := cp.PlanOptimize()\n-\trequire.Equal(t, 1, len(tsm), \"compaction group\")\n-\trequire.Equal(t, int64(len(tsm)), pLen, \"compaction group length\")\n-\trequire.Equal(t, int64(1), gLen, \"generation of TSM files\")\n-\trequire.Equal(t, len(expFiles), len(tsm[0]), \"expected TSM files\")\n-}\n-\n-// This test is added to account for a single generation that has a group size\n-// under 2 GB and has less then level 4 files it should be further compacted to a single file.\n-func TestDefaultPlanner_PlanOptimize_SmallSingleGenerationUnderLevel4(t *testing.T) {\n-\t// ~650 MB total group size\n-\tdata := []tsm1.FileStat{\n-\t\t{\n-\t\t\tPath: \"01-02.tsm1\",\n-\t\t\tSize: 300 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-03.tsm1\",\n-\t\t\tSize: 200 * 1024 * 1024,\n-\t\t},\n-\t\t{\n-\t\t\tPath: \"01-04.tsm1\",\n-\t\t\tSize: 100 * 1024 * 1024,\n-\t\t},\n-\t}\n-\n-\tcp := tsm1.NewDefaultPlanner(\n-\t\t&fakeFileStore{\n+\t\t\t[]int{},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t// ~650 MB total group size with generations under 4\n+\t\t{\n+\t\t\t\"Small group size with single generation and levels under 4\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-02.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-03.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-04.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\t[]int{},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t\"Small group size with single generation all at DefaultMaxPointsPerBlock\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock, tsdb.DefaultMaxPointsPerBlock},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t// > 2 GB total group size\n+\t\t// 50% of files are at aggressive max block size\n+\t\t{\n+\t\t\t\"Small group size with single generation 50% at DefaultMaxPointsPerBlock and 50% at AggressiveMaxPointsPerBlock\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 300 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-09.tsm1\",\n+\t\t\t\t\tSize: 200 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-10.tsm1\",\n+\t\t\t\t\tSize: 100 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-11.tsm1\",\n+\t\t\t\t\tSize: 50 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-12.tsm1\",\n+\t\t\t\t\tSize: 25 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\t[]int{\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t\"Group size over 2GB with single generation\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-13.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-14.tsm1\",\n+\t\t\t\t\tSize: 650 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-15.tsm1\",\n+\t\t\t\t\tSize: 450 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t},\n+\t\t\ttsdb.SingleGenerationReasonText,\n+\t\t\t1,\n+\t\t},\n+\t\t{\n+\t\t\t// Last files are lower than first files generations\n+\t\t\t// Mix of less than 4 level files and > level 4 files\n+\t\t\t\"Generations with files under level 4\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-08.tsm1\",\n+\t\t\t\t\tSize: 1048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 600 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-06.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{}, \"not fully compacted and not idle because of more than one generation\", 3,\n+\t\t},\n+\t\t{\n+\t\t\t// This test will mock a 'backfill' condition where we have a single\n+\t\t\t// shard with many generations. The initial generation should be fully\n+\t\t\t// compacted, but we have some new generations that are not. We need to ensure\n+\t\t\t// the optimize planner will pick these up and compact everything together.\n+\t\t\t\"Backfill mock condition\",\n+\t\t\t[]tsm1.FileStat{\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-05.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-06.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"01-07.tsm1\",\n+\t\t\t\t\tSize: 2048 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-04.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-05.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"02-06.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-02.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-04.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"04-01.tsm1\",\n+\t\t\t\t\tSize: 700 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"04-02.tsm1\",\n+\t\t\t\t\tSize: 500 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tPath: \"03-03.tsm1\",\n+\t\t\t\t\tSize: 400 * 1024 * 1024,\n+\t\t\t\t},\n+\t\t\t}, []int{\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.AggressiveMaxPointsPerBlock,\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\n+\t\t\ttsdb.DefaultMaxPointsPerBlock,\n+\t\t\t// Use some magic numbers but these are just small values for block counts\n+\t\t\t100,\n+\t\t\t10,\n+\t\t},\n+\t\t\t\"not fully compacted and not idle because of more than one generation\",\n+\t\t\t4,\n+\t\t},\n+\t}\n+\n+\texpectedNotFullyCompacted := func(cp *tsm1.DefaultPlanner, reasonExp string, generationCountExp int64, testName string) {\n+\t\tcompacted, reason := cp.FullyCompacted()\n+\t\trequire.Equal(t, reason, reasonExp, \"fullyCompacted reason\", testName)\n+\t\trequire.False(t, compacted, \"is fully compacted\", testName)\n+\n+\t\t_, cgLen := cp.PlanLevel(1)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(1)\", testName)\n+\t\t_, cgLen = cp.PlanLevel(2)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(2)\", testName)\n+\t\t_, cgLen = cp.PlanLevel(3)\n+\t\trequire.Zero(t, cgLen, \"compaction group length; PlanLevel(3)\", testName)\n+\n+\t\ttsmP, pLenP := cp.Plan(time.Now().Add(-time.Second))\n+\t\trequire.Zero(t, len(tsmP), \"compaction group; Plan()\", testName)\n+\t\trequire.Zero(t, pLenP, \"compaction group length; Plan()\", testName)\n+\n+\t\t_, cgLen, genLen := cp.PlanOptimize()\n+\t\trequire.Equal(t, int64(1), cgLen, \"compaction group length\", testName)\n+\t\trequire.Equal(t, generationCountExp, genLen, \"generation count\", testName)\n+\n+\t}\n+\n+\tfor _, test := range furtherCompactedTests {",
        "comment_created_at": "2025-01-14T04:03:46+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "For example: [in the middleware](https://github.com/influxdata/influxdb/blob/7999835ac39fe9e197dd1c8024b1cd52cd3fca31/kit/transport/http/middleware_test.go#L13C1-L47C2)\r\n\r\nI should have mentioned this in my last review.  Not a critical change, particularly if it complicates your test structure.  But, it gets rid of all the `testName` arguments to your `require.` methods (which may need formatting?).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1803830280",
    "pr_number": 25398,
    "pr_file": "tsdb/store_test.go",
    "created_at": "2024-10-16T21:40:07+00:00",
    "commented_code": "}\n }\n \n+func TestStore_BadShardClear(t *testing.T) {\n+\tconst errStr = \"a shard open error\"\n+\tindexes := tsdb.RegisteredIndexes()\n+\tfor _, idx := range indexes {\n+\t\tfunc() {\n+\t\t\ts := MustOpenStore(t, idx)\n+\t\t\tdefer require.NoErrorf(t, s.Close(), \"closing store with index type: %s\", idx)\n+\n+\t\t\tsh := tsdb.NewTempShard(idx)\n+\t\t\tshId := sh.ID()\n+\t\t\terr := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.NoError(t, err, \"opening temp shard\")\n+\t\t\tdefer require.NoError(t, sh.Close(), \"closing temporary shard\")\n+\n+\t\t\texpErr := errors.New(errStr)\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))\n+\n+\t\t\tbadShards := s.ClearBadShardList()\n+\t\t\tif len(badShards) != 1 {\n+\t\t\t\tt.Fatalf(\"expected 1 shard, got %d\", len(badShards))\n+\t\t\t}\n+\n+\t\t\t// Check that bad shard list has been cleared\n+\t\t\trequire.Equal(t, 0, len(s.Store.GetBadShardList()))",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1803830280",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25398,
        "pr_file": "tsdb/store_test.go",
        "discussion_id": "1803830280",
        "commented_code": "@@ -262,6 +262,74 @@ func TestStore_BadShard(t *testing.T) {\n \t}\n }\n \n+func TestStore_BadShardClear(t *testing.T) {\n+\tconst errStr = \"a shard open error\"\n+\tindexes := tsdb.RegisteredIndexes()\n+\tfor _, idx := range indexes {\n+\t\tfunc() {\n+\t\t\ts := MustOpenStore(t, idx)\n+\t\t\tdefer require.NoErrorf(t, s.Close(), \"closing store with index type: %s\", idx)\n+\n+\t\t\tsh := tsdb.NewTempShard(idx)\n+\t\t\tshId := sh.ID()\n+\t\t\terr := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.NoError(t, err, \"opening temp shard\")\n+\t\t\tdefer require.NoError(t, sh.Close(), \"closing temporary shard\")\n+\n+\t\t\texpErr := errors.New(errStr)\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))\n+\n+\t\t\tbadShards := s.ClearBadShardList()\n+\t\t\tif len(badShards) != 1 {\n+\t\t\t\tt.Fatalf(\"expected 1 shard, got %d\", len(badShards))\n+\t\t\t}\n+\n+\t\t\t// Check that bad shard list has been cleared\n+\t\t\trequire.Equal(t, 0, len(s.Store.GetBadShardList()))",
        "comment_created_at": "2024-10-16T21:40:07+00:00",
        "comment_author": "gwossum",
        "comment_body": "Even though this uses `require` and works, `require.Empty(t, s.GetBadShardList)` is better because it will output what was in the slice / map if it isn't actually empty. This makes debugging issues simpler.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1803831037",
    "pr_number": 25398,
    "pr_file": "tsdb/store_test.go",
    "created_at": "2024-10-16T21:40:51+00:00",
    "commented_code": "}\n }\n \n+func TestStore_BadShardClear(t *testing.T) {\n+\tconst errStr = \"a shard open error\"\n+\tindexes := tsdb.RegisteredIndexes()\n+\tfor _, idx := range indexes {\n+\t\tfunc() {\n+\t\t\ts := MustOpenStore(t, idx)\n+\t\t\tdefer require.NoErrorf(t, s.Close(), \"closing store with index type: %s\", idx)\n+\n+\t\t\tsh := tsdb.NewTempShard(idx)\n+\t\t\tshId := sh.ID()\n+\t\t\terr := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.NoError(t, err, \"opening temp shard\")\n+\t\t\tdefer require.NoError(t, sh.Close(), \"closing temporary shard\")\n+\n+\t\t\texpErr := errors.New(errStr)\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))\n+\n+\t\t\tbadShards := s.ClearBadShardList()\n+\t\t\tif len(badShards) != 1 {\n+\t\t\t\tt.Fatalf(\"expected 1 shard, got %d\", len(badShards))\n+\t\t\t}\n+\n+\t\t\t// Check that bad shard list has been cleared\n+\t\t\trequire.Equal(t, 0, len(s.Store.GetBadShardList()))\n+\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 = s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\t// Check that bad shard list now has a bad shard in it\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1803831037",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25398,
        "pr_file": "tsdb/store_test.go",
        "discussion_id": "1803831037",
        "commented_code": "@@ -262,6 +262,74 @@ func TestStore_BadShard(t *testing.T) {\n \t}\n }\n \n+func TestStore_BadShardClear(t *testing.T) {\n+\tconst errStr = \"a shard open error\"\n+\tindexes := tsdb.RegisteredIndexes()\n+\tfor _, idx := range indexes {\n+\t\tfunc() {\n+\t\t\ts := MustOpenStore(t, idx)\n+\t\t\tdefer require.NoErrorf(t, s.Close(), \"closing store with index type: %s\", idx)\n+\n+\t\t\tsh := tsdb.NewTempShard(idx)\n+\t\t\tshId := sh.ID()\n+\t\t\terr := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.NoError(t, err, \"opening temp shard\")\n+\t\t\tdefer require.NoError(t, sh.Close(), \"closing temporary shard\")\n+\n+\t\t\texpErr := errors.New(errStr)\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))\n+\n+\t\t\tbadShards := s.ClearBadShardList()\n+\t\t\tif len(badShards) != 1 {\n+\t\t\t\tt.Fatalf(\"expected 1 shard, got %d\", len(badShards))\n+\t\t\t}\n+\n+\t\t\t// Check that bad shard list has been cleared\n+\t\t\trequire.Equal(t, 0, len(s.Store.GetBadShardList()))\n+\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 = s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\t// Check that bad shard list now has a bad shard in it\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))",
        "comment_created_at": "2024-10-16T21:40:51+00:00",
        "comment_author": "gwossum",
        "comment_body": "Use `require.Len(t, s.GetBadShardList, 1)` so the contents of map will be printed out if there isn't exactly 1 item in the list.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1803831780",
    "pr_number": 25398,
    "pr_file": "tsdb/store_test.go",
    "created_at": "2024-10-16T21:41:38+00:00",
    "commented_code": "}\n }\n \n+func TestStore_BadShardClear(t *testing.T) {\n+\tconst errStr = \"a shard open error\"\n+\tindexes := tsdb.RegisteredIndexes()\n+\tfor _, idx := range indexes {\n+\t\tfunc() {\n+\t\t\ts := MustOpenStore(t, idx)\n+\t\t\tdefer require.NoErrorf(t, s.Close(), \"closing store with index type: %s\", idx)\n+\n+\t\t\tsh := tsdb.NewTempShard(idx)\n+\t\t\tshId := sh.ID()\n+\t\t\terr := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.NoError(t, err, \"opening temp shard\")\n+\t\t\tdefer require.NoError(t, sh.Close(), \"closing temporary shard\")\n+\n+\t\t\texpErr := errors.New(errStr)\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))\n+\n+\t\t\tbadShards := s.ClearBadShardList()\n+\t\t\tif len(badShards) != 1 {\n+\t\t\t\tt.Fatalf(\"expected 1 shard, got %d\", len(badShards))\n+\t\t\t}\n+\n+\t\t\t// Check that bad shard list has been cleared\n+\t\t\trequire.Equal(t, 0, len(s.Store.GetBadShardList()))\n+\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 = s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\t// Check that bad shard list now has a bad shard in it\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))\n+\t\t}()\n+\t}\n+}\n+\n+func TestStore_BadShardClearNoBadShards(t *testing.T) {\n+\tindexes := tsdb.RegisteredIndexes()\n+\tfor _, idx := range indexes {\n+\t\tfunc() {\n+\t\t\ts := MustOpenStore(t, idx)\n+\t\t\tdefer require.NoErrorf(t, s.Close(), \"closing store with index type: %s\", idx)\n+\n+\t\t\tsh := tsdb.NewTempShard(idx)\n+\t\t\terr := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.NoError(t, err, \"opening temp shard\")\n+\t\t\tdefer require.NoError(t, sh.Close(), \"closing temporary shard\")\n+\n+\t\t\trequire.Equal(t, 0, len(s.Store.GetBadShardList()))",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1803831780",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25398,
        "pr_file": "tsdb/store_test.go",
        "discussion_id": "1803831780",
        "commented_code": "@@ -262,6 +262,74 @@ func TestStore_BadShard(t *testing.T) {\n \t}\n }\n \n+func TestStore_BadShardClear(t *testing.T) {\n+\tconst errStr = \"a shard open error\"\n+\tindexes := tsdb.RegisteredIndexes()\n+\tfor _, idx := range indexes {\n+\t\tfunc() {\n+\t\t\ts := MustOpenStore(t, idx)\n+\t\t\tdefer require.NoErrorf(t, s.Close(), \"closing store with index type: %s\", idx)\n+\n+\t\t\tsh := tsdb.NewTempShard(idx)\n+\t\t\tshId := sh.ID()\n+\t\t\terr := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.NoError(t, err, \"opening temp shard\")\n+\t\t\tdefer require.NoError(t, sh.Close(), \"closing temporary shard\")\n+\n+\t\t\texpErr := errors.New(errStr)\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))\n+\n+\t\t\tbadShards := s.ClearBadShardList()\n+\t\t\tif len(badShards) != 1 {\n+\t\t\t\tt.Fatalf(\"expected 1 shard, got %d\", len(badShards))\n+\t\t\t}\n+\n+\t\t\t// Check that bad shard list has been cleared\n+\t\t\trequire.Equal(t, 0, len(s.Store.GetBadShardList()))\n+\n+\t\t\ts.SetShardOpenErrorForTest(sh.ID(), expErr)\n+\t\t\terr2 = s.OpenShard(sh.Shard, false)\n+\t\t\trequire.Error(t, err2, \"no error opening bad shard\")\n+\t\t\trequire.True(t, errors.Is(err2, tsdb.ErrPreviousShardFail{}), \"exp: ErrPreviousShardFail, got: %v\", err2)\n+\t\t\trequire.EqualError(t, err2, fmt.Errorf(\"not attempting to open shard %d; opening shard previously failed with: %w\", shId, expErr).Error())\n+\n+\t\t\t// Check that bad shard list now has a bad shard in it\n+\t\t\trequire.Equal(t, 1, len(s.Store.GetBadShardList()))\n+\t\t}()\n+\t}\n+}\n+\n+func TestStore_BadShardClearNoBadShards(t *testing.T) {\n+\tindexes := tsdb.RegisteredIndexes()\n+\tfor _, idx := range indexes {\n+\t\tfunc() {\n+\t\t\ts := MustOpenStore(t, idx)\n+\t\t\tdefer require.NoErrorf(t, s.Close(), \"closing store with index type: %s\", idx)\n+\n+\t\t\tsh := tsdb.NewTempShard(idx)\n+\t\t\terr := s.OpenShard(sh.Shard, false)\n+\t\t\trequire.NoError(t, err, \"opening temp shard\")\n+\t\t\tdefer require.NoError(t, sh.Close(), \"closing temporary shard\")\n+\n+\t\t\trequire.Equal(t, 0, len(s.Store.GetBadShardList()))",
        "comment_created_at": "2024-10-16T21:41:38+00:00",
        "comment_author": "gwossum",
        "comment_body": "Use `require.Empty`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1776026253",
    "pr_number": 25378,
    "pr_file": "tsdb/store_test.go",
    "created_at": "2024-09-25T21:43:18+00:00",
    "commented_code": "}\n }\n \n+// Ensure the store can create a new shard.\n+func TestStore_StartupShardProgress(t *testing.T) {\n+\tt.Parallel()\n+\n+\ttest := func(index string) {\n+\t\tfmt.Println(index)\n+\t\ts := MustOpenStore(index)\n+\t\tdefer s.Close()\n+\n+\t\t// Create a new shard and verify that it exists.\n+\t\tif err := s.CreateShard(\"db0\", \"rp0\", 1, true); err != nil {\n+\t\t\tt.Fatal(err)",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1776026253",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25378,
        "pr_file": "tsdb/store_test.go",
        "discussion_id": "1776026253",
        "commented_code": "@@ -144,6 +145,64 @@ func TestStore_CreateShard(t *testing.T) {\n \t}\n }\n \n+// Ensure the store can create a new shard.\n+func TestStore_StartupShardProgress(t *testing.T) {\n+\tt.Parallel()\n+\n+\ttest := func(index string) {\n+\t\tfmt.Println(index)\n+\t\ts := MustOpenStore(index)\n+\t\tdefer s.Close()\n+\n+\t\t// Create a new shard and verify that it exists.\n+\t\tif err := s.CreateShard(\"db0\", \"rp0\", 1, true); err != nil {\n+\t\t\tt.Fatal(err)",
        "comment_created_at": "2024-09-25T21:43:18+00:00",
        "comment_author": "gwossum",
        "comment_body": "Use [testify](https://github.com/stretchr/testify). Then this would become `require.NoError(t, CreateShard(\"db0\", \"rp0\", 1, true))`.\r\n\r\nSame other places in here. If you're using a `t.Fatalf`, there's a superior `testify` replacement for it. You do see a lot of codr using `t.Fatalf`, but that's old code that hasn't been updated. New code should use `testify`.",
        "pr_file_module": null
      },
      {
        "comment_id": "1776030381",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25378,
        "pr_file": "tsdb/store_test.go",
        "discussion_id": "1776026253",
        "commented_code": "@@ -144,6 +145,64 @@ func TestStore_CreateShard(t *testing.T) {\n \t}\n }\n \n+// Ensure the store can create a new shard.\n+func TestStore_StartupShardProgress(t *testing.T) {\n+\tt.Parallel()\n+\n+\ttest := func(index string) {\n+\t\tfmt.Println(index)\n+\t\ts := MustOpenStore(index)\n+\t\tdefer s.Close()\n+\n+\t\t// Create a new shard and verify that it exists.\n+\t\tif err := s.CreateShard(\"db0\", \"rp0\", 1, true); err != nil {\n+\t\t\tt.Fatal(err)",
        "comment_created_at": "2024-09-25T21:48:26+00:00",
        "comment_author": "devanbenz",
        "comment_body": "Awesome sounds good",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1776030530",
    "pr_number": 25378,
    "pr_file": "tsdb/store_test.go",
    "created_at": "2024-09-25T21:48:38+00:00",
    "commented_code": "}\n }\n \n+// Ensure the store can create a new shard.\n+func TestStore_StartupShardProgress(t *testing.T) {\n+\tt.Parallel()\n+\n+\ttest := func(index string) {\n+\t\tfmt.Println(index)\n+\t\ts := MustOpenStore(index)\n+\t\tdefer s.Close()\n+\n+\t\t// Create a new shard and verify that it exists.\n+\t\tif err := s.CreateShard(\"db0\", \"rp0\", 1, true); err != nil {\n+\t\t\tt.Fatal(err)\n+\t\t} else if sh := s.Shard(1); sh == nil {\n+\t\t\tt.Fatalf(\"expected shard\")\n+\t\t}\n+\n+\t\t// Create another shard and verify that it exists.\n+\t\tif err := s.CreateShard(\"db0\", \"rp0\", 2, true); err != nil {\n+\t\t\tt.Fatal(err)\n+\t\t} else if sh := s.Shard(2); sh == nil {\n+\t\t\tt.Fatalf(\"expected shard\")\n+\t\t}\n+\n+\t\tmsl := &mockStartupLogger{}\n+\n+\t\t// Reopen shard and recheck.\n+\t\tif err := s.ReopenWithStartupMetrics(msl); err != nil {\n+\t\t\tt.Fatal(err)\n+\t\t} else if sh := s.Shard(1); sh == nil {\n+\t\t\tt.Fatalf(\"expected shard(1)\")\n+\t\t} else if sh = s.Shard(2); sh == nil {\n+\t\t\tt.Fatalf(\"expected shard(2)\")\n+\t\t}\n+\n+\t\tif msl.getShardsAdded() != 2 {\n+\t\t\tt.Fatalf(\"expected 2 shards added, got %d\", msl.getShardsAdded())\n+\t\t}\n+\n+\t\tif msl.getShardsCompleted() != 2 {\n+\t\t\tt.Fatalf(\"expected 2 shards completed, got %d\", msl.getShardsCompleted())\n+\t\t}\n+\n+\t\t// Equality check to make sure shards are always added prior to\n+\t\t// completion being called.\n+\t\treflect.DeepEqual(msl.shardTracker, []string{\n+\t\t\t\"shard-add\",\n+\t\t\t\"shard-add\",\n+\t\t\t\"shard-add\",\n+\t\t\t\"shard-complete\",\n+\t\t\t\"shard-complete\",\n+\t\t})",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1776030530",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25378,
        "pr_file": "tsdb/store_test.go",
        "discussion_id": "1776030530",
        "commented_code": "@@ -144,6 +145,64 @@ func TestStore_CreateShard(t *testing.T) {\n \t}\n }\n \n+// Ensure the store can create a new shard.\n+func TestStore_StartupShardProgress(t *testing.T) {\n+\tt.Parallel()\n+\n+\ttest := func(index string) {\n+\t\tfmt.Println(index)\n+\t\ts := MustOpenStore(index)\n+\t\tdefer s.Close()\n+\n+\t\t// Create a new shard and verify that it exists.\n+\t\tif err := s.CreateShard(\"db0\", \"rp0\", 1, true); err != nil {\n+\t\t\tt.Fatal(err)\n+\t\t} else if sh := s.Shard(1); sh == nil {\n+\t\t\tt.Fatalf(\"expected shard\")\n+\t\t}\n+\n+\t\t// Create another shard and verify that it exists.\n+\t\tif err := s.CreateShard(\"db0\", \"rp0\", 2, true); err != nil {\n+\t\t\tt.Fatal(err)\n+\t\t} else if sh := s.Shard(2); sh == nil {\n+\t\t\tt.Fatalf(\"expected shard\")\n+\t\t}\n+\n+\t\tmsl := &mockStartupLogger{}\n+\n+\t\t// Reopen shard and recheck.\n+\t\tif err := s.ReopenWithStartupMetrics(msl); err != nil {\n+\t\t\tt.Fatal(err)\n+\t\t} else if sh := s.Shard(1); sh == nil {\n+\t\t\tt.Fatalf(\"expected shard(1)\")\n+\t\t} else if sh = s.Shard(2); sh == nil {\n+\t\t\tt.Fatalf(\"expected shard(2)\")\n+\t\t}\n+\n+\t\tif msl.getShardsAdded() != 2 {\n+\t\t\tt.Fatalf(\"expected 2 shards added, got %d\", msl.getShardsAdded())\n+\t\t}\n+\n+\t\tif msl.getShardsCompleted() != 2 {\n+\t\t\tt.Fatalf(\"expected 2 shards completed, got %d\", msl.getShardsCompleted())\n+\t\t}\n+\n+\t\t// Equality check to make sure shards are always added prior to\n+\t\t// completion being called.\n+\t\treflect.DeepEqual(msl.shardTracker, []string{\n+\t\t\t\"shard-add\",\n+\t\t\t\"shard-add\",\n+\t\t\t\"shard-add\",\n+\t\t\t\"shard-complete\",\n+\t\t\t\"shard-complete\",\n+\t\t})",
        "comment_created_at": "2024-09-25T21:48:38+00:00",
        "comment_author": "gwossum",
        "comment_body": "There doesn't seem to be a check for if `DeepEqual` returned true or false. Anyway, this can be replaced with a `require.Equal` call.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1750990873",
    "pr_number": 25298,
    "pr_file": "tsdb/index/tsi1/partition_test.go",
    "created_at": "2024-09-09T21:59:47+00:00",
    "commented_code": "func TestPartition_PrependLogFile_Write_Fail(t *testing.T) {\n \tt.Run(\"write MANIFEST\", func(t *testing.T) {\n \t\tsfile := MustOpenSeriesFile()\n-\t\tdefer sfile.Close()\n+\t\tt.Cleanup(func() { sfile.Close() })\n \n \t\tp := MustOpenPartition(sfile.SeriesFile)\n-\t\tdefer func() {\n+\t\tt.Cleanup(func() {",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1750990873",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25298,
        "pr_file": "tsdb/index/tsi1/partition_test.go",
        "discussion_id": "1750990873",
        "commented_code": "@@ -98,14 +100,14 @@ func TestPartition_Manifest_Write_Fail(t *testing.T) {\n func TestPartition_PrependLogFile_Write_Fail(t *testing.T) {\n \tt.Run(\"write MANIFEST\", func(t *testing.T) {\n \t\tsfile := MustOpenSeriesFile()\n-\t\tdefer sfile.Close()\n+\t\tt.Cleanup(func() { sfile.Close() })\n \n \t\tp := MustOpenPartition(sfile.SeriesFile)\n-\t\tdefer func() {\n+\t\tt.Cleanup(func() {",
        "comment_created_at": "2024-09-09T21:59:47+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "Can we use this pattern in more of the calls to `t.Cleanup`?",
        "pr_file_module": null
      },
      {
        "comment_id": "1751028770",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25298,
        "pr_file": "tsdb/index/tsi1/partition_test.go",
        "discussion_id": "1750990873",
        "commented_code": "@@ -98,14 +100,14 @@ func TestPartition_Manifest_Write_Fail(t *testing.T) {\n func TestPartition_PrependLogFile_Write_Fail(t *testing.T) {\n \tt.Run(\"write MANIFEST\", func(t *testing.T) {\n \t\tsfile := MustOpenSeriesFile()\n-\t\tdefer sfile.Close()\n+\t\tt.Cleanup(func() { sfile.Close() })\n \n \t\tp := MustOpenPartition(sfile.SeriesFile)\n-\t\tdefer func() {\n+\t\tt.Cleanup(func() {",
        "comment_created_at": "2024-09-09T22:51:22+00:00",
        "comment_author": "devanbenz",
        "comment_body": "\ud83d\udc4d Sounds good I'll go ahead and adjust",
        "pr_file_module": null
      }
    ]
  }
]