[
  {
    "discussion_id": "1899855719",
    "pr_number": 25710,
    "pr_file": "query/executor.go",
    "created_at": "2024-12-31T00:03:09+00:00",
    "commented_code": "e.TaskManager.Logger = e.Logger\n }\n \n+func initQueryLogWriter(log *zap.Logger, e *Executor, path string) (*os.File, error) {\n+\tlogFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_APPEND, 0644)\n+\tif err != nil {\n+\t\te.Logger.Error(\"failed to open log file\", zap.Error(err))",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1899855719",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25710,
        "pr_file": "query/executor.go",
        "discussion_id": "1899855719",
        "commented_code": "@@ -289,6 +292,98 @@ func (e *Executor) WithLogger(log *zap.Logger) {\n \te.TaskManager.Logger = e.Logger\n }\n \n+func initQueryLogWriter(log *zap.Logger, e *Executor, path string) (*os.File, error) {\n+\tlogFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_APPEND, 0644)\n+\tif err != nil {\n+\t\te.Logger.Error(\"failed to open log file\", zap.Error(err))",
        "comment_created_at": "2024-12-31T00:03:09+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "`fmt.Errorf` to put the file path into the error. It can help to pinpoint the problem",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1904787273",
    "pr_number": 25710,
    "pr_file": "query/file_log_watcher.go",
    "created_at": "2025-01-07T01:01:19+00:00",
    "commented_code": "+package query\n+\n+import (\n+\t\"os\"\n+\t\"sync\"\n+\n+\t\"github.com/fsnotify/fsnotify\"\n+\tl \"github.com/influxdata/influxdb/logger\"\n+\t\"go.uber.org/zap\"\n+\t\"go.uber.org/zap/zapcore\"\n+)\n+\n+type FileLogWatcher struct {\n+\tpath            string\n+\tcurrFile        *os.File\n+\tlogger          *zap.Logger\n+\tformatterConfig string\n+\texecutor        *Executor\n+\tmu              sync.Mutex\n+\tfsWatcher       *fsnotify.Watcher\n+}\n+\n+func (f *FileLogWatcher) Event() chan fsnotify.Event {\n+\treturn f.fsWatcher.Events\n+}\n+\n+func (f *FileLogWatcher) Error() chan error {\n+\treturn f.fsWatcher.Errors\n+}\n+\n+func (f *FileLogWatcher) Add(name string) error {\n+\tf.mu.Lock()\n+\tdefer f.mu.Unlock()\n+\terr := f.fsWatcher.Add(name)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn nil\n+}\n+\n+func (f *FileLogWatcher) Remove(name string) error {\n+\tf.mu.Lock()\n+\tdefer f.mu.Unlock()\n+\terr := f.fsWatcher.Remove(name)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn nil\n+}\n+\n+func NewFileLogWatcher(e *Executor, path string, logger *zap.Logger, format string) *FileLogWatcher {\n+\t// FileWatcher is not meant to be used with interactive shell sessions so\n+\t// console as a logging format will not be supported.\n+\tif format == \"console\" {\n+\t\tlogger.Error(\"unknown logging format\", zap.String(\"format\", format), zap.String(\"path\", path))\n+\t\treturn nil\n+\t}\n+\n+\tif format == \"\" || format == \"auto\" {\n+\t\tformat = \"logfmt\"\n+\t}\n+\n+\tlogFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_APPEND, 0644)\n+\tif err != nil {\n+\t\tlogger.Error(\"failed to open log file\", zap.Error(err))",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1904787273",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25710,
        "pr_file": "query/file_log_watcher.go",
        "discussion_id": "1904787273",
        "commented_code": "@@ -0,0 +1,164 @@\n+package query\n+\n+import (\n+\t\"os\"\n+\t\"sync\"\n+\n+\t\"github.com/fsnotify/fsnotify\"\n+\tl \"github.com/influxdata/influxdb/logger\"\n+\t\"go.uber.org/zap\"\n+\t\"go.uber.org/zap/zapcore\"\n+)\n+\n+type FileLogWatcher struct {\n+\tpath            string\n+\tcurrFile        *os.File\n+\tlogger          *zap.Logger\n+\tformatterConfig string\n+\texecutor        *Executor\n+\tmu              sync.Mutex\n+\tfsWatcher       *fsnotify.Watcher\n+}\n+\n+func (f *FileLogWatcher) Event() chan fsnotify.Event {\n+\treturn f.fsWatcher.Events\n+}\n+\n+func (f *FileLogWatcher) Error() chan error {\n+\treturn f.fsWatcher.Errors\n+}\n+\n+func (f *FileLogWatcher) Add(name string) error {\n+\tf.mu.Lock()\n+\tdefer f.mu.Unlock()\n+\terr := f.fsWatcher.Add(name)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn nil\n+}\n+\n+func (f *FileLogWatcher) Remove(name string) error {\n+\tf.mu.Lock()\n+\tdefer f.mu.Unlock()\n+\terr := f.fsWatcher.Remove(name)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn nil\n+}\n+\n+func NewFileLogWatcher(e *Executor, path string, logger *zap.Logger, format string) *FileLogWatcher {\n+\t// FileWatcher is not meant to be used with interactive shell sessions so\n+\t// console as a logging format will not be supported.\n+\tif format == \"console\" {\n+\t\tlogger.Error(\"unknown logging format\", zap.String(\"format\", format), zap.String(\"path\", path))\n+\t\treturn nil\n+\t}\n+\n+\tif format == \"\" || format == \"auto\" {\n+\t\tformat = \"logfmt\"\n+\t}\n+\n+\tlogFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR|os.O_APPEND, 0644)\n+\tif err != nil {\n+\t\tlogger.Error(\"failed to open log file\", zap.Error(err))",
        "comment_created_at": "2025-01-07T01:01:19+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "add `path` to errors.  Some file system errors do not include it. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1755600609",
    "pr_number": 25314,
    "pr_file": "tsdb/engine/tsm1/engine.go",
    "created_at": "2024-09-11T20:58:11+00:00",
    "commented_code": "if os.IsNotExist(err) {\n \t\treturn nil\n \t} else if err != nil {\n-\t\treturn err\n+\t\treturn fmt.Errorf(\"Engine.cleanup: %w\", err)\n \t}\n \n \text := fmt.Sprintf(\".%s\", TmpTSMFileExtension)\n \tfor _, f := range allfiles {\n \t\t// Check to see if there are any `.tmp` directories that were left over from failed shard snapshots\n \t\tif f.IsDir() && strings.HasSuffix(f.Name(), ext) {\n \t\t\tif err := os.RemoveAll(filepath.Join(e.path, f.Name())); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error removing tmp snapshot directory %q: %s\", f.Name(), err)\n+\t\t\t\treturn fmt.Errorf(\"Engine.cleanup: error removing tmp snapshot directory: %w\", err)",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1755600609",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25314,
        "pr_file": "tsdb/engine/tsm1/engine.go",
        "discussion_id": "1755600609",
        "commented_code": "@@ -2431,15 +2432,15 @@ func (e *Engine) cleanup() error {\n \tif os.IsNotExist(err) {\n \t\treturn nil\n \t} else if err != nil {\n-\t\treturn err\n+\t\treturn fmt.Errorf(\"Engine.cleanup: %w\", err)\n \t}\n \n \text := fmt.Sprintf(\".%s\", TmpTSMFileExtension)\n \tfor _, f := range allfiles {\n \t\t// Check to see if there are any `.tmp` directories that were left over from failed shard snapshots\n \t\tif f.IsDir() && strings.HasSuffix(f.Name(), ext) {\n \t\t\tif err := os.RemoveAll(filepath.Join(e.path, f.Name())); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error removing tmp snapshot directory %q: %s\", f.Name(), err)\n+\t\t\t\treturn fmt.Errorf(\"Engine.cleanup: error removing tmp snapshot directory: %w\", err)",
        "comment_created_at": "2024-09-11T20:58:11+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "Let's keep the file name in the message.",
        "pr_file_module": null
      },
      {
        "comment_id": "1755679146",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25314,
        "pr_file": "tsdb/engine/tsm1/engine.go",
        "discussion_id": "1755600609",
        "commented_code": "@@ -2431,15 +2432,15 @@ func (e *Engine) cleanup() error {\n \tif os.IsNotExist(err) {\n \t\treturn nil\n \t} else if err != nil {\n-\t\treturn err\n+\t\treturn fmt.Errorf(\"Engine.cleanup: %w\", err)\n \t}\n \n \text := fmt.Sprintf(\".%s\", TmpTSMFileExtension)\n \tfor _, f := range allfiles {\n \t\t// Check to see if there are any `.tmp` directories that were left over from failed shard snapshots\n \t\tif f.IsDir() && strings.HasSuffix(f.Name(), ext) {\n \t\t\tif err := os.RemoveAll(filepath.Join(e.path, f.Name())); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error removing tmp snapshot directory %q: %s\", f.Name(), err)\n+\t\t\t\treturn fmt.Errorf(\"Engine.cleanup: error removing tmp snapshot directory: %w\", err)",
        "comment_created_at": "2024-09-11T21:35:25+00:00",
        "comment_author": "gwossum",
        "comment_body": "I had that originally, but then I noticed that `PathError` contained the path so I removed the duplicate path. Upon closer inspection, ~these are loafers~ if you get a `SyscallError`, that doesn't include the path.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1684842816",
    "pr_number": 25047,
    "pr_file": "cmd/influx_inspect/export/export_parquet.go",
    "created_at": "2024-07-19T18:50:33+00:00",
    "commented_code": "+package export\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"os\"\n+\t\"path/filepath\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/apache/arrow/go/v16/parquet\"\n+\t\"github.com/apache/arrow/go/v16/parquet/compress\"\n+\tpqexport \"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/index\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/models\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/resultset\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/table\"\n+\ttsdb \"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/tsm1\"\n+\t\"github.com/influxdata/influxdb/tsdb/engine/tsm1\"\n+\t\"go.uber.org/zap\"\n+\t\"golang.org/x/exp/maps\"\n+)\n+\n+//\n+// Export to Parquet file(s) is done per each TSM file. The files are apparently not sorted.\n+// Therefore, neither are output files. So for example `table-00001.parquet` may contain older data\n+// than `table-00000.parquet`.\n+// If the data should be sorted then\n+//  1. writeValuesParquet must collect multiple chunks of field values, and they should be sorted later\n+//  2. exporter must call exportDone after all files were processed\n+//\n+\n+// sequence is used for the suffix of generated parquet files\n+var sequence int\n+\n+// vc is the key:field:values collection of exported values\n+var vc map[string]map[string][]tsm1.Value\n+\n+func (cmd *Command) writeValuesParquet(_ io.Writer, seriesKey []byte, field string, values []tsm1.Value) error {\n+\tif vc == nil {\n+\t\tvc = make(map[string]map[string][]tsm1.Value)\n+\t}\n+\n+\tkey := string(seriesKey)\n+\tfields, ok := vc[key]\n+\tif !ok {\n+\t\tfields = make(map[string][]tsm1.Value)\n+\t\tvc[key] = fields\n+\t}\n+\tfields[field] = values\n+\treturn nil\n+}\n+\n+func (cmd *Command) exportDoneParquet(_ string) error {\n+\tdefer func() {\n+\t\tvc = nil\n+\t}()\n+\n+\tvar schema *index.MeasurementSchema\n+\n+\tfor key, fields := range vc {\n+\t\t// since code from v2 exporter is used, we need v2 series key\n+\t\tseriesKeyEx := append([]byte(models.MeasurementTagKey+\"=\"), []byte(key)...)\n+\t\t// get tags\n+\t\ttags := models.ParseTags(models.Escaped{B: seriesKeyEx})\n+\t\ttagSet := make(map[string]struct{}, len(tags))\n+\t\tfor _, tag := range tags {\n+\t\t\ttagSet[string(tag.Key)] = struct{}{}\n+\t\t}\n+\t\t// get fields\n+\t\tfieldSet := make(map[index.MeasurementField]struct{}, len(fields))\n+\t\tfor field, values := range fields {\n+\t\t\tbt, err := tsdb.BlockTypeForType(values[0].Value())\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tmf := index.MeasurementField{\n+\t\t\t\tName: field,\n+\t\t\t\tType: bt,\n+\t\t\t}\n+\t\t\tfieldSet[mf] = struct{}{}\n+\t\t}\n+\t\t// populate schema\n+\t\tschema = &index.MeasurementSchema{\n+\t\t\tTagSet:   tagSet,\n+\t\t\tFieldSet: fieldSet,\n+\t\t}\n+\t\t// schema does not change in a table\n+\t\tbreak\n+\t}\n+\n+\tif err := cmd.writeToParquet(newResultSet(vc), schema); err != nil {\n+\t\treturn err\n+\t}\n+\n+\treturn nil\n+}\n+\n+func (cmd *Command) writeToParquet(rs resultset.ResultSet, schema *index.MeasurementSchema) error {\n+\tcfg := zap.NewDevelopmentConfig()\n+\tif cmd.usingStdOut() {\n+\t\tcfg.OutputPaths = []string{\n+\t\t\t\"stdout\",\n+\t\t}\n+\t}\n+\tlog, _ := cfg.Build()\n+\n+\tlog.Info(\"Generating parquet\", zap.Int(\"chunk_size\", cmd.pqChunkSize))\n+\n+\tctx, cancel := context.WithCancel(context.Background())\n+\tdefer cancel()\n+\n+\tt, err := table.New(rs, schema)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\texporter := pqexport.New(t,\n+\t\tpqexport.WithParquetWriterProperties(\n+\t\t\tparquet.WithCompression(compress.Codecs.Snappy)))\n+\n+\tvar wg sync.WaitGroup\n+\n+\twg.Add(1)\n+\tgo func() {\n+\t\tdefer wg.Done()\n+\n+\t\tstart := time.Now()\n+\t\tvar (\n+\t\t\tlastSeriesCount int64\n+\t\t\tlastRowCount    int64\n+\t\t)\n+\t\tfor {\n+\t\t\tticker := time.NewTicker(30 * time.Second)\n+\t\t\tselect {\n+\t\t\tcase <-ctx.Done():\n+\t\t\t\tstats := t.Stats()\n+\t\t\t\tlog.Info(\"Total Processed\", zap.Int64(\"series_count\", stats.SeriesCount), zap.Int64(\"rows\", stats.RowCount), zap.Duration(\"build_time\", time.Since(start)))\n+\t\t\t\treturn\n+\t\t\tcase <-ticker.C:\n+\t\t\t\tstats := t.Stats()\n+\t\t\t\tvar (\n+\t\t\t\t\tlastSeries int64\n+\t\t\t\t\tlastRows   int64\n+\t\t\t\t)\n+\t\t\t\tlastSeries, lastSeriesCount = stats.SeriesCount-lastSeriesCount, stats.SeriesCount\n+\t\t\t\tlastRows, lastRowCount = stats.RowCount-lastRowCount, stats.RowCount\n+\t\t\t\tlog.Info(\"Processed\",\n+\t\t\t\t\tzap.Int64(\"total_series\", stats.SeriesCount),\n+\t\t\t\t\tzap.Int64(\"total_rows\", stats.RowCount),\n+\t\t\t\t\tzap.Int64(\"last_series\", lastSeries),\n+\t\t\t\t\tzap.Int64(\"last_rows\", lastRows),\n+\t\t\t\t)\n+\t\t\t}\n+\t\t}\n+\t}()\n+\n+\tlog.Info(\"Schema info\",\n+\t\tzap.Int(\"tag_count\", len(schema.TagSet)),\n+\t\tzap.Int(\"field_count\", len(schema.FieldSet)))\n+\n+\tvar done bool\n+\tfor !done && err == nil {\n+\t\tdone, err = func() (done bool, err error) {\n+\t\t\tdefer func() { sequence++ }()\n+\n+\t\t\tvar parquetFile *os.File\n+\t\t\t{\n+\t\t\t\tfileName := filepath.Join(cmd.out, fmt.Sprintf(\"table-%05d.parquet\", sequence))\n+\t\t\t\tparquetFile, err = os.Create(fileName)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn false, err",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1684842816",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25047,
        "pr_file": "cmd/influx_inspect/export/export_parquet.go",
        "discussion_id": "1684842816",
        "commented_code": "@@ -0,0 +1,348 @@\n+package export\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"os\"\n+\t\"path/filepath\"\n+\t\"sync\"\n+\t\"time\"\n+\n+\t\"github.com/apache/arrow/go/v16/parquet\"\n+\t\"github.com/apache/arrow/go/v16/parquet/compress\"\n+\tpqexport \"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/exporter\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/index\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/models\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/resultset\"\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/table\"\n+\ttsdb \"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/tsm1\"\n+\t\"github.com/influxdata/influxdb/tsdb/engine/tsm1\"\n+\t\"go.uber.org/zap\"\n+\t\"golang.org/x/exp/maps\"\n+)\n+\n+//\n+// Export to Parquet file(s) is done per each TSM file. The files are apparently not sorted.\n+// Therefore, neither are output files. So for example `table-00001.parquet` may contain older data\n+// than `table-00000.parquet`.\n+// If the data should be sorted then\n+//  1. writeValuesParquet must collect multiple chunks of field values, and they should be sorted later\n+//  2. exporter must call exportDone after all files were processed\n+//\n+\n+// sequence is used for the suffix of generated parquet files\n+var sequence int\n+\n+// vc is the key:field:values collection of exported values\n+var vc map[string]map[string][]tsm1.Value\n+\n+func (cmd *Command) writeValuesParquet(_ io.Writer, seriesKey []byte, field string, values []tsm1.Value) error {\n+\tif vc == nil {\n+\t\tvc = make(map[string]map[string][]tsm1.Value)\n+\t}\n+\n+\tkey := string(seriesKey)\n+\tfields, ok := vc[key]\n+\tif !ok {\n+\t\tfields = make(map[string][]tsm1.Value)\n+\t\tvc[key] = fields\n+\t}\n+\tfields[field] = values\n+\treturn nil\n+}\n+\n+func (cmd *Command) exportDoneParquet(_ string) error {\n+\tdefer func() {\n+\t\tvc = nil\n+\t}()\n+\n+\tvar schema *index.MeasurementSchema\n+\n+\tfor key, fields := range vc {\n+\t\t// since code from v2 exporter is used, we need v2 series key\n+\t\tseriesKeyEx := append([]byte(models.MeasurementTagKey+\"=\"), []byte(key)...)\n+\t\t// get tags\n+\t\ttags := models.ParseTags(models.Escaped{B: seriesKeyEx})\n+\t\ttagSet := make(map[string]struct{}, len(tags))\n+\t\tfor _, tag := range tags {\n+\t\t\ttagSet[string(tag.Key)] = struct{}{}\n+\t\t}\n+\t\t// get fields\n+\t\tfieldSet := make(map[index.MeasurementField]struct{}, len(fields))\n+\t\tfor field, values := range fields {\n+\t\t\tbt, err := tsdb.BlockTypeForType(values[0].Value())\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tmf := index.MeasurementField{\n+\t\t\t\tName: field,\n+\t\t\t\tType: bt,\n+\t\t\t}\n+\t\t\tfieldSet[mf] = struct{}{}\n+\t\t}\n+\t\t// populate schema\n+\t\tschema = &index.MeasurementSchema{\n+\t\t\tTagSet:   tagSet,\n+\t\t\tFieldSet: fieldSet,\n+\t\t}\n+\t\t// schema does not change in a table\n+\t\tbreak\n+\t}\n+\n+\tif err := cmd.writeToParquet(newResultSet(vc), schema); err != nil {\n+\t\treturn err\n+\t}\n+\n+\treturn nil\n+}\n+\n+func (cmd *Command) writeToParquet(rs resultset.ResultSet, schema *index.MeasurementSchema) error {\n+\tcfg := zap.NewDevelopmentConfig()\n+\tif cmd.usingStdOut() {\n+\t\tcfg.OutputPaths = []string{\n+\t\t\t\"stdout\",\n+\t\t}\n+\t}\n+\tlog, _ := cfg.Build()\n+\n+\tlog.Info(\"Generating parquet\", zap.Int(\"chunk_size\", cmd.pqChunkSize))\n+\n+\tctx, cancel := context.WithCancel(context.Background())\n+\tdefer cancel()\n+\n+\tt, err := table.New(rs, schema)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\texporter := pqexport.New(t,\n+\t\tpqexport.WithParquetWriterProperties(\n+\t\t\tparquet.WithCompression(compress.Codecs.Snappy)))\n+\n+\tvar wg sync.WaitGroup\n+\n+\twg.Add(1)\n+\tgo func() {\n+\t\tdefer wg.Done()\n+\n+\t\tstart := time.Now()\n+\t\tvar (\n+\t\t\tlastSeriesCount int64\n+\t\t\tlastRowCount    int64\n+\t\t)\n+\t\tfor {\n+\t\t\tticker := time.NewTicker(30 * time.Second)\n+\t\t\tselect {\n+\t\t\tcase <-ctx.Done():\n+\t\t\t\tstats := t.Stats()\n+\t\t\t\tlog.Info(\"Total Processed\", zap.Int64(\"series_count\", stats.SeriesCount), zap.Int64(\"rows\", stats.RowCount), zap.Duration(\"build_time\", time.Since(start)))\n+\t\t\t\treturn\n+\t\t\tcase <-ticker.C:\n+\t\t\t\tstats := t.Stats()\n+\t\t\t\tvar (\n+\t\t\t\t\tlastSeries int64\n+\t\t\t\t\tlastRows   int64\n+\t\t\t\t)\n+\t\t\t\tlastSeries, lastSeriesCount = stats.SeriesCount-lastSeriesCount, stats.SeriesCount\n+\t\t\t\tlastRows, lastRowCount = stats.RowCount-lastRowCount, stats.RowCount\n+\t\t\t\tlog.Info(\"Processed\",\n+\t\t\t\t\tzap.Int64(\"total_series\", stats.SeriesCount),\n+\t\t\t\t\tzap.Int64(\"total_rows\", stats.RowCount),\n+\t\t\t\t\tzap.Int64(\"last_series\", lastSeries),\n+\t\t\t\t\tzap.Int64(\"last_rows\", lastRows),\n+\t\t\t\t)\n+\t\t\t}\n+\t\t}\n+\t}()\n+\n+\tlog.Info(\"Schema info\",\n+\t\tzap.Int(\"tag_count\", len(schema.TagSet)),\n+\t\tzap.Int(\"field_count\", len(schema.FieldSet)))\n+\n+\tvar done bool\n+\tfor !done && err == nil {\n+\t\tdone, err = func() (done bool, err error) {\n+\t\t\tdefer func() { sequence++ }()\n+\n+\t\t\tvar parquetFile *os.File\n+\t\t\t{\n+\t\t\t\tfileName := filepath.Join(cmd.out, fmt.Sprintf(\"table-%05d.parquet\", sequence))\n+\t\t\t\tparquetFile, err = os.Create(fileName)\n+\t\t\t\tif err != nil {\n+\t\t\t\t\treturn false, err",
        "comment_created_at": "2024-07-19T18:50:33+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "Can we wrap the error on `os.Create` and add the filename to the outer error for help debugging?\r\n\r\n`fmt.Errorf(\"failed creating parquet file %s for export: %w\", fileName, err)`\r\n\r\nMuch more useful than a simple error string like \"permission denied\".",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1684875904",
    "pr_number": 25047,
    "pr_file": "cmd/influx_inspect/export/parquet/exporter/meta.go",
    "created_at": "2024-07-19T19:07:57+00:00",
    "commented_code": "+package exporter\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/base64\"\n+\t\"errors\"\n+\t\"io\"\n+\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/models\"\n+\tjsoniter \"github.com/json-iterator/go\"\n+)\n+\n+// Meta contains information about an exported Parquet file.\n+type Meta struct {\n+\tFirstSeriesKey models.Escaped\n+\tFirstTimestamp int64\n+\tLastSeriesKey  models.Escaped\n+\tLastTimestamp  int64\n+}\n+\n+// meta is an intermediate type to facilitate encoding and decoding\n+// series keys in JSON format.\n+type meta struct {\n+\tFirstSeriesKey string `json:\"firstSeriesKeyEscaped\"`\n+\tFirstTimestamp int64  `json:\"firstTimestamp\"`\n+\tLastSeriesKey  string `json:\"lastSeriesKeyEscaped\"`\n+\tLastTimestamp  int64  `json:\"lastTimestamp\"`\n+}\n+\n+func (m *Meta) UnmarshalJSON(data []byte) error {\n+\tvar tmp meta\n+\tif err := jsoniter.Unmarshal(data, &tmp); err != nil {\n+\t\treturn err\n+\t}\n+\n+\tfirstSeriesKey, err := base64.StdEncoding.DecodeString(tmp.FirstSeriesKey)\n+\tif err != nil {\n+\t\treturn errors.New(\"failed to decode firstSeriesKeyEscaped\")",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1684875904",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 25047,
        "pr_file": "cmd/influx_inspect/export/parquet/exporter/meta.go",
        "discussion_id": "1684875904",
        "commented_code": "@@ -0,0 +1,79 @@\n+package exporter\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/base64\"\n+\t\"errors\"\n+\t\"io\"\n+\n+\t\"github.com/influxdata/influxdb/cmd/influx_inspect/export/parquet/models\"\n+\tjsoniter \"github.com/json-iterator/go\"\n+)\n+\n+// Meta contains information about an exported Parquet file.\n+type Meta struct {\n+\tFirstSeriesKey models.Escaped\n+\tFirstTimestamp int64\n+\tLastSeriesKey  models.Escaped\n+\tLastTimestamp  int64\n+}\n+\n+// meta is an intermediate type to facilitate encoding and decoding\n+// series keys in JSON format.\n+type meta struct {\n+\tFirstSeriesKey string `json:\"firstSeriesKeyEscaped\"`\n+\tFirstTimestamp int64  `json:\"firstTimestamp\"`\n+\tLastSeriesKey  string `json:\"lastSeriesKeyEscaped\"`\n+\tLastTimestamp  int64  `json:\"lastTimestamp\"`\n+}\n+\n+func (m *Meta) UnmarshalJSON(data []byte) error {\n+\tvar tmp meta\n+\tif err := jsoniter.Unmarshal(data, &tmp); err != nil {\n+\t\treturn err\n+\t}\n+\n+\tfirstSeriesKey, err := base64.StdEncoding.DecodeString(tmp.FirstSeriesKey)\n+\tif err != nil {\n+\t\treturn errors.New(\"failed to decode firstSeriesKeyEscaped\")",
        "comment_created_at": "2024-07-19T19:07:57+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "`errors.New` here hides the actual error, which might be helpful in debugging. I think either `errors.Wrap` or `fmt.Errorf` to pass back the root cause error would be more helpful.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1561309157",
    "pr_number": 24858,
    "pr_file": "pkg/fs/special_linux.go",
    "created_at": "2024-04-11T16:34:54+00:00",
    "commented_code": "+package fs\n+\n+import (\n+\t\"errors\"\n+\t\"io/fs\"\n+\t\"os\"\n+\t\"syscall\"\n+\n+\t\"golang.org/x/sys/unix\"\n+)\n+\n+// IsSpecialFSFromFileInfo determines if a file resides on a special file\n+// system (e.g. /proc, /dev/, /sys) based on its fs.FileInfo.\n+// The bool return value should be ignored if err is not nil.\n+func IsSpecialFSFromFileInfo(st fs.FileInfo) (bool, error) {\n+\t// On Linux, special file systems like /proc, /dev/, and /sys are\n+\t// considered unnamed devices (non-device mounts). These devices\n+\t// will always have a major device number of 0 per the kernels\n+\t// Documentation/devices.txt file.\n+\n+\tgetDevId := func(st fs.FileInfo) (uint64, error) {\n+\t\tst_sys_any := st.Sys()\n+\t\tif st_sys_any == nil {\n+\t\t\treturn 0, errors.New(\"nil returned by fs.FileInfo.Sys\")\n+\t\t}\n+\n+\t\tst_sys, ok := st_sys_any.(*syscall.Stat_t)\n+\t\tif !ok {\n+\t\t\treturn 0, errors.New(\"could not convert st.sys() to a *syscall.Stat_t\")",
    "repo_full_name": "influxdata/influxdb",
    "discussion_comments": [
      {
        "comment_id": "1561309157",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 24858,
        "pr_file": "pkg/fs/special_linux.go",
        "discussion_id": "1561309157",
        "commented_code": "@@ -0,0 +1,63 @@\n+package fs\n+\n+import (\n+\t\"errors\"\n+\t\"io/fs\"\n+\t\"os\"\n+\t\"syscall\"\n+\n+\t\"golang.org/x/sys/unix\"\n+)\n+\n+// IsSpecialFSFromFileInfo determines if a file resides on a special file\n+// system (e.g. /proc, /dev/, /sys) based on its fs.FileInfo.\n+// The bool return value should be ignored if err is not nil.\n+func IsSpecialFSFromFileInfo(st fs.FileInfo) (bool, error) {\n+\t// On Linux, special file systems like /proc, /dev/, and /sys are\n+\t// considered unnamed devices (non-device mounts). These devices\n+\t// will always have a major device number of 0 per the kernels\n+\t// Documentation/devices.txt file.\n+\n+\tgetDevId := func(st fs.FileInfo) (uint64, error) {\n+\t\tst_sys_any := st.Sys()\n+\t\tif st_sys_any == nil {\n+\t\t\treturn 0, errors.New(\"nil returned by fs.FileInfo.Sys\")\n+\t\t}\n+\n+\t\tst_sys, ok := st_sys_any.(*syscall.Stat_t)\n+\t\tif !ok {\n+\t\t\treturn 0, errors.New(\"could not convert st.sys() to a *syscall.Stat_t\")",
        "comment_created_at": "2024-04-11T16:34:54+00:00",
        "comment_author": "davidby-influx",
        "comment_body": "Maybe add the filename to all the errors to help diagnosis of any problems.",
        "pr_file_module": null
      },
      {
        "comment_id": "1563211372",
        "repo_full_name": "influxdata/influxdb",
        "pr_number": 24858,
        "pr_file": "pkg/fs/special_linux.go",
        "discussion_id": "1561309157",
        "commented_code": "@@ -0,0 +1,63 @@\n+package fs\n+\n+import (\n+\t\"errors\"\n+\t\"io/fs\"\n+\t\"os\"\n+\t\"syscall\"\n+\n+\t\"golang.org/x/sys/unix\"\n+)\n+\n+// IsSpecialFSFromFileInfo determines if a file resides on a special file\n+// system (e.g. /proc, /dev/, /sys) based on its fs.FileInfo.\n+// The bool return value should be ignored if err is not nil.\n+func IsSpecialFSFromFileInfo(st fs.FileInfo) (bool, error) {\n+\t// On Linux, special file systems like /proc, /dev/, and /sys are\n+\t// considered unnamed devices (non-device mounts). These devices\n+\t// will always have a major device number of 0 per the kernels\n+\t// Documentation/devices.txt file.\n+\n+\tgetDevId := func(st fs.FileInfo) (uint64, error) {\n+\t\tst_sys_any := st.Sys()\n+\t\tif st_sys_any == nil {\n+\t\t\treturn 0, errors.New(\"nil returned by fs.FileInfo.Sys\")\n+\t\t}\n+\n+\t\tst_sys, ok := st_sys_any.(*syscall.Stat_t)\n+\t\tif !ok {\n+\t\t\treturn 0, errors.New(\"could not convert st.sys() to a *syscall.Stat_t\")",
        "comment_created_at": "2024-04-12T21:20:05+00:00",
        "comment_author": "gwossum",
        "comment_body": "I'm going add the filename to the caller's error. `IsSepcialFSFromFileInfo` does not have access to the full path of the filename, only the base path.",
        "pr_file_module": null
      }
    ]
  }
]