[
  {
    "discussion_id": "2211799648",
    "pr_number": 57029,
    "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
    "created_at": "2025-07-16T23:16:42+00:00",
    "commented_code": "+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  # Metrics endpoint for monitoring/prometheus\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 15014\n+  # Readiness probe endpoint\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 8000\n+  egress:\n+  # Allow DNS resolution\n+  - to: []\n+    ports:\n+    - protocol: UDP\n+      port: 53\n+    - protocol: TCP\n+      port: 53\n+  # Allow access to Kubernetes API server in kube-system namespace",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "2211799648",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2211799648",
        "commented_code": "@@ -0,0 +1,53 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  # Metrics endpoint for monitoring/prometheus\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 15014\n+  # Readiness probe endpoint\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 8000\n+  egress:\n+  # Allow DNS resolution\n+  - to: []\n+    ports:\n+    - protocol: UDP\n+      port: 53\n+    - protocol: TCP\n+      port: 53\n+  # Allow access to Kubernetes API server in kube-system namespace",
        "comment_created_at": "2025-07-16T23:16:42+00:00",
        "comment_author": "howardjohn",
        "comment_body": "I am not confident this works in most environments. The api-server is not always in kube-system nor a pod. Nor on these ports. Unless there is documentation suggesting a supported portable way to allow access here this seems problematic",
        "pr_file_module": null
      },
      {
        "comment_id": "2212588287",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2211799648",
        "commented_code": "@@ -0,0 +1,53 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  # Metrics endpoint for monitoring/prometheus\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 15014\n+  # Readiness probe endpoint\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 8000\n+  egress:\n+  # Allow DNS resolution\n+  - to: []\n+    ports:\n+    - protocol: UDP\n+      port: 53\n+    - protocol: TCP\n+      port: 53\n+  # Allow access to Kubernetes API server in kube-system namespace",
        "comment_created_at": "2025-07-17T07:54:47+00:00",
        "comment_author": "dgn",
        "comment_body": "Good point. I see two options:\r\n1) we open up egress to allow for all traffic\r\n2) we keep these defaults and ask users with a non-standard api-server location to add their own, additional NP that allows access to it",
        "pr_file_module": null
      },
      {
        "comment_id": "2212610791",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2211799648",
        "commented_code": "@@ -0,0 +1,53 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  # Metrics endpoint for monitoring/prometheus\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 15014\n+  # Readiness probe endpoint\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 8000\n+  egress:\n+  # Allow DNS resolution\n+  - to: []\n+    ports:\n+    - protocol: UDP\n+      port: 53\n+    - protocol: TCP\n+      port: 53\n+  # Allow access to Kubernetes API server in kube-system namespace",
        "comment_created_at": "2025-07-17T08:03:23+00:00",
        "comment_author": "dgn",
        "comment_body": "I think I'm leaning towards option 1 - after all, the purpose of these is mostly to not break istio by applying a default policy, rather than shipping the most secure NP possible - that will always require knowledge about the cluster and architecture that we don't have at install time",
        "pr_file_module": null
      },
      {
        "comment_id": "2212642264",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2211799648",
        "commented_code": "@@ -0,0 +1,53 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  # Metrics endpoint for monitoring/prometheus\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 15014\n+  # Readiness probe endpoint\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 8000\n+  egress:\n+  # Allow DNS resolution\n+  - to: []\n+    ports:\n+    - protocol: UDP\n+      port: 53\n+    - protocol: TCP\n+      port: 53\n+  # Allow access to Kubernetes API server in kube-system namespace",
        "comment_created_at": "2025-07-17T08:14:59+00:00",
        "comment_author": "sridhargaddam",
        "comment_body": "One more option is...\r\n\r\nto allow users to customize the policy in `values.yaml`\r\n```\r\nnetworkPolicy:\r\n  # When enabled, a default NetworkPolicy for istio-cni will be created\r\n  enabled: false\r\n  # Override the default NetworkPolicy with custom rules\r\n  # If specified, this will replace the default egress rules\r\n  customEgress: []\r\n```\r\n\r\nAnd update the K8s NetworkPolicy template to use the customEgress rules (if specified)  \r\n```\r\n  egress:\r\n  {{- if .Values.networkPolicy.customEgress }}\r\n    {{- toYaml .Values.networkPolicy.customEgress | nindent 2 }}\r\n  {{- else }}\r\n  # Allow all egress traffic by default\r\n  - {}\r\n  {{- end }}  \r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2213305775",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2211799648",
        "commented_code": "@@ -0,0 +1,53 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  # Metrics endpoint for monitoring/prometheus\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 15014\n+  # Readiness probe endpoint\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 8000\n+  egress:\n+  # Allow DNS resolution\n+  - to: []\n+    ports:\n+    - protocol: UDP\n+      port: 53\n+    - protocol: TCP\n+      port: 53\n+  # Allow access to Kubernetes API server in kube-system namespace",
        "comment_created_at": "2025-07-17T13:09:13+00:00",
        "comment_author": "dgn",
        "comment_body": "@sridhargaddam I think that might be a good way to extend this in the future. I'd like to start small though - I went with option 1 (allow all egress) for now",
        "pr_file_module": null
      },
      {
        "comment_id": "2213343367",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2211799648",
        "commented_code": "@@ -0,0 +1,53 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  # Metrics endpoint for monitoring/prometheus\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 15014\n+  # Readiness probe endpoint\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 8000\n+  egress:\n+  # Allow DNS resolution\n+  - to: []\n+    ports:\n+    - protocol: UDP\n+      port: 53\n+    - protocol: TCP\n+      port: 53\n+  # Allow access to Kubernetes API server in kube-system namespace",
        "comment_created_at": "2025-07-17T13:24:37+00:00",
        "comment_author": "sridhargaddam",
        "comment_body": "Fine with me @dgn. Makes sense to start small and expand it in future depending on the requirements.",
        "pr_file_module": null
      },
      {
        "comment_id": "2213591141",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2211799648",
        "commented_code": "@@ -0,0 +1,53 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:\n+  - Ingress\n+  - Egress\n+  ingress:\n+  # Metrics endpoint for monitoring/prometheus\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 15014\n+  # Readiness probe endpoint\n+  - from: []\n+    ports:\n+    - protocol: TCP\n+      port: 8000\n+  egress:\n+  # Allow DNS resolution\n+  - to: []\n+    ports:\n+    - protocol: UDP\n+      port: 53\n+    - protocol: TCP\n+      port: 53\n+  # Allow access to Kubernetes API server in kube-system namespace",
        "comment_created_at": "2025-07-17T14:58:13+00:00",
        "comment_author": "Stevenjin8",
        "comment_body": "+1 for opening up egress since istio does not strictly enforce egress traffic.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2213519677",
    "pr_number": 57029,
    "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
    "created_at": "2025-07-17T14:30:03+00:00",
    "commented_code": "+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "2213519677",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2213519677",
        "commented_code": "@@ -0,0 +1,37 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:",
        "comment_created_at": "2025-07-17T14:30:03+00:00",
        "comment_author": "howardjohn",
        "comment_body": "nit: is it better to just not specify this at all for Egress? Else user cannot lock it down\r\n\r\nOr is this so when a user has a deny-all rule istiod works?",
        "pr_file_module": null
      },
      {
        "comment_id": "2218460714",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2213519677",
        "commented_code": "@@ -0,0 +1,37 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:",
        "comment_created_at": "2025-07-21T08:00:49+00:00",
        "comment_author": "dgn",
        "comment_body": "> Or is this so when a user has a deny-all rule istiod works?\r\n\r\nExactly, that's the main point: keeping istio working when there's a default deny-all rule",
        "pr_file_module": null
      },
      {
        "comment_id": "2218962344",
        "repo_full_name": "istio/istio",
        "pr_number": 57029,
        "pr_file": "manifests/charts/istio-cni/templates/networkpolicy.yaml",
        "discussion_id": "2213519677",
        "commented_code": "@@ -0,0 +1,37 @@\n+{{- if (.Values.networkPolicy).enabled }}\n+apiVersion: networking.k8s.io/v1\n+kind: NetworkPolicy\n+metadata:\n+  name: {{ template \"name\" . }}{{- if not (eq .Values.revision \"\") }}-{{ .Values.revision }}{{- end }}\n+  namespace: {{ .Release.Namespace }}\n+  labels:\n+    k8s-app: {{ template \"name\" . }}-node\n+    release: {{ .Release.Name }}\n+    istio.io/rev: {{ .Values.revision | default \"default\" | quote }}\n+    install.operator.istio.io/owning-resource: {{ .Values.ownerName | default \"unknown\" }}\n+    operator.istio.io/component: \"Cni\"\n+    app.kubernetes.io/name: {{ template \"name\" . }}\n+    {{- include \"istio.labels\" . | nindent 4 }}\n+spec:\n+  podSelector:\n+    matchLabels:\n+      k8s-app: {{ template \"name\" . }}-node\n+  policyTypes:",
        "comment_created_at": "2025-07-21T11:46:52+00:00",
        "comment_author": "dgn",
        "comment_body": "Until we implement @sridhargaddam's [suggestion](https://github.com/istio/istio/pull/57029#discussion_r2212642264), the way for users to lock it down further would be to not use the default NP but rather create their own from scratch",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1357387021",
    "pr_number": 47205,
    "pr_file": "releasenotes/notes/wds.yaml",
    "created_at": "2023-10-12T20:33:31+00:00",
    "commented_code": "+apiVersion: release-notes/v2\n+kind: feature\n+area: telemetry\n+releaseNotes:\n+- |\n+  **Added** xDS workload metadata discovery to TCP metadata exchange filter as a fallback. This requires enabling `PEER_METADATA_DISCOVERY` flag on the proxy, and `PILOT_ENABLE_AMBIENT_CONTROLLERS` on the control plane.\n+  **Added** a flag `PILOT_DISABLE_MX_ALPN` on the control plane to disable advertising TCP metadata exchange ALPN token `istio-peer-exchange`.",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "1357387021",
        "repo_full_name": "istio/istio",
        "pr_number": 47205,
        "pr_file": "releasenotes/notes/wds.yaml",
        "discussion_id": "1357387021",
        "commented_code": "@@ -0,0 +1,7 @@\n+apiVersion: release-notes/v2\n+kind: feature\n+area: telemetry\n+releaseNotes:\n+- |\n+  **Added** xDS workload metadata discovery to TCP metadata exchange filter as a fallback. This requires enabling `PEER_METADATA_DISCOVERY` flag on the proxy, and `PILOT_ENABLE_AMBIENT_CONTROLLERS` on the control plane.\n+  **Added** a flag `PILOT_DISABLE_MX_ALPN` on the control plane to disable advertising TCP metadata exchange ALPN token `istio-peer-exchange`.",
        "comment_created_at": "2023-10-12T20:33:31+00:00",
        "comment_author": "costinm",
        "comment_body": "May need more - or a separate doc/wiki explaining the process to move back/forth, how things interact with each other. \r\n\r\nWould it make sense to also add a Service annotation - for gradual opt in ? We may have cases where some services are behind east-west or other infra that requires the  peer exchange headers ( and in future as discussed JWTs or other fancy things) - while other services can use MDS. ( can be added in separate PRs, but wondering about the general adoption story in context of multi-network and other features  )",
        "pr_file_module": null
      },
      {
        "comment_id": "1358709000",
        "repo_full_name": "istio/istio",
        "pr_number": 47205,
        "pr_file": "releasenotes/notes/wds.yaml",
        "discussion_id": "1357387021",
        "commented_code": "@@ -0,0 +1,7 @@\n+apiVersion: release-notes/v2\n+kind: feature\n+area: telemetry\n+releaseNotes:\n+- |\n+  **Added** xDS workload metadata discovery to TCP metadata exchange filter as a fallback. This requires enabling `PEER_METADATA_DISCOVERY` flag on the proxy, and `PILOT_ENABLE_AMBIENT_CONTROLLERS` on the control plane.\n+  **Added** a flag `PILOT_DISABLE_MX_ALPN` on the control plane to disable advertising TCP metadata exchange ALPN token `istio-peer-exchange`.",
        "comment_created_at": "2023-10-13T19:02:45+00:00",
        "comment_author": "kyessenov",
        "comment_body": "Right now it's a fallback to existing methods, it's not taking priority. We need some burn time as a fallback before making flipping the priority - that can be done in the next version.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1114992949",
    "pr_number": 42372,
    "pr_file": "manifests/profiles/ambient.yaml",
    "created_at": "2023-02-22T21:39:45+00:00",
    "commented_code": "privileged: true\n       ambient:\n         enabled: true\n+        redirectMode: \"ebpf\"",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "1114992949",
        "repo_full_name": "istio/istio",
        "pr_number": 42372,
        "pr_file": "manifests/profiles/ambient.yaml",
        "discussion_id": "1114992949",
        "commented_code": "@@ -36,6 +36,7 @@ spec:\n       privileged: true\n       ambient:\n         enabled: true\n+        redirectMode: \"ebpf\"",
        "comment_created_at": "2023-02-22T21:39:45+00:00",
        "comment_author": "bleggett",
        "comment_body": "```suggestion\r\n```\r\nSorry, last minute thing, I promise I'm not trying to hold this PR up to an undue degree,  but I just realized this - \r\n\r\nCan I suggest we leave this _off_ for now, and continue defaulting to the existing iptables mode - making eBPF CNI an opt-in on top of the opt-in for ambient?\r\n\r\n\r\nMostly because\r\n\r\n1. Ambient is in `master` now and I would rather not destabilize it further by default in the short term.\r\n2. This has had less testing than the existing mode, and (currently) has some constraints the existing one doesn't.\r\n3. Currently doesn't work out of the box for me on e.g. `kind` where the iptables mode does.\r\n\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1114996353",
        "repo_full_name": "istio/istio",
        "pr_number": 42372,
        "pr_file": "manifests/profiles/ambient.yaml",
        "discussion_id": "1114992949",
        "commented_code": "@@ -36,6 +36,7 @@ spec:\n       privileged: true\n       ambient:\n         enabled: true\n+        redirectMode: \"ebpf\"",
        "comment_created_at": "2023-02-22T21:43:45+00:00",
        "comment_author": "howardjohn",
        "comment_body": "oops, good call. I agree, thought it was off",
        "pr_file_module": null
      },
      {
        "comment_id": "1114996478",
        "repo_full_name": "istio/istio",
        "pr_number": 42372,
        "pr_file": "manifests/profiles/ambient.yaml",
        "discussion_id": "1114992949",
        "commented_code": "@@ -36,6 +36,7 @@ spec:\n       privileged: true\n       ambient:\n         enabled: true\n+        redirectMode: \"ebpf\"",
        "comment_created_at": "2023-02-22T21:43:53+00:00",
        "comment_author": "linsun",
        "comment_body": "+1 - I would prefer to keep iptables as default as that is well tested.",
        "pr_file_module": null
      },
      {
        "comment_id": "1115190957",
        "repo_full_name": "istio/istio",
        "pr_number": 42372,
        "pr_file": "manifests/profiles/ambient.yaml",
        "discussion_id": "1114992949",
        "commented_code": "@@ -36,6 +36,7 @@ spec:\n       privileged: true\n       ambient:\n         enabled: true\n+        redirectMode: \"ebpf\"",
        "comment_created_at": "2023-02-23T02:46:28+00:00",
        "comment_author": "PlatformLC",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1012173644",
    "pr_number": 41416,
    "pr_file": "tests/integration/telemetry/api/testdata/bad-wasm-envoy-filter-fail-open.yaml",
    "created_at": "2022-11-02T18:48:53+00:00",
    "commented_code": "+apiVersion: networking.istio.io/v1alpha3\n+kind: EnvoyFilter\n+metadata:\n+  name: bad-wasm-example\n+spec:\n+  configPatches:\n+  - applyTo: EXTENSION_CONFIG\n+    patch:\n+      operation: ADD\n+      value:\n+        name: my-bad-wasm-extension\n+        typed_config:\n+          \"@type\": type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm\n+          config:\n+            root_id: my-bad-wasm-root-id\n+            vm_config:\n+              vm_id: my-bad-wasm-vm-id\n+              runtime: envoy.wasm.runtime.v8\n+              code:\n+                remote:\n+                  http_uri:\n+                    uri: http://bad-url.wasm\n+            fail_open: true\n+            configuration:\n+              \"@type\": \"type.googleapis.com/google.protobuf.StringValue\"\n+              value: |\n+                {}\n+  # The second patch instructs to apply the above Wasm filter to the listener/http connection manager.\n+  - applyTo: HTTP_FILTER\n+    match:\n+      context: SIDECAR_INBOUND\n+    patch:\n+      operation: INSERT_FIRST\n+      value:\n+        name: my-bad-wasm-extension # This must match the name above\n+        config_discovery:\n+          config_source:\n+            ads: {}\n+          type_urls:\n+            - \"envoy.extensions.filters.http.wasm.v3.Wasm\"\n+            - \"envoy.extensions.filters.http.rbac.v3.RBAC\"",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "1012173644",
        "repo_full_name": "istio/istio",
        "pr_number": 41416,
        "pr_file": "tests/integration/telemetry/api/testdata/bad-wasm-envoy-filter-fail-open.yaml",
        "discussion_id": "1012173644",
        "commented_code": "@@ -0,0 +1,42 @@\n+apiVersion: networking.istio.io/v1alpha3\n+kind: EnvoyFilter\n+metadata:\n+  name: bad-wasm-example\n+spec:\n+  configPatches:\n+  - applyTo: EXTENSION_CONFIG\n+    patch:\n+      operation: ADD\n+      value:\n+        name: my-bad-wasm-extension\n+        typed_config:\n+          \"@type\": type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm\n+          config:\n+            root_id: my-bad-wasm-root-id\n+            vm_config:\n+              vm_id: my-bad-wasm-vm-id\n+              runtime: envoy.wasm.runtime.v8\n+              code:\n+                remote:\n+                  http_uri:\n+                    uri: http://bad-url.wasm\n+            fail_open: true\n+            configuration:\n+              \"@type\": \"type.googleapis.com/google.protobuf.StringValue\"\n+              value: |\n+                {}\n+  # The second patch instructs to apply the above Wasm filter to the listener/http connection manager.\n+  - applyTo: HTTP_FILTER\n+    match:\n+      context: SIDECAR_INBOUND\n+    patch:\n+      operation: INSERT_FIRST\n+      value:\n+        name: my-bad-wasm-extension # This must match the name above\n+        config_discovery:\n+          config_source:\n+            ads: {}\n+          type_urls:\n+            - \"envoy.extensions.filters.http.wasm.v3.Wasm\"\n+            - \"envoy.extensions.filters.http.rbac.v3.RBAC\"",
        "comment_created_at": "2022-11-02T18:48:53+00:00",
        "comment_author": "howardjohn",
        "comment_body": "What if users are using EnvoyFilter to configure ECDS today, won't they break if they haven't added this?",
        "pr_file_module": null
      },
      {
        "comment_id": "1012272045",
        "repo_full_name": "istio/istio",
        "pr_number": 41416,
        "pr_file": "tests/integration/telemetry/api/testdata/bad-wasm-envoy-filter-fail-open.yaml",
        "discussion_id": "1012173644",
        "commented_code": "@@ -0,0 +1,42 @@\n+apiVersion: networking.istio.io/v1alpha3\n+kind: EnvoyFilter\n+metadata:\n+  name: bad-wasm-example\n+spec:\n+  configPatches:\n+  - applyTo: EXTENSION_CONFIG\n+    patch:\n+      operation: ADD\n+      value:\n+        name: my-bad-wasm-extension\n+        typed_config:\n+          \"@type\": type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm\n+          config:\n+            root_id: my-bad-wasm-root-id\n+            vm_config:\n+              vm_id: my-bad-wasm-vm-id\n+              runtime: envoy.wasm.runtime.v8\n+              code:\n+                remote:\n+                  http_uri:\n+                    uri: http://bad-url.wasm\n+            fail_open: true\n+            configuration:\n+              \"@type\": \"type.googleapis.com/google.protobuf.StringValue\"\n+              value: |\n+                {}\n+  # The second patch instructs to apply the above Wasm filter to the listener/http connection manager.\n+  - applyTo: HTTP_FILTER\n+    match:\n+      context: SIDECAR_INBOUND\n+    patch:\n+      operation: INSERT_FIRST\n+      value:\n+        name: my-bad-wasm-extension # This must match the name above\n+        config_discovery:\n+          config_source:\n+            ads: {}\n+          type_urls:\n+            - \"envoy.extensions.filters.http.wasm.v3.Wasm\"\n+            - \"envoy.extensions.filters.http.rbac.v3.RBAC\"",
        "comment_created_at": "2022-11-02T20:33:32+00:00",
        "comment_author": "ingwonsong",
        "comment_body": "Yes. Their configuration will be broken when they want to use \"fail-open\" and they got failed.\r\n\r\nBut, I don't think this is the big issue, because the \"fail-open\" is not working if the user configured it along the guide.\r\nEnvoy will not accept the ECDS in the second example from the last in [this guide](https://istio.io/latest/docs/reference/config/networking/envoy-filter/) because it does not have \"required fields\" like \"sha256\" or \"cluster\".\r\n\r\nIf we want to not break the existing EnvoyFilter users by this allow-list change, we may need to use some default WASM like \"attributegen\" instead of RBAC.\r\n\r\nAny idea?",
        "pr_file_module": null
      }
    ]
  }
]