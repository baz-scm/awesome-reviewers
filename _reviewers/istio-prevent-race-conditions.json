[
  {
    "discussion_id": "2245512628",
    "pr_number": 57157,
    "pr_file": "pkg/kube/krt/nestedjoinmerge_test.go",
    "created_at": "2025-07-31T14:09:14+00:00",
    "commented_code": "+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//\thttp://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+package krt_test\n+\n+import (\n+\t\"testing\"\n+\n+\tcorev1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/apimachinery/pkg/util/intstr\"\n+\n+\t\"istio.io/istio/pkg/kube\"\n+\t\"istio.io/istio/pkg/kube/krt\"\n+\t\"istio.io/istio/pkg/maps\"\n+\t\"istio.io/istio/pkg/slices\"\n+\t\"istio.io/istio/pkg/test/util/assert\"\n+)\n+\n+func TestNestedJoin2WithMergeSimpleCollection(t *testing.T) {",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "2245512628",
        "repo_full_name": "istio/istio",
        "pr_number": 57157,
        "pr_file": "pkg/kube/krt/nestedjoinmerge_test.go",
        "discussion_id": "2245512628",
        "commented_code": "@@ -0,0 +1,321 @@\n+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//\thttp://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+package krt_test\n+\n+import (\n+\t\"testing\"\n+\n+\tcorev1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/apimachinery/pkg/util/intstr\"\n+\n+\t\"istio.io/istio/pkg/kube\"\n+\t\"istio.io/istio/pkg/kube/krt\"\n+\t\"istio.io/istio/pkg/maps\"\n+\t\"istio.io/istio/pkg/slices\"\n+\t\"istio.io/istio/pkg/test/util/assert\"\n+)\n+\n+func TestNestedJoin2WithMergeSimpleCollection(t *testing.T) {",
        "comment_created_at": "2025-07-31T14:09:14+00:00",
        "comment_author": "howardjohn",
        "comment_body": "```\r\n=== RUN   TestNestedJoin2WithMergeSimpleCollection\r\n2025-07-31T14:08:34.611209Z\tinfo\tcluster \"fake\" kube client started\r\n2025-07-31T14:08:34.613144Z\tinfo\tsync complete\tname=test/Services attempt=2 time=2.090289ms\r\n2025-07-31T14:08:34.613213Z\tinfo\tkrt\ttest/Services synced\towner=test/Services\r\n2025-07-31T14:08:34.613943Z\tdebug\tkrt\tsync complete\tname=test/Services time=2.173568ms\r\n2025-07-31T14:08:34.614064Z\tinfo\tsync complete\tname=crd watcher attempt=2 time=2.739084ms\r\n2025-07-31T14:08:34.614084Z\tdebug\tkrt\tsync complete\tname=test/Services time=832ns\r\n2025-07-31T14:08:34.614110Z\tinfo\tcontrollers\tstarting\tcontroller=crd watcher\r\n2025-07-31T14:08:34.616486Z\tinfo\tsync complete\tname=test/Services handler attempt=2 time=2.366225ms\r\n2025-07-31T14:08:34.616596Z\tdebug\tkrt\thandled\towner=test/SimpleServices res=namespace/svc type=add\r\n2025-07-31T14:08:34.616633Z\tdebug\tkrt\tcalling handlers\towner=test/SimpleServices events=1\r\n2025-07-31T14:08:34.616673Z\tinfo\tkrt\ttest/SimpleServices synced (uid 3)\towner=test/SimpleServices\r\n2025-07-31T14:08:34.616711Z\tdebug\tkrt\tsync complete\tname=test/SimpleServices time=5.424606ms\r\n2025-07-31T14:08:34.618660Z\tinfo\tsync complete\tname=AllServices attempt=3 time=7.371443ms\r\n2025-07-31T14:08:34.618664Z\tinfo\tsync complete\tname=crd watcher attempt=3 time=7.204315ms\r\n2025-07-31T14:08:34.618730Z\tdebug\tkrt\thandled\towner=AllServices res=namespace/svc type=add\r\n2025-07-31T14:08:34.618982Z\tdebug\tkrt\tcalling handlers\towner=AllServices events=1\r\n2025-07-31T14:08:34.619044Z\tinfo\tkrt\tAllServices synced (uid 5)\towner=AllServices\r\n2025-07-31T14:08:34.619087Z\tdebug\tkrt\tsync complete\tname=AllServices time=7.915789ms\r\n2025-07-31T14:08:34.619895Z\tdebug\tkrt\tsync complete\tname=AllServices time=842ns\r\n2025-07-31T14:08:34.649077Z\tinfo\tcluster \"fake\" kube client started\r\n2025-07-31T14:08:34.649544Z\tinfo\tsync complete\tname=crd watcher attempt=1 time=5.14\u00b5s\r\n2025-07-31T14:08:34.649604Z\tinfo\tcontrollers\tstarting\tcontroller=crd watcher\r\n2025-07-31T14:08:34.653241Z\tinfo\tsync complete\tname=crd watcher attempt=2 time=4.108561ms\r\n2025-07-31T14:08:34.653478Z\tinfo\tsync complete\tname=test/Services attempt=2 time=4.297801ms\r\n2025-07-31T14:08:34.653525Z\tinfo\tkrt\ttest/Services synced\towner=test/Services\r\n2025-07-31T14:08:34.653554Z\tdebug\tkrt\tsync complete\tname=test/Services time=4.380459ms\r\n2025-07-31T14:08:34.653704Z\tdebug\tkrt\tsync complete\tname=test/Services time=521ns\r\n2025-07-31T14:08:34.653730Z\tinfo\tsync complete\tname=test/Services handler attempt=1 time=2.215\u00b5s\r\n2025-07-31T14:08:34.653819Z\tdebug\tkrt\thandled\towner=test/SimpleServices2 res=namespace/svc type=add\r\n2025-07-31T14:08:34.653843Z\tdebug\tkrt\tcalling handlers\towner=test/SimpleServices2 events=1\r\n2025-07-31T14:08:34.653883Z\tinfo\tkrt\ttest/SimpleServices2 synced (uid 7)\towner=test/SimpleServices2\r\n2025-07-31T14:08:34.653927Z\tdebug\tkrt\tsync complete\tname=test/SimpleServices2 time=247.201\u00b5s\r\n2025-07-31T14:08:34.654106Z\tdebug\tkrt\thandled\towner=AllServices res=namespace/svc type=update\r\n2025-07-31T14:08:34.654134Z\tdebug\tkrt\tcalling handlers\towner=AllServices events=1\r\n2025-07-31T14:08:34.658853Z\tdebug\tkrt\thandled delete\towner=AllServices res=namespace/svc\r\n2025-07-31T14:08:34.660104Z\tdebug\tkrt\thandled\towner=AllServices res=namespace/svc type=add\r\n2025-07-31T14:08:34.660316Z\tdebug\tkrt\tcalling handlers\towner=AllServices events=1\r\n2025-07-31T14:08:34.660447Z\tdebug\tkrt\thandled\towner=AllServices res=namespace/svc type=update\r\n2025-07-31T14:08:34.660475Z\tdebug\tkrt\tcalling handlers\towner=AllServices events=1\r\n    nestedjoinmerge_test.go:166: unexpected events: [update/namespace/svc]\r\n--- FAIL: TestNestedJoin2WithMergeSimpleCollection (0.61s)\r\nFAIL\r\n\r\nERROR: exit status 1\r\n```\r\n\r\nthis one happens often, about 1% of the time",
        "pr_file_module": null
      },
      {
        "comment_id": "2245523030",
        "repo_full_name": "istio/istio",
        "pr_number": 57157,
        "pr_file": "pkg/kube/krt/nestedjoinmerge_test.go",
        "discussion_id": "2245512628",
        "commented_code": "@@ -0,0 +1,321 @@\n+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//\thttp://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+package krt_test\n+\n+import (\n+\t\"testing\"\n+\n+\tcorev1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/apimachinery/pkg/util/intstr\"\n+\n+\t\"istio.io/istio/pkg/kube\"\n+\t\"istio.io/istio/pkg/kube/krt\"\n+\t\"istio.io/istio/pkg/maps\"\n+\t\"istio.io/istio/pkg/slices\"\n+\t\"istio.io/istio/pkg/test/util/assert\"\n+)\n+\n+func TestNestedJoin2WithMergeSimpleCollection(t *testing.T) {",
        "comment_created_at": "2025-07-31T14:13:05+00:00",
        "comment_author": "keithmattix",
        "comment_body": "Ah this is what I was talking to you about where 99% of the time, the events get coalesced into 1 add but apparently 1% of the time, you get an add then update. IMO, we should just change the tests to allow an update after the add, WDYT?",
        "pr_file_module": null
      },
      {
        "comment_id": "2246505093",
        "repo_full_name": "istio/istio",
        "pr_number": 57157,
        "pr_file": "pkg/kube/krt/nestedjoinmerge_test.go",
        "discussion_id": "2245512628",
        "commented_code": "@@ -0,0 +1,321 @@\n+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//\thttp://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+package krt_test\n+\n+import (\n+\t\"testing\"\n+\n+\tcorev1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/apimachinery/pkg/util/intstr\"\n+\n+\t\"istio.io/istio/pkg/kube\"\n+\t\"istio.io/istio/pkg/kube/krt\"\n+\t\"istio.io/istio/pkg/maps\"\n+\t\"istio.io/istio/pkg/slices\"\n+\t\"istio.io/istio/pkg/test/util/assert\"\n+)\n+\n+func TestNestedJoin2WithMergeSimpleCollection(t *testing.T) {",
        "comment_created_at": "2025-07-31T22:27:29+00:00",
        "comment_author": "keithmattix",
        "comment_body": "Fixed by adding event checks after each addition",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2248505553",
    "pr_number": 57157,
    "pr_file": "pkg/kube/krt/nestedjoinmerge.go",
    "created_at": "2025-08-01T17:31:09+00:00",
    "commented_code": "+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//\thttp://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+package krt\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"k8s.io/client-go/tools/cache\"\n+\n+\t\"istio.io/istio/pkg/kube\"\n+\t\"istio.io/istio/pkg/kube/controllers\"\n+\t\"istio.io/istio/pkg/maps\"\n+\t\"istio.io/istio/pkg/ptr\"\n+\t\"istio.io/istio/pkg/queue\"\n+\t\"istio.io/istio/pkg/slices\"\n+\t\"istio.io/istio/pkg/util/sets\"\n+)\n+\n+type nestedjoinmerge[T any] struct {\n+\t*mergejoin[T]\n+\tcollections internalCollection[Collection[T]]\n+\tregs        map[collectionUID]HandlerRegistration // registrations for the sub-collections, used to unsubscribe when the collection is deleted\n+}\n+\n+var _ internalCollection[any] = &nestedjoinmerge[any]{}\n+\n+func (j *nestedjoinmerge[T]) Register(f func(e Event[T])) HandlerRegistration {\n+\treturn registerHandlerAsBatched(j, f)\n+}\n+\n+func (j *nestedjoinmerge[T]) RegisterBatch(f func(e []Event[T]), runExistingState bool) HandlerRegistration {\n+\tj.mu.Lock()\n+\tdefer j.mu.Unlock()\n+\tif !runExistingState {\n+\t\t// If we don't to run the initial state this is simple, we just register the handler.\n+\t\treturn j.eventHandlers.Insert(f, j, nil, j.stop)\n+\t}\n+\n+\t// We need to run the initial state, but we don't want to get duplicate events.\n+\t// We should get \"ADD initialObject1, ADD initialObjectN, UPDATE someLaterUpdate\" without mixing the initial ADDs\n+\t// Create ADDs for the current state of the merge cache\n+\tevents := make([]Event[T], 0, len(j.outputs))\n+\tfor _, o := range j.outputs {\n+\t\tevents = append(events, Event[T]{\n+\t\t\tNew:   &o,\n+\t\t\tEvent: controllers.EventAdd,\n+\t\t})\n+\t}\n+\n+\t// Send out all the initial objects to the handler. We will then unlock the new events so it gets the future updates.\n+\treturn j.eventHandlers.Insert(f, j, events, j.stop)\n+}\n+\n+// nolint: unused // (not true, its to implement an interface)\n+func (j *nestedjoinmerge[T]) dump() CollectionDump {\n+\tinnerCols := j.collections.List()\n+\tdumpsByCollectionUID := make(map[string]InputDump, len(innerCols))\n+\tfor _, c := range innerCols {\n+\t\tif c == nil {\n+\t\t\tcontinue\n+\t\t}\n+\t\tic := c.(internalCollection[T])\n+\t\ticDump := ic.dump()\n+\t\tdumpsByCollectionUID[GetKey(ic)] = InputDump{\n+\t\t\tOutputs:      maps.Keys(icDump.Outputs),\n+\t\t\tDependencies: append(maps.Keys(icDump.Inputs), icDump.InputCollection),\n+\t\t}\n+\t}\n+\treturn CollectionDump{\n+\t\tOutputs: eraseMap(slices.GroupUnique(j.List(), getTypedKey)),\n+\t\tSynced:  j.HasSynced(),\n+\t\tInputs:  dumpsByCollectionUID,\n+\t}\n+}\n+\n+// nolint: unused // (not true, its to implement an interface)\n+func (j *nestedjoinmerge[T]) getCollections() []Collection[T] {\n+\t// This is used by the collection lister to get the collections for this join\n+\t// so it can be used in a nested join.\n+\treturn j.collections.List()\n+}\n+\n+func NestedJoinWithMergeCollection[T any](collections Collection[Collection[T]], merge func(ts []T) *T, opts ...CollectionOption) Collection[T] {\n+\to := buildCollectionOptions(opts...)\n+\tif o.name == \"\" {\n+\t\to.name = fmt.Sprintf(\"NestedJoinWithMerge[%v]\", ptr.TypeName[T]())\n+\t}\n+\n+\tics := collections.(internalCollection[Collection[T]])\n+\tsynced := make(chan struct{})\n+\n+\tj := &nestedjoinmerge[T]{\n+\t\tmergejoin: &mergejoin[T]{\n+\t\t\tid:             nextUID(),\n+\t\t\tcollectionName: o.name,\n+\t\t\tlog:            log.WithLabels(\"owner\", o.name),\n+\t\t\toutputs:        make(map[Key[T]]T),\n+\t\t\tindexes:        make(map[string]joinCollectionIndex[T]),\n+\t\t\teventHandlers:  newHandlerSet[T](),\n+\t\t\tmerge:          merge,\n+\t\t\tsynced:         synced,\n+\t\t\tstop:           o.stop,\n+\t\t},\n+\t\tcollections: ics,\n+\t\tregs:        make(map[collectionUID]HandlerRegistration),\n+\t}\n+\n+\tj.mergejoin.collections = j\n+\tj.syncer = channelSyncer{\n+\t\tname:   j.collectionName,\n+\t\tsynced: j.synced,\n+\t}\n+\n+\tmaybeRegisterCollectionForDebugging(j, o.debugger)\n+\n+\t// Create our queue. When it syncs (that is, all items that were present when Run() was called), we mark ourselves as synced.\n+\tj.queue = queue.NewWithSync(func() {\n+\t\tclose(j.synced)\n+\t\tj.log.Infof(\"%v synced (uid %v)\", j.name(), j.uid())\n+\t}, j.collectionName)\n+\n+\t// Subscribe to when collections are added or removed.\n+\t// Don't run existing because we want to ensure the first set\n+\t// of collections passed to us are synced before we mark\n+\t// ourselves as synced. Do this before returning so collections\n+\t// aren't added between now and when runQueue is called.\n+\tsubscriptionFunc := func(events []Event[T]) {\n+\t\tj.queue.Push(func() error {\n+\t\t\tj.onSubCollectionEventHandler(events)\n+\t\t\treturn nil\n+\t\t})\n+\t}\n+\treg := j.collections.RegisterBatch(func(e []Event[Collection[T]]) {\n+\t\tfor _, ev := range e {\n+\t\t\tobj := ev.Latest()\n+\t\t\tswitch ev.Event {\n+\t\t\tcase controllers.EventAdd:\n+\t\t\t\t// When a collection is added, subscribe to its events\n+\t\t\t\treg := obj.RegisterBatch(subscriptionFunc, true)\n+\t\t\t\tj.mu.Lock()\n+\t\t\t\tj.regs[obj.(internalCollection[T]).uid()] = reg\n+\t\t\t\tj.mu.Unlock()\n+\t\t\tcase controllers.EventUpdate:\n+\t\t\t\tj.handleCollectionUpdate(ev)\n+\t\t\tcase controllers.EventDelete:\n+\t\t\t\tj.handleCollectionDelete(ev)\n+\t\t\t}\n+\t\t}\n+\t}, false)\n+\tinitialCollections := j.collections.List()\n+\t// Finally, async wait for the primary to be synced. Once it has, we know it has enqueued the initial state.\n+\t// After this, we can run our queue.\n+\t// The queue will process the initial state and mark ourselves as synced (from the NewWithSync callback)\n+\tgo j.runQueue(initialCollections, subscriptionFunc, reg)\n+\n+\treturn j\n+}\n+\n+func (j *nestedjoinmerge[T]) runQueue(initialCollections []Collection[T], subscriptionFunc func([]Event[T]), reg HandlerRegistration) {\n+\t// Wait for the container of collections to be synced before we start processing events.\n+\tif !j.collections.WaitUntilSynced(j.stop) {\n+\t\treturn\n+\t}\n+\tj.mu.Lock()\n+\n+\t// Now that we've subscribed, process the current set of collections.\n+\tfor _, c := range initialCollections {\n+\t\t// Save these registrations so we can unsubscribe later if the collection is deleted.\n+\t\t// Ensure each sub-collection is synced before we're marked as synced.\n+\t\tj.regs[c.(internalCollection[T]).uid()] = c.RegisterBatch(subscriptionFunc, true)\n+\t}\n+\n+\tregs := append([]HandlerRegistration{reg}, maps.Values(j.regs)...)\n+\tj.mu.Unlock()\n+\n+\tsyncers := slices.Map(regs, func(r HandlerRegistration) cache.InformerSynced {\n+\t\treturn r.HasSynced\n+\t})\n+\n+\tif !kube.WaitForCacheSync(j.collectionName, j.stop, syncers...) {\n+\t\treturn\n+\t}\n+\tj.queue.Run(j.stop)\n+}\n+\n+func (j *nestedjoinmerge[T]) handleCollectionUpdate(e Event[Collection[T]]) {\n+\tinnerCollection := e.Latest().(internalCollection[T])\n+\tlog.Debugf(\"NestedJoinWithMergeCollection: Collection %s (uid %s) updated, recalculating merged values\", innerCollection.name(), innerCollection.uid())\n+\t// Get all of the elements in the old collection\n+\toldCollectionValue := *e.Old\n+\tnewCollectionValue := *e.New\n+\t// Wait for the new collection to be synced before we process the update.\n+\tif !newCollectionValue.WaitUntilSynced(j.stop) {\n+\t\tlog.Warnf(\"NestedJoinWithMergeCollection: Collection %s not synced, skipping update event\", newCollectionValue.(internalCollection[T]).uid())\n+\t}\n+\t// Stop the world and update our outputs with new state for everything in the collection.\n+\tj.mu.Lock()",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "2248505553",
        "repo_full_name": "istio/istio",
        "pr_number": 57157,
        "pr_file": "pkg/kube/krt/nestedjoinmerge.go",
        "discussion_id": "2248505553",
        "commented_code": "@@ -0,0 +1,365 @@\n+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//\thttp://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+package krt\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"k8s.io/client-go/tools/cache\"\n+\n+\t\"istio.io/istio/pkg/kube\"\n+\t\"istio.io/istio/pkg/kube/controllers\"\n+\t\"istio.io/istio/pkg/maps\"\n+\t\"istio.io/istio/pkg/ptr\"\n+\t\"istio.io/istio/pkg/queue\"\n+\t\"istio.io/istio/pkg/slices\"\n+\t\"istio.io/istio/pkg/util/sets\"\n+)\n+\n+type nestedjoinmerge[T any] struct {\n+\t*mergejoin[T]\n+\tcollections internalCollection[Collection[T]]\n+\tregs        map[collectionUID]HandlerRegistration // registrations for the sub-collections, used to unsubscribe when the collection is deleted\n+}\n+\n+var _ internalCollection[any] = &nestedjoinmerge[any]{}\n+\n+func (j *nestedjoinmerge[T]) Register(f func(e Event[T])) HandlerRegistration {\n+\treturn registerHandlerAsBatched(j, f)\n+}\n+\n+func (j *nestedjoinmerge[T]) RegisterBatch(f func(e []Event[T]), runExistingState bool) HandlerRegistration {\n+\tj.mu.Lock()\n+\tdefer j.mu.Unlock()\n+\tif !runExistingState {\n+\t\t// If we don't to run the initial state this is simple, we just register the handler.\n+\t\treturn j.eventHandlers.Insert(f, j, nil, j.stop)\n+\t}\n+\n+\t// We need to run the initial state, but we don't want to get duplicate events.\n+\t// We should get \"ADD initialObject1, ADD initialObjectN, UPDATE someLaterUpdate\" without mixing the initial ADDs\n+\t// Create ADDs for the current state of the merge cache\n+\tevents := make([]Event[T], 0, len(j.outputs))\n+\tfor _, o := range j.outputs {\n+\t\tevents = append(events, Event[T]{\n+\t\t\tNew:   &o,\n+\t\t\tEvent: controllers.EventAdd,\n+\t\t})\n+\t}\n+\n+\t// Send out all the initial objects to the handler. We will then unlock the new events so it gets the future updates.\n+\treturn j.eventHandlers.Insert(f, j, events, j.stop)\n+}\n+\n+// nolint: unused // (not true, its to implement an interface)\n+func (j *nestedjoinmerge[T]) dump() CollectionDump {\n+\tinnerCols := j.collections.List()\n+\tdumpsByCollectionUID := make(map[string]InputDump, len(innerCols))\n+\tfor _, c := range innerCols {\n+\t\tif c == nil {\n+\t\t\tcontinue\n+\t\t}\n+\t\tic := c.(internalCollection[T])\n+\t\ticDump := ic.dump()\n+\t\tdumpsByCollectionUID[GetKey(ic)] = InputDump{\n+\t\t\tOutputs:      maps.Keys(icDump.Outputs),\n+\t\t\tDependencies: append(maps.Keys(icDump.Inputs), icDump.InputCollection),\n+\t\t}\n+\t}\n+\treturn CollectionDump{\n+\t\tOutputs: eraseMap(slices.GroupUnique(j.List(), getTypedKey)),\n+\t\tSynced:  j.HasSynced(),\n+\t\tInputs:  dumpsByCollectionUID,\n+\t}\n+}\n+\n+// nolint: unused // (not true, its to implement an interface)\n+func (j *nestedjoinmerge[T]) getCollections() []Collection[T] {\n+\t// This is used by the collection lister to get the collections for this join\n+\t// so it can be used in a nested join.\n+\treturn j.collections.List()\n+}\n+\n+func NestedJoinWithMergeCollection[T any](collections Collection[Collection[T]], merge func(ts []T) *T, opts ...CollectionOption) Collection[T] {\n+\to := buildCollectionOptions(opts...)\n+\tif o.name == \"\" {\n+\t\to.name = fmt.Sprintf(\"NestedJoinWithMerge[%v]\", ptr.TypeName[T]())\n+\t}\n+\n+\tics := collections.(internalCollection[Collection[T]])\n+\tsynced := make(chan struct{})\n+\n+\tj := &nestedjoinmerge[T]{\n+\t\tmergejoin: &mergejoin[T]{\n+\t\t\tid:             nextUID(),\n+\t\t\tcollectionName: o.name,\n+\t\t\tlog:            log.WithLabels(\"owner\", o.name),\n+\t\t\toutputs:        make(map[Key[T]]T),\n+\t\t\tindexes:        make(map[string]joinCollectionIndex[T]),\n+\t\t\teventHandlers:  newHandlerSet[T](),\n+\t\t\tmerge:          merge,\n+\t\t\tsynced:         synced,\n+\t\t\tstop:           o.stop,\n+\t\t},\n+\t\tcollections: ics,\n+\t\tregs:        make(map[collectionUID]HandlerRegistration),\n+\t}\n+\n+\tj.mergejoin.collections = j\n+\tj.syncer = channelSyncer{\n+\t\tname:   j.collectionName,\n+\t\tsynced: j.synced,\n+\t}\n+\n+\tmaybeRegisterCollectionForDebugging(j, o.debugger)\n+\n+\t// Create our queue. When it syncs (that is, all items that were present when Run() was called), we mark ourselves as synced.\n+\tj.queue = queue.NewWithSync(func() {\n+\t\tclose(j.synced)\n+\t\tj.log.Infof(\"%v synced (uid %v)\", j.name(), j.uid())\n+\t}, j.collectionName)\n+\n+\t// Subscribe to when collections are added or removed.\n+\t// Don't run existing because we want to ensure the first set\n+\t// of collections passed to us are synced before we mark\n+\t// ourselves as synced. Do this before returning so collections\n+\t// aren't added between now and when runQueue is called.\n+\tsubscriptionFunc := func(events []Event[T]) {\n+\t\tj.queue.Push(func() error {\n+\t\t\tj.onSubCollectionEventHandler(events)\n+\t\t\treturn nil\n+\t\t})\n+\t}\n+\treg := j.collections.RegisterBatch(func(e []Event[Collection[T]]) {\n+\t\tfor _, ev := range e {\n+\t\t\tobj := ev.Latest()\n+\t\t\tswitch ev.Event {\n+\t\t\tcase controllers.EventAdd:\n+\t\t\t\t// When a collection is added, subscribe to its events\n+\t\t\t\treg := obj.RegisterBatch(subscriptionFunc, true)\n+\t\t\t\tj.mu.Lock()\n+\t\t\t\tj.regs[obj.(internalCollection[T]).uid()] = reg\n+\t\t\t\tj.mu.Unlock()\n+\t\t\tcase controllers.EventUpdate:\n+\t\t\t\tj.handleCollectionUpdate(ev)\n+\t\t\tcase controllers.EventDelete:\n+\t\t\t\tj.handleCollectionDelete(ev)\n+\t\t\t}\n+\t\t}\n+\t}, false)\n+\tinitialCollections := j.collections.List()\n+\t// Finally, async wait for the primary to be synced. Once it has, we know it has enqueued the initial state.\n+\t// After this, we can run our queue.\n+\t// The queue will process the initial state and mark ourselves as synced (from the NewWithSync callback)\n+\tgo j.runQueue(initialCollections, subscriptionFunc, reg)\n+\n+\treturn j\n+}\n+\n+func (j *nestedjoinmerge[T]) runQueue(initialCollections []Collection[T], subscriptionFunc func([]Event[T]), reg HandlerRegistration) {\n+\t// Wait for the container of collections to be synced before we start processing events.\n+\tif !j.collections.WaitUntilSynced(j.stop) {\n+\t\treturn\n+\t}\n+\tj.mu.Lock()\n+\n+\t// Now that we've subscribed, process the current set of collections.\n+\tfor _, c := range initialCollections {\n+\t\t// Save these registrations so we can unsubscribe later if the collection is deleted.\n+\t\t// Ensure each sub-collection is synced before we're marked as synced.\n+\t\tj.regs[c.(internalCollection[T]).uid()] = c.RegisterBatch(subscriptionFunc, true)\n+\t}\n+\n+\tregs := append([]HandlerRegistration{reg}, maps.Values(j.regs)...)\n+\tj.mu.Unlock()\n+\n+\tsyncers := slices.Map(regs, func(r HandlerRegistration) cache.InformerSynced {\n+\t\treturn r.HasSynced\n+\t})\n+\n+\tif !kube.WaitForCacheSync(j.collectionName, j.stop, syncers...) {\n+\t\treturn\n+\t}\n+\tj.queue.Run(j.stop)\n+}\n+\n+func (j *nestedjoinmerge[T]) handleCollectionUpdate(e Event[Collection[T]]) {\n+\tinnerCollection := e.Latest().(internalCollection[T])\n+\tlog.Debugf(\"NestedJoinWithMergeCollection: Collection %s (uid %s) updated, recalculating merged values\", innerCollection.name(), innerCollection.uid())\n+\t// Get all of the elements in the old collection\n+\toldCollectionValue := *e.Old\n+\tnewCollectionValue := *e.New\n+\t// Wait for the new collection to be synced before we process the update.\n+\tif !newCollectionValue.WaitUntilSynced(j.stop) {\n+\t\tlog.Warnf(\"NestedJoinWithMergeCollection: Collection %s not synced, skipping update event\", newCollectionValue.(internalCollection[T]).uid())\n+\t}\n+\t// Stop the world and update our outputs with new state for everything in the collection.\n+\tj.mu.Lock()",
        "comment_created_at": "2025-08-01T17:31:09+00:00",
        "comment_author": "stevenctl",
        "comment_body": "Should this locking happen before or after we wait for sync? Is it possible that we have something like:\r\n\r\n1. Collection is updated from v1 -> v2, so we start waiting\r\n2. While we are waiting, we do v2 -> v3\r\n3. Now we hit this same path for v3, whichever readies first between v2 and v3 will go first \r\n\r\nMain concerns would be v3 syncing, v2 is no longer really relevant, but it loses the race and we process it's (probably empty?) items last. \r\n\r\nI think either we lock before we wait _or_ we have an additional check to see if there was another collectionUpdate while we were waiting for sync. \r\n\r\nProbably a very rare case in practice.. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2248632140",
        "repo_full_name": "istio/istio",
        "pr_number": 57157,
        "pr_file": "pkg/kube/krt/nestedjoinmerge.go",
        "discussion_id": "2248505553",
        "commented_code": "@@ -0,0 +1,365 @@\n+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//\thttp://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+package krt\n+\n+import (\n+\t\"fmt\"\n+\n+\t\"k8s.io/client-go/tools/cache\"\n+\n+\t\"istio.io/istio/pkg/kube\"\n+\t\"istio.io/istio/pkg/kube/controllers\"\n+\t\"istio.io/istio/pkg/maps\"\n+\t\"istio.io/istio/pkg/ptr\"\n+\t\"istio.io/istio/pkg/queue\"\n+\t\"istio.io/istio/pkg/slices\"\n+\t\"istio.io/istio/pkg/util/sets\"\n+)\n+\n+type nestedjoinmerge[T any] struct {\n+\t*mergejoin[T]\n+\tcollections internalCollection[Collection[T]]\n+\tregs        map[collectionUID]HandlerRegistration // registrations for the sub-collections, used to unsubscribe when the collection is deleted\n+}\n+\n+var _ internalCollection[any] = &nestedjoinmerge[any]{}\n+\n+func (j *nestedjoinmerge[T]) Register(f func(e Event[T])) HandlerRegistration {\n+\treturn registerHandlerAsBatched(j, f)\n+}\n+\n+func (j *nestedjoinmerge[T]) RegisterBatch(f func(e []Event[T]), runExistingState bool) HandlerRegistration {\n+\tj.mu.Lock()\n+\tdefer j.mu.Unlock()\n+\tif !runExistingState {\n+\t\t// If we don't to run the initial state this is simple, we just register the handler.\n+\t\treturn j.eventHandlers.Insert(f, j, nil, j.stop)\n+\t}\n+\n+\t// We need to run the initial state, but we don't want to get duplicate events.\n+\t// We should get \"ADD initialObject1, ADD initialObjectN, UPDATE someLaterUpdate\" without mixing the initial ADDs\n+\t// Create ADDs for the current state of the merge cache\n+\tevents := make([]Event[T], 0, len(j.outputs))\n+\tfor _, o := range j.outputs {\n+\t\tevents = append(events, Event[T]{\n+\t\t\tNew:   &o,\n+\t\t\tEvent: controllers.EventAdd,\n+\t\t})\n+\t}\n+\n+\t// Send out all the initial objects to the handler. We will then unlock the new events so it gets the future updates.\n+\treturn j.eventHandlers.Insert(f, j, events, j.stop)\n+}\n+\n+// nolint: unused // (not true, its to implement an interface)\n+func (j *nestedjoinmerge[T]) dump() CollectionDump {\n+\tinnerCols := j.collections.List()\n+\tdumpsByCollectionUID := make(map[string]InputDump, len(innerCols))\n+\tfor _, c := range innerCols {\n+\t\tif c == nil {\n+\t\t\tcontinue\n+\t\t}\n+\t\tic := c.(internalCollection[T])\n+\t\ticDump := ic.dump()\n+\t\tdumpsByCollectionUID[GetKey(ic)] = InputDump{\n+\t\t\tOutputs:      maps.Keys(icDump.Outputs),\n+\t\t\tDependencies: append(maps.Keys(icDump.Inputs), icDump.InputCollection),\n+\t\t}\n+\t}\n+\treturn CollectionDump{\n+\t\tOutputs: eraseMap(slices.GroupUnique(j.List(), getTypedKey)),\n+\t\tSynced:  j.HasSynced(),\n+\t\tInputs:  dumpsByCollectionUID,\n+\t}\n+}\n+\n+// nolint: unused // (not true, its to implement an interface)\n+func (j *nestedjoinmerge[T]) getCollections() []Collection[T] {\n+\t// This is used by the collection lister to get the collections for this join\n+\t// so it can be used in a nested join.\n+\treturn j.collections.List()\n+}\n+\n+func NestedJoinWithMergeCollection[T any](collections Collection[Collection[T]], merge func(ts []T) *T, opts ...CollectionOption) Collection[T] {\n+\to := buildCollectionOptions(opts...)\n+\tif o.name == \"\" {\n+\t\to.name = fmt.Sprintf(\"NestedJoinWithMerge[%v]\", ptr.TypeName[T]())\n+\t}\n+\n+\tics := collections.(internalCollection[Collection[T]])\n+\tsynced := make(chan struct{})\n+\n+\tj := &nestedjoinmerge[T]{\n+\t\tmergejoin: &mergejoin[T]{\n+\t\t\tid:             nextUID(),\n+\t\t\tcollectionName: o.name,\n+\t\t\tlog:            log.WithLabels(\"owner\", o.name),\n+\t\t\toutputs:        make(map[Key[T]]T),\n+\t\t\tindexes:        make(map[string]joinCollectionIndex[T]),\n+\t\t\teventHandlers:  newHandlerSet[T](),\n+\t\t\tmerge:          merge,\n+\t\t\tsynced:         synced,\n+\t\t\tstop:           o.stop,\n+\t\t},\n+\t\tcollections: ics,\n+\t\tregs:        make(map[collectionUID]HandlerRegistration),\n+\t}\n+\n+\tj.mergejoin.collections = j\n+\tj.syncer = channelSyncer{\n+\t\tname:   j.collectionName,\n+\t\tsynced: j.synced,\n+\t}\n+\n+\tmaybeRegisterCollectionForDebugging(j, o.debugger)\n+\n+\t// Create our queue. When it syncs (that is, all items that were present when Run() was called), we mark ourselves as synced.\n+\tj.queue = queue.NewWithSync(func() {\n+\t\tclose(j.synced)\n+\t\tj.log.Infof(\"%v synced (uid %v)\", j.name(), j.uid())\n+\t}, j.collectionName)\n+\n+\t// Subscribe to when collections are added or removed.\n+\t// Don't run existing because we want to ensure the first set\n+\t// of collections passed to us are synced before we mark\n+\t// ourselves as synced. Do this before returning so collections\n+\t// aren't added between now and when runQueue is called.\n+\tsubscriptionFunc := func(events []Event[T]) {\n+\t\tj.queue.Push(func() error {\n+\t\t\tj.onSubCollectionEventHandler(events)\n+\t\t\treturn nil\n+\t\t})\n+\t}\n+\treg := j.collections.RegisterBatch(func(e []Event[Collection[T]]) {\n+\t\tfor _, ev := range e {\n+\t\t\tobj := ev.Latest()\n+\t\t\tswitch ev.Event {\n+\t\t\tcase controllers.EventAdd:\n+\t\t\t\t// When a collection is added, subscribe to its events\n+\t\t\t\treg := obj.RegisterBatch(subscriptionFunc, true)\n+\t\t\t\tj.mu.Lock()\n+\t\t\t\tj.regs[obj.(internalCollection[T]).uid()] = reg\n+\t\t\t\tj.mu.Unlock()\n+\t\t\tcase controllers.EventUpdate:\n+\t\t\t\tj.handleCollectionUpdate(ev)\n+\t\t\tcase controllers.EventDelete:\n+\t\t\t\tj.handleCollectionDelete(ev)\n+\t\t\t}\n+\t\t}\n+\t}, false)\n+\tinitialCollections := j.collections.List()\n+\t// Finally, async wait for the primary to be synced. Once it has, we know it has enqueued the initial state.\n+\t// After this, we can run our queue.\n+\t// The queue will process the initial state and mark ourselves as synced (from the NewWithSync callback)\n+\tgo j.runQueue(initialCollections, subscriptionFunc, reg)\n+\n+\treturn j\n+}\n+\n+func (j *nestedjoinmerge[T]) runQueue(initialCollections []Collection[T], subscriptionFunc func([]Event[T]), reg HandlerRegistration) {\n+\t// Wait for the container of collections to be synced before we start processing events.\n+\tif !j.collections.WaitUntilSynced(j.stop) {\n+\t\treturn\n+\t}\n+\tj.mu.Lock()\n+\n+\t// Now that we've subscribed, process the current set of collections.\n+\tfor _, c := range initialCollections {\n+\t\t// Save these registrations so we can unsubscribe later if the collection is deleted.\n+\t\t// Ensure each sub-collection is synced before we're marked as synced.\n+\t\tj.regs[c.(internalCollection[T]).uid()] = c.RegisterBatch(subscriptionFunc, true)\n+\t}\n+\n+\tregs := append([]HandlerRegistration{reg}, maps.Values(j.regs)...)\n+\tj.mu.Unlock()\n+\n+\tsyncers := slices.Map(regs, func(r HandlerRegistration) cache.InformerSynced {\n+\t\treturn r.HasSynced\n+\t})\n+\n+\tif !kube.WaitForCacheSync(j.collectionName, j.stop, syncers...) {\n+\t\treturn\n+\t}\n+\tj.queue.Run(j.stop)\n+}\n+\n+func (j *nestedjoinmerge[T]) handleCollectionUpdate(e Event[Collection[T]]) {\n+\tinnerCollection := e.Latest().(internalCollection[T])\n+\tlog.Debugf(\"NestedJoinWithMergeCollection: Collection %s (uid %s) updated, recalculating merged values\", innerCollection.name(), innerCollection.uid())\n+\t// Get all of the elements in the old collection\n+\toldCollectionValue := *e.Old\n+\tnewCollectionValue := *e.New\n+\t// Wait for the new collection to be synced before we process the update.\n+\tif !newCollectionValue.WaitUntilSynced(j.stop) {\n+\t\tlog.Warnf(\"NestedJoinWithMergeCollection: Collection %s not synced, skipping update event\", newCollectionValue.(internalCollection[T]).uid())\n+\t}\n+\t// Stop the world and update our outputs with new state for everything in the collection.\n+\tj.mu.Lock()",
        "comment_created_at": "2025-08-01T18:44:19+00:00",
        "comment_author": "keithmattix",
        "comment_body": "The `RegisterBatch` call on the collection of collections happens once in one goroutine, so 3) would never happen right? The event processor would block until the v1 -> v2 event had been processed by this handler. Then we'd get the v2 -> v3 event",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1382288470",
    "pr_number": 47273,
    "pr_file": "pilot/pkg/model/service.go",
    "created_at": "2023-11-03T23:37:55+00:00",
    "commented_code": "return istioEndpointCmpOpts\n }\n \n+// SortIstioEndpointByAddresses returns the IstioEndpoint address slices sorted by its addresses.\n+func (ep *IstioEndpoint) SortIstioEndpointByAddresses() []string {\n+\tif ep == nil {\n+\t\treturn nil\n+\t}\n+\n+\taddrSlice := ep.Addresses\n+\tsort.Sort(sort.Reverse(sort.StringSlice(addrSlice)))",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "1382288470",
        "repo_full_name": "istio/istio",
        "pr_number": 47273,
        "pr_file": "pilot/pkg/model/service.go",
        "discussion_id": "1382288470",
        "commented_code": "@@ -593,6 +612,29 @@ func (ep *IstioEndpoint) CmpOpts() []cmp.Option {\n \treturn istioEndpointCmpOpts\n }\n \n+// SortIstioEndpointByAddresses returns the IstioEndpoint address slices sorted by its addresses.\n+func (ep *IstioEndpoint) SortIstioEndpointByAddresses() []string {\n+\tif ep == nil {\n+\t\treturn nil\n+\t}\n+\n+\taddrSlice := ep.Addresses\n+\tsort.Sort(sort.Reverse(sort.StringSlice(addrSlice)))",
        "comment_created_at": "2023-11-03T23:37:55+00:00",
        "comment_author": "howardjohn",
        "comment_body": "not clear why we reverse.\r\n\r\nAlso we are mutating in place AND returning - seems weird?",
        "pr_file_module": null
      },
      {
        "comment_id": "1382288551",
        "repo_full_name": "istio/istio",
        "pr_number": 47273,
        "pr_file": "pilot/pkg/model/service.go",
        "discussion_id": "1382288470",
        "commented_code": "@@ -593,6 +612,29 @@ func (ep *IstioEndpoint) CmpOpts() []cmp.Option {\n \treturn istioEndpointCmpOpts\n }\n \n+// SortIstioEndpointByAddresses returns the IstioEndpoint address slices sorted by its addresses.\n+func (ep *IstioEndpoint) SortIstioEndpointByAddresses() []string {\n+\tif ep == nil {\n+\t\treturn nil\n+\t}\n+\n+\taddrSlice := ep.Addresses\n+\tsort.Sort(sort.Reverse(sort.StringSlice(addrSlice)))",
        "comment_created_at": "2023-11-03T23:38:10+00:00",
        "comment_author": "howardjohn",
        "comment_body": "I don't see how we can mutate without race conditions.",
        "pr_file_module": null
      },
      {
        "comment_id": "1382288851",
        "repo_full_name": "istio/istio",
        "pr_number": 47273,
        "pr_file": "pilot/pkg/model/service.go",
        "discussion_id": "1382288470",
        "commented_code": "@@ -593,6 +612,29 @@ func (ep *IstioEndpoint) CmpOpts() []cmp.Option {\n \treturn istioEndpointCmpOpts\n }\n \n+// SortIstioEndpointByAddresses returns the IstioEndpoint address slices sorted by its addresses.\n+func (ep *IstioEndpoint) SortIstioEndpointByAddresses() []string {\n+\tif ep == nil {\n+\t\treturn nil\n+\t}\n+\n+\taddrSlice := ep.Addresses\n+\tsort.Sort(sort.Reverse(sort.StringSlice(addrSlice)))",
        "comment_created_at": "2023-11-03T23:39:09+00:00",
        "comment_author": "howardjohn",
        "comment_body": "^ all these concerns apply to Key as well",
        "pr_file_module": null
      },
      {
        "comment_id": "1383068386",
        "repo_full_name": "istio/istio",
        "pr_number": 47273,
        "pr_file": "pilot/pkg/model/service.go",
        "discussion_id": "1382288470",
        "commented_code": "@@ -593,6 +612,29 @@ func (ep *IstioEndpoint) CmpOpts() []cmp.Option {\n \treturn istioEndpointCmpOpts\n }\n \n+// SortIstioEndpointByAddresses returns the IstioEndpoint address slices sorted by its addresses.\n+func (ep *IstioEndpoint) SortIstioEndpointByAddresses() []string {\n+\tif ep == nil {\n+\t\treturn nil\n+\t}\n+\n+\taddrSlice := ep.Addresses\n+\tsort.Sort(sort.Reverse(sort.StringSlice(addrSlice)))",
        "comment_created_at": "2023-11-06T10:19:17+00:00",
        "comment_author": "zhlsunshine",
        "comment_body": "Hi @howardjohn, based on above comment, I remove function of `SortIstioEndpointByAddresses` and add new function of `IsAddrsEqualIstioEndpoint`. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1114859167",
    "pr_number": 43537,
    "pr_file": "pilot/pkg/config/kube/gateway/deploymentcontroller.go",
    "created_at": "2023-02-22T19:28:19+00:00",
    "commented_code": "Values      map[string]any\n }\n \n+func setInputValue(input derivedInput, field string, value any) {\n+\tentry := input.Values\n+\tkeys := strings.Split(field, \".\")\n+\tfor i := 0; i < len(keys)-1; i++ {\n+\t\tval, ok := entry[keys[i]]\n+\t\tif !ok {\n+\t\t\tval = map[string]any{}\n+\t\t\tentry[keys[i]] = val\n+\t\t}\n+\t\tentry = val.(map[string]any)",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "1114859167",
        "repo_full_name": "istio/istio",
        "pr_number": 43537,
        "pr_file": "pilot/pkg/config/kube/gateway/deploymentcontroller.go",
        "discussion_id": "1114859167",
        "commented_code": "@@ -375,6 +375,20 @@ type derivedInput struct {\n \tValues      map[string]any\n }\n \n+func setInputValue(input derivedInput, field string, value any) {\n+\tentry := input.Values\n+\tkeys := strings.Split(field, \".\")\n+\tfor i := 0; i < len(keys)-1; i++ {\n+\t\tval, ok := entry[keys[i]]\n+\t\tif !ok {\n+\t\t\tval = map[string]any{}\n+\t\t\tentry[keys[i]] = val\n+\t\t}\n+\t\tentry = val.(map[string]any)",
        "comment_created_at": "2023-02-22T19:28:19+00:00",
        "comment_author": "howardjohn",
        "comment_body": "I think the map we are setting is shared between sidecar injection as well. Is mutating this going to be safe? At the very least seems like we might get a race from read+write at the same time.\r\n\r\nIt may make sense to re-use the code in overwriteClusterInfo which does this after rendering instead of as input?\r\n\r\nWe could also just add a Cluster param to TemplateInput and use that.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1114859822",
        "repo_full_name": "istio/istio",
        "pr_number": 43537,
        "pr_file": "pilot/pkg/config/kube/gateway/deploymentcontroller.go",
        "discussion_id": "1114859167",
        "commented_code": "@@ -375,6 +375,20 @@ type derivedInput struct {\n \tValues      map[string]any\n }\n \n+func setInputValue(input derivedInput, field string, value any) {\n+\tentry := input.Values\n+\tkeys := strings.Split(field, \".\")\n+\tfor i := 0; i < len(keys)-1; i++ {\n+\t\tval, ok := entry[keys[i]]\n+\t\tif !ok {\n+\t\t\tval = map[string]any{}\n+\t\t\tentry[keys[i]] = val\n+\t\t}\n+\t\tentry = val.(map[string]any)",
        "comment_created_at": "2023-02-22T19:29:02+00:00",
        "comment_author": "howardjohn",
        "comment_body": "If using overwriteClusterInfo is hard due to Unstructured, I may have a change coming that makes it easier... let me get back to you soon (if its a blocker).",
        "pr_file_module": null
      },
      {
        "comment_id": "1115003965",
        "repo_full_name": "istio/istio",
        "pr_number": 43537,
        "pr_file": "pilot/pkg/config/kube/gateway/deploymentcontroller.go",
        "discussion_id": "1114859167",
        "commented_code": "@@ -375,6 +375,20 @@ type derivedInput struct {\n \tValues      map[string]any\n }\n \n+func setInputValue(input derivedInput, field string, value any) {\n+\tentry := input.Values\n+\tkeys := strings.Split(field, \".\")\n+\tfor i := 0; i < len(keys)-1; i++ {\n+\t\tval, ok := entry[keys[i]]\n+\t\tif !ok {\n+\t\t\tval = map[string]any{}\n+\t\t\tentry[keys[i]] = val\n+\t\t}\n+\t\tentry = val.(map[string]any)",
        "comment_created_at": "2023-02-22T21:53:11+00:00",
        "comment_author": "howardjohn",
        "comment_body": "https://github.com/istio/istio/pull/43541 was the change. Not sure it helps much",
        "pr_file_module": null
      },
      {
        "comment_id": "1115856855",
        "repo_full_name": "istio/istio",
        "pr_number": 43537,
        "pr_file": "pilot/pkg/config/kube/gateway/deploymentcontroller.go",
        "discussion_id": "1114859167",
        "commented_code": "@@ -375,6 +375,20 @@ type derivedInput struct {\n \tValues      map[string]any\n }\n \n+func setInputValue(input derivedInput, field string, value any) {\n+\tentry := input.Values\n+\tkeys := strings.Split(field, \".\")\n+\tfor i := 0; i < len(keys)-1; i++ {\n+\t\tval, ok := entry[keys[i]]\n+\t\tif !ok {\n+\t\t\tval = map[string]any{}\n+\t\t\tentry[keys[i]] = val\n+\t\t}\n+\t\tentry = val.(map[string]any)",
        "comment_created_at": "2023-02-23T15:27:57+00:00",
        "comment_author": "frankbu",
        "comment_body": "Added ClusterID to TemplateInput",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "991457464",
    "pr_number": 41198,
    "pr_file": "pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go",
    "created_at": "2022-10-10T16:17:10+00:00",
    "commented_code": "// skip special kubernetes system namespaces\n \t\t\treturn false\n \t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.Filter(o) {\n+\t\t\t// This is a change to a configmap we don't watch, ignore it\n+\t\t\treturn false\n+\t\t}\n \t\treturn true\n \t}))\n \tc.namespacesInformer.AddEventHandler(controllers.FilteredObjectSpecHandler(c.queue.AddObject, func(o controllers.Object) bool {\n-\t\treturn !inject.IgnoredNamespaces.Contains(o.GetName())\n+\t\tif inject.IgnoredNamespaces.Contains(o.GetName()) {\n+\t\t\t// skip special kubernetes system namespaces\n+\t\t\treturn false\n+\t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.FilterNamespace(o.GetName()) {",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "991457464",
        "repo_full_name": "istio/istio",
        "pr_number": 41198,
        "pr_file": "pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go",
        "discussion_id": "991457464",
        "commented_code": "@@ -72,10 +80,22 @@ func NewNamespaceController(kubeClient kube.Client, caBundleWatcher *keycertbund\n \t\t\t// skip special kubernetes system namespaces\n \t\t\treturn false\n \t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.Filter(o) {\n+\t\t\t// This is a change to a configmap we don't watch, ignore it\n+\t\t\treturn false\n+\t\t}\n \t\treturn true\n \t}))\n \tc.namespacesInformer.AddEventHandler(controllers.FilteredObjectSpecHandler(c.queue.AddObject, func(o controllers.Object) bool {\n-\t\treturn !inject.IgnoredNamespaces.Contains(o.GetName())\n+\t\tif inject.IgnoredNamespaces.Contains(o.GetName()) {\n+\t\t\t// skip special kubernetes system namespaces\n+\t\t\treturn false\n+\t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.FilterNamespace(o.GetName()) {",
        "comment_created_at": "2022-10-10T16:17:10+00:00",
        "comment_author": "howardjohn",
        "comment_body": "One concern...\r\n\r\nconsider a namespace is added:\r\n* We have 2 informers for namespace, namespacecontroller and DiscoveryFilter\r\n* Events are triggered in random order to these\r\n* namespacecontroller gets \"Add ns foo\" event. It checks the discovery filter, and it is not present so it ignores it\r\n* DiscoveryFilter gets \"Add ns foo\" event. It should be included, so we add it\r\n\r\nNow we have failed to configure `foo`?",
        "pr_file_module": null
      },
      {
        "comment_id": "991601245",
        "repo_full_name": "istio/istio",
        "pr_number": 41198,
        "pr_file": "pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go",
        "discussion_id": "991457464",
        "commented_code": "@@ -72,10 +80,22 @@ func NewNamespaceController(kubeClient kube.Client, caBundleWatcher *keycertbund\n \t\t\t// skip special kubernetes system namespaces\n \t\t\treturn false\n \t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.Filter(o) {\n+\t\t\t// This is a change to a configmap we don't watch, ignore it\n+\t\t\treturn false\n+\t\t}\n \t\treturn true\n \t}))\n \tc.namespacesInformer.AddEventHandler(controllers.FilteredObjectSpecHandler(c.queue.AddObject, func(o controllers.Object) bool {\n-\t\treturn !inject.IgnoredNamespaces.Contains(o.GetName())\n+\t\tif inject.IgnoredNamespaces.Contains(o.GetName()) {\n+\t\t\t// skip special kubernetes system namespaces\n+\t\t\treturn false\n+\t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.FilterNamespace(o.GetName()) {",
        "comment_created_at": "2022-10-10T19:56:56+00:00",
        "comment_author": "irisdingbj",
        "comment_body": "Good point! Instead of utilizing the `DiscoveryNamespacesFilter.FilterNamespace`  which needs to watch the namespace creation and have the sequence issue, suppose we can just utilize the `DiscoveryNamespaceFilter.discoverySelectors`   to filter the new ns here? ",
        "pr_file_module": null
      },
      {
        "comment_id": "991747695",
        "repo_full_name": "istio/istio",
        "pr_number": 41198,
        "pr_file": "pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go",
        "discussion_id": "991457464",
        "commented_code": "@@ -72,10 +80,22 @@ func NewNamespaceController(kubeClient kube.Client, caBundleWatcher *keycertbund\n \t\t\t// skip special kubernetes system namespaces\n \t\t\treturn false\n \t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.Filter(o) {\n+\t\t\t// This is a change to a configmap we don't watch, ignore it\n+\t\t\treturn false\n+\t\t}\n \t\treturn true\n \t}))\n \tc.namespacesInformer.AddEventHandler(controllers.FilteredObjectSpecHandler(c.queue.AddObject, func(o controllers.Object) bool {\n-\t\treturn !inject.IgnoredNamespaces.Contains(o.GetName())\n+\t\tif inject.IgnoredNamespaces.Contains(o.GetName()) {\n+\t\t\t// skip special kubernetes system namespaces\n+\t\t\treturn false\n+\t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.FilterNamespace(o.GetName()) {",
        "comment_created_at": "2022-10-11T01:53:17+00:00",
        "comment_author": "hzxuzhonghu",
        "comment_body": ">Events are triggered in random order to these\r\n\r\nEvents is in determined order, because actually the two informers are sharing a underlying store",
        "pr_file_module": null
      },
      {
        "comment_id": "991748129",
        "repo_full_name": "istio/istio",
        "pr_number": 41198,
        "pr_file": "pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go",
        "discussion_id": "991457464",
        "commented_code": "@@ -72,10 +80,22 @@ func NewNamespaceController(kubeClient kube.Client, caBundleWatcher *keycertbund\n \t\t\t// skip special kubernetes system namespaces\n \t\t\treturn false\n \t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.Filter(o) {\n+\t\t\t// This is a change to a configmap we don't watch, ignore it\n+\t\t\treturn false\n+\t\t}\n \t\treturn true\n \t}))\n \tc.namespacesInformer.AddEventHandler(controllers.FilteredObjectSpecHandler(c.queue.AddObject, func(o controllers.Object) bool {\n-\t\treturn !inject.IgnoredNamespaces.Contains(o.GetName())\n+\t\tif inject.IgnoredNamespaces.Contains(o.GetName()) {\n+\t\t\t// skip special kubernetes system namespaces\n+\t\t\treturn false\n+\t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.FilterNamespace(o.GetName()) {",
        "comment_created_at": "2022-10-11T01:54:31+00:00",
        "comment_author": "hzxuzhonghu",
        "comment_body": ">namespacecontroller gets \"Add ns foo\" event. It checks the discovery filter, and it is not present so it ignores it\r\n\r\nIN order to prevent this, we should make sure the discover filter cb get executed before this one here",
        "pr_file_module": null
      },
      {
        "comment_id": "991750680",
        "repo_full_name": "istio/istio",
        "pr_number": 41198,
        "pr_file": "pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go",
        "discussion_id": "991457464",
        "commented_code": "@@ -72,10 +80,22 @@ func NewNamespaceController(kubeClient kube.Client, caBundleWatcher *keycertbund\n \t\t\t// skip special kubernetes system namespaces\n \t\t\treturn false\n \t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.Filter(o) {\n+\t\t\t// This is a change to a configmap we don't watch, ignore it\n+\t\t\treturn false\n+\t\t}\n \t\treturn true\n \t}))\n \tc.namespacesInformer.AddEventHandler(controllers.FilteredObjectSpecHandler(c.queue.AddObject, func(o controllers.Object) bool {\n-\t\treturn !inject.IgnoredNamespaces.Contains(o.GetName())\n+\t\tif inject.IgnoredNamespaces.Contains(o.GetName()) {\n+\t\t\t// skip special kubernetes system namespaces\n+\t\t\treturn false\n+\t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.FilterNamespace(o.GetName()) {",
        "comment_created_at": "2022-10-11T02:02:14+00:00",
        "comment_author": "hzxuzhonghu",
        "comment_body": "simply we can merge both cb handler into one, currently two queues can be run in random order",
        "pr_file_module": null
      },
      {
        "comment_id": "992347325",
        "repo_full_name": "istio/istio",
        "pr_number": 41198,
        "pr_file": "pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go",
        "discussion_id": "991457464",
        "commented_code": "@@ -72,10 +80,22 @@ func NewNamespaceController(kubeClient kube.Client, caBundleWatcher *keycertbund\n \t\t\t// skip special kubernetes system namespaces\n \t\t\treturn false\n \t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.Filter(o) {\n+\t\t\t// This is a change to a configmap we don't watch, ignore it\n+\t\t\treturn false\n+\t\t}\n \t\treturn true\n \t}))\n \tc.namespacesInformer.AddEventHandler(controllers.FilteredObjectSpecHandler(c.queue.AddObject, func(o controllers.Object) bool {\n-\t\treturn !inject.IgnoredNamespaces.Contains(o.GetName())\n+\t\tif inject.IgnoredNamespaces.Contains(o.GetName()) {\n+\t\t\t// skip special kubernetes system namespaces\n+\t\t\treturn false\n+\t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.FilterNamespace(o.GetName()) {",
        "comment_created_at": "2022-10-11T13:44:36+00:00",
        "comment_author": "howardjohn",
        "comment_body": "Seems simpler to do @irisdingbj's suggestion, then we are not dependent on tricky race conditions ",
        "pr_file_module": null
      },
      {
        "comment_id": "996259075",
        "repo_full_name": "istio/istio",
        "pr_number": 41198,
        "pr_file": "pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go",
        "discussion_id": "991457464",
        "commented_code": "@@ -72,10 +80,22 @@ func NewNamespaceController(kubeClient kube.Client, caBundleWatcher *keycertbund\n \t\t\t// skip special kubernetes system namespaces\n \t\t\treturn false\n \t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.Filter(o) {\n+\t\t\t// This is a change to a configmap we don't watch, ignore it\n+\t\t\treturn false\n+\t\t}\n \t\treturn true\n \t}))\n \tc.namespacesInformer.AddEventHandler(controllers.FilteredObjectSpecHandler(c.queue.AddObject, func(o controllers.Object) bool {\n-\t\treturn !inject.IgnoredNamespaces.Contains(o.GetName())\n+\t\tif inject.IgnoredNamespaces.Contains(o.GetName()) {\n+\t\t\t// skip special kubernetes system namespaces\n+\t\t\treturn false\n+\t\t}\n+\t\tif features.EnableEnhancedResourceScoping && !c.DiscoveryNamespacesFilter.FilterNamespace(o.GetName()) {",
        "comment_created_at": "2022-10-15T06:00:19+00:00",
        "comment_author": "kfaseela",
        "comment_body": "Done. @irisdingbj could you please have a relook?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "956186178",
    "pr_number": 40643,
    "pr_file": "pkg/backoff/retry.go",
    "created_at": "2022-08-26T15:55:08+00:00",
    "commented_code": "+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package backoff\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"time\"\n+)\n+\n+// RetryWithContext tries the operation until it does not return error, BackOff stops,\n+// or when the context expires, whichever happens first.\n+// o is guaranteed to be run at least once.\n+// RetryWithContext sleeps the goroutine for the duration returned by BackOff after a\n+// failed operation returns.\n+func RetryWithContext(ctx context.Context, operation func() error, b BackOff) error {\n+\tfor {\n+\t\terr := operation()\n+\t\tif err == nil {\n+\t\t\treturn nil\n+\t\t}\n+\n+\t\tselect {\n+\t\tcase <-ctx.Done():\n+\t\t\treturn fmt.Errorf(\"%v with last error: %v\", context.DeadlineExceeded, err)\n+\t\tdefault:\n+\t\t\tnext := b.NextBackOff()\n+\t\t\tif next == MaxDuration {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\ttime.Sleep(next)",
    "repo_full_name": "istio/istio",
    "discussion_comments": [
      {
        "comment_id": "956186178",
        "repo_full_name": "istio/istio",
        "pr_number": 40643,
        "pr_file": "pkg/backoff/retry.go",
        "discussion_id": "956186178",
        "commented_code": "@@ -0,0 +1,46 @@\n+// Copyright Istio Authors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package backoff\n+\n+import (\n+\t\"context\"\n+\t\"fmt\"\n+\t\"time\"\n+)\n+\n+// RetryWithContext tries the operation until it does not return error, BackOff stops,\n+// or when the context expires, whichever happens first.\n+// o is guaranteed to be run at least once.\n+// RetryWithContext sleeps the goroutine for the duration returned by BackOff after a\n+// failed operation returns.\n+func RetryWithContext(ctx context.Context, operation func() error, b BackOff) error {\n+\tfor {\n+\t\terr := operation()\n+\t\tif err == nil {\n+\t\t\treturn nil\n+\t\t}\n+\n+\t\tselect {\n+\t\tcase <-ctx.Done():\n+\t\t\treturn fmt.Errorf(\"%v with last error: %v\", context.DeadlineExceeded, err)\n+\t\tdefault:\n+\t\t\tnext := b.NextBackOff()\n+\t\t\tif next == MaxDuration {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\ttime.Sleep(next)",
        "comment_created_at": "2022-08-26T15:55:08+00:00",
        "comment_author": "howardjohn",
        "comment_body": "I think we want `<-time.After(next)` in a select with ctx.Done so it can pre-empt it?",
        "pr_file_module": null
      }
    ]
  }
]