[
  {
    "discussion_id": "1476980208",
    "pr_number": 1712,
    "pr_file": "extensions/inference-nitro-extension/src/index.ts",
    "created_at": "2024-02-03T05:35:47+00:00",
    "commented_code": "private async onModelInit(model: Model) {\n     if (model.engine !== InferenceEngine.nitro) return;\n \n-    const modelFullPath = await joinPath([\"models\", model.id]);\n+    const modelFullPath = await joinPath([\n+      JanInferenceNitroExtension._modelsDir,\n+      model.id,\n+    ]);\n+    log(`[APP]::Debug: Initializing Nitro model: ${modelFullPath}`);\n+\n+    // Set bin path for nitro binaries download during runtime\n+    const nitroSetBinPathResult = await executeOnMain(\n+      NODE,\n+      \"setBinPath\",\n+      JanInferenceNitroExtension._binDir,\n+    );\n+    if (nitroSetBinPathResult?.error) {\n+      events.emit(ModelEvent.OnModelFail, model);\n+      return;\n+    }\n \n     const nitroInitResult = await executeOnMain(NODE, \"runModel\", {\n       modelFullPath,\n-      model,\n+      promptTemplate: model.settings.prompt_template,",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1476980208",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1712,
        "pr_file": "extensions/inference-nitro-extension/src/index.ts",
        "discussion_id": "1476980208",
        "commented_code": "@@ -131,11 +137,26 @@ export default class JanInferenceNitroExtension extends InferenceExtension {\n   private async onModelInit(model: Model) {\n     if (model.engine !== InferenceEngine.nitro) return;\n \n-    const modelFullPath = await joinPath([\"models\", model.id]);\n+    const modelFullPath = await joinPath([\n+      JanInferenceNitroExtension._modelsDir,\n+      model.id,\n+    ]);\n+    log(`[APP]::Debug: Initializing Nitro model: ${modelFullPath}`);\n+\n+    // Set bin path for nitro binaries download during runtime\n+    const nitroSetBinPathResult = await executeOnMain(\n+      NODE,\n+      \"setBinPath\",\n+      JanInferenceNitroExtension._binDir,\n+    );\n+    if (nitroSetBinPathResult?.error) {\n+      events.emit(ModelEvent.OnModelFail, model);\n+      return;\n+    }\n \n     const nitroInitResult = await executeOnMain(NODE, \"runModel\", {\n       modelFullPath,\n-      model,\n+      promptTemplate: model.settings.prompt_template,",
        "comment_created_at": "2024-02-03T05:35:47+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "We need to pass CPU/GPU settings from the app (previously read from settings.json). It will also need to load setting arguments, such as ctx_len and ngl.",
        "pr_file_module": null
      },
      {
        "comment_id": "1477010178",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1712,
        "pr_file": "extensions/inference-nitro-extension/src/index.ts",
        "discussion_id": "1476980208",
        "commented_code": "@@ -131,11 +137,26 @@ export default class JanInferenceNitroExtension extends InferenceExtension {\n   private async onModelInit(model: Model) {\n     if (model.engine !== InferenceEngine.nitro) return;\n \n-    const modelFullPath = await joinPath([\"models\", model.id]);\n+    const modelFullPath = await joinPath([\n+      JanInferenceNitroExtension._modelsDir,\n+      model.id,\n+    ]);\n+    log(`[APP]::Debug: Initializing Nitro model: ${modelFullPath}`);\n+\n+    // Set bin path for nitro binaries download during runtime\n+    const nitroSetBinPathResult = await executeOnMain(\n+      NODE,\n+      \"setBinPath\",\n+      JanInferenceNitroExtension._binDir,\n+    );\n+    if (nitroSetBinPathResult?.error) {\n+      events.emit(ModelEvent.OnModelFail, model);\n+      return;\n+    }\n \n     const nitroInitResult = await executeOnMain(NODE, \"runModel\", {\n       modelFullPath,\n-      model,\n+      promptTemplate: model.settings.prompt_template,",
        "comment_created_at": "2024-02-03T07:48:20+00:00",
        "comment_author": "InNoobWeTrust",
        "comment_body": "Missed these details, will be updated then.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1366375229",
    "pr_number": 409,
    "pr_file": "plugins/inference-plugin/module.ts",
    "created_at": "2023-10-20T02:34:12+00:00",
    "commented_code": ".then(() => {\n         let binaryFolder = path.join(__dirname, \"nitro\"); // Current directory by default\n \n-        // Read the existing config\n-        const configFilePath = path.join(binaryFolder, \"config\", \"config.json\");\n-        let config: any = {};\n-        if (fs.existsSync(configFilePath)) {\n-          const rawData = fs.readFileSync(configFilePath, \"utf-8\");\n-          config = JSON.parse(rawData);\n-        }\n-\n-        // Update the llama_model_path\n-        if (!config.custom_config) {\n-          config.custom_config = {};\n+        const config = {\n+          llama_model_path: \"\",\n+          ctx_len: 2048,\n+          ngl: 100,\n+          embedding: true // Always enable embedding mode on",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1366375229",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 409,
        "pr_file": "plugins/inference-plugin/module.ts",
        "discussion_id": "1366375229",
        "commented_code": "@@ -33,30 +33,21 @@ const initModel = (fileName) => {\n       .then(() => {\n         let binaryFolder = path.join(__dirname, \"nitro\"); // Current directory by default\n \n-        // Read the existing config\n-        const configFilePath = path.join(binaryFolder, \"config\", \"config.json\");\n-        let config: any = {};\n-        if (fs.existsSync(configFilePath)) {\n-          const rawData = fs.readFileSync(configFilePath, \"utf-8\");\n-          config = JSON.parse(rawData);\n-        }\n-\n-        // Update the llama_model_path\n-        if (!config.custom_config) {\n-          config.custom_config = {};\n+        const config = {\n+          llama_model_path: \"\",\n+          ctx_len: 2048,\n+          ngl: 100,\n+          embedding: true // Always enable embedding mode on",
        "comment_created_at": "2023-10-20T02:34:12+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "```suggestion\r\n        const llama_model_path = path.join(app.getPath(\"userData\"), fileName);\r\n        const config = {\r\n          llama_model_path,\r\n          ctx_len: 2048,\r\n          ngl: 100,\r\n          embedding: true // Always enable embedding mode on\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1430254581",
    "pr_number": 1071,
    "pr_file": "extensions/inference-nitro-extension/src/module.ts",
    "created_at": "2023-12-18T14:56:17+00:00",
    "commented_code": "wrapper.model.settings.ai_prompt = prompt.ai_prompt;\n     }\n \n-    const settings = {\n+    currentSettings = {\n       llama_model_path: currentModelFile,\n       ...wrapper.model.settings,\n       // This is critical and requires real system information\n       cpu_threads: nitroResourceProbe.numCpuPhysicalCore,\n     };\n-    log.info(`Load model settings: ${JSON.stringify(settings, null, 2)}`);\n-    return (\n-      // 1. Check if the port is used, if used, attempt to unload model / kill nitro process\n-      validateModelVersion()\n-        .then(checkAndUnloadNitro)\n-        // 2. Spawn the Nitro subprocess\n-        .then(await spawnNitroProcess(nitroResourceProbe))",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1430254581",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1071,
        "pr_file": "extensions/inference-nitro-extension/src/module.ts",
        "discussion_id": "1430254581",
        "commented_code": "@@ -60,31 +53,30 @@ async function initModel(wrapper: any): Promise<ModelOperationResponse> {\n       wrapper.model.settings.ai_prompt = prompt.ai_prompt;\n     }\n \n-    const settings = {\n+    currentSettings = {\n       llama_model_path: currentModelFile,\n       ...wrapper.model.settings,\n       // This is critical and requires real system information\n       cpu_threads: nitroResourceProbe.numCpuPhysicalCore,\n     };\n-    log.info(`Load model settings: ${JSON.stringify(settings, null, 2)}`);\n-    return (\n-      // 1. Check if the port is used, if used, attempt to unload model / kill nitro process\n-      validateModelVersion()\n-        .then(checkAndUnloadNitro)\n-        // 2. Spawn the Nitro subprocess\n-        .then(await spawnNitroProcess(nitroResourceProbe))",
        "comment_created_at": "2023-12-18T14:56:17+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "CRITICAL: App spawn Nitro before finished unloading model / killing nitro process",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1430257407",
    "pr_number": 1071,
    "pr_file": "extensions/inference-nitro-extension/src/module.ts",
    "created_at": "2023-12-18T14:56:51+00:00",
    "commented_code": "wrapper.model.settings.ai_prompt = prompt.ai_prompt;\n     }\n \n-    const settings = {\n+    currentSettings = {\n       llama_model_path: currentModelFile,\n       ...wrapper.model.settings,\n       // This is critical and requires real system information\n       cpu_threads: nitroResourceProbe.numCpuPhysicalCore,\n     };\n-    log.info(`Load model settings: ${JSON.stringify(settings, null, 2)}`);\n-    return (\n-      // 1. Check if the port is used, if used, attempt to unload model / kill nitro process\n-      validateModelVersion()\n-        .then(checkAndUnloadNitro)\n-        // 2. Spawn the Nitro subprocess\n-        .then(await spawnNitroProcess(nitroResourceProbe))\n-        // 4. Load the model into the Nitro subprocess (HTTP POST request)\n-        .then(() => loadLLMModel(settings))\n-        // 5. Check if the model is loaded successfully\n-        .then(validateModelStatus)\n-        .catch((err) => {\n-          log.error(\"error: \" + JSON.stringify(err));\n-          return { error: err, currentModelFile };\n-        })\n-    );\n+    return loadModel(nitroResourceProbe);\n   }\n }\n \n+async function loadModel(nitroResourceProbe: any | undefined) {",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1430257407",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1071,
        "pr_file": "extensions/inference-nitro-extension/src/module.ts",
        "discussion_id": "1430257407",
        "commented_code": "@@ -60,31 +53,30 @@ async function initModel(wrapper: any): Promise<ModelOperationResponse> {\n       wrapper.model.settings.ai_prompt = prompt.ai_prompt;\n     }\n \n-    const settings = {\n+    currentSettings = {\n       llama_model_path: currentModelFile,\n       ...wrapper.model.settings,\n       // This is critical and requires real system information\n       cpu_threads: nitroResourceProbe.numCpuPhysicalCore,\n     };\n-    log.info(`Load model settings: ${JSON.stringify(settings, null, 2)}`);\n-    return (\n-      // 1. Check if the port is used, if used, attempt to unload model / kill nitro process\n-      validateModelVersion()\n-        .then(checkAndUnloadNitro)\n-        // 2. Spawn the Nitro subprocess\n-        .then(await spawnNitroProcess(nitroResourceProbe))\n-        // 4. Load the model into the Nitro subprocess (HTTP POST request)\n-        .then(() => loadLLMModel(settings))\n-        // 5. Check if the model is loaded successfully\n-        .then(validateModelStatus)\n-        .catch((err) => {\n-          log.error(\"error: \" + JSON.stringify(err));\n-          return { error: err, currentModelFile };\n-        })\n-    );\n+    return loadModel(nitroResourceProbe);\n   }\n }\n \n+async function loadModel(nitroResourceProbe: any | undefined) {",
        "comment_created_at": "2023-12-18T14:56:51+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "Separated function so next PR will allow app to load model when failed to generate response (nitro process is killed)",
        "pr_file_module": null
      }
    ]
  }
]