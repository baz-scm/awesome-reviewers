[
  {
    "discussion_id": "1574327796",
    "pr_number": 2773,
    "pr_file": "extensions/inference-nitro-extension/resources/models/llama3-8b-instruct/model.json",
    "created_at": "2024-04-22T08:18:35+00:00",
    "commented_code": "+{\n+    \"sources\": [\n+      {\n+        \"filename\": \"Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n+        \"url\": \"https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"\n+      }\n+    ],\n+    \"id\": \"llama3-8b-instruct\",\n+    \"object\": \"model\",\n+    \"name\": \"Llama 3 8B Q4\",\n+    \"version\": \"1.0\",\n+    \"description\": \"The latest model from MetaAI\",",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1574327796",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 2773,
        "pr_file": "extensions/inference-nitro-extension/resources/models/llama3-8b-instruct/model.json",
        "discussion_id": "1574327796",
        "commented_code": "@@ -0,0 +1,34 @@\n+{\n+    \"sources\": [\n+      {\n+        \"filename\": \"Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n+        \"url\": \"https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"\n+      }\n+    ],\n+    \"id\": \"llama3-8b-instruct\",\n+    \"object\": \"model\",\n+    \"name\": \"Llama 3 8B Q4\",\n+    \"version\": \"1.0\",\n+    \"description\": \"The latest model from MetaAI\",",
        "comment_created_at": "2024-04-22T08:18:35+00:00",
        "comment_author": "nkuehn",
        "comment_body": "```suggestion\r\n    \"description\": \"Meta's Llama 3 excels at general usage situations, including chat, general world knowledge, and coding.\",\r\n```\r\nMore descriptive option, derived from the huggingface model card that the dowload URL points to (lmstudio). Being the \"latest\" model does not age well at the current speed of developments",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1555806098",
    "pr_number": 2569,
    "pr_file": "extensions/inference-mistral-extension/resources/models.json",
    "created_at": "2024-04-08T12:59:47+00:00",
    "commented_code": "+[\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"open-mistral-7b\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Tiny Cloud Api (7b)\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\",\n+        \"Open\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"open-mixtral-8x7b\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Tiny Cloud Api (8x7b)\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\",\n+        \"Open\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"mistral-small-latest\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Small\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"mistral-medium-latest\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Medium\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Data extraction\",\n+        \"Summarizing a Document\",\n+        \"Writing a Job Description\",\n+        \"Writing Product Descriptions\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"mistral-large-latest\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Large\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1555806098",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 2569,
        "pr_file": "extensions/inference-mistral-extension/resources/models.json",
        "discussion_id": "1555806098",
        "commented_code": "@@ -0,0 +1,141 @@\n+[\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"open-mistral-7b\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Tiny Cloud Api (7b)\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\",\n+        \"Open\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"open-mixtral-8x7b\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Tiny Cloud Api (8x7b)\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\",\n+        \"Open\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"mistral-small-latest\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Small\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"mistral-medium-latest\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Medium\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Data extraction\",\n+        \"Summarizing a Document\",\n+        \"Writing a Job Description\",\n+        \"Writing Product Descriptions\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"mistral-large-latest\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Large\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",",
        "comment_created_at": "2024-04-08T12:59:47+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "```suggestion\r\n    \"description\": \"Mistral Large is ideal for complex tasks that require large reasoning capabilities or are highly specialized - like Synthetic Text Generation, Code Generation, RAG, or Agents.\",\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1555807780",
    "pr_number": 2569,
    "pr_file": "extensions/inference-mistral-extension/resources/models.json",
    "created_at": "2024-04-08T13:00:45+00:00",
    "commented_code": "+[\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"open-mistral-7b\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Tiny Cloud Api (7b)\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\",\n+        \"Open\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"open-mixtral-8x7b\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Tiny Cloud Api (8x7b)\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\",\n+        \"Open\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"mistral-small-latest\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Small\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1555807780",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 2569,
        "pr_file": "extensions/inference-mistral-extension/resources/models.json",
        "discussion_id": "1555807780",
        "commented_code": "@@ -0,0 +1,141 @@\n+[\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"open-mistral-7b\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Tiny Cloud Api (7b)\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\",\n+        \"Open\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"open-mixtral-8x7b\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Tiny Cloud Api (8x7b)\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",\n+    \"format\": \"api\",\n+    \"settings\": {},\n+    \"parameters\": {\n+      \"max_tokens\": 4096,\n+      \"temperature\": 0.7\n+    },\n+    \"metadata\": {\n+      \"author\": \"Mistral\",\n+      \"tags\": [\n+        \"Classification\",\n+        \"Customer Support\",\n+        \"Text Generation\",\n+        \"Open\"\n+      ]\n+    },\n+    \"engine\": \"mistral\"\n+  },\n+  {\n+    \"sources\": [\n+      {\n+        \"url\": \"https://docs.mistral.ai/api/\"\n+      }\n+    ],\n+    \"id\": \"mistral-small-latest\",\n+    \"object\": \"model\",\n+    \"name\": \"Mistral Small\",\n+    \"version\": \"1.0\",\n+    \"description\": \"Mistral...le fromage...et..e'pi\",",
        "comment_created_at": "2024-04-08T13:00:45+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "```suggestion\r\n    \"description\": \"Mistral Small is the ideal choice for simpe tasks that one can do in builk - like Classification, Customer Support, or Text Generation. It offers excellent performance at an affordable price point.\",\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1437984060",
    "pr_number": 1213,
    "pr_file": "models/codeninja-1.0-7b/model.json",
    "created_at": "2023-12-29T04:53:17+00:00",
    "commented_code": "+{\n+    \"source_url\": \"https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B-GGUF/resolve/main/codeninja-1.0-openchat-7b.Q4_K_M.gguf\",\n+    \"id\": \"codeninja-1.0-7b\",\n+    \"object\": \"model\",\n+    \"name\": \"CodeNinja 7B Q4\",\n+    \"version\": \"1.0\",\n+    \"description\": \"CodeNinja is an enhanced version of the renowned model openchat/openchat-3.5-1210. It is designed to be an indispensable tool for coders\",",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1437984060",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1213,
        "pr_file": "models/codeninja-1.0-7b/model.json",
        "discussion_id": "1437984060",
        "commented_code": "@@ -0,0 +1,23 @@\n+{\n+    \"source_url\": \"https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B-GGUF/resolve/main/codeninja-1.0-openchat-7b.Q4_K_M.gguf\",\n+    \"id\": \"codeninja-1.0-7b\",\n+    \"object\": \"model\",\n+    \"name\": \"CodeNinja 7B Q4\",\n+    \"version\": \"1.0\",\n+    \"description\": \"CodeNinja is an enhanced version of the renowned model openchat/openchat-3.5-1210. It is designed to be an indispensable tool for coders\",",
        "comment_created_at": "2023-12-29T04:53:17+00:00",
        "comment_author": "freelerobot",
        "comment_body": "Let's be a bit more concise and pragmatic in our descriptions. No hyperbolic statements. \r\n\r\nJust \"description\": \"CodeNinja is finetuned on openchat/openchat-3.5-1210. Good for coding.\",",
        "pr_file_module": null
      }
    ]
  }
]