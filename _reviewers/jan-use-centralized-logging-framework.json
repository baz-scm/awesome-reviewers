[
  {
    "discussion_id": "1476981686",
    "pr_number": 1712,
    "pr_file": "extensions/inference-nitro-extension/src/node/debugInspector.ts",
    "created_at": "2024-02-03T05:45:01+00:00",
    "commented_code": "+import { log } from \"@janhq/core/node\";\n+\n+export const debugInspectorSync =",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1476981686",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1712,
        "pr_file": "extensions/inference-nitro-extension/src/node/debugInspector.ts",
        "discussion_id": "1476981686",
        "commented_code": "@@ -0,0 +1,30 @@\n+import { log } from \"@janhq/core/node\";\n+\n+export const debugInspectorSync =",
        "comment_created_at": "2024-02-03T05:45:01+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "I was really impressed the first time I saw the name of this function. However, it turned out to be quite messy, and debugging with a logger in place would be simpler. It's cumbersome to add it to every function call like this here and there:\r\n\r\nprocess.env.DEBUG ? debugInspector(setBinPath) : setBinPath",
        "pr_file_module": null
      },
      {
        "comment_id": "1477011948",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1712,
        "pr_file": "extensions/inference-nitro-extension/src/node/debugInspector.ts",
        "discussion_id": "1476981686",
        "commented_code": "@@ -0,0 +1,30 @@\n+import { log } from \"@janhq/core/node\";\n+\n+export const debugInspectorSync =",
        "comment_created_at": "2024-02-03T08:04:30+00:00",
        "comment_author": "InNoobWeTrust",
        "comment_body": "Ah, this one is for quickly seeing the in/out of function without having to attach to the debugger port at runtime.\r\n\r\nSorry for the confusion, the name is a little misleading, it should be just for logging the input/output of the function only. Adding logging in every function is kind of repetitive so I made a wrapper to quickly do it on export. Not all the exports should be wrapped by this utility hence the conditional export, just the ones we want to know if the expected input will somehow result in unexpected output. Without debug env, the unnecessary LOC should not be there to avoid unintended side-effects...\r\n\r\nAnd I didn't see if `@janhq/core` support multiple log-level or I need to make another logger just for debug?\r\n\r\nTo resolve the cumbersomeness, can just move the conditional exports into this function and we will just wrap, but then the exports at runtime when we don't want to debug will have side effects. What is the approach the team would prefer considering the pros and cons? And if the logging is not needed anymore then I can just simply remove this utility to prevent further confusion, it's just a quick debugging trick to allow adding logging with just 1 LOC without changing the actual function code here and there...",
        "pr_file_module": null
      },
      {
        "comment_id": "1477023260",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1712,
        "pr_file": "extensions/inference-nitro-extension/src/node/debugInspector.ts",
        "discussion_id": "1476981686",
        "commented_code": "@@ -0,0 +1,30 @@\n+import { log } from \"@janhq/core/node\";\n+\n+export const debugInspectorSync =",
        "comment_created_at": "2024-02-03T09:00:04+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "Yeah, I can get the idea of avoiding uninteded side-effects, but just keep it clean. The log function in @janhq/core aims to route to a log file. It would be great to add a log-level option there, rather than introducing so many logging systems.",
        "pr_file_module": null
      },
      {
        "comment_id": "1477033959",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1712,
        "pr_file": "extensions/inference-nitro-extension/src/node/debugInspector.ts",
        "discussion_id": "1476981686",
        "commented_code": "@@ -0,0 +1,30 @@\n+import { log } from \"@janhq/core/node\";\n+\n+export const debugInspectorSync =",
        "comment_created_at": "2024-02-03T09:57:41+00:00",
        "comment_author": "InNoobWeTrust",
        "comment_body": "Ok, so I'm going to remove this wrapper debug trick. The log-level option in `@janhq/core` should be in another PR then.",
        "pr_file_module": null
      },
      {
        "comment_id": "1477034000",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 1712,
        "pr_file": "extensions/inference-nitro-extension/src/node/debugInspector.ts",
        "discussion_id": "1476981686",
        "commented_code": "@@ -0,0 +1,30 @@\n+import { log } from \"@janhq/core/node\";\n+\n+export const debugInspectorSync =",
        "comment_created_at": "2024-02-03T09:58:32+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "Nice, thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1391994426",
    "pr_number": 616,
    "pr_file": "plugins/inference-plugin/src/module.ts",
    "created_at": "2023-11-14T05:18:32+00:00",
    "commented_code": "+const fs = require(\"fs\");\n+const kill = require(\"kill-port\");\n const path = require(\"path\");\n const { app } = require(\"electron\");\n const { spawn } = require(\"child_process\");\n-const fs = require(\"fs\");\n const tcpPortUsed = require(\"tcp-port-used\");\n-const kill = require(\"kill-port\");\n \n+// The PORT to use for the Nitro subprocess\n const PORT = 3928;\n-let subprocess = null;\n+const LOCAL_HOST = \"127.0.0.1\";\n+const NITRO_HTTP_SERVER_URL = `http://${LOCAL_HOST}:${PORT}`;\n+const NITRO_HTTP_LOAD_MODEL_URL = `${NITRO_HTTP_SERVER_URL}/inferences/llamacpp/loadmodel`;\n+const NITRO_HTTP_UNLOAD_MODEL_URL = `${NITRO_HTTP_SERVER_URL}/inferences/llamacpp/unloadModel`;\n \n-const initModel = (fileName) => {\n-  return (\n-    new Promise<void>(async (resolve, reject) => {\n-      if (!fileName) {\n-        reject(\"Model not found, please download again.\");\n-      }\n-      resolve(fileName);\n-    })\n-      // Spawn Nitro subprocess to load model\n-      .then(() => {\n-        return tcpPortUsed.check(PORT, \"127.0.0.1\").then((inUse) => {\n-          if (!inUse) {\n-            let binaryFolder = path.join(__dirname, \"nitro\"); // Current directory by default\n-            let binaryName;\n-\n-        if (process.platform === \"win32\") {\n-          // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n-          binaryName = \"win-start.bat\";\n-        } else if (process.platform === \"darwin\") {\n-          // Mac OS platform\n-          if (process.arch === \"arm64\") {\n-            binaryFolder = path.join(binaryFolder, \"mac-arm64\")\n-          } else {\n-            binaryFolder = path.join(binaryFolder, \"mac-x64\")\n-          }\n-          binaryName = \"nitro\"\n-        } else {\n-          // Linux\n-          // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n-          binaryName = \"linux-start.sh\"; // For other platforms\n-        }\n-\n-            const binaryPath = path.join(binaryFolder, binaryName);\n-\n-        // Execute the binary\n-        subprocess = spawn(binaryPath,[\"0.0.0.0\", PORT], { cwd: binaryFolder });\n-\n-            // Handle subprocess output\n-            subprocess.stdout.on(\"data\", (data) => {\n-              console.log(`stdout: ${data}`);\n-            });\n+// The subprocess instance for Nitro\n+let subprocess = null;\n \n-            subprocess.stderr.on(\"data\", (data) => {\n-              console.error(`stderr: ${data}`);\n-            });\n+/**\n+ * The response from the initModel function.\n+ * @property error - An error message if the model fails to load.\n+ */\n+interface InitModelResponse {\n+  error?: any;\n+}\n \n-            subprocess.on(\"close\", (code) => {\n-              console.log(`child process exited with code ${code}`);\n-              subprocess = null;\n-            });\n-          }\n-        });\n-      })\n+/**\n+ * Initializes a Nitro subprocess to load a machine learning model.\n+ * @param fileName - The name of the machine learning model file.\n+ * @returns A Promise that resolves when the model is loaded successfully, or rejects with an error message if the model is not found or fails to load.\n+ * TODO: Should pass absolute of the model file instead of just the name - So we can modurize the module.ts to npm package\n+ * TODO: Should it be startModel instead?\n+ */\n+function initModel(fileName: string): Promise<InitModelResponse> {\n+  // 1. Check if the model file exists\n+  return (\n+    checkModelFileExist(fileName)\n+      // 2. Check if the port is used, if used, attempt to unload model / kill nitro process\n+      .then(checkAndUnloadNitro)\n+      // 3. Spawn the Nitro subprocess\n+      .then(spawnNitroProcess)\n+      // 4. Wait until the port is used (Nitro http server is up)\n       .then(() => tcpPortUsed.waitUntilUsed(PORT, 300, 30000))\n-      .then(() => {\n-        const llama_model_path = path.join(appPath(), fileName);\n-\n-        const config = {\n-          llama_model_path,\n-          ctx_len: 2048,\n-          ngl: 100,\n-          embedding: true, // Always enable embedding mode on\n-        };\n-\n-        // Load model config\n-        return fetch(`http://127.0.0.1:${PORT}/inferences/llamacpp/loadmodel`, {\n-          method: \"POST\",\n-          headers: {\n-            \"Content-Type\": \"application/json\",\n-          },\n-          body: JSON.stringify(config),\n-        });\n-      })\n-      .then((res) => {\n+      // 5. Load the model into the Nitro subprocess (HTTP POST request)\n+      .then(() => loadLLMModel(fileName))\n+      // 6. Check if the model is loaded successfully\n+      .then(async (res) => {\n         if (res.ok) {\n           return {};\n         }\n-        throw new Error(\"Nitro: Model failed to load.\");\n+        const json = await res.json();\n+        throw new Error(`Nitro: Model failed to load. ${json}`);\n       })\n       .catch((err) => {\n         return { error: err };\n       })\n   );\n-};\n+}\n \n-function dispose() {\n-  killSubprocess();\n-  // clean other registered resources here\n+/**\n+ * Loads a LLM model into the Nitro subprocess by sending a HTTP POST request.\n+ * @param fileName - The name of the model file.\n+ * @returns A Promise that resolves when the model is loaded successfully, or rejects with an error message if the model is not found or fails to load.\n+ */\n+function loadLLMModel(fileName: string): Promise<Response> {\n+  const llama_model_path = path.join(appPath(), fileName);\n+\n+  const config = {\n+    llama_model_path,\n+    ctx_len: 2048,\n+    ngl: 100,\n+    embedding: false, // Always enable embedding mode on\n+  };\n+\n+  // Load model config\n+  return fetch(NITRO_HTTP_LOAD_MODEL_URL, {\n+    method: \"POST\",\n+    headers: {\n+      \"Content-Type\": \"application/json\",\n+    },\n+    body: JSON.stringify(config),\n+  });\n+}\n+\n+/**\n+ * Checks if the model file exists.\n+ * @param fileName - The name of the model file.\n+ * @returns A Promise that resolves when the model file exists, or rejects with an error message if the model file does not exist.\n+ */\n+function checkModelFileExist(fileName: string): Promise<string> {\n+  return new Promise<string>(async (resolve, reject) => {\n+    if (!fileName) {\n+      reject(\"Model not found, please download again.\");\n+    }\n+    resolve(fileName);\n+  });\n }\n \n-function killSubprocess() {\n+/**\n+ * Terminates the Nitro subprocess.\n+ * @returns A Promise that resolves when the subprocess is terminated successfully, or rejects with an error message if the subprocess fails to terminate.\n+ */\n+function killSubprocess(): Promise<void> {\n   if (subprocess) {\n     subprocess.kill();\n     subprocess = null;\n     console.log(\"Subprocess terminated.\");\n   } else {\n-    kill(PORT, \"tcp\").then(console.log).catch(console.log);\n-    console.error(\"No subprocess is currently running.\");\n+    return kill(PORT, \"tcp\").then(console.log).catch(console.log);\n   }\n }\n \n+/**\n+ * Returns the path to the user data directory.\n+ * @returns The path to the user data directory.\n+ */\n function appPath() {\n-  if (app) {\n-    return app.getPath(\"userData\");\n+  return app.getPath(\"userData\");\n+}\n+\n+/**\n+ * Check port is used or not, if used, attempt to unload model\n+ * If unload failed, kill the port\n+ */\n+function checkAndUnloadNitro() {\n+  return tcpPortUsed.check(PORT, LOCAL_HOST).then((inUse) => {\n+    // If inUse - try unload or kill process, otherwise do nothing\n+    if (inUse) {\n+      // Attempt to unload model\n+      return fetch(NITRO_HTTP_UNLOAD_MODEL_URL, {\n+        method: \"GET\",\n+        headers: {\n+          \"Content-Type\": \"application/json\",\n+        },\n+      }).catch((err) => {\n+        console.log(err);\n+        // Fallback to kill the port\n+        return killSubprocess();\n+      });\n+    }\n+  });\n+}\n+\n+/**\n+ * Look for the Nitro binary and execute it\n+ * Using child-process to spawn the process\n+ * Should run exactly platform specified Nitro binary version\n+ */\n+function spawnNitroProcess() {\n+  let binaryFolder = path.join(__dirname, \"nitro\"); // Current directory by default\n+  let binaryName;\n+\n+  if (process.platform === \"win32\") {\n+    // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n+    binaryName = \"win-start.bat\";\n+  } else if (process.platform === \"darwin\") {\n+    // Mac OS platform\n+    if (process.arch === \"arm64\") {\n+      binaryFolder = path.join(binaryFolder, \"mac-arm64\");\n+    } else {\n+      binaryFolder = path.join(binaryFolder, \"mac-x64\");\n+    }\n+    binaryName = \"nitro\";\n+  } else {\n+    // Linux\n+    // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n+    binaryName = \"linux-start.sh\"; // For other platforms\n   }\n-  return process.env.APPDATA || (process.platform == 'darwin' ? process.env.HOME + '/Library/Preferences' : process.env.HOME + \"/.local/share\");\n+\n+  const binaryPath = path.join(binaryFolder, binaryName);\n+\n+  // Execute the binary\n+  subprocess = spawn(binaryPath, [1, \"0.0.0.0\", PORT], {\n+    cwd: binaryFolder,\n+  });\n+\n+  // Handle subprocess output\n+  subprocess.stdout.on(\"data\", (data) => {\n+    console.log(`stdout: ${data}`);\n+  });\n+\n+  subprocess.stderr.on(\"data\", (data) => {\n+    console.error(`stderr: ${data}`);",
    "repo_full_name": "menloresearch/jan",
    "discussion_comments": [
      {
        "comment_id": "1391994426",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 616,
        "pr_file": "plugins/inference-plugin/src/module.ts",
        "discussion_id": "1391994426",
        "commented_code": "@@ -1,119 +1,200 @@\n+const fs = require(\"fs\");\n+const kill = require(\"kill-port\");\n const path = require(\"path\");\n const { app } = require(\"electron\");\n const { spawn } = require(\"child_process\");\n-const fs = require(\"fs\");\n const tcpPortUsed = require(\"tcp-port-used\");\n-const kill = require(\"kill-port\");\n \n+// The PORT to use for the Nitro subprocess\n const PORT = 3928;\n-let subprocess = null;\n+const LOCAL_HOST = \"127.0.0.1\";\n+const NITRO_HTTP_SERVER_URL = `http://${LOCAL_HOST}:${PORT}`;\n+const NITRO_HTTP_LOAD_MODEL_URL = `${NITRO_HTTP_SERVER_URL}/inferences/llamacpp/loadmodel`;\n+const NITRO_HTTP_UNLOAD_MODEL_URL = `${NITRO_HTTP_SERVER_URL}/inferences/llamacpp/unloadModel`;\n \n-const initModel = (fileName) => {\n-  return (\n-    new Promise<void>(async (resolve, reject) => {\n-      if (!fileName) {\n-        reject(\"Model not found, please download again.\");\n-      }\n-      resolve(fileName);\n-    })\n-      // Spawn Nitro subprocess to load model\n-      .then(() => {\n-        return tcpPortUsed.check(PORT, \"127.0.0.1\").then((inUse) => {\n-          if (!inUse) {\n-            let binaryFolder = path.join(__dirname, \"nitro\"); // Current directory by default\n-            let binaryName;\n-\n-        if (process.platform === \"win32\") {\n-          // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n-          binaryName = \"win-start.bat\";\n-        } else if (process.platform === \"darwin\") {\n-          // Mac OS platform\n-          if (process.arch === \"arm64\") {\n-            binaryFolder = path.join(binaryFolder, \"mac-arm64\")\n-          } else {\n-            binaryFolder = path.join(binaryFolder, \"mac-x64\")\n-          }\n-          binaryName = \"nitro\"\n-        } else {\n-          // Linux\n-          // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n-          binaryName = \"linux-start.sh\"; // For other platforms\n-        }\n-\n-            const binaryPath = path.join(binaryFolder, binaryName);\n-\n-        // Execute the binary\n-        subprocess = spawn(binaryPath,[\"0.0.0.0\", PORT], { cwd: binaryFolder });\n-\n-            // Handle subprocess output\n-            subprocess.stdout.on(\"data\", (data) => {\n-              console.log(`stdout: ${data}`);\n-            });\n+// The subprocess instance for Nitro\n+let subprocess = null;\n \n-            subprocess.stderr.on(\"data\", (data) => {\n-              console.error(`stderr: ${data}`);\n-            });\n+/**\n+ * The response from the initModel function.\n+ * @property error - An error message if the model fails to load.\n+ */\n+interface InitModelResponse {\n+  error?: any;\n+}\n \n-            subprocess.on(\"close\", (code) => {\n-              console.log(`child process exited with code ${code}`);\n-              subprocess = null;\n-            });\n-          }\n-        });\n-      })\n+/**\n+ * Initializes a Nitro subprocess to load a machine learning model.\n+ * @param fileName - The name of the machine learning model file.\n+ * @returns A Promise that resolves when the model is loaded successfully, or rejects with an error message if the model is not found or fails to load.\n+ * TODO: Should pass absolute of the model file instead of just the name - So we can modurize the module.ts to npm package\n+ * TODO: Should it be startModel instead?\n+ */\n+function initModel(fileName: string): Promise<InitModelResponse> {\n+  // 1. Check if the model file exists\n+  return (\n+    checkModelFileExist(fileName)\n+      // 2. Check if the port is used, if used, attempt to unload model / kill nitro process\n+      .then(checkAndUnloadNitro)\n+      // 3. Spawn the Nitro subprocess\n+      .then(spawnNitroProcess)\n+      // 4. Wait until the port is used (Nitro http server is up)\n       .then(() => tcpPortUsed.waitUntilUsed(PORT, 300, 30000))\n-      .then(() => {\n-        const llama_model_path = path.join(appPath(), fileName);\n-\n-        const config = {\n-          llama_model_path,\n-          ctx_len: 2048,\n-          ngl: 100,\n-          embedding: true, // Always enable embedding mode on\n-        };\n-\n-        // Load model config\n-        return fetch(`http://127.0.0.1:${PORT}/inferences/llamacpp/loadmodel`, {\n-          method: \"POST\",\n-          headers: {\n-            \"Content-Type\": \"application/json\",\n-          },\n-          body: JSON.stringify(config),\n-        });\n-      })\n-      .then((res) => {\n+      // 5. Load the model into the Nitro subprocess (HTTP POST request)\n+      .then(() => loadLLMModel(fileName))\n+      // 6. Check if the model is loaded successfully\n+      .then(async (res) => {\n         if (res.ok) {\n           return {};\n         }\n-        throw new Error(\"Nitro: Model failed to load.\");\n+        const json = await res.json();\n+        throw new Error(`Nitro: Model failed to load. ${json}`);\n       })\n       .catch((err) => {\n         return { error: err };\n       })\n   );\n-};\n+}\n \n-function dispose() {\n-  killSubprocess();\n-  // clean other registered resources here\n+/**\n+ * Loads a LLM model into the Nitro subprocess by sending a HTTP POST request.\n+ * @param fileName - The name of the model file.\n+ * @returns A Promise that resolves when the model is loaded successfully, or rejects with an error message if the model is not found or fails to load.\n+ */\n+function loadLLMModel(fileName: string): Promise<Response> {\n+  const llama_model_path = path.join(appPath(), fileName);\n+\n+  const config = {\n+    llama_model_path,\n+    ctx_len: 2048,\n+    ngl: 100,\n+    embedding: false, // Always enable embedding mode on\n+  };\n+\n+  // Load model config\n+  return fetch(NITRO_HTTP_LOAD_MODEL_URL, {\n+    method: \"POST\",\n+    headers: {\n+      \"Content-Type\": \"application/json\",\n+    },\n+    body: JSON.stringify(config),\n+  });\n+}\n+\n+/**\n+ * Checks if the model file exists.\n+ * @param fileName - The name of the model file.\n+ * @returns A Promise that resolves when the model file exists, or rejects with an error message if the model file does not exist.\n+ */\n+function checkModelFileExist(fileName: string): Promise<string> {\n+  return new Promise<string>(async (resolve, reject) => {\n+    if (!fileName) {\n+      reject(\"Model not found, please download again.\");\n+    }\n+    resolve(fileName);\n+  });\n }\n \n-function killSubprocess() {\n+/**\n+ * Terminates the Nitro subprocess.\n+ * @returns A Promise that resolves when the subprocess is terminated successfully, or rejects with an error message if the subprocess fails to terminate.\n+ */\n+function killSubprocess(): Promise<void> {\n   if (subprocess) {\n     subprocess.kill();\n     subprocess = null;\n     console.log(\"Subprocess terminated.\");\n   } else {\n-    kill(PORT, \"tcp\").then(console.log).catch(console.log);\n-    console.error(\"No subprocess is currently running.\");\n+    return kill(PORT, \"tcp\").then(console.log).catch(console.log);\n   }\n }\n \n+/**\n+ * Returns the path to the user data directory.\n+ * @returns The path to the user data directory.\n+ */\n function appPath() {\n-  if (app) {\n-    return app.getPath(\"userData\");\n+  return app.getPath(\"userData\");\n+}\n+\n+/**\n+ * Check port is used or not, if used, attempt to unload model\n+ * If unload failed, kill the port\n+ */\n+function checkAndUnloadNitro() {\n+  return tcpPortUsed.check(PORT, LOCAL_HOST).then((inUse) => {\n+    // If inUse - try unload or kill process, otherwise do nothing\n+    if (inUse) {\n+      // Attempt to unload model\n+      return fetch(NITRO_HTTP_UNLOAD_MODEL_URL, {\n+        method: \"GET\",\n+        headers: {\n+          \"Content-Type\": \"application/json\",\n+        },\n+      }).catch((err) => {\n+        console.log(err);\n+        // Fallback to kill the port\n+        return killSubprocess();\n+      });\n+    }\n+  });\n+}\n+\n+/**\n+ * Look for the Nitro binary and execute it\n+ * Using child-process to spawn the process\n+ * Should run exactly platform specified Nitro binary version\n+ */\n+function spawnNitroProcess() {\n+  let binaryFolder = path.join(__dirname, \"nitro\"); // Current directory by default\n+  let binaryName;\n+\n+  if (process.platform === \"win32\") {\n+    // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n+    binaryName = \"win-start.bat\";\n+  } else if (process.platform === \"darwin\") {\n+    // Mac OS platform\n+    if (process.arch === \"arm64\") {\n+      binaryFolder = path.join(binaryFolder, \"mac-arm64\");\n+    } else {\n+      binaryFolder = path.join(binaryFolder, \"mac-x64\");\n+    }\n+    binaryName = \"nitro\";\n+  } else {\n+    // Linux\n+    // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n+    binaryName = \"linux-start.sh\"; // For other platforms\n   }\n-  return process.env.APPDATA || (process.platform == 'darwin' ? process.env.HOME + '/Library/Preferences' : process.env.HOME + \"/.local/share\");\n+\n+  const binaryPath = path.join(binaryFolder, binaryName);\n+\n+  // Execute the binary\n+  subprocess = spawn(binaryPath, [1, \"0.0.0.0\", PORT], {\n+    cwd: binaryFolder,\n+  });\n+\n+  // Handle subprocess output\n+  subprocess.stdout.on(\"data\", (data) => {\n+    console.log(`stdout: ${data}`);\n+  });\n+\n+  subprocess.stderr.on(\"data\", (data) => {\n+    console.error(`stderr: ${data}`);",
        "comment_created_at": "2023-11-14T05:18:32+00:00",
        "comment_author": "hiro-v",
        "comment_body": "Could we dump this to nitro-err.txt for future debugging? ",
        "pr_file_module": null
      },
      {
        "comment_id": "1391996256",
        "repo_full_name": "menloresearch/jan",
        "pr_number": 616,
        "pr_file": "plugins/inference-plugin/src/module.ts",
        "discussion_id": "1391994426",
        "commented_code": "@@ -1,119 +1,200 @@\n+const fs = require(\"fs\");\n+const kill = require(\"kill-port\");\n const path = require(\"path\");\n const { app } = require(\"electron\");\n const { spawn } = require(\"child_process\");\n-const fs = require(\"fs\");\n const tcpPortUsed = require(\"tcp-port-used\");\n-const kill = require(\"kill-port\");\n \n+// The PORT to use for the Nitro subprocess\n const PORT = 3928;\n-let subprocess = null;\n+const LOCAL_HOST = \"127.0.0.1\";\n+const NITRO_HTTP_SERVER_URL = `http://${LOCAL_HOST}:${PORT}`;\n+const NITRO_HTTP_LOAD_MODEL_URL = `${NITRO_HTTP_SERVER_URL}/inferences/llamacpp/loadmodel`;\n+const NITRO_HTTP_UNLOAD_MODEL_URL = `${NITRO_HTTP_SERVER_URL}/inferences/llamacpp/unloadModel`;\n \n-const initModel = (fileName) => {\n-  return (\n-    new Promise<void>(async (resolve, reject) => {\n-      if (!fileName) {\n-        reject(\"Model not found, please download again.\");\n-      }\n-      resolve(fileName);\n-    })\n-      // Spawn Nitro subprocess to load model\n-      .then(() => {\n-        return tcpPortUsed.check(PORT, \"127.0.0.1\").then((inUse) => {\n-          if (!inUse) {\n-            let binaryFolder = path.join(__dirname, \"nitro\"); // Current directory by default\n-            let binaryName;\n-\n-        if (process.platform === \"win32\") {\n-          // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n-          binaryName = \"win-start.bat\";\n-        } else if (process.platform === \"darwin\") {\n-          // Mac OS platform\n-          if (process.arch === \"arm64\") {\n-            binaryFolder = path.join(binaryFolder, \"mac-arm64\")\n-          } else {\n-            binaryFolder = path.join(binaryFolder, \"mac-x64\")\n-          }\n-          binaryName = \"nitro\"\n-        } else {\n-          // Linux\n-          // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n-          binaryName = \"linux-start.sh\"; // For other platforms\n-        }\n-\n-            const binaryPath = path.join(binaryFolder, binaryName);\n-\n-        // Execute the binary\n-        subprocess = spawn(binaryPath,[\"0.0.0.0\", PORT], { cwd: binaryFolder });\n-\n-            // Handle subprocess output\n-            subprocess.stdout.on(\"data\", (data) => {\n-              console.log(`stdout: ${data}`);\n-            });\n+// The subprocess instance for Nitro\n+let subprocess = null;\n \n-            subprocess.stderr.on(\"data\", (data) => {\n-              console.error(`stderr: ${data}`);\n-            });\n+/**\n+ * The response from the initModel function.\n+ * @property error - An error message if the model fails to load.\n+ */\n+interface InitModelResponse {\n+  error?: any;\n+}\n \n-            subprocess.on(\"close\", (code) => {\n-              console.log(`child process exited with code ${code}`);\n-              subprocess = null;\n-            });\n-          }\n-        });\n-      })\n+/**\n+ * Initializes a Nitro subprocess to load a machine learning model.\n+ * @param fileName - The name of the machine learning model file.\n+ * @returns A Promise that resolves when the model is loaded successfully, or rejects with an error message if the model is not found or fails to load.\n+ * TODO: Should pass absolute of the model file instead of just the name - So we can modurize the module.ts to npm package\n+ * TODO: Should it be startModel instead?\n+ */\n+function initModel(fileName: string): Promise<InitModelResponse> {\n+  // 1. Check if the model file exists\n+  return (\n+    checkModelFileExist(fileName)\n+      // 2. Check if the port is used, if used, attempt to unload model / kill nitro process\n+      .then(checkAndUnloadNitro)\n+      // 3. Spawn the Nitro subprocess\n+      .then(spawnNitroProcess)\n+      // 4. Wait until the port is used (Nitro http server is up)\n       .then(() => tcpPortUsed.waitUntilUsed(PORT, 300, 30000))\n-      .then(() => {\n-        const llama_model_path = path.join(appPath(), fileName);\n-\n-        const config = {\n-          llama_model_path,\n-          ctx_len: 2048,\n-          ngl: 100,\n-          embedding: true, // Always enable embedding mode on\n-        };\n-\n-        // Load model config\n-        return fetch(`http://127.0.0.1:${PORT}/inferences/llamacpp/loadmodel`, {\n-          method: \"POST\",\n-          headers: {\n-            \"Content-Type\": \"application/json\",\n-          },\n-          body: JSON.stringify(config),\n-        });\n-      })\n-      .then((res) => {\n+      // 5. Load the model into the Nitro subprocess (HTTP POST request)\n+      .then(() => loadLLMModel(fileName))\n+      // 6. Check if the model is loaded successfully\n+      .then(async (res) => {\n         if (res.ok) {\n           return {};\n         }\n-        throw new Error(\"Nitro: Model failed to load.\");\n+        const json = await res.json();\n+        throw new Error(`Nitro: Model failed to load. ${json}`);\n       })\n       .catch((err) => {\n         return { error: err };\n       })\n   );\n-};\n+}\n \n-function dispose() {\n-  killSubprocess();\n-  // clean other registered resources here\n+/**\n+ * Loads a LLM model into the Nitro subprocess by sending a HTTP POST request.\n+ * @param fileName - The name of the model file.\n+ * @returns A Promise that resolves when the model is loaded successfully, or rejects with an error message if the model is not found or fails to load.\n+ */\n+function loadLLMModel(fileName: string): Promise<Response> {\n+  const llama_model_path = path.join(appPath(), fileName);\n+\n+  const config = {\n+    llama_model_path,\n+    ctx_len: 2048,\n+    ngl: 100,\n+    embedding: false, // Always enable embedding mode on\n+  };\n+\n+  // Load model config\n+  return fetch(NITRO_HTTP_LOAD_MODEL_URL, {\n+    method: \"POST\",\n+    headers: {\n+      \"Content-Type\": \"application/json\",\n+    },\n+    body: JSON.stringify(config),\n+  });\n+}\n+\n+/**\n+ * Checks if the model file exists.\n+ * @param fileName - The name of the model file.\n+ * @returns A Promise that resolves when the model file exists, or rejects with an error message if the model file does not exist.\n+ */\n+function checkModelFileExist(fileName: string): Promise<string> {\n+  return new Promise<string>(async (resolve, reject) => {\n+    if (!fileName) {\n+      reject(\"Model not found, please download again.\");\n+    }\n+    resolve(fileName);\n+  });\n }\n \n-function killSubprocess() {\n+/**\n+ * Terminates the Nitro subprocess.\n+ * @returns A Promise that resolves when the subprocess is terminated successfully, or rejects with an error message if the subprocess fails to terminate.\n+ */\n+function killSubprocess(): Promise<void> {\n   if (subprocess) {\n     subprocess.kill();\n     subprocess = null;\n     console.log(\"Subprocess terminated.\");\n   } else {\n-    kill(PORT, \"tcp\").then(console.log).catch(console.log);\n-    console.error(\"No subprocess is currently running.\");\n+    return kill(PORT, \"tcp\").then(console.log).catch(console.log);\n   }\n }\n \n+/**\n+ * Returns the path to the user data directory.\n+ * @returns The path to the user data directory.\n+ */\n function appPath() {\n-  if (app) {\n-    return app.getPath(\"userData\");\n+  return app.getPath(\"userData\");\n+}\n+\n+/**\n+ * Check port is used or not, if used, attempt to unload model\n+ * If unload failed, kill the port\n+ */\n+function checkAndUnloadNitro() {\n+  return tcpPortUsed.check(PORT, LOCAL_HOST).then((inUse) => {\n+    // If inUse - try unload or kill process, otherwise do nothing\n+    if (inUse) {\n+      // Attempt to unload model\n+      return fetch(NITRO_HTTP_UNLOAD_MODEL_URL, {\n+        method: \"GET\",\n+        headers: {\n+          \"Content-Type\": \"application/json\",\n+        },\n+      }).catch((err) => {\n+        console.log(err);\n+        // Fallback to kill the port\n+        return killSubprocess();\n+      });\n+    }\n+  });\n+}\n+\n+/**\n+ * Look for the Nitro binary and execute it\n+ * Using child-process to spawn the process\n+ * Should run exactly platform specified Nitro binary version\n+ */\n+function spawnNitroProcess() {\n+  let binaryFolder = path.join(__dirname, \"nitro\"); // Current directory by default\n+  let binaryName;\n+\n+  if (process.platform === \"win32\") {\n+    // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n+    binaryName = \"win-start.bat\";\n+  } else if (process.platform === \"darwin\") {\n+    // Mac OS platform\n+    if (process.arch === \"arm64\") {\n+      binaryFolder = path.join(binaryFolder, \"mac-arm64\");\n+    } else {\n+      binaryFolder = path.join(binaryFolder, \"mac-x64\");\n+    }\n+    binaryName = \"nitro\";\n+  } else {\n+    // Linux\n+    // Todo: Need to check for CUDA support to switch between CUDA and non-CUDA binaries\n+    binaryName = \"linux-start.sh\"; // For other platforms\n   }\n-  return process.env.APPDATA || (process.platform == 'darwin' ? process.env.HOME + '/Library/Preferences' : process.env.HOME + \"/.local/share\");\n+\n+  const binaryPath = path.join(binaryFolder, binaryName);\n+\n+  // Execute the binary\n+  subprocess = spawn(binaryPath, [1, \"0.0.0.0\", PORT], {\n+    cwd: binaryFolder,\n+  });\n+\n+  // Handle subprocess output\n+  subprocess.stdout.on(\"data\", (data) => {\n+    console.log(`stdout: ${data}`);\n+  });\n+\n+  subprocess.stderr.on(\"data\", (data) => {\n+    console.error(`stderr: ${data}`);",
        "comment_created_at": "2023-11-14T05:22:14+00:00",
        "comment_author": "louis-menlo",
        "comment_body": "@vuonghoainam, this task should only be handled by the Jan Log Manager (to be implemented later). Please refer to the 'jan-001-log-framework' JIP for more details. For now we disabled log output to file from Nitro, why try to enable from app?",
        "pr_file_module": null
      }
    ]
  }
]