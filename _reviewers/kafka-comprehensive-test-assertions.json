[
  {
    "discussion_id": "1412046678",
    "pr_number": 14849,
    "pr_file": "core/src/test/scala/unit/kafka/coordinator/group/CoordinatorLoaderImplTest.scala",
    "created_at": "2023-12-01T12:27:41+00:00",
    "commented_code": "}\n   }\n \n+  @Test\n+  def testUpdateLastWrittenOffsetOnBatchLoaded(): Unit = {\n+    val tp = new TopicPartition(\"foo\", 0)\n+    val replicaManager = mock(classOf[ReplicaManager])\n+    val serde = new StringKeyValueDeserializer\n+    val log = mock(classOf[UnifiedLog])\n+    val coordinator = mock(classOf[CoordinatorPlayback[(String, String)]])\n+\n+    TestUtils.resource(new CoordinatorLoaderImpl[(String, String)](\n+      time = Time.SYSTEM,\n+      replicaManager = replicaManager,\n+      deserializer = serde,\n+      loadBufferSize = 1000\n+    )) { loader =>\n+      when(replicaManager.getLog(tp)).thenReturn(Some(log))\n+      when(log.logStartOffset).thenReturn(0L)\n+      when(log.highWatermark).thenReturn(0L).thenReturn(0L).thenReturn(2L)\n+      when(replicaManager.getLogEndOffset(tp)).thenReturn(Some(7L))\n+\n+      val readResult1 = logReadResult(startOffset = 0, records = Seq(\n+        new SimpleRecord(\"k1\".getBytes, \"v1\".getBytes),\n+        new SimpleRecord(\"k2\".getBytes, \"v2\".getBytes)\n+      ))\n+\n+      when(log.read(\n+        startOffset = 0L,\n+        maxLength = 1000,\n+        isolation = FetchIsolation.LOG_END,\n+        minOneMessage = true\n+      )).thenReturn(readResult1)\n+\n+      val readResult2 = logReadResult(startOffset = 2, records = Seq(\n+        new SimpleRecord(\"k3\".getBytes, \"v3\".getBytes),\n+        new SimpleRecord(\"k4\".getBytes, \"v4\".getBytes),\n+        new SimpleRecord(\"k5\".getBytes, \"v5\".getBytes)\n+      ))\n+\n+      when(log.read(\n+        startOffset = 2L,\n+        maxLength = 1000,\n+        isolation = FetchIsolation.LOG_END,\n+        minOneMessage = true\n+      )).thenReturn(readResult2)\n+\n+      val readResult3 = logReadResult(startOffset = 5, records = Seq(\n+        new SimpleRecord(\"k6\".getBytes, \"v6\".getBytes),\n+        new SimpleRecord(\"k7\".getBytes, \"v7\".getBytes)\n+      ))\n+\n+      when(log.read(\n+        startOffset = 5L,\n+        maxLength = 1000,\n+        isolation = FetchIsolation.LOG_END,\n+        minOneMessage = true\n+      )).thenReturn(readResult3)\n+\n+      assertNotNull(loader.load(tp, coordinator).get(10, TimeUnit.SECONDS))\n+\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k1\", \"v1\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k2\", \"v2\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k3\", \"v3\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k4\", \"v4\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k5\", \"v5\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k6\", \"v6\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k7\", \"v7\"))\n+      verify(coordinator, times(0)).updateLastWrittenOffset(0)\n+      verify(coordinator, times(1)).updateLastWrittenOffset(2)\n+      verify(coordinator, times(1)).updateLastWrittenOffset(5)\n+      verify(coordinator, times(1)).updateLastWrittenOffset(7)\n+      verify(coordinator, times(1)).updateLastCommittedOffset(0)\n+      verify(coordinator, times(1)).updateLastCommittedOffset(2)\n+      verify(coordinator, times(0)).updateLastCommittedOffset(5)\n+    }\n+  }\n+\n+  @Test\n+  def testUpdateLastWrittenOffsetAndUpdateLastCommittedOffsetNoRecordsRead(): Unit = {\n+    val tp = new TopicPartition(\"foo\", 0)\n+    val replicaManager = mock(classOf[ReplicaManager])\n+    val serde = new StringKeyValueDeserializer\n+    val log = mock(classOf[UnifiedLog])\n+    val coordinator = mock(classOf[CoordinatorPlayback[(String, String)]])\n+\n+    TestUtils.resource(new CoordinatorLoaderImpl[(String, String)](\n+      time = Time.SYSTEM,\n+      replicaManager = replicaManager,\n+      deserializer = serde,\n+      loadBufferSize = 1000\n+    )) { loader =>\n+      when(replicaManager.getLog(tp)).thenReturn(Some(log))\n+      when(log.logStartOffset).thenReturn(0L)\n+      when(log.highWatermark).thenReturn(0L)\n+      when(replicaManager.getLogEndOffset(tp)).thenReturn(Some(0L))\n+\n+      assertNotNull(loader.load(tp, coordinator).get(10, TimeUnit.SECONDS))\n+\n+      verify(coordinator, times(0)).updateLastWrittenOffset(0)",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "1412046678",
        "repo_full_name": "apache/kafka",
        "pr_number": 14849,
        "pr_file": "core/src/test/scala/unit/kafka/coordinator/group/CoordinatorLoaderImplTest.scala",
        "discussion_id": "1412046678",
        "commented_code": "@@ -366,6 +370,110 @@ class CoordinatorLoaderImplTest {\n     }\n   }\n \n+  @Test\n+  def testUpdateLastWrittenOffsetOnBatchLoaded(): Unit = {\n+    val tp = new TopicPartition(\"foo\", 0)\n+    val replicaManager = mock(classOf[ReplicaManager])\n+    val serde = new StringKeyValueDeserializer\n+    val log = mock(classOf[UnifiedLog])\n+    val coordinator = mock(classOf[CoordinatorPlayback[(String, String)]])\n+\n+    TestUtils.resource(new CoordinatorLoaderImpl[(String, String)](\n+      time = Time.SYSTEM,\n+      replicaManager = replicaManager,\n+      deserializer = serde,\n+      loadBufferSize = 1000\n+    )) { loader =>\n+      when(replicaManager.getLog(tp)).thenReturn(Some(log))\n+      when(log.logStartOffset).thenReturn(0L)\n+      when(log.highWatermark).thenReturn(0L).thenReturn(0L).thenReturn(2L)\n+      when(replicaManager.getLogEndOffset(tp)).thenReturn(Some(7L))\n+\n+      val readResult1 = logReadResult(startOffset = 0, records = Seq(\n+        new SimpleRecord(\"k1\".getBytes, \"v1\".getBytes),\n+        new SimpleRecord(\"k2\".getBytes, \"v2\".getBytes)\n+      ))\n+\n+      when(log.read(\n+        startOffset = 0L,\n+        maxLength = 1000,\n+        isolation = FetchIsolation.LOG_END,\n+        minOneMessage = true\n+      )).thenReturn(readResult1)\n+\n+      val readResult2 = logReadResult(startOffset = 2, records = Seq(\n+        new SimpleRecord(\"k3\".getBytes, \"v3\".getBytes),\n+        new SimpleRecord(\"k4\".getBytes, \"v4\".getBytes),\n+        new SimpleRecord(\"k5\".getBytes, \"v5\".getBytes)\n+      ))\n+\n+      when(log.read(\n+        startOffset = 2L,\n+        maxLength = 1000,\n+        isolation = FetchIsolation.LOG_END,\n+        minOneMessage = true\n+      )).thenReturn(readResult2)\n+\n+      val readResult3 = logReadResult(startOffset = 5, records = Seq(\n+        new SimpleRecord(\"k6\".getBytes, \"v6\".getBytes),\n+        new SimpleRecord(\"k7\".getBytes, \"v7\".getBytes)\n+      ))\n+\n+      when(log.read(\n+        startOffset = 5L,\n+        maxLength = 1000,\n+        isolation = FetchIsolation.LOG_END,\n+        minOneMessage = true\n+      )).thenReturn(readResult3)\n+\n+      assertNotNull(loader.load(tp, coordinator).get(10, TimeUnit.SECONDS))\n+\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k1\", \"v1\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k2\", \"v2\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k3\", \"v3\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k4\", \"v4\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k5\", \"v5\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k6\", \"v6\"))\n+      verify(coordinator).replay(RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, (\"k7\", \"v7\"))\n+      verify(coordinator, times(0)).updateLastWrittenOffset(0)\n+      verify(coordinator, times(1)).updateLastWrittenOffset(2)\n+      verify(coordinator, times(1)).updateLastWrittenOffset(5)\n+      verify(coordinator, times(1)).updateLastWrittenOffset(7)\n+      verify(coordinator, times(1)).updateLastCommittedOffset(0)\n+      verify(coordinator, times(1)).updateLastCommittedOffset(2)\n+      verify(coordinator, times(0)).updateLastCommittedOffset(5)\n+    }\n+  }\n+\n+  @Test\n+  def testUpdateLastWrittenOffsetAndUpdateLastCommittedOffsetNoRecordsRead(): Unit = {\n+    val tp = new TopicPartition(\"foo\", 0)\n+    val replicaManager = mock(classOf[ReplicaManager])\n+    val serde = new StringKeyValueDeserializer\n+    val log = mock(classOf[UnifiedLog])\n+    val coordinator = mock(classOf[CoordinatorPlayback[(String, String)]])\n+\n+    TestUtils.resource(new CoordinatorLoaderImpl[(String, String)](\n+      time = Time.SYSTEM,\n+      replicaManager = replicaManager,\n+      deserializer = serde,\n+      loadBufferSize = 1000\n+    )) { loader =>\n+      when(replicaManager.getLog(tp)).thenReturn(Some(log))\n+      when(log.logStartOffset).thenReturn(0L)\n+      when(log.highWatermark).thenReturn(0L)\n+      when(replicaManager.getLogEndOffset(tp)).thenReturn(Some(0L))\n+\n+      assertNotNull(loader.load(tp, coordinator).get(10, TimeUnit.SECONDS))\n+\n+      verify(coordinator, times(0)).updateLastWrittenOffset(0)",
        "comment_created_at": "2023-12-01T12:27:41+00:00",
        "comment_author": "dajac",
        "comment_body": "Should we rather use `verify(coordinator, times(0)).updateLastWrittenOffset(anyLong())` instead of verifying 0, 2 and 5?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2182283728",
    "pr_number": 20045,
    "pr_file": "core/src/test/scala/integration/kafka/server/DelayedRemoteFetchTest.scala",
    "created_at": "2025-07-03T09:14:39+00:00",
    "commented_code": "@Test\n   def testRequestExpiry(): Unit = {\n-    var actualTopicPartition: Option[TopicIdPartition] = None\n-    var fetchResultOpt: Option[FetchPartitionData] = None\n+    val responses = mutable.Map[TopicIdPartition, FetchPartitionData]()\n \n-    def callback(responses: Seq[(TopicIdPartition, FetchPartitionData)]): Unit = {\n-      assertEquals(1, responses.size)\n-      actualTopicPartition = Some(responses.head._1)\n-      fetchResultOpt = Some(responses.head._2)\n+    def callback(responseSeq: Seq[(TopicIdPartition, FetchPartitionData)]): Unit = {\n+      responseSeq.foreach { case (tp, data) =>\n+        responses.put(tp, data)\n+      }\n     }\n \n+    def expiresPerSecValue(): Double = {\n+      val allMetrics = KafkaYammerMetrics.defaultRegistry.allMetrics.asScala\n+      val metric = allMetrics.find { case (n, _) => n.getMBeanName.endsWith(\"kafka.server:type=DelayedRemoteFetchMetrics,name=ExpiresPerSec\") }\n+\n+      if (metric.isEmpty)\n+        0\n+      else\n+        metric.get._2.asInstanceOf[Meter].count\n+    }\n+\n+    val remoteFetchTaskExpired = mock(classOf[Future[Void]])\n+    val remoteFetchTask2 = mock(classOf[Future[Void]])\n+    // complete the 2nd task, and keep the 1st one expired\n+    when(remoteFetchTask2.isDone).thenReturn(true)\n+\n+    // Create futures - one completed, one not\n+    val future1: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n+    val future2: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n+    // Only complete one remote fetch\n+    future2.complete(buildRemoteReadResult(Errors.NONE))\n+\n+    val fetchInfo1 = new RemoteStorageFetchInfo(0, false, topicIdPartition, null, null)\n+    val fetchInfo2 = new RemoteStorageFetchInfo(0, false, topicIdPartition2, null, null)\n+\n     val highWatermark = 100\n     val leaderLogStartOffset = 10\n \n-    val remoteFetchTask = mock(classOf[Future[Void]])\n-    val future: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n-    val fetchInfo: RemoteStorageFetchInfo = new RemoteStorageFetchInfo(0, false, topicIdPartition.topicPartition(), null, null)\n-    val logReadInfo = buildReadResult(Errors.NONE, highWatermark, leaderLogStartOffset)\n-\n-    val delayedRemoteFetch = new DelayedRemoteFetch(remoteFetchTask, future, fetchInfo, remoteFetchMaxWaitMs,\n-      Seq(topicIdPartition -> fetchStatus), fetchParams, Seq(topicIdPartition -> logReadInfo), replicaManager, callback)\n+    val logReadInfo1 = buildReadResult(Errors.NONE, highWatermark, leaderLogStartOffset)\n+    val logReadInfo2 = buildReadResult(Errors.NONE)\n+\n+    val fetchStatus1 = FetchPartitionStatus(\n+      startOffsetMetadata = new LogOffsetMetadata(fetchOffset),\n+      fetchInfo = new FetchRequest.PartitionData(Uuid.ZERO_UUID, fetchOffset, logStartOffset, maxBytes, currentLeaderEpoch))\n+    val fetchStatus2 = FetchPartitionStatus(\n+      startOffsetMetadata = new LogOffsetMetadata(fetchOffset + 100),\n+      fetchInfo = new FetchRequest.PartitionData(Uuid.ZERO_UUID, fetchOffset + 100, logStartOffset, maxBytes, currentLeaderEpoch))\n+\n+    // Set up maps for multiple partitions\n+    val remoteFetchTasks = new java.util.HashMap[TopicIdPartition, Future[Void]]()\n+    val remoteFetchResults = new java.util.HashMap[TopicIdPartition, CompletableFuture[RemoteLogReadResult]]()\n+    val remoteFetchInfos = new java.util.HashMap[TopicIdPartition, RemoteStorageFetchInfo]()\n+\n+    remoteFetchTasks.put(topicIdPartition, remoteFetchTaskExpired)\n+    remoteFetchTasks.put(topicIdPartition2, remoteFetchTask2)\n+    remoteFetchResults.put(topicIdPartition, future1)\n+    remoteFetchResults.put(topicIdPartition2, future2)\n+    remoteFetchInfos.put(topicIdPartition, fetchInfo1)\n+    remoteFetchInfos.put(topicIdPartition2, fetchInfo2)\n+\n+    val delayedRemoteFetch = new DelayedRemoteFetch(\n+      remoteFetchTasks,\n+      remoteFetchResults,\n+      remoteFetchInfos,\n+      remoteFetchMaxWaitMs,\n+      Seq(topicIdPartition -> fetchStatus1, topicIdPartition2 -> fetchStatus2),\n+      fetchParams,\n+      Seq(topicIdPartition -> logReadInfo1, topicIdPartition2 -> logReadInfo2),\n+      replicaManager,\n+      callback)\n \n     when(replicaManager.getPartitionOrException(topicIdPartition.topicPartition))\n       .thenReturn(mock(classOf[Partition]))\n+    when(replicaManager.getPartitionOrException(topicIdPartition2.topicPartition))\n+      .thenReturn(mock(classOf[Partition]))\n \n     // Verify that the ExpiresPerSec metric is zero before fetching\n-    val metrics = KafkaYammerMetrics.defaultRegistry.allMetrics\n-    assertEquals(0, metrics.keySet.asScala.count(_.getMBeanName == \"kafka.server:type=DelayedRemoteFetchMetrics,name=ExpiresPerSec\"))\n+    val existingMetricVal = expiresPerSecValue()\n+    // Verify the delayedRemoteFetch is not completed yet\n+    assertFalse(delayedRemoteFetch.isCompleted)\n \n     // Force the delayed remote fetch to expire\n     delayedRemoteFetch.run()\n \n-    // Check that the task was cancelled and force-completed\n-    verify(remoteFetchTask).cancel(false)\n+    // Check that the expired task was cancelled and force-completed\n+    verify(remoteFetchTaskExpired).cancel(anyBoolean())\n+    verify(remoteFetchTask2, never()).cancel(anyBoolean())\n     assertTrue(delayedRemoteFetch.isCompleted)\n \n     // Check that the ExpiresPerSec metric was incremented\n-    assertEquals(1, metrics.keySet.asScala.count(_.getMBeanName == \"kafka.server:type=DelayedRemoteFetchMetrics,name=ExpiresPerSec\"))\n+    assertTrue(expiresPerSecValue() > existingMetricVal)\n \n-    // Fetch results should still include local read results\n-    assertTrue(actualTopicPartition.isDefined)\n-    assertEquals(topicIdPartition, actualTopicPartition.get)\n-    assertTrue(fetchResultOpt.isDefined)\n+    // Fetch results should include 2 results and the expired one should return local read results\n+    assertEquals(2, responses.size)\n+    assertTrue(responses.contains(topicIdPartition))\n+    assertTrue(responses.contains(topicIdPartition2))\n \n-    val fetchResult = fetchResultOpt.get\n-    assertEquals(Errors.NONE, fetchResult.error)\n-    assertEquals(highWatermark, fetchResult.highWatermark)\n-    assertEquals(leaderLogStartOffset, fetchResult.logStartOffset)\n+    assertEquals(Errors.NONE, responses(topicIdPartition).error)\n+    assertEquals(highWatermark, responses(topicIdPartition).highWatermark)\n+    assertEquals(leaderLogStartOffset, responses(topicIdPartition).logStartOffset)\n+\n+    assertEquals(Errors.NONE, responses(topicIdPartition2).error)\n+  }\n+\n+  @Test\n+  def testMultiplePartitions(): Unit = {\n+    val responses = mutable.Map[TopicIdPartition, FetchPartitionData]()\n+\n+    def callback(responseSeq: Seq[(TopicIdPartition, FetchPartitionData)]): Unit = {\n+      responseSeq.foreach { case (tp, data) =>\n+        responses.put(tp, data)\n+      }\n+    }\n+\n+    // Create futures - one completed, one not\n+    val future1: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n+    val future2: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n+    // Only complete one remote fetch\n+    future1.complete(buildRemoteReadResult(Errors.NONE))\n+\n+    val fetchInfo1 = new RemoteStorageFetchInfo(0, false, topicIdPartition, null, null)\n+    val fetchInfo2 = new RemoteStorageFetchInfo(0, false, topicIdPartition2, null, null)\n+\n+    val logReadInfo1 = buildReadResult(Errors.NONE, 100, 10)\n+    val logReadInfo2 = buildReadResult(Errors.NONE, 200, 20)\n+\n+    val fetchStatus1 = FetchPartitionStatus(\n+      startOffsetMetadata = new LogOffsetMetadata(fetchOffset),\n+      fetchInfo = new FetchRequest.PartitionData(Uuid.ZERO_UUID, fetchOffset, logStartOffset, maxBytes, currentLeaderEpoch))\n+    val fetchStatus2 = FetchPartitionStatus(\n+      startOffsetMetadata = new LogOffsetMetadata(fetchOffset + 100),\n+      fetchInfo = new FetchRequest.PartitionData(Uuid.ZERO_UUID, fetchOffset + 100, logStartOffset, maxBytes, currentLeaderEpoch))\n+\n+    // Set up maps for multiple partitions\n+    val remoteFetchResults = new java.util.HashMap[TopicIdPartition, CompletableFuture[RemoteLogReadResult]]()\n+    val remoteFetchInfos = new java.util.HashMap[TopicIdPartition, RemoteStorageFetchInfo]()\n+\n+    remoteFetchResults.put(topicIdPartition, future1)\n+    remoteFetchResults.put(topicIdPartition2, future2)\n+    remoteFetchInfos.put(topicIdPartition, fetchInfo1)\n+    remoteFetchInfos.put(topicIdPartition2, fetchInfo2)\n+\n+    val delayedRemoteFetch = new DelayedRemoteFetch(\n+      Collections.emptyMap[TopicIdPartition, Future[Void]](),\n+      remoteFetchResults,\n+      remoteFetchInfos,\n+      remoteFetchMaxWaitMs,\n+      Seq(topicIdPartition -> fetchStatus1, topicIdPartition2 -> fetchStatus2),\n+      fetchParams,\n+      Seq(topicIdPartition -> logReadInfo1, topicIdPartition2 -> logReadInfo2),\n+      replicaManager,\n+      callback)\n+\n+    when(replicaManager.getPartitionOrException(topicIdPartition.topicPartition))\n+      .thenReturn(mock(classOf[Partition]))\n+    when(replicaManager.getPartitionOrException(topicIdPartition2.topicPartition))\n+      .thenReturn(mock(classOf[Partition]))\n+\n+    // Should not complete since future2 is not done\n+    assertFalse(delayedRemoteFetch.tryComplete())\n+    assertFalse(delayedRemoteFetch.isCompleted)\n+\n+    // Complete the other future\n+    future2.complete(buildRemoteReadResult(Errors.NONE))\n+\n+    // Now it should complete\n+    assertTrue(delayedRemoteFetch.tryComplete())\n+    assertTrue(delayedRemoteFetch.isCompleted)\n+\n+    // Verify both partitions were processed without error\n+    assertEquals(2, responses.size)\n+    assertTrue(responses.contains(topicIdPartition))\n+    assertTrue(responses.contains(topicIdPartition2))\n+\n+    assertEquals(Errors.NONE, responses(topicIdPartition).error)\n+    assertEquals(Errors.NONE, responses(topicIdPartition2).error)",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2182283728",
        "repo_full_name": "apache/kafka",
        "pr_number": 20045,
        "pr_file": "core/src/test/scala/integration/kafka/server/DelayedRemoteFetchTest.scala",
        "discussion_id": "2182283728",
        "commented_code": "@@ -170,52 +206,252 @@ class DelayedRemoteFetchTest {\n \n   @Test\n   def testRequestExpiry(): Unit = {\n-    var actualTopicPartition: Option[TopicIdPartition] = None\n-    var fetchResultOpt: Option[FetchPartitionData] = None\n+    val responses = mutable.Map[TopicIdPartition, FetchPartitionData]()\n \n-    def callback(responses: Seq[(TopicIdPartition, FetchPartitionData)]): Unit = {\n-      assertEquals(1, responses.size)\n-      actualTopicPartition = Some(responses.head._1)\n-      fetchResultOpt = Some(responses.head._2)\n+    def callback(responseSeq: Seq[(TopicIdPartition, FetchPartitionData)]): Unit = {\n+      responseSeq.foreach { case (tp, data) =>\n+        responses.put(tp, data)\n+      }\n     }\n \n+    def expiresPerSecValue(): Double = {\n+      val allMetrics = KafkaYammerMetrics.defaultRegistry.allMetrics.asScala\n+      val metric = allMetrics.find { case (n, _) => n.getMBeanName.endsWith(\"kafka.server:type=DelayedRemoteFetchMetrics,name=ExpiresPerSec\") }\n+\n+      if (metric.isEmpty)\n+        0\n+      else\n+        metric.get._2.asInstanceOf[Meter].count\n+    }\n+\n+    val remoteFetchTaskExpired = mock(classOf[Future[Void]])\n+    val remoteFetchTask2 = mock(classOf[Future[Void]])\n+    // complete the 2nd task, and keep the 1st one expired\n+    when(remoteFetchTask2.isDone).thenReturn(true)\n+\n+    // Create futures - one completed, one not\n+    val future1: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n+    val future2: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n+    // Only complete one remote fetch\n+    future2.complete(buildRemoteReadResult(Errors.NONE))\n+\n+    val fetchInfo1 = new RemoteStorageFetchInfo(0, false, topicIdPartition, null, null)\n+    val fetchInfo2 = new RemoteStorageFetchInfo(0, false, topicIdPartition2, null, null)\n+\n     val highWatermark = 100\n     val leaderLogStartOffset = 10\n \n-    val remoteFetchTask = mock(classOf[Future[Void]])\n-    val future: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n-    val fetchInfo: RemoteStorageFetchInfo = new RemoteStorageFetchInfo(0, false, topicIdPartition.topicPartition(), null, null)\n-    val logReadInfo = buildReadResult(Errors.NONE, highWatermark, leaderLogStartOffset)\n-\n-    val delayedRemoteFetch = new DelayedRemoteFetch(remoteFetchTask, future, fetchInfo, remoteFetchMaxWaitMs,\n-      Seq(topicIdPartition -> fetchStatus), fetchParams, Seq(topicIdPartition -> logReadInfo), replicaManager, callback)\n+    val logReadInfo1 = buildReadResult(Errors.NONE, highWatermark, leaderLogStartOffset)\n+    val logReadInfo2 = buildReadResult(Errors.NONE)\n+\n+    val fetchStatus1 = FetchPartitionStatus(\n+      startOffsetMetadata = new LogOffsetMetadata(fetchOffset),\n+      fetchInfo = new FetchRequest.PartitionData(Uuid.ZERO_UUID, fetchOffset, logStartOffset, maxBytes, currentLeaderEpoch))\n+    val fetchStatus2 = FetchPartitionStatus(\n+      startOffsetMetadata = new LogOffsetMetadata(fetchOffset + 100),\n+      fetchInfo = new FetchRequest.PartitionData(Uuid.ZERO_UUID, fetchOffset + 100, logStartOffset, maxBytes, currentLeaderEpoch))\n+\n+    // Set up maps for multiple partitions\n+    val remoteFetchTasks = new java.util.HashMap[TopicIdPartition, Future[Void]]()\n+    val remoteFetchResults = new java.util.HashMap[TopicIdPartition, CompletableFuture[RemoteLogReadResult]]()\n+    val remoteFetchInfos = new java.util.HashMap[TopicIdPartition, RemoteStorageFetchInfo]()\n+\n+    remoteFetchTasks.put(topicIdPartition, remoteFetchTaskExpired)\n+    remoteFetchTasks.put(topicIdPartition2, remoteFetchTask2)\n+    remoteFetchResults.put(topicIdPartition, future1)\n+    remoteFetchResults.put(topicIdPartition2, future2)\n+    remoteFetchInfos.put(topicIdPartition, fetchInfo1)\n+    remoteFetchInfos.put(topicIdPartition2, fetchInfo2)\n+\n+    val delayedRemoteFetch = new DelayedRemoteFetch(\n+      remoteFetchTasks,\n+      remoteFetchResults,\n+      remoteFetchInfos,\n+      remoteFetchMaxWaitMs,\n+      Seq(topicIdPartition -> fetchStatus1, topicIdPartition2 -> fetchStatus2),\n+      fetchParams,\n+      Seq(topicIdPartition -> logReadInfo1, topicIdPartition2 -> logReadInfo2),\n+      replicaManager,\n+      callback)\n \n     when(replicaManager.getPartitionOrException(topicIdPartition.topicPartition))\n       .thenReturn(mock(classOf[Partition]))\n+    when(replicaManager.getPartitionOrException(topicIdPartition2.topicPartition))\n+      .thenReturn(mock(classOf[Partition]))\n \n     // Verify that the ExpiresPerSec metric is zero before fetching\n-    val metrics = KafkaYammerMetrics.defaultRegistry.allMetrics\n-    assertEquals(0, metrics.keySet.asScala.count(_.getMBeanName == \"kafka.server:type=DelayedRemoteFetchMetrics,name=ExpiresPerSec\"))\n+    val existingMetricVal = expiresPerSecValue()\n+    // Verify the delayedRemoteFetch is not completed yet\n+    assertFalse(delayedRemoteFetch.isCompleted)\n \n     // Force the delayed remote fetch to expire\n     delayedRemoteFetch.run()\n \n-    // Check that the task was cancelled and force-completed\n-    verify(remoteFetchTask).cancel(false)\n+    // Check that the expired task was cancelled and force-completed\n+    verify(remoteFetchTaskExpired).cancel(anyBoolean())\n+    verify(remoteFetchTask2, never()).cancel(anyBoolean())\n     assertTrue(delayedRemoteFetch.isCompleted)\n \n     // Check that the ExpiresPerSec metric was incremented\n-    assertEquals(1, metrics.keySet.asScala.count(_.getMBeanName == \"kafka.server:type=DelayedRemoteFetchMetrics,name=ExpiresPerSec\"))\n+    assertTrue(expiresPerSecValue() > existingMetricVal)\n \n-    // Fetch results should still include local read results\n-    assertTrue(actualTopicPartition.isDefined)\n-    assertEquals(topicIdPartition, actualTopicPartition.get)\n-    assertTrue(fetchResultOpt.isDefined)\n+    // Fetch results should include 2 results and the expired one should return local read results\n+    assertEquals(2, responses.size)\n+    assertTrue(responses.contains(topicIdPartition))\n+    assertTrue(responses.contains(topicIdPartition2))\n \n-    val fetchResult = fetchResultOpt.get\n-    assertEquals(Errors.NONE, fetchResult.error)\n-    assertEquals(highWatermark, fetchResult.highWatermark)\n-    assertEquals(leaderLogStartOffset, fetchResult.logStartOffset)\n+    assertEquals(Errors.NONE, responses(topicIdPartition).error)\n+    assertEquals(highWatermark, responses(topicIdPartition).highWatermark)\n+    assertEquals(leaderLogStartOffset, responses(topicIdPartition).logStartOffset)\n+\n+    assertEquals(Errors.NONE, responses(topicIdPartition2).error)\n+  }\n+\n+  @Test\n+  def testMultiplePartitions(): Unit = {\n+    val responses = mutable.Map[TopicIdPartition, FetchPartitionData]()\n+\n+    def callback(responseSeq: Seq[(TopicIdPartition, FetchPartitionData)]): Unit = {\n+      responseSeq.foreach { case (tp, data) =>\n+        responses.put(tp, data)\n+      }\n+    }\n+\n+    // Create futures - one completed, one not\n+    val future1: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n+    val future2: CompletableFuture[RemoteLogReadResult] = new CompletableFuture[RemoteLogReadResult]()\n+    // Only complete one remote fetch\n+    future1.complete(buildRemoteReadResult(Errors.NONE))\n+\n+    val fetchInfo1 = new RemoteStorageFetchInfo(0, false, topicIdPartition, null, null)\n+    val fetchInfo2 = new RemoteStorageFetchInfo(0, false, topicIdPartition2, null, null)\n+\n+    val logReadInfo1 = buildReadResult(Errors.NONE, 100, 10)\n+    val logReadInfo2 = buildReadResult(Errors.NONE, 200, 20)\n+\n+    val fetchStatus1 = FetchPartitionStatus(\n+      startOffsetMetadata = new LogOffsetMetadata(fetchOffset),\n+      fetchInfo = new FetchRequest.PartitionData(Uuid.ZERO_UUID, fetchOffset, logStartOffset, maxBytes, currentLeaderEpoch))\n+    val fetchStatus2 = FetchPartitionStatus(\n+      startOffsetMetadata = new LogOffsetMetadata(fetchOffset + 100),\n+      fetchInfo = new FetchRequest.PartitionData(Uuid.ZERO_UUID, fetchOffset + 100, logStartOffset, maxBytes, currentLeaderEpoch))\n+\n+    // Set up maps for multiple partitions\n+    val remoteFetchResults = new java.util.HashMap[TopicIdPartition, CompletableFuture[RemoteLogReadResult]]()\n+    val remoteFetchInfos = new java.util.HashMap[TopicIdPartition, RemoteStorageFetchInfo]()\n+\n+    remoteFetchResults.put(topicIdPartition, future1)\n+    remoteFetchResults.put(topicIdPartition2, future2)\n+    remoteFetchInfos.put(topicIdPartition, fetchInfo1)\n+    remoteFetchInfos.put(topicIdPartition2, fetchInfo2)\n+\n+    val delayedRemoteFetch = new DelayedRemoteFetch(\n+      Collections.emptyMap[TopicIdPartition, Future[Void]](),\n+      remoteFetchResults,\n+      remoteFetchInfos,\n+      remoteFetchMaxWaitMs,\n+      Seq(topicIdPartition -> fetchStatus1, topicIdPartition2 -> fetchStatus2),\n+      fetchParams,\n+      Seq(topicIdPartition -> logReadInfo1, topicIdPartition2 -> logReadInfo2),\n+      replicaManager,\n+      callback)\n+\n+    when(replicaManager.getPartitionOrException(topicIdPartition.topicPartition))\n+      .thenReturn(mock(classOf[Partition]))\n+    when(replicaManager.getPartitionOrException(topicIdPartition2.topicPartition))\n+      .thenReturn(mock(classOf[Partition]))\n+\n+    // Should not complete since future2 is not done\n+    assertFalse(delayedRemoteFetch.tryComplete())\n+    assertFalse(delayedRemoteFetch.isCompleted)\n+\n+    // Complete the other future\n+    future2.complete(buildRemoteReadResult(Errors.NONE))\n+\n+    // Now it should complete\n+    assertTrue(delayedRemoteFetch.tryComplete())\n+    assertTrue(delayedRemoteFetch.isCompleted)\n+\n+    // Verify both partitions were processed without error\n+    assertEquals(2, responses.size)\n+    assertTrue(responses.contains(topicIdPartition))\n+    assertTrue(responses.contains(topicIdPartition2))\n+\n+    assertEquals(Errors.NONE, responses(topicIdPartition).error)\n+    assertEquals(Errors.NONE, responses(topicIdPartition2).error)",
        "comment_created_at": "2025-07-03T09:14:39+00:00",
        "comment_author": "kamalcph",
        "comment_body": "can the highWatermark and logStartOffset also be asserted? It should also be filled in the response.",
        "pr_file_module": null
      }
    ]
  }
]