[
  {
    "discussion_id": "2215908760",
    "pr_number": 19820,
    "pr_file": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ShareGroupCommandTest.java",
    "created_at": "2025-07-18T12:18:01+00:00",
    "commented_code": "assertEquals(expectedResults, service.deleteShareGroups());\n         }\n     }\n+    \n+    @Test\n+    public void testAlterShareGroupAllTopicsSuccess() {\n+        String group = \"share-group\";\n+        String topic1 = \"topic1\";\n+        String topic2 = \"topic2\";\n+        String bootstrapServer = \"localhost:9092\";\n+        String[] cgcArgs = new String[]{\"--bootstrap-server\", bootstrapServer, \"--reset-offsets\", \"--to-earliest\", \"--execute\", \"--topic\", topic1, \"--topic\", topic2, \"--group\", group};\n+        Admin adminClient = mock(KafkaAdminClient.class);\n+\n+        ListShareGroupOffsetsResult listShareGroupOffsetsResult = AdminClientTestUtils.createListShareGroupOffsetsResult(\n+            Map.of(\n+                group,\n+                KafkaFuture.completedFuture(Map.of(new TopicPartition(topic1, 0), new OffsetAndMetadata(10L), new TopicPartition(topic1, 1), new OffsetAndMetadata(10L), \n+                    new TopicPartition(topic2, 0), new OffsetAndMetadata(0L)))\n+            )\n+        );\n+        when(adminClient.listShareGroupOffsets(any())).thenReturn(listShareGroupOffsetsResult);\n+        ListTopicsResult listTopicsResult = mock(ListTopicsResult.class);\n+        Set<String> topics = Set.of(\"topic1\", \"topic2\");\n+        when(listTopicsResult.names()).thenReturn(completedFuture(topics));\n+        when(adminClient.listTopics()).thenReturn(listTopicsResult);\n+        \n+        AlterShareGroupOffsetsResult alterShareGroupOffsetsResult = mockAlterShareGroupOffsets(adminClient, group);\n+        Map<TopicPartition, OffsetAndMetadata> partitionOffsets = Map.of(new TopicPartition(topic1, 0), new OffsetAndMetadata(0L), new TopicPartition(topic1, 1), new OffsetAndMetadata(0L),\n+            new TopicPartition(topic2, 0), new OffsetAndMetadata(0L));\n+        ListOffsetsResult listOffsetsResult = AdminClientTestUtils.createListOffsetsResult(partitionOffsets);\n+        when(adminClient.listOffsets(any(), any(ListOffsetsOptions.class))).thenReturn(listOffsetsResult);\n+\n+        ShareGroupDescription exp = new ShareGroupDescription(\n+            group,\n+            List.of(),\n+            GroupState.EMPTY,\n+            new Node(0, \"host1\", 9090), 0, 0);\n+        DescribeShareGroupsResult describeShareGroupsResult = mock(DescribeShareGroupsResult.class);\n+        when(describeShareGroupsResult.describedGroups()).thenReturn(Map.of(group, KafkaFuture.completedFuture(exp)));\n+        when(adminClient.describeShareGroups(ArgumentMatchers.anyCollection(), any(DescribeShareGroupsOptions.class))).thenReturn(describeShareGroupsResult);\n+        try (ShareGroupService service = getShareGroupService(cgcArgs, adminClient)) {\n+            service.resetOffsets();\n+            verify(adminClient).alterShareGroupOffsets(eq(group), anyMap());\n+            verify(alterShareGroupOffsetsResult, times(1)).all();\n+            verify(adminClient).describeShareGroups(ArgumentMatchers.anyCollection(), any(DescribeShareGroupsOptions.class));\n+        }\n+    }\n+\n+    @Test\n+    public void testResetOffsetsDryRunSuccess() {\n+        String group = \"share-group\";\n+        String topic = \"topic\";\n+        String bootstrapServer = \"localhost:9092\";\n+        String[] cgcArgs = new String[]{\"--bootstrap-server\", bootstrapServer, \"--reset-offsets\", \"--to-earliest\", \"--dry-run\", \"--topic\", topic, \"--group\", group};",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2215908760",
        "repo_full_name": "apache/kafka",
        "pr_number": 19820,
        "pr_file": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ShareGroupCommandTest.java",
        "discussion_id": "2215908760",
        "commented_code": "@@ -1052,6 +1059,217 @@ public void testDeleteShareGroupsPartialFailure() {\n             assertEquals(expectedResults, service.deleteShareGroups());\n         }\n     }\n+    \n+    @Test\n+    public void testAlterShareGroupAllTopicsSuccess() {\n+        String group = \"share-group\";\n+        String topic1 = \"topic1\";\n+        String topic2 = \"topic2\";\n+        String bootstrapServer = \"localhost:9092\";\n+        String[] cgcArgs = new String[]{\"--bootstrap-server\", bootstrapServer, \"--reset-offsets\", \"--to-earliest\", \"--execute\", \"--topic\", topic1, \"--topic\", topic2, \"--group\", group};\n+        Admin adminClient = mock(KafkaAdminClient.class);\n+\n+        ListShareGroupOffsetsResult listShareGroupOffsetsResult = AdminClientTestUtils.createListShareGroupOffsetsResult(\n+            Map.of(\n+                group,\n+                KafkaFuture.completedFuture(Map.of(new TopicPartition(topic1, 0), new OffsetAndMetadata(10L), new TopicPartition(topic1, 1), new OffsetAndMetadata(10L), \n+                    new TopicPartition(topic2, 0), new OffsetAndMetadata(0L)))\n+            )\n+        );\n+        when(adminClient.listShareGroupOffsets(any())).thenReturn(listShareGroupOffsetsResult);\n+        ListTopicsResult listTopicsResult = mock(ListTopicsResult.class);\n+        Set<String> topics = Set.of(\"topic1\", \"topic2\");\n+        when(listTopicsResult.names()).thenReturn(completedFuture(topics));\n+        when(adminClient.listTopics()).thenReturn(listTopicsResult);\n+        \n+        AlterShareGroupOffsetsResult alterShareGroupOffsetsResult = mockAlterShareGroupOffsets(adminClient, group);\n+        Map<TopicPartition, OffsetAndMetadata> partitionOffsets = Map.of(new TopicPartition(topic1, 0), new OffsetAndMetadata(0L), new TopicPartition(topic1, 1), new OffsetAndMetadata(0L),\n+            new TopicPartition(topic2, 0), new OffsetAndMetadata(0L));\n+        ListOffsetsResult listOffsetsResult = AdminClientTestUtils.createListOffsetsResult(partitionOffsets);\n+        when(adminClient.listOffsets(any(), any(ListOffsetsOptions.class))).thenReturn(listOffsetsResult);\n+\n+        ShareGroupDescription exp = new ShareGroupDescription(\n+            group,\n+            List.of(),\n+            GroupState.EMPTY,\n+            new Node(0, \"host1\", 9090), 0, 0);\n+        DescribeShareGroupsResult describeShareGroupsResult = mock(DescribeShareGroupsResult.class);\n+        when(describeShareGroupsResult.describedGroups()).thenReturn(Map.of(group, KafkaFuture.completedFuture(exp)));\n+        when(adminClient.describeShareGroups(ArgumentMatchers.anyCollection(), any(DescribeShareGroupsOptions.class))).thenReturn(describeShareGroupsResult);\n+        try (ShareGroupService service = getShareGroupService(cgcArgs, adminClient)) {\n+            service.resetOffsets();\n+            verify(adminClient).alterShareGroupOffsets(eq(group), anyMap());\n+            verify(alterShareGroupOffsetsResult, times(1)).all();\n+            verify(adminClient).describeShareGroups(ArgumentMatchers.anyCollection(), any(DescribeShareGroupsOptions.class));\n+        }\n+    }\n+\n+    @Test\n+    public void testResetOffsetsDryRunSuccess() {\n+        String group = \"share-group\";\n+        String topic = \"topic\";\n+        String bootstrapServer = \"localhost:9092\";\n+        String[] cgcArgs = new String[]{\"--bootstrap-server\", bootstrapServer, \"--reset-offsets\", \"--to-earliest\", \"--dry-run\", \"--topic\", topic, \"--group\", group};",
        "comment_created_at": "2025-07-18T12:18:01+00:00",
        "comment_author": "AndrewJSchofield",
        "comment_body": "Let's have some tests for `--to-latest` and `--to-datetime`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2217373514",
        "repo_full_name": "apache/kafka",
        "pr_number": 19820,
        "pr_file": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ShareGroupCommandTest.java",
        "discussion_id": "2215908760",
        "commented_code": "@@ -1052,6 +1059,217 @@ public void testDeleteShareGroupsPartialFailure() {\n             assertEquals(expectedResults, service.deleteShareGroups());\n         }\n     }\n+    \n+    @Test\n+    public void testAlterShareGroupAllTopicsSuccess() {\n+        String group = \"share-group\";\n+        String topic1 = \"topic1\";\n+        String topic2 = \"topic2\";\n+        String bootstrapServer = \"localhost:9092\";\n+        String[] cgcArgs = new String[]{\"--bootstrap-server\", bootstrapServer, \"--reset-offsets\", \"--to-earliest\", \"--execute\", \"--topic\", topic1, \"--topic\", topic2, \"--group\", group};\n+        Admin adminClient = mock(KafkaAdminClient.class);\n+\n+        ListShareGroupOffsetsResult listShareGroupOffsetsResult = AdminClientTestUtils.createListShareGroupOffsetsResult(\n+            Map.of(\n+                group,\n+                KafkaFuture.completedFuture(Map.of(new TopicPartition(topic1, 0), new OffsetAndMetadata(10L), new TopicPartition(topic1, 1), new OffsetAndMetadata(10L), \n+                    new TopicPartition(topic2, 0), new OffsetAndMetadata(0L)))\n+            )\n+        );\n+        when(adminClient.listShareGroupOffsets(any())).thenReturn(listShareGroupOffsetsResult);\n+        ListTopicsResult listTopicsResult = mock(ListTopicsResult.class);\n+        Set<String> topics = Set.of(\"topic1\", \"topic2\");\n+        when(listTopicsResult.names()).thenReturn(completedFuture(topics));\n+        when(adminClient.listTopics()).thenReturn(listTopicsResult);\n+        \n+        AlterShareGroupOffsetsResult alterShareGroupOffsetsResult = mockAlterShareGroupOffsets(adminClient, group);\n+        Map<TopicPartition, OffsetAndMetadata> partitionOffsets = Map.of(new TopicPartition(topic1, 0), new OffsetAndMetadata(0L), new TopicPartition(topic1, 1), new OffsetAndMetadata(0L),\n+            new TopicPartition(topic2, 0), new OffsetAndMetadata(0L));\n+        ListOffsetsResult listOffsetsResult = AdminClientTestUtils.createListOffsetsResult(partitionOffsets);\n+        when(adminClient.listOffsets(any(), any(ListOffsetsOptions.class))).thenReturn(listOffsetsResult);\n+\n+        ShareGroupDescription exp = new ShareGroupDescription(\n+            group,\n+            List.of(),\n+            GroupState.EMPTY,\n+            new Node(0, \"host1\", 9090), 0, 0);\n+        DescribeShareGroupsResult describeShareGroupsResult = mock(DescribeShareGroupsResult.class);\n+        when(describeShareGroupsResult.describedGroups()).thenReturn(Map.of(group, KafkaFuture.completedFuture(exp)));\n+        when(adminClient.describeShareGroups(ArgumentMatchers.anyCollection(), any(DescribeShareGroupsOptions.class))).thenReturn(describeShareGroupsResult);\n+        try (ShareGroupService service = getShareGroupService(cgcArgs, adminClient)) {\n+            service.resetOffsets();\n+            verify(adminClient).alterShareGroupOffsets(eq(group), anyMap());\n+            verify(alterShareGroupOffsetsResult, times(1)).all();\n+            verify(adminClient).describeShareGroups(ArgumentMatchers.anyCollection(), any(DescribeShareGroupsOptions.class));\n+        }\n+    }\n+\n+    @Test\n+    public void testResetOffsetsDryRunSuccess() {\n+        String group = \"share-group\";\n+        String topic = \"topic\";\n+        String bootstrapServer = \"localhost:9092\";\n+        String[] cgcArgs = new String[]{\"--bootstrap-server\", bootstrapServer, \"--reset-offsets\", \"--to-earliest\", \"--dry-run\", \"--topic\", topic, \"--group\", group};",
        "comment_created_at": "2025-07-19T16:53:33+00:00",
        "comment_author": "JimmyWang6",
        "comment_body": "I have added `testAlterShareGroupToLatestSuccess()` and `testAlterShareGroupAllTopicsToDatetimeSuccess()` tests for those two cases.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2076121829",
    "pr_number": 19589,
    "pr_file": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientAutoJoinTest.java",
    "created_at": "2025-05-06T19:29:55+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.record.MemoryRecords;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Stream;\n+\n+import static org.apache.kafka.raft.KafkaRaftClientTest.replicaKey;\n+\n+public class KafkaRaftClientAutoJoinTest {\n+    private static final int NUMBER_FETCH_TIMEOUTS_IN_ADD_REMOVE_PERIOD = 1;\n+\n+    @Test\n+    public void testAutoRemoveOldVoter() throws Exception {",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2076121829",
        "repo_full_name": "apache/kafka",
        "pr_number": 19589,
        "pr_file": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientAutoJoinTest.java",
        "discussion_id": "2076121829",
        "commented_code": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.record.MemoryRecords;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Stream;\n+\n+import static org.apache.kafka.raft.KafkaRaftClientTest.replicaKey;\n+\n+public class KafkaRaftClientAutoJoinTest {\n+    private static final int NUMBER_FETCH_TIMEOUTS_IN_ADD_REMOVE_PERIOD = 1;\n+\n+    @Test\n+    public void testAutoRemoveOldVoter() throws Exception {",
        "comment_created_at": "2025-05-06T19:29:55+00:00",
        "comment_author": "jsancio",
        "comment_body": "Do we need a test that fully does a remove follow by an add? E.g.\r\n\r\n0. Start with the local replica not in the voter set but the id in the voter set.\r\n1. Remove voter is sent and acknowledged.\r\n2. Next FETCH response send the VOTER_RECORD control batch without the voter in the old voter in the voter set.\r\n3. Add voter is sent and acknowledged.\r\n4. Next FETCH response send a VOTER_RECORD control btach with the local replica in the voter set.",
        "pr_file_module": null
      },
      {
        "comment_id": "2076298625",
        "repo_full_name": "apache/kafka",
        "pr_number": 19589,
        "pr_file": "raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientAutoJoinTest.java",
        "discussion_id": "2076121829",
        "commented_code": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.record.MemoryRecords;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Stream;\n+\n+import static org.apache.kafka.raft.KafkaRaftClientTest.replicaKey;\n+\n+public class KafkaRaftClientAutoJoinTest {\n+    private static final int NUMBER_FETCH_TIMEOUTS_IN_ADD_REMOVE_PERIOD = 1;\n+\n+    @Test\n+    public void testAutoRemoveOldVoter() throws Exception {",
        "comment_created_at": "2025-05-06T21:15:43+00:00",
        "comment_author": "kevin-wu24",
        "comment_body": "I think we can add one since it acts as like a pseudo-integration test for the feature. I'm not seeing anywhere in `KafkaRaftClientReconfigTest` or `KafkaRaftClientFetchTest` that covers step 2. I see a `KafkaRaftClientTest#testFollowerReplication` but it doesn't add control records in the `FETCH` response when `kraft.version == 1`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2223637452",
    "pr_number": 20205,
    "pr_file": "storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogManagerTest.java",
    "created_at": "2025-07-22T19:26:42+00:00",
    "commented_code": "NavigableMap<Integer, Long> refinedLeaderEpochMap = RemoteLogManager.buildFilteredLeaderEpochMap(leaderEpochToStartOffset);\n         assertEquals(expectedLeaderEpochs, refinedLeaderEpochMap);\n+\n+\n+        TreeMap<Integer, Long> leaderEpochToStartOffset2 = new TreeMap<>();",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2223637452",
        "repo_full_name": "apache/kafka",
        "pr_number": 20205,
        "pr_file": "storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogManagerTest.java",
        "discussion_id": "2223637452",
        "commented_code": "@@ -1847,6 +1847,18 @@ public void testBuildFilteredLeaderEpochMap() {\n \n         NavigableMap<Integer, Long> refinedLeaderEpochMap = RemoteLogManager.buildFilteredLeaderEpochMap(leaderEpochToStartOffset);\n         assertEquals(expectedLeaderEpochs, refinedLeaderEpochMap);\n+\n+\n+        TreeMap<Integer, Long> leaderEpochToStartOffset2 = new TreeMap<>();",
        "comment_created_at": "2025-07-22T19:26:42+00:00",
        "comment_author": "chia7712",
        "comment_body": "It would be useful to have a unit test to loop over 1000 times, using random input to ensure before/after methods have same output.",
        "pr_file_module": null
      },
      {
        "comment_id": "2225729707",
        "repo_full_name": "apache/kafka",
        "pr_number": 20205,
        "pr_file": "storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogManagerTest.java",
        "discussion_id": "2223637452",
        "commented_code": "@@ -1847,6 +1847,18 @@ public void testBuildFilteredLeaderEpochMap() {\n \n         NavigableMap<Integer, Long> refinedLeaderEpochMap = RemoteLogManager.buildFilteredLeaderEpochMap(leaderEpochToStartOffset);\n         assertEquals(expectedLeaderEpochs, refinedLeaderEpochMap);\n+\n+\n+        TreeMap<Integer, Long> leaderEpochToStartOffset2 = new TreeMap<>();",
        "comment_created_at": "2025-07-23T14:06:22+00:00",
        "comment_author": "majialoong",
        "comment_body": "Thanks, I will add the unit test results below to verify the consistency of the methods before and after this modification.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2217120785",
    "pr_number": 20170,
    "pr_file": "clients/src/test/java/org/apache/kafka/clients/producer/RoundRobinPartitionerTest.java",
    "created_at": "2025-07-19T03:18:57+00:00",
    "commented_code": "assertEquals(10, partitionCount.get(1).intValue());\n         assertEquals(10, partitionCount.get(2).intValue());\n     }\n+\n+    @Test\n+    public void testRoundRobinWithAbortForNewBatch() throws Exception {\n+        final String topicA = \"topicA\";\n+        final String topicB = \"topicB\";\n+\n+        Cluster testCluster = new Cluster(\"clusterId\", asList(NODES[0]), Collections.emptyList(),\n+                Collections.<String>emptySet(), Collections.<String>emptySet());\n+\n+        Partitioner partitioner = new RoundRobinPartitioner();\n+\n+        //abort for new batch - previous partition should be returned on subsequent call\n+        //simulate three threads producing to two topics, with race condition in producer\n+        partitioner.onNewBatch(topicA, testCluster, 7);\n+        partitioner.onNewBatch(topicA, testCluster, 8);\n+        partitioner.onNewBatch(topicB, testCluster, 1);\n+        assertEquals(7, partitioner.partition(topicA, null, null, null, null, testCluster));\n+        assertEquals(8, partitioner.partition(topicA, null, null, null, null, testCluster));\n+        assertEquals(1, partitioner.partition(topicB, null, null, null, null, testCluster));",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2217120785",
        "repo_full_name": "apache/kafka",
        "pr_number": 20170,
        "pr_file": "clients/src/test/java/org/apache/kafka/clients/producer/RoundRobinPartitionerTest.java",
        "discussion_id": "2217120785",
        "commented_code": "@@ -96,4 +96,24 @@ public void testRoundRobinWithKeyBytes() {\n         assertEquals(10, partitionCount.get(1).intValue());\n         assertEquals(10, partitionCount.get(2).intValue());\n     }\n+\n+    @Test\n+    public void testRoundRobinWithAbortForNewBatch() throws Exception {\n+        final String topicA = \"topicA\";\n+        final String topicB = \"topicB\";\n+\n+        Cluster testCluster = new Cluster(\"clusterId\", asList(NODES[0]), Collections.emptyList(),\n+                Collections.<String>emptySet(), Collections.<String>emptySet());\n+\n+        Partitioner partitioner = new RoundRobinPartitioner();\n+\n+        //abort for new batch - previous partition should be returned on subsequent call\n+        //simulate three threads producing to two topics, with race condition in producer\n+        partitioner.onNewBatch(topicA, testCluster, 7);\n+        partitioner.onNewBatch(topicA, testCluster, 8);\n+        partitioner.onNewBatch(topicB, testCluster, 1);\n+        assertEquals(7, partitioner.partition(topicA, null, null, null, null, testCluster));\n+        assertEquals(8, partitioner.partition(topicA, null, null, null, null, testCluster));\n+        assertEquals(1, partitioner.partition(topicB, null, null, null, null, testCluster));",
        "comment_created_at": "2025-07-19T03:18:57+00:00",
        "comment_author": "ash-at-github",
        "comment_body": "It's great that this test verifies the next partition selected matches the enqueued value for each topic. Consider adding test cases for some edge cases, like empty queue and error handling",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2201148519",
    "pr_number": 20124,
    "pr_file": "core/src/test/java/kafka/server/share/SharePartitionTest.java",
    "created_at": "2025-07-11T16:11:18+00:00",
    "commented_code": "assertEquals(20, sharePartition.nextFetchOffset());\n     }\n \n+    @Test\n+    public void testLsoMovementWithWriteStateRPCFailuresInAck() {\n+        Persister persister = Mockito.mock(Persister.class);\n+        SharePartition sharePartition = SharePartitionBuilder.builder()\n+            .withState(SharePartitionState.ACTIVE)\n+            .withPersister(persister)\n+            .build();\n+\n+        fetchAcquiredRecords(sharePartition, memoryRecords(5, 2), 5);\n+        fetchAcquiredRecords(sharePartition, memoryRecords(5, 7), 5);\n+\n+        // Validate that there is no ongoing transition.\n+        assertFalse(sharePartition.cachedState().get(2L).batchHasOngoingStateTransition());\n+        assertFalse(sharePartition.cachedState().get(7L).batchHasOngoingStateTransition());\n+\n+        // Return futures which will be completed later, so the batch state has ongoing transition.\n+        CompletableFuture<WriteShareGroupStateResult> future1 = new CompletableFuture<>();\n+        CompletableFuture<WriteShareGroupStateResult> future2 = new CompletableFuture<>();\n+\n+        // Mocking the persister write state RPC to return future 1 and future 2 when acknowledgement occurs for\n+        // offsets 2-6 and 7-11 respectively.\n+        Mockito.when(persister.writeState(Mockito.any())).thenReturn(future1).thenReturn(future2);\n+\n+        // Acknowledge batch to create ongoing transition.\n+        sharePartition.acknowledge(MEMBER_ID, List.of(new ShareAcknowledgementBatch(2, 6, List.of(AcknowledgeType.RELEASE.id))));\n+        sharePartition.acknowledge(MEMBER_ID, List.of(new ShareAcknowledgementBatch(7, 11, List.of(AcknowledgeType.RELEASE.id))));\n+\n+        // Validate that there is no ongoing transition.\n+        assertTrue(sharePartition.cachedState().get(2L).batchHasOngoingStateTransition());\n+        assertTrue(sharePartition.cachedState().get(7L).batchHasOngoingStateTransition());\n+\n+        // LSO is at 9.\n+        sharePartition.updateCacheAndOffsets(9);\n+\n+        // Start offset will be moved.\n+        assertEquals(9, sharePartition.nextFetchOffset());\n+        assertEquals(9, sharePartition.startOffset());\n+        assertEquals(11, sharePartition.endOffset());\n+        assertEquals(2, sharePartition.cachedState().size());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(2L).batchState());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(7L).state());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(8L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(9L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(10L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(11L).state());\n+\n+        // Complete future1 exceptionally so acknowledgement for 2-6 offsets will be completed.\n+        WriteShareGroupStateResult writeShareGroupStateResult = Mockito.mock(WriteShareGroupStateResult.class);\n+        Mockito.when(writeShareGroupStateResult.topicsData()).thenReturn(List.of(\n+            new TopicData<>(TOPIC_ID_PARTITION.topicId(), List.of(\n+                PartitionFactory.newPartitionErrorData(0, Errors.GROUP_ID_NOT_FOUND.code(), Errors.GROUP_ID_NOT_FOUND.message())))));\n+        future1.complete(writeShareGroupStateResult);\n+\n+        // The completion of future1 with exception should not impact the cached state since those records have already\n+        // been archived.\n+        assertEquals(9, sharePartition.nextFetchOffset());\n+        assertEquals(9, sharePartition.startOffset());\n+        assertEquals(11, sharePartition.endOffset());\n+        assertEquals(2, sharePartition.cachedState().size());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(2L).batchState());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(7L).state());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(8L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(9L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(10L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(11L).state());\n+    }\n+\n+    @Test\n+    public void inFlightStateRollbackAndArchiveStateTransition() throws InterruptedException {\n+        InFlightState inFlightState = new InFlightState(RecordState.ACQUIRED, 1, MEMBER_ID);\n+\n+        inFlightState.startStateTransition(RecordState.ACKNOWLEDGED, SharePartition.DeliveryCountOps.INCREASE, MAX_DELIVERY_COUNT, MEMBER_ID);\n+        assertTrue(inFlightState.hasOngoingStateTransition());\n+\n+        // We have an ongoing state transition from ACQUIRED to ACKNOWLEDGED which is not committed yet. At the same\n+        // time when we have a call to completeStateTransition with false commit value, we get a call to ARCHIVE the record.\n+        // No matter the order of the 2 calls, we should always be getting the final state as ARCHIVED.\n+        ExecutorService executorService = Executors.newFixedThreadPool(2);\n+        try {\n+            List<Callable<Void>> callables = List.of(\n+                () -> {\n+                    inFlightState.archive(\"member-2\");\n+                    return null;\n+                },\n+                () -> {\n+                    inFlightState.completeStateTransition(false);\n+                    return null;\n+                }",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2201148519",
        "repo_full_name": "apache/kafka",
        "pr_number": 20124,
        "pr_file": "core/src/test/java/kafka/server/share/SharePartitionTest.java",
        "discussion_id": "2201148519",
        "commented_code": "@@ -7467,6 +7468,164 @@ public void testNextFetchOffsetWhenOffsetsHaveOngoingTransition() {\n         assertEquals(20, sharePartition.nextFetchOffset());\n     }\n \n+    @Test\n+    public void testLsoMovementWithWriteStateRPCFailuresInAck() {\n+        Persister persister = Mockito.mock(Persister.class);\n+        SharePartition sharePartition = SharePartitionBuilder.builder()\n+            .withState(SharePartitionState.ACTIVE)\n+            .withPersister(persister)\n+            .build();\n+\n+        fetchAcquiredRecords(sharePartition, memoryRecords(5, 2), 5);\n+        fetchAcquiredRecords(sharePartition, memoryRecords(5, 7), 5);\n+\n+        // Validate that there is no ongoing transition.\n+        assertFalse(sharePartition.cachedState().get(2L).batchHasOngoingStateTransition());\n+        assertFalse(sharePartition.cachedState().get(7L).batchHasOngoingStateTransition());\n+\n+        // Return futures which will be completed later, so the batch state has ongoing transition.\n+        CompletableFuture<WriteShareGroupStateResult> future1 = new CompletableFuture<>();\n+        CompletableFuture<WriteShareGroupStateResult> future2 = new CompletableFuture<>();\n+\n+        // Mocking the persister write state RPC to return future 1 and future 2 when acknowledgement occurs for\n+        // offsets 2-6 and 7-11 respectively.\n+        Mockito.when(persister.writeState(Mockito.any())).thenReturn(future1).thenReturn(future2);\n+\n+        // Acknowledge batch to create ongoing transition.\n+        sharePartition.acknowledge(MEMBER_ID, List.of(new ShareAcknowledgementBatch(2, 6, List.of(AcknowledgeType.RELEASE.id))));\n+        sharePartition.acknowledge(MEMBER_ID, List.of(new ShareAcknowledgementBatch(7, 11, List.of(AcknowledgeType.RELEASE.id))));\n+\n+        // Validate that there is no ongoing transition.\n+        assertTrue(sharePartition.cachedState().get(2L).batchHasOngoingStateTransition());\n+        assertTrue(sharePartition.cachedState().get(7L).batchHasOngoingStateTransition());\n+\n+        // LSO is at 9.\n+        sharePartition.updateCacheAndOffsets(9);\n+\n+        // Start offset will be moved.\n+        assertEquals(9, sharePartition.nextFetchOffset());\n+        assertEquals(9, sharePartition.startOffset());\n+        assertEquals(11, sharePartition.endOffset());\n+        assertEquals(2, sharePartition.cachedState().size());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(2L).batchState());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(7L).state());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(8L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(9L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(10L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(11L).state());\n+\n+        // Complete future1 exceptionally so acknowledgement for 2-6 offsets will be completed.\n+        WriteShareGroupStateResult writeShareGroupStateResult = Mockito.mock(WriteShareGroupStateResult.class);\n+        Mockito.when(writeShareGroupStateResult.topicsData()).thenReturn(List.of(\n+            new TopicData<>(TOPIC_ID_PARTITION.topicId(), List.of(\n+                PartitionFactory.newPartitionErrorData(0, Errors.GROUP_ID_NOT_FOUND.code(), Errors.GROUP_ID_NOT_FOUND.message())))));\n+        future1.complete(writeShareGroupStateResult);\n+\n+        // The completion of future1 with exception should not impact the cached state since those records have already\n+        // been archived.\n+        assertEquals(9, sharePartition.nextFetchOffset());\n+        assertEquals(9, sharePartition.startOffset());\n+        assertEquals(11, sharePartition.endOffset());\n+        assertEquals(2, sharePartition.cachedState().size());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(2L).batchState());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(7L).state());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(8L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(9L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(10L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(11L).state());\n+    }\n+\n+    @Test\n+    public void inFlightStateRollbackAndArchiveStateTransition() throws InterruptedException {\n+        InFlightState inFlightState = new InFlightState(RecordState.ACQUIRED, 1, MEMBER_ID);\n+\n+        inFlightState.startStateTransition(RecordState.ACKNOWLEDGED, SharePartition.DeliveryCountOps.INCREASE, MAX_DELIVERY_COUNT, MEMBER_ID);\n+        assertTrue(inFlightState.hasOngoingStateTransition());\n+\n+        // We have an ongoing state transition from ACQUIRED to ACKNOWLEDGED which is not committed yet. At the same\n+        // time when we have a call to completeStateTransition with false commit value, we get a call to ARCHIVE the record.\n+        // No matter the order of the 2 calls, we should always be getting the final state as ARCHIVED.\n+        ExecutorService executorService = Executors.newFixedThreadPool(2);\n+        try {\n+            List<Callable<Void>> callables = List.of(\n+                () -> {\n+                    inFlightState.archive(\"member-2\");\n+                    return null;\n+                },\n+                () -> {\n+                    inFlightState.completeStateTransition(false);\n+                    return null;\n+                }",
        "comment_created_at": "2025-07-11T16:11:18+00:00",
        "comment_author": "apoorvmittal10",
        "comment_body": "We should add a test case when commit succeeds. It will be good to have so future refactoring do not introduce new issues.",
        "pr_file_module": null
      },
      {
        "comment_id": "2203842607",
        "repo_full_name": "apache/kafka",
        "pr_number": 20124,
        "pr_file": "core/src/test/java/kafka/server/share/SharePartitionTest.java",
        "discussion_id": "2201148519",
        "commented_code": "@@ -7467,6 +7468,164 @@ public void testNextFetchOffsetWhenOffsetsHaveOngoingTransition() {\n         assertEquals(20, sharePartition.nextFetchOffset());\n     }\n \n+    @Test\n+    public void testLsoMovementWithWriteStateRPCFailuresInAck() {\n+        Persister persister = Mockito.mock(Persister.class);\n+        SharePartition sharePartition = SharePartitionBuilder.builder()\n+            .withState(SharePartitionState.ACTIVE)\n+            .withPersister(persister)\n+            .build();\n+\n+        fetchAcquiredRecords(sharePartition, memoryRecords(5, 2), 5);\n+        fetchAcquiredRecords(sharePartition, memoryRecords(5, 7), 5);\n+\n+        // Validate that there is no ongoing transition.\n+        assertFalse(sharePartition.cachedState().get(2L).batchHasOngoingStateTransition());\n+        assertFalse(sharePartition.cachedState().get(7L).batchHasOngoingStateTransition());\n+\n+        // Return futures which will be completed later, so the batch state has ongoing transition.\n+        CompletableFuture<WriteShareGroupStateResult> future1 = new CompletableFuture<>();\n+        CompletableFuture<WriteShareGroupStateResult> future2 = new CompletableFuture<>();\n+\n+        // Mocking the persister write state RPC to return future 1 and future 2 when acknowledgement occurs for\n+        // offsets 2-6 and 7-11 respectively.\n+        Mockito.when(persister.writeState(Mockito.any())).thenReturn(future1).thenReturn(future2);\n+\n+        // Acknowledge batch to create ongoing transition.\n+        sharePartition.acknowledge(MEMBER_ID, List.of(new ShareAcknowledgementBatch(2, 6, List.of(AcknowledgeType.RELEASE.id))));\n+        sharePartition.acknowledge(MEMBER_ID, List.of(new ShareAcknowledgementBatch(7, 11, List.of(AcknowledgeType.RELEASE.id))));\n+\n+        // Validate that there is no ongoing transition.\n+        assertTrue(sharePartition.cachedState().get(2L).batchHasOngoingStateTransition());\n+        assertTrue(sharePartition.cachedState().get(7L).batchHasOngoingStateTransition());\n+\n+        // LSO is at 9.\n+        sharePartition.updateCacheAndOffsets(9);\n+\n+        // Start offset will be moved.\n+        assertEquals(9, sharePartition.nextFetchOffset());\n+        assertEquals(9, sharePartition.startOffset());\n+        assertEquals(11, sharePartition.endOffset());\n+        assertEquals(2, sharePartition.cachedState().size());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(2L).batchState());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(7L).state());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(8L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(9L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(10L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(11L).state());\n+\n+        // Complete future1 exceptionally so acknowledgement for 2-6 offsets will be completed.\n+        WriteShareGroupStateResult writeShareGroupStateResult = Mockito.mock(WriteShareGroupStateResult.class);\n+        Mockito.when(writeShareGroupStateResult.topicsData()).thenReturn(List.of(\n+            new TopicData<>(TOPIC_ID_PARTITION.topicId(), List.of(\n+                PartitionFactory.newPartitionErrorData(0, Errors.GROUP_ID_NOT_FOUND.code(), Errors.GROUP_ID_NOT_FOUND.message())))));\n+        future1.complete(writeShareGroupStateResult);\n+\n+        // The completion of future1 with exception should not impact the cached state since those records have already\n+        // been archived.\n+        assertEquals(9, sharePartition.nextFetchOffset());\n+        assertEquals(9, sharePartition.startOffset());\n+        assertEquals(11, sharePartition.endOffset());\n+        assertEquals(2, sharePartition.cachedState().size());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(2L).batchState());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(7L).state());\n+        assertEquals(RecordState.ARCHIVED, sharePartition.cachedState().get(7L).offsetState().get(8L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(9L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(10L).state());\n+        assertEquals(RecordState.AVAILABLE, sharePartition.cachedState().get(7L).offsetState().get(11L).state());\n+    }\n+\n+    @Test\n+    public void inFlightStateRollbackAndArchiveStateTransition() throws InterruptedException {\n+        InFlightState inFlightState = new InFlightState(RecordState.ACQUIRED, 1, MEMBER_ID);\n+\n+        inFlightState.startStateTransition(RecordState.ACKNOWLEDGED, SharePartition.DeliveryCountOps.INCREASE, MAX_DELIVERY_COUNT, MEMBER_ID);\n+        assertTrue(inFlightState.hasOngoingStateTransition());\n+\n+        // We have an ongoing state transition from ACQUIRED to ACKNOWLEDGED which is not committed yet. At the same\n+        // time when we have a call to completeStateTransition with false commit value, we get a call to ARCHIVE the record.\n+        // No matter the order of the 2 calls, we should always be getting the final state as ARCHIVED.\n+        ExecutorService executorService = Executors.newFixedThreadPool(2);\n+        try {\n+            List<Callable<Void>> callables = List.of(\n+                () -> {\n+                    inFlightState.archive(\"member-2\");\n+                    return null;\n+                },\n+                () -> {\n+                    inFlightState.completeStateTransition(false);\n+                    return null;\n+                }",
        "comment_created_at": "2025-07-14T04:50:38+00:00",
        "comment_author": "adixitconfluent",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2205560889",
    "pr_number": 20137,
    "pr_file": "server-common/src/test/java/org/apache/kafka/server/common/MetadataVersionTest.java",
    "created_at": "2025-07-14T18:31:42+00:00",
    "commented_code": "assertEquals(IBP_4_0_IV2, MetadataVersion.fromVersionString(\"4.0-IV2\"));\n         assertEquals(IBP_4_0_IV3, MetadataVersion.fromVersionString(\"4.0-IV3\"));\n \n+        // 4.1-IV1 is the latest production version in the 4.1 line\n         assertEquals(IBP_4_1_IV0, MetadataVersion.fromVersionString(\"4.1-IV0\"));\n+        assertEquals(IBP_4_1_IV1, MetadataVersion.fromVersionString(\"4.1-IV1\"));",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2205560889",
        "repo_full_name": "apache/kafka",
        "pr_number": 20137,
        "pr_file": "server-common/src/test/java/org/apache/kafka/server/common/MetadataVersionTest.java",
        "discussion_id": "2205560889",
        "commented_code": "@@ -83,7 +83,9 @@ public void testFromVersionString() {\n         assertEquals(IBP_4_0_IV2, MetadataVersion.fromVersionString(\"4.0-IV2\"));\n         assertEquals(IBP_4_0_IV3, MetadataVersion.fromVersionString(\"4.0-IV3\"));\n \n+        // 4.1-IV1 is the latest production version in the 4.1 line\n         assertEquals(IBP_4_1_IV0, MetadataVersion.fromVersionString(\"4.1-IV0\"));\n+        assertEquals(IBP_4_1_IV1, MetadataVersion.fromVersionString(\"4.1-IV1\"));",
        "comment_created_at": "2025-07-14T18:31:42+00:00",
        "comment_author": "junrao",
        "comment_body": "Could we add the test for string \"4.1\" and \"4.0\"?",
        "pr_file_module": null
      },
      {
        "comment_id": "2205627022",
        "repo_full_name": "apache/kafka",
        "pr_number": 20137,
        "pr_file": "server-common/src/test/java/org/apache/kafka/server/common/MetadataVersionTest.java",
        "discussion_id": "2205560889",
        "commented_code": "@@ -83,7 +83,9 @@ public void testFromVersionString() {\n         assertEquals(IBP_4_0_IV2, MetadataVersion.fromVersionString(\"4.0-IV2\"));\n         assertEquals(IBP_4_0_IV3, MetadataVersion.fromVersionString(\"4.0-IV3\"));\n \n+        // 4.1-IV1 is the latest production version in the 4.1 line\n         assertEquals(IBP_4_1_IV0, MetadataVersion.fromVersionString(\"4.1-IV0\"));\n+        assertEquals(IBP_4_1_IV1, MetadataVersion.fromVersionString(\"4.1-IV1\"));",
        "comment_created_at": "2025-07-14T19:12:47+00:00",
        "comment_author": "ppatierno",
        "comment_body": "Agree",
        "pr_file_module": null
      },
      {
        "comment_id": "2208179841",
        "repo_full_name": "apache/kafka",
        "pr_number": 20137,
        "pr_file": "server-common/src/test/java/org/apache/kafka/server/common/MetadataVersionTest.java",
        "discussion_id": "2205560889",
        "commented_code": "@@ -83,7 +83,9 @@ public void testFromVersionString() {\n         assertEquals(IBP_4_0_IV2, MetadataVersion.fromVersionString(\"4.0-IV2\"));\n         assertEquals(IBP_4_0_IV3, MetadataVersion.fromVersionString(\"4.0-IV3\"));\n \n+        // 4.1-IV1 is the latest production version in the 4.1 line\n         assertEquals(IBP_4_1_IV0, MetadataVersion.fromVersionString(\"4.1-IV0\"));\n+        assertEquals(IBP_4_1_IV1, MetadataVersion.fromVersionString(\"4.1-IV1\"));",
        "comment_created_at": "2025-07-15T17:42:04+00:00",
        "comment_author": "CalvinConfluent",
        "comment_body": "Added.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2179954736",
    "pr_number": 20040,
    "pr_file": "coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntimeTest.java",
    "created_at": "2025-07-02T12:42:07+00:00",
    "commented_code": "assertFalse(write1.isCompletedExceptionally());\n \n         int batchSize = writer.entries(TP).get(0).sizeInBytes();\n-        assertTrue(batchSize > MIN_BUFFER_SIZE && batchSize < maxBatchSize);\n+        assertTrue(batchSize > INITIAL_BUFFER_SIZE && batchSize < maxBatchSize);\n     }\n \n+    @Test\n+    public void testCoordinatorDoNotRetainBufferLargeThanMaxMessageSize() {\n+        MockTimer timer = new MockTimer();\n+        InMemoryPartitionWriter mockWriter = new InMemoryPartitionWriter(false) {\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024) // 1MB\n+                ));\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        // Generate a record larger than the maxBatchSize.\n+        List<String> largeRecords = List.of(\"A\".repeat(100 * 1024 * 1024));\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(largeRecords, \"response1\", null, true, false)\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        // Verify that the next buffer retrieved from the bufferSupplier is the initial small one, not the large buffer.\n+        assertEquals(INITIAL_BUFFER_SIZE, ctx.bufferSupplier.get(1).capacity());\n+    }\n+\n+    @Test\n+    public void testCoordinatorRetainExpandedBufferLessOrEqualToMaxMessageSize() {\n+        MockTimer timer = new MockTimer();\n+        InMemoryPartitionWriter mockWriter = new InMemoryPartitionWriter(false) {\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024 * 1024) // 1GB\n+                ));\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        // Generate enough records to create a batch that has INITIAL_BUFFER_SIZE < batchSize < maxBatchSize\n+        List<String> records = new ArrayList<>();\n+        for (int i = 0; i < 1000000; i++) {\n+            records.add(\"record-\" + i);\n+        }\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response1\")\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        // Verify that the next buffer retrieved from the bufferSupplier is the expanded buffer.\n+        assertTrue(ctx.bufferSupplier.get(1).capacity() > INITIAL_BUFFER_SIZE);\n+    }\n+\n+    @Test\n+    public void testBufferShrinkWhenMaxMessageSizeReducedBelowInitialBufferSize() {\n+        MockTimer timer = new MockTimer();\n+        var mockWriter = new InMemoryPartitionWriter(false) {\n+            private LogConfig config;\n+\n+            {\n+                config = new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024) // 1MB\n+                ));\n+            }\n+\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return config;\n+            }\n+\n+            public void updateConfig(LogConfig newConfig) {\n+                this.config = newConfig;\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        List<String> records = new ArrayList<>();\n+        for (int i = 0; i < 1000; i++) {\n+            records.add(\"record-\" + i);\n+        }\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response1\")\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        ByteBuffer cachedBuffer = ctx.bufferSupplier.get(1);\n+        assertEquals(INITIAL_BUFFER_SIZE, cachedBuffer.capacity());\n+        ctx.bufferSupplier.release(cachedBuffer);\n+\n+        // Reduce max message size below initial buffer size.\n+        mockWriter.updateConfig(new LogConfig(\n+            Map.of(TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(INITIAL_BUFFER_SIZE - 66))));\n+\n+        // Write #2.\n+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation(\"write#2\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response2\")\n+        );\n+        assertFalse(write2.isCompletedExceptionally());\n+\n+        // Verify that there is no cached buffer.\n+        assertEquals(1, ctx.bufferSupplier.get(1).capacity());\n+\n+        // Write #3.",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2179954736",
        "repo_full_name": "apache/kafka",
        "pr_number": 20040,
        "pr_file": "coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntimeTest.java",
        "discussion_id": "2179954736",
        "commented_code": "@@ -2937,9 +2939,208 @@ public void testAppendRecordBatchSize() {\n         assertFalse(write1.isCompletedExceptionally());\n \n         int batchSize = writer.entries(TP).get(0).sizeInBytes();\n-        assertTrue(batchSize > MIN_BUFFER_SIZE && batchSize < maxBatchSize);\n+        assertTrue(batchSize > INITIAL_BUFFER_SIZE && batchSize < maxBatchSize);\n     }\n \n+    @Test\n+    public void testCoordinatorDoNotRetainBufferLargeThanMaxMessageSize() {\n+        MockTimer timer = new MockTimer();\n+        InMemoryPartitionWriter mockWriter = new InMemoryPartitionWriter(false) {\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024) // 1MB\n+                ));\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        // Generate a record larger than the maxBatchSize.\n+        List<String> largeRecords = List.of(\"A\".repeat(100 * 1024 * 1024));\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(largeRecords, \"response1\", null, true, false)\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        // Verify that the next buffer retrieved from the bufferSupplier is the initial small one, not the large buffer.\n+        assertEquals(INITIAL_BUFFER_SIZE, ctx.bufferSupplier.get(1).capacity());\n+    }\n+\n+    @Test\n+    public void testCoordinatorRetainExpandedBufferLessOrEqualToMaxMessageSize() {\n+        MockTimer timer = new MockTimer();\n+        InMemoryPartitionWriter mockWriter = new InMemoryPartitionWriter(false) {\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024 * 1024) // 1GB\n+                ));\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        // Generate enough records to create a batch that has INITIAL_BUFFER_SIZE < batchSize < maxBatchSize\n+        List<String> records = new ArrayList<>();\n+        for (int i = 0; i < 1000000; i++) {\n+            records.add(\"record-\" + i);\n+        }\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response1\")\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        // Verify that the next buffer retrieved from the bufferSupplier is the expanded buffer.\n+        assertTrue(ctx.bufferSupplier.get(1).capacity() > INITIAL_BUFFER_SIZE);\n+    }\n+\n+    @Test\n+    public void testBufferShrinkWhenMaxMessageSizeReducedBelowInitialBufferSize() {\n+        MockTimer timer = new MockTimer();\n+        var mockWriter = new InMemoryPartitionWriter(false) {\n+            private LogConfig config;\n+\n+            {\n+                config = new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024) // 1MB\n+                ));\n+            }\n+\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return config;\n+            }\n+\n+            public void updateConfig(LogConfig newConfig) {\n+                this.config = newConfig;\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        List<String> records = new ArrayList<>();\n+        for (int i = 0; i < 1000; i++) {\n+            records.add(\"record-\" + i);\n+        }\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response1\")\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        ByteBuffer cachedBuffer = ctx.bufferSupplier.get(1);\n+        assertEquals(INITIAL_BUFFER_SIZE, cachedBuffer.capacity());\n+        ctx.bufferSupplier.release(cachedBuffer);\n+\n+        // Reduce max message size below initial buffer size.\n+        mockWriter.updateConfig(new LogConfig(\n+            Map.of(TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(INITIAL_BUFFER_SIZE - 66))));\n+\n+        // Write #2.\n+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation(\"write#2\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response2\")\n+        );\n+        assertFalse(write2.isCompletedExceptionally());\n+\n+        // Verify that there is no cached buffer.\n+        assertEquals(1, ctx.bufferSupplier.get(1).capacity());\n+\n+        // Write #3.",
        "comment_created_at": "2025-07-02T12:42:07+00:00",
        "comment_author": "chia7712",
        "comment_body": "Could you please add unit test to ensure the maximum value of capacity of buffer is equal to `maxMessageSize`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2180892266",
        "repo_full_name": "apache/kafka",
        "pr_number": 20040,
        "pr_file": "coordinator-common/src/test/java/org/apache/kafka/coordinator/common/runtime/CoordinatorRuntimeTest.java",
        "discussion_id": "2179954736",
        "commented_code": "@@ -2937,9 +2939,208 @@ public void testAppendRecordBatchSize() {\n         assertFalse(write1.isCompletedExceptionally());\n \n         int batchSize = writer.entries(TP).get(0).sizeInBytes();\n-        assertTrue(batchSize > MIN_BUFFER_SIZE && batchSize < maxBatchSize);\n+        assertTrue(batchSize > INITIAL_BUFFER_SIZE && batchSize < maxBatchSize);\n     }\n \n+    @Test\n+    public void testCoordinatorDoNotRetainBufferLargeThanMaxMessageSize() {\n+        MockTimer timer = new MockTimer();\n+        InMemoryPartitionWriter mockWriter = new InMemoryPartitionWriter(false) {\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024) // 1MB\n+                ));\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        // Generate a record larger than the maxBatchSize.\n+        List<String> largeRecords = List.of(\"A\".repeat(100 * 1024 * 1024));\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(largeRecords, \"response1\", null, true, false)\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        // Verify that the next buffer retrieved from the bufferSupplier is the initial small one, not the large buffer.\n+        assertEquals(INITIAL_BUFFER_SIZE, ctx.bufferSupplier.get(1).capacity());\n+    }\n+\n+    @Test\n+    public void testCoordinatorRetainExpandedBufferLessOrEqualToMaxMessageSize() {\n+        MockTimer timer = new MockTimer();\n+        InMemoryPartitionWriter mockWriter = new InMemoryPartitionWriter(false) {\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024 * 1024) // 1GB\n+                ));\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        // Generate enough records to create a batch that has INITIAL_BUFFER_SIZE < batchSize < maxBatchSize\n+        List<String> records = new ArrayList<>();\n+        for (int i = 0; i < 1000000; i++) {\n+            records.add(\"record-\" + i);\n+        }\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response1\")\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        // Verify that the next buffer retrieved from the bufferSupplier is the expanded buffer.\n+        assertTrue(ctx.bufferSupplier.get(1).capacity() > INITIAL_BUFFER_SIZE);\n+    }\n+\n+    @Test\n+    public void testBufferShrinkWhenMaxMessageSizeReducedBelowInitialBufferSize() {\n+        MockTimer timer = new MockTimer();\n+        var mockWriter = new InMemoryPartitionWriter(false) {\n+            private LogConfig config;\n+\n+            {\n+                config = new LogConfig(Map.of(\n+                    TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(1024 * 1024) // 1MB\n+                ));\n+            }\n+\n+            @Override\n+            public LogConfig config(TopicPartition tp) {\n+                return config;\n+            }\n+\n+            public void updateConfig(LogConfig newConfig) {\n+                this.config = newConfig;\n+            }\n+        };\n+        StringSerializer serializer = new StringSerializer();\n+\n+        CoordinatorRuntime<MockCoordinatorShard, String> runtime =\n+            new CoordinatorRuntime.Builder<MockCoordinatorShard, String>()\n+                .withTime(timer.time())\n+                .withTimer(timer)\n+                .withDefaultWriteTimeOut(DEFAULT_WRITE_TIMEOUT)\n+                .withLoader(new MockCoordinatorLoader())\n+                .withEventProcessor(new DirectEventProcessor())\n+                .withPartitionWriter(mockWriter)\n+                .withCoordinatorShardBuilderSupplier(new MockCoordinatorShardBuilderSupplier())\n+                .withCoordinatorRuntimeMetrics(mock(CoordinatorRuntimeMetrics.class))\n+                .withCoordinatorMetrics(mock(CoordinatorMetrics.class))\n+                .withSerializer(serializer)\n+                .withExecutorService(mock(ExecutorService.class))\n+                .build();\n+\n+        // Schedule the loading.\n+        runtime.scheduleLoadOperation(TP, 10);\n+\n+        // Verify the initial state.\n+        CoordinatorRuntime<MockCoordinatorShard, String>.CoordinatorContext ctx = runtime.contextOrThrow(TP);\n+        assertEquals(0L, ctx.coordinator.lastWrittenOffset());\n+        assertEquals(0L, ctx.coordinator.lastCommittedOffset());\n+        assertEquals(List.of(0L), ctx.coordinator.snapshotRegistry().epochsList());\n+\n+        List<String> records = new ArrayList<>();\n+        for (int i = 0; i < 1000; i++) {\n+            records.add(\"record-\" + i);\n+        }\n+\n+        // Write #1.\n+        CompletableFuture<String> write1 = runtime.scheduleWriteOperation(\"write#1\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response1\")\n+        );\n+\n+        // Verify that the write has not completed exceptionally.\n+        // This will catch any exceptions thrown including RecordTooLargeException.\n+        assertFalse(write1.isCompletedExceptionally());\n+\n+        ByteBuffer cachedBuffer = ctx.bufferSupplier.get(1);\n+        assertEquals(INITIAL_BUFFER_SIZE, cachedBuffer.capacity());\n+        ctx.bufferSupplier.release(cachedBuffer);\n+\n+        // Reduce max message size below initial buffer size.\n+        mockWriter.updateConfig(new LogConfig(\n+            Map.of(TopicConfig.MAX_MESSAGE_BYTES_CONFIG, String.valueOf(INITIAL_BUFFER_SIZE - 66))));\n+\n+        // Write #2.\n+        CompletableFuture<String> write2 = runtime.scheduleWriteOperation(\"write#2\", TP, DEFAULT_WRITE_TIMEOUT,\n+            state -> new CoordinatorResult<>(records, \"response2\")\n+        );\n+        assertFalse(write2.isCompletedExceptionally());\n+\n+        // Verify that there is no cached buffer.\n+        assertEquals(1, ctx.bufferSupplier.get(1).capacity());\n+\n+        // Write #3.",
        "comment_created_at": "2025-07-02T20:12:01+00:00",
        "comment_author": "mingyen066",
        "comment_body": "I replaced the previous assertion with `assertEquals(mockWriter.config(TP).maxMessageSize(), ctx.bufferSupplier.get(1).capacity())` to ensure that",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2163390456",
    "pr_number": 20021,
    "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
    "created_at": "2025-06-24T09:10:21+00:00",
    "commented_code": "}\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+        metrics.setFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+        for (var finalizedFeatureEntry : image.features().finalizedVersions().entrySet()) {\n+            metrics.setFinalizedFeatureLevel(\n+                finalizedFeatureEntry.getKey(),\n+                finalizedFeatureEntry.getValue()\n+            );\n+        }",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2163390456",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2163390456",
        "commented_code": "@@ -345,7 +346,18 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+        metrics.setFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+        for (var finalizedFeatureEntry : image.features().finalizedVersions().entrySet()) {\n+            metrics.setFinalizedFeatureLevel(\n+                finalizedFeatureEntry.getKey(),\n+                finalizedFeatureEntry.getValue()\n+            );\n+        }",
        "comment_created_at": "2025-06-24T09:10:21+00:00",
        "comment_author": "FrankYang0529",
        "comment_body": "Could you add related test to MetadataLoaderTest as well? For example, there is assertion about `currentMetadataVersion`.\r\n\r\nhttps://github.com/apache/kafka/blob/1ca8779bee9cdbd53b0cb4536270461a6ed0b6d2/metadata/src/test/java/org/apache/kafka/image/loader/MetadataLoaderTest.java#L356-L357",
        "pr_file_module": null
      },
      {
        "comment_id": "2164212375",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2163390456",
        "commented_code": "@@ -345,7 +346,18 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+        metrics.setFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+        for (var finalizedFeatureEntry : image.features().finalizedVersions().entrySet()) {\n+            metrics.setFinalizedFeatureLevel(\n+                finalizedFeatureEntry.getKey(),\n+                finalizedFeatureEntry.getValue()\n+            );\n+        }",
        "comment_created_at": "2025-06-24T14:39:51+00:00",
        "comment_author": "kevin-wu24",
        "comment_body": "Yeah, for checks of current metadata version metric value, I will add a check for the finalized feature level metric too.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1960774315",
    "pr_number": 18610,
    "pr_file": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java",
    "created_at": "2025-02-19T00:26:34+00:00",
    "commented_code": "}\n     }\n \n-    private class RocksDBDualCFIterator extends AbstractIterator<KeyValue<Bytes, byte[]>>\n-        implements ManagedKeyValueIterator<Bytes, byte[]> {\n-\n-        // RocksDB's JNI interface does not expose getters/setters that allow the\n-        // comparator to be pluggable, and the default is lexicographic, so it's\n-        // safe to just force lexicographic comparator here for now.\n+    /**\n+     * A range-based iterator for RocksDB that merges results from two column families.\n+     *\n+     * <p>This iterator supports traversal over two RocksDB column families: one containing timestamped values and\n+     * another containing non-timestamped values. It ensures that the keys from both column families are merged and\n+     * sorted lexicographically, respecting the iteration order (forward or reverse) and the specified range\n+     * boundaries.</p>\n+     *\n+     * <h2>Key Features</h2>\n+     *\n+     * <ul>\n+     *     <li>Merges results from the \"with-timestamp\" and \"no-timestamp\" column families.</li>\n+     *     <li>Supports range-based queries with open or closed boundaries.</li>\n+     *     <li>Handles both forward and reverse iteration seamlessly.</li>\n+     *     <li>Ensures correct handling of inclusive and exclusive upper boundaries.</li>\n+     *     <li>Integrates efficiently with Kafka Streams state store mechanisms.</li>\n+     * </ul>\n+     *\n+     * <h2>Usage</h2>\n+     *\n+     * <p>The iterator can be used for different types of range-based operations, such as:\n+     * <ul>\n+     *     <li>Iterating over all keys within a range.</li>\n+     *     <li>Prefix-based scans (when combined with dynamically calculated range endpoints).</li>\n+     *     <li>Open-ended range queries (e.g., from a given key to the end of the dataset).</li>\n+     * </ul>\n+     * </p>\n+     *\n+     * <h2>Implementation Details</h2>\n+     *\n+     * <p>The class extends {@link AbstractIterator} and implements {@link ManagedKeyValueIterator}. It uses RocksDB's\n+     * native iterators for efficient traversal of keys within the specified range. Keys from the two column families\n+     * are merged during iteration, ensuring proper order and de-duplication where applicable.</p>\n+     *\n+     * <h3>Key Methods:</h3>\n+     *\n+     * <ul>\n+     *     <li><b>{@code makeNext()}:</b> Retrieves the next key-value pair in the merged range, ensuring\n+     *     the result is within the specified range and boundary conditions.</li>\n+     *     <li><b>{@code initializeIterators()}:</b> Initializes the RocksDB iterators based on the specified range and direction.</li>\n+     *     <li><b>{@code isInRange()}:</b> Verifies if the current key-value pair is within the range defined by {@code from} and {@code to}.</li>\n+     *     <li><b>{@code fetchNextKeyValue()}:</b> Determines the next key-value pair to return based on the state of both iterators.</li>\n+     * </ul>\n+     *\n+     * <h3>Thread Safety:</h3>\n+     *\n+     * <p>The iterator is thread-safe for sequential operations but should not be accessed concurrently from multiple\n+     * threads without external synchronization.</p>\n+     *\n+     * <h2>Examples</h2>\n+     *\n+     * <h3>Iterate over a range:</h3>\n+     *\n+     * <pre>{@code\n+     * RocksIterator noTimestampIterator = accessor.newIterator(noTimestampColumnFamily);\n+     * RocksIterator withTimestampIterator = accessor.newIterator(withTimestampColumnFamily);\n+     *\n+     * try (RocksDBDualCFRangeIterator iterator = new RocksDBDualCFRangeIterator(\n+     *         new Bytes(\"keyStart\".getBytes()),\n+     *         new Bytes(\"keyEnd\".getBytes()),\n+     *         noTimestampIterator,\n+     *         withTimestampIterator,\n+     *         \"storeName\",\n+     *         true,  // Forward iteration\n+     *         true   // Inclusive upper boundary\n+     * )) {\n+     *     while (iterator.hasNext()) {\n+     *         KeyValue<Bytes, byte[]> entry = iterator.next();\n+     *         System.out.println(\"Key: \" + entry.key + \", Value: \" + Arrays.toString(entry.value));\n+     *     }\n+     * }\n+     * }</pre>\n+     *\n+     * <h2>Exceptions</h2>\n+     *\n+     * <ul>\n+     *     <li><b>{@link InvalidStateStoreException}:</b> Thrown if the iterator is accessed after being closed.</li>\n+     *     <li><b>{@link IllegalStateException}:</b> Thrown if the close callback is not properly set before usage.</li>\n+     * </ul>\n+     *\n+     * @see AbstractIterator\n+     * @see ManagedKeyValueIterator\n+     * @see RocksDBStore\n+     */\n+    private static class RocksDBDualCFRangeIterator extends AbstractIterator<KeyValue<Bytes, byte[]>> implements ManagedKeyValueIterator<Bytes, byte[]> {\n+        private Runnable closeCallback;\n+        private byte[] noTimestampNext;\n+        private byte[] withTimestampNext;\n         private final Comparator<byte[]> comparator = Bytes.BYTES_LEXICO_COMPARATOR;\n-\n+        private final RocksIterator noTimestampIterator;\n+        private final RocksIterator withTimestampIterator;\n         private final String storeName;\n-        private final RocksIterator iterWithTimestamp;\n-        private final RocksIterator iterNoTimestamp;\n         private final boolean forward;\n-\n+        private final boolean toInclusive;\n+        private final byte[] rawLastKey;\n         private volatile boolean open = true;\n \n-        private byte[] nextWithTimestamp;\n-        private byte[] nextNoTimestamp;\n-        private KeyValue<Bytes, byte[]> next;\n-        private Runnable closeCallback = null;\n-\n-        RocksDBDualCFIterator(final String storeName,\n-                              final RocksIterator iterWithTimestamp,\n-                              final RocksIterator iterNoTimestamp,\n-                              final boolean forward) {\n-            this.iterWithTimestamp = iterWithTimestamp;\n-            this.iterNoTimestamp = iterNoTimestamp;\n-            this.storeName = storeName;\n+        /**\n+         * Constructs a new {@code RocksDBDualCFRangeIterator}.\n+         *\n+         * <p>Initializes the RocksDB iterators for two column families (timestamped and non-timestamped) and sets up\n+         * the range and direction for iteration.</p>\n+         *\n+         * @param from                  The starting key of the range. Can be {@code null} for an open range.\n+         * @param to                    The ending key of the range. Can be {@code null} for an open range.\n+         * @param noTimestampIterator   The iterator for the non-timestamped column family.\n+         * @param withTimestampIterator The iterator for the timestamped column family.\n+         * @param storeName             The name of the store associated with this iterator.\n+         * @param forward               {@code true} for forward iteration; {@code false} for reverse iteration.\n+         * @param toInclusive           Whether the upper boundary of the range is inclusive.\n+         */\n+        RocksDBDualCFRangeIterator(final Bytes from,\n+                                   final Bytes to,\n+                                   final RocksIterator noTimestampIterator,\n+                                   final RocksIterator withTimestampIterator,\n+                                   final String storeName,\n+                                   final boolean forward,\n+                                   final boolean toInclusive) {\n             this.forward = forward;\n+            this.noTimestampIterator = noTimestampIterator;\n+            this.storeName = storeName;\n+            this.toInclusive = toInclusive;\n+            this.withTimestampIterator = withTimestampIterator;\n+\n+            this.rawLastKey = initializeIterators(from, to);\n         }\n \n+        /**\n+         * Retrieves the next key-value pair in the range.\n+         *\n+         * <p>This method determines the next key-value pair to return by merging the results from the two column\n+         * families. If both column families have keys, it selects the one that matches the iteration order and range\n+         * conditions. Keys outside the specified range are skipped.</p>\n+         *\n+         * @return The next {@link KeyValue} pair in the range, or {@code null} if no more elements are available.\n+         */\n         @Override\n-        public synchronized boolean hasNext() {\n-            if (!open) {\n-                throw new InvalidStateStoreException(String.format(\"RocksDB iterator for store %s has closed\", storeName));\n-            }\n-            return super.hasNext();\n+        protected KeyValue<Bytes, byte[]> makeNext() {\n+            loadNextKeys();\n+            if (noTimestampNext == null && withTimestampNext == null) return allDone();\n+            final KeyValue<Bytes, byte[]> next = fetchNextKeyValue();\n+            return isInRange(next) ? next : allDone();\n+        }\n+\n+        /**\n+         * Returns the next key in the range without advancing the iterator.\n+         *\n+         * <p>This method retrieves the key of the next {@link KeyValue} pair that would be returned by {@link #next()},\n+         * without moving the iterator forward. This is useful for inspecting the next key without affecting the\n+         * iterator's state.</p>\n+         *\n+         * @return The next key as a {@link Bytes} object.\n+         *\n+         * @throws NoSuchElementException If there are no more elements in the iterator.\n+         */\n+        @Override\n+        public Bytes peekNextKey() {",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "1960774315",
        "repo_full_name": "apache/kafka",
        "pr_number": 18610,
        "pr_file": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java",
        "discussion_id": "1960774315",
        "commented_code": "@@ -277,197 +275,361 @@ public void close() {\n         }\n     }\n \n-    private class RocksDBDualCFIterator extends AbstractIterator<KeyValue<Bytes, byte[]>>\n-        implements ManagedKeyValueIterator<Bytes, byte[]> {\n-\n-        // RocksDB's JNI interface does not expose getters/setters that allow the\n-        // comparator to be pluggable, and the default is lexicographic, so it's\n-        // safe to just force lexicographic comparator here for now.\n+    /**\n+     * A range-based iterator for RocksDB that merges results from two column families.\n+     *\n+     * <p>This iterator supports traversal over two RocksDB column families: one containing timestamped values and\n+     * another containing non-timestamped values. It ensures that the keys from both column families are merged and\n+     * sorted lexicographically, respecting the iteration order (forward or reverse) and the specified range\n+     * boundaries.</p>\n+     *\n+     * <h2>Key Features</h2>\n+     *\n+     * <ul>\n+     *     <li>Merges results from the \"with-timestamp\" and \"no-timestamp\" column families.</li>\n+     *     <li>Supports range-based queries with open or closed boundaries.</li>\n+     *     <li>Handles both forward and reverse iteration seamlessly.</li>\n+     *     <li>Ensures correct handling of inclusive and exclusive upper boundaries.</li>\n+     *     <li>Integrates efficiently with Kafka Streams state store mechanisms.</li>\n+     * </ul>\n+     *\n+     * <h2>Usage</h2>\n+     *\n+     * <p>The iterator can be used for different types of range-based operations, such as:\n+     * <ul>\n+     *     <li>Iterating over all keys within a range.</li>\n+     *     <li>Prefix-based scans (when combined with dynamically calculated range endpoints).</li>\n+     *     <li>Open-ended range queries (e.g., from a given key to the end of the dataset).</li>\n+     * </ul>\n+     * </p>\n+     *\n+     * <h2>Implementation Details</h2>\n+     *\n+     * <p>The class extends {@link AbstractIterator} and implements {@link ManagedKeyValueIterator}. It uses RocksDB's\n+     * native iterators for efficient traversal of keys within the specified range. Keys from the two column families\n+     * are merged during iteration, ensuring proper order and de-duplication where applicable.</p>\n+     *\n+     * <h3>Key Methods:</h3>\n+     *\n+     * <ul>\n+     *     <li><b>{@code makeNext()}:</b> Retrieves the next key-value pair in the merged range, ensuring\n+     *     the result is within the specified range and boundary conditions.</li>\n+     *     <li><b>{@code initializeIterators()}:</b> Initializes the RocksDB iterators based on the specified range and direction.</li>\n+     *     <li><b>{@code isInRange()}:</b> Verifies if the current key-value pair is within the range defined by {@code from} and {@code to}.</li>\n+     *     <li><b>{@code fetchNextKeyValue()}:</b> Determines the next key-value pair to return based on the state of both iterators.</li>\n+     * </ul>\n+     *\n+     * <h3>Thread Safety:</h3>\n+     *\n+     * <p>The iterator is thread-safe for sequential operations but should not be accessed concurrently from multiple\n+     * threads without external synchronization.</p>\n+     *\n+     * <h2>Examples</h2>\n+     *\n+     * <h3>Iterate over a range:</h3>\n+     *\n+     * <pre>{@code\n+     * RocksIterator noTimestampIterator = accessor.newIterator(noTimestampColumnFamily);\n+     * RocksIterator withTimestampIterator = accessor.newIterator(withTimestampColumnFamily);\n+     *\n+     * try (RocksDBDualCFRangeIterator iterator = new RocksDBDualCFRangeIterator(\n+     *         new Bytes(\"keyStart\".getBytes()),\n+     *         new Bytes(\"keyEnd\".getBytes()),\n+     *         noTimestampIterator,\n+     *         withTimestampIterator,\n+     *         \"storeName\",\n+     *         true,  // Forward iteration\n+     *         true   // Inclusive upper boundary\n+     * )) {\n+     *     while (iterator.hasNext()) {\n+     *         KeyValue<Bytes, byte[]> entry = iterator.next();\n+     *         System.out.println(\"Key: \" + entry.key + \", Value: \" + Arrays.toString(entry.value));\n+     *     }\n+     * }\n+     * }</pre>\n+     *\n+     * <h2>Exceptions</h2>\n+     *\n+     * <ul>\n+     *     <li><b>{@link InvalidStateStoreException}:</b> Thrown if the iterator is accessed after being closed.</li>\n+     *     <li><b>{@link IllegalStateException}:</b> Thrown if the close callback is not properly set before usage.</li>\n+     * </ul>\n+     *\n+     * @see AbstractIterator\n+     * @see ManagedKeyValueIterator\n+     * @see RocksDBStore\n+     */\n+    private static class RocksDBDualCFRangeIterator extends AbstractIterator<KeyValue<Bytes, byte[]>> implements ManagedKeyValueIterator<Bytes, byte[]> {\n+        private Runnable closeCallback;\n+        private byte[] noTimestampNext;\n+        private byte[] withTimestampNext;\n         private final Comparator<byte[]> comparator = Bytes.BYTES_LEXICO_COMPARATOR;\n-\n+        private final RocksIterator noTimestampIterator;\n+        private final RocksIterator withTimestampIterator;\n         private final String storeName;\n-        private final RocksIterator iterWithTimestamp;\n-        private final RocksIterator iterNoTimestamp;\n         private final boolean forward;\n-\n+        private final boolean toInclusive;\n+        private final byte[] rawLastKey;\n         private volatile boolean open = true;\n \n-        private byte[] nextWithTimestamp;\n-        private byte[] nextNoTimestamp;\n-        private KeyValue<Bytes, byte[]> next;\n-        private Runnable closeCallback = null;\n-\n-        RocksDBDualCFIterator(final String storeName,\n-                              final RocksIterator iterWithTimestamp,\n-                              final RocksIterator iterNoTimestamp,\n-                              final boolean forward) {\n-            this.iterWithTimestamp = iterWithTimestamp;\n-            this.iterNoTimestamp = iterNoTimestamp;\n-            this.storeName = storeName;\n+        /**\n+         * Constructs a new {@code RocksDBDualCFRangeIterator}.\n+         *\n+         * <p>Initializes the RocksDB iterators for two column families (timestamped and non-timestamped) and sets up\n+         * the range and direction for iteration.</p>\n+         *\n+         * @param from                  The starting key of the range. Can be {@code null} for an open range.\n+         * @param to                    The ending key of the range. Can be {@code null} for an open range.\n+         * @param noTimestampIterator   The iterator for the non-timestamped column family.\n+         * @param withTimestampIterator The iterator for the timestamped column family.\n+         * @param storeName             The name of the store associated with this iterator.\n+         * @param forward               {@code true} for forward iteration; {@code false} for reverse iteration.\n+         * @param toInclusive           Whether the upper boundary of the range is inclusive.\n+         */\n+        RocksDBDualCFRangeIterator(final Bytes from,\n+                                   final Bytes to,\n+                                   final RocksIterator noTimestampIterator,\n+                                   final RocksIterator withTimestampIterator,\n+                                   final String storeName,\n+                                   final boolean forward,\n+                                   final boolean toInclusive) {\n             this.forward = forward;\n+            this.noTimestampIterator = noTimestampIterator;\n+            this.storeName = storeName;\n+            this.toInclusive = toInclusive;\n+            this.withTimestampIterator = withTimestampIterator;\n+\n+            this.rawLastKey = initializeIterators(from, to);\n         }\n \n+        /**\n+         * Retrieves the next key-value pair in the range.\n+         *\n+         * <p>This method determines the next key-value pair to return by merging the results from the two column\n+         * families. If both column families have keys, it selects the one that matches the iteration order and range\n+         * conditions. Keys outside the specified range are skipped.</p>\n+         *\n+         * @return The next {@link KeyValue} pair in the range, or {@code null} if no more elements are available.\n+         */\n         @Override\n-        public synchronized boolean hasNext() {\n-            if (!open) {\n-                throw new InvalidStateStoreException(String.format(\"RocksDB iterator for store %s has closed\", storeName));\n-            }\n-            return super.hasNext();\n+        protected KeyValue<Bytes, byte[]> makeNext() {\n+            loadNextKeys();\n+            if (noTimestampNext == null && withTimestampNext == null) return allDone();\n+            final KeyValue<Bytes, byte[]> next = fetchNextKeyValue();\n+            return isInRange(next) ? next : allDone();\n+        }\n+\n+        /**\n+         * Returns the next key in the range without advancing the iterator.\n+         *\n+         * <p>This method retrieves the key of the next {@link KeyValue} pair that would be returned by {@link #next()},\n+         * without moving the iterator forward. This is useful for inspecting the next key without affecting the\n+         * iterator's state.</p>\n+         *\n+         * @return The next key as a {@link Bytes} object.\n+         *\n+         * @throws NoSuchElementException If there are no more elements in the iterator.\n+         */\n+        @Override\n+        public Bytes peekNextKey() {",
        "comment_created_at": "2025-02-19T00:26:34+00:00",
        "comment_author": "agavra",
        "comment_body": "existing test coverage for this class is below average (e.g. there's not `peekNextKey` in the test) - could you add that? \r\n\r\ncc @ableegoldman it may be good to refactor the existing tests alongside this one so that instead of one really big test we had individual tests for each piece of functionality so it's easier to confirm the test coverage is good",
        "pr_file_module": null
      },
      {
        "comment_id": "2017793165",
        "repo_full_name": "apache/kafka",
        "pr_number": 18610,
        "pr_file": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java",
        "discussion_id": "1960774315",
        "commented_code": "@@ -277,197 +275,361 @@ public void close() {\n         }\n     }\n \n-    private class RocksDBDualCFIterator extends AbstractIterator<KeyValue<Bytes, byte[]>>\n-        implements ManagedKeyValueIterator<Bytes, byte[]> {\n-\n-        // RocksDB's JNI interface does not expose getters/setters that allow the\n-        // comparator to be pluggable, and the default is lexicographic, so it's\n-        // safe to just force lexicographic comparator here for now.\n+    /**\n+     * A range-based iterator for RocksDB that merges results from two column families.\n+     *\n+     * <p>This iterator supports traversal over two RocksDB column families: one containing timestamped values and\n+     * another containing non-timestamped values. It ensures that the keys from both column families are merged and\n+     * sorted lexicographically, respecting the iteration order (forward or reverse) and the specified range\n+     * boundaries.</p>\n+     *\n+     * <h2>Key Features</h2>\n+     *\n+     * <ul>\n+     *     <li>Merges results from the \"with-timestamp\" and \"no-timestamp\" column families.</li>\n+     *     <li>Supports range-based queries with open or closed boundaries.</li>\n+     *     <li>Handles both forward and reverse iteration seamlessly.</li>\n+     *     <li>Ensures correct handling of inclusive and exclusive upper boundaries.</li>\n+     *     <li>Integrates efficiently with Kafka Streams state store mechanisms.</li>\n+     * </ul>\n+     *\n+     * <h2>Usage</h2>\n+     *\n+     * <p>The iterator can be used for different types of range-based operations, such as:\n+     * <ul>\n+     *     <li>Iterating over all keys within a range.</li>\n+     *     <li>Prefix-based scans (when combined with dynamically calculated range endpoints).</li>\n+     *     <li>Open-ended range queries (e.g., from a given key to the end of the dataset).</li>\n+     * </ul>\n+     * </p>\n+     *\n+     * <h2>Implementation Details</h2>\n+     *\n+     * <p>The class extends {@link AbstractIterator} and implements {@link ManagedKeyValueIterator}. It uses RocksDB's\n+     * native iterators for efficient traversal of keys within the specified range. Keys from the two column families\n+     * are merged during iteration, ensuring proper order and de-duplication where applicable.</p>\n+     *\n+     * <h3>Key Methods:</h3>\n+     *\n+     * <ul>\n+     *     <li><b>{@code makeNext()}:</b> Retrieves the next key-value pair in the merged range, ensuring\n+     *     the result is within the specified range and boundary conditions.</li>\n+     *     <li><b>{@code initializeIterators()}:</b> Initializes the RocksDB iterators based on the specified range and direction.</li>\n+     *     <li><b>{@code isInRange()}:</b> Verifies if the current key-value pair is within the range defined by {@code from} and {@code to}.</li>\n+     *     <li><b>{@code fetchNextKeyValue()}:</b> Determines the next key-value pair to return based on the state of both iterators.</li>\n+     * </ul>\n+     *\n+     * <h3>Thread Safety:</h3>\n+     *\n+     * <p>The iterator is thread-safe for sequential operations but should not be accessed concurrently from multiple\n+     * threads without external synchronization.</p>\n+     *\n+     * <h2>Examples</h2>\n+     *\n+     * <h3>Iterate over a range:</h3>\n+     *\n+     * <pre>{@code\n+     * RocksIterator noTimestampIterator = accessor.newIterator(noTimestampColumnFamily);\n+     * RocksIterator withTimestampIterator = accessor.newIterator(withTimestampColumnFamily);\n+     *\n+     * try (RocksDBDualCFRangeIterator iterator = new RocksDBDualCFRangeIterator(\n+     *         new Bytes(\"keyStart\".getBytes()),\n+     *         new Bytes(\"keyEnd\".getBytes()),\n+     *         noTimestampIterator,\n+     *         withTimestampIterator,\n+     *         \"storeName\",\n+     *         true,  // Forward iteration\n+     *         true   // Inclusive upper boundary\n+     * )) {\n+     *     while (iterator.hasNext()) {\n+     *         KeyValue<Bytes, byte[]> entry = iterator.next();\n+     *         System.out.println(\"Key: \" + entry.key + \", Value: \" + Arrays.toString(entry.value));\n+     *     }\n+     * }\n+     * }</pre>\n+     *\n+     * <h2>Exceptions</h2>\n+     *\n+     * <ul>\n+     *     <li><b>{@link InvalidStateStoreException}:</b> Thrown if the iterator is accessed after being closed.</li>\n+     *     <li><b>{@link IllegalStateException}:</b> Thrown if the close callback is not properly set before usage.</li>\n+     * </ul>\n+     *\n+     * @see AbstractIterator\n+     * @see ManagedKeyValueIterator\n+     * @see RocksDBStore\n+     */\n+    private static class RocksDBDualCFRangeIterator extends AbstractIterator<KeyValue<Bytes, byte[]>> implements ManagedKeyValueIterator<Bytes, byte[]> {\n+        private Runnable closeCallback;\n+        private byte[] noTimestampNext;\n+        private byte[] withTimestampNext;\n         private final Comparator<byte[]> comparator = Bytes.BYTES_LEXICO_COMPARATOR;\n-\n+        private final RocksIterator noTimestampIterator;\n+        private final RocksIterator withTimestampIterator;\n         private final String storeName;\n-        private final RocksIterator iterWithTimestamp;\n-        private final RocksIterator iterNoTimestamp;\n         private final boolean forward;\n-\n+        private final boolean toInclusive;\n+        private final byte[] rawLastKey;\n         private volatile boolean open = true;\n \n-        private byte[] nextWithTimestamp;\n-        private byte[] nextNoTimestamp;\n-        private KeyValue<Bytes, byte[]> next;\n-        private Runnable closeCallback = null;\n-\n-        RocksDBDualCFIterator(final String storeName,\n-                              final RocksIterator iterWithTimestamp,\n-                              final RocksIterator iterNoTimestamp,\n-                              final boolean forward) {\n-            this.iterWithTimestamp = iterWithTimestamp;\n-            this.iterNoTimestamp = iterNoTimestamp;\n-            this.storeName = storeName;\n+        /**\n+         * Constructs a new {@code RocksDBDualCFRangeIterator}.\n+         *\n+         * <p>Initializes the RocksDB iterators for two column families (timestamped and non-timestamped) and sets up\n+         * the range and direction for iteration.</p>\n+         *\n+         * @param from                  The starting key of the range. Can be {@code null} for an open range.\n+         * @param to                    The ending key of the range. Can be {@code null} for an open range.\n+         * @param noTimestampIterator   The iterator for the non-timestamped column family.\n+         * @param withTimestampIterator The iterator for the timestamped column family.\n+         * @param storeName             The name of the store associated with this iterator.\n+         * @param forward               {@code true} for forward iteration; {@code false} for reverse iteration.\n+         * @param toInclusive           Whether the upper boundary of the range is inclusive.\n+         */\n+        RocksDBDualCFRangeIterator(final Bytes from,\n+                                   final Bytes to,\n+                                   final RocksIterator noTimestampIterator,\n+                                   final RocksIterator withTimestampIterator,\n+                                   final String storeName,\n+                                   final boolean forward,\n+                                   final boolean toInclusive) {\n             this.forward = forward;\n+            this.noTimestampIterator = noTimestampIterator;\n+            this.storeName = storeName;\n+            this.toInclusive = toInclusive;\n+            this.withTimestampIterator = withTimestampIterator;\n+\n+            this.rawLastKey = initializeIterators(from, to);\n         }\n \n+        /**\n+         * Retrieves the next key-value pair in the range.\n+         *\n+         * <p>This method determines the next key-value pair to return by merging the results from the two column\n+         * families. If both column families have keys, it selects the one that matches the iteration order and range\n+         * conditions. Keys outside the specified range are skipped.</p>\n+         *\n+         * @return The next {@link KeyValue} pair in the range, or {@code null} if no more elements are available.\n+         */\n         @Override\n-        public synchronized boolean hasNext() {\n-            if (!open) {\n-                throw new InvalidStateStoreException(String.format(\"RocksDB iterator for store %s has closed\", storeName));\n-            }\n-            return super.hasNext();\n+        protected KeyValue<Bytes, byte[]> makeNext() {\n+            loadNextKeys();\n+            if (noTimestampNext == null && withTimestampNext == null) return allDone();\n+            final KeyValue<Bytes, byte[]> next = fetchNextKeyValue();\n+            return isInRange(next) ? next : allDone();\n+        }\n+\n+        /**\n+         * Returns the next key in the range without advancing the iterator.\n+         *\n+         * <p>This method retrieves the key of the next {@link KeyValue} pair that would be returned by {@link #next()},\n+         * without moving the iterator forward. This is useful for inspecting the next key without affecting the\n+         * iterator's state.</p>\n+         *\n+         * @return The next key as a {@link Bytes} object.\n+         *\n+         * @throws NoSuchElementException If there are no more elements in the iterator.\n+         */\n+        @Override\n+        public Bytes peekNextKey() {",
        "comment_created_at": "2025-03-28T01:38:00+00:00",
        "comment_author": "fonsdant",
        "comment_body": "In progress.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2174236689",
    "pr_number": 20055,
    "pr_file": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
    "created_at": "2025-06-30T05:15:59+00:00",
    "commented_code": "),\n             task.result.records()\n         );\n+\n+        assignor.prepareGroupAssignment(new GroupAssignment(Map.of(\n+            memberId1, new MemberAssignmentImpl(mkAssignment(\n+                mkTopicAssignment(fooTopicId, 0, 1, 2, 3, 4, 5),\n+                mkTopicAssignment(barTopicId, 0, 1, 2)\n+            ))\n+        )));\n+\n+        // Member heartbeats again with the same regex.\n+        CoordinatorResult<ConsumerGroupHeartbeatResponseData, CoordinatorRecord> result2 = context.consumerGroupHeartbeat(\n+            new ConsumerGroupHeartbeatRequestData()\n+                .setGroupId(groupId)\n+                .setMemberId(memberId1)\n+                .setMemberEpoch(10)\n+                .setRebalanceTimeoutMs(5000)\n+                .setSubscribedTopicRegex(\"foo*|bar*\")\n+                .setServerAssignor(\"range\")\n+                .setTopicPartitions(List.of()));\n+\n+        assertResponseEquals(\n+            new ConsumerGroupHeartbeatResponseData()\n+                .setMemberId(memberId1)\n+                .setMemberEpoch(11)\n+                .setHeartbeatIntervalMs(5000)\n+                .setAssignment(new ConsumerGroupHeartbeatResponseData.Assignment()\n+                    .setTopicPartitions(List.of(\n+                        new ConsumerGroupHeartbeatResponseData.TopicPartitions()\n+                            .setTopicId(fooTopicId)\n+                            .setPartitions(List.of(0, 1, 2, 3, 4, 5)),\n+                        new ConsumerGroupHeartbeatResponseData.TopicPartitions()\n+                            .setTopicId(barTopicId)\n+                            .setPartitions(List.of(0, 1, 2))))),\n+            result2.response()\n+        );\n+\n+        ConsumerGroupMember expectedMember2 = new ConsumerGroupMember.Builder(memberId1)\n+            .setState(MemberState.STABLE)\n+            .setMemberEpoch(11)\n+            .setPreviousMemberEpoch(10)\n+            .setClientId(DEFAULT_CLIENT_ID)\n+            .setClientHost(DEFAULT_CLIENT_ADDRESS.toString())\n+            .setRebalanceTimeoutMs(5000)\n+            .setSubscribedTopicRegex(\"foo*|bar*\")\n+            .setServerAssignorName(\"range\")\n+            .setAssignedPartitions(mkAssignment(\n+                mkTopicAssignment(fooTopicId, 0, 1, 2, 3, 4, 5),\n+                mkTopicAssignment(barTopicId, 0, 1, 2)))\n+            .build();\n+\n+        expectedRecords = List.of(\n+            GroupCoordinatorRecordHelpers.newConsumerGroupTargetAssignmentRecord(groupId, memberId1, mkAssignment(\n+                mkTopicAssignment(fooTopicId, 0, 1, 2, 3, 4, 5),\n+                mkTopicAssignment(barTopicId, 0, 1, 2)\n+            )),\n+            GroupCoordinatorRecordHelpers.newConsumerGroupTargetAssignmentEpochRecord(groupId, 11),\n+            GroupCoordinatorRecordHelpers.newConsumerGroupCurrentAssignmentRecord(groupId, expectedMember2)\n+        );\n+\n+        assertRecordsEquals(expectedRecords, result2.records());",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2174236689",
        "repo_full_name": "apache/kafka",
        "pr_number": 20055,
        "pr_file": "group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java",
        "discussion_id": "2174236689",
        "commented_code": "@@ -20437,6 +20435,65 @@ barTopicName, computeTopicHash(barTopicName, metadataImage)\n             ),\n             task.result.records()\n         );\n+\n+        assignor.prepareGroupAssignment(new GroupAssignment(Map.of(\n+            memberId1, new MemberAssignmentImpl(mkAssignment(\n+                mkTopicAssignment(fooTopicId, 0, 1, 2, 3, 4, 5),\n+                mkTopicAssignment(barTopicId, 0, 1, 2)\n+            ))\n+        )));\n+\n+        // Member heartbeats again with the same regex.\n+        CoordinatorResult<ConsumerGroupHeartbeatResponseData, CoordinatorRecord> result2 = context.consumerGroupHeartbeat(\n+            new ConsumerGroupHeartbeatRequestData()\n+                .setGroupId(groupId)\n+                .setMemberId(memberId1)\n+                .setMemberEpoch(10)\n+                .setRebalanceTimeoutMs(5000)\n+                .setSubscribedTopicRegex(\"foo*|bar*\")\n+                .setServerAssignor(\"range\")\n+                .setTopicPartitions(List.of()));\n+\n+        assertResponseEquals(\n+            new ConsumerGroupHeartbeatResponseData()\n+                .setMemberId(memberId1)\n+                .setMemberEpoch(11)\n+                .setHeartbeatIntervalMs(5000)\n+                .setAssignment(new ConsumerGroupHeartbeatResponseData.Assignment()\n+                    .setTopicPartitions(List.of(\n+                        new ConsumerGroupHeartbeatResponseData.TopicPartitions()\n+                            .setTopicId(fooTopicId)\n+                            .setPartitions(List.of(0, 1, 2, 3, 4, 5)),\n+                        new ConsumerGroupHeartbeatResponseData.TopicPartitions()\n+                            .setTopicId(barTopicId)\n+                            .setPartitions(List.of(0, 1, 2))))),\n+            result2.response()\n+        );\n+\n+        ConsumerGroupMember expectedMember2 = new ConsumerGroupMember.Builder(memberId1)\n+            .setState(MemberState.STABLE)\n+            .setMemberEpoch(11)\n+            .setPreviousMemberEpoch(10)\n+            .setClientId(DEFAULT_CLIENT_ID)\n+            .setClientHost(DEFAULT_CLIENT_ADDRESS.toString())\n+            .setRebalanceTimeoutMs(5000)\n+            .setSubscribedTopicRegex(\"foo*|bar*\")\n+            .setServerAssignorName(\"range\")\n+            .setAssignedPartitions(mkAssignment(\n+                mkTopicAssignment(fooTopicId, 0, 1, 2, 3, 4, 5),\n+                mkTopicAssignment(barTopicId, 0, 1, 2)))\n+            .build();\n+\n+        expectedRecords = List.of(\n+            GroupCoordinatorRecordHelpers.newConsumerGroupTargetAssignmentRecord(groupId, memberId1, mkAssignment(\n+                mkTopicAssignment(fooTopicId, 0, 1, 2, 3, 4, 5),\n+                mkTopicAssignment(barTopicId, 0, 1, 2)\n+            )),\n+            GroupCoordinatorRecordHelpers.newConsumerGroupTargetAssignmentEpochRecord(groupId, 11),\n+            GroupCoordinatorRecordHelpers.newConsumerGroupCurrentAssignmentRecord(groupId, expectedMember2)\n+        );\n+\n+        assertRecordsEquals(expectedRecords, result2.records());",
        "comment_created_at": "2025-06-30T05:15:59+00:00",
        "comment_author": "squah-confluent",
        "comment_body": "I added testing for completion of the regex update, from the next assignment computation to the end of the heartbeat with the updated assignment.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1912156272",
    "pr_number": 17870,
    "pr_file": "core/src/test/java/kafka/server/share/DelayedShareFetchTest.java",
    "created_at": "2025-01-11T18:45:23+00:00",
    "commented_code": ".withSharePartitions(sharePartitions)\n             .build();\n \n-        LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData = new LinkedHashMap<>();\n-        topicPartitionData.put(tp0, mock(FetchRequest.PartitionData.class));\n-        topicPartitionData.put(tp1, mock(FetchRequest.PartitionData.class));\n+        LinkedHashMap<TopicIdPartition, Long> topicPartitionData = new LinkedHashMap<>();\n+        topicPartitionData.put(tp0, 0L);\n+        topicPartitionData.put(tp1, 0L);\n \n         // Case 1 - logReadResponse contains tp0.\n         LinkedHashMap<TopicIdPartition, LogReadResult> logReadResponse = new LinkedHashMap<>();\n-        logReadResponse.put(tp0, mock(LogReadResult.class));\n+        LogReadResult logReadResult = mock(LogReadResult.class);\n+        Records records = mock(Records.class);\n+        when(records.sizeInBytes()).thenReturn(2);\n+        FetchDataInfo fetchDataInfo = new FetchDataInfo(mock(LogOffsetMetadata.class), records);\n+        when(logReadResult.info()).thenReturn(fetchDataInfo);\n+        logReadResponse.put(tp0, logReadResult);",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "1912156272",
        "repo_full_name": "apache/kafka",
        "pr_number": 17870,
        "pr_file": "core/src/test/java/kafka/server/share/DelayedShareFetchTest.java",
        "discussion_id": "1912156272",
        "commented_code": "@@ -559,13 +561,18 @@ public void testCombineLogReadResponse() {\n             .withSharePartitions(sharePartitions)\n             .build();\n \n-        LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData = new LinkedHashMap<>();\n-        topicPartitionData.put(tp0, mock(FetchRequest.PartitionData.class));\n-        topicPartitionData.put(tp1, mock(FetchRequest.PartitionData.class));\n+        LinkedHashMap<TopicIdPartition, Long> topicPartitionData = new LinkedHashMap<>();\n+        topicPartitionData.put(tp0, 0L);\n+        topicPartitionData.put(tp1, 0L);\n \n         // Case 1 - logReadResponse contains tp0.\n         LinkedHashMap<TopicIdPartition, LogReadResult> logReadResponse = new LinkedHashMap<>();\n-        logReadResponse.put(tp0, mock(LogReadResult.class));\n+        LogReadResult logReadResult = mock(LogReadResult.class);\n+        Records records = mock(Records.class);\n+        when(records.sizeInBytes()).thenReturn(2);\n+        FetchDataInfo fetchDataInfo = new FetchDataInfo(mock(LogOffsetMetadata.class), records);\n+        when(logReadResult.info()).thenReturn(fetchDataInfo);\n+        logReadResponse.put(tp0, logReadResult);",
        "comment_created_at": "2025-01-11T18:45:23+00:00",
        "comment_author": "apoorvmittal10",
        "comment_body": "Should we add a test when partitionMaxBytesStrategy.maxBytes throws an exception?\r\n\r\nAlso do we have a test which validates fetch bytes are in accordance with the share fetch max bytes?  ",
        "pr_file_module": null
      },
      {
        "comment_id": "1912169880",
        "repo_full_name": "apache/kafka",
        "pr_number": 17870,
        "pr_file": "core/src/test/java/kafka/server/share/DelayedShareFetchTest.java",
        "discussion_id": "1912156272",
        "commented_code": "@@ -559,13 +561,18 @@ public void testCombineLogReadResponse() {\n             .withSharePartitions(sharePartitions)\n             .build();\n \n-        LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData = new LinkedHashMap<>();\n-        topicPartitionData.put(tp0, mock(FetchRequest.PartitionData.class));\n-        topicPartitionData.put(tp1, mock(FetchRequest.PartitionData.class));\n+        LinkedHashMap<TopicIdPartition, Long> topicPartitionData = new LinkedHashMap<>();\n+        topicPartitionData.put(tp0, 0L);\n+        topicPartitionData.put(tp1, 0L);\n \n         // Case 1 - logReadResponse contains tp0.\n         LinkedHashMap<TopicIdPartition, LogReadResult> logReadResponse = new LinkedHashMap<>();\n-        logReadResponse.put(tp0, mock(LogReadResult.class));\n+        LogReadResult logReadResult = mock(LogReadResult.class);\n+        Records records = mock(Records.class);\n+        when(records.sizeInBytes()).thenReturn(2);\n+        FetchDataInfo fetchDataInfo = new FetchDataInfo(mock(LogOffsetMetadata.class), records);\n+        when(logReadResult.info()).thenReturn(fetchDataInfo);\n+        logReadResponse.put(tp0, logReadResult);",
        "comment_created_at": "2025-01-11T19:07:56+00:00",
        "comment_author": "adixitconfluent",
        "comment_body": "> Should we add a test when partitionMaxBytesStrategy.maxBytes throws an exception? \r\n\r\n@apoorvmittal10, since I have already created this test [testCheckValidArguments](https://github.com/apache/kafka/pull/17870/files#diff-eef861ca89cf8d6840cf0f84919915c4c18da2e327e6587552c42f7a867d83a9R45), I don't think we need it separately in DelayedShareFetch. I think it will be redundant.\r\n\r\n> Also do we have a test which validates fetch bytes are in accordance with the share fetch max bytes?\r\n\r\nYes, we have a test [testShareFetchRequestSuccessfulSharingBetweenMultipleConsumers](https://github.com/apache/kafka/pull/17870/files#diff-784cd94373b734f49edc71a0b70d4f2d6d11dbf8b345746db7340b9a5f4fedbdR1386) in `ShareFetchAcknowledgeRequestTest.scala`",
        "pr_file_module": null
      },
      {
        "comment_id": "1912538632",
        "repo_full_name": "apache/kafka",
        "pr_number": 17870,
        "pr_file": "core/src/test/java/kafka/server/share/DelayedShareFetchTest.java",
        "discussion_id": "1912156272",
        "commented_code": "@@ -559,13 +561,18 @@ public void testCombineLogReadResponse() {\n             .withSharePartitions(sharePartitions)\n             .build();\n \n-        LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData = new LinkedHashMap<>();\n-        topicPartitionData.put(tp0, mock(FetchRequest.PartitionData.class));\n-        topicPartitionData.put(tp1, mock(FetchRequest.PartitionData.class));\n+        LinkedHashMap<TopicIdPartition, Long> topicPartitionData = new LinkedHashMap<>();\n+        topicPartitionData.put(tp0, 0L);\n+        topicPartitionData.put(tp1, 0L);\n \n         // Case 1 - logReadResponse contains tp0.\n         LinkedHashMap<TopicIdPartition, LogReadResult> logReadResponse = new LinkedHashMap<>();\n-        logReadResponse.put(tp0, mock(LogReadResult.class));\n+        LogReadResult logReadResult = mock(LogReadResult.class);\n+        Records records = mock(Records.class);\n+        when(records.sizeInBytes()).thenReturn(2);\n+        FetchDataInfo fetchDataInfo = new FetchDataInfo(mock(LogOffsetMetadata.class), records);\n+        when(logReadResult.info()).thenReturn(fetchDataInfo);\n+        logReadResponse.put(tp0, logReadResult);",
        "comment_created_at": "2025-01-12T21:16:50+00:00",
        "comment_author": "apoorvmittal10",
        "comment_body": "> since I have already created this test [testCheckValidArguments](https://github.com/apache/kafka/pull/17870/files#diff-eef861ca89cf8d6840cf0f84919915c4c18da2e327e6587552c42f7a867d83a9R45), I don't think we need it separately in DelayedShareFetch. I think it will be redundant.\r\n\r\nHmmm, I am not sure how a `PartitionMaxBytesStrategyTest` can validate the correct handling of exception thrown by `PartitionMaxBytesStrategy.maxBytes` in DelayedShareFetch? i.e. The unit tests in PartitionMaxBytesStrategyTest doesn't validate whether the handling of thrown exception is correct in DelayedShareFetch.\r\n\r\n> Yes, we have a test [testShareFetchRequestSuccessfulSharingBetweenMultipleConsumers](https://github.com/apache/kafka/pull/17870/files#diff-784cd94373b734f49edc71a0b70d4f2d6d11dbf8b345746db7340b9a5f4fedbdR1386)\r\n\r\nWhy can't we have unit tests for the maxBytes in DelayedShareFetch? I think we should have some i.e. for same share fetch bytes and partition data, the output varies when acquired records are different. Meaning the bytes for partitions are in accordance with partition max bytes strategy.",
        "pr_file_module": null
      },
      {
        "comment_id": "1912824457",
        "repo_full_name": "apache/kafka",
        "pr_number": 17870,
        "pr_file": "core/src/test/java/kafka/server/share/DelayedShareFetchTest.java",
        "discussion_id": "1912156272",
        "commented_code": "@@ -559,13 +561,18 @@ public void testCombineLogReadResponse() {\n             .withSharePartitions(sharePartitions)\n             .build();\n \n-        LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData = new LinkedHashMap<>();\n-        topicPartitionData.put(tp0, mock(FetchRequest.PartitionData.class));\n-        topicPartitionData.put(tp1, mock(FetchRequest.PartitionData.class));\n+        LinkedHashMap<TopicIdPartition, Long> topicPartitionData = new LinkedHashMap<>();\n+        topicPartitionData.put(tp0, 0L);\n+        topicPartitionData.put(tp1, 0L);\n \n         // Case 1 - logReadResponse contains tp0.\n         LinkedHashMap<TopicIdPartition, LogReadResult> logReadResponse = new LinkedHashMap<>();\n-        logReadResponse.put(tp0, mock(LogReadResult.class));\n+        LogReadResult logReadResult = mock(LogReadResult.class);\n+        Records records = mock(Records.class);\n+        when(records.sizeInBytes()).thenReturn(2);\n+        FetchDataInfo fetchDataInfo = new FetchDataInfo(mock(LogOffsetMetadata.class), records);\n+        when(logReadResult.info()).thenReturn(fetchDataInfo);\n+        logReadResponse.put(tp0, logReadResult);",
        "comment_created_at": "2025-01-13T08:32:25+00:00",
        "comment_author": "adixitconfluent",
        "comment_body": "> Hmmm, I am not sure how a PartitionMaxBytesStrategyTest can validate the correct handling of exception thrown by PartitionMaxBytesStrategy.maxBytes in DelayedShareFetch? i.e. The unit tests in PartitionMaxBytesStrategyTest doesn't validate whether the handling of thrown exception is correct in DelayedShareFetch.\r\n\r\nThanks for the explanation. It explains the context of the comment. I have added a test `testTryCompleteWhenPartitionMaxBytesStrategyThrowsException` now.",
        "pr_file_module": null
      }
    ]
  }
]