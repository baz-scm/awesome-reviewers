[
  {
    "discussion_id": "2212043512",
    "pr_number": 20185,
    "pr_file": "core/src/main/scala/kafka/tools/StorageTool.scala",
    "created_at": "2025-07-17T02:55:28+00:00",
    "commented_code": "setIgnoreFormatted(namespace.getBoolean(\"ignore_formatted\")).\n       setControllerListenerName(config.controllerListenerNames.get(0)).\n       setMetadataLogDirectory(config.metadataLogDir)\n-    Option(namespace.getString(\"release_version\")).foreach(\n-      releaseVersion => formatter.\n-        setReleaseVersion(MetadataVersion.fromVersionString(releaseVersion)))\n+\n+    def metadataVersionsToString(first: MetadataVersion, last: MetadataVersion): String = {\n+      val versions = MetadataVersion.VERSIONS.slice(first.ordinal, last.ordinal + 1)\n+      versions.map(_.toString).mkString(\", \")\n+    }\n+    Option(namespace.getString(\"release_version\")).foreach(releaseVersion => {\n+      try {\n+        formatter.setReleaseVersion(MetadataVersion.fromVersionString(releaseVersion))\n+      } catch {\n+        case _: Throwable =>\n+          throw new TerseFailure(\"Unknown metadata.version \" + releaseVersion +\n+          \". Supported metadata.version are \" + metadataVersionsToString(\n+            MetadataVersion.MINIMUM_VERSION, MetadataVersion.latestProduction()))\n+      }",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2212043512",
        "repo_full_name": "apache/kafka",
        "pr_number": 20185,
        "pr_file": "core/src/main/scala/kafka/tools/StorageTool.scala",
        "discussion_id": "2212043512",
        "commented_code": "@@ -128,9 +128,22 @@ object StorageTool extends Logging {\n       setIgnoreFormatted(namespace.getBoolean(\"ignore_formatted\")).\n       setControllerListenerName(config.controllerListenerNames.get(0)).\n       setMetadataLogDirectory(config.metadataLogDir)\n-    Option(namespace.getString(\"release_version\")).foreach(\n-      releaseVersion => formatter.\n-        setReleaseVersion(MetadataVersion.fromVersionString(releaseVersion)))\n+\n+    def metadataVersionsToString(first: MetadataVersion, last: MetadataVersion): String = {\n+      val versions = MetadataVersion.VERSIONS.slice(first.ordinal, last.ordinal + 1)\n+      versions.map(_.toString).mkString(\", \")\n+    }\n+    Option(namespace.getString(\"release_version\")).foreach(releaseVersion => {\n+      try {\n+        formatter.setReleaseVersion(MetadataVersion.fromVersionString(releaseVersion))\n+      } catch {\n+        case _: Throwable =>\n+          throw new TerseFailure(\"Unknown metadata.version \" + releaseVersion +\n+          \". Supported metadata.version are \" + metadataVersionsToString(\n+            MetadataVersion.MINIMUM_VERSION, MetadataVersion.latestProduction()))\n+      }",
        "comment_created_at": "2025-07-17T02:55:28+00:00",
        "comment_author": "frankvicky",
        "comment_body": "Could we use an interpolated string here?\r\n```suggestion\r\n          throw new TerseFailure(s\"Unknown metadata.version $releaseVersion. Supported metadata.version are ${\r\n            metadataVersionsToString(\r\n              MetadataVersion.MINIMUM_VERSION, MetadataVersion.latestProduction())\r\n          }\")\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2212566834",
        "repo_full_name": "apache/kafka",
        "pr_number": 20185,
        "pr_file": "core/src/main/scala/kafka/tools/StorageTool.scala",
        "discussion_id": "2212043512",
        "commented_code": "@@ -128,9 +128,22 @@ object StorageTool extends Logging {\n       setIgnoreFormatted(namespace.getBoolean(\"ignore_formatted\")).\n       setControllerListenerName(config.controllerListenerNames.get(0)).\n       setMetadataLogDirectory(config.metadataLogDir)\n-    Option(namespace.getString(\"release_version\")).foreach(\n-      releaseVersion => formatter.\n-        setReleaseVersion(MetadataVersion.fromVersionString(releaseVersion)))\n+\n+    def metadataVersionsToString(first: MetadataVersion, last: MetadataVersion): String = {\n+      val versions = MetadataVersion.VERSIONS.slice(first.ordinal, last.ordinal + 1)\n+      versions.map(_.toString).mkString(\", \")\n+    }\n+    Option(namespace.getString(\"release_version\")).foreach(releaseVersion => {\n+      try {\n+        formatter.setReleaseVersion(MetadataVersion.fromVersionString(releaseVersion))\n+      } catch {\n+        case _: Throwable =>\n+          throw new TerseFailure(\"Unknown metadata.version \" + releaseVersion +\n+          \". Supported metadata.version are \" + metadataVersionsToString(\n+            MetadataVersion.MINIMUM_VERSION, MetadataVersion.latestProduction()))\n+      }",
        "comment_created_at": "2025-07-17T07:46:24+00:00",
        "comment_author": "DL1231",
        "comment_body": "good catch",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2169771979",
    "pr_number": 20024,
    "pr_file": "core/src/test/scala/unit/kafka/server/TxnOffsetCommitRequestTest.scala",
    "created_at": "2025-06-26T19:04:49+00:00",
    "commented_code": "val partitionRecord = topicRecord.partitions.asScala.find(_.partitionIndex == partition).head\n     partitionRecord.committedOffset\n   }\n+\n+  private def testDelayedTxnOffsetCommitWithBumpedEpochIsRejected(useNewProtocol: Boolean): Unit = {\n+    val topic = \"topic\"\n+    val partition = 0\n+    val transactionalId = \"txn\"\n+    val groupId = \"group\"\n+    val offset = 100L\n+\n+    // Creates the __consumer_offsets and __transaction_state topics because it won't be created automatically\n+    // in this test because it does not use FindCoordinator API.\n+    createOffsetsTopic()\n+    createTransactionStateTopic()\n+\n+    // Join the consumer group. Note that we don't heartbeat here so we must use\n+    // a session long enough for the duration of the test.\n+    val (memberId: String, memberEpoch: Int) = joinConsumerGroup(groupId, useNewProtocol)\n+    assertTrue(memberId != JoinGroupRequest.UNKNOWN_MEMBER_ID)",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2169771979",
        "repo_full_name": "apache/kafka",
        "pr_number": 20024,
        "pr_file": "core/src/test/scala/unit/kafka/server/TxnOffsetCommitRequestTest.scala",
        "discussion_id": "2169771979",
        "commented_code": "@@ -234,4 +244,91 @@ class TxnOffsetCommitRequestTest(cluster:ClusterInstance) extends GroupCoordinat\n     val partitionRecord = topicRecord.partitions.asScala.find(_.partitionIndex == partition).head\n     partitionRecord.committedOffset\n   }\n+\n+  private def testDelayedTxnOffsetCommitWithBumpedEpochIsRejected(useNewProtocol: Boolean): Unit = {\n+    val topic = \"topic\"\n+    val partition = 0\n+    val transactionalId = \"txn\"\n+    val groupId = \"group\"\n+    val offset = 100L\n+\n+    // Creates the __consumer_offsets and __transaction_state topics because it won't be created automatically\n+    // in this test because it does not use FindCoordinator API.\n+    createOffsetsTopic()\n+    createTransactionStateTopic()\n+\n+    // Join the consumer group. Note that we don't heartbeat here so we must use\n+    // a session long enough for the duration of the test.\n+    val (memberId: String, memberEpoch: Int) = joinConsumerGroup(groupId, useNewProtocol)\n+    assertTrue(memberId != JoinGroupRequest.UNKNOWN_MEMBER_ID)",
        "comment_created_at": "2025-06-26T19:04:49+00:00",
        "comment_author": "chia7712",
        "comment_body": "```java\r\n    assertNotEquals(JoinGroupRequest.UNKNOWN_MEMBER_ID, memberId)\r\n    assertNotEquals(JoinGroupRequest.UNKNOWN_GENERATION_ID, memberEpoch)\r\n```\r\n`assertNotEquals` could show more readable messages.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2205584011",
    "pr_number": 20164,
    "pr_file": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala",
    "created_at": "2025-07-14T18:45:00+00:00",
    "commented_code": "requestMetrics, envelope = None)\n   }\n \n+  private def buildRequestWithPrincipal(request: AbstractRequest,\n+                                        principal: KafkaPrincipal,\n+                                        clientAddress: InetAddress,\n+                                        listenerName: ListenerName = ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n+                                        fromPrivilegedListener: Boolean = false,\n+                                        requestHeader: Option[RequestHeader],\n+                                        requestMetrics: RequestChannelMetrics = requestChannelMetrics): RequestChannel.Request = {\n+    val buffer = request.serializeWithHeader(\n+      requestHeader.getOrElse(new RequestHeader(request.apiKey, request.version, clientId, 0)))\n+\n+    // read the header from the buffer first so that the body can be read next from the Request constructor\n+    val header = RequestHeader.parse(buffer)\n+    val context = new RequestContext(header, \"1\", clientAddress, Optional.empty(),\n+      principal, listenerName, SecurityProtocol.SSL,\n+      ClientInformation.EMPTY, fromPrivilegedListener, Optional.of(kafkaPrincipalSerde))\n+    new RequestChannel.Request(processor = 1, context = context, startTimeNanos = 0, MemoryPool.NONE, buffer,\n+      requestMetrics, envelope = None)\n+  }",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2205584011",
        "repo_full_name": "apache/kafka",
        "pr_number": 20164,
        "pr_file": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala",
        "discussion_id": "2205584011",
        "commented_code": "@@ -9510,6 +9620,25 @@ class KafkaApisTest extends Logging {\n       requestMetrics, envelope = None)\n   }\n \n+  private def buildRequestWithPrincipal(request: AbstractRequest,\n+                                        principal: KafkaPrincipal,\n+                                        clientAddress: InetAddress,\n+                                        listenerName: ListenerName = ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT),\n+                                        fromPrivilegedListener: Boolean = false,\n+                                        requestHeader: Option[RequestHeader],\n+                                        requestMetrics: RequestChannelMetrics = requestChannelMetrics): RequestChannel.Request = {\n+    val buffer = request.serializeWithHeader(\n+      requestHeader.getOrElse(new RequestHeader(request.apiKey, request.version, clientId, 0)))\n+\n+    // read the header from the buffer first so that the body can be read next from the Request constructor\n+    val header = RequestHeader.parse(buffer)\n+    val context = new RequestContext(header, \"1\", clientAddress, Optional.empty(),\n+      principal, listenerName, SecurityProtocol.SSL,\n+      ClientInformation.EMPTY, fromPrivilegedListener, Optional.of(kafkaPrincipalSerde))\n+    new RequestChannel.Request(processor = 1, context = context, startTimeNanos = 0, MemoryPool.NONE, buffer,\n+      requestMetrics, envelope = None)\n+  }",
        "comment_created_at": "2025-07-14T18:45:00+00:00",
        "comment_author": "apoorvmittal10",
        "comment_body": "Seems existing `buildRequest` method can be overloaded with this method. So we have common code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1668602936",
    "pr_number": 16381,
    "pr_file": "core/src/main/scala/kafka/admin/ConfigCommand.scala",
    "created_at": "2024-07-08T13:10:29+00:00",
    "commented_code": "val entityTypes = opts.entityTypes\n     val entityNames = opts.entityNames\n     val entityTypeHead = entityTypes.head\n-    val entityNameHead = entityNames.head\n     val configsToBeAddedMap = parseConfigsToBeAdded(opts).asScala.toMap // no need for mutability\n     val configsToBeAdded = configsToBeAddedMap.map { case (k, v) => (k, new ConfigEntry(k, v)) }\n     val configsToBeDeleted = parseConfigsToBeDeleted(opts)\n \n     entityTypeHead match {\n       case ConfigType.TOPIC =>\n-        val oldConfig = getResourceConfig(adminClient, entityTypeHead, entityNameHead, includeSynonyms = false, describeAll = false)\n-          .map { entry => (entry.name, entry) }.toMap\n-\n-        // fail the command if any of the configs to be deleted does not exist\n-        val invalidConfigs = configsToBeDeleted.filterNot(oldConfig.contains)\n-        if (invalidConfigs.nonEmpty)\n-          throw new InvalidConfigurationException(s\"Invalid config(s): ${invalidConfigs.mkString(\",\")}\")\n-\n-        val configResource = new ConfigResource(ConfigResource.Type.TOPIC, entityNameHead)\n-        val alterOptions = new AlterConfigsOptions().timeoutMs(30000).validateOnly(false)\n-        val alterEntries = (configsToBeAdded.values.map(new AlterConfigOp(_, AlterConfigOp.OpType.SET))\n-          ++ configsToBeDeleted.map { k => new AlterConfigOp(new ConfigEntry(k, \"\"), AlterConfigOp.OpType.DELETE) }\n-        ).asJavaCollection\n-        adminClient.incrementalAlterConfigs(Map(configResource -> alterEntries).asJava, alterOptions).all().get(60, TimeUnit.SECONDS)\n-\n+        entityNames.foreach { entityName =>\n+          val oldConfig = getResourceConfig(adminClient, entityTypeHead, entityName, includeSynonyms = false, describeAll = false)\n+            .map { entry => (entry.name, entry) }.toMap\n+\n+          // fail the command if any of the configs to be deleted does not exist\n+          val invalidConfigs = configsToBeDeleted.filterNot(oldConfig.contains)\n+          if (invalidConfigs.nonEmpty)\n+            throw new InvalidConfigurationException(s\"Invalid config(s): ${invalidConfigs.mkString(\",\")}\")\n+\n+          val configResource = new ConfigResource(ConfigResource.Type.TOPIC, entityName)\n+          val alterOptions = new AlterConfigsOptions().timeoutMs(30000).validateOnly(false)\n+          val alterEntries = (configsToBeAdded.values.map(new AlterConfigOp(_, AlterConfigOp.OpType.SET))\n+            ++ configsToBeDeleted.map { k => new AlterConfigOp(new ConfigEntry(k, \"\"), AlterConfigOp.OpType.DELETE) }\n+            ).asJavaCollection\n+          adminClient.incrementalAlterConfigs(Map(configResource -> alterEntries).asJava, alterOptions).all().get(60, TimeUnit.SECONDS)\n+        }\n       case ConfigType.BROKER =>\n-        val oldConfig = getResourceConfig(adminClient, entityTypeHead, entityNameHead, includeSynonyms = false, describeAll = false)\n-          .map { entry => (entry.name, entry) }.toMap\n-\n-        // fail the command if any of the configs to be deleted does not exist\n-        val invalidConfigs = configsToBeDeleted.filterNot(oldConfig.contains)\n-        if (invalidConfigs.nonEmpty)\n-          throw new InvalidConfigurationException(s\"Invalid config(s): ${invalidConfigs.mkString(\",\")}\")\n-\n-        val newEntries = oldConfig ++ configsToBeAdded -- configsToBeDeleted\n-        val sensitiveEntries = newEntries.filter(_._2.value == null)\n-        if (sensitiveEntries.nonEmpty)\n-          throw new InvalidConfigurationException(s\"All sensitive broker config entries must be specified for --alter, missing entries: ${sensitiveEntries.keySet}\")\n-        val newConfig = new JConfig(newEntries.asJava.values)\n-\n-        val configResource = new ConfigResource(ConfigResource.Type.BROKER, entityNameHead)\n-        val alterOptions = new AlterConfigsOptions().timeoutMs(30000).validateOnly(false)\n-        adminClient.alterConfigs(Map(configResource -> newConfig).asJava, alterOptions).all().get(60, TimeUnit.SECONDS)\n+        entityNames.foreach { entityName =>",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "1668602936",
        "repo_full_name": "apache/kafka",
        "pr_number": 16381,
        "pr_file": "core/src/main/scala/kafka/admin/ConfigCommand.scala",
        "discussion_id": "1668602936",
        "commented_code": "@@ -359,61 +359,64 @@ object ConfigCommand extends Logging {\n     val entityTypes = opts.entityTypes\n     val entityNames = opts.entityNames\n     val entityTypeHead = entityTypes.head\n-    val entityNameHead = entityNames.head\n     val configsToBeAddedMap = parseConfigsToBeAdded(opts).asScala.toMap // no need for mutability\n     val configsToBeAdded = configsToBeAddedMap.map { case (k, v) => (k, new ConfigEntry(k, v)) }\n     val configsToBeDeleted = parseConfigsToBeDeleted(opts)\n \n     entityTypeHead match {\n       case ConfigType.TOPIC =>\n-        val oldConfig = getResourceConfig(adminClient, entityTypeHead, entityNameHead, includeSynonyms = false, describeAll = false)\n-          .map { entry => (entry.name, entry) }.toMap\n-\n-        // fail the command if any of the configs to be deleted does not exist\n-        val invalidConfigs = configsToBeDeleted.filterNot(oldConfig.contains)\n-        if (invalidConfigs.nonEmpty)\n-          throw new InvalidConfigurationException(s\"Invalid config(s): ${invalidConfigs.mkString(\",\")}\")\n-\n-        val configResource = new ConfigResource(ConfigResource.Type.TOPIC, entityNameHead)\n-        val alterOptions = new AlterConfigsOptions().timeoutMs(30000).validateOnly(false)\n-        val alterEntries = (configsToBeAdded.values.map(new AlterConfigOp(_, AlterConfigOp.OpType.SET))\n-          ++ configsToBeDeleted.map { k => new AlterConfigOp(new ConfigEntry(k, \"\"), AlterConfigOp.OpType.DELETE) }\n-        ).asJavaCollection\n-        adminClient.incrementalAlterConfigs(Map(configResource -> alterEntries).asJava, alterOptions).all().get(60, TimeUnit.SECONDS)\n-\n+        entityNames.foreach { entityName =>\n+          val oldConfig = getResourceConfig(adminClient, entityTypeHead, entityName, includeSynonyms = false, describeAll = false)\n+            .map { entry => (entry.name, entry) }.toMap\n+\n+          // fail the command if any of the configs to be deleted does not exist\n+          val invalidConfigs = configsToBeDeleted.filterNot(oldConfig.contains)\n+          if (invalidConfigs.nonEmpty)\n+            throw new InvalidConfigurationException(s\"Invalid config(s): ${invalidConfigs.mkString(\",\")}\")\n+\n+          val configResource = new ConfigResource(ConfigResource.Type.TOPIC, entityName)\n+          val alterOptions = new AlterConfigsOptions().timeoutMs(30000).validateOnly(false)\n+          val alterEntries = (configsToBeAdded.values.map(new AlterConfigOp(_, AlterConfigOp.OpType.SET))\n+            ++ configsToBeDeleted.map { k => new AlterConfigOp(new ConfigEntry(k, \"\"), AlterConfigOp.OpType.DELETE) }\n+            ).asJavaCollection\n+          adminClient.incrementalAlterConfigs(Map(configResource -> alterEntries).asJava, alterOptions).all().get(60, TimeUnit.SECONDS)\n+        }\n       case ConfigType.BROKER =>\n-        val oldConfig = getResourceConfig(adminClient, entityTypeHead, entityNameHead, includeSynonyms = false, describeAll = false)\n-          .map { entry => (entry.name, entry) }.toMap\n-\n-        // fail the command if any of the configs to be deleted does not exist\n-        val invalidConfigs = configsToBeDeleted.filterNot(oldConfig.contains)\n-        if (invalidConfigs.nonEmpty)\n-          throw new InvalidConfigurationException(s\"Invalid config(s): ${invalidConfigs.mkString(\",\")}\")\n-\n-        val newEntries = oldConfig ++ configsToBeAdded -- configsToBeDeleted\n-        val sensitiveEntries = newEntries.filter(_._2.value == null)\n-        if (sensitiveEntries.nonEmpty)\n-          throw new InvalidConfigurationException(s\"All sensitive broker config entries must be specified for --alter, missing entries: ${sensitiveEntries.keySet}\")\n-        val newConfig = new JConfig(newEntries.asJava.values)\n-\n-        val configResource = new ConfigResource(ConfigResource.Type.BROKER, entityNameHead)\n-        val alterOptions = new AlterConfigsOptions().timeoutMs(30000).validateOnly(false)\n-        adminClient.alterConfigs(Map(configResource -> newConfig).asJava, alterOptions).all().get(60, TimeUnit.SECONDS)\n+        entityNames.foreach { entityName =>",
        "comment_created_at": "2024-07-08T13:10:29+00:00",
        "comment_author": "chia7712",
        "comment_body": "Could you please move those code to separate method? That can reduce the complexity and it can be test easily.",
        "pr_file_module": null
      }
    ]
  }
]