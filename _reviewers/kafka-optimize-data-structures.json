[
  {
    "discussion_id": "2229099291",
    "pr_number": 20096,
    "pr_file": "tools/src/test/java/org/apache/kafka/tools/streams/StreamsGroupCommandTest.java",
    "created_at": "2025-07-24T17:13:15+00:00",
    "commented_code": "0,\n             0,\n             0,\n-            Collections.singletonList(new StreamsGroupSubtopologyDescription(\"subtopologyId\", Collections.emptyList(), Collections.emptyList(), Map.of(), Map.of())),\n+            List.of(new StreamsGroupSubtopologyDescription(\"subtopologyId\", List.of(), List.of(), Map.of(), Map.of())),\n             List.of(memberDescription),\n             groupState,\n             new Node(1, \"localhost\", 9092),\n             Set.of());\n         KafkaFutureImpl<StreamsGroupDescription> future = new KafkaFutureImpl<>();\n         future.complete(description);\n-        return new DescribeStreamsGroupsResult(Collections.singletonMap(groupId, future));\n+        return new DescribeStreamsGroupsResult(Map.of(groupId, future));\n     }\n \n     private DescribeTopicsResult describeTopicsResult(Collection<String> topics, int numOfPartitions) {",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2229099291",
        "repo_full_name": "apache/kafka",
        "pr_number": 20096,
        "pr_file": "tools/src/test/java/org/apache/kafka/tools/streams/StreamsGroupCommandTest.java",
        "discussion_id": "2229099291",
        "commented_code": "@@ -473,23 +470,23 @@ private DescribeStreamsGroupsResult describeStreamsResult(String groupId, GroupS\n             0,\n             0,\n             0,\n-            Collections.singletonList(new StreamsGroupSubtopologyDescription(\"subtopologyId\", Collections.emptyList(), Collections.emptyList(), Map.of(), Map.of())),\n+            List.of(new StreamsGroupSubtopologyDescription(\"subtopologyId\", List.of(), List.of(), Map.of(), Map.of())),\n             List.of(memberDescription),\n             groupState,\n             new Node(1, \"localhost\", 9092),\n             Set.of());\n         KafkaFutureImpl<StreamsGroupDescription> future = new KafkaFutureImpl<>();\n         future.complete(description);\n-        return new DescribeStreamsGroupsResult(Collections.singletonMap(groupId, future));\n+        return new DescribeStreamsGroupsResult(Map.of(groupId, future));\n     }\n \n     private DescribeTopicsResult describeTopicsResult(Collection<String> topics, int numOfPartitions) {",
        "comment_created_at": "2025-07-24T17:13:15+00:00",
        "comment_author": "chia7712",
        "comment_body": "```java\r\n    private DescribeTopicsResult describeTopicsResult(Collection<String> topics, int numOfPartitions) {\r\n        var topicDescriptions = topics.stream().collect(Collectors.toMap(Function.identity(),\r\n                topic -> new TopicDescription(topic, false, IntStream.range(0, numOfPartitions)\r\n                .mapToObj(i -> new TopicPartitionInfo(i, null, List.of(), List.of()))\r\n                .toList())));\r\n        return AdminClientTestUtils.describeTopicsResult(topicDescriptions);\r\n    }\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2047541757",
    "pr_number": 19232,
    "pr_file": "metadata/src/main/java/org/apache/kafka/metadata/KRaftMetadataCache.java",
    "created_at": "2025-04-16T18:52:26+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.metadata;\n+\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.errors.InvalidTopicException;\n+import org.apache.kafka.common.internals.Topic;\n+import org.apache.kafka.common.message.DescribeClientQuotasRequestData;\n+import org.apache.kafka.common.message.DescribeClientQuotasResponseData;\n+import org.apache.kafka.common.message.DescribeTopicPartitionsResponseData;\n+import org.apache.kafka.common.message.DescribeTopicPartitionsResponseData.Cursor;\n+import org.apache.kafka.common.message.DescribeTopicPartitionsResponseData.DescribeTopicPartitionsResponsePartition;\n+import org.apache.kafka.common.message.DescribeTopicPartitionsResponseData.DescribeTopicPartitionsResponseTopic;\n+import org.apache.kafka.common.message.DescribeUserScramCredentialsRequestData;\n+import org.apache.kafka.common.message.DescribeUserScramCredentialsResponseData;\n+import org.apache.kafka.common.message.MetadataResponseData.MetadataResponsePartition;\n+import org.apache.kafka.common.message.MetadataResponseData.MetadataResponseTopic;\n+import org.apache.kafka.common.network.ListenerName;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.requests.MetadataResponse;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.image.MetadataImage;\n+import org.apache.kafka.image.TopicImage;\n+import org.apache.kafka.server.common.FinalizedFeatures;\n+import org.apache.kafka.server.common.KRaftVersion;\n+import org.apache.kafka.server.common.MetadataVersion;\n+\n+import org.slf4j.Logger;\n+\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+public class KRaftMetadataCache implements MetadataCache {\n+    private final Logger log;\n+    private final Supplier<KRaftVersion> kraftVersionSupplier;\n+\n+    // This is the cache state. Every MetadataImage instance is immutable, and updates\n+    // replace this value with a completely new one. This means reads (which are not under\n+    // any lock) need to grab the value of this variable once, and retain that read copy for\n+    // the duration of their operation. Multiple reads of this value risk getting different\n+    // image values.\n+    private volatile MetadataImage currentImage = MetadataImage.EMPTY;\n+\n+    public KRaftMetadataCache(int brokerId, Supplier<KRaftVersion> kraftVersionSupplier) {\n+        this.kraftVersionSupplier = kraftVersionSupplier;\n+        this.log = new LogContext(\"[MetadataCache brokerId=\" + brokerId + \"] \").logger(KRaftMetadataCache.class);\n+    }\n+\n+    // This method is the main hotspot when it comes to the performance of metadata requests,\n+    // we should be careful about adding additional logic here.\n+    // filterUnavailableEndpoints exists to support v0 MetadataResponses\n+    private List<Integer> maybeFilterAliveReplicas(MetadataImage image, int[] brokers, ListenerName listenerName, boolean filterUnavailableEndpoints) {\n+        if (!filterUnavailableEndpoints) {\n+            return Replicas.toList(brokers);\n+        } else {\n+            List<Integer> res = new ArrayList<>(brokers.length);\n+            for (int brokerId : brokers) {\n+                Optional.ofNullable(image.cluster().broker(brokerId)).ifPresent(b -> {\n+                    if (!b.fenced() && b.listeners().containsKey(listenerName.value())) {\n+                        res.add(brokerId);\n+                    }\n+                });\n+            }\n+            return res;\n+        }\n+    }\n+\n+    public MetadataImage currentImage() {\n+        return currentImage;\n+    }\n+\n+    // errorUnavailableEndpoints exists to support v0 MetadataResponses\n+    // If errorUnavailableListeners=true, return LISTENER_NOT_FOUND if listener is missing on the broker.\n+    // Otherwise, return LEADER_NOT_AVAILABLE for broker unavailable and missing listener (Metadata response v5 and below).\n+    private Optional<Iterator<MetadataResponsePartition>> getPartitionMetadata(\n+        MetadataImage image,\n+        String topicName,\n+        ListenerName listenerName,\n+        boolean errorUnavailableEndpoints,\n+        boolean errorUnavailableListeners\n+    ) {\n+        return Optional.ofNullable(image.topics().getTopic(topicName)).map(topic -> topic.partitions().entrySet().stream().map(entry -> {\n+            int partitionId = entry.getKey();\n+            PartitionRegistration partition = entry.getValue();\n+            List<Integer> filteredReplicas = maybeFilterAliveReplicas(image, partition.replicas, listenerName, errorUnavailableEndpoints);\n+            List<Integer> filteredIsr = maybeFilterAliveReplicas(image, partition.isr, listenerName, errorUnavailableEndpoints);\n+            List<Integer> offlineReplicas = getOfflineReplicas(image, partition, listenerName);\n+            Optional<Node> maybeLeader = getAliveEndpoint(image, partition.leader, listenerName);\n+            Errors error;\n+            if (maybeLeader.isEmpty()) {\n+                if (!image.cluster().brokers().containsKey(partition.leader)) {\n+                    log.debug(\"Error while fetching metadata for {}-{}: leader not available\", topicName, partitionId);\n+                    error = Errors.LEADER_NOT_AVAILABLE;\n+                } else {\n+                    log.debug(\"Error while fetching metadata for {}-{}: listener {} not found on leader {}\", topicName, partitionId, listenerName, partition.leader);\n+                    error = errorUnavailableListeners ? Errors.LISTENER_NOT_FOUND : Errors.LEADER_NOT_AVAILABLE;\n+                }\n+                return new MetadataResponsePartition()\n+                    .setErrorCode(error.code())\n+                    .setPartitionIndex(partitionId)\n+                    .setLeaderId(MetadataResponse.NO_LEADER_ID)\n+                    .setLeaderEpoch(partition.leaderEpoch)\n+                    .setReplicaNodes(filteredReplicas)\n+                    .setIsrNodes(filteredIsr)\n+                    .setOfflineReplicas(offlineReplicas);\n+            } else {\n+                if (filteredReplicas.size() < partition.replicas.length) {\n+                    log.debug(\"Error while fetching metadata for {}-{}: replica information not available for following brokers {}\", topicName, partitionId, Arrays.stream(partition.replicas).filter(b -> !filteredReplicas.contains(b)).mapToObj(String::valueOf).collect(Collectors.joining(\",\")));\n+                    error = Errors.REPLICA_NOT_AVAILABLE;\n+                } else if (filteredIsr.size() < partition.isr.length) {\n+                    log.debug(\"Error while fetching metadata for {}-{}: in sync replica information not available for following brokers {}\", topicName, partitionId, Arrays.stream(partition.isr).filter(b -> !filteredIsr.contains(b)).mapToObj(String::valueOf).collect(Collectors.joining(\",\")));\n+                    error = Errors.REPLICA_NOT_AVAILABLE;\n+                } else {\n+                    error = Errors.NONE;\n+                }\n+                return new MetadataResponsePartition()\n+                    .setErrorCode(error.code())\n+                    .setPartitionIndex(partitionId)\n+                    .setLeaderId(maybeLeader.get().id())\n+                    .setLeaderEpoch(partition.leaderEpoch)\n+                    .setReplicaNodes(filteredReplicas)\n+                    .setIsrNodes(filteredIsr)\n+                    .setOfflineReplicas(offlineReplicas);\n+            }\n+        }).iterator());\n+    }\n+\n+    /**\n+     * Return topic partition metadata for the given topic, listener and index range. Also, return the next partition\n+     * index that is not included in the result.\n+     *\n+     * @param image                       The metadata image\n+     * @param topicName                   The name of the topic.\n+     * @param listenerName                The listener name.\n+     * @param startIndex                  The smallest index of the partitions to be included in the result.\n+     *\n+     * @return                            A collection of topic partition metadata and next partition index (-1 means\n+     *                                    no next partition).\n+     */\n+    private Entry<Optional<List<DescribeTopicPartitionsResponsePartition>>, Integer> getPartitionMetadataForDescribeTopicResponse(\n+        MetadataImage image,\n+        String topicName,\n+        ListenerName listenerName,\n+        int startIndex,\n+        int maxCount\n+    ) {\n+        TopicImage topic = image.topics().getTopic(topicName);\n+        if (topic == null) {\n+            return new AbstractMap.SimpleEntry<>(Optional.empty(), -1);\n+        } else {\n+            List<DescribeTopicPartitionsResponsePartition> result = new ArrayList<>();\n+            Set<Integer> partitions = topic.partitions().keySet();\n+            int upperIndex = Math.min(topic.partitions().size(), startIndex + maxCount);\n+            int nextIndex = (upperIndex < partitions.size()) ? upperIndex : -1;\n+            for (int partitionId = startIndex; partitionId < upperIndex; partitionId++) {\n+                PartitionRegistration partition = topic.partitions().get(partitionId);\n+                if (partition != null) {\n+                    List<Integer> filteredReplicas = maybeFilterAliveReplicas(image, partition.replicas, listenerName, false);\n+                    List<Integer> filteredIsr = maybeFilterAliveReplicas(image, partition.isr, listenerName, false);\n+                    List<Integer> offlineReplicas = getOfflineReplicas(image, partition, listenerName);\n+                    Optional<Node> maybeLeader = getAliveEndpoint(image, partition.leader, listenerName);\n+                    if (maybeLeader.isEmpty()) {\n+                        result.add(new DescribeTopicPartitionsResponsePartition()\n+                            .setPartitionIndex(partitionId)\n+                            .setLeaderId(MetadataResponse.NO_LEADER_ID)\n+                            .setLeaderEpoch(partition.leaderEpoch)\n+                            .setReplicaNodes(filteredReplicas)\n+                            .setIsrNodes(filteredIsr)\n+                            .setOfflineReplicas(offlineReplicas)\n+                            .setEligibleLeaderReplicas(Replicas.toList(partition.elr))\n+                            .setLastKnownElr(Replicas.toList(partition.lastKnownElr)));\n+                    } else {\n+                        result.add(new DescribeTopicPartitionsResponsePartition()\n+                            .setPartitionIndex(partitionId)\n+                            .setLeaderId(maybeLeader.get().id())\n+                            .setLeaderEpoch(partition.leaderEpoch)\n+                            .setReplicaNodes(filteredReplicas)\n+                            .setIsrNodes(filteredIsr)\n+                            .setOfflineReplicas(offlineReplicas)\n+                            .setEligibleLeaderReplicas(Replicas.toList(partition.elr))\n+                            .setLastKnownElr(Replicas.toList(partition.lastKnownElr)));\n+                    }\n+                } else {\n+                    log.warn(\"The partition {} does not exist for {}\", partitionId, topicName);\n+                }\n+            }\n+            return new AbstractMap.SimpleEntry<>(Optional.of(result), nextIndex);\n+        }\n+    }\n+\n+    private List<Integer> getOfflineReplicas(MetadataImage image, PartitionRegistration partition, ListenerName listenerName) {\n+        List<Integer> offlineReplicas = new ArrayList<>(0);",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2047541757",
        "repo_full_name": "apache/kafka",
        "pr_number": 19232,
        "pr_file": "metadata/src/main/java/org/apache/kafka/metadata/KRaftMetadataCache.java",
        "discussion_id": "2047541757",
        "commented_code": "@@ -0,0 +1,518 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.metadata;\n+\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.errors.InvalidTopicException;\n+import org.apache.kafka.common.internals.Topic;\n+import org.apache.kafka.common.message.DescribeClientQuotasRequestData;\n+import org.apache.kafka.common.message.DescribeClientQuotasResponseData;\n+import org.apache.kafka.common.message.DescribeTopicPartitionsResponseData;\n+import org.apache.kafka.common.message.DescribeTopicPartitionsResponseData.Cursor;\n+import org.apache.kafka.common.message.DescribeTopicPartitionsResponseData.DescribeTopicPartitionsResponsePartition;\n+import org.apache.kafka.common.message.DescribeTopicPartitionsResponseData.DescribeTopicPartitionsResponseTopic;\n+import org.apache.kafka.common.message.DescribeUserScramCredentialsRequestData;\n+import org.apache.kafka.common.message.DescribeUserScramCredentialsResponseData;\n+import org.apache.kafka.common.message.MetadataResponseData.MetadataResponsePartition;\n+import org.apache.kafka.common.message.MetadataResponseData.MetadataResponseTopic;\n+import org.apache.kafka.common.network.ListenerName;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.requests.MetadataResponse;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.image.MetadataImage;\n+import org.apache.kafka.image.TopicImage;\n+import org.apache.kafka.server.common.FinalizedFeatures;\n+import org.apache.kafka.server.common.KRaftVersion;\n+import org.apache.kafka.server.common.MetadataVersion;\n+\n+import org.slf4j.Logger;\n+\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+public class KRaftMetadataCache implements MetadataCache {\n+    private final Logger log;\n+    private final Supplier<KRaftVersion> kraftVersionSupplier;\n+\n+    // This is the cache state. Every MetadataImage instance is immutable, and updates\n+    // replace this value with a completely new one. This means reads (which are not under\n+    // any lock) need to grab the value of this variable once, and retain that read copy for\n+    // the duration of their operation. Multiple reads of this value risk getting different\n+    // image values.\n+    private volatile MetadataImage currentImage = MetadataImage.EMPTY;\n+\n+    public KRaftMetadataCache(int brokerId, Supplier<KRaftVersion> kraftVersionSupplier) {\n+        this.kraftVersionSupplier = kraftVersionSupplier;\n+        this.log = new LogContext(\"[MetadataCache brokerId=\" + brokerId + \"] \").logger(KRaftMetadataCache.class);\n+    }\n+\n+    // This method is the main hotspot when it comes to the performance of metadata requests,\n+    // we should be careful about adding additional logic here.\n+    // filterUnavailableEndpoints exists to support v0 MetadataResponses\n+    private List<Integer> maybeFilterAliveReplicas(MetadataImage image, int[] brokers, ListenerName listenerName, boolean filterUnavailableEndpoints) {\n+        if (!filterUnavailableEndpoints) {\n+            return Replicas.toList(brokers);\n+        } else {\n+            List<Integer> res = new ArrayList<>(brokers.length);\n+            for (int brokerId : brokers) {\n+                Optional.ofNullable(image.cluster().broker(brokerId)).ifPresent(b -> {\n+                    if (!b.fenced() && b.listeners().containsKey(listenerName.value())) {\n+                        res.add(brokerId);\n+                    }\n+                });\n+            }\n+            return res;\n+        }\n+    }\n+\n+    public MetadataImage currentImage() {\n+        return currentImage;\n+    }\n+\n+    // errorUnavailableEndpoints exists to support v0 MetadataResponses\n+    // If errorUnavailableListeners=true, return LISTENER_NOT_FOUND if listener is missing on the broker.\n+    // Otherwise, return LEADER_NOT_AVAILABLE for broker unavailable and missing listener (Metadata response v5 and below).\n+    private Optional<Iterator<MetadataResponsePartition>> getPartitionMetadata(\n+        MetadataImage image,\n+        String topicName,\n+        ListenerName listenerName,\n+        boolean errorUnavailableEndpoints,\n+        boolean errorUnavailableListeners\n+    ) {\n+        return Optional.ofNullable(image.topics().getTopic(topicName)).map(topic -> topic.partitions().entrySet().stream().map(entry -> {\n+            int partitionId = entry.getKey();\n+            PartitionRegistration partition = entry.getValue();\n+            List<Integer> filteredReplicas = maybeFilterAliveReplicas(image, partition.replicas, listenerName, errorUnavailableEndpoints);\n+            List<Integer> filteredIsr = maybeFilterAliveReplicas(image, partition.isr, listenerName, errorUnavailableEndpoints);\n+            List<Integer> offlineReplicas = getOfflineReplicas(image, partition, listenerName);\n+            Optional<Node> maybeLeader = getAliveEndpoint(image, partition.leader, listenerName);\n+            Errors error;\n+            if (maybeLeader.isEmpty()) {\n+                if (!image.cluster().brokers().containsKey(partition.leader)) {\n+                    log.debug(\"Error while fetching metadata for {}-{}: leader not available\", topicName, partitionId);\n+                    error = Errors.LEADER_NOT_AVAILABLE;\n+                } else {\n+                    log.debug(\"Error while fetching metadata for {}-{}: listener {} not found on leader {}\", topicName, partitionId, listenerName, partition.leader);\n+                    error = errorUnavailableListeners ? Errors.LISTENER_NOT_FOUND : Errors.LEADER_NOT_AVAILABLE;\n+                }\n+                return new MetadataResponsePartition()\n+                    .setErrorCode(error.code())\n+                    .setPartitionIndex(partitionId)\n+                    .setLeaderId(MetadataResponse.NO_LEADER_ID)\n+                    .setLeaderEpoch(partition.leaderEpoch)\n+                    .setReplicaNodes(filteredReplicas)\n+                    .setIsrNodes(filteredIsr)\n+                    .setOfflineReplicas(offlineReplicas);\n+            } else {\n+                if (filteredReplicas.size() < partition.replicas.length) {\n+                    log.debug(\"Error while fetching metadata for {}-{}: replica information not available for following brokers {}\", topicName, partitionId, Arrays.stream(partition.replicas).filter(b -> !filteredReplicas.contains(b)).mapToObj(String::valueOf).collect(Collectors.joining(\",\")));\n+                    error = Errors.REPLICA_NOT_AVAILABLE;\n+                } else if (filteredIsr.size() < partition.isr.length) {\n+                    log.debug(\"Error while fetching metadata for {}-{}: in sync replica information not available for following brokers {}\", topicName, partitionId, Arrays.stream(partition.isr).filter(b -> !filteredIsr.contains(b)).mapToObj(String::valueOf).collect(Collectors.joining(\",\")));\n+                    error = Errors.REPLICA_NOT_AVAILABLE;\n+                } else {\n+                    error = Errors.NONE;\n+                }\n+                return new MetadataResponsePartition()\n+                    .setErrorCode(error.code())\n+                    .setPartitionIndex(partitionId)\n+                    .setLeaderId(maybeLeader.get().id())\n+                    .setLeaderEpoch(partition.leaderEpoch)\n+                    .setReplicaNodes(filteredReplicas)\n+                    .setIsrNodes(filteredIsr)\n+                    .setOfflineReplicas(offlineReplicas);\n+            }\n+        }).iterator());\n+    }\n+\n+    /**\n+     * Return topic partition metadata for the given topic, listener and index range. Also, return the next partition\n+     * index that is not included in the result.\n+     *\n+     * @param image                       The metadata image\n+     * @param topicName                   The name of the topic.\n+     * @param listenerName                The listener name.\n+     * @param startIndex                  The smallest index of the partitions to be included in the result.\n+     *\n+     * @return                            A collection of topic partition metadata and next partition index (-1 means\n+     *                                    no next partition).\n+     */\n+    private Entry<Optional<List<DescribeTopicPartitionsResponsePartition>>, Integer> getPartitionMetadataForDescribeTopicResponse(\n+        MetadataImage image,\n+        String topicName,\n+        ListenerName listenerName,\n+        int startIndex,\n+        int maxCount\n+    ) {\n+        TopicImage topic = image.topics().getTopic(topicName);\n+        if (topic == null) {\n+            return new AbstractMap.SimpleEntry<>(Optional.empty(), -1);\n+        } else {\n+            List<DescribeTopicPartitionsResponsePartition> result = new ArrayList<>();\n+            Set<Integer> partitions = topic.partitions().keySet();\n+            int upperIndex = Math.min(topic.partitions().size(), startIndex + maxCount);\n+            int nextIndex = (upperIndex < partitions.size()) ? upperIndex : -1;\n+            for (int partitionId = startIndex; partitionId < upperIndex; partitionId++) {\n+                PartitionRegistration partition = topic.partitions().get(partitionId);\n+                if (partition != null) {\n+                    List<Integer> filteredReplicas = maybeFilterAliveReplicas(image, partition.replicas, listenerName, false);\n+                    List<Integer> filteredIsr = maybeFilterAliveReplicas(image, partition.isr, listenerName, false);\n+                    List<Integer> offlineReplicas = getOfflineReplicas(image, partition, listenerName);\n+                    Optional<Node> maybeLeader = getAliveEndpoint(image, partition.leader, listenerName);\n+                    if (maybeLeader.isEmpty()) {\n+                        result.add(new DescribeTopicPartitionsResponsePartition()\n+                            .setPartitionIndex(partitionId)\n+                            .setLeaderId(MetadataResponse.NO_LEADER_ID)\n+                            .setLeaderEpoch(partition.leaderEpoch)\n+                            .setReplicaNodes(filteredReplicas)\n+                            .setIsrNodes(filteredIsr)\n+                            .setOfflineReplicas(offlineReplicas)\n+                            .setEligibleLeaderReplicas(Replicas.toList(partition.elr))\n+                            .setLastKnownElr(Replicas.toList(partition.lastKnownElr)));\n+                    } else {\n+                        result.add(new DescribeTopicPartitionsResponsePartition()\n+                            .setPartitionIndex(partitionId)\n+                            .setLeaderId(maybeLeader.get().id())\n+                            .setLeaderEpoch(partition.leaderEpoch)\n+                            .setReplicaNodes(filteredReplicas)\n+                            .setIsrNodes(filteredIsr)\n+                            .setOfflineReplicas(offlineReplicas)\n+                            .setEligibleLeaderReplicas(Replicas.toList(partition.elr))\n+                            .setLastKnownElr(Replicas.toList(partition.lastKnownElr)));\n+                    }\n+                } else {\n+                    log.warn(\"The partition {} does not exist for {}\", partitionId, topicName);\n+                }\n+            }\n+            return new AbstractMap.SimpleEntry<>(Optional.of(result), nextIndex);\n+        }\n+    }\n+\n+    private List<Integer> getOfflineReplicas(MetadataImage image, PartitionRegistration partition, ListenerName listenerName) {\n+        List<Integer> offlineReplicas = new ArrayList<>(0);",
        "comment_created_at": "2025-04-16T18:52:26+00:00",
        "comment_author": "chia7712",
        "comment_body": "```java\r\n        List<Integer> offlineReplicas = new ArrayList<>(0);\r\n        for (var brokerId : partition.replicas) {\r\n            var broker = image.cluster().broker(brokerId);\r\n            if (broker == null || isReplicaOffline(partition, listenerName, broker))\r\n                offlineReplicas.add(brokerId);\r\n        }\r\n        return offlineReplicas;\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1813419731",
    "pr_number": 17584,
    "pr_file": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java",
    "created_at": "2024-10-23T19:29:03+00:00",
    "commented_code": "final Generation generationForOffsetRequest = generationIfStable();\n         if (pendingCommittedOffsetRequest != null &&\n-            !pendingCommittedOffsetRequest.sameRequest(partitions, generationForOffsetRequest)) {\n+            !pendingCommittedOffsetRequest.sameOrSubsetRequest(partitions, generationForOffsetRequest)) {",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "1813419731",
        "repo_full_name": "apache/kafka",
        "pr_number": 17584,
        "pr_file": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java",
        "discussion_id": "1813419731",
        "commented_code": "@@ -925,7 +925,7 @@ public Map<TopicPartition, OffsetAndMetadata> fetchCommittedOffsets(final Set<To\n \n         final Generation generationForOffsetRequest = generationIfStable();\n         if (pendingCommittedOffsetRequest != null &&\n-            !pendingCommittedOffsetRequest.sameRequest(partitions, generationForOffsetRequest)) {\n+            !pendingCommittedOffsetRequest.sameOrSubsetRequest(partitions, generationForOffsetRequest)) {",
        "comment_created_at": "2024-10-23T19:29:03+00:00",
        "comment_author": "chia7712",
        "comment_body": "If we allow reusing the previous commit when matching subsets, the returned `Map<TopicPartition, OffsetAndMetadata>` will contain unrelated topic partitions. Therefore, should we adjust the return map of `consumer#committed` to match the input?",
        "pr_file_module": null
      },
      {
        "comment_id": "1815974636",
        "repo_full_name": "apache/kafka",
        "pr_number": 17584,
        "pr_file": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java",
        "discussion_id": "1813419731",
        "commented_code": "@@ -925,7 +925,7 @@ public Map<TopicPartition, OffsetAndMetadata> fetchCommittedOffsets(final Set<To\n \n         final Generation generationForOffsetRequest = generationIfStable();\n         if (pendingCommittedOffsetRequest != null &&\n-            !pendingCommittedOffsetRequest.sameRequest(partitions, generationForOffsetRequest)) {\n+            !pendingCommittedOffsetRequest.sameOrSubsetRequest(partitions, generationForOffsetRequest)) {",
        "comment_created_at": "2024-10-25T04:22:13+00:00",
        "comment_author": "TaiJuWu",
        "comment_body": "Thanks for your remainder, updated.\r\n\r\n-- update --\r\nI filter the result in `ConsumerCoordinator#fetchCommittedOffsets` since I feel it is better.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1908440457",
    "pr_number": 17870,
    "pr_file": "core/src/main/java/kafka/server/share/DelayedShareFetch.java",
    "created_at": "2025-01-09T09:38:50+00:00",
    "commented_code": "LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData = new LinkedHashMap<>();\n \n         sharePartitions.forEach((topicIdPartition, sharePartition) -> {\n-            int partitionMaxBytes = shareFetch.partitionMaxBytes().getOrDefault(topicIdPartition, 0);\n             // Add the share partition to the list of partitions to be fetched only if we can\n             // acquire the fetch lock on it.\n             if (sharePartition.maybeAcquireFetchLock()) {\n                 try {\n                     // If the share partition is already at capacity, we should not attempt to fetch.\n                     if (sharePartition.canAcquireRecords()) {\n+                        // We do not know the total partitions that can be acquired at this stage, hence we set maxBytes\n+                        // to 0 for now and will update it before doing the replica manager fetch.\n                         topicPartitionData.put(\n                             topicIdPartition,\n                             new FetchRequest.PartitionData(\n                                 topicIdPartition.topicId(),\n                                 sharePartition.nextFetchOffset(),\n                                 0,\n-                                partitionMaxBytes,\n+                                0,",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "1908440457",
        "repo_full_name": "apache/kafka",
        "pr_number": 17870,
        "pr_file": "core/src/main/java/kafka/server/share/DelayedShareFetch.java",
        "discussion_id": "1908440457",
        "commented_code": "@@ -207,20 +212,21 @@ LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> acquirablePartitions\n         LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData = new LinkedHashMap<>();\n \n         sharePartitions.forEach((topicIdPartition, sharePartition) -> {\n-            int partitionMaxBytes = shareFetch.partitionMaxBytes().getOrDefault(topicIdPartition, 0);\n             // Add the share partition to the list of partitions to be fetched only if we can\n             // acquire the fetch lock on it.\n             if (sharePartition.maybeAcquireFetchLock()) {\n                 try {\n                     // If the share partition is already at capacity, we should not attempt to fetch.\n                     if (sharePartition.canAcquireRecords()) {\n+                        // We do not know the total partitions that can be acquired at this stage, hence we set maxBytes\n+                        // to 0 for now and will update it before doing the replica manager fetch.\n                         topicPartitionData.put(\n                             topicIdPartition,\n                             new FetchRequest.PartitionData(\n                                 topicIdPartition.topicId(),\n                                 sharePartition.nextFetchOffset(),\n                                 0,\n-                                partitionMaxBytes,\n+                                0,",
        "comment_created_at": "2025-01-09T09:38:50+00:00",
        "comment_author": "apoorvmittal10",
        "comment_body": "Do we require to create `FetchRequest.PartitionData` at this point or can delay the creation to later when we know the maxBytes?",
        "pr_file_module": null
      },
      {
        "comment_id": "1909105571",
        "repo_full_name": "apache/kafka",
        "pr_number": 17870,
        "pr_file": "core/src/main/java/kafka/server/share/DelayedShareFetch.java",
        "discussion_id": "1908440457",
        "commented_code": "@@ -207,20 +212,21 @@ LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> acquirablePartitions\n         LinkedHashMap<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData = new LinkedHashMap<>();\n \n         sharePartitions.forEach((topicIdPartition, sharePartition) -> {\n-            int partitionMaxBytes = shareFetch.partitionMaxBytes().getOrDefault(topicIdPartition, 0);\n             // Add the share partition to the list of partitions to be fetched only if we can\n             // acquire the fetch lock on it.\n             if (sharePartition.maybeAcquireFetchLock()) {\n                 try {\n                     // If the share partition is already at capacity, we should not attempt to fetch.\n                     if (sharePartition.canAcquireRecords()) {\n+                        // We do not know the total partitions that can be acquired at this stage, hence we set maxBytes\n+                        // to 0 for now and will update it before doing the replica manager fetch.\n                         topicPartitionData.put(\n                             topicIdPartition,\n                             new FetchRequest.PartitionData(\n                                 topicIdPartition.topicId(),\n                                 sharePartition.nextFetchOffset(),\n                                 0,\n-                                partitionMaxBytes,\n+                                0,",
        "comment_created_at": "2025-01-09T16:16:14+00:00",
        "comment_author": "adixitconfluent",
        "comment_body": "I have made the code change to delay the creation of `FetchRequest.PartitionData`. Thanks.",
        "pr_file_module": null
      }
    ]
  }
]