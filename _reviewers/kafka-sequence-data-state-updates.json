[
  {
    "discussion_id": "2190637748",
    "pr_number": 19762,
    "pr_file": "core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala",
    "created_at": "2025-07-07T16:56:49+00:00",
    "commented_code": "// recovery-from-unclean-shutdown if required.\n       logManager.startup(\n         metadataCache.getAllTopics().asScala,\n-        isStray = log => JLogManager.isStrayKraftReplica(brokerId, newImage.topics(), log)\n+        isStray = log => TopicsImage.isStrayReplica(newImage.topics(), brokerId, log.topicId(), log.topicPartition().partition(), log.toString)",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2190637748",
        "repo_full_name": "apache/kafka",
        "pr_number": 19762,
        "pr_file": "core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala",
        "discussion_id": "2190637748",
        "commented_code": "@@ -340,7 +339,7 @@ class BrokerMetadataPublisher(\n       // recovery-from-unclean-shutdown if required.\n       logManager.startup(\n         metadataCache.getAllTopics().asScala,\n-        isStray = log => JLogManager.isStrayKraftReplica(brokerId, newImage.topics(), log)\n+        isStray = log => TopicsImage.isStrayReplica(newImage.topics(), brokerId, log.topicId(), log.topicPartition().partition(), log.toString)",
        "comment_created_at": "2025-07-07T16:56:49+00:00",
        "comment_author": "jsancio",
        "comment_body": "Minor but to me stray partition are in the log manager not in the topics image. Meaning the log manager has partition entries that are not in the latest topics image.\r\n\r\nIn some sense the log manager understand topics image and makes sure that they match. The topics images doesn't know anything about \"stray partitions\" and the log manager. \r\n\r\nIf you still want to move the functionality TopicsImage maybe make it a method (not static) with `Stream<TopicIdPartition> deletedPartitionsForReplica(int brokerId, Stream<TopicIdPartition>)`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1407407105",
    "pr_number": 14849,
    "pr_file": "core/src/main/scala/kafka/coordinator/group/CoordinatorLoaderImpl.scala",
    "created_at": "2023-11-28T08:43:24+00:00",
    "commented_code": "}\n               }\n \n+              val currentHighWatermark = log.highWatermark\n+              if (currentHighWatermark > previousHighWatermark) {\n+                onHighWatermarkUpdated.accept(currentHighWatermark)\n+                previousHighWatermark = currentHighWatermark\n+              }",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "1407407105",
        "repo_full_name": "apache/kafka",
        "pr_number": 14849,
        "pr_file": "core/src/main/scala/kafka/coordinator/group/CoordinatorLoaderImpl.scala",
        "discussion_id": "1407407105",
        "commented_code": "@@ -148,6 +156,16 @@ class CoordinatorLoaderImpl[T](\n                 }\n               }\n \n+              val currentHighWatermark = log.highWatermark\n+              if (currentHighWatermark > previousHighWatermark) {\n+                onHighWatermarkUpdated.accept(currentHighWatermark)\n+                previousHighWatermark = currentHighWatermark\n+              }",
        "comment_created_at": "2023-11-28T08:43:24+00:00",
        "comment_author": "dajac",
        "comment_body": "I wonder if we should rather do this after updating the last written offset. At least, conceptually it makes more sense. What do you think?",
        "pr_file_module": null
      },
      {
        "comment_id": "1408072792",
        "repo_full_name": "apache/kafka",
        "pr_number": 14849,
        "pr_file": "core/src/main/scala/kafka/coordinator/group/CoordinatorLoaderImpl.scala",
        "discussion_id": "1407407105",
        "commented_code": "@@ -148,6 +156,16 @@ class CoordinatorLoaderImpl[T](\n                 }\n               }\n \n+              val currentHighWatermark = log.highWatermark\n+              if (currentHighWatermark > previousHighWatermark) {\n+                onHighWatermarkUpdated.accept(currentHighWatermark)\n+                previousHighWatermark = currentHighWatermark\n+              }",
        "comment_created_at": "2023-11-28T16:44:16+00:00",
        "comment_author": "jeffkbkim",
        "comment_body": "yeah, that makes sense to me since updating hwm also deletes snapshots",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2193545685",
    "pr_number": 20121,
    "pr_file": "core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala",
    "created_at": "2025-07-08T22:35:09+00:00",
    "commented_code": "server2.startup()\n     updateProducer()\n     // check if leader moves to the other server\n-    leader = awaitLeaderChange(servers, topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)\n+    leader = awaitLeaderChange(Seq(server2), topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2193545685",
        "repo_full_name": "apache/kafka",
        "pr_number": 20121,
        "pr_file": "core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala",
        "discussion_id": "2193545685",
        "commented_code": "@@ -215,7 +215,7 @@ class LogRecoveryTest extends QuorumTestHarness {\n     server2.startup()\n     updateProducer()\n     // check if leader moves to the other server\n-    leader = awaitLeaderChange(servers, topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)\n+    leader = awaitLeaderChange(Seq(server2), topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)",
        "comment_created_at": "2025-07-08T22:35:09+00:00",
        "comment_author": "junrao",
        "comment_body": "Thanks, but this doesn't seem to be the root cause. Note that `awaitLeaderChange()` will ignore server1 even if it's included in the brokers since it's the same as the old leader.\r\n\r\nI dug a bit deeper on this. The following is what I found.\r\n\r\nWhen the test passes, we have the following sequence. We fence broker 0 first, which causes 0 to be removed from ISR. We then fence broker 1. Since broker 1 is the last member in ISR. It's preserved. When broker 1 is restarted, it can be elected as the leader since it's in ISR.\r\n\r\n```\r\n[2025-07-08 18:09:24,746] INFO [QuorumController id=1000] Fencing broker 0 at epoch 9 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager:1700)\r\n[2025-07-08 18:09:24,747] INFO [QuorumController id=1000] handleBrokerFenced: changing partition(s): new-topic-0 : PartitionChangeRecord(partitionId=0, topicId=ygLg0ywdTcCWn2_pMjpvKg, isr=[1], leader=1, replicas=null, removingReplicas=null, addingReplicas=null, leaderRecoveryState=-1, directories=null, eligibleLeaderReplicas=null, lastKnownElr=null) (org.apache.kafka.controller.ReplicationControlManager:2057)\r\n\r\n[2025-07-08 18:09:24,758] INFO [QuorumController id=1000] Fencing broker 1 at epoch 11 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager:1700)\r\n[2025-07-08 18:09:24,759] INFO [QuorumController id=1000] handleBrokerFenced: changing partition(s): new-topic-0 : PartitionChangeRecord(partitionId=0, topicId=ygLg0ywdTcCWn2_pMjpvKg, isr=null, leader=-1, replicas=null, removingReplicas=null, addingReplicas=null, leaderRecoveryState=-1, directories=null, eligibleLeaderReplicas=null, lastKnownElr=null) (org.apache.kafka.controller.ReplicationControlManager:2057)\r\n\r\n[2025-07-08 18:09:26,145] INFO [QuorumController id=1000] The request from broker 1 to unfence has been granted because it has caught up with the offset of its register broker record 44. (org.apache.kafka.controller.BrokerHeartbeatManager:413)\r\n[2025-07-08 18:09:26,146] INFO [QuorumController id=1000] handleBrokerUnfenced: changing partition(s): new-topic-0 : PartitionChangeRecord(partitionId=0, topicId=ygLg0ywdTcCWn2_pMjpvKg, isr=null, leader=1, replicas=null, removingReplicas=null, addingReplicas=null, leaderRecoveryState=-1, directories=null, eligibleLeaderReplicas=null, lastKnownElr=null) (org.apache.kafka.controller.ReplicationControlManager:2057)\r\n```\r\n\r\nWhen the test fails, we have the following sequence. First, broker 1 is re-registered after starting. This causes broker 1 to be removed from ISR. We then fence broker 0. Since it's the last member in ISR, 0 is preserved. This leads to no leader since broker 1 is not in ISR and unclean leader election is disabled.\r\n\r\n```\r\n[2025-07-08 18:09:37,659] INFO [QuorumController id=1000] handleBrokerShutdown: changing partition(s): new-topic-0 : PartitionChangeRecord(partitionId=0, topicId=jAy5ExUJRHODDr8kttB-cA, isr=[0], leader=-2, replicas=null, removingReplicas=null, addingReplicas=null, leaderRecoveryState=-1, directories=null, eligibleLeaderReplicas=null, lastKnownElr=null) (org.apache.kafka.controller.ReplicationControlManager:2057)\r\n[2025-07-08 18:09:37,659] INFO [QuorumController id=1000] Registering a new incarnation of broker 1. Previous incarnation ID was wCXN8auETdmv29caNaaBAQ; new incarnation ID is In8FF4IGRjig4e5IgmwPVQ. Generated 1 record(s) to clean up previous incarnations. Broker epoch will become 34. (org.apache.kafka.controller.ClusterControlManager:444)\r\n\r\n[2025-07-08 18:09:38,012] INFO [QuorumController id=1000] Fencing broker 0 at epoch 9 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager:1700)\r\n[2025-07-08 18:09:38,013] INFO [QuorumController id=1000] handleBrokerFenced: changing partition(s): new-topic-0 : PartitionChangeRecord(partitionId=0, topicId=jAy5ExUJRHODDr8kttB-cA, isr=null, leader=-1, replicas=null, removingReplicas=null, addingReplicas=null, leaderRecoveryState=-1, directories=null, eligibleLeaderReplicas=null, lastKnownElr=null) (org.apache.kafka.controller.ReplicationControlManager:2057)\r\n```\r\n\r\nI tried to enable unclean leader election in the test. But the test still fails. Need to investigate further on that.",
        "pr_file_module": null
      },
      {
        "comment_id": "2196778751",
        "repo_full_name": "apache/kafka",
        "pr_number": 20121,
        "pr_file": "core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala",
        "discussion_id": "2193545685",
        "commented_code": "@@ -215,7 +215,7 @@ class LogRecoveryTest extends QuorumTestHarness {\n     server2.startup()\n     updateProducer()\n     // check if leader moves to the other server\n-    leader = awaitLeaderChange(servers, topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)\n+    leader = awaitLeaderChange(Seq(server2), topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)",
        "comment_created_at": "2025-07-10T06:57:40+00:00",
        "comment_author": "karuturi",
        "comment_body": "Thanks for the detailed comment. Just started on the codebase and getting my hands dirty. I will investigate on this and get back to you. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2198952441",
        "repo_full_name": "apache/kafka",
        "pr_number": 20121,
        "pr_file": "core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala",
        "discussion_id": "2193545685",
        "commented_code": "@@ -215,7 +215,7 @@ class LogRecoveryTest extends QuorumTestHarness {\n     server2.startup()\n     updateProducer()\n     // check if leader moves to the other server\n-    leader = awaitLeaderChange(servers, topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)\n+    leader = awaitLeaderChange(Seq(server2), topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)",
        "comment_created_at": "2025-07-10T23:02:43+00:00",
        "comment_author": "junrao",
        "comment_body": "@karuturi : I added the following to the test. It seems that fixes the issue. I was able to run the test for 400 iterations with no failure. Before, the test will fail within 50 iterations. Do you want to include that in your PR?\r\n\r\n```\r\n  override def kraftControllerConfigs(testInfo: TestInfo): Seq[Properties] = {\r\n    val properties = new Properties()\r\n    properties.put(ReplicationConfigs.UNCLEAN_LEADER_ELECTION_ENABLE_CONFIG, \"true\")\r\n    Seq(properties)\r\n  }\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2200524063",
        "repo_full_name": "apache/kafka",
        "pr_number": 20121,
        "pr_file": "core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala",
        "discussion_id": "2193545685",
        "commented_code": "@@ -215,7 +215,7 @@ class LogRecoveryTest extends QuorumTestHarness {\n     server2.startup()\n     updateProducer()\n     // check if leader moves to the other server\n-    leader = awaitLeaderChange(servers, topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)\n+    leader = awaitLeaderChange(Seq(server2), topicPartition, oldLeaderOpt = Some(leader), timeout = 30000L)",
        "comment_created_at": "2025-07-11T11:53:53+00:00",
        "comment_author": "karuturi",
        "comment_body": "Thanks @junrao I made the changes as suggested. Post fix, I ran 100 runs with no failure.",
        "pr_file_module": null
      }
    ]
  }
]