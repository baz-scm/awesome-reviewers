[
  {
    "discussion_id": "2190826625",
    "pr_number": 20087,
    "pr_file": "storage/src/main/java/org/apache/kafka/storage/internals/checkpoint/CleanShutdownFileHandler.java",
    "created_at": "2025-07-07T18:56:57+00:00",
    "commented_code": "Content content = Json.parseStringAs(text, Content.class);\n             return OptionalLong.of(content.brokerEpoch);\n         } catch (Exception e) {\n-            logger.debug(\"Fail to read the clean shutdown file in \" + cleanShutdownFile.toPath() + \":\" + e);\n+            logger.debug(\"Fail to read the clean shutdown file in {}:{}\", cleanShutdownFile.toPath(), e);",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2190826625",
        "repo_full_name": "apache/kafka",
        "pr_number": 20087,
        "pr_file": "storage/src/main/java/org/apache/kafka/storage/internals/checkpoint/CleanShutdownFileHandler.java",
        "discussion_id": "2190826625",
        "commented_code": "@@ -94,7 +94,7 @@ public OptionalLong read() {\n             Content content = Json.parseStringAs(text, Content.class);\n             return OptionalLong.of(content.brokerEpoch);\n         } catch (Exception e) {\n-            logger.debug(\"Fail to read the clean shutdown file in \" + cleanShutdownFile.toPath() + \":\" + e);\n+            logger.debug(\"Fail to read the clean shutdown file in {}:{}\", cleanShutdownFile.toPath(), e);",
        "comment_created_at": "2025-07-07T18:56:57+00:00",
        "comment_author": "chia7712",
        "comment_body": "the last parameter could be handled well if it is an exception object. Perhaps, we could use `{}` instead of `{}:{}`?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2217873563",
    "pr_number": 20087,
    "pr_file": "storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentFileset.java",
    "created_at": "2025-07-20T16:08:49+00:00",
    "commented_code": "public static boolean deleteQuietly(final File file) {\n         try {\n-            LOGGER.trace(\"Deleting \" + file.getAbsolutePath());\n+            LOGGER.trace(\"Deleting {}\", file.getAbsolutePath());\n             if (!file.exists()) {\n                 return true;\n             }\n             return file.delete();\n         } catch (final Exception e) {\n-            LOGGER.error(format(\"Encountered error while deleting %s\", file.getAbsolutePath()));\n+            LOGGER.error(\"Encountered error while deleting {}\", file.getAbsolutePath());",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2217873563",
        "repo_full_name": "apache/kafka",
        "pr_number": 20087,
        "pr_file": "storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentFileset.java",
        "discussion_id": "2217873563",
        "commented_code": "@@ -254,13 +254,13 @@ public static boolean deleteFilesOnly(final Collection<File> files) {\n \n     public static boolean deleteQuietly(final File file) {\n         try {\n-            LOGGER.trace(\"Deleting \" + file.getAbsolutePath());\n+            LOGGER.trace(\"Deleting {}\", file.getAbsolutePath());\n             if (!file.exists()) {\n                 return true;\n             }\n             return file.delete();\n         } catch (final Exception e) {\n-            LOGGER.error(format(\"Encountered error while deleting %s\", file.getAbsolutePath()));\n+            LOGGER.error(\"Encountered error while deleting {}\", file.getAbsolutePath());",
        "comment_created_at": "2025-07-20T16:08:49+00:00",
        "comment_author": "chia7712",
        "comment_body": "Perhaps we should add the `e`  to the logger\r\n```java\r\nLOGGER.error(\"Encountered error while deleting {}\", file.getAbsolutePath(), e);\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2207432260",
    "pr_number": 20089,
    "pr_file": "coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorLoaderImpl.java",
    "created_at": "2025-07-15T13:08:09+00:00",
    "commented_code": "+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.coordinator.common.runtime;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.NotLeaderOrFollowerException;\n+import org.apache.kafka.common.record.ControlRecordType;\n+import org.apache.kafka.common.record.FileRecords;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.MutableRecordBatch;\n+import org.apache.kafka.common.record.Record;\n+import org.apache.kafka.common.record.Records;\n+import org.apache.kafka.common.requests.TransactionResult;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.server.storage.log.FetchIsolation;\n+import org.apache.kafka.server.util.KafkaScheduler;\n+import org.apache.kafka.storage.internals.log.FetchDataInfo;\n+import org.apache.kafka.storage.internals.log.UnifiedLog;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Function;\n+\n+/**\n+ * Coordinator loader which reads records from a partition and replays them\n+ * to a group coordinator.\n+ *\n+ * @param <T> The record type.\n+ */\n+public class CoordinatorLoaderImpl<T> implements CoordinatorLoader<T> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(CoordinatorLoaderImpl.class);\n+\n+    private final Time time;\n+    private final Function<TopicPartition, Optional<UnifiedLog>> partitionLogSupplier;\n+    private final Function<TopicPartition, Optional<Long>> partitionLogEndOffsetSupplier;\n+    private final Deserializer<T> deserializer;\n+    private final int loadBufferSize;\n+\n+    private final AtomicBoolean isRunning = new AtomicBoolean(true);\n+    private final KafkaScheduler scheduler = new KafkaScheduler(1);\n+\n+    public CoordinatorLoaderImpl(\n+            Time time,\n+            Function<TopicPartition, Optional<UnifiedLog>> partitionLogSupplier,\n+            Function<TopicPartition, Optional<Long>> partitionLogEndOffsetSupplier,\n+            Deserializer<T> deserializer,\n+            int loadBufferSize\n+    ) {\n+        this.time = time;\n+        this.partitionLogSupplier = partitionLogSupplier;\n+        this.partitionLogEndOffsetSupplier = partitionLogEndOffsetSupplier;\n+        this.deserializer = deserializer;\n+        this.loadBufferSize = loadBufferSize;\n+        this.scheduler.startup();\n+    }\n+\n+    /**\n+     * Loads the coordinator by reading all the records from the TopicPartition\n+     * and applying them to the Replayable object.\n+     *\n+     * @param tp          The TopicPartition to read from.\n+     * @param coordinator The object to apply records to.\n+     */\n+    @Override\n+    public CompletableFuture<LoadSummary> load(TopicPartition tp, CoordinatorPlayback<T> coordinator) {\n+        final CompletableFuture<LoadSummary> future = new CompletableFuture<>();\n+        long startTimeMs = time.milliseconds();\n+        try {\n+            ScheduledFuture<?> result = scheduler.scheduleOnce(String.format(\"Load coordinator from %s\", tp),\n+                    () -> doLoad(tp, coordinator, future, startTimeMs));\n+            if (result.isCancelled()) {\n+                future.completeExceptionally(new RuntimeException(\"Coordinator loader is closed.\"));\n+            }\n+        } catch (Exception e) {\n+            future.completeExceptionally(e);\n+        }\n+        return future;\n+    }\n+\n+    private void doLoad(\n+            TopicPartition tp,\n+            CoordinatorPlayback<T> coordinator,\n+            CompletableFuture<LoadSummary> future,\n+            long startTimeMs\n+    ) {\n+        long schedulerQueueTimeMs = time.milliseconds() - startTimeMs;\n+        try {\n+            Optional<UnifiedLog> logOpt = partitionLogSupplier.apply(tp);\n+            if (logOpt.isEmpty()) {\n+                future.completeExceptionally(new NotLeaderOrFollowerException(\n+                        \"Could not load records from \" + tp + \" because the log does not exist.\"));\n+                return;\n+            }\n+\n+            UnifiedLog log = logOpt.get();\n+\n+            // Buffer may not be needed if records are read from memory.\n+            ByteBuffer buffer = ByteBuffer.allocate(0);\n+            long currentOffset = log.logStartOffset();\n+            LoadStats stats = new LoadStats();\n+\n+            long previousHighWatermark = -1L;\n+            while (shouldFetchNextBatch(currentOffset, logEndOffset(tp), stats.readAtLeastOneRecord)) {\n+                FetchDataInfo fetchDataInfo = log.read(currentOffset, loadBufferSize, FetchIsolation.LOG_END, true);\n+\n+                stats.readAtLeastOneRecord = fetchDataInfo.records.sizeInBytes() > 0;\n+\n+                MemoryRecords memoryRecords = toReadableMemoryRecords(tp, fetchDataInfo.records, buffer);\n+                if (fetchDataInfo.records instanceof FileRecords) {\n+                    buffer = memoryRecords.buffer();\n+                }\n+\n+                ReplayResult replayResult = processMemoryRecords(tp, log, memoryRecords, coordinator, stats, currentOffset, previousHighWatermark);\n+                currentOffset = replayResult.nextOffset;\n+                previousHighWatermark = replayResult.highWatermark;\n+            }\n+\n+            long endTimeMs = time.milliseconds();\n+\n+            if (logEndOffset(tp) == -1L) {\n+                future.completeExceptionally(new NotLeaderOrFollowerException(\n+                        String.format(\"Stopped loading records from %s because the partition is not online or is no longer the leader.\", tp)\n+                ));\n+            } else if (isRunning.get()) {\n+                future.complete(new LoadSummary(startTimeMs, endTimeMs, schedulerQueueTimeMs, stats.numRecords, stats.numBytes));\n+            } else {\n+                future.completeExceptionally(new RuntimeException(\"Coordinator loader is closed.\"));\n+            }\n+        } catch (Throwable ex) {\n+            future.completeExceptionally(ex);\n+        }\n+    }\n+\n+    private long logEndOffset(TopicPartition tp) {\n+        return partitionLogEndOffsetSupplier.apply(tp).orElse(-1L);\n+    }\n+\n+    /**\n+     * Returns true if it's still valid to fetch the next batch of records.\n+     * <p>\n+     * This method ensures fetching continues only under safe and meaningful conditions:\n+     * <ul>\n+     * <li>The current offset is less than the log end offset.</li>\n+     * <li>At least one record was read in the previous fetch. This ensures that fetching stops even if the\n+     * current offset remains smaller than the log end offset but the log is empty. This could happen with compacted topics.</li>\n+     * <li>The log end offset is not -1L, which ensures the partition is online and is still the leader.</li>\n+     * <li>The loader is still running.</li>\n+     * </ul>\n+     */\n+    private boolean shouldFetchNextBatch(long currentOffset, long logEndOffset, boolean readAtLeastOneRecord) {\n+        return currentOffset < logEndOffset && readAtLeastOneRecord && isRunning.get();\n+    }\n+\n+    private MemoryRecords toReadableMemoryRecords(TopicPartition tp, Records records, ByteBuffer buffer) throws IOException {\n+        if (records instanceof MemoryRecords memoryRecords) {\n+            return memoryRecords;\n+        } else if (records instanceof FileRecords fileRecords) {\n+            int sizeInBytes = fileRecords.sizeInBytes();\n+            int bytesNeeded = Math.max(loadBufferSize, sizeInBytes);\n+\n+            // \"minOneMessage = true in the above log.read() means that the buffer may need to\n+            // be grown to ensure progress can be made.\n+            if (buffer.capacity() < bytesNeeded) {\n+                if (loadBufferSize < bytesNeeded) {\n+                    LOG.warn(\"Loaded metadata from {} with buffer larger ({} bytes) than\" +\n+                            \" configured buffer size ({} bytes).\", tp, bytesNeeded, loadBufferSize);\n+                }\n+\n+                buffer = ByteBuffer.allocate(bytesNeeded);\n+            } else {\n+                buffer.clear();\n+            }\n+\n+            fileRecords.readInto(buffer, 0);\n+            return MemoryRecords.readableRecords(buffer);\n+        } else {\n+            throw new IllegalArgumentException(\"Unsupported record type: \" + records.getClass());\n+        }\n+    }\n+\n+    private ReplayResult processMemoryRecords(\n+            TopicPartition tp,\n+            UnifiedLog log,\n+            MemoryRecords memoryRecords,\n+            CoordinatorPlayback<T> coordinator,\n+            LoadStats loadStats,\n+            long currentOffset,\n+            long previousHighWatermark\n+    ) {\n+\n+        for (MutableRecordBatch batch : memoryRecords.batches()) {\n+            if (batch.isControlBatch()) {\n+                for (Record record : batch) {\n+                    ControlRecordType controlRecord = ControlRecordType.parse(record.key());\n+                    if (controlRecord == ControlRecordType.COMMIT) {\n+                        if (LOG.isTraceEnabled()) {\n+                            LOG.trace(\"Replaying end transaction marker from {} at offset {} to commit\" +\n+                                            \" transaction with producer id {} and producer epoch {}.\",\n+                                    tp, record.offset(), batch.producerId(), batch.producerEpoch());\n+                        }\n+                        coordinator.replayEndTransactionMarker(\n+                                batch.producerId(),\n+                                batch.producerEpoch(),\n+                                TransactionResult.COMMIT\n+                        );\n+                    } else if (controlRecord == ControlRecordType.ABORT) {\n+                        if (LOG.isTraceEnabled()) {\n+                            LOG.trace(\"Replaying end transaction marker from {} at offset {} to abort\" +\n+                                            \" transaction with producer id {} and producer epoch {}.\",\n+                                    tp, record.offset(), batch.producerId(), batch.producerEpoch());\n+                        }\n+                        coordinator.replayEndTransactionMarker(\n+                                batch.producerId(),\n+                                batch.producerEpoch(),\n+                                TransactionResult.ABORT\n+                        );\n+                    }\n+                }\n+            } else {\n+                for (Record record : batch) {\n+                    loadStats.numRecords++;\n+\n+                    Optional<T> coordinatorRecordOpt = Optional.empty();\n+                    try {\n+                        coordinatorRecordOpt = Optional.ofNullable(deserializer.deserialize(record.key(), record.value()));\n+                    } catch (Deserializer.UnknownRecordTypeException ex) {\n+                        LOG.warn(\"Unknown record type {} while loading offsets and group metadata from {}.\" +\n+                                \" Ignoring it. It could be a left over from an aborted upgrade.\", ex.unknownType(), tp);\n+                    } catch (RuntimeException ex) {\n+                        String msg = String.format(\"Deserializing record %s from %s failed due to: %s\", record, tp, ex.getMessage());\n+                        LOG.error(msg);\n+                        throw new RuntimeException(msg, ex);",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2207432260",
        "repo_full_name": "apache/kafka",
        "pr_number": 20089,
        "pr_file": "coordinator-common/src/main/java/org/apache/kafka/coordinator/common/runtime/CoordinatorLoaderImpl.java",
        "discussion_id": "2207432260",
        "commented_code": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.coordinator.common.runtime;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.NotLeaderOrFollowerException;\n+import org.apache.kafka.common.record.ControlRecordType;\n+import org.apache.kafka.common.record.FileRecords;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.MutableRecordBatch;\n+import org.apache.kafka.common.record.Record;\n+import org.apache.kafka.common.record.Records;\n+import org.apache.kafka.common.requests.TransactionResult;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.server.storage.log.FetchIsolation;\n+import org.apache.kafka.server.util.KafkaScheduler;\n+import org.apache.kafka.storage.internals.log.FetchDataInfo;\n+import org.apache.kafka.storage.internals.log.UnifiedLog;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Function;\n+\n+/**\n+ * Coordinator loader which reads records from a partition and replays them\n+ * to a group coordinator.\n+ *\n+ * @param <T> The record type.\n+ */\n+public class CoordinatorLoaderImpl<T> implements CoordinatorLoader<T> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(CoordinatorLoaderImpl.class);\n+\n+    private final Time time;\n+    private final Function<TopicPartition, Optional<UnifiedLog>> partitionLogSupplier;\n+    private final Function<TopicPartition, Optional<Long>> partitionLogEndOffsetSupplier;\n+    private final Deserializer<T> deserializer;\n+    private final int loadBufferSize;\n+\n+    private final AtomicBoolean isRunning = new AtomicBoolean(true);\n+    private final KafkaScheduler scheduler = new KafkaScheduler(1);\n+\n+    public CoordinatorLoaderImpl(\n+            Time time,\n+            Function<TopicPartition, Optional<UnifiedLog>> partitionLogSupplier,\n+            Function<TopicPartition, Optional<Long>> partitionLogEndOffsetSupplier,\n+            Deserializer<T> deserializer,\n+            int loadBufferSize\n+    ) {\n+        this.time = time;\n+        this.partitionLogSupplier = partitionLogSupplier;\n+        this.partitionLogEndOffsetSupplier = partitionLogEndOffsetSupplier;\n+        this.deserializer = deserializer;\n+        this.loadBufferSize = loadBufferSize;\n+        this.scheduler.startup();\n+    }\n+\n+    /**\n+     * Loads the coordinator by reading all the records from the TopicPartition\n+     * and applying them to the Replayable object.\n+     *\n+     * @param tp          The TopicPartition to read from.\n+     * @param coordinator The object to apply records to.\n+     */\n+    @Override\n+    public CompletableFuture<LoadSummary> load(TopicPartition tp, CoordinatorPlayback<T> coordinator) {\n+        final CompletableFuture<LoadSummary> future = new CompletableFuture<>();\n+        long startTimeMs = time.milliseconds();\n+        try {\n+            ScheduledFuture<?> result = scheduler.scheduleOnce(String.format(\"Load coordinator from %s\", tp),\n+                    () -> doLoad(tp, coordinator, future, startTimeMs));\n+            if (result.isCancelled()) {\n+                future.completeExceptionally(new RuntimeException(\"Coordinator loader is closed.\"));\n+            }\n+        } catch (Exception e) {\n+            future.completeExceptionally(e);\n+        }\n+        return future;\n+    }\n+\n+    private void doLoad(\n+            TopicPartition tp,\n+            CoordinatorPlayback<T> coordinator,\n+            CompletableFuture<LoadSummary> future,\n+            long startTimeMs\n+    ) {\n+        long schedulerQueueTimeMs = time.milliseconds() - startTimeMs;\n+        try {\n+            Optional<UnifiedLog> logOpt = partitionLogSupplier.apply(tp);\n+            if (logOpt.isEmpty()) {\n+                future.completeExceptionally(new NotLeaderOrFollowerException(\n+                        \"Could not load records from \" + tp + \" because the log does not exist.\"));\n+                return;\n+            }\n+\n+            UnifiedLog log = logOpt.get();\n+\n+            // Buffer may not be needed if records are read from memory.\n+            ByteBuffer buffer = ByteBuffer.allocate(0);\n+            long currentOffset = log.logStartOffset();\n+            LoadStats stats = new LoadStats();\n+\n+            long previousHighWatermark = -1L;\n+            while (shouldFetchNextBatch(currentOffset, logEndOffset(tp), stats.readAtLeastOneRecord)) {\n+                FetchDataInfo fetchDataInfo = log.read(currentOffset, loadBufferSize, FetchIsolation.LOG_END, true);\n+\n+                stats.readAtLeastOneRecord = fetchDataInfo.records.sizeInBytes() > 0;\n+\n+                MemoryRecords memoryRecords = toReadableMemoryRecords(tp, fetchDataInfo.records, buffer);\n+                if (fetchDataInfo.records instanceof FileRecords) {\n+                    buffer = memoryRecords.buffer();\n+                }\n+\n+                ReplayResult replayResult = processMemoryRecords(tp, log, memoryRecords, coordinator, stats, currentOffset, previousHighWatermark);\n+                currentOffset = replayResult.nextOffset;\n+                previousHighWatermark = replayResult.highWatermark;\n+            }\n+\n+            long endTimeMs = time.milliseconds();\n+\n+            if (logEndOffset(tp) == -1L) {\n+                future.completeExceptionally(new NotLeaderOrFollowerException(\n+                        String.format(\"Stopped loading records from %s because the partition is not online or is no longer the leader.\", tp)\n+                ));\n+            } else if (isRunning.get()) {\n+                future.complete(new LoadSummary(startTimeMs, endTimeMs, schedulerQueueTimeMs, stats.numRecords, stats.numBytes));\n+            } else {\n+                future.completeExceptionally(new RuntimeException(\"Coordinator loader is closed.\"));\n+            }\n+        } catch (Throwable ex) {\n+            future.completeExceptionally(ex);\n+        }\n+    }\n+\n+    private long logEndOffset(TopicPartition tp) {\n+        return partitionLogEndOffsetSupplier.apply(tp).orElse(-1L);\n+    }\n+\n+    /**\n+     * Returns true if it's still valid to fetch the next batch of records.\n+     * <p>\n+     * This method ensures fetching continues only under safe and meaningful conditions:\n+     * <ul>\n+     * <li>The current offset is less than the log end offset.</li>\n+     * <li>At least one record was read in the previous fetch. This ensures that fetching stops even if the\n+     * current offset remains smaller than the log end offset but the log is empty. This could happen with compacted topics.</li>\n+     * <li>The log end offset is not -1L, which ensures the partition is online and is still the leader.</li>\n+     * <li>The loader is still running.</li>\n+     * </ul>\n+     */\n+    private boolean shouldFetchNextBatch(long currentOffset, long logEndOffset, boolean readAtLeastOneRecord) {\n+        return currentOffset < logEndOffset && readAtLeastOneRecord && isRunning.get();\n+    }\n+\n+    private MemoryRecords toReadableMemoryRecords(TopicPartition tp, Records records, ByteBuffer buffer) throws IOException {\n+        if (records instanceof MemoryRecords memoryRecords) {\n+            return memoryRecords;\n+        } else if (records instanceof FileRecords fileRecords) {\n+            int sizeInBytes = fileRecords.sizeInBytes();\n+            int bytesNeeded = Math.max(loadBufferSize, sizeInBytes);\n+\n+            // \"minOneMessage = true in the above log.read() means that the buffer may need to\n+            // be grown to ensure progress can be made.\n+            if (buffer.capacity() < bytesNeeded) {\n+                if (loadBufferSize < bytesNeeded) {\n+                    LOG.warn(\"Loaded metadata from {} with buffer larger ({} bytes) than\" +\n+                            \" configured buffer size ({} bytes).\", tp, bytesNeeded, loadBufferSize);\n+                }\n+\n+                buffer = ByteBuffer.allocate(bytesNeeded);\n+            } else {\n+                buffer.clear();\n+            }\n+\n+            fileRecords.readInto(buffer, 0);\n+            return MemoryRecords.readableRecords(buffer);\n+        } else {\n+            throw new IllegalArgumentException(\"Unsupported record type: \" + records.getClass());\n+        }\n+    }\n+\n+    private ReplayResult processMemoryRecords(\n+            TopicPartition tp,\n+            UnifiedLog log,\n+            MemoryRecords memoryRecords,\n+            CoordinatorPlayback<T> coordinator,\n+            LoadStats loadStats,\n+            long currentOffset,\n+            long previousHighWatermark\n+    ) {\n+\n+        for (MutableRecordBatch batch : memoryRecords.batches()) {\n+            if (batch.isControlBatch()) {\n+                for (Record record : batch) {\n+                    ControlRecordType controlRecord = ControlRecordType.parse(record.key());\n+                    if (controlRecord == ControlRecordType.COMMIT) {\n+                        if (LOG.isTraceEnabled()) {\n+                            LOG.trace(\"Replaying end transaction marker from {} at offset {} to commit\" +\n+                                            \" transaction with producer id {} and producer epoch {}.\",\n+                                    tp, record.offset(), batch.producerId(), batch.producerEpoch());\n+                        }\n+                        coordinator.replayEndTransactionMarker(\n+                                batch.producerId(),\n+                                batch.producerEpoch(),\n+                                TransactionResult.COMMIT\n+                        );\n+                    } else if (controlRecord == ControlRecordType.ABORT) {\n+                        if (LOG.isTraceEnabled()) {\n+                            LOG.trace(\"Replaying end transaction marker from {} at offset {} to abort\" +\n+                                            \" transaction with producer id {} and producer epoch {}.\",\n+                                    tp, record.offset(), batch.producerId(), batch.producerEpoch());\n+                        }\n+                        coordinator.replayEndTransactionMarker(\n+                                batch.producerId(),\n+                                batch.producerEpoch(),\n+                                TransactionResult.ABORT\n+                        );\n+                    }\n+                }\n+            } else {\n+                for (Record record : batch) {\n+                    loadStats.numRecords++;\n+\n+                    Optional<T> coordinatorRecordOpt = Optional.empty();\n+                    try {\n+                        coordinatorRecordOpt = Optional.ofNullable(deserializer.deserialize(record.key(), record.value()));\n+                    } catch (Deserializer.UnknownRecordTypeException ex) {\n+                        LOG.warn(\"Unknown record type {} while loading offsets and group metadata from {}.\" +\n+                                \" Ignoring it. It could be a left over from an aborted upgrade.\", ex.unknownType(), tp);\n+                    } catch (RuntimeException ex) {\n+                        String msg = String.format(\"Deserializing record %s from %s failed due to: %s\", record, tp, ex.getMessage());\n+                        LOG.error(msg);\n+                        throw new RuntimeException(msg, ex);",
        "comment_created_at": "2025-07-15T13:08:09+00:00",
        "comment_author": "chia7712",
        "comment_body": "`LOG.error` could accept exception directly. Perhaps we could replace `due to: %s` by `LOG.error(msg, ex)`?",
        "pr_file_module": null
      }
    ]
  }
]