[
  {
    "discussion_id": "2228260726",
    "pr_number": 20235,
    "pr_file": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ResetConsumerGroupOffsetTest.java",
    "created_at": "2025-07-24T11:35:36+00:00",
    "commented_code": "}\n     }\n \n+    @ClusterTest(brokers = 5)",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2228260726",
        "repo_full_name": "apache/kafka",
        "pr_number": 20235,
        "pr_file": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ResetConsumerGroupOffsetTest.java",
        "discussion_id": "2228260726",
        "commented_code": "@@ -153,6 +154,26 @@ public void testResetOffsetsNotExistingGroup(ClusterInstance cluster) throws Exc\n         }\n     }\n \n+    @ClusterTest(brokers = 5)",
        "comment_created_at": "2025-07-24T11:35:36+00:00",
        "comment_author": "Yunyung",
        "comment_body": "I think we could just use 2 brokers for this test? 5 is overkill.",
        "pr_file_module": null
      },
      {
        "comment_id": "2228543903",
        "repo_full_name": "apache/kafka",
        "pr_number": 20235,
        "pr_file": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ResetConsumerGroupOffsetTest.java",
        "discussion_id": "2228260726",
        "commented_code": "@@ -153,6 +154,26 @@ public void testResetOffsetsNotExistingGroup(ClusterInstance cluster) throws Exc\n         }\n     }\n \n+    @ClusterTest(brokers = 5)",
        "comment_created_at": "2025-07-24T13:34:44+00:00",
        "comment_author": "TaiJuWu",
        "comment_body": "Thanks for review.\r\nI use 3 broker as default since we need to ensure `__consumere_offset` always have two brokers in any condition and we need to shutdown a broker.\r\n\r\nIf the replica of `__consumer_offset` is 1, we can't sure the leader is not shutdown broker.",
        "pr_file_module": null
      },
      {
        "comment_id": "2230496206",
        "repo_full_name": "apache/kafka",
        "pr_number": 20235,
        "pr_file": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ResetConsumerGroupOffsetTest.java",
        "discussion_id": "2228260726",
        "commented_code": "@@ -153,6 +154,26 @@ public void testResetOffsetsNotExistingGroup(ClusterInstance cluster) throws Exc\n         }\n     }\n \n+    @ClusterTest(brokers = 5)",
        "comment_created_at": "2025-07-25T08:26:30+00:00",
        "comment_author": "chia7712",
        "comment_body": "Can two brokers with two replicas fix the issue you mentioned?",
        "pr_file_module": null
      },
      {
        "comment_id": "2230581343",
        "repo_full_name": "apache/kafka",
        "pr_number": 20235,
        "pr_file": "tools/src/test/java/org/apache/kafka/tools/consumer/group/ResetConsumerGroupOffsetTest.java",
        "discussion_id": "2228260726",
        "commented_code": "@@ -153,6 +154,26 @@ public void testResetOffsetsNotExistingGroup(ClusterInstance cluster) throws Exc\n         }\n     }\n \n+    @ClusterTest(brokers = 5)",
        "comment_created_at": "2025-07-25T09:06:50+00:00",
        "comment_author": "TaiJuWu",
        "comment_body": "Change to two brokers and explicitly define min_insyn_replica equals one.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2076088627",
    "pr_number": 19589,
    "pr_file": "raft/src/main/java/org/apache/kafka/raft/QuorumConfig.java",
    "created_at": "2025-05-06T19:06:01+00:00",
    "commented_code": "public static final String QUORUM_RETRY_BACKOFF_MS_DOC = CommonClientConfigs.RETRY_BACKOFF_MS_DOC;\n     public static final int DEFAULT_QUORUM_RETRY_BACKOFF_MS = 20;\n \n+    public static final String QUORUM_AUTO_JOIN_ENABLE = QUORUM_PREFIX + \"auto.join.enable\";\n+    public static final String QUORUM_AUTO_JOIN_ENABLE_DOC = \"Controls whether a KRaft controller should automatically \" +\n+        \"join the cluster metadata partition for its cluster id.\";\n+    public static final boolean DEFAULT_QUORUM_AUTO_JOIN_ENABLE = false;",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2076088627",
        "repo_full_name": "apache/kafka",
        "pr_number": 19589,
        "pr_file": "raft/src/main/java/org/apache/kafka/raft/QuorumConfig.java",
        "discussion_id": "2076088627",
        "commented_code": "@@ -99,6 +100,11 @@ public class QuorumConfig {\n     public static final String QUORUM_RETRY_BACKOFF_MS_DOC = CommonClientConfigs.RETRY_BACKOFF_MS_DOC;\n     public static final int DEFAULT_QUORUM_RETRY_BACKOFF_MS = 20;\n \n+    public static final String QUORUM_AUTO_JOIN_ENABLE = QUORUM_PREFIX + \"auto.join.enable\";\n+    public static final String QUORUM_AUTO_JOIN_ENABLE_DOC = \"Controls whether a KRaft controller should automatically \" +\n+        \"join the cluster metadata partition for its cluster id.\";\n+    public static final boolean DEFAULT_QUORUM_AUTO_JOIN_ENABLE = false;",
        "comment_created_at": "2025-05-06T19:06:01+00:00",
        "comment_author": "jsancio",
        "comment_body": "`KafkaConfig` should validate that this is `true` only when `process.role` contains `controller`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172251635",
    "pr_number": 20021,
    "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
    "created_at": "2025-06-27T15:08:18+00:00",
    "commented_code": "}\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2172251635",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-06-27T15:08:18+00:00",
        "comment_author": "jsancio",
        "comment_body": "Hmm. Is the argument here that broker and controller registration would fail if there is a finalized feature version that this node doesn't know about?\r\n\r\nMeaning it is not possible for the cluster metadata partition to have a finalized feature version that this node doesn't know about.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172269289",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-06-27T15:17:51+00:00",
        "comment_author": "kevin-wu24",
        "comment_body": "> Is the argument here that broker and controller registration would fail if there is a finalized feature version that this node doesn't know about?\r\n\r\nYes, the registration fails if there is a finalized feature version the registering node does not support. `ClusterControlManager#processRegistrationFeature` has this code.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172330425",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-06-27T15:54:09+00:00",
        "comment_author": "chia7712",
        "comment_body": "Perhaps we should ensure the exposed metrics are always based on current image. The benefit is the exposed finalized versions will be consistent to current image, and we won't  need to use `minimumProduction` which puts us in a weird position\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2172456145",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-06-27T16:54:13+00:00",
        "comment_author": "jsancio",
        "comment_body": "Yeah. It would be nice to not make this assumption. The less assumptions you make the more resilient is the code to future changes. How about iterating through all of the features in the delta and only updating those values? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2172704788",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-06-27T19:14:00+00:00",
        "comment_author": "kevin-wu24",
        "comment_body": "I'm a bit confused. Doesn't this mean we won't have metrics for features that do not have a finalized level? From the discussion thread, it said:\r\n```\r\nAny missing finalized feature version can be configured to its minimum\r\nvalue.\r\n```\r\nwhich is why I made the change to iterate over all features instead of the ones in the image.\r\n\r\nEDIT: If the idea is to add a separate `maybeRegisterMissingFeatures` method, I don't see how that ends up any different than this code, since we'll have to loop over the `PRODUCTION_FEATURES` in that method instead.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172744552",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-06-27T19:36:30+00:00",
        "comment_author": "kevin-wu24",
        "comment_body": "> Perhaps we should ensure the exposed metrics are always based on current image. The benefit is the exposed finalized versions will be consistent to current image, and we won't need to use minimumProduction which puts us in a weird position\r\n\r\nIIUC, we should only expose the finalizedLevel metrics for features that actually have a finalized level? If the feature does not have a finalized level, it does not have a finalizedLevel metric? I'm okay with this approach, but I think we'll just need to update the KIP since the specific language is:\r\n```\r\nThe FinalizedLevel  metric will report the finalized feature level for each production feature. If the feature level is not set, the metric will return a value of 0, since that means the feature is not enabled. \r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2173227692",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-06-28T12:23:25+00:00",
        "comment_author": "chia7712",
        "comment_body": "> If the feature does not have a finalized level, it does not have a finalizedLevel metric?\n\nYes, that is what I meant",
        "pr_file_module": null
      },
      {
        "comment_id": "2175206091",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-06-30T14:24:37+00:00",
        "comment_author": "jsancio",
        "comment_body": "> IIUC, we should only expose the finalizedLevel metrics for features that actually have a finalized level? If the feature does not have a finalized level, it does not have a finalizedLevel metric?\r\n\r\nYes. Let's implement this definition. Sorry for the confusion and back and forth. Please feel free to update the KIP. This also means that if the finalized feature version is \"removed\" (set to 0), the code needs to remove the associated metric.",
        "pr_file_module": null
      },
      {
        "comment_id": "2193382168",
        "repo_full_name": "apache/kafka",
        "pr_number": 20021,
        "pr_file": "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
        "discussion_id": "2172251635",
        "commented_code": "@@ -345,7 +347,23 @@ private void maybePublishMetadata(MetadataDelta delta, MetadataImage image, Load\n             }\n         }\n         metrics.updateLastAppliedImageProvenance(image.provenance());\n-        metrics.setCurrentMetadataVersion(image.features().metadataVersionOrThrow());\n+        MetadataVersion metadataVersion = image.features().metadataVersionOrThrow();\n+        metrics.setCurrentMetadataVersion(metadataVersion);\n+\n+        // Set the metadata version feature level, since it is handled separately from other features\n+        metrics.recordFinalizedFeatureLevel(\n+            MetadataVersion.FEATURE_NAME,\n+            metadataVersion.featureLevel()\n+        );\n+\n+        // Set all production feature levels from the image, defaulting to their minimum production values\n+        for (var feature : Feature.PRODUCTION_FEATURES) {",
        "comment_created_at": "2025-07-08T20:25:19+00:00",
        "comment_author": "kevin-wu24",
        "comment_body": "> This also means that if the finalized feature version is \"removed\" (set to 0), the code needs to remove the associated metric.\r\n\r\nI guess this applies to all features besides metadata.version (whose minimum level is 7), and kraft.version (whose minimum is 0, but 0 does not mean that KRaft is \"disabled\" like other features). Since these two features are never part of the features image, I think their associated metrics should never be removed. Other features are removed from the image when their level is set to 0, so I think it makes sense to remove their metrics too. I'm going to update the KIP to document the exceptions for metadata + kraft version.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2038879355",
    "pr_number": 19306,
    "pr_file": "tools/src/main/java/org/apache/kafka/tools/DelegationTokenCommand.java",
    "created_at": "2025-04-11T06:29:44+00:00",
    "commented_code": "public void checkArgs() {\n             // check required args\n-            CommandLineUtils.checkRequiredArgs(parser, options, bootstrapServerOpt, commandConfigOpt);\n+            CommandLineUtils.checkRequiredArgs(parser, options, commandConfigOpt);",
    "repo_full_name": "apache/kafka",
    "discussion_comments": [
      {
        "comment_id": "2038879355",
        "repo_full_name": "apache/kafka",
        "pr_number": 19306,
        "pr_file": "tools/src/main/java/org/apache/kafka/tools/DelegationTokenCommand.java",
        "discussion_id": "2038879355",
        "commented_code": "@@ -284,7 +294,7 @@ public String hmac() {\n         \n         public void checkArgs() {\n             // check required args\n-            CommandLineUtils.checkRequiredArgs(parser, options, bootstrapServerOpt, commandConfigOpt);\n+            CommandLineUtils.checkRequiredArgs(parser, options, commandConfigOpt);",
        "comment_created_at": "2025-04-11T06:29:44+00:00",
        "comment_author": "johnny94",
        "comment_body": "Addressed https://github.com/apache/kafka/pull/19306/files#r2034702639 here.\r\nRemaining checks can be done by `CommandLineUtils.initializeBootstrapProperties`\r\n ",
        "pr_file_module": null
      }
    ]
  }
]