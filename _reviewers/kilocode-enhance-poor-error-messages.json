[
  {
    "discussion_id": "1994247423",
    "pr_number": 16,
    "pr_file": "src/api/providers/fireworks.ts",
    "created_at": "2025-03-13T20:17:12+00:00",
    "commented_code": "+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\n\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\n\" +\n+\t\t\t\t\t\"3. Enter your API key\n\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}",
    "repo_full_name": "Kilo-Org/kilocode",
    "discussion_comments": [
      {
        "comment_id": "1994247423",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994247423",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}",
        "comment_created_at": "2025-03-13T20:17:12+00:00",
        "comment_author": "janpaul123",
        "comment_body": "Why is this not present in the other providers? I see two possibilities: either those other providers crash out in a bad way when there's no API key set (in which case you should open a ticket with some screenshots to fix those), or they also have such error messages but this is not the right place.",
        "pr_file_module": null
      },
      {
        "comment_id": "1994976763",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994247423",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}",
        "comment_created_at": "2025-03-14T07:10:15+00:00",
        "comment_author": "ofou",
        "comment_body": "Other providers raise errors showing the raw response. In the case of Fireworks' [error codes](https://docs.fireworks.ai/troubleshooting/status_error_codes/inference_error_code#error-codes) are a bit ambiguous(when using OAI-compatible)\r\n\r\nCompare OAI:\r\n\r\n```json\r\n\"error\": {\r\n    \"message\": \"Incorrect API key provided: INVALID_KEY_HERE. You can find your API key at https://platform.openai.com/account/api-keys.\",\r\n    \"type\": \"invalid_request_error\",\r\n    \"param\": null,\r\n    \"code\": \"invalid_api_key\"\r\n}\r\n``` \r\n\r\nto Fireworks:\r\n```json\r\n{\"error\":\"unauthorized\"}\r\n```\r\n\r\nIt's a valid point, though. Should we standardize providers' errors?",
        "pr_file_module": null
      },
      {
        "comment_id": "1995280866",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994247423",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}",
        "comment_created_at": "2025-03-14T10:09:31+00:00",
        "comment_author": "janpaul123",
        "comment_body": "The Fireworks API responses being so terse is a good enough reason to keep this IMO. Maybe you can add a comment explaining this, e.g. `// Other providers provide these instructions in their JSON errors, but Fireworks doesn't, so we do it here.`",
        "pr_file_module": null
      },
      {
        "comment_id": "1995282109",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994247423",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}",
        "comment_created_at": "2025-03-14T10:10:32+00:00",
        "comment_author": "janpaul123",
        "comment_body": "> It's a valid point, though. Should we standardize providers' errors?\r\n\r\nIf the user experience is okay with showing errors in this way, then I don't think we need to standardize. If it's a bad experience, then please create a ticket with a screenshot showing how it's bad, and some ideas for how to improve.",
        "pr_file_module": null
      },
      {
        "comment_id": "1998325333",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994247423",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}",
        "comment_created_at": "2025-03-17T09:36:13+00:00",
        "comment_author": "kevinvandijk",
        "comment_body": "Adding on: in practice it's nearly impossible to reach this state so this should be fine. The configuration dialog does not let you save Fireworks as the provider when you don't also provide an API key. The only way to reach this state is if we somewhere in the code manually delete this api key.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1994257741",
    "pr_number": 16,
    "pr_file": "src/api/providers/fireworks.ts",
    "created_at": "2025-03-13T20:26:09+00:00",
    "commented_code": "+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\n\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\n\" +\n+\t\t\t\t\t\"3. Enter your API key\n\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Format the messages according to OpenAI format, with special handling for DeepSeek models\n+\t\tlet openAiMessages: OpenAI.Chat.ChatCompletionMessageParam[] = []\n+\t\tconst isDeepSeek = model.id.includes(\"deepseek\")\n+\n+\t\tif (isDeepSeek) {\n+\t\t\t// DeepSeek models handle system prompts differently\n+\t\t\tif (systemPrompt.trim() !== \"\") {\n+\t\t\t\topenAiMessages.push({ role: \"system\", content: systemPrompt })\n+\t\t\t}\n+\t\t\topenAiMessages = [...openAiMessages, ...convertToOpenAiMessages(messages)]\n+\t\t} else {\n+\t\t\t// Standard OpenAI format\n+\t\t\topenAiMessages = [{ role: \"system\", content: systemPrompt }, ...convertToOpenAiMessages(messages)]\n+\t\t}\n+\n+\t\t// Calculate token usage for prompt size limits\n+\t\tlet estimatedTokens = 0\n+\t\tconst contextWindow = model.info.contextWindow || 8191\n+\n+\t\ttry {\n+\t\t\tconst contentBlocks = openAiMessages.map((msg) => ({\n+\t\t\t\ttext: typeof msg.content === \"string\" ? msg.content : JSON.stringify(msg.content),\n+\t\t\t}))\n+\n+\t\t\testimatedTokens = await this.countTokens(contentBlocks as any)\n+\t\t} catch (error) {\n+\t\t\t// Silently handle token counting errors\n+\t\t}\n+\n+\t\t// Calculate max output tokens based on context window and input size\n+\t\tconst maxOutputTokens = Math.min(model.info.maxTokens || 4096, Math.max(100, contextWindow - estimatedTokens))\n+\n+\t\ttry {\n+\t\t\t// Prepare request parameters\n+\t\t\tconst requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n+\t\t\t\tmodel: model.id,\n+\t\t\t\tmessages: openAiMessages,\n+\t\t\t\tmax_tokens: maxOutputTokens,\n+\t\t\t\ttemperature: isDeepSeek ? 0.7 : 0.6, // DeepSeek works better with slightly higher temperature\n+\t\t\t\ttop_p: 1,\n+\t\t\t\tpresence_penalty: 0,\n+\t\t\t\tfrequency_penalty: 0,\n+\t\t\t\tstream: true,\n+\t\t\t\tstream_options: { include_usage: true },\n+\t\t\t}\n+\n+\t\t\t// Send the request\n+\t\t\tconst stream = await this.client.chat.completions.create(requestOptions)\n+\n+\t\t\t// Process the streaming response\n+\t\t\tlet contentStarted = false\n+\t\t\tlet accumulatedText = \"\"\n+\n+\t\t\tfor await (const chunk of stream) {\n+\t\t\t\tconst delta = chunk.choices[0]?.delta\n+\t\t\t\tif (delta?.content) {\n+\t\t\t\t\tcontentStarted = true\n+\t\t\t\t\taccumulatedText += delta.content\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\t\ttext: delta.content,\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// Process usage information if available\n+\t\t\t\tif (chunk.usage) {\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"usage\",\n+\t\t\t\t\t\tinputTokens: chunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\toutputTokens: chunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\ttotalCost: calculateApiCostOpenAI(\n+\t\t\t\t\t\t\tmodel.info,\n+\t\t\t\t\t\t\tchunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\t\tchunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\t),\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Handle the case where no content was received\n+\t\t\tif (!contentStarted) {\n+\t\t\t\tyield {\n+\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\ttext: \"No content was received from the model. This could be an issue with the API or the model.\",\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} catch (error: any) {\n+\t\t\t// Handle specific error cases with user-friendly messages\n+\t\t\tif (error.status === 401) {\n+\t\t\t\tyield {",
    "repo_full_name": "Kilo-Org/kilocode",
    "discussion_comments": [
      {
        "comment_id": "1994257741",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994257741",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Format the messages according to OpenAI format, with special handling for DeepSeek models\n+\t\tlet openAiMessages: OpenAI.Chat.ChatCompletionMessageParam[] = []\n+\t\tconst isDeepSeek = model.id.includes(\"deepseek\")\n+\n+\t\tif (isDeepSeek) {\n+\t\t\t// DeepSeek models handle system prompts differently\n+\t\t\tif (systemPrompt.trim() !== \"\") {\n+\t\t\t\topenAiMessages.push({ role: \"system\", content: systemPrompt })\n+\t\t\t}\n+\t\t\topenAiMessages = [...openAiMessages, ...convertToOpenAiMessages(messages)]\n+\t\t} else {\n+\t\t\t// Standard OpenAI format\n+\t\t\topenAiMessages = [{ role: \"system\", content: systemPrompt }, ...convertToOpenAiMessages(messages)]\n+\t\t}\n+\n+\t\t// Calculate token usage for prompt size limits\n+\t\tlet estimatedTokens = 0\n+\t\tconst contextWindow = model.info.contextWindow || 8191\n+\n+\t\ttry {\n+\t\t\tconst contentBlocks = openAiMessages.map((msg) => ({\n+\t\t\t\ttext: typeof msg.content === \"string\" ? msg.content : JSON.stringify(msg.content),\n+\t\t\t}))\n+\n+\t\t\testimatedTokens = await this.countTokens(contentBlocks as any)\n+\t\t} catch (error) {\n+\t\t\t// Silently handle token counting errors\n+\t\t}\n+\n+\t\t// Calculate max output tokens based on context window and input size\n+\t\tconst maxOutputTokens = Math.min(model.info.maxTokens || 4096, Math.max(100, contextWindow - estimatedTokens))\n+\n+\t\ttry {\n+\t\t\t// Prepare request parameters\n+\t\t\tconst requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n+\t\t\t\tmodel: model.id,\n+\t\t\t\tmessages: openAiMessages,\n+\t\t\t\tmax_tokens: maxOutputTokens,\n+\t\t\t\ttemperature: isDeepSeek ? 0.7 : 0.6, // DeepSeek works better with slightly higher temperature\n+\t\t\t\ttop_p: 1,\n+\t\t\t\tpresence_penalty: 0,\n+\t\t\t\tfrequency_penalty: 0,\n+\t\t\t\tstream: true,\n+\t\t\t\tstream_options: { include_usage: true },\n+\t\t\t}\n+\n+\t\t\t// Send the request\n+\t\t\tconst stream = await this.client.chat.completions.create(requestOptions)\n+\n+\t\t\t// Process the streaming response\n+\t\t\tlet contentStarted = false\n+\t\t\tlet accumulatedText = \"\"\n+\n+\t\t\tfor await (const chunk of stream) {\n+\t\t\t\tconst delta = chunk.choices[0]?.delta\n+\t\t\t\tif (delta?.content) {\n+\t\t\t\t\tcontentStarted = true\n+\t\t\t\t\taccumulatedText += delta.content\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\t\ttext: delta.content,\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// Process usage information if available\n+\t\t\t\tif (chunk.usage) {\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"usage\",\n+\t\t\t\t\t\tinputTokens: chunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\toutputTokens: chunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\ttotalCost: calculateApiCostOpenAI(\n+\t\t\t\t\t\t\tmodel.info,\n+\t\t\t\t\t\t\tchunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\t\tchunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\t),\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Handle the case where no content was received\n+\t\t\tif (!contentStarted) {\n+\t\t\t\tyield {\n+\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\ttext: \"No content was received from the model. This could be an issue with the API or the model.\",\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} catch (error: any) {\n+\t\t\t// Handle specific error cases with user-friendly messages\n+\t\t\tif (error.status === 401) {\n+\t\t\t\tyield {",
        "comment_created_at": "2025-03-13T20:26:09+00:00",
        "comment_author": "janpaul123",
        "comment_body": "Similar to my comment above about the API key, I don't see any of the other providers do this. Is this AI slop? What happens with those other providers if they hit e.g. a rate limit?",
        "pr_file_module": null
      },
      {
        "comment_id": "1994990946",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994257741",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Format the messages according to OpenAI format, with special handling for DeepSeek models\n+\t\tlet openAiMessages: OpenAI.Chat.ChatCompletionMessageParam[] = []\n+\t\tconst isDeepSeek = model.id.includes(\"deepseek\")\n+\n+\t\tif (isDeepSeek) {\n+\t\t\t// DeepSeek models handle system prompts differently\n+\t\t\tif (systemPrompt.trim() !== \"\") {\n+\t\t\t\topenAiMessages.push({ role: \"system\", content: systemPrompt })\n+\t\t\t}\n+\t\t\topenAiMessages = [...openAiMessages, ...convertToOpenAiMessages(messages)]\n+\t\t} else {\n+\t\t\t// Standard OpenAI format\n+\t\t\topenAiMessages = [{ role: \"system\", content: systemPrompt }, ...convertToOpenAiMessages(messages)]\n+\t\t}\n+\n+\t\t// Calculate token usage for prompt size limits\n+\t\tlet estimatedTokens = 0\n+\t\tconst contextWindow = model.info.contextWindow || 8191\n+\n+\t\ttry {\n+\t\t\tconst contentBlocks = openAiMessages.map((msg) => ({\n+\t\t\t\ttext: typeof msg.content === \"string\" ? msg.content : JSON.stringify(msg.content),\n+\t\t\t}))\n+\n+\t\t\testimatedTokens = await this.countTokens(contentBlocks as any)\n+\t\t} catch (error) {\n+\t\t\t// Silently handle token counting errors\n+\t\t}\n+\n+\t\t// Calculate max output tokens based on context window and input size\n+\t\tconst maxOutputTokens = Math.min(model.info.maxTokens || 4096, Math.max(100, contextWindow - estimatedTokens))\n+\n+\t\ttry {\n+\t\t\t// Prepare request parameters\n+\t\t\tconst requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n+\t\t\t\tmodel: model.id,\n+\t\t\t\tmessages: openAiMessages,\n+\t\t\t\tmax_tokens: maxOutputTokens,\n+\t\t\t\ttemperature: isDeepSeek ? 0.7 : 0.6, // DeepSeek works better with slightly higher temperature\n+\t\t\t\ttop_p: 1,\n+\t\t\t\tpresence_penalty: 0,\n+\t\t\t\tfrequency_penalty: 0,\n+\t\t\t\tstream: true,\n+\t\t\t\tstream_options: { include_usage: true },\n+\t\t\t}\n+\n+\t\t\t// Send the request\n+\t\t\tconst stream = await this.client.chat.completions.create(requestOptions)\n+\n+\t\t\t// Process the streaming response\n+\t\t\tlet contentStarted = false\n+\t\t\tlet accumulatedText = \"\"\n+\n+\t\t\tfor await (const chunk of stream) {\n+\t\t\t\tconst delta = chunk.choices[0]?.delta\n+\t\t\t\tif (delta?.content) {\n+\t\t\t\t\tcontentStarted = true\n+\t\t\t\t\taccumulatedText += delta.content\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\t\ttext: delta.content,\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// Process usage information if available\n+\t\t\t\tif (chunk.usage) {\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"usage\",\n+\t\t\t\t\t\tinputTokens: chunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\toutputTokens: chunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\ttotalCost: calculateApiCostOpenAI(\n+\t\t\t\t\t\t\tmodel.info,\n+\t\t\t\t\t\t\tchunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\t\tchunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\t),\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Handle the case where no content was received\n+\t\t\tif (!contentStarted) {\n+\t\t\t\tyield {\n+\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\ttext: \"No content was received from the model. This could be an issue with the API or the model.\",\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} catch (error: any) {\n+\t\t\t// Handle specific error cases with user-friendly messages\n+\t\t\tif (error.status === 401) {\n+\t\t\t\tyield {",
        "comment_created_at": "2025-03-14T07:22:01+00:00",
        "comment_author": "ofou",
        "comment_body": "see https://github.com/Kilo-Org/kilocode/pull/16#discussion_r1994247423, but I'd say that maybe using the native SDK from Fireworks (instead of OAI compatible) might be more verbose in the raw error responses, haven't checked yet. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1995555298",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994257741",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Format the messages according to OpenAI format, with special handling for DeepSeek models\n+\t\tlet openAiMessages: OpenAI.Chat.ChatCompletionMessageParam[] = []\n+\t\tconst isDeepSeek = model.id.includes(\"deepseek\")\n+\n+\t\tif (isDeepSeek) {\n+\t\t\t// DeepSeek models handle system prompts differently\n+\t\t\tif (systemPrompt.trim() !== \"\") {\n+\t\t\t\topenAiMessages.push({ role: \"system\", content: systemPrompt })\n+\t\t\t}\n+\t\t\topenAiMessages = [...openAiMessages, ...convertToOpenAiMessages(messages)]\n+\t\t} else {\n+\t\t\t// Standard OpenAI format\n+\t\t\topenAiMessages = [{ role: \"system\", content: systemPrompt }, ...convertToOpenAiMessages(messages)]\n+\t\t}\n+\n+\t\t// Calculate token usage for prompt size limits\n+\t\tlet estimatedTokens = 0\n+\t\tconst contextWindow = model.info.contextWindow || 8191\n+\n+\t\ttry {\n+\t\t\tconst contentBlocks = openAiMessages.map((msg) => ({\n+\t\t\t\ttext: typeof msg.content === \"string\" ? msg.content : JSON.stringify(msg.content),\n+\t\t\t}))\n+\n+\t\t\testimatedTokens = await this.countTokens(contentBlocks as any)\n+\t\t} catch (error) {\n+\t\t\t// Silently handle token counting errors\n+\t\t}\n+\n+\t\t// Calculate max output tokens based on context window and input size\n+\t\tconst maxOutputTokens = Math.min(model.info.maxTokens || 4096, Math.max(100, contextWindow - estimatedTokens))\n+\n+\t\ttry {\n+\t\t\t// Prepare request parameters\n+\t\t\tconst requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n+\t\t\t\tmodel: model.id,\n+\t\t\t\tmessages: openAiMessages,\n+\t\t\t\tmax_tokens: maxOutputTokens,\n+\t\t\t\ttemperature: isDeepSeek ? 0.7 : 0.6, // DeepSeek works better with slightly higher temperature\n+\t\t\t\ttop_p: 1,\n+\t\t\t\tpresence_penalty: 0,\n+\t\t\t\tfrequency_penalty: 0,\n+\t\t\t\tstream: true,\n+\t\t\t\tstream_options: { include_usage: true },\n+\t\t\t}\n+\n+\t\t\t// Send the request\n+\t\t\tconst stream = await this.client.chat.completions.create(requestOptions)\n+\n+\t\t\t// Process the streaming response\n+\t\t\tlet contentStarted = false\n+\t\t\tlet accumulatedText = \"\"\n+\n+\t\t\tfor await (const chunk of stream) {\n+\t\t\t\tconst delta = chunk.choices[0]?.delta\n+\t\t\t\tif (delta?.content) {\n+\t\t\t\t\tcontentStarted = true\n+\t\t\t\t\taccumulatedText += delta.content\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\t\ttext: delta.content,\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// Process usage information if available\n+\t\t\t\tif (chunk.usage) {\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"usage\",\n+\t\t\t\t\t\tinputTokens: chunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\toutputTokens: chunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\ttotalCost: calculateApiCostOpenAI(\n+\t\t\t\t\t\t\tmodel.info,\n+\t\t\t\t\t\t\tchunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\t\tchunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\t),\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Handle the case where no content was received\n+\t\t\tif (!contentStarted) {\n+\t\t\t\tyield {\n+\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\ttext: \"No content was received from the model. This could be an issue with the API or the model.\",\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} catch (error: any) {\n+\t\t\t// Handle specific error cases with user-friendly messages\n+\t\t\tif (error.status === 401) {\n+\t\t\t\tyield {",
        "comment_created_at": "2025-03-14T13:19:34+00:00",
        "comment_author": "janpaul123",
        "comment_body": "Actually this is a great point. I would be worth quickly checking if the SDK does this better. That would save us a bunch of custom code. You can just load the SDK in a nodejs REPL and see what errors it gives.",
        "pr_file_module": null
      },
      {
        "comment_id": "1998617429",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994257741",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Format the messages according to OpenAI format, with special handling for DeepSeek models\n+\t\tlet openAiMessages: OpenAI.Chat.ChatCompletionMessageParam[] = []\n+\t\tconst isDeepSeek = model.id.includes(\"deepseek\")\n+\n+\t\tif (isDeepSeek) {\n+\t\t\t// DeepSeek models handle system prompts differently\n+\t\t\tif (systemPrompt.trim() !== \"\") {\n+\t\t\t\topenAiMessages.push({ role: \"system\", content: systemPrompt })\n+\t\t\t}\n+\t\t\topenAiMessages = [...openAiMessages, ...convertToOpenAiMessages(messages)]\n+\t\t} else {\n+\t\t\t// Standard OpenAI format\n+\t\t\topenAiMessages = [{ role: \"system\", content: systemPrompt }, ...convertToOpenAiMessages(messages)]\n+\t\t}\n+\n+\t\t// Calculate token usage for prompt size limits\n+\t\tlet estimatedTokens = 0\n+\t\tconst contextWindow = model.info.contextWindow || 8191\n+\n+\t\ttry {\n+\t\t\tconst contentBlocks = openAiMessages.map((msg) => ({\n+\t\t\t\ttext: typeof msg.content === \"string\" ? msg.content : JSON.stringify(msg.content),\n+\t\t\t}))\n+\n+\t\t\testimatedTokens = await this.countTokens(contentBlocks as any)\n+\t\t} catch (error) {\n+\t\t\t// Silently handle token counting errors\n+\t\t}\n+\n+\t\t// Calculate max output tokens based on context window and input size\n+\t\tconst maxOutputTokens = Math.min(model.info.maxTokens || 4096, Math.max(100, contextWindow - estimatedTokens))\n+\n+\t\ttry {\n+\t\t\t// Prepare request parameters\n+\t\t\tconst requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n+\t\t\t\tmodel: model.id,\n+\t\t\t\tmessages: openAiMessages,\n+\t\t\t\tmax_tokens: maxOutputTokens,\n+\t\t\t\ttemperature: isDeepSeek ? 0.7 : 0.6, // DeepSeek works better with slightly higher temperature\n+\t\t\t\ttop_p: 1,\n+\t\t\t\tpresence_penalty: 0,\n+\t\t\t\tfrequency_penalty: 0,\n+\t\t\t\tstream: true,\n+\t\t\t\tstream_options: { include_usage: true },\n+\t\t\t}\n+\n+\t\t\t// Send the request\n+\t\t\tconst stream = await this.client.chat.completions.create(requestOptions)\n+\n+\t\t\t// Process the streaming response\n+\t\t\tlet contentStarted = false\n+\t\t\tlet accumulatedText = \"\"\n+\n+\t\t\tfor await (const chunk of stream) {\n+\t\t\t\tconst delta = chunk.choices[0]?.delta\n+\t\t\t\tif (delta?.content) {\n+\t\t\t\t\tcontentStarted = true\n+\t\t\t\t\taccumulatedText += delta.content\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\t\ttext: delta.content,\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// Process usage information if available\n+\t\t\t\tif (chunk.usage) {\n+\t\t\t\t\tyield {\n+\t\t\t\t\t\ttype: \"usage\",\n+\t\t\t\t\t\tinputTokens: chunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\toutputTokens: chunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\ttotalCost: calculateApiCostOpenAI(\n+\t\t\t\t\t\t\tmodel.info,\n+\t\t\t\t\t\t\tchunk.usage.prompt_tokens || 0,\n+\t\t\t\t\t\t\tchunk.usage.completion_tokens || 0,\n+\t\t\t\t\t\t),\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Handle the case where no content was received\n+\t\t\tif (!contentStarted) {\n+\t\t\t\tyield {\n+\t\t\t\t\ttype: \"text\",\n+\t\t\t\t\ttext: \"No content was received from the model. This could be an issue with the API or the model.\",\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} catch (error: any) {\n+\t\t\t// Handle specific error cases with user-friendly messages\n+\t\t\tif (error.status === 401) {\n+\t\t\t\tyield {",
        "comment_created_at": "2025-03-17T12:23:00+00:00",
        "comment_author": "ofou",
        "comment_body": "They use the same, so I think it's not worth replacing it by now\r\n\r\n```bash\r\n{\"error\":\"unauthorized\"}% \r\n```",
        "pr_file_module": null
      }
    ]
  }
]