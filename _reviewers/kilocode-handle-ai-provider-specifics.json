[
  {
    "discussion_id": "2256489681",
    "pr_number": 1784,
    "pr_file": "src/services/ghost/GhostStrategy.ts",
    "created_at": "2025-08-06T09:07:54+00:00",
    "commented_code": "\")}\n `\n \t}\n \n+\tprivate safeJsonParse(jsonContent: string): any {\n+\t\ttry {\n+\t\t\t// First, try direct parsing\n+\t\t\treturn JSON.parse(jsonContent)\n+\t\t} catch (error) {\n+\t\t\t// If direct parsing fails, try to fix common issues with escaped content",
    "repo_full_name": "Kilo-Org/kilocode",
    "discussion_comments": [
      {
        "comment_id": "2256489681",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 1784,
        "pr_file": "src/services/ghost/GhostStrategy.ts",
        "discussion_id": "2256489681",
        "commented_code": "@@ -283,6 +284,121 @@ ${sections.filter(Boolean).join(\"\\n\\n\")}\n `\n \t}\n \n+\tprivate safeJsonParse(jsonContent: string): any {\n+\t\ttry {\n+\t\t\t// First, try direct parsing\n+\t\t\treturn JSON.parse(jsonContent)\n+\t\t} catch (error) {\n+\t\t\t// If direct parsing fails, try to fix common issues with escaped content",
        "comment_created_at": "2025-08-06T09:07:54+00:00",
        "comment_author": "chrarnoldus",
        "comment_body": "Have you checked if there's an existing function that does this? I think there might be.",
        "pr_file_module": null
      },
      {
        "comment_id": "2256733343",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 1784,
        "pr_file": "src/services/ghost/GhostStrategy.ts",
        "discussion_id": "2256489681",
        "commented_code": "@@ -283,6 +284,121 @@ ${sections.filter(Boolean).join(\"\\n\\n\")}\n `\n \t}\n \n+\tprivate safeJsonParse(jsonContent: string): any {\n+\t\ttry {\n+\t\t\t// First, try direct parsing\n+\t\t\treturn JSON.parse(jsonContent)\n+\t\t} catch (error) {\n+\t\t\t// If direct parsing fails, try to fix common issues with escaped content",
        "comment_created_at": "2025-08-06T10:34:26+00:00",
        "comment_author": "catrielmuller",
        "comment_body": "Yes, there is a similar function like safeJsonParse in the codebase. I found an existing shared utility function at src/shared/safeJsonParse.ts.\r\n\r\nComparison Summary:\r\n\r\nShared safeJsonParse() (20 lines):\r\n-Simple JSON parsing with basic error handling\r\n-Returns default value on parsing failure\r\n-Generic TypeScript support\r\n-Used across multiple files (combineCommandSequences.ts, gemini.ts, anthropic-vertex.ts)\r\n\r\nGhostStrategy safeJsonParse() (113 lines):\r\n-Complex JSON recovery system for malformed LLM responses\r\n-Advanced parsing with string state tracking and brace counting\r\n-Handles escaped content, multi-line spans, and structural issues\r\n-Two-tier error handling with content extraction and re-escaping\r\n-Specifically designed for parsing potentially broken JSON from AI models\r\n\r\nConclusion:\r\nBoth functions serve different purposes. The shared function is a general-purpose utility for standard JSON parsing, while your GhostStrategy implementation is a specialized tool for recovering malformed JSON from LLM responses. Your complex implementation should be kept as it handles edge cases the shared function cannot address.",
        "pr_file_module": null
      },
      {
        "comment_id": "2256744093",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 1784,
        "pr_file": "src/services/ghost/GhostStrategy.ts",
        "discussion_id": "2256489681",
        "commented_code": "@@ -283,6 +284,121 @@ ${sections.filter(Boolean).join(\"\\n\\n\")}\n `\n \t}\n \n+\tprivate safeJsonParse(jsonContent: string): any {\n+\t\ttry {\n+\t\t\t// First, try direct parsing\n+\t\t\treturn JSON.parse(jsonContent)\n+\t\t} catch (error) {\n+\t\t\t// If direct parsing fails, try to fix common issues with escaped content",
        "comment_created_at": "2025-08-06T10:39:39+00:00",
        "comment_author": "chrarnoldus",
        "comment_body": "That other function is similar in name only.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2247315078",
    "pr_number": 1666,
    "pr_file": "src/api/providers/cerebras.ts",
    "created_at": "2025-08-01T08:28:09+00:00",
    "commented_code": "? message.content\n \t\t\t\t\t\t\t.map((block) => {\n \t\t\t\t\t\t\t\tif (block.type === \"text\") {\n-\t\t\t\t\t\t\t\t\treturn block.text\n+\t\t\t\t\t\t\t\t\treturn filterThinkingTags(block.text)",
    "repo_full_name": "Kilo-Org/kilocode",
    "discussion_comments": [
      {
        "comment_id": "2247315078",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 1666,
        "pr_file": "src/api/providers/cerebras.ts",
        "discussion_id": "2247315078",
        "commented_code": "@@ -40,26 +45,26 @@ export class CerebrasHandler implements ApiHandler {\n \t\t\t\t\t? message.content\n \t\t\t\t\t\t\t.map((block) => {\n \t\t\t\t\t\t\t\tif (block.type === \"text\") {\n-\t\t\t\t\t\t\t\t\treturn block.text\n+\t\t\t\t\t\t\t\t\treturn filterThinkingTags(block.text)",
        "comment_created_at": "2025-08-01T08:28:09+00:00",
        "comment_author": "chrarnoldus",
        "comment_body": "Why is this necessary?",
        "pr_file_module": null
      },
      {
        "comment_id": "2249178869",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 1666,
        "pr_file": "src/api/providers/cerebras.ts",
        "discussion_id": "2247315078",
        "commented_code": "@@ -40,26 +45,26 @@ export class CerebrasHandler implements ApiHandler {\n \t\t\t\t\t? message.content\n \t\t\t\t\t\t\t.map((block) => {\n \t\t\t\t\t\t\t\tif (block.type === \"text\") {\n-\t\t\t\t\t\t\t\t\treturn block.text\n+\t\t\t\t\t\t\t\t\treturn filterThinkingTags(block.text)",
        "comment_created_at": "2025-08-02T09:12:36+00:00",
        "comment_author": "chrarnoldus",
        "comment_body": "It's reminiscent of this pattern we have in a few places: https://github.com/Kilo-Org/kilocode/blob/a3276c0feab4300731d9294bbfc44c0bf85db98a/src/api/providers/openai.ts#L173",
        "pr_file_module": null
      },
      {
        "comment_id": "2249274274",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 1666,
        "pr_file": "src/api/providers/cerebras.ts",
        "discussion_id": "2247315078",
        "commented_code": "@@ -40,26 +45,26 @@ export class CerebrasHandler implements ApiHandler {\n \t\t\t\t\t? message.content\n \t\t\t\t\t\t\t.map((block) => {\n \t\t\t\t\t\t\t\tif (block.type === \"text\") {\n-\t\t\t\t\t\t\t\t\treturn block.text\n+\t\t\t\t\t\t\t\t\treturn filterThinkingTags(block.text)",
        "comment_created_at": "2025-08-02T15:01:07+00:00",
        "comment_author": "chrarnoldus",
        "comment_body": "https://github.com/RooCodeInc/Roo-Code/pull/6392/files#r2243281411",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1994253368",
    "pr_number": 16,
    "pr_file": "src/api/providers/fireworks.ts",
    "created_at": "2025-03-13T20:22:27+00:00",
    "commented_code": "+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\n\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\n\" +\n+\t\t\t\t\t\"3. Enter your API key\n\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Format the messages according to OpenAI format, with special handling for DeepSeek models\n+\t\tlet openAiMessages: OpenAI.Chat.ChatCompletionMessageParam[] = []\n+\t\tconst isDeepSeek = model.id.includes(\"deepseek\")\n+\n+\t\tif (isDeepSeek) {\n+\t\t\t// DeepSeek models handle system prompts differently\n+\t\t\tif (systemPrompt.trim() !== \"\") {\n+\t\t\t\topenAiMessages.push({ role: \"system\", content: systemPrompt })\n+\t\t\t}\n+\t\t\topenAiMessages = [...openAiMessages, ...convertToOpenAiMessages(messages)]\n+\t\t} else {\n+\t\t\t// Standard OpenAI format\n+\t\t\topenAiMessages = [{ role: \"system\", content: systemPrompt }, ...convertToOpenAiMessages(messages)]\n+\t\t}\n+\n+\t\t// Calculate token usage for prompt size limits\n+\t\tlet estimatedTokens = 0\n+\t\tconst contextWindow = model.info.contextWindow || 8191\n+\n+\t\ttry {\n+\t\t\tconst contentBlocks = openAiMessages.map((msg) => ({\n+\t\t\t\ttext: typeof msg.content === \"string\" ? msg.content : JSON.stringify(msg.content),\n+\t\t\t}))\n+\n+\t\t\testimatedTokens = await this.countTokens(contentBlocks as any)\n+\t\t} catch (error) {\n+\t\t\t// Silently handle token counting errors\n+\t\t}\n+\n+\t\t// Calculate max output tokens based on context window and input size\n+\t\tconst maxOutputTokens = Math.min(model.info.maxTokens || 4096, Math.max(100, contextWindow - estimatedTokens))\n+\n+\t\ttry {\n+\t\t\t// Prepare request parameters\n+\t\t\tconst requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n+\t\t\t\tmodel: model.id,\n+\t\t\t\tmessages: openAiMessages,\n+\t\t\t\tmax_tokens: maxOutputTokens,",
    "repo_full_name": "Kilo-Org/kilocode",
    "discussion_comments": [
      {
        "comment_id": "1994253368",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994253368",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Format the messages according to OpenAI format, with special handling for DeepSeek models\n+\t\tlet openAiMessages: OpenAI.Chat.ChatCompletionMessageParam[] = []\n+\t\tconst isDeepSeek = model.id.includes(\"deepseek\")\n+\n+\t\tif (isDeepSeek) {\n+\t\t\t// DeepSeek models handle system prompts differently\n+\t\t\tif (systemPrompt.trim() !== \"\") {\n+\t\t\t\topenAiMessages.push({ role: \"system\", content: systemPrompt })\n+\t\t\t}\n+\t\t\topenAiMessages = [...openAiMessages, ...convertToOpenAiMessages(messages)]\n+\t\t} else {\n+\t\t\t// Standard OpenAI format\n+\t\t\topenAiMessages = [{ role: \"system\", content: systemPrompt }, ...convertToOpenAiMessages(messages)]\n+\t\t}\n+\n+\t\t// Calculate token usage for prompt size limits\n+\t\tlet estimatedTokens = 0\n+\t\tconst contextWindow = model.info.contextWindow || 8191\n+\n+\t\ttry {\n+\t\t\tconst contentBlocks = openAiMessages.map((msg) => ({\n+\t\t\t\ttext: typeof msg.content === \"string\" ? msg.content : JSON.stringify(msg.content),\n+\t\t\t}))\n+\n+\t\t\testimatedTokens = await this.countTokens(contentBlocks as any)\n+\t\t} catch (error) {\n+\t\t\t// Silently handle token counting errors\n+\t\t}\n+\n+\t\t// Calculate max output tokens based on context window and input size\n+\t\tconst maxOutputTokens = Math.min(model.info.maxTokens || 4096, Math.max(100, contextWindow - estimatedTokens))\n+\n+\t\ttry {\n+\t\t\t// Prepare request parameters\n+\t\t\tconst requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n+\t\t\t\tmodel: model.id,\n+\t\t\t\tmessages: openAiMessages,\n+\t\t\t\tmax_tokens: maxOutputTokens,",
        "comment_created_at": "2025-03-13T20:22:27+00:00",
        "comment_author": "janpaul123",
        "comment_body": "Can you elaborate why this is necessary, given that I'm not seeing it in other providers? Except `includeMaxTokens` in `/src/api/providers/openai.ts`?",
        "pr_file_module": null
      },
      {
        "comment_id": "1995031400",
        "repo_full_name": "Kilo-Org/kilocode",
        "pr_number": 16,
        "pr_file": "src/api/providers/fireworks.ts",
        "discussion_id": "1994253368",
        "commented_code": "@@ -0,0 +1,222 @@\n+import { Anthropic } from \"@anthropic-ai/sdk\"\n+import OpenAI from \"openai\"\n+import { ApiHandler, SingleCompletionHandler } from \"../\"\n+import {\n+\tApiHandlerOptions,\n+\tFireworksModelId,\n+\tModelInfo,\n+\tfireworksDefaultModelId,\n+\tfireworksModels,\n+} from \"../../shared/api\"\n+import { calculateApiCostOpenAI } from \"../../utils/cost\"\n+import { convertToOpenAiMessages } from \"../transform/openai-format\"\n+import { ApiStream } from \"../transform/stream\"\n+import { BaseProvider } from \"./base-provider\"\n+\n+export class FireworksHandler extends BaseProvider implements ApiHandler, SingleCompletionHandler {\n+\tprivate options: ApiHandlerOptions\n+\tprivate client: OpenAI\n+\tprivate baseUrl: string = \"https://api.fireworks.ai/inference/v1\"\n+\n+\tconstructor(options: ApiHandlerOptions) {\n+\t\tsuper()\n+\t\tthis.options = options\n+\t\tthis.client = new OpenAI({\n+\t\t\tbaseURL: this.baseUrl,\n+\t\t\tapiKey: this.options.fireworksApiKey,\n+\t\t})\n+\t}\n+\n+\tasync *createMessage(systemPrompt: string, messages: Anthropic.Messages.MessageParam[]): ApiStream {\n+\t\tconst model = this.getModel()\n+\n+\t\tif (!this.options.fireworksApiKey) {\n+\t\t\tyield {\n+\t\t\t\ttype: \"text\",\n+\t\t\t\ttext:\n+\t\t\t\t\t\"ERROR: Fireworks API key is required but was not provided.\\n\\n\" +\n+\t\t\t\t\t\"Please set your API key in the extension settings:\\n\" +\n+\t\t\t\t\t\"1. Open the KiloCode settings panel\\n\" +\n+\t\t\t\t\t\"2. Select 'Fireworks' as your provider\\n\" +\n+\t\t\t\t\t\"3. Enter your API key\\n\\n\" +\n+\t\t\t\t\t\"You can get your API key from: https://fireworks.ai/account/api-keys\",\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Format the messages according to OpenAI format, with special handling for DeepSeek models\n+\t\tlet openAiMessages: OpenAI.Chat.ChatCompletionMessageParam[] = []\n+\t\tconst isDeepSeek = model.id.includes(\"deepseek\")\n+\n+\t\tif (isDeepSeek) {\n+\t\t\t// DeepSeek models handle system prompts differently\n+\t\t\tif (systemPrompt.trim() !== \"\") {\n+\t\t\t\topenAiMessages.push({ role: \"system\", content: systemPrompt })\n+\t\t\t}\n+\t\t\topenAiMessages = [...openAiMessages, ...convertToOpenAiMessages(messages)]\n+\t\t} else {\n+\t\t\t// Standard OpenAI format\n+\t\t\topenAiMessages = [{ role: \"system\", content: systemPrompt }, ...convertToOpenAiMessages(messages)]\n+\t\t}\n+\n+\t\t// Calculate token usage for prompt size limits\n+\t\tlet estimatedTokens = 0\n+\t\tconst contextWindow = model.info.contextWindow || 8191\n+\n+\t\ttry {\n+\t\t\tconst contentBlocks = openAiMessages.map((msg) => ({\n+\t\t\t\ttext: typeof msg.content === \"string\" ? msg.content : JSON.stringify(msg.content),\n+\t\t\t}))\n+\n+\t\t\testimatedTokens = await this.countTokens(contentBlocks as any)\n+\t\t} catch (error) {\n+\t\t\t// Silently handle token counting errors\n+\t\t}\n+\n+\t\t// Calculate max output tokens based on context window and input size\n+\t\tconst maxOutputTokens = Math.min(model.info.maxTokens || 4096, Math.max(100, contextWindow - estimatedTokens))\n+\n+\t\ttry {\n+\t\t\t// Prepare request parameters\n+\t\t\tconst requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming = {\n+\t\t\t\tmodel: model.id,\n+\t\t\t\tmessages: openAiMessages,\n+\t\t\t\tmax_tokens: maxOutputTokens,",
        "comment_created_at": "2025-03-14T07:46:49+00:00",
        "comment_author": "ofou",
        "comment_body": "Check Fireworks [playground](https://fireworks.ai/models/fireworks/deepseek-r1/playground), it shows that `max_tokens` is needed in params. Also, the amount of the `max_tokens` might truncate the output in `r1` if not calculated dynamically.\r\n\r\nUnder troubleshooting advice, Fireworks recommend: _\"Increase max_tokens to avoid truncation\". Fireworks API specifically expects this parameter to control output generation limits._\r\n",
        "pr_file_module": null
      }
    ]
  }
]