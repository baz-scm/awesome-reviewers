[
  {
    "discussion_id": "2204445080",
    "pr_number": 132522,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/allocateddevices.go",
    "created_at": "2025-07-14T10:06:11+00:00",
    "commented_code": "// Locking of the mutex gets minimized by pre-computing what needs to be done\n \t// without holding the lock.\n \tdeviceIDs := make([]structured.DeviceID, 0, 20)\n-\tforeachAllocatedDevice(claim, func(deviceID structured.DeviceID) {\n-\t\ta.logger.V(6).Info(\"Observed device allocation\", \"device\", deviceID, \"claim\", klog.KObj(claim))\n-\t\tdeviceIDs = append(deviceIDs, deviceID)\n-\t})\n+\tshareIDs := make([]structured.SharedDeviceID, 0, 20)\n+\tdeviceCapacities := make([]structured.DeviceConsumedCapacity, 0, 20)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2204445080",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/allocateddevices.go",
        "discussion_id": "2204445080",
        "commented_code": "@@ -142,16 +173,39 @@ func (a *allocatedDevices) addDevices(claim *resourceapi.ResourceClaim) {\n \t// Locking of the mutex gets minimized by pre-computing what needs to be done\n \t// without holding the lock.\n \tdeviceIDs := make([]structured.DeviceID, 0, 20)\n-\tforeachAllocatedDevice(claim, func(deviceID structured.DeviceID) {\n-\t\ta.logger.V(6).Info(\"Observed device allocation\", \"device\", deviceID, \"claim\", klog.KObj(claim))\n-\t\tdeviceIDs = append(deviceIDs, deviceID)\n-\t})\n+\tshareIDs := make([]structured.SharedDeviceID, 0, 20)\n+\tdeviceCapacities := make([]structured.DeviceConsumedCapacity, 0, 20)",
        "comment_created_at": "2025-07-14T10:06:11+00:00",
        "comment_author": "macsko",
        "comment_body": "Do we want to keep allocating all of them with 20 entries, even if the feature is disabled?",
        "pr_file_module": null
      },
      {
        "comment_id": "2206462354",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/allocateddevices.go",
        "discussion_id": "2204445080",
        "commented_code": "@@ -142,16 +173,39 @@ func (a *allocatedDevices) addDevices(claim *resourceapi.ResourceClaim) {\n \t// Locking of the mutex gets minimized by pre-computing what needs to be done\n \t// without holding the lock.\n \tdeviceIDs := make([]structured.DeviceID, 0, 20)\n-\tforeachAllocatedDevice(claim, func(deviceID structured.DeviceID) {\n-\t\ta.logger.V(6).Info(\"Observed device allocation\", \"device\", deviceID, \"claim\", klog.KObj(claim))\n-\t\tdeviceIDs = append(deviceIDs, deviceID)\n-\t})\n+\tshareIDs := make([]structured.SharedDeviceID, 0, 20)\n+\tdeviceCapacities := make([]structured.DeviceConsumedCapacity, 0, 20)",
        "comment_created_at": "2025-07-15T05:54:47+00:00",
        "comment_author": "sunya-ch",
        "comment_body": "I changed to nil define if the feature is not enabled (https://github.com/kubernetes/kubernetes/pull/132522/commits/51f6603e2a60894f060ab9cd5f69ed6a54a6ac10)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2224598892",
    "pr_number": 132522,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/allocateddevices.go",
    "created_at": "2025-07-23T07:01:45+00:00",
    "commented_code": "// Locking of the mutex gets minimized by pre-computing what needs to be done\n \t// without holding the lock.\n \tdeviceIDs := make([]structured.DeviceID, 0, 20)\n-\tforeachAllocatedDevice(claim, func(deviceID structured.DeviceID) {\n-\t\ta.logger.V(6).Info(\"Observed device deallocation\", \"device\", deviceID, \"claim\", klog.KObj(claim))\n-\t\tdeviceIDs = append(deviceIDs, deviceID)\n-\t})\n-\n+\tvar shareIDs []structured.SharedDeviceID\n+\tvar deviceCapacities []structured.DeviceConsumedCapacity\n+\tif a.enabledConsumableCapacity {\n+\t\tshareIDs = make([]structured.SharedDeviceID, 0, 20)\n+\t\tdeviceCapacities = make([]structured.DeviceConsumedCapacity, 0, 20)\n+\t}\n+\tforeachAllocatedDevice(claim,\n+\t\tfunc(deviceID structured.DeviceID) {\n+\t\t\ta.logger.V(6).Info(\"Observed device deallocation\", \"device\", deviceID, \"claim\", klog.KObj(claim))\n+\t\t\tdeviceIDs = append(deviceIDs, deviceID)\n+\t\t},\n+\t\ta.enabledConsumableCapacity,\n+\t\tfunc(sharedDeviceID structured.SharedDeviceID) {\n+\t\t\ta.logger.V(6).Info(\"Observed shared device deallocation\", \"shared device\", sharedDeviceID, \"claim\", klog.KObj(claim))\n+\t\t\tshareIDs = append(shareIDs, sharedDeviceID)\n+\t\t},\n+\t\tfunc(capacity structured.DeviceConsumedCapacity) {\n+\t\t\ta.logger.V(6).Info(\"Observed consumed capacity release\", \"device id\", capacity.DeviceID, \"consumed capacity\", capacity.ConsumedCapacity, \"claim\", klog.KObj(claim))\n+\t\t\tdeviceCapacities = append(deviceCapacities, capacity)\n+\t\t})\n \ta.mutex.Lock()\n \tdefer a.mutex.Unlock()\n \tfor _, deviceID := range deviceIDs {\n \t\ta.ids.Delete(deviceID)\n \t}\n+\tfor _, shareID := range shareIDs {\n+\t\ta.shareIDs.Delete(shareID)\n+\t}\n+\tfor _, capacity := range deviceCapacities {\n+\t\tdeviceID := capacity.DeviceID\n+\t\tif _, found := a.capacities[deviceID]; found {\n+\t\t\ta.capacities[deviceID].Sub(capacity.ConsumedCapacity)\n+\t\t\tif a.capacities[deviceID].Empty() {\n+\t\t\t\ta.capacities.Remove(capacity)\n+\t\t\t}\n+\t\t}",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2224598892",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/allocateddevices.go",
        "discussion_id": "2224598892",
        "commented_code": "@@ -162,14 +224,41 @@ func (a *allocatedDevices) removeDevices(claim *resourceapi.ResourceClaim) {\n \t// Locking of the mutex gets minimized by pre-computing what needs to be done\n \t// without holding the lock.\n \tdeviceIDs := make([]structured.DeviceID, 0, 20)\n-\tforeachAllocatedDevice(claim, func(deviceID structured.DeviceID) {\n-\t\ta.logger.V(6).Info(\"Observed device deallocation\", \"device\", deviceID, \"claim\", klog.KObj(claim))\n-\t\tdeviceIDs = append(deviceIDs, deviceID)\n-\t})\n-\n+\tvar shareIDs []structured.SharedDeviceID\n+\tvar deviceCapacities []structured.DeviceConsumedCapacity\n+\tif a.enabledConsumableCapacity {\n+\t\tshareIDs = make([]structured.SharedDeviceID, 0, 20)\n+\t\tdeviceCapacities = make([]structured.DeviceConsumedCapacity, 0, 20)\n+\t}\n+\tforeachAllocatedDevice(claim,\n+\t\tfunc(deviceID structured.DeviceID) {\n+\t\t\ta.logger.V(6).Info(\"Observed device deallocation\", \"device\", deviceID, \"claim\", klog.KObj(claim))\n+\t\t\tdeviceIDs = append(deviceIDs, deviceID)\n+\t\t},\n+\t\ta.enabledConsumableCapacity,\n+\t\tfunc(sharedDeviceID structured.SharedDeviceID) {\n+\t\t\ta.logger.V(6).Info(\"Observed shared device deallocation\", \"shared device\", sharedDeviceID, \"claim\", klog.KObj(claim))\n+\t\t\tshareIDs = append(shareIDs, sharedDeviceID)\n+\t\t},\n+\t\tfunc(capacity structured.DeviceConsumedCapacity) {\n+\t\t\ta.logger.V(6).Info(\"Observed consumed capacity release\", \"device id\", capacity.DeviceID, \"consumed capacity\", capacity.ConsumedCapacity, \"claim\", klog.KObj(claim))\n+\t\t\tdeviceCapacities = append(deviceCapacities, capacity)\n+\t\t})\n \ta.mutex.Lock()\n \tdefer a.mutex.Unlock()\n \tfor _, deviceID := range deviceIDs {\n \t\ta.ids.Delete(deviceID)\n \t}\n+\tfor _, shareID := range shareIDs {\n+\t\ta.shareIDs.Delete(shareID)\n+\t}\n+\tfor _, capacity := range deviceCapacities {\n+\t\tdeviceID := capacity.DeviceID\n+\t\tif _, found := a.capacities[deviceID]; found {\n+\t\t\ta.capacities[deviceID].Sub(capacity.ConsumedCapacity)\n+\t\t\tif a.capacities[deviceID].Empty() {\n+\t\t\t\ta.capacities.Remove(capacity)\n+\t\t\t}\n+\t\t}",
        "comment_created_at": "2025-07-23T07:01:45+00:00",
        "comment_author": "macsko",
        "comment_body": "Shouldn't it be just like this?\r\n```suggestion\r\n\t\ta.capacities.Remove(capacity)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2003087334",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-03-19T11:12:32+00:00",
    "commented_code": "return claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\treturn false, nil\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingFailureConditions {\n+\t\t\tif apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\treturn false, fmt.Errorf(\"claim %s failed to bind\", claim.Name)\n+\t\t\t}\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingConditions {\n+\t\t\tif !apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\treturn false, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// hasDeviceBindingStatus checks the binding status of devices within the\n+// given state claims.\n+func (pl *DynamicResources) hasDeviceBindingStatus(ctx context.Context, state *stateData, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor claimIndex, claim := range state.claims {\n+\t\tclaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).Get(ctx, claim.Name, metav1.GetOptions{})\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tstate.claims[claimIndex] = claim\n+\t\tbound, err := pl.isClaimBound(claim)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tif !bound {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, pod, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for binding conditions for device on node %s.\", nodeName)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2003087334",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "2003087334",
        "commented_code": "@@ -874,6 +937,65 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\treturn false, nil\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingFailureConditions {\n+\t\t\tif apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\treturn false, fmt.Errorf(\"claim %s failed to bind\", claim.Name)\n+\t\t\t}\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingConditions {\n+\t\t\tif !apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\treturn false, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// hasDeviceBindingStatus checks the binding status of devices within the\n+// given state claims.\n+func (pl *DynamicResources) hasDeviceBindingStatus(ctx context.Context, state *stateData, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor claimIndex, claim := range state.claims {\n+\t\tclaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).Get(ctx, claim.Name, metav1.GetOptions{})\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tstate.claims[claimIndex] = claim\n+\t\tbound, err := pl.isClaimBound(claim)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tif !bound {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, pod, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for binding conditions for device on node %s.\", nodeName)",
        "comment_created_at": "2025-03-19T11:12:32+00:00",
        "comment_author": "pohly",
        "comment_body": "This gets emitted every five second (polling interval in wait.PollUntilContextTimeout). That seems a bit excessive.\r\n\r\nWouldn't it be enough to emit once, when starting to wait?",
        "pr_file_module": null
      },
      {
        "comment_id": "2013304568",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "2003087334",
        "commented_code": "@@ -874,6 +937,65 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\treturn false, nil\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingFailureConditions {\n+\t\t\tif apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\treturn false, fmt.Errorf(\"claim %s failed to bind\", claim.Name)\n+\t\t\t}\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingConditions {\n+\t\t\tif !apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\treturn false, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// hasDeviceBindingStatus checks the binding status of devices within the\n+// given state claims.\n+func (pl *DynamicResources) hasDeviceBindingStatus(ctx context.Context, state *stateData, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor claimIndex, claim := range state.claims {\n+\t\tclaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).Get(ctx, claim.Name, metav1.GetOptions{})\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tstate.claims[claimIndex] = claim\n+\t\tbound, err := pl.isClaimBound(claim)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tif !bound {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, pod, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for binding conditions for device on node %s.\", nodeName)",
        "comment_created_at": "2025-03-26T02:48:19+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "Changes the Event to be emitted only when the wait begins.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2218695881",
    "pr_number": 130160,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/incubating/allocator_incubating.go",
    "created_at": "2025-07-21T09:55:35+00:00",
    "commented_code": "Device:      internal.id.Device.String(),\n \t\t\t\tAdminAccess: internal.adminAccess,\n \t\t\t}\n+\n+\t\t\t// If deviceBindingConditions are enabled, we need to populate the AllocatedDeviceStatus.\n+\t\t\tif a.features.DeviceBinding {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2218695881",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/incubating/allocator_incubating.go",
        "discussion_id": "2218695881",
        "commented_code": "@@ -293,6 +294,17 @@ func (a *Allocator) Allocate(ctx context.Context, node *v1.Node, claims []*resou\n \t\t\t\tDevice:      internal.id.Device.String(),\n \t\t\t\tAdminAccess: internal.adminAccess,\n \t\t\t}\n+\n+\t\t\t// If deviceBindingConditions are enabled, we need to populate the AllocatedDeviceStatus.\n+\t\t\tif a.features.DeviceBinding {",
        "comment_created_at": "2025-07-21T09:55:35+00:00",
        "comment_author": "pohly",
        "comment_body": "```suggestion\r\n\r\n\t\t\t// Performance optimization: skip the for loop if the feature is off.\r\n\t\t\t// Not needed for correctness because if the feature is off, the selected\r\n\t\t\t// device should not have binding conditions.\r\n\t\t\tif a.features.DeviceBinding {\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2210622949",
    "pr_number": 131357,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
    "created_at": "2025-07-16T14:36:18+00:00",
    "commented_code": "return false, nil\n \t}\n \n+\t// If the device is referencing mixins but the feature is not enabled, we can't\n+\t// select the device.\n+\tif !alloc.features.ResourceSliceMixins && isDeviceReferencingMixins(device) {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2210622949",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131357,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
        "discussion_id": "2210622949",
        "commented_code": "@@ -868,6 +868,19 @@ func (alloc *allocator) isSelectable(r requestIndices, requestData requestData,\n \t\treturn false, nil\n \t}\n \n+\t// If the device is referencing mixins but the feature is not enabled, we can't\n+\t// select the device.\n+\tif !alloc.features.ResourceSliceMixins && isDeviceReferencingMixins(device) {",
        "comment_created_at": "2025-07-16T14:36:18+00:00",
        "comment_author": "pohly",
        "comment_body": "Is this something that we can check once when gathering pools?\r\n\r\nJust an idea for a future optimization. As usual we first have to have it working, add a benchmark, then test whether it's worth it.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2210667069",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131357,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
        "discussion_id": "2210622949",
        "commented_code": "@@ -868,6 +868,19 @@ func (alloc *allocator) isSelectable(r requestIndices, requestData requestData,\n \t\treturn false, nil\n \t}\n \n+\t// If the device is referencing mixins but the feature is not enabled, we can't\n+\t// select the device.\n+\tif !alloc.features.ResourceSliceMixins && isDeviceReferencingMixins(device) {",
        "comment_created_at": "2025-07-16T14:53:12+00:00",
        "comment_author": "pohly",
        "comment_body": "Or do what I did originally and expand mixins when converting to the internal representation? Did you discard that approach out of concerns about memory consumption (probably higher than when keeping this disaggregated)?\r\n\r\nOne advantage of the conversion approach is that it's much easier to review because all support for mixins is in one place. We could start with that approach, ensure that we have a large number of unit tests, then do something more complicated.",
        "pr_file_module": null
      },
      {
        "comment_id": "2210921119",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131357,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
        "discussion_id": "2210622949",
        "commented_code": "@@ -868,6 +868,19 @@ func (alloc *allocator) isSelectable(r requestIndices, requestData requestData,\n \t\treturn false, nil\n \t}\n \n+\t// If the device is referencing mixins but the feature is not enabled, we can't\n+\t// select the device.\n+\tif !alloc.features.ResourceSliceMixins && isDeviceReferencingMixins(device) {",
        "comment_created_at": "2025-07-16T16:31:16+00:00",
        "comment_author": "mortent",
        "comment_body": "We could filter out devices when gathering the pools, but the drawback is that we will have to do this work for all devices up-front, which might lead to worse performance for situations where the allocator can get a match on one of the first devices attempted. I think we should look more into this, not just for this, but in general. Reducing the number of devices that the allocator needs to consider could help improve performance. But we need to measure it, since a change might improve performance in some situations and worsen it for other scenarios.\r\n\r\nSo the original idea was to do it during conversion, but concerns were raised in the KEP about the memory consumption at scale if we expand all devices. So the agreement then was to expand them on-demand.I think there are things we can do to improve the efficiency of how we manage the selectors, but I tried to avoid making this more complicated in the initial implementation.\r\n\r\nI'm uncertain about the idea of moving it into conversion for safety. It moves the logic outside of the allocator, which means we no longer get the benefits of the stable/incubating/experimental variants of the allocator. Also, since we will be aiming to promote this and the other features relatively quickly, we might ed up having to put the new implementation into stable or incubating rather than experimental.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2210650138",
    "pr_number": 131357,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
    "created_at": "2025-07-16T14:46:44+00:00",
    "commented_code": "return false, nil\n \t}\n \n+\t// If the device is referencing mixins but the feature is not enabled, we can't\n+\t// select the device.\n+\tif !alloc.features.ResourceSliceMixins && isDeviceReferencingMixins(device) {\n+\t\treturn false, nil\n+\t}\n+\n+\t// Create a map of only the DeviceMixins that are needed for the device so it can\n+\t// be provided to both calls to selectorsMatch.\n+\tdeviceMixins := make([]draapi.DeviceMixin, len(device.MixinRefs))\n+\tfor _, mixinRef := range device.MixinRefs {\n+\t\tdeviceMixins = append(deviceMixins, slice.Spec.Mixins.Device[mixinRef])\n+\t}",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2210650138",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131357,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
        "discussion_id": "2210650138",
        "commented_code": "@@ -868,6 +868,19 @@ func (alloc *allocator) isSelectable(r requestIndices, requestData requestData,\n \t\treturn false, nil\n \t}\n \n+\t// If the device is referencing mixins but the feature is not enabled, we can't\n+\t// select the device.\n+\tif !alloc.features.ResourceSliceMixins && isDeviceReferencingMixins(device) {\n+\t\treturn false, nil\n+\t}\n+\n+\t// Create a map of only the DeviceMixins that are needed for the device so it can\n+\t// be provided to both calls to selectorsMatch.\n+\tdeviceMixins := make([]draapi.DeviceMixin, len(device.MixinRefs))\n+\tfor _, mixinRef := range device.MixinRefs {\n+\t\tdeviceMixins = append(deviceMixins, slice.Spec.Mixins.Device[mixinRef])\n+\t}",
        "comment_created_at": "2025-07-16T14:46:44+00:00",
        "comment_author": "pohly",
        "comment_body": "Some handling of mixins is done here in all cases, some more inside `selectorsMatch`. The second part seems to be same work regardless of whether selectors from the class or from the request are checked. Was the motivation to only do more expensive work on demand?\r\n\r\nThen perhaps go one step further and do both (i.e. also this lookup of mixins here) only when `selectorsMatch` really needs it? You could create a helper struct which contains `device.MixinRefs`, `slice.Spec.Mixins.Device`, a field to cache the result, and then has a helper function which computes the result or returns the cached value. Pass that to `selectorsMatch` and then the overall work will only be done once.\r\n\r\nThat's also a bit easier to review because it's all in one place.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2211902361",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131357,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
        "discussion_id": "2210650138",
        "commented_code": "@@ -868,6 +868,19 @@ func (alloc *allocator) isSelectable(r requestIndices, requestData requestData,\n \t\treturn false, nil\n \t}\n \n+\t// If the device is referencing mixins but the feature is not enabled, we can't\n+\t// select the device.\n+\tif !alloc.features.ResourceSliceMixins && isDeviceReferencingMixins(device) {\n+\t\treturn false, nil\n+\t}\n+\n+\t// Create a map of only the DeviceMixins that are needed for the device so it can\n+\t// be provided to both calls to selectorsMatch.\n+\tdeviceMixins := make([]draapi.DeviceMixin, len(device.MixinRefs))\n+\tfor _, mixinRef := range device.MixinRefs {\n+\t\tdeviceMixins = append(deviceMixins, slice.Spec.Mixins.Device[mixinRef])\n+\t}",
        "comment_created_at": "2025-07-17T00:46:21+00:00",
        "comment_author": "mortent",
        "comment_body": "I've updated the code with your suggestion. I'm not completely happy about the naming for the helper struct, but I think the logic works out quite well. I've still left the final transformation in the CEL package, as that seems like it belongs there.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188674192",
    "pr_number": 132626,
    "pr_file": "pkg/apis/core/validation/validation.go",
    "created_at": "2025-07-06T22:29:31+00:00",
    "commented_code": "return allErrs\n }\n \n+// validateFileKeyRefVolume validates that volumes referenced by FileKeyRef environment variables\n+// are of type emptyDir. FileKeyRef requires emptyDir volumes to ensure proper file access.\n+func validateFileKeyRefVolume(spec *core.PodSpec, fldPath *field.Path) field.ErrorList {\n+\tallErrs := field.ErrorList{}\n+\n+\t// Collect all volume names referenced by FileKeyRef environment variables\n+\treferencedVolumeNames := sets.Set[string]{}\n+\tfor _, container := range spec.Containers {\n+\t\tfor _, env := range container.Env {\n+\t\t\tif env.ValueFrom != nil && env.ValueFrom.FileKeyRef != nil {\n+\t\t\t\treferencedVolumeNames.Insert(env.ValueFrom.FileKeyRef.VolumeName)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\texistingVolumeNames := sets.Set[string]{}\n+\tfor _, volume := range spec.Volumes {\n+\t\texistingVolumeNames.Insert(volume.Name)\n+\t}\n+\n+\t// Check if all referenced volumes exist\n+\tfor i, container := range spec.Containers {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2188674192",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132626,
        "pr_file": "pkg/apis/core/validation/validation.go",
        "discussion_id": "2188674192",
        "commented_code": "@@ -3682,6 +3701,57 @@ func validateHostUsers(spec *core.PodSpec, fldPath *field.Path) field.ErrorList\n \treturn allErrs\n }\n \n+// validateFileKeyRefVolume validates that volumes referenced by FileKeyRef environment variables\n+// are of type emptyDir. FileKeyRef requires emptyDir volumes to ensure proper file access.\n+func validateFileKeyRefVolume(spec *core.PodSpec, fldPath *field.Path) field.ErrorList {\n+\tallErrs := field.ErrorList{}\n+\n+\t// Collect all volume names referenced by FileKeyRef environment variables\n+\treferencedVolumeNames := sets.Set[string]{}\n+\tfor _, container := range spec.Containers {\n+\t\tfor _, env := range container.Env {\n+\t\t\tif env.ValueFrom != nil && env.ValueFrom.FileKeyRef != nil {\n+\t\t\t\treferencedVolumeNames.Insert(env.ValueFrom.FileKeyRef.VolumeName)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\texistingVolumeNames := sets.Set[string]{}\n+\tfor _, volume := range spec.Volumes {\n+\t\texistingVolumeNames.Insert(volume.Name)\n+\t}\n+\n+\t// Check if all referenced volumes exist\n+\tfor i, container := range spec.Containers {",
        "comment_created_at": "2025-07-06T22:29:31+00:00",
        "comment_author": "thockin",
        "comment_body": "Why do you have 2 loops iterating Containers and then env?  Can you merge the above loop here and add a comment why you are accumulating the referenced volumes?",
        "pr_file_module": null
      },
      {
        "comment_id": "2191249987",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132626,
        "pr_file": "pkg/apis/core/validation/validation.go",
        "discussion_id": "2188674192",
        "commented_code": "@@ -3682,6 +3701,57 @@ func validateHostUsers(spec *core.PodSpec, fldPath *field.Path) field.ErrorList\n \treturn allErrs\n }\n \n+// validateFileKeyRefVolume validates that volumes referenced by FileKeyRef environment variables\n+// are of type emptyDir. FileKeyRef requires emptyDir volumes to ensure proper file access.\n+func validateFileKeyRefVolume(spec *core.PodSpec, fldPath *field.Path) field.ErrorList {\n+\tallErrs := field.ErrorList{}\n+\n+\t// Collect all volume names referenced by FileKeyRef environment variables\n+\treferencedVolumeNames := sets.Set[string]{}\n+\tfor _, container := range spec.Containers {\n+\t\tfor _, env := range container.Env {\n+\t\t\tif env.ValueFrom != nil && env.ValueFrom.FileKeyRef != nil {\n+\t\t\t\treferencedVolumeNames.Insert(env.ValueFrom.FileKeyRef.VolumeName)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\texistingVolumeNames := sets.Set[string]{}\n+\tfor _, volume := range spec.Volumes {\n+\t\texistingVolumeNames.Insert(volume.Name)\n+\t}\n+\n+\t// Check if all referenced volumes exist\n+\tfor i, container := range spec.Containers {",
        "comment_created_at": "2025-07-08T00:38:04+00:00",
        "comment_author": "HirazawaUi",
        "comment_body": "LOL, upon closer inspection of this code, I must admit I'm not sure why I wrote such an ugly implementation back then. This code was implemented very early on, and during subsequent code reviews I focused on unit tests while overlooking this part.\r\n\r\nRegardless, this definitely needs refactoring. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2192823465",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132626,
        "pr_file": "pkg/apis/core/validation/validation.go",
        "discussion_id": "2188674192",
        "commented_code": "@@ -3682,6 +3701,57 @@ func validateHostUsers(spec *core.PodSpec, fldPath *field.Path) field.ErrorList\n \treturn allErrs\n }\n \n+// validateFileKeyRefVolume validates that volumes referenced by FileKeyRef environment variables\n+// are of type emptyDir. FileKeyRef requires emptyDir volumes to ensure proper file access.\n+func validateFileKeyRefVolume(spec *core.PodSpec, fldPath *field.Path) field.ErrorList {\n+\tallErrs := field.ErrorList{}\n+\n+\t// Collect all volume names referenced by FileKeyRef environment variables\n+\treferencedVolumeNames := sets.Set[string]{}\n+\tfor _, container := range spec.Containers {\n+\t\tfor _, env := range container.Env {\n+\t\t\tif env.ValueFrom != nil && env.ValueFrom.FileKeyRef != nil {\n+\t\t\t\treferencedVolumeNames.Insert(env.ValueFrom.FileKeyRef.VolumeName)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\texistingVolumeNames := sets.Set[string]{}\n+\tfor _, volume := range spec.Volumes {\n+\t\texistingVolumeNames.Insert(volume.Name)\n+\t}\n+\n+\t// Check if all referenced volumes exist\n+\tfor i, container := range spec.Containers {",
        "comment_created_at": "2025-07-08T15:22:19+00:00",
        "comment_author": "HirazawaUi",
        "comment_body": "I've completed the refactoring of this function. Using map here would be more efficient, as it only requires iterating through Volumes and Containers once to obtain the desired results.\r\n\r\nYou can review the latest refactored function. After completing the refactoring, it produces the following validation effects:\r\n\r\n  * If fileKeyRef references a non-existent volume:\r\n    * `The Pod \"dapi-test-static-pod\" is invalid: spec.containers[0].env[0].valueFrom.fileKeyRef.volumeName: Not found: \"config\"`\r\n  \r\n  * If fileKeyRef references a volume that is not of type emptyDir:\r\n    * `The Pod \"dapi-test-static-pod\" is invalid: spec.containers[0].env[0].valueFrom.fileKeyRef.volumeName: Invalid value: \"config\": referenced volume must be of type emptyDir`",
        "pr_file_module": null
      },
      {
        "comment_id": "2192949649",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132626,
        "pr_file": "pkg/apis/core/validation/validation.go",
        "discussion_id": "2188674192",
        "commented_code": "@@ -3682,6 +3701,57 @@ func validateHostUsers(spec *core.PodSpec, fldPath *field.Path) field.ErrorList\n \treturn allErrs\n }\n \n+// validateFileKeyRefVolume validates that volumes referenced by FileKeyRef environment variables\n+// are of type emptyDir. FileKeyRef requires emptyDir volumes to ensure proper file access.\n+func validateFileKeyRefVolume(spec *core.PodSpec, fldPath *field.Path) field.ErrorList {\n+\tallErrs := field.ErrorList{}\n+\n+\t// Collect all volume names referenced by FileKeyRef environment variables\n+\treferencedVolumeNames := sets.Set[string]{}\n+\tfor _, container := range spec.Containers {\n+\t\tfor _, env := range container.Env {\n+\t\t\tif env.ValueFrom != nil && env.ValueFrom.FileKeyRef != nil {\n+\t\t\t\treferencedVolumeNames.Insert(env.ValueFrom.FileKeyRef.VolumeName)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\texistingVolumeNames := sets.Set[string]{}\n+\tfor _, volume := range spec.Volumes {\n+\t\texistingVolumeNames.Insert(volume.Name)\n+\t}\n+\n+\t// Check if all referenced volumes exist\n+\tfor i, container := range spec.Containers {",
        "comment_created_at": "2025-07-08T16:25:22+00:00",
        "comment_author": "thockin",
        "comment_body": "using a map is very expensive, and doing direct lookups is almost always faster for small (< 100) sets of data.  Validation is run a lot.",
        "pr_file_module": null
      }
    ]
  }
]