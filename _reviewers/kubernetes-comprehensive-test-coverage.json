[
  {
    "discussion_id": "2187569042",
    "pr_number": 132522,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/cel/compile.go",
    "created_at": "2025-07-05T18:36:30+00:00",
    "commented_code": "capacity[domain].(map[string]apiservercel.Quantity)[id] = apiservercel.Quantity{Quantity: &cap.Value}\n \t}\n \n+\tmultiAlloc := false\n+\tif input.AllowMultipleAllocations != nil {\n+\t\tmultiAlloc = *input.AllowMultipleAllocations\n+\t}\n+\n \tvariables := map[string]any{\n \t\tdeviceVar: map[string]any{\n \t\t\tdriverVar:     input.Driver,\n+\t\t\tmultiAllocVar: multiAlloc,",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2187569042",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/cel/compile.go",
        "discussion_id": "2187569042",
        "commented_code": "@@ -250,9 +252,15 @@ func (c CompilationResult) DeviceMatches(ctx context.Context, input Device) (boo\n \t\tcapacity[domain].(map[string]apiservercel.Quantity)[id] = apiservercel.Quantity{Quantity: &cap.Value}\n \t}\n \n+\tmultiAlloc := false\n+\tif input.AllowMultipleAllocations != nil {\n+\t\tmultiAlloc = *input.AllowMultipleAllocations\n+\t}\n+\n \tvariables := map[string]any{\n \t\tdeviceVar: map[string]any{\n \t\t\tdriverVar:     input.Driver,\n+\t\t\tmultiAllocVar: multiAlloc,",
        "comment_created_at": "2025-07-05T18:36:30+00:00",
        "comment_author": "mortent",
        "comment_body": "We probably want unit tests for the CEL changes.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2187685078",
    "pr_number": 132522,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator_test.go",
    "created_at": "2025-07-05T20:33:02+00:00",
    "commented_code": "deviceAllocationResult(req1, driverA, pool1, device1, false),\n \t\t\t)},\n \t\t},\n+\t\t\"consumable-capacity-multi-allocatable-device-with-consumable-capacity\": {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2187685078",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator_test.go",
        "discussion_id": "2187685078",
        "commented_code": "@@ -3464,6 +3562,350 @@ func TestAllocator(t *testing.T) {\n \t\t\t\tdeviceAllocationResult(req1, driverA, pool1, device1, false),\n \t\t\t)},\n \t\t},\n+\t\t\"consumable-capacity-multi-allocatable-device-with-consumable-capacity\": {",
        "comment_created_at": "2025-07-05T20:33:02+00:00",
        "comment_author": "mortent",
        "comment_body": "Add some tests to verify that the consumable capacity feature works with the prioritized list feature.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2198984716",
    "pr_number": 132522,
    "pr_file": "pkg/registry/resource/resourceclaim/strategy_test.go",
    "created_at": "2025-07-10T23:23:05+00:00",
    "commented_code": "}\n \t\t\t},\n \t\t},\n+\t\t\"keep-existing-fields-consumable-capacity\": {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2198984716",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceclaim/strategy_test.go",
        "discussion_id": "2198984716",
        "commented_code": "@@ -587,6 +646,52 @@ func TestStrategyUpdate(t *testing.T) {\n \t\t\t\t}\n \t\t\t},\n \t\t},\n+\t\t\"keep-existing-fields-consumable-capacity\": {",
        "comment_created_at": "2025-07-10T23:23:05+00:00",
        "comment_author": "mortent",
        "comment_body": "I think we should have some additional test cases here:\r\n* Make sure that adding these fields when the feature is enabled works.\r\n* Make sure that existing fields remain even for updates with the feature off\r\n\r\nTake a look at the cases covered for the PrioritizedList feature.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2201212079",
    "pr_number": 132522,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator_test.go",
    "created_at": "2025-07-11T16:29:06+00:00",
    "commented_code": "deviceAllocationResult(req1, driverA, pool1, device1, false),\n \t\t\t)},\n \t\t},\n+\t\t\"consumable-capacity-multi-allocatable-device-with-consumable-capacity\": {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2201212079",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator_test.go",
        "discussion_id": "2201212079",
        "commented_code": "@@ -3464,6 +3562,382 @@ func TestAllocator(t *testing.T) {\n \t\t\t\tdeviceAllocationResult(req1, driverA, pool1, device1, false),\n \t\t\t)},\n \t\t},\n+\t\t\"consumable-capacity-multi-allocatable-device-with-consumable-capacity\": {",
        "comment_created_at": "2025-07-11T16:29:06+00:00",
        "comment_author": "mortent",
        "comment_body": "Do we have tests here that cover situations where the allocator will have to backtrack? I just want to make sure we have sufficient test coverage when the allocator updates the `allocatingCapacity` data structure, but then have to remove capacity from it since it needs to try a different set of devices. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2204326605",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator_test.go",
        "discussion_id": "2201212079",
        "commented_code": "@@ -3464,6 +3562,382 @@ func TestAllocator(t *testing.T) {\n \t\t\t\tdeviceAllocationResult(req1, driverA, pool1, device1, false),\n \t\t\t)},\n \t\t},\n+\t\t\"consumable-capacity-multi-allocatable-device-with-consumable-capacity\": {",
        "comment_created_at": "2025-07-14T09:18:12+00:00",
        "comment_author": "sunya-ch",
        "comment_body": "I added a new test case:\r\n`consumable-capacity-with-multi-allocatable-device-backtrack` having two claims. First claim has two requests (one per each) while the second claim request only one. The expected allocated devices are in reversed order. One has one capacity and the other has two. I added an attribute match constraint to make sure that the first claim will get the second device. Otherwise, it took one capacity for the first request from the first device and one capacity for the second request from the second device.\r\n\r\nhttps://github.com/kubernetes/kubernetes/pull/132522/commits/1577fd1e062a813882c650215384dd6c03ff9451",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2208796036",
    "pr_number": 132522,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/consumable_capacity_test.go",
    "created_at": "2025-07-15T22:02:41+00:00",
    "commented_code": "+/*\n+Copyright 2024 The Kubernetes Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package structured\n+\n+import (\n+\t\"testing\"\n+\n+\t. \"github.com/onsi/gomega\"\n+\tresourceapi \"k8s.io/api/resource/v1beta1\"\n+\t\"k8s.io/apimachinery/pkg/api/resource\"\n+\t\"k8s.io/utils/ptr\"\n+)\n+\n+var (\n+\tone   = resource.MustParse(\"1\")\n+\ttwo   = resource.MustParse(\"2\")\n+\tthree = resource.MustParse(\"3\")\n+)\n+\n+func deviceConsumedCapacity(deviceID DeviceID) DeviceConsumedCapacity {\n+\tcapaicty := map[resourceapi.QualifiedName]resource.Quantity{\n+\t\tcapacity0: one,\n+\t}\n+\treturn NewDeviceConsumedCapacity(deviceID, capaicty)\n+}\n+\n+func TestConsumedCapacity(t *testing.T) {\n+\tg := NewWithT(t)\n+\tallocatedCapacity := NewConsumedCapacity()\n+\tg.Expect(allocatedCapacity.Empty()).To(BeTrueBecause(\"allocated capacity should start from zero\"))\n+\toneAllocated := ConsumedCapacity{\n+\t\tcapacity0: &one,\n+\t}\n+\tallocatedCapacity.Add(oneAllocated)\n+\tg.Expect(allocatedCapacity.Empty()).To(BeFalseBecause(\"capacity is added\"))\n+\tallocatedCapacity.Sub(oneAllocated)\n+\tg.Expect(allocatedCapacity.Empty()).To(BeTrueBecause(\"capacity is subtracted to zero\"))\n+}\n+\n+func TestConsumedCapacityCollection(t *testing.T) {\n+\tg := NewWithT(t)\n+\tdeviceID := MakeDeviceID(driverA, pool1, device1)\n+\taggregatedCapacity := NewConsumedCapacityCollection()\n+\taggregatedCapacity.Insert(deviceConsumedCapacity(deviceID))\n+\taggregatedCapacity.Insert(deviceConsumedCapacity(deviceID))\n+\tallocatedCapacity, found := aggregatedCapacity[deviceID]\n+\tg.Expect(found).To(BeTrueBecause(\"expected deviceID to be found\"))\n+\tg.Expect(allocatedCapacity[capacity0].Cmp(two)).To(BeZero())\n+\taggregatedCapacity.Remove(deviceConsumedCapacity(deviceID))\n+\tg.Expect(allocatedCapacity[capacity0].Cmp(one)).To(BeZero())\n+}\n+\n+func TestViolateCapacitySharingPolicy(t *testing.T) {\n+\ttestcases := map[string]struct {\n+\t\trequestedVal resource.Quantity\n+\t\tconsumable   *resourceapi.CapacitySharingPolicy\n+\n+\t\texpectResult bool\n+\t}{\n+\t\t\"no constraint\": {one, nil, false},\n+\t\t\"less than maximum\": {\n+\t\t\tone,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:    one,\n+\t\t\t\tValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, Maximum: &two},\n+\t\t\t},\n+\t\t\tfalse,\n+\t\t},\n+\t\t\"more than maximum\": {\n+\t\t\ttwo,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:    one,\n+\t\t\t\tValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, Maximum: &one},\n+\t\t\t},\n+\t\t\ttrue,\n+\t\t},\n+\t\t\"in set\": {\n+\t\t\tone,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:     one,\n+\t\t\t\tValidValues: []resource.Quantity{one},\n+\t\t\t},\n+\t\t\tfalse,\n+\t\t},\n+\t\t\"not in set\": {\n+\t\t\ttwo,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:     one,\n+\t\t\t\tValidValues: []resource.Quantity{one},\n+\t\t\t},\n+\t\t\ttrue,\n+\t\t},\n+\t}\n+\tfor name, tc := range testcases {\n+\t\tt.Run(name, func(t *testing.T) {\n+\t\t\tg := NewWithT(t)\n+\t\t\tviolate := violatePolicy(tc.requestedVal, tc.consumable)\n+\t\t\tg.Expect(violate).To(BeEquivalentTo(tc.expectResult))\n+\t\t})\n+\t}\n+}\n+\n+func TestCalculateConsumedCapacity(t *testing.T) {\n+\ttestcases := map[string]struct {\n+\t\trequestedVal *resource.Quantity\n+\t\tconsumable   resourceapi.CapacitySharingPolicy\n+\n+\t\texpectResult *resource.Quantity\n+\t}{\n+\t\t\"empty\": {nil, resourceapi.CapacitySharingPolicy{}, nil},\n+\t\t\"min in range\": {\n+\t\t\tnil,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one}},\n+\t\t\t&one,\n+\t\t},\n+\t\t\"default in set\": {\n+\t\t\tnil,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidValues: []resource.Quantity{one}},\n+\t\t\t&one,\n+\t\t},\n+\t\t\"more than min in range\": {\n+\t\t\t&two,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one}},\n+\t\t\t&two,\n+\t\t},\n+\t\t\"less than min in range\": {\n+\t\t\t&one,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: two}},\n+\t\t\t&two,\n+\t\t},\n+\t\t\"with step (round up)\": {\n+\t\t\t&two,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, ChunkSize: ptr.To(two.DeepCopy())}},\n+\t\t\t&three,\n+\t\t},\n+\t\t\"with step (no remaining)\": {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2208796036",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/consumable_capacity_test.go",
        "discussion_id": "2208796036",
        "commented_code": "@@ -0,0 +1,203 @@\n+/*\n+Copyright 2024 The Kubernetes Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package structured\n+\n+import (\n+\t\"testing\"\n+\n+\t. \"github.com/onsi/gomega\"\n+\tresourceapi \"k8s.io/api/resource/v1beta1\"\n+\t\"k8s.io/apimachinery/pkg/api/resource\"\n+\t\"k8s.io/utils/ptr\"\n+)\n+\n+var (\n+\tone   = resource.MustParse(\"1\")\n+\ttwo   = resource.MustParse(\"2\")\n+\tthree = resource.MustParse(\"3\")\n+)\n+\n+func deviceConsumedCapacity(deviceID DeviceID) DeviceConsumedCapacity {\n+\tcapaicty := map[resourceapi.QualifiedName]resource.Quantity{\n+\t\tcapacity0: one,\n+\t}\n+\treturn NewDeviceConsumedCapacity(deviceID, capaicty)\n+}\n+\n+func TestConsumedCapacity(t *testing.T) {\n+\tg := NewWithT(t)\n+\tallocatedCapacity := NewConsumedCapacity()\n+\tg.Expect(allocatedCapacity.Empty()).To(BeTrueBecause(\"allocated capacity should start from zero\"))\n+\toneAllocated := ConsumedCapacity{\n+\t\tcapacity0: &one,\n+\t}\n+\tallocatedCapacity.Add(oneAllocated)\n+\tg.Expect(allocatedCapacity.Empty()).To(BeFalseBecause(\"capacity is added\"))\n+\tallocatedCapacity.Sub(oneAllocated)\n+\tg.Expect(allocatedCapacity.Empty()).To(BeTrueBecause(\"capacity is subtracted to zero\"))\n+}\n+\n+func TestConsumedCapacityCollection(t *testing.T) {\n+\tg := NewWithT(t)\n+\tdeviceID := MakeDeviceID(driverA, pool1, device1)\n+\taggregatedCapacity := NewConsumedCapacityCollection()\n+\taggregatedCapacity.Insert(deviceConsumedCapacity(deviceID))\n+\taggregatedCapacity.Insert(deviceConsumedCapacity(deviceID))\n+\tallocatedCapacity, found := aggregatedCapacity[deviceID]\n+\tg.Expect(found).To(BeTrueBecause(\"expected deviceID to be found\"))\n+\tg.Expect(allocatedCapacity[capacity0].Cmp(two)).To(BeZero())\n+\taggregatedCapacity.Remove(deviceConsumedCapacity(deviceID))\n+\tg.Expect(allocatedCapacity[capacity0].Cmp(one)).To(BeZero())\n+}\n+\n+func TestViolateCapacitySharingPolicy(t *testing.T) {\n+\ttestcases := map[string]struct {\n+\t\trequestedVal resource.Quantity\n+\t\tconsumable   *resourceapi.CapacitySharingPolicy\n+\n+\t\texpectResult bool\n+\t}{\n+\t\t\"no constraint\": {one, nil, false},\n+\t\t\"less than maximum\": {\n+\t\t\tone,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:    one,\n+\t\t\t\tValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, Maximum: &two},\n+\t\t\t},\n+\t\t\tfalse,\n+\t\t},\n+\t\t\"more than maximum\": {\n+\t\t\ttwo,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:    one,\n+\t\t\t\tValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, Maximum: &one},\n+\t\t\t},\n+\t\t\ttrue,\n+\t\t},\n+\t\t\"in set\": {\n+\t\t\tone,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:     one,\n+\t\t\t\tValidValues: []resource.Quantity{one},\n+\t\t\t},\n+\t\t\tfalse,\n+\t\t},\n+\t\t\"not in set\": {\n+\t\t\ttwo,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:     one,\n+\t\t\t\tValidValues: []resource.Quantity{one},\n+\t\t\t},\n+\t\t\ttrue,\n+\t\t},\n+\t}\n+\tfor name, tc := range testcases {\n+\t\tt.Run(name, func(t *testing.T) {\n+\t\t\tg := NewWithT(t)\n+\t\t\tviolate := violatePolicy(tc.requestedVal, tc.consumable)\n+\t\t\tg.Expect(violate).To(BeEquivalentTo(tc.expectResult))\n+\t\t})\n+\t}\n+}\n+\n+func TestCalculateConsumedCapacity(t *testing.T) {\n+\ttestcases := map[string]struct {\n+\t\trequestedVal *resource.Quantity\n+\t\tconsumable   resourceapi.CapacitySharingPolicy\n+\n+\t\texpectResult *resource.Quantity\n+\t}{\n+\t\t\"empty\": {nil, resourceapi.CapacitySharingPolicy{}, nil},\n+\t\t\"min in range\": {\n+\t\t\tnil,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one}},\n+\t\t\t&one,\n+\t\t},\n+\t\t\"default in set\": {\n+\t\t\tnil,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidValues: []resource.Quantity{one}},\n+\t\t\t&one,\n+\t\t},\n+\t\t\"more than min in range\": {\n+\t\t\t&two,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one}},\n+\t\t\t&two,\n+\t\t},\n+\t\t\"less than min in range\": {\n+\t\t\t&one,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: two}},\n+\t\t\t&two,\n+\t\t},\n+\t\t\"with step (round up)\": {\n+\t\t\t&two,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, ChunkSize: ptr.To(two.DeepCopy())}},\n+\t\t\t&three,\n+\t\t},\n+\t\t\"with step (no remaining)\": {",
        "comment_created_at": "2025-07-15T22:02:41+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "I don't see any tests of the actual allocator when chunk size is used. I think we should test that to make sure the appropriate chunk size is used. I am pretty sure you calculate that prior to entering the allocator but still seems having a test that uses it in the allocation would be good.",
        "pr_file_module": null
      },
      {
        "comment_id": "2214950892",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/consumable_capacity_test.go",
        "discussion_id": "2208796036",
        "commented_code": "@@ -0,0 +1,203 @@\n+/*\n+Copyright 2024 The Kubernetes Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package structured\n+\n+import (\n+\t\"testing\"\n+\n+\t. \"github.com/onsi/gomega\"\n+\tresourceapi \"k8s.io/api/resource/v1beta1\"\n+\t\"k8s.io/apimachinery/pkg/api/resource\"\n+\t\"k8s.io/utils/ptr\"\n+)\n+\n+var (\n+\tone   = resource.MustParse(\"1\")\n+\ttwo   = resource.MustParse(\"2\")\n+\tthree = resource.MustParse(\"3\")\n+)\n+\n+func deviceConsumedCapacity(deviceID DeviceID) DeviceConsumedCapacity {\n+\tcapaicty := map[resourceapi.QualifiedName]resource.Quantity{\n+\t\tcapacity0: one,\n+\t}\n+\treturn NewDeviceConsumedCapacity(deviceID, capaicty)\n+}\n+\n+func TestConsumedCapacity(t *testing.T) {\n+\tg := NewWithT(t)\n+\tallocatedCapacity := NewConsumedCapacity()\n+\tg.Expect(allocatedCapacity.Empty()).To(BeTrueBecause(\"allocated capacity should start from zero\"))\n+\toneAllocated := ConsumedCapacity{\n+\t\tcapacity0: &one,\n+\t}\n+\tallocatedCapacity.Add(oneAllocated)\n+\tg.Expect(allocatedCapacity.Empty()).To(BeFalseBecause(\"capacity is added\"))\n+\tallocatedCapacity.Sub(oneAllocated)\n+\tg.Expect(allocatedCapacity.Empty()).To(BeTrueBecause(\"capacity is subtracted to zero\"))\n+}\n+\n+func TestConsumedCapacityCollection(t *testing.T) {\n+\tg := NewWithT(t)\n+\tdeviceID := MakeDeviceID(driverA, pool1, device1)\n+\taggregatedCapacity := NewConsumedCapacityCollection()\n+\taggregatedCapacity.Insert(deviceConsumedCapacity(deviceID))\n+\taggregatedCapacity.Insert(deviceConsumedCapacity(deviceID))\n+\tallocatedCapacity, found := aggregatedCapacity[deviceID]\n+\tg.Expect(found).To(BeTrueBecause(\"expected deviceID to be found\"))\n+\tg.Expect(allocatedCapacity[capacity0].Cmp(two)).To(BeZero())\n+\taggregatedCapacity.Remove(deviceConsumedCapacity(deviceID))\n+\tg.Expect(allocatedCapacity[capacity0].Cmp(one)).To(BeZero())\n+}\n+\n+func TestViolateCapacitySharingPolicy(t *testing.T) {\n+\ttestcases := map[string]struct {\n+\t\trequestedVal resource.Quantity\n+\t\tconsumable   *resourceapi.CapacitySharingPolicy\n+\n+\t\texpectResult bool\n+\t}{\n+\t\t\"no constraint\": {one, nil, false},\n+\t\t\"less than maximum\": {\n+\t\t\tone,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:    one,\n+\t\t\t\tValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, Maximum: &two},\n+\t\t\t},\n+\t\t\tfalse,\n+\t\t},\n+\t\t\"more than maximum\": {\n+\t\t\ttwo,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:    one,\n+\t\t\t\tValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, Maximum: &one},\n+\t\t\t},\n+\t\t\ttrue,\n+\t\t},\n+\t\t\"in set\": {\n+\t\t\tone,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:     one,\n+\t\t\t\tValidValues: []resource.Quantity{one},\n+\t\t\t},\n+\t\t\tfalse,\n+\t\t},\n+\t\t\"not in set\": {\n+\t\t\ttwo,\n+\t\t\t&resourceapi.CapacitySharingPolicy{\n+\t\t\t\tDefault:     one,\n+\t\t\t\tValidValues: []resource.Quantity{one},\n+\t\t\t},\n+\t\t\ttrue,\n+\t\t},\n+\t}\n+\tfor name, tc := range testcases {\n+\t\tt.Run(name, func(t *testing.T) {\n+\t\t\tg := NewWithT(t)\n+\t\t\tviolate := violatePolicy(tc.requestedVal, tc.consumable)\n+\t\t\tg.Expect(violate).To(BeEquivalentTo(tc.expectResult))\n+\t\t})\n+\t}\n+}\n+\n+func TestCalculateConsumedCapacity(t *testing.T) {\n+\ttestcases := map[string]struct {\n+\t\trequestedVal *resource.Quantity\n+\t\tconsumable   resourceapi.CapacitySharingPolicy\n+\n+\t\texpectResult *resource.Quantity\n+\t}{\n+\t\t\"empty\": {nil, resourceapi.CapacitySharingPolicy{}, nil},\n+\t\t\"min in range\": {\n+\t\t\tnil,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one}},\n+\t\t\t&one,\n+\t\t},\n+\t\t\"default in set\": {\n+\t\t\tnil,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidValues: []resource.Quantity{one}},\n+\t\t\t&one,\n+\t\t},\n+\t\t\"more than min in range\": {\n+\t\t\t&two,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one}},\n+\t\t\t&two,\n+\t\t},\n+\t\t\"less than min in range\": {\n+\t\t\t&one,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: two}},\n+\t\t\t&two,\n+\t\t},\n+\t\t\"with step (round up)\": {\n+\t\t\t&two,\n+\t\t\tresourceapi.CapacitySharingPolicy{Default: one, ValidRange: &resourceapi.CapacitySharingPolicyRange{Minimum: one, ChunkSize: ptr.To(two.DeepCopy())}},\n+\t\t\t&three,\n+\t\t},\n+\t\t\"with step (no remaining)\": {",
        "comment_created_at": "2025-07-18T05:32:05+00:00",
        "comment_author": "sunya-ch",
        "comment_body": "@johnbelamaric \r\nThank you. I changed the sharingPolicy in allocator to have a default chunkSize and add the test case here:\r\nhttps://github.com/kubernetes/kubernetes/blob/0a3b052588e911ed38d1955620b069abacc78c4b/staging/src/k8s.io/dynamic-resource-allocation/structured/internal/allocatortesting/allocator_testing.go#L3735",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2100898368",
    "pr_number": 130160,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
    "created_at": "2025-05-21T18:13:15+00:00",
    "commented_code": null,
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2100898368",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-21T18:13:15+00:00",
        "comment_author": "pohly",
        "comment_body": "Wasn't the decision to favor devices without binding conditions? That's not in the code yet, is it?\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2101062447",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-21T19:59:27+00:00",
        "comment_author": "pohly",
        "comment_body": "I had started writing an integration test for this and then simplified it so that it only uses a single device. You can find the ground work (reusable also for other features) in https://github.com/kubernetes/kubernetes/pull/131869. The Device Binding Conditions test is in https://github.com/pohly/kubernetes/commits/dra-device-binding-conditions/\r\n\r\nI think I outlined in some previous message all the various edge cases that can occur once we consider more complex scenarios (multiple pods referencing the same claim; binding a pod fails after updating the ResourceClaim; and probably more that I don't remember right now). Those are scenarios which need integration tests. A unit test alone is not enough because we might make incorrect assumptions about what the right behavior of the plugin needs to be and what the scheduler then does. Having those tests before merging would be nice, otherwise they are needed before beta and then should be listed in the KEP's test plan.",
        "pr_file_module": null
      },
      {
        "comment_id": "2101665245",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-22T05:32:25+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "> Wasn't the decision to favor devices without binding conditions? That's not in the code yet, is it?\r\n\r\nYes, you're right, the decision was to favor devices without binding conditions.\r\nThat logic has already been implemented in `GatherPools()` within \"staging/src/k8s.io/dynamic-resource-allocation/structured/pools.go.\"",
        "pr_file_module": null
      },
      {
        "comment_id": "2101684798",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-22T05:47:14+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I'll check the tests you implemented since they're not working properly. I'll also try the dra integration test.",
        "pr_file_module": null
      },
      {
        "comment_id": "2101729887",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-22T06:24:18+00:00",
        "comment_author": "pohly",
        "comment_body": "There's a commented out \"without-binding\" device. My expectation was that the first claim would allocate that, but it ended up getting \"with-binding\" instead.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2101765621",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-22T06:43:32+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I wasn't expecting these to be mixed in a single ResourceSlice. As it stands, each slice is evaluated for its own BindingConditions, so this doesn't work for this test.",
        "pr_file_module": null
      },
      {
        "comment_id": "2101773644",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-22T06:48:53+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "It seems appropriate to change it to evaluate for each device. Is that correct?",
        "pr_file_module": null
      },
      {
        "comment_id": "2101912245",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-22T08:04:35+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I updated the implementation to handle mixed conditions, and also confirmed that the dra test prefers the \"without-binding\" device.\r\nI would appreciate it if you could check the test results and the implementation of allocator and 'GatherPools()'.",
        "pr_file_module": null
      },
      {
        "comment_id": "2101954743",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-22T08:27:05+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": ">Having those tests before merging would be nice, otherwise they are needed before beta and then should be listed in the KEP's test plan.\r\n\r\nThank you for highlighting those edge cases. I understand the importance of integration tests for covering these complex scenarios. I will prepare additional integration tests to address them, and I will ensure they are ready before the merge. I appreciate your patience while I create them.",
        "pr_file_module": null
      },
      {
        "comment_id": "2101956530",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-05-22T08:27:59+00:00",
        "comment_author": "pohly",
        "comment_body": "> I wasn't expecting these to be mixed in a single ResourceSlice. As it stands, each slice is evaluated for its own BindingConditions, so this doesn't work for this test.\r\n\r\nWe could make that assumption, as long as we document it properly. It might even make sense if it keeps the implementation simpler - I need to look at that.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2206659046",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go",
        "discussion_id": "2100898368",
        "commented_code": null,
        "comment_created_at": "2025-07-15T07:31:06+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I have added an integration test. Can we resolve this comment?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2223252079",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources_test.go",
    "created_at": "2025-07-22T17:02:52+00:00",
    "commented_code": "Value:  \"taint-value\",\n \t\tEffect: resourceapi.DeviceTaintEffectNoSchedule,\n \t}\n+\n+\t// for DRA Device Binding Conditions\n+\tbindingConditions        = []string{\"condition\"}\n+\tbindingFailureConditions = []string{\"failed\"}\n+\tbindingTimeout           = int64(15)\n+\n+\tfabricSlice = func() *resourceapi.ResourceSlice {\n+\t\tres := st.MakeResourceSlice(nodeName, driver).Device(\"instance-1\").Obj()\n+\t\tres.Spec.Devices[0].Basic.BindsToNode = ptr.To(true)\n+\t\tres.Spec.Devices[0].Basic.BindingConditions = bindingConditions\n+\t\tres.Spec.Devices[0].Basic.BindingFailureConditions = bindingFailureConditions\n+\t\tres.Spec.Devices[0].Basic.BindingTimeoutSeconds = &bindingTimeout\n+\t\tres.Spec.NodeSelector = st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj()\n+\t\treturn res\n+\t}()\n+\n+\tfabricSlice2 = func() *resourceapi.ResourceSlice {\n+\t\tres := st.MakeResourceSlice(nodeName, driver2).Device(\"instance-2\").Obj()\n+\t\tres.Spec.Devices[0].Basic.BindsToNode = ptr.To(true)\n+\t\tres.Spec.Devices[0].Basic.BindingConditions = bindingConditions\n+\t\tres.Spec.Devices[0].Basic.BindingFailureConditions = bindingFailureConditions\n+\t\tres.Spec.Devices[0].Basic.BindingTimeoutSeconds = &bindingTimeout\n+\t\tres.Spec.NodeSelector = st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj()\n+\t\treturn res\n+\t}()\n+\n+\tallocationResultWithBindingConditions = &resourceapi.AllocationResult{\n+\t\tDevices: resourceapi.DeviceAllocationResult{\n+\t\t\tResults: []resourceapi.DeviceRequestAllocationResult{{\n+\t\t\t\tDriver:                   driver,\n+\t\t\t\tPool:                     nodeName,\n+\t\t\t\tDevice:                   \"instance-1\",\n+\t\t\t\tRequest:                  \"req-1\",\n+\t\t\t\tBindingConditions:        bindingConditions,\n+\t\t\t\tBindingFailureConditions: bindingFailureConditions,\n+\t\t\t\tBindingTimeoutSeconds:    &bindingTimeout,\n+\t\t\t}},\n+\t\t},\n+\t\tNodeSelector: st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj(),\n+\t}\n+\n+\tallocationResultWithBindingConditions2 = &resourceapi.AllocationResult{\n+\t\tDevices: resourceapi.DeviceAllocationResult{\n+\t\t\tResults: []resourceapi.DeviceRequestAllocationResult{{\n+\t\t\t\tDriver:                   driver2,\n+\t\t\t\tPool:                     nodeName,\n+\t\t\t\tDevice:                   \"instance-2\",\n+\t\t\t\tRequest:                  \"req-2\",\n+\t\t\t\tBindingConditions:        bindingConditions,\n+\t\t\t\tBindingFailureConditions: bindingFailureConditions,\n+\t\t\t\tBindingTimeoutSeconds:    &bindingTimeout,\n+\t\t\t}},\n+\t\t},\n+\t\tNodeSelector: st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj(),\n+\t}\n+\n+\tboundClaim = st.FromResourceClaim(allocatedClaim).\n+\t\t\tAllocation(allocationResultWithBindingConditions).\n+\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-1\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionTrue},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionFalse},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tboundClaim2 = st.FromResourceClaim(allocatedClaim2).\n+\t\t\tAllocation(allocationResultWithBindingConditions2).\n+\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver2,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-2\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionTrue},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionFalse},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tfailedBindingClaim = st.FromResourceClaim(allocatedClaim).\n+\t\t\t\tAllocation(allocationResultWithBindingConditions).\n+\t\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-1\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionFalse},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionTrue},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tfailedBindingClaim2 = st.FromResourceClaim(allocatedClaim2).\n+\t\t\t\tAllocation(allocationResultWithBindingConditions2).\n+\t\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver2,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-2\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionFalse},",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2223252079",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources_test.go",
        "discussion_id": "2223252079",
        "commented_code": "@@ -204,6 +236,129 @@ var (\n \t\tValue:  \"taint-value\",\n \t\tEffect: resourceapi.DeviceTaintEffectNoSchedule,\n \t}\n+\n+\t// for DRA Device Binding Conditions\n+\tbindingConditions        = []string{\"condition\"}\n+\tbindingFailureConditions = []string{\"failed\"}\n+\tbindingTimeout           = int64(15)\n+\n+\tfabricSlice = func() *resourceapi.ResourceSlice {\n+\t\tres := st.MakeResourceSlice(nodeName, driver).Device(\"instance-1\").Obj()\n+\t\tres.Spec.Devices[0].Basic.BindsToNode = ptr.To(true)\n+\t\tres.Spec.Devices[0].Basic.BindingConditions = bindingConditions\n+\t\tres.Spec.Devices[0].Basic.BindingFailureConditions = bindingFailureConditions\n+\t\tres.Spec.Devices[0].Basic.BindingTimeoutSeconds = &bindingTimeout\n+\t\tres.Spec.NodeSelector = st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj()\n+\t\treturn res\n+\t}()\n+\n+\tfabricSlice2 = func() *resourceapi.ResourceSlice {\n+\t\tres := st.MakeResourceSlice(nodeName, driver2).Device(\"instance-2\").Obj()\n+\t\tres.Spec.Devices[0].Basic.BindsToNode = ptr.To(true)\n+\t\tres.Spec.Devices[0].Basic.BindingConditions = bindingConditions\n+\t\tres.Spec.Devices[0].Basic.BindingFailureConditions = bindingFailureConditions\n+\t\tres.Spec.Devices[0].Basic.BindingTimeoutSeconds = &bindingTimeout\n+\t\tres.Spec.NodeSelector = st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj()\n+\t\treturn res\n+\t}()\n+\n+\tallocationResultWithBindingConditions = &resourceapi.AllocationResult{\n+\t\tDevices: resourceapi.DeviceAllocationResult{\n+\t\t\tResults: []resourceapi.DeviceRequestAllocationResult{{\n+\t\t\t\tDriver:                   driver,\n+\t\t\t\tPool:                     nodeName,\n+\t\t\t\tDevice:                   \"instance-1\",\n+\t\t\t\tRequest:                  \"req-1\",\n+\t\t\t\tBindingConditions:        bindingConditions,\n+\t\t\t\tBindingFailureConditions: bindingFailureConditions,\n+\t\t\t\tBindingTimeoutSeconds:    &bindingTimeout,\n+\t\t\t}},\n+\t\t},\n+\t\tNodeSelector: st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj(),\n+\t}\n+\n+\tallocationResultWithBindingConditions2 = &resourceapi.AllocationResult{\n+\t\tDevices: resourceapi.DeviceAllocationResult{\n+\t\t\tResults: []resourceapi.DeviceRequestAllocationResult{{\n+\t\t\t\tDriver:                   driver2,\n+\t\t\t\tPool:                     nodeName,\n+\t\t\t\tDevice:                   \"instance-2\",\n+\t\t\t\tRequest:                  \"req-2\",\n+\t\t\t\tBindingConditions:        bindingConditions,\n+\t\t\t\tBindingFailureConditions: bindingFailureConditions,\n+\t\t\t\tBindingTimeoutSeconds:    &bindingTimeout,\n+\t\t\t}},\n+\t\t},\n+\t\tNodeSelector: st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj(),\n+\t}\n+\n+\tboundClaim = st.FromResourceClaim(allocatedClaim).\n+\t\t\tAllocation(allocationResultWithBindingConditions).\n+\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-1\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionTrue},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionFalse},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tboundClaim2 = st.FromResourceClaim(allocatedClaim2).\n+\t\t\tAllocation(allocationResultWithBindingConditions2).\n+\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver2,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-2\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionTrue},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionFalse},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tfailedBindingClaim = st.FromResourceClaim(allocatedClaim).\n+\t\t\t\tAllocation(allocationResultWithBindingConditions).\n+\t\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-1\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionFalse},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionTrue},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tfailedBindingClaim2 = st.FromResourceClaim(allocatedClaim2).\n+\t\t\t\tAllocation(allocationResultWithBindingConditions2).\n+\t\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver2,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-2\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionFalse},",
        "comment_created_at": "2025-07-22T17:02:52+00:00",
        "comment_author": "ania-borowiec",
        "comment_body": "Can you please add a test with failedBindingClaim that contains 2 bindingConditions (one with status True, the other False) and no bindingFailureConditions? Based on my understanding that's an option.\r\nI'm okay with testing a smaller chunk of code as well (as in, not testing the entire PreFilter method, but unit testing isolated helper functions), if that's consistent with the code base / DRA style guide.",
        "pr_file_module": null
      },
      {
        "comment_id": "2224325776",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources_test.go",
        "discussion_id": "2223252079",
        "commented_code": "@@ -204,6 +236,129 @@ var (\n \t\tValue:  \"taint-value\",\n \t\tEffect: resourceapi.DeviceTaintEffectNoSchedule,\n \t}\n+\n+\t// for DRA Device Binding Conditions\n+\tbindingConditions        = []string{\"condition\"}\n+\tbindingFailureConditions = []string{\"failed\"}\n+\tbindingTimeout           = int64(15)\n+\n+\tfabricSlice = func() *resourceapi.ResourceSlice {\n+\t\tres := st.MakeResourceSlice(nodeName, driver).Device(\"instance-1\").Obj()\n+\t\tres.Spec.Devices[0].Basic.BindsToNode = ptr.To(true)\n+\t\tres.Spec.Devices[0].Basic.BindingConditions = bindingConditions\n+\t\tres.Spec.Devices[0].Basic.BindingFailureConditions = bindingFailureConditions\n+\t\tres.Spec.Devices[0].Basic.BindingTimeoutSeconds = &bindingTimeout\n+\t\tres.Spec.NodeSelector = st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj()\n+\t\treturn res\n+\t}()\n+\n+\tfabricSlice2 = func() *resourceapi.ResourceSlice {\n+\t\tres := st.MakeResourceSlice(nodeName, driver2).Device(\"instance-2\").Obj()\n+\t\tres.Spec.Devices[0].Basic.BindsToNode = ptr.To(true)\n+\t\tres.Spec.Devices[0].Basic.BindingConditions = bindingConditions\n+\t\tres.Spec.Devices[0].Basic.BindingFailureConditions = bindingFailureConditions\n+\t\tres.Spec.Devices[0].Basic.BindingTimeoutSeconds = &bindingTimeout\n+\t\tres.Spec.NodeSelector = st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj()\n+\t\treturn res\n+\t}()\n+\n+\tallocationResultWithBindingConditions = &resourceapi.AllocationResult{\n+\t\tDevices: resourceapi.DeviceAllocationResult{\n+\t\t\tResults: []resourceapi.DeviceRequestAllocationResult{{\n+\t\t\t\tDriver:                   driver,\n+\t\t\t\tPool:                     nodeName,\n+\t\t\t\tDevice:                   \"instance-1\",\n+\t\t\t\tRequest:                  \"req-1\",\n+\t\t\t\tBindingConditions:        bindingConditions,\n+\t\t\t\tBindingFailureConditions: bindingFailureConditions,\n+\t\t\t\tBindingTimeoutSeconds:    &bindingTimeout,\n+\t\t\t}},\n+\t\t},\n+\t\tNodeSelector: st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj(),\n+\t}\n+\n+\tallocationResultWithBindingConditions2 = &resourceapi.AllocationResult{\n+\t\tDevices: resourceapi.DeviceAllocationResult{\n+\t\t\tResults: []resourceapi.DeviceRequestAllocationResult{{\n+\t\t\t\tDriver:                   driver2,\n+\t\t\t\tPool:                     nodeName,\n+\t\t\t\tDevice:                   \"instance-2\",\n+\t\t\t\tRequest:                  \"req-2\",\n+\t\t\t\tBindingConditions:        bindingConditions,\n+\t\t\t\tBindingFailureConditions: bindingFailureConditions,\n+\t\t\t\tBindingTimeoutSeconds:    &bindingTimeout,\n+\t\t\t}},\n+\t\t},\n+\t\tNodeSelector: st.MakeNodeSelector().In(\"metadata.name\", []string{nodeName}, st.NodeSelectorTypeMatchFields).Obj(),\n+\t}\n+\n+\tboundClaim = st.FromResourceClaim(allocatedClaim).\n+\t\t\tAllocation(allocationResultWithBindingConditions).\n+\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-1\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionTrue},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionFalse},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tboundClaim2 = st.FromResourceClaim(allocatedClaim2).\n+\t\t\tAllocation(allocationResultWithBindingConditions2).\n+\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver2,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-2\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionTrue},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionFalse},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tfailedBindingClaim = st.FromResourceClaim(allocatedClaim).\n+\t\t\t\tAllocation(allocationResultWithBindingConditions).\n+\t\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-1\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionFalse},\n+\t\t\t\t\t{Type: \"failed\", Status: metav1.ConditionTrue},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}).\n+\t\tObj()\n+\n+\tfailedBindingClaim2 = st.FromResourceClaim(allocatedClaim2).\n+\t\t\t\tAllocation(allocationResultWithBindingConditions2).\n+\t\t\t\tAllocatedDeviceStatuses([]resourceapi.AllocatedDeviceStatus{\n+\t\t\t{\n+\t\t\t\tDriver: driver2,\n+\t\t\t\tPool:   nodeName,\n+\t\t\t\tDevice: \"instance-2\",\n+\t\t\t\tConditions: []metav1.Condition{\n+\t\t\t\t\t{Type: \"condition\", Status: metav1.ConditionFalse},",
        "comment_created_at": "2025-07-23T04:38:20+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I have added the suggested test case.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2200651281",
    "pr_number": 130653,
    "pr_file": "test/e2e/dra/dra.go",
    "created_at": "2025-07-11T12:50:23+00:00",
    "commented_code": "podStartTimeout = 5 * time.Minute\n )\n \n+var (",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2200651281",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/dra.go",
        "discussion_id": "2200651281",
        "commented_code": "@@ -64,6 +66,10 @@ const (\n \tpodStartTimeout = 5 * time.Minute\n )\n \n+var (",
        "comment_created_at": "2025-07-11T12:50:23+00:00",
        "comment_author": "pohly",
        "comment_body": "Please cover the following scenarios:\r\n- one container, one extended resource\r\n- one container, three different extended resources\r\n- three containers, one extended resource each\r\n- three containers, with one/two/three resources.\r\n\r\nThis is relevant for the `listMap` handling of `RequestMapping`. Needs to be an E2E test to verify the full flow, including kubelet. The test must verify that each container gets the right devices injected. I don't remember whether we already inject some env variable representing the device name. If not, then you can add that in the CDI file generated by the test/e2e/dra/test-driver, for example by setting `DRA_DEVICE_<driver name>_<pool name>_<device name>=1`. Then you can check the allocation result, determine which env variables should be set, and check them. Make sure that no unexpected env variables are set.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2203580704",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/dra.go",
        "discussion_id": "2200651281",
        "commented_code": "@@ -64,6 +66,10 @@ const (\n \tpodStartTimeout = 5 * time.Minute\n )\n \n+var (",
        "comment_created_at": "2025-07-13T22:53:46+00:00",
        "comment_author": "yliaog",
        "comment_body": "the first case is already there, added three more test cases for the other 3 cases mentioned above.\r\n\r\nThe test asserts right env variable container-x-request-y is present in the right container x, the allocation is like below in one example run, each device is allocated to exactly one request.\r\n\r\n            allocation:\r\n              devices:\r\n                results:\r\n                - device: device-0\r\n                  driver: dra-7258.k8s.io\r\n                  pool: network\r\n                  request: container-2-request-2\r\n                - device: device-1\r\n                  driver: dra-7258.k8s.io\r\n                  pool: network\r\n                  request: container-0-request-0\r\n                - device: device-2\r\n                  driver: dra-7258.k8s.io\r\n                  pool: network\r\n                  request: container-1-request-0\r\n                - device: device-3\r\n                  driver: dra-7258.k8s.io\r\n                  pool: network\r\n                  request: container-1-request-1\r\n                - device: device-4\r\n                  driver: dra-7258.k8s.io\r\n                  pool: network\r\n                  request: container-2-request-0\r\n                - device: device-5\r\n                  driver: dra-7258.k8s.io\r\n                  pool: network\r\n                  request: container-2-request-1",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2202200806",
    "pr_number": 130653,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator_test.go",
    "created_at": "2025-07-12T00:40:23+00:00",
    "commented_code": "allocator, err := NewAllocator(ctx, tc.features, unwrap(claimsToAllocate...), sets.New(allocatedDevices...), classLister, slices, cel.NewCache(1))\n \t\t\tg.Expect(err).ToNot(gomega.HaveOccurred())\n \n-\t\t\tresults, err := allocator.Allocate(ctx, tc.node)\n+\t\t\tresults, err := allocator.Allocate(ctx, tc.node, nil)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2202200806",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator_test.go",
        "discussion_id": "2202200806",
        "commented_code": "@@ -3485,7 +3485,7 @@ func TestAllocator(t *testing.T) {\n \t\t\tallocator, err := NewAllocator(ctx, tc.features, unwrap(claimsToAllocate...), sets.New(allocatedDevices...), classLister, slices, cel.NewCache(1))\n \t\t\tg.Expect(err).ToNot(gomega.HaveOccurred())\n \n-\t\t\tresults, err := allocator.Allocate(ctx, tc.node)\n+\t\t\tresults, err := allocator.Allocate(ctx, tc.node, nil)",
        "comment_created_at": "2025-07-12T00:40:23+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "do we not need a test with this non-nil?",
        "pr_file_module": null
      },
      {
        "comment_id": "2203158688",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/allocator_test.go",
        "discussion_id": "2202200806",
        "commented_code": "@@ -3485,7 +3485,7 @@ func TestAllocator(t *testing.T) {\n \t\t\tallocator, err := NewAllocator(ctx, tc.features, unwrap(claimsToAllocate...), sets.New(allocatedDevices...), classLister, slices, cel.NewCache(1))\n \t\t\tg.Expect(err).ToNot(gomega.HaveOccurred())\n \n-\t\t\tresults, err := allocator.Allocate(ctx, tc.node)\n+\t\t\tresults, err := allocator.Allocate(ctx, tc.node, nil)",
        "comment_created_at": "2025-07-13T05:01:00+00:00",
        "comment_author": "yliaog",
        "comment_body": "Yes, we do. added a test case for non-nil",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2204108215",
    "pr_number": 130653,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/resourceclaim/pod.go",
    "created_at": "2025-07-14T07:57:29+00:00",
    "commented_code": "}\n \treturn true\n }\n+\n+func PodExtendedStatusEqual(statusA, statusB *corev1.PodExtendedResourceClaimStatus) bool {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2204108215",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/resourceclaim/pod.go",
        "discussion_id": "2204108215",
        "commented_code": "@@ -46,3 +46,33 @@ func PodStatusEqual(statusA, statusB []corev1.PodResourceClaimStatus) bool {\n \t}\n \treturn true\n }\n+\n+func PodExtendedStatusEqual(statusA, statusB *corev1.PodExtendedResourceClaimStatus) bool {",
        "comment_created_at": "2025-07-14T07:57:29+00:00",
        "comment_author": "pohly",
        "comment_body": "Unit tests for this, please. Keep this comment open until the new unit tests have been reviewed.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2205427295",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/resourceclaim/pod.go",
        "discussion_id": "2204108215",
        "commented_code": "@@ -46,3 +46,33 @@ func PodStatusEqual(statusA, statusB []corev1.PodResourceClaimStatus) bool {\n \t}\n \treturn true\n }\n+\n+func PodExtendedStatusEqual(statusA, statusB *corev1.PodExtendedResourceClaimStatus) bool {",
        "comment_created_at": "2025-07-14T17:17:32+00:00",
        "comment_author": "yliaog",
        "comment_body": "added unittest, please take a look.",
        "pr_file_module": null
      },
      {
        "comment_id": "2219058360",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/resourceclaim/pod.go",
        "discussion_id": "2204108215",
        "commented_code": "@@ -46,3 +46,33 @@ func PodStatusEqual(statusA, statusB []corev1.PodResourceClaimStatus) bool {\n \t}\n \treturn true\n }\n+\n+func PodExtendedStatusEqual(statusA, statusB *corev1.PodExtendedResourceClaimStatus) bool {",
        "comment_created_at": "2025-07-21T12:27:54+00:00",
        "comment_author": "pohly",
        "comment_body": ":+1:, but see https://github.com/kubernetes/kubernetes/pull/130653/files#r2219055313.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2222061569",
    "pr_number": 131357,
    "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
    "created_at": "2025-07-22T10:22:49+00:00",
    "commented_code": null,
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2222061569",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131357,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
        "discussion_id": "2222061569",
        "commented_code": null,
        "comment_created_at": "2025-07-22T10:22:49+00:00",
        "comment_author": "macsko",
        "comment_body": "Don't we want to have any unit test for this?",
        "pr_file_module": null
      },
      {
        "comment_id": "2222192178",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131357,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
        "discussion_id": "2222061569",
        "commented_code": null,
        "comment_created_at": "2025-07-22T11:07:51+00:00",
        "comment_author": "mortent",
        "comment_body": "The unit tests for all the different variants of the allocator in the `allocatortesting` package. We only keep a single set of unit tests for the allocators and the tests that doesn't apply because the necessary feature gates are not enabled get skipped. So we do have unit test coverage for experimental allocator.",
        "pr_file_module": null
      },
      {
        "comment_id": "2222260348",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131357,
        "pr_file": "staging/src/k8s.io/dynamic-resource-allocation/structured/internal/experimental/allocator_experimental.go",
        "discussion_id": "2222061569",
        "commented_code": null,
        "comment_created_at": "2025-07-22T11:40:59+00:00",
        "comment_author": "macsko",
        "comment_body": "Ack, I missed that file",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2223091893",
    "pr_number": 132642,
    "pr_file": "pkg/api/v1/pod/util_test.go",
    "created_at": "2025-07-22T16:21:03+00:00",
    "commented_code": "})\n \t}\n }\n+\n+func TestIsContainerRestartable(t *testing.T) {\n+\tvar (\n+\t\tcontainerRestartPolicyAlways    = v1.ContainerRestartPolicyAlways\n+\t\tcontainerRestartPolicyOnFailure = v1.ContainerRestartPolicyOnFailure\n+\t\tcontainerRestartPolicyNever     = v1.ContainerRestartPolicyNever\n+\t)\n+\n+\ttestCases := []struct {\n+\t\tname      string\n+\t\tpodSpec   v1.PodSpec\n+\t\tcontainer v1.Container\n+\t\texpected  bool\n+\t}{\n+\t\t{\n+\t\t\tname:    \"Container: Rule action 'Restart' should always be true\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{Action: v1.ContainerRestartRuleActionRestart},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'Always' is restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyAlways,\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'OnFailure' is restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyOnFailure,\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'Never' is not restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyAlways},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t},\n+\t\t\texpected: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'Always' is restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyAlways},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'OnFailure' is restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyOnFailure},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'Never' is not restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  false,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Default empty policy is restartable (since it's not 'Never')\",\n+\t\t\tpodSpec:   v1.PodSpec{},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t}\n+\n+\tfor _, tc := range testCases {\n+\t\tt.Run(tc.name, func(t *testing.T) {\n+\t\t\tgot := IsContainerRestartable(tc.podSpec, tc.container)\n+\t\t\tif got != tc.expected {\n+\t\t\t\tt.Errorf(\"IsContainerRestartable() = %v, want %v\", got, tc.expected)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+func TestContainerHasRestartablePolicy(t *testing.T) {\n+\tvar (\n+\t\tcontainerRestartPolicyAlways    = v1.ContainerRestartPolicyAlways\n+\t\tcontainerRestartPolicyOnFailure = v1.ContainerRestartPolicyOnFailure\n+\t\tcontainerRestartPolicyNever     = v1.ContainerRestartPolicyNever\n+\t)\n+\n+\ttestCases := []struct {\n+\t\tname      string\n+\t\tcontainer v1.Container\n+\t\tpodSpec   v1.PodSpec\n+\t\texitCode  int32\n+\t\texpected  bool\n+\t}{\n+\t\t{\n+\t\t\tname: \"Rule: 'In' operator matches with 'Restart' action\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpIn,\n+\t\t\t\t\t\t\tValues:   []int32{42, 50, 60},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 42,\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Rule: 'NotIn' operator matches with 'Restart' action\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpNotIn,\n+\t\t\t\t\t\t\tValues:   []int32{0, 1, 2},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 99,\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Rule: 'In' operator does not match, should fall back to container policy\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpIn,\n+\t\t\t\t\t\t\tValues:   []int32{10, 20},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 30,\n+\t\t\texpected: false,\n+\t\t},\n+\t\t{",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2223091893",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/api/v1/pod/util_test.go",
        "discussion_id": "2223091893",
        "commented_code": "@@ -1172,3 +1172,224 @@ func TestCalculatePodConditionObservedGeneration(t *testing.T) {\n \t\t})\n \t}\n }\n+\n+func TestIsContainerRestartable(t *testing.T) {\n+\tvar (\n+\t\tcontainerRestartPolicyAlways    = v1.ContainerRestartPolicyAlways\n+\t\tcontainerRestartPolicyOnFailure = v1.ContainerRestartPolicyOnFailure\n+\t\tcontainerRestartPolicyNever     = v1.ContainerRestartPolicyNever\n+\t)\n+\n+\ttestCases := []struct {\n+\t\tname      string\n+\t\tpodSpec   v1.PodSpec\n+\t\tcontainer v1.Container\n+\t\texpected  bool\n+\t}{\n+\t\t{\n+\t\t\tname:    \"Container: Rule action 'Restart' should always be true\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{Action: v1.ContainerRestartRuleActionRestart},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'Always' is restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyAlways,\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'OnFailure' is restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyOnFailure,\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'Never' is not restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyAlways},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t},\n+\t\t\texpected: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'Always' is restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyAlways},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'OnFailure' is restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyOnFailure},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'Never' is not restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  false,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Default empty policy is restartable (since it's not 'Never')\",\n+\t\t\tpodSpec:   v1.PodSpec{},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t}\n+\n+\tfor _, tc := range testCases {\n+\t\tt.Run(tc.name, func(t *testing.T) {\n+\t\t\tgot := IsContainerRestartable(tc.podSpec, tc.container)\n+\t\t\tif got != tc.expected {\n+\t\t\t\tt.Errorf(\"IsContainerRestartable() = %v, want %v\", got, tc.expected)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+func TestContainerHasRestartablePolicy(t *testing.T) {\n+\tvar (\n+\t\tcontainerRestartPolicyAlways    = v1.ContainerRestartPolicyAlways\n+\t\tcontainerRestartPolicyOnFailure = v1.ContainerRestartPolicyOnFailure\n+\t\tcontainerRestartPolicyNever     = v1.ContainerRestartPolicyNever\n+\t)\n+\n+\ttestCases := []struct {\n+\t\tname      string\n+\t\tcontainer v1.Container\n+\t\tpodSpec   v1.PodSpec\n+\t\texitCode  int32\n+\t\texpected  bool\n+\t}{\n+\t\t{\n+\t\t\tname: \"Rule: 'In' operator matches with 'Restart' action\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpIn,\n+\t\t\t\t\t\t\tValues:   []int32{42, 50, 60},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 42,\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Rule: 'NotIn' operator matches with 'Restart' action\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpNotIn,\n+\t\t\t\t\t\t\tValues:   []int32{0, 1, 2},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 99,\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Rule: 'In' operator does not match, should fall back to container policy\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpIn,\n+\t\t\t\t\t\t\tValues:   []int32{10, 20},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 30,\n+\t\t\texpected: false,\n+\t\t},\n+\t\t{",
        "comment_created_at": "2025-07-22T16:21:03+00:00",
        "comment_author": "haircommander",
        "comment_body": "also: maybe a unit test on conflicting rules (basically codifying in tests that we react to the earliest signal we do restart)?",
        "pr_file_module": null
      },
      {
        "comment_id": "2223829432",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/api/v1/pod/util_test.go",
        "discussion_id": "2223091893",
        "commented_code": "@@ -1172,3 +1172,224 @@ func TestCalculatePodConditionObservedGeneration(t *testing.T) {\n \t\t})\n \t}\n }\n+\n+func TestIsContainerRestartable(t *testing.T) {\n+\tvar (\n+\t\tcontainerRestartPolicyAlways    = v1.ContainerRestartPolicyAlways\n+\t\tcontainerRestartPolicyOnFailure = v1.ContainerRestartPolicyOnFailure\n+\t\tcontainerRestartPolicyNever     = v1.ContainerRestartPolicyNever\n+\t)\n+\n+\ttestCases := []struct {\n+\t\tname      string\n+\t\tpodSpec   v1.PodSpec\n+\t\tcontainer v1.Container\n+\t\texpected  bool\n+\t}{\n+\t\t{\n+\t\t\tname:    \"Container: Rule action 'Restart' should always be true\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{Action: v1.ContainerRestartRuleActionRestart},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'Always' is restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyAlways,\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'OnFailure' is restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyOnFailure,\n+\t\t\t},\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Container: Policy 'Never' is not restartable\",\n+\t\t\tpodSpec: v1.PodSpec{RestartPolicy: v1.RestartPolicyAlways},\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t},\n+\t\t\texpected: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'Always' is restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyAlways},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'OnFailure' is restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyOnFailure},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Policy 'Never' is not restartable\",\n+\t\t\tpodSpec:   v1.PodSpec{RestartPolicy: v1.RestartPolicyNever},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  false,\n+\t\t},\n+\t\t{\n+\t\t\tname:      \"Pod Fallback: Default empty policy is restartable (since it's not 'Never')\",\n+\t\t\tpodSpec:   v1.PodSpec{},\n+\t\t\tcontainer: v1.Container{},\n+\t\t\texpected:  true,\n+\t\t},\n+\t}\n+\n+\tfor _, tc := range testCases {\n+\t\tt.Run(tc.name, func(t *testing.T) {\n+\t\t\tgot := IsContainerRestartable(tc.podSpec, tc.container)\n+\t\t\tif got != tc.expected {\n+\t\t\t\tt.Errorf(\"IsContainerRestartable() = %v, want %v\", got, tc.expected)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+func TestContainerHasRestartablePolicy(t *testing.T) {\n+\tvar (\n+\t\tcontainerRestartPolicyAlways    = v1.ContainerRestartPolicyAlways\n+\t\tcontainerRestartPolicyOnFailure = v1.ContainerRestartPolicyOnFailure\n+\t\tcontainerRestartPolicyNever     = v1.ContainerRestartPolicyNever\n+\t)\n+\n+\ttestCases := []struct {\n+\t\tname      string\n+\t\tcontainer v1.Container\n+\t\tpodSpec   v1.PodSpec\n+\t\texitCode  int32\n+\t\texpected  bool\n+\t}{\n+\t\t{\n+\t\t\tname: \"Rule: 'In' operator matches with 'Restart' action\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpIn,\n+\t\t\t\t\t\t\tValues:   []int32{42, 50, 60},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 42,\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Rule: 'NotIn' operator matches with 'Restart' action\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpNotIn,\n+\t\t\t\t\t\t\tValues:   []int32{0, 1, 2},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 99,\n+\t\t\texpected: true,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Rule: 'In' operator does not match, should fall back to container policy\",\n+\t\t\tcontainer: v1.Container{\n+\t\t\t\tRestartPolicy: &containerRestartPolicyNever,\n+\t\t\t\tRestartPolicyRules: []v1.ContainerRestartRule{\n+\t\t\t\t\t{\n+\t\t\t\t\t\tAction: v1.ContainerRestartRuleActionRestart,\n+\t\t\t\t\t\tExitCodes: &v1.ContainerRestartRuleOnExitCodes{\n+\t\t\t\t\t\t\tOperator: v1.ContainerRestartRuleOnExitCodesOpIn,\n+\t\t\t\t\t\t\tValues:   []int32{10, 20},\n+\t\t\t\t\t\t},\n+\t\t\t\t\t},\n+\t\t\t\t},\n+\t\t\t},\n+\t\t\texitCode: 30,\n+\t\t\texpected: false,\n+\t\t},\n+\t\t{",
        "comment_created_at": "2025-07-22T21:06:47+00:00",
        "comment_author": "yuanwang04",
        "comment_body": "SG, will add unit test coverage for `findMatchingContainerRestartRule`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219292879",
    "pr_number": 132439,
    "pr_file": "test/integration/scheduler/preemption/preemption_test.go",
    "created_at": "2025-07-21T13:56:10+00:00",
    "commented_code": "deleteFakeNode:   true,\n \t\t\tpodNamesToDelete: []string{\"low\"},\n \t\t},\n+\t\t{\n+\t\t\tname:         \"node removal causes unschedulable pods to be re-enqueued with feature gate enabled\",",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2219292879",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132439,
        "pr_file": "test/integration/scheduler/preemption/preemption_test.go",
        "discussion_id": "2219292879",
        "commented_code": "@@ -1621,6 +1624,26 @@ func TestNominatedNodeCleanUp(t *testing.T) {\n \t\t\tdeleteFakeNode:   true,\n \t\t\tpodNamesToDelete: []string{\"low\"},\n \t\t},\n+\t\t{\n+\t\t\tname:         \"node removal causes unschedulable pods to be re-enqueued with feature gate enabled\",",
        "comment_created_at": "2025-07-21T13:56:10+00:00",
        "comment_author": "macsko",
        "comment_body": "I think we need to cover more test cases with the new feature enabled, as it's enabled by default. Can you just run **all** test cases with it both enabled and disabled and just check the clearing logic accordingly?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2217836566",
    "pr_number": 133066,
    "pr_file": "pkg/kubelet/pod_workers_test.go",
    "created_at": "2025-07-20T14:08:42+00:00",
    "commented_code": "}\n }\n \n+func TestCompleteWorkWithSyncError(t *testing.T) {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2217836566",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133066,
        "pr_file": "pkg/kubelet/pod_workers_test.go",
        "discussion_id": "2217836566",
        "commented_code": "@@ -882,6 +885,88 @@ func TestUpdatePod(t *testing.T) {\n \t}\n }\n \n+func TestCompleteWorkWithSyncError(t *testing.T) {",
        "comment_created_at": "2025-07-20T14:08:42+00:00",
        "comment_author": "HirazawaUi",
        "comment_body": "Since you've added unit tests for this function, why not consider covering other scenarios? e.g.\r\n\r\n* phaseTransition is true\r\n\r\n* syncErr is nil\r\n\r\n* syncErr is NetworkNotReady\r\n\r\n* status.pendingUpdate is not nil\r\n\r\n.....",
        "pr_file_module": null
      },
      {
        "comment_id": "2219633874",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133066,
        "pr_file": "pkg/kubelet/pod_workers_test.go",
        "discussion_id": "2217836566",
        "commented_code": "@@ -882,6 +885,88 @@ func TestUpdatePod(t *testing.T) {\n \t}\n }\n \n+func TestCompleteWorkWithSyncError(t *testing.T) {",
        "comment_created_at": "2025-07-21T16:03:55+00:00",
        "comment_author": "hankfreund",
        "comment_body": "Added more test cases to round out this test and added a second test for the pendingUpdate checks.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2202904692",
    "pr_number": 132558,
    "pr_file": "pkg/kubelet/kubelet_pods.go",
    "created_at": "2025-07-12T20:23:37+00:00",
    "commented_code": "func (kl *Kubelet) GeneratePodHostNameAndDomain(pod *v1.Pod) (string, string, error) {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2202904692",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132558,
        "pr_file": "pkg/kubelet/kubelet_pods.go",
        "discussion_id": "2202904692",
        "commented_code": "@@ -570,6 +570,11 @@ func (kl *Kubelet) GetOrCreateUserNamespaceMappings(pod *v1.Pod, runtimeHandler\n func (kl *Kubelet) GeneratePodHostNameAndDomain(pod *v1.Pod) (string, string, error) {",
        "comment_created_at": "2025-07-12T20:23:37+00:00",
        "comment_author": "thockin",
        "comment_body": "I see no unit test for this function - can you please write one?  It shouldn't be too difficult.  Something like:\n\n* each case has vars for podName, podHostname, podSubdomain, and podHostnameOverride\n* each case has a field for the gate value\n* loop over cases\n* set the gate\n* construct a simple pod and set the fields as per above\n* run this function and look for expected results\n\nYou can test for too-long inputs and non-DNS-label as well as the final host/domain names",
        "pr_file_module": null
      },
      {
        "comment_id": "2203394526",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132558,
        "pr_file": "pkg/kubelet/kubelet_pods.go",
        "discussion_id": "2202904692",
        "commented_code": "@@ -570,6 +570,11 @@ func (kl *Kubelet) GetOrCreateUserNamespaceMappings(pod *v1.Pod, runtimeHandler\n func (kl *Kubelet) GeneratePodHostNameAndDomain(pod *v1.Pod) (string, string, error) {",
        "comment_created_at": "2025-07-13T13:38:44+00:00",
        "comment_author": "HirazawaUi",
        "comment_body": "Unit tests added.\r\n\r\nBut your comment sounds like an AI prompt...",
        "pr_file_module": null
      },
      {
        "comment_id": "2203580465",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132558,
        "pr_file": "pkg/kubelet/kubelet_pods.go",
        "discussion_id": "2202904692",
        "commented_code": "@@ -570,6 +570,11 @@ func (kl *Kubelet) GetOrCreateUserNamespaceMappings(pod *v1.Pod, runtimeHandler\n func (kl *Kubelet) GeneratePodHostNameAndDomain(pod *v1.Pod) (string, string, error) {",
        "comment_created_at": "2025-07-13T22:52:51+00:00",
        "comment_author": "thockin",
        "comment_body": "LOL - AI is a fine tool for stuff like this, IMO, as long as a human verifies it is right.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2210532623",
    "pr_number": 132626,
    "pr_file": "pkg/kubelet/util/env/env_util_test.go",
    "created_at": "2025-07-16T14:00:35+00:00",
    "commented_code": "+/*\n+Copyright 2025 The Kubernetes Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package env\n+\n+import (\n+\t\"os\"\n+\t\"strings\"\n+\t\"testing\"\n+)\n+\n+func TestParseEnv(t *testing.T) {\n+\ttempDir := t.TempDir()\n+\n+\ttype testCase struct {\n+\t\tname        string\n+\t\tenvContent  string\n+\t\tkey         string\n+\t\twantValue   string\n+\t\twantErr     bool\n+\t\terrContains string\n+\t}\n+\ttests := []testCase{\n+\t\t{\n+\t\t\tname: \"ignore leading whitespace\",\n+\t\t\tenvContent: `   KEY1=val1\n+\t\tKEY2=val2\n+KEY3=val3\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"val2\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"ignore blank and comment lines\",\n+\t\t\tenvContent: `# comment\n+\n+KEY1=foo\n+   # another comment\n+\t\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"bar\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"whitespace around = and trailing\",\n+\t\t\tenvContent: `KEY1 = val1   \n+KEY2=   val2\t\n+KEY3=val3   \t\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"val2\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation line with \\\\\",\n+\t\t\tenvContent: `KEY1=foo \\\n+bar \\\n+baz\n+KEY2=val2\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo bar baz\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation with whitespace and comment\",\n+\t\t\tenvContent: `KEY1=foo \\\n+  bar\n+# comment\n+KEY2=val2\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo bar\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"invalid line triggers error\",\n+\t\t\tenvContent: `KEY1=foo\n+INVALID_LINE\n+KEY2=bar\n+`,\n+\t\t\tkey:         \"KEY2\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"invalid environment variable format\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"unfinished continuation triggers error\",\n+\t\t\tenvContent: `KEY1=foo \\\n+bar \\\n+`,\n+\t\t\tkey:         \"KEY1\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"unfinished line continuation\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"key not found returns empty\",\n+\t\t\tenvContent: `KEY1=foo\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY3\",\n+\t\t\twantValue: \"\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"value with embedded #\",\n+\t\t\tenvContent: `KEY1=foo#notcomment\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo#notcomment\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"key with trailing whitespace\",\n+\t\t\tenvContent: `KEY1 =foo\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"value with leading and trailing whitespace\",\n+\t\t\tenvContent: `KEY1=   foo bar   \n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo bar\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"multiple comments and blank lines\",\n+\t\t\tenvContent: `# first comment\n+\n+# second comment\n+KEY1=foo\n+\n+# third comment\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"bar\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation with blank line in between (should error)\",\n+\t\t\tenvContent: `KEY1=foo \\\n+\n+bar\n+`,\n+\t\t\tkey:         \"KEY1\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"invalid environment variable format\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation with comment in between (should error)\",\n+\t\t\tenvContent: `KEY1=foo \\\n+# comment\n+bar\n+`,\n+\t\t\tkey:         \"KEY1\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"invalid environment variable format\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"empty value\",\n+\t\t\tenvContent: `KEY1=foo\n+KEY2=\n+KEY3=bar\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"value with only spaces\",\n+\t\t\tenvContent: `KEY1=foo\n+KEY2=   \n+KEY3=bar\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"key is empty (should error)\",\n+\t\t\tenvContent: `=foo\n+KEY2=bar\n+`,\n+\t\t\tkey:         \"KEY2\",\n+\t\t\twantValue:   \"bar\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"invalid environment variable format\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"multiple = in value\",\n+\t\t\tenvContent: `KEY1=foo=bar=baz\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo=bar=baz\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation with trailing spaces\",\n+\t\t\tenvContent: `KEY1=foo   \\\n+  bar   \\\n+  baz   \n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo bar baz\",\n+\t\t},",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2210532623",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132626,
        "pr_file": "pkg/kubelet/util/env/env_util_test.go",
        "discussion_id": "2210532623",
        "commented_code": "@@ -0,0 +1,253 @@\n+/*\n+Copyright 2025 The Kubernetes Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package env\n+\n+import (\n+\t\"os\"\n+\t\"strings\"\n+\t\"testing\"\n+)\n+\n+func TestParseEnv(t *testing.T) {\n+\ttempDir := t.TempDir()\n+\n+\ttype testCase struct {\n+\t\tname        string\n+\t\tenvContent  string\n+\t\tkey         string\n+\t\twantValue   string\n+\t\twantErr     bool\n+\t\terrContains string\n+\t}\n+\ttests := []testCase{\n+\t\t{\n+\t\t\tname: \"ignore leading whitespace\",\n+\t\t\tenvContent: `   KEY1=val1\n+\t\tKEY2=val2\n+KEY3=val3\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"val2\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"ignore blank and comment lines\",\n+\t\t\tenvContent: `# comment\n+\n+KEY1=foo\n+   # another comment\n+\t\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"bar\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"whitespace around = and trailing\",\n+\t\t\tenvContent: `KEY1 = val1   \n+KEY2=   val2\t\n+KEY3=val3   \t\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"val2\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation line with \\\\\",\n+\t\t\tenvContent: `KEY1=foo \\\n+bar \\\n+baz\n+KEY2=val2\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo bar baz\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation with whitespace and comment\",\n+\t\t\tenvContent: `KEY1=foo \\\n+  bar\n+# comment\n+KEY2=val2\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo bar\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"invalid line triggers error\",\n+\t\t\tenvContent: `KEY1=foo\n+INVALID_LINE\n+KEY2=bar\n+`,\n+\t\t\tkey:         \"KEY2\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"invalid environment variable format\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"unfinished continuation triggers error\",\n+\t\t\tenvContent: `KEY1=foo \\\n+bar \\\n+`,\n+\t\t\tkey:         \"KEY1\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"unfinished line continuation\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"key not found returns empty\",\n+\t\t\tenvContent: `KEY1=foo\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY3\",\n+\t\t\twantValue: \"\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"value with embedded #\",\n+\t\t\tenvContent: `KEY1=foo#notcomment\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo#notcomment\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"key with trailing whitespace\",\n+\t\t\tenvContent: `KEY1 =foo\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"value with leading and trailing whitespace\",\n+\t\t\tenvContent: `KEY1=   foo bar   \n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo bar\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"multiple comments and blank lines\",\n+\t\t\tenvContent: `# first comment\n+\n+# second comment\n+KEY1=foo\n+\n+# third comment\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"bar\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation with blank line in between (should error)\",\n+\t\t\tenvContent: `KEY1=foo \\\n+\n+bar\n+`,\n+\t\t\tkey:         \"KEY1\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"invalid environment variable format\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation with comment in between (should error)\",\n+\t\t\tenvContent: `KEY1=foo \\\n+# comment\n+bar\n+`,\n+\t\t\tkey:         \"KEY1\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"invalid environment variable format\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"empty value\",\n+\t\t\tenvContent: `KEY1=foo\n+KEY2=\n+KEY3=bar\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"value with only spaces\",\n+\t\t\tenvContent: `KEY1=foo\n+KEY2=   \n+KEY3=bar\n+`,\n+\t\t\tkey:       \"KEY2\",\n+\t\t\twantValue: \"\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"key is empty (should error)\",\n+\t\t\tenvContent: `=foo\n+KEY2=bar\n+`,\n+\t\t\tkey:         \"KEY2\",\n+\t\t\twantValue:   \"bar\",\n+\t\t\twantErr:     true,\n+\t\t\terrContains: \"invalid environment variable format\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"multiple = in value\",\n+\t\t\tenvContent: `KEY1=foo=bar=baz\n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo=bar=baz\",\n+\t\t},\n+\t\t{\n+\t\t\tname: \"continuation with trailing spaces\",\n+\t\t\tenvContent: `KEY1=foo   \\\n+  bar   \\\n+  baz   \n+KEY2=bar\n+`,\n+\t\t\tkey:       \"KEY1\",\n+\t\t\twantValue: \"foo bar baz\",\n+\t\t},",
        "comment_created_at": "2025-07-16T14:00:35+00:00",
        "comment_author": "toVersus",
        "comment_body": "Since we don\u2019t support cases where a comment starts after a key-value pair, it might be better to include one in the test cases.\r\n\r\n```go\r\n\t\t{\r\n\t\t\tname: \"comment after key value pair\",\r\n\t\t\tenvContent: `KEY1=foo\r\nKEY2=bar # comment\r\n`,\r\n\t\t\tkey:       \"KEY2\",\r\n\t\t\twantValue: \"bar # comment\",\r\n\t\t},\r\n```",
        "pr_file_module": null
      }
    ]
  }
]