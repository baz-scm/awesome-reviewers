[
  {
    "discussion_id": "2187453157",
    "pr_number": 132522,
    "pr_file": "pkg/registry/resource/resourceclaim/strategy.go",
    "created_at": "2025-07-05T17:01:05+00:00",
    "commented_code": "}\n \n // TODO: add tests after partitionable devices is merged (code conflict!)\n+\n+// dropDisabledDRAResourceClaimConsumableCapacity drops capacity request.\n+// However, shareID and consumed capacity result are kept.\n+func dropDisabledDRAResourceClaimConsumableCapacity(newClaim, oldClaim *resource.ResourceClaim) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) || newClaim == nil {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2187453157",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceclaim/strategy.go",
        "discussion_id": "2187453157",
        "commented_code": "@@ -345,3 +358,19 @@ func dropDeallocatedStatusDevices(newClaim, oldClaim *resource.ResourceClaim) {\n }\n \n // TODO: add tests after partitionable devices is merged (code conflict!)\n+\n+// dropDisabledDRAResourceClaimConsumableCapacity drops capacity request.\n+// However, shareID and consumed capacity result are kept.\n+func dropDisabledDRAResourceClaimConsumableCapacity(newClaim, oldClaim *resource.ResourceClaim) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) || newClaim == nil {",
        "comment_created_at": "2025-07-05T17:01:05+00:00",
        "comment_author": "mortent",
        "comment_body": "This seems to drop the `CapacityRequests` fields even if they were already in use by the object. This will lead to the fields being dropped on unrelated changes, which is probably not what we want. Any reason why this is not following the same pattern as the other features?",
        "pr_file_module": null
      },
      {
        "comment_id": "2188805934",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceclaim/strategy.go",
        "discussion_id": "2187453157",
        "commented_code": "@@ -345,3 +358,19 @@ func dropDeallocatedStatusDevices(newClaim, oldClaim *resource.ResourceClaim) {\n }\n \n // TODO: add tests after partitionable devices is merged (code conflict!)\n+\n+// dropDisabledDRAResourceClaimConsumableCapacity drops capacity request.\n+// However, shareID and consumed capacity result are kept.\n+func dropDisabledDRAResourceClaimConsumableCapacity(newClaim, oldClaim *resource.ResourceClaim) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) || newClaim == nil {",
        "comment_created_at": "2025-07-07T01:44:28+00:00",
        "comment_author": "sunya-ch",
        "comment_body": "@mortent I kept only the consumed capacity and shareID in the status since it is already allocated values. However, the value in resourceclaim's spec is dropped to not have it in the further use. Do you think should I also keep those fields?",
        "pr_file_module": null
      },
      {
        "comment_id": "2189081268",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceclaim/strategy.go",
        "discussion_id": "2187453157",
        "commented_code": "@@ -345,3 +358,19 @@ func dropDeallocatedStatusDevices(newClaim, oldClaim *resource.ResourceClaim) {\n }\n \n // TODO: add tests after partitionable devices is merged (code conflict!)\n+\n+// dropDisabledDRAResourceClaimConsumableCapacity drops capacity request.\n+// However, shareID and consumed capacity result are kept.\n+func dropDisabledDRAResourceClaimConsumableCapacity(newClaim, oldClaim *resource.ResourceClaim) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) || newClaim == nil {",
        "comment_created_at": "2025-07-07T06:22:48+00:00",
        "comment_author": "pohly",
        "comment_body": "Yes, always keep the fields if used anywhere in the old object. Think of adding a label to a ResourceClaim: that should not remove existing fields.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2190104112",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceclaim/strategy.go",
        "discussion_id": "2187453157",
        "commented_code": "@@ -345,3 +358,19 @@ func dropDeallocatedStatusDevices(newClaim, oldClaim *resource.ResourceClaim) {\n }\n \n // TODO: add tests after partitionable devices is merged (code conflict!)\n+\n+// dropDisabledDRAResourceClaimConsumableCapacity drops capacity request.\n+// However, shareID and consumed capacity result are kept.\n+func dropDisabledDRAResourceClaimConsumableCapacity(newClaim, oldClaim *resource.ResourceClaim) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) || newClaim == nil {",
        "comment_created_at": "2025-07-07T13:24:11+00:00",
        "comment_author": "sunya-ch",
        "comment_body": "Got it. For sure, I should keep the CapacityRequests if there is any allocation result. \r\nHowever, by the following change,\r\n- Throw error if the requests contain capacity request but feature is disabled  according to the comment https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2187652313\r\n\r\nCan I drop the CapacityRequests if no allocation yet and allow the ResourceClaim to be allocated\r\nor should I also keep the CapacityRequests even if no allocation result and the claim will become always-failed (with information that there is capacity request but feature is disabled), and users are forced to delete and recreate that ResourceClaim without CapacityRequests?  \r\n\r\nFor ResourceSlice, I believe that I can drop the allowMultipleAllocations and sharingPolicy field even though it was used for computing previous allocation, can't I?",
        "pr_file_module": null
      },
      {
        "comment_id": "2190533857",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceclaim/strategy.go",
        "discussion_id": "2187453157",
        "commented_code": "@@ -345,3 +358,19 @@ func dropDeallocatedStatusDevices(newClaim, oldClaim *resource.ResourceClaim) {\n }\n \n // TODO: add tests after partitionable devices is merged (code conflict!)\n+\n+// dropDisabledDRAResourceClaimConsumableCapacity drops capacity request.\n+// However, shareID and consumed capacity result are kept.\n+func dropDisabledDRAResourceClaimConsumableCapacity(newClaim, oldClaim *resource.ResourceClaim) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) || newClaim == nil {",
        "comment_created_at": "2025-07-07T16:19:41+00:00",
        "comment_author": "mortent",
        "comment_body": "I think we always want to keep the fields on update, even if the feature is disabled. So for the ResourceClaim, we want to error out if the ResourceClaim is using a feature that is not currently enabled. Anything else could lead to suprising behavior for users.\r\n\r\nFor ResourceSlice we usually handle this situation by not considering devices that uses fields for disabled features. We have an example here: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go#L1012-L1017. So even for ResourceSlice, we don't just drop the fields. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2187502109",
    "pr_number": 132522,
    "pr_file": "pkg/registry/resource/resourceslice/strategy.go",
    "created_at": "2025-07-05T17:26:59+00:00",
    "commented_code": "}\n \treturn false\n }\n+\n+func dropDisabledDRAConsumableCapacityFields(newSlice, oldSlice *resource.ResourceSlice) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2187502109",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceslice/strategy.go",
        "discussion_id": "2187502109",
        "commented_code": "@@ -230,3 +231,22 @@ func draPartitionableDevicesFeatureInUse(slice *resource.ResourceSlice) bool {\n \t}\n \treturn false\n }\n+\n+func dropDisabledDRAConsumableCapacityFields(newSlice, oldSlice *resource.ResourceSlice) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) {",
        "comment_created_at": "2025-07-05T17:26:59+00:00",
        "comment_author": "mortent",
        "comment_body": "Similar to the ResourceClaim, we should not drop existing fields when the feature is disabled.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2198950363",
    "pr_number": 132522,
    "pr_file": "pkg/apis/resource/types.go",
    "created_at": "2025-07-10T23:01:35+00:00",
    "commented_code": "Value resource.Quantity\n }\n \n+// CapacitySharingPolicyDiscreteMaxOptions limits the number of discrete capacity values allowed in a sharing policy.\n+const CapacitySharingPolicyDiscreteMaxOptions = 10\n+\n+// CapacitySharingPolicy defines how requests consume the available capacity.\n+// A policy must have a default value to be applied when no value is explicitly provided.",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2198950363",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/apis/resource/types.go",
        "discussion_id": "2198950363",
        "commented_code": "@@ -363,6 +389,96 @@ type Counter struct {\n \tValue resource.Quantity\n }\n \n+// CapacitySharingPolicyDiscreteMaxOptions limits the number of discrete capacity values allowed in a sharing policy.\n+const CapacitySharingPolicyDiscreteMaxOptions = 10\n+\n+// CapacitySharingPolicy defines how requests consume the available capacity.\n+// A policy must have a default value to be applied when no value is explicitly provided.",
        "comment_created_at": "2025-07-10T23:01:35+00:00",
        "comment_author": "mortent",
        "comment_body": "```suggestion\r\n// A policy must have a default value to be applied when no value is explicitly provided in the ResourceClaim.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2066032538",
    "pr_number": 130160,
    "pr_file": "pkg/registry/resource/resourceslice/strategy.go",
    "created_at": "2025-04-29T10:41:59+00:00",
    "commented_code": "}\n }\n \n+func dropDisabledDRADeviceBindingConditionsFields(newSlice, oldSlice *resource.ResourceSlice) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRADeviceBindingConditions) && utilfeature.DefaultFeatureGate.Enabled(features.DRAResourceClaimDeviceStatus) {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2066032538",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/registry/resource/resourceslice/strategy.go",
        "discussion_id": "2066032538",
        "commented_code": "@@ -210,6 +211,19 @@ func dropDisabledDRAPartitionableDevicesFields(newSlice, oldSlice *resource.Reso\n \t}\n }\n \n+func dropDisabledDRADeviceBindingConditionsFields(newSlice, oldSlice *resource.ResourceSlice) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRADeviceBindingConditions) && utilfeature.DefaultFeatureGate.Enabled(features.DRAResourceClaimDeviceStatus) {",
        "comment_created_at": "2025-04-29T10:41:59+00:00",
        "comment_author": "pohly",
        "comment_body": "Don't drop the fields if the old slice already had them. Otherwise adding a label to an existing slice would wipe out the fields.\r\n\r\nI'm torn about checking both features here. On the one hand, we don't want to allow setting fields that the cluster is not configured to support, so we have to check both. On the other hand the fields are said to be feature-gated only by DRADeviceBindingConditions.\r\n\r\nI'm leaning towards updating the field comments to:\r\n```\r\n\t// This is an alpha field and requires enabling the DRADeviceBindingConditions and DRAResourceClaimDeviceStatus\r\n\t// feature gates.\r\n\t//\r\n\t// +optional\r\n\t// +featureGate=DRADeviceBindingConditions,DRAResourceClaimDeviceStatus\r\n```\r\n\r\nBut I need to ask around regarding this, it is unusual.",
        "pr_file_module": null
      },
      {
        "comment_id": "2076659735",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/registry/resource/resourceslice/strategy.go",
        "discussion_id": "2066032538",
        "commented_code": "@@ -210,6 +211,19 @@ func dropDisabledDRAPartitionableDevicesFields(newSlice, oldSlice *resource.Reso\n \t}\n }\n \n+func dropDisabledDRADeviceBindingConditionsFields(newSlice, oldSlice *resource.ResourceSlice) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRADeviceBindingConditions) && utilfeature.DefaultFeatureGate.Enabled(features.DRAResourceClaimDeviceStatus) {",
        "comment_created_at": "2025-05-07T02:26:41+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "For now, I will add a process to check the contents of oldslice.\r\n\r\nShould I add comments? Is it okay to just provide instructions in the documentation?",
        "pr_file_module": null
      },
      {
        "comment_id": "2076899049",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/registry/resource/resourceslice/strategy.go",
        "discussion_id": "2066032538",
        "commented_code": "@@ -210,6 +211,19 @@ func dropDisabledDRAPartitionableDevicesFields(newSlice, oldSlice *resource.Reso\n \t}\n }\n \n+func dropDisabledDRADeviceBindingConditionsFields(newSlice, oldSlice *resource.ResourceSlice) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRADeviceBindingConditions) && utilfeature.DefaultFeatureGate.Enabled(features.DRAResourceClaimDeviceStatus) {",
        "comment_created_at": "2025-05-07T06:38:46+00:00",
        "comment_author": "pohly",
        "comment_body": "Jordan confirmed on Slack that the comment that I had proposed above is correct. Please add it.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2201037292",
    "pr_number": 130653,
    "pr_file": "staging/src/k8s.io/api/resource/v1beta1/types.go",
    "created_at": "2025-07-11T15:31:25+00:00",
    "commented_code": "// it got removed. May be reused once decoding v1alpha3 is no longer\n \t// supported.\n \t// SuitableNodes *v1.NodeSelector `json:\"suitableNodes,omitempty\" protobuf:\"bytes,3,opt,name=suitableNodes\"`\n+\n+\t// ExtendedResourceName is the extended resource name for the devices of this class.\n+\t// This is an alpha field.\n+\t// +optional\n+\t// +featureGate=DRAExtendedResource",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2201037292",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "staging/src/k8s.io/api/resource/v1beta1/types.go",
        "discussion_id": "2201037292",
        "commented_code": "@@ -1387,6 +1387,12 @@ type DeviceClassSpec struct {\n \t// it got removed. May be reused once decoding v1alpha3 is no longer\n \t// supported.\n \t// SuitableNodes *v1.NodeSelector `json:\"suitableNodes,omitempty\" protobuf:\"bytes,3,opt,name=suitableNodes\"`\n+\n+\t// ExtendedResourceName is the extended resource name for the devices of this class.\n+\t// This is an alpha field.\n+\t// +optional\n+\t// +featureGate=DRAExtendedResource",
        "comment_created_at": "2025-07-11T15:31:25+00:00",
        "comment_author": "pohly",
        "comment_body": "Feature gated fields must be dropped if the feature gate is disabled and the field is not set already.\r\n\r\nSee https://github.com/kubernetes/kubernetes/blob/9822e51403e46cc0aeae49cf64eae653eff29f9e/pkg/registry/resource/resourceclaim/strategy.go#L212-L345 for how that is done for ResourceClaim. You need to add similar code to https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/resource/deviceclass/strategy.go and to https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/pod/strategy.go for `ExtendedResourceClaimStatus` - currently both is missing.",
        "pr_file_module": null
      },
      {
        "comment_id": "2203488716",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "staging/src/k8s.io/api/resource/v1beta1/types.go",
        "discussion_id": "2201037292",
        "commented_code": "@@ -1387,6 +1387,12 @@ type DeviceClassSpec struct {\n \t// it got removed. May be reused once decoding v1alpha3 is no longer\n \t// supported.\n \t// SuitableNodes *v1.NodeSelector `json:\"suitableNodes,omitempty\" protobuf:\"bytes,3,opt,name=suitableNodes\"`\n+\n+\t// ExtendedResourceName is the extended resource name for the devices of this class.\n+\t// This is an alpha field.\n+\t// +optional\n+\t// +featureGate=DRAExtendedResource",
        "comment_created_at": "2025-07-13T18:35:50+00:00",
        "comment_author": "yliaog",
        "comment_body": "added strategy",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2209071867",
    "pr_number": 130653,
    "pr_file": "pkg/api/pod/util.go",
    "created_at": "2025-07-16T02:24:03+00:00",
    "commented_code": "dropAllocatedResourcesField(podStatus.EphemeralContainerStatuses)\n \t}\n \n-\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec) {\n+\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec, oldPodStatus) {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2209071867",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2209071867",
        "commented_code": "@@ -906,8 +906,9 @@ func dropDisabledPodStatusFields(podStatus, oldPodStatus *api.PodStatus, podSpec\n \t\tdropAllocatedResourcesField(podStatus.EphemeralContainerStatuses)\n \t}\n \n-\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec) {\n+\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec, oldPodStatus) {",
        "comment_created_at": "2025-07-16T02:24:03+00:00",
        "comment_author": "liggitt",
        "comment_body": "this block should not be modified\r\n\r\nadd a new if block that checks the DRAExtendedResource gate and just clears the ExtendedResourceClaimStatus field",
        "pr_file_module": null
      },
      {
        "comment_id": "2211731303",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2209071867",
        "commented_code": "@@ -906,8 +906,9 @@ func dropDisabledPodStatusFields(podStatus, oldPodStatus *api.PodStatus, podSpec\n \t\tdropAllocatedResourcesField(podStatus.EphemeralContainerStatuses)\n \t}\n \n-\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec) {\n+\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec, oldPodStatus) {",
        "comment_created_at": "2025-07-16T22:28:59+00:00",
        "comment_author": "yliaog",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2209074218",
    "pr_number": 130653,
    "pr_file": "pkg/api/pod/util.go",
    "created_at": "2025-07-16T02:25:09+00:00",
    "commented_code": "// container specs and pod-level resource claims unless they are already used\n // by the old pod spec.\n func dropDisabledDynamicResourceAllocationFields(podSpec, oldPodSpec *api.PodSpec) {\n-\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec) {\n+\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec, nil) {\n \t\tdropResourceClaimRequests(podSpec.Containers)\n \t\tdropResourceClaimRequests(podSpec.InitContainers)\n \t\tdropEphemeralResourceClaimRequests(podSpec.EphemeralContainers)\n \t\tpodSpec.ResourceClaims = nil\n \t}\n }\n \n-func dynamicResourceAllocationInUse(podSpec *api.PodSpec) bool {\n-\tif podSpec == nil {\n-\t\treturn false\n-\t}\n-\n+func dynamicResourceAllocationInUse(podSpec *api.PodSpec, podStatus *api.PodStatus) bool {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2209074218",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2209074218",
        "commented_code": "@@ -957,23 +958,25 @@ func dropDisabledPodStatusFields(podStatus, oldPodStatus *api.PodStatus, podSpec\n // container specs and pod-level resource claims unless they are already used\n // by the old pod spec.\n func dropDisabledDynamicResourceAllocationFields(podSpec, oldPodSpec *api.PodSpec) {\n-\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec) {\n+\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec, nil) {\n \t\tdropResourceClaimRequests(podSpec.Containers)\n \t\tdropResourceClaimRequests(podSpec.InitContainers)\n \t\tdropEphemeralResourceClaimRequests(podSpec.EphemeralContainers)\n \t\tpodSpec.ResourceClaims = nil\n \t}\n }\n \n-func dynamicResourceAllocationInUse(podSpec *api.PodSpec) bool {\n-\tif podSpec == nil {\n-\t\treturn false\n-\t}\n-\n+func dynamicResourceAllocationInUse(podSpec *api.PodSpec, podStatus *api.PodStatus) bool {",
        "comment_created_at": "2025-07-16T02:25:09+00:00",
        "comment_author": "liggitt",
        "comment_body": "don't modify this function, add a distinct function that checks if the ExtendedResourceClaimStatus field is set",
        "pr_file_module": null
      },
      {
        "comment_id": "2211731726",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2209074218",
        "commented_code": "@@ -957,23 +958,25 @@ func dropDisabledPodStatusFields(podStatus, oldPodStatus *api.PodStatus, podSpec\n // container specs and pod-level resource claims unless they are already used\n // by the old pod spec.\n func dropDisabledDynamicResourceAllocationFields(podSpec, oldPodSpec *api.PodSpec) {\n-\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec) {\n+\tif !utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) && !dynamicResourceAllocationInUse(oldPodSpec, nil) {\n \t\tdropResourceClaimRequests(podSpec.Containers)\n \t\tdropResourceClaimRequests(podSpec.InitContainers)\n \t\tdropEphemeralResourceClaimRequests(podSpec.EphemeralContainers)\n \t\tpodSpec.ResourceClaims = nil\n \t}\n }\n \n-func dynamicResourceAllocationInUse(podSpec *api.PodSpec) bool {\n-\tif podSpec == nil {\n-\t\treturn false\n-\t}\n-\n+func dynamicResourceAllocationInUse(podSpec *api.PodSpec, podStatus *api.PodStatus) bool {",
        "comment_created_at": "2025-07-16T22:29:14+00:00",
        "comment_author": "yliaog",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2221993037",
    "pr_number": 133021,
    "pr_file": "pkg/scheduler/schedule_one.go",
    "created_at": "2025-07-22T10:06:58+00:00",
    "commented_code": "assumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2221993037",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133021,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2221993037",
        "commented_code": "@@ -279,6 +281,26 @@ func (sched *Scheduler) bindingCycle(\n \n \tassumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {",
        "comment_created_at": "2025-07-22T10:06:58+00:00",
        "comment_author": "macsko",
        "comment_body": "Use a `Scheduler` struct field to check the feature gate state similarly to the #132439",
        "pr_file_module": null
      },
      {
        "comment_id": "2224388387",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133021,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2221993037",
        "commented_code": "@@ -279,6 +281,26 @@ func (sched *Scheduler) bindingCycle(\n \n \tassumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {",
        "comment_created_at": "2025-07-23T05:27:51+00:00",
        "comment_author": "sanposhiho",
        "comment_body": "Why? We already do feature.DefaultFeatureGate.Enabled a lot everywhere",
        "pr_file_module": null
      },
      {
        "comment_id": "2224579491",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133021,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2221993037",
        "commented_code": "@@ -279,6 +281,26 @@ func (sched *Scheduler) bindingCycle(\n \n \tassumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {",
        "comment_created_at": "2025-07-23T06:51:29+00:00",
        "comment_author": "macsko",
        "comment_body": "It's not very cheap operation to obtain the feature's value. If we can easily use a field, I'd do that.",
        "pr_file_module": null
      },
      {
        "comment_id": "2225734715",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133021,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2221993037",
        "commented_code": "@@ -279,6 +281,26 @@ func (sched *Scheduler) bindingCycle(\n \n \tassumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {",
        "comment_created_at": "2025-07-23T14:08:12+00:00",
        "comment_author": "sanposhiho",
        "comment_body": "Fixed with your suggestion because we have a close deadline.\r\nBut, I'm still not convinced that we should be concerned about the performance here. Everyone including the scheduler just does feature.DefaultFeatureGate.Enabled in every single code. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2220446035",
    "pr_number": 132642,
    "pr_file": "staging/src/k8s.io/api/core/v1/types.go",
    "created_at": "2025-07-21T21:58:26+00:00",
    "commented_code": "// +featureGate=SidecarContainers\n \t// +optional\n \tRestartPolicy *ContainerRestartPolicy `json:\"restartPolicy,omitempty\" protobuf:\"bytes,24,opt,name=restartPolicy,casttype=ContainerRestartPolicy\"`\n+\t// Represents a list of rules to be checked to determine if the\n+\t// container should be restarted on exit. The rules are evaluated in\n+\t// order. Once a rule matches a container exit condition, the remaining\n+\t// rules are ignored. If no rule matches the container exit condition,\n+\t// the Pod-level restart policy determines the whether the container\n+\t// is restarted or not. Constraints on the rules:\n+\t// - At most 20 rules are allowed.\n+\t// - Rules can have the same action.\n+\t// - Identical rules are not forbidden in validations.",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2220446035",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "staging/src/k8s.io/api/core/v1/types.go",
        "discussion_id": "2220446035",
        "commented_code": "@@ -2847,6 +2847,19 @@ type Container struct {\n \t// +featureGate=SidecarContainers\n \t// +optional\n \tRestartPolicy *ContainerRestartPolicy `json:\"restartPolicy,omitempty\" protobuf:\"bytes,24,opt,name=restartPolicy,casttype=ContainerRestartPolicy\"`\n+\t// Represents a list of rules to be checked to determine if the\n+\t// container should be restarted on exit. The rules are evaluated in\n+\t// order. Once a rule matches a container exit condition, the remaining\n+\t// rules are ignored. If no rule matches the container exit condition,\n+\t// the Pod-level restart policy determines the whether the container\n+\t// is restarted or not. Constraints on the rules:\n+\t// - At most 20 rules are allowed.\n+\t// - Rules can have the same action.\n+\t// - Identical rules are not forbidden in validations.",
        "comment_created_at": "2025-07-21T21:58:26+00:00",
        "comment_author": "SergeyKanzhelev",
        "comment_body": "```suggestion\r\n\t// - Identical rules are not forbidden in validations.\r\n\t// When rules are specified, container MUST set the RestartPolicy explicitly even if it matches the Pod's restartPolicy\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2220610001",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "staging/src/k8s.io/api/core/v1/types.go",
        "discussion_id": "2220446035",
        "commented_code": "@@ -2847,6 +2847,19 @@ type Container struct {\n \t// +featureGate=SidecarContainers\n \t// +optional\n \tRestartPolicy *ContainerRestartPolicy `json:\"restartPolicy,omitempty\" protobuf:\"bytes,24,opt,name=restartPolicy,casttype=ContainerRestartPolicy\"`\n+\t// Represents a list of rules to be checked to determine if the\n+\t// container should be restarted on exit. The rules are evaluated in\n+\t// order. Once a rule matches a container exit condition, the remaining\n+\t// rules are ignored. If no rule matches the container exit condition,\n+\t// the Pod-level restart policy determines the whether the container\n+\t// is restarted or not. Constraints on the rules:\n+\t// - At most 20 rules are allowed.\n+\t// - Rules can have the same action.\n+\t// - Identical rules are not forbidden in validations.",
        "comment_created_at": "2025-07-22T00:04:34+00:00",
        "comment_author": "yuanwang04",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2223021828",
    "pr_number": 132642,
    "pr_file": "pkg/api/pod/util.go",
    "created_at": "2025-07-22T15:50:01+00:00",
    "commented_code": "// length or if the annotation has an unrecognized value\n \treturn nil\n }\n+\n+func dropContainerRestartRules(podSpec *api.PodSpec) {\n+\tif podSpec == nil {\n+\t\treturn\n+\t}\n+\tfor i, c := range podSpec.InitContainers {\n+\t\tif c.RestartPolicy != nil && *c.RestartPolicy != api.ContainerRestartPolicyAlways {\n+\t\t\tpodSpec.InitContainers[i].RestartPolicy = nil\n+\t\t\tpodSpec.InitContainers[i].RestartPolicyRules = nil",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2223021828",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2223021828",
        "commented_code": "@@ -1558,3 +1562,39 @@ func ApparmorFieldForAnnotation(annotation string) *api.AppArmorProfile {\n \t// length or if the annotation has an unrecognized value\n \treturn nil\n }\n+\n+func dropContainerRestartRules(podSpec *api.PodSpec) {\n+\tif podSpec == nil {\n+\t\treturn\n+\t}\n+\tfor i, c := range podSpec.InitContainers {\n+\t\tif c.RestartPolicy != nil && *c.RestartPolicy != api.ContainerRestartPolicyAlways {\n+\t\t\tpodSpec.InitContainers[i].RestartPolicy = nil\n+\t\t\tpodSpec.InitContainers[i].RestartPolicyRules = nil",
        "comment_created_at": "2025-07-22T15:50:01+00:00",
        "comment_author": "haircommander",
        "comment_body": "shouldn't this be done regardless of whether the policy is ContainerRestartPolicyAlways?",
        "pr_file_module": null
      },
      {
        "comment_id": "2223736170",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2223021828",
        "commented_code": "@@ -1558,3 +1562,39 @@ func ApparmorFieldForAnnotation(annotation string) *api.AppArmorProfile {\n \t// length or if the annotation has an unrecognized value\n \treturn nil\n }\n+\n+func dropContainerRestartRules(podSpec *api.PodSpec) {\n+\tif podSpec == nil {\n+\t\treturn\n+\t}\n+\tfor i, c := range podSpec.InitContainers {\n+\t\tif c.RestartPolicy != nil && *c.RestartPolicy != api.ContainerRestartPolicyAlways {\n+\t\t\tpodSpec.InitContainers[i].RestartPolicy = nil\n+\t\t\tpodSpec.InitContainers[i].RestartPolicyRules = nil",
        "comment_created_at": "2025-07-22T20:13:12+00:00",
        "comment_author": "yuanwang04",
        "comment_body": "Our KEP was that init containers (except sidecar containers) can also have restart policies (OnFailure / Never) and restart rules, so we are only dropping the rules for sidecar containers",
        "pr_file_module": null
      },
      {
        "comment_id": "2224095229",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2223021828",
        "commented_code": "@@ -1558,3 +1562,39 @@ func ApparmorFieldForAnnotation(annotation string) *api.AppArmorProfile {\n \t// length or if the annotation has an unrecognized value\n \treturn nil\n }\n+\n+func dropContainerRestartRules(podSpec *api.PodSpec) {\n+\tif podSpec == nil {\n+\t\treturn\n+\t}\n+\tfor i, c := range podSpec.InitContainers {\n+\t\tif c.RestartPolicy != nil && *c.RestartPolicy != api.ContainerRestartPolicyAlways {\n+\t\t\tpodSpec.InitContainers[i].RestartPolicy = nil\n+\t\t\tpodSpec.InitContainers[i].RestartPolicyRules = nil",
        "comment_created_at": "2025-07-23T00:59:26+00:00",
        "comment_author": "msau42",
        "comment_body": "Since `RestartPolicyRules` is a new field, we should drop it for all InitContainers.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2224214510",
    "pr_number": 132642,
    "pr_file": "pkg/api/pod/util.go",
    "created_at": "2025-07-23T03:02:48+00:00",
    "commented_code": "// length or if the annotation has an unrecognized value\n \treturn nil\n }\n+\n+func dropContainerRestartRules(podSpec *api.PodSpec) {\n+\tif podSpec == nil {\n+\t\treturn\n+\t}\n+\tfor i, c := range podSpec.InitContainers {\n+\t\tif c.RestartPolicy != nil && *c.RestartPolicy != api.ContainerRestartPolicyAlways {\n+\t\t\tpodSpec.InitContainers[i].RestartPolicy = nil\n+\t\t\tpodSpec.InitContainers[i].RestartPolicyRules = nil",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2224214510",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2224214510",
        "commented_code": "@@ -1677,3 +1681,39 @@ func ApparmorFieldForAnnotation(annotation string) *api.AppArmorProfile {\n \t// length or if the annotation has an unrecognized value\n \treturn nil\n }\n+\n+func dropContainerRestartRules(podSpec *api.PodSpec) {\n+\tif podSpec == nil {\n+\t\treturn\n+\t}\n+\tfor i, c := range podSpec.InitContainers {\n+\t\tif c.RestartPolicy != nil && *c.RestartPolicy != api.ContainerRestartPolicyAlways {\n+\t\t\tpodSpec.InitContainers[i].RestartPolicy = nil\n+\t\t\tpodSpec.InitContainers[i].RestartPolicyRules = nil",
        "comment_created_at": "2025-07-23T03:02:48+00:00",
        "comment_author": "msau42",
        "comment_body": "Starting a new comment here since I can't unresolve the previous comment:\r\n\r\nSince RestartPolicyRules is a new field, we should drop it for all InitContainers.",
        "pr_file_module": null
      },
      {
        "comment_id": "2226782798",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/api/pod/util.go",
        "discussion_id": "2224214510",
        "commented_code": "@@ -1677,3 +1681,39 @@ func ApparmorFieldForAnnotation(annotation string) *api.AppArmorProfile {\n \t// length or if the annotation has an unrecognized value\n \treturn nil\n }\n+\n+func dropContainerRestartRules(podSpec *api.PodSpec) {\n+\tif podSpec == nil {\n+\t\treturn\n+\t}\n+\tfor i, c := range podSpec.InitContainers {\n+\t\tif c.RestartPolicy != nil && *c.RestartPolicy != api.ContainerRestartPolicyAlways {\n+\t\t\tpodSpec.InitContainers[i].RestartPolicy = nil\n+\t\t\tpodSpec.InitContainers[i].RestartPolicyRules = nil",
        "comment_created_at": "2025-07-23T21:53:29+00:00",
        "comment_author": "yuanwang04",
        "comment_body": "SG, dropping it for initContainers.",
        "pr_file_module": null
      }
    ]
  }
]