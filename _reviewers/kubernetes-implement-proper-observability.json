[
  {
    "discussion_id": "1985659505",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-03-07T20:26:44+00:00",
    "commented_code": "}\n \treturn framework.AsStatus(err)\n }\n+\n+func getAllocatedDeviceStatus(claim *resourceapi.ResourceClaim, deviceRequest *resourceapi.DeviceRequestAllocationResult) *resourceapi.AllocatedDeviceStatus {\n+\tfor _, device := range claim.Status.Devices {\n+\t\tif deviceRequest.Device == device.Device && deviceRequest.Driver == device.Driver && deviceRequest.Pool == device.Pool {\n+\t\t\treturn &device\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func isClaimBound(logger klog.Logger, claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\treturn false, nil",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "1985659505",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1985659505",
        "commented_code": "@@ -895,3 +965,73 @@ func statusError(logger klog.Logger, err error, kv ...interface{}) *framework.St\n \t}\n \treturn framework.AsStatus(err)\n }\n+\n+func getAllocatedDeviceStatus(claim *resourceapi.ResourceClaim, deviceRequest *resourceapi.DeviceRequestAllocationResult) *resourceapi.AllocatedDeviceStatus {\n+\tfor _, device := range claim.Status.Devices {\n+\t\tif deviceRequest.Device == device.Device && deviceRequest.Driver == device.Driver && deviceRequest.Pool == device.Pool {\n+\t\t\treturn &device\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func isClaimBound(logger klog.Logger, claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\treturn false, nil",
        "comment_created_at": "2025-03-07T20:26:44+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "if we remove the code above that creates the status (which I think we should), then this case is likely to hit for a while. we can add an logger as we do in the other cases, saying we are waiting for the driver to report status\r\n\r\nIn fact this should be an event, not just a log.",
        "pr_file_module": null
      },
      {
        "comment_id": "1986473734",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1985659505",
        "commented_code": "@@ -895,3 +965,73 @@ func statusError(logger klog.Logger, err error, kv ...interface{}) *framework.St\n \t}\n \treturn framework.AsStatus(err)\n }\n+\n+func getAllocatedDeviceStatus(claim *resourceapi.ResourceClaim, deviceRequest *resourceapi.DeviceRequestAllocationResult) *resourceapi.AllocatedDeviceStatus {\n+\tfor _, device := range claim.Status.Devices {\n+\t\tif deviceRequest.Device == device.Device && deviceRequest.Driver == device.Driver && deviceRequest.Pool == device.Pool {\n+\t\t\treturn &device\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func isClaimBound(logger klog.Logger, claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\treturn false, nil",
        "comment_created_at": "2025-03-10T00:46:38+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": ">In fact this should be an event, not just a log.\r\n\r\nWhat does this mean in terms of implementation?\r\nFor logs, I think I can use logger.Info etc., but for events, what would be appropriate to use?\r\n[EDIT] I found `pl.fh.EventRecorder().Eventf()`, is it appropriate?",
        "pr_file_module": null
      },
      {
        "comment_id": "1987685714",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1985659505",
        "commented_code": "@@ -895,3 +965,73 @@ func statusError(logger klog.Logger, err error, kv ...interface{}) *framework.St\n \t}\n \treturn framework.AsStatus(err)\n }\n+\n+func getAllocatedDeviceStatus(claim *resourceapi.ResourceClaim, deviceRequest *resourceapi.DeviceRequestAllocationResult) *resourceapi.AllocatedDeviceStatus {\n+\tfor _, device := range claim.Status.Devices {\n+\t\tif deviceRequest.Device == device.Device && deviceRequest.Driver == device.Driver && deviceRequest.Pool == device.Pool {\n+\t\t\treturn &device\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func isClaimBound(logger klog.Logger, claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\treturn false, nil",
        "comment_created_at": "2025-03-10T16:56:27+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "Yes, probably that's it. @pohly can say for sure.\r\n\r\nEvents are visible when the user does `kubectl describe` on the pod that's being scheduled. Log entries are not visible to users.",
        "pr_file_module": null
      },
      {
        "comment_id": "1987880587",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1985659505",
        "commented_code": "@@ -895,3 +965,73 @@ func statusError(logger klog.Logger, err error, kv ...interface{}) *framework.St\n \t}\n \treturn framework.AsStatus(err)\n }\n+\n+func getAllocatedDeviceStatus(claim *resourceapi.ResourceClaim, deviceRequest *resourceapi.DeviceRequestAllocationResult) *resourceapi.AllocatedDeviceStatus {\n+\tfor _, device := range claim.Status.Devices {\n+\t\tif deviceRequest.Device == device.Device && deviceRequest.Driver == device.Driver && deviceRequest.Pool == device.Pool {\n+\t\t\treturn &device\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func isClaimBound(logger klog.Logger, claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\treturn false, nil",
        "comment_created_at": "2025-03-10T19:15:43+00:00",
        "comment_author": "pohly",
        "comment_body": "Yes, that seems appropriate. There are two event APIs, we should use the same as the rest of the scheduler. See https://kubernetes.slack.com/archives/C20HH14P7/p1740748575144709",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1987702366",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-03-10T17:07:10+00:00",
    "commented_code": "return claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"allocatedDeviceStatus is not updated.\")",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "1987702366",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1987702366",
        "commented_code": "@@ -871,6 +928,67 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"allocatedDeviceStatus is not updated.\")",
        "comment_created_at": "2025-03-10T17:07:10+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "This is not a failure, we're just waiting. This should be an informational `EventTypeNormal` and the reason should be `BindingConditionsPending`, and the message should be more user-oriented - how about: \"waiting for driver to report status for device %s on node %s\".\r\n\r\nTwo things I am not sure of:\r\n1) What the EventSource should be here; you used `Scheduling` it looks like. Do we fire events in other places and what do we use for event source there?\r\n1) Should make the involved object the pod or the claim? Users are more likely to describe the pod, but the claim is more accurate.\r\n\r\n@pohly opinions?",
        "pr_file_module": null
      },
      {
        "comment_id": "1987878242",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1987702366",
        "commented_code": "@@ -871,6 +928,67 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"allocatedDeviceStatus is not updated.\")",
        "comment_created_at": "2025-03-10T19:13:58+00:00",
        "comment_author": "pohly",
        "comment_body": "> What the EventSource should be here; you used Scheduling it looks like. Do we fire events in other places and what do we use for event source there?\r\n\r\nI have to defer to SIG Scheduling here. I don't know what precedence we have for a scheduler plugin to emit events.\r\n\r\n/cc @dom4ha @sanposhiho \r\n\r\n> Should make the involved object the pod or the claim? Users are more likely to describe the pod, but the claim is more accurate.\r\n\r\nTricky. Quite a while ago I asked similar questions and got as answer that users should know that they need to dig down into the dependency chain. The main reason is that they already need to do that for pods: when a Deployment doesn't become ready, there are no events posted for it that explain why pods are not starting. The user has to describe the pods, not the Deployment. That would be an argument in favor of emitting for the claim.\r\n\r\nBut this is coming from the scheduler, which is processing pods. So pod would also make sense and is arguably easier for a user to find.\r\n\r\n\r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1987723040",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-03-10T17:21:14+00:00",
    "commented_code": "return claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"allocatedDeviceStatus is not updated.\")\n+\t\t\treturn false, nil\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingFailureConditions {\n+\t\t\tif apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"device %v binding to node %v failed. pod: %v/%v. \", deviceStatus.Device, nodeName, pod.Namespace, pod.Name)\n+\t\t\t\treturn false, fmt.Errorf(\"claim %s failed to bind\", claim.Name)\n+\t\t\t}\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingConditions {\n+\t\t\tif !apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"device binding conditions are not met.\")",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "1987723040",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1987723040",
        "commented_code": "@@ -871,6 +928,67 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"allocatedDeviceStatus is not updated.\")\n+\t\t\treturn false, nil\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingFailureConditions {\n+\t\t\tif apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"device %v binding to node %v failed. pod: %v/%v. \", deviceStatus.Device, nodeName, pod.Namespace, pod.Name)\n+\t\t\t\treturn false, fmt.Errorf(\"claim %s failed to bind\", claim.Name)\n+\t\t\t}\n+\t\t}\n+\t\tfor _, cond := range deviceRequest.BindingConditions {\n+\t\t\tif !apimeta.IsStatusConditionTrue(deviceStatus.Conditions, cond) {\n+\t\t\t\tpl.fh.EventRecorder().Eventf(claim, nil, v1.EventTypeWarning, \"FailedDeviceBinding\", \"Scheduling\", \"device binding conditions are not met.\")",
        "comment_created_at": "2025-03-10T17:21:14+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "* `EventTypeNormal`\r\n* `BindingConditionsPending`\r\n* Messages across all three events should be consistent in identifying the node and device. As stated above, something like \"waiting for binding conditions for device %s on node %s\"\r\n\r\nWe also should send a warning event when binding times out, with an appropriate message",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1989349120",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-03-11T14:01:11+00:00",
    "commented_code": "state.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax || timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\tif errors.Is(err, context.DeadlineExceeded) {\n+\t\t\tpl.fh.EventRecorder().Eventf(pod, pod, v1.EventTypeWarning, \"BindingConditionsFailed\", \"Scheduling\", \"waiting for binding conditions timed out, error: %s\", err)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "1989349120",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1989349120",
        "commented_code": "@@ -779,6 +796,48 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs *framework.CycleStat\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax || timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\tif errors.Is(err, context.DeadlineExceeded) {\n+\t\t\tpl.fh.EventRecorder().Eventf(pod, pod, v1.EventTypeWarning, \"BindingConditionsFailed\", \"Scheduling\", \"waiting for binding conditions timed out, error: %s\", err)",
        "comment_created_at": "2025-03-11T14:01:11+00:00",
        "comment_author": "macsko",
        "comment_body": "Do we need to send an event here? Scheduler will send the event with the status message anyway if the PreBind failed. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1990045497",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1989349120",
        "commented_code": "@@ -779,6 +796,48 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs *framework.CycleStat\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax || timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\tif errors.Is(err, context.DeadlineExceeded) {\n+\t\t\tpl.fh.EventRecorder().Eventf(pod, pod, v1.EventTypeWarning, \"BindingConditionsFailed\", \"Scheduling\", \"waiting for binding conditions timed out, error: %s\", err)",
        "comment_created_at": "2025-03-11T19:58:35+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "Ok, great point, makes sense to me.",
        "pr_file_module": null
      },
      {
        "comment_id": "1990335886",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1989349120",
        "commented_code": "@@ -779,6 +796,48 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs *framework.CycleStat\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax || timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\tif errors.Is(err, context.DeadlineExceeded) {\n+\t\t\tpl.fh.EventRecorder().Eventf(pod, pod, v1.EventTypeWarning, \"BindingConditionsFailed\", \"Scheduling\", \"waiting for binding conditions timed out, error: %s\", err)",
        "comment_created_at": "2025-03-12T00:13:00+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I will remove this Event",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1989360541",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-03-11T14:05:28+00:00",
    "commented_code": "return claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, pod, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for driver to report status for device %s/%s/%s on node %s.\", deviceRequest.Driver, deviceRequest.Pool, deviceRequest.Device, nodeName)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "1989360541",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1989360541",
        "commented_code": "@@ -871,6 +931,67 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, pod, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for driver to report status for device %s/%s/%s on node %s.\", deviceRequest.Driver, deviceRequest.Pool, deviceRequest.Device, nodeName)",
        "comment_created_at": "2025-03-11T14:05:28+00:00",
        "comment_author": "macsko",
        "comment_body": "isClaimBound is used also in the Filter stage. Do we want to send the events there? I don't think we need. Could we send the events only in PreBind?",
        "pr_file_module": null
      },
      {
        "comment_id": "1990045601",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1989360541",
        "commented_code": "@@ -871,6 +931,67 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, pod, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for driver to report status for device %s/%s/%s on node %s.\", deviceRequest.Driver, deviceRequest.Pool, deviceRequest.Device, nodeName)",
        "comment_created_at": "2025-03-11T19:58:39+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "SGTM, thanks @macsko.\r\n\r\nWhat should the event source be?",
        "pr_file_module": null
      },
      {
        "comment_id": "1990358352",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1989360541",
        "commented_code": "@@ -871,6 +931,67 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, pod, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for driver to report status for device %s/%s/%s on node %s.\", deviceRequest.Driver, deviceRequest.Pool, deviceRequest.Device, nodeName)",
        "comment_created_at": "2025-03-12T00:43:25+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "To send an event, I will use `hasDeviceBindingStatus` which only runs in PreBind.",
        "pr_file_module": null
      },
      {
        "comment_id": "1991178156",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1989360541",
        "commented_code": "@@ -871,6 +931,67 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func (pl *DynamicResources) isClaimBound(claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, deviceRequest := range claim.Status.Allocation.Devices.Results {\n+\t\tif len(deviceRequest.BindingConditions) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tdeviceStatus := getAllocatedDeviceStatus(claim, &deviceRequest)\n+\t\tif deviceStatus == nil {\n+\t\t\tpl.fh.EventRecorder().Eventf(claim, pod, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for driver to report status for device %s/%s/%s on node %s.\", deviceRequest.Driver, deviceRequest.Pool, deviceRequest.Device, nodeName)",
        "comment_created_at": "2025-03-12T10:36:19+00:00",
        "comment_author": "macsko",
        "comment_body": "> What should the event source be?\r\n\r\nWe don't have any specific rules for that. Probably `Scheduling` should be fine - we use it to report scheduling failures, including PreBind.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2222278401",
    "pr_number": 133120,
    "pr_file": "pkg/scheduler/metrics/metrics.go",
    "created_at": "2025-07-22T11:50:20+00:00",
    "commented_code": "},\n \t\t[]string{\"result\"})\n \n+\t// The below are only available when the SchedulerAsyncAPICalls feature gate is enabled.\n+\tAsyncAPICallsQueuedTotal = metrics.NewCounterVec(\n+\t\t&metrics.CounterOpts{\n+\t\t\tSubsystem:      SchedulerSubsystem,\n+\t\t\tName:           \"async_api_calls_queued_total\",\n+\t\t\tHelp:           \"Total number of API calls added to the async queue.\",\n+\t\t\tStabilityLevel: metrics.ALPHA,\n+\t\t},\n+\t\t[]string{\"call_type\"})\n+\n+\tAsyncAPICallsTotal = metrics.NewCounterVec(\n+\t\t&metrics.CounterOpts{\n+\t\t\tSubsystem:      SchedulerSubsystem,\n+\t\t\tName:           \"async_api_call_execution_total\",\n+\t\t\tHelp:           \"Total number of API calls executed by the async dispatcher.\",\n+\t\t\tStabilityLevel: metrics.ALPHA,\n+\t\t},\n+\t\t[]string{\"call_type\", \"result\"})\n+\n+\tAsyncAPICallDuration = metrics.NewHistogramVec(\n+\t\t&metrics.HistogramOpts{\n+\t\t\tSubsystem:      SchedulerSubsystem,\n+\t\t\tName:           \"async_api_call_duration_seconds\",\n+\t\t\tHelp:           \"Duration in seconds for executing API calls in the async dispatcher.\",\n+\t\t\tBuckets:        metrics.ExponentialBuckets(0.001, 2, 15),\n+\t\t\tStabilityLevel: metrics.ALPHA,\n+\t\t},\n+\t\t[]string{\"call_type\", \"result\"})\n+\n+\tAsyncAPIPendingCalls = metrics.NewGauge(\n+\t\t&metrics.GaugeOpts{\n+\t\t\tSubsystem:      SchedulerSubsystem,\n+\t\t\tName:           \"pending_async_api_calls\",",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2222278401",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133120,
        "pr_file": "pkg/scheduler/metrics/metrics.go",
        "discussion_id": "2222278401",
        "commented_code": "@@ -334,6 +348,43 @@ func InitMetrics() {\n \t\t},\n \t\t[]string{\"result\"})\n \n+\t// The below are only available when the SchedulerAsyncAPICalls feature gate is enabled.\n+\tAsyncAPICallsQueuedTotal = metrics.NewCounterVec(\n+\t\t&metrics.CounterOpts{\n+\t\t\tSubsystem:      SchedulerSubsystem,\n+\t\t\tName:           \"async_api_calls_queued_total\",\n+\t\t\tHelp:           \"Total number of API calls added to the async queue.\",\n+\t\t\tStabilityLevel: metrics.ALPHA,\n+\t\t},\n+\t\t[]string{\"call_type\"})\n+\n+\tAsyncAPICallsTotal = metrics.NewCounterVec(\n+\t\t&metrics.CounterOpts{\n+\t\t\tSubsystem:      SchedulerSubsystem,\n+\t\t\tName:           \"async_api_call_execution_total\",\n+\t\t\tHelp:           \"Total number of API calls executed by the async dispatcher.\",\n+\t\t\tStabilityLevel: metrics.ALPHA,\n+\t\t},\n+\t\t[]string{\"call_type\", \"result\"})\n+\n+\tAsyncAPICallDuration = metrics.NewHistogramVec(\n+\t\t&metrics.HistogramOpts{\n+\t\t\tSubsystem:      SchedulerSubsystem,\n+\t\t\tName:           \"async_api_call_duration_seconds\",\n+\t\t\tHelp:           \"Duration in seconds for executing API calls in the async dispatcher.\",\n+\t\t\tBuckets:        metrics.ExponentialBuckets(0.001, 2, 15),\n+\t\t\tStabilityLevel: metrics.ALPHA,\n+\t\t},\n+\t\t[]string{\"call_type\", \"result\"})\n+\n+\tAsyncAPIPendingCalls = metrics.NewGauge(\n+\t\t&metrics.GaugeOpts{\n+\t\t\tSubsystem:      SchedulerSubsystem,\n+\t\t\tName:           \"pending_async_api_calls\",",
        "comment_created_at": "2025-07-22T11:50:20+00:00",
        "comment_author": "macsko",
        "comment_body": "Add `call_type` to the labels",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2221968732",
    "pr_number": 133021,
    "pr_file": "pkg/scheduler/framework/runtime/framework.go",
    "created_at": "2025-07-22T10:00:48+00:00",
    "commented_code": "return nil\n }\n \n+// RunPreBindPreFlightPlugins runs the set of configured PreBindPreFlight plugins.\n+// The returning value is:\n+// - Success: one or more plugins return success, meaning, some PreBind plugins will work for this pod.\n+// - Skip: all plugins return skip.\n+// - Error: any plugin return error.\n+func (f *frameworkImpl) RunPreBindPreFlightPlugins(ctx context.Context, state fwk.CycleState, pod *v1.Pod, nodeName string) (status *fwk.Status) {\n+\tstartTime := time.Now()\n+\tdefer func() {\n+\t\tmetrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.PreBind, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))\n+\t}()\n+\tlogger := klog.FromContext(ctx)\n+\tverboseLogs := logger.V(4).Enabled()\n+\tif verboseLogs {\n+\t\tlogger = klog.LoggerWithName(logger, \"PreBindPreFlight\")\n+\t\tlogger = klog.LoggerWithValues(logger, \"node\", klog.ObjectRef{Name: nodeName})\n+\t}\n+\tskipPlugins := sets.New[string]()\n+\treturningStatus := fwk.NewStatus(fwk.Skip)\n+\tfor _, pl := range f.preBindPlugins {\n+\t\tctx := ctx\n+\t\tif verboseLogs {\n+\t\t\tlogger := klog.LoggerWithName(logger, pl.Name())\n+\t\t\tctx = klog.NewContext(ctx, logger)\n+\t\t}\n+\t\tstatus = f.runPreBindPreFlightPlugin(ctx, pl, state, pod, nodeName)\n+\t\tswitch {\n+\t\tcase status.Code() == fwk.Error:\n+\t\t\terr := status.AsError()\n+\t\t\tlogger.Error(err, \"Plugin failed\", \"plugin\", pl.Name(), \"pod\", klog.KObj(pod), \"node\", nodeName)\n+\t\t\treturn fwk.AsStatus(fmt.Errorf(\"running PreBindPreFlight %q: %w\", pl.Name(), err))\n+\t\tcase status.IsSuccess():\n+\t\t\t// We return success when one or more plugins return success.\n+\t\t\treturningStatus = nil\n+\t\tcase status.IsSkip():\n+\t\t\tskipPlugins.Insert(pl.Name())\n+\t\tdefault:\n+\t\t\t// Other status are unexpected\n+\t\t\treturn fwk.AsStatus(fmt.Errorf(\"PreBindPreFlight %s returned %s, which is unsupported. It is supposed to return Success, Skip, or Error status\", pl.Name(), status.Code().String()))\n+\t\t}\n+\t}\n+\tstate.SetSkipPreBindPlugins(skipPlugins)\n+\treturn returningStatus\n+}\n+\n+func (f *frameworkImpl) runPreBindPreFlightPlugin(ctx context.Context, pl framework.PreBindPlugin, state fwk.CycleState, pod *v1.Pod, nodeName string) *fwk.Status {\n+\tif !state.ShouldRecordPluginMetrics() {\n+\t\treturn pl.PreBindPreFlight(ctx, state, pod, nodeName)\n+\t}\n+\tstartTime := time.Now()\n+\tstatus := pl.PreBindPreFlight(ctx, state, pod, nodeName)\n+\tf.metricsRecorder.ObservePluginDurationAsync(metrics.PreBind, pl.Name(), status.Code().String(), metrics.SinceInSeconds(startTime))",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2221968732",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133021,
        "pr_file": "pkg/scheduler/framework/runtime/framework.go",
        "discussion_id": "2221968732",
        "commented_code": "@@ -1278,6 +1282,60 @@ func (f *frameworkImpl) RunPreBindPlugins(ctx context.Context, state fwk.CycleSt\n \treturn nil\n }\n \n+// RunPreBindPreFlightPlugins runs the set of configured PreBindPreFlight plugins.\n+// The returning value is:\n+// - Success: one or more plugins return success, meaning, some PreBind plugins will work for this pod.\n+// - Skip: all plugins return skip.\n+// - Error: any plugin return error.\n+func (f *frameworkImpl) RunPreBindPreFlightPlugins(ctx context.Context, state fwk.CycleState, pod *v1.Pod, nodeName string) (status *fwk.Status) {\n+\tstartTime := time.Now()\n+\tdefer func() {\n+\t\tmetrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.PreBind, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))\n+\t}()\n+\tlogger := klog.FromContext(ctx)\n+\tverboseLogs := logger.V(4).Enabled()\n+\tif verboseLogs {\n+\t\tlogger = klog.LoggerWithName(logger, \"PreBindPreFlight\")\n+\t\tlogger = klog.LoggerWithValues(logger, \"node\", klog.ObjectRef{Name: nodeName})\n+\t}\n+\tskipPlugins := sets.New[string]()\n+\treturningStatus := fwk.NewStatus(fwk.Skip)\n+\tfor _, pl := range f.preBindPlugins {\n+\t\tctx := ctx\n+\t\tif verboseLogs {\n+\t\t\tlogger := klog.LoggerWithName(logger, pl.Name())\n+\t\t\tctx = klog.NewContext(ctx, logger)\n+\t\t}\n+\t\tstatus = f.runPreBindPreFlightPlugin(ctx, pl, state, pod, nodeName)\n+\t\tswitch {\n+\t\tcase status.Code() == fwk.Error:\n+\t\t\terr := status.AsError()\n+\t\t\tlogger.Error(err, \"Plugin failed\", \"plugin\", pl.Name(), \"pod\", klog.KObj(pod), \"node\", nodeName)\n+\t\t\treturn fwk.AsStatus(fmt.Errorf(\"running PreBindPreFlight %q: %w\", pl.Name(), err))\n+\t\tcase status.IsSuccess():\n+\t\t\t// We return success when one or more plugins return success.\n+\t\t\treturningStatus = nil\n+\t\tcase status.IsSkip():\n+\t\t\tskipPlugins.Insert(pl.Name())\n+\t\tdefault:\n+\t\t\t// Other status are unexpected\n+\t\t\treturn fwk.AsStatus(fmt.Errorf(\"PreBindPreFlight %s returned %s, which is unsupported. It is supposed to return Success, Skip, or Error status\", pl.Name(), status.Code().String()))\n+\t\t}\n+\t}\n+\tstate.SetSkipPreBindPlugins(skipPlugins)\n+\treturn returningStatus\n+}\n+\n+func (f *frameworkImpl) runPreBindPreFlightPlugin(ctx context.Context, pl framework.PreBindPlugin, state fwk.CycleState, pod *v1.Pod, nodeName string) *fwk.Status {\n+\tif !state.ShouldRecordPluginMetrics() {\n+\t\treturn pl.PreBindPreFlight(ctx, state, pod, nodeName)\n+\t}\n+\tstartTime := time.Now()\n+\tstatus := pl.PreBindPreFlight(ctx, state, pod, nodeName)\n+\tf.metricsRecorder.ObservePluginDurationAsync(metrics.PreBind, pl.Name(), status.Code().String(), metrics.SinceInSeconds(startTime))",
        "comment_created_at": "2025-07-22T10:00:48+00:00",
        "comment_author": "macsko",
        "comment_body": "```suggestion\r\n\tf.metricsRecorder.ObservePluginDurationAsync(metrics.PreBindPreFlight, pl.Name(), status.Code().String(), metrics.SinceInSeconds(startTime))\r\n```",
        "pr_file_module": null
      }
    ]
  }
]