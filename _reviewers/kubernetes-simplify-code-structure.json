[
  {
    "discussion_id": "2223243963",
    "pr_number": 133109,
    "pr_file": "pkg/kubelet/prober/worker.go",
    "created_at": "2025-07-22T16:58:48+00:00",
    "commented_code": "}\n \t}\n \n-\t// Note, exec probe does NOT have access to pod environment variables or downward API\n-\tresult, err := w.probeManager.prober.probe(ctx, w.probeType, w.pod, status, w.container, w.containerID)\n+\tvar result results.Result\n+\tvar err error\n+\t// Use cached HTTP request if available",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2223243963",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133109,
        "pr_file": "pkg/kubelet/prober/worker.go",
        "discussion_id": "2223243963",
        "commented_code": "@@ -288,8 +321,22 @@ func (w *worker) doProbe(ctx context.Context) (keepGoing bool) {\n \t\t}\n \t}\n \n-\t// Note, exec probe does NOT have access to pod environment variables or downward API\n-\tresult, err := w.probeManager.prober.probe(ctx, w.probeType, w.pod, status, w.container, w.containerID)\n+\tvar result results.Result\n+\tvar err error\n+\t// Use cached HTTP request if available",
        "comment_created_at": "2025-07-22T16:58:48+00:00",
        "comment_author": "haircommander",
        "comment_body": "I think this could be simplified if you do \r\n```\r\nvar req *http.Request\r\nif w.httpProbeRequest != nil {\r\n\t\treq, reqErr := w.httpProbeRequest.getRequest(&w.container) // either returns cached request or creates a new one\r\n\t\tif reqErr != nil {\r\n\t\t\tklog.V(4).InfoS(\"HTTP-Probe failed to create request\", \"error\", reqErr)\r\n\t\t\treturn true\r\n\t\t}\r\n}\r\n\t\t// Note, exec probe does NOT have access to pod environment variables or downward API\r\n\t\tresult, err = w.probeManager.prober.probe(ctx, w.probeType, w.pod, status, w.container, w.containerID, req)\r\n```\r\ni.e. drop the else and duplicatred probe call. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2223926851",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133109,
        "pr_file": "pkg/kubelet/prober/worker.go",
        "discussion_id": "2223243963",
        "commented_code": "@@ -288,8 +321,22 @@ func (w *worker) doProbe(ctx context.Context) (keepGoing bool) {\n \t\t}\n \t}\n \n-\t// Note, exec probe does NOT have access to pod environment variables or downward API\n-\tresult, err := w.probeManager.prober.probe(ctx, w.probeType, w.pod, status, w.container, w.containerID)\n+\tvar result results.Result\n+\tvar err error\n+\t// Use cached HTTP request if available",
        "comment_created_at": "2025-07-22T22:13:36+00:00",
        "comment_author": "Ahmed-Elgamel",
        "comment_body": "updated the code to simplify as you suggested 👍",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2222974133",
    "pr_number": 132522,
    "pr_file": "pkg/registry/resource/resourceclaimtemplate/strategy.go",
    "created_at": "2025-07-22T15:36:57+00:00",
    "commented_code": "return false\n }\n+\n+func draConsumableCapacityFeatureInUse(claimTemplate *resource.ResourceClaimTemplate) bool {\n+\tif claimTemplate == nil {\n+\t\treturn false\n+\t}\n+\n+\tfor _, request := range claimTemplate.Spec.Spec.Devices.Requests {\n+\t\tif request.Exactly != nil && request.Exactly.CapacityRequests != nil {\n+\t\t\treturn true\n+\t\t}\n+\t\tif len(request.FirstAvailable) > 0 {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2222974133",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceclaimtemplate/strategy.go",
        "discussion_id": "2222974133",
        "commented_code": "@@ -174,3 +175,46 @@ func draAdminAccessFeatureInUse(claimTemplate *resource.ResourceClaimTemplate) b\n \n \treturn false\n }\n+\n+func draConsumableCapacityFeatureInUse(claimTemplate *resource.ResourceClaimTemplate) bool {\n+\tif claimTemplate == nil {\n+\t\treturn false\n+\t}\n+\n+\tfor _, request := range claimTemplate.Spec.Spec.Devices.Requests {\n+\t\tif request.Exactly != nil && request.Exactly.CapacityRequests != nil {\n+\t\t\treturn true\n+\t\t}\n+\t\tif len(request.FirstAvailable) > 0 {",
        "comment_created_at": "2025-07-22T15:36:57+00:00",
        "comment_author": "liggitt",
        "comment_body": "simplify to drop the length check, ranging over a zero-item list is safe and a no-op",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2222978271",
    "pr_number": 132522,
    "pr_file": "pkg/registry/resource/resourceclaimtemplate/strategy.go",
    "created_at": "2025-07-22T15:38:05+00:00",
    "commented_code": "return false\n }\n+\n+func draConsumableCapacityFeatureInUse(claimTemplate *resource.ResourceClaimTemplate) bool {\n+\tif claimTemplate == nil {\n+\t\treturn false\n+\t}\n+\n+\tfor _, request := range claimTemplate.Spec.Spec.Devices.Requests {\n+\t\tif request.Exactly != nil && request.Exactly.CapacityRequests != nil {\n+\t\t\treturn true\n+\t\t}\n+\t\tif len(request.FirstAvailable) > 0 {\n+\t\t\tfor _, subRequest := range request.FirstAvailable {\n+\t\t\t\tif subRequest.CapacityRequests != nil {\n+\t\t\t\t\treturn true\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn false\n+}\n+\n+// dropDisabledDRAResourceClaimConsumableCapacityFields drops any new CapacityRequests field\n+// from the newClaimTemplate if they were not used in the oldClaimTemplate.\n+func dropDisabledDRAResourceClaimConsumableCapacityFields(newClaimTemplate, oldClaimTemplate *resource.ResourceClaimTemplate) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) ||\n+\t\tdraConsumableCapacityFeatureInUse(oldClaimTemplate) {\n+\t\t// No need to drop anything.\n+\t\treturn\n+\t}\n+\n+\tfor i := range newClaimTemplate.Spec.Spec.Devices.Requests {\n+\t\tif newClaimTemplate.Spec.Spec.Devices.Requests[i].Exactly != nil {\n+\t\t\tnewClaimTemplate.Spec.Spec.Devices.Requests[i].Exactly.CapacityRequests = nil\n+\t\t}\n+\t\trequest := newClaimTemplate.Spec.Spec.Devices.Requests[i]\n+\t\tif len(request.FirstAvailable) > 0 {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2222978271",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/registry/resource/resourceclaimtemplate/strategy.go",
        "discussion_id": "2222978271",
        "commented_code": "@@ -174,3 +175,46 @@ func draAdminAccessFeatureInUse(claimTemplate *resource.ResourceClaimTemplate) b\n \n \treturn false\n }\n+\n+func draConsumableCapacityFeatureInUse(claimTemplate *resource.ResourceClaimTemplate) bool {\n+\tif claimTemplate == nil {\n+\t\treturn false\n+\t}\n+\n+\tfor _, request := range claimTemplate.Spec.Spec.Devices.Requests {\n+\t\tif request.Exactly != nil && request.Exactly.CapacityRequests != nil {\n+\t\t\treturn true\n+\t\t}\n+\t\tif len(request.FirstAvailable) > 0 {\n+\t\t\tfor _, subRequest := range request.FirstAvailable {\n+\t\t\t\tif subRequest.CapacityRequests != nil {\n+\t\t\t\t\treturn true\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn false\n+}\n+\n+// dropDisabledDRAResourceClaimConsumableCapacityFields drops any new CapacityRequests field\n+// from the newClaimTemplate if they were not used in the oldClaimTemplate.\n+func dropDisabledDRAResourceClaimConsumableCapacityFields(newClaimTemplate, oldClaimTemplate *resource.ResourceClaimTemplate) {\n+\tif utilfeature.DefaultFeatureGate.Enabled(features.DRAConsumableCapacity) ||\n+\t\tdraConsumableCapacityFeatureInUse(oldClaimTemplate) {\n+\t\t// No need to drop anything.\n+\t\treturn\n+\t}\n+\n+\tfor i := range newClaimTemplate.Spec.Spec.Devices.Requests {\n+\t\tif newClaimTemplate.Spec.Spec.Devices.Requests[i].Exactly != nil {\n+\t\t\tnewClaimTemplate.Spec.Spec.Devices.Requests[i].Exactly.CapacityRequests = nil\n+\t\t}\n+\t\trequest := newClaimTemplate.Spec.Spec.Devices.Requests[i]\n+\t\tif len(request.FirstAvailable) > 0 {",
        "comment_created_at": "2025-07-22T15:38:05+00:00",
        "comment_author": "liggitt",
        "comment_body": "simplify to drop the length check, ranging over a zero-item list is safe and a no-op",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2204024464",
    "pr_number": 130653,
    "pr_file": "test/e2e/dra/dra.go",
    "created_at": "2025-07-14T07:17:00+00:00",
    "commented_code": "podStartTimeout = 5 * time.Minute\n )\n \n+var (\n+\textendedResourceName  = \"example.com/gpu\"",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2204024464",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/dra.go",
        "discussion_id": "2204024464",
        "commented_code": "@@ -64,6 +66,15 @@ const (\n \tpodStartTimeout = 5 * time.Minute\n )\n \n+var (\n+\textendedResourceName  = \"example.com/gpu\"",
        "comment_created_at": "2025-07-14T07:17:00+00:00",
        "comment_author": "pohly",
        "comment_body": "Let's replace with a function `extendedResourceName(index int)`. Then you can replace some code below with a for loop instead of using cut-and-paste.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2208914402",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/dra.go",
        "discussion_id": "2204024464",
        "commented_code": "@@ -64,6 +66,15 @@ const (\n \tpodStartTimeout = 5 * time.Minute\n )\n \n+var (\n+\textendedResourceName  = \"example.com/gpu\"",
        "comment_created_at": "2025-07-15T23:52:43+00:00",
        "comment_author": "yliaog",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2204044428",
    "pr_number": 130653,
    "pr_file": "test/e2e/dra/dra.go",
    "created_at": "2025-07-14T07:28:17+00:00",
    "commented_code": "})\n \t})\n \n+\tframework.Context(f.WithFeatureGate(features.DRAExtendedResource), \"single node\", func() {\n+\t\tnodes := NewNodes(f, 1, 1)\n+\t\tdriver := NewDriver(f, nodes, networkResources(10, false))\n+\t\tb := newBuilder(f, driver)\n+\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tres := v1.ResourceList{}\n+\t\t\tres[v1.ResourceName(extendedResourceName)] = resource.MustParse(\"1\")\n+\t\t\tpod.Spec.Containers[0].Resources.Requests = res\n+\t\t\tpod.Spec.Containers[0].Resources.Limits = res\n+\n+\t\t\tb.create(ctx, pod)\n+\t\t\terr := e2epod.WaitForPodRunningInNamespace(ctx, f.ClientSet, pod)\n+\t\t\tframework.ExpectNoError(err, \"start pod\")\n+\t\t\tcontainerEnv := []string{\n+\t\t\t\t\"container_0_request_0\", \"true\",\n+\t\t\t}\n+\t\t\ttestContainerEnv(ctx, f, pod, pod.Spec.Containers[0].Name, false, containerEnv...)\n+\t\t})\n+\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node three-extended-resources\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tres := v1.ResourceList{}\n+\t\t\tres[v1.ResourceName(extendedResourceName)] = resource.MustParse(\"1\")\n+\t\t\tres[v1.ResourceName(extendedResourceName2)] = resource.MustParse(\"1\")\n+\t\t\tres[v1.ResourceName(extendedResourceName3)] = resource.MustParse(\"1\")\n+\t\t\tpod.Spec.Containers[0].Resources.Requests = res\n+\t\t\tpod.Spec.Containers[0].Resources.Limits = res\n+\n+\t\t\tb.create(ctx, pod)\n+\t\t\terr := e2epod.WaitForPodRunningInNamespace(ctx, f.ClientSet, pod)\n+\t\t\tframework.ExpectNoError(err, \"start pod\")\n+\t\t\tcontainerEnv := []string{\n+\t\t\t\t\"container_0_request_0\", \"true\",\n+\t\t\t\t\"container_0_request_1\", \"true\",\n+\t\t\t\t\"container_0_request_2\", \"true\",\n+\t\t\t}\n+\t\t\ttestContainerEnv(ctx, f, pod, pod.Spec.Containers[0].Name, false, containerEnv...)\n+\t\t})\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node three-containers\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tpod.Spec.Containers = append(pod.Spec.Containers, *pod.Spec.Containers[0].DeepCopy())\n+\t\t\tpod.Spec.Containers = append(pod.Spec.Containers, *pod.Spec.Containers[0].DeepCopy())\n+\t\t\tpod.Spec.Containers[0].Name = \"container0\"\n+\t\t\tpod.Spec.Containers[1].Name = \"container1\"\n+\t\t\tpod.Spec.Containers[1].Name = \"container2\"",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2204044428",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/dra.go",
        "discussion_id": "2204044428",
        "commented_code": "@@ -1911,6 +1921,175 @@ var _ = framework.SIGDescribe(\"node\")(framework.WithLabel(\"DRA\"), framework.With\n \t\t})\n \t})\n \n+\tframework.Context(f.WithFeatureGate(features.DRAExtendedResource), \"single node\", func() {\n+\t\tnodes := NewNodes(f, 1, 1)\n+\t\tdriver := NewDriver(f, nodes, networkResources(10, false))\n+\t\tb := newBuilder(f, driver)\n+\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tres := v1.ResourceList{}\n+\t\t\tres[v1.ResourceName(extendedResourceName)] = resource.MustParse(\"1\")\n+\t\t\tpod.Spec.Containers[0].Resources.Requests = res\n+\t\t\tpod.Spec.Containers[0].Resources.Limits = res\n+\n+\t\t\tb.create(ctx, pod)\n+\t\t\terr := e2epod.WaitForPodRunningInNamespace(ctx, f.ClientSet, pod)\n+\t\t\tframework.ExpectNoError(err, \"start pod\")\n+\t\t\tcontainerEnv := []string{\n+\t\t\t\t\"container_0_request_0\", \"true\",\n+\t\t\t}\n+\t\t\ttestContainerEnv(ctx, f, pod, pod.Spec.Containers[0].Name, false, containerEnv...)\n+\t\t})\n+\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node three-extended-resources\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tres := v1.ResourceList{}\n+\t\t\tres[v1.ResourceName(extendedResourceName)] = resource.MustParse(\"1\")\n+\t\t\tres[v1.ResourceName(extendedResourceName2)] = resource.MustParse(\"1\")\n+\t\t\tres[v1.ResourceName(extendedResourceName3)] = resource.MustParse(\"1\")\n+\t\t\tpod.Spec.Containers[0].Resources.Requests = res\n+\t\t\tpod.Spec.Containers[0].Resources.Limits = res\n+\n+\t\t\tb.create(ctx, pod)\n+\t\t\terr := e2epod.WaitForPodRunningInNamespace(ctx, f.ClientSet, pod)\n+\t\t\tframework.ExpectNoError(err, \"start pod\")\n+\t\t\tcontainerEnv := []string{\n+\t\t\t\t\"container_0_request_0\", \"true\",\n+\t\t\t\t\"container_0_request_1\", \"true\",\n+\t\t\t\t\"container_0_request_2\", \"true\",\n+\t\t\t}\n+\t\t\ttestContainerEnv(ctx, f, pod, pod.Spec.Containers[0].Name, false, containerEnv...)\n+\t\t})\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node three-containers\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tpod.Spec.Containers = append(pod.Spec.Containers, *pod.Spec.Containers[0].DeepCopy())\n+\t\t\tpod.Spec.Containers = append(pod.Spec.Containers, *pod.Spec.Containers[0].DeepCopy())\n+\t\t\tpod.Spec.Containers[0].Name = \"container0\"\n+\t\t\tpod.Spec.Containers[1].Name = \"container1\"\n+\t\t\tpod.Spec.Containers[1].Name = \"container2\"",
        "comment_created_at": "2025-07-14T07:28:17+00:00",
        "comment_author": "pohly",
        "comment_body": "```suggestion\r\n\t\t\tpod.Spec.Containers[2].Name = \"container2\"\r\n```\r\n\r\nBut really, please use a for loop instead to avoid such cut-and-paste errors. Same applies to several other places above and below.\r\n\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2208915699",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/dra.go",
        "discussion_id": "2204044428",
        "commented_code": "@@ -1911,6 +1921,175 @@ var _ = framework.SIGDescribe(\"node\")(framework.WithLabel(\"DRA\"), framework.With\n \t\t})\n \t})\n \n+\tframework.Context(f.WithFeatureGate(features.DRAExtendedResource), \"single node\", func() {\n+\t\tnodes := NewNodes(f, 1, 1)\n+\t\tdriver := NewDriver(f, nodes, networkResources(10, false))\n+\t\tb := newBuilder(f, driver)\n+\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tres := v1.ResourceList{}\n+\t\t\tres[v1.ResourceName(extendedResourceName)] = resource.MustParse(\"1\")\n+\t\t\tpod.Spec.Containers[0].Resources.Requests = res\n+\t\t\tpod.Spec.Containers[0].Resources.Limits = res\n+\n+\t\t\tb.create(ctx, pod)\n+\t\t\terr := e2epod.WaitForPodRunningInNamespace(ctx, f.ClientSet, pod)\n+\t\t\tframework.ExpectNoError(err, \"start pod\")\n+\t\t\tcontainerEnv := []string{\n+\t\t\t\t\"container_0_request_0\", \"true\",\n+\t\t\t}\n+\t\t\ttestContainerEnv(ctx, f, pod, pod.Spec.Containers[0].Name, false, containerEnv...)\n+\t\t})\n+\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node three-extended-resources\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tres := v1.ResourceList{}\n+\t\t\tres[v1.ResourceName(extendedResourceName)] = resource.MustParse(\"1\")\n+\t\t\tres[v1.ResourceName(extendedResourceName2)] = resource.MustParse(\"1\")\n+\t\t\tres[v1.ResourceName(extendedResourceName3)] = resource.MustParse(\"1\")\n+\t\t\tpod.Spec.Containers[0].Resources.Requests = res\n+\t\t\tpod.Spec.Containers[0].Resources.Limits = res\n+\n+\t\t\tb.create(ctx, pod)\n+\t\t\terr := e2epod.WaitForPodRunningInNamespace(ctx, f.ClientSet, pod)\n+\t\t\tframework.ExpectNoError(err, \"start pod\")\n+\t\t\tcontainerEnv := []string{\n+\t\t\t\t\"container_0_request_0\", \"true\",\n+\t\t\t\t\"container_0_request_1\", \"true\",\n+\t\t\t\t\"container_0_request_2\", \"true\",\n+\t\t\t}\n+\t\t\ttestContainerEnv(ctx, f, pod, pod.Spec.Containers[0].Name, false, containerEnv...)\n+\t\t})\n+\t\tginkgo.It(\"must run a pod with extended resource\", ginkgo.Label(\"dra extended resource single-node three-containers\"), func(ctx context.Context) {\n+\t\t\tpod := b.pod()\n+\t\t\tpod.Spec.Containers = append(pod.Spec.Containers, *pod.Spec.Containers[0].DeepCopy())\n+\t\t\tpod.Spec.Containers = append(pod.Spec.Containers, *pod.Spec.Containers[0].DeepCopy())\n+\t\t\tpod.Spec.Containers[0].Name = \"container0\"\n+\t\t\tpod.Spec.Containers[1].Name = \"container1\"\n+\t\t\tpod.Spec.Containers[1].Name = \"container2\"",
        "comment_created_at": "2025-07-15T23:53:25+00:00",
        "comment_author": "yliaog",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2207815386",
    "pr_number": 125271,
    "pr_file": "staging/src/k8s.io/pod-security-admission/policy/check_hostProbesAndhostLifecycle.go",
    "created_at": "2025-07-15T15:18:42+00:00",
    "commented_code": "+/*\n+Copyright 2024 The Kubernetes Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package policy\n+\n+import (\n+\t\"fmt\"\n+\n+\tcorev1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/apimachinery/pkg/util/sets\"\n+\t\"k8s.io/pod-security-admission/api\"\n+)\n+\n+/*\n+Host field is restricted in the probes and lifecycle handlers.\n+\n+**Restricted Fields:**\n+\n+* spec.containers[*].livenessProbe.httpGet.host\n+* spec.containers[*].readinessProbe.httpGet.host\n+* spec.containers[*].startupProbe.httpGet.host\n+* spec.containers[*].livenessProbe.tcpSocket.host\n+* spec.containers[*].readinessProbe.tcpSocket.host\n+* spec.containers[*].startupProbe.tcpSocket.host\n+* spec.containers[*].lifecycle.postStart.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.containers[*].lifecycle.preStop.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.containers[*].lifecycle.postStart.httpGet.host\n+* spec.containers[*].lifecycle.preStop.httpGet.host\n+* spec.initContainers[*].livenessProbe.httpGet.host\n+* spec.initContainers[*].readinessProbe.httpGet.host\n+* spec.initContainers[*].startupProbe.httpGet.host\n+* spec.initContainers[*].livenessProbe.tcpSocket.host\n+* spec.initContainers[*].readinessProbe.tcpSocket.host\n+* spec.initContainers[*].startupProbe.tcpSocket.host\n+* spec.initContainers[*].lifecycle.postStart.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.initContainers[*].lifecycle.preStop.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.initContainers[*].lifecycle.postStart.httpGet.host\n+* spec.initContainers[*].lifecycle.preStop.httpGet.host\n+\n+\n+**Allowed Values:** \"127.0.0.1\", \"::1\"\n+*/\n+\n+const (\n+\tallowedLocalHostIPv4 = \"127.0.0.1\"\n+\tallowedLocalHostIPv6 = \"::1\"\n+)\n+\n+func init() {\n+\taddCheck(CheckHostProbesAndHostLifecycle)\n+}\n+\n+// CheckHostProbesAndHostLifecycle returns a baseline level check\n+// that forbids setting host field in probes and lifecycle handlers in 1.34+\n+// the only allowed values are `127.0.0.1` and `::1`\n+func CheckHostProbesAndHostLifecycle() Check {\n+\treturn Check{\n+\t\tID:    \"hostProbesAndHostLifecycle\",\n+\t\tLevel: api.LevelBaseline,\n+\t\tVersions: []VersionedCheck{\n+\t\t\t{\n+\t\t\t\tMinimumVersion: api.MajorMinorVersion(1, 34),\n+\t\t\t\tCheckPod:       hostProbesAndHostLifecycleV1Dot34,\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+func hostProbesAndHostLifecycleV1Dot34(podMetadata *metav1.ObjectMeta, podSpec *corev1.PodSpec) CheckResult {\n+\tvar badContainers []string\n+\tforbidden := sets.New[string]()\n+\tallowed := sets.New[string](allowedLocalHostIPv4, allowedLocalHostIPv6)\n+\tvisitContainers(podSpec, func(container *corev1.Container) {\n+\t\tisValid := true\n+\n+\t\t// Check probes\n+\t\tfor _, probe := range []*corev1.Probe{container.LivenessProbe, container.ReadinessProbe, container.StartupProbe} {\n+\t\t\tif probe == nil {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tif probe.HTTPGet != nil {\n+\t\t\t\tif forbiddenHosts := getForbiddenHosts(probe.HTTPGet.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\tisValid = false\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif probe.TCPSocket != nil {\n+\t\t\t\tif forbiddenHosts := getForbiddenHosts(probe.TCPSocket.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\tisValid = false\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Check lifecycle handlers\n+\t\tif container.Lifecycle != nil {\n+\t\t\tfor _, handler := range []*corev1.LifecycleHandler{container.Lifecycle.PostStart, container.Lifecycle.PreStop} {\n+\t\t\t\tif handler == nil {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\tif handler.HTTPGet != nil {\n+\t\t\t\t\tif forbiddenHosts := getForbiddenHosts(handler.HTTPGet.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\t\tisValid = false\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tif handler.TCPSocket != nil {\n+\t\t\t\t\tif forbiddenHosts := getForbiddenHosts(handler.TCPSocket.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\t\tisValid = false\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tif !isValid {\n+\t\t\tbadContainers = append(badContainers, container.Name)\n+\t\t}\n+\n+\t})\n+\n+\tif len(badContainers) > 0 {\n+\t\treturn CheckResult{\n+\t\t\tAllowed:         false,\n+\t\t\tForbiddenReason: \"hostProbesOrHostLifecycle\",\n+\t\t\tForbiddenDetail: fmt.Sprintf(\n+\t\t\t\t\"%s %s %s %s %s\",\n+\t\t\t\tpluralize(\"container\", \"containers\", len(badContainers)),\n+\t\t\t\tjoinQuote(badContainers),\n+\t\t\t\tpluralize(\"uses\", \"use\", len(badContainers)),\n+\t\t\t\tpluralize(\"hostProbeOrHostLifecycle\", \"hostProbesOrHostLifecycles\", len(forbidden)),\n+\t\t\t\tjoinQuote(sets.List(forbidden)),\n+\t\t\t),\n+\t\t}\n+\t}\n+\treturn CheckResult{Allowed: true}\n+}\n+\n+func getForbiddenHosts(host string, allowed sets.Set[string]) sets.Set[string] {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2207815386",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 125271,
        "pr_file": "staging/src/k8s.io/pod-security-admission/policy/check_hostProbesAndhostLifecycle.go",
        "discussion_id": "2207815386",
        "commented_code": "@@ -0,0 +1,159 @@\n+/*\n+Copyright 2024 The Kubernetes Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package policy\n+\n+import (\n+\t\"fmt\"\n+\n+\tcorev1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/apimachinery/pkg/util/sets\"\n+\t\"k8s.io/pod-security-admission/api\"\n+)\n+\n+/*\n+Host field is restricted in the probes and lifecycle handlers.\n+\n+**Restricted Fields:**\n+\n+* spec.containers[*].livenessProbe.httpGet.host\n+* spec.containers[*].readinessProbe.httpGet.host\n+* spec.containers[*].startupProbe.httpGet.host\n+* spec.containers[*].livenessProbe.tcpSocket.host\n+* spec.containers[*].readinessProbe.tcpSocket.host\n+* spec.containers[*].startupProbe.tcpSocket.host\n+* spec.containers[*].lifecycle.postStart.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.containers[*].lifecycle.preStop.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.containers[*].lifecycle.postStart.httpGet.host\n+* spec.containers[*].lifecycle.preStop.httpGet.host\n+* spec.initContainers[*].livenessProbe.httpGet.host\n+* spec.initContainers[*].readinessProbe.httpGet.host\n+* spec.initContainers[*].startupProbe.httpGet.host\n+* spec.initContainers[*].livenessProbe.tcpSocket.host\n+* spec.initContainers[*].readinessProbe.tcpSocket.host\n+* spec.initContainers[*].startupProbe.tcpSocket.host\n+* spec.initContainers[*].lifecycle.postStart.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.initContainers[*].lifecycle.preStop.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.initContainers[*].lifecycle.postStart.httpGet.host\n+* spec.initContainers[*].lifecycle.preStop.httpGet.host\n+\n+\n+**Allowed Values:** \"127.0.0.1\", \"::1\"\n+*/\n+\n+const (\n+\tallowedLocalHostIPv4 = \"127.0.0.1\"\n+\tallowedLocalHostIPv6 = \"::1\"\n+)\n+\n+func init() {\n+\taddCheck(CheckHostProbesAndHostLifecycle)\n+}\n+\n+// CheckHostProbesAndHostLifecycle returns a baseline level check\n+// that forbids setting host field in probes and lifecycle handlers in 1.34+\n+// the only allowed values are `127.0.0.1` and `::1`\n+func CheckHostProbesAndHostLifecycle() Check {\n+\treturn Check{\n+\t\tID:    \"hostProbesAndHostLifecycle\",\n+\t\tLevel: api.LevelBaseline,\n+\t\tVersions: []VersionedCheck{\n+\t\t\t{\n+\t\t\t\tMinimumVersion: api.MajorMinorVersion(1, 34),\n+\t\t\t\tCheckPod:       hostProbesAndHostLifecycleV1Dot34,\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+func hostProbesAndHostLifecycleV1Dot34(podMetadata *metav1.ObjectMeta, podSpec *corev1.PodSpec) CheckResult {\n+\tvar badContainers []string\n+\tforbidden := sets.New[string]()\n+\tallowed := sets.New[string](allowedLocalHostIPv4, allowedLocalHostIPv6)\n+\tvisitContainers(podSpec, func(container *corev1.Container) {\n+\t\tisValid := true\n+\n+\t\t// Check probes\n+\t\tfor _, probe := range []*corev1.Probe{container.LivenessProbe, container.ReadinessProbe, container.StartupProbe} {\n+\t\t\tif probe == nil {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tif probe.HTTPGet != nil {\n+\t\t\t\tif forbiddenHosts := getForbiddenHosts(probe.HTTPGet.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\tisValid = false\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif probe.TCPSocket != nil {\n+\t\t\t\tif forbiddenHosts := getForbiddenHosts(probe.TCPSocket.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\tisValid = false\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Check lifecycle handlers\n+\t\tif container.Lifecycle != nil {\n+\t\t\tfor _, handler := range []*corev1.LifecycleHandler{container.Lifecycle.PostStart, container.Lifecycle.PreStop} {\n+\t\t\t\tif handler == nil {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\tif handler.HTTPGet != nil {\n+\t\t\t\t\tif forbiddenHosts := getForbiddenHosts(handler.HTTPGet.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\t\tisValid = false\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tif handler.TCPSocket != nil {\n+\t\t\t\t\tif forbiddenHosts := getForbiddenHosts(handler.TCPSocket.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\t\tisValid = false\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tif !isValid {\n+\t\t\tbadContainers = append(badContainers, container.Name)\n+\t\t}\n+\n+\t})\n+\n+\tif len(badContainers) > 0 {\n+\t\treturn CheckResult{\n+\t\t\tAllowed:         false,\n+\t\t\tForbiddenReason: \"hostProbesOrHostLifecycle\",\n+\t\t\tForbiddenDetail: fmt.Sprintf(\n+\t\t\t\t\"%s %s %s %s %s\",\n+\t\t\t\tpluralize(\"container\", \"containers\", len(badContainers)),\n+\t\t\t\tjoinQuote(badContainers),\n+\t\t\t\tpluralize(\"uses\", \"use\", len(badContainers)),\n+\t\t\t\tpluralize(\"hostProbeOrHostLifecycle\", \"hostProbesOrHostLifecycles\", len(forbidden)),\n+\t\t\t\tjoinQuote(sets.List(forbidden)),\n+\t\t\t),\n+\t\t}\n+\t}\n+\treturn CheckResult{Allowed: true}\n+}\n+\n+func getForbiddenHosts(host string, allowed sets.Set[string]) sets.Set[string] {",
        "comment_created_at": "2025-07-15T15:18:42+00:00",
        "comment_author": "liggitt",
        "comment_body": "passing in a single string and getting back a set containing either 0 items or the single input item is pretty strange... I think `allowed sets.Set[string]` can be a static private var, and anything calling this function can check `if !allowed.Has(...Host)` directly instead of calling a helper function and constructing a set",
        "pr_file_module": null
      },
      {
        "comment_id": "2225116779",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 125271,
        "pr_file": "staging/src/k8s.io/pod-security-admission/policy/check_hostProbesAndhostLifecycle.go",
        "discussion_id": "2207815386",
        "commented_code": "@@ -0,0 +1,159 @@\n+/*\n+Copyright 2024 The Kubernetes Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package policy\n+\n+import (\n+\t\"fmt\"\n+\n+\tcorev1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/apimachinery/pkg/util/sets\"\n+\t\"k8s.io/pod-security-admission/api\"\n+)\n+\n+/*\n+Host field is restricted in the probes and lifecycle handlers.\n+\n+**Restricted Fields:**\n+\n+* spec.containers[*].livenessProbe.httpGet.host\n+* spec.containers[*].readinessProbe.httpGet.host\n+* spec.containers[*].startupProbe.httpGet.host\n+* spec.containers[*].livenessProbe.tcpSocket.host\n+* spec.containers[*].readinessProbe.tcpSocket.host\n+* spec.containers[*].startupProbe.tcpSocket.host\n+* spec.containers[*].lifecycle.postStart.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.containers[*].lifecycle.preStop.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.containers[*].lifecycle.postStart.httpGet.host\n+* spec.containers[*].lifecycle.preStop.httpGet.host\n+* spec.initContainers[*].livenessProbe.httpGet.host\n+* spec.initContainers[*].readinessProbe.httpGet.host\n+* spec.initContainers[*].startupProbe.httpGet.host\n+* spec.initContainers[*].livenessProbe.tcpSocket.host\n+* spec.initContainers[*].readinessProbe.tcpSocket.host\n+* spec.initContainers[*].startupProbe.tcpSocket.host\n+* spec.initContainers[*].lifecycle.postStart.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.initContainers[*].lifecycle.preStop.tcpSocket.host // Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for backward compatibility.\n+* spec.initContainers[*].lifecycle.postStart.httpGet.host\n+* spec.initContainers[*].lifecycle.preStop.httpGet.host\n+\n+\n+**Allowed Values:** \"127.0.0.1\", \"::1\"\n+*/\n+\n+const (\n+\tallowedLocalHostIPv4 = \"127.0.0.1\"\n+\tallowedLocalHostIPv6 = \"::1\"\n+)\n+\n+func init() {\n+\taddCheck(CheckHostProbesAndHostLifecycle)\n+}\n+\n+// CheckHostProbesAndHostLifecycle returns a baseline level check\n+// that forbids setting host field in probes and lifecycle handlers in 1.34+\n+// the only allowed values are `127.0.0.1` and `::1`\n+func CheckHostProbesAndHostLifecycle() Check {\n+\treturn Check{\n+\t\tID:    \"hostProbesAndHostLifecycle\",\n+\t\tLevel: api.LevelBaseline,\n+\t\tVersions: []VersionedCheck{\n+\t\t\t{\n+\t\t\t\tMinimumVersion: api.MajorMinorVersion(1, 34),\n+\t\t\t\tCheckPod:       hostProbesAndHostLifecycleV1Dot34,\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+func hostProbesAndHostLifecycleV1Dot34(podMetadata *metav1.ObjectMeta, podSpec *corev1.PodSpec) CheckResult {\n+\tvar badContainers []string\n+\tforbidden := sets.New[string]()\n+\tallowed := sets.New[string](allowedLocalHostIPv4, allowedLocalHostIPv6)\n+\tvisitContainers(podSpec, func(container *corev1.Container) {\n+\t\tisValid := true\n+\n+\t\t// Check probes\n+\t\tfor _, probe := range []*corev1.Probe{container.LivenessProbe, container.ReadinessProbe, container.StartupProbe} {\n+\t\t\tif probe == nil {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tif probe.HTTPGet != nil {\n+\t\t\t\tif forbiddenHosts := getForbiddenHosts(probe.HTTPGet.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\tisValid = false\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif probe.TCPSocket != nil {\n+\t\t\t\tif forbiddenHosts := getForbiddenHosts(probe.TCPSocket.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\tisValid = false\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Check lifecycle handlers\n+\t\tif container.Lifecycle != nil {\n+\t\t\tfor _, handler := range []*corev1.LifecycleHandler{container.Lifecycle.PostStart, container.Lifecycle.PreStop} {\n+\t\t\t\tif handler == nil {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\tif handler.HTTPGet != nil {\n+\t\t\t\t\tif forbiddenHosts := getForbiddenHosts(handler.HTTPGet.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\t\tisValid = false\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tif handler.TCPSocket != nil {\n+\t\t\t\t\tif forbiddenHosts := getForbiddenHosts(handler.TCPSocket.Host, allowed); len(forbiddenHosts) > 0 {\n+\t\t\t\t\t\tforbidden.Insert(sets.List(forbiddenHosts)...)\n+\t\t\t\t\t\tisValid = false\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tif !isValid {\n+\t\t\tbadContainers = append(badContainers, container.Name)\n+\t\t}\n+\n+\t})\n+\n+\tif len(badContainers) > 0 {\n+\t\treturn CheckResult{\n+\t\t\tAllowed:         false,\n+\t\t\tForbiddenReason: \"hostProbesOrHostLifecycle\",\n+\t\t\tForbiddenDetail: fmt.Sprintf(\n+\t\t\t\t\"%s %s %s %s %s\",\n+\t\t\t\tpluralize(\"container\", \"containers\", len(badContainers)),\n+\t\t\t\tjoinQuote(badContainers),\n+\t\t\t\tpluralize(\"uses\", \"use\", len(badContainers)),\n+\t\t\t\tpluralize(\"hostProbeOrHostLifecycle\", \"hostProbesOrHostLifecycles\", len(forbidden)),\n+\t\t\t\tjoinQuote(sets.List(forbidden)),\n+\t\t\t),\n+\t\t}\n+\t}\n+\treturn CheckResult{Allowed: true}\n+}\n+\n+func getForbiddenHosts(host string, allowed sets.Set[string]) sets.Set[string] {",
        "comment_created_at": "2025-07-23T10:37:01+00:00",
        "comment_author": "tssurya",
        "comment_body": "changed the structure to match your earlier suggestion..",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2224107368",
    "pr_number": 132642,
    "pr_file": "pkg/apis/core/validation/validation.go",
    "created_at": "2025-07-23T01:12:09+00:00",
    "commented_code": "return allErrors\n }\n \n+// validateContainerRestartPolicy checks the container-level restartPolicy and restartPolicyRules are valid for\n+// regular containers and non-sidecar init containers.\n+func validateContainerRestartPolicy(policy *core.ContainerRestartPolicy, rules []core.ContainerRestartRule, fldPath *field.Path) field.ErrorList {\n+\tvar allErrs field.ErrorList\n+\trestartPolicyFld := fldPath.Child(\"restartPolicy\")\n+\tif policy == nil {\n+\t\tif len(rules) == 0 {\n+\t\t\treturn allErrs\n+\t\t} else {\n+\t\t\tallErrs = append(allErrs, field.Required(restartPolicyFld, \"must specify restartPolicy when restart rules are used\"))\n+\t\t}\n+\t} else {\n+\t\tswitch *policy {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2224107368",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/apis/core/validation/validation.go",
        "discussion_id": "2224107368",
        "commented_code": "@@ -3566,6 +3579,60 @@ func validateResizePolicy(policyList []core.ContainerResizePolicy, fldPath *fiel\n \treturn allErrors\n }\n \n+// validateContainerRestartPolicy checks the container-level restartPolicy and restartPolicyRules are valid for\n+// regular containers and non-sidecar init containers.\n+func validateContainerRestartPolicy(policy *core.ContainerRestartPolicy, rules []core.ContainerRestartRule, fldPath *field.Path) field.ErrorList {\n+\tvar allErrs field.ErrorList\n+\trestartPolicyFld := fldPath.Child(\"restartPolicy\")\n+\tif policy == nil {\n+\t\tif len(rules) == 0 {\n+\t\t\treturn allErrs\n+\t\t} else {\n+\t\t\tallErrs = append(allErrs, field.Required(restartPolicyFld, \"must specify restartPolicy when restart rules are used\"))\n+\t\t}\n+\t} else {\n+\t\tswitch *policy {",
        "comment_created_at": "2025-07-23T01:12:09+00:00",
        "comment_author": "msau42",
        "comment_body": "This section can be simplified by using the sets library, like: https://github.com/kubernetes/kubernetes/blob/c5ffd67cd212b6455436315d5f569742eb1da2e3/pkg/apis/core/validation/validation.go#L1347\r\n\r\nSame thing with `validActions` below.",
        "pr_file_module": null
      },
      {
        "comment_id": "2226777074",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132642,
        "pr_file": "pkg/apis/core/validation/validation.go",
        "discussion_id": "2224107368",
        "commented_code": "@@ -3566,6 +3579,60 @@ func validateResizePolicy(policyList []core.ContainerResizePolicy, fldPath *fiel\n \treturn allErrors\n }\n \n+// validateContainerRestartPolicy checks the container-level restartPolicy and restartPolicyRules are valid for\n+// regular containers and non-sidecar init containers.\n+func validateContainerRestartPolicy(policy *core.ContainerRestartPolicy, rules []core.ContainerRestartRule, fldPath *field.Path) field.ErrorList {\n+\tvar allErrs field.ErrorList\n+\trestartPolicyFld := fldPath.Child(\"restartPolicy\")\n+\tif policy == nil {\n+\t\tif len(rules) == 0 {\n+\t\t\treturn allErrs\n+\t\t} else {\n+\t\t\tallErrs = append(allErrs, field.Required(restartPolicyFld, \"must specify restartPolicy when restart rules are used\"))\n+\t\t}\n+\t} else {\n+\t\tswitch *policy {",
        "comment_created_at": "2025-07-23T21:51:03+00:00",
        "comment_author": "yuanwang04",
        "comment_body": "Refactored this to use the sets.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2213651621",
    "pr_number": 132959,
    "pr_file": "test/e2e/network/service.go",
    "created_at": "2025-07-17T15:21:32+00:00",
    "commented_code": "service, err := t.CreateService(service)\n \t\tframework.ExpectNoError(err)\n \n-\t\tcheckServiceReachabilityFromExecPod(ctx, f.ClientSet, ns, service.Name, service.Spec.ClusterIP, port)\n+\t\t_ = checkServiceReachabilityFromExecPod(ctx, f.ClientSet, ns, service.Name, service.Spec.ClusterIP, port)\n+\t})\n+\n+\tginkgo.It(\"should connect to the named ports during port number changing\", func(ctx context.Context) {\n+\t\tserviceName := \"mutable-named-port\"\n+\t\tns := f.Namespace.Name\n+\n+\t\tt := NewServerTest(cs, ns, serviceName)\n+\t\tdefer func() {\n+\t\t\tdefer ginkgo.GinkgoRecover()\n+\t\t\terrs := t.Cleanup()\n+\t\t\tif len(errs) != 0 {\n+\t\t\t\tframework.Failf(\"errors in cleanup: %v\", errs)\n+\t\t\t}\n+\t\t}()\n+\n+\t\tport := int32(8080)\n+\t\tportName := \"mutable-port\"\n+\t\tports := []v1.ContainerPort{{Name: portName, ContainerPort: port, Protocol: v1.ProtocolTCP}}\n+\t\targs := []string{\"netexec\", fmt.Sprintf(\"--http-port=%d\", port)}\n+\t\tpods := []*v1.Pod{}",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2213651621",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132959,
        "pr_file": "test/e2e/network/service.go",
        "discussion_id": "2213651621",
        "commented_code": "@@ -4226,7 +4227,81 @@ var _ = common.SIGDescribe(\"Services\", func() {\n \t\tservice, err := t.CreateService(service)\n \t\tframework.ExpectNoError(err)\n \n-\t\tcheckServiceReachabilityFromExecPod(ctx, f.ClientSet, ns, service.Name, service.Spec.ClusterIP, port)\n+\t\t_ = checkServiceReachabilityFromExecPod(ctx, f.ClientSet, ns, service.Name, service.Spec.ClusterIP, port)\n+\t})\n+\n+\tginkgo.It(\"should connect to the named ports during port number changing\", func(ctx context.Context) {\n+\t\tserviceName := \"mutable-named-port\"\n+\t\tns := f.Namespace.Name\n+\n+\t\tt := NewServerTest(cs, ns, serviceName)\n+\t\tdefer func() {\n+\t\t\tdefer ginkgo.GinkgoRecover()\n+\t\t\terrs := t.Cleanup()\n+\t\t\tif len(errs) != 0 {\n+\t\t\t\tframework.Failf(\"errors in cleanup: %v\", errs)\n+\t\t\t}\n+\t\t}()\n+\n+\t\tport := int32(8080)\n+\t\tportName := \"mutable-port\"\n+\t\tports := []v1.ContainerPort{{Name: portName, ContainerPort: port, Protocol: v1.ProtocolTCP}}\n+\t\targs := []string{\"netexec\", fmt.Sprintf(\"--http-port=%d\", port)}\n+\t\tpods := []*v1.Pod{}",
        "comment_created_at": "2025-07-17T15:21:32+00:00",
        "comment_author": "danwinship",
        "comment_body": "You don't really use the `pods` array... You reference `len(pods)` in a few places, but it's always `2`. And you reference `pods[0].Name` and `pods[1].Name` in some places, but you know that those values are going to be `\"testpod0\"` and `\"testpod1\"` at the point where you reference them.\r\n\r\nSo you could just drop `pods`. And then you don't need to modify `createPodOrFail` either.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186079126",
    "pr_number": 132558,
    "pr_file": "pkg/apis/core/validation/validation_test.go",
    "created_at": "2025-07-04T20:27:06+00:00",
    "commented_code": "}\n }\n \n+func TestValidatePodHostName(t *testing.T) {\n+\ttrueVal := true\n+\tfalseVal := false\n+\ttests := []struct {\n+\t\tname         string\n+\t\tspec         core.PodSpec\n+\t\texpectedErrs field.ErrorList\n+\t}{\n+\t\t{\n+\t\t\tname: \"SetHostnameAsFQDN=true and HostnameOverride is set should error\",\n+\t\t\tspec: core.PodSpec{\n+\t\t\t\tSetHostnameAsFQDN: &trueVal,",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2186079126",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132558,
        "pr_file": "pkg/apis/core/validation/validation_test.go",
        "discussion_id": "2186079126",
        "commented_code": "@@ -25002,6 +25003,129 @@ func TestValidateHostUsers(t *testing.T) {\n \t}\n }\n \n+func TestValidatePodHostName(t *testing.T) {\n+\ttrueVal := true\n+\tfalseVal := false\n+\ttests := []struct {\n+\t\tname         string\n+\t\tspec         core.PodSpec\n+\t\texpectedErrs field.ErrorList\n+\t}{\n+\t\t{\n+\t\t\tname: \"SetHostnameAsFQDN=true and HostnameOverride is set should error\",\n+\t\t\tspec: core.PodSpec{\n+\t\t\t\tSetHostnameAsFQDN: &trueVal,",
        "comment_created_at": "2025-07-04T20:27:06+00:00",
        "comment_author": "thockin",
        "comment_body": "super nit, but these should just be `ptr.To(true)` - no need for a local variable, it just makes the code harder to read.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2199089124",
    "pr_number": 132626,
    "pr_file": "pkg/apis/core/validation/validation.go",
    "created_at": "2025-07-11T00:28:47+00:00",
    "commented_code": "return allErrs\n }\n \n+// validateFileKeyRefVolumes validates that volumes referenced by FileKeyRef environment variables\n+// are of type emptyDir. FileKeyRef requires emptyDir volumes to ensure proper file access.\n+func validateFileKeyRefVolumes(spec *core.PodSpec, fldPath *field.Path) field.ErrorList {\n+\tallErrs := field.ErrorList{}\n+\n+\tvolumeSources := make(map[string]*core.VolumeSource)\n+\tfor i := range spec.Volumes {\n+\t\tvolume := &spec.Volumes[i]\n+\t\tvolumeSources[volume.Name] = &volume.VolumeSource\n+\t}\n+\n+\tvalidateFileKeyRefEnv := func(containers []core.Container, containerPath *field.Path) {\n+\t\tfor i, container := range containers {\n+\t\t\tenvPath := containerPath.Index(i).Child(\"env\")\n+\t\t\tfor j, env := range container.Env {\n+\t\t\t\t// Only care about environment variables that use FileKeyRef.\n+\t\t\t\tif env.ValueFrom == nil || env.ValueFrom.FileKeyRef == nil {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\n+\t\t\t\tvolumeName := env.ValueFrom.FileKeyRef.VolumeName\n+\t\t\t\tfileKeyRefPath := envPath.Index(j).Child(\"valueFrom\").Child(\"fileKeyRef\")\n+\t\t\t\tvolumeNamePath := fileKeyRefPath.Child(\"volumeName\")\n+\n+\t\t\t\tsource, found := volumeSources[volumeName]\n+\t\t\t\tif !found {\n+\t\t\t\t\t// The referenced volume does not exist in the pod spec.\n+\t\t\t\t\tallErrs = append(allErrs, field.NotFound(volumeNamePath, volumeName))\n+\t\t\t\t} else if source.EmptyDir == nil {\n+\t\t\t\t\t// The volume exists, but it is not of type emptyDir, which is required.\n+\t\t\t\t\tallErrs = append(allErrs, field.Invalid(volumeNamePath, volumeName, \"referenced volume must be of type emptyDir\"))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Validate for regular containers\n+\tvalidateFileKeyRefEnv(spec.Containers, fldPath.Child(\"containers\"))\n+\t// Validate for init containers\n+\tvalidateFileKeyRefEnv(spec.InitContainers, fldPath.Child(\"initContainers\"))",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2199089124",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132626,
        "pr_file": "pkg/apis/core/validation/validation.go",
        "discussion_id": "2199089124",
        "commented_code": "@@ -3682,6 +3720,58 @@ func validateHostUsers(spec *core.PodSpec, fldPath *field.Path) field.ErrorList\n \treturn allErrs\n }\n \n+// validateFileKeyRefVolumes validates that volumes referenced by FileKeyRef environment variables\n+// are of type emptyDir. FileKeyRef requires emptyDir volumes to ensure proper file access.\n+func validateFileKeyRefVolumes(spec *core.PodSpec, fldPath *field.Path) field.ErrorList {\n+\tallErrs := field.ErrorList{}\n+\n+\tvolumeSources := make(map[string]*core.VolumeSource)\n+\tfor i := range spec.Volumes {\n+\t\tvolume := &spec.Volumes[i]\n+\t\tvolumeSources[volume.Name] = &volume.VolumeSource\n+\t}\n+\n+\tvalidateFileKeyRefEnv := func(containers []core.Container, containerPath *field.Path) {\n+\t\tfor i, container := range containers {\n+\t\t\tenvPath := containerPath.Index(i).Child(\"env\")\n+\t\t\tfor j, env := range container.Env {\n+\t\t\t\t// Only care about environment variables that use FileKeyRef.\n+\t\t\t\tif env.ValueFrom == nil || env.ValueFrom.FileKeyRef == nil {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\n+\t\t\t\tvolumeName := env.ValueFrom.FileKeyRef.VolumeName\n+\t\t\t\tfileKeyRefPath := envPath.Index(j).Child(\"valueFrom\").Child(\"fileKeyRef\")\n+\t\t\t\tvolumeNamePath := fileKeyRefPath.Child(\"volumeName\")\n+\n+\t\t\t\tsource, found := volumeSources[volumeName]\n+\t\t\t\tif !found {\n+\t\t\t\t\t// The referenced volume does not exist in the pod spec.\n+\t\t\t\t\tallErrs = append(allErrs, field.NotFound(volumeNamePath, volumeName))\n+\t\t\t\t} else if source.EmptyDir == nil {\n+\t\t\t\t\t// The volume exists, but it is not of type emptyDir, which is required.\n+\t\t\t\t\tallErrs = append(allErrs, field.Invalid(volumeNamePath, volumeName, \"referenced volume must be of type emptyDir\"))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Validate for regular containers\n+\tvalidateFileKeyRefEnv(spec.Containers, fldPath.Child(\"containers\"))\n+\t// Validate for init containers\n+\tvalidateFileKeyRefEnv(spec.InitContainers, fldPath.Child(\"initContainers\"))",
        "comment_created_at": "2025-07-11T00:28:47+00:00",
        "comment_author": "thockin",
        "comment_body": "Should this use `podshelper.VisitContainersWithPath()` ?",
        "pr_file_module": null
      },
      {
        "comment_id": "2200946972",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132626,
        "pr_file": "pkg/apis/core/validation/validation.go",
        "discussion_id": "2199089124",
        "commented_code": "@@ -3682,6 +3720,58 @@ func validateHostUsers(spec *core.PodSpec, fldPath *field.Path) field.ErrorList\n \treturn allErrs\n }\n \n+// validateFileKeyRefVolumes validates that volumes referenced by FileKeyRef environment variables\n+// are of type emptyDir. FileKeyRef requires emptyDir volumes to ensure proper file access.\n+func validateFileKeyRefVolumes(spec *core.PodSpec, fldPath *field.Path) field.ErrorList {\n+\tallErrs := field.ErrorList{}\n+\n+\tvolumeSources := make(map[string]*core.VolumeSource)\n+\tfor i := range spec.Volumes {\n+\t\tvolume := &spec.Volumes[i]\n+\t\tvolumeSources[volume.Name] = &volume.VolumeSource\n+\t}\n+\n+\tvalidateFileKeyRefEnv := func(containers []core.Container, containerPath *field.Path) {\n+\t\tfor i, container := range containers {\n+\t\t\tenvPath := containerPath.Index(i).Child(\"env\")\n+\t\t\tfor j, env := range container.Env {\n+\t\t\t\t// Only care about environment variables that use FileKeyRef.\n+\t\t\t\tif env.ValueFrom == nil || env.ValueFrom.FileKeyRef == nil {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\n+\t\t\t\tvolumeName := env.ValueFrom.FileKeyRef.VolumeName\n+\t\t\t\tfileKeyRefPath := envPath.Index(j).Child(\"valueFrom\").Child(\"fileKeyRef\")\n+\t\t\t\tvolumeNamePath := fileKeyRefPath.Child(\"volumeName\")\n+\n+\t\t\t\tsource, found := volumeSources[volumeName]\n+\t\t\t\tif !found {\n+\t\t\t\t\t// The referenced volume does not exist in the pod spec.\n+\t\t\t\t\tallErrs = append(allErrs, field.NotFound(volumeNamePath, volumeName))\n+\t\t\t\t} else if source.EmptyDir == nil {\n+\t\t\t\t\t// The volume exists, but it is not of type emptyDir, which is required.\n+\t\t\t\t\tallErrs = append(allErrs, field.Invalid(volumeNamePath, volumeName, \"referenced volume must be of type emptyDir\"))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Validate for regular containers\n+\tvalidateFileKeyRefEnv(spec.Containers, fldPath.Child(\"containers\"))\n+\t// Validate for init containers\n+\tvalidateFileKeyRefEnv(spec.InitContainers, fldPath.Child(\"initContainers\"))",
        "comment_created_at": "2025-07-11T15:00:34+00:00",
        "comment_author": "HirazawaUi",
        "comment_body": "Yes, using this function is definitely the preferred choice. I previously spent some time reviewing the codebase but couldn't find similar functions, so I implemented a similar solution myself.",
        "pr_file_module": null
      }
    ]
  }
]