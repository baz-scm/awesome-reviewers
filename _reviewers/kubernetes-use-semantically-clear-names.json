[
  {
    "discussion_id": "2179423704",
    "pr_number": 132522,
    "pr_file": "pkg/apis/resource/types.go",
    "created_at": "2025-07-02T08:15:39+00:00",
    "commented_code": "// +listType=atomic\n \t// +featureGate=DRADeviceTaints\n \tTolerations []DeviceToleration\n+\n+\t// CapacityRequests define resource requirements against each capacity.\n+\t//\n+\t// +optional\n+\t// +featureGate=DRAConsumableCapacity\n+\tCapacityRequests *CapacityRequirements\n+}\n+\n+// CapacityRequirements defines the capacity requirements for a specific device request.\n+type CapacityRequirements struct {\n+\t// Minimum defines the minimum amount of each device capacity required for the request.\n+\t//\n+\t// If the capacity has a sharing policy, this value is rounded up to the nearest valid amount\n+\t// according to that policy. The rounded value is used during scheduling to determine how much capacity to consume.\n+\t//\n+\t// If the quantity does not have a sharing policy, this value is used as an additional filtering\n+\t// condition against the available capacity on the device.\n+\t// This is semantically equivalent to a CEL selector with\n+\t// `device.capacity[<domain>].<name>.compareTo(quantity(<minimum quantity>)) >= 0`\n+\t// For example, device.capacity['test-driver.cdi.k8s.io'].counters.compareTo(quantity('2')) >= 0\n+\t//\n+\t// +optional\n+\tMinimum map[QualifiedName]resource.Quantity",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2179423704",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/apis/resource/types.go",
        "discussion_id": "2179423704",
        "commented_code": "@@ -825,6 +915,29 @@ type DeviceSubRequest struct {\n \t// +listType=atomic\n \t// +featureGate=DRADeviceTaints\n \tTolerations []DeviceToleration\n+\n+\t// CapacityRequests define resource requirements against each capacity.\n+\t//\n+\t// +optional\n+\t// +featureGate=DRAConsumableCapacity\n+\tCapacityRequests *CapacityRequirements\n+}\n+\n+// CapacityRequirements defines the capacity requirements for a specific device request.\n+type CapacityRequirements struct {\n+\t// Minimum defines the minimum amount of each device capacity required for the request.\n+\t//\n+\t// If the capacity has a sharing policy, this value is rounded up to the nearest valid amount\n+\t// according to that policy. The rounded value is used during scheduling to determine how much capacity to consume.\n+\t//\n+\t// If the quantity does not have a sharing policy, this value is used as an additional filtering\n+\t// condition against the available capacity on the device.\n+\t// This is semantically equivalent to a CEL selector with\n+\t// `device.capacity[<domain>].<name>.compareTo(quantity(<minimum quantity>)) >= 0`\n+\t// For example, device.capacity['test-driver.cdi.k8s.io'].counters.compareTo(quantity('2')) >= 0\n+\t//\n+\t// +optional\n+\tMinimum map[QualifiedName]resource.Quantity",
        "comment_created_at": "2025-07-02T08:15:39+00:00",
        "comment_author": "LionelJouin",
        "comment_body": "This topic was already in the KEP review. Maybe the naming of the field might be misleading.\r\nFor the bandwidth use-case, the idea is to have the bandwidth guaranteed, so to claim the amount of bandwidth (not `at least` the bandwidth as `Minimum` could indicate).\r\n\r\nThe other names proposed were `Required`, `Reservation`, `Consumption`, `Requests`.\r\n\r\nHere is the related discussion https://github.com/kubernetes/enhancements/pull/5104#discussion_r2054378844",
        "pr_file_module": null
      },
      {
        "comment_id": "2185638722",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/apis/resource/types.go",
        "discussion_id": "2179423704",
        "commented_code": "@@ -825,6 +915,29 @@ type DeviceSubRequest struct {\n \t// +listType=atomic\n \t// +featureGate=DRADeviceTaints\n \tTolerations []DeviceToleration\n+\n+\t// CapacityRequests define resource requirements against each capacity.\n+\t//\n+\t// +optional\n+\t// +featureGate=DRAConsumableCapacity\n+\tCapacityRequests *CapacityRequirements\n+}\n+\n+// CapacityRequirements defines the capacity requirements for a specific device request.\n+type CapacityRequirements struct {\n+\t// Minimum defines the minimum amount of each device capacity required for the request.\n+\t//\n+\t// If the capacity has a sharing policy, this value is rounded up to the nearest valid amount\n+\t// according to that policy. The rounded value is used during scheduling to determine how much capacity to consume.\n+\t//\n+\t// If the quantity does not have a sharing policy, this value is used as an additional filtering\n+\t// condition against the available capacity on the device.\n+\t// This is semantically equivalent to a CEL selector with\n+\t// `device.capacity[<domain>].<name>.compareTo(quantity(<minimum quantity>)) >= 0`\n+\t// For example, device.capacity['test-driver.cdi.k8s.io'].counters.compareTo(quantity('2')) >= 0\n+\t//\n+\t// +optional\n+\tMinimum map[QualifiedName]resource.Quantity",
        "comment_created_at": "2025-07-04T15:18:51+00:00",
        "comment_author": "pohly",
        "comment_body": "@sunya-ch: please go through all opens in the KEP discussion and copy each into a separate thread attached to an appropriate line.",
        "pr_file_module": null
      },
      {
        "comment_id": "2224760126",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132522,
        "pr_file": "pkg/apis/resource/types.go",
        "discussion_id": "2179423704",
        "commented_code": "@@ -825,6 +915,29 @@ type DeviceSubRequest struct {\n \t// +listType=atomic\n \t// +featureGate=DRADeviceTaints\n \tTolerations []DeviceToleration\n+\n+\t// CapacityRequests define resource requirements against each capacity.\n+\t//\n+\t// +optional\n+\t// +featureGate=DRAConsumableCapacity\n+\tCapacityRequests *CapacityRequirements\n+}\n+\n+// CapacityRequirements defines the capacity requirements for a specific device request.\n+type CapacityRequirements struct {\n+\t// Minimum defines the minimum amount of each device capacity required for the request.\n+\t//\n+\t// If the capacity has a sharing policy, this value is rounded up to the nearest valid amount\n+\t// according to that policy. The rounded value is used during scheduling to determine how much capacity to consume.\n+\t//\n+\t// If the quantity does not have a sharing policy, this value is used as an additional filtering\n+\t// condition against the available capacity on the device.\n+\t// This is semantically equivalent to a CEL selector with\n+\t// `device.capacity[<domain>].<name>.compareTo(quantity(<minimum quantity>)) >= 0`\n+\t// For example, device.capacity['test-driver.cdi.k8s.io'].counters.compareTo(quantity('2')) >= 0\n+\t//\n+\t// +optional\n+\tMinimum map[QualifiedName]resource.Quantity",
        "comment_created_at": "2025-07-23T08:09:53+00:00",
        "comment_author": "sunya-ch",
        "comment_body": "@pohly  Added and removed some of them according to @liggitt's suggestions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1983413372",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-03-06T14:03:21+00:00",
    "commented_code": "}\n \treturn framework.AsStatus(err)\n }\n+\n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func isClaimBound(logger klog.Logger, claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, device := range claim.Status.Devices {\n+\t\tfor _, cond := range device.BindingFailureConditions {\n+\t\t\tif apimeta.IsStatusConditionTrue(device.Conditions, cond) {\n+\t\t\t\tlogger.V(5).Info(\"device binding failed\", \"pod\", klog.KObj(pod), \"node\", klog.ObjectRef{Name: nodeName}, \"resourceclaim\", klog.KObj(claim), \"device\", device.Device, \"condition\", cond)\n+\t\t\t\treturn false, fmt.Errorf(\"claim %s failed to bind\", claim.Name)\n+\t\t\t}\n+\t\t}\n+\t\tfor _, cond := range device.BindingConditions {\n+\t\t\tif !apimeta.IsStatusConditionTrue(device.Conditions, cond) {\n+\t\t\t\tlogger.V(5).Info(\"device binding condition not met\", \"pod\", klog.KObj(pod), \"node\", klog.ObjectRef{Name: nodeName}, \"resourceclaim\", klog.KObj(claim), \"device\", device.Device, \"condition\", cond)\n+\t\t\t\treturn false, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// checkDeviceBindingStatus checks the binding status of devices within the\n+// given state claims.\n+func checkDeviceBindingStatus(ctx context.Context, logger klog.Logger, state *stateData, pl *DynamicResources, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor claimIndex, claim := range state.claims {\n+\t\tclaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).Get(ctx, claim.Name, metav1.GetOptions{})\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tstate.claims[claimIndex] = claim\n+\t\tbound, err := isClaimBound(logger, claim, pod, nodeName)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tif !bound {\n+\t\t\treturn false, nil\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// checkBindingConditions checks all claims in the given state.\n+// It returns true if any device has non-empty binding conditions.\n+func checkBindingConditions(state *stateData) bool {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "1983413372",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1983413372",
        "commented_code": "@@ -876,3 +955,57 @@ func statusError(logger klog.Logger, err error, kv ...interface{}) *framework.St\n \t}\n \treturn framework.AsStatus(err)\n }\n+\n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func isClaimBound(logger klog.Logger, claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, device := range claim.Status.Devices {\n+\t\tfor _, cond := range device.BindingFailureConditions {\n+\t\t\tif apimeta.IsStatusConditionTrue(device.Conditions, cond) {\n+\t\t\t\tlogger.V(5).Info(\"device binding failed\", \"pod\", klog.KObj(pod), \"node\", klog.ObjectRef{Name: nodeName}, \"resourceclaim\", klog.KObj(claim), \"device\", device.Device, \"condition\", cond)\n+\t\t\t\treturn false, fmt.Errorf(\"claim %s failed to bind\", claim.Name)\n+\t\t\t}\n+\t\t}\n+\t\tfor _, cond := range device.BindingConditions {\n+\t\t\tif !apimeta.IsStatusConditionTrue(device.Conditions, cond) {\n+\t\t\t\tlogger.V(5).Info(\"device binding condition not met\", \"pod\", klog.KObj(pod), \"node\", klog.ObjectRef{Name: nodeName}, \"resourceclaim\", klog.KObj(claim), \"device\", device.Device, \"condition\", cond)\n+\t\t\t\treturn false, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// checkDeviceBindingStatus checks the binding status of devices within the\n+// given state claims.\n+func checkDeviceBindingStatus(ctx context.Context, logger klog.Logger, state *stateData, pl *DynamicResources, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor claimIndex, claim := range state.claims {\n+\t\tclaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).Get(ctx, claim.Name, metav1.GetOptions{})\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tstate.claims[claimIndex] = claim\n+\t\tbound, err := isClaimBound(logger, claim, pod, nodeName)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tif !bound {\n+\t\t\treturn false, nil\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// checkBindingConditions checks all claims in the given state.\n+// It returns true if any device has non-empty binding conditions.\n+func checkBindingConditions(state *stateData) bool {",
        "comment_created_at": "2025-03-06T14:03:21+00:00",
        "comment_author": "macsko",
        "comment_body": "Maybe rename to `hasBindingConditions` to be more meaningful?",
        "pr_file_module": null
      },
      {
        "comment_id": "2003095097",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1983413372",
        "commented_code": "@@ -876,3 +955,57 @@ func statusError(logger klog.Logger, err error, kv ...interface{}) *framework.St\n \t}\n \treturn framework.AsStatus(err)\n }\n+\n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.\n+func isClaimBound(logger klog.Logger, claim *resourceapi.ResourceClaim, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor _, device := range claim.Status.Devices {\n+\t\tfor _, cond := range device.BindingFailureConditions {\n+\t\t\tif apimeta.IsStatusConditionTrue(device.Conditions, cond) {\n+\t\t\t\tlogger.V(5).Info(\"device binding failed\", \"pod\", klog.KObj(pod), \"node\", klog.ObjectRef{Name: nodeName}, \"resourceclaim\", klog.KObj(claim), \"device\", device.Device, \"condition\", cond)\n+\t\t\t\treturn false, fmt.Errorf(\"claim %s failed to bind\", claim.Name)\n+\t\t\t}\n+\t\t}\n+\t\tfor _, cond := range device.BindingConditions {\n+\t\t\tif !apimeta.IsStatusConditionTrue(device.Conditions, cond) {\n+\t\t\t\tlogger.V(5).Info(\"device binding condition not met\", \"pod\", klog.KObj(pod), \"node\", klog.ObjectRef{Name: nodeName}, \"resourceclaim\", klog.KObj(claim), \"device\", device.Device, \"condition\", cond)\n+\t\t\t\treturn false, nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// checkDeviceBindingStatus checks the binding status of devices within the\n+// given state claims.\n+func checkDeviceBindingStatus(ctx context.Context, logger klog.Logger, state *stateData, pl *DynamicResources, pod *v1.Pod, nodeName string) (bool, error) {\n+\tfor claimIndex, claim := range state.claims {\n+\t\tclaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).Get(ctx, claim.Name, metav1.GetOptions{})\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tstate.claims[claimIndex] = claim\n+\t\tbound, err := isClaimBound(logger, claim, pod, nodeName)\n+\t\tif err != nil {\n+\t\t\treturn false, err\n+\t\t}\n+\t\tif !bound {\n+\t\t\treturn false, nil\n+\t\t}\n+\t}\n+\treturn true, nil\n+}\n+\n+// checkBindingConditions checks all claims in the given state.\n+// It returns true if any device has non-empty binding conditions.\n+func checkBindingConditions(state *stateData) bool {",
        "comment_created_at": "2025-03-19T11:16:46+00:00",
        "comment_author": "pohly",
        "comment_body": "Not resolved yet.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2201771610",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-07-11T20:19:25+00:00",
    "commented_code": "return claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2201771610",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "2201771610",
        "commented_code": "@@ -884,6 +959,98 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.",
        "comment_created_at": "2025-07-11T20:19:25+00:00",
        "comment_author": "johnbelamaric",
        "comment_body": "I think we're overusing the word \"binding\" here. The \"binding conditions\" are the conditions needed to be able to proceed to the Bind() phase for the Pod. They are not about binding a device to a claim. I think instead what you want is \"isClaimBindable\" or \"isClaimReadyForBinding\" or maybe just \"areBindingConditionsMet\" or \"claimNotBlockingBinding\" or something like that.",
        "pr_file_module": null
      },
      {
        "comment_id": "2204877347",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "2201771610",
        "commented_code": "@@ -884,6 +959,98 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.",
        "comment_created_at": "2025-07-14T12:54:51+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "OK, I see. I will chang the function name to `isClaimReadyForBinding`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2206530435",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "2201771610",
        "commented_code": "@@ -884,6 +959,98 @@ func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, ind\n \treturn claim, nil\n }\n \n+// isClaimBound checks whether a given resource claim is successfully\n+// bound to a device.",
        "comment_created_at": "2025-07-15T06:34:17+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I've updated.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2218902975",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-07-21T11:26:27+00:00",
    "commented_code": "state.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions || !pl.enableDeviceStatus {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := hasBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\tconst timeoutDefaultSeconds int64 = 600\n+\ttimeoutMax := 20 * time.Minute\n+\ttimeout := 0 * time.Second\n+\ttimeStartWaiting := time.Now()\n+\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif deviceTimeout := time.Duration(ptr.Deref(device.BindingTimeoutSeconds, timeoutDefaultSeconds)) * time.Second; timeout < deviceTimeout {\n+\t\t\t\ttimeout = deviceTimeout\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\tdeadlineCtx, cancelFunc := context.WithDeadline(ctx, timeStartWaiting.Add(timeout))\n+\tdefer cancelFunc()\n+\n+\t// We need to wait for the device to be attached to the node.\n+\tpl.fh.EventRecorder().Eventf(pod, nil, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for binding conditions for device on node %s\", nodeName)\n+\terr = wait.PollUntilContextTimeout(deadlineCtx, 5*time.Second, timeout, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(state)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2218902975",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "2218902975",
        "commented_code": "@@ -822,6 +845,53 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs fwk.CycleState, pod\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions || !pl.enableDeviceStatus {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := hasBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\tconst timeoutDefaultSeconds int64 = 600\n+\ttimeoutMax := 20 * time.Minute\n+\ttimeout := 0 * time.Second\n+\ttimeStartWaiting := time.Now()\n+\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif deviceTimeout := time.Duration(ptr.Deref(device.BindingTimeoutSeconds, timeoutDefaultSeconds)) * time.Second; timeout < deviceTimeout {\n+\t\t\t\ttimeout = deviceTimeout\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\tdeadlineCtx, cancelFunc := context.WithDeadline(ctx, timeStartWaiting.Add(timeout))\n+\tdefer cancelFunc()\n+\n+\t// We need to wait for the device to be attached to the node.\n+\tpl.fh.EventRecorder().Eventf(pod, nil, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for binding conditions for device on node %s\", nodeName)\n+\terr = wait.PollUntilContextTimeout(deadlineCtx, 5*time.Second, timeout, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(state)",
        "comment_created_at": "2025-07-21T11:26:27+00:00",
        "comment_author": "pohly",
        "comment_body": "`hasDeviceBindingStatus` could be anything. Perhaps rename to `isPodReadyForBinding`, which is symmetric with `isClaimReadyForBinding`?\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2220845183",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "2218902975",
        "commented_code": "@@ -822,6 +845,53 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs fwk.CycleState, pod\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions || !pl.enableDeviceStatus {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := hasBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\tconst timeoutDefaultSeconds int64 = 600\n+\ttimeoutMax := 20 * time.Minute\n+\ttimeout := 0 * time.Second\n+\ttimeStartWaiting := time.Now()\n+\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif deviceTimeout := time.Duration(ptr.Deref(device.BindingTimeoutSeconds, timeoutDefaultSeconds)) * time.Second; timeout < deviceTimeout {\n+\t\t\t\ttimeout = deviceTimeout\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\tdeadlineCtx, cancelFunc := context.WithDeadline(ctx, timeStartWaiting.Add(timeout))\n+\tdefer cancelFunc()\n+\n+\t// We need to wait for the device to be attached to the node.\n+\tpl.fh.EventRecorder().Eventf(pod, nil, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for binding conditions for device on node %s\", nodeName)\n+\terr = wait.PollUntilContextTimeout(deadlineCtx, 5*time.Second, timeout, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(state)",
        "comment_created_at": "2025-07-22T02:23:37+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I have renamed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219081839",
    "pr_number": 130653,
    "pr_file": "test/e2e/dra/utils/builder.go",
    "created_at": "2025-07-21T12:36:37+00:00",
    "commented_code": "return b\n }\n \n+func NewBuilderExtended(f *framework.Framework, driver *Driver) *Builder {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2219081839",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/utils/builder.go",
        "discussion_id": "2219081839",
        "commented_code": "@@ -385,10 +405,41 @@ func NewBuilderNow(ctx context.Context, f *framework.Framework, driver *Driver)\n \treturn b\n }\n \n+func NewBuilderExtended(f *framework.Framework, driver *Driver) *Builder {",
        "comment_created_at": "2025-07-21T12:36:37+00:00",
        "comment_author": "pohly",
        "comment_body": "Please, *document* your methods and avoid appending just `Extended`. That's way to generic.\r\n\r\nBut in this case you don't even need these methods. The caller can instead do:\r\n```\r\nb := drautils.NewBuilder(f, driver)\r\nb.extended = true\r\n...\r\n```\r\n\r\nThen when `b.setUp` runs, it uses the non-default `b.extended`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2220153795",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/utils/builder.go",
        "discussion_id": "2219081839",
        "commented_code": "@@ -385,10 +405,41 @@ func NewBuilderNow(ctx context.Context, f *framework.Framework, driver *Driver)\n \treturn b\n }\n \n+func NewBuilderExtended(f *framework.Framework, driver *Driver) *Builder {",
        "comment_created_at": "2025-07-21T19:52:13+00:00",
        "comment_author": "yliaog",
        "comment_body": "renamed the methods, it's easier to read with new Builder methods, than changing fields inside builder directly, IMO, so keep two new builder methods.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219103760",
    "pr_number": 130653,
    "pr_file": "test/e2e/dra/utils/builder.go",
    "created_at": "2025-07-21T12:44:36+00:00",
    "commented_code": "\"k8s.io/utils/ptr\"\n )\n \n+var (\n+\tExtendedResourceName = func(i int) string { return \"example.com/gpu\" + strconv.Itoa(i) }\n+)\n+\n // Builder contains a running counter to make objects unique within thir\n // namespace.\n type Builder struct {\n-\tf      *framework.Framework\n-\tdriver *Driver\n+\tf                  *framework.Framework\n+\tdriver             *Driver\n+\textended           bool\n+\textendedMultiNodes bool",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2219103760",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/utils/builder.go",
        "discussion_id": "2219103760",
        "commented_code": "@@ -44,11 +48,17 @@ import (\n \t\"k8s.io/utils/ptr\"\n )\n \n+var (\n+\tExtendedResourceName = func(i int) string { return \"example.com/gpu\" + strconv.Itoa(i) }\n+)\n+\n // Builder contains a running counter to make objects unique within thir\n // namespace.\n type Builder struct {\n-\tf      *framework.Framework\n-\tdriver *Driver\n+\tf                  *framework.Framework\n+\tdriver             *Driver\n+\textended           bool\n+\textendedMultiNodes bool",
        "comment_created_at": "2025-07-21T12:44:36+00:00",
        "comment_author": "pohly",
        "comment_body": "\"extended\" how?\r\n\r\nPlease, document code that you are adding and try to pick descriptive names.\r\n\r\nHow about `extended` -> `useExtendedResourceName` and `extendedMultiNodes` -> `simulateDevicePlugin`?\r\n\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2220155139",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130653,
        "pr_file": "test/e2e/dra/utils/builder.go",
        "discussion_id": "2219103760",
        "commented_code": "@@ -44,11 +48,17 @@ import (\n \t\"k8s.io/utils/ptr\"\n )\n \n+var (\n+\tExtendedResourceName = func(i int) string { return \"example.com/gpu\" + strconv.Itoa(i) }\n+)\n+\n // Builder contains a running counter to make objects unique within thir\n // namespace.\n type Builder struct {\n-\tf      *framework.Framework\n-\tdriver *Driver\n+\tf                  *framework.Framework\n+\tdriver             *Driver\n+\textended           bool\n+\textendedMultiNodes bool",
        "comment_created_at": "2025-07-21T19:52:41+00:00",
        "comment_author": "yliaog",
        "comment_body": "ok, renamed extended -> useExtendedResourceName and extendedMultiNodes -> simulateDevicePlugin",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2206022498",
    "pr_number": 132579,
    "pr_file": "pkg/kubelet/images/pullmanager/fs_pullrecords.go",
    "created_at": "2025-07-15T00:14:01+00:00",
    "commented_code": "return nil, err\n \t}\n \n+\tif err := accessor.initialize(); err != nil {\n+\t\treturn nil, fmt.Errorf(\"failed to initialize fsPullRecordsAccessor: %w\", err)\n+\t}\n+\n \treturn accessor, nil\n }\n \n+func (f *fsPullRecordsAccessor) initialize() error {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2206022498",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132579,
        "pr_file": "pkg/kubelet/images/pullmanager/fs_pullrecords.go",
        "discussion_id": "2206022498",
        "commented_code": "@@ -74,9 +76,50 @@ func NewFSPullRecordsAccessor(kubeletDir string) (*fsPullRecordsAccessor, error)\n \t\treturn nil, err\n \t}\n \n+\tif err := accessor.initialize(); err != nil {\n+\t\treturn nil, fmt.Errorf(\"failed to initialize fsPullRecordsAccessor: %w\", err)\n+\t}\n+\n \treturn accessor, nil\n }\n \n+func (f *fsPullRecordsAccessor) initialize() error {",
        "comment_created_at": "2025-07-15T00:14:01+00:00",
        "comment_author": "liggitt",
        "comment_body": "nit: maybe name this migrateRecordVersions or something to clarify what it is doing? this doesn't actually store any state in fsPullRecordsAccessor",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070510505",
    "pr_number": 131571,
    "pr_file": "test/e2e/network/pod_lifecycle.go",
    "created_at": "2025-05-01T16:55:56+00:00",
    "commented_code": "fr.NamespacePodSecurityLevel = admissionapi.LevelPrivileged\n \n \tvar (\n-\t\tcs        clientset.Interface\n-\t\tns        string\n-\t\tpodClient *e2epod.PodClient\n+\t\tcs                                    clientset.Interface\n+\t\tns                                    string\n+\t\tpodClient                             *e2epod.PodClient\n+\t\tnetworkGracePeriodOfOneHundredSeconds int64 = 100\n+\t\tnetworkGracePeriodOfThirtySeconds     int64 = 30\n+\t\tnetworkHttpPort                       int   = 80\n+\t\tdecimalFormat                         int   = 10",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2070510505",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131571,
        "pr_file": "test/e2e/network/pod_lifecycle.go",
        "discussion_id": "2070510505",
        "commented_code": "@@ -45,9 +45,13 @@ var _ = common.SIGDescribe(\"Connectivity Pod Lifecycle\", func() {\n \tfr.NamespacePodSecurityLevel = admissionapi.LevelPrivileged\n \n \tvar (\n-\t\tcs        clientset.Interface\n-\t\tns        string\n-\t\tpodClient *e2epod.PodClient\n+\t\tcs                                    clientset.Interface\n+\t\tns                                    string\n+\t\tpodClient                             *e2epod.PodClient\n+\t\tnetworkGracePeriodOfOneHundredSeconds int64 = 100\n+\t\tnetworkGracePeriodOfThirtySeconds     int64 = 30\n+\t\tnetworkHttpPort                       int   = 80\n+\t\tdecimalFormat                         int   = 10",
        "comment_created_at": "2025-05-01T16:55:56+00:00",
        "comment_author": "royalsflush",
        "comment_body": "A couple of comments for these:\r\n1. Why not make these local constants? It seems like they're used thorough the file?\r\n2. Could you make the name agnostic to the values? Perhaps shortNetworkGracePeriod / longNetworkGracePeriod?\r\n3. Nit: No need to add \"decimal format\", you can just use fmt.Sprintf for printing that as well\r\n4. Nit: you can shorten the name (which is normally golang best practices, see https://google.github.io/styleguide/go/decisions) to httpPort",
        "pr_file_module": null
      },
      {
        "comment_id": "2070545660",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131571,
        "pr_file": "test/e2e/network/pod_lifecycle.go",
        "discussion_id": "2070510505",
        "commented_code": "@@ -45,9 +45,13 @@ var _ = common.SIGDescribe(\"Connectivity Pod Lifecycle\", func() {\n \tfr.NamespacePodSecurityLevel = admissionapi.LevelPrivileged\n \n \tvar (\n-\t\tcs        clientset.Interface\n-\t\tns        string\n-\t\tpodClient *e2epod.PodClient\n+\t\tcs                                    clientset.Interface\n+\t\tns                                    string\n+\t\tpodClient                             *e2epod.PodClient\n+\t\tnetworkGracePeriodOfOneHundredSeconds int64 = 100\n+\t\tnetworkGracePeriodOfThirtySeconds     int64 = 30\n+\t\tnetworkHttpPort                       int   = 80\n+\t\tdecimalFormat                         int   = 10",
        "comment_created_at": "2025-05-01T17:18:47+00:00",
        "comment_author": "mulatinho",
        "comment_body": "@royalsflush I did the changes you asked, I also thought in use them as constants but we cannot get the address of a constants in the lines we do this, for example:\r\n\r\n```go\r\nwebserverPod.Spec.TerminationGracePeriodSeconds = &longNetworkGracePeriod\r\n```\r\n`invalid operation: cannot take address of longNetworkGracePeriod (constant 100 of type int64)`\r\n\r\nwaiting for your feedback :-)",
        "pr_file_module": null
      },
      {
        "comment_id": "2071275639",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 131571,
        "pr_file": "test/e2e/network/pod_lifecycle.go",
        "discussion_id": "2070510505",
        "commented_code": "@@ -45,9 +45,13 @@ var _ = common.SIGDescribe(\"Connectivity Pod Lifecycle\", func() {\n \tfr.NamespacePodSecurityLevel = admissionapi.LevelPrivileged\n \n \tvar (\n-\t\tcs        clientset.Interface\n-\t\tns        string\n-\t\tpodClient *e2epod.PodClient\n+\t\tcs                                    clientset.Interface\n+\t\tns                                    string\n+\t\tpodClient                             *e2epod.PodClient\n+\t\tnetworkGracePeriodOfOneHundredSeconds int64 = 100\n+\t\tnetworkGracePeriodOfThirtySeconds     int64 = 30\n+\t\tnetworkHttpPort                       int   = 80\n+\t\tdecimalFormat                         int   = 10",
        "comment_created_at": "2025-05-02T08:25:41+00:00",
        "comment_author": "royalsflush",
        "comment_body": "It does, yes! Thanks for the changes",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2217248987",
    "pr_number": 133072,
    "pr_file": "pkg/kubelet/prober/worker_test.go",
    "created_at": "2025-07-19T09:00:26+00:00",
    "commented_code": "}\n }\n \n+func TestDoProbe_SidecarContainerWithRestartPolicy(t *testing.T) {\n+\tlogger, ctx := ktesting.NewTestContext(t)\n+\tm := newTestManager()\n+\n+\t// Test sidecar container (init container with restartPolicy=Always) behavior\n+\tw := newTestWorker(m, startup, v1.Probe{})\n+\n+\t// Set container restart policy to Always (sidecar behavior)\n+\trestartPolicy := v1.ContainerRestartPolicyAlways\n+\tw.container.RestartPolicy = &restartPolicy\n+\n+\t// Set pod restart policy to Never\n+\tw.pod.Spec.RestartPolicy = v1.RestartPolicyNever\n+\n+\t// Create terminated status for sidecar container\n+\tterminatedStatus := getTestRunningStatus()\n+\tterminatedStatus.ContainerStatuses[0].State.Running = nil\n+\tterminatedStatus.ContainerStatuses[0].State.Terminated = &v1.ContainerStateTerminated{\n+\t\tStartedAt: metav1.Now(),\n+\t}\n+\n+\tm.statusManager.SetPodStatus(logger, w.pod, terminatedStatus)\n+\n+\t// Test: Terminated sidecar container with restartPolicy=Always should continue probing\n+\t// even when pod has restartPolicy=Never\n+\tif !w.doProbe(ctx) {\n+\t\tt.Error(\"Expected to continue probing for terminated sidecar container with restart policy Always\")\n+\t}\n+\n+\t// Verify result is set to Failure for terminated container\n+\texpectResult(t, w, results.Failure, \"terminated sidecar with restart policy Always\")\n+}\n+\n+func TestDoProbe_RegularContainerBehaviorUnchanged(t *testing.T) {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2217248987",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133072,
        "pr_file": "pkg/kubelet/prober/worker_test.go",
        "discussion_id": "2217248987",
        "commented_code": "@@ -530,6 +530,70 @@ func TestResultRunOnStartupCheckFailure(t *testing.T) {\n \t}\n }\n \n+func TestDoProbe_SidecarContainerWithRestartPolicy(t *testing.T) {\n+\tlogger, ctx := ktesting.NewTestContext(t)\n+\tm := newTestManager()\n+\n+\t// Test sidecar container (init container with restartPolicy=Always) behavior\n+\tw := newTestWorker(m, startup, v1.Probe{})\n+\n+\t// Set container restart policy to Always (sidecar behavior)\n+\trestartPolicy := v1.ContainerRestartPolicyAlways\n+\tw.container.RestartPolicy = &restartPolicy\n+\n+\t// Set pod restart policy to Never\n+\tw.pod.Spec.RestartPolicy = v1.RestartPolicyNever\n+\n+\t// Create terminated status for sidecar container\n+\tterminatedStatus := getTestRunningStatus()\n+\tterminatedStatus.ContainerStatuses[0].State.Running = nil\n+\tterminatedStatus.ContainerStatuses[0].State.Terminated = &v1.ContainerStateTerminated{\n+\t\tStartedAt: metav1.Now(),\n+\t}\n+\n+\tm.statusManager.SetPodStatus(logger, w.pod, terminatedStatus)\n+\n+\t// Test: Terminated sidecar container with restartPolicy=Always should continue probing\n+\t// even when pod has restartPolicy=Never\n+\tif !w.doProbe(ctx) {\n+\t\tt.Error(\"Expected to continue probing for terminated sidecar container with restart policy Always\")\n+\t}\n+\n+\t// Verify result is set to Failure for terminated container\n+\texpectResult(t, w, results.Failure, \"terminated sidecar with restart policy Always\")\n+}\n+\n+func TestDoProbe_RegularContainerBehaviorUnchanged(t *testing.T) {",
        "comment_created_at": "2025-07-19T09:00:26+00:00",
        "comment_author": "gjkim42",
        "comment_body": "RegularContainerBehaviorUnchanged doesn't look valid in a general context (outside of this issue).\r\nCan we be more specific?\r\n\r\nTestDoProbe_TerminatedContainerWithRestartPolicyNever ?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219271180",
    "pr_number": 132439,
    "pr_file": "pkg/scheduler/schedule_one.go",
    "created_at": "2025-07-21T13:48:39+00:00",
    "commented_code": "var clearNominatedNode = &framework.NominatingInfo{NominatingMode: framework.ModeOverride, NominatedNodeName: \"\"}\n \n+// newFailureScheduleResult creates a ScheduleResult for scheduling failures.\n+// It returns an empty ScheduleResult when NominatedNodeNameForExpectation feature is enabled,\n+// otherwise returns a ScheduleResult with clearNominatedNode to clear the pod's nominated node.\n+func (sched *Scheduler) newFailureScheduleResult() ScheduleResult {",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2219271180",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132439,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2219271180",
        "commented_code": "@@ -138,6 +138,16 @@ func (sched *Scheduler) ScheduleOne(ctx context.Context) {\n \n var clearNominatedNode = &framework.NominatingInfo{NominatingMode: framework.ModeOverride, NominatedNodeName: \"\"}\n \n+// newFailureScheduleResult creates a ScheduleResult for scheduling failures.\n+// It returns an empty ScheduleResult when NominatedNodeNameForExpectation feature is enabled,\n+// otherwise returns a ScheduleResult with clearNominatedNode to clear the pod's nominated node.\n+func (sched *Scheduler) newFailureScheduleResult() ScheduleResult {",
        "comment_created_at": "2025-07-21T13:48:39+00:00",
        "comment_author": "macsko",
        "comment_body": "Now I think it would be better to have something like `newFailureNominatingInfo` to cover the `sched.FailureHandler` call as well",
        "pr_file_module": null
      },
      {
        "comment_id": "2219274945",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132439,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2219271180",
        "commented_code": "@@ -138,6 +138,16 @@ func (sched *Scheduler) ScheduleOne(ctx context.Context) {\n \n var clearNominatedNode = &framework.NominatingInfo{NominatingMode: framework.ModeOverride, NominatedNodeName: \"\"}\n \n+// newFailureScheduleResult creates a ScheduleResult for scheduling failures.\n+// It returns an empty ScheduleResult when NominatedNodeNameForExpectation feature is enabled,\n+// otherwise returns a ScheduleResult with clearNominatedNode to clear the pod's nominated node.\n+func (sched *Scheduler) newFailureScheduleResult() ScheduleResult {",
        "comment_created_at": "2025-07-21T13:49:48+00:00",
        "comment_author": "macsko",
        "comment_body": "Also, `clearNominatedNodeName` should be removed from the package scope to prevent misuse. You can just return it by this func",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2223921916",
    "pr_number": 132959,
    "pr_file": "test/e2e/network/service.go",
    "created_at": "2025-07-22T22:10:29+00:00",
    "commented_code": "checkServiceReachabilityFromExecPod(ctx, f.ClientSet, ns, service.Name, service.Spec.ClusterIP, port)\n \t})\n+\n+\tginkgo.It(\"should support named targetPorts that resolve to different ports on different endpoints\", func(ctx context.Context) {\n+\t\tserviceName := \"mutable-named-port\"\n+\t\tns := f.Namespace.Name\n+\n+\t\tt := NewServerTest(cs, ns, serviceName)\n+\t\tdefer func() {\n+\t\t\tdefer ginkgo.GinkgoRecover()\n+\t\t\terrs := t.Cleanup()\n+\t\t\tif len(errs) != 0 {\n+\t\t\t\tframework.Failf(\"errors in cleanup: %v\", errs)\n+\t\t\t}\n+\t\t}()\n+\n+\t\tcontainerPortName := \"mutable-port\"\n+\t\toldContainerPortNum := int32(8080)\n+\t\tmodifiedPortNum := int32(8000)\n+\t\tcontainerPortDesc := []v1.ContainerPort{{Name: containerPortName, ContainerPort: oldContainerPortNum, Protocol: v1.ProtocolTCP}}\n+\t\tmodifiedPortDesc := []v1.ContainerPort{{Name: containerPortName, ContainerPort: modifiedPortNum, Protocol: v1.ProtocolTCP}}\n+\t\targs := []string{\"serve-hostname\", fmt.Sprintf(\"--port=%d\", oldContainerPortNum)}\n+\t\tfor _, podName := range []string{\"testpod0\", \"testpod1\"} {\n+\t\t\tcreatePodOrFail(ctx, f, ns, podName, t.Labels, containerPortDesc, args...)\n+\t\t}\n+\n+\t\tservice := t.BuildServiceSpec()\n+\t\tservicePort := int32(8001)\n+\t\tservice.Spec.Ports = []v1.ServicePort{\n+\t\t\t{\n+\t\t\t\tName:       serviceName,\n+\t\t\t\tPort:       servicePort,\n+\t\t\t\tTargetPort: intstr.FromString(containerPortName),\n+\t\t\t},\n+\t\t}\n+\n+\t\tginkgo.By(fmt.Sprintf(\"creating Service %v with selectors %v\", service.Name, service.Spec.Selector))\n+\t\tservice, err := t.CreateService(service)\n+\t\tframework.ExpectNoError(err)\n+\t\tjig := e2eservice.NewTestJig(cs, ns, serviceName)\n+\t\texecPod := e2epod.CreateExecPodOrFail(ctx, cs, ns, \"execpod-affinity\", nil)\n+\t\tginkgo.DeferCleanup(func(ctx context.Context) {\n+\t\t\tframework.Logf(\"Cleaning up the exec pod\")\n+\t\t\terr := cs.CoreV1().Pods(ns).Delete(ctx, execPod.Name, metav1.DeleteOptions{})\n+\t\t\tframework.ExpectNoError(err, \"failed to delete pod: %s in namespace: %s\", execPod.Name, ns)\n+\t\t})\n+\n+\t\terr = jig.CheckServiceReachability(ctx, service, execPod)\n+\t\tframework.ExpectNoError(err)\n+\t\tif !checkAffinity(ctx, cs, execPod, service.Spec.ClusterIP, int(servicePort), false) {\n+\t\t\tframework.Failf(\"session affinity check failed on the service %s (%s:%d), expected reaching 2 endpoints\", service.Name, service.Spec.ClusterIP, servicePort)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2223921916",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132959,
        "pr_file": "test/e2e/network/service.go",
        "discussion_id": "2223921916",
        "commented_code": "@@ -4228,6 +4228,76 @@ var _ = common.SIGDescribe(\"Services\", func() {\n \n \t\tcheckServiceReachabilityFromExecPod(ctx, f.ClientSet, ns, service.Name, service.Spec.ClusterIP, port)\n \t})\n+\n+\tginkgo.It(\"should support named targetPorts that resolve to different ports on different endpoints\", func(ctx context.Context) {\n+\t\tserviceName := \"mutable-named-port\"\n+\t\tns := f.Namespace.Name\n+\n+\t\tt := NewServerTest(cs, ns, serviceName)\n+\t\tdefer func() {\n+\t\t\tdefer ginkgo.GinkgoRecover()\n+\t\t\terrs := t.Cleanup()\n+\t\t\tif len(errs) != 0 {\n+\t\t\t\tframework.Failf(\"errors in cleanup: %v\", errs)\n+\t\t\t}\n+\t\t}()\n+\n+\t\tcontainerPortName := \"mutable-port\"\n+\t\toldContainerPortNum := int32(8080)\n+\t\tmodifiedPortNum := int32(8000)\n+\t\tcontainerPortDesc := []v1.ContainerPort{{Name: containerPortName, ContainerPort: oldContainerPortNum, Protocol: v1.ProtocolTCP}}\n+\t\tmodifiedPortDesc := []v1.ContainerPort{{Name: containerPortName, ContainerPort: modifiedPortNum, Protocol: v1.ProtocolTCP}}\n+\t\targs := []string{\"serve-hostname\", fmt.Sprintf(\"--port=%d\", oldContainerPortNum)}\n+\t\tfor _, podName := range []string{\"testpod0\", \"testpod1\"} {\n+\t\t\tcreatePodOrFail(ctx, f, ns, podName, t.Labels, containerPortDesc, args...)\n+\t\t}\n+\n+\t\tservice := t.BuildServiceSpec()\n+\t\tservicePort := int32(8001)\n+\t\tservice.Spec.Ports = []v1.ServicePort{\n+\t\t\t{\n+\t\t\t\tName:       serviceName,\n+\t\t\t\tPort:       servicePort,\n+\t\t\t\tTargetPort: intstr.FromString(containerPortName),\n+\t\t\t},\n+\t\t}\n+\n+\t\tginkgo.By(fmt.Sprintf(\"creating Service %v with selectors %v\", service.Name, service.Spec.Selector))\n+\t\tservice, err := t.CreateService(service)\n+\t\tframework.ExpectNoError(err)\n+\t\tjig := e2eservice.NewTestJig(cs, ns, serviceName)\n+\t\texecPod := e2epod.CreateExecPodOrFail(ctx, cs, ns, \"execpod-affinity\", nil)\n+\t\tginkgo.DeferCleanup(func(ctx context.Context) {\n+\t\t\tframework.Logf(\"Cleaning up the exec pod\")\n+\t\t\terr := cs.CoreV1().Pods(ns).Delete(ctx, execPod.Name, metav1.DeleteOptions{})\n+\t\t\tframework.ExpectNoError(err, \"failed to delete pod: %s in namespace: %s\", execPod.Name, ns)\n+\t\t})\n+\n+\t\terr = jig.CheckServiceReachability(ctx, service, execPod)\n+\t\tframework.ExpectNoError(err)\n+\t\tif !checkAffinity(ctx, cs, execPod, service.Spec.ClusterIP, int(servicePort), false) {\n+\t\t\tframework.Failf(\"session affinity check failed on the service %s (%s:%d), expected reaching 2 endpoints\", service.Name, service.Spec.ClusterIP, servicePort)",
        "comment_created_at": "2025-07-22T22:10:29+00:00",
        "comment_author": "danwinship",
        "comment_body": "while we're using the session affinity helper function, we aren't _actually_ testing affinity, so you shouldn't mention it in the user-visible messages.\r\n\r\n```suggestion\r\n\t\t\tframework.Failf(\"reachability check failed on the service %s (%s:%d), expected to reach both endpoints\", service.Name, service.Spec.ClusterIP, servicePort)\r\n```\r\n\r\nand likewise in the other two Failf calls below.\r\n\r\n(Likewise, maybe just call the execpod `\"execpod\"` rather than `\"execpod-affinity\"`",
        "pr_file_module": null
      },
      {
        "comment_id": "2224245869",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 132959,
        "pr_file": "test/e2e/network/service.go",
        "discussion_id": "2223921916",
        "commented_code": "@@ -4228,6 +4228,76 @@ var _ = common.SIGDescribe(\"Services\", func() {\n \n \t\tcheckServiceReachabilityFromExecPod(ctx, f.ClientSet, ns, service.Name, service.Spec.ClusterIP, port)\n \t})\n+\n+\tginkgo.It(\"should support named targetPorts that resolve to different ports on different endpoints\", func(ctx context.Context) {\n+\t\tserviceName := \"mutable-named-port\"\n+\t\tns := f.Namespace.Name\n+\n+\t\tt := NewServerTest(cs, ns, serviceName)\n+\t\tdefer func() {\n+\t\t\tdefer ginkgo.GinkgoRecover()\n+\t\t\terrs := t.Cleanup()\n+\t\t\tif len(errs) != 0 {\n+\t\t\t\tframework.Failf(\"errors in cleanup: %v\", errs)\n+\t\t\t}\n+\t\t}()\n+\n+\t\tcontainerPortName := \"mutable-port\"\n+\t\toldContainerPortNum := int32(8080)\n+\t\tmodifiedPortNum := int32(8000)\n+\t\tcontainerPortDesc := []v1.ContainerPort{{Name: containerPortName, ContainerPort: oldContainerPortNum, Protocol: v1.ProtocolTCP}}\n+\t\tmodifiedPortDesc := []v1.ContainerPort{{Name: containerPortName, ContainerPort: modifiedPortNum, Protocol: v1.ProtocolTCP}}\n+\t\targs := []string{\"serve-hostname\", fmt.Sprintf(\"--port=%d\", oldContainerPortNum)}\n+\t\tfor _, podName := range []string{\"testpod0\", \"testpod1\"} {\n+\t\t\tcreatePodOrFail(ctx, f, ns, podName, t.Labels, containerPortDesc, args...)\n+\t\t}\n+\n+\t\tservice := t.BuildServiceSpec()\n+\t\tservicePort := int32(8001)\n+\t\tservice.Spec.Ports = []v1.ServicePort{\n+\t\t\t{\n+\t\t\t\tName:       serviceName,\n+\t\t\t\tPort:       servicePort,\n+\t\t\t\tTargetPort: intstr.FromString(containerPortName),\n+\t\t\t},\n+\t\t}\n+\n+\t\tginkgo.By(fmt.Sprintf(\"creating Service %v with selectors %v\", service.Name, service.Spec.Selector))\n+\t\tservice, err := t.CreateService(service)\n+\t\tframework.ExpectNoError(err)\n+\t\tjig := e2eservice.NewTestJig(cs, ns, serviceName)\n+\t\texecPod := e2epod.CreateExecPodOrFail(ctx, cs, ns, \"execpod-affinity\", nil)\n+\t\tginkgo.DeferCleanup(func(ctx context.Context) {\n+\t\t\tframework.Logf(\"Cleaning up the exec pod\")\n+\t\t\terr := cs.CoreV1().Pods(ns).Delete(ctx, execPod.Name, metav1.DeleteOptions{})\n+\t\t\tframework.ExpectNoError(err, \"failed to delete pod: %s in namespace: %s\", execPod.Name, ns)\n+\t\t})\n+\n+\t\terr = jig.CheckServiceReachability(ctx, service, execPod)\n+\t\tframework.ExpectNoError(err)\n+\t\tif !checkAffinity(ctx, cs, execPod, service.Spec.ClusterIP, int(servicePort), false) {\n+\t\t\tframework.Failf(\"session affinity check failed on the service %s (%s:%d), expected reaching 2 endpoints\", service.Name, service.Spec.ClusterIP, servicePort)",
        "comment_created_at": "2025-07-23T03:23:15+00:00",
        "comment_author": "ylink-lfs",
        "comment_body": "Added the suggested naming changes",
        "pr_file_module": null
      }
    ]
  }
]