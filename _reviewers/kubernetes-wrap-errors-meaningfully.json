[
  {
    "discussion_id": "1991226724",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-03-12T11:03:23+00:00",
    "commented_code": "state.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\treturn statusError(logger, err)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "1991226724",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1991226724",
        "commented_code": "@@ -779,6 +796,48 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs *framework.CycleStat\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\treturn statusError(logger, err)",
        "comment_created_at": "2025-03-12T11:03:23+00:00",
        "comment_author": "macsko",
        "comment_body": "Error should be wrapped to be more meaningful, especially when it's a timeout.",
        "pr_file_module": null
      },
      {
        "comment_id": "1992935285",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1991226724",
        "commented_code": "@@ -779,6 +796,48 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs *framework.CycleStat\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\treturn statusError(logger, err)",
        "comment_created_at": "2025-03-13T07:31:15+00:00",
        "comment_author": "KobayashiD27",
        "comment_body": "I'll wrap err in the next update.\r\nI'm thinking of something like this:\r\n```\r\n\tif err != nil {\r\n\t\tif errors.Is(err, context.DeadlineExceeded) {\r\n\t\t\terr = fmt.Errorf(\"device binding timeout, %s\", err)\r\n\t\t}\r\n\t\treturn statusError(logger, err)\r\n\t}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1993151984",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1991226724",
        "commented_code": "@@ -779,6 +796,48 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs *framework.CycleStat\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\treturn statusError(logger, err)",
        "comment_created_at": "2025-03-13T09:50:36+00:00",
        "comment_author": "macsko",
        "comment_body": "Sounds good, but keep the way of wrapping errors:\r\n`err = fmt.Errorf(\"device binding timeout: %w\", err)`",
        "pr_file_module": null
      },
      {
        "comment_id": "2222290157",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "1991226724",
        "commented_code": "@@ -779,6 +796,48 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs *framework.CycleStat\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := checkBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\ttimeoutDefault := int64(600)\n+\ttimeoutMax := int64(1200)\n+\ttimeout := int64(0)\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif device.BindingTimeoutSeconds != nil && timeout < *device.BindingTimeoutSeconds {\n+\t\t\t\ttimeout = *device.BindingTimeoutSeconds\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout <= 0 {\n+\t\ttimeout = timeoutDefault\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\n+\t// We need to wait for the device to be attached to the node.\n+\terr = wait.PollUntilContextTimeout(ctx, 5*time.Second, time.Duration(timeout)*time.Second, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(ctx, state, pod, nodeName)\n+\t\t})\n+\tif err != nil {\n+\t\treturn statusError(logger, err)",
        "comment_created_at": "2025-07-22T11:56:08+00:00",
        "comment_author": "ania-borowiec",
        "comment_body": "bumping Maciej's last comment, as it's still applicable: \r\n\r\nSounds good, but keep the way of wrapping errors:\r\nerr = fmt.Errorf(\"device binding timeout: %w\", err)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2218917346",
    "pr_number": 130160,
    "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
    "created_at": "2025-07-21T11:30:44+00:00",
    "commented_code": "state.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions || !pl.enableDeviceStatus {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := hasBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\tconst timeoutDefaultSeconds int64 = 600\n+\ttimeoutMax := 20 * time.Minute\n+\ttimeout := 0 * time.Second\n+\ttimeStartWaiting := time.Now()\n+\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif deviceTimeout := time.Duration(ptr.Deref(device.BindingTimeoutSeconds, timeoutDefaultSeconds)) * time.Second; timeout < deviceTimeout {\n+\t\t\t\ttimeout = deviceTimeout\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\tdeadlineCtx, cancelFunc := context.WithDeadline(ctx, timeStartWaiting.Add(timeout))\n+\tdefer cancelFunc()\n+\n+\t// We need to wait for the device to be attached to the node.\n+\tpl.fh.EventRecorder().Eventf(pod, nil, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for binding conditions for device on node %s\", nodeName)\n+\terr = wait.PollUntilContextTimeout(deadlineCtx, 5*time.Second, timeout, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(state)\n+\t\t})\n+\tif err != nil {\n+\t\tif errors.Is(err, context.DeadlineExceeded) {\n+\t\t\terr = fmt.Errorf(\"device binding timeout: %w\", err)\n+\t\t}\n+\t\treturn statusError(logger, err)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2218917346",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 130160,
        "pr_file": "pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go",
        "discussion_id": "2218917346",
        "commented_code": "@@ -822,6 +845,53 @@ func (pl *DynamicResources) PreBind(ctx context.Context, cs fwk.CycleState, pod\n \t\t\tstate.claims[index] = claim\n \t\t}\n \t}\n+\n+\tif !pl.enableDeviceBindingConditions || !pl.enableDeviceStatus {\n+\t\t// If we get here, we know that reserving the claim for\n+\t\t// the pod worked and we can proceed with binding it.\n+\t\treturn nil\n+\t}\n+\n+\t// We need to check if the device is attached to the node.\n+\tneedWait := hasBindingConditions(state)\n+\n+\t// If no device needs to be prepared, we can return early.\n+\tif !needWait {\n+\t\treturn nil\n+\t}\n+\n+\t// We need to decide how long we should wait for the device to be attached to the node.\n+\tconst timeoutDefaultSeconds int64 = 600\n+\ttimeoutMax := 20 * time.Minute\n+\ttimeout := 0 * time.Second\n+\ttimeStartWaiting := time.Now()\n+\n+\tfor _, claim := range state.claims {\n+\t\tfor _, device := range claim.Status.Allocation.Devices.Results {\n+\t\t\tif deviceTimeout := time.Duration(ptr.Deref(device.BindingTimeoutSeconds, timeoutDefaultSeconds)) * time.Second; timeout < deviceTimeout {\n+\t\t\t\ttimeout = deviceTimeout\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif timeout > timeoutMax {\n+\t\ttimeout = timeoutMax\n+\t}\n+\tdeadlineCtx, cancelFunc := context.WithDeadline(ctx, timeStartWaiting.Add(timeout))\n+\tdefer cancelFunc()\n+\n+\t// We need to wait for the device to be attached to the node.\n+\tpl.fh.EventRecorder().Eventf(pod, nil, v1.EventTypeNormal, \"BindingConditionsPending\", \"Scheduling\", \"waiting for binding conditions for device on node %s\", nodeName)\n+\terr = wait.PollUntilContextTimeout(deadlineCtx, 5*time.Second, timeout, true,\n+\t\tfunc(ctx context.Context) (bool, error) {\n+\t\t\treturn pl.hasDeviceBindingStatus(state)\n+\t\t})\n+\tif err != nil {\n+\t\tif errors.Is(err, context.DeadlineExceeded) {\n+\t\t\terr = fmt.Errorf(\"device binding timeout: %w\", err)\n+\t\t}\n+\t\treturn statusError(logger, err)",
        "comment_created_at": "2025-07-21T11:30:44+00:00",
        "comment_author": "pohly",
        "comment_body": "```suggestion\r\n\t\t// Returning an error here causes another scheduling attempt.\r\n\t\t// In that next attempt, PreFilter will detect the timeout or\r\n\t\t// error and try to recover.\r\n\t\treturn statusError(logger, err)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2218940714",
    "pr_number": 133021,
    "pr_file": "pkg/scheduler/schedule_one.go",
    "created_at": "2025-07-21T11:39:37+00:00",
    "commented_code": "assumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {\n+\t\tpreFlightStatus := schedFramework.RunPreBindPreFlightPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)\n+\t\tif preFlightStatus.Code() == fwk.Error || preFlightStatus.IsRejected() {\n+\t\t\treturn preFlightStatus",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2218940714",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133021,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2218940714",
        "commented_code": "@@ -279,6 +281,23 @@ func (sched *Scheduler) bindingCycle(\n \n \tassumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {\n+\t\tpreFlightStatus := schedFramework.RunPreBindPreFlightPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)\n+\t\tif preFlightStatus.Code() == fwk.Error || preFlightStatus.IsRejected() {\n+\t\t\treturn preFlightStatus",
        "comment_created_at": "2025-07-21T11:39:37+00:00",
        "comment_author": "macsko",
        "comment_body": "Shouldn't we return `FitError` when it rejects?",
        "pr_file_module": null
      },
      {
        "comment_id": "2219305938",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133021,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2218940714",
        "commented_code": "@@ -279,6 +281,23 @@ func (sched *Scheduler) bindingCycle(\n \n \tassumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {\n+\t\tpreFlightStatus := schedFramework.RunPreBindPreFlightPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)\n+\t\tif preFlightStatus.Code() == fwk.Error || preFlightStatus.IsRejected() {\n+\t\t\treturn preFlightStatus",
        "comment_created_at": "2025-07-21T14:01:13+00:00",
        "comment_author": "sanposhiho",
        "comment_body": "No, Unschedulable returned from PreBindPreFlight is an error because PreBindPreFlight should only return success, skip, or error.\r\nIf you see `RunPreBindPreFlightPlugins`, it translates Unschedulable into error actually. So, here is just a safeguard for the same purpose.",
        "pr_file_module": null
      },
      {
        "comment_id": "2219312835",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133021,
        "pr_file": "pkg/scheduler/schedule_one.go",
        "discussion_id": "2218940714",
        "commented_code": "@@ -279,6 +281,23 @@ func (sched *Scheduler) bindingCycle(\n \n \tassumedPod := assumedPodInfo.Pod\n \n+\tif feature.DefaultFeatureGate.Enabled(features.NominatedNodeNameForExpectation) {\n+\t\tpreFlightStatus := schedFramework.RunPreBindPreFlightPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)\n+\t\tif preFlightStatus.Code() == fwk.Error || preFlightStatus.IsRejected() {\n+\t\t\treturn preFlightStatus",
        "comment_created_at": "2025-07-21T14:03:41+00:00",
        "comment_author": "macsko",
        "comment_body": "Sounds good. Please add a short comment for clarity here",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2226983654",
    "pr_number": 133138,
    "pr_file": "pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go",
    "created_at": "2025-07-24T00:06:58+00:00",
    "commented_code": "return nil, err\n \t\t}\n \n-\t\t// Read the current inhibitDelay again, if the override was successful, currentInhibitDelay will be equal to shutdownGracePeriodRequested.\n-\t\tupdatedInhibitDelay, err := m.dbusCon.CurrentInhibitDelay()\n+\t\t// The ReloadLogindConf() call is asynchronous so it might take a short period before logind finishes reloading its configuration.\n+\t\tbackoff := wait.Backoff{\n+\t\t\tDuration: 100 * time.Millisecond,\n+\t\t\tFactor:   2.0,\n+\t\t\tSteps:    5,\n+\t\t}\n+\t\tvar updatedInhibitDelay time.Duration\n+\t\terr = wait.ExponentialBackoff(backoff, func() (bool, error) {\n+\t\t\t// Read the current inhibitDelay again, if the override was successful, currentInhibitDelay will be equal to shutdownGracePeriodRequested.\n+\t\t\tupdatedInhibitDelay, err = m.dbusCon.CurrentInhibitDelay()\n+\t\t\tif err != nil {\n+\t\t\t\treturn false, err\n+\t\t\t}\n+\t\t\treturn periodRequested <= updatedInhibitDelay, nil\n+\t\t})\n \t\tif err != nil {\n-\t\t\treturn nil, err\n+\t\t\tif !wait.Interrupted(err) {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t\tif periodRequested > updatedInhibitDelay {\n+\t\t\t\treturn nil, fmt.Errorf(\"node shutdown manager was unable to update logind InhibitDelayMaxSec to %v (ShutdownGracePeriod), current value of InhibitDelayMaxSec (%v) is less than requested ShutdownGracePeriod\", periodRequested, updatedInhibitDelay)",
    "repo_full_name": "kubernetes/kubernetes",
    "discussion_comments": [
      {
        "comment_id": "2226983654",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133138,
        "pr_file": "pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go",
        "discussion_id": "2226983654",
        "commented_code": "@@ -191,15 +192,30 @@ func (m *managerImpl) start() (chan struct{}, error) {\n \t\t\treturn nil, err\n \t\t}\n \n-\t\t// Read the current inhibitDelay again, if the override was successful, currentInhibitDelay will be equal to shutdownGracePeriodRequested.\n-\t\tupdatedInhibitDelay, err := m.dbusCon.CurrentInhibitDelay()\n+\t\t// The ReloadLogindConf() call is asynchronous so it might take a short period before logind finishes reloading its configuration.\n+\t\tbackoff := wait.Backoff{\n+\t\t\tDuration: 100 * time.Millisecond,\n+\t\t\tFactor:   2.0,\n+\t\t\tSteps:    5,\n+\t\t}\n+\t\tvar updatedInhibitDelay time.Duration\n+\t\terr = wait.ExponentialBackoff(backoff, func() (bool, error) {\n+\t\t\t// Read the current inhibitDelay again, if the override was successful, currentInhibitDelay will be equal to shutdownGracePeriodRequested.\n+\t\t\tupdatedInhibitDelay, err = m.dbusCon.CurrentInhibitDelay()\n+\t\t\tif err != nil {\n+\t\t\t\treturn false, err\n+\t\t\t}\n+\t\t\treturn periodRequested <= updatedInhibitDelay, nil\n+\t\t})\n \t\tif err != nil {\n-\t\t\treturn nil, err\n+\t\t\tif !wait.Interrupted(err) {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t\tif periodRequested > updatedInhibitDelay {\n+\t\t\t\treturn nil, fmt.Errorf(\"node shutdown manager was unable to update logind InhibitDelayMaxSec to %v (ShutdownGracePeriod), current value of InhibitDelayMaxSec (%v) is less than requested ShutdownGracePeriod\", periodRequested, updatedInhibitDelay)",
        "comment_created_at": "2025-07-24T00:06:58+00:00",
        "comment_author": "ajaysundark",
        "comment_body": "Could the final error message be more descriptive and mention retries attempted?",
        "pr_file_module": null
      },
      {
        "comment_id": "2227346353",
        "repo_full_name": "kubernetes/kubernetes",
        "pr_number": 133138,
        "pr_file": "pkg/kubelet/nodeshutdown/nodeshutdown_manager_linux.go",
        "discussion_id": "2226983654",
        "commented_code": "@@ -191,15 +192,30 @@ func (m *managerImpl) start() (chan struct{}, error) {\n \t\t\treturn nil, err\n \t\t}\n \n-\t\t// Read the current inhibitDelay again, if the override was successful, currentInhibitDelay will be equal to shutdownGracePeriodRequested.\n-\t\tupdatedInhibitDelay, err := m.dbusCon.CurrentInhibitDelay()\n+\t\t// The ReloadLogindConf() call is asynchronous so it might take a short period before logind finishes reloading its configuration.\n+\t\tbackoff := wait.Backoff{\n+\t\t\tDuration: 100 * time.Millisecond,\n+\t\t\tFactor:   2.0,\n+\t\t\tSteps:    5,\n+\t\t}\n+\t\tvar updatedInhibitDelay time.Duration\n+\t\terr = wait.ExponentialBackoff(backoff, func() (bool, error) {\n+\t\t\t// Read the current inhibitDelay again, if the override was successful, currentInhibitDelay will be equal to shutdownGracePeriodRequested.\n+\t\t\tupdatedInhibitDelay, err = m.dbusCon.CurrentInhibitDelay()\n+\t\t\tif err != nil {\n+\t\t\t\treturn false, err\n+\t\t\t}\n+\t\t\treturn periodRequested <= updatedInhibitDelay, nil\n+\t\t})\n \t\tif err != nil {\n-\t\t\treturn nil, err\n+\t\t\tif !wait.Interrupted(err) {\n+\t\t\t\treturn nil, err\n+\t\t\t}\n+\t\t\tif periodRequested > updatedInhibitDelay {\n+\t\t\t\treturn nil, fmt.Errorf(\"node shutdown manager was unable to update logind InhibitDelayMaxSec to %v (ShutdownGracePeriod), current value of InhibitDelayMaxSec (%v) is less than requested ShutdownGracePeriod\", periodRequested, updatedInhibitDelay)",
        "comment_created_at": "2025-07-24T05:01:22+00:00",
        "comment_author": "linxiulei",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  }
]