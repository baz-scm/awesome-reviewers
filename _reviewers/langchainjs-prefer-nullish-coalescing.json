[
  {
    "discussion_id": "2093795865",
    "pr_number": 8141,
    "pr_file": "libs/langchain-anthropic/src/utils/message_inputs.ts",
    "created_at": "2025-05-17T00:38:48+00:00",
    "commented_code": "content: [\n              {\n                type: \"tool_result\",\n                content: _formatContent(message.content),\n                // rare case: message.content could be undefined\n                ...(message.content != null ? { content: _formatContent(message.content) } : {}),",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "2093795865",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8141,
        "pr_file": "libs/langchain-anthropic/src/utils/message_inputs.ts",
        "discussion_id": "2093795865",
        "commented_code": "@@ -116,7 +116,8 @@ function _ensureMessageContents(\n             content: [\n               {\n                 type: \"tool_result\",\n-                content: _formatContent(message.content),\n+                // rare case: message.content could be undefined\n+                ...(message.content != null ? { content: _formatContent(message.content) } : {}),",
        "comment_created_at": "2025-05-17T00:38:48+00:00",
        "comment_author": "benjamincburns",
        "comment_body": "Slightly modified from the original submission. Anthropic's API reference shows `content` here as being an _optional_ (but not nullable) field of type `string | object[]`. The changes in `core` will make this case no longer possible, but this should handle it for older versions that weren't coalescing `undefined` tool output to `\"\"`.\r\n\r\n![image](https://github.com/user-attachments/assets/5e282e7d-5766-47b7-8d5b-39231a4fbc8c)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1433347005",
    "pr_number": 3448,
    "pr_file": "langchain/src/chat_models/azure_ml.ts",
    "created_at": "2023-12-21T01:37:48+00:00",
    "commented_code": "import { SimpleChatModel, BaseChatModelParams } from \"./base.js\";\nimport { AzureMLHttpClient } from \"../llms/azure_ml.js\";\nimport { getEnvironmentVariable } from \"../util/env.js\";\nimport { BaseMessage } from \"../schema/index.js\";\n\nexport interface ChatContentFormatter {\n  /**\n   * Formats the request payload for the AzureML endpoint. It takes a\n   * prompt and a dictionary of model arguments as input and returns a\n   * string representing the formatted request payload.\n   * @param messages A list of messages for the chat so far.\n   * @param modelArgs A dictionary of model arguments.\n   * @returns A string representing the formatted request payload.\n   */\n  formatRequestPayload: (\n    messages: BaseMessage[],\n    modelArgs: Record<string, unknown>\n  ) => string;\n  /**\n   * Formats the response payload from the AzureML endpoint. It takes a\n   * response payload as input and returns a string representing the\n   * formatted response.\n   * @param responsePayload The response payload from the AzureML endpoint.\n   * @returns A string representing the formatted response.\n   */\n  formatResponsePayload: (output: string) => string;\n}\n\nexport class LlamaContentFormatter implements ChatContentFormatter {\n  _convertMessageToRecord(message: BaseMessage): Record<string, unknown> {\n    if (message._getType() === \"human\") {\n      return { role: \"user\", content: message.content };\n    } else if (message._getType() === \"ai\") {\n      return { role: \"assistant\", content: message.content };\n    } else {\n      return { role: message._getType(), content: message.content };\n    }\n  }\n\n  formatRequestPayload(\n    messages: BaseMessage[],\n    modelArgs: Record<string, unknown>\n  ): string {\n    let msgs = messages.map((message) => {\n      this._convertMessageToRecord(message);\n    });\n    return JSON.stringify({\n      input_data: {\n        input_string: msgs,\n        parameters: modelArgs,\n      },\n    });\n  }\n\n  formatResponsePayload(responsePayload: string) {\n    const response = JSON.parse(responsePayload);\n    return response.output;\n  }\n}\n\n/**\n * Type definition for the input parameters of the AzureMLChatOnlineEndpoint class.\n */\nexport interface AzureMLChatParams extends BaseChatModelParams {\n  endpointUrl?: string;\n  endpointApiKey?: string;\n  modelArgs?: Record<string, unknown>;\n  contentFormatter?: ChatContentFormatter;\n}\n\n/**\n * Class that represents the chat model. It extends the SimpleChatModel class and implements the AzureMLChatInput interface.\n */\nexport class AzureMLChatOnlineEndpoint\n  extends SimpleChatModel\n  implements AzureMLChatParams\n{\n  static lc_name() {\n    return \"AzureMLChatOnlineEndpoint\";\n  }\n  static lc_description() {\n    return \"A class for interacting with AzureML Chat models.\";\n  }\n  endpointUrl: string;\n  endpointApiKey: string;\n  modelArgs?: Record<string, unknown>;\n  contentFormatter: ChatContentFormatter;\n  httpClient: AzureMLHttpClient;\n\n  constructor(fields: AzureMLChatParams) {\n    super(fields ?? {});\n    if (!fields?.endpointUrl && !getEnvironmentVariable(\"AZUREML_URL\")) {\n      throw new Error(\"No Azure ML Url found.\");\n    }\n    if (!fields?.endpointApiKey && !getEnvironmentVariable(\"AZUREML_API_KEY\")) {\n      throw new Error(\"No Azure ML ApiKey found.\");\n    }\n    if (!fields?.contentFormatter) {\n      throw new Error(\"No Content Formatter provided.\");\n    }\n\n    this.endpointUrl =\n      fields.endpointUrl || getEnvironmentVariable(\"AZUREML_URL\") + \"\";",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1433347005",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 3448,
        "pr_file": "langchain/src/chat_models/azure_ml.ts",
        "discussion_id": "1433347005",
        "commented_code": "@@ -0,0 +1,142 @@\n+import { SimpleChatModel, BaseChatModelParams } from \"./base.js\";\n+import { AzureMLHttpClient } from \"../llms/azure_ml.js\";\n+import { getEnvironmentVariable } from \"../util/env.js\";\n+import { BaseMessage } from \"../schema/index.js\";\n+\n+export interface ChatContentFormatter {\n+  /**\n+   * Formats the request payload for the AzureML endpoint. It takes a\n+   * prompt and a dictionary of model arguments as input and returns a\n+   * string representing the formatted request payload.\n+   * @param messages A list of messages for the chat so far.\n+   * @param modelArgs A dictionary of model arguments.\n+   * @returns A string representing the formatted request payload.\n+   */\n+  formatRequestPayload: (\n+    messages: BaseMessage[],\n+    modelArgs: Record<string, unknown>\n+  ) => string;\n+  /**\n+   * Formats the response payload from the AzureML endpoint. It takes a\n+   * response payload as input and returns a string representing the\n+   * formatted response.\n+   * @param responsePayload The response payload from the AzureML endpoint.\n+   * @returns A string representing the formatted response.\n+   */\n+  formatResponsePayload: (output: string) => string;\n+}\n+\n+export class LlamaContentFormatter implements ChatContentFormatter {\n+  _convertMessageToRecord(message: BaseMessage): Record<string, unknown> {\n+    if (message._getType() === \"human\") {\n+      return { role: \"user\", content: message.content };\n+    } else if (message._getType() === \"ai\") {\n+      return { role: \"assistant\", content: message.content };\n+    } else {\n+      return { role: message._getType(), content: message.content };\n+    }\n+  }\n+\n+  formatRequestPayload(\n+    messages: BaseMessage[],\n+    modelArgs: Record<string, unknown>\n+  ): string {\n+    let msgs = messages.map((message) => {\n+      this._convertMessageToRecord(message);\n+    });\n+    return JSON.stringify({\n+      input_data: {\n+        input_string: msgs,\n+        parameters: modelArgs,\n+      },\n+    });\n+  }\n+\n+  formatResponsePayload(responsePayload: string) {\n+    const response = JSON.parse(responsePayload);\n+    return response.output;\n+  }\n+}\n+\n+/**\n+ * Type definition for the input parameters of the AzureMLChatOnlineEndpoint class.\n+ */\n+export interface AzureMLChatParams extends BaseChatModelParams {\n+  endpointUrl?: string;\n+  endpointApiKey?: string;\n+  modelArgs?: Record<string, unknown>;\n+  contentFormatter?: ChatContentFormatter;\n+}\n+\n+/**\n+ * Class that represents the chat model. It extends the SimpleChatModel class and implements the AzureMLChatInput interface.\n+ */\n+export class AzureMLChatOnlineEndpoint\n+  extends SimpleChatModel\n+  implements AzureMLChatParams\n+{\n+  static lc_name() {\n+    return \"AzureMLChatOnlineEndpoint\";\n+  }\n+  static lc_description() {\n+    return \"A class for interacting with AzureML Chat models.\";\n+  }\n+  endpointUrl: string;\n+  endpointApiKey: string;\n+  modelArgs?: Record<string, unknown>;\n+  contentFormatter: ChatContentFormatter;\n+  httpClient: AzureMLHttpClient;\n+\n+  constructor(fields: AzureMLChatParams) {\n+    super(fields ?? {});\n+    if (!fields?.endpointUrl && !getEnvironmentVariable(\"AZUREML_URL\")) {\n+      throw new Error(\"No Azure ML Url found.\");\n+    }\n+    if (!fields?.endpointApiKey && !getEnvironmentVariable(\"AZUREML_API_KEY\")) {\n+      throw new Error(\"No Azure ML ApiKey found.\");\n+    }\n+    if (!fields?.contentFormatter) {\n+      throw new Error(\"No Content Formatter provided.\");\n+    }\n+\n+    this.endpointUrl =\n+      fields.endpointUrl || getEnvironmentVariable(\"AZUREML_URL\") + \"\";",
        "comment_created_at": "2023-12-21T01:37:48+00:00",
        "comment_author": "bracesproul",
        "comment_body": "Prefer `??`. Please change for the rest. Also why is there `+ \"\"`?\r\n```suggestion\r\n      fields.endpointUrl ?? getEnvironmentVariable(\"AZUREML_URL\") + \"\";\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1933082744",
    "pr_number": 7616,
    "pr_file": "libs/langchain-openai/src/chat_models.ts",
    "created_at": "2025-01-29T01:27:02+00:00",
    "commented_code": "this.reasoningEffort = fields?.reasoningEffort;\n\n    if (this.model === \"o1\") {\n      this.disableStreaming = true;\n      this.disableStreaming =\n        fields?.disableStreaming != null ? fields?.disableStreaming : true;",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1933082744",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7616,
        "pr_file": "libs/langchain-openai/src/chat_models.ts",
        "discussion_id": "1933082744",
        "commented_code": "@@ -1002,7 +1002,8 @@ export class ChatOpenAI<\n     this.reasoningEffort = fields?.reasoningEffort;\n \n     if (this.model === \"o1\") {\n-      this.disableStreaming = true;\n+      this.disableStreaming =\n+        fields?.disableStreaming != null ? fields?.disableStreaming : true;",
        "comment_created_at": "2025-01-29T01:27:02+00:00",
        "comment_author": "bracesproul",
        "comment_body": "this is the one area which is technically breaking",
        "pr_file_module": null
      },
      {
        "comment_id": "1933085046",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7616,
        "pr_file": "libs/langchain-openai/src/chat_models.ts",
        "discussion_id": "1933082744",
        "commented_code": "@@ -1002,7 +1002,8 @@ export class ChatOpenAI<\n     this.reasoningEffort = fields?.reasoningEffort;\n \n     if (this.model === \"o1\") {\n-      this.disableStreaming = true;\n+      this.disableStreaming =\n+        fields?.disableStreaming != null ? fields?.disableStreaming : true;",
        "comment_created_at": "2025-01-29T01:29:26+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "Only if someone's passing `disableStreaming: false` right? I think fine",
        "pr_file_module": null
      },
      {
        "comment_id": "1933085495",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7616,
        "pr_file": "libs/langchain-openai/src/chat_models.ts",
        "discussion_id": "1933082744",
        "commented_code": "@@ -1002,7 +1002,8 @@ export class ChatOpenAI<\n     this.reasoningEffort = fields?.reasoningEffort;\n \n     if (this.model === \"o1\") {\n-      this.disableStreaming = true;\n+      this.disableStreaming =\n+        fields?.disableStreaming != null ? fields?.disableStreaming : true;",
        "comment_created_at": "2025-01-29T01:29:45+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "You can just do `fields?.disableStreaming ?? true`",
        "pr_file_module": null
      },
      {
        "comment_id": "1933086166",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7616,
        "pr_file": "libs/langchain-openai/src/chat_models.ts",
        "discussion_id": "1933082744",
        "commented_code": "@@ -1002,7 +1002,8 @@ export class ChatOpenAI<\n     this.reasoningEffort = fields?.reasoningEffort;\n \n     if (this.model === \"o1\") {\n-      this.disableStreaming = true;\n+      this.disableStreaming =\n+        fields?.disableStreaming != null ? fields?.disableStreaming : true;",
        "comment_created_at": "2025-01-29T01:30:43+00:00",
        "comment_author": "bracesproul",
        "comment_body": "ya `??` would work",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1456374070",
    "pr_number": 4044,
    "pr_file": "libs/langchain-community/src/vectorstores/azure_aisearch.ts",
    "created_at": "2024-01-17T19:33:07+00:00",
    "commented_code": "import * as uuid from \"uuid\";\nimport {\n  SearchClient,\n  SearchIndexClient,\n  AzureKeyCredential,\n  IndexingResult,\n  SearchIndex,\n} from \"@azure/search-documents\";\nimport {\n  MaxMarginalRelevanceSearchOptions,\n  VectorStore,\n} from \"@langchain/core/vectorstores\";\nimport type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\nimport { Document } from \"@langchain/core/documents\";\nimport { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n\n/**\n * Azure AI Search query type.\n */\nexport const AzureAISearchQueryType = {\n  /** Vector search. */\n  Similarity: \"similarity\",\n  /** Hybrid full text and vector search. */\n  SimilarityHybrid: \"similarity_hybrid\",\n  /** Hybrid full text and vector search with semantic ranking. */\n  SemanticHybrid: \"semantic_hybrid\",\n} as const;\n\n/**\n * Azure AI Search query type.\n */\nexport type AzureAISearchQueryType =\n  (typeof AzureAISearchQueryType)[keyof typeof AzureAISearchQueryType];\n\n/**\n * Azure AI Search settings.\n */\nexport interface AzureAISearchQueryOptions {\n  readonly type: AzureAISearchQueryType;\n  readonly semanticConfigurationName?: string;\n}\n\n/**\n * Configuration options for the `AzureAISearchStore` constructor.\n */\nexport interface AzureAISearchConfig {\n  readonly client?: SearchClient<AzureAISearchDocument>;\n  readonly indexName?: string;\n  readonly endpoint?: string;\n  readonly key?: string;\n  readonly search: AzureAISearchQueryOptions;\n  /**\n   * The amount of documents to chunk by when adding vectors.\n   * @default 100\n   */\n  readonly chunkSize?: number;\n  /**\n   * The amount of documents to embed at once when adding documents.\n   * Note that some providers like Azure OpenAI can only embed 16 documents\n   * at a time.\n   * @default 16\n   */\n  readonly embeddingBatchSize?: number;\n}\n\n/**\n * Azure AI Search options metadata schema.\n * If yout want to add custom data, use the attributes property.\n */\nexport type AzureAISearchDocumentMetadata = {\n  source: string;\n  attributes?: Array<{ key: string; value: string }>;\n};\n\n/**\n * Azure AI Search indexed document.\n */\nexport type AzureAISearchDocument = {\n  id: string;\n  content: string;\n  content_vector: number[];\n  metadata: AzureAISearchDocumentMetadata;\n};\n\n/**\n * Azure AI Search options for adding documents.\n */\nexport type AzureAISearchAddDocumentsOptions = {\n  ids?: string[];\n};\n\nconst DEFAULT_FIELD_ID = \"id\";\nconst DEFAULT_FIELD_CONTENT = \"content\";\nconst DEFAULT_FIELD_CONTENT_VECTOR = \"content_vector\";\nconst DEFAULT_FIELD_METADATA = \"metadata\";\nconst DEFAULT_FIELD_METADATA_SOURCE = \"source\";\nconst DEFAULT_FIELD_METADATA_ATTRS = \"attributes\";\n\n/**\n * Azure AI Search vector store.\n * To use this, you should have:\n * - the `@azure/search-documents` NPM package installed\n * - an endpoint and key to the Azure AI Search instance\n *\n * If you directly provide a `SearchClient` instance, you need to ensure that\n * an index has been created. When using and endpoint and key, the index will\n * be created automatically if it does not exist.\n */\nexport class AzureAISearchVectorStore extends VectorStore {\n  declare FilterType: string;\n\n  get lc_secrets(): { [key: string]: string } {\n    return {\n      endpoint: \"AZURE_AISEARCH_ENDPOINT\",\n      key: \"AZURE_AISEARCH_KEY\",\n    };\n  }\n\n  _vectorstoreType(): string {\n    return \"azure_aisearch\";\n  }\n\n  private readonly initPromise: Promise<void>;\n\n  private readonly client: SearchClient<AzureAISearchDocument>;\n\n  private readonly indexName: string;\n\n  private readonly chunkSize: number;\n\n  private readonly embeddingBatchSize: number;\n\n  private readonly options: AzureAISearchQueryOptions;\n\n  constructor(embeddings: EmbeddingsInterface, config: AzureAISearchConfig) {\n    super(embeddings, config);\n\n    const endpoint =\n      config.endpoint ?? getEnvironmentVariable(\"AZURE_AISEARCH_ENDPOINT\");\n    const key = config.key ?? getEnvironmentVariable(\"AZURE_AISEARCH_KEY\");\n\n    if (!config.client && !endpoint && !key) {",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1456374070",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 4044,
        "pr_file": "libs/langchain-community/src/vectorstores/azure_aisearch.ts",
        "discussion_id": "1456374070",
        "commented_code": "@@ -0,0 +1,694 @@\n+import * as uuid from \"uuid\";\n+import {\n+  SearchClient,\n+  SearchIndexClient,\n+  AzureKeyCredential,\n+  IndexingResult,\n+  SearchIndex,\n+} from \"@azure/search-documents\";\n+import {\n+  MaxMarginalRelevanceSearchOptions,\n+  VectorStore,\n+} from \"@langchain/core/vectorstores\";\n+import type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { Document } from \"@langchain/core/documents\";\n+import { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+\n+/**\n+ * Azure AI Search query type.\n+ */\n+export const AzureAISearchQueryType = {\n+  /** Vector search. */\n+  Similarity: \"similarity\",\n+  /** Hybrid full text and vector search. */\n+  SimilarityHybrid: \"similarity_hybrid\",\n+  /** Hybrid full text and vector search with semantic ranking. */\n+  SemanticHybrid: \"semantic_hybrid\",\n+} as const;\n+\n+/**\n+ * Azure AI Search query type.\n+ */\n+export type AzureAISearchQueryType =\n+  (typeof AzureAISearchQueryType)[keyof typeof AzureAISearchQueryType];\n+\n+/**\n+ * Azure AI Search settings.\n+ */\n+export interface AzureAISearchQueryOptions {\n+  readonly type: AzureAISearchQueryType;\n+  readonly semanticConfigurationName?: string;\n+}\n+\n+/**\n+ * Configuration options for the `AzureAISearchStore` constructor.\n+ */\n+export interface AzureAISearchConfig {\n+  readonly client?: SearchClient<AzureAISearchDocument>;\n+  readonly indexName?: string;\n+  readonly endpoint?: string;\n+  readonly key?: string;\n+  readonly search: AzureAISearchQueryOptions;\n+  /**\n+   * The amount of documents to chunk by when adding vectors.\n+   * @default 100\n+   */\n+  readonly chunkSize?: number;\n+  /**\n+   * The amount of documents to embed at once when adding documents.\n+   * Note that some providers like Azure OpenAI can only embed 16 documents\n+   * at a time.\n+   * @default 16\n+   */\n+  readonly embeddingBatchSize?: number;\n+}\n+\n+/**\n+ * Azure AI Search options metadata schema.\n+ * If yout want to add custom data, use the attributes property.\n+ */\n+export type AzureAISearchDocumentMetadata = {\n+  source: string;\n+  attributes?: Array<{ key: string; value: string }>;\n+};\n+\n+/**\n+ * Azure AI Search indexed document.\n+ */\n+export type AzureAISearchDocument = {\n+  id: string;\n+  content: string;\n+  content_vector: number[];\n+  metadata: AzureAISearchDocumentMetadata;\n+};\n+\n+/**\n+ * Azure AI Search options for adding documents.\n+ */\n+export type AzureAISearchAddDocumentsOptions = {\n+  ids?: string[];\n+};\n+\n+const DEFAULT_FIELD_ID = \"id\";\n+const DEFAULT_FIELD_CONTENT = \"content\";\n+const DEFAULT_FIELD_CONTENT_VECTOR = \"content_vector\";\n+const DEFAULT_FIELD_METADATA = \"metadata\";\n+const DEFAULT_FIELD_METADATA_SOURCE = \"source\";\n+const DEFAULT_FIELD_METADATA_ATTRS = \"attributes\";\n+\n+/**\n+ * Azure AI Search vector store.\n+ * To use this, you should have:\n+ * - the `@azure/search-documents` NPM package installed\n+ * - an endpoint and key to the Azure AI Search instance\n+ *\n+ * If you directly provide a `SearchClient` instance, you need to ensure that\n+ * an index has been created. When using and endpoint and key, the index will\n+ * be created automatically if it does not exist.\n+ */\n+export class AzureAISearchVectorStore extends VectorStore {\n+  declare FilterType: string;\n+\n+  get lc_secrets(): { [key: string]: string } {\n+    return {\n+      endpoint: \"AZURE_AISEARCH_ENDPOINT\",\n+      key: \"AZURE_AISEARCH_KEY\",\n+    };\n+  }\n+\n+  _vectorstoreType(): string {\n+    return \"azure_aisearch\";\n+  }\n+\n+  private readonly initPromise: Promise<void>;\n+\n+  private readonly client: SearchClient<AzureAISearchDocument>;\n+\n+  private readonly indexName: string;\n+\n+  private readonly chunkSize: number;\n+\n+  private readonly embeddingBatchSize: number;\n+\n+  private readonly options: AzureAISearchQueryOptions;\n+\n+  constructor(embeddings: EmbeddingsInterface, config: AzureAISearchConfig) {\n+    super(embeddings, config);\n+\n+    const endpoint =\n+      config.endpoint ?? getEnvironmentVariable(\"AZURE_AISEARCH_ENDPOINT\");\n+    const key = config.key ?? getEnvironmentVariable(\"AZURE_AISEARCH_KEY\");\n+\n+    if (!config.client && !endpoint && !key) {",
        "comment_created_at": "2024-01-17T19:33:07+00:00",
        "comment_author": "pablocastro",
        "comment_body": "Wouldn't you want `(!config.client && (!endpoint || !key))` so we also raise an error if either the endpoint or the key are missing when not providing a pre-created client?",
        "pr_file_module": null
      },
      {
        "comment_id": "1457265765",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 4044,
        "pr_file": "libs/langchain-community/src/vectorstores/azure_aisearch.ts",
        "discussion_id": "1456374070",
        "commented_code": "@@ -0,0 +1,694 @@\n+import * as uuid from \"uuid\";\n+import {\n+  SearchClient,\n+  SearchIndexClient,\n+  AzureKeyCredential,\n+  IndexingResult,\n+  SearchIndex,\n+} from \"@azure/search-documents\";\n+import {\n+  MaxMarginalRelevanceSearchOptions,\n+  VectorStore,\n+} from \"@langchain/core/vectorstores\";\n+import type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { Document } from \"@langchain/core/documents\";\n+import { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+\n+/**\n+ * Azure AI Search query type.\n+ */\n+export const AzureAISearchQueryType = {\n+  /** Vector search. */\n+  Similarity: \"similarity\",\n+  /** Hybrid full text and vector search. */\n+  SimilarityHybrid: \"similarity_hybrid\",\n+  /** Hybrid full text and vector search with semantic ranking. */\n+  SemanticHybrid: \"semantic_hybrid\",\n+} as const;\n+\n+/**\n+ * Azure AI Search query type.\n+ */\n+export type AzureAISearchQueryType =\n+  (typeof AzureAISearchQueryType)[keyof typeof AzureAISearchQueryType];\n+\n+/**\n+ * Azure AI Search settings.\n+ */\n+export interface AzureAISearchQueryOptions {\n+  readonly type: AzureAISearchQueryType;\n+  readonly semanticConfigurationName?: string;\n+}\n+\n+/**\n+ * Configuration options for the `AzureAISearchStore` constructor.\n+ */\n+export interface AzureAISearchConfig {\n+  readonly client?: SearchClient<AzureAISearchDocument>;\n+  readonly indexName?: string;\n+  readonly endpoint?: string;\n+  readonly key?: string;\n+  readonly search: AzureAISearchQueryOptions;\n+  /**\n+   * The amount of documents to chunk by when adding vectors.\n+   * @default 100\n+   */\n+  readonly chunkSize?: number;\n+  /**\n+   * The amount of documents to embed at once when adding documents.\n+   * Note that some providers like Azure OpenAI can only embed 16 documents\n+   * at a time.\n+   * @default 16\n+   */\n+  readonly embeddingBatchSize?: number;\n+}\n+\n+/**\n+ * Azure AI Search options metadata schema.\n+ * If yout want to add custom data, use the attributes property.\n+ */\n+export type AzureAISearchDocumentMetadata = {\n+  source: string;\n+  attributes?: Array<{ key: string; value: string }>;\n+};\n+\n+/**\n+ * Azure AI Search indexed document.\n+ */\n+export type AzureAISearchDocument = {\n+  id: string;\n+  content: string;\n+  content_vector: number[];\n+  metadata: AzureAISearchDocumentMetadata;\n+};\n+\n+/**\n+ * Azure AI Search options for adding documents.\n+ */\n+export type AzureAISearchAddDocumentsOptions = {\n+  ids?: string[];\n+};\n+\n+const DEFAULT_FIELD_ID = \"id\";\n+const DEFAULT_FIELD_CONTENT = \"content\";\n+const DEFAULT_FIELD_CONTENT_VECTOR = \"content_vector\";\n+const DEFAULT_FIELD_METADATA = \"metadata\";\n+const DEFAULT_FIELD_METADATA_SOURCE = \"source\";\n+const DEFAULT_FIELD_METADATA_ATTRS = \"attributes\";\n+\n+/**\n+ * Azure AI Search vector store.\n+ * To use this, you should have:\n+ * - the `@azure/search-documents` NPM package installed\n+ * - an endpoint and key to the Azure AI Search instance\n+ *\n+ * If you directly provide a `SearchClient` instance, you need to ensure that\n+ * an index has been created. When using and endpoint and key, the index will\n+ * be created automatically if it does not exist.\n+ */\n+export class AzureAISearchVectorStore extends VectorStore {\n+  declare FilterType: string;\n+\n+  get lc_secrets(): { [key: string]: string } {\n+    return {\n+      endpoint: \"AZURE_AISEARCH_ENDPOINT\",\n+      key: \"AZURE_AISEARCH_KEY\",\n+    };\n+  }\n+\n+  _vectorstoreType(): string {\n+    return \"azure_aisearch\";\n+  }\n+\n+  private readonly initPromise: Promise<void>;\n+\n+  private readonly client: SearchClient<AzureAISearchDocument>;\n+\n+  private readonly indexName: string;\n+\n+  private readonly chunkSize: number;\n+\n+  private readonly embeddingBatchSize: number;\n+\n+  private readonly options: AzureAISearchQueryOptions;\n+\n+  constructor(embeddings: EmbeddingsInterface, config: AzureAISearchConfig) {\n+    super(embeddings, config);\n+\n+    const endpoint =\n+      config.endpoint ?? getEnvironmentVariable(\"AZURE_AISEARCH_ENDPOINT\");\n+    const key = config.key ?? getEnvironmentVariable(\"AZURE_AISEARCH_KEY\");\n+\n+    if (!config.client && !endpoint && !key) {",
        "comment_created_at": "2024-01-18T10:49:33+00:00",
        "comment_author": "sinedied",
        "comment_body": "Thanks, fixed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1887551586",
    "pr_number": 7372,
    "pr_file": "langchain/src/chains/combine_documents/base.ts",
    "created_at": "2024-12-16T20:32:22+00:00",
    "commented_code": "documents: Document[];\n  config?: RunnableConfig;\n}) {\n  if (documents == null || documents.length == 0) {",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1887551586",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7372,
        "pr_file": "langchain/src/chains/combine_documents/base.ts",
        "discussion_id": "1887551586",
        "commented_code": "@@ -21,6 +21,9 @@ export async function formatDocuments({\n   documents: Document[];\n   config?: RunnableConfig;\n }) {\n+  if (documents == null || documents.length == 0) {",
        "comment_created_at": "2024-12-16T20:32:22+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "Don't think we need the `.length` check\r\n\r\nAlso should use triple equals if at all",
        "pr_file_module": null
      },
      {
        "comment_id": "1887691735",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7372,
        "pr_file": "langchain/src/chains/combine_documents/base.ts",
        "discussion_id": "1887551586",
        "commented_code": "@@ -21,6 +21,9 @@ export async function formatDocuments({\n   documents: Document[];\n   config?: RunnableConfig;\n }) {\n+  if (documents == null || documents.length == 0) {",
        "comment_created_at": "2024-12-16T23:09:03+00:00",
        "comment_author": "ahm750",
        "comment_body": "Triple equals added",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1805321749",
    "pr_number": 7012,
    "pr_file": "libs/langchain-openai/src/chat_models.ts",
    "created_at": "2024-10-17T19:38:23+00:00",
    "commented_code": "seed: options?.seed,\n      ...streamOptionsConfig,\n      parallel_tool_calls: options?.parallel_tool_calls,\n      audio: this.audio,",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1805321749",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7012,
        "pr_file": "libs/langchain-openai/src/chat_models.ts",
        "discussion_id": "1805321749",
        "commented_code": "@@ -1190,6 +1207,8 @@ export class ChatOpenAI<\n       seed: options?.seed,\n       ...streamOptionsConfig,\n       parallel_tool_calls: options?.parallel_tool_calls,\n+      audio: this.audio,",
        "comment_created_at": "2024-10-17T19:38:23+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "May be slightly safer to add these fields only if they are present - not sure how Azure or other model proxies will react to adding `audio: undefined`\r\n\r\n",
        "pr_file_module": null
      }
    ]
  }
]