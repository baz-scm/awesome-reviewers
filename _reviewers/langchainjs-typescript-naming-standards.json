[
  {
    "discussion_id": "2106342464",
    "pr_number": 8248,
    "pr_file": "libs/langchain-anthropic/src/chat_models.ts",
    "created_at": "2025-05-25T23:52:50+00:00",
    "commented_code": "return \"input_schema\" in tool;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nfunction isBuiltinTool(\n  tool: any",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "2106342464",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8248,
        "pr_file": "libs/langchain-anthropic/src/chat_models.ts",
        "discussion_id": "2106342464",
        "commented_code": "@@ -100,6 +100,26 @@ function isAnthropicTool(tool: any): tool is Anthropic.Messages.Tool {\n   return \"input_schema\" in tool;\n }\n \n+// eslint-disable-next-line @typescript-eslint/no-explicit-any\n+function isBuiltinTool(\n+  tool: any",
        "comment_created_at": "2025-05-25T23:52:50+00:00",
        "comment_author": "benjamincburns",
        "comment_body": "nit: use `unknown` - that's why we have the ESLint error",
        "pr_file_module": null
      },
      {
        "comment_id": "2106392558",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8248,
        "pr_file": "libs/langchain-anthropic/src/chat_models.ts",
        "discussion_id": "2106342464",
        "commented_code": "@@ -100,6 +100,26 @@ function isAnthropicTool(tool: any): tool is Anthropic.Messages.Tool {\n   return \"input_schema\" in tool;\n }\n \n+// eslint-disable-next-line @typescript-eslint/no-explicit-any\n+function isBuiltinTool(\n+  tool: any",
        "comment_created_at": "2025-05-26T01:49:53+00:00",
        "comment_author": "bleafman",
        "comment_body": "Done!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1899705386",
    "pr_number": 7300,
    "pr_file": "libs/langchain-community/src/document_loaders/web/reddit.ts",
    "created_at": "2024-12-30T17:55:31+00:00",
    "commented_code": "import { BaseDocumentLoader } from \"@langchain/core/document_loaders/base\";\nimport { Document } from \"@langchain/core/documents\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport {\n  RedditAPIWrapper,\n  RedditPost,\n  RedditAPIConfig,\n} from \"../../utils/reddit.js\";\n\n/**\n * Class representing a document loader for loading Reddit posts. It extends\n * the BaseDocumentLoader and implements the RedditAPIConfig interface.\n * @example\n * ```typescript\n * const loader = new RedditPostsLoader({\n *   clientId: \"REDDIT_CLIENT_ID\",\n *   clientSecret: \"REDDIT_CLIENT_SECRET\",\n *   userAgent: \"REDDIT_USER_AGENT\",\n *   searchQueries: [\"LangChain\", \"Langchaindev\"],\n *   mode: \"subreddit\",\n *   categories: [\"hot\", \"new\"],\n *   numberPosts: 5\n * });\n * const docs = await loader.load();\n * ```\n */\nexport class RedditPostsLoader\n  extends BaseDocumentLoader\n  implements RedditAPIConfig\n{\n  public clientId: string;\n\n  public clientSecret: string;\n\n  public userAgent: string;\n\n  private redditApiWrapper: RedditAPIWrapper;\n\n  private searchQueries: string[];\n\n  private mode: string;\n\n  private categories: string[];\n\n  private numberPosts: number;\n\n  constructor({\n    clientId = getEnvironmentVariable(\"REDDIT_CLIENT_ID\") as string,\n    clientSecret = getEnvironmentVariable(\"REDDIT_CLIENT_SECRET\") as string,\n    userAgent = getEnvironmentVariable(\"REDDIT_USER_AGENT\") as string,\n    searchQueries,\n    mode,\n    categories = [\"new\"],\n    numberPosts = 10,\n  }: RedditAPIConfig & {\n    searchQueries: string[];\n    mode: string;\n    categories?: string[];\n    numberPosts?: number;\n  }) {\n    super();\n    this.clientId = clientId;\n    this.clientSecret = clientSecret;\n    this.userAgent = userAgent;\n    this.redditApiWrapper = new RedditAPIWrapper({\n      clientId: this.clientId,\n      clientSecret: this.clientSecret,\n      userAgent: this.userAgent,\n    });\n    this.searchQueries = searchQueries;\n    this.mode = mode;",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1899705386",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7300,
        "pr_file": "libs/langchain-community/src/document_loaders/web/reddit.ts",
        "discussion_id": "1899705386",
        "commented_code": "@@ -0,0 +1,133 @@\n+import { BaseDocumentLoader } from \"@langchain/core/document_loaders/base\";\n+import { Document } from \"@langchain/core/documents\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import {\n+  RedditAPIWrapper,\n+  RedditPost,\n+  RedditAPIConfig,\n+} from \"../../utils/reddit.js\";\n+\n+/**\n+ * Class representing a document loader for loading Reddit posts. It extends\n+ * the BaseDocumentLoader and implements the RedditAPIConfig interface.\n+ * @example\n+ * ```typescript\n+ * const loader = new RedditPostsLoader({\n+ *   clientId: \"REDDIT_CLIENT_ID\",\n+ *   clientSecret: \"REDDIT_CLIENT_SECRET\",\n+ *   userAgent: \"REDDIT_USER_AGENT\",\n+ *   searchQueries: [\"LangChain\", \"Langchaindev\"],\n+ *   mode: \"subreddit\",\n+ *   categories: [\"hot\", \"new\"],\n+ *   numberPosts: 5\n+ * });\n+ * const docs = await loader.load();\n+ * ```\n+ */\n+export class RedditPostsLoader\n+  extends BaseDocumentLoader\n+  implements RedditAPIConfig\n+{\n+  public clientId: string;\n+\n+  public clientSecret: string;\n+\n+  public userAgent: string;\n+\n+  private redditApiWrapper: RedditAPIWrapper;\n+\n+  private searchQueries: string[];\n+\n+  private mode: string;\n+\n+  private categories: string[];\n+\n+  private numberPosts: number;\n+\n+  constructor({\n+    clientId = getEnvironmentVariable(\"REDDIT_CLIENT_ID\") as string,\n+    clientSecret = getEnvironmentVariable(\"REDDIT_CLIENT_SECRET\") as string,\n+    userAgent = getEnvironmentVariable(\"REDDIT_USER_AGENT\") as string,\n+    searchQueries,\n+    mode,\n+    categories = [\"new\"],\n+    numberPosts = 10,\n+  }: RedditAPIConfig & {\n+    searchQueries: string[];\n+    mode: string;\n+    categories?: string[];\n+    numberPosts?: number;\n+  }) {\n+    super();\n+    this.clientId = clientId;\n+    this.clientSecret = clientSecret;\n+    this.userAgent = userAgent;\n+    this.redditApiWrapper = new RedditAPIWrapper({\n+      clientId: this.clientId,\n+      clientSecret: this.clientSecret,\n+      userAgent: this.userAgent,\n+    });\n+    this.searchQueries = searchQueries;\n+    this.mode = mode;",
        "comment_created_at": "2024-12-30T17:55:31+00:00",
        "comment_author": "nick-w-nick",
        "comment_body": "Would be a lot nicer if the \"mode\" parameter was typed instead since different values cause errors to be thrown, like so:\r\n\r\n```suggestion\r\n\r\nexport type SearchMode = \"subreddit\" | \"username\";\r\n\r\n/**\r\n * Class representing a document loader for loading Reddit posts. It extends\r\n * the BaseDocumentLoader and implements the RedditAPIConfig interface.\r\n * @example\r\n * ```typescript\r\n * const loader = new RedditPostsLoader({\r\n *   clientId: \"REDDIT_CLIENT_ID\",\r\n *   clientSecret: \"REDDIT_CLIENT_SECRET\",\r\n *   userAgent: \"REDDIT_USER_AGENT\",\r\n *   searchQueries: [\"LangChain\", \"Langchaindev\"],\r\n *   mode: \"subreddit\",\r\n *   categories: [\"hot\", \"new\"],\r\n *   numberPosts: 5\r\n * });\r\n * const docs = await loader.load();\r\n * ```\r\n */\r\nexport class RedditPostsLoader\r\n  extends BaseDocumentLoader\r\n  implements RedditAPIConfig\r\n{\r\n  public clientId: string;\r\n\r\n  public clientSecret: string;\r\n\r\n  public userAgent: string;\r\n\r\n  private redditApiWrapper: RedditAPIWrapper;\r\n\r\n  private searchQueries: string[];\r\n\r\n  private mode: SearchMode;\r\n\r\n  private categories: string[];\r\n\r\n  private numberPosts: number;\r\n\r\n  constructor({\r\n    clientId = getEnvironmentVariable(\"REDDIT_CLIENT_ID\") as string,\r\n    clientSecret = getEnvironmentVariable(\"REDDIT_CLIENT_SECRET\") as string,\r\n    userAgent = getEnvironmentVariable(\"REDDIT_USER_AGENT\") as string,\r\n    searchQueries,\r\n    mode,\r\n    categories = [\"new\"],\r\n    numberPosts = 10,\r\n  }: RedditAPIConfig & {\r\n    searchQueries: string[];\r\n    mode: SearchMode;\r\n    categories?: string[];\r\n    numberPosts?: number;\r\n  }) {\r\n    super();\r\n    this.clientId = clientId;\r\n    this.clientSecret = clientSecret;\r\n    this.userAgent = userAgent;\r\n    this.redditApiWrapper = new RedditAPIWrapper({\r\n      clientId: this.clientId,\r\n      clientSecret: this.clientSecret,\r\n      userAgent: this.userAgent,\r\n    });\r\n    this.searchQueries = searchQueries;\r\n    this.mode = mode;\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1992603122",
    "pr_number": 7817,
    "pr_file": "libs/langchain-community/src/chat_models/perplexity.ts",
    "created_at": "2025-03-13T02:58:54+00:00",
    "commented_code": "import {\n  AIMessage,\n  AIMessageChunk,\n  BaseMessage,\n  BaseMessageChunk,\n  ChatMessage,\n  ChatMessageChunk,\n  HumanMessage,\n  HumanMessageChunk,\n  SystemMessage,\n  SystemMessageChunk,\n} from \"@langchain/core/messages\";\nimport {\n  BaseChatModel,\n  BaseChatModelParams,\n} from \"@langchain/core/language_models/chat_models\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  ChatGeneration,\n  ChatGenerationChunk,\n  ChatResult,\n} from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport OpenAI from \"openai\";\n\n/**\n * Type representing the role of a message in the Perplexity chat model.\n */\nexport type PerplexityRole = \"system\" | \"user\" | \"assistant\";\n\n/**\n * Interface defining the parameters for the Perplexity chat model.\n */\nexport interface PerplexityChatInput {\n  /** Model name to use */\n  modelName?: string;\n\n  /** Maximum number of tokens to generate */\n  maxTokens?: number;\n\n  /** Temperature parameter between 0 and 2 */\n  temperature?: number;\n\n  /** Top P parameter between 0 and 1 */\n  topP?: number;\n\n  /** Search domain filter - limit the citations used by the online model to URLs from the specified domains. */\n  searchDomainFilter?: any[];\n\n  /** Whether to return images */\n  returnImages?: boolean;\n\n  /** Determines whether or not a request to an online model should return related questions. */\n  returnRelatedQuestions?: boolean;\n\n  /** Returns search results within the specified time interval - does not apply to images. Values include month, week, day, hour. */\n  searchRecencyFilter?: string;\n\n  /** Top K parameter between 1 and 2048 */\n  topK?: number;\n\n  /** Presence penalty between -2 and 2 */\n  presencePenalty?: number;\n\n  /** Frequency penalty greater than 0 */\n  frequencyPenalty?: number;\n\n  /** API key for Perplexity.  Defaults to the value of\n   * PERPLEXITY_API_KEY environment variable.\n   */\n  apiKey?: string;\n\n  /** Whether to stream the results or not */\n  streaming?: boolean;\n\n  /** Timeout for requests to Perplexity */\n  timeout?: number;\n}\n\n/**\n * Wrapper around Perplexity large language models that use the Chat endpoint.\n */\nexport class ChatPerplexity\n  extends BaseChatModel\n  implements PerplexityChatInput\n{\n  static lc_name() {\n    return \"ChatPerplexity\";\n  }\n\n  modelName = \"llama-3.1-sonar-small-128k-online\";",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1992603122",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7817,
        "pr_file": "libs/langchain-community/src/chat_models/perplexity.ts",
        "discussion_id": "1992603122",
        "commented_code": "@@ -0,0 +1,290 @@\n+import {\n+  AIMessage,\n+  AIMessageChunk,\n+  BaseMessage,\n+  BaseMessageChunk,\n+  ChatMessage,\n+  ChatMessageChunk,\n+  HumanMessage,\n+  HumanMessageChunk,\n+  SystemMessage,\n+  SystemMessageChunk,\n+} from \"@langchain/core/messages\";\n+import {\n+  BaseChatModel,\n+  BaseChatModelParams,\n+} from \"@langchain/core/language_models/chat_models\";\n+import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n+import {\n+  ChatGeneration,\n+  ChatGenerationChunk,\n+  ChatResult,\n+} from \"@langchain/core/outputs\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import OpenAI from \"openai\";\n+\n+/**\n+ * Type representing the role of a message in the Perplexity chat model.\n+ */\n+export type PerplexityRole = \"system\" | \"user\" | \"assistant\";\n+\n+/**\n+ * Interface defining the parameters for the Perplexity chat model.\n+ */\n+export interface PerplexityChatInput {\n+  /** Model name to use */\n+  modelName?: string;\n+\n+  /** Maximum number of tokens to generate */\n+  maxTokens?: number;\n+\n+  /** Temperature parameter between 0 and 2 */\n+  temperature?: number;\n+\n+  /** Top P parameter between 0 and 1 */\n+  topP?: number;\n+\n+  /** Search domain filter - limit the citations used by the online model to URLs from the specified domains. */\n+  searchDomainFilter?: any[];\n+\n+  /** Whether to return images */\n+  returnImages?: boolean;\n+\n+  /** Determines whether or not a request to an online model should return related questions. */\n+  returnRelatedQuestions?: boolean;\n+\n+  /** Returns search results within the specified time interval - does not apply to images. Values include month, week, day, hour. */\n+  searchRecencyFilter?: string;\n+\n+  /** Top K parameter between 1 and 2048 */\n+  topK?: number;\n+\n+  /** Presence penalty between -2 and 2 */\n+  presencePenalty?: number;\n+\n+  /** Frequency penalty greater than 0 */\n+  frequencyPenalty?: number;\n+\n+  /** API key for Perplexity.  Defaults to the value of\n+   * PERPLEXITY_API_KEY environment variable.\n+   */\n+  apiKey?: string;\n+\n+  /** Whether to stream the results or not */\n+  streaming?: boolean;\n+\n+  /** Timeout for requests to Perplexity */\n+  timeout?: number;\n+}\n+\n+/**\n+ * Wrapper around Perplexity large language models that use the Chat endpoint.\n+ */\n+export class ChatPerplexity\n+  extends BaseChatModel\n+  implements PerplexityChatInput\n+{\n+  static lc_name() {\n+    return \"ChatPerplexity\";\n+  }\n+\n+  modelName = \"llama-3.1-sonar-small-128k-online\";",
        "comment_created_at": "2025-03-13T02:58:54+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "Let's not take a default here\r\n\r\nAlso, we are standardizing on `model` over `modelName`",
        "pr_file_module": null
      },
      {
        "comment_id": "1996233446",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7817,
        "pr_file": "libs/langchain-community/src/chat_models/perplexity.ts",
        "discussion_id": "1992603122",
        "commented_code": "@@ -0,0 +1,290 @@\n+import {\n+  AIMessage,\n+  AIMessageChunk,\n+  BaseMessage,\n+  BaseMessageChunk,\n+  ChatMessage,\n+  ChatMessageChunk,\n+  HumanMessage,\n+  HumanMessageChunk,\n+  SystemMessage,\n+  SystemMessageChunk,\n+} from \"@langchain/core/messages\";\n+import {\n+  BaseChatModel,\n+  BaseChatModelParams,\n+} from \"@langchain/core/language_models/chat_models\";\n+import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n+import {\n+  ChatGeneration,\n+  ChatGenerationChunk,\n+  ChatResult,\n+} from \"@langchain/core/outputs\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import OpenAI from \"openai\";\n+\n+/**\n+ * Type representing the role of a message in the Perplexity chat model.\n+ */\n+export type PerplexityRole = \"system\" | \"user\" | \"assistant\";\n+\n+/**\n+ * Interface defining the parameters for the Perplexity chat model.\n+ */\n+export interface PerplexityChatInput {\n+  /** Model name to use */\n+  modelName?: string;\n+\n+  /** Maximum number of tokens to generate */\n+  maxTokens?: number;\n+\n+  /** Temperature parameter between 0 and 2 */\n+  temperature?: number;\n+\n+  /** Top P parameter between 0 and 1 */\n+  topP?: number;\n+\n+  /** Search domain filter - limit the citations used by the online model to URLs from the specified domains. */\n+  searchDomainFilter?: any[];\n+\n+  /** Whether to return images */\n+  returnImages?: boolean;\n+\n+  /** Determines whether or not a request to an online model should return related questions. */\n+  returnRelatedQuestions?: boolean;\n+\n+  /** Returns search results within the specified time interval - does not apply to images. Values include month, week, day, hour. */\n+  searchRecencyFilter?: string;\n+\n+  /** Top K parameter between 1 and 2048 */\n+  topK?: number;\n+\n+  /** Presence penalty between -2 and 2 */\n+  presencePenalty?: number;\n+\n+  /** Frequency penalty greater than 0 */\n+  frequencyPenalty?: number;\n+\n+  /** API key for Perplexity.  Defaults to the value of\n+   * PERPLEXITY_API_KEY environment variable.\n+   */\n+  apiKey?: string;\n+\n+  /** Whether to stream the results or not */\n+  streaming?: boolean;\n+\n+  /** Timeout for requests to Perplexity */\n+  timeout?: number;\n+}\n+\n+/**\n+ * Wrapper around Perplexity large language models that use the Chat endpoint.\n+ */\n+export class ChatPerplexity\n+  extends BaseChatModel\n+  implements PerplexityChatInput\n+{\n+  static lc_name() {\n+    return \"ChatPerplexity\";\n+  }\n+\n+  modelName = \"llama-3.1-sonar-small-128k-online\";",
        "comment_created_at": "2025-03-14T20:25:36+00:00",
        "comment_author": "anadi45",
        "comment_body": "changed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1434581965",
    "pr_number": 3577,
    "pr_file": "libs/langchain-community/src/chat_models/watsonx_ai.ts",
    "created_at": "2023-12-21T23:11:45+00:00",
    "commented_code": "import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  type BaseChatModelParams,\n  SimpleChatModel,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  AIMessageChunk,\n  BaseMessage,\n  ChatMessage,\n} from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport type {\n  WatsonModelParameters,\n  WatsonxAIParams,\n} from \"../types/watsonx-types.js\";\nimport { WatsonApiClient } from \"../utils/watsonx-client.js\";\n\nexport class WatsonxAIChat extends SimpleChatModel {\n  private readonly watsonApiClient: WatsonApiClient;\n\n  readonly modelId!: string;\n\n  readonly modelParameters?: WatsonModelParameters;\n\n  readonly projectId!: string;\n\n  constructor(fields: WatsonxAIParams & BaseChatModelParams) {\n    super(fields);\n\n    const {\n      clientConfig = {},\n      modelId = \"meta-llama/llama-2-70b-chat\",\n      modelParameters,\n      projectId = getEnvironmentVariable(\"WATSONX_PROJECT_ID\") ?? \"\",\n    } = fields;\n\n    this.modelId = modelId;\n    this.modelParameters = modelParameters;\n    this.projectId = projectId;\n\n    const {\n      apiKey = getEnvironmentVariable(\"IBM_CLOUD_API_KEY\"),\n      apiVersion = \"2023-05-29\",\n      region = \"us-south\",\n    } = clientConfig;\n\n    if (!apiKey) {\n      throw new Error(\"Missing IBM Cloud API Key\");\n    }\n\n    if (!this.projectId) {\n      throw new Error(\"Missing WatsonX AI Project ID\");\n    }\n\n    this.watsonApiClient = new WatsonApiClient({\n      apiKey,\n      apiVersion,\n      region,\n    });\n  }\n\n  protected _formatMessagesAsPrompt(messages: BaseMessage[]): string {\n    return messages\n      .map((message) => {\n        let messageText;\n        if (message._getType() === \"human\") {\n          messageText = `[INST] ${message.content} [/INST]`;\n        } else if (message._getType() === \"ai\") {\n          messageText = message.content;\n        } else if (message._getType() === \"system\") {\n          messageText = `<<SYS>> ${message.content} <</SYS>>`;\n        } else if (ChatMessage.isInstance(message)) {\n          messageText = `\\n\\n${message.role[0].toUpperCase()}${message.role.slice(\n            1\n          )}: ${message.content}`;\n        } else {\n          console.warn(\n            `Unsupported message type passed to Watson: \"${message._getType()}\"`\n          );\n          messageText = \"\";\n        }\n        return messageText;\n      })\n      .join(\"\\n\");\n  }\n\n  _combineLLMOutput() {\n    return {};\n  }\n\n  async _call(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager: CallbackManagerForLLMRun | undefined\n  ): Promise<string> {\n    const chunks = [];\n    const stream = this._streamResponseChunks(messages, options, runManager);\n    for await (const chunk of stream) {\n      chunks.push(chunk.message.content);\n    }\n    return chunks.join(\"\");\n  }\n\n  override async *_streamResponseChunks(\n    _messages: BaseMessage[],\n    _options: this[\"ParsedCallOptions\"],\n    _runManager?: CallbackManagerForLLMRun",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1434581965",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 3577,
        "pr_file": "libs/langchain-community/src/chat_models/watsonx_ai.ts",
        "discussion_id": "1434581965",
        "commented_code": "@@ -0,0 +1,157 @@\n+import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n+import {\n+  type BaseChatModelParams,\n+  SimpleChatModel,\n+} from \"@langchain/core/language_models/chat_models\";\n+import {\n+  AIMessageChunk,\n+  BaseMessage,\n+  ChatMessage,\n+} from \"@langchain/core/messages\";\n+import { ChatGenerationChunk } from \"@langchain/core/outputs\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import type {\n+  WatsonModelParameters,\n+  WatsonxAIParams,\n+} from \"../types/watsonx-types.js\";\n+import { WatsonApiClient } from \"../utils/watsonx-client.js\";\n+\n+export class WatsonxAIChat extends SimpleChatModel {\n+  private readonly watsonApiClient: WatsonApiClient;\n+\n+  readonly modelId!: string;\n+\n+  readonly modelParameters?: WatsonModelParameters;\n+\n+  readonly projectId!: string;\n+\n+  constructor(fields: WatsonxAIParams & BaseChatModelParams) {\n+    super(fields);\n+\n+    const {\n+      clientConfig = {},\n+      modelId = \"meta-llama/llama-2-70b-chat\",\n+      modelParameters,\n+      projectId = getEnvironmentVariable(\"WATSONX_PROJECT_ID\") ?? \"\",\n+    } = fields;\n+\n+    this.modelId = modelId;\n+    this.modelParameters = modelParameters;\n+    this.projectId = projectId;\n+\n+    const {\n+      apiKey = getEnvironmentVariable(\"IBM_CLOUD_API_KEY\"),\n+      apiVersion = \"2023-05-29\",\n+      region = \"us-south\",\n+    } = clientConfig;\n+\n+    if (!apiKey) {\n+      throw new Error(\"Missing IBM Cloud API Key\");\n+    }\n+\n+    if (!this.projectId) {\n+      throw new Error(\"Missing WatsonX AI Project ID\");\n+    }\n+\n+    this.watsonApiClient = new WatsonApiClient({\n+      apiKey,\n+      apiVersion,\n+      region,\n+    });\n+  }\n+\n+  protected _formatMessagesAsPrompt(messages: BaseMessage[]): string {\n+    return messages\n+      .map((message) => {\n+        let messageText;\n+        if (message._getType() === \"human\") {\n+          messageText = `[INST] ${message.content} [/INST]`;\n+        } else if (message._getType() === \"ai\") {\n+          messageText = message.content;\n+        } else if (message._getType() === \"system\") {\n+          messageText = `<<SYS>> ${message.content} <</SYS>>`;\n+        } else if (ChatMessage.isInstance(message)) {\n+          messageText = `\\n\\n${message.role[0].toUpperCase()}${message.role.slice(\n+            1\n+          )}: ${message.content}`;\n+        } else {\n+          console.warn(\n+            `Unsupported message type passed to Watson: \"${message._getType()}\"`\n+          );\n+          messageText = \"\";\n+        }\n+        return messageText;\n+      })\n+      .join(\"\\n\");\n+  }\n+\n+  _combineLLMOutput() {\n+    return {};\n+  }\n+\n+  async _call(\n+    messages: BaseMessage[],\n+    options: this[\"ParsedCallOptions\"],\n+    runManager: CallbackManagerForLLMRun | undefined\n+  ): Promise<string> {\n+    const chunks = [];\n+    const stream = this._streamResponseChunks(messages, options, runManager);\n+    for await (const chunk of stream) {\n+      chunks.push(chunk.message.content);\n+    }\n+    return chunks.join(\"\");\n+  }\n+\n+  override async *_streamResponseChunks(\n+    _messages: BaseMessage[],\n+    _options: this[\"ParsedCallOptions\"],\n+    _runManager?: CallbackManagerForLLMRun",
        "comment_created_at": "2023-12-21T23:11:45+00:00",
        "comment_author": "bracesproul",
        "comment_body": "Drop the underscore if they're being used. Typically, variables prefixed with an underscore are unused, and the underscore is used to bypass a lint rule for no-unused-variables",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1418032747",
    "pr_number": 3091,
    "pr_file": "langchain/src/experimental/tools/graphql.ts",
    "created_at": "2023-12-06T22:09:04+00:00",
    "commented_code": "import { z } from \"zod\";\nimport { StructuredTool } from \"../../tools/base.js\";\n\nexport class GraphQLClientTool extends StructuredTool {\n  static lc_name() {\n    return \"GraphQLClientTool\";\n  }\n\n  name = \"gql_client\";\n\n  description = `You can make a GraphQL request with this tool`;\n\n  _endpoint: string;\n\n  _headers: HeadersInit | undefined;",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1418032747",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 3091,
        "pr_file": "langchain/src/experimental/tools/graphql.ts",
        "discussion_id": "1418032747",
        "commented_code": "@@ -0,0 +1,47 @@\n+import { z } from \"zod\";\n+import { StructuredTool } from \"../../tools/base.js\";\n+\n+export class GraphQLClientTool extends StructuredTool {\n+  static lc_name() {\n+    return \"GraphQLClientTool\";\n+  }\n+\n+  name = \"gql_client\";\n+\n+  description = `You can make a GraphQL request with this tool`;\n+\n+  _endpoint: string;\n+\n+  _headers: HeadersInit | undefined;",
        "comment_created_at": "2023-12-06T22:09:04+00:00",
        "comment_author": "bracesproul",
        "comment_body": "Instead of prefixing with `_` lets just make them `private`/`protected`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1908162540",
    "pr_number": 7344,
    "pr_file": "libs/langchain-community/src/vectorstores/azion_edgesql.ts",
    "created_at": "2025-01-09T04:50:04+00:00",
    "commented_code": "import { VectorStore } from '@langchain/core/vectorstores';\nimport { useQuery, useExecute, getDatabases, createDatabase, getTables, type AzionDatabaseResponse, QueryResult, AzionDatabaseQueryResponse } from 'azion/sql';\nimport type { EmbeddingsInterface } from '@langchain/core/embeddings';\nimport { Document } from '@langchain/core/documents';\n\n/**\n * Represents a filter condition for querying the Azion database\n * @property operator - The comparison operator to use (e.g. =, !=, >, <, etc)\n * @property column - The database column to filter on\n * @property value - The value to compare against\n */\nexport type AzionFilter = {operator: Operator, column: Column, value: string};\n\n/**\n * Represents a database column name\n */\nexport type Column = string;\n\n/**\n * Valid SQL operators that can be used in filter conditions\n */\nexport type Operator = \n  | '=' | '!=' | '>' | '<>' | '<'  // Basic comparison operators\n  | '>=' | '<='                    // Range operators\n  | 'LIKE' | 'NOT LIKE'           // Pattern matching\n  | 'IN' | 'NOT IN'              // Set membership\n  | 'IS NULL' | 'IS NOT NULL';   // NULL checks\n\n\n/**\n * Interface for configuring the Azion vector store setup\n * @property {string[]} columns - Additional columns to create in the database table beyond the required ones\n * @property {\"vector\" | \"hybrid\"} mode - The search mode to enable:\n *                                       \"vector\" - Only vector similarity search\n *                                       \"hybrid\" - Both vector and full-text search capabilities\n */\ninterface AzionSetupOptions {\n  columns: string[],\n  mode: \"vector\" | \"hybrid\"\n}\n\n/**\n * Interface representing the structure of a row in the vector store\n * @property content - The text content of the document\n * @property embedding - The vector embedding of the content as an array of numbers\n * @property metadata - Additional metadata associated with the document as key-value pairs\n */\ninterface rowsInterface {\n  content: string;\n  embedding: number[];\n  metadata: Record<string, any>;\n}\n\nexport type AzionMetadata = Record<string, any>;\n\n/**\n * Interface for the response returned when searching embeddings.\n */\ninterface SearchEmbeddingsResponse {\n  id: number;\n  content: string;\n  similarity: number;\n  metadata: {\n    searchtype: string;\n    [key: string]: any;\n  };\n}\n\n/**\n * Interface for configuring hybrid search options that combines vector and full-text search\n * @property {number} kfts - Number of results to return from full-text search\n * @property {number} kvector - Number of results to return from vector similarity search\n * @property {AzionFilter[]} [filter] - Optional array of filters to apply to search results\n * @property {string[]} [metadataItems] - Optional array of metadata fields to include in results\n */\ninterface hybridSearchOptions {",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1908162540",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7344,
        "pr_file": "libs/langchain-community/src/vectorstores/azion_edgesql.ts",
        "discussion_id": "1908162540",
        "commented_code": "@@ -0,0 +1,924 @@\n+import { VectorStore } from '@langchain/core/vectorstores';\n+import { useQuery, useExecute, getDatabases, createDatabase, getTables, type AzionDatabaseResponse, QueryResult, AzionDatabaseQueryResponse } from 'azion/sql';\n+import type { EmbeddingsInterface } from '@langchain/core/embeddings';\n+import { Document } from '@langchain/core/documents';\n+\n+/**\n+ * Represents a filter condition for querying the Azion database\n+ * @property operator - The comparison operator to use (e.g. =, !=, >, <, etc)\n+ * @property column - The database column to filter on\n+ * @property value - The value to compare against\n+ */\n+export type AzionFilter = {operator: Operator, column: Column, value: string};\n+\n+/**\n+ * Represents a database column name\n+ */\n+export type Column = string;\n+\n+/**\n+ * Valid SQL operators that can be used in filter conditions\n+ */\n+export type Operator = \n+  | '=' | '!=' | '>' | '<>' | '<'  // Basic comparison operators\n+  | '>=' | '<='                    // Range operators\n+  | 'LIKE' | 'NOT LIKE'           // Pattern matching\n+  | 'IN' | 'NOT IN'              // Set membership\n+  | 'IS NULL' | 'IS NOT NULL';   // NULL checks\n+\n+\n+/**\n+ * Interface for configuring the Azion vector store setup\n+ * @property {string[]} columns - Additional columns to create in the database table beyond the required ones\n+ * @property {\"vector\" | \"hybrid\"} mode - The search mode to enable:\n+ *                                       \"vector\" - Only vector similarity search\n+ *                                       \"hybrid\" - Both vector and full-text search capabilities\n+ */\n+interface AzionSetupOptions {\n+  columns: string[],\n+  mode: \"vector\" | \"hybrid\"\n+}\n+\n+/**\n+ * Interface representing the structure of a row in the vector store\n+ * @property content - The text content of the document\n+ * @property embedding - The vector embedding of the content as an array of numbers\n+ * @property metadata - Additional metadata associated with the document as key-value pairs\n+ */\n+interface rowsInterface {\n+  content: string;\n+  embedding: number[];\n+  metadata: Record<string, any>;\n+}\n+\n+export type AzionMetadata = Record<string, any>;\n+\n+/**\n+ * Interface for the response returned when searching embeddings.\n+ */\n+interface SearchEmbeddingsResponse {\n+  id: number;\n+  content: string;\n+  similarity: number;\n+  metadata: {\n+    searchtype: string;\n+    [key: string]: any;\n+  };\n+}\n+\n+/**\n+ * Interface for configuring hybrid search options that combines vector and full-text search\n+ * @property {number} kfts - Number of results to return from full-text search\n+ * @property {number} kvector - Number of results to return from vector similarity search\n+ * @property {AzionFilter[]} [filter] - Optional array of filters to apply to search results\n+ * @property {string[]} [metadataItems] - Optional array of metadata fields to include in results\n+ */\n+interface hybridSearchOptions {",
        "comment_created_at": "2025-01-09T04:50:04+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "By convention we capitalize types/interfaces",
        "pr_file_module": null
      },
      {
        "comment_id": "1941033662",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7344,
        "pr_file": "libs/langchain-community/src/vectorstores/azion_edgesql.ts",
        "discussion_id": "1908162540",
        "commented_code": "@@ -0,0 +1,924 @@\n+import { VectorStore } from '@langchain/core/vectorstores';\n+import { useQuery, useExecute, getDatabases, createDatabase, getTables, type AzionDatabaseResponse, QueryResult, AzionDatabaseQueryResponse } from 'azion/sql';\n+import type { EmbeddingsInterface } from '@langchain/core/embeddings';\n+import { Document } from '@langchain/core/documents';\n+\n+/**\n+ * Represents a filter condition for querying the Azion database\n+ * @property operator - The comparison operator to use (e.g. =, !=, >, <, etc)\n+ * @property column - The database column to filter on\n+ * @property value - The value to compare against\n+ */\n+export type AzionFilter = {operator: Operator, column: Column, value: string};\n+\n+/**\n+ * Represents a database column name\n+ */\n+export type Column = string;\n+\n+/**\n+ * Valid SQL operators that can be used in filter conditions\n+ */\n+export type Operator = \n+  | '=' | '!=' | '>' | '<>' | '<'  // Basic comparison operators\n+  | '>=' | '<='                    // Range operators\n+  | 'LIKE' | 'NOT LIKE'           // Pattern matching\n+  | 'IN' | 'NOT IN'              // Set membership\n+  | 'IS NULL' | 'IS NOT NULL';   // NULL checks\n+\n+\n+/**\n+ * Interface for configuring the Azion vector store setup\n+ * @property {string[]} columns - Additional columns to create in the database table beyond the required ones\n+ * @property {\"vector\" | \"hybrid\"} mode - The search mode to enable:\n+ *                                       \"vector\" - Only vector similarity search\n+ *                                       \"hybrid\" - Both vector and full-text search capabilities\n+ */\n+interface AzionSetupOptions {\n+  columns: string[],\n+  mode: \"vector\" | \"hybrid\"\n+}\n+\n+/**\n+ * Interface representing the structure of a row in the vector store\n+ * @property content - The text content of the document\n+ * @property embedding - The vector embedding of the content as an array of numbers\n+ * @property metadata - Additional metadata associated with the document as key-value pairs\n+ */\n+interface rowsInterface {\n+  content: string;\n+  embedding: number[];\n+  metadata: Record<string, any>;\n+}\n+\n+export type AzionMetadata = Record<string, any>;\n+\n+/**\n+ * Interface for the response returned when searching embeddings.\n+ */\n+interface SearchEmbeddingsResponse {\n+  id: number;\n+  content: string;\n+  similarity: number;\n+  metadata: {\n+    searchtype: string;\n+    [key: string]: any;\n+  };\n+}\n+\n+/**\n+ * Interface for configuring hybrid search options that combines vector and full-text search\n+ * @property {number} kfts - Number of results to return from full-text search\n+ * @property {number} kvector - Number of results to return from vector similarity search\n+ * @property {AzionFilter[]} [filter] - Optional array of filters to apply to search results\n+ * @property {string[]} [metadataItems] - Optional array of metadata fields to include in results\n+ */\n+interface hybridSearchOptions {",
        "comment_created_at": "2025-02-04T11:57:24+00:00",
        "comment_author": "PedroMiolaSilva",
        "comment_body": "Fixed!\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1908152860",
    "pr_number": 7450,
    "pr_file": "libs/langchain-community/src/embeddings/bytedance_doubao.ts",
    "created_at": "2025-01-09T04:32:48+00:00",
    "commented_code": "import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { Embeddings, type EmbeddingsParams } from \"@langchain/core/embeddings\";\nimport { chunkArray } from \"@langchain/core/utils/chunk_array\";\n\nexport interface ByteDanceDoubaoEmbeddingsParams extends EmbeddingsParams {\n  /** Model name to use */\n  modelName: string;",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1908152860",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7450,
        "pr_file": "libs/langchain-community/src/embeddings/bytedance_doubao.ts",
        "discussion_id": "1908152860",
        "commented_code": "@@ -0,0 +1,175 @@\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import { Embeddings, type EmbeddingsParams } from \"@langchain/core/embeddings\";\n+import { chunkArray } from \"@langchain/core/utils/chunk_array\";\n+\n+export interface ByteDanceDoubaoEmbeddingsParams extends EmbeddingsParams {\n+  /** Model name to use */\n+  modelName: string;",
        "comment_created_at": "2025-01-09T04:32:48+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "We are standardizing as `model` instead of `modelName`",
        "pr_file_module": null
      },
      {
        "comment_id": "1908213190",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7450,
        "pr_file": "libs/langchain-community/src/embeddings/bytedance_doubao.ts",
        "discussion_id": "1908152860",
        "commented_code": "@@ -0,0 +1,175 @@\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import { Embeddings, type EmbeddingsParams } from \"@langchain/core/embeddings\";\n+import { chunkArray } from \"@langchain/core/utils/chunk_array\";\n+\n+export interface ByteDanceDoubaoEmbeddingsParams extends EmbeddingsParams {\n+  /** Model name to use */\n+  modelName: string;",
        "comment_created_at": "2025-01-09T06:06:56+00:00",
        "comment_author": "ucev",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1118002128",
    "pr_number": 139,
    "pr_file": "langchain/chains/question_answering/load.ts",
    "created_at": "2023-02-26T01:12:56+00:00",
    "commented_code": "DEFAULT_COMBINE_QA_PROMPT,\n} from \"./map_reduce_prompts\";\n\ninterface qaChainParams {\n  prompt?: PromptTemplate;\n  combineMapPrompt?: PromptTemplate;\n  combinePrompt?: PromptTemplate;\n  type?: string;\n}\nexport const loadQAChain = (llm: BaseLLM, params: qaChainParams = {}) => {\ntype chainTypeName = \"stuff\" | \"map_reduce\";",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1118002128",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 139,
        "pr_file": "langchain/chains/question_answering/load.ts",
        "discussion_id": "1118002128",
        "commented_code": "@@ -11,13 +11,23 @@ import {\n   DEFAULT_COMBINE_QA_PROMPT,\n } from \"./map_reduce_prompts\";\n \n-interface qaChainParams {\n-  prompt?: PromptTemplate;\n-  combineMapPrompt?: PromptTemplate;\n-  combinePrompt?: PromptTemplate;\n-  type?: string;\n-}\n-export const loadQAChain = (llm: BaseLLM, params: qaChainParams = {}) => {\n+type chainTypeName = \"stuff\" | \"map_reduce\";",
        "comment_created_at": "2023-02-26T01:12:56+00:00",
        "comment_author": "atbe",
        "comment_body": "I would give this type a more strict name and follow upper casing for it too (given that its a type):\r\n\r\n`QuestionAnsweringChainType` for example\r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1119986832",
    "pr_number": 139,
    "pr_file": "langchain/chains/question_answering/load.ts",
    "created_at": "2023-02-28T12:24:23+00:00",
    "commented_code": "DEFAULT_COMBINE_QA_PROMPT,\n} from \"./map_reduce_prompts\";\n\ninterface qaChainParams {\n  prompt?: PromptTemplate;\n  combineMapPrompt?: PromptTemplate;\n  combinePrompt?: PromptTemplate;\n  type?: string;\n}\nexport const loadQAChain = (llm: BaseLLM, params: qaChainParams = {}) => {\ntype chainTypeName = \"stuff\" | \"map_reduce\";\n\ntype chainType<T> = T extends \"stuff\"",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1119986832",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 139,
        "pr_file": "langchain/chains/question_answering/load.ts",
        "discussion_id": "1119986832",
        "commented_code": "@@ -11,13 +11,23 @@ import {\n   DEFAULT_COMBINE_QA_PROMPT,\n } from \"./map_reduce_prompts\";\n \n-interface qaChainParams {\n-  prompt?: PromptTemplate;\n-  combineMapPrompt?: PromptTemplate;\n-  combinePrompt?: PromptTemplate;\n-  type?: string;\n-}\n-export const loadQAChain = (llm: BaseLLM, params: qaChainParams = {}) => {\n+type chainTypeName = \"stuff\" | \"map_reduce\";\n+\n+type chainType<T> = T extends \"stuff\"",
        "comment_created_at": "2023-02-28T12:24:23+00:00",
        "comment_author": "nfcampos",
        "comment_body": "Types should start with an uppercase letter",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1183234558",
    "pr_number": 1096,
    "pr_file": "langchain/src/document_loaders/fs/directory.ts",
    "created_at": "2023-05-03T04:47:50+00:00",
    "commented_code": "constructor(\n    public directoryPath: string,\n    public loaders: {\n      [extension: string]: (filePath: string) => BaseDocumentLoader;\n      [extension: `.${string}`]: (filePath: string) => BaseDocumentLoader;",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1183234558",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 1096,
        "pr_file": "langchain/src/document_loaders/fs/directory.ts",
        "discussion_id": "1183234558",
        "commented_code": "@@ -19,7 +19,7 @@ export class DirectoryLoader extends BaseDocumentLoader {\n   constructor(\n     public directoryPath: string,\n     public loaders: {\n-      [extension: string]: (filePath: string) => BaseDocumentLoader;\n+      [extension: `.${string}`]: (filePath: string) => BaseDocumentLoader;",
        "comment_created_at": "2023-05-03T04:47:50+00:00",
        "comment_author": "rpidanny",
        "comment_body": "Since we're always expecting the extension to start with a `.`, this should help TS users to have better DX. wdyt?",
        "pr_file_module": null
      }
    ]
  }
]