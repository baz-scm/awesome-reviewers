[
  {
    "discussion_id": "2099231081",
    "pr_number": 8007,
    "pr_file": "libs/langchain-textsplitters/src/oracle_text_splitter.ts",
    "created_at": "2025-05-21T03:31:55+00:00",
    "commented_code": "import oracledb from \"oracledb\";\nimport { TextSplitter, TextSplitterParams } from \"./text_splitter.js\";\n\n/**\n * Split text into smaller pieces\n * @example\n * ```typescript\n * const splitter = new OracleTextSplitter(conn, params);\n * let chunks = await splitter.splitText(doc.pageContent);\n * ```\n */\nexport class OracleTextSplitter extends TextSplitter {\n  protected conn: oracledb.Connection;\n\n  protected pref: Record<string, unknown>;\n\n  static lc_name() {\n    return \"OracleTextSplitter\";\n  }\n\n  constructor(\n    conn: oracledb.Connection,\n    pref: Record<string, unknown>,\n    fields?: TextSplitterParams\n  ) {\n    super(fields);\n    this.conn = conn;\n    this.pref = pref;\n  }\n\n  async splitText(text: string) {\n    const chunks: string[] = [];\n\n    const result = await this.conn.execute(\n      <string>(\n        `select t.column_value as data from dbms_vector_chain.utl_to_chunks(:content, json(:pref)) t`\n      ),\n      <oracledb.BindParameters>{\n        content: { val: text, dir: oracledb.BIND_IN, type: oracledb.CLOB },\n        pref: JSON.stringify(this.pref),",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "2099231081",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8007,
        "pr_file": "libs/langchain-textsplitters/src/oracle_text_splitter.ts",
        "discussion_id": "2099231081",
        "commented_code": "@@ -0,0 +1,56 @@\n+import oracledb from \"oracledb\";\n+import { TextSplitter, TextSplitterParams } from \"./text_splitter.js\";\n+\n+/**\n+ * Split text into smaller pieces\n+ * @example\n+ * ```typescript\n+ * const splitter = new OracleTextSplitter(conn, params);\n+ * let chunks = await splitter.splitText(doc.pageContent);\n+ * ```\n+ */\n+export class OracleTextSplitter extends TextSplitter {\n+  protected conn: oracledb.Connection;\n+\n+  protected pref: Record<string, unknown>;\n+\n+  static lc_name() {\n+    return \"OracleTextSplitter\";\n+  }\n+\n+  constructor(\n+    conn: oracledb.Connection,\n+    pref: Record<string, unknown>,\n+    fields?: TextSplitterParams\n+  ) {\n+    super(fields);\n+    this.conn = conn;\n+    this.pref = pref;\n+  }\n+\n+  async splitText(text: string) {\n+    const chunks: string[] = [];\n+\n+    const result = await this.conn.execute(\n+      <string>(\n+        `select t.column_value as data from dbms_vector_chain.utl_to_chunks(:content, json(:pref)) t`\n+      ),\n+      <oracledb.BindParameters>{\n+        content: { val: text, dir: oracledb.BIND_IN, type: oracledb.CLOB },\n+        pref: JSON.stringify(this.pref),",
        "comment_created_at": "2025-05-21T03:31:55+00:00",
        "comment_author": "cjbj",
        "comment_body": "What DBs are being targeted?  Can you bind JSON as DB_TYPE_JSON?  DItto other places JSON is used.",
        "pr_file_module": null
      },
      {
        "comment_id": "2100349110",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8007,
        "pr_file": "libs/langchain-textsplitters/src/oracle_text_splitter.ts",
        "discussion_id": "2099231081",
        "commented_code": "@@ -0,0 +1,56 @@\n+import oracledb from \"oracledb\";\n+import { TextSplitter, TextSplitterParams } from \"./text_splitter.js\";\n+\n+/**\n+ * Split text into smaller pieces\n+ * @example\n+ * ```typescript\n+ * const splitter = new OracleTextSplitter(conn, params);\n+ * let chunks = await splitter.splitText(doc.pageContent);\n+ * ```\n+ */\n+export class OracleTextSplitter extends TextSplitter {\n+  protected conn: oracledb.Connection;\n+\n+  protected pref: Record<string, unknown>;\n+\n+  static lc_name() {\n+    return \"OracleTextSplitter\";\n+  }\n+\n+  constructor(\n+    conn: oracledb.Connection,\n+    pref: Record<string, unknown>,\n+    fields?: TextSplitterParams\n+  ) {\n+    super(fields);\n+    this.conn = conn;\n+    this.pref = pref;\n+  }\n+\n+  async splitText(text: string) {\n+    const chunks: string[] = [];\n+\n+    const result = await this.conn.execute(\n+      <string>(\n+        `select t.column_value as data from dbms_vector_chain.utl_to_chunks(:content, json(:pref)) t`\n+      ),\n+      <oracledb.BindParameters>{\n+        content: { val: text, dir: oracledb.BIND_IN, type: oracledb.CLOB },\n+        pref: JSON.stringify(this.pref),",
        "comment_created_at": "2025-05-21T13:47:56+00:00",
        "comment_author": "hackerdave",
        "comment_body": "I'm not sure if we were clear in the docs but the langchain integration is targeting Oracle Database 23ai which is the latest version. The vector features will only work on 23. Yes we could have used DB_TYPE_JSON which will work from 21 but in this case we are passing as a string.",
        "pr_file_module": null
      },
      {
        "comment_id": "2101324757",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8007,
        "pr_file": "libs/langchain-textsplitters/src/oracle_text_splitter.ts",
        "discussion_id": "2099231081",
        "commented_code": "@@ -0,0 +1,56 @@\n+import oracledb from \"oracledb\";\n+import { TextSplitter, TextSplitterParams } from \"./text_splitter.js\";\n+\n+/**\n+ * Split text into smaller pieces\n+ * @example\n+ * ```typescript\n+ * const splitter = new OracleTextSplitter(conn, params);\n+ * let chunks = await splitter.splitText(doc.pageContent);\n+ * ```\n+ */\n+export class OracleTextSplitter extends TextSplitter {\n+  protected conn: oracledb.Connection;\n+\n+  protected pref: Record<string, unknown>;\n+\n+  static lc_name() {\n+    return \"OracleTextSplitter\";\n+  }\n+\n+  constructor(\n+    conn: oracledb.Connection,\n+    pref: Record<string, unknown>,\n+    fields?: TextSplitterParams\n+  ) {\n+    super(fields);\n+    this.conn = conn;\n+    this.pref = pref;\n+  }\n+\n+  async splitText(text: string) {\n+    const chunks: string[] = [];\n+\n+    const result = await this.conn.execute(\n+      <string>(\n+        `select t.column_value as data from dbms_vector_chain.utl_to_chunks(:content, json(:pref)) t`\n+      ),\n+      <oracledb.BindParameters>{\n+        content: { val: text, dir: oracledb.BIND_IN, type: oracledb.CLOB },\n+        pref: JSON.stringify(this.pref),",
        "comment_created_at": "2025-05-21T23:02:07+00:00",
        "comment_author": "cjbj",
        "comment_body": "Binding as DB_TYPE_JSON where possible will be more efficient.",
        "pr_file_module": null
      },
      {
        "comment_id": "2101476268",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8007,
        "pr_file": "libs/langchain-textsplitters/src/oracle_text_splitter.ts",
        "discussion_id": "2099231081",
        "commented_code": "@@ -0,0 +1,56 @@\n+import oracledb from \"oracledb\";\n+import { TextSplitter, TextSplitterParams } from \"./text_splitter.js\";\n+\n+/**\n+ * Split text into smaller pieces\n+ * @example\n+ * ```typescript\n+ * const splitter = new OracleTextSplitter(conn, params);\n+ * let chunks = await splitter.splitText(doc.pageContent);\n+ * ```\n+ */\n+export class OracleTextSplitter extends TextSplitter {\n+  protected conn: oracledb.Connection;\n+\n+  protected pref: Record<string, unknown>;\n+\n+  static lc_name() {\n+    return \"OracleTextSplitter\";\n+  }\n+\n+  constructor(\n+    conn: oracledb.Connection,\n+    pref: Record<string, unknown>,\n+    fields?: TextSplitterParams\n+  ) {\n+    super(fields);\n+    this.conn = conn;\n+    this.pref = pref;\n+  }\n+\n+  async splitText(text: string) {\n+    const chunks: string[] = [];\n+\n+    const result = await this.conn.execute(\n+      <string>(\n+        `select t.column_value as data from dbms_vector_chain.utl_to_chunks(:content, json(:pref)) t`\n+      ),\n+      <oracledb.BindParameters>{\n+        content: { val: text, dir: oracledb.BIND_IN, type: oracledb.CLOB },\n+        pref: JSON.stringify(this.pref),",
        "comment_created_at": "2025-05-22T02:00:59+00:00",
        "comment_author": "hackerdave",
        "comment_body": "Switched to DB_TYPE_JSON",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1752684909",
    "pr_number": 6676,
    "pr_file": "langchain/src/retrievers/recency_ranked.ts",
    "created_at": "2024-09-10T20:31:26+00:00",
    "commented_code": "import { BaseRetriever } from \"@langchain/core/retrievers\";\nimport { VectorStoreInterface } from \"@langchain/core/vectorstores\";\nimport { Document } from \"@langchain/core/documents\";\n\nexport interface RecencyRankedRetrieverConfig {\n  vectorStore: VectorStoreInterface;\n  k: number;\n  topK?: number;\n  recencyWeight: number;\n}\n\nexport class RecencyRankedRetriever extends BaseRetriever {\n  static lc_name() {\n    return \"RecencyRankedRetriever\";\n  }\n\n  lc_namespace = [\"langchain\", \"retrievers\", \"recency_ranked\"];\n  \n  private vectorStore: VectorStoreInterface;\n\n  private k: number;\n\n  private topK: number;\n\n  private recencyWeight: number;\n\n  constructor(config: RecencyRankedRetrieverConfig) {\n    super();\n    this.vectorStore = config.vectorStore;\n    this.k = config.k;\n    this.topK = config.topK ?? config.k;\n    this.recencyWeight = config.recencyWeight;\n  }\n\n  async _getRelevantDocuments(query: string): Promise<Document[]> {\n    const relevantDocs = await this.vectorStore.similaritySearchWithScore(query, this.k);\n    const rerankedDocs = this.recentDocumentRanker(relevantDocs, this.topK, this.recencyWeight);\n    return rerankedDocs.map(([doc, _]) => doc);\n  }\n\n  async invoke(query: string): Promise<Document[]> {\n    return this._getRelevantDocuments(query);\n  }\n\n  private recentDocumentRanker(\n    documents: [Document, number][],\n    topK: number,\n    recencyWeight: number\n  ): [Document, number][] {\n    if (documents.length === 0) return [];\n\n    if (!documents.every(([doc, _]) => doc.metadata.date instanceof Date)) {",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1752684909",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 6676,
        "pr_file": "langchain/src/retrievers/recency_ranked.ts",
        "discussion_id": "1752684909",
        "commented_code": "@@ -0,0 +1,78 @@\n+import { BaseRetriever } from \"@langchain/core/retrievers\";\n+import { VectorStoreInterface } from \"@langchain/core/vectorstores\";\n+import { Document } from \"@langchain/core/documents\";\n+\n+export interface RecencyRankedRetrieverConfig {\n+  vectorStore: VectorStoreInterface;\n+  k: number;\n+  topK?: number;\n+  recencyWeight: number;\n+}\n+\n+export class RecencyRankedRetriever extends BaseRetriever {\n+  static lc_name() {\n+    return \"RecencyRankedRetriever\";\n+  }\n+\n+  lc_namespace = [\"langchain\", \"retrievers\", \"recency_ranked\"];\n+  \n+  private vectorStore: VectorStoreInterface;\n+\n+  private k: number;\n+\n+  private topK: number;\n+\n+  private recencyWeight: number;\n+\n+  constructor(config: RecencyRankedRetrieverConfig) {\n+    super();\n+    this.vectorStore = config.vectorStore;\n+    this.k = config.k;\n+    this.topK = config.topK ?? config.k;\n+    this.recencyWeight = config.recencyWeight;\n+  }\n+\n+  async _getRelevantDocuments(query: string): Promise<Document[]> {\n+    const relevantDocs = await this.vectorStore.similaritySearchWithScore(query, this.k);\n+    const rerankedDocs = this.recentDocumentRanker(relevantDocs, this.topK, this.recencyWeight);\n+    return rerankedDocs.map(([doc, _]) => doc);\n+  }\n+\n+  async invoke(query: string): Promise<Document[]> {\n+    return this._getRelevantDocuments(query);\n+  }\n+\n+  private recentDocumentRanker(\n+    documents: [Document, number][],\n+    topK: number,\n+    recencyWeight: number\n+  ): [Document, number][] {\n+    if (documents.length === 0) return [];\n+\n+    if (!documents.every(([doc, _]) => doc.metadata.date instanceof Date)) {",
        "comment_created_at": "2024-09-10T20:31:26+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "Would suggest storing this as an ISO string -  not all (most?) vector stores will not deserialize dates",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2004667909",
    "pr_number": 7852,
    "pr_file": "libs/langchain-google-cloud-sql-pg/src/vectorStore.ts",
    "created_at": "2025-03-20T02:08:43+00:00",
    "commented_code": "import { EmbeddingsInterface } from \"@langchain/core/embeddings\";\nimport { MaxMarginalRelevanceSearchOptions, VectorStore } from \"@langchain/core/vectorstores\";\nimport { Document } from \"@langchain/core/documents\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\nimport { DEFAULT_DISTANCE_STRATEGY, DistanceStrategy, QueryOptions } from \"./indexes.js\";\nimport PostgresEngine from \"./engine.js\";\nimport { customZip } from \"./utils/utils.js\";\n\nexport interface PostgresVectorStoreArgs {\n  schemaName?: string,\n  contentColumn?: string,\n  embeddingColumn?: string,\n  metadataColumns?: Array<string>,\n  idColumn?: string,\n  distanceStrategy?: DistanceStrategy,\n  k?: number,\n  fetchK?: number,\n  lambdaMult?: number,\n  ignoreMetadataColumns?: Array<string>,\n  metadataJsonColumn?: string,\n  indexQueryOptions?: QueryOptions\n}\n\nexport interface dbConfigArgs {\n  engine: PostgresEngine;\n  tableName: string;\n  dbConfig?: PostgresVectorStoreArgs;\n}\n\nexport class PostgresVectorStore extends VectorStore {\n  declare FilterType: string;\n\n  engine: PostgresEngine;\n\n  embeddings: EmbeddingsInterface;\n\n  tableName: string;\n\n  schemaName: string;\n\n  contentColumn: string;\n\n  embeddingColumn: string;\n\n  metadataColumns: Array<string>;\n\n  ignoreMetadataColumns: Array<string>;\n\n  idColumn: string;\n\n  metadataJsonColumn: string;\n\n  distanceStrategy: DistanceStrategy;\n\n  k: number;\n\n  fetchK: number;\n\n  lambdaMult: number;\n\n  indexQueryOptions: QueryOptions;\n\n  /**\n   * Initializes a new vector store with embeddings and database configuration.\n   *\n   * @param embeddings - Instance of `EmbeddingsInterface` used to embed queries.\n   * @param dbConfig - Configuration settings for the database or storage system.\n   */\n  constructor(embeddings: EmbeddingsInterface, dbConfig: Record) {\n    super(embeddings, dbConfig);\n    this.embeddings = embeddings;\n    this.engine = dbConfig.engine;\n    this.tableName = dbConfig.tableName;\n    this.schemaName = dbConfig.schemaName;\n    this.contentColumn = dbConfig.contentColumn;\n    this.embeddingColumn = dbConfig.embeddingColumn;\n    this.metadataColumns = dbConfig.metadataColumns ? dbConfig.metadataColumns : [];\n    this.ignoreMetadataColumns = dbConfig.ignoreMetadataColumns;\n    this.idColumn = dbConfig.idColumn;\n    this.metadataJsonColumn = dbConfig.metadataJsonColumn;\n    this.distanceStrategy = dbConfig.distanceStrategy;\n    this.k = dbConfig.k;\n    this.fetchK = dbConfig.fetchK;\n    this.lambdaMult = dbConfig.lambdaMult;\n    this.indexQueryOptions = dbConfig.indexQueryOptions;\n  }\n\n  /**\n   * Create a new PostgresVectorStore instance.\n   * @param {PostgresEngine} engine Required - Connection pool engine for managing connections to Cloud SQL for PostgreSQL database.\n   * @param {Embeddings} embeddings Required - Text embedding model to use.\n   * @param {string} tableName Required - Name of an existing table or table to be created.\n   * @param {string} schemaName Database schema name of the table. Defaults to \"public\".\n   * @param {string} contentColumn Column that represent a Document's page_content. Defaults to \"content\".\n   * @param {string} embeddingColumn Column for embedding vectors. The embedding is generated from the document value. Defaults to \"embedding\".\n   * @param {Array<string>} metadataColumns Column(s) that represent a document's metadata.\n   * @param {Array<string>} ignoreMetadataColumns Optional - Column(s) to ignore in pre-existing tables for a document's metadata. Can not be used with metadata_columns.\n   * @param {string} idColumn Column that represents the Document's id. Defaults to \"langchain_id\".\n   * @param {string} metadataJsonColumn Optional - Column to store metadata as JSON. Defaults to \"langchain_metadata\".\n   * @param {DistanceStrategy} distanceStrategy Distance strategy to use for vector similarity search. Defaults to COSINE_DISTANCE.\n   * @param {number} k Number of Documents to return from search. Defaults to 4.\n   * @param {number} fetchK Number of Documents to fetch to pass to MMR algorithm.\n   * @param {number} lambdaMult Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.\n   * @param {QueryOptions} indexQueryOptions Optional - Index query option.\n   * @returns PostgresVectorStore instance.\n   */\n  static async create(\n    engine: PostgresEngine,\n    embeddings: EmbeddingsInterface,\n    tableName: string,\n    {\n      schemaName = \"public\",\n      contentColumn = \"content\",\n      embeddingColumn = \"embedding\",\n      metadataColumns = [],\n      ignoreMetadataColumns,\n      idColumn = \"langchain_id\",\n      metadataJsonColumn = \"langchain_metadata\",\n      distanceStrategy = DEFAULT_DISTANCE_STRATEGY,\n      k = 4,\n      fetchK = 20,\n      lambdaMult = 0.5,\n      indexQueryOptions\n    }: PostgresVectorStoreArgs = {}\n  ): Promise<PostgresVectorStore> {\n\n    if (metadataColumns !== undefined && ignoreMetadataColumns !== undefined) {\n      throw Error(\"Can not use both metadata_columns and ignore_metadata_columns.\");\n    }\n\n    const { rows } = await engine.pool.raw(`SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '${tableName}' AND table_schema = '${schemaName}'`);\n    const columns: { [key: string] } = {};\n\n    for (const index in rows) {\n      if (rows[index]) {\n        const row = rows[index];\n        columns[row.column_name] = row.data_type\n      }\n    }\n\n    if (!Object.prototype.hasOwnProperty.call(columns, idColumn)) {\n      throw Error(`Id column: ${idColumn}, does not exist.`);\n    }\n\n    if (!Object.prototype.hasOwnProperty.call(columns, contentColumn)) {\n      throw Error(`Content column: ${contentColumn}, does not exist.`);\n    }\n\n    const contentType = columns[contentColumn];\n\n    if (contentType !== \"text\" && !contentType.includes(\"char\")) {\n      throw Error(`Content column: ${contentColumn}, is type: ${contentType}. It must be a type of character string.`);\n    }\n\n    if (!Object.prototype.hasOwnProperty.call(columns, embeddingColumn)) {\n      throw Error(`Embedding column: ${embeddingColumn}, does not exist.`);\n    }\n\n    if (columns[embeddingColumn] !== \"USER-DEFINED\") {\n      throw Error(`Embedding column: ${embeddingColumn} is not of type Vector.`);\n    }\n\n    const jsonColumn = Object.prototype.hasOwnProperty.call(columns, metadataJsonColumn) ? metadataJsonColumn : \"\";\n\n    for (const column of metadataColumns) {\n      if (!Object.prototype.hasOwnProperty.call(columns, column)) {\n        throw Error(`Metadata column: ${column}, does not exist.`);\n      }\n    }\n\n    const allColumns = columns;\n    let allMetadataColumns: string[];\n    if (ignoreMetadataColumns !== undefined && ignoreMetadataColumns.length > 0) {\n      for (const column of ignoreMetadataColumns) {\n        delete allColumns[column];\n      }\n\n      delete allColumns[idColumn];\n      delete allColumns[contentColumn];\n      delete allColumns[embeddingColumn];\n      allMetadataColumns = Object.keys(allColumns);\n    }\n    return new PostgresVectorStore(\n      embeddings,\n      {\n        engine,\n        tableName,\n        schemaName,\n        contentColumn,\n        embeddingColumn,\n        metadataColumns: allMetadataColumns,\n        ignoreMetadataColumns,\n        idColumn,\n        metadataJsonColumn: jsonColumn,\n        distanceStrategy,\n        k,\n        fetchK,\n        lambdaMult,\n        indexQueryOptions\n      }\n    )\n  }\n\n  static async fromTexts(texts: string[], metadatas: object[] | object, embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n    const documents: Document[] = [];\n\n    for (let i = 0; i < texts.length; i += 1) {\n      const doc = new Document({\n        pageContent: texts[i],\n        metadata: Array.isArray(metadatas) ? metadatas[i] : metadatas\n      })\n      documents.push(doc);\n    }\n\n    return PostgresVectorStore.fromDocuments(documents, embeddings, dbConfig);\n  }\n\n  static async fromDocuments(docs: Document[], embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n    const { engine } = dbConfig;\n    const { tableName } = dbConfig;\n    const config = dbConfig.dbConfig;\n    const vectorStore = await this.create(engine, embeddings, tableName, config);\n\n    await vectorStore.addDocuments(docs)\n\n    return vectorStore;\n  }\n\n  async addVectors(vectors: number[][], documents: Document[], options?: { ids?: string[] }): Promise<string[] | void> {\n    let ids: string[] = [];\n    const metadatas: Record[] = []\n\n    if (vectors.length !== documents.length) {\n      throw new Error(\"The number of vectors must match the number of documents provided.\");\n    }\n\n    if (options?.ids) {\n      ids = options.ids;\n    } else {\n      documents.forEach(document => {\n        if (document.id !== undefined) {\n          ids.push(document.id);\n        } else {\n          ids.push(uuidv4());\n        }\n      });\n    }\n\n    if (options && options.ids && options.ids.length !== documents.length) {\n      throw new Error(\"The number of ids must match the number of documents provided.\");\n    }\n\n    documents.forEach(document => {\n      metadatas.push(document.metadata)\n    });\n\n    const tuples = customZip(ids, documents, vectors, metadatas);\n\n    // Insert embeddings\n    for (const [id, document, embedding, metadata] of tuples) {\n      const metadataColNames = this.metadataColumns.length > 0 ? `, \"${this.metadataColumns.join(\"\\\",\\\"\")}\"` : \"\";\n\n      let stmt = `INSERT INTO \"${this.schemaName}\".\"${this.tableName}\"(\"${this.idColumn}\", \"${this.contentColumn}\", \"${this.embeddingColumn}\" ${metadataColNames}`\n      const values: { [key: string] } = {\n        id,\n        content: document.pageContent,\n        embedding: `[${embedding.toString()}]`\n      }\n      let valuesStmt = \" VALUES (:id, :content, :embedding\";\n\n      // Add metadata\n      const extra = metadata;\n      for (const metadataColumn of this.metadataColumns) {\n        if (Object.prototype.hasOwnProperty.call(metadata, metadataColumn)) {\n          valuesStmt += `, :${metadataColumn}`;\n          values[metadataColumn] = metadata[metadataColumn]\n          delete extra[metadataColumn]\n        } else {\n          valuesStmt += \" ,null\"\n        }\n      }\n\n      // Add JSON column and/or close statement\n      stmt += this.metadataJsonColumn ? `, ${this.metadataJsonColumn})` : \")\";",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "2004667909",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7852,
        "pr_file": "libs/langchain-google-cloud-sql-pg/src/vectorStore.ts",
        "discussion_id": "2004667909",
        "commented_code": "@@ -0,0 +1,423 @@\n+import { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { MaxMarginalRelevanceSearchOptions, VectorStore } from \"@langchain/core/vectorstores\";\n+import { Document } from \"@langchain/core/documents\";\n+import { v4 as uuidv4 } from \"uuid\";\n+import { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\n+import { DEFAULT_DISTANCE_STRATEGY, DistanceStrategy, QueryOptions } from \"./indexes.js\";\n+import PostgresEngine from \"./engine.js\";\n+import { customZip } from \"./utils/utils.js\";\n+\n+export interface PostgresVectorStoreArgs {\n+  schemaName?: string,\n+  contentColumn?: string,\n+  embeddingColumn?: string,\n+  metadataColumns?: Array<string>,\n+  idColumn?: string,\n+  distanceStrategy?: DistanceStrategy,\n+  k?: number,\n+  fetchK?: number,\n+  lambdaMult?: number,\n+  ignoreMetadataColumns?: Array<string>,\n+  metadataJsonColumn?: string,\n+  indexQueryOptions?: QueryOptions\n+}\n+\n+export interface dbConfigArgs {\n+  engine: PostgresEngine;\n+  tableName: string;\n+  dbConfig?: PostgresVectorStoreArgs;\n+}\n+\n+export class PostgresVectorStore extends VectorStore {\n+  declare FilterType: string;\n+\n+  engine: PostgresEngine;\n+\n+  embeddings: EmbeddingsInterface;\n+\n+  tableName: string;\n+\n+  schemaName: string;\n+\n+  contentColumn: string;\n+\n+  embeddingColumn: string;\n+\n+  metadataColumns: Array<string>;\n+\n+  ignoreMetadataColumns: Array<string>;\n+\n+  idColumn: string;\n+\n+  metadataJsonColumn: string;\n+\n+  distanceStrategy: DistanceStrategy;\n+\n+  k: number;\n+\n+  fetchK: number;\n+\n+  lambdaMult: number;\n+\n+  indexQueryOptions: QueryOptions;\n+\n+  /**\n+   * Initializes a new vector store with embeddings and database configuration.\n+   *\n+   * @param embeddings - Instance of `EmbeddingsInterface` used to embed queries.\n+   * @param dbConfig - Configuration settings for the database or storage system.\n+   */\n+  constructor(embeddings: EmbeddingsInterface, dbConfig: Record) {\n+    super(embeddings, dbConfig);\n+    this.embeddings = embeddings;\n+    this.engine = dbConfig.engine;\n+    this.tableName = dbConfig.tableName;\n+    this.schemaName = dbConfig.schemaName;\n+    this.contentColumn = dbConfig.contentColumn;\n+    this.embeddingColumn = dbConfig.embeddingColumn;\n+    this.metadataColumns = dbConfig.metadataColumns ? dbConfig.metadataColumns : [];\n+    this.ignoreMetadataColumns = dbConfig.ignoreMetadataColumns;\n+    this.idColumn = dbConfig.idColumn;\n+    this.metadataJsonColumn = dbConfig.metadataJsonColumn;\n+    this.distanceStrategy = dbConfig.distanceStrategy;\n+    this.k = dbConfig.k;\n+    this.fetchK = dbConfig.fetchK;\n+    this.lambdaMult = dbConfig.lambdaMult;\n+    this.indexQueryOptions = dbConfig.indexQueryOptions;\n+  }\n+\n+  /**\n+   * Create a new PostgresVectorStore instance.\n+   * @param {PostgresEngine} engine Required - Connection pool engine for managing connections to Cloud SQL for PostgreSQL database.\n+   * @param {Embeddings} embeddings Required - Text embedding model to use.\n+   * @param {string} tableName Required - Name of an existing table or table to be created.\n+   * @param {string} schemaName Database schema name of the table. Defaults to \"public\".\n+   * @param {string} contentColumn Column that represent a Document's page_content. Defaults to \"content\".\n+   * @param {string} embeddingColumn Column for embedding vectors. The embedding is generated from the document value. Defaults to \"embedding\".\n+   * @param {Array<string>} metadataColumns Column(s) that represent a document's metadata.\n+   * @param {Array<string>} ignoreMetadataColumns Optional - Column(s) to ignore in pre-existing tables for a document's metadata. Can not be used with metadata_columns.\n+   * @param {string} idColumn Column that represents the Document's id. Defaults to \"langchain_id\".\n+   * @param {string} metadataJsonColumn Optional - Column to store metadata as JSON. Defaults to \"langchain_metadata\".\n+   * @param {DistanceStrategy} distanceStrategy Distance strategy to use for vector similarity search. Defaults to COSINE_DISTANCE.\n+   * @param {number} k Number of Documents to return from search. Defaults to 4.\n+   * @param {number} fetchK Number of Documents to fetch to pass to MMR algorithm.\n+   * @param {number} lambdaMult Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.\n+   * @param {QueryOptions} indexQueryOptions Optional - Index query option.\n+   * @returns PostgresVectorStore instance.\n+   */\n+  static async create(\n+    engine: PostgresEngine,\n+    embeddings: EmbeddingsInterface,\n+    tableName: string,\n+    {\n+      schemaName = \"public\",\n+      contentColumn = \"content\",\n+      embeddingColumn = \"embedding\",\n+      metadataColumns = [],\n+      ignoreMetadataColumns,\n+      idColumn = \"langchain_id\",\n+      metadataJsonColumn = \"langchain_metadata\",\n+      distanceStrategy = DEFAULT_DISTANCE_STRATEGY,\n+      k = 4,\n+      fetchK = 20,\n+      lambdaMult = 0.5,\n+      indexQueryOptions\n+    }: PostgresVectorStoreArgs = {}\n+  ): Promise<PostgresVectorStore> {\n+\n+    if (metadataColumns !== undefined && ignoreMetadataColumns !== undefined) {\n+      throw Error(\"Can not use both metadata_columns and ignore_metadata_columns.\");\n+    }\n+\n+    const { rows } = await engine.pool.raw(`SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '${tableName}' AND table_schema = '${schemaName}'`);\n+    const columns: { [key: string] } = {};\n+\n+    for (const index in rows) {\n+      if (rows[index]) {\n+        const row = rows[index];\n+        columns[row.column_name] = row.data_type\n+      }\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, idColumn)) {\n+      throw Error(`Id column: ${idColumn}, does not exist.`);\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, contentColumn)) {\n+      throw Error(`Content column: ${contentColumn}, does not exist.`);\n+    }\n+\n+    const contentType = columns[contentColumn];\n+\n+    if (contentType !== \"text\" && !contentType.includes(\"char\")) {\n+      throw Error(`Content column: ${contentColumn}, is type: ${contentType}. It must be a type of character string.`);\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, embeddingColumn)) {\n+      throw Error(`Embedding column: ${embeddingColumn}, does not exist.`);\n+    }\n+\n+    if (columns[embeddingColumn] !== \"USER-DEFINED\") {\n+      throw Error(`Embedding column: ${embeddingColumn} is not of type Vector.`);\n+    }\n+\n+    const jsonColumn = Object.prototype.hasOwnProperty.call(columns, metadataJsonColumn) ? metadataJsonColumn : \"\";\n+\n+    for (const column of metadataColumns) {\n+      if (!Object.prototype.hasOwnProperty.call(columns, column)) {\n+        throw Error(`Metadata column: ${column}, does not exist.`);\n+      }\n+    }\n+\n+    const allColumns = columns;\n+    let allMetadataColumns: string[];\n+    if (ignoreMetadataColumns !== undefined && ignoreMetadataColumns.length > 0) {\n+      for (const column of ignoreMetadataColumns) {\n+        delete allColumns[column];\n+      }\n+\n+      delete allColumns[idColumn];\n+      delete allColumns[contentColumn];\n+      delete allColumns[embeddingColumn];\n+      allMetadataColumns = Object.keys(allColumns);\n+    }\n+    return new PostgresVectorStore(\n+      embeddings,\n+      {\n+        engine,\n+        tableName,\n+        schemaName,\n+        contentColumn,\n+        embeddingColumn,\n+        metadataColumns: allMetadataColumns,\n+        ignoreMetadataColumns,\n+        idColumn,\n+        metadataJsonColumn: jsonColumn,\n+        distanceStrategy,\n+        k,\n+        fetchK,\n+        lambdaMult,\n+        indexQueryOptions\n+      }\n+    )\n+  }\n+\n+  static async fromTexts(texts: string[], metadatas: object[] | object, embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n+    const documents: Document[] = [];\n+\n+    for (let i = 0; i < texts.length; i += 1) {\n+      const doc = new Document({\n+        pageContent: texts[i],\n+        metadata: Array.isArray(metadatas) ? metadatas[i] : metadatas\n+      })\n+      documents.push(doc);\n+    }\n+\n+    return PostgresVectorStore.fromDocuments(documents, embeddings, dbConfig);\n+  }\n+\n+  static async fromDocuments(docs: Document[], embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n+    const { engine } = dbConfig;\n+    const { tableName } = dbConfig;\n+    const config = dbConfig.dbConfig;\n+    const vectorStore = await this.create(engine, embeddings, tableName, config);\n+\n+    await vectorStore.addDocuments(docs)\n+\n+    return vectorStore;\n+  }\n+\n+  async addVectors(vectors: number[][], documents: Document[], options?: { ids?: string[] }): Promise<string[] | void> {\n+    let ids: string[] = [];\n+    const metadatas: Record[] = []\n+\n+    if (vectors.length !== documents.length) {\n+      throw new Error(\"The number of vectors must match the number of documents provided.\");\n+    }\n+\n+    if (options?.ids) {\n+      ids = options.ids;\n+    } else {\n+      documents.forEach(document => {\n+        if (document.id !== undefined) {\n+          ids.push(document.id);\n+        } else {\n+          ids.push(uuidv4());\n+        }\n+      });\n+    }\n+\n+    if (options && options.ids && options.ids.length !== documents.length) {\n+      throw new Error(\"The number of ids must match the number of documents provided.\");\n+    }\n+\n+    documents.forEach(document => {\n+      metadatas.push(document.metadata)\n+    });\n+\n+    const tuples = customZip(ids, documents, vectors, metadatas);\n+\n+    // Insert embeddings\n+    for (const [id, document, embedding, metadata] of tuples) {\n+      const metadataColNames = this.metadataColumns.length > 0 ? `, \"${this.metadataColumns.join(\"\\\",\\\"\")}\"` : \"\";\n+\n+      let stmt = `INSERT INTO \"${this.schemaName}\".\"${this.tableName}\"(\"${this.idColumn}\", \"${this.contentColumn}\", \"${this.embeddingColumn}\" ${metadataColNames}`\n+      const values: { [key: string] } = {\n+        id,\n+        content: document.pageContent,\n+        embedding: `[${embedding.toString()}]`\n+      }\n+      let valuesStmt = \" VALUES (:id, :content, :embedding\";\n+\n+      // Add metadata\n+      const extra = metadata;\n+      for (const metadataColumn of this.metadataColumns) {\n+        if (Object.prototype.hasOwnProperty.call(metadata, metadataColumn)) {\n+          valuesStmt += `, :${metadataColumn}`;\n+          values[metadataColumn] = metadata[metadataColumn]\n+          delete extra[metadataColumn]\n+        } else {\n+          valuesStmt += \" ,null\"\n+        }\n+      }\n+\n+      // Add JSON column and/or close statement\n+      stmt += this.metadataJsonColumn ? `, ${this.metadataJsonColumn})` : \")\";",
        "comment_created_at": "2025-03-20T02:08:43+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "OOC why not just support `metadataJsonColumn` vs spreading metadata into other columns?",
        "pr_file_module": null
      },
      {
        "comment_id": "2008274415",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7852,
        "pr_file": "libs/langchain-google-cloud-sql-pg/src/vectorStore.ts",
        "discussion_id": "2004667909",
        "commented_code": "@@ -0,0 +1,423 @@\n+import { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { MaxMarginalRelevanceSearchOptions, VectorStore } from \"@langchain/core/vectorstores\";\n+import { Document } from \"@langchain/core/documents\";\n+import { v4 as uuidv4 } from \"uuid\";\n+import { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\n+import { DEFAULT_DISTANCE_STRATEGY, DistanceStrategy, QueryOptions } from \"./indexes.js\";\n+import PostgresEngine from \"./engine.js\";\n+import { customZip } from \"./utils/utils.js\";\n+\n+export interface PostgresVectorStoreArgs {\n+  schemaName?: string,\n+  contentColumn?: string,\n+  embeddingColumn?: string,\n+  metadataColumns?: Array<string>,\n+  idColumn?: string,\n+  distanceStrategy?: DistanceStrategy,\n+  k?: number,\n+  fetchK?: number,\n+  lambdaMult?: number,\n+  ignoreMetadataColumns?: Array<string>,\n+  metadataJsonColumn?: string,\n+  indexQueryOptions?: QueryOptions\n+}\n+\n+export interface dbConfigArgs {\n+  engine: PostgresEngine;\n+  tableName: string;\n+  dbConfig?: PostgresVectorStoreArgs;\n+}\n+\n+export class PostgresVectorStore extends VectorStore {\n+  declare FilterType: string;\n+\n+  engine: PostgresEngine;\n+\n+  embeddings: EmbeddingsInterface;\n+\n+  tableName: string;\n+\n+  schemaName: string;\n+\n+  contentColumn: string;\n+\n+  embeddingColumn: string;\n+\n+  metadataColumns: Array<string>;\n+\n+  ignoreMetadataColumns: Array<string>;\n+\n+  idColumn: string;\n+\n+  metadataJsonColumn: string;\n+\n+  distanceStrategy: DistanceStrategy;\n+\n+  k: number;\n+\n+  fetchK: number;\n+\n+  lambdaMult: number;\n+\n+  indexQueryOptions: QueryOptions;\n+\n+  /**\n+   * Initializes a new vector store with embeddings and database configuration.\n+   *\n+   * @param embeddings - Instance of `EmbeddingsInterface` used to embed queries.\n+   * @param dbConfig - Configuration settings for the database or storage system.\n+   */\n+  constructor(embeddings: EmbeddingsInterface, dbConfig: Record) {\n+    super(embeddings, dbConfig);\n+    this.embeddings = embeddings;\n+    this.engine = dbConfig.engine;\n+    this.tableName = dbConfig.tableName;\n+    this.schemaName = dbConfig.schemaName;\n+    this.contentColumn = dbConfig.contentColumn;\n+    this.embeddingColumn = dbConfig.embeddingColumn;\n+    this.metadataColumns = dbConfig.metadataColumns ? dbConfig.metadataColumns : [];\n+    this.ignoreMetadataColumns = dbConfig.ignoreMetadataColumns;\n+    this.idColumn = dbConfig.idColumn;\n+    this.metadataJsonColumn = dbConfig.metadataJsonColumn;\n+    this.distanceStrategy = dbConfig.distanceStrategy;\n+    this.k = dbConfig.k;\n+    this.fetchK = dbConfig.fetchK;\n+    this.lambdaMult = dbConfig.lambdaMult;\n+    this.indexQueryOptions = dbConfig.indexQueryOptions;\n+  }\n+\n+  /**\n+   * Create a new PostgresVectorStore instance.\n+   * @param {PostgresEngine} engine Required - Connection pool engine for managing connections to Cloud SQL for PostgreSQL database.\n+   * @param {Embeddings} embeddings Required - Text embedding model to use.\n+   * @param {string} tableName Required - Name of an existing table or table to be created.\n+   * @param {string} schemaName Database schema name of the table. Defaults to \"public\".\n+   * @param {string} contentColumn Column that represent a Document's page_content. Defaults to \"content\".\n+   * @param {string} embeddingColumn Column for embedding vectors. The embedding is generated from the document value. Defaults to \"embedding\".\n+   * @param {Array<string>} metadataColumns Column(s) that represent a document's metadata.\n+   * @param {Array<string>} ignoreMetadataColumns Optional - Column(s) to ignore in pre-existing tables for a document's metadata. Can not be used with metadata_columns.\n+   * @param {string} idColumn Column that represents the Document's id. Defaults to \"langchain_id\".\n+   * @param {string} metadataJsonColumn Optional - Column to store metadata as JSON. Defaults to \"langchain_metadata\".\n+   * @param {DistanceStrategy} distanceStrategy Distance strategy to use for vector similarity search. Defaults to COSINE_DISTANCE.\n+   * @param {number} k Number of Documents to return from search. Defaults to 4.\n+   * @param {number} fetchK Number of Documents to fetch to pass to MMR algorithm.\n+   * @param {number} lambdaMult Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.\n+   * @param {QueryOptions} indexQueryOptions Optional - Index query option.\n+   * @returns PostgresVectorStore instance.\n+   */\n+  static async create(\n+    engine: PostgresEngine,\n+    embeddings: EmbeddingsInterface,\n+    tableName: string,\n+    {\n+      schemaName = \"public\",\n+      contentColumn = \"content\",\n+      embeddingColumn = \"embedding\",\n+      metadataColumns = [],\n+      ignoreMetadataColumns,\n+      idColumn = \"langchain_id\",\n+      metadataJsonColumn = \"langchain_metadata\",\n+      distanceStrategy = DEFAULT_DISTANCE_STRATEGY,\n+      k = 4,\n+      fetchK = 20,\n+      lambdaMult = 0.5,\n+      indexQueryOptions\n+    }: PostgresVectorStoreArgs = {}\n+  ): Promise<PostgresVectorStore> {\n+\n+    if (metadataColumns !== undefined && ignoreMetadataColumns !== undefined) {\n+      throw Error(\"Can not use both metadata_columns and ignore_metadata_columns.\");\n+    }\n+\n+    const { rows } = await engine.pool.raw(`SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '${tableName}' AND table_schema = '${schemaName}'`);\n+    const columns: { [key: string] } = {};\n+\n+    for (const index in rows) {\n+      if (rows[index]) {\n+        const row = rows[index];\n+        columns[row.column_name] = row.data_type\n+      }\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, idColumn)) {\n+      throw Error(`Id column: ${idColumn}, does not exist.`);\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, contentColumn)) {\n+      throw Error(`Content column: ${contentColumn}, does not exist.`);\n+    }\n+\n+    const contentType = columns[contentColumn];\n+\n+    if (contentType !== \"text\" && !contentType.includes(\"char\")) {\n+      throw Error(`Content column: ${contentColumn}, is type: ${contentType}. It must be a type of character string.`);\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, embeddingColumn)) {\n+      throw Error(`Embedding column: ${embeddingColumn}, does not exist.`);\n+    }\n+\n+    if (columns[embeddingColumn] !== \"USER-DEFINED\") {\n+      throw Error(`Embedding column: ${embeddingColumn} is not of type Vector.`);\n+    }\n+\n+    const jsonColumn = Object.prototype.hasOwnProperty.call(columns, metadataJsonColumn) ? metadataJsonColumn : \"\";\n+\n+    for (const column of metadataColumns) {\n+      if (!Object.prototype.hasOwnProperty.call(columns, column)) {\n+        throw Error(`Metadata column: ${column}, does not exist.`);\n+      }\n+    }\n+\n+    const allColumns = columns;\n+    let allMetadataColumns: string[];\n+    if (ignoreMetadataColumns !== undefined && ignoreMetadataColumns.length > 0) {\n+      for (const column of ignoreMetadataColumns) {\n+        delete allColumns[column];\n+      }\n+\n+      delete allColumns[idColumn];\n+      delete allColumns[contentColumn];\n+      delete allColumns[embeddingColumn];\n+      allMetadataColumns = Object.keys(allColumns);\n+    }\n+    return new PostgresVectorStore(\n+      embeddings,\n+      {\n+        engine,\n+        tableName,\n+        schemaName,\n+        contentColumn,\n+        embeddingColumn,\n+        metadataColumns: allMetadataColumns,\n+        ignoreMetadataColumns,\n+        idColumn,\n+        metadataJsonColumn: jsonColumn,\n+        distanceStrategy,\n+        k,\n+        fetchK,\n+        lambdaMult,\n+        indexQueryOptions\n+      }\n+    )\n+  }\n+\n+  static async fromTexts(texts: string[], metadatas: object[] | object, embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n+    const documents: Document[] = [];\n+\n+    for (let i = 0; i < texts.length; i += 1) {\n+      const doc = new Document({\n+        pageContent: texts[i],\n+        metadata: Array.isArray(metadatas) ? metadatas[i] : metadatas\n+      })\n+      documents.push(doc);\n+    }\n+\n+    return PostgresVectorStore.fromDocuments(documents, embeddings, dbConfig);\n+  }\n+\n+  static async fromDocuments(docs: Document[], embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n+    const { engine } = dbConfig;\n+    const { tableName } = dbConfig;\n+    const config = dbConfig.dbConfig;\n+    const vectorStore = await this.create(engine, embeddings, tableName, config);\n+\n+    await vectorStore.addDocuments(docs)\n+\n+    return vectorStore;\n+  }\n+\n+  async addVectors(vectors: number[][], documents: Document[], options?: { ids?: string[] }): Promise<string[] | void> {\n+    let ids: string[] = [];\n+    const metadatas: Record[] = []\n+\n+    if (vectors.length !== documents.length) {\n+      throw new Error(\"The number of vectors must match the number of documents provided.\");\n+    }\n+\n+    if (options?.ids) {\n+      ids = options.ids;\n+    } else {\n+      documents.forEach(document => {\n+        if (document.id !== undefined) {\n+          ids.push(document.id);\n+        } else {\n+          ids.push(uuidv4());\n+        }\n+      });\n+    }\n+\n+    if (options && options.ids && options.ids.length !== documents.length) {\n+      throw new Error(\"The number of ids must match the number of documents provided.\");\n+    }\n+\n+    documents.forEach(document => {\n+      metadatas.push(document.metadata)\n+    });\n+\n+    const tuples = customZip(ids, documents, vectors, metadatas);\n+\n+    // Insert embeddings\n+    for (const [id, document, embedding, metadata] of tuples) {\n+      const metadataColNames = this.metadataColumns.length > 0 ? `, \"${this.metadataColumns.join(\"\\\",\\\"\")}\"` : \"\";\n+\n+      let stmt = `INSERT INTO \"${this.schemaName}\".\"${this.tableName}\"(\"${this.idColumn}\", \"${this.contentColumn}\", \"${this.embeddingColumn}\" ${metadataColNames}`\n+      const values: { [key: string] } = {\n+        id,\n+        content: document.pageContent,\n+        embedding: `[${embedding.toString()}]`\n+      }\n+      let valuesStmt = \" VALUES (:id, :content, :embedding\";\n+\n+      // Add metadata\n+      const extra = metadata;\n+      for (const metadataColumn of this.metadataColumns) {\n+        if (Object.prototype.hasOwnProperty.call(metadata, metadataColumn)) {\n+          valuesStmt += `, :${metadataColumn}`;\n+          values[metadataColumn] = metadata[metadataColumn]\n+          delete extra[metadataColumn]\n+        } else {\n+          valuesStmt += \" ,null\"\n+        }\n+      }\n+\n+      // Add JSON column and/or close statement\n+      stmt += this.metadataJsonColumn ? `, ${this.metadataJsonColumn})` : \")\";",
        "comment_created_at": "2025-03-21T20:14:26+00:00",
        "comment_author": "averikitsch",
        "comment_body": "This pattern follows database best practices for relational databases by using individual columns versus all data in a JSON column. This allows for low latency filtering and type checking. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2009500129",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7852,
        "pr_file": "libs/langchain-google-cloud-sql-pg/src/vectorStore.ts",
        "discussion_id": "2004667909",
        "commented_code": "@@ -0,0 +1,423 @@\n+import { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { MaxMarginalRelevanceSearchOptions, VectorStore } from \"@langchain/core/vectorstores\";\n+import { Document } from \"@langchain/core/documents\";\n+import { v4 as uuidv4 } from \"uuid\";\n+import { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\n+import { DEFAULT_DISTANCE_STRATEGY, DistanceStrategy, QueryOptions } from \"./indexes.js\";\n+import PostgresEngine from \"./engine.js\";\n+import { customZip } from \"./utils/utils.js\";\n+\n+export interface PostgresVectorStoreArgs {\n+  schemaName?: string,\n+  contentColumn?: string,\n+  embeddingColumn?: string,\n+  metadataColumns?: Array<string>,\n+  idColumn?: string,\n+  distanceStrategy?: DistanceStrategy,\n+  k?: number,\n+  fetchK?: number,\n+  lambdaMult?: number,\n+  ignoreMetadataColumns?: Array<string>,\n+  metadataJsonColumn?: string,\n+  indexQueryOptions?: QueryOptions\n+}\n+\n+export interface dbConfigArgs {\n+  engine: PostgresEngine;\n+  tableName: string;\n+  dbConfig?: PostgresVectorStoreArgs;\n+}\n+\n+export class PostgresVectorStore extends VectorStore {\n+  declare FilterType: string;\n+\n+  engine: PostgresEngine;\n+\n+  embeddings: EmbeddingsInterface;\n+\n+  tableName: string;\n+\n+  schemaName: string;\n+\n+  contentColumn: string;\n+\n+  embeddingColumn: string;\n+\n+  metadataColumns: Array<string>;\n+\n+  ignoreMetadataColumns: Array<string>;\n+\n+  idColumn: string;\n+\n+  metadataJsonColumn: string;\n+\n+  distanceStrategy: DistanceStrategy;\n+\n+  k: number;\n+\n+  fetchK: number;\n+\n+  lambdaMult: number;\n+\n+  indexQueryOptions: QueryOptions;\n+\n+  /**\n+   * Initializes a new vector store with embeddings and database configuration.\n+   *\n+   * @param embeddings - Instance of `EmbeddingsInterface` used to embed queries.\n+   * @param dbConfig - Configuration settings for the database or storage system.\n+   */\n+  constructor(embeddings: EmbeddingsInterface, dbConfig: Record) {\n+    super(embeddings, dbConfig);\n+    this.embeddings = embeddings;\n+    this.engine = dbConfig.engine;\n+    this.tableName = dbConfig.tableName;\n+    this.schemaName = dbConfig.schemaName;\n+    this.contentColumn = dbConfig.contentColumn;\n+    this.embeddingColumn = dbConfig.embeddingColumn;\n+    this.metadataColumns = dbConfig.metadataColumns ? dbConfig.metadataColumns : [];\n+    this.ignoreMetadataColumns = dbConfig.ignoreMetadataColumns;\n+    this.idColumn = dbConfig.idColumn;\n+    this.metadataJsonColumn = dbConfig.metadataJsonColumn;\n+    this.distanceStrategy = dbConfig.distanceStrategy;\n+    this.k = dbConfig.k;\n+    this.fetchK = dbConfig.fetchK;\n+    this.lambdaMult = dbConfig.lambdaMult;\n+    this.indexQueryOptions = dbConfig.indexQueryOptions;\n+  }\n+\n+  /**\n+   * Create a new PostgresVectorStore instance.\n+   * @param {PostgresEngine} engine Required - Connection pool engine for managing connections to Cloud SQL for PostgreSQL database.\n+   * @param {Embeddings} embeddings Required - Text embedding model to use.\n+   * @param {string} tableName Required - Name of an existing table or table to be created.\n+   * @param {string} schemaName Database schema name of the table. Defaults to \"public\".\n+   * @param {string} contentColumn Column that represent a Document's page_content. Defaults to \"content\".\n+   * @param {string} embeddingColumn Column for embedding vectors. The embedding is generated from the document value. Defaults to \"embedding\".\n+   * @param {Array<string>} metadataColumns Column(s) that represent a document's metadata.\n+   * @param {Array<string>} ignoreMetadataColumns Optional - Column(s) to ignore in pre-existing tables for a document's metadata. Can not be used with metadata_columns.\n+   * @param {string} idColumn Column that represents the Document's id. Defaults to \"langchain_id\".\n+   * @param {string} metadataJsonColumn Optional - Column to store metadata as JSON. Defaults to \"langchain_metadata\".\n+   * @param {DistanceStrategy} distanceStrategy Distance strategy to use for vector similarity search. Defaults to COSINE_DISTANCE.\n+   * @param {number} k Number of Documents to return from search. Defaults to 4.\n+   * @param {number} fetchK Number of Documents to fetch to pass to MMR algorithm.\n+   * @param {number} lambdaMult Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.\n+   * @param {QueryOptions} indexQueryOptions Optional - Index query option.\n+   * @returns PostgresVectorStore instance.\n+   */\n+  static async create(\n+    engine: PostgresEngine,\n+    embeddings: EmbeddingsInterface,\n+    tableName: string,\n+    {\n+      schemaName = \"public\",\n+      contentColumn = \"content\",\n+      embeddingColumn = \"embedding\",\n+      metadataColumns = [],\n+      ignoreMetadataColumns,\n+      idColumn = \"langchain_id\",\n+      metadataJsonColumn = \"langchain_metadata\",\n+      distanceStrategy = DEFAULT_DISTANCE_STRATEGY,\n+      k = 4,\n+      fetchK = 20,\n+      lambdaMult = 0.5,\n+      indexQueryOptions\n+    }: PostgresVectorStoreArgs = {}\n+  ): Promise<PostgresVectorStore> {\n+\n+    if (metadataColumns !== undefined && ignoreMetadataColumns !== undefined) {\n+      throw Error(\"Can not use both metadata_columns and ignore_metadata_columns.\");\n+    }\n+\n+    const { rows } = await engine.pool.raw(`SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '${tableName}' AND table_schema = '${schemaName}'`);\n+    const columns: { [key: string] } = {};\n+\n+    for (const index in rows) {\n+      if (rows[index]) {\n+        const row = rows[index];\n+        columns[row.column_name] = row.data_type\n+      }\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, idColumn)) {\n+      throw Error(`Id column: ${idColumn}, does not exist.`);\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, contentColumn)) {\n+      throw Error(`Content column: ${contentColumn}, does not exist.`);\n+    }\n+\n+    const contentType = columns[contentColumn];\n+\n+    if (contentType !== \"text\" && !contentType.includes(\"char\")) {\n+      throw Error(`Content column: ${contentColumn}, is type: ${contentType}. It must be a type of character string.`);\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, embeddingColumn)) {\n+      throw Error(`Embedding column: ${embeddingColumn}, does not exist.`);\n+    }\n+\n+    if (columns[embeddingColumn] !== \"USER-DEFINED\") {\n+      throw Error(`Embedding column: ${embeddingColumn} is not of type Vector.`);\n+    }\n+\n+    const jsonColumn = Object.prototype.hasOwnProperty.call(columns, metadataJsonColumn) ? metadataJsonColumn : \"\";\n+\n+    for (const column of metadataColumns) {\n+      if (!Object.prototype.hasOwnProperty.call(columns, column)) {\n+        throw Error(`Metadata column: ${column}, does not exist.`);\n+      }\n+    }\n+\n+    const allColumns = columns;\n+    let allMetadataColumns: string[];\n+    if (ignoreMetadataColumns !== undefined && ignoreMetadataColumns.length > 0) {\n+      for (const column of ignoreMetadataColumns) {\n+        delete allColumns[column];\n+      }\n+\n+      delete allColumns[idColumn];\n+      delete allColumns[contentColumn];\n+      delete allColumns[embeddingColumn];\n+      allMetadataColumns = Object.keys(allColumns);\n+    }\n+    return new PostgresVectorStore(\n+      embeddings,\n+      {\n+        engine,\n+        tableName,\n+        schemaName,\n+        contentColumn,\n+        embeddingColumn,\n+        metadataColumns: allMetadataColumns,\n+        ignoreMetadataColumns,\n+        idColumn,\n+        metadataJsonColumn: jsonColumn,\n+        distanceStrategy,\n+        k,\n+        fetchK,\n+        lambdaMult,\n+        indexQueryOptions\n+      }\n+    )\n+  }\n+\n+  static async fromTexts(texts: string[], metadatas: object[] | object, embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n+    const documents: Document[] = [];\n+\n+    for (let i = 0; i < texts.length; i += 1) {\n+      const doc = new Document({\n+        pageContent: texts[i],\n+        metadata: Array.isArray(metadatas) ? metadatas[i] : metadatas\n+      })\n+      documents.push(doc);\n+    }\n+\n+    return PostgresVectorStore.fromDocuments(documents, embeddings, dbConfig);\n+  }\n+\n+  static async fromDocuments(docs: Document[], embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n+    const { engine } = dbConfig;\n+    const { tableName } = dbConfig;\n+    const config = dbConfig.dbConfig;\n+    const vectorStore = await this.create(engine, embeddings, tableName, config);\n+\n+    await vectorStore.addDocuments(docs)\n+\n+    return vectorStore;\n+  }\n+\n+  async addVectors(vectors: number[][], documents: Document[], options?: { ids?: string[] }): Promise<string[] | void> {\n+    let ids: string[] = [];\n+    const metadatas: Record[] = []\n+\n+    if (vectors.length !== documents.length) {\n+      throw new Error(\"The number of vectors must match the number of documents provided.\");\n+    }\n+\n+    if (options?.ids) {\n+      ids = options.ids;\n+    } else {\n+      documents.forEach(document => {\n+        if (document.id !== undefined) {\n+          ids.push(document.id);\n+        } else {\n+          ids.push(uuidv4());\n+        }\n+      });\n+    }\n+\n+    if (options && options.ids && options.ids.length !== documents.length) {\n+      throw new Error(\"The number of ids must match the number of documents provided.\");\n+    }\n+\n+    documents.forEach(document => {\n+      metadatas.push(document.metadata)\n+    });\n+\n+    const tuples = customZip(ids, documents, vectors, metadatas);\n+\n+    // Insert embeddings\n+    for (const [id, document, embedding, metadata] of tuples) {\n+      const metadataColNames = this.metadataColumns.length > 0 ? `, \"${this.metadataColumns.join(\"\\\",\\\"\")}\"` : \"\";\n+\n+      let stmt = `INSERT INTO \"${this.schemaName}\".\"${this.tableName}\"(\"${this.idColumn}\", \"${this.contentColumn}\", \"${this.embeddingColumn}\" ${metadataColNames}`\n+      const values: { [key: string] } = {\n+        id,\n+        content: document.pageContent,\n+        embedding: `[${embedding.toString()}]`\n+      }\n+      let valuesStmt = \" VALUES (:id, :content, :embedding\";\n+\n+      // Add metadata\n+      const extra = metadata;\n+      for (const metadataColumn of this.metadataColumns) {\n+        if (Object.prototype.hasOwnProperty.call(metadata, metadataColumn)) {\n+          valuesStmt += `, :${metadataColumn}`;\n+          values[metadataColumn] = metadata[metadataColumn]\n+          delete extra[metadataColumn]\n+        } else {\n+          valuesStmt += \" ,null\"\n+        }\n+      }\n+\n+      // Add JSON column and/or close statement\n+      stmt += this.metadataJsonColumn ? `, ${this.metadataJsonColumn})` : \")\";",
        "comment_created_at": "2025-03-24T05:49:51+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "And the JSON column is just to make onboarding easier?",
        "pr_file_module": null
      },
      {
        "comment_id": "2017533960",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7852,
        "pr_file": "libs/langchain-google-cloud-sql-pg/src/vectorStore.ts",
        "discussion_id": "2004667909",
        "commented_code": "@@ -0,0 +1,423 @@\n+import { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { MaxMarginalRelevanceSearchOptions, VectorStore } from \"@langchain/core/vectorstores\";\n+import { Document } from \"@langchain/core/documents\";\n+import { v4 as uuidv4 } from \"uuid\";\n+import { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\n+import { DEFAULT_DISTANCE_STRATEGY, DistanceStrategy, QueryOptions } from \"./indexes.js\";\n+import PostgresEngine from \"./engine.js\";\n+import { customZip } from \"./utils/utils.js\";\n+\n+export interface PostgresVectorStoreArgs {\n+  schemaName?: string,\n+  contentColumn?: string,\n+  embeddingColumn?: string,\n+  metadataColumns?: Array<string>,\n+  idColumn?: string,\n+  distanceStrategy?: DistanceStrategy,\n+  k?: number,\n+  fetchK?: number,\n+  lambdaMult?: number,\n+  ignoreMetadataColumns?: Array<string>,\n+  metadataJsonColumn?: string,\n+  indexQueryOptions?: QueryOptions\n+}\n+\n+export interface dbConfigArgs {\n+  engine: PostgresEngine;\n+  tableName: string;\n+  dbConfig?: PostgresVectorStoreArgs;\n+}\n+\n+export class PostgresVectorStore extends VectorStore {\n+  declare FilterType: string;\n+\n+  engine: PostgresEngine;\n+\n+  embeddings: EmbeddingsInterface;\n+\n+  tableName: string;\n+\n+  schemaName: string;\n+\n+  contentColumn: string;\n+\n+  embeddingColumn: string;\n+\n+  metadataColumns: Array<string>;\n+\n+  ignoreMetadataColumns: Array<string>;\n+\n+  idColumn: string;\n+\n+  metadataJsonColumn: string;\n+\n+  distanceStrategy: DistanceStrategy;\n+\n+  k: number;\n+\n+  fetchK: number;\n+\n+  lambdaMult: number;\n+\n+  indexQueryOptions: QueryOptions;\n+\n+  /**\n+   * Initializes a new vector store with embeddings and database configuration.\n+   *\n+   * @param embeddings - Instance of `EmbeddingsInterface` used to embed queries.\n+   * @param dbConfig - Configuration settings for the database or storage system.\n+   */\n+  constructor(embeddings: EmbeddingsInterface, dbConfig: Record) {\n+    super(embeddings, dbConfig);\n+    this.embeddings = embeddings;\n+    this.engine = dbConfig.engine;\n+    this.tableName = dbConfig.tableName;\n+    this.schemaName = dbConfig.schemaName;\n+    this.contentColumn = dbConfig.contentColumn;\n+    this.embeddingColumn = dbConfig.embeddingColumn;\n+    this.metadataColumns = dbConfig.metadataColumns ? dbConfig.metadataColumns : [];\n+    this.ignoreMetadataColumns = dbConfig.ignoreMetadataColumns;\n+    this.idColumn = dbConfig.idColumn;\n+    this.metadataJsonColumn = dbConfig.metadataJsonColumn;\n+    this.distanceStrategy = dbConfig.distanceStrategy;\n+    this.k = dbConfig.k;\n+    this.fetchK = dbConfig.fetchK;\n+    this.lambdaMult = dbConfig.lambdaMult;\n+    this.indexQueryOptions = dbConfig.indexQueryOptions;\n+  }\n+\n+  /**\n+   * Create a new PostgresVectorStore instance.\n+   * @param {PostgresEngine} engine Required - Connection pool engine for managing connections to Cloud SQL for PostgreSQL database.\n+   * @param {Embeddings} embeddings Required - Text embedding model to use.\n+   * @param {string} tableName Required - Name of an existing table or table to be created.\n+   * @param {string} schemaName Database schema name of the table. Defaults to \"public\".\n+   * @param {string} contentColumn Column that represent a Document's page_content. Defaults to \"content\".\n+   * @param {string} embeddingColumn Column for embedding vectors. The embedding is generated from the document value. Defaults to \"embedding\".\n+   * @param {Array<string>} metadataColumns Column(s) that represent a document's metadata.\n+   * @param {Array<string>} ignoreMetadataColumns Optional - Column(s) to ignore in pre-existing tables for a document's metadata. Can not be used with metadata_columns.\n+   * @param {string} idColumn Column that represents the Document's id. Defaults to \"langchain_id\".\n+   * @param {string} metadataJsonColumn Optional - Column to store metadata as JSON. Defaults to \"langchain_metadata\".\n+   * @param {DistanceStrategy} distanceStrategy Distance strategy to use for vector similarity search. Defaults to COSINE_DISTANCE.\n+   * @param {number} k Number of Documents to return from search. Defaults to 4.\n+   * @param {number} fetchK Number of Documents to fetch to pass to MMR algorithm.\n+   * @param {number} lambdaMult Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.\n+   * @param {QueryOptions} indexQueryOptions Optional - Index query option.\n+   * @returns PostgresVectorStore instance.\n+   */\n+  static async create(\n+    engine: PostgresEngine,\n+    embeddings: EmbeddingsInterface,\n+    tableName: string,\n+    {\n+      schemaName = \"public\",\n+      contentColumn = \"content\",\n+      embeddingColumn = \"embedding\",\n+      metadataColumns = [],\n+      ignoreMetadataColumns,\n+      idColumn = \"langchain_id\",\n+      metadataJsonColumn = \"langchain_metadata\",\n+      distanceStrategy = DEFAULT_DISTANCE_STRATEGY,\n+      k = 4,\n+      fetchK = 20,\n+      lambdaMult = 0.5,\n+      indexQueryOptions\n+    }: PostgresVectorStoreArgs = {}\n+  ): Promise<PostgresVectorStore> {\n+\n+    if (metadataColumns !== undefined && ignoreMetadataColumns !== undefined) {\n+      throw Error(\"Can not use both metadata_columns and ignore_metadata_columns.\");\n+    }\n+\n+    const { rows } = await engine.pool.raw(`SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '${tableName}' AND table_schema = '${schemaName}'`);\n+    const columns: { [key: string] } = {};\n+\n+    for (const index in rows) {\n+      if (rows[index]) {\n+        const row = rows[index];\n+        columns[row.column_name] = row.data_type\n+      }\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, idColumn)) {\n+      throw Error(`Id column: ${idColumn}, does not exist.`);\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, contentColumn)) {\n+      throw Error(`Content column: ${contentColumn}, does not exist.`);\n+    }\n+\n+    const contentType = columns[contentColumn];\n+\n+    if (contentType !== \"text\" && !contentType.includes(\"char\")) {\n+      throw Error(`Content column: ${contentColumn}, is type: ${contentType}. It must be a type of character string.`);\n+    }\n+\n+    if (!Object.prototype.hasOwnProperty.call(columns, embeddingColumn)) {\n+      throw Error(`Embedding column: ${embeddingColumn}, does not exist.`);\n+    }\n+\n+    if (columns[embeddingColumn] !== \"USER-DEFINED\") {\n+      throw Error(`Embedding column: ${embeddingColumn} is not of type Vector.`);\n+    }\n+\n+    const jsonColumn = Object.prototype.hasOwnProperty.call(columns, metadataJsonColumn) ? metadataJsonColumn : \"\";\n+\n+    for (const column of metadataColumns) {\n+      if (!Object.prototype.hasOwnProperty.call(columns, column)) {\n+        throw Error(`Metadata column: ${column}, does not exist.`);\n+      }\n+    }\n+\n+    const allColumns = columns;\n+    let allMetadataColumns: string[];\n+    if (ignoreMetadataColumns !== undefined && ignoreMetadataColumns.length > 0) {\n+      for (const column of ignoreMetadataColumns) {\n+        delete allColumns[column];\n+      }\n+\n+      delete allColumns[idColumn];\n+      delete allColumns[contentColumn];\n+      delete allColumns[embeddingColumn];\n+      allMetadataColumns = Object.keys(allColumns);\n+    }\n+    return new PostgresVectorStore(\n+      embeddings,\n+      {\n+        engine,\n+        tableName,\n+        schemaName,\n+        contentColumn,\n+        embeddingColumn,\n+        metadataColumns: allMetadataColumns,\n+        ignoreMetadataColumns,\n+        idColumn,\n+        metadataJsonColumn: jsonColumn,\n+        distanceStrategy,\n+        k,\n+        fetchK,\n+        lambdaMult,\n+        indexQueryOptions\n+      }\n+    )\n+  }\n+\n+  static async fromTexts(texts: string[], metadatas: object[] | object, embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n+    const documents: Document[] = [];\n+\n+    for (let i = 0; i < texts.length; i += 1) {\n+      const doc = new Document({\n+        pageContent: texts[i],\n+        metadata: Array.isArray(metadatas) ? metadatas[i] : metadatas\n+      })\n+      documents.push(doc);\n+    }\n+\n+    return PostgresVectorStore.fromDocuments(documents, embeddings, dbConfig);\n+  }\n+\n+  static async fromDocuments(docs: Document[], embeddings: EmbeddingsInterface, dbConfig: dbConfigArgs): Promise<VectorStore> {\n+    const { engine } = dbConfig;\n+    const { tableName } = dbConfig;\n+    const config = dbConfig.dbConfig;\n+    const vectorStore = await this.create(engine, embeddings, tableName, config);\n+\n+    await vectorStore.addDocuments(docs)\n+\n+    return vectorStore;\n+  }\n+\n+  async addVectors(vectors: number[][], documents: Document[], options?: { ids?: string[] }): Promise<string[] | void> {\n+    let ids: string[] = [];\n+    const metadatas: Record[] = []\n+\n+    if (vectors.length !== documents.length) {\n+      throw new Error(\"The number of vectors must match the number of documents provided.\");\n+    }\n+\n+    if (options?.ids) {\n+      ids = options.ids;\n+    } else {\n+      documents.forEach(document => {\n+        if (document.id !== undefined) {\n+          ids.push(document.id);\n+        } else {\n+          ids.push(uuidv4());\n+        }\n+      });\n+    }\n+\n+    if (options && options.ids && options.ids.length !== documents.length) {\n+      throw new Error(\"The number of ids must match the number of documents provided.\");\n+    }\n+\n+    documents.forEach(document => {\n+      metadatas.push(document.metadata)\n+    });\n+\n+    const tuples = customZip(ids, documents, vectors, metadatas);\n+\n+    // Insert embeddings\n+    for (const [id, document, embedding, metadata] of tuples) {\n+      const metadataColNames = this.metadataColumns.length > 0 ? `, \"${this.metadataColumns.join(\"\\\",\\\"\")}\"` : \"\";\n+\n+      let stmt = `INSERT INTO \"${this.schemaName}\".\"${this.tableName}\"(\"${this.idColumn}\", \"${this.contentColumn}\", \"${this.embeddingColumn}\" ${metadataColNames}`\n+      const values: { [key: string] } = {\n+        id,\n+        content: document.pageContent,\n+        embedding: `[${embedding.toString()}]`\n+      }\n+      let valuesStmt = \" VALUES (:id, :content, :embedding\";\n+\n+      // Add metadata\n+      const extra = metadata;\n+      for (const metadataColumn of this.metadataColumns) {\n+        if (Object.prototype.hasOwnProperty.call(metadata, metadataColumn)) {\n+          valuesStmt += `, :${metadataColumn}`;\n+          values[metadataColumn] = metadata[metadataColumn]\n+          delete extra[metadataColumn]\n+        } else {\n+          valuesStmt += \" ,null\"\n+        }\n+      }\n+\n+      // Add JSON column and/or close statement\n+      stmt += this.metadataJsonColumn ? `, ${this.metadataJsonColumn})` : \")\";",
        "comment_created_at": "2025-03-27T20:07:53+00:00",
        "comment_author": "averikitsch",
        "comment_body": "Yes and we don't want users to lose metadata if not specifically defined ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1456406645",
    "pr_number": 4044,
    "pr_file": "libs/langchain-community/src/vectorstores/azure_aisearch.ts",
    "created_at": "2024-01-17T19:58:12+00:00",
    "commented_code": "import * as uuid from \"uuid\";\nimport {\n  SearchClient,\n  SearchIndexClient,\n  AzureKeyCredential,\n  IndexingResult,\n  SearchIndex,\n} from \"@azure/search-documents\";\nimport {\n  MaxMarginalRelevanceSearchOptions,\n  VectorStore,\n} from \"@langchain/core/vectorstores\";\nimport type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\nimport { Document } from \"@langchain/core/documents\";\nimport { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n\n/**\n * Azure AI Search query type.\n */\nexport const AzureAISearchQueryType = {\n  /** Vector search. */\n  Similarity: \"similarity\",\n  /** Hybrid full text and vector search. */\n  SimilarityHybrid: \"similarity_hybrid\",\n  /** Hybrid full text and vector search with semantic ranking. */\n  SemanticHybrid: \"semantic_hybrid\",\n} as const;\n\n/**\n * Azure AI Search query type.\n */\nexport type AzureAISearchQueryType =\n  (typeof AzureAISearchQueryType)[keyof typeof AzureAISearchQueryType];\n\n/**\n * Azure AI Search settings.\n */\nexport interface AzureAISearchQueryOptions {\n  readonly type: AzureAISearchQueryType;\n  readonly semanticConfigurationName?: string;\n}\n\n/**\n * Configuration options for the `AzureAISearchStore` constructor.\n */\nexport interface AzureAISearchConfig {\n  readonly client?: SearchClient<AzureAISearchDocument>;\n  readonly indexName?: string;\n  readonly endpoint?: string;\n  readonly key?: string;\n  readonly search: AzureAISearchQueryOptions;\n  /**\n   * The amount of documents to chunk by when adding vectors.\n   * @default 100\n   */\n  readonly chunkSize?: number;\n  /**\n   * The amount of documents to embed at once when adding documents.\n   * Note that some providers like Azure OpenAI can only embed 16 documents\n   * at a time.\n   * @default 16\n   */\n  readonly embeddingBatchSize?: number;\n}\n\n/**\n * Azure AI Search options metadata schema.\n * If yout want to add custom data, use the attributes property.\n */\nexport type AzureAISearchDocumentMetadata = {\n  source: string;\n  attributes?: Array<{ key: string; value: string }>;\n};\n\n/**\n * Azure AI Search indexed document.\n */\nexport type AzureAISearchDocument = {\n  id: string;\n  content: string;\n  content_vector: number[];\n  metadata: AzureAISearchDocumentMetadata;\n};\n\n/**\n * Azure AI Search options for adding documents.\n */\nexport type AzureAISearchAddDocumentsOptions = {\n  ids?: string[];\n};\n\nconst DEFAULT_FIELD_ID = \"id\";\nconst DEFAULT_FIELD_CONTENT = \"content\";\nconst DEFAULT_FIELD_CONTENT_VECTOR = \"content_vector\";\nconst DEFAULT_FIELD_METADATA = \"metadata\";\nconst DEFAULT_FIELD_METADATA_SOURCE = \"source\";\nconst DEFAULT_FIELD_METADATA_ATTRS = \"attributes\";\n\n/**\n * Azure AI Search vector store.\n * To use this, you should have:\n * - the `@azure/search-documents` NPM package installed\n * - an endpoint and key to the Azure AI Search instance\n *\n * If you directly provide a `SearchClient` instance, you need to ensure that\n * an index has been created. When using and endpoint and key, the index will\n * be created automatically if it does not exist.\n */\nexport class AzureAISearchVectorStore extends VectorStore {\n  declare FilterType: string;\n\n  get lc_secrets(): { [key: string]: string } {\n    return {\n      endpoint: \"AZURE_AISEARCH_ENDPOINT\",\n      key: \"AZURE_AISEARCH_KEY\",\n    };\n  }\n\n  _vectorstoreType(): string {\n    return \"azure_aisearch\";\n  }\n\n  private readonly initPromise: Promise<void>;\n\n  private readonly client: SearchClient<AzureAISearchDocument>;\n\n  private readonly indexName: string;\n\n  private readonly chunkSize: number;\n\n  private readonly embeddingBatchSize: number;\n\n  private readonly options: AzureAISearchQueryOptions;\n\n  constructor(embeddings: EmbeddingsInterface, config: AzureAISearchConfig) {\n    super(embeddings, config);\n\n    const endpoint =\n      config.endpoint ?? getEnvironmentVariable(\"AZURE_AISEARCH_ENDPOINT\");\n    const key = config.key ?? getEnvironmentVariable(\"AZURE_AISEARCH_KEY\");\n\n    if (!config.client && !endpoint && !key) {\n      throw new Error(\n        \"Azure AI Search client or connection string must be set.\"\n      );\n    }\n\n    this.indexName = config.indexName ?? \"vectorsearch\";\n    this.chunkSize = config.chunkSize ?? 100;\n    this.embeddingBatchSize = config.embeddingBatchSize ?? 16;\n\n    if (!config.client) {\n      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n      const credential = new AzureKeyCredential(key!);\n      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n      this.client = new SearchClient(endpoint!, this.indexName, credential);\n      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n      const indexClient = new SearchIndexClient(endpoint!, credential);\n\n      // Start initialization, but don't wait for it to finish here\n      this.initPromise = this.ensureIndexExists(indexClient).catch((error) => {\n        console.error(\n          \"Error during Azure AI Search index initialization:\",\n          error\n        );\n      });\n    } else {\n      this.client = config.client;\n    }\n\n    this.options = config.search;\n    this.embeddings = embeddings;\n  }\n\n  /**\n   * Removes specified documents from the AzureAISearchVectorStore using a filter.\n   * @param filter OData filter to find documents to delete.\n   * @returns A promise that resolves when the documents have been removed.\n   */\n  async deleteMany(filter: string): Promise<IndexingResult[]> {\n    const { results } = await this.client.search(\"*\", {\n      filter,\n    });\n\n    const ids: string[] = [];\n    for await (const item of results) {\n      ids.push(item.document.id);\n    }\n\n    const { results: deleteResults } = await this.client.deleteDocuments(\n      DEFAULT_FIELD_ID,\n      ids\n    );\n    return deleteResults;\n  }\n\n  /**\n   * Removes specified documents from the AzureAISearchVectorStore.\n   * @param ids IDs of the documents to be removed.\n   * @returns A promise that resolves when the documents have been removed.\n   */\n  async deleteById(ids: string | string[]): Promise<IndexingResult[]> {\n    await this.initPromise;\n    const { results } = await this.client.deleteDocuments(\n      DEFAULT_FIELD_ID,\n      Array.isArray(ids) ? ids : [ids]\n    );\n    return results;\n  }\n\n  /**\n   * Adds documents to the AzureAISearchVectorStore.\n   * Documents are chunked into batches of size `embeddingBatchSize` then\n   * embedded and added to the AzureAISearchVectorStore.\n   * @param documents The documents to add.\n   * @param options Options for adding documents.\n   * @returns A promise that resolves to the ids of the added documents.\n   */\n  async addDocuments(\n    documents: Document[],\n    options?: AzureAISearchAddDocumentsOptions\n  ) {\n    const texts = documents.map(({ pageContent }) => pageContent);\n    const results: string[] = [];\n\n    for (let i = 0; i < texts.length; i += this.embeddingBatchSize) {\n      const batch = texts.slice(i, i + this.embeddingBatchSize);\n      const docsBatch = documents.slice(i, i + this.embeddingBatchSize);\n      const batchEmbeddings: number[][] = await this.embeddings.embedDocuments(\n        batch\n      );\n      const batchResult = await this.addVectors(\n        batchEmbeddings,\n        docsBatch,\n        options\n      );\n\n      results.push(...batchResult);\n    }\n\n    return results;\n  }\n\n  /**\n   * Adds vectors to the AzureAISearchVectorStore.\n   * @param vectors Vectors to be added.\n   * @param documents Corresponding documents to be added.\n   * @param options Options for adding documents.\n   * @returns A promise that resolves to the ids of the added documents.\n   */\n  async addVectors(\n    vectors: number[][],\n    documents: Document[],\n    options?: AzureAISearchAddDocumentsOptions\n  ): Promise<string[]> {\n    const ids = options?.ids ?? documents.map(() => uuid.v4());\n    const entities: AzureAISearchDocument[] = documents.map((doc, idx) => ({\n      id: ids[idx],\n      content: doc.pageContent,\n      content_vector: vectors[idx],\n      metadata: {\n        source: doc.metadata?.source,\n        attributes: doc.metadata?.attributes ?? [],\n      },\n    }));\n\n    await this.initPromise;\n    for (let i = 0; i < entities.length; i += this.chunkSize) {\n      const chunk = entities.slice(i, i + this.chunkSize);\n      await this.client.uploadDocuments(chunk, { throwOnAnyFailure: true });\n    }\n\n    return ids;\n  }\n\n  /**\n   * Performs a similarity search using query type specified in configuration.\n   * @param query Query text for the similarity search.\n   * @param k=4 Number of nearest neighbors to return.\n   * @param filter Optional OData filter for the documents.\n   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n   */\n  async similaritySearch(\n    query: string,\n    k = 4,\n    filter: this[\"FilterType\"] | undefined = undefined\n  ): Promise<Document[]> {\n    const results = await this.similaritySearchWithScore(query, k, filter);\n\n    return results.map((result) => result[0]);\n  }\n\n  /**\n   * Performs a similarity search using query type specified in configuration.\n   * @param query Query text for the similarity search.\n   * @param k=4 Number of nearest neighbors to return.\n   * @param filter Optional OData filter for the documents.\n   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n   */\n  async similaritySearchWithScore(\n    query: string,\n    k = 4,\n    filter: this[\"FilterType\"] | undefined = undefined\n  ): Promise<[Document, number][]> {\n    const searchType = this.options.type;\n\n    if (searchType === AzureAISearchQueryType.Similarity) {\n      return this.similaritySearchVectorWithScore(\n        await this.embeddings.embedQuery(query),\n        k,\n        filter\n      );\n    } else if (searchType === AzureAISearchQueryType.SimilarityHybrid) {\n      return this.hybridSearchVectorWithScore(\n        query,\n        await this.embeddings.embedQuery(query),\n        k,\n        filter\n      );\n    } else if (searchType === AzureAISearchQueryType.SemanticHybrid) {\n      return this.semanticHybridSearchVectorWithScore(\n        query,\n        await this.embeddings.embedQuery(query),\n        k,\n        filter\n      );\n    }\n\n    throw new Error(`Unrecognized search type '${searchType}'`);\n  }\n\n  /**\n   * Performs a hybrid search using query text.\n   * @param query Query text for the similarity search.\n   * @param queryVector Query vector for the similarity search.\n   *    If not provided, the query text will be embedded.\n   * @param k=4 Number of nearest neighbors to return.\n   * @param filter Optional OData filter for the documents.\n   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n   */\n  async hybridSearchVectorWithScore(\n    query: string,\n    queryVector?: number[],\n    k = 4,\n    filter: string | undefined = undefined\n  ): Promise<[Document, number][]> {\n    const vector = queryVector ?? (await this.embeddings.embedQuery(query));\n\n    await this.initPromise;\n    const { results } = await this.client.search(query, {\n      vectorSearchOptions: {\n        queries: [\n          {\n            kind: \"vector\",\n            vector,\n            kNearestNeighborsCount: k,\n            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n          },\n        ],\n      },\n      filter,\n      top: k,\n    });\n\n    const docsWithScore: [Document, number][] = [];\n\n    for await (const item of results) {\n      const document = new Document<\n        AzureAISearchDocumentMetadata & { embedding: number[] }\n      >({\n        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n        metadata: {\n          ...item.document[DEFAULT_FIELD_METADATA],\n          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n        },\n      });\n      docsWithScore.push([document, item.score]);\n    }\n\n    return docsWithScore;\n  }\n\n  /**\n   * Performs a hybrid search with semantic reranker using query text.\n   * @param query Query text for the similarity search.\n   * @param queryVector Query vector for the similarity search.\n   *    If not provided, the query text will be embedded.\n   * @param k=4 Number of nearest neighbors to return.\n   * @param filter Optional OData filter for the documents.\n   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n   */\n  async semanticHybridSearchVectorWithScore(\n    query: string,\n    queryVector?: number[],\n    k = 4,\n    filter: string | undefined = undefined\n  ): Promise<[Document, number][]> {\n    const vector = queryVector ?? (await this.embeddings.embedQuery(query));\n\n    await this.initPromise;\n    const { results } = await this.client.search(query, {\n      vectorSearchOptions: {\n        queries: [\n          {\n            kind: \"vector\",\n            vector,\n            kNearestNeighborsCount: k,\n            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n          },\n        ],\n      },\n      filter,\n      top: k,\n      queryType: \"semantic\",\n      semanticSearchOptions: {\n        configurationName: \"semantic-search-config\",\n        captions: {\n          captionType: \"extractive\",\n        },\n        answers: {\n          answerType: \"extractive\",\n        },\n      },\n    });\n\n    const docsWithScore: [Document, number][] = [];\n\n    for await (const item of results) {\n      const document = new Document<\n        AzureAISearchDocumentMetadata & { embedding: number[] }\n      >({\n        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n        metadata: {\n          ...item.document[DEFAULT_FIELD_METADATA],\n          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n        },\n      });\n      docsWithScore.push([document, item.score]);\n    }\n\n    return docsWithScore;\n  }\n\n  /**\n   * Performs a similarity search on the vectors stored in the collection.\n   * @param queryVector Query vector for the similarity search.\n   * @param k=4 Number of nearest neighbors to return.\n   * @param filter string OData filter for the documents.\n   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n   */\n  async similaritySearchVectorWithScore(\n    query: number[],\n    k: number,\n    filter?: string\n  ): Promise<[Document, number][]> {\n    await this.initPromise;\n\n    const { results } = await this.client.search(\"*\", {\n      vectorSearchOptions: {\n        queries: [\n          {\n            kind: \"vector\",\n            vector: query,\n            kNearestNeighborsCount: k,\n            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n          },\n        ],\n      },\n      filter,\n    });\n\n    const docsWithScore: [Document, number][] = [];\n\n    for await (const item of results) {\n      const document = new Document<\n        AzureAISearchDocumentMetadata & { embedding: number[] }\n      >({\n        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n        metadata: {\n          ...item.document[DEFAULT_FIELD_METADATA],\n          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n        },\n      });\n      docsWithScore.push([document, item.score]);\n    }\n\n    return docsWithScore;\n  }\n\n  /**\n   * Return documents selected using the maximal marginal relevance.\n   * Maximal marginal relevance optimizes for similarity to the query AND\n   * diversity among selected documents.\n   * @param query Text to look up documents similar to.\n   * @param options.k Number of documents to return.\n   * @param options.fetchK=20 Number of documents to fetch before passing to\n   *     the MMR algorithm.\n   * @param options.lambda=0.5 Number between 0 and 1 that determines the\n   *     degree of diversity among the results, where 0 corresponds to maximum\n   *     diversity and 1 to minimum diversity.\n   * @returns List of documents selected by maximal marginal relevance.\n   */\n  async maxMarginalRelevanceSearch(\n    query: string,\n    options: MaxMarginalRelevanceSearchOptions<this[\"FilterType\"]>\n  ): Promise<Document[]> {\n    const { k, fetchK = 20, lambda = 0.5 } = options;\n\n    const queryEmbedding = await this.embeddings.embedQuery(query);\n    const docs = await this.similaritySearchVectorWithScore(\n      queryEmbedding,\n      fetchK\n    );\n    const embeddingList = docs.map((doc) => doc[0].metadata.embedding);\n\n    // Re-rank the results using MMR\n    const mmrIndexes = maximalMarginalRelevance(\n      queryEmbedding,\n      embeddingList,\n      lambda,\n      k\n    );\n\n    const mmrDocs = mmrIndexes.map((index) => docs[index][0]);\n    return mmrDocs;\n  }\n\n  /**\n   * Ensures that an index exists on the AzureAISearchVectorStore.\n   * @param indexClient The Azure AI Search index client.\n   * @returns A promise that resolves when the AzureAISearchVectorStore index has been initialized.\n   * @protected\n   */\n  protected async ensureIndexExists(\n    indexClient: SearchIndexClient\n  ): Promise<void> {\n    try {\n      await indexClient.getIndex(this.indexName);\n    } catch (e) {\n      // Index does not exists, create it\n      const searchIndex = this.createSearchIndexDefinition(this.indexName);\n      await indexClient.createIndex(searchIndex);\n    }\n  }\n\n  /**\n   * Prepares the search index definition for Azure AI Search.\n   * @param indexName The name of the index.\n   * @returns The SearchIndex object.\n   * @protected\n   */\n  protected createSearchIndexDefinition(indexName: string): SearchIndex {\n    return {\n      name: indexName,\n      vectorSearch: {\n        algorithms: [\n          {\n            name: \"vector-search-algorithm\",\n            kind: \"hnsw\",\n            parameters: {\n              m: 4,\n              efSearch: 500,\n              metric: \"cosine\",\n              efConstruction: 400,\n            },\n          },\n        ],\n        profiles: [\n          {\n            name: \"vector-search-profile\",\n            algorithmConfigurationName: \"vector-search-algorithm\",\n          },\n        ],\n      },\n      semanticSearch: {\n        defaultConfigurationName: \"semantic-search-config\",\n        configurations: [\n          {\n            name: \"semantic-search-config\",\n            prioritizedFields: {\n              contentFields: [\n                {\n                  name: DEFAULT_FIELD_CONTENT,\n                },\n              ],\n              keywordsFields: [\n                {\n                  name: DEFAULT_FIELD_CONTENT,\n                },\n              ],\n            },\n          },\n        ],\n      },\n      fields: [\n        {\n          name: DEFAULT_FIELD_ID,\n          filterable: true,\n          key: true,\n          type: \"Edm.String\",\n        },\n        {\n          name: DEFAULT_FIELD_CONTENT,\n          searchable: true,\n          filterable: true,\n          type: \"Edm.String\",\n        },\n        {\n          name: DEFAULT_FIELD_CONTENT_VECTOR,\n          searchable: true,\n          type: \"Collection(Edm.Single)\",\n          vectorSearchDimensions: 1536,\n          vectorSearchProfileName: \"vector-search-profile\",\n        },\n        {\n          name: DEFAULT_FIELD_METADATA,\n          type: \"Edm.ComplexType\",",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1456406645",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 4044,
        "pr_file": "libs/langchain-community/src/vectorstores/azure_aisearch.ts",
        "discussion_id": "1456406645",
        "commented_code": "@@ -0,0 +1,694 @@\n+import * as uuid from \"uuid\";\n+import {\n+  SearchClient,\n+  SearchIndexClient,\n+  AzureKeyCredential,\n+  IndexingResult,\n+  SearchIndex,\n+} from \"@azure/search-documents\";\n+import {\n+  MaxMarginalRelevanceSearchOptions,\n+  VectorStore,\n+} from \"@langchain/core/vectorstores\";\n+import type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { Document } from \"@langchain/core/documents\";\n+import { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+\n+/**\n+ * Azure AI Search query type.\n+ */\n+export const AzureAISearchQueryType = {\n+  /** Vector search. */\n+  Similarity: \"similarity\",\n+  /** Hybrid full text and vector search. */\n+  SimilarityHybrid: \"similarity_hybrid\",\n+  /** Hybrid full text and vector search with semantic ranking. */\n+  SemanticHybrid: \"semantic_hybrid\",\n+} as const;\n+\n+/**\n+ * Azure AI Search query type.\n+ */\n+export type AzureAISearchQueryType =\n+  (typeof AzureAISearchQueryType)[keyof typeof AzureAISearchQueryType];\n+\n+/**\n+ * Azure AI Search settings.\n+ */\n+export interface AzureAISearchQueryOptions {\n+  readonly type: AzureAISearchQueryType;\n+  readonly semanticConfigurationName?: string;\n+}\n+\n+/**\n+ * Configuration options for the `AzureAISearchStore` constructor.\n+ */\n+export interface AzureAISearchConfig {\n+  readonly client?: SearchClient<AzureAISearchDocument>;\n+  readonly indexName?: string;\n+  readonly endpoint?: string;\n+  readonly key?: string;\n+  readonly search: AzureAISearchQueryOptions;\n+  /**\n+   * The amount of documents to chunk by when adding vectors.\n+   * @default 100\n+   */\n+  readonly chunkSize?: number;\n+  /**\n+   * The amount of documents to embed at once when adding documents.\n+   * Note that some providers like Azure OpenAI can only embed 16 documents\n+   * at a time.\n+   * @default 16\n+   */\n+  readonly embeddingBatchSize?: number;\n+}\n+\n+/**\n+ * Azure AI Search options metadata schema.\n+ * If yout want to add custom data, use the attributes property.\n+ */\n+export type AzureAISearchDocumentMetadata = {\n+  source: string;\n+  attributes?: Array<{ key: string; value: string }>;\n+};\n+\n+/**\n+ * Azure AI Search indexed document.\n+ */\n+export type AzureAISearchDocument = {\n+  id: string;\n+  content: string;\n+  content_vector: number[];\n+  metadata: AzureAISearchDocumentMetadata;\n+};\n+\n+/**\n+ * Azure AI Search options for adding documents.\n+ */\n+export type AzureAISearchAddDocumentsOptions = {\n+  ids?: string[];\n+};\n+\n+const DEFAULT_FIELD_ID = \"id\";\n+const DEFAULT_FIELD_CONTENT = \"content\";\n+const DEFAULT_FIELD_CONTENT_VECTOR = \"content_vector\";\n+const DEFAULT_FIELD_METADATA = \"metadata\";\n+const DEFAULT_FIELD_METADATA_SOURCE = \"source\";\n+const DEFAULT_FIELD_METADATA_ATTRS = \"attributes\";\n+\n+/**\n+ * Azure AI Search vector store.\n+ * To use this, you should have:\n+ * - the `@azure/search-documents` NPM package installed\n+ * - an endpoint and key to the Azure AI Search instance\n+ *\n+ * If you directly provide a `SearchClient` instance, you need to ensure that\n+ * an index has been created. When using and endpoint and key, the index will\n+ * be created automatically if it does not exist.\n+ */\n+export class AzureAISearchVectorStore extends VectorStore {\n+  declare FilterType: string;\n+\n+  get lc_secrets(): { [key: string]: string } {\n+    return {\n+      endpoint: \"AZURE_AISEARCH_ENDPOINT\",\n+      key: \"AZURE_AISEARCH_KEY\",\n+    };\n+  }\n+\n+  _vectorstoreType(): string {\n+    return \"azure_aisearch\";\n+  }\n+\n+  private readonly initPromise: Promise<void>;\n+\n+  private readonly client: SearchClient<AzureAISearchDocument>;\n+\n+  private readonly indexName: string;\n+\n+  private readonly chunkSize: number;\n+\n+  private readonly embeddingBatchSize: number;\n+\n+  private readonly options: AzureAISearchQueryOptions;\n+\n+  constructor(embeddings: EmbeddingsInterface, config: AzureAISearchConfig) {\n+    super(embeddings, config);\n+\n+    const endpoint =\n+      config.endpoint ?? getEnvironmentVariable(\"AZURE_AISEARCH_ENDPOINT\");\n+    const key = config.key ?? getEnvironmentVariable(\"AZURE_AISEARCH_KEY\");\n+\n+    if (!config.client && !endpoint && !key) {\n+      throw new Error(\n+        \"Azure AI Search client or connection string must be set.\"\n+      );\n+    }\n+\n+    this.indexName = config.indexName ?? \"vectorsearch\";\n+    this.chunkSize = config.chunkSize ?? 100;\n+    this.embeddingBatchSize = config.embeddingBatchSize ?? 16;\n+\n+    if (!config.client) {\n+      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n+      const credential = new AzureKeyCredential(key!);\n+      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n+      this.client = new SearchClient(endpoint!, this.indexName, credential);\n+      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n+      const indexClient = new SearchIndexClient(endpoint!, credential);\n+\n+      // Start initialization, but don't wait for it to finish here\n+      this.initPromise = this.ensureIndexExists(indexClient).catch((error) => {\n+        console.error(\n+          \"Error during Azure AI Search index initialization:\",\n+          error\n+        );\n+      });\n+    } else {\n+      this.client = config.client;\n+    }\n+\n+    this.options = config.search;\n+    this.embeddings = embeddings;\n+  }\n+\n+  /**\n+   * Removes specified documents from the AzureAISearchVectorStore using a filter.\n+   * @param filter OData filter to find documents to delete.\n+   * @returns A promise that resolves when the documents have been removed.\n+   */\n+  async deleteMany(filter: string): Promise<IndexingResult[]> {\n+    const { results } = await this.client.search(\"*\", {\n+      filter,\n+    });\n+\n+    const ids: string[] = [];\n+    for await (const item of results) {\n+      ids.push(item.document.id);\n+    }\n+\n+    const { results: deleteResults } = await this.client.deleteDocuments(\n+      DEFAULT_FIELD_ID,\n+      ids\n+    );\n+    return deleteResults;\n+  }\n+\n+  /**\n+   * Removes specified documents from the AzureAISearchVectorStore.\n+   * @param ids IDs of the documents to be removed.\n+   * @returns A promise that resolves when the documents have been removed.\n+   */\n+  async deleteById(ids: string | string[]): Promise<IndexingResult[]> {\n+    await this.initPromise;\n+    const { results } = await this.client.deleteDocuments(\n+      DEFAULT_FIELD_ID,\n+      Array.isArray(ids) ? ids : [ids]\n+    );\n+    return results;\n+  }\n+\n+  /**\n+   * Adds documents to the AzureAISearchVectorStore.\n+   * Documents are chunked into batches of size `embeddingBatchSize` then\n+   * embedded and added to the AzureAISearchVectorStore.\n+   * @param documents The documents to add.\n+   * @param options Options for adding documents.\n+   * @returns A promise that resolves to the ids of the added documents.\n+   */\n+  async addDocuments(\n+    documents: Document[],\n+    options?: AzureAISearchAddDocumentsOptions\n+  ) {\n+    const texts = documents.map(({ pageContent }) => pageContent);\n+    const results: string[] = [];\n+\n+    for (let i = 0; i < texts.length; i += this.embeddingBatchSize) {\n+      const batch = texts.slice(i, i + this.embeddingBatchSize);\n+      const docsBatch = documents.slice(i, i + this.embeddingBatchSize);\n+      const batchEmbeddings: number[][] = await this.embeddings.embedDocuments(\n+        batch\n+      );\n+      const batchResult = await this.addVectors(\n+        batchEmbeddings,\n+        docsBatch,\n+        options\n+      );\n+\n+      results.push(...batchResult);\n+    }\n+\n+    return results;\n+  }\n+\n+  /**\n+   * Adds vectors to the AzureAISearchVectorStore.\n+   * @param vectors Vectors to be added.\n+   * @param documents Corresponding documents to be added.\n+   * @param options Options for adding documents.\n+   * @returns A promise that resolves to the ids of the added documents.\n+   */\n+  async addVectors(\n+    vectors: number[][],\n+    documents: Document[],\n+    options?: AzureAISearchAddDocumentsOptions\n+  ): Promise<string[]> {\n+    const ids = options?.ids ?? documents.map(() => uuid.v4());\n+    const entities: AzureAISearchDocument[] = documents.map((doc, idx) => ({\n+      id: ids[idx],\n+      content: doc.pageContent,\n+      content_vector: vectors[idx],\n+      metadata: {\n+        source: doc.metadata?.source,\n+        attributes: doc.metadata?.attributes ?? [],\n+      },\n+    }));\n+\n+    await this.initPromise;\n+    for (let i = 0; i < entities.length; i += this.chunkSize) {\n+      const chunk = entities.slice(i, i + this.chunkSize);\n+      await this.client.uploadDocuments(chunk, { throwOnAnyFailure: true });\n+    }\n+\n+    return ids;\n+  }\n+\n+  /**\n+   * Performs a similarity search using query type specified in configuration.\n+   * @param query Query text for the similarity search.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter Optional OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async similaritySearch(\n+    query: string,\n+    k = 4,\n+    filter: this[\"FilterType\"] | undefined = undefined\n+  ): Promise<Document[]> {\n+    const results = await this.similaritySearchWithScore(query, k, filter);\n+\n+    return results.map((result) => result[0]);\n+  }\n+\n+  /**\n+   * Performs a similarity search using query type specified in configuration.\n+   * @param query Query text for the similarity search.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter Optional OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async similaritySearchWithScore(\n+    query: string,\n+    k = 4,\n+    filter: this[\"FilterType\"] | undefined = undefined\n+  ): Promise<[Document, number][]> {\n+    const searchType = this.options.type;\n+\n+    if (searchType === AzureAISearchQueryType.Similarity) {\n+      return this.similaritySearchVectorWithScore(\n+        await this.embeddings.embedQuery(query),\n+        k,\n+        filter\n+      );\n+    } else if (searchType === AzureAISearchQueryType.SimilarityHybrid) {\n+      return this.hybridSearchVectorWithScore(\n+        query,\n+        await this.embeddings.embedQuery(query),\n+        k,\n+        filter\n+      );\n+    } else if (searchType === AzureAISearchQueryType.SemanticHybrid) {\n+      return this.semanticHybridSearchVectorWithScore(\n+        query,\n+        await this.embeddings.embedQuery(query),\n+        k,\n+        filter\n+      );\n+    }\n+\n+    throw new Error(`Unrecognized search type '${searchType}'`);\n+  }\n+\n+  /**\n+   * Performs a hybrid search using query text.\n+   * @param query Query text for the similarity search.\n+   * @param queryVector Query vector for the similarity search.\n+   *    If not provided, the query text will be embedded.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter Optional OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async hybridSearchVectorWithScore(\n+    query: string,\n+    queryVector?: number[],\n+    k = 4,\n+    filter: string | undefined = undefined\n+  ): Promise<[Document, number][]> {\n+    const vector = queryVector ?? (await this.embeddings.embedQuery(query));\n+\n+    await this.initPromise;\n+    const { results } = await this.client.search(query, {\n+      vectorSearchOptions: {\n+        queries: [\n+          {\n+            kind: \"vector\",\n+            vector,\n+            kNearestNeighborsCount: k,\n+            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n+          },\n+        ],\n+      },\n+      filter,\n+      top: k,\n+    });\n+\n+    const docsWithScore: [Document, number][] = [];\n+\n+    for await (const item of results) {\n+      const document = new Document<\n+        AzureAISearchDocumentMetadata & { embedding: number[] }\n+      >({\n+        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n+        metadata: {\n+          ...item.document[DEFAULT_FIELD_METADATA],\n+          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n+        },\n+      });\n+      docsWithScore.push([document, item.score]);\n+    }\n+\n+    return docsWithScore;\n+  }\n+\n+  /**\n+   * Performs a hybrid search with semantic reranker using query text.\n+   * @param query Query text for the similarity search.\n+   * @param queryVector Query vector for the similarity search.\n+   *    If not provided, the query text will be embedded.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter Optional OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async semanticHybridSearchVectorWithScore(\n+    query: string,\n+    queryVector?: number[],\n+    k = 4,\n+    filter: string | undefined = undefined\n+  ): Promise<[Document, number][]> {\n+    const vector = queryVector ?? (await this.embeddings.embedQuery(query));\n+\n+    await this.initPromise;\n+    const { results } = await this.client.search(query, {\n+      vectorSearchOptions: {\n+        queries: [\n+          {\n+            kind: \"vector\",\n+            vector,\n+            kNearestNeighborsCount: k,\n+            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n+          },\n+        ],\n+      },\n+      filter,\n+      top: k,\n+      queryType: \"semantic\",\n+      semanticSearchOptions: {\n+        configurationName: \"semantic-search-config\",\n+        captions: {\n+          captionType: \"extractive\",\n+        },\n+        answers: {\n+          answerType: \"extractive\",\n+        },\n+      },\n+    });\n+\n+    const docsWithScore: [Document, number][] = [];\n+\n+    for await (const item of results) {\n+      const document = new Document<\n+        AzureAISearchDocumentMetadata & { embedding: number[] }\n+      >({\n+        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n+        metadata: {\n+          ...item.document[DEFAULT_FIELD_METADATA],\n+          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n+        },\n+      });\n+      docsWithScore.push([document, item.score]);\n+    }\n+\n+    return docsWithScore;\n+  }\n+\n+  /**\n+   * Performs a similarity search on the vectors stored in the collection.\n+   * @param queryVector Query vector for the similarity search.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter string OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async similaritySearchVectorWithScore(\n+    query: number[],\n+    k: number,\n+    filter?: string\n+  ): Promise<[Document, number][]> {\n+    await this.initPromise;\n+\n+    const { results } = await this.client.search(\"*\", {\n+      vectorSearchOptions: {\n+        queries: [\n+          {\n+            kind: \"vector\",\n+            vector: query,\n+            kNearestNeighborsCount: k,\n+            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n+          },\n+        ],\n+      },\n+      filter,\n+    });\n+\n+    const docsWithScore: [Document, number][] = [];\n+\n+    for await (const item of results) {\n+      const document = new Document<\n+        AzureAISearchDocumentMetadata & { embedding: number[] }\n+      >({\n+        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n+        metadata: {\n+          ...item.document[DEFAULT_FIELD_METADATA],\n+          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n+        },\n+      });\n+      docsWithScore.push([document, item.score]);\n+    }\n+\n+    return docsWithScore;\n+  }\n+\n+  /**\n+   * Return documents selected using the maximal marginal relevance.\n+   * Maximal marginal relevance optimizes for similarity to the query AND\n+   * diversity among selected documents.\n+   * @param query Text to look up documents similar to.\n+   * @param options.k Number of documents to return.\n+   * @param options.fetchK=20 Number of documents to fetch before passing to\n+   *     the MMR algorithm.\n+   * @param options.lambda=0.5 Number between 0 and 1 that determines the\n+   *     degree of diversity among the results, where 0 corresponds to maximum\n+   *     diversity and 1 to minimum diversity.\n+   * @returns List of documents selected by maximal marginal relevance.\n+   */\n+  async maxMarginalRelevanceSearch(\n+    query: string,\n+    options: MaxMarginalRelevanceSearchOptions<this[\"FilterType\"]>\n+  ): Promise<Document[]> {\n+    const { k, fetchK = 20, lambda = 0.5 } = options;\n+\n+    const queryEmbedding = await this.embeddings.embedQuery(query);\n+    const docs = await this.similaritySearchVectorWithScore(\n+      queryEmbedding,\n+      fetchK\n+    );\n+    const embeddingList = docs.map((doc) => doc[0].metadata.embedding);\n+\n+    // Re-rank the results using MMR\n+    const mmrIndexes = maximalMarginalRelevance(\n+      queryEmbedding,\n+      embeddingList,\n+      lambda,\n+      k\n+    );\n+\n+    const mmrDocs = mmrIndexes.map((index) => docs[index][0]);\n+    return mmrDocs;\n+  }\n+\n+  /**\n+   * Ensures that an index exists on the AzureAISearchVectorStore.\n+   * @param indexClient The Azure AI Search index client.\n+   * @returns A promise that resolves when the AzureAISearchVectorStore index has been initialized.\n+   * @protected\n+   */\n+  protected async ensureIndexExists(\n+    indexClient: SearchIndexClient\n+  ): Promise<void> {\n+    try {\n+      await indexClient.getIndex(this.indexName);\n+    } catch (e) {\n+      // Index does not exists, create it\n+      const searchIndex = this.createSearchIndexDefinition(this.indexName);\n+      await indexClient.createIndex(searchIndex);\n+    }\n+  }\n+\n+  /**\n+   * Prepares the search index definition for Azure AI Search.\n+   * @param indexName The name of the index.\n+   * @returns The SearchIndex object.\n+   * @protected\n+   */\n+  protected createSearchIndexDefinition(indexName: string): SearchIndex {\n+    return {\n+      name: indexName,\n+      vectorSearch: {\n+        algorithms: [\n+          {\n+            name: \"vector-search-algorithm\",\n+            kind: \"hnsw\",\n+            parameters: {\n+              m: 4,\n+              efSearch: 500,\n+              metric: \"cosine\",\n+              efConstruction: 400,\n+            },\n+          },\n+        ],\n+        profiles: [\n+          {\n+            name: \"vector-search-profile\",\n+            algorithmConfigurationName: \"vector-search-algorithm\",\n+          },\n+        ],\n+      },\n+      semanticSearch: {\n+        defaultConfigurationName: \"semantic-search-config\",\n+        configurations: [\n+          {\n+            name: \"semantic-search-config\",\n+            prioritizedFields: {\n+              contentFields: [\n+                {\n+                  name: DEFAULT_FIELD_CONTENT,\n+                },\n+              ],\n+              keywordsFields: [\n+                {\n+                  name: DEFAULT_FIELD_CONTENT,\n+                },\n+              ],\n+            },\n+          },\n+        ],\n+      },\n+      fields: [\n+        {\n+          name: DEFAULT_FIELD_ID,\n+          filterable: true,\n+          key: true,\n+          type: \"Edm.String\",\n+        },\n+        {\n+          name: DEFAULT_FIELD_CONTENT,\n+          searchable: true,\n+          filterable: true,\n+          type: \"Edm.String\",\n+        },\n+        {\n+          name: DEFAULT_FIELD_CONTENT_VECTOR,\n+          searchable: true,\n+          type: \"Collection(Edm.Single)\",\n+          vectorSearchDimensions: 1536,\n+          vectorSearchProfileName: \"vector-search-profile\",\n+        },\n+        {\n+          name: DEFAULT_FIELD_METADATA,\n+          type: \"Edm.ComplexType\",",
        "comment_created_at": "2024-01-17T19:58:12+00:00",
        "comment_author": "pablocastro",
        "comment_body": "Are the metadata fields known at index time? If they were, creating actual filterable fields of the right types instead of a dictionary-style complex type would be better, and you'd get more type fidelity (e.g. numbers, dates, etc.) and filter expressions would be richer (e.g. numeric/date range comparisons).",
        "pr_file_module": null
      },
      {
        "comment_id": "1457277213",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 4044,
        "pr_file": "libs/langchain-community/src/vectorstores/azure_aisearch.ts",
        "discussion_id": "1456406645",
        "commented_code": "@@ -0,0 +1,694 @@\n+import * as uuid from \"uuid\";\n+import {\n+  SearchClient,\n+  SearchIndexClient,\n+  AzureKeyCredential,\n+  IndexingResult,\n+  SearchIndex,\n+} from \"@azure/search-documents\";\n+import {\n+  MaxMarginalRelevanceSearchOptions,\n+  VectorStore,\n+} from \"@langchain/core/vectorstores\";\n+import type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { Document } from \"@langchain/core/documents\";\n+import { maximalMarginalRelevance } from \"@langchain/core/utils/math\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+\n+/**\n+ * Azure AI Search query type.\n+ */\n+export const AzureAISearchQueryType = {\n+  /** Vector search. */\n+  Similarity: \"similarity\",\n+  /** Hybrid full text and vector search. */\n+  SimilarityHybrid: \"similarity_hybrid\",\n+  /** Hybrid full text and vector search with semantic ranking. */\n+  SemanticHybrid: \"semantic_hybrid\",\n+} as const;\n+\n+/**\n+ * Azure AI Search query type.\n+ */\n+export type AzureAISearchQueryType =\n+  (typeof AzureAISearchQueryType)[keyof typeof AzureAISearchQueryType];\n+\n+/**\n+ * Azure AI Search settings.\n+ */\n+export interface AzureAISearchQueryOptions {\n+  readonly type: AzureAISearchQueryType;\n+  readonly semanticConfigurationName?: string;\n+}\n+\n+/**\n+ * Configuration options for the `AzureAISearchStore` constructor.\n+ */\n+export interface AzureAISearchConfig {\n+  readonly client?: SearchClient<AzureAISearchDocument>;\n+  readonly indexName?: string;\n+  readonly endpoint?: string;\n+  readonly key?: string;\n+  readonly search: AzureAISearchQueryOptions;\n+  /**\n+   * The amount of documents to chunk by when adding vectors.\n+   * @default 100\n+   */\n+  readonly chunkSize?: number;\n+  /**\n+   * The amount of documents to embed at once when adding documents.\n+   * Note that some providers like Azure OpenAI can only embed 16 documents\n+   * at a time.\n+   * @default 16\n+   */\n+  readonly embeddingBatchSize?: number;\n+}\n+\n+/**\n+ * Azure AI Search options metadata schema.\n+ * If yout want to add custom data, use the attributes property.\n+ */\n+export type AzureAISearchDocumentMetadata = {\n+  source: string;\n+  attributes?: Array<{ key: string; value: string }>;\n+};\n+\n+/**\n+ * Azure AI Search indexed document.\n+ */\n+export type AzureAISearchDocument = {\n+  id: string;\n+  content: string;\n+  content_vector: number[];\n+  metadata: AzureAISearchDocumentMetadata;\n+};\n+\n+/**\n+ * Azure AI Search options for adding documents.\n+ */\n+export type AzureAISearchAddDocumentsOptions = {\n+  ids?: string[];\n+};\n+\n+const DEFAULT_FIELD_ID = \"id\";\n+const DEFAULT_FIELD_CONTENT = \"content\";\n+const DEFAULT_FIELD_CONTENT_VECTOR = \"content_vector\";\n+const DEFAULT_FIELD_METADATA = \"metadata\";\n+const DEFAULT_FIELD_METADATA_SOURCE = \"source\";\n+const DEFAULT_FIELD_METADATA_ATTRS = \"attributes\";\n+\n+/**\n+ * Azure AI Search vector store.\n+ * To use this, you should have:\n+ * - the `@azure/search-documents` NPM package installed\n+ * - an endpoint and key to the Azure AI Search instance\n+ *\n+ * If you directly provide a `SearchClient` instance, you need to ensure that\n+ * an index has been created. When using and endpoint and key, the index will\n+ * be created automatically if it does not exist.\n+ */\n+export class AzureAISearchVectorStore extends VectorStore {\n+  declare FilterType: string;\n+\n+  get lc_secrets(): { [key: string]: string } {\n+    return {\n+      endpoint: \"AZURE_AISEARCH_ENDPOINT\",\n+      key: \"AZURE_AISEARCH_KEY\",\n+    };\n+  }\n+\n+  _vectorstoreType(): string {\n+    return \"azure_aisearch\";\n+  }\n+\n+  private readonly initPromise: Promise<void>;\n+\n+  private readonly client: SearchClient<AzureAISearchDocument>;\n+\n+  private readonly indexName: string;\n+\n+  private readonly chunkSize: number;\n+\n+  private readonly embeddingBatchSize: number;\n+\n+  private readonly options: AzureAISearchQueryOptions;\n+\n+  constructor(embeddings: EmbeddingsInterface, config: AzureAISearchConfig) {\n+    super(embeddings, config);\n+\n+    const endpoint =\n+      config.endpoint ?? getEnvironmentVariable(\"AZURE_AISEARCH_ENDPOINT\");\n+    const key = config.key ?? getEnvironmentVariable(\"AZURE_AISEARCH_KEY\");\n+\n+    if (!config.client && !endpoint && !key) {\n+      throw new Error(\n+        \"Azure AI Search client or connection string must be set.\"\n+      );\n+    }\n+\n+    this.indexName = config.indexName ?? \"vectorsearch\";\n+    this.chunkSize = config.chunkSize ?? 100;\n+    this.embeddingBatchSize = config.embeddingBatchSize ?? 16;\n+\n+    if (!config.client) {\n+      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n+      const credential = new AzureKeyCredential(key!);\n+      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n+      this.client = new SearchClient(endpoint!, this.indexName, credential);\n+      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n+      const indexClient = new SearchIndexClient(endpoint!, credential);\n+\n+      // Start initialization, but don't wait for it to finish here\n+      this.initPromise = this.ensureIndexExists(indexClient).catch((error) => {\n+        console.error(\n+          \"Error during Azure AI Search index initialization:\",\n+          error\n+        );\n+      });\n+    } else {\n+      this.client = config.client;\n+    }\n+\n+    this.options = config.search;\n+    this.embeddings = embeddings;\n+  }\n+\n+  /**\n+   * Removes specified documents from the AzureAISearchVectorStore using a filter.\n+   * @param filter OData filter to find documents to delete.\n+   * @returns A promise that resolves when the documents have been removed.\n+   */\n+  async deleteMany(filter: string): Promise<IndexingResult[]> {\n+    const { results } = await this.client.search(\"*\", {\n+      filter,\n+    });\n+\n+    const ids: string[] = [];\n+    for await (const item of results) {\n+      ids.push(item.document.id);\n+    }\n+\n+    const { results: deleteResults } = await this.client.deleteDocuments(\n+      DEFAULT_FIELD_ID,\n+      ids\n+    );\n+    return deleteResults;\n+  }\n+\n+  /**\n+   * Removes specified documents from the AzureAISearchVectorStore.\n+   * @param ids IDs of the documents to be removed.\n+   * @returns A promise that resolves when the documents have been removed.\n+   */\n+  async deleteById(ids: string | string[]): Promise<IndexingResult[]> {\n+    await this.initPromise;\n+    const { results } = await this.client.deleteDocuments(\n+      DEFAULT_FIELD_ID,\n+      Array.isArray(ids) ? ids : [ids]\n+    );\n+    return results;\n+  }\n+\n+  /**\n+   * Adds documents to the AzureAISearchVectorStore.\n+   * Documents are chunked into batches of size `embeddingBatchSize` then\n+   * embedded and added to the AzureAISearchVectorStore.\n+   * @param documents The documents to add.\n+   * @param options Options for adding documents.\n+   * @returns A promise that resolves to the ids of the added documents.\n+   */\n+  async addDocuments(\n+    documents: Document[],\n+    options?: AzureAISearchAddDocumentsOptions\n+  ) {\n+    const texts = documents.map(({ pageContent }) => pageContent);\n+    const results: string[] = [];\n+\n+    for (let i = 0; i < texts.length; i += this.embeddingBatchSize) {\n+      const batch = texts.slice(i, i + this.embeddingBatchSize);\n+      const docsBatch = documents.slice(i, i + this.embeddingBatchSize);\n+      const batchEmbeddings: number[][] = await this.embeddings.embedDocuments(\n+        batch\n+      );\n+      const batchResult = await this.addVectors(\n+        batchEmbeddings,\n+        docsBatch,\n+        options\n+      );\n+\n+      results.push(...batchResult);\n+    }\n+\n+    return results;\n+  }\n+\n+  /**\n+   * Adds vectors to the AzureAISearchVectorStore.\n+   * @param vectors Vectors to be added.\n+   * @param documents Corresponding documents to be added.\n+   * @param options Options for adding documents.\n+   * @returns A promise that resolves to the ids of the added documents.\n+   */\n+  async addVectors(\n+    vectors: number[][],\n+    documents: Document[],\n+    options?: AzureAISearchAddDocumentsOptions\n+  ): Promise<string[]> {\n+    const ids = options?.ids ?? documents.map(() => uuid.v4());\n+    const entities: AzureAISearchDocument[] = documents.map((doc, idx) => ({\n+      id: ids[idx],\n+      content: doc.pageContent,\n+      content_vector: vectors[idx],\n+      metadata: {\n+        source: doc.metadata?.source,\n+        attributes: doc.metadata?.attributes ?? [],\n+      },\n+    }));\n+\n+    await this.initPromise;\n+    for (let i = 0; i < entities.length; i += this.chunkSize) {\n+      const chunk = entities.slice(i, i + this.chunkSize);\n+      await this.client.uploadDocuments(chunk, { throwOnAnyFailure: true });\n+    }\n+\n+    return ids;\n+  }\n+\n+  /**\n+   * Performs a similarity search using query type specified in configuration.\n+   * @param query Query text for the similarity search.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter Optional OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async similaritySearch(\n+    query: string,\n+    k = 4,\n+    filter: this[\"FilterType\"] | undefined = undefined\n+  ): Promise<Document[]> {\n+    const results = await this.similaritySearchWithScore(query, k, filter);\n+\n+    return results.map((result) => result[0]);\n+  }\n+\n+  /**\n+   * Performs a similarity search using query type specified in configuration.\n+   * @param query Query text for the similarity search.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter Optional OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async similaritySearchWithScore(\n+    query: string,\n+    k = 4,\n+    filter: this[\"FilterType\"] | undefined = undefined\n+  ): Promise<[Document, number][]> {\n+    const searchType = this.options.type;\n+\n+    if (searchType === AzureAISearchQueryType.Similarity) {\n+      return this.similaritySearchVectorWithScore(\n+        await this.embeddings.embedQuery(query),\n+        k,\n+        filter\n+      );\n+    } else if (searchType === AzureAISearchQueryType.SimilarityHybrid) {\n+      return this.hybridSearchVectorWithScore(\n+        query,\n+        await this.embeddings.embedQuery(query),\n+        k,\n+        filter\n+      );\n+    } else if (searchType === AzureAISearchQueryType.SemanticHybrid) {\n+      return this.semanticHybridSearchVectorWithScore(\n+        query,\n+        await this.embeddings.embedQuery(query),\n+        k,\n+        filter\n+      );\n+    }\n+\n+    throw new Error(`Unrecognized search type '${searchType}'`);\n+  }\n+\n+  /**\n+   * Performs a hybrid search using query text.\n+   * @param query Query text for the similarity search.\n+   * @param queryVector Query vector for the similarity search.\n+   *    If not provided, the query text will be embedded.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter Optional OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async hybridSearchVectorWithScore(\n+    query: string,\n+    queryVector?: number[],\n+    k = 4,\n+    filter: string | undefined = undefined\n+  ): Promise<[Document, number][]> {\n+    const vector = queryVector ?? (await this.embeddings.embedQuery(query));\n+\n+    await this.initPromise;\n+    const { results } = await this.client.search(query, {\n+      vectorSearchOptions: {\n+        queries: [\n+          {\n+            kind: \"vector\",\n+            vector,\n+            kNearestNeighborsCount: k,\n+            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n+          },\n+        ],\n+      },\n+      filter,\n+      top: k,\n+    });\n+\n+    const docsWithScore: [Document, number][] = [];\n+\n+    for await (const item of results) {\n+      const document = new Document<\n+        AzureAISearchDocumentMetadata & { embedding: number[] }\n+      >({\n+        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n+        metadata: {\n+          ...item.document[DEFAULT_FIELD_METADATA],\n+          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n+        },\n+      });\n+      docsWithScore.push([document, item.score]);\n+    }\n+\n+    return docsWithScore;\n+  }\n+\n+  /**\n+   * Performs a hybrid search with semantic reranker using query text.\n+   * @param query Query text for the similarity search.\n+   * @param queryVector Query vector for the similarity search.\n+   *    If not provided, the query text will be embedded.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter Optional OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async semanticHybridSearchVectorWithScore(\n+    query: string,\n+    queryVector?: number[],\n+    k = 4,\n+    filter: string | undefined = undefined\n+  ): Promise<[Document, number][]> {\n+    const vector = queryVector ?? (await this.embeddings.embedQuery(query));\n+\n+    await this.initPromise;\n+    const { results } = await this.client.search(query, {\n+      vectorSearchOptions: {\n+        queries: [\n+          {\n+            kind: \"vector\",\n+            vector,\n+            kNearestNeighborsCount: k,\n+            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n+          },\n+        ],\n+      },\n+      filter,\n+      top: k,\n+      queryType: \"semantic\",\n+      semanticSearchOptions: {\n+        configurationName: \"semantic-search-config\",\n+        captions: {\n+          captionType: \"extractive\",\n+        },\n+        answers: {\n+          answerType: \"extractive\",\n+        },\n+      },\n+    });\n+\n+    const docsWithScore: [Document, number][] = [];\n+\n+    for await (const item of results) {\n+      const document = new Document<\n+        AzureAISearchDocumentMetadata & { embedding: number[] }\n+      >({\n+        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n+        metadata: {\n+          ...item.document[DEFAULT_FIELD_METADATA],\n+          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n+        },\n+      });\n+      docsWithScore.push([document, item.score]);\n+    }\n+\n+    return docsWithScore;\n+  }\n+\n+  /**\n+   * Performs a similarity search on the vectors stored in the collection.\n+   * @param queryVector Query vector for the similarity search.\n+   * @param k=4 Number of nearest neighbors to return.\n+   * @param filter string OData filter for the documents.\n+   * @returns Promise that resolves to a list of documents and their corresponding similarity scores.\n+   */\n+  async similaritySearchVectorWithScore(\n+    query: number[],\n+    k: number,\n+    filter?: string\n+  ): Promise<[Document, number][]> {\n+    await this.initPromise;\n+\n+    const { results } = await this.client.search(\"*\", {\n+      vectorSearchOptions: {\n+        queries: [\n+          {\n+            kind: \"vector\",\n+            vector: query,\n+            kNearestNeighborsCount: k,\n+            fields: [DEFAULT_FIELD_CONTENT_VECTOR],\n+          },\n+        ],\n+      },\n+      filter,\n+    });\n+\n+    const docsWithScore: [Document, number][] = [];\n+\n+    for await (const item of results) {\n+      const document = new Document<\n+        AzureAISearchDocumentMetadata & { embedding: number[] }\n+      >({\n+        pageContent: item.document[DEFAULT_FIELD_CONTENT],\n+        metadata: {\n+          ...item.document[DEFAULT_FIELD_METADATA],\n+          embedding: item.document[DEFAULT_FIELD_CONTENT_VECTOR],\n+        },\n+      });\n+      docsWithScore.push([document, item.score]);\n+    }\n+\n+    return docsWithScore;\n+  }\n+\n+  /**\n+   * Return documents selected using the maximal marginal relevance.\n+   * Maximal marginal relevance optimizes for similarity to the query AND\n+   * diversity among selected documents.\n+   * @param query Text to look up documents similar to.\n+   * @param options.k Number of documents to return.\n+   * @param options.fetchK=20 Number of documents to fetch before passing to\n+   *     the MMR algorithm.\n+   * @param options.lambda=0.5 Number between 0 and 1 that determines the\n+   *     degree of diversity among the results, where 0 corresponds to maximum\n+   *     diversity and 1 to minimum diversity.\n+   * @returns List of documents selected by maximal marginal relevance.\n+   */\n+  async maxMarginalRelevanceSearch(\n+    query: string,\n+    options: MaxMarginalRelevanceSearchOptions<this[\"FilterType\"]>\n+  ): Promise<Document[]> {\n+    const { k, fetchK = 20, lambda = 0.5 } = options;\n+\n+    const queryEmbedding = await this.embeddings.embedQuery(query);\n+    const docs = await this.similaritySearchVectorWithScore(\n+      queryEmbedding,\n+      fetchK\n+    );\n+    const embeddingList = docs.map((doc) => doc[0].metadata.embedding);\n+\n+    // Re-rank the results using MMR\n+    const mmrIndexes = maximalMarginalRelevance(\n+      queryEmbedding,\n+      embeddingList,\n+      lambda,\n+      k\n+    );\n+\n+    const mmrDocs = mmrIndexes.map((index) => docs[index][0]);\n+    return mmrDocs;\n+  }\n+\n+  /**\n+   * Ensures that an index exists on the AzureAISearchVectorStore.\n+   * @param indexClient The Azure AI Search index client.\n+   * @returns A promise that resolves when the AzureAISearchVectorStore index has been initialized.\n+   * @protected\n+   */\n+  protected async ensureIndexExists(\n+    indexClient: SearchIndexClient\n+  ): Promise<void> {\n+    try {\n+      await indexClient.getIndex(this.indexName);\n+    } catch (e) {\n+      // Index does not exists, create it\n+      const searchIndex = this.createSearchIndexDefinition(this.indexName);\n+      await indexClient.createIndex(searchIndex);\n+    }\n+  }\n+\n+  /**\n+   * Prepares the search index definition for Azure AI Search.\n+   * @param indexName The name of the index.\n+   * @returns The SearchIndex object.\n+   * @protected\n+   */\n+  protected createSearchIndexDefinition(indexName: string): SearchIndex {\n+    return {\n+      name: indexName,\n+      vectorSearch: {\n+        algorithms: [\n+          {\n+            name: \"vector-search-algorithm\",\n+            kind: \"hnsw\",\n+            parameters: {\n+              m: 4,\n+              efSearch: 500,\n+              metric: \"cosine\",\n+              efConstruction: 400,\n+            },\n+          },\n+        ],\n+        profiles: [\n+          {\n+            name: \"vector-search-profile\",\n+            algorithmConfigurationName: \"vector-search-algorithm\",\n+          },\n+        ],\n+      },\n+      semanticSearch: {\n+        defaultConfigurationName: \"semantic-search-config\",\n+        configurations: [\n+          {\n+            name: \"semantic-search-config\",\n+            prioritizedFields: {\n+              contentFields: [\n+                {\n+                  name: DEFAULT_FIELD_CONTENT,\n+                },\n+              ],\n+              keywordsFields: [\n+                {\n+                  name: DEFAULT_FIELD_CONTENT,\n+                },\n+              ],\n+            },\n+          },\n+        ],\n+      },\n+      fields: [\n+        {\n+          name: DEFAULT_FIELD_ID,\n+          filterable: true,\n+          key: true,\n+          type: \"Edm.String\",\n+        },\n+        {\n+          name: DEFAULT_FIELD_CONTENT,\n+          searchable: true,\n+          filterable: true,\n+          type: \"Edm.String\",\n+        },\n+        {\n+          name: DEFAULT_FIELD_CONTENT_VECTOR,\n+          searchable: true,\n+          type: \"Collection(Edm.Single)\",\n+          vectorSearchDimensions: 1536,\n+          vectorSearchProfileName: \"vector-search-profile\",\n+        },\n+        {\n+          name: DEFAULT_FIELD_METADATA,\n+          type: \"Edm.ComplexType\",",
        "comment_created_at": "2024-01-18T10:59:17+00:00",
        "comment_author": "sinedied",
        "comment_body": "Yes, I'm not quite happy with that one and it comes from the base implementation I started from. \r\nBut I thought as a default as least it makes it easier for the users who do not want to bother with the schema, as LC allows for arbitrary metadata.\r\n\r\nThat's also why I said I wand to provide more customizations options in another PR to allow for custom index schema for the users who wants better control over the data and filters.",
        "pr_file_module": null
      }
    ]
  }
]