[
  {
    "discussion_id": "2101515141",
    "pr_number": 8007,
    "pr_file": "libs/langchain-community/src/document_loaders/fs/oracle.ts",
    "created_at": "2025-05-22T02:44:48+00:00",
    "commented_code": "import { Document } from \"@langchain/core/documents\";\nimport fs from \"node:fs\";\nimport path from \"node:path\";\nimport oracledb from \"oracledb\";\nimport * as htmlparser2 from \"htmlparser2\";\nimport { BaseDocumentLoader } from \"@langchain/core/document_loaders/base\";\n\nfunction* listDir(dir: string): Generator<string> {\n  const files = fs.readdirSync(dir, { withFileTypes: true });\n\n  for (const file of files) {\n    if (file.isDirectory()) {\n      yield* listDir(path.join(dir, file.name));\n    } else {\n      yield path.join(dir, file.name);\n    }\n  }\n}\n\n/**\n * Load documents from a file, a directory, or a table\n * If the document isn't a plain text file such as a PDF,\n * a plain text version will be extracted using utl_to_text\n * @example\n * ```typescript\n * const loader = new OracleDocLoader(conn, params);\n * const docs = await loader.load();\n * ```\n */\nexport class OracleDocLoader extends BaseDocumentLoader {\n  protected conn: oracledb.Connection;\n\n  protected pref: Record<string, any>;\n\n  constructor(conn: oracledb.Connection, pref: Record<string, any>) {\n    super();\n    this.conn = conn;\n    this.pref = pref;\n  }\n\n  /**\n   * A method that loads the text file or blob and returns a promise that\n   * resolves to an array of `Document` instances. It reads the text from\n   * the file or blob using the `readFile` function from the\n   * `node:fs/promises` module or the `text()` method of the blob. It then\n   * parses the text using the `parse()` method and creates a `Document`\n   * instance for each parsed page. The metadata includes the source of the\n   * text (file path or blob) and, if there are multiple pages, the line\n   * number of each page.\n   * @returns A promise that resolves to an array of `Document` instances.\n   */\n  async load() {\n    const docs = [];\n\n    if (\"file\" in this.pref) {\n      // don't specify an encoding to use binary\n      const data = fs.readFileSync(this.pref.file);\n\n      const result = await this.conn.execute(\n        <string>(\n          `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n        ),\n        <oracledb.BindParameters>{\n          content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n          pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n        },\n        <oracledb.ExecuteOptions>(<unknown>{\n          resultSet: true, // return a ResultSet (default is false)\n          fetchInfo: {\n            TEXT: { type: oracledb.STRING },\n            METADATA: { type: oracledb.STRING },\n          },\n        })\n      );\n\n      const rs = result.resultSet;\n      let row;\n      if (rs != null) {\n        while ((row = await rs.getRow())) {\n          const [plain_text, metadata] = await this._extract(row);\n          docs.push(\n            new Document({\n              pageContent: <string>plain_text,\n              metadata: <Record<string, any>>metadata,\n            })\n          );\n        }\n        await rs.close();\n      }\n    } else if (\"dir\" in this.pref) {\n      for (const file of listDir(this.pref.dir)) {\n        // don't specify an encoding to use binary\n        const data = fs.readFileSync(file);\n\n        const result = await this.conn.execute(\n          <string>(\n            `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n          ),\n          <oracledb.BindParameters>{\n            content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n            pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n          },\n          <oracledb.ExecuteOptions>(<unknown>{\n            resultSet: true, // return a ResultSet (default is false)\n            fetchInfo: {\n              TEXT: { type: oracledb.STRING },\n              METADATA: { type: oracledb.STRING },\n            },\n          })\n        );\n\n        const rs = result.resultSet;\n        let row;\n        if (rs != null) {\n          while ((row = await rs.getRow())) {\n            const [plain_text, metadata] = await this._extract(row);\n            docs.push(\n              new Document({\n                pageContent: <string>plain_text,\n                metadata: <Record<string, any>>metadata,\n              })\n            );\n          }\n          await rs.close();\n        }\n      }\n    } else if (\"tablename\" in this.pref) {\n      if (!(\"owner\" in this.pref) || !(\"colname\" in this.pref)) {\n        throw new Error(`Invalid preferences: missing owner or colname`);\n      }\n      // SQL doesn't accept backslash to escape a double quote (\\\"). If string contains \\\" change to \"\n      if (\n        this.pref.tablename.startsWith('\\\\\"') &&\n        this.pref.tablename.endsWith('\\\\\"')\n      ) {\n        this.pref.tablename = this.pref.tablename.replaceAll(\"\\\\\", \"\");\n      }\n      const result = await this.conn.execute(\n        <string>(\n          `select dbms_vector_chain.utl_to_text(t.${this.pref.colname}, :pref) text, dbms_vector_chain.utl_to_text(t.${this.pref.colname}, json('{\"plaintext\": \"false\"}')) metadata from ${this.pref.owner}.${this.pref.tablename} t`",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "2101515141",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8007,
        "pr_file": "libs/langchain-community/src/document_loaders/fs/oracle.ts",
        "discussion_id": "2101515141",
        "commented_code": "@@ -0,0 +1,197 @@\n+import { Document } from \"@langchain/core/documents\";\n+import fs from \"node:fs\";\n+import path from \"node:path\";\n+import oracledb from \"oracledb\";\n+import * as htmlparser2 from \"htmlparser2\";\n+import { BaseDocumentLoader } from \"@langchain/core/document_loaders/base\";\n+\n+function* listDir(dir: string): Generator<string> {\n+  const files = fs.readdirSync(dir, { withFileTypes: true });\n+\n+  for (const file of files) {\n+    if (file.isDirectory()) {\n+      yield* listDir(path.join(dir, file.name));\n+    } else {\n+      yield path.join(dir, file.name);\n+    }\n+  }\n+}\n+\n+/**\n+ * Load documents from a file, a directory, or a table\n+ * If the document isn't a plain text file such as a PDF,\n+ * a plain text version will be extracted using utl_to_text\n+ * @example\n+ * ```typescript\n+ * const loader = new OracleDocLoader(conn, params);\n+ * const docs = await loader.load();\n+ * ```\n+ */\n+export class OracleDocLoader extends BaseDocumentLoader {\n+  protected conn: oracledb.Connection;\n+\n+  protected pref: Record<string, any>;\n+\n+  constructor(conn: oracledb.Connection, pref: Record<string, any>) {\n+    super();\n+    this.conn = conn;\n+    this.pref = pref;\n+  }\n+\n+  /**\n+   * A method that loads the text file or blob and returns a promise that\n+   * resolves to an array of `Document` instances. It reads the text from\n+   * the file or blob using the `readFile` function from the\n+   * `node:fs/promises` module or the `text()` method of the blob. It then\n+   * parses the text using the `parse()` method and creates a `Document`\n+   * instance for each parsed page. The metadata includes the source of the\n+   * text (file path or blob) and, if there are multiple pages, the line\n+   * number of each page.\n+   * @returns A promise that resolves to an array of `Document` instances.\n+   */\n+  async load() {\n+    const docs = [];\n+\n+    if (\"file\" in this.pref) {\n+      // don't specify an encoding to use binary\n+      const data = fs.readFileSync(this.pref.file);\n+\n+      const result = await this.conn.execute(\n+        <string>(\n+          `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n+        ),\n+        <oracledb.BindParameters>{\n+          content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n+          pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n+        },\n+        <oracledb.ExecuteOptions>(<unknown>{\n+          resultSet: true, // return a ResultSet (default is false)\n+          fetchInfo: {\n+            TEXT: { type: oracledb.STRING },\n+            METADATA: { type: oracledb.STRING },\n+          },\n+        })\n+      );\n+\n+      const rs = result.resultSet;\n+      let row;\n+      if (rs != null) {\n+        while ((row = await rs.getRow())) {\n+          const [plain_text, metadata] = await this._extract(row);\n+          docs.push(\n+            new Document({\n+              pageContent: <string>plain_text,\n+              metadata: <Record<string, any>>metadata,\n+            })\n+          );\n+        }\n+        await rs.close();\n+      }\n+    } else if (\"dir\" in this.pref) {\n+      for (const file of listDir(this.pref.dir)) {\n+        // don't specify an encoding to use binary\n+        const data = fs.readFileSync(file);\n+\n+        const result = await this.conn.execute(\n+          <string>(\n+            `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n+          ),\n+          <oracledb.BindParameters>{\n+            content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n+            pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n+          },\n+          <oracledb.ExecuteOptions>(<unknown>{\n+            resultSet: true, // return a ResultSet (default is false)\n+            fetchInfo: {\n+              TEXT: { type: oracledb.STRING },\n+              METADATA: { type: oracledb.STRING },\n+            },\n+          })\n+        );\n+\n+        const rs = result.resultSet;\n+        let row;\n+        if (rs != null) {\n+          while ((row = await rs.getRow())) {\n+            const [plain_text, metadata] = await this._extract(row);\n+            docs.push(\n+              new Document({\n+                pageContent: <string>plain_text,\n+                metadata: <Record<string, any>>metadata,\n+              })\n+            );\n+          }\n+          await rs.close();\n+        }\n+      }\n+    } else if (\"tablename\" in this.pref) {\n+      if (!(\"owner\" in this.pref) || !(\"colname\" in this.pref)) {\n+        throw new Error(`Invalid preferences: missing owner or colname`);\n+      }\n+      // SQL doesn't accept backslash to escape a double quote (\\\"). If string contains \\\" change to \"\n+      if (\n+        this.pref.tablename.startsWith('\\\\\"') &&\n+        this.pref.tablename.endsWith('\\\\\"')\n+      ) {\n+        this.pref.tablename = this.pref.tablename.replaceAll(\"\\\\\", \"\");\n+      }\n+      const result = await this.conn.execute(\n+        <string>(\n+          `select dbms_vector_chain.utl_to_text(t.${this.pref.colname}, :pref) text, dbms_vector_chain.utl_to_text(t.${this.pref.colname}, json('{\"plaintext\": \"false\"}')) metadata from ${this.pref.owner}.${this.pref.tablename} t`",
        "comment_created_at": "2025-05-22T02:44:48+00:00",
        "comment_author": "cjbj",
        "comment_body": "Have the substituted values been filtered to avoid any SQL Injection issues?",
        "pr_file_module": null
      },
      {
        "comment_id": "2103199788",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8007,
        "pr_file": "libs/langchain-community/src/document_loaders/fs/oracle.ts",
        "discussion_id": "2101515141",
        "commented_code": "@@ -0,0 +1,197 @@\n+import { Document } from \"@langchain/core/documents\";\n+import fs from \"node:fs\";\n+import path from \"node:path\";\n+import oracledb from \"oracledb\";\n+import * as htmlparser2 from \"htmlparser2\";\n+import { BaseDocumentLoader } from \"@langchain/core/document_loaders/base\";\n+\n+function* listDir(dir: string): Generator<string> {\n+  const files = fs.readdirSync(dir, { withFileTypes: true });\n+\n+  for (const file of files) {\n+    if (file.isDirectory()) {\n+      yield* listDir(path.join(dir, file.name));\n+    } else {\n+      yield path.join(dir, file.name);\n+    }\n+  }\n+}\n+\n+/**\n+ * Load documents from a file, a directory, or a table\n+ * If the document isn't a plain text file such as a PDF,\n+ * a plain text version will be extracted using utl_to_text\n+ * @example\n+ * ```typescript\n+ * const loader = new OracleDocLoader(conn, params);\n+ * const docs = await loader.load();\n+ * ```\n+ */\n+export class OracleDocLoader extends BaseDocumentLoader {\n+  protected conn: oracledb.Connection;\n+\n+  protected pref: Record<string, any>;\n+\n+  constructor(conn: oracledb.Connection, pref: Record<string, any>) {\n+    super();\n+    this.conn = conn;\n+    this.pref = pref;\n+  }\n+\n+  /**\n+   * A method that loads the text file or blob and returns a promise that\n+   * resolves to an array of `Document` instances. It reads the text from\n+   * the file or blob using the `readFile` function from the\n+   * `node:fs/promises` module or the `text()` method of the blob. It then\n+   * parses the text using the `parse()` method and creates a `Document`\n+   * instance for each parsed page. The metadata includes the source of the\n+   * text (file path or blob) and, if there are multiple pages, the line\n+   * number of each page.\n+   * @returns A promise that resolves to an array of `Document` instances.\n+   */\n+  async load() {\n+    const docs = [];\n+\n+    if (\"file\" in this.pref) {\n+      // don't specify an encoding to use binary\n+      const data = fs.readFileSync(this.pref.file);\n+\n+      const result = await this.conn.execute(\n+        <string>(\n+          `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n+        ),\n+        <oracledb.BindParameters>{\n+          content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n+          pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n+        },\n+        <oracledb.ExecuteOptions>(<unknown>{\n+          resultSet: true, // return a ResultSet (default is false)\n+          fetchInfo: {\n+            TEXT: { type: oracledb.STRING },\n+            METADATA: { type: oracledb.STRING },\n+          },\n+        })\n+      );\n+\n+      const rs = result.resultSet;\n+      let row;\n+      if (rs != null) {\n+        while ((row = await rs.getRow())) {\n+          const [plain_text, metadata] = await this._extract(row);\n+          docs.push(\n+            new Document({\n+              pageContent: <string>plain_text,\n+              metadata: <Record<string, any>>metadata,\n+            })\n+          );\n+        }\n+        await rs.close();\n+      }\n+    } else if (\"dir\" in this.pref) {\n+      for (const file of listDir(this.pref.dir)) {\n+        // don't specify an encoding to use binary\n+        const data = fs.readFileSync(file);\n+\n+        const result = await this.conn.execute(\n+          <string>(\n+            `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n+          ),\n+          <oracledb.BindParameters>{\n+            content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n+            pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n+          },\n+          <oracledb.ExecuteOptions>(<unknown>{\n+            resultSet: true, // return a ResultSet (default is false)\n+            fetchInfo: {\n+              TEXT: { type: oracledb.STRING },\n+              METADATA: { type: oracledb.STRING },\n+            },\n+          })\n+        );\n+\n+        const rs = result.resultSet;\n+        let row;\n+        if (rs != null) {\n+          while ((row = await rs.getRow())) {\n+            const [plain_text, metadata] = await this._extract(row);\n+            docs.push(\n+              new Document({\n+                pageContent: <string>plain_text,\n+                metadata: <Record<string, any>>metadata,\n+              })\n+            );\n+          }\n+          await rs.close();\n+        }\n+      }\n+    } else if (\"tablename\" in this.pref) {\n+      if (!(\"owner\" in this.pref) || !(\"colname\" in this.pref)) {\n+        throw new Error(`Invalid preferences: missing owner or colname`);\n+      }\n+      // SQL doesn't accept backslash to escape a double quote (\\\"). If string contains \\\" change to \"\n+      if (\n+        this.pref.tablename.startsWith('\\\\\"') &&\n+        this.pref.tablename.endsWith('\\\\\"')\n+      ) {\n+        this.pref.tablename = this.pref.tablename.replaceAll(\"\\\\\", \"\");\n+      }\n+      const result = await this.conn.execute(\n+        <string>(\n+          `select dbms_vector_chain.utl_to_text(t.${this.pref.colname}, :pref) text, dbms_vector_chain.utl_to_text(t.${this.pref.colname}, json('{\"plaintext\": \"false\"}')) metadata from ${this.pref.owner}.${this.pref.tablename} t`",
        "comment_created_at": "2025-05-22T18:46:39+00:00",
        "comment_author": "hackerdave",
        "comment_body": "Have added a check",
        "pr_file_module": null
      },
      {
        "comment_id": "2103462930",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8007,
        "pr_file": "libs/langchain-community/src/document_loaders/fs/oracle.ts",
        "discussion_id": "2101515141",
        "commented_code": "@@ -0,0 +1,197 @@\n+import { Document } from \"@langchain/core/documents\";\n+import fs from \"node:fs\";\n+import path from \"node:path\";\n+import oracledb from \"oracledb\";\n+import * as htmlparser2 from \"htmlparser2\";\n+import { BaseDocumentLoader } from \"@langchain/core/document_loaders/base\";\n+\n+function* listDir(dir: string): Generator<string> {\n+  const files = fs.readdirSync(dir, { withFileTypes: true });\n+\n+  for (const file of files) {\n+    if (file.isDirectory()) {\n+      yield* listDir(path.join(dir, file.name));\n+    } else {\n+      yield path.join(dir, file.name);\n+    }\n+  }\n+}\n+\n+/**\n+ * Load documents from a file, a directory, or a table\n+ * If the document isn't a plain text file such as a PDF,\n+ * a plain text version will be extracted using utl_to_text\n+ * @example\n+ * ```typescript\n+ * const loader = new OracleDocLoader(conn, params);\n+ * const docs = await loader.load();\n+ * ```\n+ */\n+export class OracleDocLoader extends BaseDocumentLoader {\n+  protected conn: oracledb.Connection;\n+\n+  protected pref: Record<string, any>;\n+\n+  constructor(conn: oracledb.Connection, pref: Record<string, any>) {\n+    super();\n+    this.conn = conn;\n+    this.pref = pref;\n+  }\n+\n+  /**\n+   * A method that loads the text file or blob and returns a promise that\n+   * resolves to an array of `Document` instances. It reads the text from\n+   * the file or blob using the `readFile` function from the\n+   * `node:fs/promises` module or the `text()` method of the blob. It then\n+   * parses the text using the `parse()` method and creates a `Document`\n+   * instance for each parsed page. The metadata includes the source of the\n+   * text (file path or blob) and, if there are multiple pages, the line\n+   * number of each page.\n+   * @returns A promise that resolves to an array of `Document` instances.\n+   */\n+  async load() {\n+    const docs = [];\n+\n+    if (\"file\" in this.pref) {\n+      // don't specify an encoding to use binary\n+      const data = fs.readFileSync(this.pref.file);\n+\n+      const result = await this.conn.execute(\n+        <string>(\n+          `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n+        ),\n+        <oracledb.BindParameters>{\n+          content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n+          pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n+        },\n+        <oracledb.ExecuteOptions>(<unknown>{\n+          resultSet: true, // return a ResultSet (default is false)\n+          fetchInfo: {\n+            TEXT: { type: oracledb.STRING },\n+            METADATA: { type: oracledb.STRING },\n+          },\n+        })\n+      );\n+\n+      const rs = result.resultSet;\n+      let row;\n+      if (rs != null) {\n+        while ((row = await rs.getRow())) {\n+          const [plain_text, metadata] = await this._extract(row);\n+          docs.push(\n+            new Document({\n+              pageContent: <string>plain_text,\n+              metadata: <Record<string, any>>metadata,\n+            })\n+          );\n+        }\n+        await rs.close();\n+      }\n+    } else if (\"dir\" in this.pref) {\n+      for (const file of listDir(this.pref.dir)) {\n+        // don't specify an encoding to use binary\n+        const data = fs.readFileSync(file);\n+\n+        const result = await this.conn.execute(\n+          <string>(\n+            `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n+          ),\n+          <oracledb.BindParameters>{\n+            content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n+            pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n+          },\n+          <oracledb.ExecuteOptions>(<unknown>{\n+            resultSet: true, // return a ResultSet (default is false)\n+            fetchInfo: {\n+              TEXT: { type: oracledb.STRING },\n+              METADATA: { type: oracledb.STRING },\n+            },\n+          })\n+        );\n+\n+        const rs = result.resultSet;\n+        let row;\n+        if (rs != null) {\n+          while ((row = await rs.getRow())) {\n+            const [plain_text, metadata] = await this._extract(row);\n+            docs.push(\n+              new Document({\n+                pageContent: <string>plain_text,\n+                metadata: <Record<string, any>>metadata,\n+              })\n+            );\n+          }\n+          await rs.close();\n+        }\n+      }\n+    } else if (\"tablename\" in this.pref) {\n+      if (!(\"owner\" in this.pref) || !(\"colname\" in this.pref)) {\n+        throw new Error(`Invalid preferences: missing owner or colname`);\n+      }\n+      // SQL doesn't accept backslash to escape a double quote (\\\"). If string contains \\\" change to \"\n+      if (\n+        this.pref.tablename.startsWith('\\\\\"') &&\n+        this.pref.tablename.endsWith('\\\\\"')\n+      ) {\n+        this.pref.tablename = this.pref.tablename.replaceAll(\"\\\\\", \"\");\n+      }\n+      const result = await this.conn.execute(\n+        <string>(\n+          `select dbms_vector_chain.utl_to_text(t.${this.pref.colname}, :pref) text, dbms_vector_chain.utl_to_text(t.${this.pref.colname}, json('{\"plaintext\": \"false\"}')) metadata from ${this.pref.owner}.${this.pref.tablename} t`",
        "comment_created_at": "2025-05-22T22:07:14+00:00",
        "comment_author": "cjbj",
        "comment_body": "- You need to bind those values !\r\n- Would it be nice to throw a custom error message if the check fails?\r\n\r\n```\r\n  const cn = 'empno';\r\n  const ow = 'scott';\r\n  const tn = 'emp';\r\n  const qn = `${ow}.${tn}`;\r\n  const sql = `select sys.dbms_assert.simple_sql_name(:sn), sys.dbms_assert.qualified_sql_name(:qn) from dual`;\r\n  const binds = [cn, qn];\r\n  await connection.execute(sql, binds);\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2110532871",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 8007,
        "pr_file": "libs/langchain-community/src/document_loaders/fs/oracle.ts",
        "discussion_id": "2101515141",
        "commented_code": "@@ -0,0 +1,197 @@\n+import { Document } from \"@langchain/core/documents\";\n+import fs from \"node:fs\";\n+import path from \"node:path\";\n+import oracledb from \"oracledb\";\n+import * as htmlparser2 from \"htmlparser2\";\n+import { BaseDocumentLoader } from \"@langchain/core/document_loaders/base\";\n+\n+function* listDir(dir: string): Generator<string> {\n+  const files = fs.readdirSync(dir, { withFileTypes: true });\n+\n+  for (const file of files) {\n+    if (file.isDirectory()) {\n+      yield* listDir(path.join(dir, file.name));\n+    } else {\n+      yield path.join(dir, file.name);\n+    }\n+  }\n+}\n+\n+/**\n+ * Load documents from a file, a directory, or a table\n+ * If the document isn't a plain text file such as a PDF,\n+ * a plain text version will be extracted using utl_to_text\n+ * @example\n+ * ```typescript\n+ * const loader = new OracleDocLoader(conn, params);\n+ * const docs = await loader.load();\n+ * ```\n+ */\n+export class OracleDocLoader extends BaseDocumentLoader {\n+  protected conn: oracledb.Connection;\n+\n+  protected pref: Record<string, any>;\n+\n+  constructor(conn: oracledb.Connection, pref: Record<string, any>) {\n+    super();\n+    this.conn = conn;\n+    this.pref = pref;\n+  }\n+\n+  /**\n+   * A method that loads the text file or blob and returns a promise that\n+   * resolves to an array of `Document` instances. It reads the text from\n+   * the file or blob using the `readFile` function from the\n+   * `node:fs/promises` module or the `text()` method of the blob. It then\n+   * parses the text using the `parse()` method and creates a `Document`\n+   * instance for each parsed page. The metadata includes the source of the\n+   * text (file path or blob) and, if there are multiple pages, the line\n+   * number of each page.\n+   * @returns A promise that resolves to an array of `Document` instances.\n+   */\n+  async load() {\n+    const docs = [];\n+\n+    if (\"file\" in this.pref) {\n+      // don't specify an encoding to use binary\n+      const data = fs.readFileSync(this.pref.file);\n+\n+      const result = await this.conn.execute(\n+        <string>(\n+          `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n+        ),\n+        <oracledb.BindParameters>{\n+          content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n+          pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n+        },\n+        <oracledb.ExecuteOptions>(<unknown>{\n+          resultSet: true, // return a ResultSet (default is false)\n+          fetchInfo: {\n+            TEXT: { type: oracledb.STRING },\n+            METADATA: { type: oracledb.STRING },\n+          },\n+        })\n+      );\n+\n+      const rs = result.resultSet;\n+      let row;\n+      if (rs != null) {\n+        while ((row = await rs.getRow())) {\n+          const [plain_text, metadata] = await this._extract(row);\n+          docs.push(\n+            new Document({\n+              pageContent: <string>plain_text,\n+              metadata: <Record<string, any>>metadata,\n+            })\n+          );\n+        }\n+        await rs.close();\n+      }\n+    } else if (\"dir\" in this.pref) {\n+      for (const file of listDir(this.pref.dir)) {\n+        // don't specify an encoding to use binary\n+        const data = fs.readFileSync(file);\n+\n+        const result = await this.conn.execute(\n+          <string>(\n+            `select dbms_vector_chain.utl_to_text(:content, :pref) text, dbms_vector_chain.utl_to_text(:content, json('{\"plaintext\": \"false\"}')) metadata from dual`\n+          ),\n+          <oracledb.BindParameters>{\n+            content: { val: data, dir: oracledb.BIND_IN, type: oracledb.BLOB },\n+            pref: { val: this.pref, type: oracledb.DB_TYPE_JSON },\n+          },\n+          <oracledb.ExecuteOptions>(<unknown>{\n+            resultSet: true, // return a ResultSet (default is false)\n+            fetchInfo: {\n+              TEXT: { type: oracledb.STRING },\n+              METADATA: { type: oracledb.STRING },\n+            },\n+          })\n+        );\n+\n+        const rs = result.resultSet;\n+        let row;\n+        if (rs != null) {\n+          while ((row = await rs.getRow())) {\n+            const [plain_text, metadata] = await this._extract(row);\n+            docs.push(\n+              new Document({\n+                pageContent: <string>plain_text,\n+                metadata: <Record<string, any>>metadata,\n+              })\n+            );\n+          }\n+          await rs.close();\n+        }\n+      }\n+    } else if (\"tablename\" in this.pref) {\n+      if (!(\"owner\" in this.pref) || !(\"colname\" in this.pref)) {\n+        throw new Error(`Invalid preferences: missing owner or colname`);\n+      }\n+      // SQL doesn't accept backslash to escape a double quote (\\\"). If string contains \\\" change to \"\n+      if (\n+        this.pref.tablename.startsWith('\\\\\"') &&\n+        this.pref.tablename.endsWith('\\\\\"')\n+      ) {\n+        this.pref.tablename = this.pref.tablename.replaceAll(\"\\\\\", \"\");\n+      }\n+      const result = await this.conn.execute(\n+        <string>(\n+          `select dbms_vector_chain.utl_to_text(t.${this.pref.colname}, :pref) text, dbms_vector_chain.utl_to_text(t.${this.pref.colname}, json('{\"plaintext\": \"false\"}')) metadata from ${this.pref.owner}.${this.pref.tablename} t`",
        "comment_created_at": "2025-05-27T23:37:33+00:00",
        "comment_author": "hackerdave",
        "comment_body": "Changed to bind values and added a custom error message as suggested",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2004662752",
    "pr_number": 7852,
    "pr_file": "libs/langchain-google-cloud-sql-pg/src/chatMessageHistory.ts",
    "created_at": "2025-03-20T02:03:37+00:00",
    "commented_code": "import { BaseChatMessageHistory } from \"@langchain/core/chat_history\";\nimport { AIMessage, BaseMessage, HumanMessage, StoredMessage, mapStoredMessagesToChatMessages } from \"@langchain/core/messages\";\nimport PostgresEngine from \"./engine.js\";\n\nexport interface PostgresChatMessageHistoryInput {\n  engine: PostgresEngine;\n  sessionId: string;\n  tableName: string;\n  schemaName: string;\n}\n\nexport class PostgresChatMessageHistory extends BaseChatMessageHistory {\n  lc_namespace: string[] = [\"langchain\", \"stores\", \"message\", \"google-cloud-sql-pg\"];\n\n  engine: PostgresEngine;\n\n  sessionId: string;\n\n  tableName: string;\n\n  schemaName: string;\n\n  constructor({ engine, sessionId, tableName, schemaName = \"public\" }: PostgresChatMessageHistoryInput) {\n    super();\n    this.engine = engine;\n    this.sessionId = sessionId;\n    this.tableName = tableName;\n    this.schemaName = schemaName;\n  }\n\n  /**\n   * Create a new PostgresChatMessageHistory instance.\n   *\n   * @param {PostgresEngine} engine Postgres engine instance to use.\n   * @param {string} sessionId Retrieve the table content witht this session ID.\n   * @param {string} tableName Table name that stores that chat message history.\n   * @param {string} schemaName Schema name for the chat message history table. Default: \"public\".\n   * @returns PostgresChatMessageHistory instance.\n   */\n  static async create(\n    engine: PostgresEngine,\n    sessionId: string,\n    tableName: string,\n    schemaName: string = \"public\"\n  ) {\n    const query = `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '${tableName}' AND table_schema = '${schemaName}'`;",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "2004662752",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7852,
        "pr_file": "libs/langchain-google-cloud-sql-pg/src/chatMessageHistory.ts",
        "discussion_id": "2004662752",
        "commented_code": "@@ -0,0 +1,143 @@\n+import { BaseChatMessageHistory } from \"@langchain/core/chat_history\";\n+import { AIMessage, BaseMessage, HumanMessage, StoredMessage, mapStoredMessagesToChatMessages } from \"@langchain/core/messages\";\n+import PostgresEngine from \"./engine.js\";\n+\n+export interface PostgresChatMessageHistoryInput {\n+  engine: PostgresEngine;\n+  sessionId: string;\n+  tableName: string;\n+  schemaName: string;\n+}\n+\n+export class PostgresChatMessageHistory extends BaseChatMessageHistory {\n+  lc_namespace: string[] = [\"langchain\", \"stores\", \"message\", \"google-cloud-sql-pg\"];\n+\n+  engine: PostgresEngine;\n+\n+  sessionId: string;\n+\n+  tableName: string;\n+\n+  schemaName: string;\n+\n+  constructor({ engine, sessionId, tableName, schemaName = \"public\" }: PostgresChatMessageHistoryInput) {\n+    super();\n+    this.engine = engine;\n+    this.sessionId = sessionId;\n+    this.tableName = tableName;\n+    this.schemaName = schemaName;\n+  }\n+\n+  /**\n+   * Create a new PostgresChatMessageHistory instance.\n+   *\n+   * @param {PostgresEngine} engine Postgres engine instance to use.\n+   * @param {string} sessionId Retrieve the table content witht this session ID.\n+   * @param {string} tableName Table name that stores that chat message history.\n+   * @param {string} schemaName Schema name for the chat message history table. Default: \"public\".\n+   * @returns PostgresChatMessageHistory instance.\n+   */\n+  static async create(\n+    engine: PostgresEngine,\n+    sessionId: string,\n+    tableName: string,\n+    schemaName: string = \"public\"\n+  ) {\n+    const query = `SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '${tableName}' AND table_schema = '${schemaName}'`;",
        "comment_created_at": "2025-03-20T02:03:37+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "May be worth a small note in docs emphasizing that `tableName` and `schemaName` in these files are not escaped and to not pass in end-user input here",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1901267827",
    "pr_number": 7360,
    "pr_file": "libs/langchain-community/src/vectorstores/mariadb.ts",
    "created_at": "2025-01-02T21:19:03+00:00",
    "commented_code": "import mariadb, { type Pool, type PoolConfig } from \"mariadb\";\nimport { VectorStore } from \"@langchain/core/vectorstores\";\nimport type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\nimport { Document } from \"@langchain/core/documents\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport {\n  FilterExpressionConverter,\n  StringBuilder,\n  BaseFilterExpressionConverter,\n  Expression,\n  Value,\n  Key,\n  Group,\n} from \"@langchain/core/filter\";\n\ntype Metadata = Record<string, unknown>;\n\nexport type DistanceStrategy = \"COSINE\" | \"EUCLIDEAN\";\n\n/**\n * Converts Expression into JSON metadata filter expression format\n * for MariaDB\n */\nclass MariaDBFilterExpressionConverter\n  extends BaseFilterExpressionConverter\n  implements FilterExpressionConverter\n{\n  private metadataFieldName: string;\n\n  constructor(metadataFieldName: string) {\n    super();\n    this.metadataFieldName = metadataFieldName;\n  }\n\n  convertExpressionToContext(\n    expression: Expression,\n    context: StringBuilder\n  ): void {\n    super.convertOperandToContext(expression.left, context);\n    super.convertSymbolToContext(expression, context);\n    if (expression.right) {\n      super.convertOperandToContext(expression.right, context);\n    }\n  }\n\n  convertKeyToContext(key: Key, context: StringBuilder): void {\n    context.append(`JSON_VALUE(${this.metadataFieldName}, '$.${key.key}')`);\n  }\n\n  writeValueRangeStart(_listValue: Value, context: StringBuilder): void {\n    context.append(\"(\");\n  }\n\n  writeValueRangeEnd(_listValue: Value, context: StringBuilder): void {\n    context.append(\")\");\n  }\n\n  writeGroupStart(_group: Group, context: StringBuilder): void {\n    context.append(\"(\");\n  }\n\n  writeGroupEnd(_group: Group, context: StringBuilder): void {\n    context.append(\")\");\n  }\n}\n\n/**\n * Interface that defines the arguments required to create a\n * `MariaDBStore` instance. It includes MariaDB connection options,\n * table name and verbosity level.\n */\nexport interface MariaDBStoreArgs {\n  connectionOptions?: PoolConfig;\n  pool?: Pool;\n  tableName?: string;\n  collectionTableName?: string;\n  collectionName?: string;\n  collectionMetadata?: Metadata | null;\n  schemaName?: string | null;\n  columns?: {\n    idColumnName?: string;\n    vectorColumnName?: string;\n    contentColumnName?: string;\n    metadataColumnName?: string;\n  };\n  verbose?: boolean;\n  /**\n   * The amount of documents to chunk by when\n   * adding vectors.\n   * @default 500\n   */\n  chunkSize?: number;\n  ids?: string[];\n  distanceStrategy?: DistanceStrategy;\n}\n\n/**\n * MariaDB vector store integration.\n *\n * Setup:\n * Install `@langchain/community` and `mariadb`.\n *\n * If you wish to generate ids, you should also install the `uuid` package.\n *\n * ```bash\n * npm install @langchain/community mariadb uuid\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/_langchain_community.vectorstores_mariadb.MariaDB.html#constructor)\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import {\n *   MariaDBStore,\n *   DistanceStrategy,\n * } from \"@langchain/community/vectorstores/mariadb\";\n *\n * // Or other embeddings\n * import { OpenAIEmbeddings } from \"@langchain/openai\";\n * import { PoolConfig } from \"mariadb\";\n *\n * const embeddings = new OpenAIEmbeddings({\n *   model: \"text-embedding-3-small\",\n * });\n *\n * // Sample config\n * const config = {\n *   connectionOptions: {\n *     host: \"127.0.0.1\",\n *     port: 3306,\n *     user: \"myuser\",\n *     password: \"ChangeMe\",\n *     database: \"api\",\n *   } as PoolConfig,\n *   tableName: \"testlangchainjs\",\n *   columns: {\n *     idColumnName: \"id\",\n *     vectorColumnName: \"vector\",\n *     contentColumnName: \"content\",\n *     metadataColumnName: \"metadata\",\n *   },\n *   // supported distance strategies: COSINE (default) or EUCLIDEAN\n *   distanceStrategy: \"COSINE\" as DistanceStrategy,\n * };\n *\n * const vectorStore = await MariaDBStore.initialize(embeddings, config);\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Add documents</strong></summary>\n *\n * ```typescript\n * import type { Document } from '@langchain/core/documents';\n *\n * const document1 = { pageContent: \"foo\", metadata: { baz: \"bar\" } };\n * const document2 = { pageContent: \"thud\", metadata: { bar: \"baz\" } };\n * const document3 = { pageContent: \"i will be deleted :(\", metadata: {} };\n *\n * const documents: Document[] = [document1, document2, document3];\n * const ids = [\"1\", \"2\", \"3\"];\n * await vectorStore.addDocuments(documents, { ids });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Delete documents</strong></summary>\n *\n * ```typescript\n * await vectorStore.delete({ ids: [\"3\"] });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Similarity search</strong></summary>\n *\n * ```typescript\n * const results = await vectorStore.similaritySearch(\"thud\", 1);\n * for (const doc of results) {\n *   console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n * }\n * // Output: * thud [{\"baz\":\"bar\"}]\n * ```\n * </details>\n *\n * <br />\n *\n *\n * <details>\n * <summary><strong>Similarity search with filter</strong></summary>\n *\n * ```typescript\n * const resultsWithFilter = await vectorStore.similaritySearch(\"thud\", 1, new Expression( ExpressionType.EQ, new Key(\"country\"), new Value(\"BG\")));\n *\n * for (const doc of resultsWithFilter) {\n *   console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n * }\n * // Output: * foo [{\"baz\":\"bar\"}]\n * ```\n * </details>\n *\n * <br />\n *\n *\n * <details>\n * <summary><strong>Similarity search with score</strong></summary>\n *\n * ```typescript\n * const resultsWithScore = await vectorStore.similaritySearchWithScore(\"qux\", 1);\n * for (const [doc, score] of resultsWithScore) {\n *   console.log(`* [SIM=${score.toFixed(6)}] ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n * }\n * // Output: * [SIM=0.000000] qux [{\"bar\":\"baz\",\"baz\":\"bar\"}]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>As a retriever</strong></summary>\n *\n * ```typescript\n * const retriever = vectorStore.asRetriever({\n *   searchType: \"mmr\", // Leave blank for standard similarity search\n *   k: 1,\n * });\n * const resultAsRetriever = await retriever.invoke(\"thud\");\n * console.log(resultAsRetriever);\n *\n * // Output: [Document({ metadata: { \"baz\":\"bar\" }, pageContent: \"thud\" })]\n * ```\n * </details>\n *\n * <br />\n */\nexport class MariaDBStore extends VectorStore {\n  tableName: string;\n\n  collectionTableName?: string;\n\n  collectionName = \"langchain\";\n\n  collectionId?: string;\n\n  collectionMetadata: Metadata | null;\n\n  schemaName: string | null;\n\n  idColumnName: string;\n\n  vectorColumnName: string;\n\n  contentColumnName: string;\n\n  metadataColumnName: string;\n\n  _verbose?: boolean;\n\n  pool: Pool;\n\n  chunkSize = 500;\n\n  distanceStrategy: DistanceStrategy;\n\n  expressionConverter: MariaDBFilterExpressionConverter;\n\n  constructor(embeddings: EmbeddingsInterface, config: MariaDBStoreArgs) {\n    super(embeddings, config);\n    this.tableName = this.escapeId(config.tableName ?? \"langchain\", false);\n    if (\n      config.collectionName !== undefined &&\n      config.collectionTableName === undefined\n    ) {\n      throw new Error(\n        `If supplying a \"collectionName\", you must also supply a \"collectionTableName\".`\n      );\n    }\n\n    this.collectionTableName = config.collectionTableName\n      ? this.escapeId(config.collectionTableName, false)\n      : undefined;\n\n    this.collectionName = config.collectionName\n      ? this.escapeId(config.collectionName, false)\n      : \"langchaincol\";\n\n    this.collectionMetadata = config.collectionMetadata ?? null;\n    this.schemaName = config.schemaName\n      ? this.escapeId(config.schemaName, false)\n      : null;\n\n    this.vectorColumnName = this.escapeId(\n      config.columns?.vectorColumnName ?? \"embedding\",\n      false\n    );\n    this.contentColumnName = this.escapeId(\n      config.columns?.contentColumnName ?? \"text\",\n      false\n    );\n    this.idColumnName = this.escapeId(\n      config.columns?.idColumnName ?? \"id\",\n      false\n    );\n    this.metadataColumnName = this.escapeId(\n      config.columns?.metadataColumnName ?? \"metadata\",\n      false\n    );\n    this.expressionConverter = new MariaDBFilterExpressionConverter(\n      this.metadataColumnName\n    );\n\n    if (!config.connectionOptions && !config.pool) {\n      throw new Error(\n        \"You must provide either a `connectionOptions` object or a `pool` instance.\"\n      );\n    }\n\n    const langchainVerbose = getEnvironmentVariable(\"LANGCHAIN_VERBOSE\");\n\n    if (langchainVerbose === \"true\") {\n      this._verbose = true;\n    } else if (langchainVerbose === \"false\") {\n      this._verbose = false;\n    } else {\n      this._verbose = config.verbose;\n    }\n\n    if (config.pool) {\n      this.pool = config.pool;\n    } else {\n      const poolConf = { ...config.connectionOptions, rowsAsArray: true };\n      // add query to log if verbose\n      if (this._verbose) poolConf.logger = { query: console.log };\n      this.pool = mariadb.createPool(poolConf);\n    }\n    this.chunkSize = config.chunkSize ?? 500;\n\n    this.distanceStrategy =\n      config.distanceStrategy ?? (\"COSINE\" as DistanceStrategy);\n  }\n\n  get computedTableName() {\n    return this.schemaName == null\n      ? this.tableName\n      : `${this.schemaName}.${this.tableName}`;\n  }\n\n  get computedCollectionTableName() {\n    return this.schemaName == null\n      ? `${this.collectionTableName}`\n      : `\"${this.schemaName}\".\"${this.collectionTableName}\"`;\n  }\n\n  /**\n   * Escape identifier\n   *\n   * @param identifier identifier value\n   * @param alwaysQuote must identifier be quoted if not required\n   */\n  private escapeId(identifier: string, alwaysQuote: boolean): string {\n    if (!identifier || identifier === \"\")\n      throw new Error(\"Identifier is required\");\n\n    const len = identifier.length;\n    const simpleIdentifier = /^[0-9a-zA-Z$_]*$/;\n    if (simpleIdentifier.test(identifier)) {\n      if (len < 1 || len > 64) {\n        throw new Error(\"Invalid identifier length\");\n      }\n      if (alwaysQuote) return `\\`${identifier}\\``;\n\n      // Identifier names may begin with a numeral, but can't only contain numerals unless quoted.\n      if (/^\\d+$/.test(identifier)) {\n        // identifier containing only numerals must be quoted\n        return `\\`${identifier}\\``;\n      }\n      // identifier containing only numerals must be quoted\n      return identifier;\n    } else {\n      if (identifier.includes(\"\\u0000\")) {\n        throw new Error(\"Invalid name - containing u0000 character\");\n      }\n      let ident = identifier;\n      if (/^`.+`$/.test(identifier)) {\n        ident = identifier.substring(1, identifier.length - 1);\n      }\n      if (len < 1 || len > 64) {\n        throw new Error(\"Invalid identifier length\");\n      }\n      return `\\`${ident.replace(/`/g, \"``\")}\\``;\n    }\n  }\n\n  private printable(definition: string): string {\n    return definition.replaceAll(/[^0-9a-zA-Z_]/g, \"\");\n  }\n\n  /**\n   * Static method to create a new `MariaDBStore` instance from a\n   * connection. It creates a table if one does not exist, and calls\n   * `connect` to return a new instance of `MariaDBStore`.\n   *\n   * @param embeddings - Embeddings instance.\n   * @param fields - `MariaDBStoreArgs` instance\n   * @param fields.dimensions Number of dimensions in your vector data type. default to 1536.\n   * @returns A new instance of `MariaDBStore`.\n   */\n  static async initialize(\n    embeddings: EmbeddingsInterface,\n    config: MariaDBStoreArgs & { dimensions?: number }\n  ): Promise<MariaDBStore> {\n    const { dimensions, ...rest } = config;\n    const mariadbStore = new MariaDBStore(embeddings, rest);\n    await mariadbStore.ensureTableInDatabase(dimensions);\n    await mariadbStore.ensureCollectionTableInDatabase();\n    await mariadbStore.loadCollectionId();\n\n    return mariadbStore;\n  }\n\n  /**\n   * Static method to create a new `MariaDBStore` instance from an\n   * array of texts and their metadata. It converts the texts into\n   * `Document` instances and adds them to the store.\n   *\n   * @param texts - Array of texts.\n   * @param metadatas - Array of metadata objects or a single metadata object.\n   * @param embeddings - Embeddings instance.\n   * @param dbConfig - `MariaDBStoreArgs` instance.\n   * @returns Promise that resolves with a new instance of `MariaDBStore`.\n   */\n  static async fromTexts(\n    texts: string[],\n    metadatas: object[] | object,\n    embeddings: EmbeddingsInterface,\n    dbConfig: MariaDBStoreArgs & { dimensions?: number }\n  ): Promise<MariaDBStore> {\n    const docs = [];\n    for (let i = 0; i < texts.length; i += 1) {\n      const metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\n      const newDoc = new Document({\n        pageContent: texts[i],\n        metadata,\n      });\n      docs.push(newDoc);\n    }\n\n    return MariaDBStore.fromDocuments(docs, embeddings, dbConfig);\n  }\n\n  /**\n   * Static method to create a new `MariaDBStore` instance from an\n   * array of `Document` instances. It adds the documents to the store.\n   *\n   * @param docs - Array of `Document` instances.\n   * @param embeddings - Embeddings instance.\n   * @param dbConfig - `MariaDBStoreArgs` instance.\n   * @returns Promise that resolves with a new instance of `MariaDBStore`.\n   */\n  static async fromDocuments(\n    docs: Document[],\n    embeddings: EmbeddingsInterface,\n    dbConfig: MariaDBStoreArgs & { dimensions?: number }\n  ): Promise<MariaDBStore> {\n    const instance = await MariaDBStore.initialize(embeddings, dbConfig);\n    await instance.addDocuments(docs, { ids: dbConfig.ids });\n    return instance;\n  }\n\n  _vectorstoreType(): string {\n    return \"mariadb\";\n  }\n\n  /**\n   * Method to add documents to the vector store. It converts the documents into\n   * vectors, and adds them to the store.\n   *\n   * @param documents - Array of `Document` instances.\n   * @param options - Optional arguments for adding documents\n   * @returns Promise that resolves when the documents have been added.\n   */\n  async addDocuments(\n    documents: Document[],\n    options?: { ids?: string[] }\n  ): Promise<void> {\n    const texts = documents.map(({ pageContent }) => pageContent);\n\n    return this.addVectors(\n      await this.embeddings.embedDocuments(texts),\n      documents,\n      options\n    );\n  }\n\n  /**\n   * Inserts a row for the collectionName provided at initialization if it does not\n   * exist and set the collectionId.\n   */\n  private async loadCollectionId(): Promise<void> {\n    if (this.collectionId) {\n      return;\n    }\n\n    if (this.collectionTableName) {\n      const queryResult = await this.pool.query(\n        {\n          sql: `SELECT uuid from ${this.computedCollectionTableName} WHERE label = ?`,\n          rowsAsArray: true,\n        },\n        [this.collectionName]\n      );\n      if (queryResult.length > 0) {\n        this.collectionId = queryResult[0][0];\n      } else {\n        const insertString = `INSERT INTO ${this.computedCollectionTableName}(label, cmetadata) VALUES (?, ?) RETURNING uuid`;\n        const insertResult = await this.pool.query(\n          { sql: insertString, rowsAsArray: true },\n          [this.collectionName, this.collectionMetadata]\n        );\n        this.collectionId = insertResult[0][0];\n      }\n    }\n  }\n\n  /**\n   * Method to add vectors to the vector store. It converts the vectors into\n   * rows and inserts them into the database.\n   *\n   * @param vectors - Array of vectors.\n   * @param documents - Array of `Document` instances.\n   * @param options - Optional arguments for adding documents\n   * @returns Promise that resolves when the vectors have been added.\n   */\n  async addVectors(\n    vectors: number[][],\n    documents: Document[],\n    options?: { ids?: string[] }\n  ): Promise<void> {\n    const ids = options?.ids;\n\n    // Either all documents have ids or none of them do to avoid confusion.\n    if (ids !== undefined && ids.length !== vectors.length) {\n      throw new Error(\n        \"The number of ids must match the number of vectors provided.\"\n      );\n    }\n    await this.loadCollectionId();\n\n    const insertQuery = `INSERT INTO ${this.computedTableName}(${",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1901267827",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7360,
        "pr_file": "libs/langchain-community/src/vectorstores/mariadb.ts",
        "discussion_id": "1901267827",
        "commented_code": "@@ -0,0 +1,760 @@\n+import mariadb, { type Pool, type PoolConfig } from \"mariadb\";\n+import { VectorStore } from \"@langchain/core/vectorstores\";\n+import type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { Document } from \"@langchain/core/documents\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import {\n+  FilterExpressionConverter,\n+  StringBuilder,\n+  BaseFilterExpressionConverter,\n+  Expression,\n+  Value,\n+  Key,\n+  Group,\n+} from \"@langchain/core/filter\";\n+\n+type Metadata = Record<string, unknown>;\n+\n+export type DistanceStrategy = \"COSINE\" | \"EUCLIDEAN\";\n+\n+/**\n+ * Converts Expression into JSON metadata filter expression format\n+ * for MariaDB\n+ */\n+class MariaDBFilterExpressionConverter\n+  extends BaseFilterExpressionConverter\n+  implements FilterExpressionConverter\n+{\n+  private metadataFieldName: string;\n+\n+  constructor(metadataFieldName: string) {\n+    super();\n+    this.metadataFieldName = metadataFieldName;\n+  }\n+\n+  convertExpressionToContext(\n+    expression: Expression,\n+    context: StringBuilder\n+  ): void {\n+    super.convertOperandToContext(expression.left, context);\n+    super.convertSymbolToContext(expression, context);\n+    if (expression.right) {\n+      super.convertOperandToContext(expression.right, context);\n+    }\n+  }\n+\n+  convertKeyToContext(key: Key, context: StringBuilder): void {\n+    context.append(`JSON_VALUE(${this.metadataFieldName}, '$.${key.key}')`);\n+  }\n+\n+  writeValueRangeStart(_listValue: Value, context: StringBuilder): void {\n+    context.append(\"(\");\n+  }\n+\n+  writeValueRangeEnd(_listValue: Value, context: StringBuilder): void {\n+    context.append(\")\");\n+  }\n+\n+  writeGroupStart(_group: Group, context: StringBuilder): void {\n+    context.append(\"(\");\n+  }\n+\n+  writeGroupEnd(_group: Group, context: StringBuilder): void {\n+    context.append(\")\");\n+  }\n+}\n+\n+/**\n+ * Interface that defines the arguments required to create a\n+ * `MariaDBStore` instance. It includes MariaDB connection options,\n+ * table name and verbosity level.\n+ */\n+export interface MariaDBStoreArgs {\n+  connectionOptions?: PoolConfig;\n+  pool?: Pool;\n+  tableName?: string;\n+  collectionTableName?: string;\n+  collectionName?: string;\n+  collectionMetadata?: Metadata | null;\n+  schemaName?: string | null;\n+  columns?: {\n+    idColumnName?: string;\n+    vectorColumnName?: string;\n+    contentColumnName?: string;\n+    metadataColumnName?: string;\n+  };\n+  verbose?: boolean;\n+  /**\n+   * The amount of documents to chunk by when\n+   * adding vectors.\n+   * @default 500\n+   */\n+  chunkSize?: number;\n+  ids?: string[];\n+  distanceStrategy?: DistanceStrategy;\n+}\n+\n+/**\n+ * MariaDB vector store integration.\n+ *\n+ * Setup:\n+ * Install `@langchain/community` and `mariadb`.\n+ *\n+ * If you wish to generate ids, you should also install the `uuid` package.\n+ *\n+ * ```bash\n+ * npm install @langchain/community mariadb uuid\n+ * ```\n+ *\n+ * ## [Constructor args](https://api.js.langchain.com/classes/_langchain_community.vectorstores_mariadb.MariaDB.html#constructor)\n+ *\n+ * <details open>\n+ * <summary><strong>Instantiate</strong></summary>\n+ *\n+ * ```typescript\n+ * import {\n+ *   MariaDBStore,\n+ *   DistanceStrategy,\n+ * } from \"@langchain/community/vectorstores/mariadb\";\n+ *\n+ * // Or other embeddings\n+ * import { OpenAIEmbeddings } from \"@langchain/openai\";\n+ * import { PoolConfig } from \"mariadb\";\n+ *\n+ * const embeddings = new OpenAIEmbeddings({\n+ *   model: \"text-embedding-3-small\",\n+ * });\n+ *\n+ * // Sample config\n+ * const config = {\n+ *   connectionOptions: {\n+ *     host: \"127.0.0.1\",\n+ *     port: 3306,\n+ *     user: \"myuser\",\n+ *     password: \"ChangeMe\",\n+ *     database: \"api\",\n+ *   } as PoolConfig,\n+ *   tableName: \"testlangchainjs\",\n+ *   columns: {\n+ *     idColumnName: \"id\",\n+ *     vectorColumnName: \"vector\",\n+ *     contentColumnName: \"content\",\n+ *     metadataColumnName: \"metadata\",\n+ *   },\n+ *   // supported distance strategies: COSINE (default) or EUCLIDEAN\n+ *   distanceStrategy: \"COSINE\" as DistanceStrategy,\n+ * };\n+ *\n+ * const vectorStore = await MariaDBStore.initialize(embeddings, config);\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ * <details>\n+ * <summary><strong>Add documents</strong></summary>\n+ *\n+ * ```typescript\n+ * import type { Document } from '@langchain/core/documents';\n+ *\n+ * const document1 = { pageContent: \"foo\", metadata: { baz: \"bar\" } };\n+ * const document2 = { pageContent: \"thud\", metadata: { bar: \"baz\" } };\n+ * const document3 = { pageContent: \"i will be deleted :(\", metadata: {} };\n+ *\n+ * const documents: Document[] = [document1, document2, document3];\n+ * const ids = [\"1\", \"2\", \"3\"];\n+ * await vectorStore.addDocuments(documents, { ids });\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ * <details>\n+ * <summary><strong>Delete documents</strong></summary>\n+ *\n+ * ```typescript\n+ * await vectorStore.delete({ ids: [\"3\"] });\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ * <details>\n+ * <summary><strong>Similarity search</strong></summary>\n+ *\n+ * ```typescript\n+ * const results = await vectorStore.similaritySearch(\"thud\", 1);\n+ * for (const doc of results) {\n+ *   console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n+ * }\n+ * // Output: * thud [{\"baz\":\"bar\"}]\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ *\n+ * <details>\n+ * <summary><strong>Similarity search with filter</strong></summary>\n+ *\n+ * ```typescript\n+ * const resultsWithFilter = await vectorStore.similaritySearch(\"thud\", 1, new Expression( ExpressionType.EQ, new Key(\"country\"), new Value(\"BG\")));\n+ *\n+ * for (const doc of resultsWithFilter) {\n+ *   console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n+ * }\n+ * // Output: * foo [{\"baz\":\"bar\"}]\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ *\n+ * <details>\n+ * <summary><strong>Similarity search with score</strong></summary>\n+ *\n+ * ```typescript\n+ * const resultsWithScore = await vectorStore.similaritySearchWithScore(\"qux\", 1);\n+ * for (const [doc, score] of resultsWithScore) {\n+ *   console.log(`* [SIM=${score.toFixed(6)}] ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n+ * }\n+ * // Output: * [SIM=0.000000] qux [{\"bar\":\"baz\",\"baz\":\"bar\"}]\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ * <details>\n+ * <summary><strong>As a retriever</strong></summary>\n+ *\n+ * ```typescript\n+ * const retriever = vectorStore.asRetriever({\n+ *   searchType: \"mmr\", // Leave blank for standard similarity search\n+ *   k: 1,\n+ * });\n+ * const resultAsRetriever = await retriever.invoke(\"thud\");\n+ * console.log(resultAsRetriever);\n+ *\n+ * // Output: [Document({ metadata: { \"baz\":\"bar\" }, pageContent: \"thud\" })]\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ */\n+export class MariaDBStore extends VectorStore {\n+  tableName: string;\n+\n+  collectionTableName?: string;\n+\n+  collectionName = \"langchain\";\n+\n+  collectionId?: string;\n+\n+  collectionMetadata: Metadata | null;\n+\n+  schemaName: string | null;\n+\n+  idColumnName: string;\n+\n+  vectorColumnName: string;\n+\n+  contentColumnName: string;\n+\n+  metadataColumnName: string;\n+\n+  _verbose?: boolean;\n+\n+  pool: Pool;\n+\n+  chunkSize = 500;\n+\n+  distanceStrategy: DistanceStrategy;\n+\n+  expressionConverter: MariaDBFilterExpressionConverter;\n+\n+  constructor(embeddings: EmbeddingsInterface, config: MariaDBStoreArgs) {\n+    super(embeddings, config);\n+    this.tableName = this.escapeId(config.tableName ?? \"langchain\", false);\n+    if (\n+      config.collectionName !== undefined &&\n+      config.collectionTableName === undefined\n+    ) {\n+      throw new Error(\n+        `If supplying a \"collectionName\", you must also supply a \"collectionTableName\".`\n+      );\n+    }\n+\n+    this.collectionTableName = config.collectionTableName\n+      ? this.escapeId(config.collectionTableName, false)\n+      : undefined;\n+\n+    this.collectionName = config.collectionName\n+      ? this.escapeId(config.collectionName, false)\n+      : \"langchaincol\";\n+\n+    this.collectionMetadata = config.collectionMetadata ?? null;\n+    this.schemaName = config.schemaName\n+      ? this.escapeId(config.schemaName, false)\n+      : null;\n+\n+    this.vectorColumnName = this.escapeId(\n+      config.columns?.vectorColumnName ?? \"embedding\",\n+      false\n+    );\n+    this.contentColumnName = this.escapeId(\n+      config.columns?.contentColumnName ?? \"text\",\n+      false\n+    );\n+    this.idColumnName = this.escapeId(\n+      config.columns?.idColumnName ?? \"id\",\n+      false\n+    );\n+    this.metadataColumnName = this.escapeId(\n+      config.columns?.metadataColumnName ?? \"metadata\",\n+      false\n+    );\n+    this.expressionConverter = new MariaDBFilterExpressionConverter(\n+      this.metadataColumnName\n+    );\n+\n+    if (!config.connectionOptions && !config.pool) {\n+      throw new Error(\n+        \"You must provide either a `connectionOptions` object or a `pool` instance.\"\n+      );\n+    }\n+\n+    const langchainVerbose = getEnvironmentVariable(\"LANGCHAIN_VERBOSE\");\n+\n+    if (langchainVerbose === \"true\") {\n+      this._verbose = true;\n+    } else if (langchainVerbose === \"false\") {\n+      this._verbose = false;\n+    } else {\n+      this._verbose = config.verbose;\n+    }\n+\n+    if (config.pool) {\n+      this.pool = config.pool;\n+    } else {\n+      const poolConf = { ...config.connectionOptions, rowsAsArray: true };\n+      // add query to log if verbose\n+      if (this._verbose) poolConf.logger = { query: console.log };\n+      this.pool = mariadb.createPool(poolConf);\n+    }\n+    this.chunkSize = config.chunkSize ?? 500;\n+\n+    this.distanceStrategy =\n+      config.distanceStrategy ?? (\"COSINE\" as DistanceStrategy);\n+  }\n+\n+  get computedTableName() {\n+    return this.schemaName == null\n+      ? this.tableName\n+      : `${this.schemaName}.${this.tableName}`;\n+  }\n+\n+  get computedCollectionTableName() {\n+    return this.schemaName == null\n+      ? `${this.collectionTableName}`\n+      : `\"${this.schemaName}\".\"${this.collectionTableName}\"`;\n+  }\n+\n+  /**\n+   * Escape identifier\n+   *\n+   * @param identifier identifier value\n+   * @param alwaysQuote must identifier be quoted if not required\n+   */\n+  private escapeId(identifier: string, alwaysQuote: boolean): string {\n+    if (!identifier || identifier === \"\")\n+      throw new Error(\"Identifier is required\");\n+\n+    const len = identifier.length;\n+    const simpleIdentifier = /^[0-9a-zA-Z$_]*$/;\n+    if (simpleIdentifier.test(identifier)) {\n+      if (len < 1 || len > 64) {\n+        throw new Error(\"Invalid identifier length\");\n+      }\n+      if (alwaysQuote) return `\\`${identifier}\\``;\n+\n+      // Identifier names may begin with a numeral, but can't only contain numerals unless quoted.\n+      if (/^\\d+$/.test(identifier)) {\n+        // identifier containing only numerals must be quoted\n+        return `\\`${identifier}\\``;\n+      }\n+      // identifier containing only numerals must be quoted\n+      return identifier;\n+    } else {\n+      if (identifier.includes(\"\\u0000\")) {\n+        throw new Error(\"Invalid name - containing u0000 character\");\n+      }\n+      let ident = identifier;\n+      if (/^`.+`$/.test(identifier)) {\n+        ident = identifier.substring(1, identifier.length - 1);\n+      }\n+      if (len < 1 || len > 64) {\n+        throw new Error(\"Invalid identifier length\");\n+      }\n+      return `\\`${ident.replace(/`/g, \"``\")}\\``;\n+    }\n+  }\n+\n+  private printable(definition: string): string {\n+    return definition.replaceAll(/[^0-9a-zA-Z_]/g, \"\");\n+  }\n+\n+  /**\n+   * Static method to create a new `MariaDBStore` instance from a\n+   * connection. It creates a table if one does not exist, and calls\n+   * `connect` to return a new instance of `MariaDBStore`.\n+   *\n+   * @param embeddings - Embeddings instance.\n+   * @param fields - `MariaDBStoreArgs` instance\n+   * @param fields.dimensions Number of dimensions in your vector data type. default to 1536.\n+   * @returns A new instance of `MariaDBStore`.\n+   */\n+  static async initialize(\n+    embeddings: EmbeddingsInterface,\n+    config: MariaDBStoreArgs & { dimensions?: number }\n+  ): Promise<MariaDBStore> {\n+    const { dimensions, ...rest } = config;\n+    const mariadbStore = new MariaDBStore(embeddings, rest);\n+    await mariadbStore.ensureTableInDatabase(dimensions);\n+    await mariadbStore.ensureCollectionTableInDatabase();\n+    await mariadbStore.loadCollectionId();\n+\n+    return mariadbStore;\n+  }\n+\n+  /**\n+   * Static method to create a new `MariaDBStore` instance from an\n+   * array of texts and their metadata. It converts the texts into\n+   * `Document` instances and adds them to the store.\n+   *\n+   * @param texts - Array of texts.\n+   * @param metadatas - Array of metadata objects or a single metadata object.\n+   * @param embeddings - Embeddings instance.\n+   * @param dbConfig - `MariaDBStoreArgs` instance.\n+   * @returns Promise that resolves with a new instance of `MariaDBStore`.\n+   */\n+  static async fromTexts(\n+    texts: string[],\n+    metadatas: object[] | object,\n+    embeddings: EmbeddingsInterface,\n+    dbConfig: MariaDBStoreArgs & { dimensions?: number }\n+  ): Promise<MariaDBStore> {\n+    const docs = [];\n+    for (let i = 0; i < texts.length; i += 1) {\n+      const metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\n+      const newDoc = new Document({\n+        pageContent: texts[i],\n+        metadata,\n+      });\n+      docs.push(newDoc);\n+    }\n+\n+    return MariaDBStore.fromDocuments(docs, embeddings, dbConfig);\n+  }\n+\n+  /**\n+   * Static method to create a new `MariaDBStore` instance from an\n+   * array of `Document` instances. It adds the documents to the store.\n+   *\n+   * @param docs - Array of `Document` instances.\n+   * @param embeddings - Embeddings instance.\n+   * @param dbConfig - `MariaDBStoreArgs` instance.\n+   * @returns Promise that resolves with a new instance of `MariaDBStore`.\n+   */\n+  static async fromDocuments(\n+    docs: Document[],\n+    embeddings: EmbeddingsInterface,\n+    dbConfig: MariaDBStoreArgs & { dimensions?: number }\n+  ): Promise<MariaDBStore> {\n+    const instance = await MariaDBStore.initialize(embeddings, dbConfig);\n+    await instance.addDocuments(docs, { ids: dbConfig.ids });\n+    return instance;\n+  }\n+\n+  _vectorstoreType(): string {\n+    return \"mariadb\";\n+  }\n+\n+  /**\n+   * Method to add documents to the vector store. It converts the documents into\n+   * vectors, and adds them to the store.\n+   *\n+   * @param documents - Array of `Document` instances.\n+   * @param options - Optional arguments for adding documents\n+   * @returns Promise that resolves when the documents have been added.\n+   */\n+  async addDocuments(\n+    documents: Document[],\n+    options?: { ids?: string[] }\n+  ): Promise<void> {\n+    const texts = documents.map(({ pageContent }) => pageContent);\n+\n+    return this.addVectors(\n+      await this.embeddings.embedDocuments(texts),\n+      documents,\n+      options\n+    );\n+  }\n+\n+  /**\n+   * Inserts a row for the collectionName provided at initialization if it does not\n+   * exist and set the collectionId.\n+   */\n+  private async loadCollectionId(): Promise<void> {\n+    if (this.collectionId) {\n+      return;\n+    }\n+\n+    if (this.collectionTableName) {\n+      const queryResult = await this.pool.query(\n+        {\n+          sql: `SELECT uuid from ${this.computedCollectionTableName} WHERE label = ?`,\n+          rowsAsArray: true,\n+        },\n+        [this.collectionName]\n+      );\n+      if (queryResult.length > 0) {\n+        this.collectionId = queryResult[0][0];\n+      } else {\n+        const insertString = `INSERT INTO ${this.computedCollectionTableName}(label, cmetadata) VALUES (?, ?) RETURNING uuid`;\n+        const insertResult = await this.pool.query(\n+          { sql: insertString, rowsAsArray: true },\n+          [this.collectionName, this.collectionMetadata]\n+        );\n+        this.collectionId = insertResult[0][0];\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Method to add vectors to the vector store. It converts the vectors into\n+   * rows and inserts them into the database.\n+   *\n+   * @param vectors - Array of vectors.\n+   * @param documents - Array of `Document` instances.\n+   * @param options - Optional arguments for adding documents\n+   * @returns Promise that resolves when the vectors have been added.\n+   */\n+  async addVectors(\n+    vectors: number[][],\n+    documents: Document[],\n+    options?: { ids?: string[] }\n+  ): Promise<void> {\n+    const ids = options?.ids;\n+\n+    // Either all documents have ids or none of them do to avoid confusion.\n+    if (ids !== undefined && ids.length !== vectors.length) {\n+      throw new Error(\n+        \"The number of ids must match the number of vectors provided.\"\n+      );\n+    }\n+    await this.loadCollectionId();\n+\n+    const insertQuery = `INSERT INTO ${this.computedTableName}(${",
        "comment_created_at": "2025-01-02T21:19:03+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "nit: can we escape column names too/disallow nonalphanumeric?",
        "pr_file_module": null
      },
      {
        "comment_id": "1901592943",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 7360,
        "pr_file": "libs/langchain-community/src/vectorstores/mariadb.ts",
        "discussion_id": "1901267827",
        "commented_code": "@@ -0,0 +1,760 @@\n+import mariadb, { type Pool, type PoolConfig } from \"mariadb\";\n+import { VectorStore } from \"@langchain/core/vectorstores\";\n+import type { EmbeddingsInterface } from \"@langchain/core/embeddings\";\n+import { Document } from \"@langchain/core/documents\";\n+import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n+import {\n+  FilterExpressionConverter,\n+  StringBuilder,\n+  BaseFilterExpressionConverter,\n+  Expression,\n+  Value,\n+  Key,\n+  Group,\n+} from \"@langchain/core/filter\";\n+\n+type Metadata = Record<string, unknown>;\n+\n+export type DistanceStrategy = \"COSINE\" | \"EUCLIDEAN\";\n+\n+/**\n+ * Converts Expression into JSON metadata filter expression format\n+ * for MariaDB\n+ */\n+class MariaDBFilterExpressionConverter\n+  extends BaseFilterExpressionConverter\n+  implements FilterExpressionConverter\n+{\n+  private metadataFieldName: string;\n+\n+  constructor(metadataFieldName: string) {\n+    super();\n+    this.metadataFieldName = metadataFieldName;\n+  }\n+\n+  convertExpressionToContext(\n+    expression: Expression,\n+    context: StringBuilder\n+  ): void {\n+    super.convertOperandToContext(expression.left, context);\n+    super.convertSymbolToContext(expression, context);\n+    if (expression.right) {\n+      super.convertOperandToContext(expression.right, context);\n+    }\n+  }\n+\n+  convertKeyToContext(key: Key, context: StringBuilder): void {\n+    context.append(`JSON_VALUE(${this.metadataFieldName}, '$.${key.key}')`);\n+  }\n+\n+  writeValueRangeStart(_listValue: Value, context: StringBuilder): void {\n+    context.append(\"(\");\n+  }\n+\n+  writeValueRangeEnd(_listValue: Value, context: StringBuilder): void {\n+    context.append(\")\");\n+  }\n+\n+  writeGroupStart(_group: Group, context: StringBuilder): void {\n+    context.append(\"(\");\n+  }\n+\n+  writeGroupEnd(_group: Group, context: StringBuilder): void {\n+    context.append(\")\");\n+  }\n+}\n+\n+/**\n+ * Interface that defines the arguments required to create a\n+ * `MariaDBStore` instance. It includes MariaDB connection options,\n+ * table name and verbosity level.\n+ */\n+export interface MariaDBStoreArgs {\n+  connectionOptions?: PoolConfig;\n+  pool?: Pool;\n+  tableName?: string;\n+  collectionTableName?: string;\n+  collectionName?: string;\n+  collectionMetadata?: Metadata | null;\n+  schemaName?: string | null;\n+  columns?: {\n+    idColumnName?: string;\n+    vectorColumnName?: string;\n+    contentColumnName?: string;\n+    metadataColumnName?: string;\n+  };\n+  verbose?: boolean;\n+  /**\n+   * The amount of documents to chunk by when\n+   * adding vectors.\n+   * @default 500\n+   */\n+  chunkSize?: number;\n+  ids?: string[];\n+  distanceStrategy?: DistanceStrategy;\n+}\n+\n+/**\n+ * MariaDB vector store integration.\n+ *\n+ * Setup:\n+ * Install `@langchain/community` and `mariadb`.\n+ *\n+ * If you wish to generate ids, you should also install the `uuid` package.\n+ *\n+ * ```bash\n+ * npm install @langchain/community mariadb uuid\n+ * ```\n+ *\n+ * ## [Constructor args](https://api.js.langchain.com/classes/_langchain_community.vectorstores_mariadb.MariaDB.html#constructor)\n+ *\n+ * <details open>\n+ * <summary><strong>Instantiate</strong></summary>\n+ *\n+ * ```typescript\n+ * import {\n+ *   MariaDBStore,\n+ *   DistanceStrategy,\n+ * } from \"@langchain/community/vectorstores/mariadb\";\n+ *\n+ * // Or other embeddings\n+ * import { OpenAIEmbeddings } from \"@langchain/openai\";\n+ * import { PoolConfig } from \"mariadb\";\n+ *\n+ * const embeddings = new OpenAIEmbeddings({\n+ *   model: \"text-embedding-3-small\",\n+ * });\n+ *\n+ * // Sample config\n+ * const config = {\n+ *   connectionOptions: {\n+ *     host: \"127.0.0.1\",\n+ *     port: 3306,\n+ *     user: \"myuser\",\n+ *     password: \"ChangeMe\",\n+ *     database: \"api\",\n+ *   } as PoolConfig,\n+ *   tableName: \"testlangchainjs\",\n+ *   columns: {\n+ *     idColumnName: \"id\",\n+ *     vectorColumnName: \"vector\",\n+ *     contentColumnName: \"content\",\n+ *     metadataColumnName: \"metadata\",\n+ *   },\n+ *   // supported distance strategies: COSINE (default) or EUCLIDEAN\n+ *   distanceStrategy: \"COSINE\" as DistanceStrategy,\n+ * };\n+ *\n+ * const vectorStore = await MariaDBStore.initialize(embeddings, config);\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ * <details>\n+ * <summary><strong>Add documents</strong></summary>\n+ *\n+ * ```typescript\n+ * import type { Document } from '@langchain/core/documents';\n+ *\n+ * const document1 = { pageContent: \"foo\", metadata: { baz: \"bar\" } };\n+ * const document2 = { pageContent: \"thud\", metadata: { bar: \"baz\" } };\n+ * const document3 = { pageContent: \"i will be deleted :(\", metadata: {} };\n+ *\n+ * const documents: Document[] = [document1, document2, document3];\n+ * const ids = [\"1\", \"2\", \"3\"];\n+ * await vectorStore.addDocuments(documents, { ids });\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ * <details>\n+ * <summary><strong>Delete documents</strong></summary>\n+ *\n+ * ```typescript\n+ * await vectorStore.delete({ ids: [\"3\"] });\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ * <details>\n+ * <summary><strong>Similarity search</strong></summary>\n+ *\n+ * ```typescript\n+ * const results = await vectorStore.similaritySearch(\"thud\", 1);\n+ * for (const doc of results) {\n+ *   console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n+ * }\n+ * // Output: * thud [{\"baz\":\"bar\"}]\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ *\n+ * <details>\n+ * <summary><strong>Similarity search with filter</strong></summary>\n+ *\n+ * ```typescript\n+ * const resultsWithFilter = await vectorStore.similaritySearch(\"thud\", 1, new Expression( ExpressionType.EQ, new Key(\"country\"), new Value(\"BG\")));\n+ *\n+ * for (const doc of resultsWithFilter) {\n+ *   console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n+ * }\n+ * // Output: * foo [{\"baz\":\"bar\"}]\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ *\n+ * <details>\n+ * <summary><strong>Similarity search with score</strong></summary>\n+ *\n+ * ```typescript\n+ * const resultsWithScore = await vectorStore.similaritySearchWithScore(\"qux\", 1);\n+ * for (const [doc, score] of resultsWithScore) {\n+ *   console.log(`* [SIM=${score.toFixed(6)}] ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n+ * }\n+ * // Output: * [SIM=0.000000] qux [{\"bar\":\"baz\",\"baz\":\"bar\"}]\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ *\n+ * <details>\n+ * <summary><strong>As a retriever</strong></summary>\n+ *\n+ * ```typescript\n+ * const retriever = vectorStore.asRetriever({\n+ *   searchType: \"mmr\", // Leave blank for standard similarity search\n+ *   k: 1,\n+ * });\n+ * const resultAsRetriever = await retriever.invoke(\"thud\");\n+ * console.log(resultAsRetriever);\n+ *\n+ * // Output: [Document({ metadata: { \"baz\":\"bar\" }, pageContent: \"thud\" })]\n+ * ```\n+ * </details>\n+ *\n+ * <br />\n+ */\n+export class MariaDBStore extends VectorStore {\n+  tableName: string;\n+\n+  collectionTableName?: string;\n+\n+  collectionName = \"langchain\";\n+\n+  collectionId?: string;\n+\n+  collectionMetadata: Metadata | null;\n+\n+  schemaName: string | null;\n+\n+  idColumnName: string;\n+\n+  vectorColumnName: string;\n+\n+  contentColumnName: string;\n+\n+  metadataColumnName: string;\n+\n+  _verbose?: boolean;\n+\n+  pool: Pool;\n+\n+  chunkSize = 500;\n+\n+  distanceStrategy: DistanceStrategy;\n+\n+  expressionConverter: MariaDBFilterExpressionConverter;\n+\n+  constructor(embeddings: EmbeddingsInterface, config: MariaDBStoreArgs) {\n+    super(embeddings, config);\n+    this.tableName = this.escapeId(config.tableName ?? \"langchain\", false);\n+    if (\n+      config.collectionName !== undefined &&\n+      config.collectionTableName === undefined\n+    ) {\n+      throw new Error(\n+        `If supplying a \"collectionName\", you must also supply a \"collectionTableName\".`\n+      );\n+    }\n+\n+    this.collectionTableName = config.collectionTableName\n+      ? this.escapeId(config.collectionTableName, false)\n+      : undefined;\n+\n+    this.collectionName = config.collectionName\n+      ? this.escapeId(config.collectionName, false)\n+      : \"langchaincol\";\n+\n+    this.collectionMetadata = config.collectionMetadata ?? null;\n+    this.schemaName = config.schemaName\n+      ? this.escapeId(config.schemaName, false)\n+      : null;\n+\n+    this.vectorColumnName = this.escapeId(\n+      config.columns?.vectorColumnName ?? \"embedding\",\n+      false\n+    );\n+    this.contentColumnName = this.escapeId(\n+      config.columns?.contentColumnName ?? \"text\",\n+      false\n+    );\n+    this.idColumnName = this.escapeId(\n+      config.columns?.idColumnName ?? \"id\",\n+      false\n+    );\n+    this.metadataColumnName = this.escapeId(\n+      config.columns?.metadataColumnName ?? \"metadata\",\n+      false\n+    );\n+    this.expressionConverter = new MariaDBFilterExpressionConverter(\n+      this.metadataColumnName\n+    );\n+\n+    if (!config.connectionOptions && !config.pool) {\n+      throw new Error(\n+        \"You must provide either a `connectionOptions` object or a `pool` instance.\"\n+      );\n+    }\n+\n+    const langchainVerbose = getEnvironmentVariable(\"LANGCHAIN_VERBOSE\");\n+\n+    if (langchainVerbose === \"true\") {\n+      this._verbose = true;\n+    } else if (langchainVerbose === \"false\") {\n+      this._verbose = false;\n+    } else {\n+      this._verbose = config.verbose;\n+    }\n+\n+    if (config.pool) {\n+      this.pool = config.pool;\n+    } else {\n+      const poolConf = { ...config.connectionOptions, rowsAsArray: true };\n+      // add query to log if verbose\n+      if (this._verbose) poolConf.logger = { query: console.log };\n+      this.pool = mariadb.createPool(poolConf);\n+    }\n+    this.chunkSize = config.chunkSize ?? 500;\n+\n+    this.distanceStrategy =\n+      config.distanceStrategy ?? (\"COSINE\" as DistanceStrategy);\n+  }\n+\n+  get computedTableName() {\n+    return this.schemaName == null\n+      ? this.tableName\n+      : `${this.schemaName}.${this.tableName}`;\n+  }\n+\n+  get computedCollectionTableName() {\n+    return this.schemaName == null\n+      ? `${this.collectionTableName}`\n+      : `\"${this.schemaName}\".\"${this.collectionTableName}\"`;\n+  }\n+\n+  /**\n+   * Escape identifier\n+   *\n+   * @param identifier identifier value\n+   * @param alwaysQuote must identifier be quoted if not required\n+   */\n+  private escapeId(identifier: string, alwaysQuote: boolean): string {\n+    if (!identifier || identifier === \"\")\n+      throw new Error(\"Identifier is required\");\n+\n+    const len = identifier.length;\n+    const simpleIdentifier = /^[0-9a-zA-Z$_]*$/;\n+    if (simpleIdentifier.test(identifier)) {\n+      if (len < 1 || len > 64) {\n+        throw new Error(\"Invalid identifier length\");\n+      }\n+      if (alwaysQuote) return `\\`${identifier}\\``;\n+\n+      // Identifier names may begin with a numeral, but can't only contain numerals unless quoted.\n+      if (/^\\d+$/.test(identifier)) {\n+        // identifier containing only numerals must be quoted\n+        return `\\`${identifier}\\``;\n+      }\n+      // identifier containing only numerals must be quoted\n+      return identifier;\n+    } else {\n+      if (identifier.includes(\"\\u0000\")) {\n+        throw new Error(\"Invalid name - containing u0000 character\");\n+      }\n+      let ident = identifier;\n+      if (/^`.+`$/.test(identifier)) {\n+        ident = identifier.substring(1, identifier.length - 1);\n+      }\n+      if (len < 1 || len > 64) {\n+        throw new Error(\"Invalid identifier length\");\n+      }\n+      return `\\`${ident.replace(/`/g, \"``\")}\\``;\n+    }\n+  }\n+\n+  private printable(definition: string): string {\n+    return definition.replaceAll(/[^0-9a-zA-Z_]/g, \"\");\n+  }\n+\n+  /**\n+   * Static method to create a new `MariaDBStore` instance from a\n+   * connection. It creates a table if one does not exist, and calls\n+   * `connect` to return a new instance of `MariaDBStore`.\n+   *\n+   * @param embeddings - Embeddings instance.\n+   * @param fields - `MariaDBStoreArgs` instance\n+   * @param fields.dimensions Number of dimensions in your vector data type. default to 1536.\n+   * @returns A new instance of `MariaDBStore`.\n+   */\n+  static async initialize(\n+    embeddings: EmbeddingsInterface,\n+    config: MariaDBStoreArgs & { dimensions?: number }\n+  ): Promise<MariaDBStore> {\n+    const { dimensions, ...rest } = config;\n+    const mariadbStore = new MariaDBStore(embeddings, rest);\n+    await mariadbStore.ensureTableInDatabase(dimensions);\n+    await mariadbStore.ensureCollectionTableInDatabase();\n+    await mariadbStore.loadCollectionId();\n+\n+    return mariadbStore;\n+  }\n+\n+  /**\n+   * Static method to create a new `MariaDBStore` instance from an\n+   * array of texts and their metadata. It converts the texts into\n+   * `Document` instances and adds them to the store.\n+   *\n+   * @param texts - Array of texts.\n+   * @param metadatas - Array of metadata objects or a single metadata object.\n+   * @param embeddings - Embeddings instance.\n+   * @param dbConfig - `MariaDBStoreArgs` instance.\n+   * @returns Promise that resolves with a new instance of `MariaDBStore`.\n+   */\n+  static async fromTexts(\n+    texts: string[],\n+    metadatas: object[] | object,\n+    embeddings: EmbeddingsInterface,\n+    dbConfig: MariaDBStoreArgs & { dimensions?: number }\n+  ): Promise<MariaDBStore> {\n+    const docs = [];\n+    for (let i = 0; i < texts.length; i += 1) {\n+      const metadata = Array.isArray(metadatas) ? metadatas[i] : metadatas;\n+      const newDoc = new Document({\n+        pageContent: texts[i],\n+        metadata,\n+      });\n+      docs.push(newDoc);\n+    }\n+\n+    return MariaDBStore.fromDocuments(docs, embeddings, dbConfig);\n+  }\n+\n+  /**\n+   * Static method to create a new `MariaDBStore` instance from an\n+   * array of `Document` instances. It adds the documents to the store.\n+   *\n+   * @param docs - Array of `Document` instances.\n+   * @param embeddings - Embeddings instance.\n+   * @param dbConfig - `MariaDBStoreArgs` instance.\n+   * @returns Promise that resolves with a new instance of `MariaDBStore`.\n+   */\n+  static async fromDocuments(\n+    docs: Document[],\n+    embeddings: EmbeddingsInterface,\n+    dbConfig: MariaDBStoreArgs & { dimensions?: number }\n+  ): Promise<MariaDBStore> {\n+    const instance = await MariaDBStore.initialize(embeddings, dbConfig);\n+    await instance.addDocuments(docs, { ids: dbConfig.ids });\n+    return instance;\n+  }\n+\n+  _vectorstoreType(): string {\n+    return \"mariadb\";\n+  }\n+\n+  /**\n+   * Method to add documents to the vector store. It converts the documents into\n+   * vectors, and adds them to the store.\n+   *\n+   * @param documents - Array of `Document` instances.\n+   * @param options - Optional arguments for adding documents\n+   * @returns Promise that resolves when the documents have been added.\n+   */\n+  async addDocuments(\n+    documents: Document[],\n+    options?: { ids?: string[] }\n+  ): Promise<void> {\n+    const texts = documents.map(({ pageContent }) => pageContent);\n+\n+    return this.addVectors(\n+      await this.embeddings.embedDocuments(texts),\n+      documents,\n+      options\n+    );\n+  }\n+\n+  /**\n+   * Inserts a row for the collectionName provided at initialization if it does not\n+   * exist and set the collectionId.\n+   */\n+  private async loadCollectionId(): Promise<void> {\n+    if (this.collectionId) {\n+      return;\n+    }\n+\n+    if (this.collectionTableName) {\n+      const queryResult = await this.pool.query(\n+        {\n+          sql: `SELECT uuid from ${this.computedCollectionTableName} WHERE label = ?`,\n+          rowsAsArray: true,\n+        },\n+        [this.collectionName]\n+      );\n+      if (queryResult.length > 0) {\n+        this.collectionId = queryResult[0][0];\n+      } else {\n+        const insertString = `INSERT INTO ${this.computedCollectionTableName}(label, cmetadata) VALUES (?, ?) RETURNING uuid`;\n+        const insertResult = await this.pool.query(\n+          { sql: insertString, rowsAsArray: true },\n+          [this.collectionName, this.collectionMetadata]\n+        );\n+        this.collectionId = insertResult[0][0];\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Method to add vectors to the vector store. It converts the vectors into\n+   * rows and inserts them into the database.\n+   *\n+   * @param vectors - Array of vectors.\n+   * @param documents - Array of `Document` instances.\n+   * @param options - Optional arguments for adding documents\n+   * @returns Promise that resolves when the vectors have been added.\n+   */\n+  async addVectors(\n+    vectors: number[][],\n+    documents: Document[],\n+    options?: { ids?: string[] }\n+  ): Promise<void> {\n+    const ids = options?.ids;\n+\n+    // Either all documents have ids or none of them do to avoid confusion.\n+    if (ids !== undefined && ids.length !== vectors.length) {\n+      throw new Error(\n+        \"The number of ids must match the number of vectors provided.\"\n+      );\n+    }\n+    await this.loadCollectionId();\n+\n+    const insertQuery = `INSERT INTO ${this.computedTableName}(${",
        "comment_created_at": "2025-01-03T09:12:12+00:00",
        "comment_author": "rusher",
        "comment_body": "table name and columns names are all escaped to prevent any injection. see `escapeId` function",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1755519345",
    "pr_number": 6736,
    "pr_file": "langchain/src/storage/tests/file_system.test.ts",
    "created_at": "2024-09-11T20:08:42+00:00",
    "commented_code": ").toEqual([value1, value2]);\n    await fs.promises.rm(secondaryRootPath, { recursive: true, force: true });\n  });\n\n  test(\"Should disallow attempts to traverse paths outside of a subfolder\", async () => {\n    const encoder = new TextEncoder();\n    const store = await LocalFileStore.fromPath(secondaryRootPath);\n    const value1 = new Date().toISOString();\n    await expect(\n      store.mset([[\"../foo\", encoder.encode(value1)]])\n    ).rejects.toThrowError();\n    await expect(\n      store.mset([[\"/foo\", encoder.encode(value1)]])\n    ).rejects.toThrowError();\n    await expect(store.mget([\"../foo\"])).rejects.toThrowError();",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1755519345",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 6736,
        "pr_file": "langchain/src/storage/tests/file_system.test.ts",
        "discussion_id": "1755519345",
        "commented_code": "@@ -88,4 +88,19 @@ describe(\"LocalFileStore\", () => {\n     ).toEqual([value1, value2]);\n     await fs.promises.rm(secondaryRootPath, { recursive: true, force: true });\n   });\n+\n+  test(\"Should disallow attempts to traverse paths outside of a subfolder\", async () => {\n+    const encoder = new TextEncoder();\n+    const store = await LocalFileStore.fromPath(secondaryRootPath);\n+    const value1 = new Date().toISOString();\n+    await expect(\n+      store.mset([[\"../foo\", encoder.encode(value1)]])\n+    ).rejects.toThrowError();\n+    await expect(\n+      store.mset([[\"/foo\", encoder.encode(value1)]])\n+    ).rejects.toThrowError();\n+    await expect(store.mget([\"../foo\"])).rejects.toThrowError();",
        "comment_created_at": "2024-09-11T20:08:42+00:00",
        "comment_author": "eyurtsev",
        "comment_body": "maybe add test for `\\` as well?",
        "pr_file_module": null
      },
      {
        "comment_id": "1755531830",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 6736,
        "pr_file": "langchain/src/storage/tests/file_system.test.ts",
        "discussion_id": "1755519345",
        "commented_code": "@@ -88,4 +88,19 @@ describe(\"LocalFileStore\", () => {\n     ).toEqual([value1, value2]);\n     await fs.promises.rm(secondaryRootPath, { recursive: true, force: true });\n   });\n+\n+  test(\"Should disallow attempts to traverse paths outside of a subfolder\", async () => {\n+    const encoder = new TextEncoder();\n+    const store = await LocalFileStore.fromPath(secondaryRootPath);\n+    const value1 = new Date().toISOString();\n+    await expect(\n+      store.mset([[\"../foo\", encoder.encode(value1)]])\n+    ).rejects.toThrowError();\n+    await expect(\n+      store.mset([[\"/foo\", encoder.encode(value1)]])\n+    ).rejects.toThrowError();\n+    await expect(store.mget([\"../foo\"])).rejects.toThrowError();",
        "comment_created_at": "2024-09-11T20:18:07+00:00",
        "comment_author": "jacoblee93",
        "comment_body": "Yeah good call",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1667178276",
    "pr_number": 5835,
    "pr_file": "libs/langchain-google-common/src/experimental/utils/media_core.ts",
    "created_at": "2024-07-05T21:49:09+00:00",
    "commented_code": "import { v4 as uuidv4 } from \"uuid\";\nimport { BaseStore } from \"@langchain/core/stores\";\nimport { Serializable } from \"@langchain/core/load/serializable\";\n\nexport interface MediaBlobParameters {\n  data?: Blob;\n\n  metadata?: Record<string, unknown>;\n\n  path?: string;\n}\n\n/**\n * Represents a chunk of data that can be identified by the path where the\n * data is (or will be) located, along with optional metadata about the data.\n */\nexport class MediaBlob\n  extends Serializable // FIXME - I'm not sure this serializes or deserializes correctly\n  implements MediaBlobParameters\n{\n  lc_serializable = true;\n\n  lc_namespace = [\"langchain\", \"google-common\"]; // FIXME - What should this be? And why?\n\n  data?: Blob;\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  metadata?: Record<string, any>;\n\n  path?: string;\n\n  constructor(params?: MediaBlobParameters) {\n    super(params);\n    this.data = params?.data;\n    this.metadata = params?.metadata;\n    this.path = params?.path;\n  }\n\n  get size(): number {\n    return this.data?.size ?? 0;\n  }\n\n  get dataType(): string {\n    return this.data?.type ?? \"\";\n  }\n\n  get encoding(): string {\n    const charsetEquals = this.dataType.indexOf(\"charset=\");\n    return charsetEquals === -1\n      ? \"utf-8\"\n      : this.dataType.substring(charsetEquals + 8);\n  }\n\n  get mimetype(): string {\n    const semicolon = this.dataType.indexOf(\";\");\n    return semicolon === -1\n      ? this.dataType\n      : this.dataType.substring(0, semicolon);\n  }\n\n  async asString(): Promise<string> {\n    const data = this.data ?? new Blob([]);\n    const dataBuffer = await data.arrayBuffer();\n    const dataArray = new Uint8Array(dataBuffer);\n\n    // Need to handle the array in smaller chunks to deal with stack size limits\n    let ret = \"\";\n    const chunkSize = 102400;\n    for (let i = 0; i < dataArray.length; i += chunkSize) {\n      const chunk = dataArray.subarray(i, i + chunkSize);\n      ret += String.fromCharCode(...chunk);\n    }\n\n    return ret;\n  }\n\n  async asBase64(): Promise<string> {\n    return btoa(await this.asString());\n  }\n\n  async asDataUrl(): Promise<string> {\n    return `data:${this.mimetype};base64,${await this.asBase64()}`;\n  }\n\n  async asUri(): Promise<string> {\n    return this.path ?? (await this.asDataUrl());\n  }\n\n  async encode(): Promise<{ encoded: string; encoding: string }> {\n    const dataUrl = await this.asDataUrl();\n    const comma = dataUrl.indexOf(\",\");\n    const encoded = dataUrl.substring(comma + 1);\n    const encoding: string = dataUrl.indexOf(\"base64\") > -1 ? \"base64\" : \"8bit\";\n    return {\n      encoded,\n      encoding,\n    };\n  }\n}\n\nexport type ActionIfInvalidAction =\n  | \"ignore\"\n  | \"prefixPath\"\n  | \"prefixUuid\"\n  | \"removePath\";\n\nexport interface BlobStoreStoreOptions {\n  /**\n   * If the path is missing or invalid in the blob, how should we create\n   * a new path?\n   * Subclasses may define their own methods, but the following are supported\n   * by default:\n   * - Undefined or an emtpy string: Reject the blob\n   * - \"ignore\": Attempt to store it anyway (but this may fail)\n   * - \"prefixPath\": Use the default prefix for the BlobStore and get the\n   *   unique portion from the URL. The original path is stored in the metadata\n   * - \"prefixUuid\": Use the default prefix for the BlobStore and get the\n   *   unique portion from a generated UUID. The original path is stored\n   *   in the metadata\n   */\n  actionIfInvalid?: ActionIfInvalidAction;\n\n  /**\n   * The expected prefix for URIs that are stored.\n   * This may be used to test if a MediaBlob is valid and used to create a new\n   * path if \"prefixPath\" or \"prefixUuid\" is set for actionIfInvalid.\n   */\n  pathPrefix?: string;\n}\n\nexport type ActionIfBlobMissingAction = \"emptyBlob\";\n\nexport interface BlobStoreFetchOptions {\n  /**\n   * If the blob is not found when fetching, what should we do?\n   * Subclasses may define their own methods, but the following are supported\n   * by default:\n   * - Undefined or an empty string: return undefined\n   * - \"emptyBlob\": return a new MediaBlob that has the path set, but nothing else.\n   */\n  actionIfBlobMissing?: ActionIfBlobMissingAction;\n}\n\nexport interface BlobStoreOptions {\n  defaultStoreOptions?: BlobStoreStoreOptions;\n\n  defaultFetchOptions?: BlobStoreFetchOptions;\n}\n\n/**\n * A specialized Store that is designed to handle MediaBlobs and use the\n * key that is included in the blob to determine exactly how it is stored.\n *\n * The full details of a MediaBlob may be changed when it is stored.\n * For example, it may get additional or different Metadata. This should be\n * what is returned when the store() method is called.\n *\n * Although BlobStore extends BaseStore, not all of the methods from\n * BaseStore may be implemented (or even possible). Those that are not\n * implemented should be documented and throw an Error if called.\n */\nexport abstract class BlobStore extends BaseStore<string, MediaBlob> {\n  lc_namespace = [\"langchain\", \"google-common\"]; // FIXME - What should this be? And why?\n\n  defaultStoreOptions: BlobStoreStoreOptions;\n\n  defaultFetchOptions: BlobStoreFetchOptions;\n\n  constructor(opts?: BlobStoreOptions) {\n    super(opts);\n    this.defaultStoreOptions = opts?.defaultStoreOptions ?? {};\n    this.defaultFetchOptions = opts?.defaultFetchOptions ?? {};\n  }\n\n  protected async _realKey(key: string | MediaBlob): Promise<string> {\n    return typeof key === \"string\" ? key : await key.asUri();\n  }\n\n  /**\n   * Is the path set in the MediaBlob supported by this BlobStore?\n   * Subclasses must implement and evaluate `blob.path` to make this\n   * determination.\n   *\n   * Although this is async, this is expected to be a relatively fast operation\n   * (ie - you shouldn't make network calls).\n   *\n   * The default implementation assumes that undefined blob.paths are invalid\n   * and then uses the replacePathPrefix (or an empty string) as an assumed path\n   * to start with.\n   *\n   * @param blob The blob to test\n   * @param opts Any options (if needed) that may be used to determine if it is valid\n   * @return If the string represented by blob.path is supported.\n   */\n  protected _hasValidPath(\n    blob: MediaBlob,\n    opts?: BlobStoreStoreOptions\n  ): Promise<boolean> {\n    const path = blob.path ?? \"\";\n    const prefix = opts?.pathPrefix ?? \"\";\n    const isPrefixed =\n      typeof blob.path !== \"undefined\" && path.startsWith(prefix);\n    return Promise.resolve(isPrefixed);\n  }\n\n  protected _blobPathSuffix(blob: MediaBlob): string {\n    // Get the path currently set and make sure we treat it as a string\n    const blobPath = `${blob.path}`;\n\n    // Advance past the first set of /\n    let pathStart = blobPath.indexOf(\"/\") + 1;\n    while (blobPath.charAt(pathStart) === \"/\") {\n      pathStart += 1;\n    }\n\n    // We will use the rest as the path for a replacement\n    return blobPath.substring(pathStart);\n  }\n\n  protected async _newBlob(\n    oldBlob: MediaBlob,\n    newPath?: string\n  ): Promise<MediaBlob> {\n    const oldPath = oldBlob.path;\n    const metadata = oldBlob?.metadata ?? {};\n    metadata.langchainOldPath = oldPath;\n    const newBlob = new MediaBlob({\n      ...oldBlob,\n      metadata,\n    });\n    if (newPath) {\n      newBlob.path = newPath;\n    } else if (newBlob.path) {\n      delete newBlob.path;\n    }\n    return newBlob;\n  }\n\n  protected async _validBlobPrefixPath(\n    blob: MediaBlob,\n    opts?: BlobStoreStoreOptions\n  ): Promise<MediaBlob> {\n    const prefix = opts?.pathPrefix ?? \"\";\n    const suffix = this._blobPathSuffix(blob);\n    const newPath = `${prefix}${suffix}`;\n    return this._newBlob(blob, newPath);\n  }\n\n  protected async _validBlobPrefixUuid(\n    blob: MediaBlob,\n    opts?: BlobStoreStoreOptions\n  ): Promise<MediaBlob> {\n    const prefix = opts?.pathPrefix ?? \"\";\n    const suffix = uuidv4(); // TODO - option to specify version?\n    const newPath = `${prefix}${suffix}`;\n    return this._newBlob(blob, newPath);\n  }\n\n  protected async _validBlobRemovePath(\n    blob: MediaBlob,\n    _opts?: BlobStoreStoreOptions\n  ): Promise<MediaBlob> {\n    return this._newBlob(blob, undefined);\n  }\n\n  /**\n   * Based on the blob and options, return a blob that has a valid path\n   * that can be saved.\n   * @param blob\n   * @param opts\n   */\n  protected async _validStoreBlob(\n    blob: MediaBlob,\n    opts?: BlobStoreStoreOptions\n  ): Promise<MediaBlob | undefined> {\n    if (await this._hasValidPath(blob, opts)) {\n      return blob;\n    }\n    switch (opts?.actionIfInvalid) {\n      case \"ignore\":\n        return blob;\n      case \"prefixPath\":\n        return this._validBlobPrefixPath(blob, opts);\n      case \"prefixUuid\":\n        return this._validBlobPrefixUuid(blob, opts);\n      case \"removePath\":\n        return this._validBlobRemovePath(blob, opts);\n      default:\n        return undefined;\n    }\n  }\n\n  async store(\n    blob: MediaBlob,\n    opts: BlobStoreStoreOptions = {}\n  ): Promise<MediaBlob | undefined> {\n    const allOpts: BlobStoreStoreOptions = {\n      ...this.defaultStoreOptions,\n      ...opts,\n    };\n    const validBlob = await this._validStoreBlob(blob, allOpts);\n    if (typeof validBlob !== \"undefined\") {\n      const validKey = await validBlob.asUri();\n      await this.mset([[validKey, validBlob]]);\n      const savedKey = await validBlob.asUri();\n      return await this.fetch(savedKey);\n    }\n    return undefined;\n  }\n\n  protected async _missingFetchBlobEmpty(\n    path: string,\n    _opts?: BlobStoreFetchOptions\n  ): Promise<MediaBlob> {\n    return new MediaBlob({ path });\n  }\n\n  protected async _missingFetchBlob(\n    path: string,\n    opts?: BlobStoreFetchOptions\n  ): Promise<MediaBlob | undefined> {\n    switch (opts?.actionIfBlobMissing) {\n      case \"emptyBlob\":\n        return this._missingFetchBlobEmpty(path, opts);\n      default:\n        return undefined;\n    }\n  }\n\n  async fetch(\n    key: string | MediaBlob,\n    opts: BlobStoreFetchOptions = {}\n  ): Promise<MediaBlob | undefined> {\n    const allOpts: BlobStoreFetchOptions = {\n      ...this.defaultFetchOptions,\n      ...opts,\n    };\n    const realKey = await this._realKey(key);\n    const ret = await this.mget([realKey]);\n    return ret?.[0] ?? (await this._missingFetchBlob(realKey, allOpts));\n  }\n}\n\nexport interface BackedBlobStoreOptions extends BlobStoreOptions {\n  backingStore: BaseStore<string, MediaBlob>;\n}\n\nexport class BackedBlobStore extends BlobStore {\n  backingStore: BaseStore<string, MediaBlob>;\n\n  constructor(opts: BackedBlobStoreOptions) {\n    super(opts);\n    this.backingStore = opts.backingStore;\n  }\n\n  mdelete(keys: string[]): Promise<void> {\n    return this.backingStore.mdelete(keys);\n  }\n\n  mget(keys: string[]): Promise<(MediaBlob | undefined)[]> {\n    return this.backingStore.mget(keys);\n  }\n\n  mset(keyValuePairs: [string, MediaBlob][]): Promise<void> {\n    return this.backingStore.mset(keyValuePairs);\n  }\n\n  yieldKeys(prefix: string | undefined): AsyncGenerator<string> {\n    return this.backingStore.yieldKeys(prefix);\n  }\n}\n\nexport class SimpleWebBlobStore extends BlobStore {",
    "repo_full_name": "langchain-ai/langchainjs",
    "discussion_comments": [
      {
        "comment_id": "1667178276",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 5835,
        "pr_file": "libs/langchain-google-common/src/experimental/utils/media_core.ts",
        "discussion_id": "1667178276",
        "commented_code": "@@ -0,0 +1,504 @@\n+import { v4 as uuidv4 } from \"uuid\";\n+import { BaseStore } from \"@langchain/core/stores\";\n+import { Serializable } from \"@langchain/core/load/serializable\";\n+\n+export interface MediaBlobParameters {\n+  data?: Blob;\n+\n+  metadata?: Record<string, unknown>;\n+\n+  path?: string;\n+}\n+\n+/**\n+ * Represents a chunk of data that can be identified by the path where the\n+ * data is (or will be) located, along with optional metadata about the data.\n+ */\n+export class MediaBlob\n+  extends Serializable // FIXME - I'm not sure this serializes or deserializes correctly\n+  implements MediaBlobParameters\n+{\n+  lc_serializable = true;\n+\n+  lc_namespace = [\"langchain\", \"google-common\"]; // FIXME - What should this be? And why?\n+\n+  data?: Blob;\n+\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  metadata?: Record<string, any>;\n+\n+  path?: string;\n+\n+  constructor(params?: MediaBlobParameters) {\n+    super(params);\n+    this.data = params?.data;\n+    this.metadata = params?.metadata;\n+    this.path = params?.path;\n+  }\n+\n+  get size(): number {\n+    return this.data?.size ?? 0;\n+  }\n+\n+  get dataType(): string {\n+    return this.data?.type ?? \"\";\n+  }\n+\n+  get encoding(): string {\n+    const charsetEquals = this.dataType.indexOf(\"charset=\");\n+    return charsetEquals === -1\n+      ? \"utf-8\"\n+      : this.dataType.substring(charsetEquals + 8);\n+  }\n+\n+  get mimetype(): string {\n+    const semicolon = this.dataType.indexOf(\";\");\n+    return semicolon === -1\n+      ? this.dataType\n+      : this.dataType.substring(0, semicolon);\n+  }\n+\n+  async asString(): Promise<string> {\n+    const data = this.data ?? new Blob([]);\n+    const dataBuffer = await data.arrayBuffer();\n+    const dataArray = new Uint8Array(dataBuffer);\n+\n+    // Need to handle the array in smaller chunks to deal with stack size limits\n+    let ret = \"\";\n+    const chunkSize = 102400;\n+    for (let i = 0; i < dataArray.length; i += chunkSize) {\n+      const chunk = dataArray.subarray(i, i + chunkSize);\n+      ret += String.fromCharCode(...chunk);\n+    }\n+\n+    return ret;\n+  }\n+\n+  async asBase64(): Promise<string> {\n+    return btoa(await this.asString());\n+  }\n+\n+  async asDataUrl(): Promise<string> {\n+    return `data:${this.mimetype};base64,${await this.asBase64()}`;\n+  }\n+\n+  async asUri(): Promise<string> {\n+    return this.path ?? (await this.asDataUrl());\n+  }\n+\n+  async encode(): Promise<{ encoded: string; encoding: string }> {\n+    const dataUrl = await this.asDataUrl();\n+    const comma = dataUrl.indexOf(\",\");\n+    const encoded = dataUrl.substring(comma + 1);\n+    const encoding: string = dataUrl.indexOf(\"base64\") > -1 ? \"base64\" : \"8bit\";\n+    return {\n+      encoded,\n+      encoding,\n+    };\n+  }\n+}\n+\n+export type ActionIfInvalidAction =\n+  | \"ignore\"\n+  | \"prefixPath\"\n+  | \"prefixUuid\"\n+  | \"removePath\";\n+\n+export interface BlobStoreStoreOptions {\n+  /**\n+   * If the path is missing or invalid in the blob, how should we create\n+   * a new path?\n+   * Subclasses may define their own methods, but the following are supported\n+   * by default:\n+   * - Undefined or an emtpy string: Reject the blob\n+   * - \"ignore\": Attempt to store it anyway (but this may fail)\n+   * - \"prefixPath\": Use the default prefix for the BlobStore and get the\n+   *   unique portion from the URL. The original path is stored in the metadata\n+   * - \"prefixUuid\": Use the default prefix for the BlobStore and get the\n+   *   unique portion from a generated UUID. The original path is stored\n+   *   in the metadata\n+   */\n+  actionIfInvalid?: ActionIfInvalidAction;\n+\n+  /**\n+   * The expected prefix for URIs that are stored.\n+   * This may be used to test if a MediaBlob is valid and used to create a new\n+   * path if \"prefixPath\" or \"prefixUuid\" is set for actionIfInvalid.\n+   */\n+  pathPrefix?: string;\n+}\n+\n+export type ActionIfBlobMissingAction = \"emptyBlob\";\n+\n+export interface BlobStoreFetchOptions {\n+  /**\n+   * If the blob is not found when fetching, what should we do?\n+   * Subclasses may define their own methods, but the following are supported\n+   * by default:\n+   * - Undefined or an empty string: return undefined\n+   * - \"emptyBlob\": return a new MediaBlob that has the path set, but nothing else.\n+   */\n+  actionIfBlobMissing?: ActionIfBlobMissingAction;\n+}\n+\n+export interface BlobStoreOptions {\n+  defaultStoreOptions?: BlobStoreStoreOptions;\n+\n+  defaultFetchOptions?: BlobStoreFetchOptions;\n+}\n+\n+/**\n+ * A specialized Store that is designed to handle MediaBlobs and use the\n+ * key that is included in the blob to determine exactly how it is stored.\n+ *\n+ * The full details of a MediaBlob may be changed when it is stored.\n+ * For example, it may get additional or different Metadata. This should be\n+ * what is returned when the store() method is called.\n+ *\n+ * Although BlobStore extends BaseStore, not all of the methods from\n+ * BaseStore may be implemented (or even possible). Those that are not\n+ * implemented should be documented and throw an Error if called.\n+ */\n+export abstract class BlobStore extends BaseStore<string, MediaBlob> {\n+  lc_namespace = [\"langchain\", \"google-common\"]; // FIXME - What should this be? And why?\n+\n+  defaultStoreOptions: BlobStoreStoreOptions;\n+\n+  defaultFetchOptions: BlobStoreFetchOptions;\n+\n+  constructor(opts?: BlobStoreOptions) {\n+    super(opts);\n+    this.defaultStoreOptions = opts?.defaultStoreOptions ?? {};\n+    this.defaultFetchOptions = opts?.defaultFetchOptions ?? {};\n+  }\n+\n+  protected async _realKey(key: string | MediaBlob): Promise<string> {\n+    return typeof key === \"string\" ? key : await key.asUri();\n+  }\n+\n+  /**\n+   * Is the path set in the MediaBlob supported by this BlobStore?\n+   * Subclasses must implement and evaluate `blob.path` to make this\n+   * determination.\n+   *\n+   * Although this is async, this is expected to be a relatively fast operation\n+   * (ie - you shouldn't make network calls).\n+   *\n+   * The default implementation assumes that undefined blob.paths are invalid\n+   * and then uses the replacePathPrefix (or an empty string) as an assumed path\n+   * to start with.\n+   *\n+   * @param blob The blob to test\n+   * @param opts Any options (if needed) that may be used to determine if it is valid\n+   * @return If the string represented by blob.path is supported.\n+   */\n+  protected _hasValidPath(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<boolean> {\n+    const path = blob.path ?? \"\";\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const isPrefixed =\n+      typeof blob.path !== \"undefined\" && path.startsWith(prefix);\n+    return Promise.resolve(isPrefixed);\n+  }\n+\n+  protected _blobPathSuffix(blob: MediaBlob): string {\n+    // Get the path currently set and make sure we treat it as a string\n+    const blobPath = `${blob.path}`;\n+\n+    // Advance past the first set of /\n+    let pathStart = blobPath.indexOf(\"/\") + 1;\n+    while (blobPath.charAt(pathStart) === \"/\") {\n+      pathStart += 1;\n+    }\n+\n+    // We will use the rest as the path for a replacement\n+    return blobPath.substring(pathStart);\n+  }\n+\n+  protected async _newBlob(\n+    oldBlob: MediaBlob,\n+    newPath?: string\n+  ): Promise<MediaBlob> {\n+    const oldPath = oldBlob.path;\n+    const metadata = oldBlob?.metadata ?? {};\n+    metadata.langchainOldPath = oldPath;\n+    const newBlob = new MediaBlob({\n+      ...oldBlob,\n+      metadata,\n+    });\n+    if (newPath) {\n+      newBlob.path = newPath;\n+    } else if (newBlob.path) {\n+      delete newBlob.path;\n+    }\n+    return newBlob;\n+  }\n+\n+  protected async _validBlobPrefixPath(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const suffix = this._blobPathSuffix(blob);\n+    const newPath = `${prefix}${suffix}`;\n+    return this._newBlob(blob, newPath);\n+  }\n+\n+  protected async _validBlobPrefixUuid(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const suffix = uuidv4(); // TODO - option to specify version?\n+    const newPath = `${prefix}${suffix}`;\n+    return this._newBlob(blob, newPath);\n+  }\n+\n+  protected async _validBlobRemovePath(\n+    blob: MediaBlob,\n+    _opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    return this._newBlob(blob, undefined);\n+  }\n+\n+  /**\n+   * Based on the blob and options, return a blob that has a valid path\n+   * that can be saved.\n+   * @param blob\n+   * @param opts\n+   */\n+  protected async _validStoreBlob(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob | undefined> {\n+    if (await this._hasValidPath(blob, opts)) {\n+      return blob;\n+    }\n+    switch (opts?.actionIfInvalid) {\n+      case \"ignore\":\n+        return blob;\n+      case \"prefixPath\":\n+        return this._validBlobPrefixPath(blob, opts);\n+      case \"prefixUuid\":\n+        return this._validBlobPrefixUuid(blob, opts);\n+      case \"removePath\":\n+        return this._validBlobRemovePath(blob, opts);\n+      default:\n+        return undefined;\n+    }\n+  }\n+\n+  async store(\n+    blob: MediaBlob,\n+    opts: BlobStoreStoreOptions = {}\n+  ): Promise<MediaBlob | undefined> {\n+    const allOpts: BlobStoreStoreOptions = {\n+      ...this.defaultStoreOptions,\n+      ...opts,\n+    };\n+    const validBlob = await this._validStoreBlob(blob, allOpts);\n+    if (typeof validBlob !== \"undefined\") {\n+      const validKey = await validBlob.asUri();\n+      await this.mset([[validKey, validBlob]]);\n+      const savedKey = await validBlob.asUri();\n+      return await this.fetch(savedKey);\n+    }\n+    return undefined;\n+  }\n+\n+  protected async _missingFetchBlobEmpty(\n+    path: string,\n+    _opts?: BlobStoreFetchOptions\n+  ): Promise<MediaBlob> {\n+    return new MediaBlob({ path });\n+  }\n+\n+  protected async _missingFetchBlob(\n+    path: string,\n+    opts?: BlobStoreFetchOptions\n+  ): Promise<MediaBlob | undefined> {\n+    switch (opts?.actionIfBlobMissing) {\n+      case \"emptyBlob\":\n+        return this._missingFetchBlobEmpty(path, opts);\n+      default:\n+        return undefined;\n+    }\n+  }\n+\n+  async fetch(\n+    key: string | MediaBlob,\n+    opts: BlobStoreFetchOptions = {}\n+  ): Promise<MediaBlob | undefined> {\n+    const allOpts: BlobStoreFetchOptions = {\n+      ...this.defaultFetchOptions,\n+      ...opts,\n+    };\n+    const realKey = await this._realKey(key);\n+    const ret = await this.mget([realKey]);\n+    return ret?.[0] ?? (await this._missingFetchBlob(realKey, allOpts));\n+  }\n+}\n+\n+export interface BackedBlobStoreOptions extends BlobStoreOptions {\n+  backingStore: BaseStore<string, MediaBlob>;\n+}\n+\n+export class BackedBlobStore extends BlobStore {\n+  backingStore: BaseStore<string, MediaBlob>;\n+\n+  constructor(opts: BackedBlobStoreOptions) {\n+    super(opts);\n+    this.backingStore = opts.backingStore;\n+  }\n+\n+  mdelete(keys: string[]): Promise<void> {\n+    return this.backingStore.mdelete(keys);\n+  }\n+\n+  mget(keys: string[]): Promise<(MediaBlob | undefined)[]> {\n+    return this.backingStore.mget(keys);\n+  }\n+\n+  mset(keyValuePairs: [string, MediaBlob][]): Promise<void> {\n+    return this.backingStore.mset(keyValuePairs);\n+  }\n+\n+  yieldKeys(prefix: string | undefined): AsyncGenerator<string> {\n+    return this.backingStore.yieldKeys(prefix);\n+  }\n+}\n+\n+export class SimpleWebBlobStore extends BlobStore {",
        "comment_created_at": "2024-07-05T21:49:09+00:00",
        "comment_author": "eyurtsev",
        "comment_body": "If this code can be executed on a server, then it allows for an SSRF attack",
        "pr_file_module": null
      },
      {
        "comment_id": "1669466959",
        "repo_full_name": "langchain-ai/langchainjs",
        "pr_number": 5835,
        "pr_file": "libs/langchain-google-common/src/experimental/utils/media_core.ts",
        "discussion_id": "1667178276",
        "commented_code": "@@ -0,0 +1,504 @@\n+import { v4 as uuidv4 } from \"uuid\";\n+import { BaseStore } from \"@langchain/core/stores\";\n+import { Serializable } from \"@langchain/core/load/serializable\";\n+\n+export interface MediaBlobParameters {\n+  data?: Blob;\n+\n+  metadata?: Record<string, unknown>;\n+\n+  path?: string;\n+}\n+\n+/**\n+ * Represents a chunk of data that can be identified by the path where the\n+ * data is (or will be) located, along with optional metadata about the data.\n+ */\n+export class MediaBlob\n+  extends Serializable // FIXME - I'm not sure this serializes or deserializes correctly\n+  implements MediaBlobParameters\n+{\n+  lc_serializable = true;\n+\n+  lc_namespace = [\"langchain\", \"google-common\"]; // FIXME - What should this be? And why?\n+\n+  data?: Blob;\n+\n+  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+  metadata?: Record<string, any>;\n+\n+  path?: string;\n+\n+  constructor(params?: MediaBlobParameters) {\n+    super(params);\n+    this.data = params?.data;\n+    this.metadata = params?.metadata;\n+    this.path = params?.path;\n+  }\n+\n+  get size(): number {\n+    return this.data?.size ?? 0;\n+  }\n+\n+  get dataType(): string {\n+    return this.data?.type ?? \"\";\n+  }\n+\n+  get encoding(): string {\n+    const charsetEquals = this.dataType.indexOf(\"charset=\");\n+    return charsetEquals === -1\n+      ? \"utf-8\"\n+      : this.dataType.substring(charsetEquals + 8);\n+  }\n+\n+  get mimetype(): string {\n+    const semicolon = this.dataType.indexOf(\";\");\n+    return semicolon === -1\n+      ? this.dataType\n+      : this.dataType.substring(0, semicolon);\n+  }\n+\n+  async asString(): Promise<string> {\n+    const data = this.data ?? new Blob([]);\n+    const dataBuffer = await data.arrayBuffer();\n+    const dataArray = new Uint8Array(dataBuffer);\n+\n+    // Need to handle the array in smaller chunks to deal with stack size limits\n+    let ret = \"\";\n+    const chunkSize = 102400;\n+    for (let i = 0; i < dataArray.length; i += chunkSize) {\n+      const chunk = dataArray.subarray(i, i + chunkSize);\n+      ret += String.fromCharCode(...chunk);\n+    }\n+\n+    return ret;\n+  }\n+\n+  async asBase64(): Promise<string> {\n+    return btoa(await this.asString());\n+  }\n+\n+  async asDataUrl(): Promise<string> {\n+    return `data:${this.mimetype};base64,${await this.asBase64()}`;\n+  }\n+\n+  async asUri(): Promise<string> {\n+    return this.path ?? (await this.asDataUrl());\n+  }\n+\n+  async encode(): Promise<{ encoded: string; encoding: string }> {\n+    const dataUrl = await this.asDataUrl();\n+    const comma = dataUrl.indexOf(\",\");\n+    const encoded = dataUrl.substring(comma + 1);\n+    const encoding: string = dataUrl.indexOf(\"base64\") > -1 ? \"base64\" : \"8bit\";\n+    return {\n+      encoded,\n+      encoding,\n+    };\n+  }\n+}\n+\n+export type ActionIfInvalidAction =\n+  | \"ignore\"\n+  | \"prefixPath\"\n+  | \"prefixUuid\"\n+  | \"removePath\";\n+\n+export interface BlobStoreStoreOptions {\n+  /**\n+   * If the path is missing or invalid in the blob, how should we create\n+   * a new path?\n+   * Subclasses may define their own methods, but the following are supported\n+   * by default:\n+   * - Undefined or an emtpy string: Reject the blob\n+   * - \"ignore\": Attempt to store it anyway (but this may fail)\n+   * - \"prefixPath\": Use the default prefix for the BlobStore and get the\n+   *   unique portion from the URL. The original path is stored in the metadata\n+   * - \"prefixUuid\": Use the default prefix for the BlobStore and get the\n+   *   unique portion from a generated UUID. The original path is stored\n+   *   in the metadata\n+   */\n+  actionIfInvalid?: ActionIfInvalidAction;\n+\n+  /**\n+   * The expected prefix for URIs that are stored.\n+   * This may be used to test if a MediaBlob is valid and used to create a new\n+   * path if \"prefixPath\" or \"prefixUuid\" is set for actionIfInvalid.\n+   */\n+  pathPrefix?: string;\n+}\n+\n+export type ActionIfBlobMissingAction = \"emptyBlob\";\n+\n+export interface BlobStoreFetchOptions {\n+  /**\n+   * If the blob is not found when fetching, what should we do?\n+   * Subclasses may define their own methods, but the following are supported\n+   * by default:\n+   * - Undefined or an empty string: return undefined\n+   * - \"emptyBlob\": return a new MediaBlob that has the path set, but nothing else.\n+   */\n+  actionIfBlobMissing?: ActionIfBlobMissingAction;\n+}\n+\n+export interface BlobStoreOptions {\n+  defaultStoreOptions?: BlobStoreStoreOptions;\n+\n+  defaultFetchOptions?: BlobStoreFetchOptions;\n+}\n+\n+/**\n+ * A specialized Store that is designed to handle MediaBlobs and use the\n+ * key that is included in the blob to determine exactly how it is stored.\n+ *\n+ * The full details of a MediaBlob may be changed when it is stored.\n+ * For example, it may get additional or different Metadata. This should be\n+ * what is returned when the store() method is called.\n+ *\n+ * Although BlobStore extends BaseStore, not all of the methods from\n+ * BaseStore may be implemented (or even possible). Those that are not\n+ * implemented should be documented and throw an Error if called.\n+ */\n+export abstract class BlobStore extends BaseStore<string, MediaBlob> {\n+  lc_namespace = [\"langchain\", \"google-common\"]; // FIXME - What should this be? And why?\n+\n+  defaultStoreOptions: BlobStoreStoreOptions;\n+\n+  defaultFetchOptions: BlobStoreFetchOptions;\n+\n+  constructor(opts?: BlobStoreOptions) {\n+    super(opts);\n+    this.defaultStoreOptions = opts?.defaultStoreOptions ?? {};\n+    this.defaultFetchOptions = opts?.defaultFetchOptions ?? {};\n+  }\n+\n+  protected async _realKey(key: string | MediaBlob): Promise<string> {\n+    return typeof key === \"string\" ? key : await key.asUri();\n+  }\n+\n+  /**\n+   * Is the path set in the MediaBlob supported by this BlobStore?\n+   * Subclasses must implement and evaluate `blob.path` to make this\n+   * determination.\n+   *\n+   * Although this is async, this is expected to be a relatively fast operation\n+   * (ie - you shouldn't make network calls).\n+   *\n+   * The default implementation assumes that undefined blob.paths are invalid\n+   * and then uses the replacePathPrefix (or an empty string) as an assumed path\n+   * to start with.\n+   *\n+   * @param blob The blob to test\n+   * @param opts Any options (if needed) that may be used to determine if it is valid\n+   * @return If the string represented by blob.path is supported.\n+   */\n+  protected _hasValidPath(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<boolean> {\n+    const path = blob.path ?? \"\";\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const isPrefixed =\n+      typeof blob.path !== \"undefined\" && path.startsWith(prefix);\n+    return Promise.resolve(isPrefixed);\n+  }\n+\n+  protected _blobPathSuffix(blob: MediaBlob): string {\n+    // Get the path currently set and make sure we treat it as a string\n+    const blobPath = `${blob.path}`;\n+\n+    // Advance past the first set of /\n+    let pathStart = blobPath.indexOf(\"/\") + 1;\n+    while (blobPath.charAt(pathStart) === \"/\") {\n+      pathStart += 1;\n+    }\n+\n+    // We will use the rest as the path for a replacement\n+    return blobPath.substring(pathStart);\n+  }\n+\n+  protected async _newBlob(\n+    oldBlob: MediaBlob,\n+    newPath?: string\n+  ): Promise<MediaBlob> {\n+    const oldPath = oldBlob.path;\n+    const metadata = oldBlob?.metadata ?? {};\n+    metadata.langchainOldPath = oldPath;\n+    const newBlob = new MediaBlob({\n+      ...oldBlob,\n+      metadata,\n+    });\n+    if (newPath) {\n+      newBlob.path = newPath;\n+    } else if (newBlob.path) {\n+      delete newBlob.path;\n+    }\n+    return newBlob;\n+  }\n+\n+  protected async _validBlobPrefixPath(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const suffix = this._blobPathSuffix(blob);\n+    const newPath = `${prefix}${suffix}`;\n+    return this._newBlob(blob, newPath);\n+  }\n+\n+  protected async _validBlobPrefixUuid(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    const prefix = opts?.pathPrefix ?? \"\";\n+    const suffix = uuidv4(); // TODO - option to specify version?\n+    const newPath = `${prefix}${suffix}`;\n+    return this._newBlob(blob, newPath);\n+  }\n+\n+  protected async _validBlobRemovePath(\n+    blob: MediaBlob,\n+    _opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob> {\n+    return this._newBlob(blob, undefined);\n+  }\n+\n+  /**\n+   * Based on the blob and options, return a blob that has a valid path\n+   * that can be saved.\n+   * @param blob\n+   * @param opts\n+   */\n+  protected async _validStoreBlob(\n+    blob: MediaBlob,\n+    opts?: BlobStoreStoreOptions\n+  ): Promise<MediaBlob | undefined> {\n+    if (await this._hasValidPath(blob, opts)) {\n+      return blob;\n+    }\n+    switch (opts?.actionIfInvalid) {\n+      case \"ignore\":\n+        return blob;\n+      case \"prefixPath\":\n+        return this._validBlobPrefixPath(blob, opts);\n+      case \"prefixUuid\":\n+        return this._validBlobPrefixUuid(blob, opts);\n+      case \"removePath\":\n+        return this._validBlobRemovePath(blob, opts);\n+      default:\n+        return undefined;\n+    }\n+  }\n+\n+  async store(\n+    blob: MediaBlob,\n+    opts: BlobStoreStoreOptions = {}\n+  ): Promise<MediaBlob | undefined> {\n+    const allOpts: BlobStoreStoreOptions = {\n+      ...this.defaultStoreOptions,\n+      ...opts,\n+    };\n+    const validBlob = await this._validStoreBlob(blob, allOpts);\n+    if (typeof validBlob !== \"undefined\") {\n+      const validKey = await validBlob.asUri();\n+      await this.mset([[validKey, validBlob]]);\n+      const savedKey = await validBlob.asUri();\n+      return await this.fetch(savedKey);\n+    }\n+    return undefined;\n+  }\n+\n+  protected async _missingFetchBlobEmpty(\n+    path: string,\n+    _opts?: BlobStoreFetchOptions\n+  ): Promise<MediaBlob> {\n+    return new MediaBlob({ path });\n+  }\n+\n+  protected async _missingFetchBlob(\n+    path: string,\n+    opts?: BlobStoreFetchOptions\n+  ): Promise<MediaBlob | undefined> {\n+    switch (opts?.actionIfBlobMissing) {\n+      case \"emptyBlob\":\n+        return this._missingFetchBlobEmpty(path, opts);\n+      default:\n+        return undefined;\n+    }\n+  }\n+\n+  async fetch(\n+    key: string | MediaBlob,\n+    opts: BlobStoreFetchOptions = {}\n+  ): Promise<MediaBlob | undefined> {\n+    const allOpts: BlobStoreFetchOptions = {\n+      ...this.defaultFetchOptions,\n+      ...opts,\n+    };\n+    const realKey = await this._realKey(key);\n+    const ret = await this.mget([realKey]);\n+    return ret?.[0] ?? (await this._missingFetchBlob(realKey, allOpts));\n+  }\n+}\n+\n+export interface BackedBlobStoreOptions extends BlobStoreOptions {\n+  backingStore: BaseStore<string, MediaBlob>;\n+}\n+\n+export class BackedBlobStore extends BlobStore {\n+  backingStore: BaseStore<string, MediaBlob>;\n+\n+  constructor(opts: BackedBlobStoreOptions) {\n+    super(opts);\n+    this.backingStore = opts.backingStore;\n+  }\n+\n+  mdelete(keys: string[]): Promise<void> {\n+    return this.backingStore.mdelete(keys);\n+  }\n+\n+  mget(keys: string[]): Promise<(MediaBlob | undefined)[]> {\n+    return this.backingStore.mget(keys);\n+  }\n+\n+  mset(keyValuePairs: [string, MediaBlob][]): Promise<void> {\n+    return this.backingStore.mset(keyValuePairs);\n+  }\n+\n+  yieldKeys(prefix: string | undefined): AsyncGenerator<string> {\n+    return this.backingStore.yieldKeys(prefix);\n+  }\n+}\n+\n+export class SimpleWebBlobStore extends BlobStore {",
        "comment_created_at": "2024-07-09T00:10:10+00:00",
        "comment_author": "afirstenberg",
        "comment_body": "A fair point.\r\n\r\nHowever, we have code that is already doing this in the Gemini implementations. (See https://github.com/langchain-ai/langchain-google/blob/7c678dcc59bb4e4d7a1efdbb9572d74b75e87665/libs/genai/langchain_google_genai/_image_utils.py#L137 for where it is done in Python, for example.)\r\n\r\nMaking this a separate class puts controls of the URL retrieval in the hands of the developer. the `SimpleWebBlobStore` is just what it says - simple. But it can serve as a base for developers to make subclasses (that do things such as authentication if necessary) that still work against a \"normal\" http/https URL.\r\n\r\nAnd we do need *something* that does the fetch for the `MediaManager` so that Gemini can work the same as OpenAI models (where you're expected to pass a public URL for images). Making it have the same signature as other `BlobStore`s makes sense for flexibility.",
        "pr_file_module": null
      }
    ]
  }
]