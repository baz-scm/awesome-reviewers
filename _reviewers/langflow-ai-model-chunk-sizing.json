[
  {
    "discussion_id": "2305422140",
    "pr_number": 9474,
    "pr_file": "docs/docs/Components/components-processing.mdx",
    "created_at": "2025-08-27T22:27:09+00:00",
    "commented_code": "|------|--------------|------|\n | data_inputs | Input | Input parameter. The data to split. Input must be in `Message`, `Data`, or `DataFrame` format. |\n | chunk_overlap | Chunk Overlap | Input parameter. The number of characters to overlap between chunks. This helps maintain context across chunks. When a separator is encountered, the overlap is applied at the point of the separator so that the subsequent chunk contains the last _n_ characters of the preceding chunk. Default: `200`. |\n-| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the the `chunk_size`. Default: `1000`. |\n+| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the the `chunk_size`. Default: `1000`. See [Tokenization errors due to chunk size](#chunk-size) for important considerations. |\n | separator | Separator | Input parameter. A string defining a character to split on, such as `\n` to split on new line characters, `\n\n` to split at paragraph breaks, or `},` to split at the end of JSON objects. You can directly provide the separator string, or pass a separator string from another component as `Message` input. |\n | text_key | Text Key | Input parameter. The key to use for the text column that is extracted from the input and then split. Default: `text`. |\n | keep_separator | Keep Separator | Input parameter. Select how to handle separators in output chunks. If False, separators are omitted from output chunks. Options include `False` (remove separators), `True` (keep separators in chunks without preference for placement), `Start` (place separators at the beginning of chunks), or `End` (place separators at the end of chunks). Default: `False`. |\n \n+### Tokenization errors due to chunk size {#chunk-size}\n+\n+When using **Split Text** with embedding models (especially NVIDIA models like `nvidia/nv-embed-v1`), you may need to use smaller chunk sizes (`500` or less) even though the model supports larger token limits.\n+The **Split Text** component doesn't always respect the exact chunk size you set, and individual chunks may exceed your specified limit.",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2305422140",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9474,
        "pr_file": "docs/docs/Components/components-processing.mdx",
        "discussion_id": "2305422140",
        "commented_code": "@@ -755,11 +755,17 @@ You can toggle parameters through the <Icon name=\"SlidersHorizontal\" aria-hidden\n |------|--------------|------|\n | data_inputs | Input | Input parameter. The data to split. Input must be in `Message`, `Data`, or `DataFrame` format. |\n | chunk_overlap | Chunk Overlap | Input parameter. The number of characters to overlap between chunks. This helps maintain context across chunks. When a separator is encountered, the overlap is applied at the point of the separator so that the subsequent chunk contains the last _n_ characters of the preceding chunk. Default: `200`. |\n-| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the the `chunk_size`. Default: `1000`. |\n+| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the the `chunk_size`. Default: `1000`. See [Tokenization errors due to chunk size](#chunk-size) for important considerations. |\n | separator | Separator | Input parameter. A string defining a character to split on, such as `\\n` to split on new line characters, `\\n\\n` to split at paragraph breaks, or `},` to split at the end of JSON objects. You can directly provide the separator string, or pass a separator string from another component as `Message` input. |\n | text_key | Text Key | Input parameter. The key to use for the text column that is extracted from the input and then split. Default: `text`. |\n | keep_separator | Keep Separator | Input parameter. Select how to handle separators in output chunks. If False, separators are omitted from output chunks. Options include `False` (remove separators), `True` (keep separators in chunks without preference for placement), `Start` (place separators at the beginning of chunks), or `End` (place separators at the end of chunks). Default: `False`. |\n \n+### Tokenization errors due to chunk size {#chunk-size}\n+\n+When using **Split Text** with embedding models (especially NVIDIA models like `nvidia/nv-embed-v1`), you may need to use smaller chunk sizes (`500` or less) even though the model supports larger token limits.\n+The **Split Text** component doesn't always respect the exact chunk size you set, and individual chunks may exceed your specified limit.",
        "comment_created_at": "2025-08-27T22:27:09+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\nThe **Split Text** component doesn't always enforce the exact chunk size you set, and individual chunks may exceed your specified limit.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2305437671",
    "pr_number": 9474,
    "pr_file": "docs/docs/Components/components-processing.mdx",
    "created_at": "2025-08-27T22:32:20+00:00",
    "commented_code": "|------|--------------|------|\n | data_inputs | Input | Input parameter. The data to split. Input must be in `Message`, `Data`, or `DataFrame` format. |\n | chunk_overlap | Chunk Overlap | Input parameter. The number of characters to overlap between chunks. This helps maintain context across chunks. When a separator is encountered, the overlap is applied at the point of the separator so that the subsequent chunk contains the last _n_ characters of the preceding chunk. Default: `200`. |\n-| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the the `chunk_size`. Default: `1000`. |\n+| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the the `chunk_size`. Default: `1000`. See [Tokenization errors due to chunk size](#chunk-size) for important considerations. |\n | separator | Separator | Input parameter. A string defining a character to split on, such as `\n` to split on new line characters, `\n\n` to split at paragraph breaks, or `},` to split at the end of JSON objects. You can directly provide the separator string, or pass a separator string from another component as `Message` input. |\n | text_key | Text Key | Input parameter. The key to use for the text column that is extracted from the input and then split. Default: `text`. |\n | keep_separator | Keep Separator | Input parameter. Select how to handle separators in output chunks. If False, separators are omitted from output chunks. Options include `False` (remove separators), `True` (keep separators in chunks without preference for placement), `Start` (place separators at the beginning of chunks), or `End` (place separators at the end of chunks). Default: `False`. |\n \n+### Tokenization errors due to chunk size {#chunk-size}\n+\n+When using **Split Text** with embedding models (especially NVIDIA models like `nvidia/nv-embed-v1`), you may need to use smaller chunk sizes (`500` or less) even though the model supports larger token limits.\n+The **Split Text** component doesn't always respect the exact chunk size you set, and individual chunks may exceed your specified limit.\n+If you encounter tokenization errors, try reducing the chunk size and test your configuration by inspecting the output.",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2305437671",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9474,
        "pr_file": "docs/docs/Components/components-processing.mdx",
        "discussion_id": "2305437671",
        "commented_code": "@@ -755,11 +755,17 @@ You can toggle parameters through the <Icon name=\"SlidersHorizontal\" aria-hidden\n |------|--------------|------|\n | data_inputs | Input | Input parameter. The data to split. Input must be in `Message`, `Data`, or `DataFrame` format. |\n | chunk_overlap | Chunk Overlap | Input parameter. The number of characters to overlap between chunks. This helps maintain context across chunks. When a separator is encountered, the overlap is applied at the point of the separator so that the subsequent chunk contains the last _n_ characters of the preceding chunk. Default: `200`. |\n-| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the the `chunk_size`. Default: `1000`. |\n+| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the the `chunk_size`. Default: `1000`. See [Tokenization errors due to chunk size](#chunk-size) for important considerations. |\n | separator | Separator | Input parameter. A string defining a character to split on, such as `\\n` to split on new line characters, `\\n\\n` to split at paragraph breaks, or `},` to split at the end of JSON objects. You can directly provide the separator string, or pass a separator string from another component as `Message` input. |\n | text_key | Text Key | Input parameter. The key to use for the text column that is extracted from the input and then split. Default: `text`. |\n | keep_separator | Keep Separator | Input parameter. Select how to handle separators in output chunks. If False, separators are omitted from output chunks. Options include `False` (remove separators), `True` (keep separators in chunks without preference for placement), `Start` (place separators at the beginning of chunks), or `End` (place separators at the end of chunks). Default: `False`. |\n \n+### Tokenization errors due to chunk size {#chunk-size}\n+\n+When using **Split Text** with embedding models (especially NVIDIA models like `nvidia/nv-embed-v1`), you may need to use smaller chunk sizes (`500` or less) even though the model supports larger token limits.\n+The **Split Text** component doesn't always respect the exact chunk size you set, and individual chunks may exceed your specified limit.\n+If you encounter tokenization errors, try reducing the chunk size and test your configuration by inspecting the output.",
        "comment_created_at": "2025-08-27T22:32:20+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\nIf you encounter tokenization errors, modify your text splitting strategy by reducing the chunk size, changing the overlap length, or using a more common separator.\r\nThen, test your configuration by running the flow and inspecting the component's output.\r\n```\r\n\r\nBased on the definition of the `chunk_size` parameter, chunk size is a minimum target.\r\nThey may need to change the other settings to achieve consistent chunk sizes. For example, if the separator is rare, the component wont subdivide the chunks, even if they well exceed the chunk size.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2305451634",
    "pr_number": 9474,
    "pr_file": "docs/docs/Components/bundles-nvidia.mdx",
    "created_at": "2025-08-27T22:36:28+00:00",
    "commented_code": "| temperature | Float | Input parameter. The model temperature for embedding generation. Default: `0.1`. |\n | embeddings | Embeddings | Output parameter. An `NVIDIAEmbeddings` instance for generating embeddings. |\n \n+:::tip Tokenization considerations\n+When using NVIDIA embedding models like `nvidia/nv-embed-v1`, you may encounter tokenization errors if your text chunks are too large. Consider using smaller chunk sizes (500 tokens or less) in your text splitting strategy.\n+For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2305451634",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9474,
        "pr_file": "docs/docs/Components/bundles-nvidia.mdx",
        "discussion_id": "2305451634",
        "commented_code": "@@ -44,6 +44,11 @@ For more information about using embedding model components in flows, see [**Emb\n | temperature | Float | Input parameter. The model temperature for embedding generation. Default: `0.1`. |\n | embeddings | Embeddings | Output parameter. An `NVIDIAEmbeddings` instance for generating embeddings. |\n \n+:::tip Tokenization considerations\n+When using NVIDIA embedding models like `nvidia/nv-embed-v1`, you may encounter tokenization errors if your text chunks are too large. Consider using smaller chunk sizes (500 tokens or less) in your text splitting strategy.\n+For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).",
        "comment_created_at": "2025-08-27T22:36:28+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\nBe aware of your embedding model's chunk size limit.\r\nTokenization errors can occur if your text chunks are too large. \r\nFor more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).\r\n```\r\n\r\nTo minimize maintenance, only put the bare minimum info here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2305472130",
    "pr_number": 9474,
    "pr_file": "docs/docs/Integrations/Nvidia/integrations-nvidia-ingest.mdx",
    "created_at": "2025-08-27T22:42:16+00:00",
    "commented_code": "| extract_infographics | Extract Infographics | Extract infographics from document. Default: false. |\n | text_depth | Text Depth | The level at which text is extracted. Options: 'document', 'page', 'block', 'line', 'span'. Default: `page`. |\n | split_text | Split Text | Split text into smaller chunks. Default: true. |\n-| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. |\n+| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. **Note:** When using NVIDIA embedding models like `nvidia/nv-embed-v1`, you may need to use smaller chunk sizes (`500` or less) to avoid tokenization errors. For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size). |",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2305472130",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9474,
        "pr_file": "docs/docs/Integrations/Nvidia/integrations-nvidia-ingest.mdx",
        "discussion_id": "2305472130",
        "commented_code": "@@ -78,7 +78,7 @@ For more information, see the [NV-Ingest documentation](https://nvidia.github.io\n | extract_infographics | Extract Infographics | Extract infographics from document. Default: false. |\n | text_depth | Text Depth | The level at which text is extracted. Options: 'document', 'page', 'block', 'line', 'span'. Default: `page`. |\n | split_text | Split Text | Split text into smaller chunks. Default: true. |\n-| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. |\n+| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. **Note:** When using NVIDIA embedding models like `nvidia/nv-embed-v1`, you may need to use smaller chunk sizes (`500` or less) to avoid tokenization errors. For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size). |",
        "comment_created_at": "2025-08-27T22:42:16+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\n| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. Make sure the chunk size is compatible with your embedding model. For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size). |\r\n```",
        "pr_file_module": null
      }
    ]
  }
]