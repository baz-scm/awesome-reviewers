[
  {
    "discussion_id": "2205971024",
    "pr_number": 9022,
    "pr_file": "docs/docs/Tutorials/mcp-tutorial.md",
    "created_at": "2025-07-14T23:12:24+00:00",
    "commented_code": "+---\n+title: Connect to MCP servers from your application\n+slug: /mcp-tutorial\n+---\n+\n+import Icon from \"@site/src/components/icon\";\n+import Tabs from '@theme/Tabs';\n+import TabItem from '@theme/TabItem';\n+\n+This tutorial shows you how to connect MCP tools to your applications using Langflow's [**MCP Tools**](/mcp-client) component.\n+\n+The [Model Context Protocol](https://modelcontextprotocol.io/), or MCP, helps agents integrate with LLMs.\n+Langflow can be run as an [MCP client](/mcp-client) and [MCP server](/mcp-server).\n+In this tutorial, you will use Langflow as a client to connect to MCP servers, and then connect a Python application to Langflow.\n+\n+## Prerequisites\n+\n+* [A running Langflow instance](/get-started-installation)\n+* [A Langflow API key](/configuration-api-keys)\n+* [An OpenAI API key](https://platform.openai.com/api-keys)\n+\n+This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n+\n+## Create an agentic flow\n+\n+1. In Langflow, click **New Flow**, and then select the [**Simple agent**](/simple-agent) template.\n+2. In the **Agent** component, enter your OpenAI API key.\n+    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n+3. To test the flow, click <Icon name=\"Play\" aria-hidden=\"true\" /> **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n+\n+    The LLM response is vague, though the Agent does know the current date by using its internal `get_current_date` function.",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2205971024",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9022,
        "pr_file": "docs/docs/Tutorials/mcp-tutorial.md",
        "discussion_id": "2205971024",
        "commented_code": "@@ -0,0 +1,238 @@\n+---\n+title: Connect to MCP servers from your application\n+slug: /mcp-tutorial\n+---\n+\n+import Icon from \"@site/src/components/icon\";\n+import Tabs from '@theme/Tabs';\n+import TabItem from '@theme/TabItem';\n+\n+This tutorial shows you how to connect MCP tools to your applications using Langflow's [**MCP Tools**](/mcp-client) component.\n+\n+The [Model Context Protocol](https://modelcontextprotocol.io/), or MCP, helps agents integrate with LLMs.\n+Langflow can be run as an [MCP client](/mcp-client) and [MCP server](/mcp-server).\n+In this tutorial, you will use Langflow as a client to connect to MCP servers, and then connect a Python application to Langflow.\n+\n+## Prerequisites\n+\n+* [A running Langflow instance](/get-started-installation)\n+* [A Langflow API key](/configuration-api-keys)\n+* [An OpenAI API key](https://platform.openai.com/api-keys)\n+\n+This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n+\n+## Create an agentic flow\n+\n+1. In Langflow, click **New Flow**, and then select the [**Simple agent**](/simple-agent) template.\n+2. In the **Agent** component, enter your OpenAI API key.\n+    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n+3. To test the flow, click <Icon name=\"Play\" aria-hidden=\"true\" /> **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n+\n+    The LLM response is vague, though the Agent does know the current date by using its internal `get_current_date` function.",
        "comment_created_at": "2025-07-14T23:12:24+00:00",
        "comment_author": "aimurphy",
        "comment_body": "To account for differences in responses or if they change the model:\r\n\r\n```suggestion\r\n    This query demonstrates how an LLM, by itself, might not have access to information or functions designed to address specialized queries. In this example, the default OpenAI model provides a vague response, although the agent does know the current date by using its internal `get_current_date` function.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2206011422",
    "pr_number": 9022,
    "pr_file": "docs/docs/Tutorials/mcp-tutorial.md",
    "created_at": "2025-07-14T23:59:48+00:00",
    "commented_code": "+---\n+title: Connect to MCP servers from your application\n+slug: /mcp-tutorial\n+---\n+\n+import Icon from \"@site/src/components/icon\";\n+import Tabs from '@theme/Tabs';\n+import TabItem from '@theme/TabItem';\n+\n+This tutorial shows you how to connect MCP tools to your applications using Langflow's [**MCP Tools**](/mcp-client) component.\n+\n+The [Model Context Protocol](https://modelcontextprotocol.io/), or MCP, helps agents integrate with LLMs.\n+Langflow can be run as an [MCP client](/mcp-client) and [MCP server](/mcp-server).\n+In this tutorial, you will use Langflow as a client to connect to MCP servers, and then connect a Python application to Langflow.\n+\n+## Prerequisites\n+\n+* [A running Langflow instance](/get-started-installation)\n+* [A Langflow API key](/configuration-api-keys)\n+* [An OpenAI API key](https://platform.openai.com/api-keys)\n+\n+This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n+\n+## Create an agentic flow\n+\n+1. In Langflow, click **New Flow**, and then select the [**Simple agent**](/simple-agent) template.\n+2. In the **Agent** component, enter your OpenAI API key.\n+    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n+3. To test the flow, click <Icon name=\"Play\" aria-hidden=\"true\" /> **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n+\n+    The LLM response is vague, though the Agent does know the current date by using its internal `get_current_date` function.\n+\n+    ```\n+    Today is July 11, 2025.\n+    To determine if it's safe to go hiking in the Adirondacks today, you should check the current weather conditions, trail advisories, and any local alerts (such as bear activity or flooding).\n+    Would you like a detailed weather forecast or information on trail conditions for the Adirondacks today?\n+    ```\n+\n+    For improved, up-to-date context for your Agent, replace the connected tools with a connection to a weather MCP server.\n+\n+## Add an MCP Tools component\n+\n+    :::tip Manage MCP Servers\n+    You can manage your MCP servers' configurations in the **Settings** menu, but you still need an **MCP Tools** component in your flow for each individual server.\n+    :::\n+\n+1. Remove the **URL** and **Calculator** tools, and then drag the [**MCP Tools**](/mcp-client) component into your workspace.\n+2. In the **MCP Tools** component, click <Icon name=\"Plus\" aria-hidden=\"true\"/> **Add MCP Server**.\n+\n+3. There are multiple ways to install MCP servers, which are covered in the [**MCP Tools**](/mcp-client) component page.\n+\n+    This example installs an [MCP weather server](https://github.com/isdaniel/mcp_weather_server) to your local machine with uv and Python.\n+\n+    Make sure you install the server in the same Python environment where Langflow is running.\n+    * If you are running Langflow in a virtual environment, activate that environment before installing the server.\n+    * If you are using Docker, install the package inside the Docker container.\n+    * If you are running Langflow system-wide, install the package globally or in the same user environment.\n+\n+    To install the server, run the following command:\n+    ```shell\n+    uv pip install mcp_weather_server\n+    ```\n+\n+4. Configure the MCP server.\n+\n+    In the **Add MCP Server** pane, select either **JSON** or **STDIO** to enter the configuration for starting your server.\n+    Both options configure an MCP server in Langflow with a command and arguments, which Langflow executes to launch the server process when needed.\n+    Both are included here to demonstrate how the STDIO commands can be filled in from the JSON configuration values.\n+\n+    <Tabs>\n+    <TabItem value=\"JSON\" label=\"JSON\" default>\n+\n+    Paste the server configuration into the **JSON** field.\n+    ```json\n+    {\n+      \"mcpServers\": {\n+        \"weather\": {\n+          \"command\": \"python\",\n+          \"args\": [\n+            \"-m\",\n+            \"mcp_weather_server\"\n+          ],\n+          \"disabled\": false,\n+          \"autoApprove\": []\n+        }\n+      }\n+    }\n+    ```\n+\n+    </TabItem>\n+    <TabItem value=\"STDIO\" label=\"STDIO\">\n+\n+    Enter the values from the configuration JSON manually into the STDIO fields.\n+    Some MCP server repositories only offer JSON files, which you can parse out into the STDIO fields.\n+\n+    - **Name:** `weather`\n+    - **Command:** `python`\n+    - **Arguments:**\n+      - `-m`\n+      - `mcp_weather_server`\n+\n+    </TabItem>\n+    </Tabs>\n+\n+5. Click **Add Server**.\n+    When the **Actions** list populates, the MCP server is ready.\n+    In the **MCP Tools** component, a field for **City** appears, but you don't need to fill in any more specific values.\n+    Connecting your MCP server to an **Agent** will define those values based on the request.\n+\n+3. In the **MCP Tools** component, enable **Tool Mode**, and then connect the **Toolset** port to the **Agent** component's **Tools** port.\n+\n+    At this point your flow has four components.\n+    The Chat Input is connected to the Agent component's input port.\n+    The MCP Tools component is connected to the Agent's Tools port.\n+    Finally, the Agent component's output port is connected to the Chat Output component, which returns the final response to the application.\n+\n+    ![An agent component connected to an MCP weather server](/img/tutorial-mcp-weather.png)\n+\n+4. Click <Icon name=\"Play\" aria-hidden=\"true\" /> **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n+\n+    The Agent's response is more useful than the previous response, because you provided context with the MCP server.\n+\n+    <details closed>\n+    <summary>Response</summary>\n+\n+    The following is an example of a response returned from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.\n+\n+    ```\n+    The current weather in Lake Placid, a central location in the Adirondacks,\n+    is foggy with a temperature of 17.2\u00b0C (about 63\u00b0F).\n+    If you plan to go hiking today, be cautious as fog can reduce visibility\n+    on trails and make navigation more difficult.\n+    ```\n+\n+    </details>\n+\n+    This is improved, but what makes adding MCP servers different from just calling a weather API?\n+\n+    The `weather` MCP server is just **one** MCP server, and it's already improved your LLM's context.\n+    You can add more servers depending on the problems you want your application to solve. The MCP protocol ensures they are all added in the same way to the Agent, without having to know how the endpoints are structured or write custom integrations.\n+\n+    In the next section, add a `ip_geolocation` MCP server so the user can discover the weather without having to fill in their location.\n+    If the user wants to know the weather elsewhere instead, the Agent understands the difference and dynamically selects the correct MCP server.\n+\n+## Add a geolocation server\n+\n+The [Toolkit MCP Server](https://github.com/cyanheads/toolkit-mcp-server) includes multiple MCP servers for network monitoring, including IP geolocation. It isn't extremely precise, but it doesn't require an API key.\n+\n+This MCP server can be started with [npx](https://docs.npmjs.com/cli/v8/commands/npx), which downloads and runs the [Node registry package](https://www.npmjs.com/package/@cyanheads/toolkit-mcp-server) with one command without installing the package locally.\n+\n+1. To add the Toolkit MCP Server to your flow, drag another MCP Tools component to your existing flow, and then click <Icon name=\"Plus\" aria-hidden=\"true\"/> **Add MCP Server**.\n+2. Click **Add Server**.\n+    In the **STDIO** pane, enter the following:\n+\n+    - **Name:** `ip_geolocation`\n+    - **Command:** `npx @cyanheads/toolkit-mcp-server`\n+\n+    When the **Actions** list populates, the server is ready.\n+\n+3. In the **MCP Tools** component, enable **Tool Mode**, and then connect the **Toolset** port to the **Agent** component's **Tools** port.\n+\n+    At this point, the flow has an additional `ip_geolocation` MCP tools component connected to the Agent.\n+\n+    ![An agent component connected to MCP weather and geolocation servers](/img/tutorial-mcp-geolocation.png)\n+\n+## Create a Python application that connects to Langflow\n+\n+At this point, you can open the Playground and ask what the weather is, but an additional feature of using MCP with Langflow is connecting your flows to your code.",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2206011422",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9022,
        "pr_file": "docs/docs/Tutorials/mcp-tutorial.md",
        "discussion_id": "2206011422",
        "commented_code": "@@ -0,0 +1,238 @@\n+---\n+title: Connect to MCP servers from your application\n+slug: /mcp-tutorial\n+---\n+\n+import Icon from \"@site/src/components/icon\";\n+import Tabs from '@theme/Tabs';\n+import TabItem from '@theme/TabItem';\n+\n+This tutorial shows you how to connect MCP tools to your applications using Langflow's [**MCP Tools**](/mcp-client) component.\n+\n+The [Model Context Protocol](https://modelcontextprotocol.io/), or MCP, helps agents integrate with LLMs.\n+Langflow can be run as an [MCP client](/mcp-client) and [MCP server](/mcp-server).\n+In this tutorial, you will use Langflow as a client to connect to MCP servers, and then connect a Python application to Langflow.\n+\n+## Prerequisites\n+\n+* [A running Langflow instance](/get-started-installation)\n+* [A Langflow API key](/configuration-api-keys)\n+* [An OpenAI API key](https://platform.openai.com/api-keys)\n+\n+This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n+\n+## Create an agentic flow\n+\n+1. In Langflow, click **New Flow**, and then select the [**Simple agent**](/simple-agent) template.\n+2. In the **Agent** component, enter your OpenAI API key.\n+    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n+3. To test the flow, click <Icon name=\"Play\" aria-hidden=\"true\" /> **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n+\n+    The LLM response is vague, though the Agent does know the current date by using its internal `get_current_date` function.\n+\n+    ```\n+    Today is July 11, 2025.\n+    To determine if it's safe to go hiking in the Adirondacks today, you should check the current weather conditions, trail advisories, and any local alerts (such as bear activity or flooding).\n+    Would you like a detailed weather forecast or information on trail conditions for the Adirondacks today?\n+    ```\n+\n+    For improved, up-to-date context for your Agent, replace the connected tools with a connection to a weather MCP server.\n+\n+## Add an MCP Tools component\n+\n+    :::tip Manage MCP Servers\n+    You can manage your MCP servers' configurations in the **Settings** menu, but you still need an **MCP Tools** component in your flow for each individual server.\n+    :::\n+\n+1. Remove the **URL** and **Calculator** tools, and then drag the [**MCP Tools**](/mcp-client) component into your workspace.\n+2. In the **MCP Tools** component, click <Icon name=\"Plus\" aria-hidden=\"true\"/> **Add MCP Server**.\n+\n+3. There are multiple ways to install MCP servers, which are covered in the [**MCP Tools**](/mcp-client) component page.\n+\n+    This example installs an [MCP weather server](https://github.com/isdaniel/mcp_weather_server) to your local machine with uv and Python.\n+\n+    Make sure you install the server in the same Python environment where Langflow is running.\n+    * If you are running Langflow in a virtual environment, activate that environment before installing the server.\n+    * If you are using Docker, install the package inside the Docker container.\n+    * If you are running Langflow system-wide, install the package globally or in the same user environment.\n+\n+    To install the server, run the following command:\n+    ```shell\n+    uv pip install mcp_weather_server\n+    ```\n+\n+4. Configure the MCP server.\n+\n+    In the **Add MCP Server** pane, select either **JSON** or **STDIO** to enter the configuration for starting your server.\n+    Both options configure an MCP server in Langflow with a command and arguments, which Langflow executes to launch the server process when needed.\n+    Both are included here to demonstrate how the STDIO commands can be filled in from the JSON configuration values.\n+\n+    <Tabs>\n+    <TabItem value=\"JSON\" label=\"JSON\" default>\n+\n+    Paste the server configuration into the **JSON** field.\n+    ```json\n+    {\n+      \"mcpServers\": {\n+        \"weather\": {\n+          \"command\": \"python\",\n+          \"args\": [\n+            \"-m\",\n+            \"mcp_weather_server\"\n+          ],\n+          \"disabled\": false,\n+          \"autoApprove\": []\n+        }\n+      }\n+    }\n+    ```\n+\n+    </TabItem>\n+    <TabItem value=\"STDIO\" label=\"STDIO\">\n+\n+    Enter the values from the configuration JSON manually into the STDIO fields.\n+    Some MCP server repositories only offer JSON files, which you can parse out into the STDIO fields.\n+\n+    - **Name:** `weather`\n+    - **Command:** `python`\n+    - **Arguments:**\n+      - `-m`\n+      - `mcp_weather_server`\n+\n+    </TabItem>\n+    </Tabs>\n+\n+5. Click **Add Server**.\n+    When the **Actions** list populates, the MCP server is ready.\n+    In the **MCP Tools** component, a field for **City** appears, but you don't need to fill in any more specific values.\n+    Connecting your MCP server to an **Agent** will define those values based on the request.\n+\n+3. In the **MCP Tools** component, enable **Tool Mode**, and then connect the **Toolset** port to the **Agent** component's **Tools** port.\n+\n+    At this point your flow has four components.\n+    The Chat Input is connected to the Agent component's input port.\n+    The MCP Tools component is connected to the Agent's Tools port.\n+    Finally, the Agent component's output port is connected to the Chat Output component, which returns the final response to the application.\n+\n+    ![An agent component connected to an MCP weather server](/img/tutorial-mcp-weather.png)\n+\n+4. Click <Icon name=\"Play\" aria-hidden=\"true\" /> **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n+\n+    The Agent's response is more useful than the previous response, because you provided context with the MCP server.\n+\n+    <details closed>\n+    <summary>Response</summary>\n+\n+    The following is an example of a response returned from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.\n+\n+    ```\n+    The current weather in Lake Placid, a central location in the Adirondacks,\n+    is foggy with a temperature of 17.2\u00b0C (about 63\u00b0F).\n+    If you plan to go hiking today, be cautious as fog can reduce visibility\n+    on trails and make navigation more difficult.\n+    ```\n+\n+    </details>\n+\n+    This is improved, but what makes adding MCP servers different from just calling a weather API?\n+\n+    The `weather` MCP server is just **one** MCP server, and it's already improved your LLM's context.\n+    You can add more servers depending on the problems you want your application to solve. The MCP protocol ensures they are all added in the same way to the Agent, without having to know how the endpoints are structured or write custom integrations.\n+\n+    In the next section, add a `ip_geolocation` MCP server so the user can discover the weather without having to fill in their location.\n+    If the user wants to know the weather elsewhere instead, the Agent understands the difference and dynamically selects the correct MCP server.\n+\n+## Add a geolocation server\n+\n+The [Toolkit MCP Server](https://github.com/cyanheads/toolkit-mcp-server) includes multiple MCP servers for network monitoring, including IP geolocation. It isn't extremely precise, but it doesn't require an API key.\n+\n+This MCP server can be started with [npx](https://docs.npmjs.com/cli/v8/commands/npx), which downloads and runs the [Node registry package](https://www.npmjs.com/package/@cyanheads/toolkit-mcp-server) with one command without installing the package locally.\n+\n+1. To add the Toolkit MCP Server to your flow, drag another MCP Tools component to your existing flow, and then click <Icon name=\"Plus\" aria-hidden=\"true\"/> **Add MCP Server**.\n+2. Click **Add Server**.\n+    In the **STDIO** pane, enter the following:\n+\n+    - **Name:** `ip_geolocation`\n+    - **Command:** `npx @cyanheads/toolkit-mcp-server`\n+\n+    When the **Actions** list populates, the server is ready.\n+\n+3. In the **MCP Tools** component, enable **Tool Mode**, and then connect the **Toolset** port to the **Agent** component's **Tools** port.\n+\n+    At this point, the flow has an additional `ip_geolocation` MCP tools component connected to the Agent.\n+\n+    ![An agent component connected to MCP weather and geolocation servers](/img/tutorial-mcp-geolocation.png)\n+\n+## Create a Python application that connects to Langflow\n+\n+At this point, you can open the Playground and ask what the weather is, but an additional feature of using MCP with Langflow is connecting your flows to your code.",
        "comment_created_at": "2025-07-14T23:59:48+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\nAt this point, you can open the **Playground** and ask about the weather in your current location to test the IP geolocation tool.\r\nHowever, the IP geolocation MCP server is most useful in an application where you or your users want to ask about the weather from different places around the world.\r\n\r\nIn the last part of this tutorial, you'll learn how to use the Langflow API to run a flow in a script.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2198104488",
    "pr_number": 8961,
    "pr_file": "docs/docs/Tutorials/agent.md",
    "created_at": "2025-07-10T15:43:52+00:00",
    "commented_code": "+---\n+title: Connect applications to agents\n+slug: /agent-tutorial\n+---\n+\n+import Icon from \"@site/src/components/icon\";\n+import Tabs from '@theme/Tabs';\n+import TabItem from '@theme/TabItem';\n+\n+This tutorial shows you how to connect a JavaScript application to a Langflow [agent](/agents).\n+\n+With the agent connected, your application can use any connected tools to retrieve more contextual and timely data without changing any application code. The tools are selected by the agent's internal LLM to solve problems and answer questions.\n+\n+## Prerequisites\n+\n+- [A running Langflow instance](/get-started-installation)\n+- [A Langflow API key](/configuration-api-keys)\n+- [An OpenAI API key](https://platform.openai.com/api-keys)\n+- [Langflow JavaScript client installed](/typescript-client)\n+\n+This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n+\n+## Create an agentic flow\n+\n+The following steps modify the [**Simple agent**](/simple-agent) template to connect [**Directory**](/components-data#directory) and [**Web search**](/components-data#web-search) components as tools for the **Agent**.\n+The Directory component loads all files of a given type from a target directory on your local machine, and the Web search component performs a DuckDuckGo search.\n+\n+1. In Langflow, click **New Flow**, and then select the **Simple agent** template.\n+2. Remove the **URL** and **Calculator** tools, and drag the [**Directory**](/components-data#directory) and [**Web search**](/components-data#web-search) components to your workspace.\n+3. In the **Directory** component's **Path** field, enter the path to your directory, and the types of files you want to provide to your agent.\n+In this example, the directory name is `customer_orders` and the file type is `.csv`, because you want the agent to have access to a record of customer purchases.\n+If you don't have a `.csv` file available, you can download [customer-orders.csv](/files/customer_orders/customer_orders.csv) and save it in a folder on your local machine called `customer_orders`.\n+This example assumes you're searching `customer_orders` for an `email` value, but you can adapt the tutorial to suit your data.\n+4. In the **Directory** and **Web search** components, enable **Tool Mode**, and connect the **Toolset** port to the agent's **Tools** port.\n+This mode registers the connected tools with your Agent, so it understands what they do and how to use them.\n+5. In the **Agent** component, enter your OpenAI API key.\n+\n+    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n+\n+6. To verify that your flow is operational, click <Icon name=\"Play\" aria-hidden=\"true\" /> **Playground**, and then ask the LLM a question, such as `Recommend 3 used items for carol.davis@example.com, based on previous orders.`\n+The LLM should respond with recommendations and web links for items based on previous orders in `customer_orders.csv`.\n+The Playground displays the agent's chain of thought as it uses the Directory component's `as_dataframe` tool to retrieve a [DataFrame](/concepts-objects#dataframe-object), and the Web search component's `perform_search` tool to find links to related items.",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2198104488",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 8961,
        "pr_file": "docs/docs/Tutorials/agent.md",
        "discussion_id": "2198104488",
        "commented_code": "@@ -0,0 +1,175 @@\n+---\n+title: Connect applications to agents\n+slug: /agent-tutorial\n+---\n+\n+import Icon from \"@site/src/components/icon\";\n+import Tabs from '@theme/Tabs';\n+import TabItem from '@theme/TabItem';\n+\n+This tutorial shows you how to connect a JavaScript application to a Langflow [agent](/agents).\n+\n+With the agent connected, your application can use any connected tools to retrieve more contextual and timely data without changing any application code. The tools are selected by the agent's internal LLM to solve problems and answer questions.\n+\n+## Prerequisites\n+\n+- [A running Langflow instance](/get-started-installation)\n+- [A Langflow API key](/configuration-api-keys)\n+- [An OpenAI API key](https://platform.openai.com/api-keys)\n+- [Langflow JavaScript client installed](/typescript-client)\n+\n+This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n+\n+## Create an agentic flow\n+\n+The following steps modify the [**Simple agent**](/simple-agent) template to connect [**Directory**](/components-data#directory) and [**Web search**](/components-data#web-search) components as tools for the **Agent**.\n+The Directory component loads all files of a given type from a target directory on your local machine, and the Web search component performs a DuckDuckGo search.\n+\n+1. In Langflow, click **New Flow**, and then select the **Simple agent** template.\n+2. Remove the **URL** and **Calculator** tools, and drag the [**Directory**](/components-data#directory) and [**Web search**](/components-data#web-search) components to your workspace.\n+3. In the **Directory** component's **Path** field, enter the path to your directory, and the types of files you want to provide to your agent.\n+In this example, the directory name is `customer_orders` and the file type is `.csv`, because you want the agent to have access to a record of customer purchases.\n+If you don't have a `.csv` file available, you can download [customer-orders.csv](/files/customer_orders/customer_orders.csv) and save it in a folder on your local machine called `customer_orders`.\n+This example assumes you're searching `customer_orders` for an `email` value, but you can adapt the tutorial to suit your data.\n+4. In the **Directory** and **Web search** components, enable **Tool Mode**, and connect the **Toolset** port to the agent's **Tools** port.\n+This mode registers the connected tools with your Agent, so it understands what they do and how to use them.\n+5. In the **Agent** component, enter your OpenAI API key.\n+\n+    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n+\n+6. To verify that your flow is operational, click <Icon name=\"Play\" aria-hidden=\"true\" /> **Playground**, and then ask the LLM a question, such as `Recommend 3 used items for carol.davis@example.com, based on previous orders.`\n+The LLM should respond with recommendations and web links for items based on previous orders in `customer_orders.csv`.\n+The Playground displays the agent's chain of thought as it uses the Directory component's `as_dataframe` tool to retrieve a [DataFrame](/concepts-objects#dataframe-object), and the Web search component's `perform_search` tool to find links to related items.",
        "comment_created_at": "2025-07-10T15:43:52+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\n\r\n    Given the example prompt, the LLM would respond with recommendations and web links for items based on previous orders in `customer_orders.csv`.\r\n    \r\n    The **Playground** prints the agent's chain of thought as it selects tools to use and interacts with functionality provided by those tools.\r\n    For example, the agent can use the **Directory** component's `as_dataframe` tool to retrieve a [DataFrame](/concepts-objects#dataframe-object), and the **Web search** component's `perform_search` tool to find links to related items.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2180470847",
    "pr_number": 8387,
    "pr_file": "docs/docs/Components/components-embedding-models.md",
    "created_at": "2025-07-02T16:14:32+00:00",
    "commented_code": "---\n-title: Embeddings\n+title: Embedding models\n slug: /components-embedding-models\n ---\n \n import Icon from \"@site/src/components/icon\";\n \n-# Embeddings models in Langflow\n-\n-Embeddings models convert text into numerical vectors. These embeddings capture the semantic meaning of the input text, and allow LLMs to understand context.\n-\n-Refer to your specific component's documentation for more information on parameters.\n-\n-## Use an embeddings model component in a flow\n-\n-In this example of a document ingestion pipeline, the **OpenAI** embeddings model is connected to a vector database. The component converts the text chunks into vectors and stores them in the vector database. The vectorized data can be used to inform AI workloads like chatbots, similarity searches, and agents.\n-\n-This embeddings component uses an OpenAI API key for authentication. Refer to your specific embeddings component's documentation for more information on authentication.\n-\n-![URL component in a data ingestion pipeline](/img/url-component.png)\n-\n-## AI/ML\n-\n-This component generates embeddings using the [AI/ML API](https://docs.aimlapi.com/api-overview/embeddings).\n-\n-<details>\n-<summary>Parameters</summary>\n-\n-**Inputs**\n-\n-| Name | Type | Description |\n-|------|------|-------------|\n-| model_name | String | The name of the AI/ML embedding model to use. |\n-| aiml_api_key | SecretString | The API key required for authenticating with the AI/ML service. |\n-\n-**Outputs**\n-\n-| Name | Type | Description |\n-|------|------|-------------|\n-| embeddings | Embeddings | An instance of `AIMLEmbeddingsImpl` for generating embeddings. |\n-\n-</details>\n-\n-## Amazon Bedrock Embeddings\n-\n-This component is used to load embedding models from [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n-\n-<details>\n-<summary>Parameters</summary>\n-\n-**Inputs**\n-\n-| Name | Type | Description |\n-|------|------|-------------|\n-| credentials_profile_name | String | The name of the AWS credentials profile in `~/.aws/credentials` or `~/.aws/config`, which has access keys or role information. |\n-| model_id | String | The ID of the model to call, such as `amazon.titan-embed-text-v1`. This is equivalent to the `modelId` property in the `list-foundation-models` API. |\n-| endpoint_url | String | The URL to set a specific service endpoint other than the default AWS endpoint. |\n-| region_name | String | The AWS region to use, such as `us-west-2`. Falls back to the `AWS_DEFAULT_REGION` environment variable or region specified in `~/.aws/config` if not provided. |\n-\n-**Outputs**\n-\n-| Name | Type | Description |\n-|------|------|-------------|\n-| embeddings | Embeddings | An instance for generating embeddings using Amazon Bedrock. |\n-\n-</details>\n-\n-## Astra DB vectorize\n-\n :::important\n-This component is deprecated as of Langflow version 1.1.2.\n-Instead, use the [Astra DB vector store component](/components-vector-stores#astra-db-vector-store).\n+In [Langflow version 1.5](/release-notes), the singular **Embedding model** component replaces many provider-specific embedding models components. Any provider-specific embedding model components that weren't incorporated into the singular component were moved to [**Bundles**](/components-bundle-components).\n :::\n \n-Connect this component to the **Embeddings** port of the [Astra DB vector store component](/components-vector-stores#astra-db-vector-store) to generate embeddings.\n+Embedding model components in Langflow generate text embeddings using the selected Large Language Model (LLM). The core **Embedding model** component supports many LLM providers, models, and use cases. For additional providers and models not supported by the core **Embedding model** component, see [**Bundles**](/components-bundle-components).\n \n-This component requires that your Astra DB database has a collection that uses a vectorize embedding provider integration.\n-For more information and instructions, see [Embedding Generation](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html).\n+Most use cases can be performed with the **Language Model** and **Embedding Model** components.\n \n-<details>\n-<summary>Parameters</summary>\n+If you want to try additional providers not supported by the new components, the single-provider LLM components of both the **Language Model** and **Embedding Model** types are now found in **Bundles**, and are still available for use.\n \n-**Inputs**\n+## Use an Embedding Model component in a flow\n \n-| Name | Display Name | Info |\n-|------|--------------|------|\n-| provider | Embedding Provider | The embedding provider to use. |\n-| model_name | Model Name | The embedding model to use. |\n-| authentication | Authentication | The name of the API key in Astra that stores your [vectorize embedding provider credentials](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html#embedding-provider-authentication). (Not required if using an [Astra-hosted embedding provider](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html#supported-embedding-providers).) |\n-| provider_api_key | Provider API Key | As an alternative to `authentication`, directly provide your embedding provider credentials. |\n-| model_parameters | Model Parameters | Additional model parameters. |\n+Create a semantic search system with the **Embedding model** component.\n \n-**Outputs**\n+1. Add the **Embedding model** component to your flow.\n+   The default model is OpenAI's `text-embedding-3-small`. Based on [OpenAI's recommendations](https://platform.openai.com/docs/guides/embeddings#embedding-models), this model is a good balance of performance and cost.",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2180470847",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 8387,
        "pr_file": "docs/docs/Components/components-embedding-models.md",
        "discussion_id": "2180470847",
        "commented_code": "@@ -1,546 +1,76 @@\n ---\n-title: Embeddings\n+title: Embedding models\n slug: /components-embedding-models\n ---\n \n import Icon from \"@site/src/components/icon\";\n \n-# Embeddings models in Langflow\n-\n-Embeddings models convert text into numerical vectors. These embeddings capture the semantic meaning of the input text, and allow LLMs to understand context.\n-\n-Refer to your specific component's documentation for more information on parameters.\n-\n-## Use an embeddings model component in a flow\n-\n-In this example of a document ingestion pipeline, the **OpenAI** embeddings model is connected to a vector database. The component converts the text chunks into vectors and stores them in the vector database. The vectorized data can be used to inform AI workloads like chatbots, similarity searches, and agents.\n-\n-This embeddings component uses an OpenAI API key for authentication. Refer to your specific embeddings component's documentation for more information on authentication.\n-\n-![URL component in a data ingestion pipeline](/img/url-component.png)\n-\n-## AI/ML\n-\n-This component generates embeddings using the [AI/ML API](https://docs.aimlapi.com/api-overview/embeddings).\n-\n-<details>\n-<summary>Parameters</summary>\n-\n-**Inputs**\n-\n-| Name | Type | Description |\n-|------|------|-------------|\n-| model_name | String | The name of the AI/ML embedding model to use. |\n-| aiml_api_key | SecretString | The API key required for authenticating with the AI/ML service. |\n-\n-**Outputs**\n-\n-| Name | Type | Description |\n-|------|------|-------------|\n-| embeddings | Embeddings | An instance of `AIMLEmbeddingsImpl` for generating embeddings. |\n-\n-</details>\n-\n-## Amazon Bedrock Embeddings\n-\n-This component is used to load embedding models from [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n-\n-<details>\n-<summary>Parameters</summary>\n-\n-**Inputs**\n-\n-| Name | Type | Description |\n-|------|------|-------------|\n-| credentials_profile_name | String | The name of the AWS credentials profile in `~/.aws/credentials` or `~/.aws/config`, which has access keys or role information. |\n-| model_id | String | The ID of the model to call, such as `amazon.titan-embed-text-v1`. This is equivalent to the `modelId` property in the `list-foundation-models` API. |\n-| endpoint_url | String | The URL to set a specific service endpoint other than the default AWS endpoint. |\n-| region_name | String | The AWS region to use, such as `us-west-2`. Falls back to the `AWS_DEFAULT_REGION` environment variable or region specified in `~/.aws/config` if not provided. |\n-\n-**Outputs**\n-\n-| Name | Type | Description |\n-|------|------|-------------|\n-| embeddings | Embeddings | An instance for generating embeddings using Amazon Bedrock. |\n-\n-</details>\n-\n-## Astra DB vectorize\n-\n :::important\n-This component is deprecated as of Langflow version 1.1.2.\n-Instead, use the [Astra DB vector store component](/components-vector-stores#astra-db-vector-store).\n+In [Langflow version 1.5](/release-notes), the singular **Embedding model** component replaces many provider-specific embedding models components. Any provider-specific embedding model components that weren't incorporated into the singular component were moved to [**Bundles**](/components-bundle-components).\n :::\n \n-Connect this component to the **Embeddings** port of the [Astra DB vector store component](/components-vector-stores#astra-db-vector-store) to generate embeddings.\n+Embedding model components in Langflow generate text embeddings using the selected Large Language Model (LLM). The core **Embedding model** component supports many LLM providers, models, and use cases. For additional providers and models not supported by the core **Embedding model** component, see [**Bundles**](/components-bundle-components).\n \n-This component requires that your Astra DB database has a collection that uses a vectorize embedding provider integration.\n-For more information and instructions, see [Embedding Generation](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html).\n+Most use cases can be performed with the **Language Model** and **Embedding Model** components.\n \n-<details>\n-<summary>Parameters</summary>\n+If you want to try additional providers not supported by the new components, the single-provider LLM components of both the **Language Model** and **Embedding Model** types are now found in **Bundles**, and are still available for use.\n \n-**Inputs**\n+## Use an Embedding Model component in a flow\n \n-| Name | Display Name | Info |\n-|------|--------------|------|\n-| provider | Embedding Provider | The embedding provider to use. |\n-| model_name | Model Name | The embedding model to use. |\n-| authentication | Authentication | The name of the API key in Astra that stores your [vectorize embedding provider credentials](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html#embedding-provider-authentication). (Not required if using an [Astra-hosted embedding provider](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html#supported-embedding-providers).) |\n-| provider_api_key | Provider API Key | As an alternative to `authentication`, directly provide your embedding provider credentials. |\n-| model_parameters | Model Parameters | Additional model parameters. |\n+Create a semantic search system with the **Embedding model** component.\n \n-**Outputs**\n+1. Add the **Embedding model** component to your flow.\n+   The default model is OpenAI's `text-embedding-3-small`. Based on [OpenAI's recommendations](https://platform.openai.com/docs/guides/embeddings#embedding-models), this model is a good balance of performance and cost.",
        "comment_created_at": "2025-07-02T16:14:32+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\n   The default model is OpenAI's `text-embedding-3-small`, which is a balanced model, based on [OpenAI's recommendations](https://platform.openai.com/docs/guides/embeddings#embedding-models).\r\n```\r\n\r\nFuture proofing by avoiding references to cost and performance. Generally describing it as \"balanced\" is sufficient.",
        "pr_file_module": null
      }
    ]
  }
]