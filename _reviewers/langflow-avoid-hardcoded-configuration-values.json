[
  {
    "discussion_id": "2291890557",
    "pr_number": 9460,
    "pr_file": "src/backend/base/langflow/components/vlmrun/vlmrun_transcription.py",
    "created_at": "2025-08-21T18:56:39+00:00",
    "commented_code": "+from pathlib import Path\n+\n+from loguru import logger\n+\n+from langflow.custom.custom_component.component import Component\n+from langflow.io import (\n+    DropdownInput,\n+    FileInput,\n+    MessageTextInput,\n+    Output,\n+    SecretStrInput,\n+)\n+from langflow.schema.data import Data\n+\n+\n+class VLMRunTranscription(Component):\n+    display_name = \"VLM Run Transcription\"\n+    description = \"Extract structured data from audio and video using [VLM Run AI](https://app.vlm.run)\"\n+    documentation = \"https://docs.vlm.run\"\n+    icon = \"VLMRun\"\n+    beta = True\n+\n+    inputs = [\n+        SecretStrInput(\n+            name=\"api_key\",\n+            display_name=\"VLM Run API Key\",\n+            info=\"Get your API key from https://app.vlm.run\",\n+            required=True,\n+        ),\n+        DropdownInput(\n+            name=\"media_type\",\n+            display_name=\"Media Type\",\n+            options=[\"audio\", \"video\"],\n+            value=\"audio\",\n+            info=\"Select the type of media to process\",\n+        ),\n+        FileInput(\n+            name=\"media_files\",\n+            display_name=\"Media Files\",\n+            file_types=[\n+                \"mp3\",\n+                \"wav\",\n+                \"m4a\",\n+                \"flac\",\n+                \"ogg\",\n+                \"opus\",\n+                \"webm\",\n+                \"aac\",\n+                \"mp4\",\n+                \"mov\",\n+                \"avi\",\n+                \"mkv\",\n+                \"flv\",\n+                \"wmv\",\n+                \"m4v\",\n+            ],\n+            info=\"Upload one or more audio/video files\",\n+            required=False,\n+            is_list=True,\n+        ),\n+        MessageTextInput(\n+            name=\"media_url\",\n+            display_name=\"Media URL\",\n+            info=\"URL to media file (alternative to file upload)\",\n+            required=False,\n+            advanced=True,\n+        ),\n+    ]\n+\n+    outputs = [\n+        Output(\n+            display_name=\"Result\",\n+            name=\"result\",\n+            method=\"process_media\",\n+        ),\n+    ]\n+\n+    def _check_inputs(self) -> str | None:\n+        \"\"\"Validate that either media files or URL is provided.\"\"\"\n+        if not self.media_files and not self.media_url:\n+            return \"Either media files or media URL must be provided\"\n+        return None\n+\n+    def _import_vlmrun(self):\n+        \"\"\"Import and return VLMRun client class.\"\"\"\n+        try:\n+            from vlmrun.client import VLMRun\n+        except ImportError as e:\n+            error_msg = \"VLM Run SDK not installed. Run: pip install 'vlmrun[all]'\"\n+            raise ImportError(error_msg) from e\n+        else:\n+            return VLMRun\n+\n+    def _generate_media_response(self, client, media_source):\n+        \"\"\"Generate response for audio or video media.\"\"\"\n+        if self.media_type == \"audio\":\n+            if isinstance(media_source, Path):\n+                return client.audio.generate(file=media_source, domain=\"audio.transcription\", batch=True)\n+            return client.audio.generate(url=media_source, domain=\"audio.transcription\", batch=True)\n+        # video\n+        if isinstance(media_source, Path):\n+            return client.video.generate(file=media_source, domain=\"video.transcription\", batch=True)\n+        return client.video.generate(url=media_source, domain=\"video.transcription\", batch=True)\n+\n+    def _wait_for_response(self, client, response):\n+        \"\"\"Wait for batch processing to complete if needed.\"\"\"\n+        if hasattr(response, \"id\"):\n+            return client.predictions.wait(response.id, timeout=600)",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2291890557",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9460,
        "pr_file": "src/backend/base/langflow/components/vlmrun/vlmrun_transcription.py",
        "discussion_id": "2291890557",
        "commented_code": "@@ -0,0 +1,215 @@\n+from pathlib import Path\n+\n+from loguru import logger\n+\n+from langflow.custom.custom_component.component import Component\n+from langflow.io import (\n+    DropdownInput,\n+    FileInput,\n+    MessageTextInput,\n+    Output,\n+    SecretStrInput,\n+)\n+from langflow.schema.data import Data\n+\n+\n+class VLMRunTranscription(Component):\n+    display_name = \"VLM Run Transcription\"\n+    description = \"Extract structured data from audio and video using [VLM Run AI](https://app.vlm.run)\"\n+    documentation = \"https://docs.vlm.run\"\n+    icon = \"VLMRun\"\n+    beta = True\n+\n+    inputs = [\n+        SecretStrInput(\n+            name=\"api_key\",\n+            display_name=\"VLM Run API Key\",\n+            info=\"Get your API key from https://app.vlm.run\",\n+            required=True,\n+        ),\n+        DropdownInput(\n+            name=\"media_type\",\n+            display_name=\"Media Type\",\n+            options=[\"audio\", \"video\"],\n+            value=\"audio\",\n+            info=\"Select the type of media to process\",\n+        ),\n+        FileInput(\n+            name=\"media_files\",\n+            display_name=\"Media Files\",\n+            file_types=[\n+                \"mp3\",\n+                \"wav\",\n+                \"m4a\",\n+                \"flac\",\n+                \"ogg\",\n+                \"opus\",\n+                \"webm\",\n+                \"aac\",\n+                \"mp4\",\n+                \"mov\",\n+                \"avi\",\n+                \"mkv\",\n+                \"flv\",\n+                \"wmv\",\n+                \"m4v\",\n+            ],\n+            info=\"Upload one or more audio/video files\",\n+            required=False,\n+            is_list=True,\n+        ),\n+        MessageTextInput(\n+            name=\"media_url\",\n+            display_name=\"Media URL\",\n+            info=\"URL to media file (alternative to file upload)\",\n+            required=False,\n+            advanced=True,\n+        ),\n+    ]\n+\n+    outputs = [\n+        Output(\n+            display_name=\"Result\",\n+            name=\"result\",\n+            method=\"process_media\",\n+        ),\n+    ]\n+\n+    def _check_inputs(self) -> str | None:\n+        \"\"\"Validate that either media files or URL is provided.\"\"\"\n+        if not self.media_files and not self.media_url:\n+            return \"Either media files or media URL must be provided\"\n+        return None\n+\n+    def _import_vlmrun(self):\n+        \"\"\"Import and return VLMRun client class.\"\"\"\n+        try:\n+            from vlmrun.client import VLMRun\n+        except ImportError as e:\n+            error_msg = \"VLM Run SDK not installed. Run: pip install 'vlmrun[all]'\"\n+            raise ImportError(error_msg) from e\n+        else:\n+            return VLMRun\n+\n+    def _generate_media_response(self, client, media_source):\n+        \"\"\"Generate response for audio or video media.\"\"\"\n+        if self.media_type == \"audio\":\n+            if isinstance(media_source, Path):\n+                return client.audio.generate(file=media_source, domain=\"audio.transcription\", batch=True)\n+            return client.audio.generate(url=media_source, domain=\"audio.transcription\", batch=True)\n+        # video\n+        if isinstance(media_source, Path):\n+            return client.video.generate(file=media_source, domain=\"video.transcription\", batch=True)\n+        return client.video.generate(url=media_source, domain=\"video.transcription\", batch=True)\n+\n+    def _wait_for_response(self, client, response):\n+        \"\"\"Wait for batch processing to complete if needed.\"\"\"\n+        if hasattr(response, \"id\"):\n+            return client.predictions.wait(response.id, timeout=600)",
        "comment_created_at": "2025-08-21T18:56:39+00:00",
        "comment_author": "dineshreddy91",
        "comment_body": "Can we expose timeout for client.predictions.wait(..., timeout=600) as an advanced input (e.g., timeout_seconds, default 600)? This avoids hard-coding and helps long videos.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2291894765",
    "pr_number": 9460,
    "pr_file": "src/backend/base/langflow/components/vlmrun/vlmrun_transcription.py",
    "created_at": "2025-08-21T18:58:46+00:00",
    "commented_code": "+from pathlib import Path\n+\n+from loguru import logger\n+\n+from langflow.custom.custom_component.component import Component\n+from langflow.io import (\n+    DropdownInput,\n+    FileInput,\n+    MessageTextInput,\n+    Output,\n+    SecretStrInput,\n+)\n+from langflow.schema.data import Data\n+\n+\n+class VLMRunTranscription(Component):\n+    display_name = \"VLM Run Transcription\"\n+    description = \"Extract structured data from audio and video using [VLM Run AI](https://app.vlm.run)\"\n+    documentation = \"https://docs.vlm.run\"\n+    icon = \"VLMRun\"\n+    beta = True\n+\n+    inputs = [\n+        SecretStrInput(\n+            name=\"api_key\",\n+            display_name=\"VLM Run API Key\",\n+            info=\"Get your API key from https://app.vlm.run\",\n+            required=True,\n+        ),\n+        DropdownInput(\n+            name=\"media_type\",\n+            display_name=\"Media Type\",\n+            options=[\"audio\", \"video\"],\n+            value=\"audio\",\n+            info=\"Select the type of media to process\",\n+        ),\n+        FileInput(\n+            name=\"media_files\",\n+            display_name=\"Media Files\",\n+            file_types=[\n+                \"mp3\",\n+                \"wav\",\n+                \"m4a\",\n+                \"flac\",\n+                \"ogg\",\n+                \"opus\",\n+                \"webm\",\n+                \"aac\",\n+                \"mp4\",\n+                \"mov\",\n+                \"avi\",\n+                \"mkv\",\n+                \"flv\",\n+                \"wmv\",\n+                \"m4v\",\n+            ],\n+            info=\"Upload one or more audio/video files\",\n+            required=False,\n+            is_list=True,\n+        ),\n+        MessageTextInput(\n+            name=\"media_url\",\n+            display_name=\"Media URL\",\n+            info=\"URL to media file (alternative to file upload)\",\n+            required=False,\n+            advanced=True,\n+        ),\n+    ]\n+\n+    outputs = [\n+        Output(\n+            display_name=\"Result\",\n+            name=\"result\",\n+            method=\"process_media\",\n+        ),\n+    ]\n+\n+    def _check_inputs(self) -> str | None:\n+        \"\"\"Validate that either media files or URL is provided.\"\"\"\n+        if not self.media_files and not self.media_url:\n+            return \"Either media files or media URL must be provided\"\n+        return None\n+\n+    def _import_vlmrun(self):\n+        \"\"\"Import and return VLMRun client class.\"\"\"\n+        try:\n+            from vlmrun.client import VLMRun\n+        except ImportError as e:\n+            error_msg = \"VLM Run SDK not installed. Run: pip install 'vlmrun[all]'\"\n+            raise ImportError(error_msg) from e\n+        else:\n+            return VLMRun\n+\n+    def _generate_media_response(self, client, media_source):\n+        \"\"\"Generate response for audio or video media.\"\"\"\n+        if self.media_type == \"audio\":\n+            if isinstance(media_source, Path):",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2291894765",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9460,
        "pr_file": "src/backend/base/langflow/components/vlmrun/vlmrun_transcription.py",
        "discussion_id": "2291894765",
        "commented_code": "@@ -0,0 +1,215 @@\n+from pathlib import Path\n+\n+from loguru import logger\n+\n+from langflow.custom.custom_component.component import Component\n+from langflow.io import (\n+    DropdownInput,\n+    FileInput,\n+    MessageTextInput,\n+    Output,\n+    SecretStrInput,\n+)\n+from langflow.schema.data import Data\n+\n+\n+class VLMRunTranscription(Component):\n+    display_name = \"VLM Run Transcription\"\n+    description = \"Extract structured data from audio and video using [VLM Run AI](https://app.vlm.run)\"\n+    documentation = \"https://docs.vlm.run\"\n+    icon = \"VLMRun\"\n+    beta = True\n+\n+    inputs = [\n+        SecretStrInput(\n+            name=\"api_key\",\n+            display_name=\"VLM Run API Key\",\n+            info=\"Get your API key from https://app.vlm.run\",\n+            required=True,\n+        ),\n+        DropdownInput(\n+            name=\"media_type\",\n+            display_name=\"Media Type\",\n+            options=[\"audio\", \"video\"],\n+            value=\"audio\",\n+            info=\"Select the type of media to process\",\n+        ),\n+        FileInput(\n+            name=\"media_files\",\n+            display_name=\"Media Files\",\n+            file_types=[\n+                \"mp3\",\n+                \"wav\",\n+                \"m4a\",\n+                \"flac\",\n+                \"ogg\",\n+                \"opus\",\n+                \"webm\",\n+                \"aac\",\n+                \"mp4\",\n+                \"mov\",\n+                \"avi\",\n+                \"mkv\",\n+                \"flv\",\n+                \"wmv\",\n+                \"m4v\",\n+            ],\n+            info=\"Upload one or more audio/video files\",\n+            required=False,\n+            is_list=True,\n+        ),\n+        MessageTextInput(\n+            name=\"media_url\",\n+            display_name=\"Media URL\",\n+            info=\"URL to media file (alternative to file upload)\",\n+            required=False,\n+            advanced=True,\n+        ),\n+    ]\n+\n+    outputs = [\n+        Output(\n+            display_name=\"Result\",\n+            name=\"result\",\n+            method=\"process_media\",\n+        ),\n+    ]\n+\n+    def _check_inputs(self) -> str | None:\n+        \"\"\"Validate that either media files or URL is provided.\"\"\"\n+        if not self.media_files and not self.media_url:\n+            return \"Either media files or media URL must be provided\"\n+        return None\n+\n+    def _import_vlmrun(self):\n+        \"\"\"Import and return VLMRun client class.\"\"\"\n+        try:\n+            from vlmrun.client import VLMRun\n+        except ImportError as e:\n+            error_msg = \"VLM Run SDK not installed. Run: pip install 'vlmrun[all]'\"\n+            raise ImportError(error_msg) from e\n+        else:\n+            return VLMRun\n+\n+    def _generate_media_response(self, client, media_source):\n+        \"\"\"Generate response for audio or video media.\"\"\"\n+        if self.media_type == \"audio\":\n+            if isinstance(media_source, Path):",
        "comment_created_at": "2025-08-21T18:58:46+00:00",
        "comment_author": "dineshreddy91",
        "comment_body": "Make domain configurable (\"audio.transcription\" / \"video.transcription\") as an advanced input, in case VLM Run adds new domains or variants (e.g., diarization).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2282559668",
    "pr_number": 9317,
    "pr_file": "src/backend/base/langflow/services/tracing/traceloop.py",
    "created_at": "2025-08-18T14:21:49+00:00",
    "commented_code": "+from __future__ import annotations\n+\n+import os\n+from typing import TYPE_CHECKING, Any\n+from urllib.parse import urlparse\n+\n+from loguru import logger\n+from opentelemetry import trace\n+from traceloop.sdk import Traceloop\n+from traceloop.sdk.instruments import Instruments\n+from typing_extensions import override\n+\n+from langflow.services.tracing.base import BaseTracer\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+    from uuid import UUID\n+\n+    from langchain.callbacks.base import BaseCallbackHandler\n+\n+    from langflow.graph.vertex.base import Vertex\n+    from langflow.services.tracing.schema import Log\n+\n+\n+class TraceloopTracer(BaseTracer):\n+    \"\"\"Traceloop tracer for Langflow.\"\"\"\n+\n+    def __init__(\n+        self,\n+        trace_name: str,\n+        trace_type: str,\n+        project_name: str,\n+        trace_id: UUID,\n+        user_id: str | None = None,\n+        session_id: str | None = None,\n+    ):\n+        self.trace_id = trace_id\n+        self.trace_name = trace_name\n+        self.trace_type = trace_type\n+        self.project_name = project_name\n+        self.user_id = user_id\n+        self.session_id = session_id\n+        self._span_map: dict[str, trace.Span] = {}  # store spans by trace_name\n+\n+        if not self._validate_configuration():\n+            self._ready = False\n+            return\n+\n+        api_key = os.getenv(\"TRACELOOP_API_KEY\", \"\").strip()\n+        try:\n+            Traceloop.init(\n+                instruments={Instruments.LANGCHAIN},\n+                app_name=project_name,\n+                disable_batch=True,\n+                api_key=api_key,\n+                api_endpoint=os.getenv(\"TRACELOOP_BASE_URL\", \"https://api.traceloop.com\"),\n+            )\n+            self._ready = True\n+            self._tracer = trace.get_tracer(\"langflow\")\n+            logger.info(\"Traceloop tracer initialized successfully\")\n+        except (ValueError, RuntimeError, OSError) as e:\n+            logger.error(f\"Failed to initialize Traceloop tracer: {e}\")\n+            self._ready = False\n+\n+    @property\n+    def ready(self) -> bool:\n+        return self._ready\n+\n+    def _validate_configuration(self) -> bool:\n+        api_key = os.getenv(\"TRACELOOP_API_KEY\", \"\").strip()\n+        if not api_key:\n+            logger.warning(\"TRACELOOP_API_KEY not set or empty.\")\n+            return False\n+\n+        base_url = os.getenv(\"TRACELOOP_BASE_URL\", \"https://api.traceloop.com\")\n+        parsed = urlparse(base_url)\n+        if not (parsed.scheme in {\"https\"} and parsed.netloc):",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2282559668",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9317,
        "pr_file": "src/backend/base/langflow/services/tracing/traceloop.py",
        "discussion_id": "2282559668",
        "commented_code": "@@ -0,0 +1,178 @@\n+from __future__ import annotations\n+\n+import os\n+from typing import TYPE_CHECKING, Any\n+from urllib.parse import urlparse\n+\n+from loguru import logger\n+from opentelemetry import trace\n+from traceloop.sdk import Traceloop\n+from traceloop.sdk.instruments import Instruments\n+from typing_extensions import override\n+\n+from langflow.services.tracing.base import BaseTracer\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+    from uuid import UUID\n+\n+    from langchain.callbacks.base import BaseCallbackHandler\n+\n+    from langflow.graph.vertex.base import Vertex\n+    from langflow.services.tracing.schema import Log\n+\n+\n+class TraceloopTracer(BaseTracer):\n+    \"\"\"Traceloop tracer for Langflow.\"\"\"\n+\n+    def __init__(\n+        self,\n+        trace_name: str,\n+        trace_type: str,\n+        project_name: str,\n+        trace_id: UUID,\n+        user_id: str | None = None,\n+        session_id: str | None = None,\n+    ):\n+        self.trace_id = trace_id\n+        self.trace_name = trace_name\n+        self.trace_type = trace_type\n+        self.project_name = project_name\n+        self.user_id = user_id\n+        self.session_id = session_id\n+        self._span_map: dict[str, trace.Span] = {}  # store spans by trace_name\n+\n+        if not self._validate_configuration():\n+            self._ready = False\n+            return\n+\n+        api_key = os.getenv(\"TRACELOOP_API_KEY\", \"\").strip()\n+        try:\n+            Traceloop.init(\n+                instruments={Instruments.LANGCHAIN},\n+                app_name=project_name,\n+                disable_batch=True,\n+                api_key=api_key,\n+                api_endpoint=os.getenv(\"TRACELOOP_BASE_URL\", \"https://api.traceloop.com\"),\n+            )\n+            self._ready = True\n+            self._tracer = trace.get_tracer(\"langflow\")\n+            logger.info(\"Traceloop tracer initialized successfully\")\n+        except (ValueError, RuntimeError, OSError) as e:\n+            logger.error(f\"Failed to initialize Traceloop tracer: {e}\")\n+            self._ready = False\n+\n+    @property\n+    def ready(self) -> bool:\n+        return self._ready\n+\n+    def _validate_configuration(self) -> bool:\n+        api_key = os.getenv(\"TRACELOOP_API_KEY\", \"\").strip()\n+        if not api_key:\n+            logger.warning(\"TRACELOOP_API_KEY not set or empty.\")\n+            return False\n+\n+        base_url = os.getenv(\"TRACELOOP_BASE_URL\", \"https://api.traceloop.com\")\n+        parsed = urlparse(base_url)\n+        if not (parsed.scheme in {\"https\"} and parsed.netloc):",
        "comment_created_at": "2025-08-18T14:21:49+00:00",
        "comment_author": "ronensc",
        "comment_body": "```suggestion\r\n        if not parsed.netloc:\r\n```\r\nCould we relax the `https` constraint? Right now it prevents testing this feature against a local Jaeger instance running on `http://`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2214203352",
    "pr_number": 9077,
    "pr_file": "src/backend/base/langflow/base/mcp/util.py",
    "created_at": "2025-07-17T20:12:37+00:00",
    "commented_code": "HTTP_BAD_REQUEST = 400\n HTTP_INTERNAL_SERVER_ERROR = 500\n \n+# MCP Session Manager constants\n+MAX_SESSIONS_PER_SERVER = 5  # Maximum number of sessions per server to prevent resource exhaustion\n+SESSION_IDLE_TIMEOUT = 300  # 5 minutes idle timeout for sessions\n+SESSION_CLEANUP_INTERVAL = 60  # Cleanup interval in seconds",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2214203352",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9077,
        "pr_file": "src/backend/base/langflow/base/mcp/util.py",
        "discussion_id": "2214203352",
        "commented_code": "@@ -30,6 +30,11 @@\n HTTP_BAD_REQUEST = 400\n HTTP_INTERNAL_SERVER_ERROR = 500\n \n+# MCP Session Manager constants\n+MAX_SESSIONS_PER_SERVER = 5  # Maximum number of sessions per server to prevent resource exhaustion\n+SESSION_IDLE_TIMEOUT = 300  # 5 minutes idle timeout for sessions\n+SESSION_CLEANUP_INTERVAL = 60  # Cleanup interval in seconds",
        "comment_created_at": "2025-07-17T20:12:37+00:00",
        "comment_author": "jordanrfrazier",
        "comment_body": "I'm unfamiliar with the expected behaviors of the mcp servers. Can you explain the reasoning for each of these? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2214330450",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9077,
        "pr_file": "src/backend/base/langflow/base/mcp/util.py",
        "discussion_id": "2214203352",
        "commented_code": "@@ -30,6 +30,11 @@\n HTTP_BAD_REQUEST = 400\n HTTP_INTERNAL_SERVER_ERROR = 500\n \n+# MCP Session Manager constants\n+MAX_SESSIONS_PER_SERVER = 5  # Maximum number of sessions per server to prevent resource exhaustion\n+SESSION_IDLE_TIMEOUT = 300  # 5 minutes idle timeout for sessions\n+SESSION_CLEANUP_INTERVAL = 60  # Cleanup interval in seconds",
        "comment_created_at": "2025-07-17T21:17:05+00:00",
        "comment_author": "edwinjosechittilappilly",
        "comment_body": "These are not specific to MCP, but some common heuristics for session handling. May be if user wants to configure it we can add these values to the settings also.",
        "pr_file_module": null
      }
    ]
  }
]