[
  {
    "discussion_id": "2214208578",
    "pr_number": 9077,
    "pr_file": "src/backend/base/langflow/base/mcp/util.py",
    "created_at": "2025-07-17T20:16:02+00:00",
    "commented_code": "class MCPSessionManager:\n-    \"\"\"Manages persistent MCP sessions with proper context manager lifecycle.\"\"\"\n+    \"\"\"Manages persistent MCP sessions with proper context manager lifecycle.\n+\n+    Fixed version that addresses the memory leak issue by:\n+    1. Session reuse based on server identity rather than unique context IDs\n+    2. Maximum session limits per server to prevent resource exhaustion\n+    3. Idle timeout for automatic session cleanup\n+    4. Periodic cleanup of stale sessions\n+    \"\"\"\n \n     def __init__(self):\n-        self.sessions = {}  # context_id -> session_info\n+        # Structure: server_key -> {\"sessions\": {session_id: session_info}, \"last_cleanup\": timestamp}\n+        self.sessions_by_server = {}\n         self._background_tasks = set()  # Keep references to background tasks\n-        self._last_server_by_session = {}  # context_id -> server_name for tracking switches\n+        self._cleanup_task = None\n+        self._start_cleanup_task()\n+\n+    def _start_cleanup_task(self):\n+        \"\"\"Start the periodic cleanup task.\"\"\"\n+        if self._cleanup_task is None or self._cleanup_task.done():\n+            self._cleanup_task = asyncio.create_task(self._periodic_cleanup())\n+            self._background_tasks.add(self._cleanup_task)\n+            self._cleanup_task.add_done_callback(self._background_tasks.discard)\n+\n+    async def _periodic_cleanup(self):\n+        \"\"\"Periodically clean up idle sessions.\"\"\"\n+        while True:\n+            try:\n+                await asyncio.sleep(SESSION_CLEANUP_INTERVAL)\n+                await self._cleanup_idle_sessions()\n+            except asyncio.CancelledError:\n+                break\n+            except Exception as e:\n+                logger.warning(f\"Error in periodic cleanup: {e}\")\n+\n+    async def _cleanup_idle_sessions(self):\n+        \"\"\"Clean up sessions that have been idle for too long.\"\"\"\n+        current_time = asyncio.get_event_loop().time()\n+        servers_to_remove = []\n+\n+        for server_key, server_data in self.sessions_by_server.items():\n+            sessions = server_data.get(\"sessions\", {})\n+            sessions_to_remove = []\n+\n+            for session_id, session_info in sessions.items():\n+                if current_time - session_info[\"last_used\"] > SESSION_IDLE_TIMEOUT:\n+                    sessions_to_remove.append(session_id)\n+\n+            # Clean up idle sessions\n+            for session_id in sessions_to_remove:\n+                logger.info(f\"Cleaning up idle session {session_id} for server {server_key}\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+\n+            # Remove server entry if no sessions left\n+            if not sessions:\n+                servers_to_remove.append(server_key)\n+\n+        # Clean up empty server entries\n+        for server_key in servers_to_remove:\n+            del self.sessions_by_server[server_key]\n+\n+    def _get_server_key(self, connection_params, transport_type: str) -> str:\n+        \"\"\"Generate a consistent server key based on connection parameters.\"\"\"\n+        if transport_type == \"stdio\":\n+            if hasattr(connection_params, \"command\"):\n+                # Include command, args, and environment for uniqueness\n+                command_str = f\"{connection_params.command} {' '.join(connection_params.args or [])}\"\n+                env_str = str(sorted((connection_params.env or {}).items()))\n+                key_input = f\"{command_str}|{env_str}\"\n+                return f\"stdio_{hash(key_input)}\"\n+        elif transport_type == \"sse\":\n+            if isinstance(connection_params, dict) and \"url\" in connection_params:\n+                # Include URL and headers for uniqueness\n+                url = connection_params[\"url\"]\n+                headers = str(sorted((connection_params.get(\"headers\", {})).items()))\n+                key_input = f\"{url}|{headers}\"\n+                return f\"sse_{hash(key_input)}\"\n+\n+        # Fallback to a generic key\n+        return f\"{transport_type}_{hash(str(connection_params))}\"\n \n     async def _validate_session_connectivity(self, session) -> bool:\n         \"\"\"Validate that the session is actually usable by testing a simple operation.\"\"\"\n         try:\n-            # Try to list tools as a connectivity test (this is a lightweight operation)\n-            # Use a shorter timeout for the connectivity test to fail fast\n             response = await asyncio.wait_for(session.list_tools(), timeout=3.0)\n-        except (asyncio.TimeoutError, ConnectionError, OSError, ValueError) as e:\n-            logger.debug(f\"Session connectivity test failed (standard error): {e}\")\n-            return False\n-        except Exception as e:\n-            # Handle MCP-specific errors that might not be in the standard list\n-            error_str = str(e)\n-            if (\n-                \"ClosedResourceError\" in str(type(e))\n-                or \"Connection closed\" in error_str\n-                or \"Connection lost\" in error_str\n-                or \"Transport closed\" in error_str\n-                or \"Stream closed\" in error_str\n-            ):\n-                logger.debug(f\"Session connectivity test failed (MCP connection error): {e}\")\n-                return False\n-            # Re-raise unexpected errors\n-            logger.warning(f\"Unexpected error in connectivity test: {e}\")\n-            raise\n-        else:\n-            # Validate that we got a meaningful response\n             if response is None:\n-                logger.debug(\"Session connectivity test failed: received None response\")\n-                return False\n-            try:\n-                # Check if we can access the tools list (even if empty)\n-                tools = getattr(response, \"tools\", None)\n-                if tools is None:\n-                    logger.debug(\"Session connectivity test failed: no tools attribute in response\")\n-                    return False\n-            except (AttributeError, TypeError) as e:\n-                logger.debug(f\"Session connectivity test failed while validating response: {e}\")\n                 return False\n-            else:\n-                logger.debug(f\"Session connectivity test passed: found {len(tools)} tools\")\n-                return True\n+            tools = getattr(response, \"tools\", None)\n+            return tools is not None\n+        except Exception as e:\n+            logger.debug(f\"Session connectivity test failed: {e}\")\n+            return False\n \n     async def get_session(self, context_id: str, connection_params, transport_type: str):\n-        \"\"\"Get or create a persistent session.\"\"\"\n-        # Extract server identifier from connection params for tracking\n-        server_identifier = None\n-        if transport_type == \"stdio\" and hasattr(connection_params, \"command\"):\n-            server_identifier = f\"stdio_{connection_params.command}\"\n-        elif transport_type == \"sse\" and isinstance(connection_params, dict) and \"url\" in connection_params:\n-            server_identifier = f\"sse_{connection_params['url']}\"\n-\n-        # Check if we're switching servers for this context\n-        server_switched = False\n-        if context_id in self._last_server_by_session:\n-            last_server = self._last_server_by_session[context_id]\n-            if last_server != server_identifier:\n-                server_switched = True\n-                logger.info(f\"Detected server switch for context {context_id}: {last_server} -> {server_identifier}\")\n-\n-        # Update server tracking\n-        if server_identifier:\n-            self._last_server_by_session[context_id] = server_identifier\n-\n-        if context_id in self.sessions:\n-            session_info = self.sessions[context_id]\n-            # Check if session and background task are still alive\n-            try:\n-                session = session_info[\"session\"]\n-                task = session_info[\"task\"]\n+        \"\"\"Get or create a session with improved reuse strategy.\n \n-                # Break down the health check to understand why cleanup is triggered\n-                task_not_done = not task.done()\n+        The key insight is that we should reuse sessions based on the server\n+        identity (command + args for stdio, URL for SSE) rather than the context_id.\n+        This prevents creating a new subprocess for each unique context.\n+        \"\"\"\n+        server_key = self._get_server_key(connection_params, transport_type)\n \n-                # Additional check for stream health\n-                stream_is_healthy = True\n-                try:\n-                    # Check if the session's write stream is still open\n-                    if hasattr(session, \"_write_stream\"):\n-                        write_stream = session._write_stream\n-\n-                        # Check for explicit closed state\n-                        if hasattr(write_stream, \"_closed\") and write_stream._closed:\n-                            stream_is_healthy = False\n-                        # Check anyio stream state for send channels\n-                        elif hasattr(write_stream, \"_state\") and hasattr(write_stream._state, \"open_send_channels\"):\n-                            # Stream is healthy if there are open send channels\n-                            stream_is_healthy = write_stream._state.open_send_channels > 0\n-                        # Check for other stream closed indicators\n-                        elif hasattr(write_stream, \"is_closing\") and callable(write_stream.is_closing):\n-                            stream_is_healthy = not write_stream.is_closing()\n-                        # If we can't determine state definitively, try a simple write test\n-                        else:\n-                            # For streams we can't easily check, assume healthy unless proven otherwise\n-                            # The actual tool call will reveal if the stream is truly dead\n-                            stream_is_healthy = True\n-\n-                except (AttributeError, TypeError) as e:\n-                    # If we can't check stream health due to missing attributes,\n-                    # assume it's healthy and let the tool call fail if it's not\n-                    logger.debug(f\"Could not check stream health for context_id {context_id}: {e}\")\n-                    stream_is_healthy = True\n-\n-                logger.debug(f\"Session health check for context_id {context_id}:\")\n-                logger.debug(f\"  - task_not_done: {task_not_done}\")\n-                logger.debug(f\"  - stream_is_healthy: {stream_is_healthy}\")\n-\n-                # For MCP ClientSession, we need both task and stream to be healthy\n-                session_is_healthy = task_not_done and stream_is_healthy\n-\n-                logger.debug(f\"  - session_is_healthy: {session_is_healthy}\")\n-\n-                # If we switched servers, always recreate the session to avoid cross-server contamination\n-                if server_switched:\n-                    logger.info(f\"Server switch detected for context_id {context_id}, forcing session recreation\")\n-                    session_is_healthy = False\n-\n-                # Always run connectivity test for sessions to ensure they're truly responsive\n-                # This is especially important when switching between servers\n-                elif session_is_healthy:\n-                    logger.debug(f\"Running connectivity test for context_id {context_id}\")\n-                    connectivity_ok = await self._validate_session_connectivity(session)\n-                    logger.debug(f\"  - connectivity_ok: {connectivity_ok}\")\n-                    if not connectivity_ok:\n-                        session_is_healthy = False\n-                        logger.info(\n-                            f\"Session for context_id {context_id} failed connectivity test, marking as unhealthy\"\n-                        )\n-\n-                if session_is_healthy:\n-                    logger.debug(f\"Session for context_id {context_id} is healthy and responsive, reusing\")\n-                    return session\n+        # Ensure server entry exists\n+        if server_key not in self.sessions_by_server:\n+            self.sessions_by_server[server_key] = {\"sessions\": {}, \"last_cleanup\": asyncio.get_event_loop().time()}\n+\n+        server_data = self.sessions_by_server[server_key]\n+        sessions = server_data[\"sessions\"]\n \n-                if not task_not_done:\n-                    msg = f\"Session for context_id {context_id} failed health check: background task is done\"\n-                    logger.info(msg)\n-                elif not stream_is_healthy:\n-                    msg = f\"Session for context_id {context_id} failed health check: stream is closed\"\n-                    logger.info(msg)\n+        # Try to find a healthy existing session\n+        for session_id, session_info in sessions.items():\n+            session = session_info[\"session\"]\n+            task = session_info[\"task\"]\n \n-            except Exception as e:  # noqa: BLE001\n-                msg = f\"Session for context_id {context_id} is dead due to exception: {e}\"\n-                logger.info(msg)\n-            # Session is dead, clean it up\n-            await self._cleanup_session(context_id)\n+            # Check if session is still alive\n+            if not task.done():\n+                # Update last used time\n+                session_info[\"last_used\"] = asyncio.get_event_loop().time()\n+\n+                # Quick health check\n+                if await self._validate_session_connectivity(session):\n+                    logger.debug(f\"Reusing existing session {session_id} for server {server_key}\")\n+                    return session\n+                logger.info(f\"Session {session_id} for server {server_key} failed health check, cleaning up\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+            else:\n+                # Task is done, clean up\n+                logger.info(f\"Session {session_id} for server {server_key} task is done, cleaning up\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+\n+        # Check if we've reached the maximum number of sessions for this server\n+        if len(sessions) >= MAX_SESSIONS_PER_SERVER:\n+            # Remove the oldest session\n+            oldest_session_id = min(sessions.keys(), key=lambda x: sessions[x][\"last_used\"])\n+            logger.info(\n+                f\"Maximum sessions reached for server {server_key}, removing oldest session {oldest_session_id}\"\n+            )\n+            await self._cleanup_session_by_id(server_key, oldest_session_id)",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2214208578",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9077,
        "pr_file": "src/backend/base/langflow/base/mcp/util.py",
        "discussion_id": "2214208578",
        "commented_code": "@@ -380,167 +385,166 @@ async def _validate_connection_params(mode: str, command: str | None = None, url\n \n \n class MCPSessionManager:\n-    \"\"\"Manages persistent MCP sessions with proper context manager lifecycle.\"\"\"\n+    \"\"\"Manages persistent MCP sessions with proper context manager lifecycle.\n+\n+    Fixed version that addresses the memory leak issue by:\n+    1. Session reuse based on server identity rather than unique context IDs\n+    2. Maximum session limits per server to prevent resource exhaustion\n+    3. Idle timeout for automatic session cleanup\n+    4. Periodic cleanup of stale sessions\n+    \"\"\"\n \n     def __init__(self):\n-        self.sessions = {}  # context_id -> session_info\n+        # Structure: server_key -> {\"sessions\": {session_id: session_info}, \"last_cleanup\": timestamp}\n+        self.sessions_by_server = {}\n         self._background_tasks = set()  # Keep references to background tasks\n-        self._last_server_by_session = {}  # context_id -> server_name for tracking switches\n+        self._cleanup_task = None\n+        self._start_cleanup_task()\n+\n+    def _start_cleanup_task(self):\n+        \"\"\"Start the periodic cleanup task.\"\"\"\n+        if self._cleanup_task is None or self._cleanup_task.done():\n+            self._cleanup_task = asyncio.create_task(self._periodic_cleanup())\n+            self._background_tasks.add(self._cleanup_task)\n+            self._cleanup_task.add_done_callback(self._background_tasks.discard)\n+\n+    async def _periodic_cleanup(self):\n+        \"\"\"Periodically clean up idle sessions.\"\"\"\n+        while True:\n+            try:\n+                await asyncio.sleep(SESSION_CLEANUP_INTERVAL)\n+                await self._cleanup_idle_sessions()\n+            except asyncio.CancelledError:\n+                break\n+            except Exception as e:\n+                logger.warning(f\"Error in periodic cleanup: {e}\")\n+\n+    async def _cleanup_idle_sessions(self):\n+        \"\"\"Clean up sessions that have been idle for too long.\"\"\"\n+        current_time = asyncio.get_event_loop().time()\n+        servers_to_remove = []\n+\n+        for server_key, server_data in self.sessions_by_server.items():\n+            sessions = server_data.get(\"sessions\", {})\n+            sessions_to_remove = []\n+\n+            for session_id, session_info in sessions.items():\n+                if current_time - session_info[\"last_used\"] > SESSION_IDLE_TIMEOUT:\n+                    sessions_to_remove.append(session_id)\n+\n+            # Clean up idle sessions\n+            for session_id in sessions_to_remove:\n+                logger.info(f\"Cleaning up idle session {session_id} for server {server_key}\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+\n+            # Remove server entry if no sessions left\n+            if not sessions:\n+                servers_to_remove.append(server_key)\n+\n+        # Clean up empty server entries\n+        for server_key in servers_to_remove:\n+            del self.sessions_by_server[server_key]\n+\n+    def _get_server_key(self, connection_params, transport_type: str) -> str:\n+        \"\"\"Generate a consistent server key based on connection parameters.\"\"\"\n+        if transport_type == \"stdio\":\n+            if hasattr(connection_params, \"command\"):\n+                # Include command, args, and environment for uniqueness\n+                command_str = f\"{connection_params.command} {' '.join(connection_params.args or [])}\"\n+                env_str = str(sorted((connection_params.env or {}).items()))\n+                key_input = f\"{command_str}|{env_str}\"\n+                return f\"stdio_{hash(key_input)}\"\n+        elif transport_type == \"sse\":\n+            if isinstance(connection_params, dict) and \"url\" in connection_params:\n+                # Include URL and headers for uniqueness\n+                url = connection_params[\"url\"]\n+                headers = str(sorted((connection_params.get(\"headers\", {})).items()))\n+                key_input = f\"{url}|{headers}\"\n+                return f\"sse_{hash(key_input)}\"\n+\n+        # Fallback to a generic key\n+        return f\"{transport_type}_{hash(str(connection_params))}\"\n \n     async def _validate_session_connectivity(self, session) -> bool:\n         \"\"\"Validate that the session is actually usable by testing a simple operation.\"\"\"\n         try:\n-            # Try to list tools as a connectivity test (this is a lightweight operation)\n-            # Use a shorter timeout for the connectivity test to fail fast\n             response = await asyncio.wait_for(session.list_tools(), timeout=3.0)\n-        except (asyncio.TimeoutError, ConnectionError, OSError, ValueError) as e:\n-            logger.debug(f\"Session connectivity test failed (standard error): {e}\")\n-            return False\n-        except Exception as e:\n-            # Handle MCP-specific errors that might not be in the standard list\n-            error_str = str(e)\n-            if (\n-                \"ClosedResourceError\" in str(type(e))\n-                or \"Connection closed\" in error_str\n-                or \"Connection lost\" in error_str\n-                or \"Transport closed\" in error_str\n-                or \"Stream closed\" in error_str\n-            ):\n-                logger.debug(f\"Session connectivity test failed (MCP connection error): {e}\")\n-                return False\n-            # Re-raise unexpected errors\n-            logger.warning(f\"Unexpected error in connectivity test: {e}\")\n-            raise\n-        else:\n-            # Validate that we got a meaningful response\n             if response is None:\n-                logger.debug(\"Session connectivity test failed: received None response\")\n-                return False\n-            try:\n-                # Check if we can access the tools list (even if empty)\n-                tools = getattr(response, \"tools\", None)\n-                if tools is None:\n-                    logger.debug(\"Session connectivity test failed: no tools attribute in response\")\n-                    return False\n-            except (AttributeError, TypeError) as e:\n-                logger.debug(f\"Session connectivity test failed while validating response: {e}\")\n                 return False\n-            else:\n-                logger.debug(f\"Session connectivity test passed: found {len(tools)} tools\")\n-                return True\n+            tools = getattr(response, \"tools\", None)\n+            return tools is not None\n+        except Exception as e:\n+            logger.debug(f\"Session connectivity test failed: {e}\")\n+            return False\n \n     async def get_session(self, context_id: str, connection_params, transport_type: str):\n-        \"\"\"Get or create a persistent session.\"\"\"\n-        # Extract server identifier from connection params for tracking\n-        server_identifier = None\n-        if transport_type == \"stdio\" and hasattr(connection_params, \"command\"):\n-            server_identifier = f\"stdio_{connection_params.command}\"\n-        elif transport_type == \"sse\" and isinstance(connection_params, dict) and \"url\" in connection_params:\n-            server_identifier = f\"sse_{connection_params['url']}\"\n-\n-        # Check if we're switching servers for this context\n-        server_switched = False\n-        if context_id in self._last_server_by_session:\n-            last_server = self._last_server_by_session[context_id]\n-            if last_server != server_identifier:\n-                server_switched = True\n-                logger.info(f\"Detected server switch for context {context_id}: {last_server} -> {server_identifier}\")\n-\n-        # Update server tracking\n-        if server_identifier:\n-            self._last_server_by_session[context_id] = server_identifier\n-\n-        if context_id in self.sessions:\n-            session_info = self.sessions[context_id]\n-            # Check if session and background task are still alive\n-            try:\n-                session = session_info[\"session\"]\n-                task = session_info[\"task\"]\n+        \"\"\"Get or create a session with improved reuse strategy.\n \n-                # Break down the health check to understand why cleanup is triggered\n-                task_not_done = not task.done()\n+        The key insight is that we should reuse sessions based on the server\n+        identity (command + args for stdio, URL for SSE) rather than the context_id.\n+        This prevents creating a new subprocess for each unique context.\n+        \"\"\"\n+        server_key = self._get_server_key(connection_params, transport_type)\n \n-                # Additional check for stream health\n-                stream_is_healthy = True\n-                try:\n-                    # Check if the session's write stream is still open\n-                    if hasattr(session, \"_write_stream\"):\n-                        write_stream = session._write_stream\n-\n-                        # Check for explicit closed state\n-                        if hasattr(write_stream, \"_closed\") and write_stream._closed:\n-                            stream_is_healthy = False\n-                        # Check anyio stream state for send channels\n-                        elif hasattr(write_stream, \"_state\") and hasattr(write_stream._state, \"open_send_channels\"):\n-                            # Stream is healthy if there are open send channels\n-                            stream_is_healthy = write_stream._state.open_send_channels > 0\n-                        # Check for other stream closed indicators\n-                        elif hasattr(write_stream, \"is_closing\") and callable(write_stream.is_closing):\n-                            stream_is_healthy = not write_stream.is_closing()\n-                        # If we can't determine state definitively, try a simple write test\n-                        else:\n-                            # For streams we can't easily check, assume healthy unless proven otherwise\n-                            # The actual tool call will reveal if the stream is truly dead\n-                            stream_is_healthy = True\n-\n-                except (AttributeError, TypeError) as e:\n-                    # If we can't check stream health due to missing attributes,\n-                    # assume it's healthy and let the tool call fail if it's not\n-                    logger.debug(f\"Could not check stream health for context_id {context_id}: {e}\")\n-                    stream_is_healthy = True\n-\n-                logger.debug(f\"Session health check for context_id {context_id}:\")\n-                logger.debug(f\"  - task_not_done: {task_not_done}\")\n-                logger.debug(f\"  - stream_is_healthy: {stream_is_healthy}\")\n-\n-                # For MCP ClientSession, we need both task and stream to be healthy\n-                session_is_healthy = task_not_done and stream_is_healthy\n-\n-                logger.debug(f\"  - session_is_healthy: {session_is_healthy}\")\n-\n-                # If we switched servers, always recreate the session to avoid cross-server contamination\n-                if server_switched:\n-                    logger.info(f\"Server switch detected for context_id {context_id}, forcing session recreation\")\n-                    session_is_healthy = False\n-\n-                # Always run connectivity test for sessions to ensure they're truly responsive\n-                # This is especially important when switching between servers\n-                elif session_is_healthy:\n-                    logger.debug(f\"Running connectivity test for context_id {context_id}\")\n-                    connectivity_ok = await self._validate_session_connectivity(session)\n-                    logger.debug(f\"  - connectivity_ok: {connectivity_ok}\")\n-                    if not connectivity_ok:\n-                        session_is_healthy = False\n-                        logger.info(\n-                            f\"Session for context_id {context_id} failed connectivity test, marking as unhealthy\"\n-                        )\n-\n-                if session_is_healthy:\n-                    logger.debug(f\"Session for context_id {context_id} is healthy and responsive, reusing\")\n-                    return session\n+        # Ensure server entry exists\n+        if server_key not in self.sessions_by_server:\n+            self.sessions_by_server[server_key] = {\"sessions\": {}, \"last_cleanup\": asyncio.get_event_loop().time()}\n+\n+        server_data = self.sessions_by_server[server_key]\n+        sessions = server_data[\"sessions\"]\n \n-                if not task_not_done:\n-                    msg = f\"Session for context_id {context_id} failed health check: background task is done\"\n-                    logger.info(msg)\n-                elif not stream_is_healthy:\n-                    msg = f\"Session for context_id {context_id} failed health check: stream is closed\"\n-                    logger.info(msg)\n+        # Try to find a healthy existing session\n+        for session_id, session_info in sessions.items():\n+            session = session_info[\"session\"]\n+            task = session_info[\"task\"]\n \n-            except Exception as e:  # noqa: BLE001\n-                msg = f\"Session for context_id {context_id} is dead due to exception: {e}\"\n-                logger.info(msg)\n-            # Session is dead, clean it up\n-            await self._cleanup_session(context_id)\n+            # Check if session is still alive\n+            if not task.done():\n+                # Update last used time\n+                session_info[\"last_used\"] = asyncio.get_event_loop().time()\n+\n+                # Quick health check\n+                if await self._validate_session_connectivity(session):\n+                    logger.debug(f\"Reusing existing session {session_id} for server {server_key}\")\n+                    return session\n+                logger.info(f\"Session {session_id} for server {server_key} failed health check, cleaning up\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+            else:\n+                # Task is done, clean up\n+                logger.info(f\"Session {session_id} for server {server_key} task is done, cleaning up\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+\n+        # Check if we've reached the maximum number of sessions for this server\n+        if len(sessions) >= MAX_SESSIONS_PER_SERVER:\n+            # Remove the oldest session\n+            oldest_session_id = min(sessions.keys(), key=lambda x: sessions[x][\"last_used\"])\n+            logger.info(\n+                f\"Maximum sessions reached for server {server_key}, removing oldest session {oldest_session_id}\"\n+            )\n+            await self._cleanup_session_by_id(server_key, oldest_session_id)",
        "comment_created_at": "2025-07-17T20:16:02+00:00",
        "comment_author": "jordanrfrazier",
        "comment_body": "This feels like giving the user the opportunity to unintentionally disrupt expected behavior, by automatically removing sessions past some maximum. I would think that the standard behavior should be to fail to create a new session. But perhaps I need some explaining on what a session is. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2214340306",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9077,
        "pr_file": "src/backend/base/langflow/base/mcp/util.py",
        "discussion_id": "2214208578",
        "commented_code": "@@ -380,167 +385,166 @@ async def _validate_connection_params(mode: str, command: str | None = None, url\n \n \n class MCPSessionManager:\n-    \"\"\"Manages persistent MCP sessions with proper context manager lifecycle.\"\"\"\n+    \"\"\"Manages persistent MCP sessions with proper context manager lifecycle.\n+\n+    Fixed version that addresses the memory leak issue by:\n+    1. Session reuse based on server identity rather than unique context IDs\n+    2. Maximum session limits per server to prevent resource exhaustion\n+    3. Idle timeout for automatic session cleanup\n+    4. Periodic cleanup of stale sessions\n+    \"\"\"\n \n     def __init__(self):\n-        self.sessions = {}  # context_id -> session_info\n+        # Structure: server_key -> {\"sessions\": {session_id: session_info}, \"last_cleanup\": timestamp}\n+        self.sessions_by_server = {}\n         self._background_tasks = set()  # Keep references to background tasks\n-        self._last_server_by_session = {}  # context_id -> server_name for tracking switches\n+        self._cleanup_task = None\n+        self._start_cleanup_task()\n+\n+    def _start_cleanup_task(self):\n+        \"\"\"Start the periodic cleanup task.\"\"\"\n+        if self._cleanup_task is None or self._cleanup_task.done():\n+            self._cleanup_task = asyncio.create_task(self._periodic_cleanup())\n+            self._background_tasks.add(self._cleanup_task)\n+            self._cleanup_task.add_done_callback(self._background_tasks.discard)\n+\n+    async def _periodic_cleanup(self):\n+        \"\"\"Periodically clean up idle sessions.\"\"\"\n+        while True:\n+            try:\n+                await asyncio.sleep(SESSION_CLEANUP_INTERVAL)\n+                await self._cleanup_idle_sessions()\n+            except asyncio.CancelledError:\n+                break\n+            except Exception as e:\n+                logger.warning(f\"Error in periodic cleanup: {e}\")\n+\n+    async def _cleanup_idle_sessions(self):\n+        \"\"\"Clean up sessions that have been idle for too long.\"\"\"\n+        current_time = asyncio.get_event_loop().time()\n+        servers_to_remove = []\n+\n+        for server_key, server_data in self.sessions_by_server.items():\n+            sessions = server_data.get(\"sessions\", {})\n+            sessions_to_remove = []\n+\n+            for session_id, session_info in sessions.items():\n+                if current_time - session_info[\"last_used\"] > SESSION_IDLE_TIMEOUT:\n+                    sessions_to_remove.append(session_id)\n+\n+            # Clean up idle sessions\n+            for session_id in sessions_to_remove:\n+                logger.info(f\"Cleaning up idle session {session_id} for server {server_key}\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+\n+            # Remove server entry if no sessions left\n+            if not sessions:\n+                servers_to_remove.append(server_key)\n+\n+        # Clean up empty server entries\n+        for server_key in servers_to_remove:\n+            del self.sessions_by_server[server_key]\n+\n+    def _get_server_key(self, connection_params, transport_type: str) -> str:\n+        \"\"\"Generate a consistent server key based on connection parameters.\"\"\"\n+        if transport_type == \"stdio\":\n+            if hasattr(connection_params, \"command\"):\n+                # Include command, args, and environment for uniqueness\n+                command_str = f\"{connection_params.command} {' '.join(connection_params.args or [])}\"\n+                env_str = str(sorted((connection_params.env or {}).items()))\n+                key_input = f\"{command_str}|{env_str}\"\n+                return f\"stdio_{hash(key_input)}\"\n+        elif transport_type == \"sse\":\n+            if isinstance(connection_params, dict) and \"url\" in connection_params:\n+                # Include URL and headers for uniqueness\n+                url = connection_params[\"url\"]\n+                headers = str(sorted((connection_params.get(\"headers\", {})).items()))\n+                key_input = f\"{url}|{headers}\"\n+                return f\"sse_{hash(key_input)}\"\n+\n+        # Fallback to a generic key\n+        return f\"{transport_type}_{hash(str(connection_params))}\"\n \n     async def _validate_session_connectivity(self, session) -> bool:\n         \"\"\"Validate that the session is actually usable by testing a simple operation.\"\"\"\n         try:\n-            # Try to list tools as a connectivity test (this is a lightweight operation)\n-            # Use a shorter timeout for the connectivity test to fail fast\n             response = await asyncio.wait_for(session.list_tools(), timeout=3.0)\n-        except (asyncio.TimeoutError, ConnectionError, OSError, ValueError) as e:\n-            logger.debug(f\"Session connectivity test failed (standard error): {e}\")\n-            return False\n-        except Exception as e:\n-            # Handle MCP-specific errors that might not be in the standard list\n-            error_str = str(e)\n-            if (\n-                \"ClosedResourceError\" in str(type(e))\n-                or \"Connection closed\" in error_str\n-                or \"Connection lost\" in error_str\n-                or \"Transport closed\" in error_str\n-                or \"Stream closed\" in error_str\n-            ):\n-                logger.debug(f\"Session connectivity test failed (MCP connection error): {e}\")\n-                return False\n-            # Re-raise unexpected errors\n-            logger.warning(f\"Unexpected error in connectivity test: {e}\")\n-            raise\n-        else:\n-            # Validate that we got a meaningful response\n             if response is None:\n-                logger.debug(\"Session connectivity test failed: received None response\")\n-                return False\n-            try:\n-                # Check if we can access the tools list (even if empty)\n-                tools = getattr(response, \"tools\", None)\n-                if tools is None:\n-                    logger.debug(\"Session connectivity test failed: no tools attribute in response\")\n-                    return False\n-            except (AttributeError, TypeError) as e:\n-                logger.debug(f\"Session connectivity test failed while validating response: {e}\")\n                 return False\n-            else:\n-                logger.debug(f\"Session connectivity test passed: found {len(tools)} tools\")\n-                return True\n+            tools = getattr(response, \"tools\", None)\n+            return tools is not None\n+        except Exception as e:\n+            logger.debug(f\"Session connectivity test failed: {e}\")\n+            return False\n \n     async def get_session(self, context_id: str, connection_params, transport_type: str):\n-        \"\"\"Get or create a persistent session.\"\"\"\n-        # Extract server identifier from connection params for tracking\n-        server_identifier = None\n-        if transport_type == \"stdio\" and hasattr(connection_params, \"command\"):\n-            server_identifier = f\"stdio_{connection_params.command}\"\n-        elif transport_type == \"sse\" and isinstance(connection_params, dict) and \"url\" in connection_params:\n-            server_identifier = f\"sse_{connection_params['url']}\"\n-\n-        # Check if we're switching servers for this context\n-        server_switched = False\n-        if context_id in self._last_server_by_session:\n-            last_server = self._last_server_by_session[context_id]\n-            if last_server != server_identifier:\n-                server_switched = True\n-                logger.info(f\"Detected server switch for context {context_id}: {last_server} -> {server_identifier}\")\n-\n-        # Update server tracking\n-        if server_identifier:\n-            self._last_server_by_session[context_id] = server_identifier\n-\n-        if context_id in self.sessions:\n-            session_info = self.sessions[context_id]\n-            # Check if session and background task are still alive\n-            try:\n-                session = session_info[\"session\"]\n-                task = session_info[\"task\"]\n+        \"\"\"Get or create a session with improved reuse strategy.\n \n-                # Break down the health check to understand why cleanup is triggered\n-                task_not_done = not task.done()\n+        The key insight is that we should reuse sessions based on the server\n+        identity (command + args for stdio, URL for SSE) rather than the context_id.\n+        This prevents creating a new subprocess for each unique context.\n+        \"\"\"\n+        server_key = self._get_server_key(connection_params, transport_type)\n \n-                # Additional check for stream health\n-                stream_is_healthy = True\n-                try:\n-                    # Check if the session's write stream is still open\n-                    if hasattr(session, \"_write_stream\"):\n-                        write_stream = session._write_stream\n-\n-                        # Check for explicit closed state\n-                        if hasattr(write_stream, \"_closed\") and write_stream._closed:\n-                            stream_is_healthy = False\n-                        # Check anyio stream state for send channels\n-                        elif hasattr(write_stream, \"_state\") and hasattr(write_stream._state, \"open_send_channels\"):\n-                            # Stream is healthy if there are open send channels\n-                            stream_is_healthy = write_stream._state.open_send_channels > 0\n-                        # Check for other stream closed indicators\n-                        elif hasattr(write_stream, \"is_closing\") and callable(write_stream.is_closing):\n-                            stream_is_healthy = not write_stream.is_closing()\n-                        # If we can't determine state definitively, try a simple write test\n-                        else:\n-                            # For streams we can't easily check, assume healthy unless proven otherwise\n-                            # The actual tool call will reveal if the stream is truly dead\n-                            stream_is_healthy = True\n-\n-                except (AttributeError, TypeError) as e:\n-                    # If we can't check stream health due to missing attributes,\n-                    # assume it's healthy and let the tool call fail if it's not\n-                    logger.debug(f\"Could not check stream health for context_id {context_id}: {e}\")\n-                    stream_is_healthy = True\n-\n-                logger.debug(f\"Session health check for context_id {context_id}:\")\n-                logger.debug(f\"  - task_not_done: {task_not_done}\")\n-                logger.debug(f\"  - stream_is_healthy: {stream_is_healthy}\")\n-\n-                # For MCP ClientSession, we need both task and stream to be healthy\n-                session_is_healthy = task_not_done and stream_is_healthy\n-\n-                logger.debug(f\"  - session_is_healthy: {session_is_healthy}\")\n-\n-                # If we switched servers, always recreate the session to avoid cross-server contamination\n-                if server_switched:\n-                    logger.info(f\"Server switch detected for context_id {context_id}, forcing session recreation\")\n-                    session_is_healthy = False\n-\n-                # Always run connectivity test for sessions to ensure they're truly responsive\n-                # This is especially important when switching between servers\n-                elif session_is_healthy:\n-                    logger.debug(f\"Running connectivity test for context_id {context_id}\")\n-                    connectivity_ok = await self._validate_session_connectivity(session)\n-                    logger.debug(f\"  - connectivity_ok: {connectivity_ok}\")\n-                    if not connectivity_ok:\n-                        session_is_healthy = False\n-                        logger.info(\n-                            f\"Session for context_id {context_id} failed connectivity test, marking as unhealthy\"\n-                        )\n-\n-                if session_is_healthy:\n-                    logger.debug(f\"Session for context_id {context_id} is healthy and responsive, reusing\")\n-                    return session\n+        # Ensure server entry exists\n+        if server_key not in self.sessions_by_server:\n+            self.sessions_by_server[server_key] = {\"sessions\": {}, \"last_cleanup\": asyncio.get_event_loop().time()}\n+\n+        server_data = self.sessions_by_server[server_key]\n+        sessions = server_data[\"sessions\"]\n \n-                if not task_not_done:\n-                    msg = f\"Session for context_id {context_id} failed health check: background task is done\"\n-                    logger.info(msg)\n-                elif not stream_is_healthy:\n-                    msg = f\"Session for context_id {context_id} failed health check: stream is closed\"\n-                    logger.info(msg)\n+        # Try to find a healthy existing session\n+        for session_id, session_info in sessions.items():\n+            session = session_info[\"session\"]\n+            task = session_info[\"task\"]\n \n-            except Exception as e:  # noqa: BLE001\n-                msg = f\"Session for context_id {context_id} is dead due to exception: {e}\"\n-                logger.info(msg)\n-            # Session is dead, clean it up\n-            await self._cleanup_session(context_id)\n+            # Check if session is still alive\n+            if not task.done():\n+                # Update last used time\n+                session_info[\"last_used\"] = asyncio.get_event_loop().time()\n+\n+                # Quick health check\n+                if await self._validate_session_connectivity(session):\n+                    logger.debug(f\"Reusing existing session {session_id} for server {server_key}\")\n+                    return session\n+                logger.info(f\"Session {session_id} for server {server_key} failed health check, cleaning up\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+            else:\n+                # Task is done, clean up\n+                logger.info(f\"Session {session_id} for server {server_key} task is done, cleaning up\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+\n+        # Check if we've reached the maximum number of sessions for this server\n+        if len(sessions) >= MAX_SESSIONS_PER_SERVER:\n+            # Remove the oldest session\n+            oldest_session_id = min(sessions.keys(), key=lambda x: sessions[x][\"last_used\"])\n+            logger.info(\n+                f\"Maximum sessions reached for server {server_key}, removing oldest session {oldest_session_id}\"\n+            )\n+            await self._cleanup_session_by_id(server_key, oldest_session_id)",
        "comment_created_at": "2025-07-17T21:23:32+00:00",
        "comment_author": "edwinjosechittilappilly",
        "comment_body": "Session is basically a connectin to a subporcess. Each Execution will be created as a new sub process.\r\nEarlier the issue was the old sub process was not closed after execution also not reused.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2246315863",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9077,
        "pr_file": "src/backend/base/langflow/base/mcp/util.py",
        "discussion_id": "2214208578",
        "commented_code": "@@ -380,167 +385,166 @@ async def _validate_connection_params(mode: str, command: str | None = None, url\n \n \n class MCPSessionManager:\n-    \"\"\"Manages persistent MCP sessions with proper context manager lifecycle.\"\"\"\n+    \"\"\"Manages persistent MCP sessions with proper context manager lifecycle.\n+\n+    Fixed version that addresses the memory leak issue by:\n+    1. Session reuse based on server identity rather than unique context IDs\n+    2. Maximum session limits per server to prevent resource exhaustion\n+    3. Idle timeout for automatic session cleanup\n+    4. Periodic cleanup of stale sessions\n+    \"\"\"\n \n     def __init__(self):\n-        self.sessions = {}  # context_id -> session_info\n+        # Structure: server_key -> {\"sessions\": {session_id: session_info}, \"last_cleanup\": timestamp}\n+        self.sessions_by_server = {}\n         self._background_tasks = set()  # Keep references to background tasks\n-        self._last_server_by_session = {}  # context_id -> server_name for tracking switches\n+        self._cleanup_task = None\n+        self._start_cleanup_task()\n+\n+    def _start_cleanup_task(self):\n+        \"\"\"Start the periodic cleanup task.\"\"\"\n+        if self._cleanup_task is None or self._cleanup_task.done():\n+            self._cleanup_task = asyncio.create_task(self._periodic_cleanup())\n+            self._background_tasks.add(self._cleanup_task)\n+            self._cleanup_task.add_done_callback(self._background_tasks.discard)\n+\n+    async def _periodic_cleanup(self):\n+        \"\"\"Periodically clean up idle sessions.\"\"\"\n+        while True:\n+            try:\n+                await asyncio.sleep(SESSION_CLEANUP_INTERVAL)\n+                await self._cleanup_idle_sessions()\n+            except asyncio.CancelledError:\n+                break\n+            except Exception as e:\n+                logger.warning(f\"Error in periodic cleanup: {e}\")\n+\n+    async def _cleanup_idle_sessions(self):\n+        \"\"\"Clean up sessions that have been idle for too long.\"\"\"\n+        current_time = asyncio.get_event_loop().time()\n+        servers_to_remove = []\n+\n+        for server_key, server_data in self.sessions_by_server.items():\n+            sessions = server_data.get(\"sessions\", {})\n+            sessions_to_remove = []\n+\n+            for session_id, session_info in sessions.items():\n+                if current_time - session_info[\"last_used\"] > SESSION_IDLE_TIMEOUT:\n+                    sessions_to_remove.append(session_id)\n+\n+            # Clean up idle sessions\n+            for session_id in sessions_to_remove:\n+                logger.info(f\"Cleaning up idle session {session_id} for server {server_key}\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+\n+            # Remove server entry if no sessions left\n+            if not sessions:\n+                servers_to_remove.append(server_key)\n+\n+        # Clean up empty server entries\n+        for server_key in servers_to_remove:\n+            del self.sessions_by_server[server_key]\n+\n+    def _get_server_key(self, connection_params, transport_type: str) -> str:\n+        \"\"\"Generate a consistent server key based on connection parameters.\"\"\"\n+        if transport_type == \"stdio\":\n+            if hasattr(connection_params, \"command\"):\n+                # Include command, args, and environment for uniqueness\n+                command_str = f\"{connection_params.command} {' '.join(connection_params.args or [])}\"\n+                env_str = str(sorted((connection_params.env or {}).items()))\n+                key_input = f\"{command_str}|{env_str}\"\n+                return f\"stdio_{hash(key_input)}\"\n+        elif transport_type == \"sse\":\n+            if isinstance(connection_params, dict) and \"url\" in connection_params:\n+                # Include URL and headers for uniqueness\n+                url = connection_params[\"url\"]\n+                headers = str(sorted((connection_params.get(\"headers\", {})).items()))\n+                key_input = f\"{url}|{headers}\"\n+                return f\"sse_{hash(key_input)}\"\n+\n+        # Fallback to a generic key\n+        return f\"{transport_type}_{hash(str(connection_params))}\"\n \n     async def _validate_session_connectivity(self, session) -> bool:\n         \"\"\"Validate that the session is actually usable by testing a simple operation.\"\"\"\n         try:\n-            # Try to list tools as a connectivity test (this is a lightweight operation)\n-            # Use a shorter timeout for the connectivity test to fail fast\n             response = await asyncio.wait_for(session.list_tools(), timeout=3.0)\n-        except (asyncio.TimeoutError, ConnectionError, OSError, ValueError) as e:\n-            logger.debug(f\"Session connectivity test failed (standard error): {e}\")\n-            return False\n-        except Exception as e:\n-            # Handle MCP-specific errors that might not be in the standard list\n-            error_str = str(e)\n-            if (\n-                \"ClosedResourceError\" in str(type(e))\n-                or \"Connection closed\" in error_str\n-                or \"Connection lost\" in error_str\n-                or \"Transport closed\" in error_str\n-                or \"Stream closed\" in error_str\n-            ):\n-                logger.debug(f\"Session connectivity test failed (MCP connection error): {e}\")\n-                return False\n-            # Re-raise unexpected errors\n-            logger.warning(f\"Unexpected error in connectivity test: {e}\")\n-            raise\n-        else:\n-            # Validate that we got a meaningful response\n             if response is None:\n-                logger.debug(\"Session connectivity test failed: received None response\")\n-                return False\n-            try:\n-                # Check if we can access the tools list (even if empty)\n-                tools = getattr(response, \"tools\", None)\n-                if tools is None:\n-                    logger.debug(\"Session connectivity test failed: no tools attribute in response\")\n-                    return False\n-            except (AttributeError, TypeError) as e:\n-                logger.debug(f\"Session connectivity test failed while validating response: {e}\")\n                 return False\n-            else:\n-                logger.debug(f\"Session connectivity test passed: found {len(tools)} tools\")\n-                return True\n+            tools = getattr(response, \"tools\", None)\n+            return tools is not None\n+        except Exception as e:\n+            logger.debug(f\"Session connectivity test failed: {e}\")\n+            return False\n \n     async def get_session(self, context_id: str, connection_params, transport_type: str):\n-        \"\"\"Get or create a persistent session.\"\"\"\n-        # Extract server identifier from connection params for tracking\n-        server_identifier = None\n-        if transport_type == \"stdio\" and hasattr(connection_params, \"command\"):\n-            server_identifier = f\"stdio_{connection_params.command}\"\n-        elif transport_type == \"sse\" and isinstance(connection_params, dict) and \"url\" in connection_params:\n-            server_identifier = f\"sse_{connection_params['url']}\"\n-\n-        # Check if we're switching servers for this context\n-        server_switched = False\n-        if context_id in self._last_server_by_session:\n-            last_server = self._last_server_by_session[context_id]\n-            if last_server != server_identifier:\n-                server_switched = True\n-                logger.info(f\"Detected server switch for context {context_id}: {last_server} -> {server_identifier}\")\n-\n-        # Update server tracking\n-        if server_identifier:\n-            self._last_server_by_session[context_id] = server_identifier\n-\n-        if context_id in self.sessions:\n-            session_info = self.sessions[context_id]\n-            # Check if session and background task are still alive\n-            try:\n-                session = session_info[\"session\"]\n-                task = session_info[\"task\"]\n+        \"\"\"Get or create a session with improved reuse strategy.\n \n-                # Break down the health check to understand why cleanup is triggered\n-                task_not_done = not task.done()\n+        The key insight is that we should reuse sessions based on the server\n+        identity (command + args for stdio, URL for SSE) rather than the context_id.\n+        This prevents creating a new subprocess for each unique context.\n+        \"\"\"\n+        server_key = self._get_server_key(connection_params, transport_type)\n \n-                # Additional check for stream health\n-                stream_is_healthy = True\n-                try:\n-                    # Check if the session's write stream is still open\n-                    if hasattr(session, \"_write_stream\"):\n-                        write_stream = session._write_stream\n-\n-                        # Check for explicit closed state\n-                        if hasattr(write_stream, \"_closed\") and write_stream._closed:\n-                            stream_is_healthy = False\n-                        # Check anyio stream state for send channels\n-                        elif hasattr(write_stream, \"_state\") and hasattr(write_stream._state, \"open_send_channels\"):\n-                            # Stream is healthy if there are open send channels\n-                            stream_is_healthy = write_stream._state.open_send_channels > 0\n-                        # Check for other stream closed indicators\n-                        elif hasattr(write_stream, \"is_closing\") and callable(write_stream.is_closing):\n-                            stream_is_healthy = not write_stream.is_closing()\n-                        # If we can't determine state definitively, try a simple write test\n-                        else:\n-                            # For streams we can't easily check, assume healthy unless proven otherwise\n-                            # The actual tool call will reveal if the stream is truly dead\n-                            stream_is_healthy = True\n-\n-                except (AttributeError, TypeError) as e:\n-                    # If we can't check stream health due to missing attributes,\n-                    # assume it's healthy and let the tool call fail if it's not\n-                    logger.debug(f\"Could not check stream health for context_id {context_id}: {e}\")\n-                    stream_is_healthy = True\n-\n-                logger.debug(f\"Session health check for context_id {context_id}:\")\n-                logger.debug(f\"  - task_not_done: {task_not_done}\")\n-                logger.debug(f\"  - stream_is_healthy: {stream_is_healthy}\")\n-\n-                # For MCP ClientSession, we need both task and stream to be healthy\n-                session_is_healthy = task_not_done and stream_is_healthy\n-\n-                logger.debug(f\"  - session_is_healthy: {session_is_healthy}\")\n-\n-                # If we switched servers, always recreate the session to avoid cross-server contamination\n-                if server_switched:\n-                    logger.info(f\"Server switch detected for context_id {context_id}, forcing session recreation\")\n-                    session_is_healthy = False\n-\n-                # Always run connectivity test for sessions to ensure they're truly responsive\n-                # This is especially important when switching between servers\n-                elif session_is_healthy:\n-                    logger.debug(f\"Running connectivity test for context_id {context_id}\")\n-                    connectivity_ok = await self._validate_session_connectivity(session)\n-                    logger.debug(f\"  - connectivity_ok: {connectivity_ok}\")\n-                    if not connectivity_ok:\n-                        session_is_healthy = False\n-                        logger.info(\n-                            f\"Session for context_id {context_id} failed connectivity test, marking as unhealthy\"\n-                        )\n-\n-                if session_is_healthy:\n-                    logger.debug(f\"Session for context_id {context_id} is healthy and responsive, reusing\")\n-                    return session\n+        # Ensure server entry exists\n+        if server_key not in self.sessions_by_server:\n+            self.sessions_by_server[server_key] = {\"sessions\": {}, \"last_cleanup\": asyncio.get_event_loop().time()}\n+\n+        server_data = self.sessions_by_server[server_key]\n+        sessions = server_data[\"sessions\"]\n \n-                if not task_not_done:\n-                    msg = f\"Session for context_id {context_id} failed health check: background task is done\"\n-                    logger.info(msg)\n-                elif not stream_is_healthy:\n-                    msg = f\"Session for context_id {context_id} failed health check: stream is closed\"\n-                    logger.info(msg)\n+        # Try to find a healthy existing session\n+        for session_id, session_info in sessions.items():\n+            session = session_info[\"session\"]\n+            task = session_info[\"task\"]\n \n-            except Exception as e:  # noqa: BLE001\n-                msg = f\"Session for context_id {context_id} is dead due to exception: {e}\"\n-                logger.info(msg)\n-            # Session is dead, clean it up\n-            await self._cleanup_session(context_id)\n+            # Check if session is still alive\n+            if not task.done():\n+                # Update last used time\n+                session_info[\"last_used\"] = asyncio.get_event_loop().time()\n+\n+                # Quick health check\n+                if await self._validate_session_connectivity(session):\n+                    logger.debug(f\"Reusing existing session {session_id} for server {server_key}\")\n+                    return session\n+                logger.info(f\"Session {session_id} for server {server_key} failed health check, cleaning up\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+            else:\n+                # Task is done, clean up\n+                logger.info(f\"Session {session_id} for server {server_key} task is done, cleaning up\")\n+                await self._cleanup_session_by_id(server_key, session_id)\n+\n+        # Check if we've reached the maximum number of sessions for this server\n+        if len(sessions) >= MAX_SESSIONS_PER_SERVER:\n+            # Remove the oldest session\n+            oldest_session_id = min(sessions.keys(), key=lambda x: sessions[x][\"last_used\"])\n+            logger.info(\n+                f\"Maximum sessions reached for server {server_key}, removing oldest session {oldest_session_id}\"\n+            )\n+            await self._cleanup_session_by_id(server_key, oldest_session_id)",
        "comment_created_at": "2025-07-31T20:29:02+00:00",
        "comment_author": "jordanrfrazier",
        "comment_body": "Clarified this that the session will be restored on new connection with no disruption to user experience, aside from perhaps needed more time to restore connection. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2207966913",
    "pr_number": 9019,
    "pr_file": "src/backend/tests/unit/test_setup_superuser.py",
    "created_at": "2025-07-15T16:22:37+00:00",
    "commented_code": "mock_session.delete.assert_not_awaited()\n     mock_session.commit.assert_not_awaited()\n+\n+\n+@pytest.mark.asyncio\n+async def test_create_super_user_race_condition():\n+    \"\"\"Test create_super_user handles race conditions gracefully when multiple workers try to create the same user.\"\"\"\n+    # Mock the database session\n+    mock_session = AsyncMock()\n+\n+    # Create a mock user that will be \"created\" by the first worker\n+    mock_user = MagicMock(spec=User)\n+    mock_user.username = \"testuser\"\n+    mock_user.is_superuser = True\n+\n+    # Mock get_password_hash to return a fixed value\n+    mock_get_password_hash = MagicMock(return_value=\"hashed_password\")\n+\n+    # Set up the race condition scenario:\n+    # 1. First call to get_user_by_username returns None (user doesn't exist)\n+    # 2. commit() raises IntegrityError (simulating race condition)\n+    # 3. After rollback, second call to get_user_by_username returns the existing user\n+    mock_get_user_by_username = AsyncMock()\n+    mock_get_user_by_username.side_effect = [None, mock_user]  # None first, then existing user\n+\n+    mock_session.commit.side_effect = IntegrityError(\"statement\", \"params\", \"orig\")\n+    with (\n+        patch(\"langflow.services.auth.utils.get_user_by_username\", mock_get_user_by_username),\n+        patch(\"langflow.services.auth.utils.get_password_hash\", mock_get_password_hash),\n+        patch(\"langflow.services.database.models.user.model.User\") as mock_user_class,\n+    ):\n+        # Configure the User class mock to return our mock_user when instantiated\n+        mock_user_class.return_value = mock_user\n+\n+        result = await create_super_user(\"testuser\", \"password\", mock_session)\n+\n+    # Verify that the function handled the race condition correctly\n+    assert result == mock_user\n+    assert mock_session.add.call_count == 1  # User was added to session\n+    assert mock_session.commit.call_count == 1  # Commit was attempted once (and failed)\n+    assert mock_session.rollback.call_count == 1  # Session was rolled back after IntegrityError\n+    assert mock_get_user_by_username.call_count == 2  # Called twice: initial check + after rollback\n+\n+\n+@pytest.mark.asyncio\n+async def test_create_super_user_race_condition_no_user_found():\n+    \"\"\"Test that create_super_user re-raises exception if no user is found after IntegrityError.\"\"\"\n+    # Mock the database session\n+    mock_session = AsyncMock()\n+\n+    # Mock get_user_by_username to always return None (even after rollback)\n+    mock_get_user_by_username = AsyncMock()\n+    mock_get_user_by_username.side_effect = [None, None]  # None for initial check and after rollback\n+\n+    # Mock other dependencies\n+    mock_get_password_hash = MagicMock(return_value=\"hashed_password\")\n+    mock_user = MagicMock(spec=User)\n+\n+    # Set up scenario where IntegrityError occurs but no user is found afterward\n+    integrity_error = IntegrityError(\"statement\", \"params\", \"orig\")\n+    mock_session.commit.side_effect = integrity_error\n+\n+    with (\n+        patch(\"langflow.services.auth.utils.get_user_by_username\", mock_get_user_by_username),\n+        patch(\"langflow.services.auth.utils.get_password_hash\", mock_get_password_hash),\n+        patch(\"langflow.services.database.models.user.model.User\", return_value=mock_user),\n+    ):\n+        with pytest.raises(IntegrityError):\n+            await create_super_user(\"testuser\", \"password\", mock_session)\n+\n+    # Verify rollback was called but exception was re-raised\n+    assert mock_session.rollback.call_count == 1\n+    assert mock_get_user_by_username.call_count == 2  # Initial + after rollback\n+\n+\n+@pytest.mark.asyncio\n+async def test_create_super_user_concurrent_workers():\n+    \"\"\"Test multiple concurrent calls to create_super_user with the same username.\"\"\"\n+    # This would require a real database to properly test, but we can simulate\n+    # the behavior with mocks to verify the logic works correctly\n+\n+    mock_session1 = AsyncMock()\n+    mock_session2 = AsyncMock()\n+\n+    # Create mock users\n+    mock_user = MagicMock(spec=User)\n+    mock_user.username = \"admin\"\n+    mock_user.is_superuser = True\n+\n+    mock_get_user_by_username = AsyncMock()\n+\n+    # Worker 1 succeeds, Worker 2 gets IntegrityError then finds existing user\n+    mock_session1.commit.return_value = None  # Success\n+    mock_session2.commit.side_effect = IntegrityError(\"statement\", \"params\", \"orig\")  # Race condition\n+\n+    # get_user_by_username returns None initially, then the created user for worker 2\n+    mock_get_user_by_username.side_effect = [None, None, mock_user]\n+\n+    with patch(\"langflow.services.auth.utils.get_user_by_username\", mock_get_user_by_username):\n+        # Simulate concurrent execution\n+        result1 = await create_super_user(\"admin\", \"password\", mock_session1)\n+        result2 = await create_super_user(\"admin\", \"password\", mock_session2)",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2207966913",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9019,
        "pr_file": "src/backend/tests/unit/test_setup_superuser.py",
        "discussion_id": "2207966913",
        "commented_code": "@@ -130,3 +134,110 @@ async def test_teardown_superuser_no_default_superuser():\n \n     mock_session.delete.assert_not_awaited()\n     mock_session.commit.assert_not_awaited()\n+\n+\n+@pytest.mark.asyncio\n+async def test_create_super_user_race_condition():\n+    \"\"\"Test create_super_user handles race conditions gracefully when multiple workers try to create the same user.\"\"\"\n+    # Mock the database session\n+    mock_session = AsyncMock()\n+\n+    # Create a mock user that will be \"created\" by the first worker\n+    mock_user = MagicMock(spec=User)\n+    mock_user.username = \"testuser\"\n+    mock_user.is_superuser = True\n+\n+    # Mock get_password_hash to return a fixed value\n+    mock_get_password_hash = MagicMock(return_value=\"hashed_password\")\n+\n+    # Set up the race condition scenario:\n+    # 1. First call to get_user_by_username returns None (user doesn't exist)\n+    # 2. commit() raises IntegrityError (simulating race condition)\n+    # 3. After rollback, second call to get_user_by_username returns the existing user\n+    mock_get_user_by_username = AsyncMock()\n+    mock_get_user_by_username.side_effect = [None, mock_user]  # None first, then existing user\n+\n+    mock_session.commit.side_effect = IntegrityError(\"statement\", \"params\", \"orig\")\n+    with (\n+        patch(\"langflow.services.auth.utils.get_user_by_username\", mock_get_user_by_username),\n+        patch(\"langflow.services.auth.utils.get_password_hash\", mock_get_password_hash),\n+        patch(\"langflow.services.database.models.user.model.User\") as mock_user_class,\n+    ):\n+        # Configure the User class mock to return our mock_user when instantiated\n+        mock_user_class.return_value = mock_user\n+\n+        result = await create_super_user(\"testuser\", \"password\", mock_session)\n+\n+    # Verify that the function handled the race condition correctly\n+    assert result == mock_user\n+    assert mock_session.add.call_count == 1  # User was added to session\n+    assert mock_session.commit.call_count == 1  # Commit was attempted once (and failed)\n+    assert mock_session.rollback.call_count == 1  # Session was rolled back after IntegrityError\n+    assert mock_get_user_by_username.call_count == 2  # Called twice: initial check + after rollback\n+\n+\n+@pytest.mark.asyncio\n+async def test_create_super_user_race_condition_no_user_found():\n+    \"\"\"Test that create_super_user re-raises exception if no user is found after IntegrityError.\"\"\"\n+    # Mock the database session\n+    mock_session = AsyncMock()\n+\n+    # Mock get_user_by_username to always return None (even after rollback)\n+    mock_get_user_by_username = AsyncMock()\n+    mock_get_user_by_username.side_effect = [None, None]  # None for initial check and after rollback\n+\n+    # Mock other dependencies\n+    mock_get_password_hash = MagicMock(return_value=\"hashed_password\")\n+    mock_user = MagicMock(spec=User)\n+\n+    # Set up scenario where IntegrityError occurs but no user is found afterward\n+    integrity_error = IntegrityError(\"statement\", \"params\", \"orig\")\n+    mock_session.commit.side_effect = integrity_error\n+\n+    with (\n+        patch(\"langflow.services.auth.utils.get_user_by_username\", mock_get_user_by_username),\n+        patch(\"langflow.services.auth.utils.get_password_hash\", mock_get_password_hash),\n+        patch(\"langflow.services.database.models.user.model.User\", return_value=mock_user),\n+    ):\n+        with pytest.raises(IntegrityError):\n+            await create_super_user(\"testuser\", \"password\", mock_session)\n+\n+    # Verify rollback was called but exception was re-raised\n+    assert mock_session.rollback.call_count == 1\n+    assert mock_get_user_by_username.call_count == 2  # Initial + after rollback\n+\n+\n+@pytest.mark.asyncio\n+async def test_create_super_user_concurrent_workers():\n+    \"\"\"Test multiple concurrent calls to create_super_user with the same username.\"\"\"\n+    # This would require a real database to properly test, but we can simulate\n+    # the behavior with mocks to verify the logic works correctly\n+\n+    mock_session1 = AsyncMock()\n+    mock_session2 = AsyncMock()\n+\n+    # Create mock users\n+    mock_user = MagicMock(spec=User)\n+    mock_user.username = \"admin\"\n+    mock_user.is_superuser = True\n+\n+    mock_get_user_by_username = AsyncMock()\n+\n+    # Worker 1 succeeds, Worker 2 gets IntegrityError then finds existing user\n+    mock_session1.commit.return_value = None  # Success\n+    mock_session2.commit.side_effect = IntegrityError(\"statement\", \"params\", \"orig\")  # Race condition\n+\n+    # get_user_by_username returns None initially, then the created user for worker 2\n+    mock_get_user_by_username.side_effect = [None, None, mock_user]\n+\n+    with patch(\"langflow.services.auth.utils.get_user_by_username\", mock_get_user_by_username):\n+        # Simulate concurrent execution\n+        result1 = await create_super_user(\"admin\", \"password\", mock_session1)\n+        result2 = await create_super_user(\"admin\", \"password\", mock_session2)",
        "comment_created_at": "2025-07-15T16:22:37+00:00",
        "comment_author": "ogabrielluiz",
        "comment_body": "What if we create both tasks and call `asyncio.gather`? That might be closer to running two workers.",
        "pr_file_module": null
      }
    ]
  }
]