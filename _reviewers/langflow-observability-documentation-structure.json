[
  {
    "discussion_id": "2240846185",
    "pr_number": 9226,
    "pr_file": "docs/docs/Deployment/deployment-prod-best-practices.mdx",
    "created_at": "2025-07-29T20:04:11+00:00",
    "commented_code": "* **Resource allocation**\n   * **Optimized resource usage and cost efficiency**: By separating the two environments, you can allocate resources more effectively. Each flow can be deployed independently, providing fine-grained resource control.\n   * **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.\n+\n+## Scaling Resources\n+\n+Langflow's resource requirements vary depending on whether you're deploying the IDE for development or the runtime for production flows. Below are detailed strategies for scaling RAM, disk, instances, and users per instance.\n+\n+**IDE vs. Runtime**:\n+\n+* **IDE**: Deploy for developers using the UI. Requires frontend (512Mi RAM, 0.3 CPU) and backend (1Gi RAM, 0.5 CPU) services.\n+* **Runtime**: Deploy for production flows. Headless, requiring 2Gi RAM and 1 CPU per instance, focused on API endpoints.\n+\n+### Example Resource Configuration\n+\n+| Component | RAM Request | CPU Request | Replica Count | Notes |\n+|-----------|-------------|-------------|---------------|-------|\n+| IDE Backend | 1Gi | 0.5 | 1 | Scale replicas for more developers. |\n+| IDE Frontend | 512Mi | 0.3 | 1 | Adjust based on UI load. |\n+| Runtime | 2Gi | 1000m | 3 | Use HPA for dynamic scaling. |\n+| PostgreSQL | 4Gi | 2 | 1+ | Use replication for high availability. |\n+\n+### RAM\n+\n+**Overview**: RAM usage in Langflow depends on flow complexity, the size of language models, and concurrent request volume. The runtime, used for production flows, has a baseline requirement, while the IDE (for developers) requires additional resources for the frontend and backend.\n+\n+**Base Requirements**:\n+\n+* **Runtime**: 2Gi per instance.\n+* **IDE Backend**: 1Gi per instance.\n+* **IDE Frontend**: 512Mi per instance.\n+\n+**Factors Affecting RAM Usage**:\n+\n+* **Flow Complexity**: Flows with many nodes or large datasets (e.g., RAG pipelines) increase memory needs.\n+* **Language Models**: Larger LLMs or embeddings loaded in-memory consume significant RAM.\n+* **Concurrent Requests**: More simultaneous API calls or UI sessions require additional memory.\n+\n+**Scaling Recommendations**:\n+\n+* Start with 2Gi for runtime instances and 1.5Gi total (1Gi backend + 512Mi frontend) for IDE instances.\n+* Monitor memory usage with tools like Prometheus to identify bottlenecks.\n+* Adjust memory requests and limits in Kubernetes. For example:\n+\n+    ```yaml\n+    resources:\n+      requests:\n+        memory: \"2Gi\"\n+        cpu: \"1000m\"\n+      limits:\n+        memory: \"4Gi\"\n+        cpu: \"2000m\"\n+    ```\n+\n+* For intensive use (e.g., multi-core CPU setups), allocate 4Gi or more per instance.\n+\n+### Disk\n+\n+**Overview**: Disk storage is used for the SQLite or PostgreSQL database and file storage for large files, such as documents for RAG. The database stores flow configurations, user data, and settings, while files are managed on disk.\n+\n+**Usage**:\n+\n+* **Database**: Stores flow definitions, user profiles, and logs. Size depends on the number of flows and users.\n+* **File Storage**: Large files are stored in directories like `/opt/langflow/data/` or `.cache/langflow/`.\n+\n+**Scaling Recommendations**:\n+\n+* Use an external PostgreSQL database for production to improve scalability and reliability.\n+* Configure shared file storage such as NFS or cloud storage for multi-instance setups to ensure file access across replicas.\n+* Estimate initial database size based on expected flows and users, such as 10GB for moderate use, and monitor growth.\n+* Use Persistent Volumes in Kubernetes for database and file storage, with dynamic provisioning for scalability.\n+\n+### Instances\n+\n+**Overview**: Scaling instances involves adding more replicas to handle increased load, applicable to both IDE and runtime deployments. Horizontal scaling is preferred for production environments.\n+\n+**Base Configuration**:\n+\n+* **Runtime**: Default replica count of 3.\n+* **IDE**: Default replica count of 1 for both backend and frontend.\n+\n+**Scaling Recommendations**:\n+\n+* Implement Horizontal Pod Autoscaler (HPA) in Kubernetes to dynamically adjust replicas based on CPU or memory usage. Example HPA configuration:\n+\n+    ```yaml\n+    apiVersion: autoscaling/v2\n+    kind: HorizontalPodAutoscaler\n+    metadata:\n+      name: langflow-runtime-hpa\n+    spec:\n+      scaleTargetRef:\n+        apiVersion: apps/v1\n+        kind: Deployment\n+        name: langflow-runtime\n+      minReplicas: 1\n+      maxReplicas: 10\n+      metrics:\n+      - type: Resource\n+        resource:\n+          name: cpu\n+          target:\n+            type: Utilization\n+            averageUtilization: 80\n+    ```\n+\n+* For bursty workloads, such as 100,000s of tokens per second, ensure sufficient replicas to handle spikes.\n+* Vertically scale by increasing CPU/memory requests for complex flows, but prioritize horizontal scaling for reliability.\n+\n+### Users per Instance\n+\n+**Overview**: The number of users or requests per instance depends on whether you're supporting developers via the IDE or API clients via the runtime.\n+\n+**UI (IDE)**:\n+\n+* Each developer using the UI generates requests to the backend, requiring resources for session handling.\n+* Scale backend replicas based on concurrent developers (e.g., 1 replica per 10-20 developers, adjusted via load testing).\n+\n+**Production Flows (Runtime)**:\n+\n+* Users are typically API clients making requests to flow endpoints.\n+* Capacity depends on request rate and flow complexity. For example, a single instance with 2vCPUs and 2GB RAM can handle ~30 concurrent connections for simple flows.\n+* Perform load testing to determine the number of requests per instance and scale replicas accordingly.\n+\n+**Scaling Recommendations**:\n+\n+* Use load balancers to distribute requests across replicas.\n+* Monitor API request rates and response times to adjust replica counts.\n+* For IDE, ensure frontend and backend replicas are balanced to avoid bottlenecks.\n+\n+## Failure Points\n+\n+Langflow's reliability in production depends on mitigating key failure points, particularly around the database, file system, and instance availability.\n+\n+**Database Failure**:\n+\n+* **Impact**: Disrupts flow retrieval, saving, user authentication, user management, project collection access, configuration updates, and log writing.\n+* **Mitigation**: Use a replicated PostgreSQL setup with high availability and regular backups. Flows already loaded in memory may continue to function.\n+\n+**File System Issues**:\n+\n+* **Impact**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.\n+* **Mitigation**: Use a shared, POSIX-compliant file system or cloud storage. Avoid ramdisk solutions due to data loss on container shutdown.\n+\n+**Instance Failures**:\n+\n+* **Impact**: A single instance failure can disrupt service if not replicated.\n+* **Mitigation**: Deploy multiple replicas with Kubernetes to ensure availability. Use health checks to detect and replace failed pods.\n+\n+**Network and Dependency Failures**:",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2240846185",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9226,
        "pr_file": "docs/docs/Deployment/deployment-prod-best-practices.mdx",
        "discussion_id": "2240846185",
        "commented_code": "@@ -57,3 +57,223 @@ This separation is designed to enhance security, optimize resource allocation, a\n * **Resource allocation**\n   * **Optimized resource usage and cost efficiency**: By separating the two environments, you can allocate resources more effectively. Each flow can be deployed independently, providing fine-grained resource control.\n   * **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.\n+\n+## Scaling Resources\n+\n+Langflow's resource requirements vary depending on whether you're deploying the IDE for development or the runtime for production flows. Below are detailed strategies for scaling RAM, disk, instances, and users per instance.\n+\n+**IDE vs. Runtime**:\n+\n+* **IDE**: Deploy for developers using the UI. Requires frontend (512Mi RAM, 0.3 CPU) and backend (1Gi RAM, 0.5 CPU) services.\n+* **Runtime**: Deploy for production flows. Headless, requiring 2Gi RAM and 1 CPU per instance, focused on API endpoints.\n+\n+### Example Resource Configuration\n+\n+| Component | RAM Request | CPU Request | Replica Count | Notes |\n+|-----------|-------------|-------------|---------------|-------|\n+| IDE Backend | 1Gi | 0.5 | 1 | Scale replicas for more developers. |\n+| IDE Frontend | 512Mi | 0.3 | 1 | Adjust based on UI load. |\n+| Runtime | 2Gi | 1000m | 3 | Use HPA for dynamic scaling. |\n+| PostgreSQL | 4Gi | 2 | 1+ | Use replication for high availability. |\n+\n+### RAM\n+\n+**Overview**: RAM usage in Langflow depends on flow complexity, the size of language models, and concurrent request volume. The runtime, used for production flows, has a baseline requirement, while the IDE (for developers) requires additional resources for the frontend and backend.\n+\n+**Base Requirements**:\n+\n+* **Runtime**: 2Gi per instance.\n+* **IDE Backend**: 1Gi per instance.\n+* **IDE Frontend**: 512Mi per instance.\n+\n+**Factors Affecting RAM Usage**:\n+\n+* **Flow Complexity**: Flows with many nodes or large datasets (e.g., RAG pipelines) increase memory needs.\n+* **Language Models**: Larger LLMs or embeddings loaded in-memory consume significant RAM.\n+* **Concurrent Requests**: More simultaneous API calls or UI sessions require additional memory.\n+\n+**Scaling Recommendations**:\n+\n+* Start with 2Gi for runtime instances and 1.5Gi total (1Gi backend + 512Mi frontend) for IDE instances.\n+* Monitor memory usage with tools like Prometheus to identify bottlenecks.\n+* Adjust memory requests and limits in Kubernetes. For example:\n+\n+    ```yaml\n+    resources:\n+      requests:\n+        memory: \"2Gi\"\n+        cpu: \"1000m\"\n+      limits:\n+        memory: \"4Gi\"\n+        cpu: \"2000m\"\n+    ```\n+\n+* For intensive use (e.g., multi-core CPU setups), allocate 4Gi or more per instance.\n+\n+### Disk\n+\n+**Overview**: Disk storage is used for the SQLite or PostgreSQL database and file storage for large files, such as documents for RAG. The database stores flow configurations, user data, and settings, while files are managed on disk.\n+\n+**Usage**:\n+\n+* **Database**: Stores flow definitions, user profiles, and logs. Size depends on the number of flows and users.\n+* **File Storage**: Large files are stored in directories like `/opt/langflow/data/` or `.cache/langflow/`.\n+\n+**Scaling Recommendations**:\n+\n+* Use an external PostgreSQL database for production to improve scalability and reliability.\n+* Configure shared file storage such as NFS or cloud storage for multi-instance setups to ensure file access across replicas.\n+* Estimate initial database size based on expected flows and users, such as 10GB for moderate use, and monitor growth.\n+* Use Persistent Volumes in Kubernetes for database and file storage, with dynamic provisioning for scalability.\n+\n+### Instances\n+\n+**Overview**: Scaling instances involves adding more replicas to handle increased load, applicable to both IDE and runtime deployments. Horizontal scaling is preferred for production environments.\n+\n+**Base Configuration**:\n+\n+* **Runtime**: Default replica count of 3.\n+* **IDE**: Default replica count of 1 for both backend and frontend.\n+\n+**Scaling Recommendations**:\n+\n+* Implement Horizontal Pod Autoscaler (HPA) in Kubernetes to dynamically adjust replicas based on CPU or memory usage. Example HPA configuration:\n+\n+    ```yaml\n+    apiVersion: autoscaling/v2\n+    kind: HorizontalPodAutoscaler\n+    metadata:\n+      name: langflow-runtime-hpa\n+    spec:\n+      scaleTargetRef:\n+        apiVersion: apps/v1\n+        kind: Deployment\n+        name: langflow-runtime\n+      minReplicas: 1\n+      maxReplicas: 10\n+      metrics:\n+      - type: Resource\n+        resource:\n+          name: cpu\n+          target:\n+            type: Utilization\n+            averageUtilization: 80\n+    ```\n+\n+* For bursty workloads, such as 100,000s of tokens per second, ensure sufficient replicas to handle spikes.\n+* Vertically scale by increasing CPU/memory requests for complex flows, but prioritize horizontal scaling for reliability.\n+\n+### Users per Instance\n+\n+**Overview**: The number of users or requests per instance depends on whether you're supporting developers via the IDE or API clients via the runtime.\n+\n+**UI (IDE)**:\n+\n+* Each developer using the UI generates requests to the backend, requiring resources for session handling.\n+* Scale backend replicas based on concurrent developers (e.g., 1 replica per 10-20 developers, adjusted via load testing).\n+\n+**Production Flows (Runtime)**:\n+\n+* Users are typically API clients making requests to flow endpoints.\n+* Capacity depends on request rate and flow complexity. For example, a single instance with 2vCPUs and 2GB RAM can handle ~30 concurrent connections for simple flows.\n+* Perform load testing to determine the number of requests per instance and scale replicas accordingly.\n+\n+**Scaling Recommendations**:\n+\n+* Use load balancers to distribute requests across replicas.\n+* Monitor API request rates and response times to adjust replica counts.\n+* For IDE, ensure frontend and backend replicas are balanced to avoid bottlenecks.\n+\n+## Failure Points\n+\n+Langflow's reliability in production depends on mitigating key failure points, particularly around the database, file system, and instance availability.\n+\n+**Database Failure**:\n+\n+* **Impact**: Disrupts flow retrieval, saving, user authentication, user management, project collection access, configuration updates, and log writing.\n+* **Mitigation**: Use a replicated PostgreSQL setup with high availability and regular backups. Flows already loaded in memory may continue to function.\n+\n+**File System Issues**:\n+\n+* **Impact**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.\n+* **Mitigation**: Use a shared, POSIX-compliant file system or cloud storage. Avoid ramdisk solutions due to data loss on container shutdown.\n+\n+**Instance Failures**:\n+\n+* **Impact**: A single instance failure can disrupt service if not replicated.\n+* **Mitigation**: Deploy multiple replicas with Kubernetes to ensure availability. Use health checks to detect and replace failed pods.\n+\n+**Network and Dependency Failures**:",
        "comment_created_at": "2025-07-29T20:04:11+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\n### Database failure\r\n\r\n* **Impact**: Disrupts flow retrieval, saving, user authentication, user management, project collection access, configuration updates, and log writing.\r\n* **Mitigation**: Use a replicated PostgreSQL setup with high availability and regular backups. Flows already loaded in memory may continue to function.\r\n\r\n### File system issues\r\n\r\n* **Impact**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.\r\n* **Mitigation**: Use a shared, POSIX-compliant file system or cloud storage. Avoid ramdisk solutions due to data loss on container shutdown.\r\n\r\n### Instance failures\r\n\r\n* **Impact**: A single instance failure can disrupt service if not replicated.\r\n* **Mitigation**: Deploy multiple replicas with Kubernetes to ensure availability. Use health checks to detect and replace failed pods.\r\n\r\n### Network and dependency failures\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2240849904",
    "pr_number": 9226,
    "pr_file": "docs/docs/Deployment/deployment-prod-best-practices.mdx",
    "created_at": "2025-07-29T20:05:11+00:00",
    "commented_code": "* **Resource allocation**\n   * **Optimized resource usage and cost efficiency**: By separating the two environments, you can allocate resources more effectively. Each flow can be deployed independently, providing fine-grained resource control.\n   * **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.\n+\n+## Scaling Resources\n+\n+Langflow's resource requirements vary depending on whether you're deploying the IDE for development or the runtime for production flows. Below are detailed strategies for scaling RAM, disk, instances, and users per instance.\n+\n+**IDE vs. Runtime**:\n+\n+* **IDE**: Deploy for developers using the UI. Requires frontend (512Mi RAM, 0.3 CPU) and backend (1Gi RAM, 0.5 CPU) services.\n+* **Runtime**: Deploy for production flows. Headless, requiring 2Gi RAM and 1 CPU per instance, focused on API endpoints.\n+\n+### Example Resource Configuration\n+\n+| Component | RAM Request | CPU Request | Replica Count | Notes |\n+|-----------|-------------|-------------|---------------|-------|\n+| IDE Backend | 1Gi | 0.5 | 1 | Scale replicas for more developers. |\n+| IDE Frontend | 512Mi | 0.3 | 1 | Adjust based on UI load. |\n+| Runtime | 2Gi | 1000m | 3 | Use HPA for dynamic scaling. |\n+| PostgreSQL | 4Gi | 2 | 1+ | Use replication for high availability. |\n+\n+### RAM\n+\n+**Overview**: RAM usage in Langflow depends on flow complexity, the size of language models, and concurrent request volume. The runtime, used for production flows, has a baseline requirement, while the IDE (for developers) requires additional resources for the frontend and backend.\n+\n+**Base Requirements**:\n+\n+* **Runtime**: 2Gi per instance.\n+* **IDE Backend**: 1Gi per instance.\n+* **IDE Frontend**: 512Mi per instance.\n+\n+**Factors Affecting RAM Usage**:\n+\n+* **Flow Complexity**: Flows with many nodes or large datasets (e.g., RAG pipelines) increase memory needs.\n+* **Language Models**: Larger LLMs or embeddings loaded in-memory consume significant RAM.\n+* **Concurrent Requests**: More simultaneous API calls or UI sessions require additional memory.\n+\n+**Scaling Recommendations**:\n+\n+* Start with 2Gi for runtime instances and 1.5Gi total (1Gi backend + 512Mi frontend) for IDE instances.\n+* Monitor memory usage with tools like Prometheus to identify bottlenecks.\n+* Adjust memory requests and limits in Kubernetes. For example:\n+\n+    ```yaml\n+    resources:\n+      requests:\n+        memory: \"2Gi\"\n+        cpu: \"1000m\"\n+      limits:\n+        memory: \"4Gi\"\n+        cpu: \"2000m\"\n+    ```\n+\n+* For intensive use (e.g., multi-core CPU setups), allocate 4Gi or more per instance.\n+\n+### Disk\n+\n+**Overview**: Disk storage is used for the SQLite or PostgreSQL database and file storage for large files, such as documents for RAG. The database stores flow configurations, user data, and settings, while files are managed on disk.\n+\n+**Usage**:\n+\n+* **Database**: Stores flow definitions, user profiles, and logs. Size depends on the number of flows and users.\n+* **File Storage**: Large files are stored in directories like `/opt/langflow/data/` or `.cache/langflow/`.\n+\n+**Scaling Recommendations**:\n+\n+* Use an external PostgreSQL database for production to improve scalability and reliability.\n+* Configure shared file storage such as NFS or cloud storage for multi-instance setups to ensure file access across replicas.\n+* Estimate initial database size based on expected flows and users, such as 10GB for moderate use, and monitor growth.\n+* Use Persistent Volumes in Kubernetes for database and file storage, with dynamic provisioning for scalability.\n+\n+### Instances\n+\n+**Overview**: Scaling instances involves adding more replicas to handle increased load, applicable to both IDE and runtime deployments. Horizontal scaling is preferred for production environments.\n+\n+**Base Configuration**:\n+\n+* **Runtime**: Default replica count of 3.\n+* **IDE**: Default replica count of 1 for both backend and frontend.\n+\n+**Scaling Recommendations**:\n+\n+* Implement Horizontal Pod Autoscaler (HPA) in Kubernetes to dynamically adjust replicas based on CPU or memory usage. Example HPA configuration:\n+\n+    ```yaml\n+    apiVersion: autoscaling/v2\n+    kind: HorizontalPodAutoscaler\n+    metadata:\n+      name: langflow-runtime-hpa\n+    spec:\n+      scaleTargetRef:\n+        apiVersion: apps/v1\n+        kind: Deployment\n+        name: langflow-runtime\n+      minReplicas: 1\n+      maxReplicas: 10\n+      metrics:\n+      - type: Resource\n+        resource:\n+          name: cpu\n+          target:\n+            type: Utilization\n+            averageUtilization: 80\n+    ```\n+\n+* For bursty workloads, such as 100,000s of tokens per second, ensure sufficient replicas to handle spikes.\n+* Vertically scale by increasing CPU/memory requests for complex flows, but prioritize horizontal scaling for reliability.\n+\n+### Users per Instance\n+\n+**Overview**: The number of users or requests per instance depends on whether you're supporting developers via the IDE or API clients via the runtime.\n+\n+**UI (IDE)**:\n+\n+* Each developer using the UI generates requests to the backend, requiring resources for session handling.\n+* Scale backend replicas based on concurrent developers (e.g., 1 replica per 10-20 developers, adjusted via load testing).\n+\n+**Production Flows (Runtime)**:\n+\n+* Users are typically API clients making requests to flow endpoints.\n+* Capacity depends on request rate and flow complexity. For example, a single instance with 2vCPUs and 2GB RAM can handle ~30 concurrent connections for simple flows.\n+* Perform load testing to determine the number of requests per instance and scale replicas accordingly.\n+\n+**Scaling Recommendations**:\n+\n+* Use load balancers to distribute requests across replicas.\n+* Monitor API request rates and response times to adjust replica counts.\n+* For IDE, ensure frontend and backend replicas are balanced to avoid bottlenecks.\n+\n+## Failure Points\n+\n+Langflow's reliability in production depends on mitigating key failure points, particularly around the database, file system, and instance availability.\n+\n+**Database Failure**:\n+\n+* **Impact**: Disrupts flow retrieval, saving, user authentication, user management, project collection access, configuration updates, and log writing.\n+* **Mitigation**: Use a replicated PostgreSQL setup with high availability and regular backups. Flows already loaded in memory may continue to function.\n+\n+**File System Issues**:\n+\n+* **Impact**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.\n+* **Mitigation**: Use a shared, POSIX-compliant file system or cloud storage. Avoid ramdisk solutions due to data loss on container shutdown.\n+\n+**Instance Failures**:\n+\n+* **Impact**: A single instance failure can disrupt service if not replicated.\n+* **Mitigation**: Deploy multiple replicas with Kubernetes to ensure availability. Use health checks to detect and replace failed pods.\n+\n+**Network and Dependency Failures**:\n+\n+* **Impact**: External APIs or services used in flows may fail, causing flow errors.\n+* **Mitigation**: Implement retry logic and error handling in flows. Monitor network latency and dependency health.\n+\n+## Monitoring Recommendations\n+\n+Effective monitoring ensures Langflow operates reliably and performs well under varying loads.\n+\n+**Database Health**:\n+\n+* Monitor availability, query performance, and resource usage (CPU, memory, disk).\n+* Use tools like pgAdmin or cloud-native monitoring for PostgreSQL.\n+\n+**Application Logs**:\n+\n+* Collect and analyze logs for errors, warnings, and flow execution issues.\n+* Centralize logs using tools like ELK Stack or Fluentd.\n+\n+**Resource Usage**:\n+\n+* Track CPU, memory, and disk usage of Langflow instances.\n+* Use Prometheus and Grafana for real-time monitoring in Kubernetes.\n+\n+**API Performance**:\n+\n+* Monitor response times, error rates, and request throughput.\n+* Set alerts for high latency or error spikes.\n+\n+**Observability Tools**:\n+\n+* Integrate with LangSmith or LangFuse for detailed flow tracing and metrics.\n+* Use these tools to debug flow performance and optimize execution.\n+\n+**Example Monitoring Setup**:\n+\n+* Deploy Prometheus for metrics collection.",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2240849904",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9226,
        "pr_file": "docs/docs/Deployment/deployment-prod-best-practices.mdx",
        "discussion_id": "2240849904",
        "commented_code": "@@ -57,3 +57,223 @@ This separation is designed to enhance security, optimize resource allocation, a\n * **Resource allocation**\n   * **Optimized resource usage and cost efficiency**: By separating the two environments, you can allocate resources more effectively. Each flow can be deployed independently, providing fine-grained resource control.\n   * **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.\n+\n+## Scaling Resources\n+\n+Langflow's resource requirements vary depending on whether you're deploying the IDE for development or the runtime for production flows. Below are detailed strategies for scaling RAM, disk, instances, and users per instance.\n+\n+**IDE vs. Runtime**:\n+\n+* **IDE**: Deploy for developers using the UI. Requires frontend (512Mi RAM, 0.3 CPU) and backend (1Gi RAM, 0.5 CPU) services.\n+* **Runtime**: Deploy for production flows. Headless, requiring 2Gi RAM and 1 CPU per instance, focused on API endpoints.\n+\n+### Example Resource Configuration\n+\n+| Component | RAM Request | CPU Request | Replica Count | Notes |\n+|-----------|-------------|-------------|---------------|-------|\n+| IDE Backend | 1Gi | 0.5 | 1 | Scale replicas for more developers. |\n+| IDE Frontend | 512Mi | 0.3 | 1 | Adjust based on UI load. |\n+| Runtime | 2Gi | 1000m | 3 | Use HPA for dynamic scaling. |\n+| PostgreSQL | 4Gi | 2 | 1+ | Use replication for high availability. |\n+\n+### RAM\n+\n+**Overview**: RAM usage in Langflow depends on flow complexity, the size of language models, and concurrent request volume. The runtime, used for production flows, has a baseline requirement, while the IDE (for developers) requires additional resources for the frontend and backend.\n+\n+**Base Requirements**:\n+\n+* **Runtime**: 2Gi per instance.\n+* **IDE Backend**: 1Gi per instance.\n+* **IDE Frontend**: 512Mi per instance.\n+\n+**Factors Affecting RAM Usage**:\n+\n+* **Flow Complexity**: Flows with many nodes or large datasets (e.g., RAG pipelines) increase memory needs.\n+* **Language Models**: Larger LLMs or embeddings loaded in-memory consume significant RAM.\n+* **Concurrent Requests**: More simultaneous API calls or UI sessions require additional memory.\n+\n+**Scaling Recommendations**:\n+\n+* Start with 2Gi for runtime instances and 1.5Gi total (1Gi backend + 512Mi frontend) for IDE instances.\n+* Monitor memory usage with tools like Prometheus to identify bottlenecks.\n+* Adjust memory requests and limits in Kubernetes. For example:\n+\n+    ```yaml\n+    resources:\n+      requests:\n+        memory: \"2Gi\"\n+        cpu: \"1000m\"\n+      limits:\n+        memory: \"4Gi\"\n+        cpu: \"2000m\"\n+    ```\n+\n+* For intensive use (e.g., multi-core CPU setups), allocate 4Gi or more per instance.\n+\n+### Disk\n+\n+**Overview**: Disk storage is used for the SQLite or PostgreSQL database and file storage for large files, such as documents for RAG. The database stores flow configurations, user data, and settings, while files are managed on disk.\n+\n+**Usage**:\n+\n+* **Database**: Stores flow definitions, user profiles, and logs. Size depends on the number of flows and users.\n+* **File Storage**: Large files are stored in directories like `/opt/langflow/data/` or `.cache/langflow/`.\n+\n+**Scaling Recommendations**:\n+\n+* Use an external PostgreSQL database for production to improve scalability and reliability.\n+* Configure shared file storage such as NFS or cloud storage for multi-instance setups to ensure file access across replicas.\n+* Estimate initial database size based on expected flows and users, such as 10GB for moderate use, and monitor growth.\n+* Use Persistent Volumes in Kubernetes for database and file storage, with dynamic provisioning for scalability.\n+\n+### Instances\n+\n+**Overview**: Scaling instances involves adding more replicas to handle increased load, applicable to both IDE and runtime deployments. Horizontal scaling is preferred for production environments.\n+\n+**Base Configuration**:\n+\n+* **Runtime**: Default replica count of 3.\n+* **IDE**: Default replica count of 1 for both backend and frontend.\n+\n+**Scaling Recommendations**:\n+\n+* Implement Horizontal Pod Autoscaler (HPA) in Kubernetes to dynamically adjust replicas based on CPU or memory usage. Example HPA configuration:\n+\n+    ```yaml\n+    apiVersion: autoscaling/v2\n+    kind: HorizontalPodAutoscaler\n+    metadata:\n+      name: langflow-runtime-hpa\n+    spec:\n+      scaleTargetRef:\n+        apiVersion: apps/v1\n+        kind: Deployment\n+        name: langflow-runtime\n+      minReplicas: 1\n+      maxReplicas: 10\n+      metrics:\n+      - type: Resource\n+        resource:\n+          name: cpu\n+          target:\n+            type: Utilization\n+            averageUtilization: 80\n+    ```\n+\n+* For bursty workloads, such as 100,000s of tokens per second, ensure sufficient replicas to handle spikes.\n+* Vertically scale by increasing CPU/memory requests for complex flows, but prioritize horizontal scaling for reliability.\n+\n+### Users per Instance\n+\n+**Overview**: The number of users or requests per instance depends on whether you're supporting developers via the IDE or API clients via the runtime.\n+\n+**UI (IDE)**:\n+\n+* Each developer using the UI generates requests to the backend, requiring resources for session handling.\n+* Scale backend replicas based on concurrent developers (e.g., 1 replica per 10-20 developers, adjusted via load testing).\n+\n+**Production Flows (Runtime)**:\n+\n+* Users are typically API clients making requests to flow endpoints.\n+* Capacity depends on request rate and flow complexity. For example, a single instance with 2vCPUs and 2GB RAM can handle ~30 concurrent connections for simple flows.\n+* Perform load testing to determine the number of requests per instance and scale replicas accordingly.\n+\n+**Scaling Recommendations**:\n+\n+* Use load balancers to distribute requests across replicas.\n+* Monitor API request rates and response times to adjust replica counts.\n+* For IDE, ensure frontend and backend replicas are balanced to avoid bottlenecks.\n+\n+## Failure Points\n+\n+Langflow's reliability in production depends on mitigating key failure points, particularly around the database, file system, and instance availability.\n+\n+**Database Failure**:\n+\n+* **Impact**: Disrupts flow retrieval, saving, user authentication, user management, project collection access, configuration updates, and log writing.\n+* **Mitigation**: Use a replicated PostgreSQL setup with high availability and regular backups. Flows already loaded in memory may continue to function.\n+\n+**File System Issues**:\n+\n+* **Impact**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.\n+* **Mitigation**: Use a shared, POSIX-compliant file system or cloud storage. Avoid ramdisk solutions due to data loss on container shutdown.\n+\n+**Instance Failures**:\n+\n+* **Impact**: A single instance failure can disrupt service if not replicated.\n+* **Mitigation**: Deploy multiple replicas with Kubernetes to ensure availability. Use health checks to detect and replace failed pods.\n+\n+**Network and Dependency Failures**:\n+\n+* **Impact**: External APIs or services used in flows may fail, causing flow errors.\n+* **Mitigation**: Implement retry logic and error handling in flows. Monitor network latency and dependency health.\n+\n+## Monitoring Recommendations\n+\n+Effective monitoring ensures Langflow operates reliably and performs well under varying loads.\n+\n+**Database Health**:\n+\n+* Monitor availability, query performance, and resource usage (CPU, memory, disk).\n+* Use tools like pgAdmin or cloud-native monitoring for PostgreSQL.\n+\n+**Application Logs**:\n+\n+* Collect and analyze logs for errors, warnings, and flow execution issues.\n+* Centralize logs using tools like ELK Stack or Fluentd.\n+\n+**Resource Usage**:\n+\n+* Track CPU, memory, and disk usage of Langflow instances.\n+* Use Prometheus and Grafana for real-time monitoring in Kubernetes.\n+\n+**API Performance**:\n+\n+* Monitor response times, error rates, and request throughput.\n+* Set alerts for high latency or error spikes.\n+\n+**Observability Tools**:\n+\n+* Integrate with LangSmith or LangFuse for detailed flow tracing and metrics.\n+* Use these tools to debug flow performance and optimize execution.\n+\n+**Example Monitoring Setup**:\n+\n+* Deploy Prometheus for metrics collection.",
        "comment_created_at": "2025-07-29T20:05:11+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\n### Database health\r\n\r\n* Monitor availability, query performance, and resource usage (CPU, memory, disk).\r\n* Use tools like pgAdmin or cloud-native monitoring for PostgreSQL.\r\n\r\n### Application logs\r\n\r\n* Collect and analyze logs for errors, warnings, and flow execution issues.\r\n* Centralize logs using tools like ELK Stack or Fluentd.\r\n\r\n### Resource usage\r\n\r\n* Track CPU, memory, and disk usage of Langflow instances.\r\n* Use Prometheus and Grafana for real-time monitoring in Kubernetes.\r\n\r\n### API performance\r\n\r\n* Monitor response times, error rates, and request throughput.\r\n* Set alerts for high latency or error spikes.\r\n\r\n### Observability tools\r\n\r\n* Integrate with LangSmith or LangFuse for detailed flow tracing and metrics.\r\n* Use these tools to debug flow performance and optimize execution.\r\n\r\n### Example monitoring setup\r\n\r\n* Deploy Prometheus for metrics collection.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2245969576",
    "pr_number": 9272,
    "pr_file": "docs/docs/Contributing/contributing-telemetry.mdx",
    "created_at": "2025-07-31T17:21:06+00:00",
    "commented_code": "## Data that Langflow collects\n \n-### Run {#2d427dca4f0148ae867997f6789e8bfb}\n+### Run",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2245969576",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9272,
        "pr_file": "docs/docs/Contributing/contributing-telemetry.mdx",
        "discussion_id": "2245969576",
        "commented_code": "@@ -13,18 +13,18 @@ To opt out of telemetry, set the\u00a0`LANGFLOW_DO_NOT_TRACK`\u00a0or\u00a0`DO_NOT_TRACK`\u00a0e\n \n ## Data that Langflow collects\n \n-### Run {#2d427dca4f0148ae867997f6789e8bfb}\n+### Run",
        "comment_created_at": "2025-07-31T17:21:06+00:00",
        "comment_author": "aimurphy",
        "comment_body": "```suggestion\r\nLangflow telemetry collects data on flow runs, your environment, and component usage.\r\n\r\n### Run\r\n```",
        "pr_file_module": null
      }
    ]
  }
]