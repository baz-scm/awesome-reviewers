[
  {
    "discussion_id": "2291892384",
    "pr_number": 9460,
    "pr_file": "src/backend/base/langflow/components/vlmrun/vlmrun_transcription.py",
    "created_at": "2025-08-21T18:57:32+00:00",
    "commented_code": "+from pathlib import Path\n+\n+from loguru import logger\n+\n+from langflow.custom.custom_component.component import Component\n+from langflow.io import (\n+    DropdownInput,\n+    FileInput,\n+    MessageTextInput,\n+    Output,\n+    SecretStrInput,\n+)\n+from langflow.schema.data import Data\n+\n+\n+class VLMRunTranscription(Component):\n+    display_name = \"VLM Run Transcription\"\n+    description = \"Extract structured data from audio and video using [VLM Run AI](https://app.vlm.run)\"\n+    documentation = \"https://docs.vlm.run\"\n+    icon = \"VLMRun\"\n+    beta = True\n+\n+    inputs = [\n+        SecretStrInput(\n+            name=\"api_key\",\n+            display_name=\"VLM Run API Key\",\n+            info=\"Get your API key from https://app.vlm.run\",\n+            required=True,\n+        ),\n+        DropdownInput(\n+            name=\"media_type\",\n+            display_name=\"Media Type\",\n+            options=[\"audio\", \"video\"],\n+            value=\"audio\",\n+            info=\"Select the type of media to process\",\n+        ),\n+        FileInput(\n+            name=\"media_files\",\n+            display_name=\"Media Files\",\n+            file_types=[\n+                \"mp3\",\n+                \"wav\",\n+                \"m4a\",\n+                \"flac\",\n+                \"ogg\",\n+                \"opus\",\n+                \"webm\",\n+                \"aac\",\n+                \"mp4\",\n+                \"mov\",\n+                \"avi\",\n+                \"mkv\",\n+                \"flv\",\n+                \"wmv\",\n+                \"m4v\",\n+            ],\n+            info=\"Upload one or more audio/video files\",\n+            required=False,\n+            is_list=True,\n+        ),\n+        MessageTextInput(\n+            name=\"media_url\",\n+            display_name=\"Media URL\",\n+            info=\"URL to media file (alternative to file upload)\",\n+            required=False,\n+            advanced=True,\n+        ),\n+    ]\n+\n+    outputs = [\n+        Output(\n+            display_name=\"Result\",\n+            name=\"result\",\n+            method=\"process_media\",\n+        ),\n+    ]\n+\n+    def _check_inputs(self) -> str | None:\n+        \"\"\"Validate that either media files or URL is provided.\"\"\"\n+        if not self.media_files and not self.media_url:\n+            return \"Either media files or media URL must be provided\"\n+        return None\n+\n+    def _import_vlmrun(self):\n+        \"\"\"Import and return VLMRun client class.\"\"\"\n+        try:\n+            from vlmrun.client import VLMRun\n+        except ImportError as e:\n+            error_msg = \"VLM Run SDK not installed. Run: pip install 'vlmrun[all]'\"\n+            raise ImportError(error_msg) from e\n+        else:\n+            return VLMRun\n+\n+    def _generate_media_response(self, client, media_source):\n+        \"\"\"Generate response for audio or video media.\"\"\"\n+        if self.media_type == \"audio\":\n+            if isinstance(media_source, Path):\n+                return client.audio.generate(file=media_source, domain=\"audio.transcription\", batch=True)\n+            return client.audio.generate(url=media_source, domain=\"audio.transcription\", batch=True)\n+        # video\n+        if isinstance(media_source, Path):\n+            return client.video.generate(file=media_source, domain=\"video.transcription\", batch=True)\n+        return client.video.generate(url=media_source, domain=\"video.transcription\", batch=True)\n+\n+    def _wait_for_response(self, client, response):\n+        \"\"\"Wait for batch processing to complete if needed.\"\"\"\n+        if hasattr(response, \"id\"):\n+            return client.predictions.wait(response.id, timeout=600)\n+        return response\n+\n+    def _extract_transcription(self, segments: list) -> list[str]:\n+        \"\"\"Extract transcription parts from segments.\"\"\"\n+        transcription_parts = []\n+        for segment in segments:\n+            if self.media_type == \"audio\" and \"audio\" in segment:\n+                transcription_parts.append(segment[\"audio\"].get(\"content\", \"\"))\n+            elif self.media_type == \"video\" and \"video\" in segment:\n+                transcription_parts.append(segment[\"video\"].get(\"content\", \"\"))\n+                # Also include audio if available for video\n+                if \"audio\" in segment:\n+                    audio_content = segment[\"audio\"].get(\"content\", \"\")\n+                    if audio_content and audio_content.strip():\n+                        transcription_parts.append(f\"[Audio: {audio_content}]\")\n+        return transcription_parts\n+\n+    def _create_result_dict(self, response, transcription_parts: list, source_name: str) -> dict:\n+        \"\"\"Create a standardized result dictionary.\"\"\"\n+        response_data = response.response if hasattr(response, \"response\") else {}\n+        result = {\n+            \"prediction_id\": response.id if hasattr(response, \"id\") else None,\n+            \"transcription\": \" \".join(transcription_parts),\n+            \"full_response\": response_data,\n+            \"metadata\": {\n+                \"media_type\": self.media_type,\n+                \"duration\": response_data.get(\"metadata\", {}).get(\"duration\", 0),\n+            },\n+            \"usage\": response.usage.__dict__ if hasattr(response, \"usage\") else None,\n+            \"status\": response.status if hasattr(response, \"status\") else \"completed\",\n+        }\n+\n+        # Add source-specific field\n+        if source_name.startswith(\"http\"):",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2291892384",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9460,
        "pr_file": "src/backend/base/langflow/components/vlmrun/vlmrun_transcription.py",
        "discussion_id": "2291892384",
        "commented_code": "@@ -0,0 +1,215 @@\n+from pathlib import Path\n+\n+from loguru import logger\n+\n+from langflow.custom.custom_component.component import Component\n+from langflow.io import (\n+    DropdownInput,\n+    FileInput,\n+    MessageTextInput,\n+    Output,\n+    SecretStrInput,\n+)\n+from langflow.schema.data import Data\n+\n+\n+class VLMRunTranscription(Component):\n+    display_name = \"VLM Run Transcription\"\n+    description = \"Extract structured data from audio and video using [VLM Run AI](https://app.vlm.run)\"\n+    documentation = \"https://docs.vlm.run\"\n+    icon = \"VLMRun\"\n+    beta = True\n+\n+    inputs = [\n+        SecretStrInput(\n+            name=\"api_key\",\n+            display_name=\"VLM Run API Key\",\n+            info=\"Get your API key from https://app.vlm.run\",\n+            required=True,\n+        ),\n+        DropdownInput(\n+            name=\"media_type\",\n+            display_name=\"Media Type\",\n+            options=[\"audio\", \"video\"],\n+            value=\"audio\",\n+            info=\"Select the type of media to process\",\n+        ),\n+        FileInput(\n+            name=\"media_files\",\n+            display_name=\"Media Files\",\n+            file_types=[\n+                \"mp3\",\n+                \"wav\",\n+                \"m4a\",\n+                \"flac\",\n+                \"ogg\",\n+                \"opus\",\n+                \"webm\",\n+                \"aac\",\n+                \"mp4\",\n+                \"mov\",\n+                \"avi\",\n+                \"mkv\",\n+                \"flv\",\n+                \"wmv\",\n+                \"m4v\",\n+            ],\n+            info=\"Upload one or more audio/video files\",\n+            required=False,\n+            is_list=True,\n+        ),\n+        MessageTextInput(\n+            name=\"media_url\",\n+            display_name=\"Media URL\",\n+            info=\"URL to media file (alternative to file upload)\",\n+            required=False,\n+            advanced=True,\n+        ),\n+    ]\n+\n+    outputs = [\n+        Output(\n+            display_name=\"Result\",\n+            name=\"result\",\n+            method=\"process_media\",\n+        ),\n+    ]\n+\n+    def _check_inputs(self) -> str | None:\n+        \"\"\"Validate that either media files or URL is provided.\"\"\"\n+        if not self.media_files and not self.media_url:\n+            return \"Either media files or media URL must be provided\"\n+        return None\n+\n+    def _import_vlmrun(self):\n+        \"\"\"Import and return VLMRun client class.\"\"\"\n+        try:\n+            from vlmrun.client import VLMRun\n+        except ImportError as e:\n+            error_msg = \"VLM Run SDK not installed. Run: pip install 'vlmrun[all]'\"\n+            raise ImportError(error_msg) from e\n+        else:\n+            return VLMRun\n+\n+    def _generate_media_response(self, client, media_source):\n+        \"\"\"Generate response for audio or video media.\"\"\"\n+        if self.media_type == \"audio\":\n+            if isinstance(media_source, Path):\n+                return client.audio.generate(file=media_source, domain=\"audio.transcription\", batch=True)\n+            return client.audio.generate(url=media_source, domain=\"audio.transcription\", batch=True)\n+        # video\n+        if isinstance(media_source, Path):\n+            return client.video.generate(file=media_source, domain=\"video.transcription\", batch=True)\n+        return client.video.generate(url=media_source, domain=\"video.transcription\", batch=True)\n+\n+    def _wait_for_response(self, client, response):\n+        \"\"\"Wait for batch processing to complete if needed.\"\"\"\n+        if hasattr(response, \"id\"):\n+            return client.predictions.wait(response.id, timeout=600)\n+        return response\n+\n+    def _extract_transcription(self, segments: list) -> list[str]:\n+        \"\"\"Extract transcription parts from segments.\"\"\"\n+        transcription_parts = []\n+        for segment in segments:\n+            if self.media_type == \"audio\" and \"audio\" in segment:\n+                transcription_parts.append(segment[\"audio\"].get(\"content\", \"\"))\n+            elif self.media_type == \"video\" and \"video\" in segment:\n+                transcription_parts.append(segment[\"video\"].get(\"content\", \"\"))\n+                # Also include audio if available for video\n+                if \"audio\" in segment:\n+                    audio_content = segment[\"audio\"].get(\"content\", \"\")\n+                    if audio_content and audio_content.strip():\n+                        transcription_parts.append(f\"[Audio: {audio_content}]\")\n+        return transcription_parts\n+\n+    def _create_result_dict(self, response, transcription_parts: list, source_name: str) -> dict:\n+        \"\"\"Create a standardized result dictionary.\"\"\"\n+        response_data = response.response if hasattr(response, \"response\") else {}\n+        result = {\n+            \"prediction_id\": response.id if hasattr(response, \"id\") else None,\n+            \"transcription\": \" \".join(transcription_parts),\n+            \"full_response\": response_data,\n+            \"metadata\": {\n+                \"media_type\": self.media_type,\n+                \"duration\": response_data.get(\"metadata\", {}).get(\"duration\", 0),\n+            },\n+            \"usage\": response.usage.__dict__ if hasattr(response, \"usage\") else None,\n+            \"status\": response.status if hasattr(response, \"status\") else \"completed\",\n+        }\n+\n+        # Add source-specific field\n+        if source_name.startswith(\"http\"):",
        "comment_created_at": "2025-08-21T18:57:32+00:00",
        "comment_author": "dineshreddy91",
        "comment_body": "Use a more robust URL check than source_name.startswith(\"http\") (e.g., urllib.parse.urlparse(source_name).scheme), supporting schemes like https, s3, gs.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2283647213",
    "pr_number": 9069,
    "pr_file": "src/backend/base/langflow/api/v1/openai_responses.py",
    "created_at": "2025-08-18T22:43:11+00:00",
    "commented_code": "+import asyncio\n+import json\n+import time\n+import uuid\n+from collections.abc import AsyncGenerator\n+from typing import Annotated\n+\n+from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Request\n+from fastapi.responses import StreamingResponse\n+from loguru import logger\n+\n+from langflow.api.v1.endpoints import consume_and_yield, run_flow_generator, simple_run_flow\n+from langflow.api.v1.schemas import SimplifiedAPIRequest\n+from langflow.events.event_manager import create_stream_tokens_event_manager\n+from langflow.helpers.flow import get_flow_by_id_or_endpoint_name\n+from langflow.schema.content_types import ToolContent\n+from langflow.schema.openai_responses_schemas import (\n+    OpenAIErrorResponse,\n+    OpenAIResponsesRequest,\n+    OpenAIResponsesResponse,\n+    OpenAIResponsesStreamChunk,\n+    create_openai_error,\n+)\n+from langflow.services.auth.utils import api_key_security\n+from langflow.services.database.models.flow.model import FlowRead\n+from langflow.services.database.models.user.model import UserRead\n+from langflow.services.deps import get_telemetry_service\n+from langflow.services.telemetry.schema import RunPayload\n+\n+router = APIRouter(tags=[\"OpenAI Responses API\"])\n+\n+\n+def has_chat_input(flow_data: dict | None) -> bool:\n+    \"\"\"Check if the flow has a chat input component.\"\"\"\n+    if not flow_data or \"nodes\" not in flow_data:\n+        return False\n+\n+    return any(node.get(\"data\", {}).get(\"type\") in [\"ChatInput\", \"Chat Input\"] for node in flow_data[\"nodes\"])",
    "repo_full_name": "langflow-ai/langflow",
    "discussion_comments": [
      {
        "comment_id": "2283647213",
        "repo_full_name": "langflow-ai/langflow",
        "pr_number": 9069,
        "pr_file": "src/backend/base/langflow/api/v1/openai_responses.py",
        "discussion_id": "2283647213",
        "commented_code": "@@ -0,0 +1,531 @@\n+import asyncio\n+import json\n+import time\n+import uuid\n+from collections.abc import AsyncGenerator\n+from typing import Annotated\n+\n+from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Request\n+from fastapi.responses import StreamingResponse\n+from loguru import logger\n+\n+from langflow.api.v1.endpoints import consume_and_yield, run_flow_generator, simple_run_flow\n+from langflow.api.v1.schemas import SimplifiedAPIRequest\n+from langflow.events.event_manager import create_stream_tokens_event_manager\n+from langflow.helpers.flow import get_flow_by_id_or_endpoint_name\n+from langflow.schema.content_types import ToolContent\n+from langflow.schema.openai_responses_schemas import (\n+    OpenAIErrorResponse,\n+    OpenAIResponsesRequest,\n+    OpenAIResponsesResponse,\n+    OpenAIResponsesStreamChunk,\n+    create_openai_error,\n+)\n+from langflow.services.auth.utils import api_key_security\n+from langflow.services.database.models.flow.model import FlowRead\n+from langflow.services.database.models.user.model import UserRead\n+from langflow.services.deps import get_telemetry_service\n+from langflow.services.telemetry.schema import RunPayload\n+\n+router = APIRouter(tags=[\"OpenAI Responses API\"])\n+\n+\n+def has_chat_input(flow_data: dict | None) -> bool:\n+    \"\"\"Check if the flow has a chat input component.\"\"\"\n+    if not flow_data or \"nodes\" not in flow_data:\n+        return False\n+\n+    return any(node.get(\"data\", {}).get(\"type\") in [\"ChatInput\", \"Chat Input\"] for node in flow_data[\"nodes\"])",
        "comment_created_at": "2025-08-18T22:43:11+00:00",
        "comment_author": "ogabrielluiz",
        "comment_body": "We may need to check for ChatOutput too because otherwise we might not know what to return",
        "pr_file_module": null
      }
    ]
  }
]