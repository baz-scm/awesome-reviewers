[
  {
    "discussion_id": "2268175352",
    "pr_number": 12992,
    "pr_file": "docs/my-website/docs/providers/heroku.md",
    "created_at": "2025-08-11T22:51:04+00:00",
    "commented_code": "+# Heroku\n+\n+## Provision a Model\n+\n+To use the Heroku provider for LiteLLM, you must first configure a Heroku app, and attach one of the models listed in the [Supported Models](#supported-models) section.\n+\n+To get configure a Heroku app with an attached model, please refer to [Heroku's documentation](https://devcenter.heroku.com/articles/heroku-inference).\n+\n+## Supported Models\n+\n+The Heroku provider for LiteLLM currently, only supports [chat](https://devcenter.heroku.com/articles/heroku-inference-api-v1-chat-completions). Supported chat models are:\n+\n+| Model                             | Region  |\n+|-----------------------------------|---------|\n+| [`heroku/claude-sonnet-4`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-4-sonnet)          | US, EU  |\n+| [`heroku/claude-3-7-sonnet`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-7-sonnet)        | US, EU  |\n+| [`heroku/claude-3-5-sonnet-latest`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-sonnet-latest) | US      |\n+| [`heroku/claude-3-5-haiku`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-haiku)         | US      |\n+| [`heroku/claude-3`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-haiku)                 | EU      |\n+\n+## Environment Variables\n+\n+When a model is attached to a Heroku app, three config variables are set:\n+\n+- `INFERENCE_KEY`: The API key used for authenticating requests to the model.\n+- `INFERENCE_MODEL_ID`: The name of the model. E.g. `claude-3-5-haiku`.\n+- `INFERENCE_URL`: The base URL for calling the model.\n+\n+It is important to note that the values for `INFERENCE_KEY` and `INFERENCE_URL` will be required for making calls to your model. More details follow in the [Usage Examples](#usage-examples) section.",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2268175352",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12992,
        "pr_file": "docs/my-website/docs/providers/heroku.md",
        "discussion_id": "2268175352",
        "commented_code": "@@ -0,0 +1,77 @@\n+# Heroku\n+\n+## Provision a Model\n+\n+To use the Heroku provider for LiteLLM, you must first configure a Heroku app, and attach one of the models listed in the [Supported Models](#supported-models) section.\n+\n+To get configure a Heroku app with an attached model, please refer to [Heroku's documentation](https://devcenter.heroku.com/articles/heroku-inference).\n+\n+## Supported Models\n+\n+The Heroku provider for LiteLLM currently, only supports [chat](https://devcenter.heroku.com/articles/heroku-inference-api-v1-chat-completions). Supported chat models are:\n+\n+| Model                             | Region  |\n+|-----------------------------------|---------|\n+| [`heroku/claude-sonnet-4`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-4-sonnet)          | US, EU  |\n+| [`heroku/claude-3-7-sonnet`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-7-sonnet)        | US, EU  |\n+| [`heroku/claude-3-5-sonnet-latest`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-sonnet-latest) | US      |\n+| [`heroku/claude-3-5-haiku`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-haiku)         | US      |\n+| [`heroku/claude-3`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-haiku)                 | EU      |\n+\n+## Environment Variables\n+\n+When a model is attached to a Heroku app, three config variables are set:\n+\n+- `INFERENCE_KEY`: The API key used for authenticating requests to the model.\n+- `INFERENCE_MODEL_ID`: The name of the model. E.g. `claude-3-5-haiku`.\n+- `INFERENCE_URL`: The base URL for calling the model.\n+\n+It is important to note that the values for `INFERENCE_KEY` and `INFERENCE_URL` will be required for making calls to your model. More details follow in the [Usage Examples](#usage-examples) section.",
        "comment_created_at": "2025-08-11T22:51:04+00:00",
        "comment_author": "claire-riley",
        "comment_body": "```suggestion\r\nBoth `INFERENCE_KEY` and `INFERENCE_URL` are required to make calls to your model.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2270467313",
    "pr_number": 12992,
    "pr_file": "docs/my-website/docs/providers/heroku.md",
    "created_at": "2025-08-12T16:12:58+00:00",
    "commented_code": "+# Heroku\n+\n+## Provision a Model\n+\n+To use Heroku with LiteLLM, [configure a Heroku app and attach a supported model](https://devcenter.heroku.com/articles/heroku-inference#provision-access-to-an-ai-model-resource).\n+\n+\n+## Supported Models\n+\n+Heroku for LiteLLM supports various [chat](https://devcenter.heroku.com/articles/heroku-inference-api-v1-chat-completions) models:\n+\n+| Model                             | Region  |\n+|-----------------------------------|---------|\n+| [`heroku/claude-sonnet-4`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-4-sonnet)          | US, EU  |\n+| [`heroku/claude-3-7-sonnet`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-7-sonnet)        | US, EU  |\n+| [`heroku/claude-3-5-sonnet-latest`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-sonnet-latest) | US      |\n+| [`heroku/claude-3-5-haiku`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-haiku)         | US      |\n+| [`heroku/claude-3`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-haiku)                 | EU      |\n+\n+## Environment Variables\n+\n+When you attach a model to a Heroku app, three config variables are set:\n+\n+- `INFERENCE_KEY`: The API key used for authenticating requests to the model.\n+- `INFERENCE_MODEL_ID`: The name of the model, e.g. `claude-3-5-haiku`.\n+- `INFERENCE_URL`: The base URL for calling the model.\n+\n+Both `INFERENCE_KEY` and `INFERENCE_URL` are required to make calls to your model.\n+\n+For more information on these variables, see the [Heroku documentation](https://devcenter.heroku.com/articles/heroku-inference#model-resource-config-vars).\n+\n+## Usage Examples\n+### Using Config Variables\n+\n+Heroku uses the following LiteLLM API config variables:\n+\n+- `HEROKU_API_KEY`: This value corresponds to [LiteLLM's `api_key` param](https://docs.litellm.ai/docs/set_keys#litellmapi_key). Set this to the value of Heroku's `INFERENCE_KEY` config variable.\n+- `HEROKU_API_BASE`: This value corresponds to [LiteLLM's `api_base` param](https://docs.litellm.ai/docs/set_keys#litellmapi_base). Set this to the value of Heroku's `INFERENCE_URL` config variable.\n+\n+In this example, we don't explicitly pass the `api_key` and `api_base` variables. Instead, we set the config variables which will be used by Heroku:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2270467313",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12992,
        "pr_file": "docs/my-website/docs/providers/heroku.md",
        "discussion_id": "2270467313",
        "commented_code": "@@ -0,0 +1,76 @@\n+# Heroku\n+\n+## Provision a Model\n+\n+To use Heroku with LiteLLM, [configure a Heroku app and attach a supported model](https://devcenter.heroku.com/articles/heroku-inference#provision-access-to-an-ai-model-resource).\n+\n+\n+## Supported Models\n+\n+Heroku for LiteLLM supports various [chat](https://devcenter.heroku.com/articles/heroku-inference-api-v1-chat-completions) models:\n+\n+| Model                             | Region  |\n+|-----------------------------------|---------|\n+| [`heroku/claude-sonnet-4`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-4-sonnet)          | US, EU  |\n+| [`heroku/claude-3-7-sonnet`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-7-sonnet)        | US, EU  |\n+| [`heroku/claude-3-5-sonnet-latest`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-sonnet-latest) | US      |\n+| [`heroku/claude-3-5-haiku`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-haiku)         | US      |\n+| [`heroku/claude-3`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-haiku)                 | EU      |\n+\n+## Environment Variables\n+\n+When you attach a model to a Heroku app, three config variables are set:\n+\n+- `INFERENCE_KEY`: The API key used for authenticating requests to the model.\n+- `INFERENCE_MODEL_ID`: The name of the model, e.g. `claude-3-5-haiku`.\n+- `INFERENCE_URL`: The base URL for calling the model.\n+\n+Both `INFERENCE_KEY` and `INFERENCE_URL` are required to make calls to your model.\n+\n+For more information on these variables, see the [Heroku documentation](https://devcenter.heroku.com/articles/heroku-inference#model-resource-config-vars).\n+\n+## Usage Examples\n+### Using Config Variables\n+\n+Heroku uses the following LiteLLM API config variables:\n+\n+- `HEROKU_API_KEY`: This value corresponds to [LiteLLM's `api_key` param](https://docs.litellm.ai/docs/set_keys#litellmapi_key). Set this to the value of Heroku's `INFERENCE_KEY` config variable.\n+- `HEROKU_API_BASE`: This value corresponds to [LiteLLM's `api_base` param](https://docs.litellm.ai/docs/set_keys#litellmapi_base). Set this to the value of Heroku's `INFERENCE_URL` config variable.\n+\n+In this example, we don't explicitly pass the `api_key` and `api_base` variables. Instead, we set the config variables which will be used by Heroku:",
        "comment_created_at": "2025-08-12T16:12:58+00:00",
        "comment_author": "claire-riley",
        "comment_body": "```suggestion\r\nIn this example, we don't explicitly pass the `api_key` and `api_base` variables. Instead, we set the config variables which Heroku will use:\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1945124145",
    "pr_number": 8258,
    "pr_file": "docs/my-website/docs/providers/huggingface.md",
    "created_at": "2025-02-06T17:09:46+00:00",
    "commented_code": "import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n ```\n-\n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-2. Start the proxy\n+### Getting Started\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n ```\n+huggingface/<provider>/<model_id>\n+```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.\n+By default, if you don't specify a provider, LiteLLM will use the [HF Inference API](https://huggingface.co/docs/api-inference/en/index).\n \n-3. Test it!\n+Examples:\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"llama-3.1-8B-instruct\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+# Run DeepSeek-R1 inference through Together AI\n+completion(model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",...)\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"classification\" label=\"Text Classification\">\n+# Run Qwen2.5-72B-Instruct inference through Sambanova\n+completion(model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\",...)\n+```\n \n-Append `text-classification` to the model name\n+For Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client.",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1945124145",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8258,
        "pr_file": "docs/my-website/docs/providers/huggingface.md",
        "discussion_id": "1945124145",
        "commented_code": "@@ -2,466 +2,363 @@ import Image from '@theme/IdealImage';\n import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n ```\n-\n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-2. Start the proxy\n+### Getting Started\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n ```\n+huggingface/<provider>/<model_id>\n+```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.\n+By default, if you don't specify a provider, LiteLLM will use the [HF Inference API](https://huggingface.co/docs/api-inference/en/index).\n \n-3. Test it!\n+Examples:\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"llama-3.1-8B-instruct\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+# Run DeepSeek-R1 inference through Together AI\n+completion(model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",...)\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"classification\" label=\"Text Classification\">\n+# Run Qwen2.5-72B-Instruct inference through Sambanova\n+completion(model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\",...)\n+```\n \n-Append `text-classification` to the model name\n+For Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client.",
        "comment_created_at": "2025-02-06T17:09:46+00:00",
        "comment_author": "Wauplin",
        "comment_body": "```suggestion\r\nFor Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client with some extra features to handle providers.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1945127313",
    "pr_number": 8258,
    "pr_file": "docs/my-website/docs/providers/huggingface.md",
    "created_at": "2025-02-06T17:12:02+00:00",
    "commented_code": "import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n ```\n-\n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-2. Start the proxy\n+### Getting Started\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n ```\n+huggingface/<provider>/<model_id>\n+```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.\n+By default, if you don't specify a provider, LiteLLM will use the [HF Inference API](https://huggingface.co/docs/api-inference/en/index).\n \n-3. Test it!\n+Examples:\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"llama-3.1-8B-instruct\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+# Run DeepSeek-R1 inference through Together AI\n+completion(model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",...)\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"classification\" label=\"Text Classification\">\n+# Run Qwen2.5-72B-Instruct inference through Sambanova\n+completion(model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\",...)\n+```\n \n-Append `text-classification` to the model name\n+For Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client.\n \n-e.g. `huggingface/text-classification/<model-name>`\n+<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n+  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n+</a>\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Basic Completion\n+Here's an example of chat completion using the DeepSeek-R1 model through Together AI:\n \n ```python\n import os\n from litellm import completion\n \n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n-\n-messages = [{ \"content\": \"I like you, I love you!\",\"role\": \"user\"}]\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-# e.g. Call 'shahrukhx01/question-vs-statement-classifier' hosted on HF Inference endpoints\n response = completion(\n-  model=\"huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\",\n-  messages=messages,\n-  api_base=\"https://my-endpoint.endpoints.huggingface.cloud\",\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}]\n )\n-\n print(response)\n ```\n \n-</TabItem> \n-<TabItem value=\"proxy\" label=\"PROXY\">\n+### Streaming\n+Now, let's see what a streaming request looks like.\n \n-1. Add models to your config.yaml\n+```python\n+import os\n+from litellm import completion\n \n-```yaml\n-model_list:\n-  - model_name: bert-classifier\n-    litellm_params:\n-      model: huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://my-endpoint.endpoints.huggingface.cloud\"\n-```\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-2. Start the proxy\n+response = completion(\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}],\n+    stream=True\n+)\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+for chunk in response:\n+    print(chunk)\n ```\n \n-3. Test it!\n+### Image Input\n+You can also pass images to the model.",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1945127313",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8258,
        "pr_file": "docs/my-website/docs/providers/huggingface.md",
        "discussion_id": "1945127313",
        "commented_code": "@@ -2,466 +2,363 @@ import Image from '@theme/IdealImage';\n import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n ```\n-\n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-2. Start the proxy\n+### Getting Started\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n ```\n+huggingface/<provider>/<model_id>\n+```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.\n+By default, if you don't specify a provider, LiteLLM will use the [HF Inference API](https://huggingface.co/docs/api-inference/en/index).\n \n-3. Test it!\n+Examples:\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"llama-3.1-8B-instruct\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+# Run DeepSeek-R1 inference through Together AI\n+completion(model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",...)\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"classification\" label=\"Text Classification\">\n+# Run Qwen2.5-72B-Instruct inference through Sambanova\n+completion(model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\",...)\n+```\n \n-Append `text-classification` to the model name\n+For Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client.\n \n-e.g. `huggingface/text-classification/<model-name>`\n+<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n+  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n+</a>\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Basic Completion\n+Here's an example of chat completion using the DeepSeek-R1 model through Together AI:\n \n ```python\n import os\n from litellm import completion\n \n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n-\n-messages = [{ \"content\": \"I like you, I love you!\",\"role\": \"user\"}]\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-# e.g. Call 'shahrukhx01/question-vs-statement-classifier' hosted on HF Inference endpoints\n response = completion(\n-  model=\"huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\",\n-  messages=messages,\n-  api_base=\"https://my-endpoint.endpoints.huggingface.cloud\",\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}]\n )\n-\n print(response)\n ```\n \n-</TabItem> \n-<TabItem value=\"proxy\" label=\"PROXY\">\n+### Streaming\n+Now, let's see what a streaming request looks like.\n \n-1. Add models to your config.yaml\n+```python\n+import os\n+from litellm import completion\n \n-```yaml\n-model_list:\n-  - model_name: bert-classifier\n-    litellm_params:\n-      model: huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://my-endpoint.endpoints.huggingface.cloud\"\n-```\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-2. Start the proxy\n+response = completion(\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}],\n+    stream=True\n+)\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+for chunk in response:\n+    print(chunk)\n ```\n \n-3. Test it!\n+### Image Input\n+You can also pass images to the model.",
        "comment_created_at": "2025-02-06T17:12:02+00:00",
        "comment_author": "Wauplin",
        "comment_body": "```suggestion\r\nYou can also pass images when the model supports it. Here is an example using [Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) model through Sambanova.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1945150455",
    "pr_number": 8258,
    "pr_file": "docs/my-website/docs/providers/huggingface.md",
    "created_at": "2025-02-06T17:29:26+00:00",
    "commented_code": "import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n ```\n-\n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-2. Start the proxy\n+### Getting Started\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n ```\n+huggingface/<provider>/<model_id>\n+```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.\n+By default, if you don't specify a provider, LiteLLM will use the [HF Inference API](https://huggingface.co/docs/api-inference/en/index).\n \n-3. Test it!\n+Examples:\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"llama-3.1-8B-instruct\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+# Run DeepSeek-R1 inference through Together AI\n+completion(model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",...)\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"classification\" label=\"Text Classification\">\n+# Run Qwen2.5-72B-Instruct inference through Sambanova\n+completion(model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\",...)\n+```\n \n-Append `text-classification` to the model name\n+For Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client.\n \n-e.g. `huggingface/text-classification/<model-name>`\n+<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n+  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n+</a>\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Basic Completion\n+Here's an example of chat completion using the DeepSeek-R1 model through Together AI:\n \n ```python\n import os\n from litellm import completion\n \n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n-\n-messages = [{ \"content\": \"I like you, I love you!\",\"role\": \"user\"}]\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-# e.g. Call 'shahrukhx01/question-vs-statement-classifier' hosted on HF Inference endpoints\n response = completion(\n-  model=\"huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\",\n-  messages=messages,\n-  api_base=\"https://my-endpoint.endpoints.huggingface.cloud\",\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}]\n )\n-\n print(response)\n ```\n \n-</TabItem> \n-<TabItem value=\"proxy\" label=\"PROXY\">\n+### Streaming\n+Now, let's see what a streaming request looks like.\n \n-1. Add models to your config.yaml\n+```python\n+import os\n+from litellm import completion\n \n-```yaml\n-model_list:\n-  - model_name: bert-classifier\n-    litellm_params:\n-      model: huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://my-endpoint.endpoints.huggingface.cloud\"\n-```\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-2. Start the proxy\n+response = completion(\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}],\n+    stream=True\n+)\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+for chunk in response:\n+    print(chunk)\n ```\n \n-3. Test it!\n+### Image Input\n+You can also pass images to the model.\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"bert-classifier\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+from litellm import completion\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"dedicated\" label=\"Dedicated Inference Endpoints\">\n+# Set your Hugging Face Token\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+messages=[\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\n+                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n+                    }\n+                },\n+            ],\n+        }\n+    ]\n \n-Steps to use\n-* Create your own Hugging Face dedicated endpoint here: https://ui.endpoints.huggingface.co/\n-* Set `api_base` to your deployed api base\n-* Add the `huggingface/` prefix to your model so litellm knows it's a huggingface Deployed Inference Endpoint\n+response = completion(\n+    model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\", \n+    messages=messages,\n+)\n+print(response.choices[0])\n+```\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Function Calling\n+You can extend the model's capabilities by giving them access to tools, here is an example with function calling using the Qwen2.5-72B-Instruct model through Sambanova.\n \n ```python\n import os\n from litellm import completion\n \n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n+# Set your Hugging Face Token\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+tools = [\n+  {\n+    \"type\": \"function\",\n+    \"function\": {\n+      \"name\": \"get_current_weather\",\n+      \"description\": \"Get the current weather in a given location\",\n+      \"parameters\": {\n+        \"type\": \"object\",\n+        \"properties\": {\n+          \"location\": {\n+            \"type\": \"string\",\n+            \"description\": \"The city and state, e.g. San Francisco, CA\",\n+          },\n+          \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n+        },\n+        \"required\": [\"location\"],\n+      },\n+    }\n+  }\n+]\n+messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n \n-# TGI model: Call https://huggingface.co/glaiveai/glaive-coder-7b\n-# add the 'huggingface/' prefix to the model to set huggingface as the provider\n-# set api base to your deployed api endpoint from hugging face\n response = completion(\n-    model=\"huggingface/glaiveai/glaive-coder-7b\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    api_base=\"https://wjiegasee9bmqke2.us-east-1.aws.endpoints.huggingface.cloud\"\n+    model=\"huggingface/sambanova/meta-llama/Llama-3.3-70B-Instruct\", \n+    messages=messages,\n+    tools=tools,\n+    tool_choice=\"auto\"\n )\n print(response)\n ```\n \n </TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: glaive-coder\n-    litellm_params:\n-      model: huggingface/glaiveai/glaive-coder-7b\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://wjiegasee9bmqke2.us-east-1.aws.endpoints.huggingface.cloud\"\n-```\n \n-2. Start the proxy\n-\n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n-```\n-\n-3. Test it!\n-\n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"glaive-coder\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n-\n-</TabItem> \n-</Tabs>\n-\n-</TabItem>\n-</Tabs>\n-\n-## Streaming\n+<TabItem value=\"endpoints\" label=\"Inference Endpoints\">\n \n <a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n </a>\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+### Basic Completion\n+Connect to your deployed Hugging Face Inference Endpoint for model inference. This provides dedicated infrastructure for your specific use case.\n \n ```python\n import os\n from litellm import completion\n \n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n-\n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-# e.g. Call 'facebook/blenderbot-400M-distill' hosted on HF Inference endpoints\n response = completion(\n-  model=\"huggingface/facebook/blenderbot-400M-distill\",\n-  messages=messages,\n-  api_base=\"https://my-endpoint.huggingface.cloud\",\n-  stream=True\n+    model=\"huggingface/tgi\",\n+    messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}],\n+    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\"\n )\n-\n print(response)\n-for chunk in response:\n-  print(chunk)\n ```\n \n-## Embedding\n-\n-LiteLLM supports Hugging Face's [text-embedding-inference](https://github.com/huggingface/text-embeddings-inference) format.\n+### Streaming\n \n ```python\n-from litellm import embedding\n import os\n-os.environ['HUGGINGFACE_API_KEY'] = \"\"\n-response = embedding(\n-    model='huggingface/microsoft/codebert-base',\n-    input=[\"good morning from litellm\"]\n-)\n-```\n+from litellm import completion\n \n-## Advanced\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-### Setting API KEYS + API BASE\n+response = completion(\n+    model=\"huggingface/tgi\",\n+    messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}],\n+    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\",\n+    stream=True\n+)\n+\n+for chunk in response:\n+    print(chunk)\n+```\n \n-If required, you can set the api key + api base, set it in your os environment. [Code for how it's sent](https://github.com/BerriAI/litellm/blob/0100ab2382a0e720c7978fbf662cc6e6920e7e03/litellm/llms/huggingface_restapi.py#L25)\n+### Image Input\n \n ```python\n import os\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n-os.environ[\"HUGGINGFACE_API_BASE\"] = \"\"\n-```\n-\n-### Viewing Log probs\n+from litellm import completion\n \n-#### Using `decoder_input_details` - OpenAI `echo`\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+messages=[\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\n+                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n+                    }\n+                },\n+            ],\n+        }\n+    ]\n+response = completion(\n+    model=\"huggingface/tgi\",\n+    messages=messages,\n+    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\"\"\n+)\n+print(response.choices[0])\n+```\n \n-The `echo` param is supported by OpenAI Completions - Use `litellm.text_completion()` for this\n+### Function Calling\n \n ```python\n-from litellm import text_completion\n-response = text_completion(\n-    model=\"huggingface/bigcode/starcoder\",\n-    prompt=\"good morning\",\n-    max_tokens=10, logprobs=10,\n-    echo=True\n-)\n-```\n+import os\n+from litellm import completion\n \n-#### Output\n-\n-```json\n-{\n-  \"id\": \"chatcmpl-3fc71792-c442-4ba1-a611-19dd0ac371ad\",\n-  \"object\": \"text_completion\",\n-  \"created\": 1698801125.936519,\n-  \"model\": \"bigcode/starcoder\",\n-  \"choices\": [\n-    {\n-      \"text\": \", I'm going to make you a sand\",\n-      \"index\": 0,\n-      \"logprobs\": {\n-        \"tokens\": [\n-          \"good\",\n-          \" morning\",\n-          \",\",\n-          \" I\",\n-          \"'m\",\n-          \" going\",\n-          \" to\",\n-          \" make\",\n-          \" you\",\n-          \" a\",\n-          \" s\",\n-          \"and\"\n-        ],\n-        \"token_logprobs\": [\n-          \"None\",\n-          -14.96875,\n-          -2.2285156,\n-          -2.734375,\n-          -2.0957031,\n-          -2.0917969,\n-          -0.09429932,\n-          -3.1132812,\n-          -1.3203125,\n-          -1.2304688,\n-          -1.6201172,\n-          -0.010292053\n-        ]\n-      },\n-      \"finish_reason\": \"length\"\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+functions = [{\n+    \"name\": \"get_weather\",\n+    \"description\": \"Get the weather in a given location\",\n+    \"parameters\": {\n+        \"type\": \"object\",\n+        \"properties\": {\n+            \"location\": {\n+                \"type\": \"string\",\n+                \"description\": \"The location to get weather for\"\n+            }\n+        },\n+        \"required\": [\"location\"]\n     }\n-  ],\n-  \"usage\": {\n-    \"completion_tokens\": 9,\n-    \"prompt_tokens\": 2,\n-    \"total_tokens\": 11\n-  }\n-}\n-```\n-\n-### Models with Prompt Formatting\n+}]\n \n-For models with special prompt templates (e.g. Llama2), we format the prompt to fit their template.\n+response = completion(\n+    model=\"huggingface/tgi\",\n+    messages=[{\"content\": \"What's the weather like in San Francisco?\", \"role\": \"user\"}],\n+    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\",\n+    functions=functions\n+)\n+print(response)\n+```\n \n-#### Models with natively Supported Prompt Templates\n+</TabItem>\n+</Tabs>\n \n-| Model Name                           | Works for Models                   | Function Call                                                                                                           | Required OS Variables               |\n-| ------------------------------------ | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ----------------------------------- |\n-| mistralai/Mistral-7B-Instruct-v0.1   | mistralai/Mistral-7B-Instruct-v0.1 | `completion(model='huggingface/mistralai/Mistral-7B-Instruct-v0.1', messages=messages, api_base=\"your_api_endpoint\")`   | `os.environ['HUGGINGFACE_API_KEY']` |\n-| meta-llama/Llama-2-7b-chat           | All meta-llama llama2 chat models  | `completion(model='huggingface/meta-llama/Llama-2-7b', messages=messages, api_base=\"your_api_endpoint\")`                | `os.environ['HUGGINGFACE_API_KEY']` |\n-| tiiuae/falcon-7b-instruct            | All falcon instruct models         | `completion(model='huggingface/tiiuae/falcon-7b-instruct', messages=messages, api_base=\"your_api_endpoint\")`            | `os.environ['HUGGINGFACE_API_KEY']` |\n-| mosaicml/mpt-7b-chat                 | All mpt chat models                | `completion(model='huggingface/mosaicml/mpt-7b-chat', messages=messages, api_base=\"your_api_endpoint\")`                 | `os.environ['HUGGINGFACE_API_KEY']` |\n-| codellama/CodeLlama-34b-Instruct-hf  | All codellama instruct models      | `completion(model='huggingface/codellama/CodeLlama-34b-Instruct-hf', messages=messages, api_base=\"your_api_endpoint\")`  | `os.environ['HUGGINGFACE_API_KEY']` |\n-| WizardLM/WizardCoder-Python-34B-V1.0 | All wizardcoder models             | `completion(model='huggingface/WizardLM/WizardCoder-Python-34B-V1.0', messages=messages, api_base=\"your_api_endpoint\")` | `os.environ['HUGGINGFACE_API_KEY']` |\n-| Phind/Phind-CodeLlama-34B-v2         | All phind-codellama models         | `completion(model='huggingface/Phind/Phind-CodeLlama-34B-v2', messages=messages, api_base=\"your_api_endpoint\")`         | `os.environ['HUGGINGFACE_API_KEY']` |\n+## LiteLLM Proxy Server with Hugging Face models\n+Here is how to setup a [LiteLLM Proxy Server](https://docs.litellm.ai/#litellm-proxy-server-llm-gateway) to use Hugging Face models and how to use it:\n \n-**What if we don't support a model you need?**\n-You can also specify you're own custom prompt formatting, in case we don't have your model covered yet.\n+### Step 1. Setup the config file\n+```yaml\n+model_list:\n+  - model_name: my-r1-model\n+    litellm_params:\n+      model: huggingface/together/deepseek-ai/DeepSeek-R1\n+      api_key: os.environ/HF_TOKEN # ensure you have `HF_TOKEN` in your .env\n+```\n \n-**Does this mean you have to specify a prompt for all models?**\n-No. By default we'll concatenate your message content to make a prompt.\n+### Step 2. Start the server\n+```bash\n+litellm --config /path/to/config.yaml\n+```\n \n-**Default Prompt Template**\n+### Step 3. Make a request to the server\n+<Tabs>\n+<TabItem value=\"curl\" label=\"curl\">\n \n-```python\n-def default_pt(messages):\n-    return \" \".join(message[\"content\"] for message in messages)\n+```shell\n+curl --location 'http://0.0.0.0:4000/chat/completions' \\\n+    --header 'Content-Type: application/json' \\\n+    --data '{\n+    \"model\": \"my-r1-model\",\n+    \"messages\": [\n+        {\n+            \"role\": \"user\",\n+            \"content\": \"Hello, how are you?\"\n+        }\n+    ]\n+}'\n ```\n \n-[Code for how prompt formats work in LiteLLM](https://github.com/BerriAI/litellm/blob/main/litellm/llms/prompt_templates/factory.py)\n-\n-#### Custom prompt templates\n+</TabItem>\n+<TabItem value=\"python\" label=\"python\">\n \n ```python\n-import litellm\n-\n-# Create your own custom prompt template works\n-litellm.register_prompt_template(\n-\t    model=\"togethercomputer/LLaMA-2-7B-32K\",\n-\t    roles={\n-            \"system\": {\n-                \"pre_message\": \"[INST] <<SYS>>\n\",\n-                \"post_message\": \"\n<</SYS>>\n [/INST]\n\"\n-            },\n-            \"user\": {\n-                \"pre_message\": \"[INST] \",\n-                \"post_message\": \" [/INST]\n\"\n-            },\n-            \"assistant\": {\n-                \"post_message\": \"\n\"\n-            }\n-        }\n-    )\n+# pip install huggingface-hub\n+from huggingface_hub import InferenceClient\n \n-def test_huggingface_custom_model():\n-    model = \"huggingface/togethercomputer/LLaMA-2-7B-32K\"\n-    response = completion(model=model, messages=messages, api_base=\"https://ecd4sb5n09bo4ei2.us-east-1.aws.endpoints.huggingface.cloud\")\n-    print(response['choices'][0]['message']['content'])\n-    return response\n+client = InferenceClient(\n+    base_url=\"http://0.0.0.0:4000\"\n+)\n \n-test_huggingface_custom_model()\n+response = client.chat.completions.create(\n+    model=\"my-r1-model\",\n+    messages=[\n+        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n+    ]\n+)\n+print(response)\n ```\n \n-[Implementation Code](https://github.com/BerriAI/litellm/blob/c0b3da2c14c791a0b755f0b1e5a9ef065951ecbf/litellm/llms/huggingface_restapi.py#L52)\n-\n-### Deploying a model on huggingface\n-\n-You can use any chat/text model from Hugging Face with the following steps:\n-\n-- Copy your model id/url from Huggingface Inference Endpoints\n-  - [ ] Go to https://ui.endpoints.huggingface.co/\n-  - [ ] Copy the url of the specific model you'd like to use\n-        <Image img={require('../../img/hf_inference_endpoint.png')} alt=\"HF_Dashboard\" style={{ maxWidth: '50%', height: 'auto' }}/>\n-- Set it as your model name\n-- Set your HUGGINGFACE_API_KEY as an environment variable\n+</TabItem>\n+</Tabs>\n \n-Need help deploying a model on huggingface? [Check out this guide.](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint)\n \n-# output\n+## Embedding\n \n-Same as the OpenAI format, but also includes logprobs. [See the code](https://github.com/BerriAI/litellm/blob/b4b2dbf005142e0a483d46a07a88a19814899403/litellm/llms/huggingface_restapi.py#L115)\n+LiteLLM supports Hugging Face's [text-embedding-inference](https://github.com/huggingface/text-embeddings-inference) models as well.\n \n-```json\n-{\n-  \"choices\": [\n-    {\n-      \"finish_reason\": \"stop\",\n-      \"index\": 0,\n-      \"message\": {\n-        \"content\": \"\\ud83d\\ude31\n\nComment: @SarahSzabo I'm\",\n-        \"role\": \"assistant\",\n-        \"logprobs\": -22.697942825499993\n-      }\n-    }\n-  ],\n-  \"created\": 1693436637.38206,\n-  \"model\": \"https://ji16r2iys9a8rjk2.us-east-1.aws.endpoints.huggingface.cloud\",\n-  \"usage\": {\n-    \"prompt_tokens\": 14,\n-    \"completion_tokens\": 11,\n-    \"total_tokens\": 25\n-  }\n-}\n+```python\n+from litellm import embedding\n+import os\n+os.environ['HF_TOKEN'] = \"hf_xxxxxx\"\n+response = embedding(\n+    model='huggingface/microsoft/codebert-base',\n+    input=[\"good morning from litellm\"]\n+)\n ```\n \n # FAQ\n \n-**Does this support stop sequences?**\n+**How does billing work with Hugging Face Inference Providers?**\n \n-Yes, we support stop sequences - and you can pass as many as allowed by Hugging Face (or any provider!)\n+> You pay the standard provider API rates with no additional markup - Hugging Face simply passes through the provider costs. Note that [Hugging Face PRO](https://huggingface.co/subscribe/pro) users get $2 worth of Inference credits every month that can be used across providers.",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1945150455",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8258,
        "pr_file": "docs/my-website/docs/providers/huggingface.md",
        "discussion_id": "1945150455",
        "commented_code": "@@ -2,466 +2,363 @@ import Image from '@theme/IdealImage';\n import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n ```\n-\n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-2. Start the proxy\n+### Getting Started\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n ```\n+huggingface/<provider>/<model_id>\n+```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.\n+By default, if you don't specify a provider, LiteLLM will use the [HF Inference API](https://huggingface.co/docs/api-inference/en/index).\n \n-3. Test it!\n+Examples:\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"llama-3.1-8B-instruct\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+# Run DeepSeek-R1 inference through Together AI\n+completion(model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",...)\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"classification\" label=\"Text Classification\">\n+# Run Qwen2.5-72B-Instruct inference through Sambanova\n+completion(model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\",...)\n+```\n \n-Append `text-classification` to the model name\n+For Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client.\n \n-e.g. `huggingface/text-classification/<model-name>`\n+<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n+  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n+</a>\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Basic Completion\n+Here's an example of chat completion using the DeepSeek-R1 model through Together AI:\n \n ```python\n import os\n from litellm import completion\n \n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n-\n-messages = [{ \"content\": \"I like you, I love you!\",\"role\": \"user\"}]\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-# e.g. Call 'shahrukhx01/question-vs-statement-classifier' hosted on HF Inference endpoints\n response = completion(\n-  model=\"huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\",\n-  messages=messages,\n-  api_base=\"https://my-endpoint.endpoints.huggingface.cloud\",\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}]\n )\n-\n print(response)\n ```\n \n-</TabItem> \n-<TabItem value=\"proxy\" label=\"PROXY\">\n+### Streaming\n+Now, let's see what a streaming request looks like.\n \n-1. Add models to your config.yaml\n+```python\n+import os\n+from litellm import completion\n \n-```yaml\n-model_list:\n-  - model_name: bert-classifier\n-    litellm_params:\n-      model: huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://my-endpoint.endpoints.huggingface.cloud\"\n-```\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-2. Start the proxy\n+response = completion(\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}],\n+    stream=True\n+)\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+for chunk in response:\n+    print(chunk)\n ```\n \n-3. Test it!\n+### Image Input\n+You can also pass images to the model.\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"bert-classifier\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+from litellm import completion\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"dedicated\" label=\"Dedicated Inference Endpoints\">\n+# Set your Hugging Face Token\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+messages=[\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\n+                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n+                    }\n+                },\n+            ],\n+        }\n+    ]\n \n-Steps to use\n-* Create your own Hugging Face dedicated endpoint here: https://ui.endpoints.huggingface.co/\n-* Set `api_base` to your deployed api base\n-* Add the `huggingface/` prefix to your model so litellm knows it's a huggingface Deployed Inference Endpoint\n+response = completion(\n+    model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\", \n+    messages=messages,\n+)\n+print(response.choices[0])\n+```\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Function Calling\n+You can extend the model's capabilities by giving them access to tools, here is an example with function calling using the Qwen2.5-72B-Instruct model through Sambanova.\n \n ```python\n import os\n from litellm import completion\n \n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n+# Set your Hugging Face Token\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+tools = [\n+  {\n+    \"type\": \"function\",\n+    \"function\": {\n+      \"name\": \"get_current_weather\",\n+      \"description\": \"Get the current weather in a given location\",\n+      \"parameters\": {\n+        \"type\": \"object\",\n+        \"properties\": {\n+          \"location\": {\n+            \"type\": \"string\",\n+            \"description\": \"The city and state, e.g. San Francisco, CA\",\n+          },\n+          \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n+        },\n+        \"required\": [\"location\"],\n+      },\n+    }\n+  }\n+]\n+messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n \n-# TGI model: Call https://huggingface.co/glaiveai/glaive-coder-7b\n-# add the 'huggingface/' prefix to the model to set huggingface as the provider\n-# set api base to your deployed api endpoint from hugging face\n response = completion(\n-    model=\"huggingface/glaiveai/glaive-coder-7b\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    api_base=\"https://wjiegasee9bmqke2.us-east-1.aws.endpoints.huggingface.cloud\"\n+    model=\"huggingface/sambanova/meta-llama/Llama-3.3-70B-Instruct\", \n+    messages=messages,\n+    tools=tools,\n+    tool_choice=\"auto\"\n )\n print(response)\n ```\n \n </TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: glaive-coder\n-    litellm_params:\n-      model: huggingface/glaiveai/glaive-coder-7b\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://wjiegasee9bmqke2.us-east-1.aws.endpoints.huggingface.cloud\"\n-```\n \n-2. Start the proxy\n-\n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n-```\n-\n-3. Test it!\n-\n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"glaive-coder\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n-\n-</TabItem> \n-</Tabs>\n-\n-</TabItem>\n-</Tabs>\n-\n-## Streaming\n+<TabItem value=\"endpoints\" label=\"Inference Endpoints\">\n \n <a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n </a>\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+### Basic Completion\n+Connect to your deployed Hugging Face Inference Endpoint for model inference. This provides dedicated infrastructure for your specific use case.\n \n ```python\n import os\n from litellm import completion\n \n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n-\n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-# e.g. Call 'facebook/blenderbot-400M-distill' hosted on HF Inference endpoints\n response = completion(\n-  model=\"huggingface/facebook/blenderbot-400M-distill\",\n-  messages=messages,\n-  api_base=\"https://my-endpoint.huggingface.cloud\",\n-  stream=True\n+    model=\"huggingface/tgi\",\n+    messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}],\n+    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\"\n )\n-\n print(response)\n-for chunk in response:\n-  print(chunk)\n ```\n \n-## Embedding\n-\n-LiteLLM supports Hugging Face's [text-embedding-inference](https://github.com/huggingface/text-embeddings-inference) format.\n+### Streaming\n \n ```python\n-from litellm import embedding\n import os\n-os.environ['HUGGINGFACE_API_KEY'] = \"\"\n-response = embedding(\n-    model='huggingface/microsoft/codebert-base',\n-    input=[\"good morning from litellm\"]\n-)\n-```\n+from litellm import completion\n \n-## Advanced\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-### Setting API KEYS + API BASE\n+response = completion(\n+    model=\"huggingface/tgi\",\n+    messages=[{\"content\": \"Hello, how are you?\", \"role\": \"user\"}],\n+    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\",\n+    stream=True\n+)\n+\n+for chunk in response:\n+    print(chunk)\n+```\n \n-If required, you can set the api key + api base, set it in your os environment. [Code for how it's sent](https://github.com/BerriAI/litellm/blob/0100ab2382a0e720c7978fbf662cc6e6920e7e03/litellm/llms/huggingface_restapi.py#L25)\n+### Image Input\n \n ```python\n import os\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n-os.environ[\"HUGGINGFACE_API_BASE\"] = \"\"\n-```\n-\n-### Viewing Log probs\n+from litellm import completion\n \n-#### Using `decoder_input_details` - OpenAI `echo`\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+messages=[\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\n+                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n+                    }\n+                },\n+            ],\n+        }\n+    ]\n+response = completion(\n+    model=\"huggingface/tgi\",\n+    messages=messages,\n+    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\"\"\n+)\n+print(response.choices[0])\n+```\n \n-The `echo` param is supported by OpenAI Completions - Use `litellm.text_completion()` for this\n+### Function Calling\n \n ```python\n-from litellm import text_completion\n-response = text_completion(\n-    model=\"huggingface/bigcode/starcoder\",\n-    prompt=\"good morning\",\n-    max_tokens=10, logprobs=10,\n-    echo=True\n-)\n-```\n+import os\n+from litellm import completion\n \n-#### Output\n-\n-```json\n-{\n-  \"id\": \"chatcmpl-3fc71792-c442-4ba1-a611-19dd0ac371ad\",\n-  \"object\": \"text_completion\",\n-  \"created\": 1698801125.936519,\n-  \"model\": \"bigcode/starcoder\",\n-  \"choices\": [\n-    {\n-      \"text\": \", I'm going to make you a sand\",\n-      \"index\": 0,\n-      \"logprobs\": {\n-        \"tokens\": [\n-          \"good\",\n-          \" morning\",\n-          \",\",\n-          \" I\",\n-          \"'m\",\n-          \" going\",\n-          \" to\",\n-          \" make\",\n-          \" you\",\n-          \" a\",\n-          \" s\",\n-          \"and\"\n-        ],\n-        \"token_logprobs\": [\n-          \"None\",\n-          -14.96875,\n-          -2.2285156,\n-          -2.734375,\n-          -2.0957031,\n-          -2.0917969,\n-          -0.09429932,\n-          -3.1132812,\n-          -1.3203125,\n-          -1.2304688,\n-          -1.6201172,\n-          -0.010292053\n-        ]\n-      },\n-      \"finish_reason\": \"length\"\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+functions = [{\n+    \"name\": \"get_weather\",\n+    \"description\": \"Get the weather in a given location\",\n+    \"parameters\": {\n+        \"type\": \"object\",\n+        \"properties\": {\n+            \"location\": {\n+                \"type\": \"string\",\n+                \"description\": \"The location to get weather for\"\n+            }\n+        },\n+        \"required\": [\"location\"]\n     }\n-  ],\n-  \"usage\": {\n-    \"completion_tokens\": 9,\n-    \"prompt_tokens\": 2,\n-    \"total_tokens\": 11\n-  }\n-}\n-```\n-\n-### Models with Prompt Formatting\n+}]\n \n-For models with special prompt templates (e.g. Llama2), we format the prompt to fit their template.\n+response = completion(\n+    model=\"huggingface/tgi\",\n+    messages=[{\"content\": \"What's the weather like in San Francisco?\", \"role\": \"user\"}],\n+    api_base=\"https://my-endpoint.endpoints.huggingface.cloud/v1/\",\n+    functions=functions\n+)\n+print(response)\n+```\n \n-#### Models with natively Supported Prompt Templates\n+</TabItem>\n+</Tabs>\n \n-| Model Name                           | Works for Models                   | Function Call                                                                                                           | Required OS Variables               |\n-| ------------------------------------ | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ----------------------------------- |\n-| mistralai/Mistral-7B-Instruct-v0.1   | mistralai/Mistral-7B-Instruct-v0.1 | `completion(model='huggingface/mistralai/Mistral-7B-Instruct-v0.1', messages=messages, api_base=\"your_api_endpoint\")`   | `os.environ['HUGGINGFACE_API_KEY']` |\n-| meta-llama/Llama-2-7b-chat           | All meta-llama llama2 chat models  | `completion(model='huggingface/meta-llama/Llama-2-7b', messages=messages, api_base=\"your_api_endpoint\")`                | `os.environ['HUGGINGFACE_API_KEY']` |\n-| tiiuae/falcon-7b-instruct            | All falcon instruct models         | `completion(model='huggingface/tiiuae/falcon-7b-instruct', messages=messages, api_base=\"your_api_endpoint\")`            | `os.environ['HUGGINGFACE_API_KEY']` |\n-| mosaicml/mpt-7b-chat                 | All mpt chat models                | `completion(model='huggingface/mosaicml/mpt-7b-chat', messages=messages, api_base=\"your_api_endpoint\")`                 | `os.environ['HUGGINGFACE_API_KEY']` |\n-| codellama/CodeLlama-34b-Instruct-hf  | All codellama instruct models      | `completion(model='huggingface/codellama/CodeLlama-34b-Instruct-hf', messages=messages, api_base=\"your_api_endpoint\")`  | `os.environ['HUGGINGFACE_API_KEY']` |\n-| WizardLM/WizardCoder-Python-34B-V1.0 | All wizardcoder models             | `completion(model='huggingface/WizardLM/WizardCoder-Python-34B-V1.0', messages=messages, api_base=\"your_api_endpoint\")` | `os.environ['HUGGINGFACE_API_KEY']` |\n-| Phind/Phind-CodeLlama-34B-v2         | All phind-codellama models         | `completion(model='huggingface/Phind/Phind-CodeLlama-34B-v2', messages=messages, api_base=\"your_api_endpoint\")`         | `os.environ['HUGGINGFACE_API_KEY']` |\n+## LiteLLM Proxy Server with Hugging Face models\n+Here is how to setup a [LiteLLM Proxy Server](https://docs.litellm.ai/#litellm-proxy-server-llm-gateway) to use Hugging Face models and how to use it:\n \n-**What if we don't support a model you need?**\n-You can also specify you're own custom prompt formatting, in case we don't have your model covered yet.\n+### Step 1. Setup the config file\n+```yaml\n+model_list:\n+  - model_name: my-r1-model\n+    litellm_params:\n+      model: huggingface/together/deepseek-ai/DeepSeek-R1\n+      api_key: os.environ/HF_TOKEN # ensure you have `HF_TOKEN` in your .env\n+```\n \n-**Does this mean you have to specify a prompt for all models?**\n-No. By default we'll concatenate your message content to make a prompt.\n+### Step 2. Start the server\n+```bash\n+litellm --config /path/to/config.yaml\n+```\n \n-**Default Prompt Template**\n+### Step 3. Make a request to the server\n+<Tabs>\n+<TabItem value=\"curl\" label=\"curl\">\n \n-```python\n-def default_pt(messages):\n-    return \" \".join(message[\"content\"] for message in messages)\n+```shell\n+curl --location 'http://0.0.0.0:4000/chat/completions' \\\n+    --header 'Content-Type: application/json' \\\n+    --data '{\n+    \"model\": \"my-r1-model\",\n+    \"messages\": [\n+        {\n+            \"role\": \"user\",\n+            \"content\": \"Hello, how are you?\"\n+        }\n+    ]\n+}'\n ```\n \n-[Code for how prompt formats work in LiteLLM](https://github.com/BerriAI/litellm/blob/main/litellm/llms/prompt_templates/factory.py)\n-\n-#### Custom prompt templates\n+</TabItem>\n+<TabItem value=\"python\" label=\"python\">\n \n ```python\n-import litellm\n-\n-# Create your own custom prompt template works\n-litellm.register_prompt_template(\n-\t    model=\"togethercomputer/LLaMA-2-7B-32K\",\n-\t    roles={\n-            \"system\": {\n-                \"pre_message\": \"[INST] <<SYS>>\\n\",\n-                \"post_message\": \"\\n<</SYS>>\\n [/INST]\\n\"\n-            },\n-            \"user\": {\n-                \"pre_message\": \"[INST] \",\n-                \"post_message\": \" [/INST]\\n\"\n-            },\n-            \"assistant\": {\n-                \"post_message\": \"\\n\"\n-            }\n-        }\n-    )\n+# pip install huggingface-hub\n+from huggingface_hub import InferenceClient\n \n-def test_huggingface_custom_model():\n-    model = \"huggingface/togethercomputer/LLaMA-2-7B-32K\"\n-    response = completion(model=model, messages=messages, api_base=\"https://ecd4sb5n09bo4ei2.us-east-1.aws.endpoints.huggingface.cloud\")\n-    print(response['choices'][0]['message']['content'])\n-    return response\n+client = InferenceClient(\n+    base_url=\"http://0.0.0.0:4000\"\n+)\n \n-test_huggingface_custom_model()\n+response = client.chat.completions.create(\n+    model=\"my-r1-model\",\n+    messages=[\n+        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n+    ]\n+)\n+print(response)\n ```\n \n-[Implementation Code](https://github.com/BerriAI/litellm/blob/c0b3da2c14c791a0b755f0b1e5a9ef065951ecbf/litellm/llms/huggingface_restapi.py#L52)\n-\n-### Deploying a model on huggingface\n-\n-You can use any chat/text model from Hugging Face with the following steps:\n-\n-- Copy your model id/url from Huggingface Inference Endpoints\n-  - [ ] Go to https://ui.endpoints.huggingface.co/\n-  - [ ] Copy the url of the specific model you'd like to use\n-        <Image img={require('../../img/hf_inference_endpoint.png')} alt=\"HF_Dashboard\" style={{ maxWidth: '50%', height: 'auto' }}/>\n-- Set it as your model name\n-- Set your HUGGINGFACE_API_KEY as an environment variable\n+</TabItem>\n+</Tabs>\n \n-Need help deploying a model on huggingface? [Check out this guide.](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint)\n \n-# output\n+## Embedding\n \n-Same as the OpenAI format, but also includes logprobs. [See the code](https://github.com/BerriAI/litellm/blob/b4b2dbf005142e0a483d46a07a88a19814899403/litellm/llms/huggingface_restapi.py#L115)\n+LiteLLM supports Hugging Face's [text-embedding-inference](https://github.com/huggingface/text-embeddings-inference) models as well.\n \n-```json\n-{\n-  \"choices\": [\n-    {\n-      \"finish_reason\": \"stop\",\n-      \"index\": 0,\n-      \"message\": {\n-        \"content\": \"\\ud83d\\ude31\\n\\nComment: @SarahSzabo I'm\",\n-        \"role\": \"assistant\",\n-        \"logprobs\": -22.697942825499993\n-      }\n-    }\n-  ],\n-  \"created\": 1693436637.38206,\n-  \"model\": \"https://ji16r2iys9a8rjk2.us-east-1.aws.endpoints.huggingface.cloud\",\n-  \"usage\": {\n-    \"prompt_tokens\": 14,\n-    \"completion_tokens\": 11,\n-    \"total_tokens\": 25\n-  }\n-}\n+```python\n+from litellm import embedding\n+import os\n+os.environ['HF_TOKEN'] = \"hf_xxxxxx\"\n+response = embedding(\n+    model='huggingface/microsoft/codebert-base',\n+    input=[\"good morning from litellm\"]\n+)\n ```\n \n # FAQ\n \n-**Does this support stop sequences?**\n+**How does billing work with Hugging Face Inference Providers?**\n \n-Yes, we support stop sequences - and you can pass as many as allowed by Hugging Face (or any provider!)\n+> You pay the standard provider API rates with no additional markup - Hugging Face simply passes through the provider costs. Note that [Hugging Face PRO](https://huggingface.co/subscribe/pro) users get $2 worth of Inference credits every month that can be used across providers.",
        "comment_created_at": "2025-02-06T17:29:26+00:00",
        "comment_author": "Wauplin",
        "comment_body": "```suggestion\r\n> Billing is centralized on your Hugging Face account, no matter which providers you are using. You are billed the standard provider API rates with no additional markup - Hugging Face simply passes through the provider costs. Note that [Hugging Face PRO](https://huggingface.co/subscribe/pro) users get $2 worth of Inference credits every month that can be used across providers.\r\n\r\n**Do I need to create an account for each Inference Provider?**\r\n\r\n> No, you don't need to create separate accounts. All requests are routed through Hugging Face, so you only need your HF token. This allows you to easily benchmark different providers and choose the one that best fits your needs.\r\n```\r\n\r\nI'd add a sentence to say that billing is centralized + an explicit question to emphasize why it's nice to use HF routing (==no need for separate accounts)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1945493298",
    "pr_number": 8258,
    "pr_file": "docs/my-website/docs/providers/huggingface.md",
    "created_at": "2025-02-06T21:31:23+00:00",
    "commented_code": "import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n ```\n-\n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-2. Start the proxy\n+### Getting Started\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n ```\n+huggingface/<provider>/<model_id>\n+```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.\n+By default, if you don't specify a provider, LiteLLM will use the [HF Inference API](https://huggingface.co/docs/api-inference/en/index).\n \n-3. Test it!\n+Examples:\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"llama-3.1-8B-instruct\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+# Run DeepSeek-R1 inference through Together AI\n+completion(model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",...)\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"classification\" label=\"Text Classification\">\n+# Run Qwen2.5-72B-Instruct inference through Sambanova\n+completion(model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\",...)\n+```\n \n-Append `text-classification` to the model name\n+For Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client.\n \n-e.g. `huggingface/text-classification/<model-name>`\n+<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n+  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n+</a>\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Basic Completion\n+Here's an example of chat completion using the DeepSeek-R1 model through Together AI:\n \n ```python\n import os\n from litellm import completion\n \n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n-\n-messages = [{ \"content\": \"I like you, I love you!\",\"role\": \"user\"}]\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-# e.g. Call 'shahrukhx01/question-vs-statement-classifier' hosted on HF Inference endpoints\n response = completion(\n-  model=\"huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\",\n-  messages=messages,\n-  api_base=\"https://my-endpoint.endpoints.huggingface.cloud\",\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}]\n )\n-\n print(response)\n ```\n \n-</TabItem> \n-<TabItem value=\"proxy\" label=\"PROXY\">\n+### Streaming\n+Now, let's see what a streaming request looks like.\n \n-1. Add models to your config.yaml\n+```python\n+import os\n+from litellm import completion\n \n-```yaml\n-model_list:\n-  - model_name: bert-classifier\n-    litellm_params:\n-      model: huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://my-endpoint.endpoints.huggingface.cloud\"\n-```\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-2. Start the proxy\n+response = completion(\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}],\n+    stream=True\n+)\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+for chunk in response:\n+    print(chunk)\n ```\n \n-3. Test it!\n+### Image Input\n+You can also pass images to the model.\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"bert-classifier\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+from litellm import completion\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"dedicated\" label=\"Dedicated Inference Endpoints\">\n+# Set your Hugging Face Token\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+messages=[\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\n+                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n+                    }\n+                },\n+            ],\n+        }\n+    ]\n \n-Steps to use\n-* Create your own Hugging Face dedicated endpoint here: https://ui.endpoints.huggingface.co/\n-* Set `api_base` to your deployed api base\n-* Add the `huggingface/` prefix to your model so litellm knows it's a huggingface Deployed Inference Endpoint\n+response = completion(\n+    model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\", \n+    messages=messages,\n+)\n+print(response.choices[0])\n+```\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Function Calling\n+You can extend the model's capabilities by giving them access to tools, here is an example with function calling using the Qwen2.5-72B-Instruct model through Sambanova.\n \n ```python\n import os\n from litellm import completion\n \n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n+# Set your Hugging Face Token\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+tools = [\n+  {\n+    \"type\": \"function\",\n+    \"function\": {\n+      \"name\": \"get_current_weather\",\n+      \"description\": \"Get the current weather in a given location\",\n+      \"parameters\": {\n+        \"type\": \"object\",\n+        \"properties\": {\n+          \"location\": {\n+            \"type\": \"string\",\n+            \"description\": \"The city and state, e.g. San Francisco, CA\",\n+          },\n+          \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n+        },\n+        \"required\": [\"location\"],\n+      },\n+    }\n+  }\n+]\n+messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n \n-# TGI model: Call https://huggingface.co/glaiveai/glaive-coder-7b\n-# add the 'huggingface/' prefix to the model to set huggingface as the provider\n-# set api base to your deployed api endpoint from hugging face\n response = completion(\n-    model=\"huggingface/glaiveai/glaive-coder-7b\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    api_base=\"https://wjiegasee9bmqke2.us-east-1.aws.endpoints.huggingface.cloud\"\n+    model=\"huggingface/sambanova/meta-llama/Llama-3.3-70B-Instruct\", \n+    messages=messages,\n+    tools=tools,\n+    tool_choice=\"auto\"\n )\n print(response)\n ```\n \n </TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: glaive-coder\n-    litellm_params:\n-      model: huggingface/glaiveai/glaive-coder-7b\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://wjiegasee9bmqke2.us-east-1.aws.endpoints.huggingface.cloud\"\n-```\n \n-2. Start the proxy\n-\n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n-```\n-\n-3. Test it!\n-\n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"glaive-coder\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n-\n-</TabItem> \n-</Tabs>\n-\n-</TabItem>\n-</Tabs>\n-\n-## Streaming\n+<TabItem value=\"endpoints\" label=\"Inference Endpoints\">\n \n <a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n </a>\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+### Basic Completion\n+Connect to your deployed Hugging Face Inference Endpoint for model inference. This provides dedicated infrastructure for your specific use case.",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1945493298",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8258,
        "pr_file": "docs/my-website/docs/providers/huggingface.md",
        "discussion_id": "1945493298",
        "commented_code": "@@ -2,466 +2,363 @@ import Image from '@theme/IdealImage';\n import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n ```\n-\n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-2. Start the proxy\n+### Getting Started\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n ```\n+huggingface/<provider>/<model_id>\n+```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.\n+By default, if you don't specify a provider, LiteLLM will use the [HF Inference API](https://huggingface.co/docs/api-inference/en/index).\n \n-3. Test it!\n+Examples:\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"llama-3.1-8B-instruct\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+# Run DeepSeek-R1 inference through Together AI\n+completion(model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",...)\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"classification\" label=\"Text Classification\">\n+# Run Qwen2.5-72B-Instruct inference through Sambanova\n+completion(model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\",...)\n+```\n \n-Append `text-classification` to the model name\n+For Hugging Face models, LiteLLM uses Hugging Face's Python client [InferenceClient](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) under the hood. `InferenceClient` is a drop-in replacement for OpenAI client.\n \n-e.g. `huggingface/text-classification/<model-name>`\n+<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n+  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n+</a>\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Basic Completion\n+Here's an example of chat completion using the DeepSeek-R1 model through Together AI:\n \n ```python\n import os\n from litellm import completion\n \n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n-\n-messages = [{ \"content\": \"I like you, I love you!\",\"role\": \"user\"}]\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-# e.g. Call 'shahrukhx01/question-vs-statement-classifier' hosted on HF Inference endpoints\n response = completion(\n-  model=\"huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\",\n-  messages=messages,\n-  api_base=\"https://my-endpoint.endpoints.huggingface.cloud\",\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}]\n )\n-\n print(response)\n ```\n \n-</TabItem> \n-<TabItem value=\"proxy\" label=\"PROXY\">\n+### Streaming\n+Now, let's see what a streaming request looks like.\n \n-1. Add models to your config.yaml\n+```python\n+import os\n+from litellm import completion\n \n-```yaml\n-model_list:\n-  - model_name: bert-classifier\n-    litellm_params:\n-      model: huggingface/text-classification/shahrukhx01/question-vs-statement-classifier\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://my-endpoint.endpoints.huggingface.cloud\"\n-```\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n \n-2. Start the proxy\n+response = completion(\n+    model=\"huggingface/together/deepseek-ai/DeepSeek-R1\",\n+    messages=[{\"content\": \"How many r's are in the word `strawberry`?\", \"role\": \"user\"}],\n+    stream=True\n+)\n \n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n+for chunk in response:\n+    print(chunk)\n ```\n \n-3. Test it!\n+### Image Input\n+You can also pass images to the model.\n \n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"bert-classifier\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n+```python\n+from litellm import completion\n \n-</TabItem> \n-</Tabs>\n-</TabItem>\n-<TabItem value=\"dedicated\" label=\"Dedicated Inference Endpoints\">\n+# Set your Hugging Face Token\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+messages=[\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\n+                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n+                    }\n+                },\n+            ],\n+        }\n+    ]\n \n-Steps to use\n-* Create your own Hugging Face dedicated endpoint here: https://ui.endpoints.huggingface.co/\n-* Set `api_base` to your deployed api base\n-* Add the `huggingface/` prefix to your model so litellm knows it's a huggingface Deployed Inference Endpoint\n+response = completion(\n+    model=\"huggingface/sambanova/Qwen/Qwen2.5-72B-Instruct\", \n+    messages=messages,\n+)\n+print(response.choices[0])\n+```\n \n-<Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n+### Function Calling\n+You can extend the model's capabilities by giving them access to tools, here is an example with function calling using the Qwen2.5-72B-Instruct model through Sambanova.\n \n ```python\n import os\n from litellm import completion\n \n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"\n+# Set your Hugging Face Token\n+os.environ[\"HF_TOKEN\"] = \"hf_xxxxxx\"\n+\n+tools = [\n+  {\n+    \"type\": \"function\",\n+    \"function\": {\n+      \"name\": \"get_current_weather\",\n+      \"description\": \"Get the current weather in a given location\",\n+      \"parameters\": {\n+        \"type\": \"object\",\n+        \"properties\": {\n+          \"location\": {\n+            \"type\": \"string\",\n+            \"description\": \"The city and state, e.g. San Francisco, CA\",\n+          },\n+          \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n+        },\n+        \"required\": [\"location\"],\n+      },\n+    }\n+  }\n+]\n+messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n \n-# TGI model: Call https://huggingface.co/glaiveai/glaive-coder-7b\n-# add the 'huggingface/' prefix to the model to set huggingface as the provider\n-# set api base to your deployed api endpoint from hugging face\n response = completion(\n-    model=\"huggingface/glaiveai/glaive-coder-7b\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    api_base=\"https://wjiegasee9bmqke2.us-east-1.aws.endpoints.huggingface.cloud\"\n+    model=\"huggingface/sambanova/meta-llama/Llama-3.3-70B-Instruct\", \n+    messages=messages,\n+    tools=tools,\n+    tool_choice=\"auto\"\n )\n print(response)\n ```\n \n </TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n-\n-```yaml\n-model_list:\n-  - model_name: glaive-coder\n-    litellm_params:\n-      model: huggingface/glaiveai/glaive-coder-7b\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n-      api_base: \"https://wjiegasee9bmqke2.us-east-1.aws.endpoints.huggingface.cloud\"\n-```\n \n-2. Start the proxy\n-\n-```bash\n-$ litellm --config /path/to/config.yaml --debug\n-```\n-\n-3. Test it!\n-\n-```shell\n-curl --location 'http://0.0.0.0:4000/chat/completions' \\\n-    --header 'Authorization: Bearer sk-1234' \\\n-    --header 'Content-Type: application/json' \\\n-    --data '{\n-    \"model\": \"glaive-coder\",\n-    \"messages\": [\n-      {\n-          \"role\": \"user\",\n-          \"content\": \"I like you!\"\n-      }\n-      ],\n-}'\n-```\n-\n-</TabItem> \n-</Tabs>\n-\n-</TabItem>\n-</Tabs>\n-\n-## Streaming\n+<TabItem value=\"endpoints\" label=\"Inference Endpoints\">\n \n <a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n </a>\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+### Basic Completion\n+Connect to your deployed Hugging Face Inference Endpoint for model inference. This provides dedicated infrastructure for your specific use case.",
        "comment_created_at": "2025-02-06T21:31:23+00:00",
        "comment_author": "pcuenca",
        "comment_body": "```suggestion\r\nAfter you have [deployed your Hugging Face Inference Endpoint](https://endpoints.huggingface.co/new) on dedicated infrastructure, you can run inference on it by providing the endpoint base URL in `api_base`, and indicating `huggingface/tgi` as the model name:\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1946248130",
    "pr_number": 8258,
    "pr_file": "docs/my-website/docs/providers/huggingface.md",
    "created_at": "2025-02-07T09:41:25+00:00",
    "commented_code": "import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n+```\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n+### Getting Started\n \n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n+```\n+huggingface/<provider>/<model_id>\n ```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1946248130",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8258,
        "pr_file": "docs/my-website/docs/providers/huggingface.md",
        "discussion_id": "1946248130",
        "commented_code": "@@ -2,466 +2,370 @@ import Image from '@theme/IdealImage';\n import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n+```\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n+### Getting Started\n \n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n+```\n+huggingface/<provider>/<model_id>\n ```\n+Where `<model_id>` is the Hugging Face model ID and `<provider>` is the inference provider.",
        "comment_created_at": "2025-02-07T09:41:25+00:00",
        "comment_author": "julien-c",
        "comment_body": "```suggestion\r\nWhere `<hf_org_or_user>/<hf_model>` is the Hugging Face model ID and `<provider>` is the inference provider.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2134154084",
    "pr_number": 11515,
    "pr_file": "docs/my-website/docs/completion/web_search.md",
    "created_at": "2025-06-07T21:08:13+00:00",
    "commented_code": "\"content\": \"What was a positive news story from today?\",\n         }\n     ],\n+    web_search_options={\n+        \"search_context_size\": \"medium\"  # Options: \"low\", \"medium\", \"high\"\n+    }\n )\n ```\n+\n </TabItem>\n <TabItem value=\"proxy\" label=\"PROXY\">\n \n 1. Setup config.yaml\n \n ```yaml\n model_list:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2134154084",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11515,
        "pr_file": "docs/my-website/docs/completion/web_search.md",
        "discussion_id": "2134154084",
        "commented_code": "@@ -31,19 +31,43 @@ response = completion(\n             \"content\": \"What was a positive news story from today?\",\n         }\n     ],\n+    web_search_options={\n+        \"search_context_size\": \"medium\"  # Options: \"low\", \"medium\", \"high\"\n+    }\n )\n ```\n+\n </TabItem>\n <TabItem value=\"proxy\" label=\"PROXY\">\n \n 1. Setup config.yaml\n \n ```yaml\n model_list:",
        "comment_created_at": "2025-06-07T21:08:13+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "@colesmcintosh i think the user wanted to see how to set search options in the config - which is currently missing in the docs ",
        "pr_file_module": null
      },
      {
        "comment_id": "2134154122",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11515,
        "pr_file": "docs/my-website/docs/completion/web_search.md",
        "discussion_id": "2134154084",
        "commented_code": "@@ -31,19 +31,43 @@ response = completion(\n             \"content\": \"What was a positive news story from today?\",\n         }\n     ],\n+    web_search_options={\n+        \"search_context_size\": \"medium\"  # Options: \"low\", \"medium\", \"high\"\n+    }\n )\n ```\n+\n </TabItem>\n <TabItem value=\"proxy\" label=\"PROXY\">\n \n 1. Setup config.yaml\n \n ```yaml\n model_list:",
        "comment_created_at": "2025-06-07T21:08:41+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "```yaml\r\n  - model_name: grok-3\r\n    litellm_params:\r\n      model: xai/grok-3\r\n      api_key: os.environ/XAI_API_KEY\r\n      web_search_options: {} # \ud83d\udc48 KEY CHANGE\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2078762086",
    "pr_number": 10638,
    "pr_file": "docs/my-website/docs/providers/nscale.md",
    "created_at": "2025-05-08T01:52:50+00:00",
    "commented_code": "+# NScale (EU Sovereign)\n+**[NScale docs](https://docs.nscale.com/docs/getting-started/overview)**\n+\n+NScale is a European-domiciled full-stack AI cloud platform that allows you to scale your workloads securely, sustainably and cost-effectively - without sacrificing control. It provides production-grade reliability for serverless deployments of LLMs with full data sovereignty and compliance baked in.\n+\n+## Key Features\n+- **EU Sovereign**: Full data sovereignty and compliance with European regulations\n+- **Ultra-Low Cost (starting at $0.01 / M tokens)**: Extremely competitive pricing for both text and image generation models\n+- **Production Grade**: Reliable serverless deployments with full isolation\n+- **No Setup Required**: Instant access to compute without infrastructure management\n+- **Full Control**: Your data remains private and isolated\n+\n+## API Key\n+```python\n+# env variable\n+os.environ['NSCALE_API_KEY']\n+```\n+\n+## Sample Usage - Text Generation\n+```python\n+from litellm import completion\n+import os\n+\n+os.environ['NSCALE_API_KEY'] = \"\"\n+response = completion(\n+    model=\"nscale/meta-llama/Llama-4-Scout-17B-16E-Instruct\", \n+    messages=[\n+        {\"role\": \"user\", \"content\": \"What is LiteLLM?\"}\n+    ]\n+)\n+print(response)\n+```\n+\n+## Sample Usage - Image Generation\n+```python\n+from litellm import image_generation\n+import os\n+\n+os.environ['NSCALE_API_KEY'] = \"\"\n+response = image_generation(\n+    model=\"nscale/stabilityai/stable-diffusion-xl-base-1.0\",\n+    prompt=\"A beautiful sunset over mountains\",\n+    n=1,\n+    size=\"1024x1024\"\n+)\n+print(response)\n+```",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2078762086",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10638,
        "pr_file": "docs/my-website/docs/providers/nscale.md",
        "discussion_id": "2078762086",
        "commented_code": "@@ -0,0 +1,85 @@\n+# NScale (EU Sovereign)\n+**[NScale docs](https://docs.nscale.com/docs/getting-started/overview)**\n+\n+NScale is a European-domiciled full-stack AI cloud platform that allows you to scale your workloads securely, sustainably and cost-effectively - without sacrificing control. It provides production-grade reliability for serverless deployments of LLMs with full data sovereignty and compliance baked in.\n+\n+## Key Features\n+- **EU Sovereign**: Full data sovereignty and compliance with European regulations\n+- **Ultra-Low Cost (starting at $0.01 / M tokens)**: Extremely competitive pricing for both text and image generation models\n+- **Production Grade**: Reliable serverless deployments with full isolation\n+- **No Setup Required**: Instant access to compute without infrastructure management\n+- **Full Control**: Your data remains private and isolated\n+\n+## API Key\n+```python\n+# env variable\n+os.environ['NSCALE_API_KEY']\n+```\n+\n+## Sample Usage - Text Generation\n+```python\n+from litellm import completion\n+import os\n+\n+os.environ['NSCALE_API_KEY'] = \"\"\n+response = completion(\n+    model=\"nscale/meta-llama/Llama-4-Scout-17B-16E-Instruct\", \n+    messages=[\n+        {\"role\": \"user\", \"content\": \"What is LiteLLM?\"}\n+    ]\n+)\n+print(response)\n+```\n+\n+## Sample Usage - Image Generation\n+```python\n+from litellm import image_generation\n+import os\n+\n+os.environ['NSCALE_API_KEY'] = \"\"\n+response = image_generation(\n+    model=\"nscale/stabilityai/stable-diffusion-xl-base-1.0\",\n+    prompt=\"A beautiful sunset over mountains\",\n+    n=1,\n+    size=\"1024x1024\"\n+)\n+print(response)\n+```",
        "comment_created_at": "2025-05-08T01:52:50+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "add litellm proxy usage here too: https://docs.litellm.ai/docs/providers/meta_llama#usage---litellm-proxy ",
        "pr_file_module": null
      }
    ]
  }
]