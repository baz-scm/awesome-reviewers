[
  {
    "discussion_id": "2268188948",
    "pr_number": 12992,
    "pr_file": "docs/my-website/docs/providers/heroku.md",
    "created_at": "2025-08-11T23:04:13+00:00",
    "commented_code": "+# Heroku\n+\n+## Provision a Model\n+\n+To use the Heroku provider for LiteLLM, you must first configure a Heroku app, and attach one of the models listed in the [Supported Models](#supported-models) section.\n+\n+To get configure a Heroku app with an attached model, please refer to [Heroku's documentation](https://devcenter.heroku.com/articles/heroku-inference).\n+\n+## Supported Models\n+\n+The Heroku provider for LiteLLM currently, only supports [chat](https://devcenter.heroku.com/articles/heroku-inference-api-v1-chat-completions). Supported chat models are:\n+\n+| Model                             | Region  |\n+|-----------------------------------|---------|\n+| [`heroku/claude-sonnet-4`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-4-sonnet)          | US, EU  |\n+| [`heroku/claude-3-7-sonnet`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-7-sonnet)        | US, EU  |\n+| [`heroku/claude-3-5-sonnet-latest`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-sonnet-latest) | US      |\n+| [`heroku/claude-3-5-haiku`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-haiku)         | US      |\n+| [`heroku/claude-3`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-haiku)                 | EU      |\n+\n+## Environment Variables\n+\n+When a model is attached to a Heroku app, three config variables are set:\n+\n+- `INFERENCE_KEY`: The API key used for authenticating requests to the model.\n+- `INFERENCE_MODEL_ID`: The name of the model. E.g. `claude-3-5-haiku`.\n+- `INFERENCE_URL`: The base URL for calling the model.\n+\n+It is important to note that the values for `INFERENCE_KEY` and `INFERENCE_URL` will be required for making calls to your model. More details follow in the [Usage Examples](#usage-examples) section.\n+\n+For a deeper explanation of these variables, see the official [Heroku documentation](https://devcenter.heroku.com/articles/heroku-inference#model-resource-config-vars).\n+\n+## Usage Examples\n+### Using Config Variables\n+\n+The Heroku provider is aware of the following config variables, and will use them, if present:\n+\n+- `HEROKU_API_KEY`: This value corresponds to the [`api_key` param](https://docs.litellm.ai/docs/set_keys#litellmapi_key). Set this to the value of Heroku's `INFERENCE_KEY` config variable.\n+- `HEROKU_API_BASE`: This value corresponds to the [`api_base` param](https://docs.litellm.ai/docs/set_keys#litellmapi_base). Set this to the value of Heroku's `INFERENCE_URL` config variable.\n+\n+In this example, we don't explicitly pass the `api_key` and `api_base`. We, instead, set the config variables which will be used by the Heroku provider.\n+\n+```python\n+import os\n+from litellm import completion\n+\n+os.environ[\"HEROKU_API_BASE\"] = \"https://us.inference.heroku.com\"\n+os.environ[\"HEROKU_API_KEY\"] = \"fake-heroku-key\"\n+\n+response = completion(\n+    model=\"heroku/claude-3-5-haiku\",\n+    messages=[\n+        {\"role\": \"user\", \"content\": \"write code for saying hey from LiteLLM\"}\n+    ]\n+)\n+\n+print(response)\n+```\n+",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2268188948",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12992,
        "pr_file": "docs/my-website/docs/providers/heroku.md",
        "discussion_id": "2268188948",
        "commented_code": "@@ -0,0 +1,77 @@\n+# Heroku\n+\n+## Provision a Model\n+\n+To use the Heroku provider for LiteLLM, you must first configure a Heroku app, and attach one of the models listed in the [Supported Models](#supported-models) section.\n+\n+To get configure a Heroku app with an attached model, please refer to [Heroku's documentation](https://devcenter.heroku.com/articles/heroku-inference).\n+\n+## Supported Models\n+\n+The Heroku provider for LiteLLM currently, only supports [chat](https://devcenter.heroku.com/articles/heroku-inference-api-v1-chat-completions). Supported chat models are:\n+\n+| Model                             | Region  |\n+|-----------------------------------|---------|\n+| [`heroku/claude-sonnet-4`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-4-sonnet)          | US, EU  |\n+| [`heroku/claude-3-7-sonnet`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-7-sonnet)        | US, EU  |\n+| [`heroku/claude-3-5-sonnet-latest`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-sonnet-latest) | US      |\n+| [`heroku/claude-3-5-haiku`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-haiku)         | US      |\n+| [`heroku/claude-3`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-haiku)                 | EU      |\n+\n+## Environment Variables\n+\n+When a model is attached to a Heroku app, three config variables are set:\n+\n+- `INFERENCE_KEY`: The API key used for authenticating requests to the model.\n+- `INFERENCE_MODEL_ID`: The name of the model. E.g. `claude-3-5-haiku`.\n+- `INFERENCE_URL`: The base URL for calling the model.\n+\n+It is important to note that the values for `INFERENCE_KEY` and `INFERENCE_URL` will be required for making calls to your model. More details follow in the [Usage Examples](#usage-examples) section.\n+\n+For a deeper explanation of these variables, see the official [Heroku documentation](https://devcenter.heroku.com/articles/heroku-inference#model-resource-config-vars).\n+\n+## Usage Examples\n+### Using Config Variables\n+\n+The Heroku provider is aware of the following config variables, and will use them, if present:\n+\n+- `HEROKU_API_KEY`: This value corresponds to the [`api_key` param](https://docs.litellm.ai/docs/set_keys#litellmapi_key). Set this to the value of Heroku's `INFERENCE_KEY` config variable.\n+- `HEROKU_API_BASE`: This value corresponds to the [`api_base` param](https://docs.litellm.ai/docs/set_keys#litellmapi_base). Set this to the value of Heroku's `INFERENCE_URL` config variable.\n+\n+In this example, we don't explicitly pass the `api_key` and `api_base`. We, instead, set the config variables which will be used by the Heroku provider.\n+\n+```python\n+import os\n+from litellm import completion\n+\n+os.environ[\"HEROKU_API_BASE\"] = \"https://us.inference.heroku.com\"\n+os.environ[\"HEROKU_API_KEY\"] = \"fake-heroku-key\"\n+\n+response = completion(\n+    model=\"heroku/claude-3-5-haiku\",\n+    messages=[\n+        {\"role\": \"user\", \"content\": \"write code for saying hey from LiteLLM\"}\n+    ]\n+)\n+\n+print(response)\n+```\n+",
        "comment_created_at": "2025-08-11T23:04:13+00:00",
        "comment_author": "claire-riley",
        "comment_body": "```suggestion\r\n\r\n> Include the `heroku/` prefix in the model name so LiteLLM knows the model provider to use.\r\n\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2268189082",
    "pr_number": 12992,
    "pr_file": "docs/my-website/docs/providers/heroku.md",
    "created_at": "2025-08-11T23:04:20+00:00",
    "commented_code": "+# Heroku\n+\n+## Provision a Model\n+\n+To use the Heroku provider for LiteLLM, you must first configure a Heroku app, and attach one of the models listed in the [Supported Models](#supported-models) section.\n+\n+To get configure a Heroku app with an attached model, please refer to [Heroku's documentation](https://devcenter.heroku.com/articles/heroku-inference).\n+\n+## Supported Models\n+\n+The Heroku provider for LiteLLM currently, only supports [chat](https://devcenter.heroku.com/articles/heroku-inference-api-v1-chat-completions). Supported chat models are:\n+\n+| Model                             | Region  |\n+|-----------------------------------|---------|\n+| [`heroku/claude-sonnet-4`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-4-sonnet)          | US, EU  |\n+| [`heroku/claude-3-7-sonnet`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-7-sonnet)        | US, EU  |\n+| [`heroku/claude-3-5-sonnet-latest`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-sonnet-latest) | US      |\n+| [`heroku/claude-3-5-haiku`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-haiku)         | US      |\n+| [`heroku/claude-3`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-haiku)                 | EU      |\n+\n+## Environment Variables\n+\n+When a model is attached to a Heroku app, three config variables are set:\n+\n+- `INFERENCE_KEY`: The API key used for authenticating requests to the model.\n+- `INFERENCE_MODEL_ID`: The name of the model. E.g. `claude-3-5-haiku`.\n+- `INFERENCE_URL`: The base URL for calling the model.\n+\n+It is important to note that the values for `INFERENCE_KEY` and `INFERENCE_URL` will be required for making calls to your model. More details follow in the [Usage Examples](#usage-examples) section.\n+\n+For a deeper explanation of these variables, see the official [Heroku documentation](https://devcenter.heroku.com/articles/heroku-inference#model-resource-config-vars).\n+\n+## Usage Examples\n+### Using Config Variables\n+\n+The Heroku provider is aware of the following config variables, and will use them, if present:\n+\n+- `HEROKU_API_KEY`: This value corresponds to the [`api_key` param](https://docs.litellm.ai/docs/set_keys#litellmapi_key). Set this to the value of Heroku's `INFERENCE_KEY` config variable.\n+- `HEROKU_API_BASE`: This value corresponds to the [`api_base` param](https://docs.litellm.ai/docs/set_keys#litellmapi_base). Set this to the value of Heroku's `INFERENCE_URL` config variable.\n+\n+In this example, we don't explicitly pass the `api_key` and `api_base`. We, instead, set the config variables which will be used by the Heroku provider.\n+\n+```python\n+import os\n+from litellm import completion\n+\n+os.environ[\"HEROKU_API_BASE\"] = \"https://us.inference.heroku.com\"\n+os.environ[\"HEROKU_API_KEY\"] = \"fake-heroku-key\"\n+\n+response = completion(\n+    model=\"heroku/claude-3-5-haiku\",\n+    messages=[\n+        {\"role\": \"user\", \"content\": \"write code for saying hey from LiteLLM\"}\n+    ]\n+)\n+\n+print(response)\n+```\n+\n+### Explicitly Setting `api_key` and `api_base`\n+\n+```python\n+from litellm import completion\n+\n+response = completion(\n+    model=\"heroku/claude-sonnet-4\",\n+    api_key=\"fake-heroku-key\",\n+    api_base=\"https://us.inference.heroku.com\",\n+    messages=[\n+        {\"role\": \"user\", \"content\": \"write code for saying hey from LiteLLM\"}\n+    ],\n+)\n+```\n+",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2268189082",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12992,
        "pr_file": "docs/my-website/docs/providers/heroku.md",
        "discussion_id": "2268189082",
        "commented_code": "@@ -0,0 +1,77 @@\n+# Heroku\n+\n+## Provision a Model\n+\n+To use the Heroku provider for LiteLLM, you must first configure a Heroku app, and attach one of the models listed in the [Supported Models](#supported-models) section.\n+\n+To get configure a Heroku app with an attached model, please refer to [Heroku's documentation](https://devcenter.heroku.com/articles/heroku-inference).\n+\n+## Supported Models\n+\n+The Heroku provider for LiteLLM currently, only supports [chat](https://devcenter.heroku.com/articles/heroku-inference-api-v1-chat-completions). Supported chat models are:\n+\n+| Model                             | Region  |\n+|-----------------------------------|---------|\n+| [`heroku/claude-sonnet-4`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-4-sonnet)          | US, EU  |\n+| [`heroku/claude-3-7-sonnet`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-7-sonnet)        | US, EU  |\n+| [`heroku/claude-3-5-sonnet-latest`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-sonnet-latest) | US      |\n+| [`heroku/claude-3-5-haiku`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-5-haiku)         | US      |\n+| [`heroku/claude-3`](https://devcenter.heroku.com/articles/heroku-inference-api-model-claude-3-haiku)                 | EU      |\n+\n+## Environment Variables\n+\n+When a model is attached to a Heroku app, three config variables are set:\n+\n+- `INFERENCE_KEY`: The API key used for authenticating requests to the model.\n+- `INFERENCE_MODEL_ID`: The name of the model. E.g. `claude-3-5-haiku`.\n+- `INFERENCE_URL`: The base URL for calling the model.\n+\n+It is important to note that the values for `INFERENCE_KEY` and `INFERENCE_URL` will be required for making calls to your model. More details follow in the [Usage Examples](#usage-examples) section.\n+\n+For a deeper explanation of these variables, see the official [Heroku documentation](https://devcenter.heroku.com/articles/heroku-inference#model-resource-config-vars).\n+\n+## Usage Examples\n+### Using Config Variables\n+\n+The Heroku provider is aware of the following config variables, and will use them, if present:\n+\n+- `HEROKU_API_KEY`: This value corresponds to the [`api_key` param](https://docs.litellm.ai/docs/set_keys#litellmapi_key). Set this to the value of Heroku's `INFERENCE_KEY` config variable.\n+- `HEROKU_API_BASE`: This value corresponds to the [`api_base` param](https://docs.litellm.ai/docs/set_keys#litellmapi_base). Set this to the value of Heroku's `INFERENCE_URL` config variable.\n+\n+In this example, we don't explicitly pass the `api_key` and `api_base`. We, instead, set the config variables which will be used by the Heroku provider.\n+\n+```python\n+import os\n+from litellm import completion\n+\n+os.environ[\"HEROKU_API_BASE\"] = \"https://us.inference.heroku.com\"\n+os.environ[\"HEROKU_API_KEY\"] = \"fake-heroku-key\"\n+\n+response = completion(\n+    model=\"heroku/claude-3-5-haiku\",\n+    messages=[\n+        {\"role\": \"user\", \"content\": \"write code for saying hey from LiteLLM\"}\n+    ]\n+)\n+\n+print(response)\n+```\n+\n+### Explicitly Setting `api_key` and `api_base`\n+\n+```python\n+from litellm import completion\n+\n+response = completion(\n+    model=\"heroku/claude-sonnet-4\",\n+    api_key=\"fake-heroku-key\",\n+    api_base=\"https://us.inference.heroku.com\",\n+    messages=[\n+        {\"role\": \"user\", \"content\": \"write code for saying hey from LiteLLM\"}\n+    ],\n+)\n+```\n+",
        "comment_created_at": "2025-08-11T23:04:20+00:00",
        "comment_author": "claire-riley",
        "comment_body": "```suggestion\r\n\r\n> Include the `heroku/` prefix in the model name so LiteLLM knows the model provider to use.\r\n\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2013490674",
    "pr_number": 9443,
    "pr_file": "docs/my-website/docs/providers/snowflake.md",
    "created_at": "2025-03-26T06:49:24+00:00",
    "commented_code": "messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n )\n ```\n+### Adding an api_base: \n \n-## Usage with LiteLLM Proxy \n+If you want to specify the API base, you don't have to put the full snowflake url:\n \n-#### 1. Required env variables\n-```bash\n-export SNOWFLAKE_JWT=\"\"\n-export SNOWFLAKE_ACCOUNT_ID = \"\"\n+e.g `https://{account-id}.snowflakecomputing.com/api/v2/cortex/inference:complete/`\n+\n+Instead, you can just pass\n+```python\n+response = completion(\n+    model=\"snowflake/mistral-7b\", \n+    messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n+    api_base = \"snowflakecomputing.com\"\n+)\n ```\n+and Litellm will add the `/api/v2/cortex/inference:complete/` and the `account-id` for you.\n+\n+## Usage with LiteLLM Proxy \n+\n \n-#### 2. Start the proxy~\n+#### 1. Start the proxy~\n ```yaml\n model_list:\n   - model_name: mistral-7b\n     litellm_params:\n         model: snowflake/mistral-7b\n-        api_key: YOUR_API_KEY\n-        api_base: https://YOUR-ACCOUNT-ID.snowflakecomputing.com/api/v2/cortex/inference:complete\n+        snowflake_jwt: <JWT>",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2013490674",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9443,
        "pr_file": "docs/my-website/docs/providers/snowflake.md",
        "discussion_id": "2013490674",
        "commented_code": "@@ -49,26 +49,38 @@ response = completion(\n     messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n )\n ```\n+### Adding an api_base: \n \n-## Usage with LiteLLM Proxy \n+If you want to specify the API base, you don't have to put the full snowflake url:\n \n-#### 1. Required env variables\n-```bash\n-export SNOWFLAKE_JWT=\"\"\n-export SNOWFLAKE_ACCOUNT_ID = \"\"\n+e.g `https://{account-id}.snowflakecomputing.com/api/v2/cortex/inference:complete/`\n+\n+Instead, you can just pass\n+```python\n+response = completion(\n+    model=\"snowflake/mistral-7b\", \n+    messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n+    api_base = \"snowflakecomputing.com\"\n+)\n ```\n+and Litellm will add the `/api/v2/cortex/inference:complete/` and the `account-id` for you.\n+\n+## Usage with LiteLLM Proxy \n+\n \n-#### 2. Start the proxy~\n+#### 1. Start the proxy~\n ```yaml\n model_list:\n   - model_name: mistral-7b\n     litellm_params:\n         model: snowflake/mistral-7b\n-        api_key: YOUR_API_KEY\n-        api_base: https://YOUR-ACCOUNT-ID.snowflakecomputing.com/api/v2/cortex/inference:complete\n+        snowflake_jwt: <JWT>",
        "comment_created_at": "2025-03-26T06:49:24+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "inconsistent param usage.\r\n\r\nThe example on sdk says api_base, that's what it should be here",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2212362611",
    "pr_number": 12650,
    "pr_file": "docs/my-website/docs/tutorials/claude_responses_api.md",
    "created_at": "2025-07-17T06:08:45+00:00",
    "commented_code": "# RUNNING on http://0.0.0.0:4000\n ```\n \n-### 3. Test it! (Curl)\n+### 3. Verify Setup\n+\n+Test that your proxy is working correctly:\n \n ```bash\n curl -X POST http://0.0.0.0:4000/v1/messages \\\n--H \"Authorization: Bearer sk-proj-1234567890\" \\\n+-H \"Authorization: Bearer $LITELLM_MASTER_KEY\" \\\n -H \"Content-Type: application/json\" \\\n -d '{\n     \"model\": \"codex-mini\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n }'\n ```\n \n-### 4. Test it! (Claude Code)\n+### 4. Configure Claude Code\n \n-- Setup environment variables\n+Setup Claude Code to use your LiteLLM proxy:\n \n ```bash\n export ANTHROPIC_BASE_URL=\"http://0.0.0.0:4000\"",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2212362611",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12650,
        "pr_file": "docs/my-website/docs/tutorials/claude_responses_api.md",
        "discussion_id": "2212362611",
        "commented_code": "@@ -30,33 +63,111 @@ litellm --config /path/to/config.yaml\n # RUNNING on http://0.0.0.0:4000\n ```\n \n-### 3. Test it! (Curl)\n+### 3. Verify Setup\n+\n+Test that your proxy is working correctly:\n \n ```bash\n curl -X POST http://0.0.0.0:4000/v1/messages \\\n--H \"Authorization: Bearer sk-proj-1234567890\" \\\n+-H \"Authorization: Bearer $LITELLM_MASTER_KEY\" \\\n -H \"Content-Type: application/json\" \\\n -d '{\n     \"model\": \"codex-mini\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n }'\n ```\n \n-### 4. Test it! (Claude Code)\n+### 4. Configure Claude Code\n \n-- Setup environment variables\n+Setup Claude Code to use your LiteLLM proxy:\n \n ```bash\n export ANTHROPIC_BASE_URL=\"http://0.0.0.0:4000\"",
        "comment_created_at": "2025-07-17T06:08:45+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "@colesmcintosh these are still not the right env vars - \r\n\r\nit should be `ANTHROPIC_AUTH_TOKEN` and `ANTHROPIC_BASE_URL` - https://docs.anthropic.com/en/docs/claude-code/llm-gateway#basic-litellm-setup",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2132639683",
    "pr_number": 11440,
    "pr_file": "README.md",
    "created_at": "2025-06-06T18:07:46+00:00",
    "commented_code": "Call any model supported by a provider, with `model=<provider_name>/<model_name>`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)\n \n+## \ud83c\udd95 Chat Provider for OpenAI Responses API\n+\n+Access OpenAI's responses-only models through the familiar Chat Completions API. Perfect for legacy tools that need to use new models like `gpt-4o` with enhanced reasoning.\n+\n+```python\n+# Use responses-only models through chat completions API\n+response = completion(model=\"chat/gpt-4o\", messages=messages)",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2132639683",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11440,
        "pr_file": "README.md",
        "discussion_id": "2132639683",
        "commented_code": "@@ -111,6 +111,32 @@ print(response)\n \n Call any model supported by a provider, with `model=<provider_name>/<model_name>`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)\n \n+## \ud83c\udd95 Chat Provider for OpenAI Responses API\n+\n+Access OpenAI's responses-only models through the familiar Chat Completions API. Perfect for legacy tools that need to use new models like `gpt-4o` with enhanced reasoning.\n+\n+```python\n+# Use responses-only models through chat completions API\n+response = completion(model=\"chat/gpt-4o\", messages=messages)",
        "comment_created_at": "2025-06-06T18:07:46+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "This route should be `openai/responses/<>` since you're really just calling openai/responses ",
        "pr_file_module": null
      }
    ]
  }
]