[
  {
    "discussion_id": "2284134784",
    "pr_number": 13691,
    "pr_file": "litellm/litellm_core_utils/get_supported_openai_params.py",
    "created_at": "2025-08-19T05:47:06+00:00",
    "commented_code": "from litellm.llms.elevenlabs.audio_transcription.transformation import (\n                 ElevenLabsAudioTranscriptionConfig,\n             )\n-            return (\n-                ElevenLabsAudioTranscriptionConfig().get_supported_openai_params(\n-                    model=model\n-                )\n+\n+            return ElevenLabsAudioTranscriptionConfig().get_supported_openai_params(\n+                model=model\n             )\n+    elif custom_llm_provider == \"github_copilot\":",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2284134784",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13691,
        "pr_file": "litellm/litellm_core_utils/get_supported_openai_params.py",
        "discussion_id": "2284134784",
        "commented_code": "@@ -266,11 +266,12 @@ def get_supported_openai_params(  # noqa: PLR0915\n             from litellm.llms.elevenlabs.audio_transcription.transformation import (\n                 ElevenLabsAudioTranscriptionConfig,\n             )\n-            return (\n-                ElevenLabsAudioTranscriptionConfig().get_supported_openai_params(\n-                    model=model\n-                )\n+\n+            return ElevenLabsAudioTranscriptionConfig().get_supported_openai_params(\n+                model=model\n             )\n+    elif custom_llm_provider == \"github_copilot\":",
        "comment_created_at": "2025-08-19T05:47:06+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "this is not needed. \r\n\r\nprovider_config_manager on line 44 should already handle this \r\n\r\n```\r\n    if provider_config and request_type == \"chat_completion\":\r\n        return provider_config.get_supported_openai_params(model=model)\r\n```\r\n\r\nplease remove this",
        "pr_file_module": null
      },
      {
        "comment_id": "2284206267",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13691,
        "pr_file": "litellm/litellm_core_utils/get_supported_openai_params.py",
        "discussion_id": "2284134784",
        "commented_code": "@@ -266,11 +266,12 @@ def get_supported_openai_params(  # noqa: PLR0915\n             from litellm.llms.elevenlabs.audio_transcription.transformation import (\n                 ElevenLabsAudioTranscriptionConfig,\n             )\n-            return (\n-                ElevenLabsAudioTranscriptionConfig().get_supported_openai_params(\n-                    model=model\n-                )\n+\n+            return ElevenLabsAudioTranscriptionConfig().get_supported_openai_params(\n+                model=model\n             )\n+    elif custom_llm_provider == \"github_copilot\":",
        "comment_created_at": "2025-08-19T06:22:02+00:00",
        "comment_author": "timelfrink",
        "comment_body": "Fixed - removed the redundant check since provider_config_manager already handles this.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1994317875",
    "pr_number": 9215,
    "pr_file": "litellm/llms/anthropic/chat/transformation.py",
    "created_at": "2025-03-13T21:20:09+00:00",
    "commented_code": "optional_params[\"metadata\"] = {\"user_id\": value}\n             if param == \"thinking\":\n                 optional_params[\"thinking\"] = value\n+            if param == \"reasoning_effort\" and optional_params.get(\"thinking\",None)==None:               \n+                thinking_config = ParameterMapping.map_reasoning_to_thinking(value)",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1994317875",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9215,
        "pr_file": "litellm/llms/anthropic/chat/transformation.py",
        "discussion_id": "1994317875",
        "commented_code": "@@ -362,6 +365,12 @@ def map_openai_params(\n                 optional_params[\"metadata\"] = {\"user_id\": value}\n             if param == \"thinking\":\n                 optional_params[\"thinking\"] = value\n+            if param == \"reasoning_effort\" and optional_params.get(\"thinking\",None)==None:               \n+                thinking_config = ParameterMapping.map_reasoning_to_thinking(value)",
        "comment_created_at": "2025-03-13T21:20:09+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "good job, let's not create a new class for this \r\n\r\nWe already have map_openai_params in anthropic transformation, just add a static method there instead of making a new `ParameterMapping` class",
        "pr_file_module": null
      },
      {
        "comment_id": "2001650021",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9215,
        "pr_file": "litellm/llms/anthropic/chat/transformation.py",
        "discussion_id": "1994317875",
        "commented_code": "@@ -362,6 +365,12 @@ def map_openai_params(\n                 optional_params[\"metadata\"] = {\"user_id\": value}\n             if param == \"thinking\":\n                 optional_params[\"thinking\"] = value\n+            if param == \"reasoning_effort\" and optional_params.get(\"thinking\",None)==None:               \n+                thinking_config = ParameterMapping.map_reasoning_to_thinking(value)",
        "comment_created_at": "2025-03-18T17:54:28+00:00",
        "comment_author": "MullaAhmed",
        "comment_body": "resolved the issues in the latest commit",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1780209134",
    "pr_number": 5974,
    "pr_file": "litellm/llms/snowflake/completion.py",
    "created_at": "2024-09-29T23:54:06+00:00",
    "commented_code": "+\"\"\"\n+Handler for Snowflake's Cortex Complete endpoint\n+\"\"\"\n+\n+import json\n+import time\n+import types\n+from typing import Callable, Optional\n+\n+import httpx\n+\n+from litellm.llms.snowflake.snowflake_utils import ModelResponseIterator\n+from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler, HTTPHandler\n+from litellm.utils import CustomStreamWrapper, ModelResponse, Usage\n+\n+from ..base import BaseLLM\n+from ..prompt_templates.factory import custom_prompt, prompt_factory\n+\n+\n+class SnowflakeError(Exception):\n+    def __init__(self, status_code, message):\n+        self.status_code = status_code\n+        self.message = message\n+        self.request = httpx.Request(method=\"POST\", url=\"https://docs.snowflake.com/\")\n+        self.response = httpx.Response(status_code=status_code, request=self.request)\n+        super().__init__(\n+            self.message\n+        )  # Call the base class constructor with the parameters it needs\n+\n+\n+class SnowflakeTextConfig:\n+    \"\"\"\n+    Reference: https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-llm-rest-api\n+    \"\"\"\n+\n+    max_tokens_to_sample: Optional[int] = None\n+    stop_sequences: Optional[list] = None\n+    temperature: Optional[int] = None\n+    top_p: Optional[int] = None\n+    top_k: Optional[int] = None\n+    metadata: Optional[dict] = None\n+\n+    def __init__(\n+        self,\n+        max_tokens_to_sample: Optional[int] = None,\n+        stop_sequences: Optional[list] = None,\n+        temperature: Optional[int] = None,\n+        top_p: Optional[int] = None,\n+        top_k: Optional[int] = None,\n+        metadata: Optional[dict] = None,\n+    ) -> None:\n+        locals_ = locals()\n+        for key, value in locals_.items():\n+            if key != \"self\" and value is not None:\n+                setattr(self.__class__, key, value)\n+\n+    @classmethod\n+    def get_config(cls):\n+        return {\n+            k: v\n+            for k, v in cls.__dict__.items()\n+            if not k.startswith(\"__\")\n+            and not isinstance(\n+                v,\n+                (\n+                    types.FunctionType,\n+                    types.BuiltinFunctionType,\n+                    classmethod,\n+                    staticmethod,\n+                ),\n+            )\n+            and v is not None\n+        }\n+\n+\n+# makes headers for API call\n+def validate_environment(api_key, user_headers):\n+    if api_key is None:\n+        raise ValueError(\n+            \"Missing Snowflake API Key - A call is being made to Snowflake but no key is set either in the environment variables or via params\"\n+        )\n+\n+    headers = {\n+        \"Accept\": \"application/json,text/stream\",\n+        \"Content-Type\": \"application/json\",\n+        \"Authorization\": f'Snowflake Token=\"{api_key}\"',\n+    }\n+\n+    if user_headers is not None and isinstance(user_headers, dict):\n+        headers = {**headers, **user_headers}\n+    return headers\n+\n+\n+class SnowflakeTextCompletion(BaseLLM):\n+    def __init__(self) -> None:\n+        super().__init__()\n+\n+    def _process_response(\n+        self, model_response: ModelResponse, response, encoding, prompt: str, model: str\n+    ):\n+        ## RESPONSE OBJECT\n+        try:\n+            completion_response = self._process_snowflake_complete_response(\n+                response.content\n+            )\n+        except:\n+            raise SnowflakeError(\n+                message=response.text, status_code=response.status_code\n+            )\n+        if \"error\" in completion_response:\n+            raise SnowflakeError(\n+                message=str(completion_response[\"error\"]),\n+                status_code=response.status_code,\n+            )\n+        else:\n+            if len(completion_response[\"completion\"]) > 0:\n+                model_response.choices[0].message.content = completion_response[\n+                    \"completion\"\n+                ]\n+\n+            model_response.choices[0].finish_reason = completion_response[\"stop_reason\"]\n+\n+        ## CALCULATING USAGE\n+        prompt_tokens = len(encoding.encode(prompt[0][\"content\"]))\n+        completion_tokens = len(encoding.encode(completion_response[\"completion\"]))\n+\n+        model_response.created = int(time.time())\n+        model_response.model = model\n+        usage = Usage(\n+            prompt_tokens=prompt_tokens,\n+            completion_tokens=completion_tokens,\n+            total_tokens=prompt_tokens + completion_tokens,\n+        )\n+\n+        setattr(model_response, \"usage\", usage)\n+\n+        return model_response\n+\n+    def _process_snowflake_complete_response(self, data: bytes):\n+\n+        # Decode the bytes object to a string\n+        data_str = data.decode(\"utf-8\")\n+\n+        # Split the data into individual JSON objects\n+        json_objects = data_str.split(\"\ndata: \")\n+\n+        # Initialize an empty list to store the JSON objects\n+        json_list = []\n+\n+        # Iterate over each JSON object\n+        for obj in json_objects:\n+            obj = obj.strip()\n+            if obj:\n+                # Remove the 'data: ' prefix if it exists\n+                if obj.startswith(\"data: \"):\n+                    obj = obj[6:]\n+                # Load the JSON object into a Python dictionary\n+                json_dict = json.loads(str(obj))\n+                # Append the JSON dictionary to the list\n+                json_list.append(json_dict)\n+\n+        completion = \"\"\n+        model = \"\"\n+        choices = {}\n+        for chunk in json_list:\n+\n+            model = chunk[\"model\"]\n+            choices = chunk[\"choices\"][0]\n+\n+            if \"content\" in choices[\"delta\"].keys():\n+\n+                completion += choices[\"delta\"][\"content\"]\n+\n+        processed_response = {\n+            \"model\": model,\n+            \"completion\": completion,\n+            \"stop_reason\": choices[\"finish_reason\"],\n+        }\n+\n+        return ModelResponse(**processed_response)\n+\n+    async def async_completion(\n+        self,\n+        model: str,\n+        model_response: ModelResponse,\n+        api_base: str,\n+        logging_obj,\n+        encoding,\n+        headers: dict,\n+        data: dict,\n+        client=None,\n+    ):\n+        if client is None:\n+            client = AsyncHTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n+\n+        response = await client.post(api_base, headers=headers, data=json.dumps(data))\n+\n+        if response.status_code != 200:\n+            raise SnowflakeError(\n+                status_code=response.status_code, message=response.text\n+            )\n+\n+        ## LOGGING\n+        logging_obj.post_call(\n+            input=data[\"content\"],\n+            api_key=headers.get(\"Authorization\"),\n+            original_response=response.text,\n+            additional_args={\"complete_input_dict\": data},\n+        )\n+\n+        response = self._process_response(\n+            model_response=model_response,\n+            response=response,\n+            encoding=encoding,\n+            prompt=data[\"content\"],\n+            model=model,\n+        )\n+        return response\n+\n+    async def async_streaming(\n+        self,\n+        model: str,\n+        api_base: str,\n+        logging_obj,\n+        headers: dict,\n+        data: Optional[dict],\n+        client=None,\n+    ):\n+        if client is None:\n+            client = AsyncHTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n+\n+        response = await client.post(api_base, headers=headers, data=json.dumps(data))\n+\n+        if response.status_code != 200:\n+            raise SnowflakeError(\n+                status_code=response.status_code, message=response.text\n+            )\n+\n+        completion_stream = response.aiter_lines()\n+\n+        streamwrapper = CustomStreamWrapper(\n+            completion_stream=completion_stream,\n+            model=model,\n+            custom_llm_provider=\"snowflake\",\n+            logging_obj=logging_obj,\n+        )\n+        return streamwrapper\n+\n+    def completion(\n+        self,\n+        model: str,\n+        messages: list,\n+        api_base: str,\n+        acompletion: str,\n+        custom_prompt_dict: dict,\n+        model_response: ModelResponse,\n+        print_verbose: Callable,\n+        encoding,\n+        api_key,\n+        logging_obj,\n+        optional_params=None,\n+        litellm_params=None,\n+        logger_fn=None,\n+        headers={},\n+        client=None,\n+    ):\n+        headers = validate_environment(api_key, headers)\n+        if model in custom_prompt_dict:\n+            # check if the model has a registered custom prompt\n+            model_prompt_details = custom_prompt_dict[model]\n+            prompt = custom_prompt(\n+                role_dict=model_prompt_details[\"roles\"],\n+                initial_prompt_value=model_prompt_details[\"initial_prompt_value\"],\n+                final_prompt_value=model_prompt_details[\"final_prompt_value\"],\n+                messages=messages,\n+            )\n+        else:\n+            prompt = prompt_factory(\n+                model=model, messages=messages, custom_llm_provider=\"snowflake\"\n+            )\n+\n+        ## Load Config\n+        config = SnowflakeTextConfig.get_config()\n+        for k, v in config.items():\n+            if k not in optional_params:\n+                optional_params[k] = v\n+\n+        data = {\n+            \"model\": model,\n+            \"messages\": [{\"content\": prompt}],",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1780209134",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 5974,
        "pr_file": "litellm/llms/snowflake/completion.py",
        "discussion_id": "1780209134",
        "commented_code": "@@ -0,0 +1,379 @@\n+\"\"\"\n+Handler for Snowflake's Cortex Complete endpoint\n+\"\"\"\n+\n+import json\n+import time\n+import types\n+from typing import Callable, Optional\n+\n+import httpx\n+\n+from litellm.llms.snowflake.snowflake_utils import ModelResponseIterator\n+from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler, HTTPHandler\n+from litellm.utils import CustomStreamWrapper, ModelResponse, Usage\n+\n+from ..base import BaseLLM\n+from ..prompt_templates.factory import custom_prompt, prompt_factory\n+\n+\n+class SnowflakeError(Exception):\n+    def __init__(self, status_code, message):\n+        self.status_code = status_code\n+        self.message = message\n+        self.request = httpx.Request(method=\"POST\", url=\"https://docs.snowflake.com/\")\n+        self.response = httpx.Response(status_code=status_code, request=self.request)\n+        super().__init__(\n+            self.message\n+        )  # Call the base class constructor with the parameters it needs\n+\n+\n+class SnowflakeTextConfig:\n+    \"\"\"\n+    Reference: https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-llm-rest-api\n+    \"\"\"\n+\n+    max_tokens_to_sample: Optional[int] = None\n+    stop_sequences: Optional[list] = None\n+    temperature: Optional[int] = None\n+    top_p: Optional[int] = None\n+    top_k: Optional[int] = None\n+    metadata: Optional[dict] = None\n+\n+    def __init__(\n+        self,\n+        max_tokens_to_sample: Optional[int] = None,\n+        stop_sequences: Optional[list] = None,\n+        temperature: Optional[int] = None,\n+        top_p: Optional[int] = None,\n+        top_k: Optional[int] = None,\n+        metadata: Optional[dict] = None,\n+    ) -> None:\n+        locals_ = locals()\n+        for key, value in locals_.items():\n+            if key != \"self\" and value is not None:\n+                setattr(self.__class__, key, value)\n+\n+    @classmethod\n+    def get_config(cls):\n+        return {\n+            k: v\n+            for k, v in cls.__dict__.items()\n+            if not k.startswith(\"__\")\n+            and not isinstance(\n+                v,\n+                (\n+                    types.FunctionType,\n+                    types.BuiltinFunctionType,\n+                    classmethod,\n+                    staticmethod,\n+                ),\n+            )\n+            and v is not None\n+        }\n+\n+\n+# makes headers for API call\n+def validate_environment(api_key, user_headers):\n+    if api_key is None:\n+        raise ValueError(\n+            \"Missing Snowflake API Key - A call is being made to Snowflake but no key is set either in the environment variables or via params\"\n+        )\n+\n+    headers = {\n+        \"Accept\": \"application/json,text/stream\",\n+        \"Content-Type\": \"application/json\",\n+        \"Authorization\": f'Snowflake Token=\"{api_key}\"',\n+    }\n+\n+    if user_headers is not None and isinstance(user_headers, dict):\n+        headers = {**headers, **user_headers}\n+    return headers\n+\n+\n+class SnowflakeTextCompletion(BaseLLM):\n+    def __init__(self) -> None:\n+        super().__init__()\n+\n+    def _process_response(\n+        self, model_response: ModelResponse, response, encoding, prompt: str, model: str\n+    ):\n+        ## RESPONSE OBJECT\n+        try:\n+            completion_response = self._process_snowflake_complete_response(\n+                response.content\n+            )\n+        except:\n+            raise SnowflakeError(\n+                message=response.text, status_code=response.status_code\n+            )\n+        if \"error\" in completion_response:\n+            raise SnowflakeError(\n+                message=str(completion_response[\"error\"]),\n+                status_code=response.status_code,\n+            )\n+        else:\n+            if len(completion_response[\"completion\"]) > 0:\n+                model_response.choices[0].message.content = completion_response[\n+                    \"completion\"\n+                ]\n+\n+            model_response.choices[0].finish_reason = completion_response[\"stop_reason\"]\n+\n+        ## CALCULATING USAGE\n+        prompt_tokens = len(encoding.encode(prompt[0][\"content\"]))\n+        completion_tokens = len(encoding.encode(completion_response[\"completion\"]))\n+\n+        model_response.created = int(time.time())\n+        model_response.model = model\n+        usage = Usage(\n+            prompt_tokens=prompt_tokens,\n+            completion_tokens=completion_tokens,\n+            total_tokens=prompt_tokens + completion_tokens,\n+        )\n+\n+        setattr(model_response, \"usage\", usage)\n+\n+        return model_response\n+\n+    def _process_snowflake_complete_response(self, data: bytes):\n+\n+        # Decode the bytes object to a string\n+        data_str = data.decode(\"utf-8\")\n+\n+        # Split the data into individual JSON objects\n+        json_objects = data_str.split(\"\\ndata: \")\n+\n+        # Initialize an empty list to store the JSON objects\n+        json_list = []\n+\n+        # Iterate over each JSON object\n+        for obj in json_objects:\n+            obj = obj.strip()\n+            if obj:\n+                # Remove the 'data: ' prefix if it exists\n+                if obj.startswith(\"data: \"):\n+                    obj = obj[6:]\n+                # Load the JSON object into a Python dictionary\n+                json_dict = json.loads(str(obj))\n+                # Append the JSON dictionary to the list\n+                json_list.append(json_dict)\n+\n+        completion = \"\"\n+        model = \"\"\n+        choices = {}\n+        for chunk in json_list:\n+\n+            model = chunk[\"model\"]\n+            choices = chunk[\"choices\"][0]\n+\n+            if \"content\" in choices[\"delta\"].keys():\n+\n+                completion += choices[\"delta\"][\"content\"]\n+\n+        processed_response = {\n+            \"model\": model,\n+            \"completion\": completion,\n+            \"stop_reason\": choices[\"finish_reason\"],\n+        }\n+\n+        return ModelResponse(**processed_response)\n+\n+    async def async_completion(\n+        self,\n+        model: str,\n+        model_response: ModelResponse,\n+        api_base: str,\n+        logging_obj,\n+        encoding,\n+        headers: dict,\n+        data: dict,\n+        client=None,\n+    ):\n+        if client is None:\n+            client = AsyncHTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n+\n+        response = await client.post(api_base, headers=headers, data=json.dumps(data))\n+\n+        if response.status_code != 200:\n+            raise SnowflakeError(\n+                status_code=response.status_code, message=response.text\n+            )\n+\n+        ## LOGGING\n+        logging_obj.post_call(\n+            input=data[\"content\"],\n+            api_key=headers.get(\"Authorization\"),\n+            original_response=response.text,\n+            additional_args={\"complete_input_dict\": data},\n+        )\n+\n+        response = self._process_response(\n+            model_response=model_response,\n+            response=response,\n+            encoding=encoding,\n+            prompt=data[\"content\"],\n+            model=model,\n+        )\n+        return response\n+\n+    async def async_streaming(\n+        self,\n+        model: str,\n+        api_base: str,\n+        logging_obj,\n+        headers: dict,\n+        data: Optional[dict],\n+        client=None,\n+    ):\n+        if client is None:\n+            client = AsyncHTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n+\n+        response = await client.post(api_base, headers=headers, data=json.dumps(data))\n+\n+        if response.status_code != 200:\n+            raise SnowflakeError(\n+                status_code=response.status_code, message=response.text\n+            )\n+\n+        completion_stream = response.aiter_lines()\n+\n+        streamwrapper = CustomStreamWrapper(\n+            completion_stream=completion_stream,\n+            model=model,\n+            custom_llm_provider=\"snowflake\",\n+            logging_obj=logging_obj,\n+        )\n+        return streamwrapper\n+\n+    def completion(\n+        self,\n+        model: str,\n+        messages: list,\n+        api_base: str,\n+        acompletion: str,\n+        custom_prompt_dict: dict,\n+        model_response: ModelResponse,\n+        print_verbose: Callable,\n+        encoding,\n+        api_key,\n+        logging_obj,\n+        optional_params=None,\n+        litellm_params=None,\n+        logger_fn=None,\n+        headers={},\n+        client=None,\n+    ):\n+        headers = validate_environment(api_key, headers)\n+        if model in custom_prompt_dict:\n+            # check if the model has a registered custom prompt\n+            model_prompt_details = custom_prompt_dict[model]\n+            prompt = custom_prompt(\n+                role_dict=model_prompt_details[\"roles\"],\n+                initial_prompt_value=model_prompt_details[\"initial_prompt_value\"],\n+                final_prompt_value=model_prompt_details[\"final_prompt_value\"],\n+                messages=messages,\n+            )\n+        else:\n+            prompt = prompt_factory(\n+                model=model, messages=messages, custom_llm_provider=\"snowflake\"\n+            )\n+\n+        ## Load Config\n+        config = SnowflakeTextConfig.get_config()\n+        for k, v in config.items():\n+            if k not in optional_params:\n+                optional_params[k] = v\n+\n+        data = {\n+            \"model\": model,\n+            \"messages\": [{\"content\": prompt}],",
        "comment_created_at": "2024-09-29T23:54:06+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "i'm confused - why does this convert the message list to a prompt and then back to a message list? \r\n\r\nif it's just to make sure content is a string, then i believe it might be better handled like Azure AI - https://github.com/BerriAI/litellm/blob/7630680690fdb85a0c239c49fa9817793b55447d/litellm/llms/azure_ai/chat/transformation.py#L27",
        "pr_file_module": null
      },
      {
        "comment_id": "1780209437",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 5974,
        "pr_file": "litellm/llms/snowflake/completion.py",
        "discussion_id": "1780209134",
        "commented_code": "@@ -0,0 +1,379 @@\n+\"\"\"\n+Handler for Snowflake's Cortex Complete endpoint\n+\"\"\"\n+\n+import json\n+import time\n+import types\n+from typing import Callable, Optional\n+\n+import httpx\n+\n+from litellm.llms.snowflake.snowflake_utils import ModelResponseIterator\n+from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler, HTTPHandler\n+from litellm.utils import CustomStreamWrapper, ModelResponse, Usage\n+\n+from ..base import BaseLLM\n+from ..prompt_templates.factory import custom_prompt, prompt_factory\n+\n+\n+class SnowflakeError(Exception):\n+    def __init__(self, status_code, message):\n+        self.status_code = status_code\n+        self.message = message\n+        self.request = httpx.Request(method=\"POST\", url=\"https://docs.snowflake.com/\")\n+        self.response = httpx.Response(status_code=status_code, request=self.request)\n+        super().__init__(\n+            self.message\n+        )  # Call the base class constructor with the parameters it needs\n+\n+\n+class SnowflakeTextConfig:\n+    \"\"\"\n+    Reference: https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-llm-rest-api\n+    \"\"\"\n+\n+    max_tokens_to_sample: Optional[int] = None\n+    stop_sequences: Optional[list] = None\n+    temperature: Optional[int] = None\n+    top_p: Optional[int] = None\n+    top_k: Optional[int] = None\n+    metadata: Optional[dict] = None\n+\n+    def __init__(\n+        self,\n+        max_tokens_to_sample: Optional[int] = None,\n+        stop_sequences: Optional[list] = None,\n+        temperature: Optional[int] = None,\n+        top_p: Optional[int] = None,\n+        top_k: Optional[int] = None,\n+        metadata: Optional[dict] = None,\n+    ) -> None:\n+        locals_ = locals()\n+        for key, value in locals_.items():\n+            if key != \"self\" and value is not None:\n+                setattr(self.__class__, key, value)\n+\n+    @classmethod\n+    def get_config(cls):\n+        return {\n+            k: v\n+            for k, v in cls.__dict__.items()\n+            if not k.startswith(\"__\")\n+            and not isinstance(\n+                v,\n+                (\n+                    types.FunctionType,\n+                    types.BuiltinFunctionType,\n+                    classmethod,\n+                    staticmethod,\n+                ),\n+            )\n+            and v is not None\n+        }\n+\n+\n+# makes headers for API call\n+def validate_environment(api_key, user_headers):\n+    if api_key is None:\n+        raise ValueError(\n+            \"Missing Snowflake API Key - A call is being made to Snowflake but no key is set either in the environment variables or via params\"\n+        )\n+\n+    headers = {\n+        \"Accept\": \"application/json,text/stream\",\n+        \"Content-Type\": \"application/json\",\n+        \"Authorization\": f'Snowflake Token=\"{api_key}\"',\n+    }\n+\n+    if user_headers is not None and isinstance(user_headers, dict):\n+        headers = {**headers, **user_headers}\n+    return headers\n+\n+\n+class SnowflakeTextCompletion(BaseLLM):\n+    def __init__(self) -> None:\n+        super().__init__()\n+\n+    def _process_response(\n+        self, model_response: ModelResponse, response, encoding, prompt: str, model: str\n+    ):\n+        ## RESPONSE OBJECT\n+        try:\n+            completion_response = self._process_snowflake_complete_response(\n+                response.content\n+            )\n+        except:\n+            raise SnowflakeError(\n+                message=response.text, status_code=response.status_code\n+            )\n+        if \"error\" in completion_response:\n+            raise SnowflakeError(\n+                message=str(completion_response[\"error\"]),\n+                status_code=response.status_code,\n+            )\n+        else:\n+            if len(completion_response[\"completion\"]) > 0:\n+                model_response.choices[0].message.content = completion_response[\n+                    \"completion\"\n+                ]\n+\n+            model_response.choices[0].finish_reason = completion_response[\"stop_reason\"]\n+\n+        ## CALCULATING USAGE\n+        prompt_tokens = len(encoding.encode(prompt[0][\"content\"]))\n+        completion_tokens = len(encoding.encode(completion_response[\"completion\"]))\n+\n+        model_response.created = int(time.time())\n+        model_response.model = model\n+        usage = Usage(\n+            prompt_tokens=prompt_tokens,\n+            completion_tokens=completion_tokens,\n+            total_tokens=prompt_tokens + completion_tokens,\n+        )\n+\n+        setattr(model_response, \"usage\", usage)\n+\n+        return model_response\n+\n+    def _process_snowflake_complete_response(self, data: bytes):\n+\n+        # Decode the bytes object to a string\n+        data_str = data.decode(\"utf-8\")\n+\n+        # Split the data into individual JSON objects\n+        json_objects = data_str.split(\"\\ndata: \")\n+\n+        # Initialize an empty list to store the JSON objects\n+        json_list = []\n+\n+        # Iterate over each JSON object\n+        for obj in json_objects:\n+            obj = obj.strip()\n+            if obj:\n+                # Remove the 'data: ' prefix if it exists\n+                if obj.startswith(\"data: \"):\n+                    obj = obj[6:]\n+                # Load the JSON object into a Python dictionary\n+                json_dict = json.loads(str(obj))\n+                # Append the JSON dictionary to the list\n+                json_list.append(json_dict)\n+\n+        completion = \"\"\n+        model = \"\"\n+        choices = {}\n+        for chunk in json_list:\n+\n+            model = chunk[\"model\"]\n+            choices = chunk[\"choices\"][0]\n+\n+            if \"content\" in choices[\"delta\"].keys():\n+\n+                completion += choices[\"delta\"][\"content\"]\n+\n+        processed_response = {\n+            \"model\": model,\n+            \"completion\": completion,\n+            \"stop_reason\": choices[\"finish_reason\"],\n+        }\n+\n+        return ModelResponse(**processed_response)\n+\n+    async def async_completion(\n+        self,\n+        model: str,\n+        model_response: ModelResponse,\n+        api_base: str,\n+        logging_obj,\n+        encoding,\n+        headers: dict,\n+        data: dict,\n+        client=None,\n+    ):\n+        if client is None:\n+            client = AsyncHTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n+\n+        response = await client.post(api_base, headers=headers, data=json.dumps(data))\n+\n+        if response.status_code != 200:\n+            raise SnowflakeError(\n+                status_code=response.status_code, message=response.text\n+            )\n+\n+        ## LOGGING\n+        logging_obj.post_call(\n+            input=data[\"content\"],\n+            api_key=headers.get(\"Authorization\"),\n+            original_response=response.text,\n+            additional_args={\"complete_input_dict\": data},\n+        )\n+\n+        response = self._process_response(\n+            model_response=model_response,\n+            response=response,\n+            encoding=encoding,\n+            prompt=data[\"content\"],\n+            model=model,\n+        )\n+        return response\n+\n+    async def async_streaming(\n+        self,\n+        model: str,\n+        api_base: str,\n+        logging_obj,\n+        headers: dict,\n+        data: Optional[dict],\n+        client=None,\n+    ):\n+        if client is None:\n+            client = AsyncHTTPHandler(timeout=httpx.Timeout(timeout=600.0, connect=5.0))\n+\n+        response = await client.post(api_base, headers=headers, data=json.dumps(data))\n+\n+        if response.status_code != 200:\n+            raise SnowflakeError(\n+                status_code=response.status_code, message=response.text\n+            )\n+\n+        completion_stream = response.aiter_lines()\n+\n+        streamwrapper = CustomStreamWrapper(\n+            completion_stream=completion_stream,\n+            model=model,\n+            custom_llm_provider=\"snowflake\",\n+            logging_obj=logging_obj,\n+        )\n+        return streamwrapper\n+\n+    def completion(\n+        self,\n+        model: str,\n+        messages: list,\n+        api_base: str,\n+        acompletion: str,\n+        custom_prompt_dict: dict,\n+        model_response: ModelResponse,\n+        print_verbose: Callable,\n+        encoding,\n+        api_key,\n+        logging_obj,\n+        optional_params=None,\n+        litellm_params=None,\n+        logger_fn=None,\n+        headers={},\n+        client=None,\n+    ):\n+        headers = validate_environment(api_key, headers)\n+        if model in custom_prompt_dict:\n+            # check if the model has a registered custom prompt\n+            model_prompt_details = custom_prompt_dict[model]\n+            prompt = custom_prompt(\n+                role_dict=model_prompt_details[\"roles\"],\n+                initial_prompt_value=model_prompt_details[\"initial_prompt_value\"],\n+                final_prompt_value=model_prompt_details[\"final_prompt_value\"],\n+                messages=messages,\n+            )\n+        else:\n+            prompt = prompt_factory(\n+                model=model, messages=messages, custom_llm_provider=\"snowflake\"\n+            )\n+\n+        ## Load Config\n+        config = SnowflakeTextConfig.get_config()\n+        for k, v in config.items():\n+            if k not in optional_params:\n+                optional_params[k] = v\n+\n+        data = {\n+            \"model\": model,\n+            \"messages\": [{\"content\": prompt}],",
        "comment_created_at": "2024-09-29T23:55:08+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "the message transformation logic should also be handled in a separate `transformation.py` file, so it's easy for someone to see how their call is being modified  - e.g. https://github.com/BerriAI/litellm/tree/main/litellm/llms/azure_ai",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1951780213",
    "pr_number": 8456,
    "pr_file": "litellm/main.py",
    "created_at": "2025-02-12T00:14:27+00:00",
    "commented_code": "return response\n \n \n+@overload\n+async def acompletion(\n+    model: str,\n+    *,\n+    # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n+    messages: List = [],\n+    functions: Optional[List] = None,\n+    function_call: Optional[str] = None,\n+    timeout: Optional[Union[float, int]] = None,\n+    temperature: Optional[float] = None,\n+    top_p: Optional[float] = None,\n+    n: Optional[int] = None,\n+    stream: Literal[False],\n+    stream_options: Optional[dict] = None,\n+    stop=None,\n+    max_tokens: Optional[int] = None,\n+    max_completion_tokens: Optional[int] = None,\n+    modalities: Optional[List[ChatCompletionModality]] = None,\n+    prediction: Optional[ChatCompletionPredictionContentParam] = None,\n+    audio: Optional[ChatCompletionAudioParam] = None,\n+    presence_penalty: Optional[float] = None,\n+    frequency_penalty: Optional[float] = None,\n+    logit_bias: Optional[dict] = None,\n+    user: Optional[str] = None,\n+    # openai v1.0+ new params\n+    response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n+    seed: Optional[int] = None,\n+    tools: Optional[List] = None,\n+    tool_choice: Optional[str] = None,\n+    parallel_tool_calls: Optional[bool] = None,\n+    logprobs: Optional[bool] = None,\n+    top_logprobs: Optional[int] = None,\n+    deployment_id=None,\n+    # set api_base, api_version, api_key\n+    base_url: Optional[str] = None,\n+    api_version: Optional[str] = None,\n+    api_key: Optional[str] = None,\n+    model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n+    extra_headers: Optional[dict] = None,\n+    # Optional liteLLM function params\n+    **kwargs,\n+) -> ModelResponse: ...\n+\n+\n+@overload\n+async def acompletion(\n+    model: str,\n+    *,\n+    # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n+    messages: List = [],\n+    functions: Optional[List] = None,\n+    function_call: Optional[str] = None,\n+    timeout: Optional[Union[float, int]] = None,\n+    temperature: Optional[float] = None,\n+    top_p: Optional[float] = None,\n+    n: Optional[int] = None,\n+    stream: Literal[True],\n+    stream_options: Optional[dict] = None,\n+    stop=None,\n+    max_tokens: Optional[int] = None,\n+    max_completion_tokens: Optional[int] = None,\n+    modalities: Optional[List[ChatCompletionModality]] = None,\n+    prediction: Optional[ChatCompletionPredictionContentParam] = None,\n+    audio: Optional[ChatCompletionAudioParam] = None,\n+    presence_penalty: Optional[float] = None,\n+    frequency_penalty: Optional[float] = None,\n+    logit_bias: Optional[dict] = None,\n+    user: Optional[str] = None,\n+    # openai v1.0+ new params\n+    response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n+    seed: Optional[int] = None,\n+    tools: Optional[List] = None,\n+    tool_choice: Optional[str] = None,\n+    parallel_tool_calls: Optional[bool] = None,\n+    logprobs: Optional[bool] = None,\n+    top_logprobs: Optional[int] = None,\n+    deployment_id=None,\n+    # set api_base, api_version, api_key\n+    base_url: Optional[str] = None,\n+    api_version: Optional[str] = None,\n+    api_key: Optional[str] = None,\n+    model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n+    extra_headers: Optional[dict] = None,\n+    # Optional liteLLM function params\n+    **kwargs,\n+) -> CustomStreamWrapper: ...\n+\n+\n+@overload\n+async def acompletion(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1951780213",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8456,
        "pr_file": "litellm/main.py",
        "discussion_id": "1951780213",
        "commented_code": "@@ -300,6 +300,182 @@ async def create(self, messages, model=None, **kwargs):\n         return response\n \n \n+@overload\n+async def acompletion(\n+    model: str,\n+    *,\n+    # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n+    messages: List = [],\n+    functions: Optional[List] = None,\n+    function_call: Optional[str] = None,\n+    timeout: Optional[Union[float, int]] = None,\n+    temperature: Optional[float] = None,\n+    top_p: Optional[float] = None,\n+    n: Optional[int] = None,\n+    stream: Literal[False],\n+    stream_options: Optional[dict] = None,\n+    stop=None,\n+    max_tokens: Optional[int] = None,\n+    max_completion_tokens: Optional[int] = None,\n+    modalities: Optional[List[ChatCompletionModality]] = None,\n+    prediction: Optional[ChatCompletionPredictionContentParam] = None,\n+    audio: Optional[ChatCompletionAudioParam] = None,\n+    presence_penalty: Optional[float] = None,\n+    frequency_penalty: Optional[float] = None,\n+    logit_bias: Optional[dict] = None,\n+    user: Optional[str] = None,\n+    # openai v1.0+ new params\n+    response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n+    seed: Optional[int] = None,\n+    tools: Optional[List] = None,\n+    tool_choice: Optional[str] = None,\n+    parallel_tool_calls: Optional[bool] = None,\n+    logprobs: Optional[bool] = None,\n+    top_logprobs: Optional[int] = None,\n+    deployment_id=None,\n+    # set api_base, api_version, api_key\n+    base_url: Optional[str] = None,\n+    api_version: Optional[str] = None,\n+    api_key: Optional[str] = None,\n+    model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n+    extra_headers: Optional[dict] = None,\n+    # Optional liteLLM function params\n+    **kwargs,\n+) -> ModelResponse: ...\n+\n+\n+@overload\n+async def acompletion(\n+    model: str,\n+    *,\n+    # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n+    messages: List = [],\n+    functions: Optional[List] = None,\n+    function_call: Optional[str] = None,\n+    timeout: Optional[Union[float, int]] = None,\n+    temperature: Optional[float] = None,\n+    top_p: Optional[float] = None,\n+    n: Optional[int] = None,\n+    stream: Literal[True],\n+    stream_options: Optional[dict] = None,\n+    stop=None,\n+    max_tokens: Optional[int] = None,\n+    max_completion_tokens: Optional[int] = None,\n+    modalities: Optional[List[ChatCompletionModality]] = None,\n+    prediction: Optional[ChatCompletionPredictionContentParam] = None,\n+    audio: Optional[ChatCompletionAudioParam] = None,\n+    presence_penalty: Optional[float] = None,\n+    frequency_penalty: Optional[float] = None,\n+    logit_bias: Optional[dict] = None,\n+    user: Optional[str] = None,\n+    # openai v1.0+ new params\n+    response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n+    seed: Optional[int] = None,\n+    tools: Optional[List] = None,\n+    tool_choice: Optional[str] = None,\n+    parallel_tool_calls: Optional[bool] = None,\n+    logprobs: Optional[bool] = None,\n+    top_logprobs: Optional[int] = None,\n+    deployment_id=None,\n+    # set api_base, api_version, api_key\n+    base_url: Optional[str] = None,\n+    api_version: Optional[str] = None,\n+    api_key: Optional[str] = None,\n+    model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n+    extra_headers: Optional[dict] = None,\n+    # Optional liteLLM function params\n+    **kwargs,\n+) -> CustomStreamWrapper: ...\n+\n+\n+@overload\n+async def acompletion(",
        "comment_created_at": "2025-02-12T00:14:27+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "there is a lot of duplicate code in this approach, this means that when OAI has a new param we'll need to edit it in 5-6 places. Do you have a better approach in mind with less code duplication @miraclebakelaser ? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175672534",
    "pr_number": 12052,
    "pr_file": "litellm/llms/custom_httpx/http_handler.py",
    "created_at": "2025-06-30T18:40:10+00:00",
    "commented_code": "# SSL certificates (a.k.a CA bundle) used to verify the identity of requested hosts.\n         # /path/to/certificate.pem\n         if ssl_verify is None:\n-            ssl_verify = os.getenv(\"SSL_VERIFY\", litellm.ssl_verify)\n+            # handles the string \"False\" coming from the shell\n+            ssl_verify = (os.getenv(\"SSL_VERIFY\") != \"False\") if os.getenv(\"SSL_VERIFY\") is not None else litellm.ssl_verify",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2175672534",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12052,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2175672534",
        "commented_code": "@@ -122,7 +122,8 @@ def create_client(\n         # SSL certificates (a.k.a CA bundle) used to verify the identity of requested hosts.\n         # /path/to/certificate.pem\n         if ssl_verify is None:\n-            ssl_verify = os.getenv(\"SSL_VERIFY\", litellm.ssl_verify)\n+            # handles the string \"False\" coming from the shell\n+            ssl_verify = (os.getenv(\"SSL_VERIFY\") != \"False\") if os.getenv(\"SSL_VERIFY\") is not None else litellm.ssl_verify",
        "comment_created_at": "2025-06-30T18:40:10+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "There's duplicate logic - please use a helper method to get ssl_verify ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2193628033",
    "pr_number": 12360,
    "pr_file": "litellm/integrations/vector_store_integrations/bedrock_vector_store.py",
    "created_at": "2025-07-08T23:38:54+00:00",
    "commented_code": ")\n             if retrieval_result_text is None:\n                 continue\n-            context_string += retrieval_result_text\n+\n+            retrieval_result_metadata: Optional[Dict[str, Any]] = retrieval_result.get(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2193628033",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12360,
        "pr_file": "litellm/integrations/vector_store_integrations/bedrock_vector_store.py",
        "discussion_id": "2193628033",
        "commented_code": "@@ -402,7 +410,35 @@ def get_chat_completion_message_from_bedrock_kb_response(\n             )\n             if retrieval_result_text is None:\n                 continue\n-            context_string += retrieval_result_text\n+\n+            retrieval_result_metadata: Optional[Dict[str, Any]] = retrieval_result.get(",
        "comment_created_at": "2025-07-08T23:38:54+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "make this a static method - `construct_retrieval_context`  ",
        "pr_file_module": null
      },
      {
        "comment_id": "2194381200",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12360,
        "pr_file": "litellm/integrations/vector_store_integrations/bedrock_vector_store.py",
        "discussion_id": "2193628033",
        "commented_code": "@@ -402,7 +410,35 @@ def get_chat_completion_message_from_bedrock_kb_response(\n             )\n             if retrieval_result_text is None:\n                 continue\n-            context_string += retrieval_result_text\n+\n+            retrieval_result_metadata: Optional[Dict[str, Any]] = retrieval_result.get(",
        "comment_created_at": "2025-07-09T08:20:34+00:00",
        "comment_author": "edwarddamato",
        "comment_body": "Thanks for the feedback @ishaan-jaff! I've moved the retrieval context construction to a separate method. Let me know if you have any more feedback.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2002527879",
    "pr_number": 9362,
    "pr_file": "tests/test_bitdeerai.py",
    "created_at": "2025-03-19T06:40:00+00:00",
    "commented_code": "+import os\n+import sys\n+import traceback\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+import pytest\n+\n+from litellm import completion, acompletion, embedding, aembedding, EmbeddingResponse\n+\n+@pytest.mark.parametrize(\"sync_mode\", [True, False])",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2002527879",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9362,
        "pr_file": "tests/test_bitdeerai.py",
        "discussion_id": "2002527879",
        "commented_code": "@@ -0,0 +1,95 @@\n+import os\n+import sys\n+import traceback\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+import pytest\n+\n+from litellm import completion, acompletion, embedding, aembedding, EmbeddingResponse\n+\n+@pytest.mark.parametrize(\"sync_mode\", [True, False])",
        "comment_created_at": "2025-03-19T06:40:00+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "why does this file exist at the root? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2002904957",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9362,
        "pr_file": "tests/test_bitdeerai.py",
        "discussion_id": "2002527879",
        "commented_code": "@@ -0,0 +1,95 @@\n+import os\n+import sys\n+import traceback\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+import pytest\n+\n+from litellm import completion, acompletion, embedding, aembedding, EmbeddingResponse\n+\n+@pytest.mark.parametrize(\"sync_mode\", [True, False])",
        "comment_created_at": "2025-03-19T09:44:50+00:00",
        "comment_author": "Yujie-zhuzhu",
        "comment_body": "move thie file to local_testing folder",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1936162380",
    "pr_number": 8116,
    "pr_file": "litellm/__init__.py",
    "created_at": "2025-01-30T19:21:34+00:00",
    "commented_code": "\"petals-team/StableBeluga2\",\n ]\n \n-ollama_models = [\"llama2\"]\n+# Since Ollama models are local, it is inexpensive to refresh the list at startup.",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1936162380",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8116,
        "pr_file": "litellm/__init__.py",
        "discussion_id": "1936162380",
        "commented_code": "@@ -852,7 +855,28 @@ def add_known_models():\n     \"petals-team/StableBeluga2\",\n ]\n \n-ollama_models = [\"llama2\"]\n+# Since Ollama models are local, it is inexpensive to refresh the list at startup.",
        "comment_created_at": "2025-01-30T19:21:34+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "please don't define code in __init__ ",
        "pr_file_module": null
      },
      {
        "comment_id": "1936177296",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8116,
        "pr_file": "litellm/__init__.py",
        "discussion_id": "1936162380",
        "commented_code": "@@ -852,7 +855,28 @@ def add_known_models():\n     \"petals-team/StableBeluga2\",\n ]\n \n-ollama_models = [\"llama2\"]\n+# Since Ollama models are local, it is inexpensive to refresh the list at startup.",
        "comment_created_at": "2025-01-30T19:34:20+00:00",
        "comment_author": "sakoht",
        "comment_body": "I was leery to do this too.\r\n\r\nWhere is the best place to set a module-level global that is not constant?",
        "pr_file_module": null
      },
      {
        "comment_id": "1936226038",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8116,
        "pr_file": "litellm/__init__.py",
        "discussion_id": "1936162380",
        "commented_code": "@@ -852,7 +855,28 @@ def add_known_models():\n     \"petals-team/StableBeluga2\",\n ]\n \n-ollama_models = [\"llama2\"]\n+# Since Ollama models are local, it is inexpensive to refresh the list at startup.",
        "comment_created_at": "2025-01-30T20:17:05+00:00",
        "comment_author": "sakoht",
        "comment_body": "I'll push up a patch that moves it to initialize().  I could also do load_config(), which is called by initialize().",
        "pr_file_module": null
      },
      {
        "comment_id": "1936234764",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8116,
        "pr_file": "litellm/__init__.py",
        "discussion_id": "1936162380",
        "commented_code": "@@ -852,7 +855,28 @@ def add_known_models():\n     \"petals-team/StableBeluga2\",\n ]\n \n-ollama_models = [\"llama2\"]\n+# Since Ollama models are local, it is inexpensive to refresh the list at startup.",
        "comment_created_at": "2025-01-30T20:25:16+00:00",
        "comment_author": "sakoht",
        "comment_body": "Pushed.  ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2180661485",
    "pr_number": 12245,
    "pr_file": "litellm/llms/mistral/mistral_chat_transformation.py",
    "created_at": "2025-07-02T18:00:20+00:00",
    "commented_code": null,
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2180661485",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12245,
        "pr_file": "litellm/llms/mistral/mistral_chat_transformation.py",
        "discussion_id": "2180661485",
        "commented_code": null,
        "comment_created_at": "2025-07-02T18:00:20+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "this file should be inside a folder called `/chat` - so a user can click into mistral and know all the supported endpoints \r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2153258742",
    "pr_number": 11439,
    "pr_file": "litellm/litellm_core_utils/get_model_cost_map.py",
    "created_at": "2025-06-17T22:14:02+00:00",
    "commented_code": "Pulls the cost + context window + provider route for known models from https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n \n This can be disabled by setting the LITELLM_LOCAL_MODEL_COST_MAP environment variable to True.\n+LITELLM_PRICE_DIR must be relative to /app ( root dir of litellm)\n \n ```\n export LITELLM_LOCAL_MODEL_COST_MAP=True\n+export LITELLM_PRICE_DIR=config\n ```\n \"\"\"\n-\n import os\n-\n+import json\n import httpx\n+import importlib.resources\n \n \n def get_model_cost_map(url: str) -> dict:\n-    if (\n-        os.getenv(\"LITELLM_LOCAL_MODEL_COST_MAP\", False)\n-        or os.getenv(\"LITELLM_LOCAL_MODEL_COST_MAP\", False) == \"True\"\n-    ):\n-        import importlib.resources\n-        import json\n-\n-        with importlib.resources.open_text(\n-            \"litellm\", \"model_prices_and_context_window_backup.json\"\n-        ) as f:\n-            content = json.load(f)\n-            return content\n+    \"\"\"\n+    Retrieves the model cost and context window data from a remote source.\n+    Falls back to a local JSON file if:\n+    - The environment variable LITELLM_LOCAL_MODEL_COST_MAP is set to \"True\", or\n+    - The remote fetch fails\n+    \"\"\"\n+\n+\n+    def load_local_backup() -> dict:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2153258742",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11439,
        "pr_file": "litellm/litellm_core_utils/get_model_cost_map.py",
        "discussion_id": "2153258742",
        "commented_code": "@@ -2,44 +2,41 @@\n Pulls the cost + context window + provider route for known models from https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n \n This can be disabled by setting the LITELLM_LOCAL_MODEL_COST_MAP environment variable to True.\n+LITELLM_PRICE_DIR must be relative to /app ( root dir of litellm)\n \n ```\n export LITELLM_LOCAL_MODEL_COST_MAP=True\n+export LITELLM_PRICE_DIR=config\n ```\n \"\"\"\n-\n import os\n-\n+import json\n import httpx\n+import importlib.resources\n \n \n def get_model_cost_map(url: str) -> dict:\n-    if (\n-        os.getenv(\"LITELLM_LOCAL_MODEL_COST_MAP\", False)\n-        or os.getenv(\"LITELLM_LOCAL_MODEL_COST_MAP\", False) == \"True\"\n-    ):\n-        import importlib.resources\n-        import json\n-\n-        with importlib.resources.open_text(\n-            \"litellm\", \"model_prices_and_context_window_backup.json\"\n-        ) as f:\n-            content = json.load(f)\n-            return content\n+    \"\"\"\n+    Retrieves the model cost and context window data from a remote source.\n+    Falls back to a local JSON file if:\n+    - The environment variable LITELLM_LOCAL_MODEL_COST_MAP is set to \"True\", or\n+    - The remote fetch fails\n+    \"\"\"\n+\n+\n+    def load_local_backup() -> dict:",
        "comment_created_at": "2025-06-17T22:14:02+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "place `load_local_backup` outside this function so it's a standalone function ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1988307532",
    "pr_number": 9118,
    "pr_file": "litellm/proxy/proxy_server.py",
    "created_at": "2025-03-11T03:05:02+00:00",
    "commented_code": "### logger ###\n \n \n+def _safely_round_response_cost(response_cost: Optional[Union[float, str]]) -> str:\n+    \"\"\"\n+    Safely round response cost to 6 decimal places.\n+    If rounding fails, return the original response cost as a string.\n+    \"\"\"\n+    if response_cost is None:\n+        return \"None\"\n+    \n+    try:\n+        return str(round(float(response_cost), 6))\n+    except Exception:\n+        # If rounding fails for any reason, return the original value\n+        return str(response_cost)\n+",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1988307532",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9118,
        "pr_file": "litellm/proxy/proxy_server.py",
        "discussion_id": "1988307532",
        "commented_code": "@@ -781,6 +781,21 @@ async def redirect_ui_middleware(request: Request, call_next):\n ### logger ###\n \n \n+def _safely_round_response_cost(response_cost: Optional[Union[float, str]]) -> str:\n+    \"\"\"\n+    Safely round response cost to 6 decimal places.\n+    If rounding fails, return the original response cost as a string.\n+    \"\"\"\n+    if response_cost is None:\n+        return \"None\"\n+    \n+    try:\n+        return str(round(float(response_cost), 6))\n+    except Exception:\n+        # If rounding fails for any reason, return the original value\n+        return str(response_cost)\n+",
        "comment_created_at": "2025-03-11T03:05:02+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "move this to proxy/utils.py ",
        "pr_file_module": null
      },
      {
        "comment_id": "2002183228",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9118,
        "pr_file": "litellm/proxy/proxy_server.py",
        "discussion_id": "1988307532",
        "commented_code": "@@ -781,6 +781,21 @@ async def redirect_ui_middleware(request: Request, call_next):\n ### logger ###\n \n \n+def _safely_round_response_cost(response_cost: Optional[Union[float, str]]) -> str:\n+    \"\"\"\n+    Safely round response cost to 6 decimal places.\n+    If rounding fails, return the original response cost as a string.\n+    \"\"\"\n+    if response_cost is None:\n+        return \"None\"\n+    \n+    try:\n+        return str(round(float(response_cost), 6))\n+    except Exception:\n+        # If rounding fails for any reason, return the original value\n+        return str(response_cost)\n+",
        "comment_created_at": "2025-03-19T00:09:30+00:00",
        "comment_author": "colesmcintosh",
        "comment_body": "@ishaan-jaff should this removed from `litellm/proxy/utils.py` as it's being handled in `litellm/proxy/common_request_processing.py` as a static method on ProxyBaseLLMRequestProcessing?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2159885871",
    "pr_number": 11854,
    "pr_file": "litellm/llms/openai_like/chat/handler.py",
    "created_at": "2025-06-21T06:22:44+00:00",
    "commented_code": ")\n \n         data = {\n-            \"model\": model,\n+            # watsonx: Deployment models do not support 'model_id' in their payload\n+            # https://github.com/BerriAI/litellm/issues/11837\n+            \"model\": None\n+            if custom_llm_provider == \"watsonx\" and model.startswith(\"deployment/\")",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2159885871",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11854,
        "pr_file": "litellm/llms/openai_like/chat/handler.py",
        "discussion_id": "2159885871",
        "commented_code": "@@ -272,7 +272,11 @@ def completion(\n                 )\n \n         data = {\n-            \"model\": model,\n+            # watsonx: Deployment models do not support 'model_id' in their payload\n+            # https://github.com/BerriAI/litellm/issues/11837\n+            \"model\": None\n+            if custom_llm_provider == \"watsonx\" and model.startswith(\"deployment/\")",
        "comment_created_at": "2025-06-21T06:22:44+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "this is the wrong place for this change. it should be inside watsonx/chat/transformation or watsonx/completion/transformation.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2145233572",
    "pr_number": 11678,
    "pr_file": "litellm/proxy/health_endpoints/_health_endpoints.py",
    "created_at": "2025-06-13T14:34:11+00:00",
    "commented_code": "health_check_details,\n         health_check_results,\n         llm_model_list,\n+        llm_router,\n         use_background_health_checks,\n         user_model,\n+        prisma_client,\n     )\n+    import time\n \n+    start_time = time.time()\n+    \n+    # Handle model_id parameter - convert to model name for health check\n+    target_model = model",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2145233572",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11678,
        "pr_file": "litellm/proxy/health_endpoints/_health_endpoints.py",
        "discussion_id": "2145233572",
        "commented_code": "@@ -329,17 +369,71 @@ async def health_endpoint(\n         health_check_details,\n         health_check_results,\n         llm_model_list,\n+        llm_router,\n         use_background_health_checks,\n         user_model,\n+        prisma_client,\n     )\n+    import time\n \n+    start_time = time.time()\n+    \n+    # Handle model_id parameter - convert to model name for health check\n+    target_model = model",
        "comment_created_at": "2025-06-13T14:34:11+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "please use `get_deployment` in router.py that already does this ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2127590244",
    "pr_number": 11420,
    "pr_file": "litellm/proxy/proxy_server.py",
    "created_at": "2025-06-04T23:26:46+00:00",
    "commented_code": "def generate_feedback_box():\n-    box_width = 60\n-\n-    # Select a random message\n-    message = random.choice(list_of_messages)\n-\n-    print()  # noqa\n-    print(\"\\033[1;37m\" + \"#\" + \"-\" * box_width + \"#\\033[0m\")  # noqa\n-    print(\"\\033[1;37m\" + \"#\" + \" \" * box_width + \"#\\033[0m\")  # noqa\n-    print(\"\\033[1;37m\" + \"# {:^59} #\\033[0m\".format(message))  # noqa\n-    print(  # noqa\n-        \"\\033[1;37m\"\n-        + \"# {:^59} #\\033[0m\".format(\"https://github.com/BerriAI/litellm/issues/new\")\n-    )  # noqa\n-    print(\"\\033[1;37m\" + \"#\" + \" \" * box_width + \"#\\033[0m\")  # noqa\n-    print(\"\\033[1;37m\" + \"#\" + \"-\" * box_width + \"#\\033[0m\")  # noqa\n-    print()  # noqa\n-    print(\" Thank you for using LiteLLM! - Krrish & Ishaan\")  # noqa\n-    print()  # noqa\n-    print()  # noqa\n-    print()  # noqa\n-    print(  # noqa\n-        \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"\n-    )  # noqa\n-    print()  # noqa\n-    print()  # noqa\n+    try:\n+        from rich.console import Console",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2127590244",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11420,
        "pr_file": "litellm/proxy/proxy_server.py",
        "discussion_id": "2127590244",
        "commented_code": "@@ -89,31 +89,75 @@ def showwarning(message, category, filename, lineno, file=None, line=None):\n \n \n def generate_feedback_box():\n-    box_width = 60\n-\n-    # Select a random message\n-    message = random.choice(list_of_messages)\n-\n-    print()  # noqa\n-    print(\"\\033[1;37m\" + \"#\" + \"-\" * box_width + \"#\\033[0m\")  # noqa\n-    print(\"\\033[1;37m\" + \"#\" + \" \" * box_width + \"#\\033[0m\")  # noqa\n-    print(\"\\033[1;37m\" + \"# {:^59} #\\033[0m\".format(message))  # noqa\n-    print(  # noqa\n-        \"\\033[1;37m\"\n-        + \"# {:^59} #\\033[0m\".format(\"https://github.com/BerriAI/litellm/issues/new\")\n-    )  # noqa\n-    print(\"\\033[1;37m\" + \"#\" + \" \" * box_width + \"#\\033[0m\")  # noqa\n-    print(\"\\033[1;37m\" + \"#\" + \"-\" * box_width + \"#\\033[0m\")  # noqa\n-    print()  # noqa\n-    print(\" Thank you for using LiteLLM! - Krrish & Ishaan\")  # noqa\n-    print()  # noqa\n-    print()  # noqa\n-    print()  # noqa\n-    print(  # noqa\n-        \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"\n-    )  # noqa\n-    print()  # noqa\n-    print()  # noqa\n+    try:\n+        from rich.console import Console",
        "comment_created_at": "2025-06-04T23:26:46+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "can we move this new logic in a different file. proxy_server.py is getting large. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2092028090",
    "pr_number": 10871,
    "pr_file": "litellm/llms/azure/responses/transformation.py",
    "created_at": "2025-05-15T22:11:50+00:00",
    "commented_code": "else:\n     LiteLLMLoggingObj = Any\n \n-\n class AzureOpenAIResponsesAPIConfig(OpenAIResponsesAPIConfig):\n     def validate_environment(\n         self,\n         headers: dict,\n         model: str,\n+        litellm_params: dict,\n         api_key: Optional[str] = None,\n     ) -> dict:\n+        azure_ad_token_provider = litellm_params.get(\"azure_ad_token_provider\")",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2092028090",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10871,
        "pr_file": "litellm/llms/azure/responses/transformation.py",
        "discussion_id": "2092028090",
        "commented_code": "@@ -18,16 +24,73 @@\n else:\n     LiteLLMLoggingObj = Any\n \n-\n class AzureOpenAIResponsesAPIConfig(OpenAIResponsesAPIConfig):\n     def validate_environment(\n         self,\n         headers: dict,\n         model: str,\n+        litellm_params: dict,\n         api_key: Optional[str] = None,\n     ) -> dict:\n+        azure_ad_token_provider = litellm_params.get(\"azure_ad_token_provider\")",
        "comment_created_at": "2025-05-15T22:11:50+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "can we make this new block of logic a simple helper in azure/common_utils.py ? \r\n\r\nThen we can re-use for new endpoints like /image/edits etc ",
        "pr_file_module": null
      },
      {
        "comment_id": "2092030203",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10871,
        "pr_file": "litellm/llms/azure/responses/transformation.py",
        "discussion_id": "2092028090",
        "commented_code": "@@ -18,16 +24,73 @@\n else:\n     LiteLLMLoggingObj = Any\n \n-\n class AzureOpenAIResponsesAPIConfig(OpenAIResponsesAPIConfig):\n     def validate_environment(\n         self,\n         headers: dict,\n         model: str,\n+        litellm_params: dict,\n         api_key: Optional[str] = None,\n     ) -> dict:\n+        azure_ad_token_provider = litellm_params.get(\"azure_ad_token_provider\")",
        "comment_created_at": "2025-05-15T22:14:11+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "I mean the entire new section of code you added, that should be a simple helper util ",
        "pr_file_module": null
      }
    ]
  }
]