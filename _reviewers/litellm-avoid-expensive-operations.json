[
  {
    "discussion_id": "2258707022",
    "pr_number": 13356,
    "pr_file": "litellm/proxy/guardrails/guardrail_hooks/bedrock_guardrails.py",
    "created_at": "2025-08-07T02:13:15+00:00",
    "commented_code": "GUARDRAIL_NAME = \"bedrock\"\n \n \n+def _redact_pii_matches(response_json: dict) -> dict:\n+    try:\n+        # Create a deep copy to avoid modifying the original response\n+        redacted_response = copy.deepcopy(response_json)",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2258707022",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13356,
        "pr_file": "litellm/proxy/guardrails/guardrail_hooks/bedrock_guardrails.py",
        "discussion_id": "2258707022",
        "commented_code": "@@ -50,6 +51,51 @@\n GUARDRAIL_NAME = \"bedrock\"\n \n \n+def _redact_pii_matches(response_json: dict) -> dict:\n+    try:\n+        # Create a deep copy to avoid modifying the original response\n+        redacted_response = copy.deepcopy(response_json)",
        "comment_created_at": "2025-08-07T02:13:15+00:00",
        "comment_author": "jugaldb",
        "comment_body": "Question: what if the response_json is too big, and make this super memory intensive, we are seeing memory spikes at some places in the repo at the moment, and would like to understand how big can the response_json be?\r\n\r\nSince we are already doing response.json() which might be memory intensive for larger values, can we atleast use some form of streaming here?\r\n\r\nIf there is documentation somewhere that the response json wont be bigger than a certain number, then happy to approve.\r\n\r\nThis case can arise with very big streaming LLM calls.",
        "pr_file_module": null
      },
      {
        "comment_id": "2258825884",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13356,
        "pr_file": "litellm/proxy/guardrails/guardrail_hooks/bedrock_guardrails.py",
        "discussion_id": "2258707022",
        "commented_code": "@@ -50,6 +51,51 @@\n GUARDRAIL_NAME = \"bedrock\"\n \n \n+def _redact_pii_matches(response_json: dict) -> dict:\n+    try:\n+        # Create a deep copy to avoid modifying the original response\n+        redacted_response = copy.deepcopy(response_json)",
        "comment_created_at": "2025-08-07T03:22:48+00:00",
        "comment_author": "AnandKhinvasara",
        "comment_body": "Good point \u2014 and you're right, copy.deepcopy(response_json) does temporarily double memory usage.\n\nThat said, since we're already calling response.json() and holding the full object in memory, this redaction layer doesn't introduce additional parsing cost \u2014 just a one-time copy.\n\nThe tradeoff here is clarity and safety (we avoid mutating the original). If memory becomes a concern, we could switch to in-place redaction to eliminate the extra copy.\n\nRegarding streaming: you're right that response.json() fully buffers the response before parsing, which could be heavy for large payloads. We could consider using a streaming JSON parser like ijson to avoid full buffering, but that would require a significant refactor. Our current redaction logic navigates deeply nested fields (like assessments -> sensitiveInformationPolicy -> piiEntities), which assumes the entire object is loaded in memory. Streaming would mean switching to event-based parsing and rethinking how we redact \u2014 happy to explore that if we start seeing large enough responses to justify the complexity.\n\nOn the response size: AWS doesn\u2019t document a strict upper limit for ApplyGuardrail response payloads in characters or bytes. Instead, the guidance focuses on throughput, measured in text units (1 unit \u2248 1,000 characters).\nAs of early 2025, Guardrails supports up to 200 text units/sec (\u2248200,000 characters/sec) for content, sensitive info, and word filters. https://aws.amazon.com/about-aws/whats-new/2025/02/amazon-bedrock-guardrails-increase-service-quota-limits/\n\nSo while responses are not trivially small, they are practically bounded. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2259090741",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13356,
        "pr_file": "litellm/proxy/guardrails/guardrail_hooks/bedrock_guardrails.py",
        "discussion_id": "2258707022",
        "commented_code": "@@ -50,6 +51,51 @@\n GUARDRAIL_NAME = \"bedrock\"\n \n \n+def _redact_pii_matches(response_json: dict) -> dict:\n+    try:\n+        # Create a deep copy to avoid modifying the original response\n+        redacted_response = copy.deepcopy(response_json)",
        "comment_created_at": "2025-08-07T05:27:03+00:00",
        "comment_author": "jugaldb",
        "comment_body": "If it is bounded at 200k char/sec, I think this approach should be good.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1974955419",
    "pr_number": 8202,
    "pr_file": "litellm/utils.py",
    "created_at": "2025-02-28T07:47:55+00:00",
    "commented_code": "# LOG SUCCESS - handle streaming success logging in the _next_ object, remove `handle_success` once it's deprecated\n             verbose_logger.info(\"Wrapper: Completed Call, calling success_handler\")\n-            executor.submit(\n-                logging_obj.success_handler,\n-                result,\n-                start_time,\n-                end_time,\n-            )\n+            logging_obj.success_handler(result, start_time, end_time)",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1974955419",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8202,
        "pr_file": "litellm/utils.py",
        "discussion_id": "1974955419",
        "commented_code": "@@ -1121,12 +1121,8 @@ def wrapper(*args, **kwargs):  # noqa: PLR0915\n \n             # LOG SUCCESS - handle streaming success logging in the _next_ object, remove `handle_success` once it's deprecated\n             verbose_logger.info(\"Wrapper: Completed Call, calling success_handler\")\n-            executor.submit(\n-                logging_obj.success_handler,\n-                result,\n-                start_time,\n-                end_time,\n-            )\n+            logging_obj.success_handler(result, start_time, end_time)",
        "comment_created_at": "2025-02-28T07:47:55+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "if moving to success_handler, make sure that uses an executor, so as to not cause high cpu consumption @B-Step62 ",
        "pr_file_module": null
      },
      {
        "comment_id": "1975111581",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8202,
        "pr_file": "litellm/utils.py",
        "discussion_id": "1974955419",
        "commented_code": "@@ -1121,12 +1121,8 @@ def wrapper(*args, **kwargs):  # noqa: PLR0915\n \n             # LOG SUCCESS - handle streaming success logging in the _next_ object, remove `handle_success` once it's deprecated\n             verbose_logger.info(\"Wrapper: Completed Call, calling success_handler\")\n-            executor.submit(\n-                logging_obj.success_handler,\n-                result,\n-                start_time,\n-                end_time,\n-            )\n+            logging_obj.success_handler(result, start_time, end_time)",
        "comment_created_at": "2025-02-28T09:40:02+00:00",
        "comment_author": "B-Step62",
        "comment_body": "Thanks for the catch, updated.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160074917",
    "pr_number": 11945,
    "pr_file": "litellm/llms/custom_httpx/http_handler.py",
    "created_at": "2025-06-21T16:13:53+00:00",
    "commented_code": "@staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2160074917",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:13:53+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "Hi @deepanshululla our testing pipeline found that with trust_env a blocking call gets added. It looks like the following happens\r\n\r\nWhen trust_env=True (the default), aiohttp attempts to:\r\nRead environment variables for proxy configuration\r\nParse the .netrc file for authentication credentials\r\nBoth operations involve synchronous file I/O that blocks the event loop",
        "pr_file_module": null
      },
      {
        "comment_id": "2160078103",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:28:09+00:00",
        "comment_author": "deepanshululla",
        "comment_body": "Sorry I just saw this. \r\n\r\n> reviewed, we'll need a different approach\r\n\r\n\r\nSure what would you recommend",
        "pr_file_module": null
      },
      {
        "comment_id": "2160078445",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:30:01+00:00",
        "comment_author": "deepanshululla",
        "comment_body": "maybe proxy settings could be read during boot time and passed into the session?\r\n\r\n\r\n\r\n@ishaan-jaff ",
        "pr_file_module": null
      },
      {
        "comment_id": "2160079894",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:38:18+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "@deepanshululla can you try this PR: https://github.com/BerriAI/litellm/pull/11947 ",
        "pr_file_module": null
      },
      {
        "comment_id": "2160079990",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:38:33+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "if it works for you then we can go with #11947 ",
        "pr_file_module": null
      },
      {
        "comment_id": "2160081367",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:45:18+00:00",
        "comment_author": "deepanshululla",
        "comment_body": "Hi @ishaan-jaff I tried #11947 it doesn't seem to work",
        "pr_file_module": null
      },
      {
        "comment_id": "2160081750",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:47:46+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "Can we try your idea of getting the env vars on startup and then passing it through ? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2160081876",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:48:32+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "httpx exposes `get_environment_proxies` you can get it on startup @deepanshululla ",
        "pr_file_module": null
      },
      {
        "comment_id": "2160082960",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:51:04+00:00",
        "comment_author": "deepanshululla",
        "comment_body": "sounds good. on it.\r\n\r\nI think we need to store the environment proxy variables somewhere on startup.. where would be a good place to call and store it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160084593",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T16:53:30+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "You can store it as variables in `LiteLLMAiohttpTransport` ",
        "pr_file_module": null
      },
      {
        "comment_id": "2160086736",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T17:03:39+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "Here's a simple test to know if you have any blocking calls. Please can you make sure it passes \r\n\r\n```python\r\nfrom blockbuster import BlockBuster\r\nblockbuster = BlockBuster()\r\nblockbuster.activate()\r\n\r\nlitellm._turn_on_debug()\r\nlitellm.set_verbose = True\r\nresponse_id = None\r\nresponse = await litellm.acompletion(\r\n    model=\"gpt-4o-mini\",\r\n    messages=[{\"role\": \"user\", \"content\": \"This is a test\"}],\r\n)\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2160131705",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T19:38:53+00:00",
        "comment_author": "deepanshululla",
        "comment_body": "@ishaan-jaff correct me if I am wrong the `_create_aiohttp_transport` is called not during request time but during initial boot, so it is a blocking io but not for every request?\r\nAnd it is done once at boot time.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160173330",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T22:50:27+00:00",
        "comment_author": "deepanshululla",
        "comment_body": "I actually rested your changes https://github.com/BerriAI/litellm/pull/11947 and it works. Lets merge your PR and release it.\r\n\r\n----\r\n There seems to be another bug going on which seems to not respect litellm.ssl_verify  and hence ssl_verify is always set to true. So when ssl_verify is true and context is None, it just assumes default certify context..\r\n it is here\r\n https://github.com/BerriAI/litellm/commit/d4b34549bcf3d4efa788c2f9742349990b09ee42\r\n \r\n Which breaks things again\r\n \r\n \r\n ",
        "pr_file_module": null
      },
      {
        "comment_id": "2160174098",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-21T22:55:10+00:00",
        "comment_author": "deepanshululla",
        "comment_body": "can we make sure in https://github.com/BerriAI/litellm/commit/d4b34549bcf3d4efa788c2f9742349990b09ee42 ssl_verify in litellm setting is respected",
        "pr_file_module": null
      },
      {
        "comment_id": "2160319023",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11945,
        "pr_file": "litellm/llms/custom_httpx/http_handler.py",
        "discussion_id": "2160074917",
        "commented_code": "@@ -568,28 +568,32 @@ def _get_ssl_connector_kwargs(\n \n     @staticmethod\n     def _create_aiohttp_transport(\n-        ssl_verify: Optional[bool] = None,\n-        ssl_context: Optional[ssl.SSLContext] = None,\n+            ssl_verify: Optional[bool] = None,\n+            ssl_context: Optional[ssl.SSLContext] = None\n     ) -> LiteLLMAiohttpTransport:\n         \"\"\"\n         Creates an AiohttpTransport with RequestNotRead error handling\n \n-        Note: aiohttp TCPConnector ssl parameter accepts:\n-        - SSLContext: custom SSL context\n-        - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n+        - If force_ipv4 is True, it will create an AiohttpTransport with local_addr set to \"0.0.0.0\"\n+        - [Default] If force_ipv4 is False, it will create an AiohttpTransport with default settings\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n \n-        connector_kwargs = AsyncHTTPHandler._get_ssl_connector_kwargs(\n-            ssl_verify=ssl_verify, ssl_context=ssl_context\n+        verbose_logger.debug(\"Creating AiohttpTransport...\")\n+        connector = TCPConnector(\n+            verify_ssl=ssl_verify or True,\n+            ssl_context=ssl_context,\n+            local_addr=(\"0.0.0.0\", 0) if getattr(litellm, \"force_ipv4\", False) else None,\n         )\n+        def client_factory():\n+            session = ClientSession(\n+                connector=connector,\n+                trust_env=True  # Fall back to env proxy settings",
        "comment_created_at": "2025-06-22T12:11:00+00:00",
        "comment_author": "deepanshululla",
        "comment_body": "I noticed one another issue the function create transport is called both during startup and during request phase which is why probably a blocking call is introduced. But recreation of client during request phase makes me think it is not leveraging http pools well. This can be another performance optimzation reducing 50ms in e2e latency ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2127783811",
    "pr_number": 11421,
    "pr_file": "litellm/proxy/pass_through_endpoints/llm_provider_handlers/anthropic_passthrough_logging_handler.py",
    "created_at": "2025-06-05T01:42:52+00:00",
    "commented_code": "if transformed_openai_chunk is not None:\n                     all_openai_chunks.append(transformed_openai_chunk)\n \n-                verbose_proxy_logger.debug(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2127783811",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11421,
        "pr_file": "litellm/proxy/pass_through_endpoints/llm_provider_handlers/anthropic_passthrough_logging_handler.py",
        "discussion_id": "2127783811",
        "commented_code": "@@ -210,10 +210,6 @@ def _build_complete_streaming_response(\n                 if transformed_openai_chunk is not None:\n                     all_openai_chunks.append(transformed_openai_chunk)\n \n-                verbose_proxy_logger.debug(",
        "comment_created_at": "2025-06-05T01:42:52+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "This was the main bottleneck\r\n\r\nSee this CPU profile \r\n\r\n```\r\nFunction: _build_complete_streaming_response at line 189\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n   189                                               @staticmethod\r\n   190                                               @profile\r\n   191                                               def _build_complete_streaming_response(\r\n   192                                                   all_chunks: List[str],\r\n   193                                                   litellm_logging_obj: LiteLLMLoggingObj,\r\n   194                                                   model: str,\r\n   195                                               ) -> Optional[Union[ModelResponse, TextCompletionResponse]]:\r\n   196                                                   \"\"\"\r\n   197                                                   Builds complete response from raw Anthropic chunks\r\n   198                                           \r\n   199                                                   - Converts str chunks to generic chunks\r\n   200                                                   - Converts generic chunks to litellm chunks (OpenAI format)\r\n   201                                                   - Builds complete response from litellm chunks\r\n   202                                                   \"\"\"\r\n   203         2        163.0     81.5      0.0          anthropic_model_response_iterator = AnthropicModelResponseIterator(\r\n   204         1          0.0      0.0      0.0              streaming_response=None,\r\n   205         1          0.0      0.0      0.0              sync_stream=False,\r\n   206                                                   )\r\n   207         1          1.0      1.0      0.0          all_openai_chunks = []\r\n   208      1031        325.0      0.3      0.0          for _chunk_str in all_chunks:\r\n   209      1030         87.0      0.1      0.0              try:\r\n   210      2060     182467.0     88.6      1.7                  transformed_openai_chunk = anthropic_model_response_iterator.convert_str_chunk_to_generic_chunk(\r\n   211      1030         77.0      0.1      0.0                      chunk=_chunk_str\r\n   212                                                           )\r\n   213      1030        180.0      0.2      0.0                  if transformed_openai_chunk is not None:\r\n   214      1030        243.0      0.2      0.0                      all_openai_chunks.append(transformed_openai_chunk)\r\n   215                                           \r\n   216      2060       2427.0      1.2      0.0                  verbose_proxy_logger.debug(\r\n   217      1030         75.0      0.1      0.0                      \"all openai chunks= %s\",\r\n   218      1030   10639591.0  10329.7     98.0                      json.dumps(all_openai_chunks, indent=4, default=str),\r\n   219                                                           )\r\n   220                                                       except (StopIteration, StopAsyncIteration):\r\n   221                                                           break\r\n   222         2      31171.0  15585.5      0.3          complete_streaming_response = litellm.stream_chunk_builder(\r\n   223         1          0.0      0.0      0.0              chunks=all_openai_chunks\r\n   224                                                   )\r\n   225         1          0.0      0.0      0.0          return complete_streaming_response\r\n\r\n\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2084844288",
    "pr_number": 10757,
    "pr_file": "litellm/proxy/guardrails/guardrail_hooks/aim.py",
    "created_at": "2025-05-12T14:41:37+00:00",
    "commented_code": "self.ws_api_base = self.api_base.replace(\"http://\", \"ws://\").replace(\n             \"https://\", \"wss://\"\n         )\n+        self.dlp_entities: list[dict] = []",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2084844288",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10757,
        "pr_file": "litellm/proxy/guardrails/guardrail_hooks/aim.py",
        "discussion_id": "2084844288",
        "commented_code": "@@ -56,6 +56,7 @@ def __init__(\n         self.ws_api_base = self.api_base.replace(\"http://\", \"ws://\").replace(\n             \"https://\", \"wss://\"\n         )\n+        self.dlp_entities: list[dict] = []",
        "comment_created_at": "2025-05-12T14:41:37+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "Can we have a max limit on this - to prevent a memory leak ",
        "pr_file_module": null
      },
      {
        "comment_id": "2086321868",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10757,
        "pr_file": "litellm/proxy/guardrails/guardrail_hooks/aim.py",
        "discussion_id": "2084844288",
        "commented_code": "@@ -56,6 +56,7 @@ def __init__(\n         self.ws_api_base = self.api_base.replace(\"http://\", \"ws://\").replace(\n             \"https://\", \"wss://\"\n         )\n+        self.dlp_entities: list[dict] = []",
        "comment_created_at": "2025-05-13T09:10:39+00:00",
        "comment_author": "hxdror",
        "comment_body": "Done \ud83d\udc4d ",
        "pr_file_module": null
      }
    ]
  }
]