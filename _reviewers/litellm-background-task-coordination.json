[
  {
    "discussion_id": "1917448600",
    "pr_number": 7737,
    "pr_file": "litellm/utils.py",
    "created_at": "2025-01-15T23:02:38+00:00",
    "commented_code": "#  'usage': {'prompt_tokens': 18, 'completion_tokens': 23, 'total_tokens': 41}\n # }\n \n+# Create a global constant to store background tasks\n+# See: https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task\n+BACKGROUND_TASKS = set()",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1917448600",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7737,
        "pr_file": "litellm/utils.py",
        "discussion_id": "1917448600",
        "commented_code": "@@ -276,6 +276,9 @@\n #  'usage': {'prompt_tokens': 18, 'completion_tokens': 23, 'total_tokens': 41}\n # }\n \n+# Create a global constant to store background tasks\n+# See: https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task\n+BACKGROUND_TASKS = set()",
        "comment_created_at": "2025-01-15T23:02:38+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "would this just keep all tasks in memory, forever? \r\n\r\nConcerned re: possible memory leak. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1917862551",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7737,
        "pr_file": "litellm/utils.py",
        "discussion_id": "1917448600",
        "commented_code": "@@ -276,6 +276,9 @@\n #  'usage': {'prompt_tokens': 18, 'completion_tokens': 23, 'total_tokens': 41}\n # }\n \n+# Create a global constant to store background tasks\n+# See: https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task\n+BACKGROUND_TASKS = set()",
        "comment_created_at": "2025-01-16T07:19:11+00:00",
        "comment_author": "wtf-is-flying",
        "comment_body": "Tasks are asked to remove themselves from the set once they are done, see line 622 below `task.add_done_callback(BACKGROUND_TASKS.discard)` ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1951965027",
    "pr_number": 8454,
    "pr_file": "litellm/proxy/proxy_server.py",
    "created_at": "2025-02-12T04:38:06+00:00",
    "commented_code": "async def _run_background_health_check():\n     \"\"\"\n     Periodically run health checks in the background on the endpoints.\n-\n-    Update health_check_results, based on this.\n+    Updates the global health_check_results based on the checks.\n     \"\"\"\n     global health_check_results, llm_model_list, health_check_interval, health_check_details\n \n-    # make 1 deep copy of llm_model_list -> use this for all background health checks\n+    # Make a deep copy of llm_model_list for background health checks\n     _llm_model_list = copy.deepcopy(llm_model_list)\n \n-    if _llm_model_list is None:\n-        return\n+    if not _llm_model_list:\n+        return  # Exit if no models are available\n \n-    while True:\n+    while use_background_health_checks:\n         healthy_endpoints, unhealthy_endpoints = await perform_health_check(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1951965027",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8454,
        "pr_file": "litellm/proxy/proxy_server.py",
        "discussion_id": "1951965027",
        "commented_code": "@@ -1410,34 +1411,36 @@ def run_ollama_serve():\n async def _run_background_health_check():\n     \"\"\"\n     Periodically run health checks in the background on the endpoints.\n-\n-    Update health_check_results, based on this.\n+    Updates the global health_check_results based on the checks.\n     \"\"\"\n     global health_check_results, llm_model_list, health_check_interval, health_check_details\n \n-    # make 1 deep copy of llm_model_list -> use this for all background health checks\n+    # Make a deep copy of llm_model_list for background health checks\n     _llm_model_list = copy.deepcopy(llm_model_list)\n \n-    if _llm_model_list is None:\n-        return\n+    if not _llm_model_list:\n+        return  # Exit if no models are available\n \n-    while True:\n+    while use_background_health_checks:\n         healthy_endpoints, unhealthy_endpoints = await perform_health_check(",
        "comment_created_at": "2025-02-12T04:38:06+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "how does this fix the problem in #8248 ? ",
        "pr_file_module": null
      },
      {
        "comment_id": "1951982208",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8454,
        "pr_file": "litellm/proxy/proxy_server.py",
        "discussion_id": "1951965027",
        "commented_code": "@@ -1410,34 +1411,36 @@ def run_ollama_serve():\n async def _run_background_health_check():\n     \"\"\"\n     Periodically run health checks in the background on the endpoints.\n-\n-    Update health_check_results, based on this.\n+    Updates the global health_check_results based on the checks.\n     \"\"\"\n     global health_check_results, llm_model_list, health_check_interval, health_check_details\n \n-    # make 1 deep copy of llm_model_list -> use this for all background health checks\n+    # Make a deep copy of llm_model_list for background health checks\n     _llm_model_list = copy.deepcopy(llm_model_list)\n \n-    if _llm_model_list is None:\n-        return\n+    if not _llm_model_list:\n+        return  # Exit if no models are available\n \n-    while True:\n+    while use_background_health_checks:\n         healthy_endpoints, unhealthy_endpoints = await perform_health_check(",
        "comment_created_at": "2025-02-12T05:04:16+00:00",
        "comment_author": "jairajc",
        "comment_body": "The thought process behind my PR was to prevent the infinite loop issue that was occurring when background_health_checks was enabled.\r\n\r\nThe previous implementation used while True:, which meant the loop would continue running indefinitely, even when use_background_health_checks was set to False. Which made it difficult to stop the background health checks in a controlled manner.\r\n\r\nNow by changing it to while use_background_health_checks:, I intended for the loop to terminate naturally when the flag is set to False, allowing for a clean shutdown of the health checks instead of them running endlessly.\r\n\r\nBut you are right, I ran it again this morning just to check and after re-evaluating, I can see that the fix did not fully resolve the issue. I\u2019m still working on it locally debugging and trying different approaches to properly fix this. Haven\u2019t had a breakthrough yet, I appreciate your feedback and will update the PR once I have the fix!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2145235191",
    "pr_number": 11678,
    "pr_file": "litellm/proxy/health_endpoints/_health_endpoints.py",
    "created_at": "2025-06-13T14:35:03+00:00",
    "commented_code": "return health_check_results\n         else:\n             healthy_endpoints, unhealthy_endpoints = await perform_health_check(\n-                _llm_model_list, model, details=health_check_details\n+                _llm_model_list, target_model, details=health_check_details\n             )\n \n+            # Optionally save health check result to database (non-blocking)\n+            if prisma_client is not None and target_model is not None:\n+                await _save_health_check_to_db(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2145235191",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11678,
        "pr_file": "litellm/proxy/health_endpoints/_health_endpoints.py",
        "discussion_id": "2145235191",
        "commented_code": "@@ -360,9 +454,21 @@ async def health_endpoint(\n             return health_check_results\n         else:\n             healthy_endpoints, unhealthy_endpoints = await perform_health_check(\n-                _llm_model_list, model, details=health_check_details\n+                _llm_model_list, target_model, details=health_check_details\n             )\n \n+            # Optionally save health check result to database (non-blocking)\n+            if prisma_client is not None and target_model is not None:\n+                await _save_health_check_to_db(",
        "comment_created_at": "2025-06-13T14:35:03+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "use asyncio.create_task so we don't need to wait for writing to the DB",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2093782179",
    "pr_number": 10895,
    "pr_file": "litellm/proxy/db/db_transaction_queue/spend_log_cleanup.py",
    "created_at": "2025-05-16T23:57:51+00:00",
    "commented_code": "verbose_proxy_logger.error(\"Retention seconds is None, cannot proceed with cleanup\")\n                 return\n \n-            cutoff_date = datetime.now(UTC) - timedelta(seconds=float(self.retention_seconds))\n-            verbose_proxy_logger.info(f\"Deleting logs older than {cutoff_date.isoformat()}\")\n-\n-            total_deleted = 0\n-            run_count = 0\n-            while True:\n-                if run_count > 100:\n-                    verbose_proxy_logger.info(\"Max logs deleted - 1,00,000, rest of the logs will be deleted in next run\")\n-                    break\n-                # Step 1: Find logs to delete\n-                logs_to_delete = await prisma_client.db.litellm_spendlogs.find_many(\n-                    where={\"startTime\": {\"lt\": cutoff_date}},\n-                    take=self.batch_size,\n+            # If we have a pod lock manager, try to acquire the lock\n+            if self.pod_lock_manager and self.pod_lock_manager.redis_cache:\n+                lock_acquired = await self.pod_lock_manager.acquire_lock(\n+                    cronjob_id=SPEND_LOG_CLEANUP_JOB_NAME,\n                 )\n-                verbose_proxy_logger.info(f\"\ud83d\uddd1\ufe0f Found {len(logs_to_delete)} logs in this batch\")\n+                verbose_proxy_logger.info(f\"Lock acquisition attempt: {'successful' if lock_acquired else 'failed'}  at {datetime.now()}\")\n \n-                if not logs_to_delete:\n-                    verbose_proxy_logger.info(f\"No more logs to delete. Total deleted: {total_deleted}\")\n-                    break\n+                if not lock_acquired:\n+                    verbose_proxy_logger.info(\"Another pod is already running cleanup\")\n+                    return\n \n-                request_ids = [log.request_id for log in logs_to_delete]\n+            try:\n+                cutoff_date = datetime.now(timezone.utc) - timedelta(seconds=float(self.retention_seconds))\n+                verbose_proxy_logger.info(f\"Deleting logs older than {cutoff_date.isoformat()}\")\n \n-                # Step 2: Delete them in one go\n-                await prisma_client.db.litellm_spendlogs.delete_many(\n-                    where={\"request_id\": {\"in\": request_ids}}\n-                )\n+                # Perform the actual deletion\n+                total_deleted = await self._delete_old_logs(prisma_client, cutoff_date)\n+\n+                verbose_proxy_logger.info(f\"Deleted {total_deleted} logs\")\n+\n+                # If we have a pod lock manager, release the lock\n+                if self.pod_lock_manager and self.pod_lock_manager.redis_cache:\n+                    await self.pod_lock_manager.release_lock(cronjob_id=SPEND_LOG_CLEANUP_JOB_NAME)\n+                    verbose_proxy_logger.info(f\"Released cleanup lock {datetime.now()}\")\n+                return  # Explicitly return after cleanup is complete\n+\n+            except Exception as e:\n+                verbose_proxy_logger.error(f\"Error during cleanup: {str(e)}\")",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2093782179",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10895,
        "pr_file": "litellm/proxy/db/db_transaction_queue/spend_log_cleanup.py",
        "discussion_id": "2093782179",
        "commented_code": "@@ -56,35 +103,40 @@ async def cleanup_old_spend_logs(self, prisma_client: PrismaClient) -> None:\n                 verbose_proxy_logger.error(\"Retention seconds is None, cannot proceed with cleanup\")\n                 return\n \n-            cutoff_date = datetime.now(UTC) - timedelta(seconds=float(self.retention_seconds))\n-            verbose_proxy_logger.info(f\"Deleting logs older than {cutoff_date.isoformat()}\")\n-\n-            total_deleted = 0\n-            run_count = 0\n-            while True:\n-                if run_count > 100:\n-                    verbose_proxy_logger.info(\"Max logs deleted - 1,00,000, rest of the logs will be deleted in next run\")\n-                    break\n-                # Step 1: Find logs to delete\n-                logs_to_delete = await prisma_client.db.litellm_spendlogs.find_many(\n-                    where={\"startTime\": {\"lt\": cutoff_date}},\n-                    take=self.batch_size,\n+            # If we have a pod lock manager, try to acquire the lock\n+            if self.pod_lock_manager and self.pod_lock_manager.redis_cache:\n+                lock_acquired = await self.pod_lock_manager.acquire_lock(\n+                    cronjob_id=SPEND_LOG_CLEANUP_JOB_NAME,\n                 )\n-                verbose_proxy_logger.info(f\"\ud83d\uddd1\ufe0f Found {len(logs_to_delete)} logs in this batch\")\n+                verbose_proxy_logger.info(f\"Lock acquisition attempt: {'successful' if lock_acquired else 'failed'}  at {datetime.now()}\")\n \n-                if not logs_to_delete:\n-                    verbose_proxy_logger.info(f\"No more logs to delete. Total deleted: {total_deleted}\")\n-                    break\n+                if not lock_acquired:\n+                    verbose_proxy_logger.info(\"Another pod is already running cleanup\")\n+                    return\n \n-                request_ids = [log.request_id for log in logs_to_delete]\n+            try:\n+                cutoff_date = datetime.now(timezone.utc) - timedelta(seconds=float(self.retention_seconds))\n+                verbose_proxy_logger.info(f\"Deleting logs older than {cutoff_date.isoformat()}\")\n \n-                # Step 2: Delete them in one go\n-                await prisma_client.db.litellm_spendlogs.delete_many(\n-                    where={\"request_id\": {\"in\": request_ids}}\n-                )\n+                # Perform the actual deletion\n+                total_deleted = await self._delete_old_logs(prisma_client, cutoff_date)\n+\n+                verbose_proxy_logger.info(f\"Deleted {total_deleted} logs\")\n+\n+                # If we have a pod lock manager, release the lock\n+                if self.pod_lock_manager and self.pod_lock_manager.redis_cache:\n+                    await self.pod_lock_manager.release_lock(cronjob_id=SPEND_LOG_CLEANUP_JOB_NAME)\n+                    verbose_proxy_logger.info(f\"Released cleanup lock {datetime.now()}\")\n+                return  # Explicitly return after cleanup is complete\n+\n+            except Exception as e:\n+                verbose_proxy_logger.error(f\"Error during cleanup: {str(e)}\")",
        "comment_created_at": "2025-05-16T23:57:51+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "you can release the lock in a finally section, that way there is no dup code in try/except blocks \r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2092052634",
    "pr_number": 10872,
    "pr_file": "litellm/proxy/db/db_transaction_queue/spend_log_cleanup.py",
    "created_at": "2025-05-15T22:33:15+00:00",
    "commented_code": "-\"\"\"\n-Handles checking if spend logs should be deleted based on maximum retention period\n-\"\"\"\n-\n-from typing import Optional, Union\n-\n-from litellm._logging import verbose_proxy_logger\n+import asyncio\n+import logging\n+from datetime import datetime, timedelta, UTC\n+import os\n+from typing import Optional\n+from litellm.proxy.utils import PrismaClient\n from litellm.litellm_core_utils.duration_parser import duration_in_seconds\n-from litellm.proxy.proxy_server import general_settings\n \n+logger = logging.getLogger(__name__)\n \n-def _should_delete_spend_logs() -> bool:\n+class SpendLogCleanup:\n     \"\"\"\n-    Checks if the Pod should delete spend logs based on maximum retention period\n-\n-    This setting enables automatic deletion of old spend logs to manage database size.\n-    The maximum_spend_logs_retention_period can be specified in:\n-    - Days (e.g., \"30d\")\n-    - Hours (e.g., \"24h\")\n-    - Minutes (e.g., \"60m\")\n-    - Seconds (e.g., \"3600s\" or just \"3600\")\n+    Handles cleaning up old spend logs based on maximum retention period.\n+    Deletes logs in batches to prevent timeouts.\n     \"\"\"\n-    _maximum_spend_logs_retention_period: Optional[Union[int, str]] = general_settings.get(\n-        \"maximum_spend_logs_retention_period\", None\n-    )\n-    \n-    if _maximum_spend_logs_retention_period is None:\n-        return False\n-\n-    try:\n-        if isinstance(_maximum_spend_logs_retention_period, int):\n-            _maximum_spend_logs_retention_period = str(_maximum_spend_logs_retention_period)\n-        duration_in_seconds(_maximum_spend_logs_retention_period)\n-        return True\n-    except ValueError as e:\n-        verbose_proxy_logger.error(\n-            f\"Invalid maximum_spend_logs_retention_period value: {_maximum_spend_logs_retention_period}, error: {str(e)}\"\n-        )\n-        return False \n\\ No newline at end of file\n+\n+    def __init__(self, general_settings=None):\n+        self.batch_size = 1000\n+        self.retention_seconds: Optional[int] = None\n+        from litellm.proxy.proxy_server import general_settings as default_settings\n+        self.general_settings = general_settings or default_settings\n+\n+    def _should_delete_spend_logs(self) -> bool:\n+        \"\"\"\n+        Determines if logs should be deleted based on the max retention period in settings.\n+        \"\"\"\n+        retention_setting = self.general_settings.get(\"maximum_spend_logs_retention_period\")\n+\n+        if retention_setting is None:\n+            return False\n+\n+        try:\n+            if isinstance(retention_setting, int):\n+                retention_setting = str(retention_setting)\n+            self.retention_seconds = duration_in_seconds(retention_setting)\n+            return True\n+        except ValueError as e:\n+            logger.error(\n+                f\"Invalid maximum_spend_logs_retention_period value: {retention_setting}, error: {str(e)}\"\n+            )\n+            return False\n+\n+    async def cleanup_old_spend_logs(self, prisma_client: PrismaClient) -> None:\n+        \"\"\"\n+        Main cleanup function. Deletes old spend logs in batches.\n+        \"\"\"\n+        try:\n+            logger.info(f\"Cleanup DB URL: {os.environ.get('DATABASE_URL')}\")\n+            logger.info(f\"Cleanup job triggered at {datetime.now()}\")\n+\n+            if not self._should_delete_spend_logs():\n+                logger.info(\"Skipping cleanup \u2014 invalid or missing retention setting.\")\n+                return\n+\n+            cutoff_date = datetime.now(UTC) - timedelta(seconds=self.retention_seconds)\n+            logger.info(f\"\ud83e\uddf9 Deleting logs older than {cutoff_date.isoformat()}\")\n+\n+            total_deleted = 0\n+            while True:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2092052634",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10872,
        "pr_file": "litellm/proxy/db/db_transaction_queue/spend_log_cleanup.py",
        "discussion_id": "2092052634",
        "commented_code": "@@ -1,39 +1,78 @@\n-\"\"\"\n-Handles checking if spend logs should be deleted based on maximum retention period\n-\"\"\"\n-\n-from typing import Optional, Union\n-\n-from litellm._logging import verbose_proxy_logger\n+import asyncio\n+import logging\n+from datetime import datetime, timedelta, UTC\n+import os\n+from typing import Optional\n+from litellm.proxy.utils import PrismaClient\n from litellm.litellm_core_utils.duration_parser import duration_in_seconds\n-from litellm.proxy.proxy_server import general_settings\n \n+logger = logging.getLogger(__name__)\n \n-def _should_delete_spend_logs() -> bool:\n+class SpendLogCleanup:\n     \"\"\"\n-    Checks if the Pod should delete spend logs based on maximum retention period\n-\n-    This setting enables automatic deletion of old spend logs to manage database size.\n-    The maximum_spend_logs_retention_period can be specified in:\n-    - Days (e.g., \"30d\")\n-    - Hours (e.g., \"24h\")\n-    - Minutes (e.g., \"60m\")\n-    - Seconds (e.g., \"3600s\" or just \"3600\")\n+    Handles cleaning up old spend logs based on maximum retention period.\n+    Deletes logs in batches to prevent timeouts.\n     \"\"\"\n-    _maximum_spend_logs_retention_period: Optional[Union[int, str]] = general_settings.get(\n-        \"maximum_spend_logs_retention_period\", None\n-    )\n-    \n-    if _maximum_spend_logs_retention_period is None:\n-        return False\n-\n-    try:\n-        if isinstance(_maximum_spend_logs_retention_period, int):\n-            _maximum_spend_logs_retention_period = str(_maximum_spend_logs_retention_period)\n-        duration_in_seconds(_maximum_spend_logs_retention_period)\n-        return True\n-    except ValueError as e:\n-        verbose_proxy_logger.error(\n-            f\"Invalid maximum_spend_logs_retention_period value: {_maximum_spend_logs_retention_period}, error: {str(e)}\"\n-        )\n-        return False \n\\ No newline at end of file\n+\n+    def __init__(self, general_settings=None):\n+        self.batch_size = 1000\n+        self.retention_seconds: Optional[int] = None\n+        from litellm.proxy.proxy_server import general_settings as default_settings\n+        self.general_settings = general_settings or default_settings\n+\n+    def _should_delete_spend_logs(self) -> bool:\n+        \"\"\"\n+        Determines if logs should be deleted based on the max retention period in settings.\n+        \"\"\"\n+        retention_setting = self.general_settings.get(\"maximum_spend_logs_retention_period\")\n+\n+        if retention_setting is None:\n+            return False\n+\n+        try:\n+            if isinstance(retention_setting, int):\n+                retention_setting = str(retention_setting)\n+            self.retention_seconds = duration_in_seconds(retention_setting)\n+            return True\n+        except ValueError as e:\n+            logger.error(\n+                f\"Invalid maximum_spend_logs_retention_period value: {retention_setting}, error: {str(e)}\"\n+            )\n+            return False\n+\n+    async def cleanup_old_spend_logs(self, prisma_client: PrismaClient) -> None:\n+        \"\"\"\n+        Main cleanup function. Deletes old spend logs in batches.\n+        \"\"\"\n+        try:\n+            logger.info(f\"Cleanup DB URL: {os.environ.get('DATABASE_URL')}\")\n+            logger.info(f\"Cleanup job triggered at {datetime.now()}\")\n+\n+            if not self._should_delete_spend_logs():\n+                logger.info(\"Skipping cleanup \u2014 invalid or missing retention setting.\")\n+                return\n+\n+            cutoff_date = datetime.now(UTC) - timedelta(seconds=self.retention_seconds)\n+            logger.info(f\"\ud83e\uddf9 Deleting logs older than {cutoff_date.isoformat()}\")\n+\n+            total_deleted = 0\n+            while True:",
        "comment_created_at": "2025-05-15T22:33:15+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "if you run every 20s and have a while true - couldn't you end up in situations where the job is still running while another is queued? ",
        "pr_file_module": null
      }
    ]
  }
]