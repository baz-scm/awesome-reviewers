[
  {
    "discussion_id": "1887009902",
    "pr_number": 6981,
    "pr_file": "litellm/litellm_core_utils/get_llm_provider_logic.py",
    "created_at": "2024-12-16T15:22:06+00:00",
    "commented_code": "if _is_non_openai_azure_model(model):\n                 custom_llm_provider = \"openai\"\n                 return model, custom_llm_provider, dynamic_api_key, api_base\n+        elif model.split(\"/\", 1)[0] == \"nvidia\" or model in litellm.nvidia_models:\n+            api_base = (\n+                api_base \n+                or get_secret(\"NVIDIA_API_BASE\")",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1887009902",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "litellm/litellm_core_utils/get_llm_provider_logic.py",
        "discussion_id": "1887009902",
        "commented_code": "@@ -86,6 +86,23 @@ def get_llm_provider(  # noqa: PLR0915\n             if _is_non_openai_azure_model(model):\n                 custom_llm_provider = \"openai\"\n                 return model, custom_llm_provider, dynamic_api_key, api_base\n+        elif model.split(\"/\", 1)[0] == \"nvidia\" or model in litellm.nvidia_models:\n+            api_base = (\n+                api_base \n+                or get_secret(\"NVIDIA_API_BASE\") ",
        "comment_created_at": "2024-12-16T15:22:06+00:00",
        "comment_author": "mattf",
        "comment_body": "we're using `NVIDIA_BASE_URL` in other frameworks. `*_API_BASE` appears idiomatic for litellm. can we do both?",
        "pr_file_module": null
      },
      {
        "comment_id": "1887012637",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "litellm/litellm_core_utils/get_llm_provider_logic.py",
        "discussion_id": "1887009902",
        "commented_code": "@@ -86,6 +86,23 @@ def get_llm_provider(  # noqa: PLR0915\n             if _is_non_openai_azure_model(model):\n                 custom_llm_provider = \"openai\"\n                 return model, custom_llm_provider, dynamic_api_key, api_base\n+        elif model.split(\"/\", 1)[0] == \"nvidia\" or model in litellm.nvidia_models:\n+            api_base = (\n+                api_base \n+                or get_secret(\"NVIDIA_API_BASE\") ",
        "comment_created_at": "2024-12-16T15:23:45+00:00",
        "comment_author": "mattf",
        "comment_body": "also keep backward compat w/ `NVIDIA_NIM_API_BASE`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1986502069",
    "pr_number": 6981,
    "pr_file": "tests/llm_translation/test_max_completion_tokens.py",
    "created_at": "2025-03-10T01:53:02+00:00",
    "commented_code": "drop_params=False,\n     ) == {\"max_new_tokens\": 10}\n \n-    from litellm.llms.nvidia_nim.chat import NvidiaNimConfig\n+    from litellm.llms.nvidia.chat import NvidiaNimConfig",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1986502069",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "tests/llm_translation/test_max_completion_tokens.py",
        "discussion_id": "1986502069",
        "commented_code": "@@ -181,7 +181,7 @@ def test_all_model_configs():\n         drop_params=False,\n     ) == {\"max_new_tokens\": 10}\n \n-    from litellm.llms.nvidia_nim.chat import NvidiaNimConfig\n+    from litellm.llms.nvidia.chat import NvidiaNimConfig",
        "comment_created_at": "2025-03-10T01:53:02+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "i'm confused \r\n\r\nnvidia_nim is a separate provider to nvidia, why is the import for nvidia nim config coming from nvidia? ",
        "pr_file_module": null
      },
      {
        "comment_id": "1986513327",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "tests/llm_translation/test_max_completion_tokens.py",
        "discussion_id": "1986502069",
        "commented_code": "@@ -181,7 +181,7 @@ def test_all_model_configs():\n         drop_params=False,\n     ) == {\"max_new_tokens\": 10}\n \n-    from litellm.llms.nvidia_nim.chat import NvidiaNimConfig\n+    from litellm.llms.nvidia.chat import NvidiaNimConfig",
        "comment_created_at": "2025-03-10T02:16:15+00:00",
        "comment_author": "raspawar",
        "comment_body": "No, just keeping naming consistent",
        "pr_file_module": null
      },
      {
        "comment_id": "1986517468",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "tests/llm_translation/test_max_completion_tokens.py",
        "discussion_id": "1986502069",
        "commented_code": "@@ -181,7 +181,7 @@ def test_all_model_configs():\n         drop_params=False,\n     ) == {\"max_new_tokens\": 10}\n \n-    from litellm.llms.nvidia_nim.chat import NvidiaNimConfig\n+    from litellm.llms.nvidia.chat import NvidiaNimConfig",
        "comment_created_at": "2025-03-10T02:23:13+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "so is nvidia nim inside nvidia folder? \r\n\r\nthis breaks the pattern for `llms/` where the provider name is the folder - https://github.com/BerriAI/litellm/tree/main/litellm/llms\r\n<img width=\"1351\" alt=\"Screenshot 2025-03-09 at 7 23 07 PM\" src=\"https://github.com/user-attachments/assets/bd45241c-138c-4692-bd05-2601fa2c9486\" />\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1986519298",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "tests/llm_translation/test_max_completion_tokens.py",
        "discussion_id": "1986502069",
        "commented_code": "@@ -181,7 +181,7 @@ def test_all_model_configs():\n         drop_params=False,\n     ) == {\"max_new_tokens\": 10}\n \n-    from litellm.llms.nvidia_nim.chat import NvidiaNimConfig\n+    from litellm.llms.nvidia.chat import NvidiaNimConfig",
        "comment_created_at": "2025-03-10T02:26:22+00:00",
        "comment_author": "raspawar",
        "comment_body": "updating the folder structure as you suggested and renaming NvidiaNimConfig to NvidiaConfig",
        "pr_file_module": null
      },
      {
        "comment_id": "1986520333",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "tests/llm_translation/test_max_completion_tokens.py",
        "discussion_id": "1986502069",
        "commented_code": "@@ -181,7 +181,7 @@ def test_all_model_configs():\n         drop_params=False,\n     ) == {\"max_new_tokens\": 10}\n \n-    from litellm.llms.nvidia_nim.chat import NvidiaNimConfig\n+    from litellm.llms.nvidia.chat import NvidiaNimConfig",
        "comment_created_at": "2025-03-10T02:28:39+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "was there an older nvidianim config? if so - what happens to it? \r\n\r\nI'm unclear on whether you're modifying the existing - `nvidia_nim/` integration, or adding a new provider\r\n\r\nif it's a new provider, can we avoid any modification of the existing `nvidia_nim/` provider ",
        "pr_file_module": null
      },
      {
        "comment_id": "1986521245",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "tests/llm_translation/test_max_completion_tokens.py",
        "discussion_id": "1986502069",
        "commented_code": "@@ -181,7 +181,7 @@ def test_all_model_configs():\n         drop_params=False,\n     ) == {\"max_new_tokens\": 10}\n \n-    from litellm.llms.nvidia_nim.chat import NvidiaNimConfig\n+    from litellm.llms.nvidia.chat import NvidiaNimConfig",
        "comment_created_at": "2025-03-10T02:30:29+00:00",
        "comment_author": "raspawar",
        "comment_body": "I am updating existing nvidia_nim integration. Will add Readme.md file with a note.",
        "pr_file_module": null
      },
      {
        "comment_id": "1988462123",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "tests/llm_translation/test_max_completion_tokens.py",
        "discussion_id": "1986502069",
        "commented_code": "@@ -181,7 +181,7 @@ def test_all_model_configs():\n         drop_params=False,\n     ) == {\"max_new_tokens\": 10}\n \n-    from litellm.llms.nvidia_nim.chat import NvidiaNimConfig\n+    from litellm.llms.nvidia.chat import NvidiaNimConfig",
        "comment_created_at": "2025-03-11T06:04:02+00:00",
        "comment_author": "raspawar",
        "comment_body": "I have addressed all review changes, can we close this @krrishdholakia ?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172954255",
    "pr_number": 12116,
    "pr_file": "litellm/types/guardrails.py",
    "created_at": "2025-06-27T22:48:14+00:00",
    "commented_code": "AIM = \"aim\"\n     PANGEA = \"pangea\"\n     LASSO = \"lasso\"\n+    PANW_PRISMA_AIRS = \"panw_prisma_airs\"",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2172954255",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12116,
        "pr_file": "litellm/types/guardrails.py",
        "discussion_id": "2172954255",
        "commented_code": "@@ -30,6 +30,7 @@ class SupportedGuardrailIntegrations(Enum):\n     AIM = \"aim\"\n     PANGEA = \"pangea\"\n     LASSO = \"lasso\"\n+    PANW_PRISMA_AIRS = \"panw_prisma_airs\"",
        "comment_created_at": "2025-06-27T22:48:14+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "nit: can we use a less verbose name e.g. `prisma_airs` ? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2138826141",
    "pr_number": 11595,
    "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
    "created_at": "2025-06-10T21:51:13+00:00",
    "commented_code": "os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2138826141",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-10T21:51:13+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "is this change backwards compatible? @vanities ",
        "pr_file_module": null
      },
      {
        "comment_id": "2138834344",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-10T21:58:47+00:00",
        "comment_author": "vanities",
        "comment_body": "I have no clue to be honest. I am also unsure of the implications of changing this line on other parts of the system.\r\n\r\nLikely `space_key` ideally would also be renamed to `space_id` everywhere?",
        "pr_file_module": null
      },
      {
        "comment_id": "2138851874",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-10T22:15:06+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "is the current code (not your PR) failing for you? @vanities ",
        "pr_file_module": null
      },
      {
        "comment_id": "2138984751",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-11T01:02:48+00:00",
        "comment_author": "vanities",
        "comment_body": "yes, this issue: https://github.com/BerriAI/litellm/issues/11465, traces wouldn't send at all\r\n\r\nIt looks like Arize changed the name from \"key\" to \"id\".",
        "pr_file_module": null
      },
      {
        "comment_id": "2141668299",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-12T04:05:01+00:00",
        "comment_author": "arizedatngo",
        "comment_body": "i would stick with space_id",
        "pr_file_module": null
      },
      {
        "comment_id": "2141670963",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-12T04:05:47+00:00",
        "comment_author": "arizedatngo",
        "comment_body": "it should be backwards compatible in our system, as we accept both, so i'm curious as to why you got that error",
        "pr_file_module": null
      },
      {
        "comment_id": "2141689357",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-12T04:27:07+00:00",
        "comment_author": "arizedatngo",
        "comment_body": "I think that PR would work, and I would push it\r\n\r\nspace_id is correct. I suspect there's an issue with the deprecated space_key in our backend (I think I found it) but would say use space_id",
        "pr_file_module": null
      },
      {
        "comment_id": "2142944595",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-12T14:35:29+00:00",
        "comment_author": "vanities",
        "comment_body": "@krrishdholakia, do you want me to change anything else in this?",
        "pr_file_module": null
      },
      {
        "comment_id": "2143837191",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11595,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2138826141",
        "commented_code": "@@ -2912,7 +2912,7 @@ def _init_custom_logger_compatible_class(  # noqa: PLR0915\n \n             os.environ[\n                 \"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"\n-            ] = f\"space_key={arize_config.space_key},api_key={arize_config.api_key}\"\n+            ] = f\"space_id={arize_config.space_key},api_key={arize_config.api_key}\"",
        "comment_created_at": "2025-06-12T23:50:28+00:00",
        "comment_author": "arizedatngo",
        "comment_body": "will ping krrish\r\n\r\nsaw a ci failure too, not sure if related to your PR\r\n\r\n`proxy/common_utils/reset_budget_job.py:527: error: \"LiteLLM_BudgetTableFull\" has no attribute \"budget_reset_at\"  [attr-defined]`\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2069852215",
    "pr_number": 10451,
    "pr_file": "litellm/llms/meta/chat.py",
    "created_at": "2025-05-01T04:59:00+00:00",
    "commented_code": null,
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2069852215",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10451,
        "pr_file": "litellm/llms/meta/chat.py",
        "discussion_id": "2069852215",
        "commented_code": null,
        "comment_created_at": "2025-05-01T04:59:00+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "the file should be `/chat/transformation.py` ",
        "pr_file_module": null
      },
      {
        "comment_id": "2069853500",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10451,
        "pr_file": "litellm/llms/meta/chat.py",
        "discussion_id": "2069852215",
        "commented_code": null,
        "comment_created_at": "2025-05-01T05:01:37+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "the folder should also match the llm provider \r\n\r\nin your case, if you're saying it's `meta-llama`, then the folder should follow the same convention (not: `meta/`) ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1997890217",
    "pr_number": 9169,
    "pr_file": "tests/litellm/llms/netmind/embed/test_netmind_embed.py",
    "created_at": "2025-03-17T03:47:23+00:00",
    "commented_code": "+from unittest.mock import MagicMock, patch\n+import pytest\n+import sys\n+import os\n+from openai.types.create_embedding_response import CreateEmbeddingResponse\n+from openai.types import Embedding\n+from openai._models import construct_type\n+\n+sys.path.insert(0, os.path.abspath(\"../../../../../\"))\n+\n+import litellm\n+from litellm import embedding\n+\n+user_message = \"How are you doing today?\"\n+messages = [{\"content\": user_message, \"role\": \"user\"}]\n+\n+\n+def mock_response():",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1997890217",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9169,
        "pr_file": "tests/litellm/llms/netmind/embed/test_netmind_embed.py",
        "discussion_id": "1997890217",
        "commented_code": "@@ -0,0 +1,49 @@\n+from unittest.mock import MagicMock, patch\n+import pytest\n+import sys\n+import os\n+from openai.types.create_embedding_response import CreateEmbeddingResponse\n+from openai.types import Embedding\n+from openai._models import construct_type\n+\n+sys.path.insert(0, os.path.abspath(\"../../../../../\"))\n+\n+import litellm\n+from litellm import embedding\n+\n+user_message = \"How are you doing today?\"\n+messages = [{\"content\": user_message, \"role\": \"user\"}]\n+\n+\n+def mock_response():",
        "comment_created_at": "2025-03-17T03:47:23+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "rename this test file to specify the file being tested in the litellm/ folder\r\n\r\ni assume this is for `embed/transformation`? \r\n\r\nso this should be `embed/test_transformation.py` or `embed/test_netmind_transformation.py` in case of conflicts",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2071105696",
    "pr_number": 10370,
    "pr_file": "tests/litellm/integrations/test_slack.py",
    "created_at": "2025-05-02T05:14:28+00:00",
    "commented_code": null,
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2071105696",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10370,
        "pr_file": "tests/litellm/integrations/test_slack.py",
        "discussion_id": "2071105696",
        "commented_code": null,
        "comment_created_at": "2025-05-02T05:14:28+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "follow same naming convention + filepath as in `litellm/` \r\n\r\nso it should be `tests/litellm/integrations/SlackAlerting/test_slack_alerting.py` ",
        "pr_file_module": null
      },
      {
        "comment_id": "2071809170",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10370,
        "pr_file": "tests/litellm/integrations/test_slack.py",
        "discussion_id": "2071105696",
        "commented_code": null,
        "comment_created_at": "2025-05-02T15:45:59+00:00",
        "comment_author": "hypermoose",
        "comment_body": "Fixed thanks",
        "pr_file_module": null
      }
    ]
  }
]