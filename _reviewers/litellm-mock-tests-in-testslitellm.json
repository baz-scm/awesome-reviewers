[
  {
    "discussion_id": "2311785242",
    "pr_number": 14028,
    "pr_file": "tests/llm_translation/test_volcengine_embedding.py",
    "created_at": "2025-08-30T04:57:47+00:00",
    "commented_code": null,
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2311785242",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 14028,
        "pr_file": "tests/llm_translation/test_volcengine_embedding.py",
        "discussion_id": "2311785242",
        "commented_code": null,
        "comment_created_at": "2025-08-30T04:57:47+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "move inside `test_litellm/` \r\n\r\nwe don't have volcengine credentials - so all testing should be mocked and be inside test_litellm ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1891031788",
    "pr_number": 7297,
    "pr_file": "litellm/llms/vllm/completion/handler.py",
    "created_at": "2024-12-19T01:50:07+00:00",
    "commented_code": ")\n     \"\"\"\n     try:\n-        llm, SamplingParams = validate_environment(model=model)\n+        llm, SamplingParams, optional_params = validate_environment(model=model, optional_params=optional_params)",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1891031788",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7297,
        "pr_file": "litellm/llms/vllm/completion/handler.py",
        "discussion_id": "1891031788",
        "commented_code": "@@ -142,7 +159,7 @@ def batch_completions(\n     )\n     \"\"\"\n     try:\n-        llm, SamplingParams = validate_environment(model=model)\n+        llm, SamplingParams, optional_params = validate_environment(model=model, optional_params=optional_params)",
        "comment_created_at": "2024-12-19T01:50:07+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "can you add a mock test w/ screenshot of this working?\r\n\r\nsimilar to this - https://github.com/BerriAI/litellm/blob/246e3bafc89e19297fa42aabbfca6d25058e5989/tests/llm_translation/test_azure_ai.py#L48\r\n\r\n\r\nideally this test would **not** add the vllm sdk as a dep on the ci/cd pipeline (so maybe use a magicmock object here) ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2103854111",
    "pr_number": 11037,
    "pr_file": "tests/llm_translation/test_cloudflare.py",
    "created_at": "2025-05-23T05:53:45+00:00",
    "commented_code": "except Exception as e:\n         pytest.fail(f\"Error occurred: {e}\")\n+\n+\n+@pytest.mark.asyncio\n+@pytest.mark.parametrize(\"stream\", [True, False])\n+async def test_completion_cloudflare_llama4_scout(stream):",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2103854111",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11037,
        "pr_file": "tests/llm_translation/test_cloudflare.py",
        "discussion_id": "2103854111",
        "commented_code": "@@ -40,3 +40,358 @@ async def test_completion_cloudflare(stream):\n \n     except Exception as e:\n         pytest.fail(f\"Error occurred: {e}\")\n+\n+\n+@pytest.mark.asyncio\n+@pytest.mark.parametrize(\"stream\", [True, False])\n+async def test_completion_cloudflare_llama4_scout(stream):",
        "comment_created_at": "2025-05-23T05:53:45+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "please move your tests to `tests/litellm/` as ideally a mock test - so we can run them on future contributor PRs as well ",
        "pr_file_module": null
      },
      {
        "comment_id": "2104414096",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11037,
        "pr_file": "tests/llm_translation/test_cloudflare.py",
        "discussion_id": "2103854111",
        "commented_code": "@@ -40,3 +40,358 @@ async def test_completion_cloudflare(stream):\n \n     except Exception as e:\n         pytest.fail(f\"Error occurred: {e}\")\n+\n+\n+@pytest.mark.asyncio\n+@pytest.mark.parametrize(\"stream\", [True, False])\n+async def test_completion_cloudflare_llama4_scout(stream):",
        "comment_created_at": "2025-05-23T11:41:22+00:00",
        "comment_author": "Ali-Razmjoo",
        "comment_body": "I moved the new tests I created \ud83d\udc4d ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2105392700",
    "pr_number": 11037,
    "pr_file": "tests/litellm/test_cloudflare.py",
    "created_at": "2025-05-23T20:42:55+00:00",
    "commented_code": "+import os\n+import sys\n+import pytest\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+sys.path.insert(\n+    0, os.path.abspath(\"../..\")\n+)  # Adds the parent directory to the system path\n+\n+import litellm\n+\n+# Skip all tests if no Cloudflare API key is available\n+pytestmark = pytest.mark.skipif(\n+    os.getenv(\"CLOUDFLARE_API_KEY\") is None, reason=\"CLOUDFLARE_API_KEY not set\"\n+)",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2105392700",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11037,
        "pr_file": "tests/litellm/test_cloudflare.py",
        "discussion_id": "2105392700",
        "commented_code": "@@ -0,0 +1,370 @@\n+import os\n+import sys\n+import pytest\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+sys.path.insert(\n+    0, os.path.abspath(\"../..\")\n+)  # Adds the parent directory to the system path\n+\n+import litellm\n+\n+# Skip all tests if no Cloudflare API key is available\n+pytestmark = pytest.mark.skipif(\n+    os.getenv(\"CLOUDFLARE_API_KEY\") is None, reason=\"CLOUDFLARE_API_KEY not set\"\n+)",
        "comment_created_at": "2025-05-23T20:42:55+00:00",
        "comment_author": "Ali-Razmjoo",
        "comment_body": "quick FYI I added this newly; the test is/will be dependent on `CLOUDFLARE_API_KEY` is set.",
        "pr_file_module": null
      },
      {
        "comment_id": "2105495216",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11037,
        "pr_file": "tests/litellm/test_cloudflare.py",
        "discussion_id": "2105392700",
        "commented_code": "@@ -0,0 +1,370 @@\n+import os\n+import sys\n+import pytest\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+sys.path.insert(\n+    0, os.path.abspath(\"../..\")\n+)  # Adds the parent directory to the system path\n+\n+import litellm\n+\n+# Skip all tests if no Cloudflare API key is available\n+pytestmark = pytest.mark.skipif(\n+    os.getenv(\"CLOUDFLARE_API_KEY\") is None, reason=\"CLOUDFLARE_API_KEY not set\"\n+)",
        "comment_created_at": "2025-05-23T22:38:35+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "@Ali-Razmjoo `tests/litellm` is all mock tests. Please mock the response if your goal is to add an e2e test here. \r\n\r\nHere's an example - https://github.com/BerriAI/litellm/blob/8c0054a77e862bbd11e9cb5f40063a6e2d0d696c/tests/litellm/test_main.py#L282",
        "pr_file_module": null
      },
      {
        "comment_id": "2105804364",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11037,
        "pr_file": "tests/litellm/test_cloudflare.py",
        "discussion_id": "2105392700",
        "commented_code": "@@ -0,0 +1,370 @@\n+import os\n+import sys\n+import pytest\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+sys.path.insert(\n+    0, os.path.abspath(\"../..\")\n+)  # Adds the parent directory to the system path\n+\n+import litellm\n+\n+# Skip all tests if no Cloudflare API key is available\n+pytestmark = pytest.mark.skipif(\n+    os.getenv(\"CLOUDFLARE_API_KEY\") is None, reason=\"CLOUDFLARE_API_KEY not set\"\n+)",
        "comment_created_at": "2025-05-24T12:33:01+00:00",
        "comment_author": "Ali-Razmjoo",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186028452",
    "pr_number": 12121,
    "pr_file": "litellm/llms/bytez/chat/tests/messages_list.py",
    "created_at": "2025-07-04T19:37:16+00:00",
    "commented_code": "+messages_list = [\n+    [",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2186028452",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12121,
        "pr_file": "litellm/llms/bytez/chat/tests/messages_list.py",
        "discussion_id": "2186028452",
        "commented_code": "@@ -0,0 +1,38 @@\n+messages_list = [\n+    [",
        "comment_created_at": "2025-07-04T19:37:16+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "please add tests to test_litellm/",
        "pr_file_module": null
      },
      {
        "comment_id": "2190595331",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12121,
        "pr_file": "litellm/llms/bytez/chat/tests/messages_list.py",
        "discussion_id": "2186028452",
        "commented_code": "@@ -0,0 +1,38 @@\n+messages_list = [\n+    [",
        "comment_created_at": "2025-07-07T16:35:27+00:00",
        "comment_author": "inf3rnus",
        "comment_body": "@ishaan-jaff Hey Ishaan, thank you so much for looking at this PR!\r\n\r\nI've kept these tests separate from the rest of the tests and have left them in the `llms/bytez` dir because they are manual integration tests that require a Bytez key, tests will break if moved into `test_litellm`.\r\n\r\nIf we just want to leave them, can leave them, if we don't, can do whatever you deem necessary, just lmk :+1: :+1: ",
        "pr_file_module": null
      },
      {
        "comment_id": "2199222614",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12121,
        "pr_file": "litellm/llms/bytez/chat/tests/messages_list.py",
        "discussion_id": "2186028452",
        "commented_code": "@@ -0,0 +1,38 @@\n+messages_list = [\n+    [",
        "comment_created_at": "2025-07-11T01:10:57+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "The current approach breaks our guidelines. Please move them to test_litellm/ and mock the api call / Bytez key requirement. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2103859563",
    "pr_number": 10940,
    "pr_file": "tests/local_testing/test_health_check.py",
    "created_at": "2025-05-23T05:57:18+00:00",
    "commented_code": "assert isinstance(updated_params[\"messages\"], list)\n     assert updated_params[\"model\"] == \"gpt-4\"\n \n+    # Test with health_check_voice for audio_speech mode\n+    model_info = {\"mode\": \"audio_speech\", \"health_check_voice\": \"en-US-JennyNeural\"}",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2103859563",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10940,
        "pr_file": "tests/local_testing/test_health_check.py",
        "discussion_id": "2103859563",
        "commented_code": "@@ -258,6 +259,34 @@ def test_update_litellm_params_for_health_check():\n     assert isinstance(updated_params[\"messages\"], list)\n     assert updated_params[\"model\"] == \"gpt-4\"\n \n+    # Test with health_check_voice for audio_speech mode\n+    model_info = {\"mode\": \"audio_speech\", \"health_check_voice\": \"en-US-JennyNeural\"}",
        "comment_created_at": "2025-05-23T05:57:18+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "please write your tests inside `tests/litellm` so the github action can run and validate they pass",
        "pr_file_module": null
      },
      {
        "comment_id": "2105395523",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10940,
        "pr_file": "tests/local_testing/test_health_check.py",
        "discussion_id": "2103859563",
        "commented_code": "@@ -258,6 +259,34 @@ def test_update_litellm_params_for_health_check():\n     assert isinstance(updated_params[\"messages\"], list)\n     assert updated_params[\"model\"] == \"gpt-4\"\n \n+    # Test with health_check_voice for audio_speech mode\n+    model_info = {\"mode\": \"audio_speech\", \"health_check_voice\": \"en-US-JennyNeural\"}",
        "comment_created_at": "2025-05-23T20:45:52+00:00",
        "comment_author": "dotmobo",
        "comment_body": "I just modified the existing test of the method \"_update_litellm_params_for_health_check\" and it was only located here ...",
        "pr_file_module": null
      },
      {
        "comment_id": "2108558185",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10940,
        "pr_file": "tests/local_testing/test_health_check.py",
        "discussion_id": "2103859563",
        "commented_code": "@@ -258,6 +259,34 @@ def test_update_litellm_params_for_health_check():\n     assert isinstance(updated_params[\"messages\"], list)\n     assert updated_params[\"model\"] == \"gpt-4\"\n \n+    # Test with health_check_voice for audio_speech mode\n+    model_info = {\"mode\": \"audio_speech\", \"health_check_voice\": \"en-US-JennyNeural\"}",
        "comment_created_at": "2025-05-27T08:22:15+00:00",
        "comment_author": "dotmobo",
        "comment_body": "Test in tests/litellm added",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186585781",
    "pr_number": 12078,
    "pr_file": "tests/proxy_unit_tests/test_proxy_config_unit_test.py",
    "created_at": "2025-07-05T04:22:28+00:00",
    "commented_code": "litellm._known_custom_logger_compatible_callbacks = []\n \n \n+def test_s3_config_temp_file_cleanup():",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2186585781",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12078,
        "pr_file": "tests/proxy_unit_tests/test_proxy_config_unit_test.py",
        "discussion_id": "2186585781",
        "commented_code": "@@ -231,6 +233,45 @@ def test_add_callbacks_from_db_config():\n     litellm._known_custom_logger_compatible_callbacks = []\n \n \n+def test_s3_config_temp_file_cleanup():",
        "comment_created_at": "2025-07-05T04:22:28+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "please add a mocked test in test_litellm/ this test looks like it wont run on CI/CD. It's better to add a test that actually tests this change so we can prevent future regressions. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070847565",
    "pr_number": 10385,
    "pr_file": "tests/local_testing/test_completion.py",
    "created_at": "2025-05-01T21:45:47+00:00",
    "commented_code": "# test_completion_openrouter1()\n \n+def test_completion_datarobot():\n+    messages = [\n+        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n+    ]\n+    try:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2070847565",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10385,
        "pr_file": "tests/local_testing/test_completion.py",
        "discussion_id": "2070847565",
        "commented_code": "@@ -2354,6 +2354,25 @@ def test_completion_openrouter_reasoning_effort():\n \n # test_completion_openrouter1()\n \n+def test_completion_datarobot():\n+    messages = [\n+        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n+    ]\n+    try:",
        "comment_created_at": "2025-05-01T21:45:47+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "can you please add mock tests for datarobot \r\n\r\nwe don't have an api key for this provider ",
        "pr_file_module": null
      },
      {
        "comment_id": "2071804292",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10385,
        "pr_file": "tests/local_testing/test_completion.py",
        "discussion_id": "2070847565",
        "commented_code": "@@ -2354,6 +2354,25 @@ def test_completion_openrouter_reasoning_effort():\n \n # test_completion_openrouter1()\n \n+def test_completion_datarobot():\n+    messages = [\n+        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n+    ]\n+    try:",
        "comment_created_at": "2025-05-02T15:42:30+00:00",
        "comment_author": "mjnitz02",
        "comment_body": "I replaced the `client.post` with a mock command so this runs anywhere. I also updated the `DataRobotConfig` unit tests to cover the `OpenAILike` functionality.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2094161601",
    "pr_number": 10596,
    "pr_file": "tests/litellm/llms/featherless_ai/chat/test_featherless_chat_transformation.py",
    "created_at": "2025-05-17T16:30:55+00:00",
    "commented_code": "drop_params=False\n             )\n         assert \"Featherless AI doesn't support tools=\" in str(excinfo.value)\n+\n+    def test_default_api_base(self):\n+        \"\"\"Test that default API base is used when none is provided\"\"\"\n+        config = FeatherlessAIConfig()\n+        headers = {}\n+        api_key = \"fake-featherless-key\"\n+\n+        # Call validate_environment without specifying api_base\n+        result = config.validate_environment(\n+            headers=headers,\n+            model=\"featherless-ai/Qwerky-72B\",\n+            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n+            optional_params={},\n+            litellm_params={},\n+            api_key=api_key,\n+            api_base=None,  # Not providing api_base\n+        )\n+\n+        # Verify headers are still set correctly\n+        assert result[\"Authorization\"] == f\"Bearer {api_key}\"\n+        assert result[\"Content-Type\"] == \"application/json\"\n+        \n+        # We can't directly test the api_base value here since validate_environment\n+        # only returns the headers, but we can verify it doesn't raise an exception\n+        # which would happen if api_base handling was incorrect\n+\n+    def test_featherless_ai_completion_mock(self):\n+        \"\"\"\n+        Mock test for Featherless AI completion using the model format from docs.\n+        This test uses patching to avoid requiring actual API credentials.\n+        \"\"\"\n+        from litellm import completion\n+        from litellm.types.utils import ModelResponse\n+        \n+        # Create a proper ModelResponse object\n+        mock_response = ModelResponse(\n+            id=\"chatcmpl-mock-id\",\n+            object=\"chat.completion\",\n+            created=1699975384,\n+            model=\"featherless-ai/Qwerky-72B\",\n+            choices=[\n+                {\n+                    \"index\": 0,\n+                    \"message\": {\n+                        \"role\": \"assistant\",\n+                        \"content\": \"```python\nprint(\\\"Hi from LiteLLM!\\\")\n```\n\nThis simple Python code prints a greeting message from LiteLLM.\"\n+                    },\n+                    \"finish_reason\": \"stop\"\n+                }\n+            ],\n+            usage={\n+                \"prompt_tokens\": 15,\n+                \"completion_tokens\": 30,\n+                \"total_tokens\": 45\n+            }\n+        )\n+        \n+        # Patch the completion function to return our mock response\n+        with patch(\"litellm.completion\", return_value=mock_response):",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2094161601",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10596,
        "pr_file": "tests/litellm/llms/featherless_ai/chat/test_featherless_chat_transformation.py",
        "discussion_id": "2094161601",
        "commented_code": "@@ -144,3 +144,77 @@ def test_map_openai_params_with_tools(self):\n                 drop_params=False\n             )\n         assert \"Featherless AI doesn't support tools=\" in str(excinfo.value)\n+\n+    def test_default_api_base(self):\n+        \"\"\"Test that default API base is used when none is provided\"\"\"\n+        config = FeatherlessAIConfig()\n+        headers = {}\n+        api_key = \"fake-featherless-key\"\n+\n+        # Call validate_environment without specifying api_base\n+        result = config.validate_environment(\n+            headers=headers,\n+            model=\"featherless-ai/Qwerky-72B\",\n+            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n+            optional_params={},\n+            litellm_params={},\n+            api_key=api_key,\n+            api_base=None,  # Not providing api_base\n+        )\n+\n+        # Verify headers are still set correctly\n+        assert result[\"Authorization\"] == f\"Bearer {api_key}\"\n+        assert result[\"Content-Type\"] == \"application/json\"\n+        \n+        # We can't directly test the api_base value here since validate_environment\n+        # only returns the headers, but we can verify it doesn't raise an exception\n+        # which would happen if api_base handling was incorrect\n+\n+    def test_featherless_ai_completion_mock(self):\n+        \"\"\"\n+        Mock test for Featherless AI completion using the model format from docs.\n+        This test uses patching to avoid requiring actual API credentials.\n+        \"\"\"\n+        from litellm import completion\n+        from litellm.types.utils import ModelResponse\n+        \n+        # Create a proper ModelResponse object\n+        mock_response = ModelResponse(\n+            id=\"chatcmpl-mock-id\",\n+            object=\"chat.completion\",\n+            created=1699975384,\n+            model=\"featherless-ai/Qwerky-72B\",\n+            choices=[\n+                {\n+                    \"index\": 0,\n+                    \"message\": {\n+                        \"role\": \"assistant\",\n+                        \"content\": \"```python\\nprint(\\\"Hi from LiteLLM!\\\")\\n```\\n\\nThis simple Python code prints a greeting message from LiteLLM.\"\n+                    },\n+                    \"finish_reason\": \"stop\"\n+                }\n+            ],\n+            usage={\n+                \"prompt_tokens\": 15,\n+                \"completion_tokens\": 30,\n+                \"total_tokens\": 45\n+            }\n+        )\n+        \n+        # Patch the completion function to return our mock response\n+        with patch(\"litellm.completion\", return_value=mock_response):",
        "comment_created_at": "2025-05-17T16:30:55+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "this is **not** a good mock test. it does not test the integration itself.\r\n\r\nPlease mock the openai call - not the `.completion` call. Eg. - https://github.com/BerriAI/litellm/blob/61d77040572c545956d32cdcf2e19ea6858f525e/tests/litellm/test_main.py#L351",
        "pr_file_module": null
      },
      {
        "comment_id": "2094164105",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10596,
        "pr_file": "tests/litellm/llms/featherless_ai/chat/test_featherless_chat_transformation.py",
        "discussion_id": "2094161601",
        "commented_code": "@@ -144,3 +144,77 @@ def test_map_openai_params_with_tools(self):\n                 drop_params=False\n             )\n         assert \"Featherless AI doesn't support tools=\" in str(excinfo.value)\n+\n+    def test_default_api_base(self):\n+        \"\"\"Test that default API base is used when none is provided\"\"\"\n+        config = FeatherlessAIConfig()\n+        headers = {}\n+        api_key = \"fake-featherless-key\"\n+\n+        # Call validate_environment without specifying api_base\n+        result = config.validate_environment(\n+            headers=headers,\n+            model=\"featherless-ai/Qwerky-72B\",\n+            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n+            optional_params={},\n+            litellm_params={},\n+            api_key=api_key,\n+            api_base=None,  # Not providing api_base\n+        )\n+\n+        # Verify headers are still set correctly\n+        assert result[\"Authorization\"] == f\"Bearer {api_key}\"\n+        assert result[\"Content-Type\"] == \"application/json\"\n+        \n+        # We can't directly test the api_base value here since validate_environment\n+        # only returns the headers, but we can verify it doesn't raise an exception\n+        # which would happen if api_base handling was incorrect\n+\n+    def test_featherless_ai_completion_mock(self):\n+        \"\"\"\n+        Mock test for Featherless AI completion using the model format from docs.\n+        This test uses patching to avoid requiring actual API credentials.\n+        \"\"\"\n+        from litellm import completion\n+        from litellm.types.utils import ModelResponse\n+        \n+        # Create a proper ModelResponse object\n+        mock_response = ModelResponse(\n+            id=\"chatcmpl-mock-id\",\n+            object=\"chat.completion\",\n+            created=1699975384,\n+            model=\"featherless-ai/Qwerky-72B\",\n+            choices=[\n+                {\n+                    \"index\": 0,\n+                    \"message\": {\n+                        \"role\": \"assistant\",\n+                        \"content\": \"```python\\nprint(\\\"Hi from LiteLLM!\\\")\\n```\\n\\nThis simple Python code prints a greeting message from LiteLLM.\"\n+                    },\n+                    \"finish_reason\": \"stop\"\n+                }\n+            ],\n+            usage={\n+                \"prompt_tokens\": 15,\n+                \"completion_tokens\": 30,\n+                \"total_tokens\": 45\n+            }\n+        )\n+        \n+        # Patch the completion function to return our mock response\n+        with patch(\"litellm.completion\", return_value=mock_response):",
        "comment_created_at": "2025-05-17T16:43:52+00:00",
        "comment_author": "DarinVerheijke",
        "comment_body": "apologies, updated!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1938841934",
    "pr_number": 8154,
    "pr_file": "tests/local_testing/test_embedding.py",
    "created_at": "2025-02-03T06:29:34+00:00",
    "commented_code": "except Exception as e:\n         pytest.fail(f\"Error occurred: {e}\")\n \n+@pytest.mark.skip(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1938841934",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8154,
        "pr_file": "tests/local_testing/test_embedding.py",
        "discussion_id": "1938841934",
        "commented_code": "@@ -905,6 +905,20 @@ def test_voyage_embeddings():\n     except Exception as e:\n         pytest.fail(f\"Error occurred: {e}\")\n \n+@pytest.mark.skip(",
        "comment_created_at": "2025-02-03T06:29:34+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "instead of skipping this test, can you make it a mock test - using magicmock? \r\n\r\nthis will at least prevent any regressions from happening\r\n\r\neg. - https://github.com/BerriAI/litellm/blob/e4566d7b1ca0e1a3610349eb249cf16216c3a96f/tests/local_testing/test_embedding.py#L199",
        "pr_file_module": null
      },
      {
        "comment_id": "1950423860",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8154,
        "pr_file": "tests/local_testing/test_embedding.py",
        "discussion_id": "1938841934",
        "commented_code": "@@ -905,6 +905,20 @@ def test_voyage_embeddings():\n     except Exception as e:\n         pytest.fail(f\"Error occurred: {e}\")\n \n+@pytest.mark.skip(",
        "comment_created_at": "2025-02-11T08:32:15+00:00",
        "comment_author": "Aktsvigun",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2046117036",
    "pr_number": 10043,
    "pr_file": "tests/logging_callback_tests/test_arize_logging.py",
    "created_at": "2025-04-16T05:44:25+00:00",
    "commented_code": null,
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2046117036",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10043,
        "pr_file": "tests/logging_callback_tests/test_arize_logging.py",
        "discussion_id": "2046117036",
        "commented_code": null,
        "comment_created_at": "2025-04-16T05:44:25+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "This is the wrong file to add contributor tests. \r\n\r\nPlease add it inside tests/litellm - https://github.com/BerriAI/litellm/tree/main/tests/litellm\r\n\r\nand place it in a corresponding `test_` file to the one you're trying to test",
        "pr_file_module": null
      },
      {
        "comment_id": "2046117654",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10043,
        "pr_file": "tests/logging_callback_tests/test_arize_logging.py",
        "discussion_id": "2046117036",
        "commented_code": null,
        "comment_created_at": "2025-04-16T05:45:08+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "in this case it would be `tests/litellm/integrations/arize/test_arize_utils.py` \r\n\r\nif this file doesn't exist, please make one, with the correct imports ",
        "pr_file_module": null
      },
      {
        "comment_id": "2046235127",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10043,
        "pr_file": "tests/logging_callback_tests/test_arize_logging.py",
        "discussion_id": "2046117036",
        "commented_code": null,
        "comment_created_at": "2025-04-16T07:10:30+00:00",
        "comment_author": "ialisaleh",
        "comment_body": "@krrishdholakia I've moved the contributor test to the correct location as suggested.\r\n\r\n**Updated Location:** `tests/litellm/integrations/arize/test_arize_utils.py`\r\n\r\nJust ran it locally and everything\u2019s working fine. Let me know if there\u2019s anything else to tweak. Thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1994916238",
    "pr_number": 7582,
    "pr_file": "tests/local_testing/test_completion.py",
    "created_at": "2025-03-14T06:10:44+00:00",
    "commented_code": "messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n     )\n \n+def test_completion_novita_ai():",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1994916238",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7582,
        "pr_file": "tests/local_testing/test_completion.py",
        "discussion_id": "1994916238",
        "commented_code": "@@ -4650,6 +4650,46 @@ def test_humanloop_completion(monkeypatch):\n         messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n     )\n \n+def test_completion_novita_ai():",
        "comment_created_at": "2025-03-14T06:10:44+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "this test will fail in prod. please make this a mock test, see `hosted_vllm/` example",
        "pr_file_module": null
      }
    ]
  }
]