[
  {
    "discussion_id": "2299880915",
    "pr_number": 13945,
    "pr_file": "litellm/main.py",
    "created_at": "2025-08-26T06:07:29+00:00",
    "commented_code": "or \"https://api.anthropic.com/v1/messages\"\n             )\n \n-            if api_base is not None and not api_base.endswith(\"/v1/messages\"):\n+            # Check if we should disable automatic URL suffix appending\n+            disable_url_suffix = get_secret_bool(\"LITELLM_ANTHROPIC_DISABLE_URL_SUFFIX\")",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2299880915",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13945,
        "pr_file": "litellm/main.py",
        "discussion_id": "2299880915",
        "commented_code": "@@ -2206,8 +2216,18 @@ def completion(  # type: ignore # noqa: PLR0915\n                 or \"https://api.anthropic.com/v1/messages\"\n             )\n \n-            if api_base is not None and not api_base.endswith(\"/v1/messages\"):\n+            # Check if we should disable automatic URL suffix appending\n+            disable_url_suffix = get_secret_bool(\"LITELLM_ANTHROPIC_DISABLE_URL_SUFFIX\")",
        "comment_created_at": "2025-08-26T06:07:29+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "is there a more generic way we can support this, instead of a one-off header? \r\n\r\n@NoWall57 ",
        "pr_file_module": null
      },
      {
        "comment_id": "2299959313",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13945,
        "pr_file": "litellm/main.py",
        "discussion_id": "2299880915",
        "commented_code": "@@ -2206,8 +2216,18 @@ def completion(  # type: ignore # noqa: PLR0915\n                 or \"https://api.anthropic.com/v1/messages\"\n             )\n \n-            if api_base is not None and not api_base.endswith(\"/v1/messages\"):\n+            # Check if we should disable automatic URL suffix appending\n+            disable_url_suffix = get_secret_bool(\"LITELLM_ANTHROPIC_DISABLE_URL_SUFFIX\")",
        "comment_created_at": "2025-08-26T06:46:39+00:00",
        "comment_author": "NoWall57",
        "comment_body": "> is there a more generic way we can support this, instead of a one-off header?\r\n> \r\n> @NoWall57\r\n\r\nYou\u2019re right, the naming here does look like it\u2019s tailored for a specific scenario. I noticed that only the Anthropic-related logic appends a path, while other LLM providers usually prioritize using the passed-in api_base. Do you have any good suggestions for me?",
        "pr_file_module": null
      },
      {
        "comment_id": "2299990214",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13945,
        "pr_file": "litellm/main.py",
        "discussion_id": "2299880915",
        "commented_code": "@@ -2206,8 +2216,18 @@ def completion(  # type: ignore # noqa: PLR0915\n                 or \"https://api.anthropic.com/v1/messages\"\n             )\n \n-            if api_base is not None and not api_base.endswith(\"/v1/messages\"):\n+            # Check if we should disable automatic URL suffix appending\n+            disable_url_suffix = get_secret_bool(\"LITELLM_ANTHROPIC_DISABLE_URL_SUFFIX\")",
        "comment_created_at": "2025-08-26T07:00:01+00:00",
        "comment_author": "NoWall57",
        "comment_body": "Thank you for your reply!  Honestly, I think there shouldn\u2019t have been a special implementation for Anthropic in the first place, but removing it now would be a breaking change for the code that\u2019s already using this feature. The only compatibility approach I can think of right now is to use one piece of special-case code to handle another special-case code.",
        "pr_file_module": null
      },
      {
        "comment_id": "2300077482",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13945,
        "pr_file": "litellm/main.py",
        "discussion_id": "2299880915",
        "commented_code": "@@ -2206,8 +2216,18 @@ def completion(  # type: ignore # noqa: PLR0915\n                 or \"https://api.anthropic.com/v1/messages\"\n             )\n \n-            if api_base is not None and not api_base.endswith(\"/v1/messages\"):\n+            # Check if we should disable automatic URL suffix appending\n+            disable_url_suffix = get_secret_bool(\"LITELLM_ANTHROPIC_DISABLE_URL_SUFFIX\")",
        "comment_created_at": "2025-08-26T07:33:41+00:00",
        "comment_author": "NoWall57",
        "comment_body": "Or could we just remove this piece of path-appending code? Because if a /v2/messages or /v2/complete is released in the future, this code would still need to be updated.",
        "pr_file_module": null
      },
      {
        "comment_id": "2306215513",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13945,
        "pr_file": "litellm/main.py",
        "discussion_id": "2299880915",
        "commented_code": "@@ -2206,8 +2216,18 @@ def completion(  # type: ignore # noqa: PLR0915\n                 or \"https://api.anthropic.com/v1/messages\"\n             )\n \n-            if api_base is not None and not api_base.endswith(\"/v1/messages\"):\n+            # Check if we should disable automatic URL suffix appending\n+            disable_url_suffix = get_secret_bool(\"LITELLM_ANTHROPIC_DISABLE_URL_SUFFIX\")",
        "comment_created_at": "2025-08-28T05:24:58+00:00",
        "comment_author": "NoWall57",
        "comment_body": "Hi @krrishdholakia , any suggestions?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1995899950",
    "pr_number": 9249,
    "pr_file": "litellm/llms/hosted_vllm/rerank/handler.py",
    "created_at": "2025-03-14T16:38:46+00:00",
    "commented_code": "+\"\"\"\n+Re rank api for Hosted VLLM\n+\n+LiteLLM supports the re rank API format, no parameter transformation occurs\n+\"\"\"\n+\n+from typing import Any, Dict, List, Optional, Union\n+\n+import litellm\n+from litellm.llms.base import BaseLLM\n+from litellm.llms.custom_httpx.http_handler import (\n+    _get_httpx_client,\n+    get_async_httpx_client,\n+)\n+from litellm.llms.hosted_vllm.rerank.transformation import HostedVLLMRerankConfig\n+from litellm.secret_managers.main import get_secret_str\n+from litellm.types.rerank import RerankRequest, RerankResponse\n+\n+\n+class HostedVLLMRerank(BaseLLM):",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1995899950",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9249,
        "pr_file": "litellm/llms/hosted_vllm/rerank/handler.py",
        "discussion_id": "1995899950",
        "commented_code": "@@ -0,0 +1,103 @@\n+\"\"\"\n+Re rank api for Hosted VLLM\n+\n+LiteLLM supports the re rank API format, no parameter transformation occurs\n+\"\"\"\n+\n+from typing import Any, Dict, List, Optional, Union\n+\n+import litellm\n+from litellm.llms.base import BaseLLM\n+from litellm.llms.custom_httpx.http_handler import (\n+    _get_httpx_client,\n+    get_async_httpx_client,\n+)\n+from litellm.llms.hosted_vllm.rerank.transformation import HostedVLLMRerankConfig\n+from litellm.secret_managers.main import get_secret_str\n+from litellm.types.rerank import RerankRequest, RerankResponse\n+\n+\n+class HostedVLLMRerank(BaseLLM):",
        "comment_created_at": "2025-03-14T16:38:46+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "there is no need for this handler. please follow the cohere integration, and use the base_llm_http_handler - https://github.com/BerriAI/litellm/blob/3875df666b8a11819eda86fdc35d582be4bd8db6/litellm/rerank_api/main.py#L183",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1680313248",
    "pr_number": 4629,
    "pr_file": "litellm/llms/azure.py",
    "created_at": "2024-07-17T02:11:16+00:00",
    "commented_code": "status_code=422, message=\"max retries must be an int\"\n                 )\n \n-            # init AzureOpenAI Client\n-            azure_client_params = {\n-                \"api_version\": api_version,\n-                \"azure_endpoint\": api_base,\n-                \"azure_deployment\": model,\n-                \"http_client\": litellm.client_session,\n-                \"max_retries\": max_retries,\n-                \"timeout\": timeout,\n-            }\n-            azure_client_params = select_azure_base_url_or_endpoint(\n-                azure_client_params=azure_client_params\n-            )\n-            if api_key is not None:\n-                azure_client_params[\"api_key\"] = api_key\n-            elif azure_ad_token is not None:\n-                if azure_ad_token.startswith(\"oidc/\"):\n-                    azure_ad_token = get_azure_ad_token_from_oidc(azure_ad_token)\n-                azure_client_params[\"azure_ad_token\"] = azure_ad_token\n+            # Check if it's Cloudflare AI Gateway\n+            if \"gateway.ai.cloudflare.com\" in api_base:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1680313248",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 4629,
        "pr_file": "litellm/llms/azure.py",
        "discussion_id": "1680313248",
        "commented_code": "@@ -989,24 +989,55 @@ def embedding(\n                     status_code=422, message=\"max retries must be an int\"\n                 )\n \n-            # init AzureOpenAI Client\n-            azure_client_params = {\n-                \"api_version\": api_version,\n-                \"azure_endpoint\": api_base,\n-                \"azure_deployment\": model,\n-                \"http_client\": litellm.client_session,\n-                \"max_retries\": max_retries,\n-                \"timeout\": timeout,\n-            }\n-            azure_client_params = select_azure_base_url_or_endpoint(\n-                azure_client_params=azure_client_params\n-            )\n-            if api_key is not None:\n-                azure_client_params[\"api_key\"] = api_key\n-            elif azure_ad_token is not None:\n-                if azure_ad_token.startswith(\"oidc/\"):\n-                    azure_ad_token = get_azure_ad_token_from_oidc(azure_ad_token)\n-                azure_client_params[\"azure_ad_token\"] = azure_ad_token\n+            # Check if it's Cloudflare AI Gateway\n+            if \"gateway.ai.cloudflare.com\" in api_base:",
        "comment_created_at": "2024-07-17T02:11:16+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "hey @Manouchehri can we move away from hardcoded api bases - e.g. gateway.ai.cloudflare.com? \r\n\r\na more generic implementation where api_base is set and used, would help simplify the code here ",
        "pr_file_module": null
      },
      {
        "comment_id": "1680313791",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 4629,
        "pr_file": "litellm/llms/azure.py",
        "discussion_id": "1680313248",
        "commented_code": "@@ -989,24 +989,55 @@ def embedding(\n                     status_code=422, message=\"max retries must be an int\"\n                 )\n \n-            # init AzureOpenAI Client\n-            azure_client_params = {\n-                \"api_version\": api_version,\n-                \"azure_endpoint\": api_base,\n-                \"azure_deployment\": model,\n-                \"http_client\": litellm.client_session,\n-                \"max_retries\": max_retries,\n-                \"timeout\": timeout,\n-            }\n-            azure_client_params = select_azure_base_url_or_endpoint(\n-                azure_client_params=azure_client_params\n-            )\n-            if api_key is not None:\n-                azure_client_params[\"api_key\"] = api_key\n-            elif azure_ad_token is not None:\n-                if azure_ad_token.startswith(\"oidc/\"):\n-                    azure_ad_token = get_azure_ad_token_from_oidc(azure_ad_token)\n-                azure_client_params[\"azure_ad_token\"] = azure_ad_token\n+            # Check if it's Cloudflare AI Gateway\n+            if \"gateway.ai.cloudflare.com\" in api_base:",
        "comment_created_at": "2024-07-17T02:12:36+00:00",
        "comment_author": "Manouchehri",
        "comment_body": "This is just copied from the other implementation that's in the same file. Trying to make the PRs as small as possible.",
        "pr_file_module": null
      },
      {
        "comment_id": "1680315439",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 4629,
        "pr_file": "litellm/llms/azure.py",
        "discussion_id": "1680313248",
        "commented_code": "@@ -989,24 +989,55 @@ def embedding(\n                     status_code=422, message=\"max retries must be an int\"\n                 )\n \n-            # init AzureOpenAI Client\n-            azure_client_params = {\n-                \"api_version\": api_version,\n-                \"azure_endpoint\": api_base,\n-                \"azure_deployment\": model,\n-                \"http_client\": litellm.client_session,\n-                \"max_retries\": max_retries,\n-                \"timeout\": timeout,\n-            }\n-            azure_client_params = select_azure_base_url_or_endpoint(\n-                azure_client_params=azure_client_params\n-            )\n-            if api_key is not None:\n-                azure_client_params[\"api_key\"] = api_key\n-            elif azure_ad_token is not None:\n-                if azure_ad_token.startswith(\"oidc/\"):\n-                    azure_ad_token = get_azure_ad_token_from_oidc(azure_ad_token)\n-                azure_client_params[\"azure_ad_token\"] = azure_ad_token\n+            # Check if it's Cloudflare AI Gateway\n+            if \"gateway.ai.cloudflare.com\" in api_base:",
        "comment_created_at": "2024-07-17T02:15:56+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "i understand, but can we move this to just a simple check if api_base is set, if so -> set the base_url\r\n\r\nif not, don't? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2036132052",
    "pr_number": 9832,
    "pr_file": "tests/llm_translation/test_cohere_v2.py",
    "created_at": "2025-04-09T20:57:06+00:00",
    "commented_code": "+import os\n+import sys\n+import traceback\n+\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+# For testing, make sure the COHERE_API_KEY or CO_API_KEY environment variable is set\n+# You can set it before running the tests with: export COHERE_API_KEY=your_api_key\n+import io\n+import os\n+\n+sys.path.insert(\n+    0, os.path.abspath(\"../..\")\n+)  # Adds the parent directory to the system path\n+import json\n+\n+import pytest\n+\n+import litellm\n+from litellm import RateLimitError, Timeout, completion, completion_cost, embedding\n+from unittest.mock import AsyncMock, patch\n+from litellm import RateLimitError, Timeout, completion, completion_cost, embedding\n+from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler\n+\n+litellm.num_retries = 3\n+\n+\n+@pytest.mark.parametrize(\"stream\", [True, False])\n+@pytest.mark.flaky(retries=3, delay=1)\n+@pytest.mark.asyncio\n+async def test_chat_completion_cohere_v2_citations(stream):\n+    try:\n+        class MockResponse:\n+            def __init__(self, status_code, json_data, is_stream=False):\n+                self.status_code = status_code\n+                self._json_data = json_data\n+                self.headers = {}\n+                self.is_stream = is_stream\n+                \n+                # For streaming responses with citations\n+                if is_stream:\n+                    # Create streaming chunks with citations at the end\n+                    self._iter_content_chunks = [\n+                        json.dumps({\"text\": \"Emperor\"}).encode(),\n+                        json.dumps({\"text\": \" penguins\"}).encode(),\n+                        json.dumps({\"text\": \" are\"}).encode(),\n+                        json.dumps({\"text\": \" the\"}).encode(),\n+                        json.dumps({\"text\": \" tallest\"}).encode(),\n+                        json.dumps({\"text\": \" and\"}).encode(),\n+                        json.dumps({\"text\": \" they\"}).encode(),\n+                        json.dumps({\"text\": \" live\"}).encode(),\n+                        json.dumps({\"text\": \" in\"}).encode(),\n+                        json.dumps({\"text\": \" Antarctica\"}).encode(),\n+                        json.dumps({\"text\": \".\"}).encode(),\n+                        # Citations in a separate chunk\n+                        json.dumps({\"citations\": [\n+                            {\n+                                \"start\": 0,\n+                                \"end\": 30,\n+                                \"text\": \"Emperor penguins are the tallest\",\n+                                \"document_ids\": [\"doc1\"]\n+                            },\n+                            {\n+                                \"start\": 31,\n+                                \"end\": 70,\n+                                \"text\": \"they live in Antarctica\",\n+                                \"document_ids\": [\"doc2\"]\n+                            }\n+                        ]}).encode(),\n+                        json.dumps({\"finish_reason\": \"COMPLETE\"}).encode(),\n+                    ]\n+\n+            def json(self):\n+                return self._json_data\n+\n+            @property\n+            def text(self):\n+                return json.dumps(self._json_data)\n+                \n+            def iter_lines(self):\n+                if self.is_stream:\n+                    for chunk in self._iter_content_chunks:\n+                        yield chunk\n+                else:\n+                    yield json.dumps(self._json_data).encode()\n+                    \n+            async def aiter_lines(self):\n+                if self.is_stream:\n+                    for chunk in self._iter_content_chunks:\n+                        yield chunk\n+                else:\n+                    yield json.dumps(self._json_data).encode()\n+                \n+        async def mock_async_post(*args, **kwargs):\n+            # For asynchronous HTTP client\n+            data = kwargs.get(\"data\", \"{}\")\n+            request_body = json.loads(data)\n+            print(\"Async Request body:\", request_body)\n+            \n+            # Verify the messages are formatted correctly for v2\n+            messages = request_body.get(\"messages\", [])\n+            assert len(messages) > 0\n+            assert \"role\" in messages[0]\n+            assert \"content\" in messages[0]\n+            \n+            # Check if documents are included\n+            documents = request_body.get(\"documents\", [])\n+            assert len(documents) > 0\n+            \n+            # Mock response with citations\n+            mock_response = {\n+                \"text\": \"Emperor penguins are the tallest penguins and they live in Antarctica.\",\n+                \"generation_id\": \"mock-id\",\n+                \"id\": \"mock-completion\",\n+                \"usage\": {\"input_tokens\": 10, \"output_tokens\": 20},\n+                \"citations\": [\n+                    {\n+                        \"start\": 0,\n+                        \"end\": 30,\n+                        \"text\": \"Emperor penguins are the tallest\",\n+                        \"document_ids\": [\"doc1\"]\n+                    },\n+                    {\n+                        \"start\": 31,\n+                        \"end\": 70,\n+                        \"text\": \"they live in Antarctica\",\n+                        \"document_ids\": [\"doc2\"]\n+                    }\n+                ]\n+            }\n+            \n+            # Create a streaming response with citations\n+            if stream:\n+                return MockResponse(\n+                    200,\n+                    {\n+                        \"text\": \"Emperor penguins are the tallest penguins and they live in Antarctica.\",\n+                        \"generation_id\": \"mock-id\",\n+                        \"id\": \"mock-completion\",\n+                        \"usage\": {\"input_tokens\": 10, \"output_tokens\": 20},\n+                        \"citations\": [\n+                            {\n+                                \"start\": 0,\n+                                \"end\": 30,\n+                                \"text\": \"Emperor penguins are the tallest\",\n+                                \"document_ids\": [\"doc1\"]\n+                            },\n+                            {\n+                                \"start\": 31,\n+                                \"end\": 70,\n+                                \"text\": \"they live in Antarctica\",\n+                                \"document_ids\": [\"doc2\"]\n+                            }\n+                        ],\n+                        \"stream\": True\n+                    },\n+                    is_stream=True\n+                )\n+            else:\n+                return MockResponse(200, mock_response)\n+            \n+        # Mock the async HTTP client\n+        with patch(\"litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler.post\", new_callable=AsyncMock, side_effect=mock_async_post):\n+            litellm.set_verbose = True\n+            messages = [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": \"Which penguins are the tallest?\",\n+                },\n+            ]\n+            response = await litellm.acompletion(\n+                model=\"cohere_chat_v2/command-r\",",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2036132052",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9832,
        "pr_file": "tests/llm_translation/test_cohere_v2.py",
        "discussion_id": "2036132052",
        "commented_code": "@@ -0,0 +1,999 @@\n+import os\n+import sys\n+import traceback\n+\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+# For testing, make sure the COHERE_API_KEY or CO_API_KEY environment variable is set\n+# You can set it before running the tests with: export COHERE_API_KEY=your_api_key\n+import io\n+import os\n+\n+sys.path.insert(\n+    0, os.path.abspath(\"../..\")\n+)  # Adds the parent directory to the system path\n+import json\n+\n+import pytest\n+\n+import litellm\n+from litellm import RateLimitError, Timeout, completion, completion_cost, embedding\n+from unittest.mock import AsyncMock, patch\n+from litellm import RateLimitError, Timeout, completion, completion_cost, embedding\n+from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler\n+\n+litellm.num_retries = 3\n+\n+\n+@pytest.mark.parametrize(\"stream\", [True, False])\n+@pytest.mark.flaky(retries=3, delay=1)\n+@pytest.mark.asyncio\n+async def test_chat_completion_cohere_v2_citations(stream):\n+    try:\n+        class MockResponse:\n+            def __init__(self, status_code, json_data, is_stream=False):\n+                self.status_code = status_code\n+                self._json_data = json_data\n+                self.headers = {}\n+                self.is_stream = is_stream\n+                \n+                # For streaming responses with citations\n+                if is_stream:\n+                    # Create streaming chunks with citations at the end\n+                    self._iter_content_chunks = [\n+                        json.dumps({\"text\": \"Emperor\"}).encode(),\n+                        json.dumps({\"text\": \" penguins\"}).encode(),\n+                        json.dumps({\"text\": \" are\"}).encode(),\n+                        json.dumps({\"text\": \" the\"}).encode(),\n+                        json.dumps({\"text\": \" tallest\"}).encode(),\n+                        json.dumps({\"text\": \" and\"}).encode(),\n+                        json.dumps({\"text\": \" they\"}).encode(),\n+                        json.dumps({\"text\": \" live\"}).encode(),\n+                        json.dumps({\"text\": \" in\"}).encode(),\n+                        json.dumps({\"text\": \" Antarctica\"}).encode(),\n+                        json.dumps({\"text\": \".\"}).encode(),\n+                        # Citations in a separate chunk\n+                        json.dumps({\"citations\": [\n+                            {\n+                                \"start\": 0,\n+                                \"end\": 30,\n+                                \"text\": \"Emperor penguins are the tallest\",\n+                                \"document_ids\": [\"doc1\"]\n+                            },\n+                            {\n+                                \"start\": 31,\n+                                \"end\": 70,\n+                                \"text\": \"they live in Antarctica\",\n+                                \"document_ids\": [\"doc2\"]\n+                            }\n+                        ]}).encode(),\n+                        json.dumps({\"finish_reason\": \"COMPLETE\"}).encode(),\n+                    ]\n+\n+            def json(self):\n+                return self._json_data\n+\n+            @property\n+            def text(self):\n+                return json.dumps(self._json_data)\n+                \n+            def iter_lines(self):\n+                if self.is_stream:\n+                    for chunk in self._iter_content_chunks:\n+                        yield chunk\n+                else:\n+                    yield json.dumps(self._json_data).encode()\n+                    \n+            async def aiter_lines(self):\n+                if self.is_stream:\n+                    for chunk in self._iter_content_chunks:\n+                        yield chunk\n+                else:\n+                    yield json.dumps(self._json_data).encode()\n+                \n+        async def mock_async_post(*args, **kwargs):\n+            # For asynchronous HTTP client\n+            data = kwargs.get(\"data\", \"{}\")\n+            request_body = json.loads(data)\n+            print(\"Async Request body:\", request_body)\n+            \n+            # Verify the messages are formatted correctly for v2\n+            messages = request_body.get(\"messages\", [])\n+            assert len(messages) > 0\n+            assert \"role\" in messages[0]\n+            assert \"content\" in messages[0]\n+            \n+            # Check if documents are included\n+            documents = request_body.get(\"documents\", [])\n+            assert len(documents) > 0\n+            \n+            # Mock response with citations\n+            mock_response = {\n+                \"text\": \"Emperor penguins are the tallest penguins and they live in Antarctica.\",\n+                \"generation_id\": \"mock-id\",\n+                \"id\": \"mock-completion\",\n+                \"usage\": {\"input_tokens\": 10, \"output_tokens\": 20},\n+                \"citations\": [\n+                    {\n+                        \"start\": 0,\n+                        \"end\": 30,\n+                        \"text\": \"Emperor penguins are the tallest\",\n+                        \"document_ids\": [\"doc1\"]\n+                    },\n+                    {\n+                        \"start\": 31,\n+                        \"end\": 70,\n+                        \"text\": \"they live in Antarctica\",\n+                        \"document_ids\": [\"doc2\"]\n+                    }\n+                ]\n+            }\n+            \n+            # Create a streaming response with citations\n+            if stream:\n+                return MockResponse(\n+                    200,\n+                    {\n+                        \"text\": \"Emperor penguins are the tallest penguins and they live in Antarctica.\",\n+                        \"generation_id\": \"mock-id\",\n+                        \"id\": \"mock-completion\",\n+                        \"usage\": {\"input_tokens\": 10, \"output_tokens\": 20},\n+                        \"citations\": [\n+                            {\n+                                \"start\": 0,\n+                                \"end\": 30,\n+                                \"text\": \"Emperor penguins are the tallest\",\n+                                \"document_ids\": [\"doc1\"]\n+                            },\n+                            {\n+                                \"start\": 31,\n+                                \"end\": 70,\n+                                \"text\": \"they live in Antarctica\",\n+                                \"document_ids\": [\"doc2\"]\n+                            }\n+                        ],\n+                        \"stream\": True\n+                    },\n+                    is_stream=True\n+                )\n+            else:\n+                return MockResponse(200, mock_response)\n+            \n+        # Mock the async HTTP client\n+        with patch(\"litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler.post\", new_callable=AsyncMock, side_effect=mock_async_post):\n+            litellm.set_verbose = True\n+            messages = [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": \"Which penguins are the tallest?\",\n+                },\n+            ]\n+            response = await litellm.acompletion(\n+                model=\"cohere_chat_v2/command-r\",",
        "comment_created_at": "2025-04-09T20:57:06+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "since v2 might be used for other cohere endpoints too - e.g. embeddings, can we do a more generic `cohere_v2/` provider instead\r\n\r\nthis will make it reusable in different places ",
        "pr_file_module": null
      },
      {
        "comment_id": "2036216167",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9832,
        "pr_file": "tests/llm_translation/test_cohere_v2.py",
        "discussion_id": "2036132052",
        "commented_code": "@@ -0,0 +1,999 @@\n+import os\n+import sys\n+import traceback\n+\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+# For testing, make sure the COHERE_API_KEY or CO_API_KEY environment variable is set\n+# You can set it before running the tests with: export COHERE_API_KEY=your_api_key\n+import io\n+import os\n+\n+sys.path.insert(\n+    0, os.path.abspath(\"../..\")\n+)  # Adds the parent directory to the system path\n+import json\n+\n+import pytest\n+\n+import litellm\n+from litellm import RateLimitError, Timeout, completion, completion_cost, embedding\n+from unittest.mock import AsyncMock, patch\n+from litellm import RateLimitError, Timeout, completion, completion_cost, embedding\n+from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler\n+\n+litellm.num_retries = 3\n+\n+\n+@pytest.mark.parametrize(\"stream\", [True, False])\n+@pytest.mark.flaky(retries=3, delay=1)\n+@pytest.mark.asyncio\n+async def test_chat_completion_cohere_v2_citations(stream):\n+    try:\n+        class MockResponse:\n+            def __init__(self, status_code, json_data, is_stream=False):\n+                self.status_code = status_code\n+                self._json_data = json_data\n+                self.headers = {}\n+                self.is_stream = is_stream\n+                \n+                # For streaming responses with citations\n+                if is_stream:\n+                    # Create streaming chunks with citations at the end\n+                    self._iter_content_chunks = [\n+                        json.dumps({\"text\": \"Emperor\"}).encode(),\n+                        json.dumps({\"text\": \" penguins\"}).encode(),\n+                        json.dumps({\"text\": \" are\"}).encode(),\n+                        json.dumps({\"text\": \" the\"}).encode(),\n+                        json.dumps({\"text\": \" tallest\"}).encode(),\n+                        json.dumps({\"text\": \" and\"}).encode(),\n+                        json.dumps({\"text\": \" they\"}).encode(),\n+                        json.dumps({\"text\": \" live\"}).encode(),\n+                        json.dumps({\"text\": \" in\"}).encode(),\n+                        json.dumps({\"text\": \" Antarctica\"}).encode(),\n+                        json.dumps({\"text\": \".\"}).encode(),\n+                        # Citations in a separate chunk\n+                        json.dumps({\"citations\": [\n+                            {\n+                                \"start\": 0,\n+                                \"end\": 30,\n+                                \"text\": \"Emperor penguins are the tallest\",\n+                                \"document_ids\": [\"doc1\"]\n+                            },\n+                            {\n+                                \"start\": 31,\n+                                \"end\": 70,\n+                                \"text\": \"they live in Antarctica\",\n+                                \"document_ids\": [\"doc2\"]\n+                            }\n+                        ]}).encode(),\n+                        json.dumps({\"finish_reason\": \"COMPLETE\"}).encode(),\n+                    ]\n+\n+            def json(self):\n+                return self._json_data\n+\n+            @property\n+            def text(self):\n+                return json.dumps(self._json_data)\n+                \n+            def iter_lines(self):\n+                if self.is_stream:\n+                    for chunk in self._iter_content_chunks:\n+                        yield chunk\n+                else:\n+                    yield json.dumps(self._json_data).encode()\n+                    \n+            async def aiter_lines(self):\n+                if self.is_stream:\n+                    for chunk in self._iter_content_chunks:\n+                        yield chunk\n+                else:\n+                    yield json.dumps(self._json_data).encode()\n+                \n+        async def mock_async_post(*args, **kwargs):\n+            # For asynchronous HTTP client\n+            data = kwargs.get(\"data\", \"{}\")\n+            request_body = json.loads(data)\n+            print(\"Async Request body:\", request_body)\n+            \n+            # Verify the messages are formatted correctly for v2\n+            messages = request_body.get(\"messages\", [])\n+            assert len(messages) > 0\n+            assert \"role\" in messages[0]\n+            assert \"content\" in messages[0]\n+            \n+            # Check if documents are included\n+            documents = request_body.get(\"documents\", [])\n+            assert len(documents) > 0\n+            \n+            # Mock response with citations\n+            mock_response = {\n+                \"text\": \"Emperor penguins are the tallest penguins and they live in Antarctica.\",\n+                \"generation_id\": \"mock-id\",\n+                \"id\": \"mock-completion\",\n+                \"usage\": {\"input_tokens\": 10, \"output_tokens\": 20},\n+                \"citations\": [\n+                    {\n+                        \"start\": 0,\n+                        \"end\": 30,\n+                        \"text\": \"Emperor penguins are the tallest\",\n+                        \"document_ids\": [\"doc1\"]\n+                    },\n+                    {\n+                        \"start\": 31,\n+                        \"end\": 70,\n+                        \"text\": \"they live in Antarctica\",\n+                        \"document_ids\": [\"doc2\"]\n+                    }\n+                ]\n+            }\n+            \n+            # Create a streaming response with citations\n+            if stream:\n+                return MockResponse(\n+                    200,\n+                    {\n+                        \"text\": \"Emperor penguins are the tallest penguins and they live in Antarctica.\",\n+                        \"generation_id\": \"mock-id\",\n+                        \"id\": \"mock-completion\",\n+                        \"usage\": {\"input_tokens\": 10, \"output_tokens\": 20},\n+                        \"citations\": [\n+                            {\n+                                \"start\": 0,\n+                                \"end\": 30,\n+                                \"text\": \"Emperor penguins are the tallest\",\n+                                \"document_ids\": [\"doc1\"]\n+                            },\n+                            {\n+                                \"start\": 31,\n+                                \"end\": 70,\n+                                \"text\": \"they live in Antarctica\",\n+                                \"document_ids\": [\"doc2\"]\n+                            }\n+                        ],\n+                        \"stream\": True\n+                    },\n+                    is_stream=True\n+                )\n+            else:\n+                return MockResponse(200, mock_response)\n+            \n+        # Mock the async HTTP client\n+        with patch(\"litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler.post\", new_callable=AsyncMock, side_effect=mock_async_post):\n+            litellm.set_verbose = True\n+            messages = [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": \"Which penguins are the tallest?\",\n+                },\n+            ]\n+            response = await litellm.acompletion(\n+                model=\"cohere_chat_v2/command-r\",",
        "comment_created_at": "2025-04-09T22:23:00+00:00",
        "comment_author": "sajdakabir",
        "comment_body": "okay",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1994913280",
    "pr_number": 7582,
    "pr_file": "litellm/main.py",
    "created_at": "2025-03-14T06:08:56+00:00",
    "commented_code": ")\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"novita\":",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1994913280",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7582,
        "pr_file": "litellm/main.py",
        "discussion_id": "1994913280",
        "commented_code": "@@ -2973,6 +2973,49 @@ def completion(  # type: ignore # noqa: PLR0915\n                 )\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"novita\":",
        "comment_created_at": "2025-03-14T06:08:56+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "there is no need for this if block, just register as `litellm.openai_compatible_providers` in init\r\n\r\nand it will route it into the openai block ",
        "pr_file_module": null
      },
      {
        "comment_id": "1995078317",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7582,
        "pr_file": "litellm/main.py",
        "discussion_id": "1994913280",
        "commented_code": "@@ -2973,6 +2973,49 @@ def completion(  # type: ignore # noqa: PLR0915\n                 )\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"novita\":",
        "comment_created_at": "2025-03-14T08:18:23+00:00",
        "comment_author": "jasonhp",
        "comment_body": "We need to add a custom header in this if block. I'm not sure where else to add it.",
        "pr_file_module": null
      },
      {
        "comment_id": "1996002299",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7582,
        "pr_file": "litellm/main.py",
        "discussion_id": "1994913280",
        "commented_code": "@@ -2973,6 +2973,49 @@ def completion(  # type: ignore # noqa: PLR0915\n                 )\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"novita\":",
        "comment_created_at": "2025-03-14T17:36:09+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "you should add it in the validate_environment in the config, that's where headers can be set. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1996004437",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7582,
        "pr_file": "litellm/main.py",
        "discussion_id": "1994913280",
        "commented_code": "@@ -2973,6 +2973,49 @@ def completion(  # type: ignore # noqa: PLR0915\n                 )\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"novita\":",
        "comment_created_at": "2025-03-14T17:37:50+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "in this case, it would be better to have it use base_llm_http_handler.py \r\n\r\nhttps://github.com/BerriAI/litellm/blob/4a6dcd64ff1a6151d7e5fec022b3905265791fcd/litellm/main.py#L2286\r\n\r\nThis is where we are moving all llm calls to - this way all your custom logic exists only within your transformation.py file ",
        "pr_file_module": null
      },
      {
        "comment_id": "1997908517",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7582,
        "pr_file": "litellm/main.py",
        "discussion_id": "1994913280",
        "commented_code": "@@ -2973,6 +2973,49 @@ def completion(  # type: ignore # noqa: PLR0915\n                 )\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"novita\":",
        "comment_created_at": "2025-03-17T04:16:45+00:00",
        "comment_author": "jasonhp",
        "comment_body": "Fixed",
        "pr_file_module": null
      }
    ]
  }
]