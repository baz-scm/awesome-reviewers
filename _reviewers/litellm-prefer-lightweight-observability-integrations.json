[
  {
    "discussion_id": "2178793210",
    "pr_number": 12171,
    "pr_file": "docs/my-website/docs/observability/laminar_integration.md",
    "created_at": "2025-07-02T01:12:32+00:00",
    "commented_code": null,
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2178793210",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12171,
        "pr_file": "docs/my-website/docs/observability/laminar_integration.md",
        "discussion_id": "2178793210",
        "commented_code": null,
        "comment_created_at": "2025-07-02T01:12:32+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "why does a user need to pip install your sdk to send you logs? \r\n\r\nIsn't it OTEL compatible? if so - a simple callback should work too? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2179957384",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12171,
        "pr_file": "docs/my-website/docs/observability/laminar_integration.md",
        "discussion_id": "2178793210",
        "commented_code": null,
        "comment_created_at": "2025-07-02T12:43:29+00:00",
        "comment_author": "dinmukhamedm",
        "comment_body": "@krrishdholakia Laminar is indeed OTel-compatible, but so are many other integrations in the folder (e.g. Arize AI, Braintrust).\r\n\r\nThe raw OTel callback didn't fit our needs completely, mostly in terms of the attributes standards and shapes. In addition, it has a small issue that has not been addressed for quite a while: https://github.com/BerriAI/litellm/pull/8169\r\n\r\nFrom Laminar SDK, we simply import `LaminarLiteLLMCallback` ([link](https://github.com/lmnr-ai/lmnr-python/blob/main/src/lmnr/opentelemetry_lib/litellm/__init__.py#L24)), which just implements `CustomBatchLogger`. Again, this is similar to many other observability integrations.",
        "pr_file_module": null
      },
      {
        "comment_id": "2197730467",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12171,
        "pr_file": "docs/my-website/docs/observability/laminar_integration.md",
        "discussion_id": "2178793210",
        "commented_code": null,
        "comment_created_at": "2025-07-10T13:24:32+00:00",
        "comment_author": "skull8888888",
        "comment_body": "hey @krrishdholakia, just pinging about it. We made really good LiteLLM integration and want to show it to the world!",
        "pr_file_module": null
      },
      {
        "comment_id": "2197732628",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12171,
        "pr_file": "docs/my-website/docs/observability/laminar_integration.md",
        "discussion_id": "2178793210",
        "commented_code": null,
        "comment_created_at": "2025-07-10T13:25:32+00:00",
        "comment_author": "skull8888888",
        "comment_body": "our own callback also help us better isolate from other otel ingestors (specifically datadog) which tend to really affect the trace context",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1933035267",
    "pr_number": 8052,
    "pr_file": "docs/my-website/docs/observability/weave_integration.md",
    "created_at": "2025-01-29T00:19:35+00:00",
    "commented_code": "+import Image from '@theme/IdealImage';\n+\n+# Weights & Biases Weave - Tracing and Evaluation\n+\n+## What is W&B Weave?\n+\n+Weights and Biases (W&B) Weave is a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications. Designed for flexibility and scalability, Weave supports every stage of your LLM application development workflow.\n+\n+W&B Weave's integration with LiteLLM enables you to trace, version control and debug your LLM applications. It enables you to easily evaluate your AI systems with the flexibility of LiteLLM.\n+\n+Get started with just 2 lines of code and track your LiteLLM calls with W&B Weave. Learn more about W&B Weave [here](https://weave-docs.wandb.ai).\n+\n+<Image img={require('../../img/weave_litellm.png')} />\n+\n+## Quick Start\n+\n+Install W&B Weave\n+```shell\n+pip install weave",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1933035267",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8052,
        "pr_file": "docs/my-website/docs/observability/weave_integration.md",
        "discussion_id": "1933035267",
        "commented_code": "@@ -0,0 +1,171 @@\n+import Image from '@theme/IdealImage';\n+\n+# Weights & Biases Weave - Tracing and Evaluation\n+\n+## What is W&B Weave?\n+\n+Weights and Biases (W&B) Weave is a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications. Designed for flexibility and scalability, Weave supports every stage of your LLM application development workflow.\n+\n+W&B Weave's integration with LiteLLM enables you to trace, version control and debug your LLM applications. It enables you to easily evaluate your AI systems with the flexibility of LiteLLM.\n+\n+Get started with just 2 lines of code and track your LiteLLM calls with W&B Weave. Learn more about W&B Weave [here](https://weave-docs.wandb.ai).\n+\n+<Image img={require('../../img/weave_litellm.png')} />\n+\n+## Quick Start\n+\n+Install W&B Weave\n+```shell\n+pip install weave",
        "comment_created_at": "2025-01-29T00:19:35+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "@ayulockin i'm confused - why is the `weave` sdk required here? \r\n\r\nIf this is patching litellm then that would be **bad**, as it can cause unexpected errors. \r\n\r\nIt would be **preferred** if weave was written as a customlogger, and **did not** have an sdk requirement (e.g. using pure httpx like langsmith - https://github.com/BerriAI/litellm/blob/9644e197f760c5cb87ae7662dbedbbce1b04c6a8/litellm/integrations/langsmith.py)",
        "pr_file_module": null
      },
      {
        "comment_id": "1933317909",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8052,
        "pr_file": "docs/my-website/docs/observability/weave_integration.md",
        "discussion_id": "1933035267",
        "commented_code": "@@ -0,0 +1,171 @@\n+import Image from '@theme/IdealImage';\n+\n+# Weights & Biases Weave - Tracing and Evaluation\n+\n+## What is W&B Weave?\n+\n+Weights and Biases (W&B) Weave is a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications. Designed for flexibility and scalability, Weave supports every stage of your LLM application development workflow.\n+\n+W&B Weave's integration with LiteLLM enables you to trace, version control and debug your LLM applications. It enables you to easily evaluate your AI systems with the flexibility of LiteLLM.\n+\n+Get started with just 2 lines of code and track your LiteLLM calls with W&B Weave. Learn more about W&B Weave [here](https://weave-docs.wandb.ai).\n+\n+<Image img={require('../../img/weave_litellm.png')} />\n+\n+## Quick Start\n+\n+Install W&B Weave\n+```shell\n+pip install weave",
        "comment_created_at": "2025-01-29T06:16:34+00:00",
        "comment_author": "ayulockin",
        "comment_body": "Hey @krrishdholakia, `weave` is required because we are patching `litellm`. It happens when we do `weave.init()`. \r\n\r\nIt's a light patching and is basically decorating the `completion` and `acompletion` functions with ` weave.op()` ([docs](https://weave-docs.wandb.ai/guides/tracking/ops)). \r\n\r\nWe would love to write a custom logger but for now this patching based integration lives in our `weave` repo and this PR is meant to update the outdated product feature. We no longer support \"Traces\" product which the [current documentation is showing](https://docs.litellm.ai/docs/observability/wandb_integration) and cause confusion for users.\r\n\r\nWe also have a documentation on our site here if you wanna take a look: https://weave-docs.wandb.ai/guides/integrations/litellm and this PR aligns the present capability on both docs.\r\n\r\n> can you point me to how weave works with litellm? Code would be helpful here\r\n\r\nI am sharing the line where the `fn` is decorated with `weave.op()` but the whole integration is living in this single file.\r\n\r\nhttps://github.com/wandb/weave/blob/9ff97cab61e62b84ab7f1760d7968b6e61ef4b87/weave/integrations/litellm/litellm.py#L93 ",
        "pr_file_module": null
      },
      {
        "comment_id": "1935546479",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8052,
        "pr_file": "docs/my-website/docs/observability/weave_integration.md",
        "discussion_id": "1933035267",
        "commented_code": "@@ -0,0 +1,171 @@\n+import Image from '@theme/IdealImage';\n+\n+# Weights & Biases Weave - Tracing and Evaluation\n+\n+## What is W&B Weave?\n+\n+Weights and Biases (W&B) Weave is a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications. Designed for flexibility and scalability, Weave supports every stage of your LLM application development workflow.\n+\n+W&B Weave's integration with LiteLLM enables you to trace, version control and debug your LLM applications. It enables you to easily evaluate your AI systems with the flexibility of LiteLLM.\n+\n+Get started with just 2 lines of code and track your LiteLLM calls with W&B Weave. Learn more about W&B Weave [here](https://weave-docs.wandb.ai).\n+\n+<Image img={require('../../img/weave_litellm.png')} />\n+\n+## Quick Start\n+\n+Install W&B Weave\n+```shell\n+pip install weave",
        "comment_created_at": "2025-01-30T12:41:16+00:00",
        "comment_author": "ayulockin",
        "comment_body": "Hey @krrishdholakia would love to know your thoughts :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2014974737",
    "pr_number": 9561,
    "pr_file": "docs/my-website/docs/observability/agentops_integration.md",
    "created_at": "2025-03-26T20:46:21+00:00",
    "commented_code": "+# \ud83d\udd87\ufe0f AgentOps - LLM Observability Platform\n+\n+:::tip\n+\n+This is community maintained. Please make an issue if you run into a bug:\n+https://github.com/BerriAI/litellm\n+\n+:::\n+\n+[AgentOps](https://docs.agentops.ai) is an observability platform that enables tracing and monitoring of LLM calls, providing detailed insights into your AI operations.\n+\n+## Using AgentOps with LiteLLM\n+\n+LiteLLM provides `success_callbacks` and `failure_callbacks`, allowing you to easily integrate AgentOps for comprehensive tracing and monitoring of your LLM operations.\n+\n+### Integration\n+\n+Use just a few lines of code to instantly trace your responses **across all providers** with AgentOps:\n+\n+```python\n+from agentops.integration.callbacks.litellm import LiteLLMCallbackHandler",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2014974737",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9561,
        "pr_file": "docs/my-website/docs/observability/agentops_integration.md",
        "discussion_id": "2014974737",
        "commented_code": "@@ -0,0 +1,107 @@\n+# \ud83d\udd87\ufe0f AgentOps - LLM Observability Platform\n+\n+:::tip\n+\n+This is community maintained. Please make an issue if you run into a bug:\n+https://github.com/BerriAI/litellm\n+\n+:::\n+\n+[AgentOps](https://docs.agentops.ai) is an observability platform that enables tracing and monitoring of LLM calls, providing detailed insights into your AI operations.\n+\n+## Using AgentOps with LiteLLM\n+\n+LiteLLM provides `success_callbacks` and `failure_callbacks`, allowing you to easily integrate AgentOps for comprehensive tracing and monitoring of your LLM operations.\n+\n+### Integration\n+\n+Use just a few lines of code to instantly trace your responses **across all providers** with AgentOps:\n+\n+```python\n+from agentops.integration.callbacks.litellm import LiteLLMCallbackHandler",
        "comment_created_at": "2025-03-26T20:46:21+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "i'm confused - why does the user need to pip install the agentops package for this? \r\n\r\nit should just work with litellm, like langsmith does via httpx ",
        "pr_file_module": null
      },
      {
        "comment_id": "2016806844",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9561,
        "pr_file": "docs/my-website/docs/observability/agentops_integration.md",
        "discussion_id": "2014974737",
        "commented_code": "@@ -0,0 +1,107 @@\n+# \ud83d\udd87\ufe0f AgentOps - LLM Observability Platform\n+\n+:::tip\n+\n+This is community maintained. Please make an issue if you run into a bug:\n+https://github.com/BerriAI/litellm\n+\n+:::\n+\n+[AgentOps](https://docs.agentops.ai) is an observability platform that enables tracing and monitoring of LLM calls, providing detailed insights into your AI operations.\n+\n+## Using AgentOps with LiteLLM\n+\n+LiteLLM provides `success_callbacks` and `failure_callbacks`, allowing you to easily integrate AgentOps for comprehensive tracing and monitoring of your LLM operations.\n+\n+### Integration\n+\n+Use just a few lines of code to instantly trace your responses **across all providers** with AgentOps:\n+\n+```python\n+from agentops.integration.callbacks.litellm import LiteLLMCallbackHandler",
        "comment_created_at": "2025-03-27T14:29:09+00:00",
        "comment_author": "Dwij1704",
        "comment_body": "While Langsmith sends data to its backend via httpx and then processes telemetry there, AgentOps handles telemetry collection and processing on the client side through its package. Our approach is similar to Langfuse which requires installation. ",
        "pr_file_module": null
      }
    ]
  }
]