[
  {
    "discussion_id": "1542243328",
    "pr_number": 2717,
    "pr_file": "litellm/llms/solar.py",
    "created_at": "2024-03-28T02:41:36+00:00",
    "commented_code": "+import os, types\n+import json\n+from enum import Enum\n+import requests\n+import time\n+from typing import Callable, Optional\n+import litellm\n+import httpx\n+from litellm.utils import ModelResponse, Usage\n+from .prompt_templates.factory import prompt_factory, custom_prompt\n+\n+\n+class SolarError(Exception):",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1542243328",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 2717,
        "pr_file": "litellm/llms/solar.py",
        "discussion_id": "1542243328",
        "commented_code": "@@ -0,0 +1,176 @@\n+import os, types\n+import json\n+from enum import Enum\n+import requests\n+import time\n+from typing import Callable, Optional\n+import litellm\n+import httpx\n+from litellm.utils import ModelResponse, Usage\n+from .prompt_templates.factory import prompt_factory, custom_prompt\n+\n+\n+class SolarError(Exception):",
        "comment_created_at": "2024-03-28T02:41:36+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "if solar is openai-compatible, a separate class is not needed @Tokkiu \r\n\r\nSee the GROQ implementation for referencehttps://github.com/BerriAI/litellm/pull/2168",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1674307160",
    "pr_number": 4118,
    "pr_file": "litellm/llms/astra_assistants.py",
    "created_at": "2024-07-11T16:15:21+00:00",
    "commented_code": "+from typing import Optional, Union\n+from typing_extensions import override\n+import httpx\n+from openai import OpenAI\n+from ..llms.openai import OpenAIAssistantsAPI\n+from astra_assistants import patch\n+import astra_assistants\n+\n+\n+\n+class AstraAssistantsAPI(OpenAIAssistantsAPI):",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1674307160",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 4118,
        "pr_file": "litellm/llms/astra_assistants.py",
        "discussion_id": "1674307160",
        "commented_code": "@@ -0,0 +1,39 @@\n+from typing import Optional, Union\n+from typing_extensions import override\n+import httpx\n+from openai import OpenAI\n+from ..llms.openai import OpenAIAssistantsAPI\n+from astra_assistants import patch\n+import astra_assistants\n+\n+\n+\n+class AstraAssistantsAPI(OpenAIAssistantsAPI):",
        "comment_created_at": "2024-07-11T16:15:21+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "i don't see why this needs to exist as a separate file / if-else block in all the assistant api endpoints. \r\n\r\nIf it's openai-compatible, all you need to do is update `get_llm_provider` like we do for groq \r\n\r\nand this should work without major changes. \r\n\r\nSee - https://github.com/BerriAI/litellm/pull/2168/files#diff-9661b0d8f1d9f48433f2323d1102e963d3098f667f7a25caea79f16fa282f6dd",
        "pr_file_module": null
      },
      {
        "comment_id": "1675100171",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 4118,
        "pr_file": "litellm/llms/astra_assistants.py",
        "discussion_id": "1674307160",
        "commented_code": "@@ -0,0 +1,39 @@\n+from typing import Optional, Union\n+from typing_extensions import override\n+import httpx\n+from openai import OpenAI\n+from ..llms.openai import OpenAIAssistantsAPI\n+from astra_assistants import patch\n+import astra_assistants\n+\n+\n+\n+class AstraAssistantsAPI(OpenAIAssistantsAPI):",
        "comment_created_at": "2024-07-12T03:35:49+00:00",
        "comment_author": "phact",
        "comment_body": "Oh cool, I didn't realize that thanks!\r\nWill fix",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1986501441",
    "pr_number": 6981,
    "pr_file": "litellm/utils.py",
    "created_at": "2025-03-10T01:51:36+00:00",
    "commented_code": "and custom_llm_provider != \"together_ai\"\n             and custom_llm_provider != \"groq\"\n             and custom_llm_provider != \"nvidia_nim\"\n+            and custom_llm_provider != \"nvidia\"",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1986501441",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 6981,
        "pr_file": "litellm/utils.py",
        "discussion_id": "1986501441",
        "commented_code": "@@ -2885,6 +2889,7 @@ def get_optional_params(  # noqa: PLR0915\n             and custom_llm_provider != \"together_ai\"\n             and custom_llm_provider != \"groq\"\n             and custom_llm_provider != \"nvidia_nim\"\n+            and custom_llm_provider != \"nvidia\"",
        "comment_created_at": "2025-03-10T01:51:36+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "remove this hardcoding, it's sufficient to add to `openai_compatible_providers` on init.py",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2002525751",
    "pr_number": 9362,
    "pr_file": "litellm/main.py",
    "created_at": "2025-03-19T06:38:47+00:00",
    "commented_code": ")\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"bitdeerai\":\n+            bitdeerai_key = (\n+                api_key\n+                or litellm.bitdeerai_key\n+                or get_secret(\"BITDEERAI_API_KEY\")\n+                or litellm.api_key\n+                )\n+            api_base = (\n+                api_base\n+                or litellm.api_base\n+                or get_secret(\"BITDEERAI_API_BASE\")\n+                or \"https://api-inference.bitdeer.ai/v1\"\n+            )\n+            model_response = openai_like_chat_completion.completion(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2002525751",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9362,
        "pr_file": "litellm/main.py",
        "discussion_id": "2002525751",
        "commented_code": "@@ -2188,6 +2189,35 @@ def completion(  # type: ignore # noqa: PLR0915\n                 )\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"bitdeerai\":\n+            bitdeerai_key = (\n+                api_key\n+                or litellm.bitdeerai_key\n+                or get_secret(\"BITDEERAI_API_KEY\")\n+                or litellm.api_key\n+                )\n+            api_base = (\n+                api_base\n+                or litellm.api_base\n+                or get_secret(\"BITDEERAI_API_BASE\")\n+                or \"https://api-inference.bitdeer.ai/v1\"\n+            )\n+            model_response = openai_like_chat_completion.completion(",
        "comment_created_at": "2025-03-19T06:38:47+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "please use `base_llm_http_handler.completion` instead",
        "pr_file_module": null
      },
      {
        "comment_id": "2002903379",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9362,
        "pr_file": "litellm/main.py",
        "discussion_id": "2002525751",
        "commented_code": "@@ -2188,6 +2189,35 @@ def completion(  # type: ignore # noqa: PLR0915\n                 )\n                 return response\n             response = model_response\n+        elif custom_llm_provider == \"bitdeerai\":\n+            bitdeerai_key = (\n+                api_key\n+                or litellm.bitdeerai_key\n+                or get_secret(\"BITDEERAI_API_KEY\")\n+                or litellm.api_key\n+                )\n+            api_base = (\n+                api_base\n+                or litellm.api_base\n+                or get_secret(\"BITDEERAI_API_BASE\")\n+                or \"https://api-inference.bitdeer.ai/v1\"\n+            )\n+            model_response = openai_like_chat_completion.completion(",
        "comment_created_at": "2025-03-19T09:44:02+00:00",
        "comment_author": "Yujie-zhuzhu",
        "comment_body": "remove this, add to openai_compatible_provider",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2099096546",
    "pr_number": 10385,
    "pr_file": "litellm/main.py",
    "created_at": "2025-05-21T00:54:19+00:00",
    "commented_code": "original_response=response,\n                     additional_args={\"headers\": headers},\n                 )\n+\n+        elif custom_llm_provider == \"datarobot\":\n+            response = openai_like_chat_completion.completion(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2099096546",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10385,
        "pr_file": "litellm/main.py",
        "discussion_id": "2099096546",
        "commented_code": "@@ -2320,6 +2320,28 @@ def completion(  # type: ignore # noqa: PLR0915\n                     original_response=response,\n                     additional_args={\"headers\": headers},\n                 )\n+\n+        elif custom_llm_provider == \"datarobot\":\n+            response = openai_like_chat_completion.completion(",
        "comment_created_at": "2025-05-21T00:54:19+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "can we use `base_llm_http_handler` here instead - see deepseek/fireworks ai/groq if blocks for reference \r\n\r\nopenai_like.. is our older implementation of that flow ",
        "pr_file_module": null
      },
      {
        "comment_id": "2100362896",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10385,
        "pr_file": "litellm/main.py",
        "discussion_id": "2099096546",
        "commented_code": "@@ -2320,6 +2320,28 @@ def completion(  # type: ignore # noqa: PLR0915\n                     original_response=response,\n                     additional_args={\"headers\": headers},\n                 )\n+\n+        elif custom_llm_provider == \"datarobot\":\n+            response = openai_like_chat_completion.completion(",
        "comment_created_at": "2025-05-21T13:54:00+00:00",
        "comment_author": "mjnitz02",
        "comment_body": "I migrated this to `base_llm_http_handler`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1938838618",
    "pr_number": 8154,
    "pr_file": "litellm/llms/nebius/chat/transformation.py",
    "created_at": "2025-02-03T06:26:00+00:00",
    "commented_code": "+\"\"\"\n+Nebius AI Studio Chat Completions API\n+\n+this is OpenAI compatible - no translation needed / occurs\n+\"\"\"\n+\n+from typing import Optional\n+\n+from litellm.llms.openai.chat.gpt_transformation import OpenAIGPTConfig\n+\n+\n+class NebiusConfig(OpenAIGPTConfig):",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1938838618",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8154,
        "pr_file": "litellm/llms/nebius/chat/transformation.py",
        "discussion_id": "1938838618",
        "commented_code": "@@ -0,0 +1,74 @@\n+\"\"\"\n+Nebius AI Studio Chat Completions API\n+\n+this is OpenAI compatible - no translation needed / occurs\n+\"\"\"\n+\n+from typing import Optional\n+\n+from litellm.llms.openai.chat.gpt_transformation import OpenAIGPTConfig\n+\n+\n+class NebiusConfig(OpenAIGPTConfig):",
        "comment_created_at": "2025-02-03T06:26:00+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "please inherit from the OpenAILikeConfig - it should be safer, and handles typical rough edges around openai compatible providers",
        "pr_file_module": null
      },
      {
        "comment_id": "1950423705",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8154,
        "pr_file": "litellm/llms/nebius/chat/transformation.py",
        "discussion_id": "1938838618",
        "commented_code": "@@ -0,0 +1,74 @@\n+\"\"\"\n+Nebius AI Studio Chat Completions API\n+\n+this is OpenAI compatible - no translation needed / occurs\n+\"\"\"\n+\n+from typing import Optional\n+\n+from litellm.llms.openai.chat.gpt_transformation import OpenAIGPTConfig\n+\n+\n+class NebiusConfig(OpenAIGPTConfig):",
        "comment_created_at": "2025-02-11T08:32:10+00:00",
        "comment_author": "Aktsvigun",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1994913758",
    "pr_number": 7582,
    "pr_file": "litellm/main.py",
    "created_at": "2025-03-14T06:09:23+00:00",
    "commented_code": "or custom_llm_provider == \"huggingface\"\n             or custom_llm_provider == \"ollama\"\n             or custom_llm_provider == \"vertex_ai\"\n+            or custom_llm_provider == \"novita\"",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1994913758",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7582,
        "pr_file": "litellm/main.py",
        "discussion_id": "1994913758",
        "commented_code": "@@ -3923,6 +3965,7 @@ async def atext_completion(\n             or custom_llm_provider == \"huggingface\"\n             or custom_llm_provider == \"ollama\"\n             or custom_llm_provider == \"vertex_ai\"\n+            or custom_llm_provider == \"novita\"",
        "comment_created_at": "2025-03-14T06:09:23+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "no need for this, see line below - just register as an openai_compatible_provider ",
        "pr_file_module": null
      },
      {
        "comment_id": "1997908553",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7582,
        "pr_file": "litellm/main.py",
        "discussion_id": "1994913758",
        "commented_code": "@@ -3923,6 +3965,7 @@ async def atext_completion(\n             or custom_llm_provider == \"huggingface\"\n             or custom_llm_provider == \"ollama\"\n             or custom_llm_provider == \"vertex_ai\"\n+            or custom_llm_provider == \"novita\"",
        "comment_created_at": "2025-03-17T04:16:50+00:00",
        "comment_author": "jasonhp",
        "comment_body": "Fixed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2078971353",
    "pr_number": 10631,
    "pr_file": "litellm/integrations/s3.py",
    "created_at": "2025-05-08T06:18:08+00:00",
    "commented_code": "s3_aws_access_key_id=None,\n         s3_aws_secret_access_key=None,\n         s3_aws_session_token=None,\n+        s3_aws_role_name=None,\n         s3_config=None,\n         **kwargs,\n     ):\n         import boto3\n \n+        # Store AWS configuration\n+        self.aws_config = {",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2078971353",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10631,
        "pr_file": "litellm/integrations/s3.py",
        "discussion_id": "2078971353",
        "commented_code": "@@ -23,11 +23,29 @@ def __init__(\n         s3_aws_access_key_id=None,\n         s3_aws_secret_access_key=None,\n         s3_aws_session_token=None,\n+        s3_aws_role_name=None,\n         s3_config=None,\n         **kwargs,\n     ):\n         import boto3\n \n+        # Store AWS configuration\n+        self.aws_config = {",
        "comment_created_at": "2025-05-08T06:18:08+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "could we reuse BaseAWSLLM here? - https://github.com/BerriAI/litellm/blob/2361bd98b08ba847657b8556671f9a9a792a0c93/litellm/llms/bedrock/base_aws_llm.py#L43",
        "pr_file_module": null
      },
      {
        "comment_id": "2079846237",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10631,
        "pr_file": "litellm/integrations/s3.py",
        "discussion_id": "2078971353",
        "commented_code": "@@ -23,11 +23,29 @@ def __init__(\n         s3_aws_access_key_id=None,\n         s3_aws_secret_access_key=None,\n         s3_aws_session_token=None,\n+        s3_aws_role_name=None,\n         s3_config=None,\n         **kwargs,\n     ):\n         import boto3\n \n+        # Store AWS configuration\n+        self.aws_config = {",
        "comment_created_at": "2025-05-08T14:33:54+00:00",
        "comment_author": "anton164",
        "comment_body": "`BaseAWSLLM` is coupled to Bedrock endpoints. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2133082457",
    "pr_number": 11420,
    "pr_file": "litellm/proxy/proxy_server.py",
    "created_at": "2025-06-06T23:52:10+00:00",
    "commented_code": "model_list = config.get(\"model_list\", None)\n         if model_list:\n             router_params[\"model_list\"] = model_list\n-            print(  # noqa\n-                \"\\033[32mLiteLLM: Proxy initialized with Config, Set models:\\033[0m\"\n-            )  # noqa\n-            for model in model_list:\n-                ### LOAD FROM os.environ/ ###\n-                for k, v in model[\"litellm_params\"].items():\n-                    if isinstance(v, str) and v.startswith(\"os.environ/\"):\n-                        model[\"litellm_params\"][k] = get_secret(v)\n-                print(f\"\\033[32m    {model.get('model_name', '')}\\033[0m\")  # noqa\n+            \n+            # Import Rich console for beautiful model list display\n+            try:\n+                from rich.console import Console\n+                from rich.panel import Panel\n+                from rich.table import Table\n+                from rich.align import Align\n+                \n+                console = Console()\n+                \n+                # Create a beautiful table for models\n+                models_table = Table(title=\"\ud83e\udd16 Configured Models\", show_header=True, header_style=\"bold cyan\")\n+                models_table.add_column(\"Model Name\", style=\"green\", min_width=25)\n+                models_table.add_column(\"Provider\", style=\"blue\", min_width=15)\n+                \n+                for model in model_list:\n+                    ### LOAD FROM os.environ/ ###\n+                    for k, v in model[\"litellm_params\"].items():\n+                        if isinstance(v, str) and v.startswith(\"os.environ/\"):\n+                            model[\"litellm_params\"][k] = get_secret(v)\n+                    \n+                    model_name = model.get('model_name', 'Unknown')\n+                    litellm_model = model[\"litellm_params\"].get(\"model\", \"Unknown\")\n+                    \n+                    # Extract provider from model name if possible\n+                    provider = \"OpenAI\"  # default",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2133082457",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11420,
        "pr_file": "litellm/proxy/proxy_server.py",
        "discussion_id": "2133082457",
        "commented_code": "@@ -1991,15 +1944,88 @@ async def load_config(  # noqa: PLR0915\n         model_list = config.get(\"model_list\", None)\n         if model_list:\n             router_params[\"model_list\"] = model_list\n-            print(  # noqa\n-                \"\\033[32mLiteLLM: Proxy initialized with Config, Set models:\\033[0m\"\n-            )  # noqa\n-            for model in model_list:\n-                ### LOAD FROM os.environ/ ###\n-                for k, v in model[\"litellm_params\"].items():\n-                    if isinstance(v, str) and v.startswith(\"os.environ/\"):\n-                        model[\"litellm_params\"][k] = get_secret(v)\n-                print(f\"\\033[32m    {model.get('model_name', '')}\\033[0m\")  # noqa\n+            \n+            # Import Rich console for beautiful model list display\n+            try:\n+                from rich.console import Console\n+                from rich.panel import Panel\n+                from rich.table import Table\n+                from rich.align import Align\n+                \n+                console = Console()\n+                \n+                # Create a beautiful table for models\n+                models_table = Table(title=\"\ud83e\udd16 Configured Models\", show_header=True, header_style=\"bold cyan\")\n+                models_table.add_column(\"Model Name\", style=\"green\", min_width=25)\n+                models_table.add_column(\"Provider\", style=\"blue\", min_width=15)\n+                \n+                for model in model_list:\n+                    ### LOAD FROM os.environ/ ###\n+                    for k, v in model[\"litellm_params\"].items():\n+                        if isinstance(v, str) and v.startswith(\"os.environ/\"):\n+                            model[\"litellm_params\"][k] = get_secret(v)\n+                    \n+                    model_name = model.get('model_name', 'Unknown')\n+                    litellm_model = model[\"litellm_params\"].get(\"model\", \"Unknown\")\n+                    \n+                    # Extract provider from model name if possible\n+                    provider = \"OpenAI\"  # default",
        "comment_created_at": "2025-06-06T23:52:10+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "we have a util for this get_llm_provider, please use that here ",
        "pr_file_module": null
      },
      {
        "comment_id": "2133082693",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11420,
        "pr_file": "litellm/proxy/proxy_server.py",
        "discussion_id": "2133082457",
        "commented_code": "@@ -1991,15 +1944,88 @@ async def load_config(  # noqa: PLR0915\n         model_list = config.get(\"model_list\", None)\n         if model_list:\n             router_params[\"model_list\"] = model_list\n-            print(  # noqa\n-                \"\\033[32mLiteLLM: Proxy initialized with Config, Set models:\\033[0m\"\n-            )  # noqa\n-            for model in model_list:\n-                ### LOAD FROM os.environ/ ###\n-                for k, v in model[\"litellm_params\"].items():\n-                    if isinstance(v, str) and v.startswith(\"os.environ/\"):\n-                        model[\"litellm_params\"][k] = get_secret(v)\n-                print(f\"\\033[32m    {model.get('model_name', '')}\\033[0m\")  # noqa\n+            \n+            # Import Rich console for beautiful model list display\n+            try:\n+                from rich.console import Console\n+                from rich.panel import Panel\n+                from rich.table import Table\n+                from rich.align import Align\n+                \n+                console = Console()\n+                \n+                # Create a beautiful table for models\n+                models_table = Table(title=\"\ud83e\udd16 Configured Models\", show_header=True, header_style=\"bold cyan\")\n+                models_table.add_column(\"Model Name\", style=\"green\", min_width=25)\n+                models_table.add_column(\"Provider\", style=\"blue\", min_width=15)\n+                \n+                for model in model_list:\n+                    ### LOAD FROM os.environ/ ###\n+                    for k, v in model[\"litellm_params\"].items():\n+                        if isinstance(v, str) and v.startswith(\"os.environ/\"):\n+                            model[\"litellm_params\"][k] = get_secret(v)\n+                    \n+                    model_name = model.get('model_name', 'Unknown')\n+                    litellm_model = model[\"litellm_params\"].get(\"model\", \"Unknown\")\n+                    \n+                    # Extract provider from model name if possible\n+                    provider = \"OpenAI\"  # default",
        "comment_created_at": "2025-06-06T23:52:55+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "make sure to try/except the util ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2129678267",
    "pr_number": 11381,
    "pr_file": "litellm/utils.py",
    "created_at": "2025-06-05T18:12:34+00:00",
    "commented_code": "):\n                     _model_info = None\n \n+            if _model_info is None and custom_llm_provider == \"litellm_proxy\" and \"/\" in model:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2129678267",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11381,
        "pr_file": "litellm/utils.py",
        "discussion_id": "2129678267",
        "commented_code": "@@ -4507,6 +4507,58 @@ def _get_model_info_helper(  # noqa: PLR0915\n                 ):\n                     _model_info = None\n \n+            if _model_info is None and custom_llm_provider == \"litellm_proxy\" and \"/\" in model:",
        "comment_created_at": "2025-06-05T18:12:34+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "we should avoid adding bloat to this function, as it's quite large already. \r\n\r\nI think this can be achieved by updating `_get_potential_model_names` instead",
        "pr_file_module": null
      },
      {
        "comment_id": "2131119329",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11381,
        "pr_file": "litellm/utils.py",
        "discussion_id": "2129678267",
        "commented_code": "@@ -4507,6 +4507,58 @@ def _get_model_info_helper(  # noqa: PLR0915\n                 ):\n                     _model_info = None\n \n+            if _model_info is None and custom_llm_provider == \"litellm_proxy\" and \"/\" in model:",
        "comment_created_at": "2025-06-06T00:39:08+00:00",
        "comment_author": "pazevedo-hyland",
        "comment_body": "@krrishdholakia , Okay, ive simplified things a bit and used that method instead. Let me know if it aligns with your idea",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2125093306",
    "pr_number": 11339,
    "pr_file": "litellm/llms/fireworks_ai/chat/transformation.py",
    "created_at": "2025-06-03T23:12:25+00:00",
    "commented_code": "\"prompt_truncate_length\",\n             \"context_length_exceeded_behavior\",\n         ]\n+        \n+        # Only add tools and tool_choice for models that support it\n+        # Currently only firefunction-v2, mixtral-8x22b-instruct-hf, and llama-v3p1-405b-instruct support tool calling\n+        if (\n+            \"firefunction-v2\" in model \n+            or \"mixtral-8x22b-instruct-hf\" in model",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2125093306",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11339,
        "pr_file": "litellm/llms/fireworks_ai/chat/transformation.py",
        "discussion_id": "2125093306",
        "commented_code": "@@ -102,6 +101,17 @@ def get_supported_openai_params(self, model: str):\n             \"prompt_truncate_length\",\n             \"context_length_exceeded_behavior\",\n         ]\n+        \n+        # Only add tools and tool_choice for models that support it\n+        # Currently only firefunction-v2, mixtral-8x22b-instruct-hf, and llama-v3p1-405b-instruct support tool calling\n+        if (\n+            \"firefunction-v2\" in model \n+            or \"mixtral-8x22b-instruct-hf\" in model",
        "comment_created_at": "2025-06-03T23:12:25+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "can we rely on the map for this - moving to use `supports_function_calling` util from the litellm.utils \r\n\r\nthis way, we can just update the model cost map for any future model additions ",
        "pr_file_module": null
      },
      {
        "comment_id": "2125111608",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11339,
        "pr_file": "litellm/llms/fireworks_ai/chat/transformation.py",
        "discussion_id": "2125093306",
        "commented_code": "@@ -102,6 +101,17 @@ def get_supported_openai_params(self, model: str):\n             \"prompt_truncate_length\",\n             \"context_length_exceeded_behavior\",\n         ]\n+        \n+        # Only add tools and tool_choice for models that support it\n+        # Currently only firefunction-v2, mixtral-8x22b-instruct-hf, and llama-v3p1-405b-instruct support tool calling\n+        if (\n+            \"firefunction-v2\" in model \n+            or \"mixtral-8x22b-instruct-hf\" in model",
        "comment_created_at": "2025-06-03T23:26:45+00:00",
        "comment_author": "colesmcintosh",
        "comment_body": "Updated to use supports_function_calling from utils",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2069852749",
    "pr_number": 10451,
    "pr_file": "litellm/litellm_core_utils/get_supported_openai_params.py",
    "created_at": "2025-05-01T05:00:07+00:00",
    "commented_code": "if custom_llm_provider == \"bedrock\":\n         return litellm.AmazonConverseConfig().get_supported_openai_params(model=model)\n+    elif custom_llm_provider == \"meta-llama\":\n+        return litellm.LlamaAPIConfig().get_supported_openai_params(model=model)",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2069852749",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10451,
        "pr_file": "litellm/litellm_core_utils/get_supported_openai_params.py",
        "discussion_id": "2069852749",
        "commented_code": "@@ -46,6 +46,8 @@ def get_supported_openai_params(  # noqa: PLR0915\n \n     if custom_llm_provider == \"bedrock\":\n         return litellm.AmazonConverseConfig().get_supported_openai_params(model=model)\n+    elif custom_llm_provider == \"meta-llama\":\n+        return litellm.LlamaAPIConfig().get_supported_openai_params(model=model)",
        "comment_created_at": "2025-05-01T05:00:07+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "please use the ProviderConfigManager.get_provider_chat_config factory instead",
        "pr_file_module": null
      }
    ]
  }
]