[
  {
    "discussion_id": "1946247872",
    "pr_number": 8258,
    "pr_file": "docs/my-website/docs/providers/huggingface.md",
    "created_at": "2025-02-07T09:41:12+00:00",
    "commented_code": "import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n+```\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n+### Getting Started\n \n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n+```\n+huggingface/<provider>/<model_id>",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1946247872",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 8258,
        "pr_file": "docs/my-website/docs/providers/huggingface.md",
        "discussion_id": "1946247872",
        "commented_code": "@@ -2,466 +2,370 @@ import Image from '@theme/IdealImage';\n import Tabs from '@theme/Tabs';\n import TabItem from '@theme/TabItem';\n \n-# Huggingface\n+# Hugging Face\n+LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.\n \n-LiteLLM supports the following types of Hugging Face models:\n+- **Serverless Inference Providers** - Hugging Face offers an easy and unified access to serverless AI inference through multiple inference providers, like [Together AI](https://together.ai) and [Sambanova](https://sambanova.ai). This is the fastest way to integrate AI in your products with a maintenance-free and scalable solution. More details in the [Inference Providers blogpost](https://huggingface.co/blog/inference-providers).\n+- **Dedicated Inference Endpoints** - which is a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. You can deploy your model on Hugging Face Inference Endpoints by following the steps [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint).\n \n-- Serverless Inference API (free) - loaded and ready to use: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation\n-- Dedicated Inference Endpoints (paid) - manual deployment: https://ui.endpoints.huggingface.co/\n-- All LLMs served via Hugging Face's Inference use [Text-generation-inference](https://huggingface.co/docs/text-generation-inference). \n \n-## Usage\n+## Supported Models\n \n-<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_HuggingFace.ipynb\">\n-  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n-</a>\n+### Serverless Inference Providers\n+You can check available models for an inference provider by going to [huggingface.co/models](https://huggingface.co/models), clicking the \"Other\" filter tab, and selecting your desired provider:\n \n-You need to tell LiteLLM when you're calling Huggingface.\n-This is done by adding the \"huggingface/\" prefix to `model`, example `completion(model=\"huggingface/<model_name>\",...)`.\n+![Filter models by Inference Provider](../../img/hf_filter_inference_providers.png)\n \n-<Tabs>\n-<TabItem value=\"serverless\" label=\"Serverless Inference API\">\n+### Dedicated Inference Endpoints\n+Refer to the [Inference Endpoints catalog](https://endpoints.huggingface.co/catalog) for a list of available models.\n \n-By default, LiteLLM will assume a Hugging Face call follows the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api), which is fully compatible with the OpenAI Chat Completion API.\n+## Usage\n \n <Tabs>\n-<TabItem value=\"sdk\" label=\"SDK\">\n-\n-```python\n-import os\n-from litellm import completion\n-\n-# [OPTIONAL] set env var\n-os.environ[\"HUGGINGFACE_API_KEY\"] = \"huggingface_api_key\"\n+<TabItem value=\"serverless\" label=\"Serverless Inference Providers\">\n \n-messages = [{ \"content\": \"There's a llama in my garden \ud83d\ude31 What should I do?\",\"role\": \"user\"}]\n+### Authentication\n+With a single Hugging Face token, you can access inference through multiple providers. Your calls are routed through Hugging Face and the usage is billed directly to your Hugging Face account at the standard provider API rates.\n \n-# e.g. Call 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct' from Serverless Inference API\n-response = completion(\n-    model=\"huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n-    stream=True\n-)\n+Simply set the `HF_TOKEN` environment variable with your Hugging Face token, you can create one here: https://huggingface.co/settings/tokens.\n \n-print(response)\n+```bash\n+export HF_TOKEN=\"hf_xxxxxx\"\n+```\n+or alternatively, you can pass your Hugging Face token as a parameter:\n+```python\n+completion(..., api_key=\"hf_xxxxxx\")\n ```\n \n-</TabItem>\n-<TabItem value=\"proxy\" label=\"PROXY\">\n-\n-1. Add models to your config.yaml\n+### Getting Started\n \n-```yaml\n-model_list:\n-  - model_name: llama-3.1-8B-instruct\n-    litellm_params:\n-      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct\n-      api_key: os.environ/HUGGINGFACE_API_KEY\n+To use a Hugging Face model, specify both the provider and model you want to use in the following format:\n+```\n+huggingface/<provider>/<model_id>",
        "comment_created_at": "2025-02-07T09:41:12+00:00",
        "comment_author": "julien-c",
        "comment_body": "```suggestion\r\nhuggingface/<provider>/<hf_org_or_user>/<hf_model>\r\n```\r\n\r\nmaybe like this to show the last part also has a `/` in it (aka., is namespaced)?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2088114170",
    "pr_number": 10815,
    "pr_file": "docs/my-website/docs/proxy/config_settings.md",
    "created_at": "2025-05-14T06:08:19+00:00",
    "commented_code": "| enable_oauth2_proxy_auth | boolean | (Enterprise Feature) If true, enables oauth2.0 authentication |\n | forward_openai_org_id | boolean | If true, forwards the OpenAI Organization ID to the backend LLM call (if it's OpenAI). |\n | forward_client_headers_to_llm_api | boolean | If true, forwards the client headers (any `x-` headers) to the backend LLM call |\n+| maximum_retention_period | str | Used to set the max retention time for spend logs in the db, after which they will be auto-purged |",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2088114170",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10815,
        "pr_file": "docs/my-website/docs/proxy/config_settings.md",
        "discussion_id": "2088114170",
        "commented_code": "@@ -211,6 +212,7 @@ general_settings:\n | enable_oauth2_proxy_auth | boolean | (Enterprise Feature) If true, enables oauth2.0 authentication |\n | forward_openai_org_id | boolean | If true, forwards the OpenAI Organization ID to the backend LLM call (if it's OpenAI). |\n | forward_client_headers_to_llm_api | boolean | If true, forwards the client headers (any `x-` headers) to the backend LLM call |\n+| maximum_retention_period | str | Used to set the max retention time for spend logs in the db, after which they will be auto-purged |",
        "comment_created_at": "2025-05-14T06:08:19+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "reading this over - can we make this more precise, e.g. `maximum_spend_logs_retention_period`? \r\n\r\nIt's more verbose but also more precise (i know exactly what it means when i read it) ",
        "pr_file_module": null
      }
    ]
  }
]