[
  {
    "discussion_id": "2295336460",
    "pr_number": 13886,
    "pr_file": "litellm/llms/databricks/chat/transformation.py",
    "created_at": "2025-08-23T06:51:24+00:00",
    "commented_code": "type=\"function\",\n             function=DatabricksFunction(\n                 name=tool[\"name\"],\n+                description=cast(dict, tool.get(\"description\")),",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2295336460",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13886,
        "pr_file": "litellm/llms/databricks/chat/transformation.py",
        "discussion_id": "2295336460",
        "commented_code": "@@ -174,6 +174,7 @@ def convert_anthropic_tool_to_databricks_tool(\n             type=\"function\",\n             function=DatabricksFunction(\n                 name=tool[\"name\"],\n+                description=cast(dict, tool.get(\"description\")),",
        "comment_created_at": "2025-08-23T06:51:24+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "this assumes description is always set and is a dictionary. \r\n\r\nthis is not always true. \r\n\r\ncan you please handle for the none case",
        "pr_file_module": null
      },
      {
        "comment_id": "2311725101",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13886,
        "pr_file": "litellm/llms/databricks/chat/transformation.py",
        "discussion_id": "2295336460",
        "commented_code": "@@ -174,6 +174,7 @@ def convert_anthropic_tool_to_databricks_tool(\n             type=\"function\",\n             function=DatabricksFunction(\n                 name=tool[\"name\"],\n+                description=cast(dict, tool.get(\"description\")),",
        "comment_created_at": "2025-08-30T01:29:54+00:00",
        "comment_author": "frankzye",
        "comment_body": "updated, please help review @krrishdholakia ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2262290032",
    "pr_number": 12125,
    "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
    "created_at": "2025-08-08T08:26:03+00:00",
    "commented_code": "return\n \n         ## CALCULATE COST FOR BATCH JOBS\n-        if (\n-            self.call_type == CallTypes.aretrieve_batch.value\n-            and isinstance(result, LiteLLMBatch)\n-            and result.status == \"completed\"\n+        if self.call_type == CallTypes.aretrieve_batch.value and isinstance(\n+            result, LiteLLMBatch\n         ):\n+            litellm_params = self.litellm_params or {}\n+            litellm_metadata = litellm_params.get(\"litellm_metadata\", {})\n+            if (\n+                litellm_metadata.get(\"batch_ignore_default_logging\", False) is True\n+            ):  # polling job will query these frequently, don't spam db logs\n+                return\n+\n             from litellm.proxy.openai_files_endpoints.common_utils import (\n                 _is_base64_encoded_unified_file_id,\n             )\n \n             # check if file id is a unified file id\n             is_base64_unified_file_id = _is_base64_encoded_unified_file_id(result.id)\n-            if not is_base64_unified_file_id:  # only run for non-unified file ids\n+\n+            batch_cost = kwargs.get(\"batch_cost\", None)\n+            batch_usage = kwargs.get(\"batch_usage\", None)\n+            batch_models = kwargs.get(\"batch_models\", None)\n+            if all([batch_cost, batch_usage, batch_models]) is not None:\n+                result._hidden_params[\"response_cost\"] = batch_cost\n+                result._hidden_params[\"batch_models\"] = batch_models\n+                result.usage = batch_usage",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2262290032",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12125,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2262290032",
        "commented_code": "@@ -1997,18 +1998,32 @@ async def async_success_handler(  # noqa: PLR0915\n             return\n \n         ## CALCULATE COST FOR BATCH JOBS\n-        if (\n-            self.call_type == CallTypes.aretrieve_batch.value\n-            and isinstance(result, LiteLLMBatch)\n-            and result.status == \"completed\"\n+        if self.call_type == CallTypes.aretrieve_batch.value and isinstance(\n+            result, LiteLLMBatch\n         ):\n+            litellm_params = self.litellm_params or {}\n+            litellm_metadata = litellm_params.get(\"litellm_metadata\", {})\n+            if (\n+                litellm_metadata.get(\"batch_ignore_default_logging\", False) is True\n+            ):  # polling job will query these frequently, don't spam db logs\n+                return\n+\n             from litellm.proxy.openai_files_endpoints.common_utils import (\n                 _is_base64_encoded_unified_file_id,\n             )\n \n             # check if file id is a unified file id\n             is_base64_unified_file_id = _is_base64_encoded_unified_file_id(result.id)\n-            if not is_base64_unified_file_id:  # only run for non-unified file ids\n+\n+            batch_cost = kwargs.get(\"batch_cost\", None)\n+            batch_usage = kwargs.get(\"batch_usage\", None)\n+            batch_models = kwargs.get(\"batch_models\", None)\n+            if all([batch_cost, batch_usage, batch_models]) is not None:\n+                result._hidden_params[\"response_cost\"] = batch_cost\n+                result._hidden_params[\"batch_models\"] = batch_models\n+                result.usage = batch_usage",
        "comment_created_at": "2025-08-08T08:26:03+00:00",
        "comment_author": "yeahyung",
        "comment_body": "@krrishdholakia do you mean if all the values(batch_cost, batch_usage, batch_models) is not None, then set it to result?\r\n\r\nI don't think it's working\r\n\r\nif batch_cost, batch_usage, batch_models is None, then all([None, None, None]) => it's False\r\nFalse is not None => True, so result is set None",
        "pr_file_module": null
      },
      {
        "comment_id": "2262303800",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12125,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2262290032",
        "commented_code": "@@ -1997,18 +1998,32 @@ async def async_success_handler(  # noqa: PLR0915\n             return\n \n         ## CALCULATE COST FOR BATCH JOBS\n-        if (\n-            self.call_type == CallTypes.aretrieve_batch.value\n-            and isinstance(result, LiteLLMBatch)\n-            and result.status == \"completed\"\n+        if self.call_type == CallTypes.aretrieve_batch.value and isinstance(\n+            result, LiteLLMBatch\n         ):\n+            litellm_params = self.litellm_params or {}\n+            litellm_metadata = litellm_params.get(\"litellm_metadata\", {})\n+            if (\n+                litellm_metadata.get(\"batch_ignore_default_logging\", False) is True\n+            ):  # polling job will query these frequently, don't spam db logs\n+                return\n+\n             from litellm.proxy.openai_files_endpoints.common_utils import (\n                 _is_base64_encoded_unified_file_id,\n             )\n \n             # check if file id is a unified file id\n             is_base64_unified_file_id = _is_base64_encoded_unified_file_id(result.id)\n-            if not is_base64_unified_file_id:  # only run for non-unified file ids\n+\n+            batch_cost = kwargs.get(\"batch_cost\", None)\n+            batch_usage = kwargs.get(\"batch_usage\", None)\n+            batch_models = kwargs.get(\"batch_models\", None)\n+            if all([batch_cost, batch_usage, batch_models]) is not None:\n+                result._hidden_params[\"response_cost\"] = batch_cost\n+                result._hidden_params[\"batch_models\"] = batch_models\n+                result.usage = batch_usage",
        "comment_created_at": "2025-08-08T08:32:20+00:00",
        "comment_author": "yeahyung",
        "comment_body": "I think `if all([batch_cost, batch_usage, batch_models]):` might work as expected",
        "pr_file_module": null
      },
      {
        "comment_id": "2289641113",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12125,
        "pr_file": "litellm/litellm_core_utils/litellm_logging.py",
        "discussion_id": "2262290032",
        "commented_code": "@@ -1997,18 +1998,32 @@ async def async_success_handler(  # noqa: PLR0915\n             return\n \n         ## CALCULATE COST FOR BATCH JOBS\n-        if (\n-            self.call_type == CallTypes.aretrieve_batch.value\n-            and isinstance(result, LiteLLMBatch)\n-            and result.status == \"completed\"\n+        if self.call_type == CallTypes.aretrieve_batch.value and isinstance(\n+            result, LiteLLMBatch\n         ):\n+            litellm_params = self.litellm_params or {}\n+            litellm_metadata = litellm_params.get(\"litellm_metadata\", {})\n+            if (\n+                litellm_metadata.get(\"batch_ignore_default_logging\", False) is True\n+            ):  # polling job will query these frequently, don't spam db logs\n+                return\n+\n             from litellm.proxy.openai_files_endpoints.common_utils import (\n                 _is_base64_encoded_unified_file_id,\n             )\n \n             # check if file id is a unified file id\n             is_base64_unified_file_id = _is_base64_encoded_unified_file_id(result.id)\n-            if not is_base64_unified_file_id:  # only run for non-unified file ids\n+\n+            batch_cost = kwargs.get(\"batch_cost\", None)\n+            batch_usage = kwargs.get(\"batch_usage\", None)\n+            batch_models = kwargs.get(\"batch_models\", None)\n+            if all([batch_cost, batch_usage, batch_models]) is not None:\n+                result._hidden_params[\"response_cost\"] = batch_cost\n+                result._hidden_params[\"batch_models\"] = batch_models\n+                result.usage = batch_usage",
        "comment_created_at": "2025-08-21T01:20:01+00:00",
        "comment_author": "yeahyung",
        "comment_body": "@krrishdholakia can you please check it?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1463291905",
    "pr_number": 1514,
    "pr_file": "litellm/llms/ollama_chat.py",
    "created_at": "2024-01-23T13:35:23+00:00",
    "commented_code": "and v is not None\n         }\n \n+# Usage metrics are only populated when the ollama response indicates `\"done\": true`\n+# https://github.com/jmorganca/ollama/blob/main/docs/api.md#generate-a-chat-completion\n+class OllamaChatUsage:\n+    def __init__(self, response_json):\n+        self.response_json = response_json\n+\n+    def get_usage(self):\n+        if self.response_json[\"done\"] == False:\n+            return litellm.Usage(\n+                prompt_tokens=None,\n+                completion_tokens=None,\n+                total_tokens=None,\n+            )\n+\n+        prompt_tokens = self.response_json[\"prompt_eval_count\"]",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1463291905",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 1514,
        "pr_file": "litellm/llms/ollama_chat.py",
        "discussion_id": "1463291905",
        "commented_code": "@@ -120,6 +120,28 @@ def get_config(cls):\n             and v is not None\n         }\n \n+# Usage metrics are only populated when the ollama response indicates `\"done\": true`\n+# https://github.com/jmorganca/ollama/blob/main/docs/api.md#generate-a-chat-completion\n+class OllamaChatUsage:\n+    def __init__(self, response_json):\n+        self.response_json = response_json\n+\n+    def get_usage(self):\n+        if self.response_json[\"done\"] == False:\n+            return litellm.Usage(\n+                prompt_tokens=None,\n+                completion_tokens=None,\n+                total_tokens=None,\n+            )\n+\n+        prompt_tokens = self.response_json[\"prompt_eval_count\"]",
        "comment_created_at": "2024-01-23T13:35:23+00:00",
        "comment_author": "ALERTua",
        "comment_body": "`prompt_eval_count` cannot be found, but there is `eval_count` in the JSON response.\r\n![image](https://github.com/BerriAI/litellm/assets/8375129/81da2539-b9f1-4d4b-8499-dd795c8cb2ed)\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1604900050",
    "pr_number": 3665,
    "pr_file": "litellm/router_strategy/lowest_tpm_rpm_v2.py",
    "created_at": "2024-05-17T12:14:04+00:00",
    "commented_code": "elif item_tpm is None:\n                 continue  # skip if unhealthy deployment\n \n-            _deployment_tpm = None\n-            if _deployment_tpm is None:\n-                _deployment_tpm = _deployment.get(\"tpm\")\n-            if _deployment_tpm is None:\n-                _deployment_tpm = _deployment.get(\"litellm_params\", {}).get(\"tpm\")\n-            if _deployment_tpm is None:\n-                _deployment_tpm = _deployment.get(\"model_info\", {}).get(\"tpm\")\n-            if _deployment_tpm is None:\n-                _deployment_tpm = float(\"inf\")\n-\n-            _deployment_rpm = None\n-            if _deployment_rpm is None:\n-                _deployment_rpm = _deployment.get(\"rpm\")\n-            if _deployment_rpm is None:\n-                _deployment_rpm = _deployment.get(\"litellm_params\", {}).get(\"rpm\")\n-            if _deployment_rpm is None:\n-                _deployment_rpm = _deployment.get(\"model_info\", {}).get(\"rpm\")\n-            if _deployment_rpm is None:\n-                _deployment_rpm = float(\"inf\")\n+            _deployment_tpm = (\n+                        _deployment.get(\"tpm\") or\n+                        _deployment.get(\"litellm_params\", {}).get(\"tpm\") or\n+                        _deployment.get(\"model_info\", {}).get(\"tpm\") or\n+                        float(\"inf\")\n+                    )\n+",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1604900050",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 3665,
        "pr_file": "litellm/router_strategy/lowest_tpm_rpm_v2.py",
        "discussion_id": "1604900050",
        "commented_code": "@@ -346,25 +346,19 @@ def _common_checks_available_deployment(\n             elif item_tpm is None:\n                 continue  # skip if unhealthy deployment\n \n-            _deployment_tpm = None\n-            if _deployment_tpm is None:\n-                _deployment_tpm = _deployment.get(\"tpm\")\n-            if _deployment_tpm is None:\n-                _deployment_tpm = _deployment.get(\"litellm_params\", {}).get(\"tpm\")\n-            if _deployment_tpm is None:\n-                _deployment_tpm = _deployment.get(\"model_info\", {}).get(\"tpm\")\n-            if _deployment_tpm is None:\n-                _deployment_tpm = float(\"inf\")\n-\n-            _deployment_rpm = None\n-            if _deployment_rpm is None:\n-                _deployment_rpm = _deployment.get(\"rpm\")\n-            if _deployment_rpm is None:\n-                _deployment_rpm = _deployment.get(\"litellm_params\", {}).get(\"rpm\")\n-            if _deployment_rpm is None:\n-                _deployment_rpm = _deployment.get(\"model_info\", {}).get(\"rpm\")\n-            if _deployment_rpm is None:\n-                _deployment_rpm = float(\"inf\")\n+            _deployment_tpm = (\n+                        _deployment.get(\"tpm\") or\n+                        _deployment.get(\"litellm_params\", {}).get(\"tpm\") or\n+                        _deployment.get(\"model_info\", {}).get(\"tpm\") or\n+                        float(\"inf\")\n+                    )\n+",
        "comment_created_at": "2024-05-17T12:14:04+00:00",
        "comment_author": "paneru-rajan",
        "comment_body": "IMO, this is not the exactly the same as before.\r\n\r\nif `_deployment = {\"tpm\": 0}` then previously  `_deployment_tpm` will be `0` whereas in your case you will look for next place. \r\n\r\n`_deployment_tpm is not None` just checks for None, if you use `or`, then other Falsely values will be affected like, `''`, `[]`, `{}` `0` etc.\r\n\r\nIf this is only the original intention, only then this should be merged with through testing.\r\n\r\nCC: @krrishdholakia ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2217006515",
    "pr_number": 12750,
    "pr_file": "litellm/caching/caching.py",
    "created_at": "2025-07-18T22:42:06+00:00",
    "commented_code": "except Exception as e:\n             verbose_logger.exception(f\"LiteLLM Cache: Excepton add_cache: {str(e)}\")\n \n+    def _convert_to_cached_embedding(self, embedding_response: Any, model: Optional[str]) -> CachedEmbedding:\n+        \"\"\"\n+        Convert any embedding response into the standardized CachedEmbedding TypedDict format.\n+        \"\"\"\n+        try:\n+            if isinstance(embedding_response, dict):\n+                return {\n+                    \"embedding\": embedding_response[\"embedding\"],\n+                    \"index\": embedding_response[\"index\"],\n+                    \"object\": embedding_response[\"object\"],\n+                    \"model\": model,\n+                }\n+            elif hasattr(embedding_response, 'model_dump'):\n+                data = embedding_response.model_dump()\n+                return {\n+                    \"embedding\": data[\"embedding\"],\n+                    \"index\": data[\"index\"],\n+                    \"object\": data[\"object\"],\n+                    \"model\": model,\n+                }\n+            else:\n+                data = vars(embedding_response)\n+                return {\n+                    \"embedding\": data[\"embedding\"],",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2217006515",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12750,
        "pr_file": "litellm/caching/caching.py",
        "discussion_id": "2217006515",
        "commented_code": "@@ -590,6 +598,38 @@ async def async_add_cache(self, result, **kwargs):\n         except Exception as e:\n             verbose_logger.exception(f\"LiteLLM Cache: Excepton add_cache: {str(e)}\")\n \n+    def _convert_to_cached_embedding(self, embedding_response: Any, model: Optional[str]) -> CachedEmbedding:\n+        \"\"\"\n+        Convert any embedding response into the standardized CachedEmbedding TypedDict format.\n+        \"\"\"\n+        try:\n+            if isinstance(embedding_response, dict):\n+                return {\n+                    \"embedding\": embedding_response[\"embedding\"],\n+                    \"index\": embedding_response[\"index\"],\n+                    \"object\": embedding_response[\"object\"],\n+                    \"model\": model,\n+                }\n+            elif hasattr(embedding_response, 'model_dump'):\n+                data = embedding_response.model_dump()\n+                return {\n+                    \"embedding\": data[\"embedding\"],\n+                    \"index\": data[\"index\"],\n+                    \"object\": data[\"object\"],\n+                    \"model\": model,\n+                }\n+            else:\n+                data = vars(embedding_response)\n+                return {\n+                    \"embedding\": data[\"embedding\"],",
        "comment_created_at": "2025-07-18T22:42:06+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "edit this to data.get(\"embedding\") do this for all fields ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2157959655",
    "pr_number": 11892,
    "pr_file": "litellm/llms/watsonx/completion/transformation.py",
    "created_at": "2025-06-20T03:15:06+00:00",
    "commented_code": "model_response.choices[0].finish_reason = map_finish_reason(\n             json_resp[\"results\"][0][\"stop_reason\"]\n         )\n+        if json_resp[\"results\"][0][\"moderations\"]:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2157959655",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11892,
        "pr_file": "litellm/llms/watsonx/completion/transformation.py",
        "discussion_id": "2157959655",
        "commented_code": "@@ -299,6 +299,11 @@ def transform_response(\n         model_response.choices[0].finish_reason = map_finish_reason(\n             json_resp[\"results\"][0][\"stop_reason\"]\n         )\n+        if json_resp[\"results\"][0][\"moderations\"]:",
        "comment_created_at": "2025-06-20T03:15:06+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "this looks like you are assuming results are always greater than 0 and moderations always exist. Can we add safety checks before ",
        "pr_file_module": null
      },
      {
        "comment_id": "2162005835",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11892,
        "pr_file": "litellm/llms/watsonx/completion/transformation.py",
        "discussion_id": "2157959655",
        "commented_code": "@@ -299,6 +299,11 @@ def transform_response(\n         model_response.choices[0].finish_reason = map_finish_reason(\n             json_resp[\"results\"][0][\"stop_reason\"]\n         )\n+        if json_resp[\"results\"][0][\"moderations\"]:",
        "comment_created_at": "2025-06-23T16:21:11+00:00",
        "comment_author": "cbjuan",
        "comment_body": "Done in https://github.com/BerriAI/litellm/pull/11892/commits/b1a3900150880763f754b206868848af19189fb9",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2157959928",
    "pr_number": 11892,
    "pr_file": "litellm/llms/watsonx/completion/transformation.py",
    "created_at": "2025-06-20T03:15:34+00:00",
    "commented_code": "finish_reason = results[0].get(\"stop_reason\")\n                 is_finished = finish_reason != \"not_finished\"\n \n+                provider_specific_fields = None\n+                if results[0][\"moderations\"]:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2157959928",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11892,
        "pr_file": "litellm/llms/watsonx/completion/transformation.py",
        "discussion_id": "2157959928",
        "commented_code": "@@ -371,6 +376,12 @@ def chunk_parser(self, chunk: dict) -> GenericStreamingChunk:\n                 finish_reason = results[0].get(\"stop_reason\")\n                 is_finished = finish_reason != \"not_finished\"\n \n+                provider_specific_fields = None\n+                if results[0][\"moderations\"]:",
        "comment_created_at": "2025-06-20T03:15:34+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "can we again check if len(results)>0 and safely access moderations ",
        "pr_file_module": null
      },
      {
        "comment_id": "2162005710",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11892,
        "pr_file": "litellm/llms/watsonx/completion/transformation.py",
        "discussion_id": "2157959928",
        "commented_code": "@@ -371,6 +376,12 @@ def chunk_parser(self, chunk: dict) -> GenericStreamingChunk:\n                 finish_reason = results[0].get(\"stop_reason\")\n                 is_finished = finish_reason != \"not_finished\"\n \n+                provider_specific_fields = None\n+                if results[0][\"moderations\"]:",
        "comment_created_at": "2025-06-23T16:21:07+00:00",
        "comment_author": "cbjuan",
        "comment_body": "Done in https://github.com/BerriAI/litellm/pull/11892/commits/b1a3900150880763f754b206868848af19189fb9",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2169355475",
    "pr_number": 12068,
    "pr_file": "litellm/llms/anthropic/chat/transformation.py",
    "created_at": "2025-06-26T15:35:06+00:00",
    "commented_code": "_litellm_metadata\n             and isinstance(_litellm_metadata, dict)\n             and \"user_id\" in _litellm_metadata\n-            and not _valid_user_id(_litellm_metadata.get(\"user_id\", None))\n+            and not _valid_user_id(cast(str, _litellm_metadata.get(\"user_id\", None)))",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2169355475",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12068,
        "pr_file": "litellm/llms/anthropic/chat/transformation.py",
        "discussion_id": "2169355475",
        "commented_code": "@@ -708,7 +708,7 @@ def transform_request(\n             _litellm_metadata\n             and isinstance(_litellm_metadata, dict)\n             and \"user_id\" in _litellm_metadata\n-            and not _valid_user_id(_litellm_metadata.get(\"user_id\", None))\n+            and not _valid_user_id(cast(str, _litellm_metadata.get(\"user_id\", None)))",
        "comment_created_at": "2025-06-26T15:35:06+00:00",
        "comment_author": "Gum-Joe",
        "comment_body": "Note: Had to change this to pass pre-commit type checks",
        "pr_file_module": null
      },
      {
        "comment_id": "2169455355",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12068,
        "pr_file": "litellm/llms/anthropic/chat/transformation.py",
        "discussion_id": "2169355475",
        "commented_code": "@@ -708,7 +708,7 @@ def transform_request(\n             _litellm_metadata\n             and isinstance(_litellm_metadata, dict)\n             and \"user_id\" in _litellm_metadata\n-            and not _valid_user_id(_litellm_metadata.get(\"user_id\", None))\n+            and not _valid_user_id(cast(str, _litellm_metadata.get(\"user_id\", None)))",
        "comment_created_at": "2025-06-26T16:30:57+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "this cast is wrong - as we could pass a None value to _valid_user_id ",
        "pr_file_module": null
      },
      {
        "comment_id": "2169460595",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12068,
        "pr_file": "litellm/llms/anthropic/chat/transformation.py",
        "discussion_id": "2169355475",
        "commented_code": "@@ -708,7 +708,7 @@ def transform_request(\n             _litellm_metadata\n             and isinstance(_litellm_metadata, dict)\n             and \"user_id\" in _litellm_metadata\n-            and not _valid_user_id(_litellm_metadata.get(\"user_id\", None))\n+            and not _valid_user_id(cast(str, _litellm_metadata.get(\"user_id\", None)))",
        "comment_created_at": "2025-06-26T16:34:23+00:00",
        "comment_author": "Gum-Joe",
        "comment_body": "Makes sense - problem is the underlying function is annotated to only accept `str` not `None`:\r\n![image](https://github.com/user-attachments/assets/b55c9b01-8c7d-4c1a-91e3-f355fb0f3650)\r\n\r\nFix options:\r\n0. Remove cast, bypass pre-commit type checker errors with `--no-verify` to keep scope closed\r\n1. Update signature of `_valid_user_id` to accept `None` and maybe add a `if user_id is None: return True` to the start of it\r\n2. Something else\r\n\r\n(and thank you for such a fast review!)",
        "pr_file_module": null
      },
      {
        "comment_id": "2169463790",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12068,
        "pr_file": "litellm/llms/anthropic/chat/transformation.py",
        "discussion_id": "2169355475",
        "commented_code": "@@ -708,7 +708,7 @@ def transform_request(\n             _litellm_metadata\n             and isinstance(_litellm_metadata, dict)\n             and \"user_id\" in _litellm_metadata\n-            and not _valid_user_id(_litellm_metadata.get(\"user_id\", None))\n+            and not _valid_user_id(cast(str, _litellm_metadata.get(\"user_id\", None)))",
        "comment_created_at": "2025-06-26T16:36:24+00:00",
        "comment_author": "Gum-Joe",
        "comment_body": "Wait sorry, realised the actual problem - need to do a None check like the ones I added to usage data, didn't read the code properly, sorry!",
        "pr_file_module": null
      },
      {
        "comment_id": "2169466407",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 12068,
        "pr_file": "litellm/llms/anthropic/chat/transformation.py",
        "discussion_id": "2169355475",
        "commented_code": "@@ -708,7 +708,7 @@ def transform_request(\n             _litellm_metadata\n             and isinstance(_litellm_metadata, dict)\n             and \"user_id\" in _litellm_metadata\n-            and not _valid_user_id(_litellm_metadata.get(\"user_id\", None))\n+            and not _valid_user_id(cast(str, _litellm_metadata.get(\"user_id\", None)))",
        "comment_created_at": "2025-06-26T16:38:12+00:00",
        "comment_author": "Gum-Joe",
        "comment_body": "Fixed in 243d4ba",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1872228188",
    "pr_number": 7049,
    "pr_file": "litellm/router.py",
    "created_at": "2024-12-05T22:13:28+00:00",
    "commented_code": "model=kwargs[\"model\"]\n             )\n             kwargs[\"model\"] = deployment[\"litellm_params\"][\"model\"]\n+            kwargs[\"api_key\"] = deployment[\"litellm_params\"][\"api_key\"]",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1872228188",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7049,
        "pr_file": "litellm/router.py",
        "discussion_id": "1872228188",
        "commented_code": "@@ -2569,6 +2569,7 @@ async def _pass_through_moderation_endpoint_factory(\n                 model=kwargs[\"model\"]\n             )\n             kwargs[\"model\"] = deployment[\"litellm_params\"][\"model\"]\n+            kwargs[\"api_key\"] = deployment[\"litellm_params\"][\"api_key\"]",
        "comment_created_at": "2024-12-05T22:13:28+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "i don't believe api_key is a guaranteed field of `litellm_params` did this pass your lint check? ",
        "pr_file_module": null
      },
      {
        "comment_id": "1872373677",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 7049,
        "pr_file": "litellm/router.py",
        "discussion_id": "1872228188",
        "commented_code": "@@ -2569,6 +2569,7 @@ async def _pass_through_moderation_endpoint_factory(\n                 model=kwargs[\"model\"]\n             )\n             kwargs[\"model\"] = deployment[\"litellm_params\"][\"model\"]\n+            kwargs[\"api_key\"] = deployment[\"litellm_params\"][\"api_key\"]",
        "comment_created_at": "2024-12-06T00:12:55+00:00",
        "comment_author": "karter-liner",
        "comment_body": "@krrishdholakia lint check via `poetry run flake8` didn't return error\r\nif you think that param is not guaranteed, how about implementing like below?\r\n\r\n```\r\n    if deployment[\"litellm_params\"].get(\"api_key\") is not None:\r\n        kwargs[\"api_key\"] = deployment[\"litellm_params\"][\"api_key\"]\r\n\r\nor\r\n\r\n    if \"api_key\" in deployment[\"litellm_params\"]:\r\n        kwargs[\"api_key\"] = deployment[\"litellm_params\"][\"api_key\"]\r\n```",
        "pr_file_module": null
      }
    ]
  }
]