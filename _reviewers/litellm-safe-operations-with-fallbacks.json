[
  {
    "discussion_id": "2276872070",
    "pr_number": 13615,
    "pr_file": "litellm/proxy/spend_tracking/cold_storage_handler.py",
    "created_at": "2025-08-14T14:53:41+00:00",
    "commented_code": "\"\"\"\n \n         try:\n-            from litellm.proxy.proxy_server import general_settings\n+            from litellm.proxy.general_settings import general_settings\n         except Exception as e:\n             verbose_proxy_logger.debug(\n-                f\"Unable to import proxy_server for cold storage logging: {e}\"\n+                f\"Unable to import general_settings for cold storage logging: {e}\"\n             )\n             return None",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2276872070",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 13615,
        "pr_file": "litellm/proxy/spend_tracking/cold_storage_handler.py",
        "discussion_id": "2276872070",
        "commented_code": "@@ -73,10 +73,10 @@ def _get_configured_cold_storage_custom_logger() -> Optional[_custom_logger_comp\n         \"\"\"\n \n         try:\n-            from litellm.proxy.proxy_server import general_settings\n+            from litellm.proxy.general_settings import general_settings\n         except Exception as e:\n             verbose_proxy_logger.debug(\n-                f\"Unable to import proxy_server for cold storage logging: {e}\"\n+                f\"Unable to import general_settings for cold storage logging: {e}\"\n             )\n             return None",
        "comment_created_at": "2025-08-14T14:53:41+00:00",
        "comment_author": "frosforever",
        "comment_body": "Ideally this `try` would be removed as this should not ever throw! The import was previously throwing because top level code in `proxy_server` was throwing. However, this was codified in a test as part of https://github.com/BerriAI/litellm/pull/13436 that patches the import to throw and then ensures that it's caught. There's no harm in keeping this `try` and the test but IMHO it's not great. Will leave that decision whether to keep it with the maintainers. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2165055929",
    "pr_number": 11540,
    "pr_file": "tests/local_testing/test_completion_cost.py",
    "created_at": "2025-06-24T22:32:01+00:00",
    "commented_code": "assert cost == 0.019922944\n \n \n+def test_cost_gpt_image_1_token_based():\n+    \"\"\"Test GPT-image-1 token-based cost calculation with actual response data\"\"\"\n+    import json\n+    from litellm import ImageResponse, Usage\n+    \n+    # Load the local model cost map directly\n+    with open('model_prices_and_context_window.json', 'r') as f:\n+        litellm.model_cost = json.load(f)\n+    \n+    # Create mock image responses with actual token usage based on quality\n+    # Token counts based on gpt-image-1 quality levels (from original test expectations)\n+    low_response = ImageResponse(\n+        created=1234567890,\n+        data=[{\"url\": \"http://example.com/image.png\"}]\n+    )\n+    low_response.usage = Usage(prompt_tokens=50, completion_tokens=272, total_tokens=322)\n+    \n+    medium_response = ImageResponse(\n+        created=1234567890,\n+        data=[{\"url\": \"http://example.com/image.png\"}]\n+    )\n+    medium_response.usage = Usage(prompt_tokens=50, completion_tokens=1056, total_tokens=1106)\n+    \n+    high_response = ImageResponse(\n+        created=1234567890,\n+        data=[{\"url\": \"http://example.com/image.png\"}]\n+    )\n+    high_response.usage = Usage(prompt_tokens=50, completion_tokens=4160, total_tokens=4210)\n+    \n+    # Test different quality levels with actual response data\n+    cost_low = litellm.completion_cost(",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2165055929",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 11540,
        "pr_file": "tests/local_testing/test_completion_cost.py",
        "discussion_id": "2165055929",
        "commented_code": "@@ -319,6 +319,80 @@ def test_cost_openai_image_gen():\n     assert cost == 0.019922944\n \n \n+def test_cost_gpt_image_1_token_based():\n+    \"\"\"Test GPT-image-1 token-based cost calculation with actual response data\"\"\"\n+    import json\n+    from litellm import ImageResponse, Usage\n+    \n+    # Load the local model cost map directly\n+    with open('model_prices_and_context_window.json', 'r') as f:\n+        litellm.model_cost = json.load(f)\n+    \n+    # Create mock image responses with actual token usage based on quality\n+    # Token counts based on gpt-image-1 quality levels (from original test expectations)\n+    low_response = ImageResponse(\n+        created=1234567890,\n+        data=[{\"url\": \"http://example.com/image.png\"}]\n+    )\n+    low_response.usage = Usage(prompt_tokens=50, completion_tokens=272, total_tokens=322)\n+    \n+    medium_response = ImageResponse(\n+        created=1234567890,\n+        data=[{\"url\": \"http://example.com/image.png\"}]\n+    )\n+    medium_response.usage = Usage(prompt_tokens=50, completion_tokens=1056, total_tokens=1106)\n+    \n+    high_response = ImageResponse(\n+        created=1234567890,\n+        data=[{\"url\": \"http://example.com/image.png\"}]\n+    )\n+    high_response.usage = Usage(prompt_tokens=50, completion_tokens=4160, total_tokens=4210)\n+    \n+    # Test different quality levels with actual response data\n+    cost_low = litellm.completion_cost(",
        "comment_created_at": "2025-06-24T22:32:01+00:00",
        "comment_author": "krrishdholakia",
        "comment_body": "add try-excepts around this - so if the tests fail due to openai internal server errors / rate limit errors - it doesn't stop ci/cd ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1988262207",
    "pr_number": 9118,
    "pr_file": "litellm/proxy/proxy_server.py",
    "created_at": "2025-03-11T02:19:46+00:00",
    "commented_code": "\"x-litellm-model-api-base\": api_base,\n         \"x-litellm-version\": version,\n         \"x-litellm-model-region\": model_region,\n-        \"x-litellm-response-cost\": str(response_cost),\n+        \"x-litellm-response-cost\": str(round(response_cost, 6)),",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "1988262207",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 9118,
        "pr_file": "litellm/proxy/proxy_server.py",
        "discussion_id": "1988262207",
        "commented_code": "@@ -806,7 +806,7 @@ def get_custom_headers(\n         \"x-litellm-model-api-base\": api_base,\n         \"x-litellm-version\": version,\n         \"x-litellm-model-region\": model_region,\n-        \"x-litellm-response-cost\": str(response_cost),\n+        \"x-litellm-response-cost\": str(round(response_cost, 6)),",
        "comment_created_at": "2025-03-11T02:19:46+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "use a helper static method here please, can it be a safe method (if it fails it still returns str(response cost) \r\n\r\nmy concern is getting the rounded response cost fails, the llm response should not fail ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089429744",
    "pr_number": 10832,
    "pr_file": "litellm/proxy/litellm_pre_call_utils.py",
    "created_at": "2025-05-14T17:37:21+00:00",
    "commented_code": "if data.get(_metadata_variable_name, None) is None:\n         data[_metadata_variable_name] = {}\n \n-    # We want to log the \"metadata\" from the client side request. Avoid circular reference by not directly assigning metadata to itself.\n+    # Parse metadata if it's a string (e.g., from multipart/form-data)\n     if \"metadata\" in data and data[\"metadata\"] is not None:\n+        if isinstance(data[\"metadata\"], str):\n+            import json\n+            try:",
    "repo_full_name": "BerriAI/litellm",
    "discussion_comments": [
      {
        "comment_id": "2089429744",
        "repo_full_name": "BerriAI/litellm",
        "pr_number": 10832,
        "pr_file": "litellm/proxy/litellm_pre_call_utils.py",
        "discussion_id": "2089429744",
        "commented_code": "@@ -533,8 +533,14 @@ async def add_litellm_data_to_request(  # noqa: PLR0915\n     if data.get(_metadata_variable_name, None) is None:\n         data[_metadata_variable_name] = {}\n \n-    # We want to log the \"metadata\" from the client side request. Avoid circular reference by not directly assigning metadata to itself.\n+    # Parse metadata if it's a string (e.g., from multipart/form-data)\n     if \"metadata\" in data and data[\"metadata\"] is not None:\n+        if isinstance(data[\"metadata\"], str):\n+            import json\n+            try:",
        "comment_created_at": "2025-05-14T17:37:21+00:00",
        "comment_author": "ishaan-jaff",
        "comment_body": "create a helper in litellm core utils \r\n\r\nsafe json loads \r\n\r\n(we have one for safe json dumps) ",
        "pr_file_module": null
      }
    ]
  }
]