[
  {
    "discussion_id": "2234119261",
    "pr_number": 14905,
    "pr_file": "tools/quantize/README.md",
    "created_at": "2025-07-27T20:07:57+00:00",
    "commented_code": "./llama-cli -m ./models/mymodel/ggml-model-Q4_K_M.gguf -cnv -p \"You are a helpful assistant\"\n ```\n \n-When running the larger models, make sure you have enough disk space to store all the intermediate files.\n+Options:\n+* `--allow-requantize` allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n+* `--leave-output-tensor` will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\n+* `--pure` disables k-quant mixtures and quantizes all tensors to the same type\n+* `--imatrix` uses data in file as importance matrix for quant optimizations (highly recommended)",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2234119261",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14905,
        "pr_file": "tools/quantize/README.md",
        "discussion_id": "2234119261",
        "commented_code": "@@ -37,40 +44,117 @@ Run the quantized model:\n ./llama-cli -m ./models/mymodel/ggml-model-Q4_K_M.gguf -cnv -p \"You are a helpful assistant\"\n ```\n \n-When running the larger models, make sure you have enough disk space to store all the intermediate files.\n+Options:\n+* `--allow-requantize` allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n+* `--leave-output-tensor` will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\n+* `--pure` disables k-quant mixtures and quantizes all tensors to the same type\n+* `--imatrix` uses data in file as importance matrix for quant optimizations (highly recommended)",
        "comment_created_at": "2025-07-27T20:07:57+00:00",
        "comment_author": "CISC",
        "comment_body": "```suggestion\r\n* `--imatrix` uses data in file generated by `llama-imatrix` as importance matrix for quant optimizations (highly recommended)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2217414689",
    "pr_number": 12718,
    "pr_file": "tools/imatrix/README.md",
    "created_at": "2025-07-19T18:51:06+00:00",
    "commented_code": "# llama.cpp/tools/imatrix\n \n Compute an importance matrix for a model and given text dataset. Can be used during quantization to enhance the quality of the quantized models.\n-More information is available here: https://github.com/ggml-org/llama.cpp/pull/4861\n+More information is [available here](https://github.com/ggml-org/llama.cpp/pull/4861)\n \n ## Usage\n \n ```\n ./llama-imatrix \\\n-    -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] \\\n-    [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\n-    [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\n-    [--parse-special]\n+    -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] [--no-ppl] \\\n+    [--chunk 123] [--output-frequency 10] [--save-frequency 0] [--show-statistics] \\\n+    [--no-ppl] [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\n+    [--parse-special] [...]\n ```\n \n-Here `-m` with a model name and `-f` with a file containing training data (such as e.g. `wiki.train.raw`) are mandatory.\n+Here `-m | --model` with a model name and `-f | --file` with a file containing calibration data (such as e.g. `wiki.train.raw`) are mandatory.\n The parameters in square brackets are optional and have the following meaning:\n-* `-o` (or `--output-file`) specifies the name of the file where the computed data will be stored. If missing `imatrix.gguf` is used.\n-* `--verbosity` specifies the verbosity level. If set to `0`, no output other than the perplexity of the processed chunks will be generated. If set to `1`, each time the results are saved a message is written to `stderr`. If `>=2`, a message is output each time data is collected for any tensor. Default verbosity level is `1`.\n-* `--output-frequency` specifies how often the so far computed result is saved to disk. Default is 10 (i.e., every 10 chunks)\n+\n+* `-h | --help` shows usage information and exits.\n+* `-lv | --verbosity` specifies the verbosity level. If set to `0`, no output other than the perplexity of the processed chunks will be generated. If set to `1`, each time the results are saved a message is written to `stderr`. If `>=2`, a message is output each time data is collected for any tensor. Default verbosity level is `1`.\n+* `-o | --output-file` specifies the name of the file where the computed data will be stored. If missing `imatrix.gguf` is used.\n+* `-ofreq | --output-frequency` specifies how often the so far computed result is saved to disk. Default is 10 (i.e., every 10 chunks)\n * `--save-frequency` specifies how often to save a copy of the imatrix in a separate file. Default is 0 (i.e., never)\n-* `--process-output` specifies if data will be collected for the `output.weight` tensor. My experience is that it is better to not utilize the importance matrix when quantizing `output.weight`, so this is set to `false` by default.\n+* `--process-output` specifies if data will be collected for the `output.weight` tensor. Typically, it is better not to utilize the importance matrix when quantizing `output.weight`, so this is set to `false` by default.\n+* `--in-file` one or more existing imatrix files to load and combine. Useful for merging files from multiple runs/datasets.\n+* `--parse-special` enables parsing of special tokens (e.g., `<|im_start|>` in some models). Useful for models with custom tokenizers.\n+* `--chunk` to skip the first `n` chunks of tokens from the input data. Useful for resuming or skipping initial low-quality data.",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2217414689",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 12718,
        "pr_file": "tools/imatrix/README.md",
        "discussion_id": "2217414689",
        "commented_code": "@@ -1,34 +1,92 @@\n # llama.cpp/tools/imatrix\n \n Compute an importance matrix for a model and given text dataset. Can be used during quantization to enhance the quality of the quantized models.\n-More information is available here: https://github.com/ggml-org/llama.cpp/pull/4861\n+More information is [available here](https://github.com/ggml-org/llama.cpp/pull/4861)\n \n ## Usage\n \n ```\n ./llama-imatrix \\\n-    -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] \\\n-    [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\n-    [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\n-    [--parse-special]\n+    -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] [--no-ppl] \\\n+    [--chunk 123] [--output-frequency 10] [--save-frequency 0] [--show-statistics] \\\n+    [--no-ppl] [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\n+    [--parse-special] [...]\n ```\n \n-Here `-m` with a model name and `-f` with a file containing training data (such as e.g. `wiki.train.raw`) are mandatory.\n+Here `-m | --model` with a model name and `-f | --file` with a file containing calibration data (such as e.g. `wiki.train.raw`) are mandatory.\n The parameters in square brackets are optional and have the following meaning:\n-* `-o` (or `--output-file`) specifies the name of the file where the computed data will be stored. If missing `imatrix.gguf` is used.\n-* `--verbosity` specifies the verbosity level. If set to `0`, no output other than the perplexity of the processed chunks will be generated. If set to `1`, each time the results are saved a message is written to `stderr`. If `>=2`, a message is output each time data is collected for any tensor. Default verbosity level is `1`.\n-* `--output-frequency` specifies how often the so far computed result is saved to disk. Default is 10 (i.e., every 10 chunks)\n+\n+* `-h | --help` shows usage information and exits.\n+* `-lv | --verbosity` specifies the verbosity level. If set to `0`, no output other than the perplexity of the processed chunks will be generated. If set to `1`, each time the results are saved a message is written to `stderr`. If `>=2`, a message is output each time data is collected for any tensor. Default verbosity level is `1`.\n+* `-o | --output-file` specifies the name of the file where the computed data will be stored. If missing `imatrix.gguf` is used.\n+* `-ofreq | --output-frequency` specifies how often the so far computed result is saved to disk. Default is 10 (i.e., every 10 chunks)\n * `--save-frequency` specifies how often to save a copy of the imatrix in a separate file. Default is 0 (i.e., never)\n-* `--process-output` specifies if data will be collected for the `output.weight` tensor. My experience is that it is better to not utilize the importance matrix when quantizing `output.weight`, so this is set to `false` by default.\n+* `--process-output` specifies if data will be collected for the `output.weight` tensor. Typically, it is better not to utilize the importance matrix when quantizing `output.weight`, so this is set to `false` by default.\n+* `--in-file` one or more existing imatrix files to load and combine. Useful for merging files from multiple runs/datasets.\n+* `--parse-special` enables parsing of special tokens (e.g., `<|im_start|>` in some models). Useful for models with custom tokenizers.\n+* `--chunk` to skip the first `n` chunks of tokens from the input data. Useful for resuming or skipping initial low-quality data.",
        "comment_created_at": "2025-07-19T18:51:06+00:00",
        "comment_author": "compilade",
        "comment_body": "```suggestion\n* `--chunk | --from-chunk` to skip the first `n` chunks of tokens from the input data. Useful for resuming or skipping initial low-quality data.\n```\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2217483776",
    "pr_number": 12718,
    "pr_file": "tools/imatrix/README.md",
    "created_at": "2025-07-19T22:12:31+00:00",
    "commented_code": "# llama.cpp/tools/imatrix\n \n Compute an importance matrix for a model and given text dataset. Can be used during quantization to enhance the quality of the quantized models.\n-More information is available here: https://github.com/ggml-org/llama.cpp/pull/4861\n+More information is [available here](https://github.com/ggml-org/llama.cpp/pull/4861)\n \n ## Usage\n \n ```\n ./llama-imatrix \\\n-    -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] \\\n-    [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\n-    [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\n-    [--parse-special]\n+    -m model.gguf -f some-text.txt [-o imatrix.gguf] [--no-ppl] \\\n+    [--process-output] [--chunk 123] [--save-frequency 0] [--output-frequency 10] \\\n+    [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] [--parse-special] \\\n+    [--show-statistics] [...]\n ```\n \n-Here `-m` with a model name and `-f` with a file containing training data (such as e.g. `wiki.train.raw`) are mandatory.\n+Here `-m | --model` with a model name and `-f | --file` with a file containing calibration data (such as e.g. `wiki.train.raw`) are mandatory.\n The parameters in square brackets are optional and have the following meaning:\n-* `-o` (or `--output-file`) specifies the name of the file where the computed data will be stored. If missing `imatrix.gguf` is used.\n-* `--verbosity` specifies the verbosity level. If set to `0`, no output other than the perplexity of the processed chunks will be generated. If set to `1`, each time the results are saved a message is written to `stderr`. If `>=2`, a message is output each time data is collected for any tensor. Default verbosity level is `1`.\n-* `--output-frequency` specifies how often the so far computed result is saved to disk. Default is 10 (i.e., every 10 chunks)\n+\n+* `-h | --help` shows usage information and exits.\n+* `-lv | --verbosity` specifies the verbosity level. If set to `0`, no output other than the perplexity of the processed chunks will be generated. If set to `1`, each time the results are saved a message is written to `stderr`. If `>=2`, a message is output each time data is collected for any tensor. Default verbosity level is `1`.\n+* `-o | --output-file` specifies the name of the file where the computed data will be stored. If missing `imatrix.gguf` is used.\n+* `-ofreq | --output-frequency` specifies how often the so far computed result is saved to disk. Default is 10 (i.e., every 10 chunks)\n * `--save-frequency` specifies how often to save a copy of the imatrix in a separate file. Default is 0 (i.e., never)\n-* `--process-output` specifies if data will be collected for the `output.weight` tensor. My experience is that it is better to not utilize the importance matrix when quantizing `output.weight`, so this is set to `false` by default.\n+* `--process-output` specifies if data will be collected for the `output.weight` tensor. Typically, it is better not to utilize the importance matrix when quantizing `output.weight`, so this is set to `false` by default.\n+* `--in-file` one or more existing imatrix files to load and combine. Useful for merging files from multiple runs/datasets.\n+* `--parse-special` enables parsing of special tokens (e.g., `<|im_start|>` in some models). Useful for models with custom tokenizers.\n+* `--chunk | --from-chunk` to skip the first `n` chunks of tokens from the input data. Useful for resuming or skipping initial low-quality data.\n+* `--chunks` maximum number of chunks to process. Default is -1 for all available chunks.\n+* `--no-ppl` disables the calculation of perplexity for the processed chunks. Useful if you want to speed up the processing and do not care about perplexity.\n+* `--show-statistics` displays imatrix file's statistics.\n+\n+For faster computation, make sure to use GPU offloading via the `-ngl | --n-gpu-layers` argument.\n \n-For faster computation, make sure to use GPU offloading via the `-ngl` argument\n+Recent versions of `llama-imatrix` store data in GGUF format by default. For the legacy format, use an extension other than `.gguf` when saving the output file. More information is [available here](https://github.com/ggml-org/llama.cpp/pull/9400)",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2217483776",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 12718,
        "pr_file": "tools/imatrix/README.md",
        "discussion_id": "2217483776",
        "commented_code": "@@ -1,34 +1,92 @@\n # llama.cpp/tools/imatrix\n \n Compute an importance matrix for a model and given text dataset. Can be used during quantization to enhance the quality of the quantized models.\n-More information is available here: https://github.com/ggml-org/llama.cpp/pull/4861\n+More information is [available here](https://github.com/ggml-org/llama.cpp/pull/4861)\n \n ## Usage\n \n ```\n ./llama-imatrix \\\n-    -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] \\\n-    [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\n-    [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\n-    [--parse-special]\n+    -m model.gguf -f some-text.txt [-o imatrix.gguf] [--no-ppl] \\\n+    [--process-output] [--chunk 123] [--save-frequency 0] [--output-frequency 10] \\\n+    [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] [--parse-special] \\\n+    [--show-statistics] [...]\n ```\n \n-Here `-m` with a model name and `-f` with a file containing training data (such as e.g. `wiki.train.raw`) are mandatory.\n+Here `-m | --model` with a model name and `-f | --file` with a file containing calibration data (such as e.g. `wiki.train.raw`) are mandatory.\n The parameters in square brackets are optional and have the following meaning:\n-* `-o` (or `--output-file`) specifies the name of the file where the computed data will be stored. If missing `imatrix.gguf` is used.\n-* `--verbosity` specifies the verbosity level. If set to `0`, no output other than the perplexity of the processed chunks will be generated. If set to `1`, each time the results are saved a message is written to `stderr`. If `>=2`, a message is output each time data is collected for any tensor. Default verbosity level is `1`.\n-* `--output-frequency` specifies how often the so far computed result is saved to disk. Default is 10 (i.e., every 10 chunks)\n+\n+* `-h | --help` shows usage information and exits.\n+* `-lv | --verbosity` specifies the verbosity level. If set to `0`, no output other than the perplexity of the processed chunks will be generated. If set to `1`, each time the results are saved a message is written to `stderr`. If `>=2`, a message is output each time data is collected for any tensor. Default verbosity level is `1`.\n+* `-o | --output-file` specifies the name of the file where the computed data will be stored. If missing `imatrix.gguf` is used.\n+* `-ofreq | --output-frequency` specifies how often the so far computed result is saved to disk. Default is 10 (i.e., every 10 chunks)\n * `--save-frequency` specifies how often to save a copy of the imatrix in a separate file. Default is 0 (i.e., never)\n-* `--process-output` specifies if data will be collected for the `output.weight` tensor. My experience is that it is better to not utilize the importance matrix when quantizing `output.weight`, so this is set to `false` by default.\n+* `--process-output` specifies if data will be collected for the `output.weight` tensor. Typically, it is better not to utilize the importance matrix when quantizing `output.weight`, so this is set to `false` by default.\n+* `--in-file` one or more existing imatrix files to load and combine. Useful for merging files from multiple runs/datasets.\n+* `--parse-special` enables parsing of special tokens (e.g., `<|im_start|>` in some models). Useful for models with custom tokenizers.\n+* `--chunk | --from-chunk` to skip the first `n` chunks of tokens from the input data. Useful for resuming or skipping initial low-quality data.\n+* `--chunks` maximum number of chunks to process. Default is -1 for all available chunks.\n+* `--no-ppl` disables the calculation of perplexity for the processed chunks. Useful if you want to speed up the processing and do not care about perplexity.\n+* `--show-statistics` displays imatrix file's statistics.\n+\n+For faster computation, make sure to use GPU offloading via the `-ngl | --n-gpu-layers` argument.\n \n-For faster computation, make sure to use GPU offloading via the `-ngl` argument\n+Recent versions of `llama-imatrix` store data in GGUF format by default. For the legacy format, use an extension other than `.gguf` when saving the output file. More information is [available here](https://github.com/ggml-org/llama.cpp/pull/9400)",
        "comment_created_at": "2025-07-19T22:12:31+00:00",
        "comment_author": "compilade",
        "comment_body": "Same here.\r\n\r\n```suggestion\r\nRecent versions of `llama-imatrix` store data in GGUF format by default. For the legacy format, use an extension other than `.gguf` when saving the output file. More information is available in <https://github.com/ggml-org/llama.cpp/pull/9400>.\r\n```",
        "pr_file_module": null
      }
    ]
  }
]