[
  {
    "discussion_id": "2116346391",
    "pr_number": 13873,
    "pr_file": "ggml/include/ggml-opt.h",
    "created_at": "2025-05-30T17:55:03+00:00",
    "commented_code": "GGML_OPT_BUILD_TYPE_OPT     = 30,\n     };\n \n+    enum ggml_opt_optimizer {\n+        GGML_OPT_OPTIMIZER_ADAMW,\n+        GGML_OPT_OPTIMIZER_SGD,\n+\n+        GGML_OPT_OPTIMIZER_COUNT\n+    };\n+\n+    // \"adamw\" or \"sgd\" (case insensitive)\n+    GGML_API const char *            ggml_opt_optimizer_name(enum ggml_opt_optimizer);\n+    GGML_API enum ggml_opt_optimizer named_ggml_opt_optimizer(const char *);\n+\n     // parameters that control which optimizer is used and how said optimizer tries to find the minimal loss\n     struct ggml_opt_optimizer_params {\n         // AdamW optimizer parameters\n         struct {\n             float alpha; // learning rate\n-            float beta1;\n-            float beta2;\n+            float beta1;  // adamw\n+            float beta2;  // adamw\n             float eps;   // epsilon for numerical stability\n-            float wd;    // weight decay for AdamW, use 0.0f to disable\n+            float wd;    // weight decay for SGD or AdamW, use 0.0f to disable\n         } adamw;\n+\n+        // only GGML_OPT_OPTIMIZER_ADMW allocates m, v per parameter\n+        enum ggml_opt_optimizer optimizer;\n+\n+        // affects finetune.cpp only so far:\n+        unsigned                epochs;  // max # of epochs sampling over training data",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2116346391",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/include/ggml-opt.h",
        "discussion_id": "2116346391",
        "commented_code": "@@ -74,16 +74,33 @@ extern \"C\" {\n         GGML_OPT_BUILD_TYPE_OPT     = 30,\n     };\n \n+    enum ggml_opt_optimizer {\n+        GGML_OPT_OPTIMIZER_ADAMW,\n+        GGML_OPT_OPTIMIZER_SGD,\n+\n+        GGML_OPT_OPTIMIZER_COUNT\n+    };\n+\n+    // \"adamw\" or \"sgd\" (case insensitive)\n+    GGML_API const char *            ggml_opt_optimizer_name(enum ggml_opt_optimizer);\n+    GGML_API enum ggml_opt_optimizer named_ggml_opt_optimizer(const char *);\n+\n     // parameters that control which optimizer is used and how said optimizer tries to find the minimal loss\n     struct ggml_opt_optimizer_params {\n         // AdamW optimizer parameters\n         struct {\n             float alpha; // learning rate\n-            float beta1;\n-            float beta2;\n+            float beta1;  // adamw\n+            float beta2;  // adamw\n             float eps;   // epsilon for numerical stability\n-            float wd;    // weight decay for AdamW, use 0.0f to disable\n+            float wd;    // weight decay for SGD or AdamW, use 0.0f to disable\n         } adamw;\n+\n+        // only GGML_OPT_OPTIMIZER_ADMW allocates m, v per parameter\n+        enum ggml_opt_optimizer optimizer;\n+\n+        // affects finetune.cpp only so far:\n+        unsigned                epochs;  // max # of epochs sampling over training data",
        "comment_created_at": "2025-05-30T17:55:03+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "The number of epochs is not a property of the optimizer.",
        "pr_file_module": null
      },
      {
        "comment_id": "2116890312",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/include/ggml-opt.h",
        "discussion_id": "2116346391",
        "commented_code": "@@ -74,16 +74,33 @@ extern \"C\" {\n         GGML_OPT_BUILD_TYPE_OPT     = 30,\n     };\n \n+    enum ggml_opt_optimizer {\n+        GGML_OPT_OPTIMIZER_ADAMW,\n+        GGML_OPT_OPTIMIZER_SGD,\n+\n+        GGML_OPT_OPTIMIZER_COUNT\n+    };\n+\n+    // \"adamw\" or \"sgd\" (case insensitive)\n+    GGML_API const char *            ggml_opt_optimizer_name(enum ggml_opt_optimizer);\n+    GGML_API enum ggml_opt_optimizer named_ggml_opt_optimizer(const char *);\n+\n     // parameters that control which optimizer is used and how said optimizer tries to find the minimal loss\n     struct ggml_opt_optimizer_params {\n         // AdamW optimizer parameters\n         struct {\n             float alpha; // learning rate\n-            float beta1;\n-            float beta2;\n+            float beta1;  // adamw\n+            float beta2;  // adamw\n             float eps;   // epsilon for numerical stability\n-            float wd;    // weight decay for AdamW, use 0.0f to disable\n+            float wd;    // weight decay for SGD or AdamW, use 0.0f to disable\n         } adamw;\n+\n+        // only GGML_OPT_OPTIMIZER_ADMW allocates m, v per parameter\n+        enum ggml_opt_optimizer optimizer;\n+\n+        // affects finetune.cpp only so far:\n+        unsigned                epochs;  // max # of epochs sampling over training data",
        "comment_created_at": "2025-05-31T00:14:03+00:00",
        "comment_author": "graehl",
        "comment_body": "ok, suppose optimizer convergence criteria (how long to run) should go elsewhere. where do you recommend?",
        "pr_file_module": null
      },
      {
        "comment_id": "2118951179",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/include/ggml-opt.h",
        "discussion_id": "2116346391",
        "commented_code": "@@ -74,16 +74,33 @@ extern \"C\" {\n         GGML_OPT_BUILD_TYPE_OPT     = 30,\n     };\n \n+    enum ggml_opt_optimizer {\n+        GGML_OPT_OPTIMIZER_ADAMW,\n+        GGML_OPT_OPTIMIZER_SGD,\n+\n+        GGML_OPT_OPTIMIZER_COUNT\n+    };\n+\n+    // \"adamw\" or \"sgd\" (case insensitive)\n+    GGML_API const char *            ggml_opt_optimizer_name(enum ggml_opt_optimizer);\n+    GGML_API enum ggml_opt_optimizer named_ggml_opt_optimizer(const char *);\n+\n     // parameters that control which optimizer is used and how said optimizer tries to find the minimal loss\n     struct ggml_opt_optimizer_params {\n         // AdamW optimizer parameters\n         struct {\n             float alpha; // learning rate\n-            float beta1;\n-            float beta2;\n+            float beta1;  // adamw\n+            float beta2;  // adamw\n             float eps;   // epsilon for numerical stability\n-            float wd;    // weight decay for AdamW, use 0.0f to disable\n+            float wd;    // weight decay for SGD or AdamW, use 0.0f to disable\n         } adamw;\n+\n+        // only GGML_OPT_OPTIMIZER_ADMW allocates m, v per parameter\n+        enum ggml_opt_optimizer optimizer;\n+\n+        // affects finetune.cpp only so far:\n+        unsigned                epochs;  // max # of epochs sampling over training data",
        "comment_created_at": "2025-06-01T09:48:07+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "It should be in \"user code\". The concept of an epoch only makes sense in conjunction with a dataset, so it should be added in functions like `ggml_opt_epoch` or `llama_opt_epoch` where there is explicit iteration over a dataset.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2132356077",
    "pr_number": 14014,
    "pr_file": "src/llama-graph.h",
    "created_at": "2025-06-06T14:57:01+00:00",
    "commented_code": "const llama_hparams & hparams;\n     const llama_cparams & cparams;\n+    const int n_swa;  // Sliding window attention size (0 = disabled)",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2132356077",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14014,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2132356077",
        "commented_code": "@@ -241,6 +249,7 @@ class llm_graph_input_attn_no_cache : public llm_graph_input_i {\n \n     const llama_hparams & hparams;\n     const llama_cparams & cparams;\n+    const int n_swa;  // Sliding window attention size (0 = disabled)",
        "comment_created_at": "2025-06-06T14:57:01+00:00",
        "comment_author": "ggerganov",
        "comment_body": "This is already available from the `hparams` - no need to duplicate it here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2204662726",
    "pr_number": 14644,
    "pr_file": "common/common.h",
    "created_at": "2025-07-14T11:26:17+00:00",
    "commented_code": "bool use_guide_tokens = false; // enable guide tokens to improve TTS accuracy            // NOLINT\n };\n \n+struct common_params_diffusion {\n+    int32_t steps       = 64;     // number of diffusion steps\n+    float   eps         = 1e-3f;  // epsilon for timesteps\n+    int32_t algorithm   = 0;      // diffusion algorithm (0=ORIGIN, 1=MASKGIT_PLUS, 2=TOPK_MARGIN, 3=ENTROPY)\n+    float   alg_temp    = 0.0f;   // algorithm temperature\n+    int32_t max_length  = 512;    // maximum generation length",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2204662726",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14644,
        "pr_file": "common/common.h",
        "discussion_id": "2204662726",
        "commented_code": "@@ -217,6 +218,15 @@ struct common_params_vocoder {\n     bool use_guide_tokens = false; // enable guide tokens to improve TTS accuracy            // NOLINT\n };\n \n+struct common_params_diffusion {\n+    int32_t steps       = 64;     // number of diffusion steps\n+    float   eps         = 1e-3f;  // epsilon for timesteps\n+    int32_t algorithm   = 0;      // diffusion algorithm (0=ORIGIN, 1=MASKGIT_PLUS, 2=TOPK_MARGIN, 3=ENTROPY)\n+    float   alg_temp    = 0.0f;   // algorithm temperature\n+    int32_t max_length  = 512;    // maximum generation length",
        "comment_created_at": "2025-07-14T11:26:17+00:00",
        "comment_author": "ggerganov",
        "comment_body": "`max_length` should be removed and the existing `n_ubatch` parameter should be used instead.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190845930",
    "pr_number": 14534,
    "pr_file": "src/llama-hparams.h",
    "created_at": "2025-07-07T19:10:46+00:00",
    "commented_code": "uint32_t ssm_d_state = 0;\n     uint32_t ssm_dt_rank = 0;\n     uint32_t ssm_n_group = 0;\n+    uint32_t ssm_head_dim   = 0;\n+    uint32_t ssm_mamba_d_ssm = 0;\n+\n+    uint32_t attn_head_dim            = 0;\n+    bool     mamba_rms_norm           = false;\n+    uint32_t vocab_size               = 0;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2190845930",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14534,
        "pr_file": "src/llama-hparams.h",
        "discussion_id": "2190845930",
        "commented_code": "@@ -115,6 +115,17 @@ struct llama_hparams {\n     uint32_t ssm_d_state = 0;\n     uint32_t ssm_dt_rank = 0;\n     uint32_t ssm_n_group = 0;\n+    uint32_t ssm_head_dim   = 0;\n+    uint32_t ssm_mamba_d_ssm = 0;\n+\n+    uint32_t attn_head_dim            = 0;\n+    bool     mamba_rms_norm           = false;\n+    uint32_t vocab_size               = 0;",
        "comment_created_at": "2025-07-07T19:10:46+00:00",
        "comment_author": "ggerganov",
        "comment_body": "Normally, we don't put the `vocab_size` as `hparam`. Instead, we pick it from the `llama_vocab`. So this is likely not needed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190849576",
    "pr_number": 14534,
    "pr_file": "src/llama-hparams.h",
    "created_at": "2025-07-07T19:13:20+00:00",
    "commented_code": "uint32_t ssm_d_state = 0;\n     uint32_t ssm_dt_rank = 0;\n     uint32_t ssm_n_group = 0;\n+    uint32_t ssm_head_dim   = 0;\n+    uint32_t ssm_mamba_d_ssm = 0;\n+\n+    uint32_t attn_head_dim            = 0;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2190849576",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14534,
        "pr_file": "src/llama-hparams.h",
        "discussion_id": "2190849576",
        "commented_code": "@@ -115,6 +115,17 @@ struct llama_hparams {\n     uint32_t ssm_d_state = 0;\n     uint32_t ssm_dt_rank = 0;\n     uint32_t ssm_n_group = 0;\n+    uint32_t ssm_head_dim   = 0;\n+    uint32_t ssm_mamba_d_ssm = 0;\n+\n+    uint32_t attn_head_dim            = 0;",
        "comment_created_at": "2025-07-07T19:13:20+00:00",
        "comment_author": "ggerganov",
        "comment_body": "This parameter should can be avoided.\r\n\r\nSee the logic here:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/a3403ae3502b5f4d562b727bca8c810a3b212198/src/llama-model.cpp#L529-L538\r\n\r\nAnd as an example how we apply it for the Gemma model which can have a custom attention head size like in your case:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/a3403ae3502b5f4d562b727bca8c810a3b212198/src/llama-model.cpp#L1021-L1025",
        "pr_file_module": null
      }
    ]
  }
]