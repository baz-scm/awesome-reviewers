[
  {
    "discussion_id": "2202422006",
    "pr_number": 14644,
    "pr_file": "src/llama-context.cpp",
    "created_at": "2025-07-12T07:52:14+00:00",
    "commented_code": "return ret;\n }\n \n+//\n+// diffusion generation\n+//\n+\n+llama_diffusion_params llama_diffusion_default_params() {\n+    llama_diffusion_params params = {};\n+    params.steps                  = 64;     // Match Python: 64 steps\n+    params.eps                    = 1e-3f;  // Match Python: eps = 0.001\n+    params.temperature            = 0.2f;\n+    params.top_p                  = 1.0f;\n+    params.top_k                  = 0;\n+    params.mask_token_id          = 151666;                                               // Default Dream mask token\n+    params.algorithm              = llama_diffusion_params::LLAMA_DIFFUSION_ALG_ENTROPY;  // Match Python: alg='entropy'\n+    params.alg_temp               = 0.0f;                                                 // Match Python: alg_temp=0.0\n+    params.step_callback          = nullptr;\n+    params.step_callback_user_data = nullptr;\n+    params.seed                    = 0;\n+    return params;\n+}\n+\n+llama_token * llama_diffusion_generate(llama_context * ctx, const llama_token * input_tokens, int32_t n_input,\n+                                       int32_t max_length, llama_diffusion_params params, int32_t * n_generated) {\n+    if (!ctx || !input_tokens || n_input <= 0 || max_length <= n_input) {",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2202422006",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14644,
        "pr_file": "src/llama-context.cpp",
        "discussion_id": "2202422006",
        "commented_code": "@@ -2780,6 +2784,310 @@ int32_t llama_decode(\n     return ret;\n }\n \n+//\n+// diffusion generation\n+//\n+\n+llama_diffusion_params llama_diffusion_default_params() {\n+    llama_diffusion_params params = {};\n+    params.steps                  = 64;     // Match Python: 64 steps\n+    params.eps                    = 1e-3f;  // Match Python: eps = 0.001\n+    params.temperature            = 0.2f;\n+    params.top_p                  = 1.0f;\n+    params.top_k                  = 0;\n+    params.mask_token_id          = 151666;                                               // Default Dream mask token\n+    params.algorithm              = llama_diffusion_params::LLAMA_DIFFUSION_ALG_ENTROPY;  // Match Python: alg='entropy'\n+    params.alg_temp               = 0.0f;                                                 // Match Python: alg_temp=0.0\n+    params.step_callback          = nullptr;\n+    params.step_callback_user_data = nullptr;\n+    params.seed                    = 0;\n+    return params;\n+}\n+\n+llama_token * llama_diffusion_generate(llama_context * ctx, const llama_token * input_tokens, int32_t n_input,\n+                                       int32_t max_length, llama_diffusion_params params, int32_t * n_generated) {\n+    if (!ctx || !input_tokens || n_input <= 0 || max_length <= n_input) {",
        "comment_created_at": "2025-07-12T07:52:14+00:00",
        "comment_author": "ggerganov",
        "comment_body": "What is the reason to implement this inside the `llama_context` as opposed to in the diffusion example itself? If we do so, the user would have much more control over the inference and we won't have to design around this new API.",
        "pr_file_module": null
      },
      {
        "comment_id": "2202424539",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14644,
        "pr_file": "src/llama-context.cpp",
        "discussion_id": "2202422006",
        "commented_code": "@@ -2780,6 +2784,310 @@ int32_t llama_decode(\n     return ret;\n }\n \n+//\n+// diffusion generation\n+//\n+\n+llama_diffusion_params llama_diffusion_default_params() {\n+    llama_diffusion_params params = {};\n+    params.steps                  = 64;     // Match Python: 64 steps\n+    params.eps                    = 1e-3f;  // Match Python: eps = 0.001\n+    params.temperature            = 0.2f;\n+    params.top_p                  = 1.0f;\n+    params.top_k                  = 0;\n+    params.mask_token_id          = 151666;                                               // Default Dream mask token\n+    params.algorithm              = llama_diffusion_params::LLAMA_DIFFUSION_ALG_ENTROPY;  // Match Python: alg='entropy'\n+    params.alg_temp               = 0.0f;                                                 // Match Python: alg_temp=0.0\n+    params.step_callback          = nullptr;\n+    params.step_callback_user_data = nullptr;\n+    params.seed                    = 0;\n+    return params;\n+}\n+\n+llama_token * llama_diffusion_generate(llama_context * ctx, const llama_token * input_tokens, int32_t n_input,\n+                                       int32_t max_length, llama_diffusion_params params, int32_t * n_generated) {\n+    if (!ctx || !input_tokens || n_input <= 0 || max_length <= n_input) {",
        "comment_created_at": "2025-07-12T07:57:05+00:00",
        "comment_author": "am17an",
        "comment_body": "Users are free to do that still, this is just a wrapper for easier user experience. i.e contains all the sampling methods in one place",
        "pr_file_module": null
      },
      {
        "comment_id": "2202509796",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14644,
        "pr_file": "src/llama-context.cpp",
        "discussion_id": "2202422006",
        "commented_code": "@@ -2780,6 +2784,310 @@ int32_t llama_decode(\n     return ret;\n }\n \n+//\n+// diffusion generation\n+//\n+\n+llama_diffusion_params llama_diffusion_default_params() {\n+    llama_diffusion_params params = {};\n+    params.steps                  = 64;     // Match Python: 64 steps\n+    params.eps                    = 1e-3f;  // Match Python: eps = 0.001\n+    params.temperature            = 0.2f;\n+    params.top_p                  = 1.0f;\n+    params.top_k                  = 0;\n+    params.mask_token_id          = 151666;                                               // Default Dream mask token\n+    params.algorithm              = llama_diffusion_params::LLAMA_DIFFUSION_ALG_ENTROPY;  // Match Python: alg='entropy'\n+    params.alg_temp               = 0.0f;                                                 // Match Python: alg_temp=0.0\n+    params.step_callback          = nullptr;\n+    params.step_callback_user_data = nullptr;\n+    params.seed                    = 0;\n+    return params;\n+}\n+\n+llama_token * llama_diffusion_generate(llama_context * ctx, const llama_token * input_tokens, int32_t n_input,\n+                                       int32_t max_length, llama_diffusion_params params, int32_t * n_generated) {\n+    if (!ctx || !input_tokens || n_input <= 0 || max_length <= n_input) {",
        "comment_created_at": "2025-07-12T10:53:11+00:00",
        "comment_author": "ggerganov",
        "comment_body": "It should be moved to the example. The `libllama` API should be as minimal as possible without redundant interfaces. Wrappers are implemented in user code (e.g. `libcommon`/`examples`/`tools`).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2200764140",
    "pr_number": 14622,
    "pr_file": "common/arg.cpp",
    "created_at": "2025-07-11T13:32:44+00:00",
    "commented_code": "}\n     ).set_examples({LLAMA_EXAMPLE_SERVER}));\n \n+    add_opt(common_arg(\n+        {\"--dataset-format\"}, \" \",\n+        string_format(\"type of input data (e.g., 'text', 'parquet') (default: %s)\", params.dataset_format.c_str()),\n+        [](common_params & params, const std::string & format) {\n+            params.dataset_format = format; //TODO ENUM CLASS\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--max-seq-len\"}, \" \",\n+        string_format(\"max sequence length (default: %d)\", params.max_seq_len),\n+        [](common_params & params, int32_t max_seq_len) {\n+            params.max_seq_len = max_seq_len;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--pre-tokenized\"},\n+        string_format(\"input file contains pre-tokenized data (space-separated token IDs)\"),\n+        [](common_params & params) {\n+            params.pre_tokenized = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--preview\"},\n+        string_format(\"read and print metadata and first sequence from the output GGUF file (enables preview)\"),\n+        [](common_params & params) {\n+            params.do_preview = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+   add_opt(common_arg(\n+        {\"--preview-count\"}, \"<N>\",\n+        string_format(\"input file contains pre-tokenized data (space-separated token IDs)\"),\n+        [](common_params & params, int preview_count) {\n+            params.preview_count = preview_count;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--detokenize-preview\"},\n+        string_format(\"detokenize previewed sequences (implies --preview)\"),\n+        [](common_params & params) {\n+            params.detokenize_preview = params.do_preview = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2200764140",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14622,
        "pr_file": "common/arg.cpp",
        "discussion_id": "2200764140",
        "commented_code": "@@ -3423,5 +3423,73 @@ common_params_context common_params_parser_init(common_params & params, llama_ex\n         }\n     ).set_examples({LLAMA_EXAMPLE_SERVER}));\n \n+    add_opt(common_arg(\n+        {\"--dataset-format\"}, \" \",\n+        string_format(\"type of input data (e.g., 'text', 'parquet') (default: %s)\", params.dataset_format.c_str()),\n+        [](common_params & params, const std::string & format) {\n+            params.dataset_format = format; //TODO ENUM CLASS\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--max-seq-len\"}, \" \",\n+        string_format(\"max sequence length (default: %d)\", params.max_seq_len),\n+        [](common_params & params, int32_t max_seq_len) {\n+            params.max_seq_len = max_seq_len;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--pre-tokenized\"},\n+        string_format(\"input file contains pre-tokenized data (space-separated token IDs)\"),\n+        [](common_params & params) {\n+            params.pre_tokenized = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--preview\"},\n+        string_format(\"read and print metadata and first sequence from the output GGUF file (enables preview)\"),\n+        [](common_params & params) {\n+            params.do_preview = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+   add_opt(common_arg(\n+        {\"--preview-count\"}, \"<N>\",\n+        string_format(\"input file contains pre-tokenized data (space-separated token IDs)\"),\n+        [](common_params & params, int preview_count) {\n+            params.preview_count = preview_count;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--detokenize-preview\"},\n+        string_format(\"detokenize previewed sequences (implies --preview)\"),\n+        [](common_params & params) {\n+            params.detokenize_preview = params.do_preview = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));",
        "comment_created_at": "2025-07-11T13:32:44+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I would say it's preferable to keep the interface simple: add the `--preview` flag, if set it always shows a fixed number of sequences, both as tokens and as text.",
        "pr_file_module": null
      },
      {
        "comment_id": "2201502038",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14622,
        "pr_file": "common/arg.cpp",
        "discussion_id": "2200764140",
        "commented_code": "@@ -3423,5 +3423,73 @@ common_params_context common_params_parser_init(common_params & params, llama_ex\n         }\n     ).set_examples({LLAMA_EXAMPLE_SERVER}));\n \n+    add_opt(common_arg(\n+        {\"--dataset-format\"}, \" \",\n+        string_format(\"type of input data (e.g., 'text', 'parquet') (default: %s)\", params.dataset_format.c_str()),\n+        [](common_params & params, const std::string & format) {\n+            params.dataset_format = format; //TODO ENUM CLASS\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--max-seq-len\"}, \" \",\n+        string_format(\"max sequence length (default: %d)\", params.max_seq_len),\n+        [](common_params & params, int32_t max_seq_len) {\n+            params.max_seq_len = max_seq_len;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--pre-tokenized\"},\n+        string_format(\"input file contains pre-tokenized data (space-separated token IDs)\"),\n+        [](common_params & params) {\n+            params.pre_tokenized = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--preview\"},\n+        string_format(\"read and print metadata and first sequence from the output GGUF file (enables preview)\"),\n+        [](common_params & params) {\n+            params.do_preview = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+   add_opt(common_arg(\n+        {\"--preview-count\"}, \"<N>\",\n+        string_format(\"input file contains pre-tokenized data (space-separated token IDs)\"),\n+        [](common_params & params, int preview_count) {\n+            params.preview_count = preview_count;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));\n+\n+    add_opt(common_arg(\n+        {\"--detokenize-preview\"},\n+        string_format(\"detokenize previewed sequences (implies --preview)\"),\n+        [](common_params & params) {\n+            params.detokenize_preview = params.do_preview = true;\n+        }\n+    ).set_examples({LLAMA_EXAMPLE_FINETUNE}));",
        "comment_created_at": "2025-07-11T18:29:38+00:00",
        "comment_author": "lexasub",
        "comment_body": "ok, will remove --preview-count, --detokenize-preview",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1822171989",
    "pr_number": 9639,
    "pr_file": "common/sampling.cpp",
    "created_at": "2024-10-30T09:08:38+00:00",
    "commented_code": "return std::string(result);\n }\n \n+bool common_sampler_trigger_grammar(const struct llama_model * model, common_sampler * gsmpl, const std::string & trigger) {\n+    if (!llama_sampler_is_grammar_empty(gsmpl->grmr)) {\n+        return false;\n+    }\n+    gsmpl->grmr   = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\");\n+    llama_sampler_accept_str(gsmpl->grmr, trigger.c_str());\n+    return true;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "1822171989",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "common/sampling.cpp",
        "discussion_id": "1822171989",
        "commented_code": "@@ -139,14 +139,23 @@ std::string common_sampler_params::print() const {\n     return std::string(result);\n }\n \n+bool common_sampler_trigger_grammar(const struct llama_model * model, common_sampler * gsmpl, const std::string & trigger) {\n+    if (!llama_sampler_is_grammar_empty(gsmpl->grmr)) {\n+        return false;\n+    }\n+    gsmpl->grmr   = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\");\n+    llama_sampler_accept_str(gsmpl->grmr, trigger.c_str());\n+    return true;",
        "comment_created_at": "2024-10-30T09:08:38+00:00",
        "comment_author": "ggerganov",
        "comment_body": "If this is the only use case of `llama_sampler_accept_str()` maybe we can simply extend `llama_sampler_init_grammar()` to accept an initial string (i.e. the trigger) and avoid extending the API.",
        "pr_file_module": null
      },
      {
        "comment_id": "1823670443",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "common/sampling.cpp",
        "discussion_id": "1822171989",
        "commented_code": "@@ -139,14 +139,23 @@ std::string common_sampler_params::print() const {\n     return std::string(result);\n }\n \n+bool common_sampler_trigger_grammar(const struct llama_model * model, common_sampler * gsmpl, const std::string & trigger) {\n+    if (!llama_sampler_is_grammar_empty(gsmpl->grmr)) {\n+        return false;\n+    }\n+    gsmpl->grmr   = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\");\n+    llama_sampler_accept_str(gsmpl->grmr, trigger.c_str());\n+    return true;",
        "comment_created_at": "2024-10-31T01:42:00+00:00",
        "comment_author": "ochafik",
        "comment_body": "Oh, you mean to move the trigger logic in the grammar itself?\r\n\r\nTechnically the triggers are even already in the grammar. It would all be *neater* if we could write `root ::= .*? (\"trigger1\" ... | \"trigger2\" ...)` (e.g. treat the head literal that comes after a reluctant `.*` as a trigger; we could even have the grammar pause itself / allow temporarily unconstrained sequences, e.g. `root ::= prefix .*? suffix` if we collect the triggers = head literals recursively in the `suffix` subgraph).",
        "pr_file_module": null
      },
      {
        "comment_id": "1824101910",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "common/sampling.cpp",
        "discussion_id": "1822171989",
        "commented_code": "@@ -139,14 +139,23 @@ std::string common_sampler_params::print() const {\n     return std::string(result);\n }\n \n+bool common_sampler_trigger_grammar(const struct llama_model * model, common_sampler * gsmpl, const std::string & trigger) {\n+    if (!llama_sampler_is_grammar_empty(gsmpl->grmr)) {\n+        return false;\n+    }\n+    gsmpl->grmr   = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\");\n+    llama_sampler_accept_str(gsmpl->grmr, trigger.c_str());\n+    return true;",
        "comment_created_at": "2024-10-31T08:57:06+00:00",
        "comment_author": "ggerganov",
        "comment_body": "> Oh, you mean to move the trigger logic in the grammar itself?\r\n\r\nNo, I had in mind something more like this:\r\n\r\n```c\r\n// llama.h\r\n    LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\r\n            const struct llama_model * model,\r\n                          const char * grammar_str,\r\n                          const char * grammar_root,\r\n                          const char * init_str); // optionally used to feed initial data to the grammar state\r\n\r\n// common/sampling.cpp\r\n    gsmpl->grmr  = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\", trigger.c_str());\r\n    \r\n    return true;\r\n```\r\n\r\nThe logic for triggering remains in user code.",
        "pr_file_module": null
      },
      {
        "comment_id": "1824604724",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "common/sampling.cpp",
        "discussion_id": "1822171989",
        "commented_code": "@@ -139,14 +139,23 @@ std::string common_sampler_params::print() const {\n     return std::string(result);\n }\n \n+bool common_sampler_trigger_grammar(const struct llama_model * model, common_sampler * gsmpl, const std::string & trigger) {\n+    if (!llama_sampler_is_grammar_empty(gsmpl->grmr)) {\n+        return false;\n+    }\n+    gsmpl->grmr   = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\");\n+    llama_sampler_accept_str(gsmpl->grmr, trigger.c_str());\n+    return true;",
        "comment_created_at": "2024-10-31T14:46:24+00:00",
        "comment_author": "ochafik",
        "comment_body": "Ah, thanks. Looks like this gets called when the server slot is set up, which is too early to know which trigger to pass. I don't like what the two step, lazy triggering does to the api, but I think the best alternative might be to pass all the *possible* triggers and let the grammar handle the lazy feature (seems logical to live there anyway). I'll fiddle around :-)",
        "pr_file_module": null
      },
      {
        "comment_id": "1924615148",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "common/sampling.cpp",
        "discussion_id": "1822171989",
        "commented_code": "@@ -139,14 +139,23 @@ std::string common_sampler_params::print() const {\n     return std::string(result);\n }\n \n+bool common_sampler_trigger_grammar(const struct llama_model * model, common_sampler * gsmpl, const std::string & trigger) {\n+    if (!llama_sampler_is_grammar_empty(gsmpl->grmr)) {\n+        return false;\n+    }\n+    gsmpl->grmr   = llama_sampler_init_grammar(model, gsmpl->params.grammar.c_str(), \"root\");\n+    llama_sampler_accept_str(gsmpl->grmr, trigger.c_str());\n+    return true;",
        "comment_created_at": "2025-01-22T02:40:21+00:00",
        "comment_author": "ochafik",
        "comment_body": "Dropped llama_sampler_accept_str.\r\n\r\nllama_grammar_accept_impl now has the trigger logic.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189543123",
    "pr_number": 14544,
    "pr_file": "tools/server/server.cpp",
    "created_at": "2025-07-07T09:52:32+00:00",
    "commented_code": "}\n \n     // register API routes\n-    svr->Get (\"/health\",              handle_health); // public endpoint (no API key check)\n-    svr->Get (\"/metrics\",             handle_metrics);\n-    svr->Get (\"/props\",               handle_props);\n-    svr->Post(\"/props\",               handle_props_change);\n-    svr->Post(\"/api/show\",            handle_api_show);\n-    svr->Get (\"/models\",              handle_models); // public endpoint (no API key check)\n-    svr->Get (\"/v1/models\",           handle_models); // public endpoint (no API key check)\n-    svr->Get (\"/api/tags\",            handle_models); // ollama specific endpoint. public endpoint (no API key check)\n-    svr->Post(\"/completion\",          handle_completions); // legacy\n-    svr->Post(\"/completions\",         handle_completions);\n-    svr->Post(\"/v1/completions\",      handle_completions_oai);\n-    svr->Post(\"/chat/completions\",    handle_chat_completions);\n-    svr->Post(\"/v1/chat/completions\", handle_chat_completions);\n-    svr->Post(\"/api/chat\",            handle_chat_completions); // ollama specific endpoint\n-    svr->Post(\"/infill\",              handle_infill);\n-    svr->Post(\"/embedding\",           handle_embeddings); // legacy\n-    svr->Post(\"/embeddings\",          handle_embeddings);\n-    svr->Post(\"/v1/embeddings\",       handle_embeddings_oai);\n-    svr->Post(\"/rerank\",              handle_rerank);\n-    svr->Post(\"/reranking\",           handle_rerank);\n-    svr->Post(\"/v1/rerank\",           handle_rerank);\n-    svr->Post(\"/v1/reranking\",        handle_rerank);\n-    svr->Post(\"/tokenize\",            handle_tokenize);\n-    svr->Post(\"/detokenize\",          handle_detokenize);\n-    svr->Post(\"/apply-template\",      handle_apply_template);\n+    svr->Get (string_format(\"%s/health\", server_prefix),              handle_health); // public endpoint (no API key check)",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2189543123",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14544,
        "pr_file": "tools/server/server.cpp",
        "discussion_id": "2189543123",
        "commented_code": "@@ -4829,37 +4831,37 @@ int main(int argc, char ** argv) {\n     }\n \n     // register API routes\n-    svr->Get (\"/health\",              handle_health); // public endpoint (no API key check)\n-    svr->Get (\"/metrics\",             handle_metrics);\n-    svr->Get (\"/props\",               handle_props);\n-    svr->Post(\"/props\",               handle_props_change);\n-    svr->Post(\"/api/show\",            handle_api_show);\n-    svr->Get (\"/models\",              handle_models); // public endpoint (no API key check)\n-    svr->Get (\"/v1/models\",           handle_models); // public endpoint (no API key check)\n-    svr->Get (\"/api/tags\",            handle_models); // ollama specific endpoint. public endpoint (no API key check)\n-    svr->Post(\"/completion\",          handle_completions); // legacy\n-    svr->Post(\"/completions\",         handle_completions);\n-    svr->Post(\"/v1/completions\",      handle_completions_oai);\n-    svr->Post(\"/chat/completions\",    handle_chat_completions);\n-    svr->Post(\"/v1/chat/completions\", handle_chat_completions);\n-    svr->Post(\"/api/chat\",            handle_chat_completions); // ollama specific endpoint\n-    svr->Post(\"/infill\",              handle_infill);\n-    svr->Post(\"/embedding\",           handle_embeddings); // legacy\n-    svr->Post(\"/embeddings\",          handle_embeddings);\n-    svr->Post(\"/v1/embeddings\",       handle_embeddings_oai);\n-    svr->Post(\"/rerank\",              handle_rerank);\n-    svr->Post(\"/reranking\",           handle_rerank);\n-    svr->Post(\"/v1/rerank\",           handle_rerank);\n-    svr->Post(\"/v1/reranking\",        handle_rerank);\n-    svr->Post(\"/tokenize\",            handle_tokenize);\n-    svr->Post(\"/detokenize\",          handle_detokenize);\n-    svr->Post(\"/apply-template\",      handle_apply_template);\n+    svr->Get (string_format(\"%s/health\", server_prefix),              handle_health); // public endpoint (no API key check)",
        "comment_created_at": "2025-07-07T09:52:32+00:00",
        "comment_author": "ngxson",
        "comment_body": "using string format here is quite overkill. you can simply make `server_prefix` a `std::string`, then:\r\n\r\n```suggestion\r\n    svr->Get (server_prefix + \"/health\",              handle_health); // public endpoint (no API key check)\r\n```",
        "pr_file_module": null
      }
    ]
  }
]