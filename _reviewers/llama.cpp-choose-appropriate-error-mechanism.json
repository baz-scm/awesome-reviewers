[
  {
    "discussion_id": "2234131267",
    "pr_number": 14898,
    "pr_file": "src/llama-model.cpp",
    "created_at": "2025-07-27T20:55:27+00:00",
    "commented_code": "}\n                     }\n                 } break;\n+            case LLM_ARCH_SMALLTHINKER:\n+                {\n+                    tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), { n_embd, n_vocab }, 0);\n+\n+                    // output\n+                    output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), { n_embd }, 0);\n+                    output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, TENSOR_NOT_REQUIRED);\n+\n+                    // if output is NULL, init from the input tok embed\n+                    if (output == NULL) {\n+                        output = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, TENSOR_DUPLICATED);\n+                    }\n+\n+                    for (int i = 0; i < n_layer; ++i) {\n+                        auto &        layer    = layers[i];\n+\n+                        layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), { n_embd }, 0);\n+\n+                        layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q, \"weight\", i), { n_embd, n_embd_head_k * n_head }, 0);\n+                        layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K, \"weight\", i), { n_embd, n_embd_gqa }, 0);\n+                        layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V, \"weight\", i), { n_embd, n_embd_gqa }, 0);\n+                        layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), { n_embd_head_k * n_head, n_embd }, 0);\n+\n+                        layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), { n_embd }, 0);\n+\n+                        if (n_expert == 0) {\n+                            throw std::runtime_error(\"n_expert must be > 0 for SMALLTHINKER\");\n+                        }\n+                        if (n_expert_used == 0) {\n+                            throw std::runtime_error(\"n_expert_used must be > 0 for SMALLTHINKER\");\n+                        }",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2234131267",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14898,
        "pr_file": "src/llama-model.cpp",
        "discussion_id": "2234131267",
        "commented_code": "@@ -5165,6 +5188,46 @@ bool llama_model::load_tensors(llama_model_loader & ml) {\n                         }\n                     }\n                 } break;\n+            case LLM_ARCH_SMALLTHINKER:\n+                {\n+                    tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), { n_embd, n_vocab }, 0);\n+\n+                    // output\n+                    output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), { n_embd }, 0);\n+                    output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, TENSOR_NOT_REQUIRED);\n+\n+                    // if output is NULL, init from the input tok embed\n+                    if (output == NULL) {\n+                        output = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, TENSOR_DUPLICATED);\n+                    }\n+\n+                    for (int i = 0; i < n_layer; ++i) {\n+                        auto &        layer    = layers[i];\n+\n+                        layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), { n_embd }, 0);\n+\n+                        layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q, \"weight\", i), { n_embd, n_embd_head_k * n_head }, 0);\n+                        layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K, \"weight\", i), { n_embd, n_embd_gqa }, 0);\n+                        layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V, \"weight\", i), { n_embd, n_embd_gqa }, 0);\n+                        layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), { n_embd_head_k * n_head, n_embd }, 0);\n+\n+                        layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), { n_embd }, 0);\n+\n+                        if (n_expert == 0) {\n+                            throw std::runtime_error(\"n_expert must be > 0 for SMALLTHINKER\");\n+                        }\n+                        if (n_expert_used == 0) {\n+                            throw std::runtime_error(\"n_expert_used must be > 0 for SMALLTHINKER\");\n+                        }",
        "comment_created_at": "2025-07-27T20:55:27+00:00",
        "comment_author": "CISC",
        "comment_body": "```suggestion\r\n                        GGML_ASSERT(n_expert > 0 && \"n_expert must be > 0 for SMALLTHINKER\");\r\n                        GGML_ASSERT(n_expert_used > 0 && \"n_expert_used must be > 0 for SMALLTHINKER\");\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2234217641",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14898,
        "pr_file": "src/llama-model.cpp",
        "discussion_id": "2234131267",
        "commented_code": "@@ -5165,6 +5188,46 @@ bool llama_model::load_tensors(llama_model_loader & ml) {\n                         }\n                     }\n                 } break;\n+            case LLM_ARCH_SMALLTHINKER:\n+                {\n+                    tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), { n_embd, n_vocab }, 0);\n+\n+                    // output\n+                    output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), { n_embd }, 0);\n+                    output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, TENSOR_NOT_REQUIRED);\n+\n+                    // if output is NULL, init from the input tok embed\n+                    if (output == NULL) {\n+                        output = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, TENSOR_DUPLICATED);\n+                    }\n+\n+                    for (int i = 0; i < n_layer; ++i) {\n+                        auto &        layer    = layers[i];\n+\n+                        layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), { n_embd }, 0);\n+\n+                        layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q, \"weight\", i), { n_embd, n_embd_head_k * n_head }, 0);\n+                        layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K, \"weight\", i), { n_embd, n_embd_gqa }, 0);\n+                        layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V, \"weight\", i), { n_embd, n_embd_gqa }, 0);\n+                        layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), { n_embd_head_k * n_head, n_embd }, 0);\n+\n+                        layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), { n_embd }, 0);\n+\n+                        if (n_expert == 0) {\n+                            throw std::runtime_error(\"n_expert must be > 0 for SMALLTHINKER\");\n+                        }\n+                        if (n_expert_used == 0) {\n+                            throw std::runtime_error(\"n_expert_used must be > 0 for SMALLTHINKER\");\n+                        }",
        "comment_created_at": "2025-07-28T01:00:52+00:00",
        "comment_author": "wdl339",
        "comment_body": "That's a good point. We initially followed the pattern from another model's implementation in the codebase, which is why we used throw. I agree that GGML_ASSERT is more appropriate here. Thanks for catching this.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2209453662",
    "pr_number": 14700,
    "pr_file": "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp",
    "created_at": "2025-07-16T06:57:36+00:00",
    "commented_code": "public:\n     int repack(struct ggml_tensor * tensor, const void * data, size_t data_size) {\n-        GGML_ASSERT(ctx.kernels);\n+        if (!ctx.kernels) {\n+            return -1;  // No suitable kernel available\n+        }",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2209453662",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14700,
        "pr_file": "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp",
        "discussion_id": "2209453662",
        "commented_code": "@@ -344,7 +350,9 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n \n public:\n     int repack(struct ggml_tensor * tensor, const void * data, size_t data_size) {\n-        GGML_ASSERT(ctx.kernels);\n+        if (!ctx.kernels) {\n+            return -1;  // No suitable kernel available\n+        }",
        "comment_created_at": "2025-07-16T06:57:36+00:00",
        "comment_author": "chaxu01",
        "comment_body": "Replacing GGML_ASSERT(ctx.kernels) with a return value silently fails if no kernel is available, but there's no fallback. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2212492072",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14700,
        "pr_file": "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp",
        "discussion_id": "2209453662",
        "commented_code": "@@ -344,7 +350,9 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n \n public:\n     int repack(struct ggml_tensor * tensor, const void * data, size_t data_size) {\n-        GGML_ASSERT(ctx.kernels);\n+        if (!ctx.kernels) {\n+            return -1;  // No suitable kernel available\n+        }",
        "comment_created_at": "2025-07-17T07:16:31+00:00",
        "comment_author": "chaxu01",
        "comment_body": "Consider keeping the GGML_ASSERT(ctx.kernels) to catch misconfigurations early, or at least add a GGML_LOG_DEBUG message to warn when no suitable kernel is available.",
        "pr_file_module": null
      },
      {
        "comment_id": "2215168851",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14700,
        "pr_file": "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp",
        "discussion_id": "2209453662",
        "commented_code": "@@ -344,7 +350,9 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n \n public:\n     int repack(struct ggml_tensor * tensor, const void * data, size_t data_size) {\n-        GGML_ASSERT(ctx.kernels);\n+        if (!ctx.kernels) {\n+            return -1;  // No suitable kernel available\n+        }",
        "comment_created_at": "2025-07-18T06:57:02+00:00",
        "comment_author": "ggerganov",
        "comment_body": "I don't understand the change - if `repack()` returns -1, we will immediately assert on line 405:\r\n\r\n```c++\r\n    auto OK            = tensor_traits->repack(tensor, data, size);\r\n\r\n    GGML_ASSERT(OK == 0);\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2215154812",
    "pr_number": 14700,
    "pr_file": "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp",
    "created_at": "2025-07-18T06:51:40+00:00",
    "commented_code": "GGML_TENSOR_BINARY_OP_LOCALS\n \n         ggml_kleidiai_kernels *kernels = ggml_kleidiai_select_kernels(ctx.features, dst);\n-        GGML_ASSERT(kernels);\n+        if (!kernels) {\n+            // No suitable KleidiAI kernel available, fallback to standard CPU implementation\n+            GGML_LOG_DEBUG(\"%s: No suitable KleidiAI kernel available for Q4_0 operation, falling back to standard CPU implementation\n\", __func__);\n+            return false;  // Let the system fallback to standard CPU implementation\n+        }",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2215154812",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14700,
        "pr_file": "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp",
        "discussion_id": "2215154812",
        "commented_code": "@@ -276,7 +284,11 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n         GGML_TENSOR_BINARY_OP_LOCALS\n \n         ggml_kleidiai_kernels *kernels = ggml_kleidiai_select_kernels(ctx.features, dst);\n-        GGML_ASSERT(kernels);\n+        if (!kernels) {\n+            // No suitable KleidiAI kernel available, fallback to standard CPU implementation\n+            GGML_LOG_DEBUG(\"%s: No suitable KleidiAI kernel available for Q4_0 operation, falling back to standard CPU implementation\\n\", __func__);\n+            return false;  // Let the system fallback to standard CPU implementation\n+        }",
        "comment_created_at": "2025-07-18T06:51:40+00:00",
        "comment_author": "ggerganov",
        "comment_body": "In the compute calls, it's better to keep the `GGML_ASSERT`s. These would never get called if `repack()` has already bailed.",
        "pr_file_module": null
      }
    ]
  }
]