[
  {
    "discussion_id": "2234550300",
    "pr_number": 14907,
    "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
    "created_at": "2025-07-28T03:19:28+00:00",
    "commented_code": "}\n #endif\n \n-static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx, std::initializer_list<enum ggml_op> ops) {\n+static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx, std::initializer_list<enum ggml_op> ops, std::initializer_list<enum ggml_unary_op> unary_ops) {\n     if (!ggml_can_fuse(cgraph, node_idx, ops)) {\n         return false;\n     }\n \n-    if (ops.size() == 2 && ops.begin()[0] == GGML_OP_RMS_NORM && ops.begin()[1] == GGML_OP_MUL) {\n-        const ggml_tensor *rms_norm = cgraph->nodes[node_idx];\n-        const ggml_tensor *mul      = cgraph->nodes[node_idx+1];\n+    switch (ops.size()) {",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2234550300",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14907,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2234550300",
        "commented_code": "@@ -2766,34 +2767,59 @@ static void update_cuda_graph_executable(ggml_backend_cuda_context * cuda_ctx) {\n }\n #endif\n \n-static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx, std::initializer_list<enum ggml_op> ops) {\n+static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx, std::initializer_list<enum ggml_op> ops, std::initializer_list<enum ggml_unary_op> unary_ops) {\n     if (!ggml_can_fuse(cgraph, node_idx, ops)) {\n         return false;\n     }\n \n-    if (ops.size() == 2 && ops.begin()[0] == GGML_OP_RMS_NORM && ops.begin()[1] == GGML_OP_MUL) {\n-        const ggml_tensor *rms_norm = cgraph->nodes[node_idx];\n-        const ggml_tensor *mul      = cgraph->nodes[node_idx+1];\n+    switch (ops.size()) {",
        "comment_created_at": "2025-07-28T03:19:28+00:00",
        "comment_author": "am17an",
        "comment_body": "Switching on `ops.size()` is premature I think, for example in Metal we fuse RMS_NORM, MUL and ADD, we might want to do the same in the future. With this code, we'll have to repeat the code for case 2 and 3. Perhaps it makes sense to just do if statements for now",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219016154",
    "pr_number": 14624,
    "pr_file": "ggml/src/ggml-cuda/mma.cuh",
    "created_at": "2025-07-21T12:14:26+00:00",
    "commented_code": "template <typename T>\n     static __device__ __forceinline__ void load_ldmatrix(\n             tile<16, 8, T> & t, const T * __restrict__ xs0, const int stride) {\n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE)\n+        int64_t* xi = (int64_t*) t.x;\n+        const int64_t* xs = (int64_t*) ((const int*) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+        xi[0] = xs[0];\n+#elif defined(NEW_MMA_AVAILABLE)",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2219016154",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mma.cuh",
        "discussion_id": "2219016154",
        "commented_code": "@@ -186,7 +220,11 @@ namespace ggml_cuda_mma {\n     template <typename T>\n     static __device__ __forceinline__ void load_ldmatrix(\n             tile<16, 8, T> & t, const T * __restrict__ xs0, const int stride) {\n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE)\n+        int64_t* xi = (int64_t*) t.x;\n+        const int64_t* xs = (int64_t*) ((const int*) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+        xi[0] = xs[0];\n+#elif defined(NEW_MMA_AVAILABLE)",
        "comment_created_at": "2025-07-21T12:14:26+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n#if defined(AMD_MMA_AVAILABLE)\r\n        int64_t * xi = (int64_t *) t.x;\r\n        const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\r\n        xi[0] = xs[0];\r\n#elif defined(NEW_MMA_AVAILABLE)\r\n```\r\n\r\nSee contributing guidelines.\r\n\r\nMore generally, if I interpret this code correctly it seems to me like it is basically the same as `load_generic` except the data is being loaded as a single 64 bit value. So I would suggest modifying that function instead.",
        "pr_file_module": null
      },
      {
        "comment_id": "2219709255",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mma.cuh",
        "discussion_id": "2219016154",
        "commented_code": "@@ -186,7 +220,11 @@ namespace ggml_cuda_mma {\n     template <typename T>\n     static __device__ __forceinline__ void load_ldmatrix(\n             tile<16, 8, T> & t, const T * __restrict__ xs0, const int stride) {\n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE)\n+        int64_t* xi = (int64_t*) t.x;\n+        const int64_t* xs = (int64_t*) ((const int*) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+        xi[0] = xs[0];\n+#elif defined(NEW_MMA_AVAILABLE)",
        "comment_created_at": "2025-07-21T16:38:55+00:00",
        "comment_author": "deepsek",
        "comment_body": "Correct me if I'm wrong. In this case as example,\r\n`load_generic` is targeted to load one 32-bit INT at a time (whether it be contiguous or otherwise). \r\n\r\nThe only reason the 64-bit read instead of 2 32-bits work in this code section is because of the data layout expected by the MFMA instructions. But it does not translate to a generic operation for any data layout unfortunately. Luckily for both tile sizes of <16,8> and <32,4> it's the same. But it will change for another MFMA instr.",
        "pr_file_module": null
      },
      {
        "comment_id": "2219840240",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mma.cuh",
        "discussion_id": "2219016154",
        "commented_code": "@@ -186,7 +220,11 @@ namespace ggml_cuda_mma {\n     template <typename T>\n     static __device__ __forceinline__ void load_ldmatrix(\n             tile<16, 8, T> & t, const T * __restrict__ xs0, const int stride) {\n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE)\n+        int64_t* xi = (int64_t*) t.x;\n+        const int64_t* xs = (int64_t*) ((const int*) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+        xi[0] = xs[0];\n+#elif defined(NEW_MMA_AVAILABLE)",
        "comment_created_at": "2025-07-21T17:37:11+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "With NVIDIA the limitation for the source data in `ldmatrix` is that it needs to be a pointer to shared memory with 16 byte alignment. I was thinking about it more in terms of the memory space. In practice I think we would want to have the pointers be aligned anyways so I think it's fine to add memory alignment as a precondition for `load_generic` (the generic is more about not needing the `ldmatrix` instruction).",
        "pr_file_module": null
      },
      {
        "comment_id": "2220842442",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mma.cuh",
        "discussion_id": "2219016154",
        "commented_code": "@@ -186,7 +220,11 @@ namespace ggml_cuda_mma {\n     template <typename T>\n     static __device__ __forceinline__ void load_ldmatrix(\n             tile<16, 8, T> & t, const T * __restrict__ xs0, const int stride) {\n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE)\n+        int64_t* xi = (int64_t*) t.x;\n+        const int64_t* xs = (int64_t*) ((const int*) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+        xi[0] = xs[0];\n+#elif defined(NEW_MMA_AVAILABLE)",
        "comment_created_at": "2025-07-22T02:21:22+00:00",
        "comment_author": "deepsek",
        "comment_body": "Resolved. AMD MFMA tiles only use load_generic now. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219226764",
    "pr_number": 14624,
    "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
    "created_at": "2025-07-21T13:31:04+00:00",
    "commented_code": "}\n }\n \n-#define MMQ_TILE_Y_K (WARP_SIZE + WARP_SIZE/QI8_1)\n-\n-static int mmq_get_granularity_host(const int mmq_x, const int cc) {\n-    return new_mma_available(cc) && mmq_x >= 48 ? 16 : 8;\n+// block_q8_1_mmq has (128 8-bit ints == 32 32-bit ints + 4 32-bit factors)\n+#define MMQ_TILE_Y_K (MMQ_TILE_NE_K + MMQ_TILE_NE_K/QI8_1)\n+\n+static int mmq_get_granularity_host(ggml_type type, const int mmq_x, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return mmq_x >= 128 ? 32 : 16;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return mmq_x >= 128 ? 32 : 16;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return mmq_x >= 192 ? 64 : 32;\n+            default:\n+                return 0;\n+        }\n+    } else if (new_mma_available(cc) && mmq_x >= 48) {\n+        return 16;\n+    } else return 8;\n }\n \n-#ifdef NEW_MMA_AVAILABLE\n-static constexpr __device__ int mmq_get_granularity_device(const int mmq_x) {\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return mmq_x >= 128 ? 32 : 16;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return mmq_x >= 128 ? 32 : 16;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return mmq_x >= 192 ? 64 : 32;\n+        default:\n+            return 0;\n+    }\n+}\n+#elif defined(NEW_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n     return mmq_x >= 48 ? 16 : 8;\n }\n #else\n-static constexpr __device__ int mmq_get_granularity_device(const int /* mmq_x */) {\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n+    GGML_UNUSED(mmq_x);\n     return 8;\n }\n-#endif // NEW_MMA_AVAILABLE\n+#endif // AMD_MMA_AVAILABLE\n+\n+static int get_mmq_nwarps_host(ggml_type type, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return 8;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return 8;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return 4;\n+            default:\n+                return 0;\n+        }\n+    } else {\n+        return 8;\n+    }\n+}\n+\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return 8;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return 8;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return 4;\n+        default:\n+            return 0;\n+    }\n+}\n+#else\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    GGML_UNUSED(type);\n+    return 8;\n+}\n+#endif // AMD_MMA_AVAILABLE\n \n // ------------------------------------------------------------\n \n-template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n+template <int mmq_y, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n     const char * __restrict__ x, int * __restrict__ x_tile, const int kbx0, const int i_max, const int stride) {\n+    constexpr int nwarps = get_mmq_nwarps_device(GGML_TYPE_Q4_0);\n+    constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n \n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE) || defined(NEW_MMA_AVAILABLE)\n     int   * x_qs = (int   *)  x_tile;\n-    float * x_df = (float *) (x_qs + 2*WARP_SIZE);\n+    float * x_df = (float *) (x_qs + 2*MMQ_TILE_NE_K);\n #else\n     constexpr tile_x_sizes txs = mmq_get_dp4a_tile_x_sizes(GGML_TYPE_Q4_0, mmq_y);\n     int   * x_qs = (int   *)  x_tile;\n     float * x_df = (float *) (x_qs + txs.qs);\n #endif // NEW_MMA_AVAILABLE\n \n-    const int kbx  = threadIdx.x / QI4_0;\n-    const int kqsx = threadIdx.x % QI4_0;\n+    constexpr int threads_per_row = MMQ_ITER_K / (4 * QR4_0);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2219226764",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219226764",
        "commented_code": "@@ -215,42 +220,182 @@ static constexpr __host__ __device__ int mmq_get_mma_tile_x_k(ggml_type type) {\n     }\n }\n \n-#define MMQ_TILE_Y_K (WARP_SIZE + WARP_SIZE/QI8_1)\n-\n-static int mmq_get_granularity_host(const int mmq_x, const int cc) {\n-    return new_mma_available(cc) && mmq_x >= 48 ? 16 : 8;\n+// block_q8_1_mmq has (128 8-bit ints == 32 32-bit ints + 4 32-bit factors)\n+#define MMQ_TILE_Y_K (MMQ_TILE_NE_K + MMQ_TILE_NE_K/QI8_1)\n+\n+static int mmq_get_granularity_host(ggml_type type, const int mmq_x, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return mmq_x >= 128 ? 32 : 16;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return mmq_x >= 128 ? 32 : 16;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return mmq_x >= 192 ? 64 : 32;\n+            default:\n+                return 0;\n+        }\n+    } else if (new_mma_available(cc) && mmq_x >= 48) {\n+        return 16;\n+    } else return 8;\n }\n \n-#ifdef NEW_MMA_AVAILABLE\n-static constexpr __device__ int mmq_get_granularity_device(const int mmq_x) {\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return mmq_x >= 128 ? 32 : 16;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return mmq_x >= 128 ? 32 : 16;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return mmq_x >= 192 ? 64 : 32;\n+        default:\n+            return 0;\n+    }\n+}\n+#elif defined(NEW_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n     return mmq_x >= 48 ? 16 : 8;\n }\n #else\n-static constexpr __device__ int mmq_get_granularity_device(const int /* mmq_x */) {\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n+    GGML_UNUSED(mmq_x);\n     return 8;\n }\n-#endif // NEW_MMA_AVAILABLE\n+#endif // AMD_MMA_AVAILABLE\n+\n+static int get_mmq_nwarps_host(ggml_type type, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return 8;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return 8;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return 4;\n+            default:\n+                return 0;\n+        }\n+    } else {\n+        return 8;\n+    }\n+}\n+\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return 8;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return 8;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return 4;\n+        default:\n+            return 0;\n+    }\n+}\n+#else\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    GGML_UNUSED(type);\n+    return 8;\n+}\n+#endif // AMD_MMA_AVAILABLE\n \n // ------------------------------------------------------------\n \n-template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n+template <int mmq_y, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n     const char * __restrict__ x, int * __restrict__ x_tile, const int kbx0, const int i_max, const int stride) {\n+    constexpr int nwarps = get_mmq_nwarps_device(GGML_TYPE_Q4_0);\n+    constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n \n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE) || defined(NEW_MMA_AVAILABLE)\n     int   * x_qs = (int   *)  x_tile;\n-    float * x_df = (float *) (x_qs + 2*WARP_SIZE);\n+    float * x_df = (float *) (x_qs + 2*MMQ_TILE_NE_K);\n #else\n     constexpr tile_x_sizes txs = mmq_get_dp4a_tile_x_sizes(GGML_TYPE_Q4_0, mmq_y);\n     int   * x_qs = (int   *)  x_tile;\n     float * x_df = (float *) (x_qs + txs.qs);\n #endif // NEW_MMA_AVAILABLE\n \n-    const int kbx  = threadIdx.x / QI4_0;\n-    const int kqsx = threadIdx.x % QI4_0;\n+    constexpr int threads_per_row = MMQ_ITER_K / (4 * QR4_0);",
        "comment_created_at": "2025-07-21T13:31:04+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "The resulting value of 32 is correct but the way it's being calculated is wrong. The idea is that each thread moves 4 bytes so `threads_per_row` should always be equal to `WARP_SIZE` or with these changes `MMQ_TILE_NE_K`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2219627734",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219226764",
        "commented_code": "@@ -215,42 +220,182 @@ static constexpr __host__ __device__ int mmq_get_mma_tile_x_k(ggml_type type) {\n     }\n }\n \n-#define MMQ_TILE_Y_K (WARP_SIZE + WARP_SIZE/QI8_1)\n-\n-static int mmq_get_granularity_host(const int mmq_x, const int cc) {\n-    return new_mma_available(cc) && mmq_x >= 48 ? 16 : 8;\n+// block_q8_1_mmq has (128 8-bit ints == 32 32-bit ints + 4 32-bit factors)\n+#define MMQ_TILE_Y_K (MMQ_TILE_NE_K + MMQ_TILE_NE_K/QI8_1)\n+\n+static int mmq_get_granularity_host(ggml_type type, const int mmq_x, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return mmq_x >= 128 ? 32 : 16;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return mmq_x >= 128 ? 32 : 16;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return mmq_x >= 192 ? 64 : 32;\n+            default:\n+                return 0;\n+        }\n+    } else if (new_mma_available(cc) && mmq_x >= 48) {\n+        return 16;\n+    } else return 8;\n }\n \n-#ifdef NEW_MMA_AVAILABLE\n-static constexpr __device__ int mmq_get_granularity_device(const int mmq_x) {\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return mmq_x >= 128 ? 32 : 16;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return mmq_x >= 128 ? 32 : 16;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return mmq_x >= 192 ? 64 : 32;\n+        default:\n+            return 0;\n+    }\n+}\n+#elif defined(NEW_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n     return mmq_x >= 48 ? 16 : 8;\n }\n #else\n-static constexpr __device__ int mmq_get_granularity_device(const int /* mmq_x */) {\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n+    GGML_UNUSED(mmq_x);\n     return 8;\n }\n-#endif // NEW_MMA_AVAILABLE\n+#endif // AMD_MMA_AVAILABLE\n+\n+static int get_mmq_nwarps_host(ggml_type type, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return 8;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return 8;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return 4;\n+            default:\n+                return 0;\n+        }\n+    } else {\n+        return 8;\n+    }\n+}\n+\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return 8;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return 8;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return 4;\n+        default:\n+            return 0;\n+    }\n+}\n+#else\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    GGML_UNUSED(type);\n+    return 8;\n+}\n+#endif // AMD_MMA_AVAILABLE\n \n // ------------------------------------------------------------\n \n-template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n+template <int mmq_y, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n     const char * __restrict__ x, int * __restrict__ x_tile, const int kbx0, const int i_max, const int stride) {\n+    constexpr int nwarps = get_mmq_nwarps_device(GGML_TYPE_Q4_0);\n+    constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n \n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE) || defined(NEW_MMA_AVAILABLE)\n     int   * x_qs = (int   *)  x_tile;\n-    float * x_df = (float *) (x_qs + 2*WARP_SIZE);\n+    float * x_df = (float *) (x_qs + 2*MMQ_TILE_NE_K);\n #else\n     constexpr tile_x_sizes txs = mmq_get_dp4a_tile_x_sizes(GGML_TYPE_Q4_0, mmq_y);\n     int   * x_qs = (int   *)  x_tile;\n     float * x_df = (float *) (x_qs + txs.qs);\n #endif // NEW_MMA_AVAILABLE\n \n-    const int kbx  = threadIdx.x / QI4_0;\n-    const int kqsx = threadIdx.x % QI4_0;\n+    constexpr int threads_per_row = MMQ_ITER_K / (4 * QR4_0);",
        "comment_created_at": "2025-07-21T16:00:52+00:00",
        "comment_author": "deepsek",
        "comment_body": "Correct me if I'm wrong, but that was intention with the above calculation.\r\nI'm reading `QR4_0` as number of values packed into a byte.\r\n`4 * QR4_0` is used because 1 thread works on 4 bytes (4 bytes for Q4_0 contains `4*QR4_0` integers)\r\nin TILE_X_K we pack `MMQ_ITER_K `, a.k.a 256 integers (weights).\r\n\r\nSo, the total number of threads on one row is `MMQ_ITER_K / (4 * QR4_0)`\r\n\r\nThat's why it equates to the correct value. But also logically designed to arrive at 32. \r\nIs my understanding above wrong?",
        "pr_file_module": null
      },
      {
        "comment_id": "2219811630",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219226764",
        "commented_code": "@@ -215,42 +220,182 @@ static constexpr __host__ __device__ int mmq_get_mma_tile_x_k(ggml_type type) {\n     }\n }\n \n-#define MMQ_TILE_Y_K (WARP_SIZE + WARP_SIZE/QI8_1)\n-\n-static int mmq_get_granularity_host(const int mmq_x, const int cc) {\n-    return new_mma_available(cc) && mmq_x >= 48 ? 16 : 8;\n+// block_q8_1_mmq has (128 8-bit ints == 32 32-bit ints + 4 32-bit factors)\n+#define MMQ_TILE_Y_K (MMQ_TILE_NE_K + MMQ_TILE_NE_K/QI8_1)\n+\n+static int mmq_get_granularity_host(ggml_type type, const int mmq_x, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return mmq_x >= 128 ? 32 : 16;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return mmq_x >= 128 ? 32 : 16;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return mmq_x >= 192 ? 64 : 32;\n+            default:\n+                return 0;\n+        }\n+    } else if (new_mma_available(cc) && mmq_x >= 48) {\n+        return 16;\n+    } else return 8;\n }\n \n-#ifdef NEW_MMA_AVAILABLE\n-static constexpr __device__ int mmq_get_granularity_device(const int mmq_x) {\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return mmq_x >= 128 ? 32 : 16;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return mmq_x >= 128 ? 32 : 16;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return mmq_x >= 192 ? 64 : 32;\n+        default:\n+            return 0;\n+    }\n+}\n+#elif defined(NEW_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n     return mmq_x >= 48 ? 16 : 8;\n }\n #else\n-static constexpr __device__ int mmq_get_granularity_device(const int /* mmq_x */) {\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n+    GGML_UNUSED(mmq_x);\n     return 8;\n }\n-#endif // NEW_MMA_AVAILABLE\n+#endif // AMD_MMA_AVAILABLE\n+\n+static int get_mmq_nwarps_host(ggml_type type, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return 8;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return 8;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return 4;\n+            default:\n+                return 0;\n+        }\n+    } else {\n+        return 8;\n+    }\n+}\n+\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return 8;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return 8;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return 4;\n+        default:\n+            return 0;\n+    }\n+}\n+#else\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    GGML_UNUSED(type);\n+    return 8;\n+}\n+#endif // AMD_MMA_AVAILABLE\n \n // ------------------------------------------------------------\n \n-template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n+template <int mmq_y, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n     const char * __restrict__ x, int * __restrict__ x_tile, const int kbx0, const int i_max, const int stride) {\n+    constexpr int nwarps = get_mmq_nwarps_device(GGML_TYPE_Q4_0);\n+    constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n \n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE) || defined(NEW_MMA_AVAILABLE)\n     int   * x_qs = (int   *)  x_tile;\n-    float * x_df = (float *) (x_qs + 2*WARP_SIZE);\n+    float * x_df = (float *) (x_qs + 2*MMQ_TILE_NE_K);\n #else\n     constexpr tile_x_sizes txs = mmq_get_dp4a_tile_x_sizes(GGML_TYPE_Q4_0, mmq_y);\n     int   * x_qs = (int   *)  x_tile;\n     float * x_df = (float *) (x_qs + txs.qs);\n #endif // NEW_MMA_AVAILABLE\n \n-    const int kbx  = threadIdx.x / QI4_0;\n-    const int kqsx = threadIdx.x % QI4_0;\n+    constexpr int threads_per_row = MMQ_ITER_K / (4 * QR4_0);",
        "comment_created_at": "2025-07-21T17:24:01+00:00",
        "comment_author": "deepsek",
        "comment_body": "For example, \r\n- if you see `load_tiles_q8_0` . According to those calculations, 64 threads is required per row. In these cases, since NVGPU can only handle 32 threads. A comment has been made and it's set to 32\r\n- On the flip side for `load_tiles_q2_K` on main branch a multiplier `WARP_SIZE/QI2_K` is used but with the similar logic in the PR: `MMQ_ITER_K / (4 * QR2_K)`.\r\n\r\nI find the calculation made this way has a more logical resonance than `WARP_SIZE` or `MMQ_TILE_NE_K`",
        "pr_file_module": null
      },
      {
        "comment_id": "2219913337",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219226764",
        "commented_code": "@@ -215,42 +220,182 @@ static constexpr __host__ __device__ int mmq_get_mma_tile_x_k(ggml_type type) {\n     }\n }\n \n-#define MMQ_TILE_Y_K (WARP_SIZE + WARP_SIZE/QI8_1)\n-\n-static int mmq_get_granularity_host(const int mmq_x, const int cc) {\n-    return new_mma_available(cc) && mmq_x >= 48 ? 16 : 8;\n+// block_q8_1_mmq has (128 8-bit ints == 32 32-bit ints + 4 32-bit factors)\n+#define MMQ_TILE_Y_K (MMQ_TILE_NE_K + MMQ_TILE_NE_K/QI8_1)\n+\n+static int mmq_get_granularity_host(ggml_type type, const int mmq_x, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return mmq_x >= 128 ? 32 : 16;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return mmq_x >= 128 ? 32 : 16;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return mmq_x >= 192 ? 64 : 32;\n+            default:\n+                return 0;\n+        }\n+    } else if (new_mma_available(cc) && mmq_x >= 48) {\n+        return 16;\n+    } else return 8;\n }\n \n-#ifdef NEW_MMA_AVAILABLE\n-static constexpr __device__ int mmq_get_granularity_device(const int mmq_x) {\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return mmq_x >= 128 ? 32 : 16;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return mmq_x >= 128 ? 32 : 16;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return mmq_x >= 192 ? 64 : 32;\n+        default:\n+            return 0;\n+    }\n+}\n+#elif defined(NEW_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n     return mmq_x >= 48 ? 16 : 8;\n }\n #else\n-static constexpr __device__ int mmq_get_granularity_device(const int /* mmq_x */) {\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n+    GGML_UNUSED(mmq_x);\n     return 8;\n }\n-#endif // NEW_MMA_AVAILABLE\n+#endif // AMD_MMA_AVAILABLE\n+\n+static int get_mmq_nwarps_host(ggml_type type, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return 8;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return 8;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return 4;\n+            default:\n+                return 0;\n+        }\n+    } else {\n+        return 8;\n+    }\n+}\n+\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return 8;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return 8;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return 4;\n+        default:\n+            return 0;\n+    }\n+}\n+#else\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    GGML_UNUSED(type);\n+    return 8;\n+}\n+#endif // AMD_MMA_AVAILABLE\n \n // ------------------------------------------------------------\n \n-template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n+template <int mmq_y, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n     const char * __restrict__ x, int * __restrict__ x_tile, const int kbx0, const int i_max, const int stride) {\n+    constexpr int nwarps = get_mmq_nwarps_device(GGML_TYPE_Q4_0);\n+    constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n \n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE) || defined(NEW_MMA_AVAILABLE)\n     int   * x_qs = (int   *)  x_tile;\n-    float * x_df = (float *) (x_qs + 2*WARP_SIZE);\n+    float * x_df = (float *) (x_qs + 2*MMQ_TILE_NE_K);\n #else\n     constexpr tile_x_sizes txs = mmq_get_dp4a_tile_x_sizes(GGML_TYPE_Q4_0, mmq_y);\n     int   * x_qs = (int   *)  x_tile;\n     float * x_df = (float *) (x_qs + txs.qs);\n #endif // NEW_MMA_AVAILABLE\n \n-    const int kbx  = threadIdx.x / QI4_0;\n-    const int kqsx = threadIdx.x % QI4_0;\n+    constexpr int threads_per_row = MMQ_ITER_K / (4 * QR4_0);",
        "comment_created_at": "2025-07-21T18:09:10+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Sorry I didn't read the PR properly. I thought `MMQ_TILE_NE_K` was the constant number of 32 bit values per row for all data types. Quite frankly I don't like the current state of things though. If the number of elements per row in a tile is a multiple of the physical warp size that is to me a sensible implementation, I don't like it being a seemingly arbitrary multiple of 32. I'm not 100% sure what would be the best solution here, maybe move `mmq_type_traits` up and define the tile shapes that way?",
        "pr_file_module": null
      },
      {
        "comment_id": "2219990621",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219226764",
        "commented_code": "@@ -215,42 +220,182 @@ static constexpr __host__ __device__ int mmq_get_mma_tile_x_k(ggml_type type) {\n     }\n }\n \n-#define MMQ_TILE_Y_K (WARP_SIZE + WARP_SIZE/QI8_1)\n-\n-static int mmq_get_granularity_host(const int mmq_x, const int cc) {\n-    return new_mma_available(cc) && mmq_x >= 48 ? 16 : 8;\n+// block_q8_1_mmq has (128 8-bit ints == 32 32-bit ints + 4 32-bit factors)\n+#define MMQ_TILE_Y_K (MMQ_TILE_NE_K + MMQ_TILE_NE_K/QI8_1)\n+\n+static int mmq_get_granularity_host(ggml_type type, const int mmq_x, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return mmq_x >= 128 ? 32 : 16;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return mmq_x >= 128 ? 32 : 16;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return mmq_x >= 192 ? 64 : 32;\n+            default:\n+                return 0;\n+        }\n+    } else if (new_mma_available(cc) && mmq_x >= 48) {\n+        return 16;\n+    } else return 8;\n }\n \n-#ifdef NEW_MMA_AVAILABLE\n-static constexpr __device__ int mmq_get_granularity_device(const int mmq_x) {\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return mmq_x >= 128 ? 32 : 16;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return mmq_x >= 128 ? 32 : 16;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return mmq_x >= 192 ? 64 : 32;\n+        default:\n+            return 0;\n+    }\n+}\n+#elif defined(NEW_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n     return mmq_x >= 48 ? 16 : 8;\n }\n #else\n-static constexpr __device__ int mmq_get_granularity_device(const int /* mmq_x */) {\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n+    GGML_UNUSED(mmq_x);\n     return 8;\n }\n-#endif // NEW_MMA_AVAILABLE\n+#endif // AMD_MMA_AVAILABLE\n+\n+static int get_mmq_nwarps_host(ggml_type type, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return 8;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return 8;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return 4;\n+            default:\n+                return 0;\n+        }\n+    } else {\n+        return 8;\n+    }\n+}\n+\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return 8;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return 8;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return 4;\n+        default:\n+            return 0;\n+    }\n+}\n+#else\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    GGML_UNUSED(type);\n+    return 8;\n+}\n+#endif // AMD_MMA_AVAILABLE\n \n // ------------------------------------------------------------\n \n-template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n+template <int mmq_y, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n     const char * __restrict__ x, int * __restrict__ x_tile, const int kbx0, const int i_max, const int stride) {\n+    constexpr int nwarps = get_mmq_nwarps_device(GGML_TYPE_Q4_0);\n+    constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n \n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE) || defined(NEW_MMA_AVAILABLE)\n     int   * x_qs = (int   *)  x_tile;\n-    float * x_df = (float *) (x_qs + 2*WARP_SIZE);\n+    float * x_df = (float *) (x_qs + 2*MMQ_TILE_NE_K);\n #else\n     constexpr tile_x_sizes txs = mmq_get_dp4a_tile_x_sizes(GGML_TYPE_Q4_0, mmq_y);\n     int   * x_qs = (int   *)  x_tile;\n     float * x_df = (float *) (x_qs + txs.qs);\n #endif // NEW_MMA_AVAILABLE\n \n-    const int kbx  = threadIdx.x / QI4_0;\n-    const int kqsx = threadIdx.x % QI4_0;\n+    constexpr int threads_per_row = MMQ_ITER_K / (4 * QR4_0);",
        "comment_created_at": "2025-07-21T18:48:55+00:00",
        "comment_author": "deepsek",
        "comment_body": "I agree. Although that would require a major redesign. It is very much preferred to anchor to warp size. But in cases of multi- warp size support, it might be best to use another common anchor and handle the computation differently?\r\n\r\n- In an attempt to simplify the changes made, my intention was to anchor it to `QK_K == 256` (the largest possible GGML block size) also since 'one' iteration for MMQ involves the same 256 values across different quantizations. \r\n- A total of 256 `8-bit` integers for INT8 mma implies 64 `32-bit` integers. All the `MMQ_MMA_TILE_X_K_*` seem to respect this by loading those values and their scale factors, etc.\r\n- Ideally if `MMQ_TILE_NE_K` was set to 64, then it would look more cleaner, but that would require changing the shared memory data loaded, etc. since `TILE_Y_K` loads only 32 `32-bit` ints. If we can change `TILE_Y` loading 256 `8-bit` ints instead of 128.\r\n- But I avoided doing that and set `MMQ_TILE_NE_K` to 32 only in order to fit into the existing design as much as possible and NOT disrupt it too much. \r\n\r\nBut I agree it is not much less clean using an arbitrary value of 32. \r\nIf we do prefer to anchor to warp size of the hardware, `mmq_type_traits` holding custom tiles could work. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2220046405",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219226764",
        "commented_code": "@@ -215,42 +220,182 @@ static constexpr __host__ __device__ int mmq_get_mma_tile_x_k(ggml_type type) {\n     }\n }\n \n-#define MMQ_TILE_Y_K (WARP_SIZE + WARP_SIZE/QI8_1)\n-\n-static int mmq_get_granularity_host(const int mmq_x, const int cc) {\n-    return new_mma_available(cc) && mmq_x >= 48 ? 16 : 8;\n+// block_q8_1_mmq has (128 8-bit ints == 32 32-bit ints + 4 32-bit factors)\n+#define MMQ_TILE_Y_K (MMQ_TILE_NE_K + MMQ_TILE_NE_K/QI8_1)\n+\n+static int mmq_get_granularity_host(ggml_type type, const int mmq_x, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return mmq_x >= 128 ? 32 : 16;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return mmq_x >= 128 ? 32 : 16;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return mmq_x >= 192 ? 64 : 32;\n+            default:\n+                return 0;\n+        }\n+    } else if (new_mma_available(cc) && mmq_x >= 48) {\n+        return 16;\n+    } else return 8;\n }\n \n-#ifdef NEW_MMA_AVAILABLE\n-static constexpr __device__ int mmq_get_granularity_device(const int mmq_x) {\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return mmq_x >= 128 ? 32 : 16;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return mmq_x >= 128 ? 32 : 16;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return mmq_x >= 192 ? 64 : 32;\n+        default:\n+            return 0;\n+    }\n+}\n+#elif defined(NEW_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n     return mmq_x >= 48 ? 16 : 8;\n }\n #else\n-static constexpr __device__ int mmq_get_granularity_device(const int /* mmq_x */) {\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n+    GGML_UNUSED(mmq_x);\n     return 8;\n }\n-#endif // NEW_MMA_AVAILABLE\n+#endif // AMD_MMA_AVAILABLE\n+\n+static int get_mmq_nwarps_host(ggml_type type, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return 8;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return 8;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return 4;\n+            default:\n+                return 0;\n+        }\n+    } else {\n+        return 8;\n+    }\n+}\n+\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return 8;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return 8;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return 4;\n+        default:\n+            return 0;\n+    }\n+}\n+#else\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    GGML_UNUSED(type);\n+    return 8;\n+}\n+#endif // AMD_MMA_AVAILABLE\n \n // ------------------------------------------------------------\n \n-template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n+template <int mmq_y, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n     const char * __restrict__ x, int * __restrict__ x_tile, const int kbx0, const int i_max, const int stride) {\n+    constexpr int nwarps = get_mmq_nwarps_device(GGML_TYPE_Q4_0);\n+    constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n \n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE) || defined(NEW_MMA_AVAILABLE)\n     int   * x_qs = (int   *)  x_tile;\n-    float * x_df = (float *) (x_qs + 2*WARP_SIZE);\n+    float * x_df = (float *) (x_qs + 2*MMQ_TILE_NE_K);\n #else\n     constexpr tile_x_sizes txs = mmq_get_dp4a_tile_x_sizes(GGML_TYPE_Q4_0, mmq_y);\n     int   * x_qs = (int   *)  x_tile;\n     float * x_df = (float *) (x_qs + txs.qs);\n #endif // NEW_MMA_AVAILABLE\n \n-    const int kbx  = threadIdx.x / QI4_0;\n-    const int kqsx = threadIdx.x % QI4_0;\n+    constexpr int threads_per_row = MMQ_ITER_K / (4 * QR4_0);",
        "comment_created_at": "2025-07-21T19:20:17+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Ultimately I currently can't think of a solution that really makes me happy. I think for this PR we can just merge it as-is and if I can later think of a better way to do it we can refactor it then (provided you'll be around for testing since I don't have access to CDNA3 hardware).",
        "pr_file_module": null
      },
      {
        "comment_id": "2220401578",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219226764",
        "commented_code": "@@ -215,42 +220,182 @@ static constexpr __host__ __device__ int mmq_get_mma_tile_x_k(ggml_type type) {\n     }\n }\n \n-#define MMQ_TILE_Y_K (WARP_SIZE + WARP_SIZE/QI8_1)\n-\n-static int mmq_get_granularity_host(const int mmq_x, const int cc) {\n-    return new_mma_available(cc) && mmq_x >= 48 ? 16 : 8;\n+// block_q8_1_mmq has (128 8-bit ints == 32 32-bit ints + 4 32-bit factors)\n+#define MMQ_TILE_Y_K (MMQ_TILE_NE_K + MMQ_TILE_NE_K/QI8_1)\n+\n+static int mmq_get_granularity_host(ggml_type type, const int mmq_x, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return mmq_x >= 128 ? 32 : 16;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return mmq_x >= 128 ? 32 : 16;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return mmq_x >= 192 ? 64 : 32;\n+            default:\n+                return 0;\n+        }\n+    } else if (new_mma_available(cc) && mmq_x >= 48) {\n+        return 16;\n+    } else return 8;\n }\n \n-#ifdef NEW_MMA_AVAILABLE\n-static constexpr __device__ int mmq_get_granularity_device(const int mmq_x) {\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return mmq_x >= 128 ? 32 : 16;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return mmq_x >= 128 ? 32 : 16;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return mmq_x >= 192 ? 64 : 32;\n+        default:\n+            return 0;\n+    }\n+}\n+#elif defined(NEW_MMA_AVAILABLE)\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n     return mmq_x >= 48 ? 16 : 8;\n }\n #else\n-static constexpr __device__ int mmq_get_granularity_device(const int /* mmq_x */) {\n+static constexpr __device__ int mmq_get_granularity_device(ggml_type type, const int mmq_x) {\n+    GGML_UNUSED(type);\n+    GGML_UNUSED(mmq_x);\n     return 8;\n }\n-#endif // NEW_MMA_AVAILABLE\n+#endif // AMD_MMA_AVAILABLE\n+\n+static int get_mmq_nwarps_host(ggml_type type, const int cc) {\n+    if (amd_mma_available(cc)) {\n+        switch (type) {\n+            // vec_dot_q8_0_q8_1_mma\n+            case GGML_TYPE_Q4_0:\n+            case GGML_TYPE_Q5_0:\n+            case GGML_TYPE_Q8_0:\n+            case GGML_TYPE_IQ2_XXS:\n+            case GGML_TYPE_IQ3_XXS:\n+            case GGML_TYPE_IQ3_S:\n+            case GGML_TYPE_IQ4_XS:\n+            case GGML_TYPE_IQ4_NL:\n+                return 8;\n+            // vec_dot_q8_1_q8_1_mma\n+            case GGML_TYPE_Q4_1:\n+            case GGML_TYPE_Q5_1:\n+            case GGML_TYPE_Q4_K:\n+            case GGML_TYPE_Q5_K:\n+            case GGML_TYPE_IQ1_S:\n+                return 8;\n+            case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+            case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+            case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+            case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+                return 4;\n+            default:\n+                return 0;\n+        }\n+    } else {\n+        return 8;\n+    }\n+}\n+\n+#if defined(AMD_MMA_AVAILABLE)\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    switch (type) {\n+        // vec_dot_q8_0_q8_1_mma\n+        case GGML_TYPE_Q4_0:\n+        case GGML_TYPE_Q5_0:\n+        case GGML_TYPE_Q8_0:\n+        case GGML_TYPE_IQ2_XXS:\n+        case GGML_TYPE_IQ3_XXS:\n+        case GGML_TYPE_IQ3_S:\n+        case GGML_TYPE_IQ4_XS:\n+        case GGML_TYPE_IQ4_NL:\n+            return 8;\n+        // vec_dot_q8_1_q8_1_mma\n+        case GGML_TYPE_Q4_1:\n+        case GGML_TYPE_Q5_1:\n+        case GGML_TYPE_Q4_K:\n+        case GGML_TYPE_Q5_K:\n+        case GGML_TYPE_IQ1_S:\n+            return 8;\n+        case GGML_TYPE_Q2_K:   // vec_dot_q2_K_q8_1_mma\n+        case GGML_TYPE_Q3_K:   // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_Q6_K:   // vec_dot_q6_K_q8_1_mma\n+        case GGML_TYPE_IQ2_XS: // vec_dot_q8_0_16_q8_1_mma\n+        case GGML_TYPE_IQ2_S:  // vec_dot_q8_0_16_q8_1_mma\n+            return 4;\n+        default:\n+            return 0;\n+    }\n+}\n+#else\n+static constexpr __device__ int get_mmq_nwarps_device(ggml_type type) {\n+    GGML_UNUSED(type);\n+    return 8;\n+}\n+#endif // AMD_MMA_AVAILABLE\n \n // ------------------------------------------------------------\n \n-template <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n+template <int mmq_y, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n     const char * __restrict__ x, int * __restrict__ x_tile, const int kbx0, const int i_max, const int stride) {\n+    constexpr int nwarps = get_mmq_nwarps_device(GGML_TYPE_Q4_0);\n+    constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n \n-#ifdef NEW_MMA_AVAILABLE\n+#if defined(AMD_MMA_AVAILABLE) || defined(NEW_MMA_AVAILABLE)\n     int   * x_qs = (int   *)  x_tile;\n-    float * x_df = (float *) (x_qs + 2*WARP_SIZE);\n+    float * x_df = (float *) (x_qs + 2*MMQ_TILE_NE_K);\n #else\n     constexpr tile_x_sizes txs = mmq_get_dp4a_tile_x_sizes(GGML_TYPE_Q4_0, mmq_y);\n     int   * x_qs = (int   *)  x_tile;\n     float * x_df = (float *) (x_qs + txs.qs);\n #endif // NEW_MMA_AVAILABLE\n \n-    const int kbx  = threadIdx.x / QI4_0;\n-    const int kqsx = threadIdx.x % QI4_0;\n+    constexpr int threads_per_row = MMQ_ITER_K / (4 * QR4_0);",
        "comment_created_at": "2025-07-21T21:34:16+00:00",
        "comment_author": "deepsek",
        "comment_body": "I will be available. llama.cpp support on AMD compute is one of my teams primary focus. I will be happy to be involved in design discussions, testing, etc. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2218943734",
    "pr_number": 14763,
    "pr_file": "ggml/src/ggml-cuda/cpy-utils.cuh",
    "created_at": "2025-07-21T11:40:30+00:00",
    "commented_code": "*dst = *src;\n }\n \n+static __device__ __forceinline__ void convert_f16_bf16(const half * src, nv_bfloat16 * dst) {\n+    *dst = float(*src);\n+}\n+\n static __device__ __forceinline__ void convert_f16_f32(const half * src, float * dst) {\n     *dst = *src;\n }\n \n+static __device__ __forceinline__ void convert_bf16_f16(const nv_bfloat16 * src, half * dst) {\n+    *dst = __float2half(*src);\n+}\n+\n+static __device__ __forceinline__ void convert_bf16_f32(const nv_bfloat16 * src, float * dst) {\n+    *dst = *src;\n+}",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2218943734",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14763,
        "pr_file": "ggml/src/ggml-cuda/cpy-utils.cuh",
        "discussion_id": "2218943734",
        "commented_code": "@@ -18,10 +18,22 @@ static __device__ __forceinline__ void convert_f16_f16(const half * src, half *\n     *dst = *src;\n }\n \n+static __device__ __forceinline__ void convert_f16_bf16(const half * src, nv_bfloat16 * dst) {\n+    *dst = float(*src);\n+}\n+\n static __device__ __forceinline__ void convert_f16_f32(const half * src, float * dst) {\n     *dst = *src;\n }\n \n+static __device__ __forceinline__ void convert_bf16_f16(const nv_bfloat16 * src, half * dst) {\n+    *dst = __float2half(*src);\n+}\n+\n+static __device__ __forceinline__ void convert_bf16_f32(const nv_bfloat16 * src, float * dst) {\n+    *dst = *src;\n+}",
        "comment_created_at": "2025-07-21T11:40:30+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I think all of these cases can be covered with `*dst = float(*src)`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2219525062",
    "pr_number": 14763,
    "pr_file": "ggml/src/ggml-cuda/cpy-utils.cuh",
    "created_at": "2025-07-21T15:19:01+00:00",
    "commented_code": "#include \"ggml-common.h\"\n \n-static __device__ __forceinline__ void convert_f32_f32(const float * src, float * dst) {\n-    *dst = *src;\n+template<typename src_t, typename dst_t>\n+static __device__ __forceinline__ void convert_to_flt(const src_t * src, dst_t * dst) {",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2219525062",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14763,
        "pr_file": "ggml/src/ggml-cuda/cpy-utils.cuh",
        "discussion_id": "2219525062",
        "commented_code": "@@ -2,26 +2,20 @@\n \n #include \"ggml-common.h\"\n \n-static __device__ __forceinline__ void convert_f32_f32(const float * src, float * dst) {\n-    *dst = *src;\n+template<typename src_t, typename dst_t>\n+static __device__ __forceinline__ void convert_to_flt(const src_t * src, dst_t * dst) {",
        "comment_created_at": "2025-07-21T15:19:01+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Add a check for whether `src_t` and `dst_t` are the same, don't do a cast to float in that case (also covers FP16 -> FP16).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2220201612",
    "pr_number": 14763,
    "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
    "created_at": "2025-07-21T20:10:52+00:00",
    "commented_code": "if (src0_type == GGML_TYPE_F16 && src1_type == GGML_TYPE_F16) {\n                     return true;\n                 }\n+                if (src0_type == GGML_TYPE_F16 && src1_type == GGML_TYPE_BF16) {\n+                    return true;\n+                }\n                 if (src0_type == GGML_TYPE_F16 && src1_type == GGML_TYPE_F32) {\n                     return true;\n                 }\n+                if (src0_type == GGML_TYPE_BF16 && src1_type == GGML_TYPE_BF16) {\n+                    return true;\n+                }\n+                if (src0_type == GGML_TYPE_BF16 && src1_type == GGML_TYPE_F16) {\n+                    return true;\n+                }\n+                if (src0_type == GGML_TYPE_BF16 && src1_type == GGML_TYPE_F32) {\n+                    return true;\n+                }",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2220201612",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14763,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2220201612",
        "commented_code": "@@ -3287,9 +3287,21 @@ static bool ggml_backend_cuda_device_supports_op(ggml_backend_dev_t dev, const g\n                 if (src0_type == GGML_TYPE_F16 && src1_type == GGML_TYPE_F16) {\n                     return true;\n                 }\n+                if (src0_type == GGML_TYPE_F16 && src1_type == GGML_TYPE_BF16) {\n+                    return true;\n+                }\n                 if (src0_type == GGML_TYPE_F16 && src1_type == GGML_TYPE_F32) {\n                     return true;\n                 }\n+                if (src0_type == GGML_TYPE_BF16 && src1_type == GGML_TYPE_BF16) {\n+                    return true;\n+                }\n+                if (src0_type == GGML_TYPE_BF16 && src1_type == GGML_TYPE_F16) {\n+                    return true;\n+                }\n+                if (src0_type == GGML_TYPE_BF16 && src1_type == GGML_TYPE_F32) {\n+                    return true;\n+                }",
        "comment_created_at": "2025-07-21T20:10:52+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Conversion between floats should not all be covered so this check can also be deduplicated by checking whether both types are either FP32, FP16, or BF16.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190142123",
    "pr_number": 14551,
    "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
    "created_at": "2025-07-07T13:39:15+00:00",
    "commented_code": "+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+static __device__ void set_rows_1_f32_f32(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    float * dst_f = (float *) dst;\n+    *dst_f = *src_f;\n+}\n+\n+static __device__ void set_rows_1_f32_f16(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    half * dst_h = (half *) dst;\n+    *dst_h = __float2half(*src_f);\n+}",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2190142123",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14551,
        "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
        "discussion_id": "2190142123",
        "commented_code": "@@ -0,0 +1,128 @@\n+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+static __device__ void set_rows_1_f32_f32(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    float * dst_f = (float *) dst;\n+    *dst_f = *src_f;\n+}\n+\n+static __device__ void set_rows_1_f32_f16(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    half * dst_h = (half *) dst;\n+    *dst_h = __float2half(*src_f);\n+}",
        "comment_created_at": "2025-07-07T13:39:15+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "This is fine for now but long-term I think it will be simpler (for floating-point data types) to define `__device__` functions that map from and to float rather than explicit mappings between 2 types.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2163935689",
    "pr_number": 14361,
    "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
    "created_at": "2025-06-24T12:56:03+00:00",
    "commented_code": null,
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2163935689",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14361,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2163935689",
        "commented_code": null,
        "comment_created_at": "2025-06-24T12:56:03+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I think it would be better to turn `ggml_cuda_mul_mat_batched_cublas` into a template with the `ggml_type` as a template parameter.",
        "pr_file_module": null
      },
      {
        "comment_id": "2163950550",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14361,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2163935689",
        "commented_code": null,
        "comment_created_at": "2025-06-24T13:02:44+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Actually, maybe turn the current function into something like `ggml_cuda_mul_mat_batched_cublas_impl` with a template parameter for the type and create a new function `ggml_cuda_mul_mat_batched_cublas` that just calls the impl function with the correct type.",
        "pr_file_module": null
      },
      {
        "comment_id": "2164009515",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14361,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2163935689",
        "commented_code": null,
        "comment_created_at": "2025-06-24T13:27:11+00:00",
        "comment_author": "am17an",
        "comment_body": "I think we won't save much duplication if we have a templated function, we still need to dispatch functions like (like `ggml_get_to_fp16_nc_cuda`) according to various type, which a template doesn't solve. Perhaps just 3 different functions and `ggml_cuda_mul_mat_batched_cublas` does the dispatch based on type?\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2164031907",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14361,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2163935689",
        "commented_code": null,
        "comment_created_at": "2025-06-24T13:36:38+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": ">ggml_get_to_fp16_nc_cuda\r\n\r\nWork with the `to_t_nc_cuda_t` template instead?\r\n\r\n>Perhaps just 3 different functions and ggml_cuda_mul_mat_batched_cublas does the dispatch based on type?\r\n\r\nI would really like to avoid this if at all possible. The cuBLAS interface should in principle be the same for FP32, FP16, and BF16. It should be possible to call cuBLAS concisely, one solution would I think be to define something like a `cublas_type_traits` template struct to get e.g. the right cuBLAS compute types (take a look at `mmq_type_traits` in `mmq.cuh` for an example of what I mean).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2163940730",
    "pr_number": 14361,
    "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
    "created_at": "2025-06-24T12:58:07+00:00",
    "commented_code": "}\n }\n \n+template<typename T>\n static __global__ void k_compute_batched_ptrs(\n-        const half * src0_as_f16, const half * src1_as_f16, char * dst,\n+        const T * src0_as_f16, const T * src1_as_f16, char * dst,",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2163940730",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14361,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2163940730",
        "commented_code": "@@ -1745,8 +1745,9 @@ static void ggml_cuda_op_mul_mat(\n     }\n }\n \n+template<typename T>\n static __global__ void k_compute_batched_ptrs(\n-        const half * src0_as_f16, const half * src1_as_f16, char * dst,\n+        const T * src0_as_f16, const T * src1_as_f16, char * dst,",
        "comment_created_at": "2025-06-24T12:58:07+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "The pointers are being cast to `char *` anyways so I think a simpler solution would be to just pass `void *` pointers.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2111228594",
    "pr_number": 13842,
    "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
    "created_at": "2025-05-28T08:11:27+00:00",
    "commented_code": "const bool use_fp16 = (src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type)) && ggml_is_contiguous(src0) && row_diff == src0->ne[1] && dst->op_params[0] == GGML_PREC_DEFAULT;\n \n-    if (src0->type == GGML_TYPE_BF16 && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {\n+    if ((GGML_CUDA_CC_IS_NVIDIA(cc) || GGML_CUDA_CC_IS_AMD(cc) || (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2)) && src0->type == GGML_TYPE_BF16 && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2111228594",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13842,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2111228594",
        "commented_code": "@@ -1200,7 +1200,7 @@ static void ggml_cuda_op_mul_mat_cublas(\n \n     const bool use_fp16 = (src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type)) && ggml_is_contiguous(src0) && row_diff == src0->ne[1] && dst->op_params[0] == GGML_PREC_DEFAULT;\n \n-    if (src0->type == GGML_TYPE_BF16 && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {\n+    if ((GGML_CUDA_CC_IS_NVIDIA(cc) || GGML_CUDA_CC_IS_AMD(cc) || (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2)) && src0->type == GGML_TYPE_BF16 && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {",
        "comment_created_at": "2025-05-28T08:11:27+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I would suggest adding something like `const bool XXX = ...` on the line above. As it is it's not clear to me what the correct interpretation of the check is. (Also it prevents the line from becoming too long.)",
        "pr_file_module": null
      },
      {
        "comment_id": "2111350577",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13842,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2111228594",
        "commented_code": "@@ -1200,7 +1200,7 @@ static void ggml_cuda_op_mul_mat_cublas(\n \n     const bool use_fp16 = (src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type)) && ggml_is_contiguous(src0) && row_diff == src0->ne[1] && dst->op_params[0] == GGML_PREC_DEFAULT;\n \n-    if (src0->type == GGML_TYPE_BF16 && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {\n+    if ((GGML_CUDA_CC_IS_NVIDIA(cc) || GGML_CUDA_CC_IS_AMD(cc) || (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2)) && src0->type == GGML_TYPE_BF16 && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {",
        "comment_created_at": "2025-05-28T09:10:54+00:00",
        "comment_author": "yeahdongcn",
        "comment_body": "Thanks for the review! I've added `support_bf16` and `support_fp16` consts. Previously, line 1203 didn't perform any compute capability check, so MTT S80 would go into that code path.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2111545663",
    "pr_number": 13842,
    "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
    "created_at": "2025-05-28T10:48:55+00:00",
    "commented_code": "const int cc = ggml_cuda_info().devices[id].cc;\n \n+    const bool support_bf16 = GGML_CUDA_CC_IS_NVIDIA(cc) || GGML_CUDA_CC_IS_AMD(cc) ||\n+        (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2);\n+\n+    const bool support_fp16 = (GGML_CUDA_CC_IS_NVIDIA(cc) && cc >= GGML_CUDA_CC_VOLTA) ||",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2111545663",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13842,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2111545663",
        "commented_code": "@@ -1198,9 +1198,14 @@ static void ggml_cuda_op_mul_mat_cublas(\n \n     const int cc = ggml_cuda_info().devices[id].cc;\n \n+    const bool support_bf16 = GGML_CUDA_CC_IS_NVIDIA(cc) || GGML_CUDA_CC_IS_AMD(cc) ||\n+        (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2);\n+\n+    const bool support_fp16 = (GGML_CUDA_CC_IS_NVIDIA(cc) && cc >= GGML_CUDA_CC_VOLTA) ||",
        "comment_created_at": "2025-05-28T10:48:55+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "This is not quite correct. For NVIDIA FP16 is supported on Pascal or newer (but CC 6.1 has terrible FP16 performance). So I would changing this to `supports_fast_fp16` and use that as one of the inputs for `use_fp16` (since they are only used together).\r\n\r\nAlso, I don't have a P100 available for testing, but the correct logic for CUDA and AMD should be `fast_fp16_hardware_available(cc)`. Consider extending that function for MUSA since it is also used to determine the optimal code paths for e.g. MoE models.",
        "pr_file_module": null
      },
      {
        "comment_id": "2111560116",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13842,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2111545663",
        "commented_code": "@@ -1198,9 +1198,14 @@ static void ggml_cuda_op_mul_mat_cublas(\n \n     const int cc = ggml_cuda_info().devices[id].cc;\n \n+    const bool support_bf16 = GGML_CUDA_CC_IS_NVIDIA(cc) || GGML_CUDA_CC_IS_AMD(cc) ||\n+        (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2);\n+\n+    const bool support_fp16 = (GGML_CUDA_CC_IS_NVIDIA(cc) && cc >= GGML_CUDA_CC_VOLTA) ||",
        "comment_created_at": "2025-05-28T10:56:25+00:00",
        "comment_author": "yeahdongcn",
        "comment_body": "Would it be better to replace this with `fast_fp16_hardware_available(cc)` since that function is already updated in this PR?\r\n\r\n```c\r\n// To be used for feature selection of external libraries, e.g. cuBLAS.\r\nstatic bool fast_fp16_hardware_available(const int cc) {\r\n    return (GGML_CUDA_CC_IS_NVIDIA(cc) && cc >= GGML_CUDA_CC_PASCAL && cc != 610) || GGML_CUDA_CC_IS_AMD(cc) ||\r\n        (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2);\r\n}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2111569053",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13842,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2111545663",
        "commented_code": "@@ -1198,9 +1198,14 @@ static void ggml_cuda_op_mul_mat_cublas(\n \n     const int cc = ggml_cuda_info().devices[id].cc;\n \n+    const bool support_bf16 = GGML_CUDA_CC_IS_NVIDIA(cc) || GGML_CUDA_CC_IS_AMD(cc) ||\n+        (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2);\n+\n+    const bool support_fp16 = (GGML_CUDA_CC_IS_NVIDIA(cc) && cc >= GGML_CUDA_CC_VOLTA) ||",
        "comment_created_at": "2025-05-28T11:01:52+00:00",
        "comment_author": "yeahdongcn",
        "comment_body": "Just pushed the code changes.",
        "pr_file_module": null
      },
      {
        "comment_id": "2111577553",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13842,
        "pr_file": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "discussion_id": "2111545663",
        "commented_code": "@@ -1198,9 +1198,14 @@ static void ggml_cuda_op_mul_mat_cublas(\n \n     const int cc = ggml_cuda_info().devices[id].cc;\n \n+    const bool support_bf16 = GGML_CUDA_CC_IS_NVIDIA(cc) || GGML_CUDA_CC_IS_AMD(cc) ||\n+        (GGML_CUDA_CC_IS_MTHREADS(cc) && cc >= GGML_CUDA_CC_QY2);\n+\n+    const bool support_fp16 = (GGML_CUDA_CC_IS_NVIDIA(cc) && cc >= GGML_CUDA_CC_VOLTA) ||",
        "comment_created_at": "2025-05-28T11:07:05+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": ">Would it be better to replace this with fast_fp16_hardware_available(cc) since that function is already updated in this PR?\r\n\r\nYes, that's what I meant.",
        "pr_file_module": null
      }
    ]
  }
]