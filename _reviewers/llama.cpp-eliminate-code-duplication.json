[
  {
    "discussion_id": "2234139265",
    "pr_number": 14898,
    "pr_file": "src/llama-graph.cpp",
    "created_at": "2025-07-27T21:27:24+00:00",
    "commented_code": "return moe_out;\n }\n \n+ggml_tensor * llm_graph_context::build_moe_ffn_from_probs(",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2234139265",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14898,
        "pr_file": "src/llama-graph.cpp",
        "discussion_id": "2234139265",
        "commented_code": "@@ -938,6 +938,100 @@ ggml_tensor * llm_graph_context::build_moe_ffn(\n     return moe_out;\n }\n \n+ggml_tensor * llm_graph_context::build_moe_ffn_from_probs(",
        "comment_created_at": "2025-07-27T21:27:24+00:00",
        "comment_author": "CISC",
        "comment_body": "The code duplication is unfortunate, is it possible to merge this into `build_moe_ffn` with `probs` as a toggle without making too much of a mess?\r\n\r\nCan be a follow-up.",
        "pr_file_module": null
      },
      {
        "comment_id": "2234441167",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14898,
        "pr_file": "src/llama-graph.cpp",
        "discussion_id": "2234139265",
        "commented_code": "@@ -938,6 +938,100 @@ ggml_tensor * llm_graph_context::build_moe_ffn(\n     return moe_out;\n }\n \n+ggml_tensor * llm_graph_context::build_moe_ffn_from_probs(",
        "comment_created_at": "2025-07-28T02:37:06+00:00",
        "comment_author": "wdl339",
        "comment_body": "That's a great point. I've been thinking about the best way to merge these and have a couple of ideas on how we could approach it.\r\n\r\n1. As you suggested, we could modify `build_moe_ffn` to accept an optional `probs` parameter. The main difficulty here is that the logic for weight normalization and activation functions diverges significantly between the two paths, so it would require some careful internal branching to keep it clean.\r\n2. Alternatively, we could extract the initial router logic (logits and probs calculation) into its own function. `build_moe_ffn` would then have a check at the beginning to decide whether to call this new router function. My main concern with this approach is that `build_moe_ffn` is a core function, and I'm a bit worried about affecting other models, so this would need careful testing.\r\n\r\nBoth approaches seem feasible. Given the complexity and your suggestion that this can be a follow-up, would you prefer I handle this in a separate PR, or should I proceed with one of these solutions here?",
        "pr_file_module": null
      },
      {
        "comment_id": "2235099293",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14898,
        "pr_file": "src/llama-graph.cpp",
        "discussion_id": "2234139265",
        "commented_code": "@@ -938,6 +938,100 @@ ggml_tensor * llm_graph_context::build_moe_ffn(\n     return moe_out;\n }\n \n+ggml_tensor * llm_graph_context::build_moe_ffn_from_probs(",
        "comment_created_at": "2025-07-28T07:37:08+00:00",
        "comment_author": "CISC",
        "comment_body": "A separate PR is probably best.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2162414029",
    "pr_number": 9400,
    "pr_file": "tools/imatrix/imatrix.cpp",
    "created_at": "2025-06-23T19:47:10+00:00",
    "commented_code": "static void print_usage(int, char ** argv) {\n     LOG(\"\nexample usage:\n\");\n     LOG(\"\n    %s \\\\\n\"\n-            \"       -m model.gguf -f some-text.txt [-o imatrix.dat] [--process-output] \\\\\n\"\n+            \"       -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] \\\\\n\"\n             \"       [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\\\n\"\n-            \"       [--in-file imatrix-prev-0.dat --in-file imatrix-prev-1.dat ...] \\\\\n\"\n+            \"       [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\\\n\"\n             \"       [--parse-special]\n\" , argv[0]);\n     LOG(\"\n\");\n }\n \n+static bool str_has_suffix(const std::string & str, const std::string & suffix) {\n+    return str.size() >= suffix.size() && str.compare(str.size() - suffix.size(), str.size(), suffix) == 0;\n+}\n+\n+static bool str_remove_suffix(std::string & str, const std::string & suffix) {\n+    bool has_suffix = str_has_suffix(str, suffix);\n+    if (has_suffix) {\n+        str = str.substr(0, str.size() - suffix.size());\n+    }\n+    return has_suffix;\n+}",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2162414029",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9400,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2162414029",
        "commented_code": "@@ -22,31 +24,49 @@\n static void print_usage(int, char ** argv) {\n     LOG(\"\\nexample usage:\\n\");\n     LOG(\"\\n    %s \\\\\\n\"\n-            \"       -m model.gguf -f some-text.txt [-o imatrix.dat] [--process-output] \\\\\\n\"\n+            \"       -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] \\\\\\n\"\n             \"       [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\\\\n\"\n-            \"       [--in-file imatrix-prev-0.dat --in-file imatrix-prev-1.dat ...] \\\\\\n\"\n+            \"       [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\\\\n\"\n             \"       [--parse-special]\\n\" , argv[0]);\n     LOG(\"\\n\");\n }\n \n+static bool str_has_suffix(const std::string & str, const std::string & suffix) {\n+    return str.size() >= suffix.size() && str.compare(str.size() - suffix.size(), str.size(), suffix) == 0;\n+}\n+\n+static bool str_remove_suffix(std::string & str, const std::string & suffix) {\n+    bool has_suffix = str_has_suffix(str, suffix);\n+    if (has_suffix) {\n+        str = str.substr(0, str.size() - suffix.size());\n+    }\n+    return has_suffix;\n+}",
        "comment_created_at": "2025-06-23T19:47:10+00:00",
        "comment_author": "CISC",
        "comment_body": "```suggestion\r\nstatic bool str_remove_suffix(std::string & str, const std::string_view & suffix) {\r\n    bool has_suffix = string_ends_with(str, suffix);\r\n    if (has_suffix) {\r\n        str = str.substr(0, str.size() - suffix.size());\r\n    }\r\n    return has_suffix;\r\n}\r\n```\r\nThis is a nice complement to `string_ends_with`, should be moved to `common.cpp` as `string_remove_suffix`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2162468541",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9400,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2162414029",
        "commented_code": "@@ -22,31 +24,49 @@\n static void print_usage(int, char ** argv) {\n     LOG(\"\\nexample usage:\\n\");\n     LOG(\"\\n    %s \\\\\\n\"\n-            \"       -m model.gguf -f some-text.txt [-o imatrix.dat] [--process-output] \\\\\\n\"\n+            \"       -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] \\\\\\n\"\n             \"       [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\\\\n\"\n-            \"       [--in-file imatrix-prev-0.dat --in-file imatrix-prev-1.dat ...] \\\\\\n\"\n+            \"       [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\\\\n\"\n             \"       [--parse-special]\\n\" , argv[0]);\n     LOG(\"\\n\");\n }\n \n+static bool str_has_suffix(const std::string & str, const std::string & suffix) {\n+    return str.size() >= suffix.size() && str.compare(str.size() - suffix.size(), str.size(), suffix) == 0;\n+}\n+\n+static bool str_remove_suffix(std::string & str, const std::string & suffix) {\n+    bool has_suffix = str_has_suffix(str, suffix);\n+    if (has_suffix) {\n+        str = str.substr(0, str.size() - suffix.size());\n+    }\n+    return has_suffix;\n+}",
        "comment_created_at": "2025-06-23T20:25:26+00:00",
        "comment_author": "compilade",
        "comment_body": "Shouldn't `std::string_view` be passed by value instead of by const reference in these functions? \r\n\r\nFor example, <https://quuxplusone.github.io/blog/2021/11/09/pass-string-view-by-value/> suggests that `std::string_view` should always be passed by value.",
        "pr_file_module": null
      },
      {
        "comment_id": "2162609519",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9400,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2162414029",
        "commented_code": "@@ -22,31 +24,49 @@\n static void print_usage(int, char ** argv) {\n     LOG(\"\\nexample usage:\\n\");\n     LOG(\"\\n    %s \\\\\\n\"\n-            \"       -m model.gguf -f some-text.txt [-o imatrix.dat] [--process-output] \\\\\\n\"\n+            \"       -m model.gguf -f some-text.txt [-o imatrix.gguf] [--process-output] \\\\\\n\"\n             \"       [--no-ppl] [--chunk 123] [--output-frequency 10] [--save-frequency 0] \\\\\\n\"\n-            \"       [--in-file imatrix-prev-0.dat --in-file imatrix-prev-1.dat ...] \\\\\\n\"\n+            \"       [--in-file imatrix-prev-0.gguf --in-file imatrix-prev-1.gguf ...] \\\\\\n\"\n             \"       [--parse-special]\\n\" , argv[0]);\n     LOG(\"\\n\");\n }\n \n+static bool str_has_suffix(const std::string & str, const std::string & suffix) {\n+    return str.size() >= suffix.size() && str.compare(str.size() - suffix.size(), str.size(), suffix) == 0;\n+}\n+\n+static bool str_remove_suffix(std::string & str, const std::string & suffix) {\n+    bool has_suffix = str_has_suffix(str, suffix);\n+    if (has_suffix) {\n+        str = str.substr(0, str.size() - suffix.size());\n+    }\n+    return has_suffix;\n+}",
        "comment_created_at": "2025-06-23T22:12:11+00:00",
        "comment_author": "CISC",
        "comment_body": "I did wonder about that, but didn't know enough of the details behind it, reading that it certainly seems so, and it should be worth going over the usage of `std::string_view` elsewhere.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2162418149",
    "pr_number": 9400,
    "pr_file": "tools/quantize/quantize.cpp",
    "created_at": "2025-06-23T19:50:19+00:00",
    "commented_code": "exit(1);\n }\n \n-static int load_imatrix(const std::string & imatrix_file, std::string & imatrix_dataset, std::unordered_map<std::string, std::vector<float>> & imatrix_data) {\n+// TODO: share with implementation in imatrix.cpp\n+static bool str_remove_suffix(std::string & str, const std::string & suffix) {\n+    bool has_suffix = str.size() >= suffix.size() && str.compare(str.size() - suffix.size(), str.size(), suffix) == 0;\n+    if (has_suffix) {\n+        str = str.substr(0, str.size() - suffix.size());\n+    }\n+    return has_suffix;\n+}\n+",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2162418149",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9400,
        "pr_file": "tools/quantize/quantize.cpp",
        "discussion_id": "2162418149",
        "commented_code": "@@ -140,7 +147,16 @@ static void usage(const char * executable) {\n     exit(1);\n }\n \n-static int load_imatrix(const std::string & imatrix_file, std::string & imatrix_dataset, std::unordered_map<std::string, std::vector<float>> & imatrix_data) {\n+// TODO: share with implementation in imatrix.cpp\n+static bool str_remove_suffix(std::string & str, const std::string & suffix) {\n+    bool has_suffix = str.size() >= suffix.size() && str.compare(str.size() - suffix.size(), str.size(), suffix) == 0;\n+    if (has_suffix) {\n+        str = str.substr(0, str.size() - suffix.size());\n+    }\n+    return has_suffix;\n+}\n+",
        "comment_created_at": "2025-06-23T19:50:19+00:00",
        "comment_author": "CISC",
        "comment_body": "Can be removed if moved to `common.cpp`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160062893",
    "pr_number": 12718,
    "pr_file": "tools/imatrix/imatrix.cpp",
    "created_at": "2025-06-21T15:18:33+00:00",
    "commented_code": "return false;\n         }\n \n-        // Recreate the state as expected by save_imatrix(), and corerct for weighted sum.\n+        // Recreate the state as expected by save_imatrix(), and correct for weighted sum.\n+        std::vector<float> activations;\n+        activations.reserve(nval);\n         for (int i = 0; i < nval; i++) {\n             e.values[i] += tmp[i];\n             e.counts[i] += ncall;\n+            activations.push_back(e.values[i] / e.counts[i]);\n         }\n         e.ncall += ncall;\n \n+        if (tstats) {",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2160062893",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 12718,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2160062893",
        "commented_code": "@@ -338,14 +384,81 @@ bool IMatrixCollector::load_imatrix(const char * fname) {\n             return false;\n         }\n \n-        // Recreate the state as expected by save_imatrix(), and corerct for weighted sum.\n+        // Recreate the state as expected by save_imatrix(), and correct for weighted sum.\n+        std::vector<float> activations;\n+        activations.reserve(nval);\n         for (int i = 0; i < nval; i++) {\n             e.values[i] += tmp[i];\n             e.counts[i] += ncall;\n+            activations.push_back(e.values[i] / e.counts[i]);\n         }\n         e.ncall += ncall;\n \n+        if (tstats) {",
        "comment_created_at": "2025-06-21T15:18:33+00:00",
        "comment_author": "compilade",
        "comment_body": "Could the stats loading be separated in its own function instead of being in `load_imatrix`? In #9400, there are two loading functions (for the old and new format) and the current proposed changes would be difficult to adapt to that.\r\n\r\nThe separate function could either be called from `load_imatrix` or separately.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160079109",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 12718,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2160062893",
        "commented_code": "@@ -338,14 +384,81 @@ bool IMatrixCollector::load_imatrix(const char * fname) {\n             return false;\n         }\n \n-        // Recreate the state as expected by save_imatrix(), and corerct for weighted sum.\n+        // Recreate the state as expected by save_imatrix(), and correct for weighted sum.\n+        std::vector<float> activations;\n+        activations.reserve(nval);\n         for (int i = 0; i < nval; i++) {\n             e.values[i] += tmp[i];\n             e.counts[i] += ncall;\n+            activations.push_back(e.values[i] / e.counts[i]);\n         }\n         e.ncall += ncall;\n \n+        if (tstats) {",
        "comment_created_at": "2025-06-21T16:34:02+00:00",
        "comment_author": "EAddario",
        "comment_body": "Will implement and re-submit",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160383645",
    "pr_number": 12718,
    "pr_file": "tools/imatrix/imatrix.cpp",
    "created_at": "2025-06-22T15:36:46+00:00",
    "commented_code": "int ncall = 0;\n };\n \n+struct tensor_statistics {\n+    std::string tensor;\n+    Stats stats;\n+    float total_bias = 0;\n+    float mean_bias  = 0;\n+    float max_bias   = 0;\n+    float min_bias   = 0;\n+    int elements     = 0;\n+    float stddev     = 0;\n+    float active     = 0;\n+    float entropy    = 0;\n+    float zd         = 0;\n+    float cossim     = 0;\n+};\n+\n class IMatrixCollector {\n public:\n     IMatrixCollector() = default;\n     void set_params(common_params params) { m_params = std::move(params); }\n     bool collect_imatrix(struct ggml_tensor * t, bool ask, void * user_data);\n     void save_imatrix(int ncall = -1) const;\n-    bool load_imatrix(const char * fname);\n+    bool load_imatrix(const char * fname, std::vector<tensor_statistics> * tstats = nullptr);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2160383645",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 12718,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2160383645",
        "commented_code": "@@ -35,13 +37,28 @@ struct Stats {\n     int ncall = 0;\n };\n \n+struct tensor_statistics {\n+    std::string tensor;\n+    Stats stats;\n+    float total_bias = 0;\n+    float mean_bias  = 0;\n+    float max_bias   = 0;\n+    float min_bias   = 0;\n+    int elements     = 0;\n+    float stddev     = 0;\n+    float active     = 0;\n+    float entropy    = 0;\n+    float zd         = 0;\n+    float cossim     = 0;\n+};\n+\n class IMatrixCollector {\n public:\n     IMatrixCollector() = default;\n     void set_params(common_params params) { m_params = std::move(params); }\n     bool collect_imatrix(struct ggml_tensor * t, bool ask, void * user_data);\n     void save_imatrix(int ncall = -1) const;\n-    bool load_imatrix(const char * fname);\n+    bool load_imatrix(const char * fname, std::vector<tensor_statistics> * tstats = nullptr);",
        "comment_created_at": "2025-06-22T15:36:46+00:00",
        "comment_author": "compilade",
        "comment_body": "Stats computing could be further uncoupled from loading by adding a separate method which would iterate over `m_stats` to compute the extra statistics.\r\n\r\nThat way, `load_imatrix` wouldn't have to know about `tstats`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160488232",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 12718,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2160383645",
        "commented_code": "@@ -35,13 +37,28 @@ struct Stats {\n     int ncall = 0;\n };\n \n+struct tensor_statistics {\n+    std::string tensor;\n+    Stats stats;\n+    float total_bias = 0;\n+    float mean_bias  = 0;\n+    float max_bias   = 0;\n+    float min_bias   = 0;\n+    int elements     = 0;\n+    float stddev     = 0;\n+    float active     = 0;\n+    float entropy    = 0;\n+    float zd         = 0;\n+    float cossim     = 0;\n+};\n+\n class IMatrixCollector {\n public:\n     IMatrixCollector() = default;\n     void set_params(common_params params) { m_params = std::move(params); }\n     bool collect_imatrix(struct ggml_tensor * t, bool ask, void * user_data);\n     void save_imatrix(int ncall = -1) const;\n-    bool load_imatrix(const char * fname);\n+    bool load_imatrix(const char * fname, std::vector<tensor_statistics> * tstats = nullptr);",
        "comment_created_at": "2025-06-22T20:55:44+00:00",
        "comment_author": "EAddario",
        "comment_body": "Good shout!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2168403107",
    "pr_number": 14274,
    "pr_file": "ggml/src/ggml-cpu/ops.cpp",
    "created_at": "2025-06-26T07:37:53+00:00",
    "commented_code": "}\n }\n \n+static void ggml_compute_forward_repeat_i64(\n+        const ggml_compute_params * params,\n+        ggml_tensor * dst) {\n+\n+    const ggml_tensor * src0 = dst->src[0];",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2168403107",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14274,
        "pr_file": "ggml/src/ggml-cpu/ops.cpp",
        "discussion_id": "2168403107",
        "commented_code": "@@ -2282,6 +2266,52 @@ static void ggml_compute_forward_repeat_f16(\n     }\n }\n \n+static void ggml_compute_forward_repeat_i64(\n+        const ggml_compute_params * params,\n+        ggml_tensor * dst) {\n+\n+    const ggml_tensor * src0 = dst->src[0];",
        "comment_created_at": "2025-06-26T07:37:53+00:00",
        "comment_author": "ggerganov",
        "comment_body": "After adding the broadcast support to `ggml_set_rows()` this is not really needed anymore, but I think it's nice to have either way.",
        "pr_file_module": null
      },
      {
        "comment_id": "2169492225",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14274,
        "pr_file": "ggml/src/ggml-cpu/ops.cpp",
        "discussion_id": "2168403107",
        "commented_code": "@@ -2282,6 +2266,52 @@ static void ggml_compute_forward_repeat_f16(\n     }\n }\n \n+static void ggml_compute_forward_repeat_i64(\n+        const ggml_compute_params * params,\n+        ggml_tensor * dst) {\n+\n+    const ggml_tensor * src0 = dst->src[0];",
        "comment_created_at": "2025-06-26T16:54:31+00:00",
        "comment_author": "slaren",
        "comment_body": "It would be nice to use a template instead of duplicating the code however. We need to start somewhere porting this code to C++.",
        "pr_file_module": null
      },
      {
        "comment_id": "2171261666",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14274,
        "pr_file": "ggml/src/ggml-cpu/ops.cpp",
        "discussion_id": "2168403107",
        "commented_code": "@@ -2282,6 +2266,52 @@ static void ggml_compute_forward_repeat_f16(\n     }\n }\n \n+static void ggml_compute_forward_repeat_i64(\n+        const ggml_compute_params * params,\n+        ggml_tensor * dst) {\n+\n+    const ggml_tensor * src0 = dst->src[0];",
        "comment_created_at": "2025-06-27T08:45:02+00:00",
        "comment_author": "ggerganov",
        "comment_body": "Ok, will add template in a follow-up PR. For now, removed the i64 support and added TODO.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2127326787",
    "pr_number": 14016,
    "pr_file": "tools/server/server.cpp",
    "created_at": "2025-06-04T19:50:12+00:00",
    "commented_code": "}\n     };\n \n-    const auto handle_completions = [&handle_completions_impl](const httplib::Request & req, httplib::Response & res) {\n+    const auto handle_completions = [&ctx_server, &handle_completions_impl](const httplib::Request & req, httplib::Response & res) {\n         json data = json::parse(req.body);\n-        std::vector<raw_buffer> files; // dummy\n+        json medias = json_value(data, \"medias\", json::array());\n+        auto & opt = ctx_server.oai_parser_opt;\n+        std::vector<raw_buffer> files;\n+\n+        if (medias.is_array()) {\n+            for (auto & m : medias) {\n+                std::string type = json_value(m, \"type\", std::string());\n+                std::string data = json_value(m, \"data\", std::string());\n+                if (type.empty() || data.empty()) {\n+                    continue;\n+                }\n+                if (type == \"image_url\" || type == \"image\" || type == \"img\") {\n+                    if (!opt.allow_image) {\n+                        throw std::runtime_error(\"image input is not supported - hint: if this is unexpected, you may need to provide the mmproj\");\n+                    }\n+                    if (string_starts_with(data, \"http\")) {\n+                        // download remote image\n+                        common_remote_params params;\n+                        params.headers.push_back(\"User-Agent: llama.cpp/\" + build_info);\n+                        params.max_size = 1024 * 1024 * 10; // 10MB\n+                        params.timeout  = 10; // seconds\n+                        SRV_INF(\"downloading image from '%s'\n\", data.c_str());\n+                        auto res = common_remote_get_content(data, params);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2127326787",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14016,
        "pr_file": "tools/server/server.cpp",
        "discussion_id": "2127326787",
        "commented_code": "@@ -4332,9 +4332,69 @@ int main(int argc, char ** argv) {\n         }\n     };\n \n-    const auto handle_completions = [&handle_completions_impl](const httplib::Request & req, httplib::Response & res) {\n+    const auto handle_completions = [&ctx_server, &handle_completions_impl](const httplib::Request & req, httplib::Response & res) {\n         json data = json::parse(req.body);\n-        std::vector<raw_buffer> files; // dummy\n+        json medias = json_value(data, \"medias\", json::array());\n+        auto & opt = ctx_server.oai_parser_opt;\n+        std::vector<raw_buffer> files;\n+\n+        if (medias.is_array()) {\n+            for (auto & m : medias) {\n+                std::string type = json_value(m, \"type\", std::string());\n+                std::string data = json_value(m, \"data\", std::string());\n+                if (type.empty() || data.empty()) {\n+                    continue;\n+                }\n+                if (type == \"image_url\" || type == \"image\" || type == \"img\") {\n+                    if (!opt.allow_image) {\n+                        throw std::runtime_error(\"image input is not supported - hint: if this is unexpected, you may need to provide the mmproj\");\n+                    }\n+                    if (string_starts_with(data, \"http\")) {\n+                        // download remote image\n+                        common_remote_params params;\n+                        params.headers.push_back(\"User-Agent: llama.cpp/\" + build_info);\n+                        params.max_size = 1024 * 1024 * 10; // 10MB\n+                        params.timeout  = 10; // seconds\n+                        SRV_INF(\"downloading image from '%s'\\n\", data.c_str());\n+                        auto res = common_remote_get_content(data, params);",
        "comment_created_at": "2025-06-04T19:50:12+00:00",
        "comment_author": "ngxson",
        "comment_body": "Instead of duplicating this whole code block, extract it to a general function and reuse it in `/chat/completion` and `/completion`. [DRY code principle](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)",
        "pr_file_module": null
      }
    ]
  }
]