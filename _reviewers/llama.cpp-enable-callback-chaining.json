[
  {
    "discussion_id": "1822170541",
    "pr_number": 9639,
    "pr_file": "include/llama.h",
    "created_at": "2024-10-30T09:07:36+00:00",
    "commented_code": "const char * grammar_str,\n                           const char * grammar_root);\n \n+    LLAMA_API bool llama_sampler_is_grammar_empty(struct llama_sampler * gsmpl);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "1822170541",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "include/llama.h",
        "discussion_id": "1822170541",
        "commented_code": "@@ -1130,6 +1132,8 @@ extern \"C\" {\n                           const char * grammar_str,\n                           const char * grammar_root);\n \n+    LLAMA_API bool llama_sampler_is_grammar_empty(struct llama_sampler * gsmpl);",
        "comment_created_at": "2024-10-30T09:07:36+00:00",
        "comment_author": "ggerganov",
        "comment_body": "I wonder if instead of extending the `llama_sampler` API, it would be better to pass necessary callbacks (such as `is_empty`, `accept_str`, etc.) through the `llama_sampler_init_grammar()` call.",
        "pr_file_module": null
      },
      {
        "comment_id": "1924611727",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "include/llama.h",
        "discussion_id": "1822170541",
        "commented_code": "@@ -1130,6 +1132,8 @@ extern \"C\" {\n                           const char * grammar_str,\n                           const char * grammar_root);\n \n+    LLAMA_API bool llama_sampler_is_grammar_empty(struct llama_sampler * gsmpl);",
        "comment_created_at": "2025-01-22T02:34:42+00:00",
        "comment_author": "ochafik",
        "comment_body": "Finally got to do that, now passing it trigger words (and trigger tokens for words that tokenize to a single token). Tests are still TODO.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2177626196",
    "pr_number": 14481,
    "pr_file": "ggml/include/ggml.h",
    "created_at": "2025-07-01T13:34:52+00:00",
    "commented_code": "extern \"C\" {\n #endif\n \n+    // Function type used in fatal error callbacks\n+    typedef void (*ggml_abort_callback_t)(const char * error_message);\n+\n+    // Set the abort callback (passing null will restore original abort functionality: printing a message to stdout)\n+    GGML_API void ggml_set_abort_callback(ggml_abort_callback_t callback);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2177626196",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14481,
        "pr_file": "ggml/include/ggml.h",
        "discussion_id": "2177626196",
        "commented_code": "@@ -314,6 +314,12 @@\n extern \"C\" {\n #endif\n \n+    // Function type used in fatal error callbacks\n+    typedef void (*ggml_abort_callback_t)(const char * error_message);\n+\n+    // Set the abort callback (passing null will restore original abort functionality: printing a message to stdout)\n+    GGML_API void ggml_set_abort_callback(ggml_abort_callback_t callback);",
        "comment_created_at": "2025-07-01T13:34:52+00:00",
        "comment_author": "slaren",
        "comment_body": "Return the previous `abort_callback` to make it possible to chain multiple.",
        "pr_file_module": null
      }
    ]
  }
]