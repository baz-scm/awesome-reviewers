[
  {
    "discussion_id": "2116363354",
    "pr_number": 13873,
    "pr_file": "ggml/src/ggml-opt.cpp",
    "created_at": "2025-05-30T18:06:27+00:00",
    "commented_code": "return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(ggml_opt_context_t opt_ctx) {\n+    return opt_ctx->opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    const bool moment     = opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && need_moment(opt_ctx);\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2116363354",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2116363354",
        "commented_code": "@@ -312,12 +338,21 @@ static ggml_cgraph * dup_graph(ggml_context * ctx, ggml_cgraph * src) {\n     return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(ggml_opt_context_t opt_ctx) {\n+    return opt_ctx->opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    const bool moment     = opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && need_moment(opt_ctx);\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
        "comment_created_at": "2025-05-30T18:06:27+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "This logic is incorrect. Whether or not the optimizer needs buffers for the momenta is independent of whether or not buffers are needed to accumulate the gradients.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2119383248",
    "pr_number": 13873,
    "pr_file": "ggml/src/ggml-opt.cpp",
    "created_at": "2025-06-01T17:57:36+00:00",
    "commented_code": "// gb_opt == graph backward optimize, forward pass, then backward pass to calculate gradients, then optimizer step.\n     opt_ctx->gb_opt = ggml_graph_dup(opt_ctx->ctx_compute, opt_ctx->gb_grad, /*force_grads =*/ true);\n \n-    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 7);\n-    ggml_set_input(opt_ctx->adamw_params);\n-    ggml_set_name(opt_ctx->adamw_params, \"adamw_params\");\n+    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 8);\n+    ggml_tensor * adamw_params = opt_ctx->adamw_params;\n+    ggml_set_input(adamw_params);\n+    ggml_set_name(adamw_params, \"adamw_params\");\n \n+    std::string step_prefix     = moment ? \"AdamW step for \" : \"SGD step for \";\n+    unsigned    step_prefix_len = step_prefix.size();\n     for (int i = opt_ctx->gf->n_nodes-1; i >= 0; --i) {\n         struct ggml_tensor * node = opt_ctx->gb_opt->nodes[i];\n         struct ggml_tensor * grad = ggml_graph_get_grad(opt_ctx->gb_opt, node);\n \n         if (grad && (node->flags & GGML_TENSOR_FLAG_PARAM)) {\n-            struct ggml_tensor * m        = opt_ctx->grad_m[i];\n-            struct ggml_tensor * v        = opt_ctx->grad_v[i];\n-            struct ggml_tensor * opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n-\n-            ggml_set_name(m,        (std::string(\"AdamW m for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(v,        (std::string(\"AdamW v for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(opt_step, (std::string(\"AdamW step for \") + std::string(node->name)).c_str());\n-\n+            struct ggml_tensor * m = 0;\n+            struct ggml_tensor * v = 0;\n+            if (moment) {\n+                m = opt_ctx->grad_m[i];\n+                v = opt_ctx->grad_v[i];\n+                ggml_set_name(m, (std::string(\"AdamW m for \") + std::string(node->name)).c_str());\n+                ggml_set_name(v, (std::string(\"AdamW v for \") + std::string(node->name)).c_str());\n+            }\n+            struct ggml_tensor * opt_step =\n+                m ? ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, adamw_params) :\n+                    ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, adamw_params);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2119383248",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2119383248",
        "commented_code": "@@ -492,23 +530,31 @@ static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     // gb_opt == graph backward optimize, forward pass, then backward pass to calculate gradients, then optimizer step.\n     opt_ctx->gb_opt = ggml_graph_dup(opt_ctx->ctx_compute, opt_ctx->gb_grad, /*force_grads =*/ true);\n \n-    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 7);\n-    ggml_set_input(opt_ctx->adamw_params);\n-    ggml_set_name(opt_ctx->adamw_params, \"adamw_params\");\n+    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 8);\n+    ggml_tensor * adamw_params = opt_ctx->adamw_params;\n+    ggml_set_input(adamw_params);\n+    ggml_set_name(adamw_params, \"adamw_params\");\n \n+    std::string step_prefix     = moment ? \"AdamW step for \" : \"SGD step for \";\n+    unsigned    step_prefix_len = step_prefix.size();\n     for (int i = opt_ctx->gf->n_nodes-1; i >= 0; --i) {\n         struct ggml_tensor * node = opt_ctx->gb_opt->nodes[i];\n         struct ggml_tensor * grad = ggml_graph_get_grad(opt_ctx->gb_opt, node);\n \n         if (grad && (node->flags & GGML_TENSOR_FLAG_PARAM)) {\n-            struct ggml_tensor * m        = opt_ctx->grad_m[i];\n-            struct ggml_tensor * v        = opt_ctx->grad_v[i];\n-            struct ggml_tensor * opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n-\n-            ggml_set_name(m,        (std::string(\"AdamW m for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(v,        (std::string(\"AdamW v for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(opt_step, (std::string(\"AdamW step for \") + std::string(node->name)).c_str());\n-\n+            struct ggml_tensor * m = 0;\n+            struct ggml_tensor * v = 0;\n+            if (moment) {\n+                m = opt_ctx->grad_m[i];\n+                v = opt_ctx->grad_v[i];\n+                ggml_set_name(m, (std::string(\"AdamW m for \") + std::string(node->name)).c_str());\n+                ggml_set_name(v, (std::string(\"AdamW v for \") + std::string(node->name)).c_str());\n+            }\n+            struct ggml_tensor * opt_step =\n+                m ? ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, adamw_params) :\n+                    ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, adamw_params);",
        "comment_created_at": "2025-06-01T17:57:36+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n            struct ggml_tensor * opt_step;\r\n            switch (opt_ctx->optimizer_type) {\r\n                case GGML_OPT_OPTIMIZER_ADAMW:\r\n                    opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, adamw_params);\r\n                    break;\r\n                case GGML_OPT_OPTIMIZER_SGD:\r\n                    opt_step = ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, adamw_params);\r\n                    break;\r\n                default:\r\n                    GGML_ABORT(\"fatal error\");\r\n                    break;\r\n            }\r\n```\r\n\r\nThis is what I meant regarding separating the logic for allocation and setting `opt_step`. I don't have a problem with whether or not the code produces the correct logic, I have a problem with the implicit reasoning behind it. Whether or not the momenta should be allocated is derived from the type of optimizer, not the other way around.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2128619874",
    "pr_number": 13873,
    "pr_file": "ggml/src/ggml-opt.cpp",
    "created_at": "2025-06-05T11:31:10+00:00",
    "commented_code": "return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(struct ggml_opt_optimizer_params const & opt_pars) {\n+    return opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    struct ggml_opt_optimizer_params const & opt_pars = opt_ctx->opt_pars();\n+    const bool                               moment =\n+        // or has_moment is per https://github.com/ggml-org/llama.cpp/pull/13873#discussion_r2116366095\n+        // - do not allow sgd epochs after adamw\n+        // (which in the current impl would require clearing the previously n_nodes-sized grad_m and grad_v)\n+        opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && (need_moment(opt_pars) || has_moment(opt_ctx));\n+\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2128619874",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2128619874",
        "commented_code": "@@ -312,12 +337,27 @@ static ggml_cgraph * dup_graph(ggml_context * ctx, ggml_cgraph * src) {\n     return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(struct ggml_opt_optimizer_params const & opt_pars) {\n+    return opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    struct ggml_opt_optimizer_params const & opt_pars = opt_ctx->opt_pars();\n+    const bool                               moment =\n+        // or has_moment is per https://github.com/ggml-org/llama.cpp/pull/13873#discussion_r2116366095\n+        // - do not allow sgd epochs after adamw\n+        // (which in the current impl would require clearing the previously n_nodes-sized grad_m and grad_v)\n+        opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && (need_moment(opt_pars) || has_moment(opt_ctx));\n+\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
        "comment_created_at": "2025-06-05T11:31:10+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "The optimizer type should be constant for the lifetime of `ggml_opt_context` so it's enough to just check whether the constant optimizer type needs the momenta.",
        "pr_file_module": null
      },
      {
        "comment_id": "2130574227",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2128619874",
        "commented_code": "@@ -312,12 +337,27 @@ static ggml_cgraph * dup_graph(ggml_context * ctx, ggml_cgraph * src) {\n     return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(struct ggml_opt_optimizer_params const & opt_pars) {\n+    return opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    struct ggml_opt_optimizer_params const & opt_pars = opt_ctx->opt_pars();\n+    const bool                               moment =\n+        // or has_moment is per https://github.com/ggml-org/llama.cpp/pull/13873#discussion_r2116366095\n+        // - do not allow sgd epochs after adamw\n+        // (which in the current impl would require clearing the previously n_nodes-sized grad_m and grad_v)\n+        opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && (need_moment(opt_pars) || has_moment(opt_ctx));\n+\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
        "comment_created_at": "2025-06-05T21:58:14+00:00",
        "comment_author": "graehl",
        "comment_body": "are saying now that we don't support changing opt type from epoch to epoch that it's frivolous for me to check !opt_ctx->grad_m.empty() (has_moment)? otherwise i'm not sure what change you're requesting",
        "pr_file_module": null
      },
      {
        "comment_id": "2131612016",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2128619874",
        "commented_code": "@@ -312,12 +337,27 @@ static ggml_cgraph * dup_graph(ggml_context * ctx, ggml_cgraph * src) {\n     return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(struct ggml_opt_optimizer_params const & opt_pars) {\n+    return opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    struct ggml_opt_optimizer_params const & opt_pars = opt_ctx->opt_pars();\n+    const bool                               moment =\n+        // or has_moment is per https://github.com/ggml-org/llama.cpp/pull/13873#discussion_r2116366095\n+        // - do not allow sgd epochs after adamw\n+        // (which in the current impl would require clearing the previously n_nodes-sized grad_m and grad_v)\n+        opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && (need_moment(opt_pars) || has_moment(opt_ctx));\n+\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
        "comment_created_at": "2025-06-06T06:47:12+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I think that if someone wants to change the optimizer type they should simply free the current `ggml_opt_context` and create a new one. The model weights will be unaffected and the overhead should be negligible vs. the runtime of one epoch.",
        "pr_file_module": null
      },
      {
        "comment_id": "2132809961",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2128619874",
        "commented_code": "@@ -312,12 +337,27 @@ static ggml_cgraph * dup_graph(ggml_context * ctx, ggml_cgraph * src) {\n     return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(struct ggml_opt_optimizer_params const & opt_pars) {\n+    return opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    struct ggml_opt_optimizer_params const & opt_pars = opt_ctx->opt_pars();\n+    const bool                               moment =\n+        // or has_moment is per https://github.com/ggml-org/llama.cpp/pull/13873#discussion_r2116366095\n+        // - do not allow sgd epochs after adamw\n+        // (which in the current impl would require clearing the previously n_nodes-sized grad_m and grad_v)\n+        opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && (need_moment(opt_pars) || has_moment(opt_ctx));\n+\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
        "comment_created_at": "2025-06-06T19:35:19+00:00",
        "comment_author": "graehl",
        "comment_body": " right, i can agree with all that. still unclear on exactly what you want changed. feel free to change what you like as long as it still works. probably you can do that with more confidence than me.",
        "pr_file_module": null
      },
      {
        "comment_id": "2132902399",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2128619874",
        "commented_code": "@@ -312,12 +337,27 @@ static ggml_cgraph * dup_graph(ggml_context * ctx, ggml_cgraph * src) {\n     return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(struct ggml_opt_optimizer_params const & opt_pars) {\n+    return opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    struct ggml_opt_optimizer_params const & opt_pars = opt_ctx->opt_pars();\n+    const bool                               moment =\n+        // or has_moment is per https://github.com/ggml-org/llama.cpp/pull/13873#discussion_r2116366095\n+        // - do not allow sgd epochs after adamw\n+        // (which in the current impl would require clearing the previously n_nodes-sized grad_m and grad_v)\n+        opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && (need_moment(opt_pars) || has_moment(opt_ctx));\n+\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
        "comment_created_at": "2025-06-06T20:59:39+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Add the optimizer type to `ggml_opt_params` and `ggml_opt_context`, set the value of the context during initialization, do not change it afterwards, use it for the `need_moment` logic. Does that make it clear enough?",
        "pr_file_module": null
      },
      {
        "comment_id": "2132973129",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2128619874",
        "commented_code": "@@ -312,12 +337,27 @@ static ggml_cgraph * dup_graph(ggml_context * ctx, ggml_cgraph * src) {\n     return dst;\n }\n \n+static inline bool has_moment(ggml_opt_context_t opt_ctx) {\n+    return !opt_ctx->grad_m.empty();\n+}\n+\n+static inline bool need_moment(struct ggml_opt_optimizer_params const & opt_pars) {\n+    return opt_pars.optimizer == GGML_OPT_OPTIMIZER_ADAMW;\n+}\n+\n static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     GGML_ASSERT(opt_ctx->ctx_compute && \"no compute context set, either use static graphs or set one with ggml_opt_prepare_alloc\");\n     GGML_ASSERT((!opt_ctx->static_graphs || opt_ctx->inputs->data) && \"when using static graphs the inputs must be allocated statically\");\n \n+    struct ggml_opt_optimizer_params const & opt_pars = opt_ctx->opt_pars();\n+    const bool                               moment =\n+        // or has_moment is per https://github.com/ggml-org/llama.cpp/pull/13873#discussion_r2116366095\n+        // - do not allow sgd epochs after adamw\n+        // (which in the current impl would require clearing the previously n_nodes-sized grad_m and grad_v)\n+        opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && (need_moment(opt_pars) || has_moment(opt_ctx));\n+\n     const bool accumulate = opt_ctx->build_type_alloc >= GGML_OPT_BUILD_TYPE_GRAD &&\n-        !(opt_ctx->static_graphs && opt_ctx->build_type_alloc == GGML_OPT_BUILD_TYPE_OPT && opt_ctx->opt_period == 1);\n+                            !(opt_ctx->static_graphs && moment && opt_ctx->opt_period == 1);",
        "comment_created_at": "2025-06-06T22:26:46+00:00",
        "comment_author": "graehl",
        "comment_body": "no, those instructions don't parse. i have already put the optimizer type in context. can you refer to the latest push where we have already:\r\n\r\n    const enum ggml_opt_optimizer_type optimizer = opt_ctx->optimizer;\r\n    ...\r\n    const bool need_moment = optimizer == GGML_OPT_OPTIMIZER_TYPE_ADAMW;\r\n\r\nit might be that i already did what you wanted, perhaps.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2200644023",
    "pr_number": 13873,
    "pr_file": "ggml/src/ggml-opt.cpp",
    "created_at": "2025-07-11T12:46:57+00:00",
    "commented_code": "void ggml_opt_eval(ggml_opt_context_t opt_ctx, ggml_opt_result_t result) {\n     GGML_ASSERT(opt_ctx->eval_ready);\n     if (opt_ctx->allocated_graph == opt_ctx->gb_opt) {\n-        struct ggml_opt_optimizer_params opt_pars = opt_ctx->get_opt_pars(opt_ctx->get_opt_pars_ud);\n-\n-        GGML_ASSERT(opt_pars.adamw.alpha >  0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta1 >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta1 <= 1.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta2 >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta2 <= 1.0f);\n-        GGML_ASSERT(opt_pars.adamw.eps   >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.wd    >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.wd    <= 1.0f);\n-\n-        // beta1, beta2 after applying warmup\n-        const float beta1h = 1.0f/(1.0f - powf(opt_pars.adamw.beta1, opt_ctx->iter));\n-        const float beta2h = 1.0f/(1.0f - powf(opt_pars.adamw.beta2, opt_ctx->iter));\n-\n-        float * adamw_par_data = ggml_get_data_f32(opt_ctx->adamw_params);\n-        adamw_par_data[0] = opt_pars.adamw.alpha;\n-        adamw_par_data[1] = opt_pars.adamw.beta1;\n-        adamw_par_data[2] = opt_pars.adamw.beta2;\n-        adamw_par_data[3] = opt_pars.adamw.eps;\n-        adamw_par_data[4] = opt_pars.adamw.wd;\n-        adamw_par_data[5] = beta1h;\n-        adamw_par_data[6] = beta2h;\n+        const ggml_opt_optimizer_params & opt_pars = opt_ctx->opt_pars();\n+\n+        switch (opt_ctx->optimizer) {\n+            case GGML_OPT_OPTIMIZER_TYPE_ADAMW:\n+                {\n+                    GGML_ASSERT(opt_pars.adamw.alpha > 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta1 >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta1 <= 1.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta2 >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta2 <= 1.0f);\n+                    GGML_ASSERT(opt_pars.adamw.eps >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.wd >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.wd <= 1.0f);\n+                    // beta1, beta2 after applying warmup\n+                    const float beta1h         = 1.0f / (1.0f - powf(opt_pars.adamw.beta1, opt_ctx->iter));\n+                    const float beta2h         = 1.0f / (1.0f - powf(opt_pars.adamw.beta2, opt_ctx->iter));\n+                    float *     adamw_par_data = ggml_get_data_f32(opt_ctx->adamw_params);\n+                    adamw_par_data[0]          = opt_pars.adamw.alpha;\n+                    adamw_par_data[1]          = opt_pars.adamw.beta1;\n+                    adamw_par_data[2]          = opt_pars.adamw.beta2;\n+                    adamw_par_data[3]          = opt_pars.adamw.eps;\n+                    adamw_par_data[4]          = opt_pars.adamw.wd;\n+                    adamw_par_data[5]          = beta1h;\n+                    adamw_par_data[6]          = beta2h;\n+                }\n+                break;\n+\n+            case GGML_OPT_OPTIMIZER_TYPE_SGD:\n+                {\n+                    GGML_ASSERT(opt_pars.sgd.alpha > 0.0f);\n+                    GGML_ASSERT(opt_pars.sgd.wd >= 0.0f);\n+                    GGML_ASSERT(opt_pars.sgd.wd <= 1.0f);\n+                    float * sgd = ggml_get_data_f32(opt_ctx->adamw_params);\n+                    sgd[1]      = 1. - (sgd[0] = opt_pars.sgd.alpha) * opt_pars.sgd.wd;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2200644023",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2200644023",
        "commented_code": "@@ -756,29 +794,46 @@ void ggml_opt_alloc(ggml_opt_context_t opt_ctx, bool backward) {\n void ggml_opt_eval(ggml_opt_context_t opt_ctx, ggml_opt_result_t result) {\n     GGML_ASSERT(opt_ctx->eval_ready);\n     if (opt_ctx->allocated_graph == opt_ctx->gb_opt) {\n-        struct ggml_opt_optimizer_params opt_pars = opt_ctx->get_opt_pars(opt_ctx->get_opt_pars_ud);\n-\n-        GGML_ASSERT(opt_pars.adamw.alpha >  0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta1 >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta1 <= 1.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta2 >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta2 <= 1.0f);\n-        GGML_ASSERT(opt_pars.adamw.eps   >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.wd    >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.wd    <= 1.0f);\n-\n-        // beta1, beta2 after applying warmup\n-        const float beta1h = 1.0f/(1.0f - powf(opt_pars.adamw.beta1, opt_ctx->iter));\n-        const float beta2h = 1.0f/(1.0f - powf(opt_pars.adamw.beta2, opt_ctx->iter));\n-\n-        float * adamw_par_data = ggml_get_data_f32(opt_ctx->adamw_params);\n-        adamw_par_data[0] = opt_pars.adamw.alpha;\n-        adamw_par_data[1] = opt_pars.adamw.beta1;\n-        adamw_par_data[2] = opt_pars.adamw.beta2;\n-        adamw_par_data[3] = opt_pars.adamw.eps;\n-        adamw_par_data[4] = opt_pars.adamw.wd;\n-        adamw_par_data[5] = beta1h;\n-        adamw_par_data[6] = beta2h;\n+        const ggml_opt_optimizer_params & opt_pars = opt_ctx->opt_pars();\n+\n+        switch (opt_ctx->optimizer) {\n+            case GGML_OPT_OPTIMIZER_TYPE_ADAMW:\n+                {\n+                    GGML_ASSERT(opt_pars.adamw.alpha > 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta1 >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta1 <= 1.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta2 >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta2 <= 1.0f);\n+                    GGML_ASSERT(opt_pars.adamw.eps >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.wd >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.wd <= 1.0f);\n+                    // beta1, beta2 after applying warmup\n+                    const float beta1h         = 1.0f / (1.0f - powf(opt_pars.adamw.beta1, opt_ctx->iter));\n+                    const float beta2h         = 1.0f / (1.0f - powf(opt_pars.adamw.beta2, opt_ctx->iter));\n+                    float *     adamw_par_data = ggml_get_data_f32(opt_ctx->adamw_params);\n+                    adamw_par_data[0]          = opt_pars.adamw.alpha;\n+                    adamw_par_data[1]          = opt_pars.adamw.beta1;\n+                    adamw_par_data[2]          = opt_pars.adamw.beta2;\n+                    adamw_par_data[3]          = opt_pars.adamw.eps;\n+                    adamw_par_data[4]          = opt_pars.adamw.wd;\n+                    adamw_par_data[5]          = beta1h;\n+                    adamw_par_data[6]          = beta2h;\n+                }\n+                break;\n+\n+            case GGML_OPT_OPTIMIZER_TYPE_SGD:\n+                {\n+                    GGML_ASSERT(opt_pars.sgd.alpha > 0.0f);\n+                    GGML_ASSERT(opt_pars.sgd.wd >= 0.0f);\n+                    GGML_ASSERT(opt_pars.sgd.wd <= 1.0f);\n+                    float * sgd = ggml_get_data_f32(opt_ctx->adamw_params);\n+                    sgd[1]      = 1. - (sgd[0] = opt_pars.sgd.alpha) * opt_pars.sgd.wd;",
        "comment_created_at": "2025-07-11T12:46:57+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "This is inconsistent with the docstring in `ggml.h`. As I outlined before for AdamW, the interface in `ggml.h` should be using the human-readable parameters. Please simply pass `alpha` and `wd` here. A derived parameter `keep` should be calculated in the backend-specific implementations for `OPT_STEP_SGD`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2205851819",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2200644023",
        "commented_code": "@@ -756,29 +794,46 @@ void ggml_opt_alloc(ggml_opt_context_t opt_ctx, bool backward) {\n void ggml_opt_eval(ggml_opt_context_t opt_ctx, ggml_opt_result_t result) {\n     GGML_ASSERT(opt_ctx->eval_ready);\n     if (opt_ctx->allocated_graph == opt_ctx->gb_opt) {\n-        struct ggml_opt_optimizer_params opt_pars = opt_ctx->get_opt_pars(opt_ctx->get_opt_pars_ud);\n-\n-        GGML_ASSERT(opt_pars.adamw.alpha >  0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta1 >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta1 <= 1.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta2 >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.beta2 <= 1.0f);\n-        GGML_ASSERT(opt_pars.adamw.eps   >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.wd    >= 0.0f);\n-        GGML_ASSERT(opt_pars.adamw.wd    <= 1.0f);\n-\n-        // beta1, beta2 after applying warmup\n-        const float beta1h = 1.0f/(1.0f - powf(opt_pars.adamw.beta1, opt_ctx->iter));\n-        const float beta2h = 1.0f/(1.0f - powf(opt_pars.adamw.beta2, opt_ctx->iter));\n-\n-        float * adamw_par_data = ggml_get_data_f32(opt_ctx->adamw_params);\n-        adamw_par_data[0] = opt_pars.adamw.alpha;\n-        adamw_par_data[1] = opt_pars.adamw.beta1;\n-        adamw_par_data[2] = opt_pars.adamw.beta2;\n-        adamw_par_data[3] = opt_pars.adamw.eps;\n-        adamw_par_data[4] = opt_pars.adamw.wd;\n-        adamw_par_data[5] = beta1h;\n-        adamw_par_data[6] = beta2h;\n+        const ggml_opt_optimizer_params & opt_pars = opt_ctx->opt_pars();\n+\n+        switch (opt_ctx->optimizer) {\n+            case GGML_OPT_OPTIMIZER_TYPE_ADAMW:\n+                {\n+                    GGML_ASSERT(opt_pars.adamw.alpha > 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta1 >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta1 <= 1.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta2 >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.beta2 <= 1.0f);\n+                    GGML_ASSERT(opt_pars.adamw.eps >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.wd >= 0.0f);\n+                    GGML_ASSERT(opt_pars.adamw.wd <= 1.0f);\n+                    // beta1, beta2 after applying warmup\n+                    const float beta1h         = 1.0f / (1.0f - powf(opt_pars.adamw.beta1, opt_ctx->iter));\n+                    const float beta2h         = 1.0f / (1.0f - powf(opt_pars.adamw.beta2, opt_ctx->iter));\n+                    float *     adamw_par_data = ggml_get_data_f32(opt_ctx->adamw_params);\n+                    adamw_par_data[0]          = opt_pars.adamw.alpha;\n+                    adamw_par_data[1]          = opt_pars.adamw.beta1;\n+                    adamw_par_data[2]          = opt_pars.adamw.beta2;\n+                    adamw_par_data[3]          = opt_pars.adamw.eps;\n+                    adamw_par_data[4]          = opt_pars.adamw.wd;\n+                    adamw_par_data[5]          = beta1h;\n+                    adamw_par_data[6]          = beta2h;\n+                }\n+                break;\n+\n+            case GGML_OPT_OPTIMIZER_TYPE_SGD:\n+                {\n+                    GGML_ASSERT(opt_pars.sgd.alpha > 0.0f);\n+                    GGML_ASSERT(opt_pars.sgd.wd >= 0.0f);\n+                    GGML_ASSERT(opt_pars.sgd.wd <= 1.0f);\n+                    float * sgd = ggml_get_data_f32(opt_ctx->adamw_params);\n+                    sgd[1]      = 1. - (sgd[0] = opt_pars.sgd.alpha) * opt_pars.sgd.wd;",
        "comment_created_at": "2025-07-14T21:21:27+00:00",
        "comment_author": "graehl",
        "comment_body": "this is fine by me but i'm holding off for now",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2204699150",
    "pr_number": 14658,
    "pr_file": "src/llama-model.cpp",
    "created_at": "2025-07-14T11:47:04+00:00",
    "commented_code": "cb(cur, \"result_output\", -1);\n         res->t_logits = cur;\n \n+        ggml_build_forward_expand(gf, cur);\n+    }\n+    };\n+\n+struct llm_build_ernie4_5_moe : public llm_graph_context {\n+    llm_build_ernie4_5_moe(const llama_model & model, const llm_graph_params & params, ggml_cgraph * gf) : llm_graph_context(params) {\n+        const int64_t n_embd_head = hparams.n_embd_head_v;\n+\n+        GGML_ASSERT(n_embd_head == hparams.n_embd_head_k);\n+        GGML_ASSERT(n_embd_head == hparams.n_rot);\n+\n+        ggml_tensor * cur;\n+        ggml_tensor * inpL;\n+\n+        inpL = build_inp_embd(model.tok_embd);\n+\n+        // inp_pos - contains the positions\n+        ggml_tensor * inp_pos = build_inp_pos();\n+\n+        auto * inp_attn = build_attn_inp_kv_unified();\n+\n+        ggml_tensor * inp_out_ids = build_inp_out_ids();\n+\n+        for (int il = 0; il < n_layer; ++il) {\n+            ggml_tensor * inpSA = inpL;\n+            // norm\n+            {\n+                cur = build_norm(inpL,\n+                        model.layers[il].attn_norm, NULL,\n+                        LLM_NORM_RMS, il);\n+                cb(cur, \"attn_norm\", il);\n+            }\n+\n+            // self-attention\n+            {\n+                // compute Q and K and RoPE them\n+                ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur);\n+                cb(Qcur, \"Qcur\", il);\n+                if (model.layers[il].bq) {\n+                    Qcur = ggml_add(ctx0, Qcur, model.layers[il].bq);\n+                    cb(Qcur, \"Qcur\", il);\n+                }\n+\n+                ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur);\n+                cb(Kcur, \"Kcur\", il);\n+                if (model.layers[il].bk) {\n+                    Kcur = ggml_add(ctx0, Kcur, model.layers[il].bk);\n+                    cb(Kcur, \"Kcur\", il);\n+                }\n+\n+                ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur);\n+                cb(Vcur, \"Vcur\", il);\n+                if (model.layers[il].bv) {\n+                    Vcur = ggml_add(ctx0, Vcur, model.layers[il].bv);\n+                    cb(Vcur, \"Vcur\", il);\n+                }\n+\n+                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n+                Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n+                Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);\n+\n+                const float freq_base_l  = model.get_rope_freq_base (cparams, il);\n+                const float freq_scale_l = model.get_rope_freq_scale(cparams, il);\n+                Qcur = ggml_rope_ext(\n+                        ctx0, Qcur, inp_pos, nullptr,\n+                        n_rot, rope_type, n_ctx_orig, freq_base_l, freq_scale_l,\n+                        ext_factor, attn_factor, beta_fast, beta_slow\n+                        );\n+\n+                Kcur = ggml_rope_ext(\n+                        ctx0, Kcur, inp_pos, nullptr,\n+                        n_rot, rope_type, n_ctx_orig, freq_base_l, freq_scale_l,\n+                        ext_factor, attn_factor, beta_fast, beta_slow\n+                        );\n+\n+                cb(Qcur, \"Qcur\", il);\n+                cb(Kcur, \"Kcur\", il);\n+                cb(Vcur, \"Vcur\", il);\n+\n+                cur = build_attn(inp_attn, gf,\n+                        model.layers[il].wo, NULL,\n+                        Qcur, Kcur, Vcur, nullptr, nullptr, 1.0f/sqrtf(float(n_embd_head)), il);\n+                cb(cur, \"attn_out\", il);\n+            }\n+\n+            if (il == n_layer - 1 && inp_out_ids) {\n+                cur   = ggml_get_rows(ctx0,   cur, inp_out_ids);\n+                inpSA = ggml_get_rows(ctx0, inpSA, inp_out_ids);\n+            }\n+\n+            ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n+            cb(ffn_inp, \"ffn_inp\", il);\n+\n+            // feed-forward network\n+            bool is_moe_layer = arch == LLM_ARCH_ERNIE4_5_MOE && hparams.n_moe_layer_step > 0 && static_cast<uint32_t>(il) >= hparams.n_moe_layer_step;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2204699150",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14658,
        "pr_file": "src/llama-model.cpp",
        "discussion_id": "2204699150",
        "commented_code": "@@ -8316,6 +8342,177 @@ struct llm_build_phi2 : public llm_graph_context {\n         cb(cur, \"result_output\", -1);\n         res->t_logits = cur;\n \n+        ggml_build_forward_expand(gf, cur);\n+    }\n+    };\n+\n+struct llm_build_ernie4_5_moe : public llm_graph_context {\n+    llm_build_ernie4_5_moe(const llama_model & model, const llm_graph_params & params, ggml_cgraph * gf) : llm_graph_context(params) {\n+        const int64_t n_embd_head = hparams.n_embd_head_v;\n+\n+        GGML_ASSERT(n_embd_head == hparams.n_embd_head_k);\n+        GGML_ASSERT(n_embd_head == hparams.n_rot);\n+\n+        ggml_tensor * cur;\n+        ggml_tensor * inpL;\n+\n+        inpL = build_inp_embd(model.tok_embd);\n+\n+        // inp_pos - contains the positions\n+        ggml_tensor * inp_pos = build_inp_pos();\n+\n+        auto * inp_attn = build_attn_inp_kv_unified();\n+\n+        ggml_tensor * inp_out_ids = build_inp_out_ids();\n+\n+        for (int il = 0; il < n_layer; ++il) {\n+            ggml_tensor * inpSA = inpL;\n+            // norm\n+            {\n+                cur = build_norm(inpL,\n+                        model.layers[il].attn_norm, NULL,\n+                        LLM_NORM_RMS, il);\n+                cb(cur, \"attn_norm\", il);\n+            }\n+\n+            // self-attention\n+            {\n+                // compute Q and K and RoPE them\n+                ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur);\n+                cb(Qcur, \"Qcur\", il);\n+                if (model.layers[il].bq) {\n+                    Qcur = ggml_add(ctx0, Qcur, model.layers[il].bq);\n+                    cb(Qcur, \"Qcur\", il);\n+                }\n+\n+                ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur);\n+                cb(Kcur, \"Kcur\", il);\n+                if (model.layers[il].bk) {\n+                    Kcur = ggml_add(ctx0, Kcur, model.layers[il].bk);\n+                    cb(Kcur, \"Kcur\", il);\n+                }\n+\n+                ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur);\n+                cb(Vcur, \"Vcur\", il);\n+                if (model.layers[il].bv) {\n+                    Vcur = ggml_add(ctx0, Vcur, model.layers[il].bv);\n+                    cb(Vcur, \"Vcur\", il);\n+                }\n+\n+                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n+                Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n+                Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);\n+\n+                const float freq_base_l  = model.get_rope_freq_base (cparams, il);\n+                const float freq_scale_l = model.get_rope_freq_scale(cparams, il);\n+                Qcur = ggml_rope_ext(\n+                        ctx0, Qcur, inp_pos, nullptr,\n+                        n_rot, rope_type, n_ctx_orig, freq_base_l, freq_scale_l,\n+                        ext_factor, attn_factor, beta_fast, beta_slow\n+                        );\n+\n+                Kcur = ggml_rope_ext(\n+                        ctx0, Kcur, inp_pos, nullptr,\n+                        n_rot, rope_type, n_ctx_orig, freq_base_l, freq_scale_l,\n+                        ext_factor, attn_factor, beta_fast, beta_slow\n+                        );\n+\n+                cb(Qcur, \"Qcur\", il);\n+                cb(Kcur, \"Kcur\", il);\n+                cb(Vcur, \"Vcur\", il);\n+\n+                cur = build_attn(inp_attn, gf,\n+                        model.layers[il].wo, NULL,\n+                        Qcur, Kcur, Vcur, nullptr, nullptr, 1.0f/sqrtf(float(n_embd_head)), il);\n+                cb(cur, \"attn_out\", il);\n+            }\n+\n+            if (il == n_layer - 1 && inp_out_ids) {\n+                cur   = ggml_get_rows(ctx0,   cur, inp_out_ids);\n+                inpSA = ggml_get_rows(ctx0, inpSA, inp_out_ids);\n+            }\n+\n+            ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n+            cb(ffn_inp, \"ffn_inp\", il);\n+\n+            // feed-forward network\n+            bool is_moe_layer = arch == LLM_ARCH_ERNIE4_5_MOE && hparams.n_moe_layer_step > 0 && static_cast<uint32_t>(il) >= hparams.n_moe_layer_step;",
        "comment_created_at": "2025-07-14T11:47:04+00:00",
        "comment_author": "CISC",
        "comment_body": "```suggestion\r\n            bool is_moe_layer = il >= hparams.n_layer_dense_lead && (il + 1) % hparams.n_moe_layer_step == 0;\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2204732149",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14658,
        "pr_file": "src/llama-model.cpp",
        "discussion_id": "2204699150",
        "commented_code": "@@ -8316,6 +8342,177 @@ struct llm_build_phi2 : public llm_graph_context {\n         cb(cur, \"result_output\", -1);\n         res->t_logits = cur;\n \n+        ggml_build_forward_expand(gf, cur);\n+    }\n+    };\n+\n+struct llm_build_ernie4_5_moe : public llm_graph_context {\n+    llm_build_ernie4_5_moe(const llama_model & model, const llm_graph_params & params, ggml_cgraph * gf) : llm_graph_context(params) {\n+        const int64_t n_embd_head = hparams.n_embd_head_v;\n+\n+        GGML_ASSERT(n_embd_head == hparams.n_embd_head_k);\n+        GGML_ASSERT(n_embd_head == hparams.n_rot);\n+\n+        ggml_tensor * cur;\n+        ggml_tensor * inpL;\n+\n+        inpL = build_inp_embd(model.tok_embd);\n+\n+        // inp_pos - contains the positions\n+        ggml_tensor * inp_pos = build_inp_pos();\n+\n+        auto * inp_attn = build_attn_inp_kv_unified();\n+\n+        ggml_tensor * inp_out_ids = build_inp_out_ids();\n+\n+        for (int il = 0; il < n_layer; ++il) {\n+            ggml_tensor * inpSA = inpL;\n+            // norm\n+            {\n+                cur = build_norm(inpL,\n+                        model.layers[il].attn_norm, NULL,\n+                        LLM_NORM_RMS, il);\n+                cb(cur, \"attn_norm\", il);\n+            }\n+\n+            // self-attention\n+            {\n+                // compute Q and K and RoPE them\n+                ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur);\n+                cb(Qcur, \"Qcur\", il);\n+                if (model.layers[il].bq) {\n+                    Qcur = ggml_add(ctx0, Qcur, model.layers[il].bq);\n+                    cb(Qcur, \"Qcur\", il);\n+                }\n+\n+                ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur);\n+                cb(Kcur, \"Kcur\", il);\n+                if (model.layers[il].bk) {\n+                    Kcur = ggml_add(ctx0, Kcur, model.layers[il].bk);\n+                    cb(Kcur, \"Kcur\", il);\n+                }\n+\n+                ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur);\n+                cb(Vcur, \"Vcur\", il);\n+                if (model.layers[il].bv) {\n+                    Vcur = ggml_add(ctx0, Vcur, model.layers[il].bv);\n+                    cb(Vcur, \"Vcur\", il);\n+                }\n+\n+                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n+                Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n+                Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);\n+\n+                const float freq_base_l  = model.get_rope_freq_base (cparams, il);\n+                const float freq_scale_l = model.get_rope_freq_scale(cparams, il);\n+                Qcur = ggml_rope_ext(\n+                        ctx0, Qcur, inp_pos, nullptr,\n+                        n_rot, rope_type, n_ctx_orig, freq_base_l, freq_scale_l,\n+                        ext_factor, attn_factor, beta_fast, beta_slow\n+                        );\n+\n+                Kcur = ggml_rope_ext(\n+                        ctx0, Kcur, inp_pos, nullptr,\n+                        n_rot, rope_type, n_ctx_orig, freq_base_l, freq_scale_l,\n+                        ext_factor, attn_factor, beta_fast, beta_slow\n+                        );\n+\n+                cb(Qcur, \"Qcur\", il);\n+                cb(Kcur, \"Kcur\", il);\n+                cb(Vcur, \"Vcur\", il);\n+\n+                cur = build_attn(inp_attn, gf,\n+                        model.layers[il].wo, NULL,\n+                        Qcur, Kcur, Vcur, nullptr, nullptr, 1.0f/sqrtf(float(n_embd_head)), il);\n+                cb(cur, \"attn_out\", il);\n+            }\n+\n+            if (il == n_layer - 1 && inp_out_ids) {\n+                cur   = ggml_get_rows(ctx0,   cur, inp_out_ids);\n+                inpSA = ggml_get_rows(ctx0, inpSA, inp_out_ids);\n+            }\n+\n+            ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n+            cb(ffn_inp, \"ffn_inp\", il);\n+\n+            // feed-forward network\n+            bool is_moe_layer = arch == LLM_ARCH_ERNIE4_5_MOE && hparams.n_moe_layer_step > 0 && static_cast<uint32_t>(il) >= hparams.n_moe_layer_step;",
        "comment_created_at": "2025-07-14T12:04:46+00:00",
        "comment_author": "pwilkin",
        "comment_body": "This would resolve to `true` everywhere though since n_moe_layer_step is 1. Should probably be\r\n\r\n```\r\nbool is_moe_layer = static_cast<uint32_t>(il) >= hparams.moe_layer_start_index\r\n```\r\n\r\n(and I need to add moe_layer_start_index to hyperparams)",
        "pr_file_module": null
      },
      {
        "comment_id": "2204806627",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14658,
        "pr_file": "src/llama-model.cpp",
        "discussion_id": "2204699150",
        "commented_code": "@@ -8316,6 +8342,177 @@ struct llm_build_phi2 : public llm_graph_context {\n         cb(cur, \"result_output\", -1);\n         res->t_logits = cur;\n \n+        ggml_build_forward_expand(gf, cur);\n+    }\n+    };\n+\n+struct llm_build_ernie4_5_moe : public llm_graph_context {\n+    llm_build_ernie4_5_moe(const llama_model & model, const llm_graph_params & params, ggml_cgraph * gf) : llm_graph_context(params) {\n+        const int64_t n_embd_head = hparams.n_embd_head_v;\n+\n+        GGML_ASSERT(n_embd_head == hparams.n_embd_head_k);\n+        GGML_ASSERT(n_embd_head == hparams.n_rot);\n+\n+        ggml_tensor * cur;\n+        ggml_tensor * inpL;\n+\n+        inpL = build_inp_embd(model.tok_embd);\n+\n+        // inp_pos - contains the positions\n+        ggml_tensor * inp_pos = build_inp_pos();\n+\n+        auto * inp_attn = build_attn_inp_kv_unified();\n+\n+        ggml_tensor * inp_out_ids = build_inp_out_ids();\n+\n+        for (int il = 0; il < n_layer; ++il) {\n+            ggml_tensor * inpSA = inpL;\n+            // norm\n+            {\n+                cur = build_norm(inpL,\n+                        model.layers[il].attn_norm, NULL,\n+                        LLM_NORM_RMS, il);\n+                cb(cur, \"attn_norm\", il);\n+            }\n+\n+            // self-attention\n+            {\n+                // compute Q and K and RoPE them\n+                ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur);\n+                cb(Qcur, \"Qcur\", il);\n+                if (model.layers[il].bq) {\n+                    Qcur = ggml_add(ctx0, Qcur, model.layers[il].bq);\n+                    cb(Qcur, \"Qcur\", il);\n+                }\n+\n+                ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur);\n+                cb(Kcur, \"Kcur\", il);\n+                if (model.layers[il].bk) {\n+                    Kcur = ggml_add(ctx0, Kcur, model.layers[il].bk);\n+                    cb(Kcur, \"Kcur\", il);\n+                }\n+\n+                ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur);\n+                cb(Vcur, \"Vcur\", il);\n+                if (model.layers[il].bv) {\n+                    Vcur = ggml_add(ctx0, Vcur, model.layers[il].bv);\n+                    cb(Vcur, \"Vcur\", il);\n+                }\n+\n+                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n+                Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n+                Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);\n+\n+                const float freq_base_l  = model.get_rope_freq_base (cparams, il);\n+                const float freq_scale_l = model.get_rope_freq_scale(cparams, il);\n+                Qcur = ggml_rope_ext(\n+                        ctx0, Qcur, inp_pos, nullptr,\n+                        n_rot, rope_type, n_ctx_orig, freq_base_l, freq_scale_l,\n+                        ext_factor, attn_factor, beta_fast, beta_slow\n+                        );\n+\n+                Kcur = ggml_rope_ext(\n+                        ctx0, Kcur, inp_pos, nullptr,\n+                        n_rot, rope_type, n_ctx_orig, freq_base_l, freq_scale_l,\n+                        ext_factor, attn_factor, beta_fast, beta_slow\n+                        );\n+\n+                cb(Qcur, \"Qcur\", il);\n+                cb(Kcur, \"Kcur\", il);\n+                cb(Vcur, \"Vcur\", il);\n+\n+                cur = build_attn(inp_attn, gf,\n+                        model.layers[il].wo, NULL,\n+                        Qcur, Kcur, Vcur, nullptr, nullptr, 1.0f/sqrtf(float(n_embd_head)), il);\n+                cb(cur, \"attn_out\", il);\n+            }\n+\n+            if (il == n_layer - 1 && inp_out_ids) {\n+                cur   = ggml_get_rows(ctx0,   cur, inp_out_ids);\n+                inpSA = ggml_get_rows(ctx0, inpSA, inp_out_ids);\n+            }\n+\n+            ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n+            cb(ffn_inp, \"ffn_inp\", il);\n+\n+            // feed-forward network\n+            bool is_moe_layer = arch == LLM_ARCH_ERNIE4_5_MOE && hparams.n_moe_layer_step > 0 && static_cast<uint32_t>(il) >= hparams.n_moe_layer_step;",
        "comment_created_at": "2025-07-14T12:36:02+00:00",
        "comment_author": "CISC",
        "comment_body": "Yes, sorry I forgot `n_layer_dense_lead`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2153850313",
    "pr_number": 14217,
    "pr_file": "src/llama-batch.cpp",
    "created_at": "2025-06-18T07:25:35+00:00",
    "commented_code": "return seq_pos[seq_id].empty() ? -1 : *seq_pos[seq_id].rbegin();\n }\n \n+void llama_batch_allocr::split_reset() {\n+    out_ids.clear();\n+\n+    used.clear();\n+    used.resize(get_n_tokens(), false);\n+\n+    ubatches.clear();\n+}\n+\n+llama_ubatch llama_batch_allocr::split_simple(uint32_t n_ubatch) {\n+    // find the first unused token\n+    uint32_t cur_idx = 0;\n+    while (cur_idx < used.size() && used[cur_idx]) {\n+        ++cur_idx;\n+    }\n+\n+    // we are done\n+    if (cur_idx >= used.size()) {\n+        return {};\n+    }\n+\n+    std::vector<int32_t> idxs;\n+\n+    while (true) {\n+        idxs.push_back(cur_idx);\n+\n+        used[cur_idx] = true;\n+\n+        ++cur_idx;\n+\n+        if (cur_idx >= used.size()) {\n+            break;\n+        }\n+\n+        if (idxs.size() >= n_ubatch) {\n+            break;\n+        }\n+    }\n+\n+    return ubatch_add(idxs, idxs.size(), false);\n+}\n+\n+llama_ubatch llama_batch_allocr::split_equal(uint32_t n_ubatch) {",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2153850313",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14217,
        "pr_file": "src/llama-batch.cpp",
        "discussion_id": "2153850313",
        "commented_code": "@@ -551,14 +373,188 @@ llama_pos llama_batch_allocr::seq_pos_max(llama_seq_id seq_id) const {\n     return seq_pos[seq_id].empty() ? -1 : *seq_pos[seq_id].rbegin();\n }\n \n+void llama_batch_allocr::split_reset() {\n+    out_ids.clear();\n+\n+    used.clear();\n+    used.resize(get_n_tokens(), false);\n+\n+    ubatches.clear();\n+}\n+\n+llama_ubatch llama_batch_allocr::split_simple(uint32_t n_ubatch) {\n+    // find the first unused token\n+    uint32_t cur_idx = 0;\n+    while (cur_idx < used.size() && used[cur_idx]) {\n+        ++cur_idx;\n+    }\n+\n+    // we are done\n+    if (cur_idx >= used.size()) {\n+        return {};\n+    }\n+\n+    std::vector<int32_t> idxs;\n+\n+    while (true) {\n+        idxs.push_back(cur_idx);\n+\n+        used[cur_idx] = true;\n+\n+        ++cur_idx;\n+\n+        if (cur_idx >= used.size()) {\n+            break;\n+        }\n+\n+        if (idxs.size() >= n_ubatch) {\n+            break;\n+        }\n+    }\n+\n+    return ubatch_add(idxs, idxs.size(), false);\n+}\n+\n+llama_ubatch llama_batch_allocr::split_equal(uint32_t n_ubatch) {",
        "comment_created_at": "2025-06-18T07:25:35+00:00",
        "comment_author": "compilade",
        "comment_body": "Note that for equal splits, some sequence sets are not compatible (i.e. they can't be put in the same `ubatch`). For example, a sequence set containing multiple `seq_ids` cannot be mixed with one having a `seq_id` in the multi-sequence set.\r\n\r\nFor example, tokens with `seq_ids = { 0, 1, 2, 3 }` are not compatible with tokens in `seq_ids = { 1 }`.\r\n\r\nThe reason is that the recurrent states are only copied to the target sequences on ubatch boundaries, and so dependant tokens cannot be mixed with a shared trunk.\r\n\r\nIs this handled here?\r\n\r\nBasically the main constraint to check would be that the sequence sets in a ubatch are independent (at least, I think that would be sufficient?).\r\n\r\n(Before this PR, it was handled by splitting multi-sequence token groups on their own before the single-sequence tokens)\r\n\r\n(I did not implement multi-sequence tests yet in #14139, but that should also be able to answer this question once implemented)",
        "pr_file_module": null
      },
      {
        "comment_id": "2153952527",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14217,
        "pr_file": "src/llama-batch.cpp",
        "discussion_id": "2153850313",
        "commented_code": "@@ -551,14 +373,188 @@ llama_pos llama_batch_allocr::seq_pos_max(llama_seq_id seq_id) const {\n     return seq_pos[seq_id].empty() ? -1 : *seq_pos[seq_id].rbegin();\n }\n \n+void llama_batch_allocr::split_reset() {\n+    out_ids.clear();\n+\n+    used.clear();\n+    used.resize(get_n_tokens(), false);\n+\n+    ubatches.clear();\n+}\n+\n+llama_ubatch llama_batch_allocr::split_simple(uint32_t n_ubatch) {\n+    // find the first unused token\n+    uint32_t cur_idx = 0;\n+    while (cur_idx < used.size() && used[cur_idx]) {\n+        ++cur_idx;\n+    }\n+\n+    // we are done\n+    if (cur_idx >= used.size()) {\n+        return {};\n+    }\n+\n+    std::vector<int32_t> idxs;\n+\n+    while (true) {\n+        idxs.push_back(cur_idx);\n+\n+        used[cur_idx] = true;\n+\n+        ++cur_idx;\n+\n+        if (cur_idx >= used.size()) {\n+            break;\n+        }\n+\n+        if (idxs.size() >= n_ubatch) {\n+            break;\n+        }\n+    }\n+\n+    return ubatch_add(idxs, idxs.size(), false);\n+}\n+\n+llama_ubatch llama_batch_allocr::split_equal(uint32_t n_ubatch) {",
        "comment_created_at": "2025-06-18T08:12:22+00:00",
        "comment_author": "ggerganov",
        "comment_body": "> For example, a sequence set containing multiple seq_ids cannot be mixed with one having a seq_id in the multi-sequence set.\r\n\r\nYes, this logic here at the beginning of the function determines the unique non-overlapping sequence sets that will be contained in this ubatch:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/034b0557987c31d624854e923b505d4541399c7b/src/llama-batch.cpp#L421-L446",
        "pr_file_module": null
      }
    ]
  }
]