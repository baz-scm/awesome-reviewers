[
  {
    "discussion_id": "2209224360",
    "pr_number": 14644,
    "pr_file": "examples/diffusion/diffusion.h",
    "created_at": "2025-07-16T05:01:17+00:00",
    "commented_code": "+#pragma once\n+\n+#include \"llama.h\"\n+\n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n+typedef bool (*diffusion_step_callback_t)(int32_t step, int32_t total_steps, const llama_token * tokens,\n+                                          int32_t n_tokens, void * user_data);\n+\n+enum diffusion_algorithm {\n+    DIFFUSION_ALG_ORIGIN       = 0,\n+    DIFFUSION_ALG_MASKGIT_PLUS = 1,\n+    DIFFUSION_ALG_TOPK_MARGIN  = 2,\n+    DIFFUSION_ALG_ENTROPY      = 3,\n+};",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2209224360",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14644,
        "pr_file": "examples/diffusion/diffusion.h",
        "discussion_id": "2209224360",
        "commented_code": "@@ -0,0 +1,45 @@\n+#pragma once\n+\n+#include \"llama.h\"\n+\n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n+typedef bool (*diffusion_step_callback_t)(int32_t step, int32_t total_steps, const llama_token * tokens,\n+                                          int32_t n_tokens, void * user_data);\n+\n+enum diffusion_algorithm {\n+    DIFFUSION_ALG_ORIGIN       = 0,\n+    DIFFUSION_ALG_MASKGIT_PLUS = 1,\n+    DIFFUSION_ALG_TOPK_MARGIN  = 2,\n+    DIFFUSION_ALG_ENTROPY      = 3,\n+};",
        "comment_created_at": "2025-07-16T05:01:17+00:00",
        "comment_author": "ggerganov",
        "comment_body": "The convention is to match the enum name with the prefix of the values:\r\n\r\n```suggestion\r\nenum diffusion_alg {\r\n    DIFFUSION_ALG_ORIGIN       = 0,\r\n    DIFFUSION_ALG_MASKGIT_PLUS = 1,\r\n    DIFFUSION_ALG_TOPK_MARGIN  = 2,\r\n    DIFFUSION_ALG_ENTROPY      = 3,\r\n};\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1935541362",
    "pr_number": 9639,
    "pr_file": "include/llama.h",
    "created_at": "2025-01-30T12:37:24+00:00",
    "commented_code": "LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\n             const struct llama_vocab * vocab,\n                           const char * grammar_str,\n-                          const char * grammar_root);\n+                          const char * grammar_root,\n+                                  bool lazy,\n+                         const char ** trigger_words,\n+                                size_t num_trigger_words,\n+                   const llama_token * trigger_tokens,\n+                                size_t num_trigger_tokens);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "1935541362",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "include/llama.h",
        "discussion_id": "1935541362",
        "commented_code": "@@ -1197,7 +1197,12 @@ extern \"C\" {\n     LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\n             const struct llama_vocab * vocab,\n                           const char * grammar_str,\n-                          const char * grammar_root);\n+                          const char * grammar_root,\n+                                  bool lazy,\n+                         const char ** trigger_words,\n+                                size_t num_trigger_words,\n+                   const llama_token * trigger_tokens,\n+                                size_t num_trigger_tokens);",
        "comment_created_at": "2025-01-30T12:37:24+00:00",
        "comment_author": "ggerganov",
        "comment_body": "Add a `DEPRECATED` overload of this call to make it easier for developers to adjust to the change.",
        "pr_file_module": null
      },
      {
        "comment_id": "1935559506",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "include/llama.h",
        "discussion_id": "1935541362",
        "commented_code": "@@ -1197,7 +1197,12 @@ extern \"C\" {\n     LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\n             const struct llama_vocab * vocab,\n                           const char * grammar_str,\n-                          const char * grammar_root);\n+                          const char * grammar_root,\n+                                  bool lazy,\n+                         const char ** trigger_words,\n+                                size_t num_trigger_words,\n+                   const llama_token * trigger_tokens,\n+                                size_t num_trigger_tokens);",
        "comment_created_at": "2025-01-30T12:50:55+00:00",
        "comment_author": "ochafik",
        "comment_body": "Done, thanks! (-> llama_sampler_grammar_init)",
        "pr_file_module": null
      },
      {
        "comment_id": "1935571823",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "include/llama.h",
        "discussion_id": "1935541362",
        "commented_code": "@@ -1197,7 +1197,12 @@ extern \"C\" {\n     LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\n             const struct llama_vocab * vocab,\n                           const char * grammar_str,\n-                          const char * grammar_root);\n+                          const char * grammar_root,\n+                                  bool lazy,\n+                         const char ** trigger_words,\n+                                size_t num_trigger_words,\n+                   const llama_token * trigger_tokens,\n+                                size_t num_trigger_tokens);",
        "comment_created_at": "2025-01-30T12:59:52+00:00",
        "comment_author": "ggerganov",
        "comment_body": "Ah, we break the `llama_sampler_init_...` pattern if we do it like this. Maybe like this instead:\r\n\r\n```c\r\nllama_sampler_init_grammar(...); // same as master\r\nllama_sampler_init_grammar_lazy(...); // same as PR but without `bool lazy`?\r\n```\r\n\r\np.s. huh, naming things is difficult \ud83d\ude04 ",
        "pr_file_module": null
      },
      {
        "comment_id": "1935679933",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 9639,
        "pr_file": "include/llama.h",
        "discussion_id": "1935541362",
        "commented_code": "@@ -1197,7 +1197,12 @@ extern \"C\" {\n     LLAMA_API struct llama_sampler * llama_sampler_init_grammar(\n             const struct llama_vocab * vocab,\n                           const char * grammar_str,\n-                          const char * grammar_root);\n+                          const char * grammar_root,\n+                                  bool lazy,\n+                         const char ** trigger_words,\n+                                size_t num_trigger_words,\n+                   const llama_token * trigger_tokens,\n+                                size_t num_trigger_tokens);",
        "comment_created_at": "2025-01-30T14:11:29+00:00",
        "comment_author": "ochafik",
        "comment_body": "Oops, updated, thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190839476",
    "pr_number": 14534,
    "pr_file": "src/llama-arch.h",
    "created_at": "2025-07-07T19:06:11+00:00",
    "commented_code": "LLM_KV_ATTENTION_VALUE_LENGTH_MLA,\n     LLM_KV_ATTENTION_LAYER_INDICES,\n \n+    // Falcon-H1 specific\n+    LLM_KV_ATTN_HEAD_DIM,\n+    LLM_KV_SSM_HEAD_DIM,\n+    LLM_KV_MAMBA_D_SSM,\n+    LLM_KV_N_LAYER,\n+    LLM_KV_FALCON_H1_MAMBA_RMS_NORM,",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2190839476",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14534,
        "pr_file": "src/llama-arch.h",
        "discussion_id": "2190839476",
        "commented_code": "@@ -156,6 +157,13 @@ enum llm_kv {\n     LLM_KV_ATTENTION_VALUE_LENGTH_MLA,\n     LLM_KV_ATTENTION_LAYER_INDICES,\n \n+    // Falcon-H1 specific\n+    LLM_KV_ATTN_HEAD_DIM,\n+    LLM_KV_SSM_HEAD_DIM,\n+    LLM_KV_MAMBA_D_SSM,\n+    LLM_KV_N_LAYER,\n+    LLM_KV_FALCON_H1_MAMBA_RMS_NORM,",
        "comment_created_at": "2025-07-07T19:06:11+00:00",
        "comment_author": "ggerganov",
        "comment_body": "Can this be simply `LLM_KV_MAMBA_RMS_NORM`? If there isn't anything very specific for Falcon H1, it's better to keep the names generic.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190855207",
    "pr_number": 14534,
    "pr_file": "src/llama-hparams.h",
    "created_at": "2025-07-07T19:17:22+00:00",
    "commented_code": "uint32_t ssm_d_state = 0;\n     uint32_t ssm_dt_rank = 0;\n     uint32_t ssm_n_group = 0;\n+    uint32_t ssm_head_dim   = 0;\n+    uint32_t ssm_mamba_d_ssm = 0;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2190855207",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14534,
        "pr_file": "src/llama-hparams.h",
        "discussion_id": "2190855207",
        "commented_code": "@@ -115,6 +115,17 @@ struct llama_hparams {\n     uint32_t ssm_d_state = 0;\n     uint32_t ssm_dt_rank = 0;\n     uint32_t ssm_n_group = 0;\n+    uint32_t ssm_head_dim   = 0;\n+    uint32_t ssm_mamba_d_ssm = 0;",
        "comment_created_at": "2025-07-07T19:17:22+00:00",
        "comment_author": "ggerganov",
        "comment_body": "Should this be just `ssm_d_ssm`?\r\n\r\nGenerally, I think the `mamba` prefix is not needed here. For example:\r\n\r\n```\r\nmamba_rms_norm -> ssm_rms_norm\r\nmamba_expand -> ssm_expand\r\n```\r\n\r\n@compilade Do you agree?",
        "pr_file_module": null
      },
      {
        "comment_id": "2190880579",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14534,
        "pr_file": "src/llama-hparams.h",
        "discussion_id": "2190855207",
        "commented_code": "@@ -115,6 +115,17 @@ struct llama_hparams {\n     uint32_t ssm_d_state = 0;\n     uint32_t ssm_dt_rank = 0;\n     uint32_t ssm_n_group = 0;\n+    uint32_t ssm_head_dim   = 0;\n+    uint32_t ssm_mamba_d_ssm = 0;",
        "comment_created_at": "2025-07-07T19:35:00+00:00",
        "comment_author": "compilade",
        "comment_body": "I agree, but I think these are not necessary, since `ssm_mamba_d_ssm` looks like `ssm_d_inner` to me, unless I'm misunderstanding.",
        "pr_file_module": null
      },
      {
        "comment_id": "2191648717",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14534,
        "pr_file": "src/llama-hparams.h",
        "discussion_id": "2190855207",
        "commented_code": "@@ -115,6 +115,17 @@ struct llama_hparams {\n     uint32_t ssm_d_state = 0;\n     uint32_t ssm_dt_rank = 0;\n     uint32_t ssm_n_group = 0;\n+    uint32_t ssm_head_dim   = 0;\n+    uint32_t ssm_mamba_d_ssm = 0;",
        "comment_created_at": "2025-07-08T06:58:40+00:00",
        "comment_author": "younesbelkada",
        "comment_body": "Indeed d_ssm and d_inner are the same, we now save d_inner to avoid confusion",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189355831",
    "pr_number": 14544,
    "pr_file": "common/common.h",
    "created_at": "2025-07-07T08:29:06+00:00",
    "commented_code": "std::string hostname      = \"127.0.0.1\";\n     std::string public_path   = \"\";                                                                         // NOLINT\n+    std::string server_prefix = \"\";                                                                         // NOLINT",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2189355831",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14544,
        "pr_file": "common/common.h",
        "discussion_id": "2189355831",
        "commented_code": "@@ -370,6 +370,7 @@ struct common_params {\n \n     std::string hostname      = \"127.0.0.1\";\n     std::string public_path   = \"\";                                                                         // NOLINT\n+    std::string server_prefix = \"\";                                                                         // NOLINT",
        "comment_created_at": "2025-07-07T08:29:06+00:00",
        "comment_author": "ggerganov",
        "comment_body": "For consistency, rename the var:\r\n\r\n```suggestion\r\n    std::string api_prefix = \"\";                                                                         // NOLINT\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2164449406",
    "pr_number": 14317,
    "pr_file": "ggml/src/ggml-cpu/simd-mappings.h",
    "created_at": "2025-06-24T16:29:30+00:00",
    "commented_code": "#include \"ggml-cpu-impl.h\"\n \n+#ifdef __ARM_FEATURE_SVE\n+#include <arm_sve.h>\n+#endif // __ARM_FEATURE_SVE\n+\n+#if defined(__ARM_NEON) && !defined(__CUDACC__) && !defined(__MUSACC__)\n+// if YCM cannot find <arm_neon.h>, make a symbolic link to it, for example:\n+//\n+//   $ ln -sfn /Library/Developer/CommandLineTools/usr/lib/clang/13.1.6/include/arm_neon.h ./src/\n+//\n+#include <arm_neon.h>\n+#endif\n+\n+#if defined(__F16C__)\n+#include <immintrin.h>\n+#endif\n+\n //\n // simd mappings\n //\n \n+// FP16 to FP32 conversion\n+\n+// 16-bit float\n+// on Arm, we use __fp16\n+// on x86, we use uint16_t\n+//\n+// for old CUDA compilers (<= 11), we use uint16_t: ref https://github.com/ggml-org/llama.cpp/pull/10616\n+// for     MUSA compilers        , we use uint16_t: ref https://github.com/ggml-org/llama.cpp/pull/11843\n+//\n+#if defined(__ARM_NEON) && !(defined(__CUDACC__) && __CUDACC_VER_MAJOR__ <= 11) && !defined(__MUSACC__)\n+    #ifdef GGML_FP16_TO_FP32\n+    #undef GGML_FP16_TO_FP32\n+    #endif",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2164449406",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14317,
        "pr_file": "ggml/src/ggml-cpu/simd-mappings.h",
        "discussion_id": "2164449406",
        "commented_code": "@@ -2,10 +2,232 @@\n \n #include \"ggml-cpu-impl.h\"\n \n+#ifdef __ARM_FEATURE_SVE\n+#include <arm_sve.h>\n+#endif // __ARM_FEATURE_SVE\n+\n+#if defined(__ARM_NEON) && !defined(__CUDACC__) && !defined(__MUSACC__)\n+// if YCM cannot find <arm_neon.h>, make a symbolic link to it, for example:\n+//\n+//   $ ln -sfn /Library/Developer/CommandLineTools/usr/lib/clang/13.1.6/include/arm_neon.h ./src/\n+//\n+#include <arm_neon.h>\n+#endif\n+\n+#if defined(__F16C__)\n+#include <immintrin.h>\n+#endif\n+\n //\n // simd mappings\n //\n \n+// FP16 to FP32 conversion\n+\n+// 16-bit float\n+// on Arm, we use __fp16\n+// on x86, we use uint16_t\n+//\n+// for old CUDA compilers (<= 11), we use uint16_t: ref https://github.com/ggml-org/llama.cpp/pull/10616\n+// for     MUSA compilers        , we use uint16_t: ref https://github.com/ggml-org/llama.cpp/pull/11843\n+//\n+#if defined(__ARM_NEON) && !(defined(__CUDACC__) && __CUDACC_VER_MAJOR__ <= 11) && !defined(__MUSACC__)\n+    #ifdef GGML_FP16_TO_FP32\n+    #undef GGML_FP16_TO_FP32\n+    #endif",
        "comment_created_at": "2025-06-24T16:29:30+00:00",
        "comment_author": "slaren",
        "comment_body": "I really don't like all these `#undefs` here to deal with the name collision from `ggml-base`. It should be easy enough to rename all occurrences of these macros in `ggml-cpu` to add a `GGML_CPU_` prefix to remove the ambiguities.",
        "pr_file_module": null
      }
    ]
  }
]