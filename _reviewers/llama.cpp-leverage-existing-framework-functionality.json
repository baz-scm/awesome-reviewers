[
  {
    "discussion_id": "2217441284",
    "pr_number": 14771,
    "pr_file": "convert_hf_to_gguf.py",
    "created_at": "2025-07-19T20:06:04+00:00",
    "commented_code": "yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"LLaDAModelLM\")\n+class LLaDAModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LLADA\n+    undo_permute = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n+        if self.hf_arch == \"VLlama3ForCausalLM\":\n+            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+\n+        vocab_dict = tokenizer.get_vocab()\n+        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n+        assert max(vocab_dict.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in vocab_dict.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            elif reverse_vocab[i] in added_vocab:\n+                tokens.append(reverse_vocab[i])\n+                # Check if it's a special token - treat special tokens as CONTROL tokens\n+                if hasattr(tokenizer, 'added_tokens_decoder') and i in tokenizer.added_tokens_decoder:\n+                    if tokenizer.added_tokens_decoder[i].special:\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    # Fallback: treat all added vocab as control tokens for special tokens like <|im_start|>\n+                    toktypes.append(gguf.TokenType.CONTROL)\n+            else:\n+                tokens.append(reverse_vocab[i])\n+                toktypes.append(gguf.TokenType.NORMAL)\n+\n+        return tokens, toktypes, tokpre\n+\n+    def set_vocab(self):\n+        try:\n+            self._set_vocab_sentencepiece()\n+        except FileNotFoundError:\n+            try:\n+                self._set_vocab_llama_hf()\n+            except (FileNotFoundError, TypeError):\n+                # Llama 3\n+                self._set_vocab_gpt2()\n+\n+        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n+        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n+            special_vocab = gguf.SpecialVocab(\n+                self.dir_model, load_merges=False,\n+                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n+            )\n+            special_vocab._set_special_token(\"prefix\", 32007)\n+            special_vocab._set_special_token(\"suffix\", 32008)\n+            special_vocab._set_special_token(\"middle\", 32009)\n+            special_vocab._set_special_token(\"eot\",    32010)\n+            special_vocab.add_to_gguf(self.gguf_writer)\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+        # Apply to granite small models only\n+        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n+            self.gguf_writer.add_add_bos_token(False)",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2217441284",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14771,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2217441284",
        "commented_code": "@@ -2851,6 +2851,159 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"LLaDAModelLM\")\n+class LLaDAModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LLADA\n+    undo_permute = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n+        if self.hf_arch == \"VLlama3ForCausalLM\":\n+            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+\n+        vocab_dict = tokenizer.get_vocab()\n+        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n+        assert max(vocab_dict.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in vocab_dict.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            elif reverse_vocab[i] in added_vocab:\n+                tokens.append(reverse_vocab[i])\n+                # Check if it's a special token - treat special tokens as CONTROL tokens\n+                if hasattr(tokenizer, 'added_tokens_decoder') and i in tokenizer.added_tokens_decoder:\n+                    if tokenizer.added_tokens_decoder[i].special:\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    # Fallback: treat all added vocab as control tokens for special tokens like <|im_start|>\n+                    toktypes.append(gguf.TokenType.CONTROL)\n+            else:\n+                tokens.append(reverse_vocab[i])\n+                toktypes.append(gguf.TokenType.NORMAL)\n+\n+        return tokens, toktypes, tokpre\n+\n+    def set_vocab(self):\n+        try:\n+            self._set_vocab_sentencepiece()\n+        except FileNotFoundError:\n+            try:\n+                self._set_vocab_llama_hf()\n+            except (FileNotFoundError, TypeError):\n+                # Llama 3\n+                self._set_vocab_gpt2()\n+\n+        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n+        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n+            special_vocab = gguf.SpecialVocab(\n+                self.dir_model, load_merges=False,\n+                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n+            )\n+            special_vocab._set_special_token(\"prefix\", 32007)\n+            special_vocab._set_special_token(\"suffix\", 32008)\n+            special_vocab._set_special_token(\"middle\", 32009)\n+            special_vocab._set_special_token(\"eot\",    32010)\n+            special_vocab.add_to_gguf(self.gguf_writer)\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+        # Apply to granite small models only\n+        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n+            self.gguf_writer.add_add_bos_token(False)",
        "comment_created_at": "2025-07-19T20:06:04+00:00",
        "comment_author": "CISC",
        "comment_body": "Ok, this is clearly an error...",
        "pr_file_module": null
      },
      {
        "comment_id": "2217540374",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14771,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2217441284",
        "commented_code": "@@ -2851,6 +2851,159 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"LLaDAModelLM\")\n+class LLaDAModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LLADA\n+    undo_permute = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n+        if self.hf_arch == \"VLlama3ForCausalLM\":\n+            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+\n+        vocab_dict = tokenizer.get_vocab()\n+        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n+        assert max(vocab_dict.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in vocab_dict.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            elif reverse_vocab[i] in added_vocab:\n+                tokens.append(reverse_vocab[i])\n+                # Check if it's a special token - treat special tokens as CONTROL tokens\n+                if hasattr(tokenizer, 'added_tokens_decoder') and i in tokenizer.added_tokens_decoder:\n+                    if tokenizer.added_tokens_decoder[i].special:\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    # Fallback: treat all added vocab as control tokens for special tokens like <|im_start|>\n+                    toktypes.append(gguf.TokenType.CONTROL)\n+            else:\n+                tokens.append(reverse_vocab[i])\n+                toktypes.append(gguf.TokenType.NORMAL)\n+\n+        return tokens, toktypes, tokpre\n+\n+    def set_vocab(self):\n+        try:\n+            self._set_vocab_sentencepiece()\n+        except FileNotFoundError:\n+            try:\n+                self._set_vocab_llama_hf()\n+            except (FileNotFoundError, TypeError):\n+                # Llama 3\n+                self._set_vocab_gpt2()\n+\n+        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n+        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n+            special_vocab = gguf.SpecialVocab(\n+                self.dir_model, load_merges=False,\n+                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n+            )\n+            special_vocab._set_special_token(\"prefix\", 32007)\n+            special_vocab._set_special_token(\"suffix\", 32008)\n+            special_vocab._set_special_token(\"middle\", 32009)\n+            special_vocab._set_special_token(\"eot\",    32010)\n+            special_vocab.add_to_gguf(self.gguf_writer)\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+        # Apply to granite small models only\n+        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n+            self.gguf_writer.add_add_bos_token(False)",
        "comment_created_at": "2025-07-20T02:44:22+00:00",
        "comment_author": "am17an",
        "comment_body": "Yeah sorry, this is copied from the llama architecture, removed it. I tried to subclass this arch with LLama but it didn't work for me because of the many changes in `set_gguf_parameters`. But perhaps it can work by just sub-classing as that is the probably the correct way to do it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2217774417",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14771,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2217441284",
        "commented_code": "@@ -2851,6 +2851,159 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"LLaDAModelLM\")\n+class LLaDAModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LLADA\n+    undo_permute = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n+        if self.hf_arch == \"VLlama3ForCausalLM\":\n+            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+\n+        vocab_dict = tokenizer.get_vocab()\n+        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n+        assert max(vocab_dict.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in vocab_dict.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            elif reverse_vocab[i] in added_vocab:\n+                tokens.append(reverse_vocab[i])\n+                # Check if it's a special token - treat special tokens as CONTROL tokens\n+                if hasattr(tokenizer, 'added_tokens_decoder') and i in tokenizer.added_tokens_decoder:\n+                    if tokenizer.added_tokens_decoder[i].special:\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    # Fallback: treat all added vocab as control tokens for special tokens like <|im_start|>\n+                    toktypes.append(gguf.TokenType.CONTROL)\n+            else:\n+                tokens.append(reverse_vocab[i])\n+                toktypes.append(gguf.TokenType.NORMAL)\n+\n+        return tokens, toktypes, tokpre\n+\n+    def set_vocab(self):\n+        try:\n+            self._set_vocab_sentencepiece()\n+        except FileNotFoundError:\n+            try:\n+                self._set_vocab_llama_hf()\n+            except (FileNotFoundError, TypeError):\n+                # Llama 3\n+                self._set_vocab_gpt2()\n+\n+        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n+        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n+            special_vocab = gguf.SpecialVocab(\n+                self.dir_model, load_merges=False,\n+                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n+            )\n+            special_vocab._set_special_token(\"prefix\", 32007)\n+            special_vocab._set_special_token(\"suffix\", 32008)\n+            special_vocab._set_special_token(\"middle\", 32009)\n+            special_vocab._set_special_token(\"eot\",    32010)\n+            special_vocab.add_to_gguf(self.gguf_writer)\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+        # Apply to granite small models only\n+        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n+            self.gguf_writer.add_add_bos_token(False)",
        "comment_created_at": "2025-07-20T11:35:13+00:00",
        "comment_author": "CISC",
        "comment_body": "I'm confused, what were you trying to achieve?",
        "pr_file_module": null
      },
      {
        "comment_id": "2217873475",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14771,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2217441284",
        "commented_code": "@@ -2851,6 +2851,159 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"LLaDAModelLM\")\n+class LLaDAModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LLADA\n+    undo_permute = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n+        if self.hf_arch == \"VLlama3ForCausalLM\":\n+            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+\n+        vocab_dict = tokenizer.get_vocab()\n+        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n+        assert max(vocab_dict.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in vocab_dict.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            elif reverse_vocab[i] in added_vocab:\n+                tokens.append(reverse_vocab[i])\n+                # Check if it's a special token - treat special tokens as CONTROL tokens\n+                if hasattr(tokenizer, 'added_tokens_decoder') and i in tokenizer.added_tokens_decoder:\n+                    if tokenizer.added_tokens_decoder[i].special:\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    # Fallback: treat all added vocab as control tokens for special tokens like <|im_start|>\n+                    toktypes.append(gguf.TokenType.CONTROL)\n+            else:\n+                tokens.append(reverse_vocab[i])\n+                toktypes.append(gguf.TokenType.NORMAL)\n+\n+        return tokens, toktypes, tokpre\n+\n+    def set_vocab(self):\n+        try:\n+            self._set_vocab_sentencepiece()\n+        except FileNotFoundError:\n+            try:\n+                self._set_vocab_llama_hf()\n+            except (FileNotFoundError, TypeError):\n+                # Llama 3\n+                self._set_vocab_gpt2()\n+\n+        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n+        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n+            special_vocab = gguf.SpecialVocab(\n+                self.dir_model, load_merges=False,\n+                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n+            )\n+            special_vocab._set_special_token(\"prefix\", 32007)\n+            special_vocab._set_special_token(\"suffix\", 32008)\n+            special_vocab._set_special_token(\"middle\", 32009)\n+            special_vocab._set_special_token(\"eot\",    32010)\n+            special_vocab.add_to_gguf(self.gguf_writer)\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+        # Apply to granite small models only\n+        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n+            self.gguf_writer.add_add_bos_token(False)",
        "comment_created_at": "2025-07-20T16:08:37+00:00",
        "comment_author": "am17an",
        "comment_body": "I was trying to re-use the llama architecture for this, since it's just the same architecture with different params. I'm not sure what is the correct way to do this which led to a bit of trial and errors with my gguf, in the end I just copied `class LLamaModel(TextModel)` but forgot to remove the other stuff. Can you tell me what is the right way to do this? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2217890855",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14771,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2217441284",
        "commented_code": "@@ -2851,6 +2851,159 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"LLaDAModelLM\")\n+class LLaDAModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LLADA\n+    undo_permute = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n+        if self.hf_arch == \"VLlama3ForCausalLM\":\n+            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+\n+        vocab_dict = tokenizer.get_vocab()\n+        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n+        assert max(vocab_dict.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in vocab_dict.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            elif reverse_vocab[i] in added_vocab:\n+                tokens.append(reverse_vocab[i])\n+                # Check if it's a special token - treat special tokens as CONTROL tokens\n+                if hasattr(tokenizer, 'added_tokens_decoder') and i in tokenizer.added_tokens_decoder:\n+                    if tokenizer.added_tokens_decoder[i].special:\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    # Fallback: treat all added vocab as control tokens for special tokens like <|im_start|>\n+                    toktypes.append(gguf.TokenType.CONTROL)\n+            else:\n+                tokens.append(reverse_vocab[i])\n+                toktypes.append(gguf.TokenType.NORMAL)\n+\n+        return tokens, toktypes, tokpre\n+\n+    def set_vocab(self):\n+        try:\n+            self._set_vocab_sentencepiece()\n+        except FileNotFoundError:\n+            try:\n+                self._set_vocab_llama_hf()\n+            except (FileNotFoundError, TypeError):\n+                # Llama 3\n+                self._set_vocab_gpt2()\n+\n+        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n+        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n+            special_vocab = gguf.SpecialVocab(\n+                self.dir_model, load_merges=False,\n+                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n+            )\n+            special_vocab._set_special_token(\"prefix\", 32007)\n+            special_vocab._set_special_token(\"suffix\", 32008)\n+            special_vocab._set_special_token(\"middle\", 32009)\n+            special_vocab._set_special_token(\"eot\",    32010)\n+            special_vocab.add_to_gguf(self.gguf_writer)\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+        # Apply to granite small models only\n+        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n+            self.gguf_writer.add_add_bos_token(False)",
        "comment_created_at": "2025-07-20T17:12:28+00:00",
        "comment_author": "CISC",
        "comment_body": "Oh, ok, I see, so you can just inherit from `LlamaModel` instead of `TextModel`, but I don't think that makes sense as you will inherit a lot of stuff you don't need and will just end up replacing all the methods.\r\n\r\nWhat specifically did you think was worth inheriting, AFAICT there's pretty much nothing?",
        "pr_file_module": null
      },
      {
        "comment_id": "2217891468",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14771,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2217441284",
        "commented_code": "@@ -2851,6 +2851,159 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"LLaDAModelLM\")\n+class LLaDAModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LLADA\n+    undo_permute = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n+        if self.hf_arch == \"VLlama3ForCausalLM\":\n+            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+\n+        vocab_dict = tokenizer.get_vocab()\n+        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n+        assert max(vocab_dict.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in vocab_dict.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            elif reverse_vocab[i] in added_vocab:\n+                tokens.append(reverse_vocab[i])\n+                # Check if it's a special token - treat special tokens as CONTROL tokens\n+                if hasattr(tokenizer, 'added_tokens_decoder') and i in tokenizer.added_tokens_decoder:\n+                    if tokenizer.added_tokens_decoder[i].special:\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    # Fallback: treat all added vocab as control tokens for special tokens like <|im_start|>\n+                    toktypes.append(gguf.TokenType.CONTROL)\n+            else:\n+                tokens.append(reverse_vocab[i])\n+                toktypes.append(gguf.TokenType.NORMAL)\n+\n+        return tokens, toktypes, tokpre\n+\n+    def set_vocab(self):\n+        try:\n+            self._set_vocab_sentencepiece()\n+        except FileNotFoundError:\n+            try:\n+                self._set_vocab_llama_hf()\n+            except (FileNotFoundError, TypeError):\n+                # Llama 3\n+                self._set_vocab_gpt2()\n+\n+        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n+        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n+            special_vocab = gguf.SpecialVocab(\n+                self.dir_model, load_merges=False,\n+                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n+            )\n+            special_vocab._set_special_token(\"prefix\", 32007)\n+            special_vocab._set_special_token(\"suffix\", 32008)\n+            special_vocab._set_special_token(\"middle\", 32009)\n+            special_vocab._set_special_token(\"eot\",    32010)\n+            special_vocab.add_to_gguf(self.gguf_writer)\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+        # Apply to granite small models only\n+        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n+            self.gguf_writer.add_add_bos_token(False)",
        "comment_created_at": "2025-07-20T17:14:28+00:00",
        "comment_author": "am17an",
        "comment_body": "The undo_permute stuff for the attention layers specifically",
        "pr_file_module": null
      },
      {
        "comment_id": "2217894211",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14771,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2217441284",
        "commented_code": "@@ -2851,6 +2851,159 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"LLaDAModelLM\")\n+class LLaDAModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LLADA\n+    undo_permute = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        # fix for SmolVLM2, missing `num_attention_heads` in config.json\n+        if self.hf_arch == \"VLlama3ForCausalLM\":\n+            self.hparams[\"num_attention_heads\"] = self.hparams.get(\"num_attention_heads\", 32)\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+\n+        vocab_dict = tokenizer.get_vocab()\n+        vocab_size = self.hparams.get(\"vocab_size\", len(vocab_dict))\n+        assert max(vocab_dict.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in vocab_dict.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            elif reverse_vocab[i] in added_vocab:\n+                tokens.append(reverse_vocab[i])\n+                # Check if it's a special token - treat special tokens as CONTROL tokens\n+                if hasattr(tokenizer, 'added_tokens_decoder') and i in tokenizer.added_tokens_decoder:\n+                    if tokenizer.added_tokens_decoder[i].special:\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    # Fallback: treat all added vocab as control tokens for special tokens like <|im_start|>\n+                    toktypes.append(gguf.TokenType.CONTROL)\n+            else:\n+                tokens.append(reverse_vocab[i])\n+                toktypes.append(gguf.TokenType.NORMAL)\n+\n+        return tokens, toktypes, tokpre\n+\n+    def set_vocab(self):\n+        try:\n+            self._set_vocab_sentencepiece()\n+        except FileNotFoundError:\n+            try:\n+                self._set_vocab_llama_hf()\n+            except (FileNotFoundError, TypeError):\n+                # Llama 3\n+                self._set_vocab_gpt2()\n+\n+        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)\n+        if self.hparams.get(\"vocab_size\", 32000) == 32016:\n+            special_vocab = gguf.SpecialVocab(\n+                self.dir_model, load_merges=False,\n+                special_token_types = ['prefix', 'suffix', 'middle', 'eot']\n+            )\n+            special_vocab._set_special_token(\"prefix\", 32007)\n+            special_vocab._set_special_token(\"suffix\", 32008)\n+            special_vocab._set_special_token(\"middle\", 32009)\n+            special_vocab._set_special_token(\"eot\",    32010)\n+            special_vocab.add_to_gguf(self.gguf_writer)\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+        # Apply to granite small models only\n+        if self.hparams.get(\"vocab_size\", 32000) == 49152:\n+            self.gguf_writer.add_add_bos_token(False)",
        "comment_created_at": "2025-07-20T17:23:57+00:00",
        "comment_author": "CISC",
        "comment_body": "Sure, but just copying that is fine.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2234128754",
    "pr_number": 14898,
    "pr_file": "convert_hf_to_gguf.py",
    "created_at": "2025-07-27T20:45:14+00:00",
    "commented_code": "return [(self.map_tensor_name(name), data_torch)]\n \n \n+@ModelBase.register(\"SmallThinkerForCausalLM\")\n+class SmallThinkerModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.SMALLTHINKER\n+\n+    def set_gguf_parameters(self):\n+        super().set_gguf_parameters()\n+        if (n_experts := self.hparams.get(\"num_experts\", self.hparams.get(\"moe_num_primary_experts\"))) is not None:\n+            self.gguf_writer.add_expert_count(n_experts)\n+        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\", self.hparams.get(\"moe_num_active_primary_experts\"))) is not None:\n+            self.gguf_writer.add_expert_used_count(n_experts_used)\n+        if (moe_intermediate_size := self.hparams.get(\"moe_ffn_hidden_size\")) is not None:\n+            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)\n+            self.gguf_writer.add_feed_forward_length(moe_intermediate_size)\n+            logger.info(f\"gguf: expert feed forward length = {moe_intermediate_size}\")\n+        if (self.hparams.get('moe_primary_router_apply_softmax')):\n+            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SOFTMAX)\n+        else:\n+            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SIGMOID)\n+        # YaRN is not enabled by default\n+        # To enable it, please refer to this guide: https://huggingface.co/Qwen/Qwen3-30B-A3B#processing-long-texts\n+        rope_scaling = self.hparams.get(\"rope_scaling\") or {}\n+        if rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\")) == \"yarn\" and \"factor\" in rope_scaling:\n+            self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.YARN)\n+            self.gguf_writer.add_rope_scaling_factor(rope_scaling[\"factor\"])\n+            self.gguf_writer.add_rope_scaling_orig_ctx_len(rope_scaling[\"original_max_position_embeddings\"])\n+\n+        sliding_window_layout = self.hparams.get(\"sliding_window_layout\")\n+        if sliding_window_layout:\n+            for i in sliding_window_layout:\n+                if i != 0:\n+                    sliding_window = self.hparams.get(\"sliding_window_size\")\n+                    if sliding_window:\n+                        self.gguf_writer.add_sliding_window(sliding_window)\n+                    break\n+\n+    _experts: list[dict[str, Tensor]] | None = None\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        # process the experts separately\n+        if name.find(\"experts\") != -1:\n+            n_experts = self.hparams.get(\"num_experts\", self.hparams.get(\"moe_num_primary_experts\"))\n+            assert bid is not None\n+\n+            if self._experts is None:\n+                self._experts = [{} for _ in range(self.block_count)]\n+\n+            self._experts[bid][name] = data_torch\n+\n+            if len(self._experts[bid]) >= n_experts * 3:\n+                tensors: list[tuple[str, Tensor]] = []\n+\n+                # merge the experts into a single 3d tensor\n+                for w_name in [\"down\", \"gate\", \"up\"]:\n+                    datas: list[Tensor] = []\n+\n+                    for xid in range(n_experts):\n+                        ename = f\"model.layers.{bid}.block_sparse_moe.experts.{xid}.{w_name}.weight\"\n+                        datas.append(self._experts[bid][ename])\n+                        del self._experts[bid][ename]\n+\n+                    data_torch = torch.stack(datas, dim=0)\n+\n+                    merged_name = f\"model.layers.{bid}.block_sparse_moe.experts.{w_name}.weight\"\n+\n+                    new_name = self.map_tensor_name(merged_name)\n+\n+                    tensors.append((new_name, data_torch))\n+                return tensors\n+            else:\n+                return []\n+\n+        return [(self.map_tensor_name(name), data_torch)]\n+\n+    def prepare_tensors(self):\n+        super().prepare_tensors()\n+\n+        if self._experts is not None:\n+            # flatten `list[dict[str, Tensor]]` into `list[str]`\n+            experts = [k for d in self._experts for k in d.keys()]\n+            if len(experts) > 0:\n+                raise ValueError(f\"Unprocessed experts: {experts}\")\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+        vocab_size = self.hparams.get(\"vocab_size\", len(tokenizer.vocab))\n+        assert max(tokenizer.vocab.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in tokenizer.vocab.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        added_tokens_decoder = tokenizer.added_tokens_decoder\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            else:\n+                token: str = reverse_vocab[i]\n+                if token in added_vocab:\n+                    # The tokenizer in llama.cpp assumes the CONTROL and USER_DEFINED tokens are pre-normalized.\n+                    # To avoid unexpected issues - we make sure to normalize non-normalized tokens\n+                    if not added_tokens_decoder[i].normalized:\n+                        previous_token = token\n+                        token = tokenizer.decode(tokenizer.encode(token, add_special_tokens=False))\n+                        if previous_token != token:\n+                            logger.info(f\"{repr(previous_token)} is encoded and decoded back to {repr(token)} using AutoTokenizer\")\n+\n+                    if added_tokens_decoder[i].special or self.does_token_look_special(token):\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        # NOTE: this was added for Gemma.\n+                        # Encoding and decoding the tokens above isn't sufficient for this case.\n+                        token = token.replace(b\"\\xe2\\x96\\x81\".decode(\"utf-8\"), \" \")  # pre-normalize user-defined spaces\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    toktypes.append(gguf.TokenType.NORMAL)\n+                tokens.append(token)\n+\n+        return tokens, toktypes, tokpre\n+",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2234128754",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14898,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2234128754",
        "commented_code": "@@ -7589,6 +7589,132 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         return [(self.map_tensor_name(name), data_torch)]\n \n \n+@ModelBase.register(\"SmallThinkerForCausalLM\")\n+class SmallThinkerModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.SMALLTHINKER\n+\n+    def set_gguf_parameters(self):\n+        super().set_gguf_parameters()\n+        if (n_experts := self.hparams.get(\"num_experts\", self.hparams.get(\"moe_num_primary_experts\"))) is not None:\n+            self.gguf_writer.add_expert_count(n_experts)\n+        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\", self.hparams.get(\"moe_num_active_primary_experts\"))) is not None:\n+            self.gguf_writer.add_expert_used_count(n_experts_used)\n+        if (moe_intermediate_size := self.hparams.get(\"moe_ffn_hidden_size\")) is not None:\n+            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)\n+            self.gguf_writer.add_feed_forward_length(moe_intermediate_size)\n+            logger.info(f\"gguf: expert feed forward length = {moe_intermediate_size}\")\n+        if (self.hparams.get('moe_primary_router_apply_softmax')):\n+            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SOFTMAX)\n+        else:\n+            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SIGMOID)\n+        # YaRN is not enabled by default\n+        # To enable it, please refer to this guide: https://huggingface.co/Qwen/Qwen3-30B-A3B#processing-long-texts\n+        rope_scaling = self.hparams.get(\"rope_scaling\") or {}\n+        if rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\")) == \"yarn\" and \"factor\" in rope_scaling:\n+            self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.YARN)\n+            self.gguf_writer.add_rope_scaling_factor(rope_scaling[\"factor\"])\n+            self.gguf_writer.add_rope_scaling_orig_ctx_len(rope_scaling[\"original_max_position_embeddings\"])\n+\n+        sliding_window_layout = self.hparams.get(\"sliding_window_layout\")\n+        if sliding_window_layout:\n+            for i in sliding_window_layout:\n+                if i != 0:\n+                    sliding_window = self.hparams.get(\"sliding_window_size\")\n+                    if sliding_window:\n+                        self.gguf_writer.add_sliding_window(sliding_window)\n+                    break\n+\n+    _experts: list[dict[str, Tensor]] | None = None\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        # process the experts separately\n+        if name.find(\"experts\") != -1:\n+            n_experts = self.hparams.get(\"num_experts\", self.hparams.get(\"moe_num_primary_experts\"))\n+            assert bid is not None\n+\n+            if self._experts is None:\n+                self._experts = [{} for _ in range(self.block_count)]\n+\n+            self._experts[bid][name] = data_torch\n+\n+            if len(self._experts[bid]) >= n_experts * 3:\n+                tensors: list[tuple[str, Tensor]] = []\n+\n+                # merge the experts into a single 3d tensor\n+                for w_name in [\"down\", \"gate\", \"up\"]:\n+                    datas: list[Tensor] = []\n+\n+                    for xid in range(n_experts):\n+                        ename = f\"model.layers.{bid}.block_sparse_moe.experts.{xid}.{w_name}.weight\"\n+                        datas.append(self._experts[bid][ename])\n+                        del self._experts[bid][ename]\n+\n+                    data_torch = torch.stack(datas, dim=0)\n+\n+                    merged_name = f\"model.layers.{bid}.block_sparse_moe.experts.{w_name}.weight\"\n+\n+                    new_name = self.map_tensor_name(merged_name)\n+\n+                    tensors.append((new_name, data_torch))\n+                return tensors\n+            else:\n+                return []\n+\n+        return [(self.map_tensor_name(name), data_torch)]\n+\n+    def prepare_tensors(self):\n+        super().prepare_tensors()\n+\n+        if self._experts is not None:\n+            # flatten `list[dict[str, Tensor]]` into `list[str]`\n+            experts = [k for d in self._experts for k in d.keys()]\n+            if len(experts) > 0:\n+                raise ValueError(f\"Unprocessed experts: {experts}\")\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+        vocab_size = self.hparams.get(\"vocab_size\", len(tokenizer.vocab))\n+        assert max(tokenizer.vocab.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in tokenizer.vocab.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        added_tokens_decoder = tokenizer.added_tokens_decoder\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            else:\n+                token: str = reverse_vocab[i]\n+                if token in added_vocab:\n+                    # The tokenizer in llama.cpp assumes the CONTROL and USER_DEFINED tokens are pre-normalized.\n+                    # To avoid unexpected issues - we make sure to normalize non-normalized tokens\n+                    if not added_tokens_decoder[i].normalized:\n+                        previous_token = token\n+                        token = tokenizer.decode(tokenizer.encode(token, add_special_tokens=False))\n+                        if previous_token != token:\n+                            logger.info(f\"{repr(previous_token)} is encoded and decoded back to {repr(token)} using AutoTokenizer\")\n+\n+                    if added_tokens_decoder[i].special or self.does_token_look_special(token):\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        # NOTE: this was added for Gemma.\n+                        # Encoding and decoding the tokens above isn't sufficient for this case.\n+                        token = token.replace(b\"\\xe2\\x96\\x81\".decode(\"utf-8\"), \" \")  # pre-normalize user-defined spaces\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    toktypes.append(gguf.TokenType.NORMAL)\n+                tokens.append(token)\n+\n+        return tokens, toktypes, tokpre\n+",
        "comment_created_at": "2025-07-27T20:45:14+00:00",
        "comment_author": "CISC",
        "comment_body": "```suggestion\r\n```\r\nAFAICT this is not needed, am I missing something?",
        "pr_file_module": null
      },
      {
        "comment_id": "2234201987",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14898,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2234128754",
        "commented_code": "@@ -7589,6 +7589,132 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         return [(self.map_tensor_name(name), data_torch)]\n \n \n+@ModelBase.register(\"SmallThinkerForCausalLM\")\n+class SmallThinkerModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.SMALLTHINKER\n+\n+    def set_gguf_parameters(self):\n+        super().set_gguf_parameters()\n+        if (n_experts := self.hparams.get(\"num_experts\", self.hparams.get(\"moe_num_primary_experts\"))) is not None:\n+            self.gguf_writer.add_expert_count(n_experts)\n+        if (n_experts_used := self.hparams.get(\"num_experts_per_tok\", self.hparams.get(\"moe_num_active_primary_experts\"))) is not None:\n+            self.gguf_writer.add_expert_used_count(n_experts_used)\n+        if (moe_intermediate_size := self.hparams.get(\"moe_ffn_hidden_size\")) is not None:\n+            self.gguf_writer.add_expert_feed_forward_length(moe_intermediate_size)\n+            self.gguf_writer.add_feed_forward_length(moe_intermediate_size)\n+            logger.info(f\"gguf: expert feed forward length = {moe_intermediate_size}\")\n+        if (self.hparams.get('moe_primary_router_apply_softmax')):\n+            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SOFTMAX)\n+        else:\n+            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SIGMOID)\n+        # YaRN is not enabled by default\n+        # To enable it, please refer to this guide: https://huggingface.co/Qwen/Qwen3-30B-A3B#processing-long-texts\n+        rope_scaling = self.hparams.get(\"rope_scaling\") or {}\n+        if rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\")) == \"yarn\" and \"factor\" in rope_scaling:\n+            self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.YARN)\n+            self.gguf_writer.add_rope_scaling_factor(rope_scaling[\"factor\"])\n+            self.gguf_writer.add_rope_scaling_orig_ctx_len(rope_scaling[\"original_max_position_embeddings\"])\n+\n+        sliding_window_layout = self.hparams.get(\"sliding_window_layout\")\n+        if sliding_window_layout:\n+            for i in sliding_window_layout:\n+                if i != 0:\n+                    sliding_window = self.hparams.get(\"sliding_window_size\")\n+                    if sliding_window:\n+                        self.gguf_writer.add_sliding_window(sliding_window)\n+                    break\n+\n+    _experts: list[dict[str, Tensor]] | None = None\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        # process the experts separately\n+        if name.find(\"experts\") != -1:\n+            n_experts = self.hparams.get(\"num_experts\", self.hparams.get(\"moe_num_primary_experts\"))\n+            assert bid is not None\n+\n+            if self._experts is None:\n+                self._experts = [{} for _ in range(self.block_count)]\n+\n+            self._experts[bid][name] = data_torch\n+\n+            if len(self._experts[bid]) >= n_experts * 3:\n+                tensors: list[tuple[str, Tensor]] = []\n+\n+                # merge the experts into a single 3d tensor\n+                for w_name in [\"down\", \"gate\", \"up\"]:\n+                    datas: list[Tensor] = []\n+\n+                    for xid in range(n_experts):\n+                        ename = f\"model.layers.{bid}.block_sparse_moe.experts.{xid}.{w_name}.weight\"\n+                        datas.append(self._experts[bid][ename])\n+                        del self._experts[bid][ename]\n+\n+                    data_torch = torch.stack(datas, dim=0)\n+\n+                    merged_name = f\"model.layers.{bid}.block_sparse_moe.experts.{w_name}.weight\"\n+\n+                    new_name = self.map_tensor_name(merged_name)\n+\n+                    tensors.append((new_name, data_torch))\n+                return tensors\n+            else:\n+                return []\n+\n+        return [(self.map_tensor_name(name), data_torch)]\n+\n+    def prepare_tensors(self):\n+        super().prepare_tensors()\n+\n+        if self._experts is not None:\n+            # flatten `list[dict[str, Tensor]]` into `list[str]`\n+            experts = [k for d in self._experts for k in d.keys()]\n+            if len(experts) > 0:\n+                raise ValueError(f\"Unprocessed experts: {experts}\")\n+\n+    def get_vocab_base(self) -> tuple[list[str], list[int], str]:\n+        tokens: list[str] = []\n+        toktypes: list[int] = []\n+\n+        from transformers import AutoTokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(self.dir_model, trust_remote_code=True)\n+        vocab_size = self.hparams.get(\"vocab_size\", len(tokenizer.vocab))\n+        assert max(tokenizer.vocab.values()) < vocab_size\n+\n+        tokpre = self.get_vocab_base_pre(tokenizer)\n+\n+        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in tokenizer.vocab.items()}\n+        added_vocab = tokenizer.get_added_vocab()\n+\n+        added_tokens_decoder = tokenizer.added_tokens_decoder\n+\n+        for i in range(vocab_size):\n+            if i not in reverse_vocab:\n+                tokens.append(f\"[PAD{i}]\")\n+                toktypes.append(gguf.TokenType.UNUSED)\n+            else:\n+                token: str = reverse_vocab[i]\n+                if token in added_vocab:\n+                    # The tokenizer in llama.cpp assumes the CONTROL and USER_DEFINED tokens are pre-normalized.\n+                    # To avoid unexpected issues - we make sure to normalize non-normalized tokens\n+                    if not added_tokens_decoder[i].normalized:\n+                        previous_token = token\n+                        token = tokenizer.decode(tokenizer.encode(token, add_special_tokens=False))\n+                        if previous_token != token:\n+                            logger.info(f\"{repr(previous_token)} is encoded and decoded back to {repr(token)} using AutoTokenizer\")\n+\n+                    if added_tokens_decoder[i].special or self.does_token_look_special(token):\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    else:\n+                        # NOTE: this was added for Gemma.\n+                        # Encoding and decoding the tokens above isn't sufficient for this case.\n+                        token = token.replace(b\"\\xe2\\x96\\x81\".decode(\"utf-8\"), \" \")  # pre-normalize user-defined spaces\n+                        toktypes.append(gguf.TokenType.USER_DEFINED)\n+                else:\n+                    toktypes.append(gguf.TokenType.NORMAL)\n+                tokens.append(token)\n+\n+        return tokens, toktypes, tokpre\n+",
        "comment_created_at": "2025-07-28T00:38:50+00:00",
        "comment_author": "wdl339",
        "comment_body": "You are absolutely right, this override is no longer needed. The only difference was the trust_remote_code=True argument, which was required during our early development.\r\n\r\nI'll remove this redundant function now. Thanks for catching it!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2115261462",
    "pr_number": 13908,
    "pr_file": "convert_hf_to_gguf.py",
    "created_at": "2025-05-30T06:56:24+00:00",
    "commented_code": "yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"Eagle2DraftForCausalLM\")\n+class Eagle2DraftModel(TextModel):",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2115261462",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13908,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2115261462",
        "commented_code": "@@ -2712,6 +2712,25 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"Eagle2DraftForCausalLM\")\n+class Eagle2DraftModel(TextModel):",
        "comment_created_at": "2025-05-30T06:56:24+00:00",
        "comment_author": "CISC",
        "comment_body": "Since this is a Qwen2 model and it has no distinguishing conversion features, make it inherit from `Qwen2Model` and leave out everything but `model_arch`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2119048820",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13908,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2115261462",
        "commented_code": "@@ -2712,6 +2712,25 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         yield from super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"Eagle2DraftForCausalLM\")\n+class Eagle2DraftModel(TextModel):",
        "comment_created_at": "2025-06-01T11:30:17+00:00",
        "comment_author": "pockers21",
        "comment_body": "> Since this is a Qwen2 model and it has no distinguishing conversion features, make it inherit from `Qwen2Model` and leave out everything but `model_arch`.\r\n\r\nFixed in latest commit ,  now inherits from Qwen2Model with only model_arch override.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2233220350",
    "pr_number": 14878,
    "pr_file": "convert_hf_to_gguf.py",
    "created_at": "2025-07-26T19:37:37+00:00",
    "commented_code": "raise ValueError(f\"Unprocessed experts: {experts}\")\n \n \n+@ModelBase.register(\"HunYuanDenseV1ForCausalLM\")\n+class HunYuanModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.HUNYUAN_DENSE\n+\n+    def set_vocab(self):\n+        if (self.dir_model / \"tokenizer.json\").is_file():\n+            self._set_vocab_gpt2()\n+            self.gguf_writer.add_add_bos_token(True)",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2233220350",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14878,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2233220350",
        "commented_code": "@@ -7531,6 +7526,99 @@ def prepare_tensors(self):\n                 raise ValueError(f\"Unprocessed experts: {experts}\")\n \n \n+@ModelBase.register(\"HunYuanDenseV1ForCausalLM\")\n+class HunYuanModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.HUNYUAN_DENSE\n+\n+    def set_vocab(self):\n+        if (self.dir_model / \"tokenizer.json\").is_file():\n+            self._set_vocab_gpt2()\n+            self.gguf_writer.add_add_bos_token(True)",
        "comment_created_at": "2025-07-26T19:37:37+00:00",
        "comment_author": "CISC",
        "comment_body": "It shouldn't be necessary to set this manually, a correctly configured model has this set in `tokenizer_config.json` and this will be picked up from there by `gguf.SpecialVocab` (called from `_set_vocab_gpt2`).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2193886767",
    "pr_number": 14560,
    "pr_file": "convert_hf_to_gguf.py",
    "created_at": "2025-07-09T02:38:30+00:00",
    "commented_code": "return [(new_name, data_torch)]\n \n \n+@ModelBase.register(\"Plamo2ForCausalLM\", \"PLaMo2ForCausalLM\")\n+class Plamo2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.PLAMO2\n+\n+    def set_vocab(self):\n+        # PLaMo 2 uses a custom tokenizer with a .jsonl file\n+        # We need to handle this specially\n+        tokenizer_jsonl_path = self.dir_model / \"tokenizer.jsonl\"\n+        tokenizer_config_path = self.dir_model / \"tokenizer_config.json\"\n+\n+        if not tokenizer_jsonl_path.is_file():\n+            raise FileNotFoundError(f\"PLaMo 2 tokenizer file not found: {tokenizer_jsonl_path}\")\n+\n+        # Load tokenizer config\n+        with open(tokenizer_config_path, 'r', encoding='utf-8') as f:\n+            tokenizer_config = json.load(f)\n+\n+        # Load tokens from JSONL file (actually a list format)\n+        tokens = []\n+        scores = []\n+        toktypes = []\n+\n+        with open(tokenizer_jsonl_path, 'r', encoding='utf-8') as f:\n+            for line_num, line in enumerate(f):\n+                if line.strip():\n+                    token_data = json.loads(line)\n+                    # Format: [token, score, type, ?, ?, ?, ?]\n+                    token = token_data[0].encode(\"utf-8\")\n+                    score = float(token_data[1])\n+                    token_type_str = token_data[2] if len(token_data) > 2 else \"NORMAL\"\n+\n+                    tokens.append(token)\n+                    scores.append(score)\n+\n+                    # Map token type strings to GGUF token types\n+                    if token_type_str == \"UNKNOWN\":\n+                        toktypes.append(gguf.TokenType.UNKNOWN)\n+                    elif token_type_str == \"CONTROL\":\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    elif token_type_str == \"BYTE\":\n+                        toktypes.append(gguf.TokenType.BYTE)\n+                    else:\n+                        # Check for PLaMo-2 special tokens\n+                        token_str = token_data[0]\n+                        if token_str.startswith(\"<|plamo:\") and token_str.endswith(\"|>\"):\n+                            toktypes.append(gguf.TokenType.CONTROL)\n+                        else:\n+                            toktypes.append(gguf.TokenType.NORMAL)\n+\n+        # Use \"plamo2\" tokenizer type for PLaMo-2's custom Aho-Corasick tokenizer\n+        self.gguf_writer.add_tokenizer_model(\"plamo2\")\n+        self.gguf_writer.add_tokenizer_pre(\"default\")\n+        self.gguf_writer.add_token_list(tokens)\n+        self.gguf_writer.add_token_scores(scores)\n+        self.gguf_writer.add_token_types(toktypes)\n+\n+        # Add special tokens from config\n+        if \"bos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_bos_token_id(tokenizer_config[\"bos_token_id\"])\n+        if \"eos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_eos_token_id(tokenizer_config[\"eos_token_id\"])\n+        if \"pad_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_pad_token_id(tokenizer_config[\"pad_token_id\"])\n+        if \"unk_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_unk_token_id(tokenizer_config[\"unk_token_id\"])\n+\n+        self.gguf_writer.add_add_space_prefix(False)\n+\n+    def set_gguf_parameters(self):\n+        hparams = self.hparams\n+        block_count = hparams[\"num_hidden_layers\"]\n+\n+        # Which layers are Mamba layers\n+        # PLaMo 2 uses mamba_step to indicate the pattern (e.g., 2 means every other layer)\n+        # This logic matches modeling_plamo.py's is_mamba function\n+        mamba_step = hparams.get(\"mamba_step\", 2)\n+        mamba_enabled = hparams.get(\"mamba_enabled\", True)\n+        mamba_layers = []\n+\n+        if mamba_enabled:\n+            for i in range(block_count):\n+                if block_count <= (mamba_step // 2):\n+                    # use attention in last layer\n+                    is_mamba = (i != block_count - 1)\n+                else:\n+                    is_mamba = (i % mamba_step) != (mamba_step // 2)\n+                if is_mamba:\n+                    mamba_layers.append(0)\n+                else:\n+                    mamba_layers.append(hparams.get(\"num_key_value_heads\", 4))\n+\n+        if mamba_layers:\n+            self.gguf_writer.add_head_count_kv(mamba_layers)\n+\n+        self.gguf_writer.add_context_length(hparams.get(\"max_position_embeddings\", 2048))\n+        self.gguf_writer.add_embedding_length(hparams.get(\"hidden_size\", 4096))\n+        self.gguf_writer.add_block_count(block_count)\n+        self.gguf_writer.add_head_count(hparams.get(\"num_attention_heads\", 32))\n+        self.gguf_writer.add_layer_norm_rms_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_group_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_layer_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_rope_freq_base(hparams.get(\"rope_theta\", 1000000.0))\n+\n+        # Mamba parameters\n+        self.gguf_writer.add_ssm_state_size(hparams.get(\"mamba_d_state\", 64))\n+        self.gguf_writer.add_ssm_conv_kernel(hparams.get(\"mamba_d_conv\", 4))\n+        self.gguf_writer.add_ssm_time_step_rank(hparams.get(\"mamba_num_heads\", 64))\n+        intermediate_size = hparams.get(\"mamba_num_heads\", 64) * hparams.get(\"hidden_size_per_head\", 128)\n+        self.gguf_writer.add_ssm_inner_size(intermediate_size)\n+        self.gguf_writer.add_ssm_group_count(0)\n+\n+        # MLP feed forward parameters (for attention layers)\n+        self.gguf_writer.add_feed_forward_length(hparams.get(\"intermediate_size\", 16384))\n+        self.gguf_writer.add_file_type(self.ftype)\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        del bid  # unused\n+\n+        if name.endswith(\".embed_tokens.weight\"):\n+            # If there is no lm_head, we need to map the token embedding to the output layer\n+            assert self.tensor_names is not None\n+            if all(['lm_head' not in name for name in self.tensor_names]):\n+                name_base = name.replace(\".embed_tokens.weight\", \"\")\n+                output_name = \"lm_head\"\n+\n+                embed_tokens_mapped = self.map_tensor_name(name)\n+                output_mapped = self.map_tensor_name(output_name) + \".weight\"\n+\n+                return [(embed_tokens_mapped, data_torch), (output_mapped, data_torch)]",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2193886767",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14560,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2193886767",
        "commented_code": "@@ -3476,6 +3476,183 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         return [(new_name, data_torch)]\n \n \n+@ModelBase.register(\"Plamo2ForCausalLM\", \"PLaMo2ForCausalLM\")\n+class Plamo2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.PLAMO2\n+\n+    def set_vocab(self):\n+        # PLaMo 2 uses a custom tokenizer with a .jsonl file\n+        # We need to handle this specially\n+        tokenizer_jsonl_path = self.dir_model / \"tokenizer.jsonl\"\n+        tokenizer_config_path = self.dir_model / \"tokenizer_config.json\"\n+\n+        if not tokenizer_jsonl_path.is_file():\n+            raise FileNotFoundError(f\"PLaMo 2 tokenizer file not found: {tokenizer_jsonl_path}\")\n+\n+        # Load tokenizer config\n+        with open(tokenizer_config_path, 'r', encoding='utf-8') as f:\n+            tokenizer_config = json.load(f)\n+\n+        # Load tokens from JSONL file (actually a list format)\n+        tokens = []\n+        scores = []\n+        toktypes = []\n+\n+        with open(tokenizer_jsonl_path, 'r', encoding='utf-8') as f:\n+            for line_num, line in enumerate(f):\n+                if line.strip():\n+                    token_data = json.loads(line)\n+                    # Format: [token, score, type, ?, ?, ?, ?]\n+                    token = token_data[0].encode(\"utf-8\")\n+                    score = float(token_data[1])\n+                    token_type_str = token_data[2] if len(token_data) > 2 else \"NORMAL\"\n+\n+                    tokens.append(token)\n+                    scores.append(score)\n+\n+                    # Map token type strings to GGUF token types\n+                    if token_type_str == \"UNKNOWN\":\n+                        toktypes.append(gguf.TokenType.UNKNOWN)\n+                    elif token_type_str == \"CONTROL\":\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    elif token_type_str == \"BYTE\":\n+                        toktypes.append(gguf.TokenType.BYTE)\n+                    else:\n+                        # Check for PLaMo-2 special tokens\n+                        token_str = token_data[0]\n+                        if token_str.startswith(\"<|plamo:\") and token_str.endswith(\"|>\"):\n+                            toktypes.append(gguf.TokenType.CONTROL)\n+                        else:\n+                            toktypes.append(gguf.TokenType.NORMAL)\n+\n+        # Use \"plamo2\" tokenizer type for PLaMo-2's custom Aho-Corasick tokenizer\n+        self.gguf_writer.add_tokenizer_model(\"plamo2\")\n+        self.gguf_writer.add_tokenizer_pre(\"default\")\n+        self.gguf_writer.add_token_list(tokens)\n+        self.gguf_writer.add_token_scores(scores)\n+        self.gguf_writer.add_token_types(toktypes)\n+\n+        # Add special tokens from config\n+        if \"bos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_bos_token_id(tokenizer_config[\"bos_token_id\"])\n+        if \"eos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_eos_token_id(tokenizer_config[\"eos_token_id\"])\n+        if \"pad_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_pad_token_id(tokenizer_config[\"pad_token_id\"])\n+        if \"unk_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_unk_token_id(tokenizer_config[\"unk_token_id\"])\n+\n+        self.gguf_writer.add_add_space_prefix(False)\n+\n+    def set_gguf_parameters(self):\n+        hparams = self.hparams\n+        block_count = hparams[\"num_hidden_layers\"]\n+\n+        # Which layers are Mamba layers\n+        # PLaMo 2 uses mamba_step to indicate the pattern (e.g., 2 means every other layer)\n+        # This logic matches modeling_plamo.py's is_mamba function\n+        mamba_step = hparams.get(\"mamba_step\", 2)\n+        mamba_enabled = hparams.get(\"mamba_enabled\", True)\n+        mamba_layers = []\n+\n+        if mamba_enabled:\n+            for i in range(block_count):\n+                if block_count <= (mamba_step // 2):\n+                    # use attention in last layer\n+                    is_mamba = (i != block_count - 1)\n+                else:\n+                    is_mamba = (i % mamba_step) != (mamba_step // 2)\n+                if is_mamba:\n+                    mamba_layers.append(0)\n+                else:\n+                    mamba_layers.append(hparams.get(\"num_key_value_heads\", 4))\n+\n+        if mamba_layers:\n+            self.gguf_writer.add_head_count_kv(mamba_layers)\n+\n+        self.gguf_writer.add_context_length(hparams.get(\"max_position_embeddings\", 2048))\n+        self.gguf_writer.add_embedding_length(hparams.get(\"hidden_size\", 4096))\n+        self.gguf_writer.add_block_count(block_count)\n+        self.gguf_writer.add_head_count(hparams.get(\"num_attention_heads\", 32))\n+        self.gguf_writer.add_layer_norm_rms_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_group_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_layer_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_rope_freq_base(hparams.get(\"rope_theta\", 1000000.0))\n+\n+        # Mamba parameters\n+        self.gguf_writer.add_ssm_state_size(hparams.get(\"mamba_d_state\", 64))\n+        self.gguf_writer.add_ssm_conv_kernel(hparams.get(\"mamba_d_conv\", 4))\n+        self.gguf_writer.add_ssm_time_step_rank(hparams.get(\"mamba_num_heads\", 64))\n+        intermediate_size = hparams.get(\"mamba_num_heads\", 64) * hparams.get(\"hidden_size_per_head\", 128)\n+        self.gguf_writer.add_ssm_inner_size(intermediate_size)\n+        self.gguf_writer.add_ssm_group_count(0)\n+\n+        # MLP feed forward parameters (for attention layers)\n+        self.gguf_writer.add_feed_forward_length(hparams.get(\"intermediate_size\", 16384))\n+        self.gguf_writer.add_file_type(self.ftype)\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        del bid  # unused\n+\n+        if name.endswith(\".embed_tokens.weight\"):\n+            # If there is no lm_head, we need to map the token embedding to the output layer\n+            assert self.tensor_names is not None\n+            if all(['lm_head' not in name for name in self.tensor_names]):\n+                name_base = name.replace(\".embed_tokens.weight\", \"\")\n+                output_name = \"lm_head\"\n+\n+                embed_tokens_mapped = self.map_tensor_name(name)\n+                output_mapped = self.map_tensor_name(output_name) + \".weight\"\n+\n+                return [(embed_tokens_mapped, data_torch), (output_mapped, data_torch)]",
        "comment_created_at": "2025-07-09T02:38:30+00:00",
        "comment_author": "compilade",
        "comment_body": "There's no need to duplicate the token embeddings tensor in the model file, it can be done at load time in the model (this is what is done for most of the other arches).\n\nSearch for `TENSOR_DUPLICATED` and `TENSOR_NOT_REQUIRED` in `src/llama-model.cpp`",
        "pr_file_module": null
      },
      {
        "comment_id": "2193982111",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14560,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2193886767",
        "commented_code": "@@ -3476,6 +3476,183 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         return [(new_name, data_torch)]\n \n \n+@ModelBase.register(\"Plamo2ForCausalLM\", \"PLaMo2ForCausalLM\")\n+class Plamo2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.PLAMO2\n+\n+    def set_vocab(self):\n+        # PLaMo 2 uses a custom tokenizer with a .jsonl file\n+        # We need to handle this specially\n+        tokenizer_jsonl_path = self.dir_model / \"tokenizer.jsonl\"\n+        tokenizer_config_path = self.dir_model / \"tokenizer_config.json\"\n+\n+        if not tokenizer_jsonl_path.is_file():\n+            raise FileNotFoundError(f\"PLaMo 2 tokenizer file not found: {tokenizer_jsonl_path}\")\n+\n+        # Load tokenizer config\n+        with open(tokenizer_config_path, 'r', encoding='utf-8') as f:\n+            tokenizer_config = json.load(f)\n+\n+        # Load tokens from JSONL file (actually a list format)\n+        tokens = []\n+        scores = []\n+        toktypes = []\n+\n+        with open(tokenizer_jsonl_path, 'r', encoding='utf-8') as f:\n+            for line_num, line in enumerate(f):\n+                if line.strip():\n+                    token_data = json.loads(line)\n+                    # Format: [token, score, type, ?, ?, ?, ?]\n+                    token = token_data[0].encode(\"utf-8\")\n+                    score = float(token_data[1])\n+                    token_type_str = token_data[2] if len(token_data) > 2 else \"NORMAL\"\n+\n+                    tokens.append(token)\n+                    scores.append(score)\n+\n+                    # Map token type strings to GGUF token types\n+                    if token_type_str == \"UNKNOWN\":\n+                        toktypes.append(gguf.TokenType.UNKNOWN)\n+                    elif token_type_str == \"CONTROL\":\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    elif token_type_str == \"BYTE\":\n+                        toktypes.append(gguf.TokenType.BYTE)\n+                    else:\n+                        # Check for PLaMo-2 special tokens\n+                        token_str = token_data[0]\n+                        if token_str.startswith(\"<|plamo:\") and token_str.endswith(\"|>\"):\n+                            toktypes.append(gguf.TokenType.CONTROL)\n+                        else:\n+                            toktypes.append(gguf.TokenType.NORMAL)\n+\n+        # Use \"plamo2\" tokenizer type for PLaMo-2's custom Aho-Corasick tokenizer\n+        self.gguf_writer.add_tokenizer_model(\"plamo2\")\n+        self.gguf_writer.add_tokenizer_pre(\"default\")\n+        self.gguf_writer.add_token_list(tokens)\n+        self.gguf_writer.add_token_scores(scores)\n+        self.gguf_writer.add_token_types(toktypes)\n+\n+        # Add special tokens from config\n+        if \"bos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_bos_token_id(tokenizer_config[\"bos_token_id\"])\n+        if \"eos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_eos_token_id(tokenizer_config[\"eos_token_id\"])\n+        if \"pad_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_pad_token_id(tokenizer_config[\"pad_token_id\"])\n+        if \"unk_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_unk_token_id(tokenizer_config[\"unk_token_id\"])\n+\n+        self.gguf_writer.add_add_space_prefix(False)\n+\n+    def set_gguf_parameters(self):\n+        hparams = self.hparams\n+        block_count = hparams[\"num_hidden_layers\"]\n+\n+        # Which layers are Mamba layers\n+        # PLaMo 2 uses mamba_step to indicate the pattern (e.g., 2 means every other layer)\n+        # This logic matches modeling_plamo.py's is_mamba function\n+        mamba_step = hparams.get(\"mamba_step\", 2)\n+        mamba_enabled = hparams.get(\"mamba_enabled\", True)\n+        mamba_layers = []\n+\n+        if mamba_enabled:\n+            for i in range(block_count):\n+                if block_count <= (mamba_step // 2):\n+                    # use attention in last layer\n+                    is_mamba = (i != block_count - 1)\n+                else:\n+                    is_mamba = (i % mamba_step) != (mamba_step // 2)\n+                if is_mamba:\n+                    mamba_layers.append(0)\n+                else:\n+                    mamba_layers.append(hparams.get(\"num_key_value_heads\", 4))\n+\n+        if mamba_layers:\n+            self.gguf_writer.add_head_count_kv(mamba_layers)\n+\n+        self.gguf_writer.add_context_length(hparams.get(\"max_position_embeddings\", 2048))\n+        self.gguf_writer.add_embedding_length(hparams.get(\"hidden_size\", 4096))\n+        self.gguf_writer.add_block_count(block_count)\n+        self.gguf_writer.add_head_count(hparams.get(\"num_attention_heads\", 32))\n+        self.gguf_writer.add_layer_norm_rms_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_group_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_layer_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_rope_freq_base(hparams.get(\"rope_theta\", 1000000.0))\n+\n+        # Mamba parameters\n+        self.gguf_writer.add_ssm_state_size(hparams.get(\"mamba_d_state\", 64))\n+        self.gguf_writer.add_ssm_conv_kernel(hparams.get(\"mamba_d_conv\", 4))\n+        self.gguf_writer.add_ssm_time_step_rank(hparams.get(\"mamba_num_heads\", 64))\n+        intermediate_size = hparams.get(\"mamba_num_heads\", 64) * hparams.get(\"hidden_size_per_head\", 128)\n+        self.gguf_writer.add_ssm_inner_size(intermediate_size)\n+        self.gguf_writer.add_ssm_group_count(0)\n+\n+        # MLP feed forward parameters (for attention layers)\n+        self.gguf_writer.add_feed_forward_length(hparams.get(\"intermediate_size\", 16384))\n+        self.gguf_writer.add_file_type(self.ftype)\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        del bid  # unused\n+\n+        if name.endswith(\".embed_tokens.weight\"):\n+            # If there is no lm_head, we need to map the token embedding to the output layer\n+            assert self.tensor_names is not None\n+            if all(['lm_head' not in name for name in self.tensor_names]):\n+                name_base = name.replace(\".embed_tokens.weight\", \"\")\n+                output_name = \"lm_head\"\n+\n+                embed_tokens_mapped = self.map_tensor_name(name)\n+                output_mapped = self.map_tensor_name(output_name) + \".weight\"\n+\n+                return [(embed_tokens_mapped, data_torch), (output_mapped, data_torch)]",
        "comment_created_at": "2025-07-09T04:23:02+00:00",
        "comment_author": "mitmul",
        "comment_body": "Thanks, I removed this part and modified `src/llama-model.cpp` to fix PLaMo-2 model building to use `TENSOR_NOT_REQUIRED` for output.weight in ea95a1da",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2199943276",
    "pr_number": 14560,
    "pr_file": "convert_hf_to_gguf.py",
    "created_at": "2025-07-11T07:47:41+00:00",
    "commented_code": "return [(new_name, data_torch)]\n \n \n+@ModelBase.register(\"Plamo2ForCausalLM\", \"PLaMo2ForCausalLM\")\n+class Plamo2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.PLAMO2\n+\n+    def set_vocab(self):\n+        # PLaMo 2 uses a custom tokenizer with a .jsonl file\n+        # We need to handle this specially\n+        tokenizer_jsonl_path = self.dir_model / \"tokenizer.jsonl\"\n+        tokenizer_config_path = self.dir_model / \"tokenizer_config.json\"\n+\n+        if not tokenizer_jsonl_path.is_file():\n+            raise FileNotFoundError(f\"PLaMo 2 tokenizer file not found: {tokenizer_jsonl_path}\")\n+\n+        # Load tokenizer config\n+        with open(tokenizer_config_path, 'r', encoding='utf-8') as f:\n+            tokenizer_config = json.load(f)\n+\n+        # Load tokens from JSONL file (actually a list format)\n+        tokens = []\n+        scores = []\n+        toktypes = []\n+\n+        with open(tokenizer_jsonl_path, 'r', encoding='utf-8') as f:\n+            for line_num, line in enumerate(f):\n+                if line.strip():\n+                    token_data = json.loads(line)\n+                    # Format: [token, score, type, ?, ?, ?, ?]\n+                    token = token_data[0].encode(\"utf-8\")\n+                    score = float(token_data[1])\n+                    token_type_str = token_data[2] if len(token_data) > 2 else \"NORMAL\"\n+\n+                    tokens.append(token)\n+                    scores.append(score)\n+\n+                    # Map token type strings to GGUF token types\n+                    if token_type_str == \"UNKNOWN\":\n+                        toktypes.append(gguf.TokenType.UNKNOWN)\n+                    elif token_type_str == \"CONTROL\":\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    elif token_type_str == \"BYTE\":\n+                        toktypes.append(gguf.TokenType.BYTE)\n+                    else:\n+                        # Check for PLaMo-2 special tokens\n+                        token_str = token_data[0]\n+                        if token_str.startswith(\"<|plamo:\") and token_str.endswith(\"|>\"):\n+                            toktypes.append(gguf.TokenType.CONTROL)\n+                        else:\n+                            toktypes.append(gguf.TokenType.NORMAL)\n+\n+        # Use \"plamo2\" tokenizer type for PLaMo-2's custom Aho-Corasick tokenizer\n+        self.gguf_writer.add_tokenizer_model(\"plamo2\")\n+        self.gguf_writer.add_tokenizer_pre(\"default\")\n+        self.gguf_writer.add_token_list(tokens)\n+        self.gguf_writer.add_token_scores(scores)\n+        self.gguf_writer.add_token_types(toktypes)\n+\n+        # Add special tokens from config\n+        if \"bos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_bos_token_id(tokenizer_config[\"bos_token_id\"])\n+        if \"eos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_eos_token_id(tokenizer_config[\"eos_token_id\"])\n+        if \"pad_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_pad_token_id(tokenizer_config[\"pad_token_id\"])\n+        if \"unk_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_unk_token_id(tokenizer_config[\"unk_token_id\"])\n+\n+        self.gguf_writer.add_add_space_prefix(False)\n+\n+    def set_gguf_parameters(self):\n+        hparams = self.hparams\n+        block_count = hparams[\"num_hidden_layers\"]\n+\n+        # Which layers are Mamba layers\n+        # PLaMo 2 uses mamba_step to indicate the pattern (e.g., 2 means every other layer)\n+        # This logic matches modeling_plamo.py's is_mamba function\n+        mamba_step = hparams.get(\"mamba_step\", 2)\n+        mamba_enabled = hparams.get(\"mamba_enabled\", True)\n+        mamba_layers = []\n+\n+        if mamba_enabled:\n+            for i in range(block_count):\n+                if block_count <= (mamba_step // 2):\n+                    # use attention in last layer\n+                    is_mamba = (i != block_count - 1)\n+                else:\n+                    is_mamba = (i % mamba_step) != (mamba_step // 2)\n+                if is_mamba:\n+                    mamba_layers.append(0)\n+                else:\n+                    mamba_layers.append(hparams.get(\"num_key_value_heads\", 4))\n+\n+        if mamba_layers:\n+            self.gguf_writer.add_head_count_kv(mamba_layers)\n+\n+        self.gguf_writer.add_context_length(hparams.get(\"max_position_embeddings\", 2048))\n+        self.gguf_writer.add_embedding_length(hparams.get(\"hidden_size\", 4096))\n+        self.gguf_writer.add_block_count(block_count)\n+        self.gguf_writer.add_head_count(hparams.get(\"num_attention_heads\", 32))\n+        self.gguf_writer.add_layer_norm_rms_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_group_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_layer_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_rope_freq_base(hparams.get(\"rope_theta\", 1000000.0))\n+\n+        # Mamba parameters\n+        self.gguf_writer.add_ssm_state_size(hparams.get(\"mamba_d_state\", 64))\n+        self.gguf_writer.add_ssm_conv_kernel(hparams.get(\"mamba_d_conv\", 4))\n+        self.gguf_writer.add_ssm_time_step_rank(hparams.get(\"mamba_num_heads\", 64))\n+        intermediate_size = hparams.get(\"mamba_num_heads\", 64) * hparams.get(\"hidden_size_per_head\", 128)\n+        self.gguf_writer.add_ssm_inner_size(intermediate_size)\n+        self.gguf_writer.add_ssm_group_count(0)\n+\n+        # MLP feed forward parameters (for attention layers)\n+        self.gguf_writer.add_feed_forward_length(hparams.get(\"intermediate_size\", 16384))\n+        self.gguf_writer.add_file_type(self.ftype)\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        del bid  # unused\n+\n+        if name.endswith(\".A_log\"):\n+            data_torch = -torch.exp(data_torch)\n+        elif name.endswith(\".dt_bias\"):\n+            name = name.rpartition(\".dt_bias\")[0] + \".dt_proj.bias\"\n+        elif name.endswith(\".dt_norm_weight\"):\n+            name = name.rpartition(\".dt_norm_weight\")[0] + \".dt_norm.weight\"\n+        elif name.endswith(\".B_norm_weight\"):\n+            name = name.rpartition(\".B_norm_weight\")[0] + \".B_norm.weight\"\n+        elif name.endswith(\".C_norm_weight\"):\n+            name = name.rpartition(\".C_norm_weight\")[0] + \".C_norm.weight\"\n+        elif name.endswith(\".k_weight\"):\n+            name = name.rpartition(\".k_weight\")[0] + \".k.weight\"\n+        elif name.endswith(\".q_weight\"):\n+            name = name.rpartition(\".q_weight\")[0] + \".q.weight\"\n+        elif name.endswith(\".conv1d.weight\"):\n+            data_torch = torch.squeeze(data_torch)  # remove (, 1, )\n+            assert data_torch.ndim == 2\n+        elif name.endswith(\".pre_mixer_norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".post_mixer_norm.weight\"):\n+            data_torch += 1.0 / 5\n+        elif name.endswith(\".pre_mlp_norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".post_mlp_norm.weight\"):\n+            data_torch += 1.0 / (5**1.5)\n+        elif name.endswith(\".norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".gate_up_proj.weight\"):\n+            # Split the combined gate_up tensor\n+            split_size = data_torch.shape[0] // 2\n+            gate_tensor = data_torch[:split_size, :]\n+            up_tensor = data_torch[split_size:, :]\n+\n+            # Return both tensors - remove .weight suffix if present\n+            name_base = name.replace(\".gate_up_proj.weight\", \"\")\n+            gate_name = name_base + \".ffn_gate.weight\"\n+            up_name = name_base + \".ffn_up.weight\"\n+\n+            gate_mapped = self.map_tensor_name(gate_name)\n+            up_mapped = self.map_tensor_name(up_name)",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2199943276",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14560,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2199943276",
        "commented_code": "@@ -3494,6 +3494,172 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         return [(new_name, data_torch)]\n \n \n+@ModelBase.register(\"Plamo2ForCausalLM\", \"PLaMo2ForCausalLM\")\n+class Plamo2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.PLAMO2\n+\n+    def set_vocab(self):\n+        # PLaMo 2 uses a custom tokenizer with a .jsonl file\n+        # We need to handle this specially\n+        tokenizer_jsonl_path = self.dir_model / \"tokenizer.jsonl\"\n+        tokenizer_config_path = self.dir_model / \"tokenizer_config.json\"\n+\n+        if not tokenizer_jsonl_path.is_file():\n+            raise FileNotFoundError(f\"PLaMo 2 tokenizer file not found: {tokenizer_jsonl_path}\")\n+\n+        # Load tokenizer config\n+        with open(tokenizer_config_path, 'r', encoding='utf-8') as f:\n+            tokenizer_config = json.load(f)\n+\n+        # Load tokens from JSONL file (actually a list format)\n+        tokens = []\n+        scores = []\n+        toktypes = []\n+\n+        with open(tokenizer_jsonl_path, 'r', encoding='utf-8') as f:\n+            for line_num, line in enumerate(f):\n+                if line.strip():\n+                    token_data = json.loads(line)\n+                    # Format: [token, score, type, ?, ?, ?, ?]\n+                    token = token_data[0].encode(\"utf-8\")\n+                    score = float(token_data[1])\n+                    token_type_str = token_data[2] if len(token_data) > 2 else \"NORMAL\"\n+\n+                    tokens.append(token)\n+                    scores.append(score)\n+\n+                    # Map token type strings to GGUF token types\n+                    if token_type_str == \"UNKNOWN\":\n+                        toktypes.append(gguf.TokenType.UNKNOWN)\n+                    elif token_type_str == \"CONTROL\":\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    elif token_type_str == \"BYTE\":\n+                        toktypes.append(gguf.TokenType.BYTE)\n+                    else:\n+                        # Check for PLaMo-2 special tokens\n+                        token_str = token_data[0]\n+                        if token_str.startswith(\"<|plamo:\") and token_str.endswith(\"|>\"):\n+                            toktypes.append(gguf.TokenType.CONTROL)\n+                        else:\n+                            toktypes.append(gguf.TokenType.NORMAL)\n+\n+        # Use \"plamo2\" tokenizer type for PLaMo-2's custom Aho-Corasick tokenizer\n+        self.gguf_writer.add_tokenizer_model(\"plamo2\")\n+        self.gguf_writer.add_tokenizer_pre(\"default\")\n+        self.gguf_writer.add_token_list(tokens)\n+        self.gguf_writer.add_token_scores(scores)\n+        self.gguf_writer.add_token_types(toktypes)\n+\n+        # Add special tokens from config\n+        if \"bos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_bos_token_id(tokenizer_config[\"bos_token_id\"])\n+        if \"eos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_eos_token_id(tokenizer_config[\"eos_token_id\"])\n+        if \"pad_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_pad_token_id(tokenizer_config[\"pad_token_id\"])\n+        if \"unk_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_unk_token_id(tokenizer_config[\"unk_token_id\"])\n+\n+        self.gguf_writer.add_add_space_prefix(False)\n+\n+    def set_gguf_parameters(self):\n+        hparams = self.hparams\n+        block_count = hparams[\"num_hidden_layers\"]\n+\n+        # Which layers are Mamba layers\n+        # PLaMo 2 uses mamba_step to indicate the pattern (e.g., 2 means every other layer)\n+        # This logic matches modeling_plamo.py's is_mamba function\n+        mamba_step = hparams.get(\"mamba_step\", 2)\n+        mamba_enabled = hparams.get(\"mamba_enabled\", True)\n+        mamba_layers = []\n+\n+        if mamba_enabled:\n+            for i in range(block_count):\n+                if block_count <= (mamba_step // 2):\n+                    # use attention in last layer\n+                    is_mamba = (i != block_count - 1)\n+                else:\n+                    is_mamba = (i % mamba_step) != (mamba_step // 2)\n+                if is_mamba:\n+                    mamba_layers.append(0)\n+                else:\n+                    mamba_layers.append(hparams.get(\"num_key_value_heads\", 4))\n+\n+        if mamba_layers:\n+            self.gguf_writer.add_head_count_kv(mamba_layers)\n+\n+        self.gguf_writer.add_context_length(hparams.get(\"max_position_embeddings\", 2048))\n+        self.gguf_writer.add_embedding_length(hparams.get(\"hidden_size\", 4096))\n+        self.gguf_writer.add_block_count(block_count)\n+        self.gguf_writer.add_head_count(hparams.get(\"num_attention_heads\", 32))\n+        self.gguf_writer.add_layer_norm_rms_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_group_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_layer_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_rope_freq_base(hparams.get(\"rope_theta\", 1000000.0))\n+\n+        # Mamba parameters\n+        self.gguf_writer.add_ssm_state_size(hparams.get(\"mamba_d_state\", 64))\n+        self.gguf_writer.add_ssm_conv_kernel(hparams.get(\"mamba_d_conv\", 4))\n+        self.gguf_writer.add_ssm_time_step_rank(hparams.get(\"mamba_num_heads\", 64))\n+        intermediate_size = hparams.get(\"mamba_num_heads\", 64) * hparams.get(\"hidden_size_per_head\", 128)\n+        self.gguf_writer.add_ssm_inner_size(intermediate_size)\n+        self.gguf_writer.add_ssm_group_count(0)\n+\n+        # MLP feed forward parameters (for attention layers)\n+        self.gguf_writer.add_feed_forward_length(hparams.get(\"intermediate_size\", 16384))\n+        self.gguf_writer.add_file_type(self.ftype)\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        del bid  # unused\n+\n+        if name.endswith(\".A_log\"):\n+            data_torch = -torch.exp(data_torch)\n+        elif name.endswith(\".dt_bias\"):\n+            name = name.rpartition(\".dt_bias\")[0] + \".dt_proj.bias\"\n+        elif name.endswith(\".dt_norm_weight\"):\n+            name = name.rpartition(\".dt_norm_weight\")[0] + \".dt_norm.weight\"\n+        elif name.endswith(\".B_norm_weight\"):\n+            name = name.rpartition(\".B_norm_weight\")[0] + \".B_norm.weight\"\n+        elif name.endswith(\".C_norm_weight\"):\n+            name = name.rpartition(\".C_norm_weight\")[0] + \".C_norm.weight\"\n+        elif name.endswith(\".k_weight\"):\n+            name = name.rpartition(\".k_weight\")[0] + \".k.weight\"\n+        elif name.endswith(\".q_weight\"):\n+            name = name.rpartition(\".q_weight\")[0] + \".q.weight\"\n+        elif name.endswith(\".conv1d.weight\"):\n+            data_torch = torch.squeeze(data_torch)  # remove (, 1, )\n+            assert data_torch.ndim == 2\n+        elif name.endswith(\".pre_mixer_norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".post_mixer_norm.weight\"):\n+            data_torch += 1.0 / 5\n+        elif name.endswith(\".pre_mlp_norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".post_mlp_norm.weight\"):\n+            data_torch += 1.0 / (5**1.5)\n+        elif name.endswith(\".norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".gate_up_proj.weight\"):\n+            # Split the combined gate_up tensor\n+            split_size = data_torch.shape[0] // 2\n+            gate_tensor = data_torch[:split_size, :]\n+            up_tensor = data_torch[split_size:, :]\n+\n+            # Return both tensors - remove .weight suffix if present\n+            name_base = name.replace(\".gate_up_proj.weight\", \"\")\n+            gate_name = name_base + \".ffn_gate.weight\"\n+            up_name = name_base + \".ffn_up.weight\"\n+\n+            gate_mapped = self.map_tensor_name(gate_name)\n+            up_mapped = self.map_tensor_name(up_name)",
        "comment_created_at": "2025-07-11T07:47:41+00:00",
        "comment_author": "CISC",
        "comment_body": "You don't need to do this, you can use `LLM_FFN_SWIGLU`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2202317806",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14560,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2199943276",
        "commented_code": "@@ -3494,6 +3494,172 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n         return [(new_name, data_torch)]\n \n \n+@ModelBase.register(\"Plamo2ForCausalLM\", \"PLaMo2ForCausalLM\")\n+class Plamo2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.PLAMO2\n+\n+    def set_vocab(self):\n+        # PLaMo 2 uses a custom tokenizer with a .jsonl file\n+        # We need to handle this specially\n+        tokenizer_jsonl_path = self.dir_model / \"tokenizer.jsonl\"\n+        tokenizer_config_path = self.dir_model / \"tokenizer_config.json\"\n+\n+        if not tokenizer_jsonl_path.is_file():\n+            raise FileNotFoundError(f\"PLaMo 2 tokenizer file not found: {tokenizer_jsonl_path}\")\n+\n+        # Load tokenizer config\n+        with open(tokenizer_config_path, 'r', encoding='utf-8') as f:\n+            tokenizer_config = json.load(f)\n+\n+        # Load tokens from JSONL file (actually a list format)\n+        tokens = []\n+        scores = []\n+        toktypes = []\n+\n+        with open(tokenizer_jsonl_path, 'r', encoding='utf-8') as f:\n+            for line_num, line in enumerate(f):\n+                if line.strip():\n+                    token_data = json.loads(line)\n+                    # Format: [token, score, type, ?, ?, ?, ?]\n+                    token = token_data[0].encode(\"utf-8\")\n+                    score = float(token_data[1])\n+                    token_type_str = token_data[2] if len(token_data) > 2 else \"NORMAL\"\n+\n+                    tokens.append(token)\n+                    scores.append(score)\n+\n+                    # Map token type strings to GGUF token types\n+                    if token_type_str == \"UNKNOWN\":\n+                        toktypes.append(gguf.TokenType.UNKNOWN)\n+                    elif token_type_str == \"CONTROL\":\n+                        toktypes.append(gguf.TokenType.CONTROL)\n+                    elif token_type_str == \"BYTE\":\n+                        toktypes.append(gguf.TokenType.BYTE)\n+                    else:\n+                        # Check for PLaMo-2 special tokens\n+                        token_str = token_data[0]\n+                        if token_str.startswith(\"<|plamo:\") and token_str.endswith(\"|>\"):\n+                            toktypes.append(gguf.TokenType.CONTROL)\n+                        else:\n+                            toktypes.append(gguf.TokenType.NORMAL)\n+\n+        # Use \"plamo2\" tokenizer type for PLaMo-2's custom Aho-Corasick tokenizer\n+        self.gguf_writer.add_tokenizer_model(\"plamo2\")\n+        self.gguf_writer.add_tokenizer_pre(\"default\")\n+        self.gguf_writer.add_token_list(tokens)\n+        self.gguf_writer.add_token_scores(scores)\n+        self.gguf_writer.add_token_types(toktypes)\n+\n+        # Add special tokens from config\n+        if \"bos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_bos_token_id(tokenizer_config[\"bos_token_id\"])\n+        if \"eos_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_eos_token_id(tokenizer_config[\"eos_token_id\"])\n+        if \"pad_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_pad_token_id(tokenizer_config[\"pad_token_id\"])\n+        if \"unk_token_id\" in tokenizer_config:\n+            self.gguf_writer.add_unk_token_id(tokenizer_config[\"unk_token_id\"])\n+\n+        self.gguf_writer.add_add_space_prefix(False)\n+\n+    def set_gguf_parameters(self):\n+        hparams = self.hparams\n+        block_count = hparams[\"num_hidden_layers\"]\n+\n+        # Which layers are Mamba layers\n+        # PLaMo 2 uses mamba_step to indicate the pattern (e.g., 2 means every other layer)\n+        # This logic matches modeling_plamo.py's is_mamba function\n+        mamba_step = hparams.get(\"mamba_step\", 2)\n+        mamba_enabled = hparams.get(\"mamba_enabled\", True)\n+        mamba_layers = []\n+\n+        if mamba_enabled:\n+            for i in range(block_count):\n+                if block_count <= (mamba_step // 2):\n+                    # use attention in last layer\n+                    is_mamba = (i != block_count - 1)\n+                else:\n+                    is_mamba = (i % mamba_step) != (mamba_step // 2)\n+                if is_mamba:\n+                    mamba_layers.append(0)\n+                else:\n+                    mamba_layers.append(hparams.get(\"num_key_value_heads\", 4))\n+\n+        if mamba_layers:\n+            self.gguf_writer.add_head_count_kv(mamba_layers)\n+\n+        self.gguf_writer.add_context_length(hparams.get(\"max_position_embeddings\", 2048))\n+        self.gguf_writer.add_embedding_length(hparams.get(\"hidden_size\", 4096))\n+        self.gguf_writer.add_block_count(block_count)\n+        self.gguf_writer.add_head_count(hparams.get(\"num_attention_heads\", 32))\n+        self.gguf_writer.add_layer_norm_rms_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_group_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_layer_norm_eps(hparams.get(\"rms_norm_eps\", 1e-06))\n+        self.gguf_writer.add_rope_freq_base(hparams.get(\"rope_theta\", 1000000.0))\n+\n+        # Mamba parameters\n+        self.gguf_writer.add_ssm_state_size(hparams.get(\"mamba_d_state\", 64))\n+        self.gguf_writer.add_ssm_conv_kernel(hparams.get(\"mamba_d_conv\", 4))\n+        self.gguf_writer.add_ssm_time_step_rank(hparams.get(\"mamba_num_heads\", 64))\n+        intermediate_size = hparams.get(\"mamba_num_heads\", 64) * hparams.get(\"hidden_size_per_head\", 128)\n+        self.gguf_writer.add_ssm_inner_size(intermediate_size)\n+        self.gguf_writer.add_ssm_group_count(0)\n+\n+        # MLP feed forward parameters (for attention layers)\n+        self.gguf_writer.add_feed_forward_length(hparams.get(\"intermediate_size\", 16384))\n+        self.gguf_writer.add_file_type(self.ftype)\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        del bid  # unused\n+\n+        if name.endswith(\".A_log\"):\n+            data_torch = -torch.exp(data_torch)\n+        elif name.endswith(\".dt_bias\"):\n+            name = name.rpartition(\".dt_bias\")[0] + \".dt_proj.bias\"\n+        elif name.endswith(\".dt_norm_weight\"):\n+            name = name.rpartition(\".dt_norm_weight\")[0] + \".dt_norm.weight\"\n+        elif name.endswith(\".B_norm_weight\"):\n+            name = name.rpartition(\".B_norm_weight\")[0] + \".B_norm.weight\"\n+        elif name.endswith(\".C_norm_weight\"):\n+            name = name.rpartition(\".C_norm_weight\")[0] + \".C_norm.weight\"\n+        elif name.endswith(\".k_weight\"):\n+            name = name.rpartition(\".k_weight\")[0] + \".k.weight\"\n+        elif name.endswith(\".q_weight\"):\n+            name = name.rpartition(\".q_weight\")[0] + \".q.weight\"\n+        elif name.endswith(\".conv1d.weight\"):\n+            data_torch = torch.squeeze(data_torch)  # remove (, 1, )\n+            assert data_torch.ndim == 2\n+        elif name.endswith(\".pre_mixer_norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".post_mixer_norm.weight\"):\n+            data_torch += 1.0 / 5\n+        elif name.endswith(\".pre_mlp_norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".post_mlp_norm.weight\"):\n+            data_torch += 1.0 / (5**1.5)\n+        elif name.endswith(\".norm.weight\"):\n+            data_torch += 1.0\n+        elif name.endswith(\".gate_up_proj.weight\"):\n+            # Split the combined gate_up tensor\n+            split_size = data_torch.shape[0] // 2\n+            gate_tensor = data_torch[:split_size, :]\n+            up_tensor = data_torch[split_size:, :]\n+\n+            # Return both tensors - remove .weight suffix if present\n+            name_base = name.replace(\".gate_up_proj.weight\", \"\")\n+            gate_name = name_base + \".ffn_gate.weight\"\n+            up_name = name_base + \".ffn_up.weight\"\n+\n+            gate_mapped = self.map_tensor_name(gate_name)\n+            up_mapped = self.map_tensor_name(up_name)",
        "comment_created_at": "2025-07-12T04:46:23+00:00",
        "comment_author": "mitmul",
        "comment_body": "Thank you very much for letting me know it. I removed this part and modified llama-model.cpp to use `LLM_FFN_SWIGLU` with `build_ffn()` in https://github.com/ggml-org/llama.cpp/pull/14560/commits/498b8b373d2fe318e702f33e7b816ce8769bc4d7",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2200026699",
    "pr_number": 14620,
    "pr_file": "convert_hf_to_gguf.py",
    "created_at": "2025-07-11T08:21:51+00:00",
    "commented_code": "chat_template = tokenizer.chat_template.replace(\"[:]\", \"\")\n             self.gguf_writer.add_chat_template(chat_template)\n \n+\n+@ModelBase.register(\"LFM2ForCausalLM\")\n+class LFM2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LFM2\n+\n+    def _add_feed_forward_length(self):\n+        ff_dim = self.hparams[\"block_ff_dim\"]\n+\n+        auto_adjust_ff_dim = self.hparams[\"block_auto_adjust_ff_dim\"]\n+        ff_dim = self.hparams[\"block_ff_dim\"]\n+        ffn_dim_multiplier = self.hparams[\"block_ffn_dim_multiplier\"]\n+        multiple_of = self.hparams[\"block_multiple_of\"]\n+\n+        if auto_adjust_ff_dim:\n+            ff_dim = int(2 * ff_dim / 3)\n+            # custom dim factor multiplier\n+            if ffn_dim_multiplier is not None:\n+                ff_dim = int(ffn_dim_multiplier * ff_dim)\n+            ff_dim = multiple_of * ((ff_dim + multiple_of - 1) // multiple_of)\n+\n+        self.gguf_writer.add_feed_forward_length(ff_dim)\n+\n+    def set_gguf_parameters(self):\n+        # set num_key_value_heads only for attention layers\n+        self.hparams[\"num_key_value_heads\"] = [\n+            self.hparams[\"num_key_value_heads\"] if layer_type == \"full_attention\" else 0\n+            for layer_type in self.hparams[\"layer_types\"]\n+        ]\n+\n+        super().set_gguf_parameters()\n+        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n+        self.gguf_writer.add_shortconv_l_cache(self.hparams[\"conv_L_cache\"])\n+        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"norm_eps\"])\n+        self._add_feed_forward_length()\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        if 'operator_norm' in name:\n+            name = name.replace('operator_norm', 'norm')\n+        elif 'attention.k_layernorm' in name or 'attention.q_layernorm' in name:\n+            name = name.replace('attention', 'self_attn')\n+        elif name.startswith(\"model.embedding_norm\"):\n+            name = name.replace(\"model.embedding_norm\", 'word_embeddings_layernorm')\n+        elif 'conv.conv' in name:\n+            # conv op requires 2d tensor\n+            data_torch = data_torch.squeeze(1)\n+        elif 'self_attn.out_proj' in name:\n+            name = name.replace('out_proj', 'o_proj')",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2200026699",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14620,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2200026699",
        "commented_code": "@@ -7070,6 +7074,57 @@ def set_vocab(self):\n             chat_template = tokenizer.chat_template.replace(\"[:]\", \"\")\n             self.gguf_writer.add_chat_template(chat_template)\n \n+\n+@ModelBase.register(\"LFM2ForCausalLM\")\n+class LFM2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LFM2\n+\n+    def _add_feed_forward_length(self):\n+        ff_dim = self.hparams[\"block_ff_dim\"]\n+\n+        auto_adjust_ff_dim = self.hparams[\"block_auto_adjust_ff_dim\"]\n+        ff_dim = self.hparams[\"block_ff_dim\"]\n+        ffn_dim_multiplier = self.hparams[\"block_ffn_dim_multiplier\"]\n+        multiple_of = self.hparams[\"block_multiple_of\"]\n+\n+        if auto_adjust_ff_dim:\n+            ff_dim = int(2 * ff_dim / 3)\n+            # custom dim factor multiplier\n+            if ffn_dim_multiplier is not None:\n+                ff_dim = int(ffn_dim_multiplier * ff_dim)\n+            ff_dim = multiple_of * ((ff_dim + multiple_of - 1) // multiple_of)\n+\n+        self.gguf_writer.add_feed_forward_length(ff_dim)\n+\n+    def set_gguf_parameters(self):\n+        # set num_key_value_heads only for attention layers\n+        self.hparams[\"num_key_value_heads\"] = [\n+            self.hparams[\"num_key_value_heads\"] if layer_type == \"full_attention\" else 0\n+            for layer_type in self.hparams[\"layer_types\"]\n+        ]\n+\n+        super().set_gguf_parameters()\n+        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n+        self.gguf_writer.add_shortconv_l_cache(self.hparams[\"conv_L_cache\"])\n+        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"norm_eps\"])\n+        self._add_feed_forward_length()\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        if 'operator_norm' in name:\n+            name = name.replace('operator_norm', 'norm')\n+        elif 'attention.k_layernorm' in name or 'attention.q_layernorm' in name:\n+            name = name.replace('attention', 'self_attn')\n+        elif name.startswith(\"model.embedding_norm\"):\n+            name = name.replace(\"model.embedding_norm\", 'word_embeddings_layernorm')\n+        elif 'conv.conv' in name:\n+            # conv op requires 2d tensor\n+            data_torch = data_torch.squeeze(1)\n+        elif 'self_attn.out_proj' in name:\n+            name = name.replace('out_proj', 'o_proj')",
        "comment_created_at": "2025-07-11T08:21:51+00:00",
        "comment_author": "ngxson",
        "comment_body": "It's recommended to have explicit mapping whenever possible, the list of mapping can be found in `tensor_mapping.py`. But this can be done in another PR.",
        "pr_file_module": null
      },
      {
        "comment_id": "2200645848",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14620,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2200026699",
        "commented_code": "@@ -7070,6 +7074,57 @@ def set_vocab(self):\n             chat_template = tokenizer.chat_template.replace(\"[:]\", \"\")\n             self.gguf_writer.add_chat_template(chat_template)\n \n+\n+@ModelBase.register(\"LFM2ForCausalLM\")\n+class LFM2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LFM2\n+\n+    def _add_feed_forward_length(self):\n+        ff_dim = self.hparams[\"block_ff_dim\"]\n+\n+        auto_adjust_ff_dim = self.hparams[\"block_auto_adjust_ff_dim\"]\n+        ff_dim = self.hparams[\"block_ff_dim\"]\n+        ffn_dim_multiplier = self.hparams[\"block_ffn_dim_multiplier\"]\n+        multiple_of = self.hparams[\"block_multiple_of\"]\n+\n+        if auto_adjust_ff_dim:\n+            ff_dim = int(2 * ff_dim / 3)\n+            # custom dim factor multiplier\n+            if ffn_dim_multiplier is not None:\n+                ff_dim = int(ffn_dim_multiplier * ff_dim)\n+            ff_dim = multiple_of * ((ff_dim + multiple_of - 1) // multiple_of)\n+\n+        self.gguf_writer.add_feed_forward_length(ff_dim)\n+\n+    def set_gguf_parameters(self):\n+        # set num_key_value_heads only for attention layers\n+        self.hparams[\"num_key_value_heads\"] = [\n+            self.hparams[\"num_key_value_heads\"] if layer_type == \"full_attention\" else 0\n+            for layer_type in self.hparams[\"layer_types\"]\n+        ]\n+\n+        super().set_gguf_parameters()\n+        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n+        self.gguf_writer.add_shortconv_l_cache(self.hparams[\"conv_L_cache\"])\n+        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"norm_eps\"])\n+        self._add_feed_forward_length()\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        if 'operator_norm' in name:\n+            name = name.replace('operator_norm', 'norm')\n+        elif 'attention.k_layernorm' in name or 'attention.q_layernorm' in name:\n+            name = name.replace('attention', 'self_attn')\n+        elif name.startswith(\"model.embedding_norm\"):\n+            name = name.replace(\"model.embedding_norm\", 'word_embeddings_layernorm')\n+        elif 'conv.conv' in name:\n+            # conv op requires 2d tensor\n+            data_torch = data_torch.squeeze(1)\n+        elif 'self_attn.out_proj' in name:\n+            name = name.replace('out_proj', 'o_proj')",
        "comment_created_at": "2025-07-11T12:47:46+00:00",
        "comment_author": "tdakhran",
        "comment_body": "I didn't want to duplicate semantically similar tensors, and I also observed that other models were doing similar remapping. I will add new entries and change to explicit mappings.",
        "pr_file_module": null
      },
      {
        "comment_id": "2200799853",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14620,
        "pr_file": "convert_hf_to_gguf.py",
        "discussion_id": "2200026699",
        "commented_code": "@@ -7070,6 +7074,57 @@ def set_vocab(self):\n             chat_template = tokenizer.chat_template.replace(\"[:]\", \"\")\n             self.gguf_writer.add_chat_template(chat_template)\n \n+\n+@ModelBase.register(\"LFM2ForCausalLM\")\n+class LFM2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.LFM2\n+\n+    def _add_feed_forward_length(self):\n+        ff_dim = self.hparams[\"block_ff_dim\"]\n+\n+        auto_adjust_ff_dim = self.hparams[\"block_auto_adjust_ff_dim\"]\n+        ff_dim = self.hparams[\"block_ff_dim\"]\n+        ffn_dim_multiplier = self.hparams[\"block_ffn_dim_multiplier\"]\n+        multiple_of = self.hparams[\"block_multiple_of\"]\n+\n+        if auto_adjust_ff_dim:\n+            ff_dim = int(2 * ff_dim / 3)\n+            # custom dim factor multiplier\n+            if ffn_dim_multiplier is not None:\n+                ff_dim = int(ffn_dim_multiplier * ff_dim)\n+            ff_dim = multiple_of * ((ff_dim + multiple_of - 1) // multiple_of)\n+\n+        self.gguf_writer.add_feed_forward_length(ff_dim)\n+\n+    def set_gguf_parameters(self):\n+        # set num_key_value_heads only for attention layers\n+        self.hparams[\"num_key_value_heads\"] = [\n+            self.hparams[\"num_key_value_heads\"] if layer_type == \"full_attention\" else 0\n+            for layer_type in self.hparams[\"layer_types\"]\n+        ]\n+\n+        super().set_gguf_parameters()\n+        self.gguf_writer.add_vocab_size(self.hparams[\"vocab_size\"])\n+        self.gguf_writer.add_shortconv_l_cache(self.hparams[\"conv_L_cache\"])\n+        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"norm_eps\"])\n+        self._add_feed_forward_length()\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        if 'operator_norm' in name:\n+            name = name.replace('operator_norm', 'norm')\n+        elif 'attention.k_layernorm' in name or 'attention.q_layernorm' in name:\n+            name = name.replace('attention', 'self_attn')\n+        elif name.startswith(\"model.embedding_norm\"):\n+            name = name.replace(\"model.embedding_norm\", 'word_embeddings_layernorm')\n+        elif 'conv.conv' in name:\n+            # conv op requires 2d tensor\n+            data_torch = data_torch.squeeze(1)\n+        elif 'self_attn.out_proj' in name:\n+            name = name.replace('out_proj', 'o_proj')",
        "comment_created_at": "2025-07-11T13:50:09+00:00",
        "comment_author": "tdakhran",
        "comment_body": "I managed to remove all tensor remappings",
        "pr_file_module": null
      }
    ]
  }
]