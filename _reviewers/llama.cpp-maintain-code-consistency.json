[
  {
    "discussion_id": "2112773377",
    "pr_number": 13873,
    "pr_file": "ggml/include/ggml-opt.h",
    "created_at": "2025-05-28T21:18:25+00:00",
    "commented_code": "GGML_OPT_BUILD_TYPE_OPT     = 30,\n     };\n \n+    enum ggml_opt_optimizer\n+    {",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2112773377",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/include/ggml-opt.h",
        "discussion_id": "2112773377",
        "commented_code": "@@ -74,6 +74,14 @@ extern \"C\" {\n         GGML_OPT_BUILD_TYPE_OPT     = 30,\n     };\n \n+    enum ggml_opt_optimizer\n+    {",
        "comment_created_at": "2025-05-28T21:18:25+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n    enum ggml_opt_optimizer {\r\n```\r\n\r\nFor consistency with the surrounding code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2137534344",
    "pr_number": 13873,
    "pr_file": "ggml/include/ggml-opt.h",
    "created_at": "2025-06-10T10:42:54+00:00",
    "commented_code": "struct ggml_tensor            * outputs,        // output tensor, must have shape [ne_label, ndata_batch] if labels are used\n             ggml_opt_dataset_t              dataset,        // dataset with data and optionally also labels\n             enum ggml_opt_loss_type         loss_type,      // loss to minimize\n+            enum ggml_opt_optimizer_type    optimizer,      // sgd or adamw\n             ggml_opt_get_optimizer_params   get_opt_pars,   // callback to get optimizer params, userdata is pointer to epoch (of type int64_t)\n             int64_t                         nepoch,         // how many times the dataset should be iterated over\n             int64_t                         nbatch_logical, // datapoints optimizer step, must be a multiple of ndata_batch in inputs/outputs\n             float                           val_split,      // fraction of the dataset to use for validation, must be in [0.0f, 1.0f)\n             bool                            silent);        // whether or not info prints to stderr should be suppressed\n \n+    GGML_API enum ggml_opt_optimizer_type ggml_opt_context_optimizer_type(ggml_opt_context_t);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2137534344",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/include/ggml-opt.h",
        "discussion_id": "2137534344",
        "commented_code": "@@ -226,12 +243,14 @@ extern \"C\" {\n             struct ggml_tensor            * outputs,        // output tensor, must have shape [ne_label, ndata_batch] if labels are used\n             ggml_opt_dataset_t              dataset,        // dataset with data and optionally also labels\n             enum ggml_opt_loss_type         loss_type,      // loss to minimize\n+            enum ggml_opt_optimizer_type    optimizer,      // sgd or adamw\n             ggml_opt_get_optimizer_params   get_opt_pars,   // callback to get optimizer params, userdata is pointer to epoch (of type int64_t)\n             int64_t                         nepoch,         // how many times the dataset should be iterated over\n             int64_t                         nbatch_logical, // datapoints optimizer step, must be a multiple of ndata_batch in inputs/outputs\n             float                           val_split,      // fraction of the dataset to use for validation, must be in [0.0f, 1.0f)\n             bool                            silent);        // whether or not info prints to stderr should be suppressed\n \n+    GGML_API enum ggml_opt_optimizer_type ggml_opt_context_optimizer_type(ggml_opt_context_t);",
        "comment_created_at": "2025-06-10T10:42:54+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Move this declaration upwards so that it's in the same place as the other getters for `ggml_opt_context` (remember to also move the implementation).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2204661031",
    "pr_number": 14644,
    "pr_file": "examples/diffusion/diffusion.h",
    "created_at": "2025-07-14T11:25:25+00:00",
    "commented_code": "+#pragma once\n+\n+#include \"llama.h\"\n+\n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n+typedef bool (*diffusion_step_callback_t)(int32_t step, int32_t total_steps, const llama_token * tokens,\n+                                          int32_t n_tokens, void * user_data);\n+\n+enum diffusion_algorithm {\n+    DIFFUSION_ALG_ORIGIN       = 0,\n+    DIFFUSION_ALG_MASKGIT_PLUS = 1,\n+    DIFFUSION_ALG_TOPK_MARGIN  = 2,\n+    DIFFUSION_ALG_ENTROPY      = 3,\n+};\n+\n+struct diffusion_params {\n+    int32_t                   steps;\n+    float                     eps;\n+    float                     temperature;\n+    float                     top_p;\n+    int32_t                   top_k;\n+    llama_token               mask_token_id;\n+    enum diffusion_algorithm  algorithm;\n+    float                     alg_temp;\n+    diffusion_step_callback_t step_callback;\n+    void *                    step_callback_user_data;\n+    int32_t                   seed;\n+};\n+\n+struct diffusion_params diffusion_default_params(void);\n+\n+void diffusion_generate(llama_context * ctx, const llama_token * input_tokens, llama_token * output_tokens,\n+                        int32_t n_input, int32_t max_length, struct diffusion_params params, int32_t * n_generated);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2204661031",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14644,
        "pr_file": "examples/diffusion/diffusion.h",
        "discussion_id": "2204661031",
        "commented_code": "@@ -0,0 +1,40 @@\n+#pragma once\n+\n+#include \"llama.h\"\n+\n+#ifdef __cplusplus\n+extern \"C\" {\n+#endif\n+\n+typedef bool (*diffusion_step_callback_t)(int32_t step, int32_t total_steps, const llama_token * tokens,\n+                                          int32_t n_tokens, void * user_data);\n+\n+enum diffusion_algorithm {\n+    DIFFUSION_ALG_ORIGIN       = 0,\n+    DIFFUSION_ALG_MASKGIT_PLUS = 1,\n+    DIFFUSION_ALG_TOPK_MARGIN  = 2,\n+    DIFFUSION_ALG_ENTROPY      = 3,\n+};\n+\n+struct diffusion_params {\n+    int32_t                   steps;\n+    float                     eps;\n+    float                     temperature;\n+    float                     top_p;\n+    int32_t                   top_k;\n+    llama_token               mask_token_id;\n+    enum diffusion_algorithm  algorithm;\n+    float                     alg_temp;\n+    diffusion_step_callback_t step_callback;\n+    void *                    step_callback_user_data;\n+    int32_t                   seed;\n+};\n+\n+struct diffusion_params diffusion_default_params(void);\n+\n+void diffusion_generate(llama_context * ctx, const llama_token * input_tokens, llama_token * output_tokens,\n+                        int32_t n_input, int32_t max_length, struct diffusion_params params, int32_t * n_generated);",
        "comment_created_at": "2025-07-14T11:25:25+00:00",
        "comment_author": "ggerganov",
        "comment_body": "For long argument lists, list them on new lines to improve readibility.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2141701738",
    "pr_number": 13979,
    "pr_file": "src/llama-graph.h",
    "created_at": "2025-06-12T04:42:22+00:00",
    "commented_code": "ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2141701738",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-12T04:42:22+00:00",
        "comment_author": "ggerganov",
        "comment_body": "This method is analogous to the `build_attn_inp_` methods, so we have to model it in a similar way.\r\n\r\nReplace this method with:\r\n\r\n```c++\r\n    // similar to build_attn_inp_kv_unified()\r\n    llm_graph_input_rs * build_rs_inp() const;\r\n```\r\n\r\nIntroduce new input class:\r\n\r\n```c++\r\n// similar to llm_graph_input_attn_kv_unified\r\n// put the `s_copy` tensor in this class (similar to `kq_mask`)\r\nclass llm_graph_input_rs : public llm_graph_input_i;\r\n```\r\n\r\nIn the future, this input class could be extended with additional input tensors that are needed by the recurrent cache if necessary (similar to the attention input classes).\r\n\r\nReplace `build_recurrent_state()` and `build_rwkv_shift_load()` with overloads:\r\n\r\n```c++\r\n    // similar to build_attn()\r\n    ggml_tensor * build_rs(\r\n        llm_graph_input_rs * inp,\r\n             ggml_cgraph * gf,\r\n             ggml_tensor * s,             \r\n                 int32_t   state_size,\r\n                 int32_t   n_seqs,\r\n                    bool   avoid_copies = false) const;\r\n\r\n    // similar to build_attn()\r\n    ggml_tensor * build_rwkv_token_shift_load(\r\n        llm_graph_input_rs * inp,\r\n             ggml_cgraph * gf,\r\n      const llama_ubatch & ubatch,\r\n                     int   il) const;\r\n```\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2141723520",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-12T05:07:01+00:00",
        "comment_author": "ggerganov",
        "comment_body": "When this change is applied, we have to do a similar addition for the hybrid implementation. The basic pattern is that you need to introduce a new input class similar to `llm_graph_input_attn_kv_unified` and `llm_graph_input_rs`, but this one will contain inputs for both the attention and for the recurrent state.\r\n\r\nSo probably something like:\r\n\r\n```c++\r\n// this input class will have both the input tensors needed for the attention and for\r\n// the recurrent state. see llm_graph_input_attn_kv_unified_iswa for example\r\nclass llm_graph_input_mem_hybrid : public llm_graph_input_i;\r\n```\r\n\r\nWe then add overloads for `build_attn()`, `build_rs()` and `build_rwkv_token_shift_load()`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2143691493",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-12T21:26:31+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "Ok, this makes sense. I'll work on this refactor which I think will also fix the comment below about needing to pull the `kv_state` from the `inp` class.",
        "pr_file_module": null
      },
      {
        "comment_id": "2143740510",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-12T22:10:06+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "I'm trying to work through all of the changes suggested here and the implications above of not mixing `unified` with `hybrid` in `build_attn`. It's resulting in a lot of code duplication, most of which seems fairly benign, but I'm now to the point of implementing `build_rs(llm_graph_input_rs_hybrid * inp, ...` which will be identical to the non-hybrid version except for the `kv_state` extraction. As is today, it's a pretty complex method, so I worry that if issues arise in the logic, they will now need to be fixed twice.\r\n\r\nOne alternative that _might_ work and would reduce a lot of the changes in `llama_graph` would be for `llama_kv_cache_hybrid_recurrent` state to use multiple inheritance and use an `isa` approach to the child state types rather than a `hasa` approach. This would require that the child state types declare their type-specific interface methods as `virtual`, but it would avoid the need to differentiate state types everywhere in `llama-graph`.\r\n\r\nFor completeness, I'm going to try to push forward with the current proposal of creating `hybrid` versions of all structs and methods in `llama-graph`, but I wanted to throw this out as an option to consider to reduce the maintenance burden.",
        "pr_file_module": null
      },
      {
        "comment_id": "2143745799",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-12T22:16:00+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "Another option that would flagrantly violate the [coding guidelines](https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md#coding-guidelines) would be to use template specialization for the various implementations of `build_attn` / `build_rs`. This would allow a common implementation where appropriate that delegates the \"get the state\" functionality to an implementation-detail function that is overloaded by type.\r\n\r\n(not suggesting it, just brainstorming)",
        "pr_file_module": null
      },
      {
        "comment_id": "2143797374",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-12T23:07:00+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "Ok, I've now got the two sets of classes / methods to be quite consistent with the duplicate `build_attn` / `build_rs` implementations to avoid abstracting over the memory type",
        "pr_file_module": null
      },
      {
        "comment_id": "2143826097",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-12T23:36:46+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "I'm trying to rebase Granite 4 now and it's pushing the code duplication further to require that the layer builder methods (`build_mamba_layer`, `build_mamba2_layer`) be duplicated so that they can take the right `inp` type.\r\n\r\nI think there might be a _much_ simpler solution though: Override the type cast operator for the hybrid state so that it correctly type casts to its children with `static_cast`. I think this would completely remove the need for `hybrid_recurrent` versions of any of the graph methods!\r\n\r\nI need to sign off for the day, but will try this tomorrow if there are no objections.",
        "pr_file_module": null
      },
      {
        "comment_id": "2144280191",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-13T06:14:07+00:00",
        "comment_author": "ggerganov",
        "comment_body": "For now it's important to follow the existing patterns even if there is some extra code duplication. We can rework the implementation if needed in separate refactor PRs.\r\n\r\nThe recommended way to avoid large duplications is how the `build_attn()` is implemented to use a helper, memory-agnostic method `build_attn_mha()`. This way, the memory-specific logic is implemented in the `build_attn()` overloads and the bulk of the remaining logic is reused by calling `build_attn_mha()`. You can apply this pattern both for `build_rs()` and `build_mambaX_layer()`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2145630430",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-13T17:35:45+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "Ok, I'm pushing on this now, but I keep coming back to the `static_cast<>` baked into the `build_rs` methods. Even if I write a shared `build_rs_common` (name TBD) method that simply takes the right dereferenced values off of the correctly cast `kv_state`, we still need multiple `build_rs` implementations that dispatch based on the type of the `inp` argument (following the pattern). This in turn pushes up to the `build_mamba*_layer` methods causing the same problem over again. The `build_rs` calls in those methods are buried inside hundreds of other lines of code, so unlike how `build_attn_mha` is the \"heart\" of the attention mechanisms and the surrounding code is relatively trivial, for the recurrent layer builders, the `build_rs` is only a small part of the complexity. Duplicating all of that surrounding code based on the `inp` type would be a lot of complexity to maintain. We could simply template those methods on the `inp` type, but that would violate the desire to avoid templates.\r\n\r\nAs an alternative while we think through these refactors, I'm _fairly_ certain that if the `llama_kv_cache_hybrid_recurrent_state` simply provided cast operators to the two child state types, all changes in `llama-graph*` could be reverted from this PR and we could tackle this refactoring separately.",
        "pr_file_module": null
      },
      {
        "comment_id": "2145655456",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-13T17:40:21+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "Ultimately, I think the challenge here stems from the desire for the \"leaf of the tree\" (the model builder itself) to be the union of two different leaves where the only difference is the memory type and the logic behind which child is used when. This could be seen as a fully separate implementation in the same way `transformers` used to _intentionally_ require code copy-paste between models before they refactored to the modular approach, but even `transformers` eventually relaxed that to avoid the maintenance burden of so many different copies of the same code.",
        "pr_file_module": null
      },
      {
        "comment_id": "2145715312",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-13T18:08:03+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "Hmm, I may be wrong about the cast operator. I'll keep digging. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2145867250",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-13T18:45:27+00:00",
        "comment_author": "ggerganov",
        "comment_body": "Looking at the `build_mamba_layer()` function, it appears complex, but actually it boils down to something like this:\r\n\r\n```c++\r\nbuild_mamba_layer(inp) {\r\n    rec_state = static_cast<...>(mstate);\r\n\r\n    conv = rec_state->update_conv(inp); // analogous to unified get_k() + cpy_k()\r\n    ssm  = rec_state->update_ssm (inp); // analogous to unified get_v() + cpy_v()\r\n\r\n    common = get_common(model);\r\n\r\n    do_something_with_conv(model, common, conv);\r\n    do_something_with_ssm (model, common, ssm);\r\n}\r\n```\r\n\r\nThe `get_common(model)` and `do_something_with_...(model, ...)` parts have to be extracted out of this function and remain implemented in `llama-model.cpp` because they use model tensors and do not depend on the memory module. The `update_conv()` and `update_ssm()` function have to be implemented only once in the `class llama_kv_cache_recurrent`. Internally, they can use a helper `build_recurrent_state()` function to deduplicate the code. Note that the hybrid cache will call these methods from the recurrent cache instance and won't have to implement them a second time.\r\n\r\nWhen you decompose the function like this, the implementation becomes much simpler:\r\n\r\n```c++\r\nbuild_mamba_layer(inp) {\r\n    rec_state = static_cast<...based_on_inp...>(m_state);\r\n\r\n    conv = rec_state->update_conv(inp);\r\n    ssm  = rec_state->update_ssm (inp);\r\n\r\n    return { conv, ssm };\r\n}\r\n```\r\n\r\nThis is much easier to overload multiple times for different `inp` types. I would even split it one more time:\r\n\r\n```c++\r\n// llama-graph:\r\n\r\n// one for recurrent and one for hybrid\r\nbuild_mamba_conv(inp) {\r\n    rec_state = static_cast<...based on inp...>(m_state);\r\n\r\n    conv = rec_state->update_conv(inp->get_s_copy());\r\n    \r\n    return conv;\r\n}\r\n\r\n// one for recurrent and one for hybrid\r\nbuild_mamba_ssm(inp) {\r\n    rec_state = static_cast<...based on inp...>(m_state);\r\n\r\n    ssm = rec_state->update_ssm(inp->get_s_copy());\r\n    \r\n    return ssm;\r\n}\r\n```\r\n\r\nI hope I'm not missing something, but I think this should work and should be clean.\r\n\r\nAlternatively, we can also just bring the PR to a working state any way you think makes sense and then I will try to do a refactoring pass and see how it goes. Would just need some sample test commands to experiment with.",
        "pr_file_module": null
      },
      {
        "comment_id": "2146078023",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-13T20:53:21+00:00",
        "comment_author": "compilade",
        "comment_body": "> The `update_conv()` and `update_ssm()` function have to be implemented only once in the `class llama_kv_cache_recurrent`.\r\n\r\nThese would be very specific to Mamba, and I don't think model-specific graph-building functions belong in `llama_kv_cache_recurrent`.\r\n\r\nAs described (`analogous to unified get_v() + cpy_v()`), `update_ssm()` would need tensors from `model.layers`, unless split into more functions (`update_conv()` doesn't have this problem).\r\n\r\nFrom my understanding, `update_ssm()` would fetch, modify, and write back the states for the ssm, but the modifying step involves `ggml_ssm_scan`. I might have misunderstood.\r\n\r\n> I hope I'm not missing something, but I think this should work and should be clean.\r\n\r\nThe `update_ssm()` and `do_something_with_ssm()` would be very much intertwined, especially since <https://github.com/ggml-org/llama.cpp/commit/2fa5f2ceb8b49bbd2835878ad5429ea74383566c> (in #9126) replaces `avoid_copies` with a passed function in `build_recurrent_state` to make it work properly:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/0b6f6becb4e916a24fcaf2966647381a21d1f084/src/llama-model.cpp#L9220-L9231\r\n\r\nwhich fixes the `FIXME` added in <https://github.com/ggml-org/llama.cpp/pull/13834/commits/62a9f34baefc657212dea8f1bad14d4fb1657da8> (see also <https://github.com/ggml-org/llama.cpp/pull/13834#issuecomment-2957652278>)\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/62a9f34baefc657212dea8f1bad14d4fb1657da8/src/llama-graph.cpp#L1447-L1457\r\n\r\n---\r\n\r\nWhat bothers me is that fundamentally, an hybrid state doesn't/shouldn't require re-implementing graph-building functions from the inner state types, since those inner state types are independent in principle.\r\n\r\n> Hmm, I may be wrong about the cast operator. I'll keep digging.\r\n\r\nI'm likely very wrong here, but this is possibly caused by how `mstate` in `llm_graph_context` is a `llama_memory_state_i` which doesn't have virtual type cast operators which means the type-cast operators of the child classes won't be used except maybe with a `dynamic_cast` (but that may be slow (or not?)).",
        "pr_file_module": null
      },
      {
        "comment_id": "2146114158",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13979,
        "pr_file": "src/llama-graph.h",
        "discussion_id": "2141701738",
        "commented_code": "@@ -508,7 +509,7 @@ struct llm_graph_context {\n     ggml_tensor * build_inp_out_ids() const;\n     ggml_tensor * build_inp_mean() const;\n     ggml_tensor * build_inp_cls() const;\n-    ggml_tensor * build_inp_s_copy() const;\n+    ggml_tensor * build_inp_s_copy(const llama_kv_cache_recurrent_state * kv_state = nullptr) const;",
        "comment_created_at": "2025-06-13T21:26:42+00:00",
        "comment_author": "gabe-l-hart",
        "comment_body": "Thanks for all the thoughts / ideas! At the moment, I'm trying to do the \"simple with lots of duplication\" version and I have it back to the [broken state](https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-2963750982) with Granite 4 that it's been in since the latest changes on the `mamba2` branch. I've been digging on that issue, so will try to resolve what I'm doing wrong there, then see about how much I can simplify the duplicate code on this branch (and the corresponding duplicate code on Granite 4).\r\n\r\n>What bothers me is that fundamentally, an hybrid state doesn't/shouldn't require re-implementing graph-building functions from the inner state types, since those inner state types are independent in principle.\r\n\r\nI think this is where my head is too. If the hybrid cache could simply _be_ both child types, it could \"just work\" everywhere the children do. I had originally tried to implement this with multiple (diamond) inheritance, but was running into the same issues I'm seeing with the same thing in the models where the common parent ends up getting double-initialized (I feel like I'm missing something important around `virtual` inheritance).\r\n\r\n>I'm likely wrong, but this is possibly caused by how mstate in llm_graph_context is a llama_memory_state_i which doesn't have virtual type cast operators which means the type-cast operators of the child classes won't be used except maybe with a dynamic_cast (but that may be slow (or not?)).\r\n\r\nI think this likely is the issue. I've been playing with a dummy version to try to understand what's going on:\r\n\r\n```c++\r\n// Example program\r\n#include <iostream>\r\n#include <string>\r\n\r\nstruct Base {\r\n    virtual void foo() const = 0;\r\n};\r\n\r\nstruct A : public Base {\r\n    int i;\r\n\r\n    A(int i) : i(i) {}\r\n\r\n    void foo() const override {\r\n        std::cout << \"A::foo(\" << i << \")\" << std::endl;\r\n    }\r\n};\r\n\r\nstruct B : public Base {\r\n    std::string s;\r\n    \r\n    B(std::string s) : s(s) {}\r\n\r\n    void foo() const override {\r\n        std::cout << \"B::foo(\" << s.c_str() << \")\" << std::endl;\r\n    }\r\n};\r\n\r\nstruct C : public Base {\r\n    C(bool be_a, int i, std::string s) : be_a(be_a), a(i), b(s) {}\r\n\r\n    void foo() const override {\r\n        std::cout << \"C::foo\" << std::endl;\r\n        if (be_a) a.foo();\r\n        else b.foo();\r\n    }\r\n\r\n    explicit operator const A*() const { return &a; }\r\n    explicit operator const B*() const { return &b; }\r\nprivate:\r\n    const bool be_a;\r\n    const A a;\r\n    const B b;\r\n};\r\n\r\nvoid use_a_as_base(const Base * b) {\r\n    // Doesn't use overloaded operators because the base\r\n    // doesn't have them as virtual\r\n    const A * a = static_cast<const A *>(b);\r\n    std::cout << \"Using a from Base: \" << a->i << std::endl;\r\n}\r\n\r\n// void use_a(const C * c) {\r\n//     // Doesn't compile because C doesn't inherit from A\r\n//     const A * a = static_cast<const A *>(c);\r\n//     std::cout << \"Using a: \" << a->i << std::endl;\r\n// }\r\n\r\nint main()\r\n{\r\n    C ca(true, 1, \"foo\");\r\n    C cb(false, 2, \"bar\");\r\n    ca.foo();\r\n    cb.foo();\r\n    \r\n    // Bad access of cb's non-existent `i`\r\n    use_a_as_base(&cb);\r\n}\r\n```\r\n\r\n```\r\nC::foo\r\nA::foo(1)\r\nC::foo\r\nB::foo(bar)\r\nUsing a from Base: 0\r\n```\r\n\r\n([link to sandbox](https://cpp.sh/?source=%2F%2F+Example+program%0A%23include+%3Ciostream%3E%0A%23include+%3Cstring%3E%0A%0Astruct+Base+%7B%0A++++virtual+void+foo()+const+%3D+0%3B%0A%7D%3B%0A%0Astruct+A+%3A+public+Base+%7B%0A++++int+i%3B%0A%0A++++A(int+i)+%3A+i(i)+%7B%7D%0A%0A++++void+foo()+const+override+%7B%0A++++++++std%3A%3Acout+%3C%3C+%22A%3A%3Afoo(%22+%3C%3C+i+%3C%3C+%22)%22+%3C%3C+std%3A%3Aendl%3B%0A++++%7D%0A%7D%3B%0A%0Astruct+B+%3A+public+Base+%7B%0A++++std%3A%3Astring+s%3B%0A++++%0A++++B(std%3A%3Astring+s)+%3A+s(s)+%7B%7D%0A%0A++++void+foo()+const+override+%7B%0A++++++++std%3A%3Acout+%3C%3C+%22B%3A%3Afoo(%22+%3C%3C+s.c_str()+%3C%3C+%22)%22+%3C%3C+std%3A%3Aendl%3B%0A++++%7D%0A%7D%3B%0A%0Astruct+C+%3A+public+Base+%7B%0A++++C(bool+be_a%2C+int+i%2C+std%3A%3Astring+s)+%3A+be_a(be_a)%2C+a(i)%2C+b(s)+%7B%7D%0A%0A++++void+foo()+const+override+%7B%0A++++++++std%3A%3Acout+%3C%3C+%22C%3A%3Afoo%22+%3C%3C+std%3A%3Aendl%3B%0A++++++++if+(be_a)+a.foo()%3B%0A++++++++else+b.foo()%3B%0A++++%7D%0A%0A++++explicit+operator+const+A*()+const+%7B+return+%26a%3B+%7D%0A++++explicit+operator+const+B*()+const+%7B+return+%26b%3B+%7D%0Aprivate%3A%0A++++const+bool+be_a%3B%0A++++const+A+a%3B%0A++++const+B+b%3B%0A%7D%3B%0A%0Avoid+use_a_as_base(const+Base+*+b)+%7B%0A++++%2F%2F+Doesn%27t+use+overloaded+operators+because+the+base%0A++++%2F%2F+doesn%27t+have+them+as+virtual%0A++++const+A+*+a+%3D+static_cast%3Cconst+A+*%3E(b)%3B%0A++++std%3A%3Acout+%3C%3C+%22Using+a+from+Base%3A+%22+%3C%3C+a-%3Ei+%3C%3C+std%3A%3Aendl%3B%0A%7D%0A%0A%2F%2F+void+use_a(const+C+*+c)+%7B%0A%2F%2F+++++%2F%2F+Doesn%27t+compile+because+C+doesn%27t+inherit+from+A%0A%2F%2F+++++const+A+*+a+%3D+static_cast%3Cconst+A+*%3E(c)%3B%0A%2F%2F+++++std%3A%3Acout+%3C%3C+%22Using+a%3A+%22+%3C%3C+a-%3Ei+%3C%3C+std%3A%3Aendl%3B%0A%2F%2F+%7D%0A%0Aint+main()%0A%7B%0A++++C+ca(true%2C+1%2C+%22foo%22)%3B%0A++++C+cb(false%2C+2%2C+%22bar%22)%3B%0A++++ca.foo()%3B%0A++++cb.foo()%3B%0A++++%0A++++%2F%2F+Bad+access+of+cb%27s+non-existent+%60i%60%0A++++use_a_as_base(%26cb)%3B%0A%7D))",
        "pr_file_module": null
      }
    ]
  }
]