[
  {
    "discussion_id": "2234042971",
    "pr_number": 14731,
    "pr_file": "tools/server/server.cpp",
    "created_at": "2025-07-27T16:03:14+00:00",
    "commented_code": "params.verbose           = params_base.verbosity > 9;\n         params.timings_per_token = json_value(data, \"timings_per_token\", false);\n \n-        params.stream           = json_value(data, \"stream\",             false);\n-        params.cache_prompt     = json_value(data, \"cache_prompt\",       true);\n-        params.return_tokens    = json_value(data, \"return_tokens\",      false);\n+        params.stream                  = json_value(data, \"stream\",                  false);\n+        params.cache_prompt            = json_value(data, \"cache_prompt\",            true);\n+        params.return_tokens           = json_value(data, \"return_tokens\",           false);\n+        params.include_prompt_progress = json_value(data, \"include_prompt_progress\", false);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2234042971",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14731,
        "pr_file": "tools/server/server.cpp",
        "discussion_id": "2234042971",
        "commented_code": "@@ -258,9 +259,10 @@ struct server_task {\n         params.verbose           = params_base.verbosity > 9;\n         params.timings_per_token = json_value(data, \"timings_per_token\", false);\n \n-        params.stream           = json_value(data, \"stream\",             false);\n-        params.cache_prompt     = json_value(data, \"cache_prompt\",       true);\n-        params.return_tokens    = json_value(data, \"return_tokens\",      false);\n+        params.stream                  = json_value(data, \"stream\",                  false);\n+        params.cache_prompt            = json_value(data, \"cache_prompt\",            true);\n+        params.return_tokens           = json_value(data, \"return_tokens\",           false);\n+        params.include_prompt_progress = json_value(data, \"include_prompt_progress\", false);",
        "comment_created_at": "2025-07-27T16:03:14+00:00",
        "comment_author": "ggerganov",
        "comment_body": "```suggestion\r\n        params.send_progress = json_value(data, \"send_progress\", false);\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2234045880",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14731,
        "pr_file": "tools/server/server.cpp",
        "discussion_id": "2234042971",
        "commented_code": "@@ -258,9 +259,10 @@ struct server_task {\n         params.verbose           = params_base.verbosity > 9;\n         params.timings_per_token = json_value(data, \"timings_per_token\", false);\n \n-        params.stream           = json_value(data, \"stream\",             false);\n-        params.cache_prompt     = json_value(data, \"cache_prompt\",       true);\n-        params.return_tokens    = json_value(data, \"return_tokens\",      false);\n+        params.stream                  = json_value(data, \"stream\",                  false);\n+        params.cache_prompt            = json_value(data, \"cache_prompt\",            true);\n+        params.return_tokens           = json_value(data, \"return_tokens\",           false);\n+        params.include_prompt_progress = json_value(data, \"include_prompt_progress\", false);",
        "comment_created_at": "2025-07-27T16:13:42+00:00",
        "comment_author": "BradHutchings",
        "comment_body": "Thanks, Georgi! And for all you do with llama.cpp. Nobody says thank you enough!",
        "pr_file_module": null
      },
      {
        "comment_id": "2234139267",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14731,
        "pr_file": "tools/server/server.cpp",
        "discussion_id": "2234042971",
        "commented_code": "@@ -258,9 +259,10 @@ struct server_task {\n         params.verbose           = params_base.verbosity > 9;\n         params.timings_per_token = json_value(data, \"timings_per_token\", false);\n \n-        params.stream           = json_value(data, \"stream\",             false);\n-        params.cache_prompt     = json_value(data, \"cache_prompt\",       true);\n-        params.return_tokens    = json_value(data, \"return_tokens\",      false);\n+        params.stream                  = json_value(data, \"stream\",                  false);\n+        params.cache_prompt            = json_value(data, \"cache_prompt\",            true);\n+        params.return_tokens           = json_value(data, \"return_tokens\",           false);\n+        params.include_prompt_progress = json_value(data, \"include_prompt_progress\", false);",
        "comment_created_at": "2025-07-27T21:27:26+00:00",
        "comment_author": "ngxson",
        "comment_body": "earlier in the code, we already had a param called `return_tokens` so maybe `return_progress` is a better naming",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188041055",
    "pr_number": 14521,
    "pr_file": "ggml/src/ggml-webgpu/ggml-webgpu.cpp",
    "created_at": "2025-07-06T05:54:04+00:00",
    "commented_code": "+#include \"ggml-webgpu.h\"\n+\n+#include <webgpu/webgpu_cpp.h>\n+\n+#include \"ggml-impl.h\"\n+#include \"ggml-backend-impl.h\"\n+\n+#include \"ggml-wgsl-shaders.hpp\"\n+\n+#include <iostream>\n+#include <vector>\n+\n+#ifdef GGML_WEBGPU_DEBUG\n+#define WEBGPU_LOG_DEBUG(msg) std::cout << msg << std::endl\n+#else\n+#define WEBGPU_LOG_DEBUG(msg) ((void) 0)\n+#endif // GGML_WEBGPU_DEBUG\n+\n+/* Constants */\n+\n+#define WEBGPU_MUL_MAT_WG_SIZE 64\n+#define WEBGPU_MUL_MAT_PARAMS_SIZE (13 * sizeof(uint32_t)) // M, N, K, batch sizes, broadcasts\n+#define WEBGPU_CPY_PARAMS_SIZE (15 * sizeof(uint32_t)) // strides and offsets\n+#define WEBGPU_STORAGE_BUF_BINDING_MULT 4 // a storage buffer binding size must be a multiple of 4\n+\n+/* End Constants */\n+\n+// This is a \"fake\" base pointer, since WebGPU buffers do not have pointers to their locations.\n+static void * const webgpu_ptr_base = (void *)(uintptr_t) 0x1000;  // NOLINT\n+\n+// Always returns the base offset of a tensor, regardless of views.\n+static uint64_t webgpu_tensor_offset(const ggml_tensor * tensor) {\n+    if (tensor->view_src) {\n+        return (uint8_t *) tensor->view_src->data - (uint8_t *) webgpu_ptr_base;\n+    }\n+    return (uint8_t *) tensor->data - (uint8_t *) webgpu_ptr_base;\n+}\n+\n+/* Struct definitions */\n+\n+// All the base objects needed to run operations on a WebGPU device\n+struct webgpu_context_struct {\n+    wgpu::Instance instance;\n+    wgpu::Adapter adapter;\n+    wgpu::Device device;\n+    wgpu::Queue queue;\n+    wgpu::Limits limits;\n+    wgpu::SupportedFeatures features;\n+\n+    std::mutex mutex;\n+    bool device_initialized = false;\n+\n+    // pipelines and parameter buffers\n+    // TODO: reuse params buffers for different pipelines when possible\n+    wgpu::ComputePipeline memset_pipeline;\n+    wgpu::Buffer memset_params_dev_buf;\n+    wgpu::Buffer memset_params_host_buf;\n+    wgpu::ComputePipeline mul_mat_pipeline;\n+    wgpu::Buffer mul_mat_params_dev_buf;\n+    wgpu::Buffer mul_mat_params_host_buf;\n+    wgpu::ComputePipeline cpy_pipeline;\n+    wgpu::Buffer cpy_params_dev_buf;\n+    wgpu::Buffer cpy_params_host_buf;\n+\n+    size_t memset_bytes_per_thread;\n+\n+    // Staging buffer for reading data from the GPU\n+    wgpu::Buffer get_tensor_staging_buf;\n+};\n+\n+typedef std::shared_ptr<webgpu_context_struct> webgpu_context;\n+\n+struct ggml_backend_webgpu_reg_context {\n+    webgpu_context webgpu_ctx;\n+\n+    size_t device_count;\n+    const char * name;\n+};\n+\n+struct ggml_backend_webgpu_device_context {\n+    webgpu_context webgpu_ctx;\n+\n+    std::string device_name;\n+    std::string device_desc;\n+};\n+\n+struct ggml_backend_webgpu_context {\n+    webgpu_context webgpu_ctx;\n+\n+    std::string name;\n+};\n+\n+struct ggml_backend_webgpu_buffer_context {\n+    webgpu_context webgpu_ctx;\n+\n+    wgpu::Buffer buffer;\n+\n+    ggml_backend_webgpu_buffer_context(webgpu_context ctx, wgpu::Buffer buf) :\n+        webgpu_ctx(ctx), buffer(buf) {\n+    }\n+};\n+\n+/* End struct definitions */\n+\n+/* WebGPU object initializations */\n+\n+static void ggml_webgpu_create_pipeline(wgpu::Device &device, wgpu::ComputePipeline &pipeline, const char * shader_code, const char * label, const std::vector<wgpu::ConstantEntry> &constants = {}) {\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_create_pipeline()\");\n+    wgpu::ShaderSourceWGSL shader_source;\n+    shader_source.code = shader_code;\n+    wgpu::ShaderModuleDescriptor shader_desc;\n+    shader_desc.nextInChain = &shader_source;\n+    wgpu::ShaderModule shader_module = device.CreateShaderModule(&shader_desc);\n+\n+    wgpu::ComputePipelineDescriptor pipeline_desc;\n+    pipeline_desc.label = label;\n+    pipeline_desc.compute.module = shader_module;\n+    pipeline_desc.compute.entryPoint = \"main\"; // Entry point in the WGSL code\n+    pipeline_desc.layout = nullptr; // nullptr means auto layout\n+    if (constants.size() > 0) {\n+        pipeline_desc.compute.constants = constants.data();\n+        pipeline_desc.compute.constantCount = constants.size();\n+    }\n+    pipeline = device.CreateComputePipeline(&pipeline_desc);\n+}\n+\n+static void ggml_webgpu_create_buffer(wgpu::Device &device, wgpu::Buffer &buffer, size_t size, wgpu::BufferUsage usage, const char* label) {\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_create_buffer()\");\n+\n+    wgpu::BufferDescriptor buffer_desc;\n+    buffer_desc.size = size;\n+    buffer_desc.usage = usage;\n+    buffer_desc.label = label;\n+    buffer_desc.mappedAtCreation = false; \n+    // TODO: error handling\n+    buffer = device.CreateBuffer(&buffer_desc);\n+}\n+\n+/** End WebGPU object initializations */\n+\n+/** WebGPU Actions */\n+\n+static void ggml_backend_webgpu_map_buffer(webgpu_context ctx, wgpu::Buffer buffer, wgpu::MapMode mode, size_t offset, size_t size) {\n+    ctx->instance.WaitAny(buffer.MapAsync(\n+        mode, offset, size, wgpu::CallbackMode::WaitAnyOnly,\n+        [](wgpu::MapAsyncStatus status, wgpu::StringView message) {\n+            if (status != wgpu::MapAsyncStatus::Success) {\n+                GGML_LOG_ERROR(\"ggml_webgpu: Failed to map buffer: %s\n\", message.data);\n+            }\n+        }),\n+        UINT64_MAX\n+    );\n+}\n+\n+static void ggml_backend_webgpu_buffer_memset(webgpu_context ctx, wgpu::Buffer buf, uint32_t value, size_t offset, size_t size) {\n+    std::lock_guard<std::mutex> lock(ctx->mutex);\n+    wgpu::Device device = ctx->device;\n+\n+    // map the host parameters buffer\n+    ggml_backend_webgpu_map_buffer(ctx, ctx->memset_params_host_buf, wgpu::MapMode::Write, 0, ctx->memset_params_host_buf.GetSize());\n+    uint32_t * params = (uint32_t *) ctx->memset_params_host_buf.GetMappedRange();\n+\n+    params[0] = (uint32_t)offset;\n+    params[1] = (uint32_t)size; \n+    params[2] = value;\n+    ctx->memset_params_host_buf.Unmap();\n+\n+    wgpu::BindGroupEntry entries[2];\n+    entries[0].binding = 0; // binding for the buffer to memset\n+    entries[0].buffer = buf;\n+    entries[0].offset = 0;\n+    entries[0].size = buf.GetSize();\n+    entries[1].binding = 1; // binding for the parameters\n+    entries[1].buffer = ctx->memset_params_dev_buf;\n+    entries[1].offset = 0;\n+    entries[1].size = ctx->memset_params_dev_buf.GetSize();\n+\n+    wgpu::BindGroupDescriptor bind_group_desc;\n+    bind_group_desc.layout = ctx->memset_pipeline.GetBindGroupLayout(0);\n+    bind_group_desc.entryCount = 2;\n+    bind_group_desc.label = \"ggml_memset\";\n+    bind_group_desc.entries = entries;\n+    wgpu::BindGroup bind_group = device.CreateBindGroup(&bind_group_desc);\n+\n+    wgpu::CommandEncoder encoder = device.CreateCommandEncoder();\n+    encoder.CopyBufferToBuffer(\n+        ctx->memset_params_host_buf, 0, \n+        ctx->memset_params_dev_buf, 0, \n+        ctx->memset_params_dev_buf.GetSize()\n+    );\n+    wgpu::ComputePassEncoder pass = encoder.BeginComputePass();\n+    pass.SetPipeline(ctx->memset_pipeline);\n+    pass.SetBindGroup(0, bind_group);\n+    size_t bytes_per_wg = ctx->limits.maxComputeWorkgroupSizeX * ctx->memset_bytes_per_thread;\n+    pass.DispatchWorkgroups(((size + 3) + bytes_per_wg - 1) / bytes_per_wg, 1, 1);\n+    pass.End();\n+    wgpu::CommandBuffer commands = encoder.Finish();\n+\n+    ctx->queue.Submit(1, &commands);\n+}\n+\n+static void ggml_backend_webgpu_wait_on_submission(webgpu_context ctx) {\n+    // Wait for the queue to finish processing all commands\n+    ctx->instance.WaitAny(ctx->queue.OnSubmittedWorkDone(wgpu::CallbackMode::WaitAnyOnly,\n+        [](wgpu::QueueWorkDoneStatus status, wgpu::StringView message) {\n+            if (status != wgpu::QueueWorkDoneStatus::Success) {\n+                GGML_LOG_ERROR(\"ggml_webgpu: Failed to wait on queue: %s\n\", message.data);\n+            }\n+        }), \n+        UINT64_MAX\n+    );\n+}\n+\n+/** End WebGPU Actions */\n+\n+/** GGML Backend Interface */\n+\n+static const char * ggml_backend_webgpu_name(ggml_backend_t backend) {\n+    ggml_backend_webgpu_context * ctx = (ggml_backend_webgpu_context *)backend->context;\n+    return ctx->name.c_str();\n+}\n+\n+static void ggml_backend_webgpu_free(ggml_backend_t backend) {\n+    ggml_backend_webgpu_context * ctx = (ggml_backend_webgpu_context *)backend->context;\n+    WEBGPU_LOG_DEBUG(\"ggml_backend_webgpu_free(\" << ctx->name << \")\");\n+\n+    // TODO: cleanup\n+    GGML_UNUSED(ctx);\n+}\n+\n+// Returns true if node has enqueued work into the queue, false otherwise\n+static bool ggml_webgpu_encode_node(webgpu_context ctx, ggml_tensor * node){\n+    if (ggml_is_empty(node)) {\n+        return false;\n+    }\n+\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_encode_node(\" << node << \", \" << ggml_op_name(node->op) << \")\");\n+\n+\n+    switch (node->op) {\n+        // no-ops\n+        case GGML_OP_NONE:\n+        case GGML_OP_VIEW:\n+        case GGML_OP_PERMUTE:\n+            return false;\n+        \n+        case GGML_OP_CPY: {\n+            std::lock_guard<std::mutex> lock(ctx->mutex);\n+            const ggml_tensor * src = node->src[0];\n+            ggml_backend_webgpu_buffer_context * src_ctx = (ggml_backend_webgpu_buffer_context *) src->buffer->context;\n+            size_t src_offset = webgpu_tensor_offset(src) + src->view_offs;\n+            // assumes power of 2 offset alignment\n+            size_t src_misalignment = src_offset & (ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            // align to minimum offset alignment\n+            src_offset &= ~(ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            ggml_backend_webgpu_buffer_context * dst_ctx = (ggml_backend_webgpu_buffer_context *) node->buffer->context;\n+            size_t dst_offset = webgpu_tensor_offset(node) + node->view_offs;\n+            size_t dst_misalignment = dst_offset & (ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            dst_offset &= ~(ctx->limits.minStorageBufferOffsetAlignment - 1);\n+\n+            wgpu::Device device = ctx->device;\n+            ggml_backend_webgpu_map_buffer(ctx, ctx->cpy_params_host_buf, \n+                wgpu::MapMode::Write, 0, ctx->cpy_params_host_buf.GetSize());\n+            uint32_t * params = (uint32_t *) ctx->cpy_params_host_buf.GetMappedRange();\n+            uint32_t ne = (uint32_t)ggml_nelements(node);\n+            params[0] = ne;\n+            params[1] = src_misalignment;\n+            params[2] = dst_misalignment;\n+\n+            // Convert byte-strides to element-strides\n+            params[3] = (uint32_t)src->nb[0]/ggml_type_size(src->type);\n+            params[4] = (uint32_t)src->nb[1]/ggml_type_size(src->type);\n+            params[5] = (uint32_t)src->nb[2]/ggml_type_size(src->type);\n+            params[6] = (uint32_t)src->nb[3]/ggml_type_size(src->type);\n+            params[7] = (uint32_t)node->nb[0]/ggml_type_size(node->type);\n+            params[8] = (uint32_t)node->nb[1]/ggml_type_size(node->type);\n+            params[9] = (uint32_t)node->nb[2]/ggml_type_size(node->type);\n+            params[10] = (uint32_t)node->nb[3]/ggml_type_size(node->type);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2188041055",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14521,
        "pr_file": "ggml/src/ggml-webgpu/ggml-webgpu.cpp",
        "discussion_id": "2188041055",
        "commented_code": "@@ -0,0 +1,904 @@\n+#include \"ggml-webgpu.h\"\n+\n+#include <webgpu/webgpu_cpp.h>\n+\n+#include \"ggml-impl.h\"\n+#include \"ggml-backend-impl.h\"\n+\n+#include \"ggml-wgsl-shaders.hpp\"\n+\n+#include <iostream>\n+#include <vector>\n+\n+#ifdef GGML_WEBGPU_DEBUG\n+#define WEBGPU_LOG_DEBUG(msg) std::cout << msg << std::endl\n+#else\n+#define WEBGPU_LOG_DEBUG(msg) ((void) 0)\n+#endif // GGML_WEBGPU_DEBUG\n+\n+/* Constants */\n+\n+#define WEBGPU_MUL_MAT_WG_SIZE 64\n+#define WEBGPU_MUL_MAT_PARAMS_SIZE (13 * sizeof(uint32_t)) // M, N, K, batch sizes, broadcasts\n+#define WEBGPU_CPY_PARAMS_SIZE (15 * sizeof(uint32_t)) // strides and offsets\n+#define WEBGPU_STORAGE_BUF_BINDING_MULT 4 // a storage buffer binding size must be a multiple of 4\n+\n+/* End Constants */\n+\n+// This is a \"fake\" base pointer, since WebGPU buffers do not have pointers to their locations.\n+static void * const webgpu_ptr_base = (void *)(uintptr_t) 0x1000;  // NOLINT\n+\n+// Always returns the base offset of a tensor, regardless of views.\n+static uint64_t webgpu_tensor_offset(const ggml_tensor * tensor) {\n+    if (tensor->view_src) {\n+        return (uint8_t *) tensor->view_src->data - (uint8_t *) webgpu_ptr_base;\n+    }\n+    return (uint8_t *) tensor->data - (uint8_t *) webgpu_ptr_base;\n+}\n+\n+/* Struct definitions */\n+\n+// All the base objects needed to run operations on a WebGPU device\n+struct webgpu_context_struct {\n+    wgpu::Instance instance;\n+    wgpu::Adapter adapter;\n+    wgpu::Device device;\n+    wgpu::Queue queue;\n+    wgpu::Limits limits;\n+    wgpu::SupportedFeatures features;\n+\n+    std::mutex mutex;\n+    bool device_initialized = false;\n+\n+    // pipelines and parameter buffers\n+    // TODO: reuse params buffers for different pipelines when possible\n+    wgpu::ComputePipeline memset_pipeline;\n+    wgpu::Buffer memset_params_dev_buf;\n+    wgpu::Buffer memset_params_host_buf;\n+    wgpu::ComputePipeline mul_mat_pipeline;\n+    wgpu::Buffer mul_mat_params_dev_buf;\n+    wgpu::Buffer mul_mat_params_host_buf;\n+    wgpu::ComputePipeline cpy_pipeline;\n+    wgpu::Buffer cpy_params_dev_buf;\n+    wgpu::Buffer cpy_params_host_buf;\n+\n+    size_t memset_bytes_per_thread;\n+\n+    // Staging buffer for reading data from the GPU\n+    wgpu::Buffer get_tensor_staging_buf;\n+};\n+\n+typedef std::shared_ptr<webgpu_context_struct> webgpu_context;\n+\n+struct ggml_backend_webgpu_reg_context {\n+    webgpu_context webgpu_ctx;\n+\n+    size_t device_count;\n+    const char * name;\n+};\n+\n+struct ggml_backend_webgpu_device_context {\n+    webgpu_context webgpu_ctx;\n+\n+    std::string device_name;\n+    std::string device_desc;\n+};\n+\n+struct ggml_backend_webgpu_context {\n+    webgpu_context webgpu_ctx;\n+\n+    std::string name;\n+};\n+\n+struct ggml_backend_webgpu_buffer_context {\n+    webgpu_context webgpu_ctx;\n+\n+    wgpu::Buffer buffer;\n+\n+    ggml_backend_webgpu_buffer_context(webgpu_context ctx, wgpu::Buffer buf) :\n+        webgpu_ctx(ctx), buffer(buf) {\n+    }\n+};\n+\n+/* End struct definitions */\n+\n+/* WebGPU object initializations */\n+\n+static void ggml_webgpu_create_pipeline(wgpu::Device &device, wgpu::ComputePipeline &pipeline, const char * shader_code, const char * label, const std::vector<wgpu::ConstantEntry> &constants = {}) {\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_create_pipeline()\");\n+    wgpu::ShaderSourceWGSL shader_source;\n+    shader_source.code = shader_code;\n+    wgpu::ShaderModuleDescriptor shader_desc;\n+    shader_desc.nextInChain = &shader_source;\n+    wgpu::ShaderModule shader_module = device.CreateShaderModule(&shader_desc);\n+\n+    wgpu::ComputePipelineDescriptor pipeline_desc;\n+    pipeline_desc.label = label;\n+    pipeline_desc.compute.module = shader_module;\n+    pipeline_desc.compute.entryPoint = \"main\"; // Entry point in the WGSL code\n+    pipeline_desc.layout = nullptr; // nullptr means auto layout\n+    if (constants.size() > 0) {\n+        pipeline_desc.compute.constants = constants.data();\n+        pipeline_desc.compute.constantCount = constants.size();\n+    }\n+    pipeline = device.CreateComputePipeline(&pipeline_desc);\n+}\n+\n+static void ggml_webgpu_create_buffer(wgpu::Device &device, wgpu::Buffer &buffer, size_t size, wgpu::BufferUsage usage, const char* label) {\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_create_buffer()\");\n+\n+    wgpu::BufferDescriptor buffer_desc;\n+    buffer_desc.size = size;\n+    buffer_desc.usage = usage;\n+    buffer_desc.label = label;\n+    buffer_desc.mappedAtCreation = false; \n+    // TODO: error handling\n+    buffer = device.CreateBuffer(&buffer_desc);\n+}\n+\n+/** End WebGPU object initializations */\n+\n+/** WebGPU Actions */\n+\n+static void ggml_backend_webgpu_map_buffer(webgpu_context ctx, wgpu::Buffer buffer, wgpu::MapMode mode, size_t offset, size_t size) {\n+    ctx->instance.WaitAny(buffer.MapAsync(\n+        mode, offset, size, wgpu::CallbackMode::WaitAnyOnly,\n+        [](wgpu::MapAsyncStatus status, wgpu::StringView message) {\n+            if (status != wgpu::MapAsyncStatus::Success) {\n+                GGML_LOG_ERROR(\"ggml_webgpu: Failed to map buffer: %s\\n\", message.data);\n+            }\n+        }),\n+        UINT64_MAX\n+    );\n+}\n+\n+static void ggml_backend_webgpu_buffer_memset(webgpu_context ctx, wgpu::Buffer buf, uint32_t value, size_t offset, size_t size) {\n+    std::lock_guard<std::mutex> lock(ctx->mutex);\n+    wgpu::Device device = ctx->device;\n+\n+    // map the host parameters buffer\n+    ggml_backend_webgpu_map_buffer(ctx, ctx->memset_params_host_buf, wgpu::MapMode::Write, 0, ctx->memset_params_host_buf.GetSize());\n+    uint32_t * params = (uint32_t *) ctx->memset_params_host_buf.GetMappedRange();\n+\n+    params[0] = (uint32_t)offset;\n+    params[1] = (uint32_t)size; \n+    params[2] = value;\n+    ctx->memset_params_host_buf.Unmap();\n+\n+    wgpu::BindGroupEntry entries[2];\n+    entries[0].binding = 0; // binding for the buffer to memset\n+    entries[0].buffer = buf;\n+    entries[0].offset = 0;\n+    entries[0].size = buf.GetSize();\n+    entries[1].binding = 1; // binding for the parameters\n+    entries[1].buffer = ctx->memset_params_dev_buf;\n+    entries[1].offset = 0;\n+    entries[1].size = ctx->memset_params_dev_buf.GetSize();\n+\n+    wgpu::BindGroupDescriptor bind_group_desc;\n+    bind_group_desc.layout = ctx->memset_pipeline.GetBindGroupLayout(0);\n+    bind_group_desc.entryCount = 2;\n+    bind_group_desc.label = \"ggml_memset\";\n+    bind_group_desc.entries = entries;\n+    wgpu::BindGroup bind_group = device.CreateBindGroup(&bind_group_desc);\n+\n+    wgpu::CommandEncoder encoder = device.CreateCommandEncoder();\n+    encoder.CopyBufferToBuffer(\n+        ctx->memset_params_host_buf, 0, \n+        ctx->memset_params_dev_buf, 0, \n+        ctx->memset_params_dev_buf.GetSize()\n+    );\n+    wgpu::ComputePassEncoder pass = encoder.BeginComputePass();\n+    pass.SetPipeline(ctx->memset_pipeline);\n+    pass.SetBindGroup(0, bind_group);\n+    size_t bytes_per_wg = ctx->limits.maxComputeWorkgroupSizeX * ctx->memset_bytes_per_thread;\n+    pass.DispatchWorkgroups(((size + 3) + bytes_per_wg - 1) / bytes_per_wg, 1, 1);\n+    pass.End();\n+    wgpu::CommandBuffer commands = encoder.Finish();\n+\n+    ctx->queue.Submit(1, &commands);\n+}\n+\n+static void ggml_backend_webgpu_wait_on_submission(webgpu_context ctx) {\n+    // Wait for the queue to finish processing all commands\n+    ctx->instance.WaitAny(ctx->queue.OnSubmittedWorkDone(wgpu::CallbackMode::WaitAnyOnly,\n+        [](wgpu::QueueWorkDoneStatus status, wgpu::StringView message) {\n+            if (status != wgpu::QueueWorkDoneStatus::Success) {\n+                GGML_LOG_ERROR(\"ggml_webgpu: Failed to wait on queue: %s\\n\", message.data);\n+            }\n+        }), \n+        UINT64_MAX\n+    );\n+}\n+\n+/** End WebGPU Actions */\n+\n+/** GGML Backend Interface */\n+\n+static const char * ggml_backend_webgpu_name(ggml_backend_t backend) {\n+    ggml_backend_webgpu_context * ctx = (ggml_backend_webgpu_context *)backend->context;\n+    return ctx->name.c_str();\n+}\n+\n+static void ggml_backend_webgpu_free(ggml_backend_t backend) {\n+    ggml_backend_webgpu_context * ctx = (ggml_backend_webgpu_context *)backend->context;\n+    WEBGPU_LOG_DEBUG(\"ggml_backend_webgpu_free(\" << ctx->name << \")\");\n+\n+    // TODO: cleanup\n+    GGML_UNUSED(ctx);\n+}\n+\n+// Returns true if node has enqueued work into the queue, false otherwise\n+static bool ggml_webgpu_encode_node(webgpu_context ctx, ggml_tensor * node){\n+    if (ggml_is_empty(node)) {\n+        return false;\n+    }\n+\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_encode_node(\" << node << \", \" << ggml_op_name(node->op) << \")\");\n+\n+\n+    switch (node->op) {\n+        // no-ops\n+        case GGML_OP_NONE:\n+        case GGML_OP_VIEW:\n+        case GGML_OP_PERMUTE:\n+            return false;\n+        \n+        case GGML_OP_CPY: {\n+            std::lock_guard<std::mutex> lock(ctx->mutex);\n+            const ggml_tensor * src = node->src[0];\n+            ggml_backend_webgpu_buffer_context * src_ctx = (ggml_backend_webgpu_buffer_context *) src->buffer->context;\n+            size_t src_offset = webgpu_tensor_offset(src) + src->view_offs;\n+            // assumes power of 2 offset alignment\n+            size_t src_misalignment = src_offset & (ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            // align to minimum offset alignment\n+            src_offset &= ~(ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            ggml_backend_webgpu_buffer_context * dst_ctx = (ggml_backend_webgpu_buffer_context *) node->buffer->context;\n+            size_t dst_offset = webgpu_tensor_offset(node) + node->view_offs;\n+            size_t dst_misalignment = dst_offset & (ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            dst_offset &= ~(ctx->limits.minStorageBufferOffsetAlignment - 1);\n+\n+            wgpu::Device device = ctx->device;\n+            ggml_backend_webgpu_map_buffer(ctx, ctx->cpy_params_host_buf, \n+                wgpu::MapMode::Write, 0, ctx->cpy_params_host_buf.GetSize());\n+            uint32_t * params = (uint32_t *) ctx->cpy_params_host_buf.GetMappedRange();\n+            uint32_t ne = (uint32_t)ggml_nelements(node);\n+            params[0] = ne;\n+            params[1] = src_misalignment;\n+            params[2] = dst_misalignment;\n+\n+            // Convert byte-strides to element-strides\n+            params[3] = (uint32_t)src->nb[0]/ggml_type_size(src->type);\n+            params[4] = (uint32_t)src->nb[1]/ggml_type_size(src->type);\n+            params[5] = (uint32_t)src->nb[2]/ggml_type_size(src->type);\n+            params[6] = (uint32_t)src->nb[3]/ggml_type_size(src->type);\n+            params[7] = (uint32_t)node->nb[0]/ggml_type_size(node->type);\n+            params[8] = (uint32_t)node->nb[1]/ggml_type_size(node->type);\n+            params[9] = (uint32_t)node->nb[2]/ggml_type_size(node->type);\n+            params[10] = (uint32_t)node->nb[3]/ggml_type_size(node->type);",
        "comment_created_at": "2025-07-06T05:54:04+00:00",
        "comment_author": "ggerganov",
        "comment_body": "For consistency, all of these should either be byte offsets or element offsets, but not both.",
        "pr_file_module": null
      },
      {
        "comment_id": "2188415208",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14521,
        "pr_file": "ggml/src/ggml-webgpu/ggml-webgpu.cpp",
        "discussion_id": "2188041055",
        "commented_code": "@@ -0,0 +1,904 @@\n+#include \"ggml-webgpu.h\"\n+\n+#include <webgpu/webgpu_cpp.h>\n+\n+#include \"ggml-impl.h\"\n+#include \"ggml-backend-impl.h\"\n+\n+#include \"ggml-wgsl-shaders.hpp\"\n+\n+#include <iostream>\n+#include <vector>\n+\n+#ifdef GGML_WEBGPU_DEBUG\n+#define WEBGPU_LOG_DEBUG(msg) std::cout << msg << std::endl\n+#else\n+#define WEBGPU_LOG_DEBUG(msg) ((void) 0)\n+#endif // GGML_WEBGPU_DEBUG\n+\n+/* Constants */\n+\n+#define WEBGPU_MUL_MAT_WG_SIZE 64\n+#define WEBGPU_MUL_MAT_PARAMS_SIZE (13 * sizeof(uint32_t)) // M, N, K, batch sizes, broadcasts\n+#define WEBGPU_CPY_PARAMS_SIZE (15 * sizeof(uint32_t)) // strides and offsets\n+#define WEBGPU_STORAGE_BUF_BINDING_MULT 4 // a storage buffer binding size must be a multiple of 4\n+\n+/* End Constants */\n+\n+// This is a \"fake\" base pointer, since WebGPU buffers do not have pointers to their locations.\n+static void * const webgpu_ptr_base = (void *)(uintptr_t) 0x1000;  // NOLINT\n+\n+// Always returns the base offset of a tensor, regardless of views.\n+static uint64_t webgpu_tensor_offset(const ggml_tensor * tensor) {\n+    if (tensor->view_src) {\n+        return (uint8_t *) tensor->view_src->data - (uint8_t *) webgpu_ptr_base;\n+    }\n+    return (uint8_t *) tensor->data - (uint8_t *) webgpu_ptr_base;\n+}\n+\n+/* Struct definitions */\n+\n+// All the base objects needed to run operations on a WebGPU device\n+struct webgpu_context_struct {\n+    wgpu::Instance instance;\n+    wgpu::Adapter adapter;\n+    wgpu::Device device;\n+    wgpu::Queue queue;\n+    wgpu::Limits limits;\n+    wgpu::SupportedFeatures features;\n+\n+    std::mutex mutex;\n+    bool device_initialized = false;\n+\n+    // pipelines and parameter buffers\n+    // TODO: reuse params buffers for different pipelines when possible\n+    wgpu::ComputePipeline memset_pipeline;\n+    wgpu::Buffer memset_params_dev_buf;\n+    wgpu::Buffer memset_params_host_buf;\n+    wgpu::ComputePipeline mul_mat_pipeline;\n+    wgpu::Buffer mul_mat_params_dev_buf;\n+    wgpu::Buffer mul_mat_params_host_buf;\n+    wgpu::ComputePipeline cpy_pipeline;\n+    wgpu::Buffer cpy_params_dev_buf;\n+    wgpu::Buffer cpy_params_host_buf;\n+\n+    size_t memset_bytes_per_thread;\n+\n+    // Staging buffer for reading data from the GPU\n+    wgpu::Buffer get_tensor_staging_buf;\n+};\n+\n+typedef std::shared_ptr<webgpu_context_struct> webgpu_context;\n+\n+struct ggml_backend_webgpu_reg_context {\n+    webgpu_context webgpu_ctx;\n+\n+    size_t device_count;\n+    const char * name;\n+};\n+\n+struct ggml_backend_webgpu_device_context {\n+    webgpu_context webgpu_ctx;\n+\n+    std::string device_name;\n+    std::string device_desc;\n+};\n+\n+struct ggml_backend_webgpu_context {\n+    webgpu_context webgpu_ctx;\n+\n+    std::string name;\n+};\n+\n+struct ggml_backend_webgpu_buffer_context {\n+    webgpu_context webgpu_ctx;\n+\n+    wgpu::Buffer buffer;\n+\n+    ggml_backend_webgpu_buffer_context(webgpu_context ctx, wgpu::Buffer buf) :\n+        webgpu_ctx(ctx), buffer(buf) {\n+    }\n+};\n+\n+/* End struct definitions */\n+\n+/* WebGPU object initializations */\n+\n+static void ggml_webgpu_create_pipeline(wgpu::Device &device, wgpu::ComputePipeline &pipeline, const char * shader_code, const char * label, const std::vector<wgpu::ConstantEntry> &constants = {}) {\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_create_pipeline()\");\n+    wgpu::ShaderSourceWGSL shader_source;\n+    shader_source.code = shader_code;\n+    wgpu::ShaderModuleDescriptor shader_desc;\n+    shader_desc.nextInChain = &shader_source;\n+    wgpu::ShaderModule shader_module = device.CreateShaderModule(&shader_desc);\n+\n+    wgpu::ComputePipelineDescriptor pipeline_desc;\n+    pipeline_desc.label = label;\n+    pipeline_desc.compute.module = shader_module;\n+    pipeline_desc.compute.entryPoint = \"main\"; // Entry point in the WGSL code\n+    pipeline_desc.layout = nullptr; // nullptr means auto layout\n+    if (constants.size() > 0) {\n+        pipeline_desc.compute.constants = constants.data();\n+        pipeline_desc.compute.constantCount = constants.size();\n+    }\n+    pipeline = device.CreateComputePipeline(&pipeline_desc);\n+}\n+\n+static void ggml_webgpu_create_buffer(wgpu::Device &device, wgpu::Buffer &buffer, size_t size, wgpu::BufferUsage usage, const char* label) {\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_create_buffer()\");\n+\n+    wgpu::BufferDescriptor buffer_desc;\n+    buffer_desc.size = size;\n+    buffer_desc.usage = usage;\n+    buffer_desc.label = label;\n+    buffer_desc.mappedAtCreation = false; \n+    // TODO: error handling\n+    buffer = device.CreateBuffer(&buffer_desc);\n+}\n+\n+/** End WebGPU object initializations */\n+\n+/** WebGPU Actions */\n+\n+static void ggml_backend_webgpu_map_buffer(webgpu_context ctx, wgpu::Buffer buffer, wgpu::MapMode mode, size_t offset, size_t size) {\n+    ctx->instance.WaitAny(buffer.MapAsync(\n+        mode, offset, size, wgpu::CallbackMode::WaitAnyOnly,\n+        [](wgpu::MapAsyncStatus status, wgpu::StringView message) {\n+            if (status != wgpu::MapAsyncStatus::Success) {\n+                GGML_LOG_ERROR(\"ggml_webgpu: Failed to map buffer: %s\\n\", message.data);\n+            }\n+        }),\n+        UINT64_MAX\n+    );\n+}\n+\n+static void ggml_backend_webgpu_buffer_memset(webgpu_context ctx, wgpu::Buffer buf, uint32_t value, size_t offset, size_t size) {\n+    std::lock_guard<std::mutex> lock(ctx->mutex);\n+    wgpu::Device device = ctx->device;\n+\n+    // map the host parameters buffer\n+    ggml_backend_webgpu_map_buffer(ctx, ctx->memset_params_host_buf, wgpu::MapMode::Write, 0, ctx->memset_params_host_buf.GetSize());\n+    uint32_t * params = (uint32_t *) ctx->memset_params_host_buf.GetMappedRange();\n+\n+    params[0] = (uint32_t)offset;\n+    params[1] = (uint32_t)size; \n+    params[2] = value;\n+    ctx->memset_params_host_buf.Unmap();\n+\n+    wgpu::BindGroupEntry entries[2];\n+    entries[0].binding = 0; // binding for the buffer to memset\n+    entries[0].buffer = buf;\n+    entries[0].offset = 0;\n+    entries[0].size = buf.GetSize();\n+    entries[1].binding = 1; // binding for the parameters\n+    entries[1].buffer = ctx->memset_params_dev_buf;\n+    entries[1].offset = 0;\n+    entries[1].size = ctx->memset_params_dev_buf.GetSize();\n+\n+    wgpu::BindGroupDescriptor bind_group_desc;\n+    bind_group_desc.layout = ctx->memset_pipeline.GetBindGroupLayout(0);\n+    bind_group_desc.entryCount = 2;\n+    bind_group_desc.label = \"ggml_memset\";\n+    bind_group_desc.entries = entries;\n+    wgpu::BindGroup bind_group = device.CreateBindGroup(&bind_group_desc);\n+\n+    wgpu::CommandEncoder encoder = device.CreateCommandEncoder();\n+    encoder.CopyBufferToBuffer(\n+        ctx->memset_params_host_buf, 0, \n+        ctx->memset_params_dev_buf, 0, \n+        ctx->memset_params_dev_buf.GetSize()\n+    );\n+    wgpu::ComputePassEncoder pass = encoder.BeginComputePass();\n+    pass.SetPipeline(ctx->memset_pipeline);\n+    pass.SetBindGroup(0, bind_group);\n+    size_t bytes_per_wg = ctx->limits.maxComputeWorkgroupSizeX * ctx->memset_bytes_per_thread;\n+    pass.DispatchWorkgroups(((size + 3) + bytes_per_wg - 1) / bytes_per_wg, 1, 1);\n+    pass.End();\n+    wgpu::CommandBuffer commands = encoder.Finish();\n+\n+    ctx->queue.Submit(1, &commands);\n+}\n+\n+static void ggml_backend_webgpu_wait_on_submission(webgpu_context ctx) {\n+    // Wait for the queue to finish processing all commands\n+    ctx->instance.WaitAny(ctx->queue.OnSubmittedWorkDone(wgpu::CallbackMode::WaitAnyOnly,\n+        [](wgpu::QueueWorkDoneStatus status, wgpu::StringView message) {\n+            if (status != wgpu::QueueWorkDoneStatus::Success) {\n+                GGML_LOG_ERROR(\"ggml_webgpu: Failed to wait on queue: %s\\n\", message.data);\n+            }\n+        }), \n+        UINT64_MAX\n+    );\n+}\n+\n+/** End WebGPU Actions */\n+\n+/** GGML Backend Interface */\n+\n+static const char * ggml_backend_webgpu_name(ggml_backend_t backend) {\n+    ggml_backend_webgpu_context * ctx = (ggml_backend_webgpu_context *)backend->context;\n+    return ctx->name.c_str();\n+}\n+\n+static void ggml_backend_webgpu_free(ggml_backend_t backend) {\n+    ggml_backend_webgpu_context * ctx = (ggml_backend_webgpu_context *)backend->context;\n+    WEBGPU_LOG_DEBUG(\"ggml_backend_webgpu_free(\" << ctx->name << \")\");\n+\n+    // TODO: cleanup\n+    GGML_UNUSED(ctx);\n+}\n+\n+// Returns true if node has enqueued work into the queue, false otherwise\n+static bool ggml_webgpu_encode_node(webgpu_context ctx, ggml_tensor * node){\n+    if (ggml_is_empty(node)) {\n+        return false;\n+    }\n+\n+    WEBGPU_LOG_DEBUG(\"ggml_webgpu_encode_node(\" << node << \", \" << ggml_op_name(node->op) << \")\");\n+\n+\n+    switch (node->op) {\n+        // no-ops\n+        case GGML_OP_NONE:\n+        case GGML_OP_VIEW:\n+        case GGML_OP_PERMUTE:\n+            return false;\n+        \n+        case GGML_OP_CPY: {\n+            std::lock_guard<std::mutex> lock(ctx->mutex);\n+            const ggml_tensor * src = node->src[0];\n+            ggml_backend_webgpu_buffer_context * src_ctx = (ggml_backend_webgpu_buffer_context *) src->buffer->context;\n+            size_t src_offset = webgpu_tensor_offset(src) + src->view_offs;\n+            // assumes power of 2 offset alignment\n+            size_t src_misalignment = src_offset & (ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            // align to minimum offset alignment\n+            src_offset &= ~(ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            ggml_backend_webgpu_buffer_context * dst_ctx = (ggml_backend_webgpu_buffer_context *) node->buffer->context;\n+            size_t dst_offset = webgpu_tensor_offset(node) + node->view_offs;\n+            size_t dst_misalignment = dst_offset & (ctx->limits.minStorageBufferOffsetAlignment - 1);\n+            dst_offset &= ~(ctx->limits.minStorageBufferOffsetAlignment - 1);\n+\n+            wgpu::Device device = ctx->device;\n+            ggml_backend_webgpu_map_buffer(ctx, ctx->cpy_params_host_buf, \n+                wgpu::MapMode::Write, 0, ctx->cpy_params_host_buf.GetSize());\n+            uint32_t * params = (uint32_t *) ctx->cpy_params_host_buf.GetMappedRange();\n+            uint32_t ne = (uint32_t)ggml_nelements(node);\n+            params[0] = ne;\n+            params[1] = src_misalignment;\n+            params[2] = dst_misalignment;\n+\n+            // Convert byte-strides to element-strides\n+            params[3] = (uint32_t)src->nb[0]/ggml_type_size(src->type);\n+            params[4] = (uint32_t)src->nb[1]/ggml_type_size(src->type);\n+            params[5] = (uint32_t)src->nb[2]/ggml_type_size(src->type);\n+            params[6] = (uint32_t)src->nb[3]/ggml_type_size(src->type);\n+            params[7] = (uint32_t)node->nb[0]/ggml_type_size(node->type);\n+            params[8] = (uint32_t)node->nb[1]/ggml_type_size(node->type);\n+            params[9] = (uint32_t)node->nb[2]/ggml_type_size(node->type);\n+            params[10] = (uint32_t)node->nb[3]/ggml_type_size(node->type);",
        "comment_created_at": "2025-07-06T15:44:50+00:00",
        "comment_author": "reeselevine",
        "comment_body": "updated the alignments to be element offsets",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192017905",
    "pr_number": 14534,
    "pr_file": "src/llama-arch.cpp",
    "created_at": "2025-07-08T09:48:48+00:00",
    "commented_code": "{ LLM_ARCH_STARCODER2,       \"starcoder2\"       },\n     { LLM_ARCH_MAMBA,            \"mamba\"            },\n     { LLM_ARCH_MAMBA2,           \"mamba2\"           },\n+    { LLM_ARCH_FALCON_H1,        \"falcon_h1\"        },",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2192017905",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14534,
        "pr_file": "src/llama-arch.cpp",
        "discussion_id": "2192017905",
        "commented_code": "@@ -46,6 +46,7 @@ static const std::map<llm_arch, const char *> LLM_ARCH_NAMES = {\n     { LLM_ARCH_STARCODER2,       \"starcoder2\"       },\n     { LLM_ARCH_MAMBA,            \"mamba\"            },\n     { LLM_ARCH_MAMBA2,           \"mamba2\"           },\n+    { LLM_ARCH_FALCON_H1,        \"falcon_h1\"        },",
        "comment_created_at": "2025-07-08T09:48:48+00:00",
        "comment_author": "ggerganov",
        "comment_body": "For consistency with the other arch names, use a dash instead of underscore:\r\n\r\n```suggestion\r\n    { LLM_ARCH_FALCON_H1,        \"falcon-h1\"        },\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2166765981",
    "pr_number": 14368,
    "pr_file": "tests/test-backend-ops.cpp",
    "created_at": "2025-06-25T13:47:17+00:00",
    "commented_code": "MODE_GRAD,\n };\n \n+// Output format support similar to llama-bench\n+enum output_formats { CONSOLE, SQL };\n+\n+static const char * output_format_str(output_formats format) {\n+    switch (format) {\n+        case CONSOLE:\n+            return \"console\";\n+        case SQL:\n+            return \"sql\";\n+        default:\n+            GGML_ABORT(\"invalid output format\");\n+    }\n+}\n+\n+static bool output_format_from_str(const std::string & s, output_formats & format) {\n+    if (s == \"console\") {\n+        format = CONSOLE;\n+    } else if (s == \"sql\") {\n+        format = SQL;\n+    } else {\n+        return false;\n+    }\n+    return true;\n+}\n+\n+// Test result structure for SQL output\n+struct test_result {\n+    std::string test_time;\n+    std::string backend_name;\n+    std::string op_name;\n+    std::string op_params;\n+    std::string test_mode;\n+    bool supported;\n+    bool passed;\n+    std::string error_message;\n+    double time_us;\n+    double flops_per_sec;\n+    double bandwidth_gb_s;\n+    size_t memory_kb;\n+    int n_runs;\n+\n+    test_result() {\n+        // Initialize with default values\n+        time_us = 0.0;\n+        flops_per_sec = 0.0;\n+        bandwidth_gb_s = 0.0;\n+        memory_kb = 0;\n+        n_runs = 0;\n+        supported = false;\n+        passed = false;\n+\n+        // Set test time\n+        time_t t = time(NULL);\n+        char buf[32];\n+        std::strftime(buf, sizeof(buf), \"%FT%TZ\", gmtime(&t));\n+        test_time = buf;\n+    }\n+\n+    static const std::vector<std::string> & get_fields() {\n+        static const std::vector<std::string> fields = {\n+            \"test_time\", \"backend_name\", \"op_name\", \"op_params\", \"test_mode\",\n+            \"supported\", \"passed\", \"error_message\", \"time_us\", \"flops_per_sec\",\n+            \"bandwidth_gb_s\", \"memory_kb\", \"n_runs\"\n+        };\n+        return fields;\n+    }\n+\n+    enum field_type { STRING, BOOL, INT, FLOAT };\n+\n+    static field_type get_field_type(const std::string & field) {\n+        if (field == \"supported\" || field == \"passed\") {\n+            return BOOL;\n+        }\n+        if (field == \"memory_kb\" || field == \"n_runs\") {\n+            return INT;\n+        }\n+        if (field == \"time_us\" || field == \"flops_per_sec\" || field == \"bandwidth_gb_s\") {\n+            return FLOAT;\n+        }\n+        return STRING;\n+    }\n+\n+    std::vector<std::string> get_values() const {\n+        return {\n+            test_time,\n+            backend_name,\n+            op_name,\n+            op_params,\n+            test_mode,\n+            std::to_string(supported),\n+            std::to_string(passed),\n+            error_message,\n+            std::to_string(time_us),\n+            std::to_string(flops_per_sec),\n+            std::to_string(bandwidth_gb_s),\n+            std::to_string(memory_kb),\n+            std::to_string(n_runs)\n+        };\n+    }\n+};\n+\n+// Printer classes for different output formats\n+struct printer {\n+    virtual ~printer() {}\n+    FILE * fout = stdout;\n+    virtual void print_header() {}\n+    virtual void print_test_result(const test_result & result) = 0;\n+    virtual void print_footer() {}\n+\n+    // General purpose output methods\n+    virtual void print_info(const char * format, ...) = 0;\n+    virtual void print_error(const char * format, ...) = 0;\n+    virtual void print_device_info(const char * format, ...) = 0;\n+    virtual void print_test_summary(const char * format, ...) = 0;\n+    virtual void print_status_ok() = 0;\n+    virtual void print_status_fail() = 0;\n+};\n+\n+struct console_printer : public printer {\n+    void print_test_result(const test_result & result) override {\n+        if (result.test_mode == \"test\") {\n+            print_test_console(result);\n+        } else if (result.test_mode == \"perf\") {\n+            print_perf_console(result);\n+        }\n+    }\n+\n+    void print_info(const char * format, ...) override {\n+        va_list args;\n+        va_start(args, format);\n+        vprintf(format, args);\n+        va_end(args);\n+    }\n+\n+    void print_error(const char * format, ...) override {\n+        va_list args;\n+        va_start(args, format);\n+        vfprintf(stderr, format, args);\n+        va_end(args);\n+    }\n+\n+    void print_device_info(const char * format, ...) override {\n+        va_list args;\n+        va_start(args, format);\n+        vprintf(format, args);\n+        va_end(args);\n+    }\n+\n+    void print_test_summary(const char * format, ...) override {\n+        va_list args;\n+        va_start(args, format);\n+        vprintf(format, args);\n+        va_end(args);\n+    }\n+\n+    void print_status_ok() override {\n+        printf(\"\\033[1;32mOK\\033[0m\n\");\n+    }\n+\n+    void print_status_fail() override {\n+        printf(\"\\033[1;31mFAIL\\033[0m\n\");\n+    }\n+\n+private:\n+    void print_test_console(const test_result & result) {\n+        printf(\"  %s(%s): \", result.op_name.c_str(), result.op_params.c_str());\n+        fflush(stdout);\n+\n+        if (!result.supported) {\n+            printf(\"not supported [%s] \", result.backend_name.c_str());\n+            printf(\"\n\");\n+            return;\n+        }\n+\n+        if (result.passed) {\n+            printf(\"\\033[1;32mOK\\033[0m\n\");\n+        } else {\n+            printf(\"\\033[1;31mFAIL\\033[0m\n\");\n+        }\n+    }\n+\n+    void print_perf_console(const test_result & result) {\n+        int len = printf(\"  %s(%s): \", result.op_name.c_str(), result.op_params.c_str());\n+        fflush(stdout);\n+\n+        if (!result.supported) {\n+            printf(\"not supported\n\");\n+            return;\n+        }\n+\n+        // align while also leaving some margin for variations in parameters\n+        int align = 8;\n+        int last = (len + align - 1) / align * align;\n+        if (last - len < 5) {\n+            last += align;\n+        }\n+        printf(\"%*s\", last - len, \"\");\n+\n+        printf(\"    %8d runs - %8.2f us/run - \",\n+            result.n_runs,\n+            result.time_us);\n+\n+        if (result.flops_per_sec > 0) {\n+            auto format_flops = [](double flops) -> std::string {\n+                char buf[256];\n+                if (flops >= 1e12) {\n+                    snprintf(buf, sizeof(buf), \"%6.2f TFLOP\", flops / 1e12);\n+                } else if (flops >= 1e9) {\n+                    snprintf(buf, sizeof(buf), \"%6.2f GFLOP\", flops / 1e9);\n+                } else if (flops >= 1e6) {\n+                    snprintf(buf, sizeof(buf), \"%6.2f MFLOP\", flops / 1e6);\n+                } else {\n+                    snprintf(buf, sizeof(buf), \"%6.2f KFLOP\", flops / 1e3);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2166765981",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14368,
        "pr_file": "tests/test-backend-ops.cpp",
        "discussion_id": "2166765981",
        "commented_code": "@@ -317,6 +319,317 @@ enum test_mode {\n     MODE_GRAD,\n };\n \n+// Output format support similar to llama-bench\n+enum output_formats { CONSOLE, SQL };\n+\n+static const char * output_format_str(output_formats format) {\n+    switch (format) {\n+        case CONSOLE:\n+            return \"console\";\n+        case SQL:\n+            return \"sql\";\n+        default:\n+            GGML_ABORT(\"invalid output format\");\n+    }\n+}\n+\n+static bool output_format_from_str(const std::string & s, output_formats & format) {\n+    if (s == \"console\") {\n+        format = CONSOLE;\n+    } else if (s == \"sql\") {\n+        format = SQL;\n+    } else {\n+        return false;\n+    }\n+    return true;\n+}\n+\n+// Test result structure for SQL output\n+struct test_result {\n+    std::string test_time;\n+    std::string backend_name;\n+    std::string op_name;\n+    std::string op_params;\n+    std::string test_mode;\n+    bool supported;\n+    bool passed;\n+    std::string error_message;\n+    double time_us;\n+    double flops_per_sec;\n+    double bandwidth_gb_s;\n+    size_t memory_kb;\n+    int n_runs;\n+\n+    test_result() {\n+        // Initialize with default values\n+        time_us = 0.0;\n+        flops_per_sec = 0.0;\n+        bandwidth_gb_s = 0.0;\n+        memory_kb = 0;\n+        n_runs = 0;\n+        supported = false;\n+        passed = false;\n+\n+        // Set test time\n+        time_t t = time(NULL);\n+        char buf[32];\n+        std::strftime(buf, sizeof(buf), \"%FT%TZ\", gmtime(&t));\n+        test_time = buf;\n+    }\n+\n+    static const std::vector<std::string> & get_fields() {\n+        static const std::vector<std::string> fields = {\n+            \"test_time\", \"backend_name\", \"op_name\", \"op_params\", \"test_mode\",\n+            \"supported\", \"passed\", \"error_message\", \"time_us\", \"flops_per_sec\",\n+            \"bandwidth_gb_s\", \"memory_kb\", \"n_runs\"\n+        };\n+        return fields;\n+    }\n+\n+    enum field_type { STRING, BOOL, INT, FLOAT };\n+\n+    static field_type get_field_type(const std::string & field) {\n+        if (field == \"supported\" || field == \"passed\") {\n+            return BOOL;\n+        }\n+        if (field == \"memory_kb\" || field == \"n_runs\") {\n+            return INT;\n+        }\n+        if (field == \"time_us\" || field == \"flops_per_sec\" || field == \"bandwidth_gb_s\") {\n+            return FLOAT;\n+        }\n+        return STRING;\n+    }\n+\n+    std::vector<std::string> get_values() const {\n+        return {\n+            test_time,\n+            backend_name,\n+            op_name,\n+            op_params,\n+            test_mode,\n+            std::to_string(supported),\n+            std::to_string(passed),\n+            error_message,\n+            std::to_string(time_us),\n+            std::to_string(flops_per_sec),\n+            std::to_string(bandwidth_gb_s),\n+            std::to_string(memory_kb),\n+            std::to_string(n_runs)\n+        };\n+    }\n+};\n+\n+// Printer classes for different output formats\n+struct printer {\n+    virtual ~printer() {}\n+    FILE * fout = stdout;\n+    virtual void print_header() {}\n+    virtual void print_test_result(const test_result & result) = 0;\n+    virtual void print_footer() {}\n+\n+    // General purpose output methods\n+    virtual void print_info(const char * format, ...) = 0;\n+    virtual void print_error(const char * format, ...) = 0;\n+    virtual void print_device_info(const char * format, ...) = 0;\n+    virtual void print_test_summary(const char * format, ...) = 0;\n+    virtual void print_status_ok() = 0;\n+    virtual void print_status_fail() = 0;\n+};\n+\n+struct console_printer : public printer {\n+    void print_test_result(const test_result & result) override {\n+        if (result.test_mode == \"test\") {\n+            print_test_console(result);\n+        } else if (result.test_mode == \"perf\") {\n+            print_perf_console(result);\n+        }\n+    }\n+\n+    void print_info(const char * format, ...) override {\n+        va_list args;\n+        va_start(args, format);\n+        vprintf(format, args);\n+        va_end(args);\n+    }\n+\n+    void print_error(const char * format, ...) override {\n+        va_list args;\n+        va_start(args, format);\n+        vfprintf(stderr, format, args);\n+        va_end(args);\n+    }\n+\n+    void print_device_info(const char * format, ...) override {\n+        va_list args;\n+        va_start(args, format);\n+        vprintf(format, args);\n+        va_end(args);\n+    }\n+\n+    void print_test_summary(const char * format, ...) override {\n+        va_list args;\n+        va_start(args, format);\n+        vprintf(format, args);\n+        va_end(args);\n+    }\n+\n+    void print_status_ok() override {\n+        printf(\"\\033[1;32mOK\\033[0m\\n\");\n+    }\n+\n+    void print_status_fail() override {\n+        printf(\"\\033[1;31mFAIL\\033[0m\\n\");\n+    }\n+\n+private:\n+    void print_test_console(const test_result & result) {\n+        printf(\"  %s(%s): \", result.op_name.c_str(), result.op_params.c_str());\n+        fflush(stdout);\n+\n+        if (!result.supported) {\n+            printf(\"not supported [%s] \", result.backend_name.c_str());\n+            printf(\"\\n\");\n+            return;\n+        }\n+\n+        if (result.passed) {\n+            printf(\"\\033[1;32mOK\\033[0m\\n\");\n+        } else {\n+            printf(\"\\033[1;31mFAIL\\033[0m\\n\");\n+        }\n+    }\n+\n+    void print_perf_console(const test_result & result) {\n+        int len = printf(\"  %s(%s): \", result.op_name.c_str(), result.op_params.c_str());\n+        fflush(stdout);\n+\n+        if (!result.supported) {\n+            printf(\"not supported\\n\");\n+            return;\n+        }\n+\n+        // align while also leaving some margin for variations in parameters\n+        int align = 8;\n+        int last = (len + align - 1) / align * align;\n+        if (last - len < 5) {\n+            last += align;\n+        }\n+        printf(\"%*s\", last - len, \"\");\n+\n+        printf(\"    %8d runs - %8.2f us/run - \",\n+            result.n_runs,\n+            result.time_us);\n+\n+        if (result.flops_per_sec > 0) {\n+            auto format_flops = [](double flops) -> std::string {\n+                char buf[256];\n+                if (flops >= 1e12) {\n+                    snprintf(buf, sizeof(buf), \"%6.2f TFLOP\", flops / 1e12);\n+                } else if (flops >= 1e9) {\n+                    snprintf(buf, sizeof(buf), \"%6.2f GFLOP\", flops / 1e9);\n+                } else if (flops >= 1e6) {\n+                    snprintf(buf, sizeof(buf), \"%6.2f MFLOP\", flops / 1e6);\n+                } else {\n+                    snprintf(buf, sizeof(buf), \"%6.2f KFLOP\", flops / 1e3);",
        "comment_created_at": "2025-06-25T13:47:17+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n                    snprintf(buf, sizeof(buf), \"%6.2f kFLOP\", flops / 1e3);\r\n```\r\n\r\nNot that it was added with this PR, but the correct capitalization for a kilo FLOP is with a k.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175093830",
    "pr_number": 14388,
    "pr_file": "ggml/src/ggml-cpu/ops.cpp",
    "created_at": "2025-06-30T13:32:32+00:00",
    "commented_code": "}\n }\n \n+static void ggml_call_mul_mat(ggml_type T, const ggml_compute_params * params, int64_t m, int64_t n, int64_t k,\n+                              void * a, void * b, void * c) {",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2175093830",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14388,
        "pr_file": "ggml/src/ggml-cpu/ops.cpp",
        "discussion_id": "2175093830",
        "commented_code": "@@ -6116,6 +6117,185 @@ void ggml_compute_forward_im2col_back_f32(\n     }\n }\n \n+static void ggml_call_mul_mat(ggml_type T, const ggml_compute_params * params, int64_t m, int64_t n, int64_t k,\n+                              void * a, void * b, void * c) {",
        "comment_created_at": "2025-06-30T13:32:32+00:00",
        "comment_author": "slaren",
        "comment_body": "For consistency, rename `T` to `type`. `c` could be a pointer to `float`, and `a` and `b` could be `const`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2175232514",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14388,
        "pr_file": "ggml/src/ggml-cpu/ops.cpp",
        "discussion_id": "2175093830",
        "commented_code": "@@ -6116,6 +6117,185 @@ void ggml_compute_forward_im2col_back_f32(\n     }\n }\n \n+static void ggml_call_mul_mat(ggml_type T, const ggml_compute_params * params, int64_t m, int64_t n, int64_t k,\n+                              void * a, void * b, void * c) {",
        "comment_created_at": "2025-06-30T14:36:21+00:00",
        "comment_author": "am17an",
        "comment_body": "a and b can't be const here because ggml_tensor expects a non-const pointer",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2116372420",
    "pr_number": 13873,
    "pr_file": "ggml/src/ggml-opt.cpp",
    "created_at": "2025-05-30T18:13:27+00:00",
    "commented_code": "// gb_opt == graph backward optimize, forward pass, then backward pass to calculate gradients, then optimizer step.\n     opt_ctx->gb_opt = ggml_graph_dup(opt_ctx->ctx_compute, opt_ctx->gb_grad, /*force_grads =*/ true);\n \n-    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 7);\n+    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 8);\n     ggml_set_input(opt_ctx->adamw_params);\n     ggml_set_name(opt_ctx->adamw_params, \"adamw_params\");\n \n     for (int i = opt_ctx->gf->n_nodes-1; i >= 0; --i) {\n         struct ggml_tensor * node = opt_ctx->gb_opt->nodes[i];\n         struct ggml_tensor * grad = ggml_graph_get_grad(opt_ctx->gb_opt, node);\n \n+        std::string step_prefix     = moment ? \"AdamW step for \" : \"SGD step for \";\n+        unsigned    step_prefix_len = step_prefix.size();\n         if (grad && (node->flags & GGML_TENSOR_FLAG_PARAM)) {\n-            struct ggml_tensor * m        = opt_ctx->grad_m[i];\n-            struct ggml_tensor * v        = opt_ctx->grad_v[i];\n-            struct ggml_tensor * opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n-\n-            ggml_set_name(m,        (std::string(\"AdamW m for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(v,        (std::string(\"AdamW v for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(opt_step, (std::string(\"AdamW step for \") + std::string(node->name)).c_str());\n-\n+            struct ggml_tensor * opt_step;\n+            if (moment) {\n+                struct ggml_tensor * m = opt_ctx->grad_m[i];\n+                struct ggml_tensor * v = opt_ctx->grad_v[i];\n+                opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n+\n+                ggml_set_name(m, (std::string(\"AdamW m for \") + std::string(node->name)).c_str());\n+                ggml_set_name(v, (std::string(\"AdamW v for \") + std::string(node->name)).c_str());\n+            } else {\n+                opt_step = ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, opt_ctx->adamw_params);\n+            }\n+            step_prefix.resize(step_prefix_len);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2116372420",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2116372420",
        "commented_code": "@@ -492,23 +529,31 @@ static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     // gb_opt == graph backward optimize, forward pass, then backward pass to calculate gradients, then optimizer step.\n     opt_ctx->gb_opt = ggml_graph_dup(opt_ctx->ctx_compute, opt_ctx->gb_grad, /*force_grads =*/ true);\n \n-    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 7);\n+    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 8);\n     ggml_set_input(opt_ctx->adamw_params);\n     ggml_set_name(opt_ctx->adamw_params, \"adamw_params\");\n \n     for (int i = opt_ctx->gf->n_nodes-1; i >= 0; --i) {\n         struct ggml_tensor * node = opt_ctx->gb_opt->nodes[i];\n         struct ggml_tensor * grad = ggml_graph_get_grad(opt_ctx->gb_opt, node);\n \n+        std::string step_prefix     = moment ? \"AdamW step for \" : \"SGD step for \";\n+        unsigned    step_prefix_len = step_prefix.size();\n         if (grad && (node->flags & GGML_TENSOR_FLAG_PARAM)) {\n-            struct ggml_tensor * m        = opt_ctx->grad_m[i];\n-            struct ggml_tensor * v        = opt_ctx->grad_v[i];\n-            struct ggml_tensor * opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n-\n-            ggml_set_name(m,        (std::string(\"AdamW m for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(v,        (std::string(\"AdamW v for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(opt_step, (std::string(\"AdamW step for \") + std::string(node->name)).c_str());\n-\n+            struct ggml_tensor * opt_step;\n+            if (moment) {\n+                struct ggml_tensor * m = opt_ctx->grad_m[i];\n+                struct ggml_tensor * v = opt_ctx->grad_v[i];\n+                opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n+\n+                ggml_set_name(m, (std::string(\"AdamW m for \") + std::string(node->name)).c_str());\n+                ggml_set_name(v, (std::string(\"AdamW v for \") + std::string(node->name)).c_str());\n+            } else {\n+                opt_step = ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, opt_ctx->adamw_params);\n+            }\n+            step_prefix.resize(step_prefix_len);",
        "comment_created_at": "2025-05-30T18:13:27+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I don't understand the purpose of this line.",
        "pr_file_module": null
      },
      {
        "comment_id": "2116429112",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2116372420",
        "commented_code": "@@ -492,23 +529,31 @@ static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     // gb_opt == graph backward optimize, forward pass, then backward pass to calculate gradients, then optimizer step.\n     opt_ctx->gb_opt = ggml_graph_dup(opt_ctx->ctx_compute, opt_ctx->gb_grad, /*force_grads =*/ true);\n \n-    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 7);\n+    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 8);\n     ggml_set_input(opt_ctx->adamw_params);\n     ggml_set_name(opt_ctx->adamw_params, \"adamw_params\");\n \n     for (int i = opt_ctx->gf->n_nodes-1; i >= 0; --i) {\n         struct ggml_tensor * node = opt_ctx->gb_opt->nodes[i];\n         struct ggml_tensor * grad = ggml_graph_get_grad(opt_ctx->gb_opt, node);\n \n+        std::string step_prefix     = moment ? \"AdamW step for \" : \"SGD step for \";\n+        unsigned    step_prefix_len = step_prefix.size();\n         if (grad && (node->flags & GGML_TENSOR_FLAG_PARAM)) {\n-            struct ggml_tensor * m        = opt_ctx->grad_m[i];\n-            struct ggml_tensor * v        = opt_ctx->grad_v[i];\n-            struct ggml_tensor * opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n-\n-            ggml_set_name(m,        (std::string(\"AdamW m for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(v,        (std::string(\"AdamW v for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(opt_step, (std::string(\"AdamW step for \") + std::string(node->name)).c_str());\n-\n+            struct ggml_tensor * opt_step;\n+            if (moment) {\n+                struct ggml_tensor * m = opt_ctx->grad_m[i];\n+                struct ggml_tensor * v = opt_ctx->grad_v[i];\n+                opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n+\n+                ggml_set_name(m, (std::string(\"AdamW m for \") + std::string(node->name)).c_str());\n+                ggml_set_name(v, (std::string(\"AdamW v for \") + std::string(node->name)).c_str());\n+            } else {\n+                opt_step = ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, opt_ctx->adamw_params);\n+            }\n+            step_prefix.resize(step_prefix_len);",
        "comment_created_at": "2025-05-30T18:36:00+00:00",
        "comment_author": "graehl",
        "comment_body": "i can add a comment, but it's just saving on creating temp strings from the same constant repeatedly ... roughly it's because of a += append to that step_prefix above",
        "pr_file_module": null
      },
      {
        "comment_id": "2118964576",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "ggml/src/ggml-opt.cpp",
        "discussion_id": "2116372420",
        "commented_code": "@@ -492,23 +529,31 @@ static void ggml_opt_build(ggml_opt_context_t opt_ctx) {\n     // gb_opt == graph backward optimize, forward pass, then backward pass to calculate gradients, then optimizer step.\n     opt_ctx->gb_opt = ggml_graph_dup(opt_ctx->ctx_compute, opt_ctx->gb_grad, /*force_grads =*/ true);\n \n-    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 7);\n+    opt_ctx->adamw_params = ggml_new_tensor_1d(opt_ctx->ctx_cpu, GGML_TYPE_F32, 8);\n     ggml_set_input(opt_ctx->adamw_params);\n     ggml_set_name(opt_ctx->adamw_params, \"adamw_params\");\n \n     for (int i = opt_ctx->gf->n_nodes-1; i >= 0; --i) {\n         struct ggml_tensor * node = opt_ctx->gb_opt->nodes[i];\n         struct ggml_tensor * grad = ggml_graph_get_grad(opt_ctx->gb_opt, node);\n \n+        std::string step_prefix     = moment ? \"AdamW step for \" : \"SGD step for \";\n+        unsigned    step_prefix_len = step_prefix.size();\n         if (grad && (node->flags & GGML_TENSOR_FLAG_PARAM)) {\n-            struct ggml_tensor * m        = opt_ctx->grad_m[i];\n-            struct ggml_tensor * v        = opt_ctx->grad_v[i];\n-            struct ggml_tensor * opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n-\n-            ggml_set_name(m,        (std::string(\"AdamW m for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(v,        (std::string(\"AdamW v for \")    + std::string(node->name)).c_str());\n-            ggml_set_name(opt_step, (std::string(\"AdamW step for \") + std::string(node->name)).c_str());\n-\n+            struct ggml_tensor * opt_step;\n+            if (moment) {\n+                struct ggml_tensor * m = opt_ctx->grad_m[i];\n+                struct ggml_tensor * v = opt_ctx->grad_v[i];\n+                opt_step = ggml_opt_step_adamw(opt_ctx->ctx_compute, node, grad, m, v, opt_ctx->adamw_params);\n+\n+                ggml_set_name(m, (std::string(\"AdamW m for \") + std::string(node->name)).c_str());\n+                ggml_set_name(v, (std::string(\"AdamW v for \") + std::string(node->name)).c_str());\n+            } else {\n+                opt_step = ggml_opt_step_sgd(opt_ctx->ctx_compute, node, grad, opt_ctx->adamw_params);\n+            }\n+            step_prefix.resize(step_prefix_len);",
        "comment_created_at": "2025-06-01T09:58:59+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Ah okay, I didn't see the inline `+=` in the line below. I think the code could be made easier to understand if you change the name of `step_prefix_len` to something like `step_prefix_len_orig` and move the `+=` to a separate line.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160386099",
    "pr_number": 12718,
    "pr_file": "tools/imatrix/imatrix.cpp",
    "created_at": "2025-06-22T15:47:09+00:00",
    "commented_code": "int ncall = 0;\n };\n \n+struct tensor_statistics {\n+    std::string tensor;\n+    Stats stats;\n+    float total_bias = 0;\n+    float mean_bias  = 0;\n+    float max_bias   = 0;\n+    float min_bias   = 0;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2160386099",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 12718,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2160386099",
        "commented_code": "@@ -35,13 +37,28 @@ struct Stats {\n     int ncall = 0;\n };\n \n+struct tensor_statistics {\n+    std::string tensor;\n+    Stats stats;\n+    float total_bias = 0;\n+    float mean_bias  = 0;\n+    float max_bias   = 0;\n+    float min_bias   = 0;",
        "comment_created_at": "2025-06-22T15:47:09+00:00",
        "comment_author": "compilade",
        "comment_body": "It's not \"bias\", it's the squared activations. There might eventually be more stuff calculated and stored in `imatrix` files, and so a more representative name for the statistics would be easier to extend while reducing eventual confusion (for example, with non-squared activations (to get the actual mean), cubed activations (for skewness), etc. Those aren't yet calculated but could be in the future)\r\n\r\nLikewise, `\u03a3(bias)` should probably be replaced with `\u03a3(act\u00b2)`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160487395",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 12718,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2160386099",
        "commented_code": "@@ -35,13 +37,28 @@ struct Stats {\n     int ncall = 0;\n };\n \n+struct tensor_statistics {\n+    std::string tensor;\n+    Stats stats;\n+    float total_bias = 0;\n+    float mean_bias  = 0;\n+    float max_bias   = 0;\n+    float min_bias   = 0;",
        "comment_created_at": "2025-06-22T20:52:24+00:00",
        "comment_author": "EAddario",
        "comment_body": "I \"borrowed\" the term from EE \ud83d\ude01 but makes more sense to replace with \u03a3(act\u00b2)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188078757",
    "pr_number": 14544,
    "pr_file": "common/arg.cpp",
    "created_at": "2025-07-06T07:21:47+00:00",
    "commented_code": "params.public_path = value;\n         }\n     ).set_examples({LLAMA_EXAMPLE_SERVER}).set_env(\"LLAMA_ARG_STATIC_PATH\"));\n+    add_opt(common_arg(\n+        {\"--prefix\"}, \"PREFIX\",",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2188078757",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14544,
        "pr_file": "common/arg.cpp",
        "discussion_id": "2188078757",
        "commented_code": "@@ -2734,6 +2734,13 @@ common_params_context common_params_parser_init(common_params & params, llama_ex\n             params.public_path = value;\n         }\n     ).set_examples({LLAMA_EXAMPLE_SERVER}).set_env(\"LLAMA_ARG_STATIC_PATH\"));\n+    add_opt(common_arg(\n+        {\"--prefix\"}, \"PREFIX\",",
        "comment_created_at": "2025-07-06T07:21:47+00:00",
        "comment_author": "ngxson",
        "comment_body": "--prefix can be quite vague to understand, risk of clashing with other features in the future. I think it should be --api-prefix for clarity ",
        "pr_file_module": null
      }
    ]
  }
]