[
  {
    "discussion_id": "2128303709",
    "pr_number": 13892,
    "pr_file": "ggml/src/ggml-cpu/arch/arm/quants.c",
    "created_at": "2025-06-05T08:50:17+00:00",
    "commented_code": "+#define GGML_COMMON_IMPL_C\n+#include \"ggml-common.h\"\n+#include \"ggml-quants.h\"\n+#include \"ggml-impl.h\"\n+#include \"ggml-cpu.h\"\n+\n+#include \"../../quants.h\"\n+#include \"../../ggml-cpu-impl.h\"\n+\n+#include <math.h>\n+#include <string.h>\n+#include <assert.h>\n+#include <float.h>\n+#include <stdlib.h> // for qsort\n+#include <stdio.h>  // for GGML_ASSERT\n+\n+#define GROUP_MAX_EPS 1e-15f\n+#define GROUP_MAX_EPS_IQ3_XXS 1e-8f\n+#define GROUP_MAX_EPS_IQ2_S 1e-8f\n+#define GROUP_MAX_EPS_IQ1_M 1e-7f\n+#define GROUP_MAX_EPS_IQ1_S 1e-12f\n+\n+#define UNUSED GGML_UNUSED\n+\n+#if defined(__ARM_NEON)\n+#define B1(c,s,n)  0x ## n ## c ,  0x ## n ## s\n+#define B2(c,s,n) B1(c,s,n ## c), B1(c,s,n ## s)\n+#define B3(c,s,n) B2(c,s,n ## c), B2(c,s,n ## s)\n+#define B4(c,s,n) B3(c,s,n ## c), B3(c,s,n ## s)\n+#define B5(c,s,n) B4(c,s,n ## c), B4(c,s,n ## s)\n+#define B6(c,s,n) B5(c,s,n ## c), B5(c,s,n ## s)\n+#define B7(c,s,n) B6(c,s,n ## c), B6(c,s,n ## s)\n+#define B8(c,s  ) B7(c,s,     c), B7(c,s,     s)\n+\n+// precomputed tables for expanding 8bits to 8 bytes:\n+static const uint64_t table_b2b_0[1 << 8] = { B8(00, 10) }; // ( b) << 4\n+static const uint64_t table_b2b_1[1 << 8] = { B8(10, 00) }; // (!b) << 4\n+#endif\n+\n+void quantize_row_q8_0(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {\n+    assert(QK8_0 == 32);\n+    assert(k % QK8_0 == 0);\n+    const int nb = k / QK8_0;\n+\n+    block_q8_0 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_NEON)\n+    for (int i = 0; i < nb; i++) {\n+        float32x4_t srcv [8];\n+        float32x4_t asrcv[8];\n+        float32x4_t amaxv[8];\n+\n+        for (int j = 0; j < 8; j++) srcv[j]  = vld1q_f32(x + i*32 + 4*j);\n+        for (int j = 0; j < 8; j++) asrcv[j] = vabsq_f32(srcv[j]);\n+\n+        for (int j = 0; j < 4; j++) amaxv[2*j] = vmaxq_f32(asrcv[2*j], asrcv[2*j+1]);\n+        for (int j = 0; j < 2; j++) amaxv[4*j] = vmaxq_f32(amaxv[4*j], amaxv[4*j+2]);\n+        for (int j = 0; j < 1; j++) amaxv[8*j] = vmaxq_f32(amaxv[8*j], amaxv[8*j+4]);\n+\n+        const float amax = vmaxvq_f32(amaxv[0]);\n+\n+        const float d = amax / ((1 << 7) - 1);\n+        const float id = d ? 1.0f/d : 0.0f;\n+\n+        y[i].d = GGML_FP32_TO_FP16(d);\n+\n+        for (int j = 0; j < 8; j++) {\n+            const float32x4_t v  = vmulq_n_f32(srcv[j], id);\n+            const int32x4_t   vi = vcvtnq_s32_f32(v);\n+\n+            y[i].qs[4*j + 0] = vgetq_lane_s32(vi, 0);\n+            y[i].qs[4*j + 1] = vgetq_lane_s32(vi, 1);\n+            y[i].qs[4*j + 2] = vgetq_lane_s32(vi, 2);\n+            y[i].qs[4*j + 3] = vgetq_lane_s32(vi, 3);\n+        }\n+    }\n+#else\n+    GGML_UNUSED(nb);\n+    // scalar\n+    quantize_row_q8_0_ref(x, y, k);\n+#endif\n+}\n+\n+void quantize_row_q8_1(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {\n+    assert(k % QK8_1 == 0);\n+    const int nb = k / QK8_1;\n+\n+    block_q8_1 * GGML_RESTRICT y = vy;\n+#if defined(__ARM_NEON)\n+    for (int i = 0; i < nb; i++) {\n+        float32x4_t srcv [8];\n+        float32x4_t asrcv[8];\n+        float32x4_t amaxv[8];\n+\n+        for (int j = 0; j < 8; j++) srcv[j]  = vld1q_f32(x + i*32 + 4*j);\n+        for (int j = 0; j < 8; j++) asrcv[j] = vabsq_f32(srcv[j]);\n+\n+        for (int j = 0; j < 4; j++) amaxv[2*j] = vmaxq_f32(asrcv[2*j], asrcv[2*j+1]);\n+        for (int j = 0; j < 2; j++) amaxv[4*j] = vmaxq_f32(amaxv[4*j], amaxv[4*j+2]);\n+        for (int j = 0; j < 1; j++) amaxv[8*j] = vmaxq_f32(amaxv[8*j], amaxv[8*j+4]);\n+\n+        const float amax = vmaxvq_f32(amaxv[0]);\n+\n+        const float d = amax / ((1 << 7) - 1);\n+        const float id = d ? 1.0f/d : 0.0f;\n+\n+        y[i].d = GGML_FP32_TO_FP16(d);\n+\n+        int32x4_t accv = vdupq_n_s32(0);\n+\n+        for (int j = 0; j < 8; j++) {\n+            const float32x4_t v  = vmulq_n_f32(srcv[j], id);\n+            const int32x4_t   vi = vcvtnq_s32_f32(v);\n+\n+            y[i].qs[4*j + 0] = vgetq_lane_s32(vi, 0);\n+            y[i].qs[4*j + 1] = vgetq_lane_s32(vi, 1);\n+            y[i].qs[4*j + 2] = vgetq_lane_s32(vi, 2);\n+            y[i].qs[4*j + 3] = vgetq_lane_s32(vi, 3);\n+\n+            accv = vaddq_s32(accv, vi);\n+        }\n+\n+        y[i].s = GGML_FP32_TO_FP16(d * vaddvq_s32(accv));\n+    }\n+#else\n+    GGML_UNUSED(nb);\n+    // scalar\n+    quantize_row_q8_1_ref(x, y, k);\n+#endif\n+}\n+\n+// placeholder implementation for Apple targets\n+void quantize_row_q8_K(const float * GGML_RESTRICT x, void * GGML_RESTRICT y, int64_t k) {\n+    quantize_row_q8_K_ref(x, y, k);\n+}\n+\n+//===================================== Dot products =================================\n+\n+void ggml_vec_dot_q4_0_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_0;\n+    const int nb = n / qk;\n+\n+    assert(n % qk == 0);\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    assert((nrc == 2) || (nrc == 1));\n+#else\n+    assert(nrc == 1);\n+#endif\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q4_0 * GGML_RESTRICT x = vx;\n+    const block_q8_0 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    if (nrc == 2) {\n+        const block_q4_0 * GGML_RESTRICT vx0 = vx;\n+        const block_q4_0 * GGML_RESTRICT vx1 = (const block_q4_0 *) ((const uint8_t*)vx + bx);\n+        const block_q8_0 * GGML_RESTRICT vy0 = vy;\n+        const block_q8_0 * GGML_RESTRICT vy1 = (const block_q8_0 *) ((const uint8_t*)vy + by);\n+\n+        float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+\n+        for (int i = 0; i < nb; i++) {\n+            const block_q4_0 * GGML_RESTRICT b_x0 = &vx0[i];\n+            const block_q4_0 * GGML_RESTRICT b_x1 = &vx1[i];\n+            const block_q8_0 * GGML_RESTRICT b_y0 = &vy0[i];\n+            const block_q8_0 * GGML_RESTRICT b_y1 = &vy1[i];\n+\n+            const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+            const int8x16_t  s8b = vdupq_n_s8(0x8);\n+\n+            const uint8x16_t v0_0 = vld1q_u8(b_x0->qs);\n+            const uint8x16_t v0_1 = vld1q_u8(b_x1->qs);\n+\n+            // 4-bit -> 8-bit\n+            const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+            const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+            const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+            const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+            // sub 8\n+            const int8x16_t x0_l = vsubq_s8(v0_0l, s8b);\n+            const int8x16_t x0_h = vsubq_s8(v0_0h, s8b);\n+            const int8x16_t x1_l = vsubq_s8(v0_1l, s8b);\n+            const int8x16_t x1_h = vsubq_s8(v0_1h, s8b);\n+\n+            // load y\n+            const int8x16_t y0_l = vld1q_s8(b_y0->qs);\n+            const int8x16_t y0_h = vld1q_s8(b_y0->qs + 16);\n+            const int8x16_t y1_l = vld1q_s8(b_y1->qs);\n+            const int8x16_t y1_h = vld1q_s8(b_y1->qs + 16);\n+\n+            float32_t _scale[4] = {\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y1->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y1->d)\n+            };\n+            float32x4_t scale = vld1q_f32(_scale);\n+\n+            int8x16_t l0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+            int8x16_t l1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+\n+            int8x16_t l2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+            int8x16_t l3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+\n+            int8x16_t r0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+            int8x16_t r1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+\n+            int8x16_t r2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+            int8x16_t r3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+\n+            sumv0 = vmlaq_f32(sumv0,(vcvtq_f32_s32(vmmlaq_s32((vmmlaq_s32((vmmlaq_s32((vmmlaq_s32(vdupq_n_s32(0), l0, r0)),\n+                                                l1, r1)), l2, r2)), l3, r3))), scale);\n+        }\n+\n+        float32x4_t sumv1 = vextq_f32 (sumv0, sumv0, 2);\n+        float32x4_t sumv2 = vzip1q_f32(sumv0, sumv1);\n+\n+        vst1_f32(s,      vget_low_f32 (sumv2));\n+        vst1_f32(s + bs, vget_high_f32(sumv2));\n+\n+        return;\n+    }\n+#endif\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+#if defined(__ARM_FEATURE_SVE)\n+    svfloat32_t sumv0 = svdup_n_f32(0.0f);\n+    svfloat32_t sumv1 = svdup_n_f32(0.0f);\n+\n+    const int vector_length = ggml_cpu_get_sve_cnt()*8;\n+\n+    // VLA Implementation using switch case\n+    switch (vector_length) {\n+        case 128:\n+            {\n+                // predicate for activating higher lanes for 4 float32 elements\n+                const svbool_t ph4 = svptrue_pat_b32(SV_VL4);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q4_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q4_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svuint8_t qx0r = svld1rq_u8(svptrue_b8(), x0->qs);\n+                    const svuint8_t qx1r = svld1rq_u8(svptrue_b8(), x1->qs);\n+\n+                    // 4-bit -> 8-bit\n+                    const svint8_t qx0l = svreinterpret_s8_u8(svand_n_u8_m(svptrue_b8(), qx0r, 0x0F));\n+                    const svint8_t qx0h = svreinterpret_s8_u8(svlsr_n_u8_m(svptrue_b8(), qx0r, 0x04));\n+                    const svint8_t qx1l = svreinterpret_s8_u8(svand_n_u8_m(svptrue_b8(), qx1r, 0x0F));\n+                    const svint8_t qx1h = svreinterpret_s8_u8(svlsr_n_u8_m(svptrue_b8(), qx1r, 0x04));\n+\n+                    // sub 8\n+                    const svint8_t qx0ls = svsub_n_s8_x(svptrue_b8(), qx0h, 8);\n+                    const svint8_t qx0hs = svsub_n_s8_x(svptrue_b8(), qx0l, 8);\n+                    const svint8_t qx1ls = svsub_n_s8_x(svptrue_b8(), qx1h, 8);\n+                    const svint8_t qx1hs = svsub_n_s8_x(svptrue_b8(), qx1l, 8);\n+\n+                    // load y\n+                    const svint8_t qy0h = svld1_s8(svptrue_b8(), y0->qs);\n+                    const svint8_t qy0l = svld1_s8(svptrue_b8(), y0->qs + 16);\n+                    const svint8_t qy1h = svld1_s8(svptrue_b8(), y1->qs);\n+                    const svint8_t qy1l = svld1_s8(svptrue_b8(), y1->qs + 16);\n+\n+                    // dot product\n+                    sumv0 = svmla_n_f32_x(ph4, sumv0, svcvt_f32_s32_x(ph4, svadd_x(ph4,\n+                                    svdot_s32(svdup_n_s32(0), qx0ls, qy0l),\n+                                    svdot_s32(svdup_n_s32(0), qx0hs, qy0h))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(ph4, sumv1, svcvt_f32_s32_x(ph4, svadd_x(ph4,\n+                                    svdot_s32(svdup_n_s32(0), qx1ls, qy1l),\n+                                    svdot_s32(svdup_n_s32(0), qx1hs, qy1h))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(svptrue_b32(), svadd_f32_x(svptrue_b32(), sumv0, sumv1));\n+            } break;\n+        case 256:\n+            {\n+                // predicate for activating higher lanes for 16 int8 elements\n+                const svbool_t ph16 = svptrue_pat_b8(SV_VL16);\n+                // predicate for activating lower lanes for  16 int8 elements\n+                const svbool_t pl16 = svnot_b_z(svptrue_b8(), ph16);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q4_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q4_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svuint8_t qx0r = svld1rq_u8(svptrue_b8(), x0->qs);\n+                    const svuint8_t qx1r = svld1rq_u8(svptrue_b8(), x1->qs);\n+\n+                    // 4-bit -> 8-bit\n+                    const svint8_t qx0 = svreinterpret_s8_u8(svlsr_n_u8_m(pl16, svand_n_u8_m(ph16, qx0r, 0x0F), 0x04));\n+                    const svint8_t qx1 = svreinterpret_s8_u8(svlsr_n_u8_m(pl16, svand_n_u8_m(ph16, qx1r, 0x0F), 0x04));\n+\n+                    // sub 8\n+                    const svint8_t qx0s = svsub_n_s8_x(svptrue_b8(), qx0, 8);\n+                    const svint8_t qx1s = svsub_n_s8_x(svptrue_b8(), qx1, 8);\n+\n+                    // load y\n+                    const svint8_t qy0 = svld1_s8(svptrue_b8(), y0->qs);\n+                    const svint8_t qy1 = svld1_s8(svptrue_b8(), y1->qs);\n+\n+                    // dot product\n+                    sumv0 = svmla_n_f32_x(svptrue_b32(), sumv0, svcvt_f32_s32_x(svptrue_b32(),\n+                                svdot_s32(svdup_n_s32(0), qx0s, qy0)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(svptrue_b32(), sumv1, svcvt_f32_s32_x(svptrue_b32(),\n+                                svdot_s32(svdup_n_s32(0), qx1s, qy1)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(svptrue_b32(), svadd_f32_x(svptrue_b32(), sumv0, sumv1));\n+            } break;\n+        case 512:\n+            {\n+                // predicate for activating higher lanes for 32 int8 elements\n+                const svbool_t ph32 = svptrue_pat_b8(SV_VL32);\n+\n+                // predicate for activating higher lanes for 16 int8 elements\n+                const svbool_t ph16 = svptrue_pat_b8(SV_VL16);\n+                // predicate for activating lower lanes for 16 int8 elements from first 32 int8 activated lanes\n+                const svbool_t pl16 = svnot_b_z(ph32, ph16);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q4_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q4_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svuint8_t qx0r = svld1rq_u8(ph32, x0->qs);\n+                    const svuint8_t qx1r = svld1rq_u8(ph32, x1->qs);\n+\n+                    // 4-bit -> 8-bit\n+                    const svint8_t qx0 = svreinterpret_s8_u8(svlsr_n_u8_m(pl16, svand_n_u8_m(ph16, qx0r, 0x0F), 0x04));\n+                    const svint8_t qx1 = svreinterpret_s8_u8(svlsr_n_u8_m(pl16, svand_n_u8_m(ph16, qx1r, 0x0F), 0x04));\n+\n+                    // sub 8\n+                    const svint8_t qx0s = svsub_n_s8_x(ph32, qx0, 8);\n+                    const svint8_t qx1s = svsub_n_s8_x(ph32, qx1, 8);\n+\n+                    // load y\n+                    const svint8_t qy0 = svld1_s8(ph32, y0->qs);\n+                    const svint8_t qy1 = svld1_s8(ph32, y1->qs);\n+\n+                    // dot product\n+                    sumv0 = svmla_n_f32_x(ph32, sumv0, svcvt_f32_s32_x(ph32,\n+                                svdot_s32(svdup_n_s32(0), qx0s, qy0)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(ph32, sumv1, svcvt_f32_s32_x(ph32,\n+                                svdot_s32(svdup_n_s32(0), qx1s, qy1)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(ph32, svadd_f32_x(ph32, sumv0, sumv1));\n+            } break;\n+        default:\n+            assert(false && \"Unsupported vector length\");\n+            break;\n+    }\n+\n+#elif defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q4_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+        const block_q4_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+        const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+        const int8x16_t  s8b = vdupq_n_s8(0x8);\n+\n+        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n+        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n+\n+        // 4-bit -> 8-bit\n+        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+        // sub 8\n+        const int8x16_t v0_0ls = vsubq_s8(v0_0l, s8b);\n+        const int8x16_t v0_0hs = vsubq_s8(v0_0h, s8b);\n+        const int8x16_t v0_1ls = vsubq_s8(v0_1l, s8b);\n+        const int8x16_t v0_1hs = vsubq_s8(v0_1h, s8b);\n+\n+        // load y\n+        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n+        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n+        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n+        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n+\n+        // dot product into int32x4_t\n+        const int32x4_t p_0 = ggml_vdotq_s32(ggml_vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0l), v0_0hs, v1_0h);\n+        const int32x4_t p_1 = ggml_vdotq_s32(ggml_vdotq_s32(vdupq_n_s32(0), v0_1ls, v1_1l), v0_1hs, v1_1h);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(p_0), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(p_1), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n+#endif\n+    for (; ib < nb; ++ib) {\n+        int sumi0 = 0;\n+        int sumi1 = 0;\n+\n+        for (int j = 0; j < qk/2; ++j) {\n+            const int v0 = (x[ib].qs[j] & 0x0F) - 8;\n+            const int v1 = (x[ib].qs[j] >>   4) - 8;\n+\n+            sumi0 += (v0 * y[ib].qs[j]);\n+            sumi1 += (v1 * y[ib].qs[j + qk/2]);\n+        }\n+\n+        int sumi = sumi0 + sumi1;\n+        sumf += sumi*GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d);\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_q4_1_q8_1(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_1;\n+    const int nb = n / qk;\n+\n+    assert(n % qk == 0);\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    assert((nrc == 2) || (nrc == 1));\n+#else\n+    assert(nrc == 1);\n+#endif\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q4_1 * GGML_RESTRICT x = vx;\n+    const block_q8_1 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    if (nrc == 2) {\n+        const block_q4_1 * GGML_RESTRICT vx0 = vx;\n+        const block_q4_1 * GGML_RESTRICT vx1 = (const block_q4_1 *) ((const uint8_t*)vx + bx);\n+        const block_q8_1 * GGML_RESTRICT vy0 = vy;\n+        const block_q8_1 * GGML_RESTRICT vy1 = (const block_q8_1 *) ((const uint8_t*)vy + by);\n+\n+        float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+        float32x4_t summs0 = vdupq_n_f32(0.0f);\n+\n+        for (int i = 0; i < nb; i++) {\n+            const block_q4_1 * GGML_RESTRICT b_x0 = &vx0[i];\n+            const block_q4_1 * GGML_RESTRICT b_x1 = &vx1[i];\n+            const block_q8_1 * GGML_RESTRICT b_y0 = &vy0[i];\n+            const block_q8_1 * GGML_RESTRICT b_y1 = &vy1[i];\n+\n+            float32_t summs_t[4] = {\n+                GGML_FP16_TO_FP32(b_x0->m) * GGML_FP16_TO_FP32(b_y0->s),\n+                GGML_FP16_TO_FP32(b_x1->m) * GGML_FP16_TO_FP32(b_y0->s),\n+                GGML_FP16_TO_FP32(b_x0->m) * GGML_FP16_TO_FP32(b_y1->s),\n+                GGML_FP16_TO_FP32(b_x1->m) * GGML_FP16_TO_FP32(b_y1->s)\n+            };\n+            summs0 = vaddq_f32(summs0, vld1q_f32(summs_t));\n+\n+            const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+\n+            const uint8x16_t v0_0 = vld1q_u8(b_x0->qs);\n+            const uint8x16_t v0_1 = vld1q_u8(b_x1->qs);\n+\n+            // 4-bit -> 8-bit\n+            const int8x16_t x0_l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+            const int8x16_t x0_h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+            const int8x16_t x1_l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+            const int8x16_t x1_h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+            // load y\n+            const int8x16_t y0_l = vld1q_s8(b_y0->qs);\n+            const int8x16_t y0_h = vld1q_s8(b_y0->qs + 16);\n+            const int8x16_t y1_l = vld1q_s8(b_y1->qs);\n+            const int8x16_t y1_h = vld1q_s8(b_y1->qs + 16);\n+\n+            // mmla into int32x4_t\n+            float32_t _scale[4] = {\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y1->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y1->d)\n+            };\n+            float32x4_t scale = vld1q_f32(_scale);\n+\n+            int8x16_t l0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+            int8x16_t l1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+\n+            int8x16_t l2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+            int8x16_t l3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+\n+            int8x16_t r0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+            int8x16_t r1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+\n+            int8x16_t r2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+            int8x16_t r3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+            sumv0 = vmlaq_f32(sumv0,(vcvtq_f32_s32(vmmlaq_s32((vmmlaq_s32((vmmlaq_s32((vmmlaq_s32(vdupq_n_s32(0), l0, r0)),\n+                                                l1, r1)), l2, r2)), l3, r3))), scale);\n+        }\n+\n+        float32x4_t sumv1 = vextq_f32 (sumv0, sumv0, 2);\n+        float32x4_t sumv2 = vzip1q_f32(sumv0, sumv1);\n+\n+        sumv2 = vaddq_f32(sumv2, summs0);\n+\n+        vst1_f32(s,      vget_low_f32 (sumv2));\n+        vst1_f32(s + bs, vget_high_f32(sumv2));\n+\n+        return;\n+    }\n+#endif\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+#if defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    float summs = 0;\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q4_1 * GGML_RESTRICT x0 = &x[ib + 0];\n+        const block_q4_1 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_1 * GGML_RESTRICT y0 = &y[ib + 0];\n+        const block_q8_1 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        summs += GGML_FP16_TO_FP32(x0->m) * GGML_FP16_TO_FP32(y0->s) + GGML_FP16_TO_FP32(x1->m) * GGML_FP16_TO_FP32(y1->s);\n+\n+        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+\n+        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n+        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n+\n+        // 4-bit -> 8-bit\n+        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+        // load y\n+        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n+        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n+        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n+        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n+\n+        // dot product into int32x4_t\n+        const int32x4_t p_0 = ggml_vdotq_s32(ggml_vdotq_s32(vdupq_n_s32(0), v0_0l, v1_0l), v0_0h, v1_0h);\n+        const int32x4_t p_1 = ggml_vdotq_s32(ggml_vdotq_s32(vdupq_n_s32(0), v0_1l, v1_1l), v0_1h, v1_1h);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(p_0), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(p_1), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1) + summs;\n+\n+#endif\n+    for (; ib < nb; ++ib) {\n+        int sumi0 = 0;\n+        int sumi1 = 0;\n+\n+        for (int j = 0; j < qk/2; ++j) {\n+            const int v0 = (x[ib].qs[j] & 0x0F);\n+            const int v1 = (x[ib].qs[j] >>   4);\n+\n+            sumi0 += (v0 * y[ib].qs[j]);\n+            sumi1 += (v1 * y[ib].qs[j + qk/2]);\n+        }\n+\n+        int sumi = sumi0 + sumi1;\n+        sumf += (GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d))*sumi + GGML_FP16_TO_FP32(x[ib].m)*GGML_FP16_TO_FP32(y[ib].s);\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_q5_0_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_0;\n+    const int nb = n / qk;\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+    assert(n % qk == 0);\n+    assert(qk == QK5_0);\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q5_0 * GGML_RESTRICT x = vx;\n+    const block_q8_0 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    uint32_t qh0;\n+    uint32_t qh1;\n+\n+    uint64_t tmp0[4];\n+    uint64_t tmp1[4];\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q5_0 * GGML_RESTRICT x0 = &x[ib];\n+        const block_q5_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_0 * GGML_RESTRICT y0 = &y[ib];\n+        const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+\n+        // extract the 5th bit via lookup table ((!b) << 4)\n+        memcpy(&qh0, x0->qh, sizeof(qh0));\n+        memcpy(&qh1, x1->qh, sizeof(qh1));\n+\n+        tmp0[0] = table_b2b_1[(qh0 >>  0) & 0xFF];\n+        tmp0[1] = table_b2b_1[(qh0 >>  8) & 0xFF];\n+        tmp0[2] = table_b2b_1[(qh0 >> 16) & 0xFF];\n+        tmp0[3] = table_b2b_1[(qh0 >> 24)       ];\n+\n+        tmp1[0] = table_b2b_1[(qh1 >>  0) & 0xFF];\n+        tmp1[1] = table_b2b_1[(qh1 >>  8) & 0xFF];\n+        tmp1[2] = table_b2b_1[(qh1 >> 16) & 0xFF];\n+        tmp1[3] = table_b2b_1[(qh1 >> 24)       ];\n+\n+        const int8x16_t qhl0 = vld1q_s8((const int8_t *)(tmp0 + 0));\n+        const int8x16_t qhh0 = vld1q_s8((const int8_t *)(tmp0 + 2));\n+        const int8x16_t qhl1 = vld1q_s8((const int8_t *)(tmp1 + 0));\n+        const int8x16_t qhh1 = vld1q_s8((const int8_t *)(tmp1 + 2));\n+\n+        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n+        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n+\n+        // 4-bit -> 8-bit\n+        int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+        int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+        int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+        int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+        // add high bit and sub 16 (equivalent to sub 0x10 when bit is zero)\n+        const int8x16_t v0_0lf = vsubq_s8(v0_0l, qhl0);\n+        const int8x16_t v0_0hf = vsubq_s8(v0_0h, qhh0);\n+        const int8x16_t v0_1lf = vsubq_s8(v0_1l, qhl1);\n+        const int8x16_t v0_1hf = vsubq_s8(v0_1h, qhh1);\n+\n+        // load y\n+        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n+        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n+        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n+        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_0lf, v1_0l),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_0hf, v1_0h))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_1lf, v1_1l),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_1hf, v1_1h))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n+\n+#endif\n+    for (; ib < nb; ++ib) {\n+        uint32_t qh;\n+        memcpy(&qh, x[ib].qh, sizeof(qh));\n+\n+        int sumi0 = 0;\n+        int sumi1 = 0;\n+\n+        for (int j = 0; j < qk/2; ++j) {\n+            const uint8_t xh_0 = ((qh & (1u << (j + 0 ))) >> (j + 0 )) << 4;\n+            const uint8_t xh_1 = ((qh & (1u << (j + 16))) >> (j + 12));\n+\n+            const int32_t x0 = (int8_t)(((x[ib].qs[j] & 0x0F) | xh_0) - 16);\n+            const int32_t x1 = (int8_t)(((x[ib].qs[j] >>   4) | xh_1) - 16);\n+\n+            sumi0 += (x0 * y[ib].qs[j]);\n+            sumi1 += (x1 * y[ib].qs[j + qk/2]);\n+        }\n+\n+        int sumi = sumi0 + sumi1;\n+        sumf += (GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d)) * sumi;\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_q5_1_q8_1(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_1;\n+    const int nb = n / qk;\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+    assert(n % qk == 0);\n+    assert(qk == QK5_1);\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q5_1 * GGML_RESTRICT x = vx;\n+    const block_q8_1 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    float summs0 = 0.0f;\n+    float summs1 = 0.0f;\n+\n+    uint32_t qh0;\n+    uint32_t qh1;\n+\n+    uint64_t tmp0[4];\n+    uint64_t tmp1[4];\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q5_1 * GGML_RESTRICT x0 = &x[ib];\n+        const block_q5_1 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_1 * GGML_RESTRICT y0 = &y[ib];\n+        const block_q8_1 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+\n+        summs0 += GGML_FP16_TO_FP32(x0->m) * GGML_FP16_TO_FP32(y0->s);\n+        summs1 += GGML_FP16_TO_FP32(x1->m) * GGML_FP16_TO_FP32(y1->s);\n+\n+        // extract the 5th bit via lookup table ((b) << 4)\n+        memcpy(&qh0, x0->qh, sizeof(qh0));\n+        memcpy(&qh1, x1->qh, sizeof(qh1));\n+\n+        tmp0[0] = table_b2b_0[(qh0 >>  0) & 0xFF];\n+        tmp0[1] = table_b2b_0[(qh0 >>  8) & 0xFF];\n+        tmp0[2] = table_b2b_0[(qh0 >> 16) & 0xFF];\n+        tmp0[3] = table_b2b_0[(qh0 >> 24)       ];\n+\n+        tmp1[0] = table_b2b_0[(qh1 >>  0) & 0xFF];\n+        tmp1[1] = table_b2b_0[(qh1 >>  8) & 0xFF];\n+        tmp1[2] = table_b2b_0[(qh1 >> 16) & 0xFF];\n+        tmp1[3] = table_b2b_0[(qh1 >> 24)       ];\n+\n+        const int8x16_t qhl0 = vld1q_s8((const int8_t *)(tmp0 + 0));\n+        const int8x16_t qhh0 = vld1q_s8((const int8_t *)(tmp0 + 2));\n+        const int8x16_t qhl1 = vld1q_s8((const int8_t *)(tmp1 + 0));\n+        const int8x16_t qhh1 = vld1q_s8((const int8_t *)(tmp1 + 2));\n+\n+        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n+        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n+\n+        // 4-bit -> 8-bit\n+        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+        // add high bit\n+        const int8x16_t v0_0lf = vorrq_s8(v0_0l, qhl0);\n+        const int8x16_t v0_0hf = vorrq_s8(v0_0h, qhh0);\n+        const int8x16_t v0_1lf = vorrq_s8(v0_1l, qhl1);\n+        const int8x16_t v0_1hf = vorrq_s8(v0_1h, qhh1);\n+\n+        // load y\n+        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n+        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n+        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n+        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_0lf, v1_0l),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_0hf, v1_0h))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_1lf, v1_1l),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_1hf, v1_1h))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1) + summs0 + summs1;\n+\n+#endif\n+    for (; ib < nb; ++ib) {\n+        uint32_t qh;\n+        memcpy(&qh, x[ib].qh, sizeof(qh));\n+\n+        int sumi0 = 0;\n+        int sumi1 = 0;\n+\n+        for (int j = 0; j < qk/2; ++j) {\n+            const uint8_t xh_0 = ((qh >> (j +  0)) << 4) & 0x10;\n+            const uint8_t xh_1 = ((qh >> (j + 12))     ) & 0x10;\n+\n+            const int32_t x0 = (x[ib].qs[j] & 0xF) | xh_0;\n+            const int32_t x1 = (x[ib].qs[j] >>  4) | xh_1;\n+\n+            sumi0 += (x0 * y[ib].qs[j]);\n+            sumi1 += (x1 * y[ib].qs[j + qk/2]);\n+        }\n+\n+        int sumi = sumi0 + sumi1;\n+        sumf += (GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d))*sumi + GGML_FP16_TO_FP32(x[ib].m)*GGML_FP16_TO_FP32(y[ib].s);\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_q8_0_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_0;\n+    const int nb = n / qk;\n+\n+    assert(n % qk == 0);\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    assert((nrc == 2) || (nrc == 1));\n+#else\n+    assert(nrc == 1);\n+#endif\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q8_0 * GGML_RESTRICT x = vx;\n+    const block_q8_0 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    if (nrc == 2) {\n+        const block_q8_0 * GGML_RESTRICT vx0 = vx;\n+        const block_q8_0 * GGML_RESTRICT vx1 = (const block_q8_0 *) ((const uint8_t*)vx + bx);\n+        const block_q8_0 * GGML_RESTRICT vy0 = vy;\n+        const block_q8_0 * GGML_RESTRICT vy1 = (const block_q8_0 *) ((const uint8_t*)vy + by);\n+\n+        float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+\n+        for (int i = 0; i < nb; i++) {\n+            const block_q8_0 * GGML_RESTRICT b_x0 = &vx0[i];\n+            const block_q8_0 * GGML_RESTRICT b_y0 = &vy0[i];\n+\n+            const block_q8_0 * GGML_RESTRICT b_x1 = &vx1[i];\n+            const block_q8_0 * GGML_RESTRICT b_y1 = &vy1[i];\n+\n+            const int8x16_t x0_l = vld1q_s8(b_x0->qs);\n+            const int8x16_t x0_h = vld1q_s8(b_x0->qs + 16);\n+            const int8x16_t x1_l = vld1q_s8(b_x1->qs);\n+            const int8x16_t x1_h = vld1q_s8(b_x1->qs + 16);\n+\n+            // load y\n+            const int8x16_t y0_l = vld1q_s8(b_y0->qs);\n+            const int8x16_t y0_h = vld1q_s8(b_y0->qs + 16);\n+            const int8x16_t y1_l = vld1q_s8(b_y1->qs);\n+            const int8x16_t y1_h = vld1q_s8(b_y1->qs + 16);\n+\n+            float32_t _scale[4] = {\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y1->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y1->d)\n+            };\n+            float32x4_t scale = vld1q_f32(_scale);\n+\n+            int8x16_t l0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+            int8x16_t l1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+\n+            int8x16_t l2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+            int8x16_t l3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+\n+            int8x16_t r0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+            int8x16_t r1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+\n+            int8x16_t r2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+            int8x16_t r3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+\n+            sumv0 = vmlaq_f32(sumv0,(vcvtq_f32_s32(vmmlaq_s32((vmmlaq_s32((vmmlaq_s32((vmmlaq_s32(vdupq_n_s32(0), l0, r0)),\n+                                                l1, r1)), l2, r2)), l3, r3))), scale);\n+        }\n+\n+        float32x4_t sumv1 = vextq_f32 (sumv0, sumv0, 2);\n+        float32x4_t sumv2 = vzip1q_f32(sumv0, sumv1);\n+\n+        vst1_f32(s,      vget_low_f32 (sumv2));\n+        vst1_f32(s + bs, vget_high_f32(sumv2));\n+\n+        return;\n+    }\n+#endif\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+#if defined(__ARM_FEATURE_SVE)\n+    svfloat32_t sumv0 = svdup_n_f32(0.0f);\n+    svfloat32_t sumv1 = svdup_n_f32(0.0f);\n+\n+    const int vector_length = ggml_cpu_get_sve_cnt()*8;\n+\n+    //VLA Implemenation for SVE\n+    switch (vector_length) {\n+        case 128:\n+            {\n+                // predicate for activating lanes for 16 Int8 elements\n+                const svbool_t ph16 = svptrue_pat_b8 (SV_VL16);\n+                const svbool_t pl16 = svptrue_pat_b32(SV_VL4);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q8_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svint8_t qx0_0 = svld1_s8(ph16, x0->qs);\n+                    const svint8_t qx0_1 = svld1_s8(ph16, x0->qs+16);\n+                    const svint8_t qx1_0 = svld1_s8(ph16, x1->qs);\n+                    const svint8_t qx1_1 = svld1_s8(ph16, x1->qs+16);\n+\n+                    // load y\n+                    const svint8_t qy0_0 = svld1_s8(ph16, y0->qs);\n+                    const svint8_t qy0_1 = svld1_s8(ph16, y0->qs+16);\n+                    const svint8_t qy1_0 = svld1_s8(ph16, y1->qs);\n+                    const svint8_t qy1_1 = svld1_s8(ph16, y1->qs+16);\n+\n+                    sumv0 = svmla_n_f32_x(pl16, sumv0, svcvt_f32_s32_x(pl16, svadd_x(pl16,\n+                                    svdot_s32(svdup_n_s32(0), qx0_0, qy0_0),\n+                                    svdot_s32(svdup_n_s32(0), qx0_1, qy0_1))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(pl16, sumv1, svcvt_f32_s32_x(pl16, svadd_x(pl16,\n+                                    svdot_s32(svdup_n_s32(0), qx1_0, qy1_0),\n+                                    svdot_s32(svdup_n_s32(0), qx1_1, qy1_1))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(pl16, svadd_f32_x(pl16, sumv0, sumv1));\n+            } break;\n+        case 256:\n+            {\n+                //printf(\"sve256\");\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q8_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svint8_t qx0 = svld1_s8(svptrue_b8(), x0->qs);\n+                    const svint8_t qx1 = svld1_s8(svptrue_b8(), x1->qs);\n+\n+                    // load y\n+                    const svint8_t qy0 = svld1_s8(svptrue_b8(), y0->qs);\n+                    const svint8_t qy1 = svld1_s8(svptrue_b8(), y1->qs);\n+\n+                    sumv0 = svmla_n_f32_x(svptrue_b32(), sumv0, svcvt_f32_s32_x(svptrue_b32(),\n+                                svdot_s32(svdup_n_s32(0), qx0, qy0)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(svptrue_b32(), sumv1, svcvt_f32_s32_x(svptrue_b32(),\n+                                svdot_s32(svdup_n_s32(0), qx1, qy1)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(svptrue_b32(), svadd_f32_x(svptrue_b32(), sumv0, sumv1));\n+            } break;\n+        case 512:\n+            {\n+                // predicate for activating high 256 bit\n+                const svbool_t ph32 = svptrue_pat_b8(SV_VL32);\n+                // predicate for activating low 256 bit\n+                const svbool_t pl32 = svnot_b_z(svptrue_b8(), ph32);\n+\n+                // predicate for activating high lanes for 8 float32 elements\n+                const svbool_t ph8 = svptrue_pat_b32(SV_VL8);\n+                // predicate for activating low lanes for 8 float32 elements\n+                const svbool_t pl8 = svnot_b_z(svptrue_b32(), ph8);\n+\n+                svfloat32_t sumv00 = svdup_n_f32(0.0f);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q8_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    //load 32 int8_t in first half of vector and put another 32 int8_t in second vector lower bits\n+                    // and add them to make one 64 element vector\n+                    // load x\n+                    const svint8_t qx_32 = svld1_s8(ph32, x0->qs);\n+                          svint8_t qx_64 = svld1_s8(pl32, x0->qs + 2);\n+\n+                    qx_64 = svadd_s8_x(svptrue_b8(), qx_32, qx_64);\n+\n+                    // load y\n+                    const svint8_t qy_32 = svld1_s8(ph32, y0->qs);\n+                          svint8_t qy_64 = svld1_s8(pl32, y0->qs + 2);\n+\n+                    qy_64 = svadd_s8_x(svptrue_b8(), qy_32, qy_64);\n+\n+                    // scale creation\n+                    const float32_t deq1 = GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d);\n+                    const float32_t deq2 = GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d);\n+\n+                    // duplicate deq1 in first half of vector and deq2 in second half of vector\n+                    const svfloat32_t temp = svdup_f32_m(svdup_f32_z(ph8, deq1), pl8, deq2);\n+\n+                    const svfloat32_t sumvt = svcvt_f32_s32_x(svptrue_b32(), svdot_s32(svdup_n_s32(0), qx_64, qy_64));\n+\n+                    sumv00 = svmla_f32_m(svptrue_b32(), sumv00, sumvt, temp);\n+                }\n+\n+                sumf = svaddv_f32(svptrue_b32(), sumv00);\n+                break;\n+            }\n+        default:\n+            assert(false && \"Unsupported vector length\");\n+            break;\n+    }\n+#elif defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q8_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+        const block_q8_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+        const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        const int8x16_t x0_0 = vld1q_s8(x0->qs);\n+        const int8x16_t x0_1 = vld1q_s8(x0->qs + 16);\n+        const int8x16_t x1_0 = vld1q_s8(x1->qs);\n+        const int8x16_t x1_1 = vld1q_s8(x1->qs + 16);\n+\n+        // load y\n+        const int8x16_t y0_0 = vld1q_s8(y0->qs);\n+        const int8x16_t y0_1 = vld1q_s8(y0->qs + 16);\n+        const int8x16_t y1_0 = vld1q_s8(y1->qs);\n+        const int8x16_t y1_1 = vld1q_s8(y1->qs + 16);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), x0_0, y0_0),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), x0_1, y0_1))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), x1_0, y1_0),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), x1_1, y1_1))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n+#endif\n+    for (; ib < nb; ++ib) {\n+        int sumi = 0;\n+\n+        for (int j = 0; j < qk; j++) {\n+            sumi += x[ib].qs[j]*y[ib].qs[j];\n+        }\n+\n+        sumf += sumi*(GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d));\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_tq1_0_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_tq1_0 * GGML_RESTRICT x = vx;\n+    const block_q8_K  * GGML_RESTRICT y = vy;\n+\n+    const int nb = n / QK_K;\n+\n+#if defined(__ARM_NEON)\n+    float sumf = 0.0f;\n+\n+    uint8_t k_shift[16] = {1, 1, 1, 1, 3, 3, 3, 3, 9, 9, 9, 9, 27, 27, 27, 27};\n+\n+    const uint8x16_t shift = vld1q_u8(k_shift);\n+\n+    for (int i = 0; i < nb; ++i) {\n+#if defined(__ARM_FEATURE_DOTPROD)\n+        int32x4_t sumi0 = vdupq_n_s32(0);\n+        int32x4_t sumi1 = vdupq_n_s32(0);\n+#else\n+        int16x8_t sumi0 = vdupq_n_s16(0);\n+        int16x8_t sumi1 = vdupq_n_s16(0);\n+#endif\n+\n+        // first 32 bytes of 5 elements\n+        {\n+            uint8x16_t qx0 = vld1q_u8(x[i].qs + 0);\n+            uint8x16_t qx1 = vld1q_u8(x[i].qs + 16);\n+            uint8x16_t qx2 = vmulq_u8(qx0, vdupq_n_u8(3));\n+            uint8x16_t qx3 = vmulq_u8(qx1, vdupq_n_u8(3));\n+            uint8x16_t qx4 = vmulq_u8(qx0, vdupq_n_u8(9));\n+            uint8x16_t qx5 = vmulq_u8(qx1, vdupq_n_u8(9));\n+            uint8x16_t qx6 = vmulq_u8(qx0, vdupq_n_u8(27));\n+            uint8x16_t qx7 = vmulq_u8(qx1, vdupq_n_u8(27));\n+            uint8x16_t qx8 = vmulq_u8(qx0, vdupq_n_u8(81));\n+            uint8x16_t qx9 = vmulq_u8(qx1, vdupq_n_u8(81));\n+\n+            // multiply by 3 and keep the 2 bits above 8 bits\n+            int8x16_t sqx0 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx0, vshrq_n_u8(qx0, 1)), 6));\n+            int8x16_t sqx1 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx1, vshrq_n_u8(qx1, 1)), 6));\n+            int8x16_t sqx2 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx2, vshrq_n_u8(qx2, 1)), 6));\n+            int8x16_t sqx3 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx3, vshrq_n_u8(qx3, 1)), 6));\n+            int8x16_t sqx4 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx4, vshrq_n_u8(qx4, 1)), 6));\n+            int8x16_t sqx5 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx5, vshrq_n_u8(qx5, 1)), 6));\n+            int8x16_t sqx6 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx6, vshrq_n_u8(qx6, 1)), 6));\n+            int8x16_t sqx7 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx7, vshrq_n_u8(qx7, 1)), 6));\n+            int8x16_t sqx8 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx8, vshrq_n_u8(qx8, 1)), 6));\n+            int8x16_t sqx9 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx9, vshrq_n_u8(qx9, 1)), 6));\n+\n+            const int8x16_t qy0 = vld1q_s8(y[i].qs +   0);\n+            const int8x16_t qy1 = vld1q_s8(y[i].qs +  16);\n+            const int8x16_t qy2 = vld1q_s8(y[i].qs +  32);\n+            const int8x16_t qy3 = vld1q_s8(y[i].qs +  48);\n+            const int8x16_t qy4 = vld1q_s8(y[i].qs +  64);\n+            const int8x16_t qy5 = vld1q_s8(y[i].qs +  80);\n+            const int8x16_t qy6 = vld1q_s8(y[i].qs +  96);\n+            const int8x16_t qy7 = vld1q_s8(y[i].qs + 112);\n+            const int8x16_t qy8 = vld1q_s8(y[i].qs + 128);\n+            const int8x16_t qy9 = vld1q_s8(y[i].qs + 144);\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+            sumi0 = vdotq_s32(sumi0, sqx0, qy0);\n+            sumi1 = vdotq_s32(sumi1, sqx1, qy1);\n+            sumi0 = vdotq_s32(sumi0, sqx2, qy2);\n+            sumi1 = vdotq_s32(sumi1, sqx3, qy3);\n+            sumi0 = vdotq_s32(sumi0, sqx4, qy4);\n+            sumi1 = vdotq_s32(sumi1, sqx5, qy5);\n+            sumi0 = vdotq_s32(sumi0, sqx6, qy6);\n+            sumi1 = vdotq_s32(sumi1, sqx7, qy7);\n+            sumi0 = vdotq_s32(sumi0, sqx8, qy8);\n+            sumi1 = vdotq_s32(sumi1, sqx9, qy9);\n+#else\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx0), vget_low_s8(qy0));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx0), vget_high_s8(qy0));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx1), vget_low_s8(qy1));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx1), vget_high_s8(qy1));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx2), vget_low_s8(qy2));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx2), vget_high_s8(qy2));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx3), vget_low_s8(qy3));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx3), vget_high_s8(qy3));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx4), vget_low_s8(qy4));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx4), vget_high_s8(qy4));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx5), vget_low_s8(qy5));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx5), vget_high_s8(qy5));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx6), vget_low_s8(qy6));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx6), vget_high_s8(qy6));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx7), vget_low_s8(qy7));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx7), vget_high_s8(qy7));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx8), vget_low_s8(qy8));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx8), vget_high_s8(qy8));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx9), vget_low_s8(qy9));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx9), vget_high_s8(qy9));\n+#endif\n+        }\n+\n+        // last 16 bytes of 5-element, along with the 4 bytes of 4 elements\n+        {\n+            uint8x16_t qx0 = vld1q_u8(x[i].qs + 32);\n+            uint8x16_t qx1 = vmulq_u8(qx0, vdupq_n_u8(3));\n+            uint8x16_t qx2 = vmulq_u8(qx0, vdupq_n_u8(9));\n+            uint8x16_t qx3 = vmulq_u8(qx0, vdupq_n_u8(27));\n+            uint8x16_t qx4 = vmulq_u8(qx0, vdupq_n_u8(81));\n+            uint32_t qh;\n+            memcpy(&qh, x[i].qh, sizeof(qh)); // potentially unaligned\n+            uint8x16_t qx5 = vreinterpretq_u8_u32(vdupq_n_u32(qh));\n+            qx5 = vmulq_u8(qx5, shift);\n+\n+            // multiply by 3 and keep the 2 bits above 8 bits\n+            int8x16_t sqx0 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx0, vshrq_n_u8(qx0, 1)), 6));\n+            int8x16_t sqx1 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx1, vshrq_n_u8(qx1, 1)), 6));\n+            int8x16_t sqx2 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx2, vshrq_n_u8(qx2, 1)), 6));\n+            int8x16_t sqx3 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx3, vshrq_n_u8(qx3, 1)), 6));\n+            int8x16_t sqx4 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx4, vshrq_n_u8(qx4, 1)), 6));\n+            int8x16_t sqx5 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx5, vshrq_n_u8(qx5, 1)), 6));\n+\n+            const int8x16_t qy0 = vld1q_s8(y[i].qs + 160);\n+            const int8x16_t qy1 = vld1q_s8(y[i].qs + 176);\n+            const int8x16_t qy2 = vld1q_s8(y[i].qs + 192);\n+            const int8x16_t qy3 = vld1q_s8(y[i].qs + 208);\n+            const int8x16_t qy4 = vld1q_s8(y[i].qs + 224);\n+            const int8x16_t qy5 = vld1q_s8(y[i].qs + 240);\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+            sumi0 = vdotq_s32(sumi0, sqx0, qy0);\n+            sumi1 = vdotq_s32(sumi1, sqx1, qy1);\n+            sumi0 = vdotq_s32(sumi0, sqx2, qy2);\n+            sumi1 = vdotq_s32(sumi1, sqx3, qy3);\n+            sumi0 = vdotq_s32(sumi0, sqx4, qy4);\n+            sumi1 = vdotq_s32(sumi1, sqx5, qy5);\n+#else\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx0), vget_low_s8(qy0));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx0), vget_high_s8(qy0));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx1), vget_low_s8(qy1));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx1), vget_high_s8(qy1));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx2), vget_low_s8(qy2));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx2), vget_high_s8(qy2));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx3), vget_low_s8(qy3));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx3), vget_high_s8(qy3));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx4), vget_low_s8(qy4));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx4), vget_high_s8(qy4));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx5), vget_low_s8(qy5));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx5), vget_high_s8(qy5));\n+#endif\n+        }\n+\n+        const int16x8_t ysum0 = vld1q_s16(y[i].bsums);\n+        const int16x8_t ysum1 = vld1q_s16(y[i].bsums + 8);\n+\n+        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+        sumi0 = vaddq_s32(sumi0, sumi1);\n+        sumi0 = vsubq_s32(sumi0, vpaddlq_s16(vaddq_s16(ysum0, ysum1)));\n+\n+        sumf += d * (float) vaddvq_s32(sumi0);\n+#else\n+        sumi0 = vaddq_s16(sumi0, sumi1);\n+        sumi0 = vsubq_s16(sumi0, vaddq_s16(ysum0, ysum1));\n+\n+        sumf += d * (float) vaddlvq_s16(sumi0);\n+#endif\n+    }\n+\n+    *s = sumf;\n+\n+#else\n+    const uint8_t pow3[6] = {1, 3, 9, 27, 81, 243};\n+\n+    float sumf = 0.0f;\n+\n+    for (int i = 0; i < nb; ++i) {\n+        int sum = 0;\n+\n+        for (size_t j = 0; j < sizeof(x->qs) - sizeof(x->qs) % 32; j += 32) {\n+            for (size_t l = 0; l < 5; ++l) {\n+                for (size_t m = 0; m < 32; ++m) {\n+                    uint8_t q = x[i].qs[j + m] * pow3[l];\n+                    uint16_t xi = ((uint16_t) q * 3) >> 8;\n+                    sum += (xi - 1) * y[i].qs[j*5 + l*32 + m];\n+                }\n+            }\n+        }\n+        for (size_t j = sizeof(x->qs) - sizeof(x->qs) % 32; j < sizeof(x->qs); j += 16) {\n+            for (size_t l = 0; l < 5; ++l) {\n+                for (size_t m = 0; m < 16; ++m) {\n+                    uint8_t q = x[i].qs[j + m] * pow3[l];\n+                    uint16_t xi = ((uint16_t) q * 3) >> 8;\n+                    sum += (xi - 1) * y[i].qs[j*5 + l*16 + m];\n+                }\n+            }\n+        }\n+\n+        for (size_t l = 0; l < 4; ++l) {\n+            for (size_t j = 0; j < sizeof(x->qh); ++j) {\n+                uint8_t q = x[i].qh[j] * pow3[l];\n+                uint16_t xi = ((uint16_t) q * 3) >> 8;\n+                sum += (xi - 1) * y[i].qs[sizeof(x->qs)*5 + l*sizeof(x->qh) + j];\n+            }\n+        }\n+\n+        sumf += (float) sum * (GGML_FP16_TO_FP32(x[i].d) * y[i].d);\n+    }\n+\n+    *s = sumf;\n+#endif\n+}\n+\n+void ggml_vec_dot_tq2_0_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_tq2_0 * GGML_RESTRICT x = vx;\n+    const block_q8_K  * GGML_RESTRICT y = vy;\n+\n+    const int nb = n / QK_K;\n+\n+#if defined(__ARM_NEON)\n+    float sumf = 0.0f;\n+\n+    const uint8x16_t m3 = vdupq_n_u8(3);\n+\n+    for (int i = 0; i < nb; ++i) {\n+#if defined(__ARM_FEATURE_DOTPROD)\n+        int32x4_t sumi0 = vdupq_n_s32(0);\n+        int32x4_t sumi1 = vdupq_n_s32(0);\n+#else\n+        int16x8_t sumi0 = vdupq_n_s16(0);\n+        int16x8_t sumi1 = vdupq_n_s16(0);\n+#endif\n+\n+        for (size_t j = 0; j < sizeof(x->qs); j += 32) {\n+            uint8x16_t qx0 = vld1q_u8(x[i].qs + j);\n+            uint8x16_t qx1 = vld1q_u8(x[i].qs + j + 16);\n+            uint8x16_t qx2 = vshrq_n_u8(qx0, 2);\n+            uint8x16_t qx3 = vshrq_n_u8(qx1, 2);\n+            uint8x16_t qx4 = vshrq_n_u8(qx0, 4);\n+            uint8x16_t qx5 = vshrq_n_u8(qx1, 4);\n+            uint8x16_t qx6 = vshrq_n_u8(qx0, 6);\n+            uint8x16_t qx7 = vshrq_n_u8(qx1, 6);\n+\n+            int8x16_t sqx0 = vreinterpretq_s8_u8(vandq_u8(qx0, m3));\n+            int8x16_t sqx1 = vreinterpretq_s8_u8(vandq_u8(qx1, m3));\n+            int8x16_t sqx2 = vreinterpretq_s8_u8(vandq_u8(qx2, m3));\n+            int8x16_t sqx3 = vreinterpretq_s8_u8(vandq_u8(qx3, m3));\n+            int8x16_t sqx4 = vreinterpretq_s8_u8(vandq_u8(qx4, m3));\n+            int8x16_t sqx5 = vreinterpretq_s8_u8(vandq_u8(qx5, m3));\n+            int8x16_t sqx6 = vreinterpretq_s8_u8(vandq_u8(qx6, m3));\n+            int8x16_t sqx7 = vreinterpretq_s8_u8(vandq_u8(qx7, m3));\n+\n+            const int8x16_t qy0 = vld1q_s8(y[i].qs + j*4 +   0);\n+            const int8x16_t qy1 = vld1q_s8(y[i].qs + j*4 +  16);\n+            const int8x16_t qy2 = vld1q_s8(y[i].qs + j*4 +  32);\n+            const int8x16_t qy3 = vld1q_s8(y[i].qs + j*4 +  48);\n+            const int8x16_t qy4 = vld1q_s8(y[i].qs + j*4 +  64);\n+            const int8x16_t qy5 = vld1q_s8(y[i].qs + j*4 +  80);\n+            const int8x16_t qy6 = vld1q_s8(y[i].qs + j*4 +  96);\n+            const int8x16_t qy7 = vld1q_s8(y[i].qs + j*4 + 112);\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+            sumi0 = vdotq_s32(sumi0, sqx0, qy0);\n+            sumi1 = vdotq_s32(sumi1, sqx1, qy1);\n+            sumi0 = vdotq_s32(sumi0, sqx2, qy2);\n+            sumi1 = vdotq_s32(sumi1, sqx3, qy3);\n+            sumi0 = vdotq_s32(sumi0, sqx4, qy4);\n+            sumi1 = vdotq_s32(sumi1, sqx5, qy5);\n+            sumi0 = vdotq_s32(sumi0, sqx6, qy6);\n+            sumi1 = vdotq_s32(sumi1, sqx7, qy7);\n+#else\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx0), vget_low_s8(qy0));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx0), vget_high_s8(qy0));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx1), vget_low_s8(qy1));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx1), vget_high_s8(qy1));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx2), vget_low_s8(qy2));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx2), vget_high_s8(qy2));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx3), vget_low_s8(qy3));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx3), vget_high_s8(qy3));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx4), vget_low_s8(qy4));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx4), vget_high_s8(qy4));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx5), vget_low_s8(qy5));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx5), vget_high_s8(qy5));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx6), vget_low_s8(qy6));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx6), vget_high_s8(qy6));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx7), vget_low_s8(qy7));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx7), vget_high_s8(qy7));\n+#endif\n+        }\n+\n+        const int16x8_t ysum0 = vld1q_s16(y[i].bsums);\n+        const int16x8_t ysum1 = vld1q_s16(y[i].bsums + 8);\n+\n+        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+        sumi0 = vaddq_s32(sumi0, sumi1);\n+        sumi0 = vsubq_s32(sumi0, vpaddlq_s16(vaddq_s16(ysum0, ysum1)));\n+\n+        sumf += d * (float) vaddvq_s32(sumi0);\n+#else\n+        sumi0 = vaddq_s16(sumi0, sumi1);\n+        sumi0 = vsubq_s16(sumi0, vaddq_s16(ysum0, ysum1));\n+\n+        sumf += d * (float) vaddlvq_s16(sumi0);\n+#endif\n+    }\n+\n+    *s = sumf;\n+\n+#else\n+    float sumf = 0.0f;\n+\n+    for (int i = 0; i < nb; ++i) {\n+        int32_t sumi = 0;\n+\n+        for (size_t j = 0; j < sizeof(x->qs); j += 32) {\n+            for (size_t l = 0; l < 4; ++l) {\n+                for (size_t k = 0; k < 32; ++k) {\n+                    sumi += y[i].qs[j*4 + l*32 + k] * (((x[i].qs[j + k] >> (l*2)) & 3) - 1);\n+                }\n+            }\n+        }\n+\n+        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+\n+        sumf += (float) sumi * d;\n+    }\n+\n+    *s = sumf;\n+#endif\n+}\n+\n+void ggml_vec_dot_q2_K_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q2_K * GGML_RESTRICT x = vx;\n+    const block_q8_K * GGML_RESTRICT y = vy;\n+\n+    const int nb = n / QK_K;\n+\n+#ifdef __ARM_FEATURE_SVE\n+    const int vector_length = svcntb()*8;\n+    const svuint8_t m3s = svdup_n_u8(0x3);\n+    const svuint32_t m4s = svdup_n_u32(0xF);\n+    const svint32_t vzero_sv = svdup_n_s32(0);\n+    svfloat32_t acc_sum = svdup_n_f32(0);\n+    svbool_t pred_s32 = svptrue_pat_b32(SV_VL4);\n+\n+    switch (vector_length) {\n+        case 128:\n+            for (int i = 0; i < nb; ++i) {\n+                const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+                svfloat32_t d_broad = svdup_n_f32((float32_t)d);\n+                const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n+                svfloat32_t dmin_broad = svdup_n_f32((float32_t)dmin);\n+\n+                const uint8_t * GGML_RESTRICT q2 = x[i].qs;\n+                const int8_t  * GGML_RESTRICT q8_sv = y[i].qs;\n+                const uint8_t * GGML_RESTRICT sc = x[i].scales;\n+\n+                svuint32_t mins_and_scales_sve = svld1ub_u32(svptrue_b32(), sc);\n+                const svint32_t mins_sv_1 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_b32(), mins_and_scales_sve, 4));\n+\n+                mins_and_scales_sve = svld1ub_u32(svptrue_b32(), sc+4);\n+                const svint32_t mins_sv_2 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_b32(), mins_and_scales_sve, 4));\n+\n+                svint32_t q8sums_sv_1 = svld1sh_s32(svptrue_b32(), y[i].bsums);\n+                svint32_t q8sums_sv_2 = svld1sh_s32(svptrue_b32(), y[i].bsums+4);\n+\n+                const svint32_t s0 = svadd_s32_x(svptrue_b32(), svmul_s32_x(svptrue_b32(), mins_sv_1, q8sums_sv_1), svmul_s32_x(svptrue_b32(), mins_sv_2, q8sums_sv_2));\n+\n+                mins_and_scales_sve = svld1ub_u32(svptrue_b32(), sc+8);\n+                const svint32_t mins_sv_3 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_b32(), mins_and_scales_sve, 4));\n+\n+                mins_and_scales_sve = svld1ub_u32(svptrue_b32(), sc+12);\n+                const svint32_t mins_sv_4 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_b32(), mins_and_scales_sve, 4));\n+\n+                q8sums_sv_1 = svld1sh_s32(svptrue_b32(), y[i].bsums+8);\n+                q8sums_sv_2 = svld1sh_s32(svptrue_b32(), y[i].bsums+12);\n+\n+                svint32_t s1 = svadd_s32_x(svptrue_b32(), svmul_s32_x(svptrue_b32(), mins_sv_3, q8sums_sv_1), svmul_s32_x(svptrue_b32(), mins_sv_4, q8sums_sv_2));\n+\n+                svfloat32_t temp = svcvt_f32_s32_x(svptrue_b32(), svadd_s32_x(svptrue_b32(), s0, s1));\n+\n+                acc_sum = svmla_f32_m(svptrue_b32(), acc_sum, temp, dmin_broad);\n+\n+                svint32_t sumi1 = svdup_n_s32(0);\n+\n+                {\n+                    const svuint8_t q2bits_1 = svld1_u8(svptrue_b8(), q2);\n+                    svint8_t q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), q2bits_1, m3s));\n+                    svint8_t q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                    const svint32_t scales_sv = svreinterpret_s32_u32(svand_u32_m(svptrue_b32(), svld1ub_u32(svptrue_b32(), sc), m4s));\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv, 0));\n+\n+                    const svuint8_t q2bits_3 = svld1_u8(svptrue_b8(), q2+16);\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), q2bits_3, m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv, 1));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_1, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv, 2));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_3, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv, 3));\n+\n+\n+                    const svint32_t scales_sv_1 = svreinterpret_s32_u32(svand_u32_m(svptrue_b32(), svld1ub_u32(svptrue_b32(), sc+4), m4s));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_1, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_1, 0));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_3, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_1, 1));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_1, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_1, 2));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_3, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_1, 3));\n+\n+                    //-------------------------------\n+\n+                    q2 += 32;\n+                    const svint32_t scales_sv_2 = svreinterpret_s32_u32(svand_u32_m(svptrue_b32(), svld1ub_u32(svptrue_b32(), sc+8), m4s));\n+                    const svuint8_t q2bits_2 = svld1_u8(svptrue_b8(), q2);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), q2bits_2, m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_2, 0));\n+\n+                    const svuint8_t q2bits_4 = svld1_u8(svptrue_b8(), q2+16);\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), q2bits_4, m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_2, 1));\n+\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_2, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_2, 2));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_4, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_2, 3));\n+\n+\n+                    const svint32_t scales_sv_3 = svreinterpret_s32_u32(svand_u32_m(svptrue_b32(), svld1ub_u32(svptrue_b32(), sc+12), m4s));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_2, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_3, 0));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_4, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_3, 1));\n+\n+\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_2, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_3, 2));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_4, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_3, 3));\n+                }\n+                acc_sum = svmla_f32_m(svptrue_b32(), acc_sum, svcvt_f32_s32_x(svptrue_b32(), sumi1), d_broad);\n+            }\n+            *s = svaddv_f32(svptrue_b32(), acc_sum);\n+            break;\n+\n+        case 256:\n+        case 512:\n+            for (int i = 0; i < nb; ++i) {\n+                const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+                svfloat32_t d_broad = svdup_n_f32((float32_t)d);\n+                const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n+                svfloat32_t dmin_broad = svdup_n_f32((float32_t)dmin);\n+\n+                const uint8_t * GGML_RESTRICT q2 = x[i].qs;\n+                const int8_t  * GGML_RESTRICT q8_sv = y[i].qs;\n+                const uint8_t * GGML_RESTRICT sc = x[i].scales;\n+\n+                const svuint32_t mins_and_scales_sve = svld1ub_u32(svptrue_pat_b32(SV_VL8), sc); sc += 8;\n+                const svint32_t scales_sv = svreinterpret_s32_u32(svand_u32_m(svptrue_pat_b32(SV_VL8), mins_and_scales_sve, m4s));\n+                const svint32_t mins_sv_1 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_pat_b32(SV_VL8), mins_and_scales_sve, 4));\n+                svint32_t q8sums_sv_1 = svld1sh_s32(svptrue_pat_b32(SV_VL8), y[i].bsums);\n+\n+                const svuint32_t mins_and_scales_sve_1 = svld1ub_u32(svptrue_pat_b32(SV_VL8), sc);\n+                const svint32_t scales_sv_1 = svreinterpret_s32_u32(svand_u32_m(svptrue_pat_b32(SV_VL8), mins_and_scales_sve_1, m4s));\n+                const svint32_t mins_sv_2 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_pat_b32(SV_VL8), mins_and_scales_sve_1, 4));\n+\n+                svint32_t q8sums_sv_2 = svld1sh_s32(svptrue_pat_b32(SV_VL8), y[i].bsums+8);\n+\n+                svfloat32_t temp = svcvt_f32_s32_x(svptrue_pat_b32(SV_VL8), svadd_s32_x(svptrue_pat_b32(SV_VL8), svmul_s32_x(svptrue_pat_b32(SV_VL8), mins_sv_1, q8sums_sv_1), svmul_s32_x(svptrue_pat_b32(SV_VL8), mins_sv_2, q8sums_sv_2)));\n+\n+                acc_sum = svmla_f32_m(svptrue_pat_b32(SV_VL8), acc_sum, temp, dmin_broad);\n+\n+                svint32_t sumi1 = svdup_n_s32(0);\n+\n+                {\n+                    const svuint8_t q2bits_1 = svld1_u8(svptrue_pat_b8(SV_VL32), q2);\n+                    svint8_t q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), q2bits_1, m3s));\n+                    svint8_t q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    svint32_t scale_1 = svsel(pred_s32, svdup_lane_s32(scales_sv, 0), svdup_lane_s32(scales_sv, 1));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_1);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_1, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    svint32_t scale_2 = svsel(pred_s32, svdup_lane_s32(scales_sv, 2), svdup_lane_s32(scales_sv, 3));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(svdup_n_s32(0), q2bytes_sv, q8bytes_sv), scale_2);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_1, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_1 = svsel(pred_s32, svdup_lane_s32(scales_sv, 4), svdup_lane_s32(scales_sv, 5));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_1);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_1, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_2 = svsel(pred_s32, svdup_lane_s32(scales_sv, 6), svdup_lane_s32(scales_sv, 7));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_2);\n+\n+                    q2 += 32;\n+\n+                    const svuint8_t q2bits_2 = svld1_u8(svptrue_pat_b8(SV_VL32), q2);\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), q2bits_2, m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_1 = svsel(pred_s32, svdup_lane_s32(scales_sv_1, 0), svdup_lane_s32(scales_sv_1, 1));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_1);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_2, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_2 = svsel(pred_s32, svdup_lane_s32(scales_sv_1, 2), svdup_lane_s32(scales_sv_1, 3));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_2);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_2, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_1 = svsel(pred_s32, svdup_lane_s32(scales_sv_1, 4), svdup_lane_s32(scales_sv_1, 5));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_1);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_2, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_2 = svsel(pred_s32, svdup_lane_s32(scales_sv_1, 6), svdup_lane_s32(scales_sv_1, 7));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_2);\n+                }\n+                acc_sum = svmla_f32_m(svptrue_pat_b32(SV_VL8), acc_sum, svcvt_f32_s32_x(svptrue_pat_b32(SV_VL8), sumi1), d_broad);\n+            }\n+            *s = svaddv_f32(svptrue_pat_b32(SV_VL8), acc_sum);\n+            break;\n+\n+        default:\n+            assert(false && \"Unsupported vector length\");\n+            break;\n+    }\n+\n+#elif __ARM_NEON\n+    const uint8x16_t m3 = vdupq_n_u8(0x3);\n+    const uint8x16_t m4 = vdupq_n_u8(0xF);\n+\n+    const int32x4_t vzero = vdupq_n_s32(0);\n+\n+    ggml_int8x16x2_t q2bytes;\n+    uint8_t aux[16];\n+\n+    float sum = 0;\n+\n+    for (int i = 0; i < nb; ++i) {\n+        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n+\n+        const uint8_t * GGML_RESTRICT q2 = x[i].qs;\n+        const int8_t  * GGML_RESTRICT q8 = y[i].qs;\n+        const uint8_t * GGML_RESTRICT sc = x[i].scales;\n+\n+        const uint8x16_t mins_and_scales = vld1q_u8(sc);\n+        const uint8x16_t scales = vandq_u8(mins_and_scales, m4);\n+        vst1q_u8(aux, scales);\n+\n+        const uint8x16_t mins = vshrq_n_u8(mins_and_scales, 4);\n+        const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\n+        const ggml_int16x8x2_t mins16 = {{vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(mins))), vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(mins)))}};\n+        const int32x4_t s0 = vaddq_s32(vmull_s16(vget_low_s16 (mins16.val[0]), vget_low_s16 (q8sums.val[0])),\n+                                       vmull_s16(vget_high_s16(mins16.val[0]), vget_high_s16(q8sums.val[0])));\n+        const int32x4_t s1 = vaddq_s32(vmull_s16(vget_low_s16 (mins16.val[1]), vget_low_s16 (q8sums.val[1])),\n+                                       vmull_s16(vget_high_s16(mins16.val[1]), vget_high_s16(q8sums.val[1])));\n+        sum += dmin * vaddvq_s32(vaddq_s32(s0, s1));\n+\n+        int isum = 0;\n+        int is = 0;\n+\n+// We use this macro instead of a function call because for some reason\n+// the code runs 2-3% slower, even if the function is declared inline\n+#define MULTIPLY_ACCUM_WITH_SCALE(index)\\\n+        isum += vaddvq_s32(ggml_vdotq_s32(vzero, q2bytes.val[0], q8bytes.val[0])) * aux[is+(index)];\\\n+        isum += vaddvq_s32(ggml_vdotq_s32(vzero, q2bytes.val[1], q8bytes.val[1])) * aux[is+1+(index)];\n+\n+#define SHIFT_MULTIPLY_ACCUM_WITH_SCALE(shift, index)\\\n+        q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\n+        q2bytes.val[0] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits.val[0], (shift)), m3));\\\n+        q2bytes.val[1] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits.val[1], (shift)), m3));\\\n+        MULTIPLY_ACCUM_WITH_SCALE((index));\n+\n+        for (int j = 0; j < QK_K/128; ++j) {\n+            const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\n+\n+            ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\n+            q2bytes.val[0] = vreinterpretq_s8_u8(vandq_u8(q2bits.val[0], m3));\n+            q2bytes.val[1] = vreinterpretq_s8_u8(vandq_u8(q2bits.val[1], m3));\n+\n+            MULTIPLY_ACCUM_WITH_SCALE(0);\n+\n+            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(2, 2);\n+            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(4, 4);\n+            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(6, 6);\n+\n+            is += 8;\n+        }\n+\n+        sum += d * isum;\n+    }\n+\n+    *s = sum;\n+\n+#else\n+\n+    float sumf = 0;\n+\n+    for (int i = 0; i < nb; ++i) {\n+\n+        const uint8_t * q2 = x[i].qs;\n+        const  int8_t * q8 = y[i].qs;\n+        const uint8_t * sc = x[i].scales;\n+\n+        int summs = 0;\n+        for (int j = 0; j < 16; ++j) {\n+            summs += y[i].bsums[j] * (sc[j] >> 4);\n+        }\n+\n+        const float dall = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+        const float dmin = y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n+\n+        int isum = 0;\n+        int is = 0;\n+        int d;\n+        for (int k = 0; k < QK_K/128; ++k) {\n+            int shift = 0;\n+            for (int j = 0; j < 4; ++j) {\n+                d = sc[is++] & 0xF;\n+                int isuml = 0;\n+                for (int l =  0; l < 16; ++l) isuml += q8[l] * ((q2[l] >> shift) & 3);\n+                isum += d * isuml;\n+                d = sc[is++] & 0xF;\n+                isuml = 0;\n+                for (int l = 16; l < 32; ++l) isuml += q8[l] * ((q2[l] >> shift) & 3);\n+                isum += d * isuml;\n+                shift += 2;\n+                q8 += 32;\n+            }\n+            q2 += 32;\n+        }\n+        sumf += dall * isum - dmin * summs;\n+    }\n+    *s = sumf;\n+#endif\n+}\n+\n+void ggml_vec_dot_q3_K_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(n % QK_K == 0);\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const uint32_t kmask1 = 0x03030303;\n+    const uint32_t kmask2 = 0x0f0f0f0f;\n+\n+    const block_q3_K * GGML_RESTRICT x = vx;\n+    const block_q8_K * GGML_RESTRICT y = vy;\n+\n+    const int nb = n / QK_K;\n+\n+#if defined(__ARM_FEATURE_SVE)\n+\n+    uint32_t aux[3];\n+    uint32_t utmp[4];\n+\n+    const int8_t m32 = 32;\n+    const int vector_length = svcntb()*8;\n+    const svuint8_t m3b_sv = svdup_n_u8(0x3);\n+    const svint32_t vzero_sv = svdup_n_s32(0);\n+\n+    const svuint8_t m0_sv = svdup_n_u8(1);\n+    const svuint8_t m1_sv = svlsl_n_u8_x(svptrue_b8(), m0_sv, 1);\n+    const svuint8_t m2_sv = svlsl_n_u8_x(svptrue_b8(), m0_sv, 2);\n+    const svuint8_t m3_sv = svlsl_n_u8_x(svptrue_b8(), m0_sv, 3);\n+\n+    float sum = 0;\n+\n+    for (int i = 0; i < nb; ++i) {\n+\n+        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+\n+        const uint8_t * GGML_RESTRICT q3_sv = x[i].qs;\n+        const uint8_t * GGML_RESTRICT qh_sv = x[i].hmask;\n+        const int8_t  * GGML_RESTRICT q8_sv = y[i].qs;\n+\n+        // Set up scales\n+        memcpy(aux, x[i].scales, 12);\n+        utmp[3] = ((aux[1] >> 4) & kmask2) | (((aux[2] >> 6) & kmask1) << 4);\n+        utmp[2] = ((aux[0] >> 4) & kmask2) | (((aux[2] >> 4) & kmask1) << 4);\n+        utmp[1] = (aux[1] & kmask2) | (((aux[2] >> 2) & kmask1) << 4);\n+        utmp[0] = (aux[0] & kmask2) | (((aux[2] >> 0) & kmask1) << 4);\n+\n+        int8_t * scale = (int8_t *)utmp;\n+\n+        for (int j = 0; j < 16; ++j) scale[j] -= m32;\n+\n+        switch (vector_length) {\n+            case 128:\n+                {\n+                    svuint8_t qhbits_sv_1 = svld1_u8(svptrue_b8(), qh_sv);\n+                    svuint8_t qhbits_sv_2 = svld1_u8(svptrue_b8(), qh_sv+16);\n+                    svuint8_t q3h_sv;\n+\n+                    svint32_t sumi1_1 = svdup_n_s32(0);\n+                    svint8_t q3bytes_sv;\n+\n+                    for (int j = 0; j < QK_K/128; ++j) {\n+\n+                        const svuint8_t q3bits_sv = svld1_u8(svptrue_b8(), q3_sv); q3_sv += 16;\n+                        const svuint8_t q3bits_sv_1 = svld1_u8(svptrue_b8(), q3_sv); q3_sv += 16;\n+                        svint8_t q8bytes_1_sv_1 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                        svint8_t q8bytes_1_sv_2 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m0_sv, qhbits_sv_1), 2);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), q3bits_sv, m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), svdup_n_s32((int32_t)scale[0]));\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m0_sv, qhbits_sv_2), 2);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), q3bits_sv_1, m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), svdup_n_s32((int32_t)scale[1]));\n+\n+                        q8bytes_1_sv_1 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                        q8bytes_1_sv_2 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m1_sv, qhbits_sv_1), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv, 2), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), svdup_n_s32((int32_t)scale[2]));\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m1_sv, qhbits_sv_2), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv_1, 2), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), svdup_n_s32((int32_t)scale[3]));\n+\n+\n+                        scale += 4;\n+                        q8bytes_1_sv_1 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                        q8bytes_1_sv_2 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                        q3h_sv = svbic_u8_x(svptrue_b8(), m2_sv, qhbits_sv_1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv, 4), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), svdup_n_s32((int32_t)scale[0]));\n+\n+                        q3h_sv = svbic_u8_x(svptrue_b8(), m2_sv, qhbits_sv_2);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv_1, 4), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), svdup_n_s32((int32_t)scale[1]));\n+\n+\n+                        q8bytes_1_sv_1 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                        q8bytes_1_sv_2 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                        q3h_sv = svlsr_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m3_sv, qhbits_sv_1), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv, 6), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), svdup_n_s32((int32_t)scale[2]));\n+\n+                        q3h_sv = svlsr_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m3_sv, qhbits_sv_2), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv_1, 6), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), svdup_n_s32((int32_t)scale[3]));\n+\n+                        if (j == 0) {\n+                            qhbits_sv_1 = svlsr_n_u8_x(svptrue_b8(), qhbits_sv_1, 4);\n+                            qhbits_sv_2 = svlsr_n_u8_x(svptrue_b8(), qhbits_sv_2, 4);\n+                        }\n+\n+                        scale += 4;\n+                    }\n+\n+                    sum += d * (svaddv_s32(svptrue_b32(), sumi1_1));\n+                } break;\n+            case 256:\n+            case 512:\n+                {\n+                    svuint8_t qhbits_sv = svld1_u8(svptrue_pat_b8(SV_VL32), qh_sv);\n+                    svuint8_t q3h_sv;\n+\n+                    svint32_t sumi1_1 = svdup_n_s32(0);\n+                    svint8_t q3bytes_sv;\n+\n+                    for (int j = 0; j < QK_K/128; ++j) {\n+\n+                        const svuint8_t q3bits_sv = svld1_u8(svptrue_pat_b8(SV_VL32), q3_sv); q3_sv += 32;\n+                        svint8_t q8bytes_1_sv_1 = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+                        svint8_t q8bytes_1_sv_2 = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_pat_b8(SV_VL32), svbic_u8_x(svptrue_pat_b8(SV_VL32), m0_sv, qhbits_sv), 2);\n+                        q3bytes_sv = svsub_s8_x(svptrue_pat_b8(SV_VL32), svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), q3bits_sv, m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+\n+                        svint32_t scale_1 = svsel_s32(svptrue_pat_b32(SV_VL4), svdup_n_s32((int32_t)scale[0]), svdup_n_s32((int32_t)scale[1]));\n+                        sumi1_1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), scale_1);\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_pat_b8(SV_VL32), svbic_u8_x(svptrue_pat_b8(SV_VL32), m1_sv, qhbits_sv), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_pat_b8(SV_VL32), svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q3bits_sv, 2), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        scale_1 = svsel_s32(svptrue_pat_b32(SV_VL4), svdup_n_s32((int32_t)scale[2]), svdup_n_s32((int32_t)scale[3]));\n+                        sumi1_1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), scale_1);\n+\n+                        scale += 4;\n+                        q8bytes_1_sv_1 = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+                        q8bytes_1_sv_2 = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                        q3h_sv = svbic_u8_x(svptrue_pat_b8(SV_VL32), m2_sv, qhbits_sv);\n+                        q3bytes_sv = svsub_s8_x(svptrue_pat_b8(SV_VL32), svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q3bits_sv, 4), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        scale_1 = svsel_s32(svptrue_pat_b32(SV_VL4), svdup_n_s32((int32_t)scale[0]), svdup_n_s32((int32_t)scale[1]));\n+                        sumi1_1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), scale_1);\n+\n+                        q3h_sv = svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), svbic_u8_x(svptrue_pat_b8(SV_VL32), m3_sv, qhbits_sv), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_pat_b8(SV_VL32), svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q3bits_sv, 6), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        scale_1 = svsel_s32(svptrue_pat_b32(SV_VL4), svdup_n_s32((int32_t)scale[2]), svdup_n_s32((int32_t)scale[3]));\n+                        sumi1_1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), scale_1);\n+\n+                        if (j == 0) {\n+                            qhbits_sv = svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), qhbits_sv, 4);\n+                        }\n+\n+                        scale += 4;\n+                    }\n+\n+                    sum += d * (svaddv_s32(svptrue_pat_b32(SV_VL8), sumi1_1));\n+                } break;\n+            default:\n+                assert(false && \"Unsupported vector length\");\n+                break;\n+        }\n+    }\n+    *s = sum;\n+\n+#elif __ARM_NEON\n+\n+    uint32_t aux[3];\n+    uint32_t utmp[4];\n+\n+    const uint8x16_t m3b = vdupq_n_u8(0x3);\n+    const int32x4_t  vzero = vdupq_n_s32(0);\n+\n+    const uint8x16_t m0 = vdupq_n_u8(1);\n+    const uint8x16_t m1 = vshlq_n_u8(m0, 1);\n+    const uint8x16_t m2 = vshlq_n_u8(m0, 2);\n+    const uint8x16_t m3 = vshlq_n_u8(m0, 3);\n+    const int8_t m32 = 32;\n+\n+    ggml_int8x16x4_t q3bytes;\n+\n+    float sum = 0;\n+\n+    for (int i = 0; i < nb; ++i) {\n+\n+        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+\n+        const uint8_t * GGML_RESTRICT q3 = x[i].qs;\n+        const uint8_t * GGML_RESTRICT qh = x[i].hmask;\n+        const int8_t  * GGML_RESTRICT q8 = y[i].qs;\n+\n+        ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\n+\n+        ggml_uint8x16x4_t q3h;\n+\n+        int32_t isum = 0;\n+\n+        // Set up scales\n+        memcpy(aux, x[i].scales, 12);\n+        utmp[3] = ((aux[1] >> 4) & kmask2) | (((aux[2] >> 6) & kmask1) << 4);\n+        utmp[2] = ((aux[0] >> 4) & kmask2) | (((aux[2] >> 4) & kmask1) << 4);\n+        utmp[1] = (aux[1] & kmask2) | (((aux[2] >> 2) & kmask1) << 4);\n+        utmp[0] = (aux[0] & kmask2) | (((aux[2] >> 0) & kmask1) << 4);\n+\n+        int8_t * scale = (int8_t *)utmp;\n+        for (int j = 0; j < 16; ++j) scale[j] -= m32;\n+\n+        for (int j = 0; j < QK_K/128; ++j) {\n+\n+            const ggml_uint8x16x2_t q3bits = ggml_vld1q_u8_x2(q3); q3 += 32;\n+            const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\n+            const ggml_int8x16x4_t q8bytes_2 = ggml_vld1q_s8_x4(q8); q8 += 64;\n+\n+            q3h.val[0] = vshlq_n_u8(vbicq_u8(m0, qhbits.val[0]), 2);\n+            q3h.val[1] = vshlq_n_u8(vbicq_u8(m0, qhbits.val[1]), 2);\n+            q3h.val[2] = vshlq_n_u8(vbicq_u8(m1, qhbits.val[0]), 1);\n+            q3h.val[3] = vshlq_n_u8(vbicq_u8(m1, qhbits.val[1]), 1);\n+\n+            q3bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(q3bits.val[0], m3b)), vreinterpretq_s8_u8(q3h.val[0]));\n+            q3bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(q3bits.val[1], m3b)), vreinterpretq_s8_u8(q3h.val[1]));\n+            q3bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 2), m3b)), vreinterpretq_s8_u8(q3h.val[2]));\n+            q3bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 2), m3b)), vreinterpretq_s8_u8(q3h.val[3]));\n+\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[0], q8bytes_1.val[0])) * scale[0];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[1], q8bytes_1.val[1])) * scale[1];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[2], q8bytes_1.val[2])) * scale[2];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[3], q8bytes_1.val[3])) * scale[3];\n+\n+            scale += 4;\n+\n+            q3h.val[0] = vbicq_u8(m2, qhbits.val[0]);\n+            q3h.val[1] = vbicq_u8(m2, qhbits.val[1]);\n+            q3h.val[2] = vshrq_n_u8(vbicq_u8(m3, qhbits.val[0]), 1);\n+            q3h.val[3] = vshrq_n_u8(vbicq_u8(m3, qhbits.val[1]), 1);\n+\n+            q3bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 4), m3b)), vreinterpretq_s8_u8(q3h.val[0]));\n+            q3bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 4), m3b)), vreinterpretq_s8_u8(q3h.val[1]));\n+            q3bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 6), m3b)), vreinterpretq_s8_u8(q3h.val[2]));\n+            q3bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 6), m3b)), vreinterpretq_s8_u8(q3h.val[3]));\n+\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[0], q8bytes_2.val[0])) * scale[0];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[1], q8bytes_2.val[1])) * scale[1];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[2], q8bytes_2.val[2])) * scale[2];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[3], q8bytes_2.val[3])) * scale[3];\n+\n+            scale += 4;\n+\n+            if (j == 0) {\n+                qhbits.val[0] = vshrq_n_u8(qhbits.val[0], 4);\n+                qhbits.val[1] = vshrq_n_u8(qhbits.val[1], 4);\n+            }\n+\n+        }\n+        sum += d * isum;\n+\n+    }\n+\n+    *s = sum;\n+\n+#else\n+    // scalar version\n+    // This function is written like this so the compiler can manage to vectorize most of it\n+    // Using -Ofast, GCC and clang manage to produce code that is within a factor of 2 or so from the\n+    // manually vectorized version above. Every other version I tried would run at least 4 times slower.\n+    // The ideal situation would be if we could just write the code once, and the compiler would\n+    // automatically produce the best possible set of machine instructions, instead of us having to manually\n+    // write vectorized versions for AVX, ARM_NEON, etc.\n+\n+    int8_t  aux8[QK_K];\n+    int16_t aux16[8];\n+    float   sums [8];\n+    int32_t aux32[8];\n+    memset(sums, 0, 8*sizeof(float));\n+\n+    uint32_t auxs[4];\n+    const int8_t * scales = (const int8_t*)auxs;\n+\n+    float sumf = 0;\n+    for (int i = 0; i < nb; ++i) {\n+        const uint8_t * GGML_RESTRICT q3 = x[i].qs;\n+        const uint8_t * GGML_RESTRICT hm = x[i].hmask;\n+        const  int8_t * GGML_RESTRICT q8 = y[i].qs;\n+        memset(aux32, 0, 8*sizeof(int32_t));\n+        int8_t * GGML_RESTRICT a = aux8;\n+        uint8_t m = 1;\n+        for (int j = 0; j < QK_K; j += 128) {\n+            for (int l = 0; l < 32; ++l) a[l] = q3[l] & 3;\n+            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n+            a += 32; m <<= 1;\n+            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 2) & 3;\n+            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n+            a += 32; m <<= 1;\n+            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 4) & 3;\n+            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n+            a += 32; m <<= 1;\n+            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 6) & 3;\n+            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n+            a += 32; m <<= 1;\n+            q3 += 32;\n+        }\n+        a = aux8;\n+\n+        memcpy(auxs, x[i].scales, 12);\n+        uint32_t tmp = auxs[2];\n+        auxs[2] = ((auxs[0] >> 4) & kmask2) | (((tmp >> 4) & kmask1) << 4);\n+        auxs[3] = ((auxs[1] >> 4) & kmask2) | (((tmp >> 6) & kmask1) << 4);\n+        auxs[0] = (auxs[0] & kmask2) | (((tmp >> 0) & kmask1) << 4);\n+        auxs[1] = (auxs[1] & kmask2) | (((tmp >> 2) & kmask1) << 4);\n+        for (int j = 0; j < QK_K/16; ++j) {\n+            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n+            for (int l = 0; l < 8; ++l) aux32[l] += (scales[j] - 32) * aux16[l];\n+            q8 += 8; a += 8;\n+            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n+            for (int l = 0; l < 8; ++l) aux32[l] += (scales[j] - 32) * aux16[l];\n+            q8 += 8; a += 8;\n+        }\n+        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n+        for (int l = 0; l < 8; ++l) sums[l] += d * aux32[l];\n+    }\n+    for (int l = 0; l < 8; ++l) sumf += sums[l];\n+    *s = sumf;\n+\n+#endif\n+\n+}\n+\n+void ggml_vec_dot_q4_K_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(n % QK_K == 0);\n+    assert(nrc == 1);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2128303709",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13892,
        "pr_file": "ggml/src/ggml-cpu/arch/arm/quants.c",
        "discussion_id": "2128303709",
        "commented_code": "@@ -0,0 +1,4109 @@\n+#define GGML_COMMON_IMPL_C\n+#include \"ggml-common.h\"\n+#include \"ggml-quants.h\"\n+#include \"ggml-impl.h\"\n+#include \"ggml-cpu.h\"\n+\n+#include \"../../quants.h\"\n+#include \"../../ggml-cpu-impl.h\"\n+\n+#include <math.h>\n+#include <string.h>\n+#include <assert.h>\n+#include <float.h>\n+#include <stdlib.h> // for qsort\n+#include <stdio.h>  // for GGML_ASSERT\n+\n+#define GROUP_MAX_EPS 1e-15f\n+#define GROUP_MAX_EPS_IQ3_XXS 1e-8f\n+#define GROUP_MAX_EPS_IQ2_S 1e-8f\n+#define GROUP_MAX_EPS_IQ1_M 1e-7f\n+#define GROUP_MAX_EPS_IQ1_S 1e-12f\n+\n+#define UNUSED GGML_UNUSED\n+\n+#if defined(__ARM_NEON)\n+#define B1(c,s,n)  0x ## n ## c ,  0x ## n ## s\n+#define B2(c,s,n) B1(c,s,n ## c), B1(c,s,n ## s)\n+#define B3(c,s,n) B2(c,s,n ## c), B2(c,s,n ## s)\n+#define B4(c,s,n) B3(c,s,n ## c), B3(c,s,n ## s)\n+#define B5(c,s,n) B4(c,s,n ## c), B4(c,s,n ## s)\n+#define B6(c,s,n) B5(c,s,n ## c), B5(c,s,n ## s)\n+#define B7(c,s,n) B6(c,s,n ## c), B6(c,s,n ## s)\n+#define B8(c,s  ) B7(c,s,     c), B7(c,s,     s)\n+\n+// precomputed tables for expanding 8bits to 8 bytes:\n+static const uint64_t table_b2b_0[1 << 8] = { B8(00, 10) }; // ( b) << 4\n+static const uint64_t table_b2b_1[1 << 8] = { B8(10, 00) }; // (!b) << 4\n+#endif\n+\n+void quantize_row_q8_0(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {\n+    assert(QK8_0 == 32);\n+    assert(k % QK8_0 == 0);\n+    const int nb = k / QK8_0;\n+\n+    block_q8_0 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_NEON)\n+    for (int i = 0; i < nb; i++) {\n+        float32x4_t srcv [8];\n+        float32x4_t asrcv[8];\n+        float32x4_t amaxv[8];\n+\n+        for (int j = 0; j < 8; j++) srcv[j]  = vld1q_f32(x + i*32 + 4*j);\n+        for (int j = 0; j < 8; j++) asrcv[j] = vabsq_f32(srcv[j]);\n+\n+        for (int j = 0; j < 4; j++) amaxv[2*j] = vmaxq_f32(asrcv[2*j], asrcv[2*j+1]);\n+        for (int j = 0; j < 2; j++) amaxv[4*j] = vmaxq_f32(amaxv[4*j], amaxv[4*j+2]);\n+        for (int j = 0; j < 1; j++) amaxv[8*j] = vmaxq_f32(amaxv[8*j], amaxv[8*j+4]);\n+\n+        const float amax = vmaxvq_f32(amaxv[0]);\n+\n+        const float d = amax / ((1 << 7) - 1);\n+        const float id = d ? 1.0f/d : 0.0f;\n+\n+        y[i].d = GGML_FP32_TO_FP16(d);\n+\n+        for (int j = 0; j < 8; j++) {\n+            const float32x4_t v  = vmulq_n_f32(srcv[j], id);\n+            const int32x4_t   vi = vcvtnq_s32_f32(v);\n+\n+            y[i].qs[4*j + 0] = vgetq_lane_s32(vi, 0);\n+            y[i].qs[4*j + 1] = vgetq_lane_s32(vi, 1);\n+            y[i].qs[4*j + 2] = vgetq_lane_s32(vi, 2);\n+            y[i].qs[4*j + 3] = vgetq_lane_s32(vi, 3);\n+        }\n+    }\n+#else\n+    GGML_UNUSED(nb);\n+    // scalar\n+    quantize_row_q8_0_ref(x, y, k);\n+#endif\n+}\n+\n+void quantize_row_q8_1(const float * GGML_RESTRICT x, void * GGML_RESTRICT vy, int64_t k) {\n+    assert(k % QK8_1 == 0);\n+    const int nb = k / QK8_1;\n+\n+    block_q8_1 * GGML_RESTRICT y = vy;\n+#if defined(__ARM_NEON)\n+    for (int i = 0; i < nb; i++) {\n+        float32x4_t srcv [8];\n+        float32x4_t asrcv[8];\n+        float32x4_t amaxv[8];\n+\n+        for (int j = 0; j < 8; j++) srcv[j]  = vld1q_f32(x + i*32 + 4*j);\n+        for (int j = 0; j < 8; j++) asrcv[j] = vabsq_f32(srcv[j]);\n+\n+        for (int j = 0; j < 4; j++) amaxv[2*j] = vmaxq_f32(asrcv[2*j], asrcv[2*j+1]);\n+        for (int j = 0; j < 2; j++) amaxv[4*j] = vmaxq_f32(amaxv[4*j], amaxv[4*j+2]);\n+        for (int j = 0; j < 1; j++) amaxv[8*j] = vmaxq_f32(amaxv[8*j], amaxv[8*j+4]);\n+\n+        const float amax = vmaxvq_f32(amaxv[0]);\n+\n+        const float d = amax / ((1 << 7) - 1);\n+        const float id = d ? 1.0f/d : 0.0f;\n+\n+        y[i].d = GGML_FP32_TO_FP16(d);\n+\n+        int32x4_t accv = vdupq_n_s32(0);\n+\n+        for (int j = 0; j < 8; j++) {\n+            const float32x4_t v  = vmulq_n_f32(srcv[j], id);\n+            const int32x4_t   vi = vcvtnq_s32_f32(v);\n+\n+            y[i].qs[4*j + 0] = vgetq_lane_s32(vi, 0);\n+            y[i].qs[4*j + 1] = vgetq_lane_s32(vi, 1);\n+            y[i].qs[4*j + 2] = vgetq_lane_s32(vi, 2);\n+            y[i].qs[4*j + 3] = vgetq_lane_s32(vi, 3);\n+\n+            accv = vaddq_s32(accv, vi);\n+        }\n+\n+        y[i].s = GGML_FP32_TO_FP16(d * vaddvq_s32(accv));\n+    }\n+#else\n+    GGML_UNUSED(nb);\n+    // scalar\n+    quantize_row_q8_1_ref(x, y, k);\n+#endif\n+}\n+\n+// placeholder implementation for Apple targets\n+void quantize_row_q8_K(const float * GGML_RESTRICT x, void * GGML_RESTRICT y, int64_t k) {\n+    quantize_row_q8_K_ref(x, y, k);\n+}\n+\n+//===================================== Dot products =================================\n+\n+void ggml_vec_dot_q4_0_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_0;\n+    const int nb = n / qk;\n+\n+    assert(n % qk == 0);\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    assert((nrc == 2) || (nrc == 1));\n+#else\n+    assert(nrc == 1);\n+#endif\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q4_0 * GGML_RESTRICT x = vx;\n+    const block_q8_0 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    if (nrc == 2) {\n+        const block_q4_0 * GGML_RESTRICT vx0 = vx;\n+        const block_q4_0 * GGML_RESTRICT vx1 = (const block_q4_0 *) ((const uint8_t*)vx + bx);\n+        const block_q8_0 * GGML_RESTRICT vy0 = vy;\n+        const block_q8_0 * GGML_RESTRICT vy1 = (const block_q8_0 *) ((const uint8_t*)vy + by);\n+\n+        float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+\n+        for (int i = 0; i < nb; i++) {\n+            const block_q4_0 * GGML_RESTRICT b_x0 = &vx0[i];\n+            const block_q4_0 * GGML_RESTRICT b_x1 = &vx1[i];\n+            const block_q8_0 * GGML_RESTRICT b_y0 = &vy0[i];\n+            const block_q8_0 * GGML_RESTRICT b_y1 = &vy1[i];\n+\n+            const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+            const int8x16_t  s8b = vdupq_n_s8(0x8);\n+\n+            const uint8x16_t v0_0 = vld1q_u8(b_x0->qs);\n+            const uint8x16_t v0_1 = vld1q_u8(b_x1->qs);\n+\n+            // 4-bit -> 8-bit\n+            const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+            const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+            const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+            const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+            // sub 8\n+            const int8x16_t x0_l = vsubq_s8(v0_0l, s8b);\n+            const int8x16_t x0_h = vsubq_s8(v0_0h, s8b);\n+            const int8x16_t x1_l = vsubq_s8(v0_1l, s8b);\n+            const int8x16_t x1_h = vsubq_s8(v0_1h, s8b);\n+\n+            // load y\n+            const int8x16_t y0_l = vld1q_s8(b_y0->qs);\n+            const int8x16_t y0_h = vld1q_s8(b_y0->qs + 16);\n+            const int8x16_t y1_l = vld1q_s8(b_y1->qs);\n+            const int8x16_t y1_h = vld1q_s8(b_y1->qs + 16);\n+\n+            float32_t _scale[4] = {\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y1->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y1->d)\n+            };\n+            float32x4_t scale = vld1q_f32(_scale);\n+\n+            int8x16_t l0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+            int8x16_t l1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+\n+            int8x16_t l2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+            int8x16_t l3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+\n+            int8x16_t r0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+            int8x16_t r1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+\n+            int8x16_t r2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+            int8x16_t r3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+\n+            sumv0 = vmlaq_f32(sumv0,(vcvtq_f32_s32(vmmlaq_s32((vmmlaq_s32((vmmlaq_s32((vmmlaq_s32(vdupq_n_s32(0), l0, r0)),\n+                                                l1, r1)), l2, r2)), l3, r3))), scale);\n+        }\n+\n+        float32x4_t sumv1 = vextq_f32 (sumv0, sumv0, 2);\n+        float32x4_t sumv2 = vzip1q_f32(sumv0, sumv1);\n+\n+        vst1_f32(s,      vget_low_f32 (sumv2));\n+        vst1_f32(s + bs, vget_high_f32(sumv2));\n+\n+        return;\n+    }\n+#endif\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+#if defined(__ARM_FEATURE_SVE)\n+    svfloat32_t sumv0 = svdup_n_f32(0.0f);\n+    svfloat32_t sumv1 = svdup_n_f32(0.0f);\n+\n+    const int vector_length = ggml_cpu_get_sve_cnt()*8;\n+\n+    // VLA Implementation using switch case\n+    switch (vector_length) {\n+        case 128:\n+            {\n+                // predicate for activating higher lanes for 4 float32 elements\n+                const svbool_t ph4 = svptrue_pat_b32(SV_VL4);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q4_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q4_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svuint8_t qx0r = svld1rq_u8(svptrue_b8(), x0->qs);\n+                    const svuint8_t qx1r = svld1rq_u8(svptrue_b8(), x1->qs);\n+\n+                    // 4-bit -> 8-bit\n+                    const svint8_t qx0l = svreinterpret_s8_u8(svand_n_u8_m(svptrue_b8(), qx0r, 0x0F));\n+                    const svint8_t qx0h = svreinterpret_s8_u8(svlsr_n_u8_m(svptrue_b8(), qx0r, 0x04));\n+                    const svint8_t qx1l = svreinterpret_s8_u8(svand_n_u8_m(svptrue_b8(), qx1r, 0x0F));\n+                    const svint8_t qx1h = svreinterpret_s8_u8(svlsr_n_u8_m(svptrue_b8(), qx1r, 0x04));\n+\n+                    // sub 8\n+                    const svint8_t qx0ls = svsub_n_s8_x(svptrue_b8(), qx0h, 8);\n+                    const svint8_t qx0hs = svsub_n_s8_x(svptrue_b8(), qx0l, 8);\n+                    const svint8_t qx1ls = svsub_n_s8_x(svptrue_b8(), qx1h, 8);\n+                    const svint8_t qx1hs = svsub_n_s8_x(svptrue_b8(), qx1l, 8);\n+\n+                    // load y\n+                    const svint8_t qy0h = svld1_s8(svptrue_b8(), y0->qs);\n+                    const svint8_t qy0l = svld1_s8(svptrue_b8(), y0->qs + 16);\n+                    const svint8_t qy1h = svld1_s8(svptrue_b8(), y1->qs);\n+                    const svint8_t qy1l = svld1_s8(svptrue_b8(), y1->qs + 16);\n+\n+                    // dot product\n+                    sumv0 = svmla_n_f32_x(ph4, sumv0, svcvt_f32_s32_x(ph4, svadd_x(ph4,\n+                                    svdot_s32(svdup_n_s32(0), qx0ls, qy0l),\n+                                    svdot_s32(svdup_n_s32(0), qx0hs, qy0h))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(ph4, sumv1, svcvt_f32_s32_x(ph4, svadd_x(ph4,\n+                                    svdot_s32(svdup_n_s32(0), qx1ls, qy1l),\n+                                    svdot_s32(svdup_n_s32(0), qx1hs, qy1h))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(svptrue_b32(), svadd_f32_x(svptrue_b32(), sumv0, sumv1));\n+            } break;\n+        case 256:\n+            {\n+                // predicate for activating higher lanes for 16 int8 elements\n+                const svbool_t ph16 = svptrue_pat_b8(SV_VL16);\n+                // predicate for activating lower lanes for  16 int8 elements\n+                const svbool_t pl16 = svnot_b_z(svptrue_b8(), ph16);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q4_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q4_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svuint8_t qx0r = svld1rq_u8(svptrue_b8(), x0->qs);\n+                    const svuint8_t qx1r = svld1rq_u8(svptrue_b8(), x1->qs);\n+\n+                    // 4-bit -> 8-bit\n+                    const svint8_t qx0 = svreinterpret_s8_u8(svlsr_n_u8_m(pl16, svand_n_u8_m(ph16, qx0r, 0x0F), 0x04));\n+                    const svint8_t qx1 = svreinterpret_s8_u8(svlsr_n_u8_m(pl16, svand_n_u8_m(ph16, qx1r, 0x0F), 0x04));\n+\n+                    // sub 8\n+                    const svint8_t qx0s = svsub_n_s8_x(svptrue_b8(), qx0, 8);\n+                    const svint8_t qx1s = svsub_n_s8_x(svptrue_b8(), qx1, 8);\n+\n+                    // load y\n+                    const svint8_t qy0 = svld1_s8(svptrue_b8(), y0->qs);\n+                    const svint8_t qy1 = svld1_s8(svptrue_b8(), y1->qs);\n+\n+                    // dot product\n+                    sumv0 = svmla_n_f32_x(svptrue_b32(), sumv0, svcvt_f32_s32_x(svptrue_b32(),\n+                                svdot_s32(svdup_n_s32(0), qx0s, qy0)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(svptrue_b32(), sumv1, svcvt_f32_s32_x(svptrue_b32(),\n+                                svdot_s32(svdup_n_s32(0), qx1s, qy1)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(svptrue_b32(), svadd_f32_x(svptrue_b32(), sumv0, sumv1));\n+            } break;\n+        case 512:\n+            {\n+                // predicate for activating higher lanes for 32 int8 elements\n+                const svbool_t ph32 = svptrue_pat_b8(SV_VL32);\n+\n+                // predicate for activating higher lanes for 16 int8 elements\n+                const svbool_t ph16 = svptrue_pat_b8(SV_VL16);\n+                // predicate for activating lower lanes for 16 int8 elements from first 32 int8 activated lanes\n+                const svbool_t pl16 = svnot_b_z(ph32, ph16);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q4_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q4_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svuint8_t qx0r = svld1rq_u8(ph32, x0->qs);\n+                    const svuint8_t qx1r = svld1rq_u8(ph32, x1->qs);\n+\n+                    // 4-bit -> 8-bit\n+                    const svint8_t qx0 = svreinterpret_s8_u8(svlsr_n_u8_m(pl16, svand_n_u8_m(ph16, qx0r, 0x0F), 0x04));\n+                    const svint8_t qx1 = svreinterpret_s8_u8(svlsr_n_u8_m(pl16, svand_n_u8_m(ph16, qx1r, 0x0F), 0x04));\n+\n+                    // sub 8\n+                    const svint8_t qx0s = svsub_n_s8_x(ph32, qx0, 8);\n+                    const svint8_t qx1s = svsub_n_s8_x(ph32, qx1, 8);\n+\n+                    // load y\n+                    const svint8_t qy0 = svld1_s8(ph32, y0->qs);\n+                    const svint8_t qy1 = svld1_s8(ph32, y1->qs);\n+\n+                    // dot product\n+                    sumv0 = svmla_n_f32_x(ph32, sumv0, svcvt_f32_s32_x(ph32,\n+                                svdot_s32(svdup_n_s32(0), qx0s, qy0)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(ph32, sumv1, svcvt_f32_s32_x(ph32,\n+                                svdot_s32(svdup_n_s32(0), qx1s, qy1)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(ph32, svadd_f32_x(ph32, sumv0, sumv1));\n+            } break;\n+        default:\n+            assert(false && \"Unsupported vector length\");\n+            break;\n+    }\n+\n+#elif defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q4_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+        const block_q4_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+        const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+        const int8x16_t  s8b = vdupq_n_s8(0x8);\n+\n+        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n+        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n+\n+        // 4-bit -> 8-bit\n+        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+        // sub 8\n+        const int8x16_t v0_0ls = vsubq_s8(v0_0l, s8b);\n+        const int8x16_t v0_0hs = vsubq_s8(v0_0h, s8b);\n+        const int8x16_t v0_1ls = vsubq_s8(v0_1l, s8b);\n+        const int8x16_t v0_1hs = vsubq_s8(v0_1h, s8b);\n+\n+        // load y\n+        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n+        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n+        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n+        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n+\n+        // dot product into int32x4_t\n+        const int32x4_t p_0 = ggml_vdotq_s32(ggml_vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0l), v0_0hs, v1_0h);\n+        const int32x4_t p_1 = ggml_vdotq_s32(ggml_vdotq_s32(vdupq_n_s32(0), v0_1ls, v1_1l), v0_1hs, v1_1h);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(p_0), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(p_1), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n+#endif\n+    for (; ib < nb; ++ib) {\n+        int sumi0 = 0;\n+        int sumi1 = 0;\n+\n+        for (int j = 0; j < qk/2; ++j) {\n+            const int v0 = (x[ib].qs[j] & 0x0F) - 8;\n+            const int v1 = (x[ib].qs[j] >>   4) - 8;\n+\n+            sumi0 += (v0 * y[ib].qs[j]);\n+            sumi1 += (v1 * y[ib].qs[j + qk/2]);\n+        }\n+\n+        int sumi = sumi0 + sumi1;\n+        sumf += sumi*GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d);\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_q4_1_q8_1(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_1;\n+    const int nb = n / qk;\n+\n+    assert(n % qk == 0);\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    assert((nrc == 2) || (nrc == 1));\n+#else\n+    assert(nrc == 1);\n+#endif\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q4_1 * GGML_RESTRICT x = vx;\n+    const block_q8_1 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    if (nrc == 2) {\n+        const block_q4_1 * GGML_RESTRICT vx0 = vx;\n+        const block_q4_1 * GGML_RESTRICT vx1 = (const block_q4_1 *) ((const uint8_t*)vx + bx);\n+        const block_q8_1 * GGML_RESTRICT vy0 = vy;\n+        const block_q8_1 * GGML_RESTRICT vy1 = (const block_q8_1 *) ((const uint8_t*)vy + by);\n+\n+        float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+        float32x4_t summs0 = vdupq_n_f32(0.0f);\n+\n+        for (int i = 0; i < nb; i++) {\n+            const block_q4_1 * GGML_RESTRICT b_x0 = &vx0[i];\n+            const block_q4_1 * GGML_RESTRICT b_x1 = &vx1[i];\n+            const block_q8_1 * GGML_RESTRICT b_y0 = &vy0[i];\n+            const block_q8_1 * GGML_RESTRICT b_y1 = &vy1[i];\n+\n+            float32_t summs_t[4] = {\n+                GGML_FP16_TO_FP32(b_x0->m) * GGML_FP16_TO_FP32(b_y0->s),\n+                GGML_FP16_TO_FP32(b_x1->m) * GGML_FP16_TO_FP32(b_y0->s),\n+                GGML_FP16_TO_FP32(b_x0->m) * GGML_FP16_TO_FP32(b_y1->s),\n+                GGML_FP16_TO_FP32(b_x1->m) * GGML_FP16_TO_FP32(b_y1->s)\n+            };\n+            summs0 = vaddq_f32(summs0, vld1q_f32(summs_t));\n+\n+            const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+\n+            const uint8x16_t v0_0 = vld1q_u8(b_x0->qs);\n+            const uint8x16_t v0_1 = vld1q_u8(b_x1->qs);\n+\n+            // 4-bit -> 8-bit\n+            const int8x16_t x0_l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+            const int8x16_t x0_h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+            const int8x16_t x1_l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+            const int8x16_t x1_h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+            // load y\n+            const int8x16_t y0_l = vld1q_s8(b_y0->qs);\n+            const int8x16_t y0_h = vld1q_s8(b_y0->qs + 16);\n+            const int8x16_t y1_l = vld1q_s8(b_y1->qs);\n+            const int8x16_t y1_h = vld1q_s8(b_y1->qs + 16);\n+\n+            // mmla into int32x4_t\n+            float32_t _scale[4] = {\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y1->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y1->d)\n+            };\n+            float32x4_t scale = vld1q_f32(_scale);\n+\n+            int8x16_t l0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+            int8x16_t l1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+\n+            int8x16_t l2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+            int8x16_t l3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+\n+            int8x16_t r0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+            int8x16_t r1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+\n+            int8x16_t r2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+            int8x16_t r3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+            sumv0 = vmlaq_f32(sumv0,(vcvtq_f32_s32(vmmlaq_s32((vmmlaq_s32((vmmlaq_s32((vmmlaq_s32(vdupq_n_s32(0), l0, r0)),\n+                                                l1, r1)), l2, r2)), l3, r3))), scale);\n+        }\n+\n+        float32x4_t sumv1 = vextq_f32 (sumv0, sumv0, 2);\n+        float32x4_t sumv2 = vzip1q_f32(sumv0, sumv1);\n+\n+        sumv2 = vaddq_f32(sumv2, summs0);\n+\n+        vst1_f32(s,      vget_low_f32 (sumv2));\n+        vst1_f32(s + bs, vget_high_f32(sumv2));\n+\n+        return;\n+    }\n+#endif\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+#if defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    float summs = 0;\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q4_1 * GGML_RESTRICT x0 = &x[ib + 0];\n+        const block_q4_1 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_1 * GGML_RESTRICT y0 = &y[ib + 0];\n+        const block_q8_1 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        summs += GGML_FP16_TO_FP32(x0->m) * GGML_FP16_TO_FP32(y0->s) + GGML_FP16_TO_FP32(x1->m) * GGML_FP16_TO_FP32(y1->s);\n+\n+        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+\n+        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n+        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n+\n+        // 4-bit -> 8-bit\n+        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+        // load y\n+        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n+        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n+        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n+        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n+\n+        // dot product into int32x4_t\n+        const int32x4_t p_0 = ggml_vdotq_s32(ggml_vdotq_s32(vdupq_n_s32(0), v0_0l, v1_0l), v0_0h, v1_0h);\n+        const int32x4_t p_1 = ggml_vdotq_s32(ggml_vdotq_s32(vdupq_n_s32(0), v0_1l, v1_1l), v0_1h, v1_1h);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(p_0), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(p_1), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1) + summs;\n+\n+#endif\n+    for (; ib < nb; ++ib) {\n+        int sumi0 = 0;\n+        int sumi1 = 0;\n+\n+        for (int j = 0; j < qk/2; ++j) {\n+            const int v0 = (x[ib].qs[j] & 0x0F);\n+            const int v1 = (x[ib].qs[j] >>   4);\n+\n+            sumi0 += (v0 * y[ib].qs[j]);\n+            sumi1 += (v1 * y[ib].qs[j + qk/2]);\n+        }\n+\n+        int sumi = sumi0 + sumi1;\n+        sumf += (GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d))*sumi + GGML_FP16_TO_FP32(x[ib].m)*GGML_FP16_TO_FP32(y[ib].s);\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_q5_0_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_0;\n+    const int nb = n / qk;\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+    assert(n % qk == 0);\n+    assert(qk == QK5_0);\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q5_0 * GGML_RESTRICT x = vx;\n+    const block_q8_0 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    uint32_t qh0;\n+    uint32_t qh1;\n+\n+    uint64_t tmp0[4];\n+    uint64_t tmp1[4];\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q5_0 * GGML_RESTRICT x0 = &x[ib];\n+        const block_q5_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_0 * GGML_RESTRICT y0 = &y[ib];\n+        const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+\n+        // extract the 5th bit via lookup table ((!b) << 4)\n+        memcpy(&qh0, x0->qh, sizeof(qh0));\n+        memcpy(&qh1, x1->qh, sizeof(qh1));\n+\n+        tmp0[0] = table_b2b_1[(qh0 >>  0) & 0xFF];\n+        tmp0[1] = table_b2b_1[(qh0 >>  8) & 0xFF];\n+        tmp0[2] = table_b2b_1[(qh0 >> 16) & 0xFF];\n+        tmp0[3] = table_b2b_1[(qh0 >> 24)       ];\n+\n+        tmp1[0] = table_b2b_1[(qh1 >>  0) & 0xFF];\n+        tmp1[1] = table_b2b_1[(qh1 >>  8) & 0xFF];\n+        tmp1[2] = table_b2b_1[(qh1 >> 16) & 0xFF];\n+        tmp1[3] = table_b2b_1[(qh1 >> 24)       ];\n+\n+        const int8x16_t qhl0 = vld1q_s8((const int8_t *)(tmp0 + 0));\n+        const int8x16_t qhh0 = vld1q_s8((const int8_t *)(tmp0 + 2));\n+        const int8x16_t qhl1 = vld1q_s8((const int8_t *)(tmp1 + 0));\n+        const int8x16_t qhh1 = vld1q_s8((const int8_t *)(tmp1 + 2));\n+\n+        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n+        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n+\n+        // 4-bit -> 8-bit\n+        int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+        int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+        int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+        int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+        // add high bit and sub 16 (equivalent to sub 0x10 when bit is zero)\n+        const int8x16_t v0_0lf = vsubq_s8(v0_0l, qhl0);\n+        const int8x16_t v0_0hf = vsubq_s8(v0_0h, qhh0);\n+        const int8x16_t v0_1lf = vsubq_s8(v0_1l, qhl1);\n+        const int8x16_t v0_1hf = vsubq_s8(v0_1h, qhh1);\n+\n+        // load y\n+        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n+        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n+        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n+        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_0lf, v1_0l),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_0hf, v1_0h))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_1lf, v1_1l),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_1hf, v1_1h))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n+\n+#endif\n+    for (; ib < nb; ++ib) {\n+        uint32_t qh;\n+        memcpy(&qh, x[ib].qh, sizeof(qh));\n+\n+        int sumi0 = 0;\n+        int sumi1 = 0;\n+\n+        for (int j = 0; j < qk/2; ++j) {\n+            const uint8_t xh_0 = ((qh & (1u << (j + 0 ))) >> (j + 0 )) << 4;\n+            const uint8_t xh_1 = ((qh & (1u << (j + 16))) >> (j + 12));\n+\n+            const int32_t x0 = (int8_t)(((x[ib].qs[j] & 0x0F) | xh_0) - 16);\n+            const int32_t x1 = (int8_t)(((x[ib].qs[j] >>   4) | xh_1) - 16);\n+\n+            sumi0 += (x0 * y[ib].qs[j]);\n+            sumi1 += (x1 * y[ib].qs[j + qk/2]);\n+        }\n+\n+        int sumi = sumi0 + sumi1;\n+        sumf += (GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d)) * sumi;\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_q5_1_q8_1(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_1;\n+    const int nb = n / qk;\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+    assert(n % qk == 0);\n+    assert(qk == QK5_1);\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q5_1 * GGML_RESTRICT x = vx;\n+    const block_q8_1 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    float summs0 = 0.0f;\n+    float summs1 = 0.0f;\n+\n+    uint32_t qh0;\n+    uint32_t qh1;\n+\n+    uint64_t tmp0[4];\n+    uint64_t tmp1[4];\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q5_1 * GGML_RESTRICT x0 = &x[ib];\n+        const block_q5_1 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_1 * GGML_RESTRICT y0 = &y[ib];\n+        const block_q8_1 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n+\n+        summs0 += GGML_FP16_TO_FP32(x0->m) * GGML_FP16_TO_FP32(y0->s);\n+        summs1 += GGML_FP16_TO_FP32(x1->m) * GGML_FP16_TO_FP32(y1->s);\n+\n+        // extract the 5th bit via lookup table ((b) << 4)\n+        memcpy(&qh0, x0->qh, sizeof(qh0));\n+        memcpy(&qh1, x1->qh, sizeof(qh1));\n+\n+        tmp0[0] = table_b2b_0[(qh0 >>  0) & 0xFF];\n+        tmp0[1] = table_b2b_0[(qh0 >>  8) & 0xFF];\n+        tmp0[2] = table_b2b_0[(qh0 >> 16) & 0xFF];\n+        tmp0[3] = table_b2b_0[(qh0 >> 24)       ];\n+\n+        tmp1[0] = table_b2b_0[(qh1 >>  0) & 0xFF];\n+        tmp1[1] = table_b2b_0[(qh1 >>  8) & 0xFF];\n+        tmp1[2] = table_b2b_0[(qh1 >> 16) & 0xFF];\n+        tmp1[3] = table_b2b_0[(qh1 >> 24)       ];\n+\n+        const int8x16_t qhl0 = vld1q_s8((const int8_t *)(tmp0 + 0));\n+        const int8x16_t qhh0 = vld1q_s8((const int8_t *)(tmp0 + 2));\n+        const int8x16_t qhl1 = vld1q_s8((const int8_t *)(tmp1 + 0));\n+        const int8x16_t qhh1 = vld1q_s8((const int8_t *)(tmp1 + 2));\n+\n+        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n+        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n+\n+        // 4-bit -> 8-bit\n+        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n+        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n+        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n+        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n+\n+        // add high bit\n+        const int8x16_t v0_0lf = vorrq_s8(v0_0l, qhl0);\n+        const int8x16_t v0_0hf = vorrq_s8(v0_0h, qhh0);\n+        const int8x16_t v0_1lf = vorrq_s8(v0_1l, qhl1);\n+        const int8x16_t v0_1hf = vorrq_s8(v0_1h, qhh1);\n+\n+        // load y\n+        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n+        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n+        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n+        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_0lf, v1_0l),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_0hf, v1_0h))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_1lf, v1_1l),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), v0_1hf, v1_1h))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1) + summs0 + summs1;\n+\n+#endif\n+    for (; ib < nb; ++ib) {\n+        uint32_t qh;\n+        memcpy(&qh, x[ib].qh, sizeof(qh));\n+\n+        int sumi0 = 0;\n+        int sumi1 = 0;\n+\n+        for (int j = 0; j < qk/2; ++j) {\n+            const uint8_t xh_0 = ((qh >> (j +  0)) << 4) & 0x10;\n+            const uint8_t xh_1 = ((qh >> (j + 12))     ) & 0x10;\n+\n+            const int32_t x0 = (x[ib].qs[j] & 0xF) | xh_0;\n+            const int32_t x1 = (x[ib].qs[j] >>  4) | xh_1;\n+\n+            sumi0 += (x0 * y[ib].qs[j]);\n+            sumi1 += (x1 * y[ib].qs[j + qk/2]);\n+        }\n+\n+        int sumi = sumi0 + sumi1;\n+        sumf += (GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d))*sumi + GGML_FP16_TO_FP32(x[ib].m)*GGML_FP16_TO_FP32(y[ib].s);\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_q8_0_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    const int qk = QK8_0;\n+    const int nb = n / qk;\n+\n+    assert(n % qk == 0);\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    assert((nrc == 2) || (nrc == 1));\n+#else\n+    assert(nrc == 1);\n+#endif\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q8_0 * GGML_RESTRICT x = vx;\n+    const block_q8_0 * GGML_RESTRICT y = vy;\n+\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    if (nrc == 2) {\n+        const block_q8_0 * GGML_RESTRICT vx0 = vx;\n+        const block_q8_0 * GGML_RESTRICT vx1 = (const block_q8_0 *) ((const uint8_t*)vx + bx);\n+        const block_q8_0 * GGML_RESTRICT vy0 = vy;\n+        const block_q8_0 * GGML_RESTRICT vy1 = (const block_q8_0 *) ((const uint8_t*)vy + by);\n+\n+        float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+\n+        for (int i = 0; i < nb; i++) {\n+            const block_q8_0 * GGML_RESTRICT b_x0 = &vx0[i];\n+            const block_q8_0 * GGML_RESTRICT b_y0 = &vy0[i];\n+\n+            const block_q8_0 * GGML_RESTRICT b_x1 = &vx1[i];\n+            const block_q8_0 * GGML_RESTRICT b_y1 = &vy1[i];\n+\n+            const int8x16_t x0_l = vld1q_s8(b_x0->qs);\n+            const int8x16_t x0_h = vld1q_s8(b_x0->qs + 16);\n+            const int8x16_t x1_l = vld1q_s8(b_x1->qs);\n+            const int8x16_t x1_h = vld1q_s8(b_x1->qs + 16);\n+\n+            // load y\n+            const int8x16_t y0_l = vld1q_s8(b_y0->qs);\n+            const int8x16_t y0_h = vld1q_s8(b_y0->qs + 16);\n+            const int8x16_t y1_l = vld1q_s8(b_y1->qs);\n+            const int8x16_t y1_h = vld1q_s8(b_y1->qs + 16);\n+\n+            float32_t _scale[4] = {\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x0->d)*GGML_FP16_TO_FP32(b_y1->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y0->d),\n+                GGML_FP16_TO_FP32(b_x1->d)*GGML_FP16_TO_FP32(b_y1->d)\n+            };\n+            float32x4_t scale = vld1q_f32(_scale);\n+\n+            int8x16_t l0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+            int8x16_t l1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_l), vreinterpretq_s64_s8(x1_l)));\n+\n+            int8x16_t l2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+            int8x16_t l3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(x0_h), vreinterpretq_s64_s8(x1_h)));\n+\n+            int8x16_t r0 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+            int8x16_t r1 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_l), vreinterpretq_s64_s8(y1_l)));\n+\n+            int8x16_t r2 = vreinterpretq_s8_s64(vzip1q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+            int8x16_t r3 = vreinterpretq_s8_s64(vzip2q_s64(vreinterpretq_s64_s8(y0_h), vreinterpretq_s64_s8(y1_h)));\n+\n+            sumv0 = vmlaq_f32(sumv0,(vcvtq_f32_s32(vmmlaq_s32((vmmlaq_s32((vmmlaq_s32((vmmlaq_s32(vdupq_n_s32(0), l0, r0)),\n+                                                l1, r1)), l2, r2)), l3, r3))), scale);\n+        }\n+\n+        float32x4_t sumv1 = vextq_f32 (sumv0, sumv0, 2);\n+        float32x4_t sumv2 = vzip1q_f32(sumv0, sumv1);\n+\n+        vst1_f32(s,      vget_low_f32 (sumv2));\n+        vst1_f32(s + bs, vget_high_f32(sumv2));\n+\n+        return;\n+    }\n+#endif\n+\n+    int ib = 0;\n+    float sumf = 0;\n+\n+#if defined(__ARM_FEATURE_SVE)\n+    svfloat32_t sumv0 = svdup_n_f32(0.0f);\n+    svfloat32_t sumv1 = svdup_n_f32(0.0f);\n+\n+    const int vector_length = ggml_cpu_get_sve_cnt()*8;\n+\n+    //VLA Implemenation for SVE\n+    switch (vector_length) {\n+        case 128:\n+            {\n+                // predicate for activating lanes for 16 Int8 elements\n+                const svbool_t ph16 = svptrue_pat_b8 (SV_VL16);\n+                const svbool_t pl16 = svptrue_pat_b32(SV_VL4);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q8_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svint8_t qx0_0 = svld1_s8(ph16, x0->qs);\n+                    const svint8_t qx0_1 = svld1_s8(ph16, x0->qs+16);\n+                    const svint8_t qx1_0 = svld1_s8(ph16, x1->qs);\n+                    const svint8_t qx1_1 = svld1_s8(ph16, x1->qs+16);\n+\n+                    // load y\n+                    const svint8_t qy0_0 = svld1_s8(ph16, y0->qs);\n+                    const svint8_t qy0_1 = svld1_s8(ph16, y0->qs+16);\n+                    const svint8_t qy1_0 = svld1_s8(ph16, y1->qs);\n+                    const svint8_t qy1_1 = svld1_s8(ph16, y1->qs+16);\n+\n+                    sumv0 = svmla_n_f32_x(pl16, sumv0, svcvt_f32_s32_x(pl16, svadd_x(pl16,\n+                                    svdot_s32(svdup_n_s32(0), qx0_0, qy0_0),\n+                                    svdot_s32(svdup_n_s32(0), qx0_1, qy0_1))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(pl16, sumv1, svcvt_f32_s32_x(pl16, svadd_x(pl16,\n+                                    svdot_s32(svdup_n_s32(0), qx1_0, qy1_0),\n+                                    svdot_s32(svdup_n_s32(0), qx1_1, qy1_1))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(pl16, svadd_f32_x(pl16, sumv0, sumv1));\n+            } break;\n+        case 256:\n+            {\n+                //printf(\"sve256\");\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q8_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    // load x\n+                    const svint8_t qx0 = svld1_s8(svptrue_b8(), x0->qs);\n+                    const svint8_t qx1 = svld1_s8(svptrue_b8(), x1->qs);\n+\n+                    // load y\n+                    const svint8_t qy0 = svld1_s8(svptrue_b8(), y0->qs);\n+                    const svint8_t qy1 = svld1_s8(svptrue_b8(), y1->qs);\n+\n+                    sumv0 = svmla_n_f32_x(svptrue_b32(), sumv0, svcvt_f32_s32_x(svptrue_b32(),\n+                                svdot_s32(svdup_n_s32(0), qx0, qy0)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+                    sumv1 = svmla_n_f32_x(svptrue_b32(), sumv1, svcvt_f32_s32_x(svptrue_b32(),\n+                                svdot_s32(svdup_n_s32(0), qx1, qy1)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+                }\n+\n+                sumf = svaddv_f32(svptrue_b32(), svadd_f32_x(svptrue_b32(), sumv0, sumv1));\n+            } break;\n+        case 512:\n+            {\n+                // predicate for activating high 256 bit\n+                const svbool_t ph32 = svptrue_pat_b8(SV_VL32);\n+                // predicate for activating low 256 bit\n+                const svbool_t pl32 = svnot_b_z(svptrue_b8(), ph32);\n+\n+                // predicate for activating high lanes for 8 float32 elements\n+                const svbool_t ph8 = svptrue_pat_b32(SV_VL8);\n+                // predicate for activating low lanes for 8 float32 elements\n+                const svbool_t pl8 = svnot_b_z(svptrue_b32(), ph8);\n+\n+                svfloat32_t sumv00 = svdup_n_f32(0.0f);\n+\n+                for (; ib + 1 < nb; ib += 2) {\n+                    const block_q8_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+                    const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+                    const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+                    //load 32 int8_t in first half of vector and put another 32 int8_t in second vector lower bits\n+                    // and add them to make one 64 element vector\n+                    // load x\n+                    const svint8_t qx_32 = svld1_s8(ph32, x0->qs);\n+                          svint8_t qx_64 = svld1_s8(pl32, x0->qs + 2);\n+\n+                    qx_64 = svadd_s8_x(svptrue_b8(), qx_32, qx_64);\n+\n+                    // load y\n+                    const svint8_t qy_32 = svld1_s8(ph32, y0->qs);\n+                          svint8_t qy_64 = svld1_s8(pl32, y0->qs + 2);\n+\n+                    qy_64 = svadd_s8_x(svptrue_b8(), qy_32, qy_64);\n+\n+                    // scale creation\n+                    const float32_t deq1 = GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d);\n+                    const float32_t deq2 = GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d);\n+\n+                    // duplicate deq1 in first half of vector and deq2 in second half of vector\n+                    const svfloat32_t temp = svdup_f32_m(svdup_f32_z(ph8, deq1), pl8, deq2);\n+\n+                    const svfloat32_t sumvt = svcvt_f32_s32_x(svptrue_b32(), svdot_s32(svdup_n_s32(0), qx_64, qy_64));\n+\n+                    sumv00 = svmla_f32_m(svptrue_b32(), sumv00, sumvt, temp);\n+                }\n+\n+                sumf = svaddv_f32(svptrue_b32(), sumv00);\n+                break;\n+            }\n+        default:\n+            assert(false && \"Unsupported vector length\");\n+            break;\n+    }\n+#elif defined(__ARM_NEON)\n+    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n+    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n+\n+    for (; ib + 1 < nb; ib += 2) {\n+        const block_q8_0 * GGML_RESTRICT x0 = &x[ib + 0];\n+        const block_q8_0 * GGML_RESTRICT x1 = &x[ib + 1];\n+        const block_q8_0 * GGML_RESTRICT y0 = &y[ib + 0];\n+        const block_q8_0 * GGML_RESTRICT y1 = &y[ib + 1];\n+\n+        const int8x16_t x0_0 = vld1q_s8(x0->qs);\n+        const int8x16_t x0_1 = vld1q_s8(x0->qs + 16);\n+        const int8x16_t x1_0 = vld1q_s8(x1->qs);\n+        const int8x16_t x1_1 = vld1q_s8(x1->qs + 16);\n+\n+        // load y\n+        const int8x16_t y0_0 = vld1q_s8(y0->qs);\n+        const int8x16_t y0_1 = vld1q_s8(y0->qs + 16);\n+        const int8x16_t y1_0 = vld1q_s8(y1->qs);\n+        const int8x16_t y1_1 = vld1q_s8(y1->qs + 16);\n+\n+        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), x0_0, y0_0),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), x0_1, y0_1))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n+\n+        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n+                        ggml_vdotq_s32(vdupq_n_s32(0), x1_0, y1_0),\n+                        ggml_vdotq_s32(vdupq_n_s32(0), x1_1, y1_1))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n+    }\n+\n+    sumf = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n+#endif\n+    for (; ib < nb; ++ib) {\n+        int sumi = 0;\n+\n+        for (int j = 0; j < qk; j++) {\n+            sumi += x[ib].qs[j]*y[ib].qs[j];\n+        }\n+\n+        sumf += sumi*(GGML_FP16_TO_FP32(x[ib].d)*GGML_FP16_TO_FP32(y[ib].d));\n+    }\n+\n+    *s = sumf;\n+}\n+\n+void ggml_vec_dot_tq1_0_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_tq1_0 * GGML_RESTRICT x = vx;\n+    const block_q8_K  * GGML_RESTRICT y = vy;\n+\n+    const int nb = n / QK_K;\n+\n+#if defined(__ARM_NEON)\n+    float sumf = 0.0f;\n+\n+    uint8_t k_shift[16] = {1, 1, 1, 1, 3, 3, 3, 3, 9, 9, 9, 9, 27, 27, 27, 27};\n+\n+    const uint8x16_t shift = vld1q_u8(k_shift);\n+\n+    for (int i = 0; i < nb; ++i) {\n+#if defined(__ARM_FEATURE_DOTPROD)\n+        int32x4_t sumi0 = vdupq_n_s32(0);\n+        int32x4_t sumi1 = vdupq_n_s32(0);\n+#else\n+        int16x8_t sumi0 = vdupq_n_s16(0);\n+        int16x8_t sumi1 = vdupq_n_s16(0);\n+#endif\n+\n+        // first 32 bytes of 5 elements\n+        {\n+            uint8x16_t qx0 = vld1q_u8(x[i].qs + 0);\n+            uint8x16_t qx1 = vld1q_u8(x[i].qs + 16);\n+            uint8x16_t qx2 = vmulq_u8(qx0, vdupq_n_u8(3));\n+            uint8x16_t qx3 = vmulq_u8(qx1, vdupq_n_u8(3));\n+            uint8x16_t qx4 = vmulq_u8(qx0, vdupq_n_u8(9));\n+            uint8x16_t qx5 = vmulq_u8(qx1, vdupq_n_u8(9));\n+            uint8x16_t qx6 = vmulq_u8(qx0, vdupq_n_u8(27));\n+            uint8x16_t qx7 = vmulq_u8(qx1, vdupq_n_u8(27));\n+            uint8x16_t qx8 = vmulq_u8(qx0, vdupq_n_u8(81));\n+            uint8x16_t qx9 = vmulq_u8(qx1, vdupq_n_u8(81));\n+\n+            // multiply by 3 and keep the 2 bits above 8 bits\n+            int8x16_t sqx0 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx0, vshrq_n_u8(qx0, 1)), 6));\n+            int8x16_t sqx1 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx1, vshrq_n_u8(qx1, 1)), 6));\n+            int8x16_t sqx2 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx2, vshrq_n_u8(qx2, 1)), 6));\n+            int8x16_t sqx3 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx3, vshrq_n_u8(qx3, 1)), 6));\n+            int8x16_t sqx4 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx4, vshrq_n_u8(qx4, 1)), 6));\n+            int8x16_t sqx5 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx5, vshrq_n_u8(qx5, 1)), 6));\n+            int8x16_t sqx6 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx6, vshrq_n_u8(qx6, 1)), 6));\n+            int8x16_t sqx7 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx7, vshrq_n_u8(qx7, 1)), 6));\n+            int8x16_t sqx8 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx8, vshrq_n_u8(qx8, 1)), 6));\n+            int8x16_t sqx9 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx9, vshrq_n_u8(qx9, 1)), 6));\n+\n+            const int8x16_t qy0 = vld1q_s8(y[i].qs +   0);\n+            const int8x16_t qy1 = vld1q_s8(y[i].qs +  16);\n+            const int8x16_t qy2 = vld1q_s8(y[i].qs +  32);\n+            const int8x16_t qy3 = vld1q_s8(y[i].qs +  48);\n+            const int8x16_t qy4 = vld1q_s8(y[i].qs +  64);\n+            const int8x16_t qy5 = vld1q_s8(y[i].qs +  80);\n+            const int8x16_t qy6 = vld1q_s8(y[i].qs +  96);\n+            const int8x16_t qy7 = vld1q_s8(y[i].qs + 112);\n+            const int8x16_t qy8 = vld1q_s8(y[i].qs + 128);\n+            const int8x16_t qy9 = vld1q_s8(y[i].qs + 144);\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+            sumi0 = vdotq_s32(sumi0, sqx0, qy0);\n+            sumi1 = vdotq_s32(sumi1, sqx1, qy1);\n+            sumi0 = vdotq_s32(sumi0, sqx2, qy2);\n+            sumi1 = vdotq_s32(sumi1, sqx3, qy3);\n+            sumi0 = vdotq_s32(sumi0, sqx4, qy4);\n+            sumi1 = vdotq_s32(sumi1, sqx5, qy5);\n+            sumi0 = vdotq_s32(sumi0, sqx6, qy6);\n+            sumi1 = vdotq_s32(sumi1, sqx7, qy7);\n+            sumi0 = vdotq_s32(sumi0, sqx8, qy8);\n+            sumi1 = vdotq_s32(sumi1, sqx9, qy9);\n+#else\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx0), vget_low_s8(qy0));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx0), vget_high_s8(qy0));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx1), vget_low_s8(qy1));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx1), vget_high_s8(qy1));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx2), vget_low_s8(qy2));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx2), vget_high_s8(qy2));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx3), vget_low_s8(qy3));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx3), vget_high_s8(qy3));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx4), vget_low_s8(qy4));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx4), vget_high_s8(qy4));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx5), vget_low_s8(qy5));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx5), vget_high_s8(qy5));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx6), vget_low_s8(qy6));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx6), vget_high_s8(qy6));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx7), vget_low_s8(qy7));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx7), vget_high_s8(qy7));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx8), vget_low_s8(qy8));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx8), vget_high_s8(qy8));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx9), vget_low_s8(qy9));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx9), vget_high_s8(qy9));\n+#endif\n+        }\n+\n+        // last 16 bytes of 5-element, along with the 4 bytes of 4 elements\n+        {\n+            uint8x16_t qx0 = vld1q_u8(x[i].qs + 32);\n+            uint8x16_t qx1 = vmulq_u8(qx0, vdupq_n_u8(3));\n+            uint8x16_t qx2 = vmulq_u8(qx0, vdupq_n_u8(9));\n+            uint8x16_t qx3 = vmulq_u8(qx0, vdupq_n_u8(27));\n+            uint8x16_t qx4 = vmulq_u8(qx0, vdupq_n_u8(81));\n+            uint32_t qh;\n+            memcpy(&qh, x[i].qh, sizeof(qh)); // potentially unaligned\n+            uint8x16_t qx5 = vreinterpretq_u8_u32(vdupq_n_u32(qh));\n+            qx5 = vmulq_u8(qx5, shift);\n+\n+            // multiply by 3 and keep the 2 bits above 8 bits\n+            int8x16_t sqx0 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx0, vshrq_n_u8(qx0, 1)), 6));\n+            int8x16_t sqx1 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx1, vshrq_n_u8(qx1, 1)), 6));\n+            int8x16_t sqx2 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx2, vshrq_n_u8(qx2, 1)), 6));\n+            int8x16_t sqx3 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx3, vshrq_n_u8(qx3, 1)), 6));\n+            int8x16_t sqx4 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx4, vshrq_n_u8(qx4, 1)), 6));\n+            int8x16_t sqx5 = vreinterpretq_s8_u8(vshrq_n_u8(vhaddq_u8(qx5, vshrq_n_u8(qx5, 1)), 6));\n+\n+            const int8x16_t qy0 = vld1q_s8(y[i].qs + 160);\n+            const int8x16_t qy1 = vld1q_s8(y[i].qs + 176);\n+            const int8x16_t qy2 = vld1q_s8(y[i].qs + 192);\n+            const int8x16_t qy3 = vld1q_s8(y[i].qs + 208);\n+            const int8x16_t qy4 = vld1q_s8(y[i].qs + 224);\n+            const int8x16_t qy5 = vld1q_s8(y[i].qs + 240);\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+            sumi0 = vdotq_s32(sumi0, sqx0, qy0);\n+            sumi1 = vdotq_s32(sumi1, sqx1, qy1);\n+            sumi0 = vdotq_s32(sumi0, sqx2, qy2);\n+            sumi1 = vdotq_s32(sumi1, sqx3, qy3);\n+            sumi0 = vdotq_s32(sumi0, sqx4, qy4);\n+            sumi1 = vdotq_s32(sumi1, sqx5, qy5);\n+#else\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx0), vget_low_s8(qy0));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx0), vget_high_s8(qy0));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx1), vget_low_s8(qy1));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx1), vget_high_s8(qy1));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx2), vget_low_s8(qy2));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx2), vget_high_s8(qy2));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx3), vget_low_s8(qy3));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx3), vget_high_s8(qy3));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx4), vget_low_s8(qy4));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx4), vget_high_s8(qy4));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx5), vget_low_s8(qy5));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx5), vget_high_s8(qy5));\n+#endif\n+        }\n+\n+        const int16x8_t ysum0 = vld1q_s16(y[i].bsums);\n+        const int16x8_t ysum1 = vld1q_s16(y[i].bsums + 8);\n+\n+        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+        sumi0 = vaddq_s32(sumi0, sumi1);\n+        sumi0 = vsubq_s32(sumi0, vpaddlq_s16(vaddq_s16(ysum0, ysum1)));\n+\n+        sumf += d * (float) vaddvq_s32(sumi0);\n+#else\n+        sumi0 = vaddq_s16(sumi0, sumi1);\n+        sumi0 = vsubq_s16(sumi0, vaddq_s16(ysum0, ysum1));\n+\n+        sumf += d * (float) vaddlvq_s16(sumi0);\n+#endif\n+    }\n+\n+    *s = sumf;\n+\n+#else\n+    const uint8_t pow3[6] = {1, 3, 9, 27, 81, 243};\n+\n+    float sumf = 0.0f;\n+\n+    for (int i = 0; i < nb; ++i) {\n+        int sum = 0;\n+\n+        for (size_t j = 0; j < sizeof(x->qs) - sizeof(x->qs) % 32; j += 32) {\n+            for (size_t l = 0; l < 5; ++l) {\n+                for (size_t m = 0; m < 32; ++m) {\n+                    uint8_t q = x[i].qs[j + m] * pow3[l];\n+                    uint16_t xi = ((uint16_t) q * 3) >> 8;\n+                    sum += (xi - 1) * y[i].qs[j*5 + l*32 + m];\n+                }\n+            }\n+        }\n+        for (size_t j = sizeof(x->qs) - sizeof(x->qs) % 32; j < sizeof(x->qs); j += 16) {\n+            for (size_t l = 0; l < 5; ++l) {\n+                for (size_t m = 0; m < 16; ++m) {\n+                    uint8_t q = x[i].qs[j + m] * pow3[l];\n+                    uint16_t xi = ((uint16_t) q * 3) >> 8;\n+                    sum += (xi - 1) * y[i].qs[j*5 + l*16 + m];\n+                }\n+            }\n+        }\n+\n+        for (size_t l = 0; l < 4; ++l) {\n+            for (size_t j = 0; j < sizeof(x->qh); ++j) {\n+                uint8_t q = x[i].qh[j] * pow3[l];\n+                uint16_t xi = ((uint16_t) q * 3) >> 8;\n+                sum += (xi - 1) * y[i].qs[sizeof(x->qs)*5 + l*sizeof(x->qh) + j];\n+            }\n+        }\n+\n+        sumf += (float) sum * (GGML_FP16_TO_FP32(x[i].d) * y[i].d);\n+    }\n+\n+    *s = sumf;\n+#endif\n+}\n+\n+void ggml_vec_dot_tq2_0_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_tq2_0 * GGML_RESTRICT x = vx;\n+    const block_q8_K  * GGML_RESTRICT y = vy;\n+\n+    const int nb = n / QK_K;\n+\n+#if defined(__ARM_NEON)\n+    float sumf = 0.0f;\n+\n+    const uint8x16_t m3 = vdupq_n_u8(3);\n+\n+    for (int i = 0; i < nb; ++i) {\n+#if defined(__ARM_FEATURE_DOTPROD)\n+        int32x4_t sumi0 = vdupq_n_s32(0);\n+        int32x4_t sumi1 = vdupq_n_s32(0);\n+#else\n+        int16x8_t sumi0 = vdupq_n_s16(0);\n+        int16x8_t sumi1 = vdupq_n_s16(0);\n+#endif\n+\n+        for (size_t j = 0; j < sizeof(x->qs); j += 32) {\n+            uint8x16_t qx0 = vld1q_u8(x[i].qs + j);\n+            uint8x16_t qx1 = vld1q_u8(x[i].qs + j + 16);\n+            uint8x16_t qx2 = vshrq_n_u8(qx0, 2);\n+            uint8x16_t qx3 = vshrq_n_u8(qx1, 2);\n+            uint8x16_t qx4 = vshrq_n_u8(qx0, 4);\n+            uint8x16_t qx5 = vshrq_n_u8(qx1, 4);\n+            uint8x16_t qx6 = vshrq_n_u8(qx0, 6);\n+            uint8x16_t qx7 = vshrq_n_u8(qx1, 6);\n+\n+            int8x16_t sqx0 = vreinterpretq_s8_u8(vandq_u8(qx0, m3));\n+            int8x16_t sqx1 = vreinterpretq_s8_u8(vandq_u8(qx1, m3));\n+            int8x16_t sqx2 = vreinterpretq_s8_u8(vandq_u8(qx2, m3));\n+            int8x16_t sqx3 = vreinterpretq_s8_u8(vandq_u8(qx3, m3));\n+            int8x16_t sqx4 = vreinterpretq_s8_u8(vandq_u8(qx4, m3));\n+            int8x16_t sqx5 = vreinterpretq_s8_u8(vandq_u8(qx5, m3));\n+            int8x16_t sqx6 = vreinterpretq_s8_u8(vandq_u8(qx6, m3));\n+            int8x16_t sqx7 = vreinterpretq_s8_u8(vandq_u8(qx7, m3));\n+\n+            const int8x16_t qy0 = vld1q_s8(y[i].qs + j*4 +   0);\n+            const int8x16_t qy1 = vld1q_s8(y[i].qs + j*4 +  16);\n+            const int8x16_t qy2 = vld1q_s8(y[i].qs + j*4 +  32);\n+            const int8x16_t qy3 = vld1q_s8(y[i].qs + j*4 +  48);\n+            const int8x16_t qy4 = vld1q_s8(y[i].qs + j*4 +  64);\n+            const int8x16_t qy5 = vld1q_s8(y[i].qs + j*4 +  80);\n+            const int8x16_t qy6 = vld1q_s8(y[i].qs + j*4 +  96);\n+            const int8x16_t qy7 = vld1q_s8(y[i].qs + j*4 + 112);\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+            sumi0 = vdotq_s32(sumi0, sqx0, qy0);\n+            sumi1 = vdotq_s32(sumi1, sqx1, qy1);\n+            sumi0 = vdotq_s32(sumi0, sqx2, qy2);\n+            sumi1 = vdotq_s32(sumi1, sqx3, qy3);\n+            sumi0 = vdotq_s32(sumi0, sqx4, qy4);\n+            sumi1 = vdotq_s32(sumi1, sqx5, qy5);\n+            sumi0 = vdotq_s32(sumi0, sqx6, qy6);\n+            sumi1 = vdotq_s32(sumi1, sqx7, qy7);\n+#else\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx0), vget_low_s8(qy0));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx0), vget_high_s8(qy0));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx1), vget_low_s8(qy1));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx1), vget_high_s8(qy1));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx2), vget_low_s8(qy2));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx2), vget_high_s8(qy2));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx3), vget_low_s8(qy3));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx3), vget_high_s8(qy3));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx4), vget_low_s8(qy4));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx4), vget_high_s8(qy4));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx5), vget_low_s8(qy5));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx5), vget_high_s8(qy5));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx6), vget_low_s8(qy6));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx6), vget_high_s8(qy6));\n+            sumi0 = vmlal_s8(sumi0, vget_low_s8(sqx7), vget_low_s8(qy7));\n+            sumi1 = vmlal_s8(sumi1, vget_high_s8(sqx7), vget_high_s8(qy7));\n+#endif\n+        }\n+\n+        const int16x8_t ysum0 = vld1q_s16(y[i].bsums);\n+        const int16x8_t ysum1 = vld1q_s16(y[i].bsums + 8);\n+\n+        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n+\n+#if defined(__ARM_FEATURE_DOTPROD)\n+        sumi0 = vaddq_s32(sumi0, sumi1);\n+        sumi0 = vsubq_s32(sumi0, vpaddlq_s16(vaddq_s16(ysum0, ysum1)));\n+\n+        sumf += d * (float) vaddvq_s32(sumi0);\n+#else\n+        sumi0 = vaddq_s16(sumi0, sumi1);\n+        sumi0 = vsubq_s16(sumi0, vaddq_s16(ysum0, ysum1));\n+\n+        sumf += d * (float) vaddlvq_s16(sumi0);\n+#endif\n+    }\n+\n+    *s = sumf;\n+\n+#else\n+    float sumf = 0.0f;\n+\n+    for (int i = 0; i < nb; ++i) {\n+        int32_t sumi = 0;\n+\n+        for (size_t j = 0; j < sizeof(x->qs); j += 32) {\n+            for (size_t l = 0; l < 4; ++l) {\n+                for (size_t k = 0; k < 32; ++k) {\n+                    sumi += y[i].qs[j*4 + l*32 + k] * (((x[i].qs[j + k] >> (l*2)) & 3) - 1);\n+                }\n+            }\n+        }\n+\n+        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+\n+        sumf += (float) sumi * d;\n+    }\n+\n+    *s = sumf;\n+#endif\n+}\n+\n+void ggml_vec_dot_q2_K_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const block_q2_K * GGML_RESTRICT x = vx;\n+    const block_q8_K * GGML_RESTRICT y = vy;\n+\n+    const int nb = n / QK_K;\n+\n+#ifdef __ARM_FEATURE_SVE\n+    const int vector_length = svcntb()*8;\n+    const svuint8_t m3s = svdup_n_u8(0x3);\n+    const svuint32_t m4s = svdup_n_u32(0xF);\n+    const svint32_t vzero_sv = svdup_n_s32(0);\n+    svfloat32_t acc_sum = svdup_n_f32(0);\n+    svbool_t pred_s32 = svptrue_pat_b32(SV_VL4);\n+\n+    switch (vector_length) {\n+        case 128:\n+            for (int i = 0; i < nb; ++i) {\n+                const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+                svfloat32_t d_broad = svdup_n_f32((float32_t)d);\n+                const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n+                svfloat32_t dmin_broad = svdup_n_f32((float32_t)dmin);\n+\n+                const uint8_t * GGML_RESTRICT q2 = x[i].qs;\n+                const int8_t  * GGML_RESTRICT q8_sv = y[i].qs;\n+                const uint8_t * GGML_RESTRICT sc = x[i].scales;\n+\n+                svuint32_t mins_and_scales_sve = svld1ub_u32(svptrue_b32(), sc);\n+                const svint32_t mins_sv_1 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_b32(), mins_and_scales_sve, 4));\n+\n+                mins_and_scales_sve = svld1ub_u32(svptrue_b32(), sc+4);\n+                const svint32_t mins_sv_2 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_b32(), mins_and_scales_sve, 4));\n+\n+                svint32_t q8sums_sv_1 = svld1sh_s32(svptrue_b32(), y[i].bsums);\n+                svint32_t q8sums_sv_2 = svld1sh_s32(svptrue_b32(), y[i].bsums+4);\n+\n+                const svint32_t s0 = svadd_s32_x(svptrue_b32(), svmul_s32_x(svptrue_b32(), mins_sv_1, q8sums_sv_1), svmul_s32_x(svptrue_b32(), mins_sv_2, q8sums_sv_2));\n+\n+                mins_and_scales_sve = svld1ub_u32(svptrue_b32(), sc+8);\n+                const svint32_t mins_sv_3 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_b32(), mins_and_scales_sve, 4));\n+\n+                mins_and_scales_sve = svld1ub_u32(svptrue_b32(), sc+12);\n+                const svint32_t mins_sv_4 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_b32(), mins_and_scales_sve, 4));\n+\n+                q8sums_sv_1 = svld1sh_s32(svptrue_b32(), y[i].bsums+8);\n+                q8sums_sv_2 = svld1sh_s32(svptrue_b32(), y[i].bsums+12);\n+\n+                svint32_t s1 = svadd_s32_x(svptrue_b32(), svmul_s32_x(svptrue_b32(), mins_sv_3, q8sums_sv_1), svmul_s32_x(svptrue_b32(), mins_sv_4, q8sums_sv_2));\n+\n+                svfloat32_t temp = svcvt_f32_s32_x(svptrue_b32(), svadd_s32_x(svptrue_b32(), s0, s1));\n+\n+                acc_sum = svmla_f32_m(svptrue_b32(), acc_sum, temp, dmin_broad);\n+\n+                svint32_t sumi1 = svdup_n_s32(0);\n+\n+                {\n+                    const svuint8_t q2bits_1 = svld1_u8(svptrue_b8(), q2);\n+                    svint8_t q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), q2bits_1, m3s));\n+                    svint8_t q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                    const svint32_t scales_sv = svreinterpret_s32_u32(svand_u32_m(svptrue_b32(), svld1ub_u32(svptrue_b32(), sc), m4s));\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv, 0));\n+\n+                    const svuint8_t q2bits_3 = svld1_u8(svptrue_b8(), q2+16);\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), q2bits_3, m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv, 1));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_1, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv, 2));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_3, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv, 3));\n+\n+\n+                    const svint32_t scales_sv_1 = svreinterpret_s32_u32(svand_u32_m(svptrue_b32(), svld1ub_u32(svptrue_b32(), sc+4), m4s));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_1, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_1, 0));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_3, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_1, 1));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_1, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_1, 2));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_3, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_1, 3));\n+\n+                    //-------------------------------\n+\n+                    q2 += 32;\n+                    const svint32_t scales_sv_2 = svreinterpret_s32_u32(svand_u32_m(svptrue_b32(), svld1ub_u32(svptrue_b32(), sc+8), m4s));\n+                    const svuint8_t q2bits_2 = svld1_u8(svptrue_b8(), q2);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), q2bits_2, m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_2, 0));\n+\n+                    const svuint8_t q2bits_4 = svld1_u8(svptrue_b8(), q2+16);\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), q2bits_4, m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_2, 1));\n+\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_2, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_2, 2));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_4, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_2, 3));\n+\n+\n+                    const svint32_t scales_sv_3 = svreinterpret_s32_u32(svand_u32_m(svptrue_b32(), svld1ub_u32(svptrue_b32(), sc+12), m4s));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_2, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_3, 0));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_4, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_3, 1));\n+\n+\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_2, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_3, 2));\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_x(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q2bits_4, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                    sumi1 = svmla_s32_m(svptrue_b32(), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), svdup_lane_s32(scales_sv_3, 3));\n+                }\n+                acc_sum = svmla_f32_m(svptrue_b32(), acc_sum, svcvt_f32_s32_x(svptrue_b32(), sumi1), d_broad);\n+            }\n+            *s = svaddv_f32(svptrue_b32(), acc_sum);\n+            break;\n+\n+        case 256:\n+        case 512:\n+            for (int i = 0; i < nb; ++i) {\n+                const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+                svfloat32_t d_broad = svdup_n_f32((float32_t)d);\n+                const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n+                svfloat32_t dmin_broad = svdup_n_f32((float32_t)dmin);\n+\n+                const uint8_t * GGML_RESTRICT q2 = x[i].qs;\n+                const int8_t  * GGML_RESTRICT q8_sv = y[i].qs;\n+                const uint8_t * GGML_RESTRICT sc = x[i].scales;\n+\n+                const svuint32_t mins_and_scales_sve = svld1ub_u32(svptrue_pat_b32(SV_VL8), sc); sc += 8;\n+                const svint32_t scales_sv = svreinterpret_s32_u32(svand_u32_m(svptrue_pat_b32(SV_VL8), mins_and_scales_sve, m4s));\n+                const svint32_t mins_sv_1 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_pat_b32(SV_VL8), mins_and_scales_sve, 4));\n+                svint32_t q8sums_sv_1 = svld1sh_s32(svptrue_pat_b32(SV_VL8), y[i].bsums);\n+\n+                const svuint32_t mins_and_scales_sve_1 = svld1ub_u32(svptrue_pat_b32(SV_VL8), sc);\n+                const svint32_t scales_sv_1 = svreinterpret_s32_u32(svand_u32_m(svptrue_pat_b32(SV_VL8), mins_and_scales_sve_1, m4s));\n+                const svint32_t mins_sv_2 = svreinterpret_s32_u32(svlsr_n_u32_x(svptrue_pat_b32(SV_VL8), mins_and_scales_sve_1, 4));\n+\n+                svint32_t q8sums_sv_2 = svld1sh_s32(svptrue_pat_b32(SV_VL8), y[i].bsums+8);\n+\n+                svfloat32_t temp = svcvt_f32_s32_x(svptrue_pat_b32(SV_VL8), svadd_s32_x(svptrue_pat_b32(SV_VL8), svmul_s32_x(svptrue_pat_b32(SV_VL8), mins_sv_1, q8sums_sv_1), svmul_s32_x(svptrue_pat_b32(SV_VL8), mins_sv_2, q8sums_sv_2)));\n+\n+                acc_sum = svmla_f32_m(svptrue_pat_b32(SV_VL8), acc_sum, temp, dmin_broad);\n+\n+                svint32_t sumi1 = svdup_n_s32(0);\n+\n+                {\n+                    const svuint8_t q2bits_1 = svld1_u8(svptrue_pat_b8(SV_VL32), q2);\n+                    svint8_t q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), q2bits_1, m3s));\n+                    svint8_t q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    svint32_t scale_1 = svsel(pred_s32, svdup_lane_s32(scales_sv, 0), svdup_lane_s32(scales_sv, 1));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_1);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_1, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    svint32_t scale_2 = svsel(pred_s32, svdup_lane_s32(scales_sv, 2), svdup_lane_s32(scales_sv, 3));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(svdup_n_s32(0), q2bytes_sv, q8bytes_sv), scale_2);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_1, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_1 = svsel(pred_s32, svdup_lane_s32(scales_sv, 4), svdup_lane_s32(scales_sv, 5));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_1);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_1, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_2 = svsel(pred_s32, svdup_lane_s32(scales_sv, 6), svdup_lane_s32(scales_sv, 7));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_2);\n+\n+                    q2 += 32;\n+\n+                    const svuint8_t q2bits_2 = svld1_u8(svptrue_pat_b8(SV_VL32), q2);\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), q2bits_2, m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_1 = svsel(pred_s32, svdup_lane_s32(scales_sv_1, 0), svdup_lane_s32(scales_sv_1, 1));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_1);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_2, 2), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_2 = svsel(pred_s32, svdup_lane_s32(scales_sv_1, 2), svdup_lane_s32(scales_sv_1, 3));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_2);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_2, 4), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_1 = svsel(pred_s32, svdup_lane_s32(scales_sv_1, 4), svdup_lane_s32(scales_sv_1, 5));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_1);\n+\n+                    q2bytes_sv = svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q2bits_2, 6), m3s));\n+                    q8bytes_sv = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                    scale_2 = svsel(pred_s32, svdup_lane_s32(scales_sv_1, 6), svdup_lane_s32(scales_sv_1, 7));\n+                    sumi1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1, svdot_s32(vzero_sv, q2bytes_sv, q8bytes_sv), scale_2);\n+                }\n+                acc_sum = svmla_f32_m(svptrue_pat_b32(SV_VL8), acc_sum, svcvt_f32_s32_x(svptrue_pat_b32(SV_VL8), sumi1), d_broad);\n+            }\n+            *s = svaddv_f32(svptrue_pat_b32(SV_VL8), acc_sum);\n+            break;\n+\n+        default:\n+            assert(false && \"Unsupported vector length\");\n+            break;\n+    }\n+\n+#elif __ARM_NEON\n+    const uint8x16_t m3 = vdupq_n_u8(0x3);\n+    const uint8x16_t m4 = vdupq_n_u8(0xF);\n+\n+    const int32x4_t vzero = vdupq_n_s32(0);\n+\n+    ggml_int8x16x2_t q2bytes;\n+    uint8_t aux[16];\n+\n+    float sum = 0;\n+\n+    for (int i = 0; i < nb; ++i) {\n+        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n+\n+        const uint8_t * GGML_RESTRICT q2 = x[i].qs;\n+        const int8_t  * GGML_RESTRICT q8 = y[i].qs;\n+        const uint8_t * GGML_RESTRICT sc = x[i].scales;\n+\n+        const uint8x16_t mins_and_scales = vld1q_u8(sc);\n+        const uint8x16_t scales = vandq_u8(mins_and_scales, m4);\n+        vst1q_u8(aux, scales);\n+\n+        const uint8x16_t mins = vshrq_n_u8(mins_and_scales, 4);\n+        const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\n+        const ggml_int16x8x2_t mins16 = {{vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(mins))), vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(mins)))}};\n+        const int32x4_t s0 = vaddq_s32(vmull_s16(vget_low_s16 (mins16.val[0]), vget_low_s16 (q8sums.val[0])),\n+                                       vmull_s16(vget_high_s16(mins16.val[0]), vget_high_s16(q8sums.val[0])));\n+        const int32x4_t s1 = vaddq_s32(vmull_s16(vget_low_s16 (mins16.val[1]), vget_low_s16 (q8sums.val[1])),\n+                                       vmull_s16(vget_high_s16(mins16.val[1]), vget_high_s16(q8sums.val[1])));\n+        sum += dmin * vaddvq_s32(vaddq_s32(s0, s1));\n+\n+        int isum = 0;\n+        int is = 0;\n+\n+// We use this macro instead of a function call because for some reason\n+// the code runs 2-3% slower, even if the function is declared inline\n+#define MULTIPLY_ACCUM_WITH_SCALE(index)\\\n+        isum += vaddvq_s32(ggml_vdotq_s32(vzero, q2bytes.val[0], q8bytes.val[0])) * aux[is+(index)];\\\n+        isum += vaddvq_s32(ggml_vdotq_s32(vzero, q2bytes.val[1], q8bytes.val[1])) * aux[is+1+(index)];\n+\n+#define SHIFT_MULTIPLY_ACCUM_WITH_SCALE(shift, index)\\\n+        q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\n+        q2bytes.val[0] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits.val[0], (shift)), m3));\\\n+        q2bytes.val[1] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits.val[1], (shift)), m3));\\\n+        MULTIPLY_ACCUM_WITH_SCALE((index));\n+\n+        for (int j = 0; j < QK_K/128; ++j) {\n+            const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\n+\n+            ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\n+            q2bytes.val[0] = vreinterpretq_s8_u8(vandq_u8(q2bits.val[0], m3));\n+            q2bytes.val[1] = vreinterpretq_s8_u8(vandq_u8(q2bits.val[1], m3));\n+\n+            MULTIPLY_ACCUM_WITH_SCALE(0);\n+\n+            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(2, 2);\n+            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(4, 4);\n+            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(6, 6);\n+\n+            is += 8;\n+        }\n+\n+        sum += d * isum;\n+    }\n+\n+    *s = sum;\n+\n+#else\n+\n+    float sumf = 0;\n+\n+    for (int i = 0; i < nb; ++i) {\n+\n+        const uint8_t * q2 = x[i].qs;\n+        const  int8_t * q8 = y[i].qs;\n+        const uint8_t * sc = x[i].scales;\n+\n+        int summs = 0;\n+        for (int j = 0; j < 16; ++j) {\n+            summs += y[i].bsums[j] * (sc[j] >> 4);\n+        }\n+\n+        const float dall = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+        const float dmin = y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n+\n+        int isum = 0;\n+        int is = 0;\n+        int d;\n+        for (int k = 0; k < QK_K/128; ++k) {\n+            int shift = 0;\n+            for (int j = 0; j < 4; ++j) {\n+                d = sc[is++] & 0xF;\n+                int isuml = 0;\n+                for (int l =  0; l < 16; ++l) isuml += q8[l] * ((q2[l] >> shift) & 3);\n+                isum += d * isuml;\n+                d = sc[is++] & 0xF;\n+                isuml = 0;\n+                for (int l = 16; l < 32; ++l) isuml += q8[l] * ((q2[l] >> shift) & 3);\n+                isum += d * isuml;\n+                shift += 2;\n+                q8 += 32;\n+            }\n+            q2 += 32;\n+        }\n+        sumf += dall * isum - dmin * summs;\n+    }\n+    *s = sumf;\n+#endif\n+}\n+\n+void ggml_vec_dot_q3_K_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(n % QK_K == 0);\n+    assert(nrc == 1);\n+    UNUSED(nrc);\n+    UNUSED(bx);\n+    UNUSED(by);\n+    UNUSED(bs);\n+\n+    const uint32_t kmask1 = 0x03030303;\n+    const uint32_t kmask2 = 0x0f0f0f0f;\n+\n+    const block_q3_K * GGML_RESTRICT x = vx;\n+    const block_q8_K * GGML_RESTRICT y = vy;\n+\n+    const int nb = n / QK_K;\n+\n+#if defined(__ARM_FEATURE_SVE)\n+\n+    uint32_t aux[3];\n+    uint32_t utmp[4];\n+\n+    const int8_t m32 = 32;\n+    const int vector_length = svcntb()*8;\n+    const svuint8_t m3b_sv = svdup_n_u8(0x3);\n+    const svint32_t vzero_sv = svdup_n_s32(0);\n+\n+    const svuint8_t m0_sv = svdup_n_u8(1);\n+    const svuint8_t m1_sv = svlsl_n_u8_x(svptrue_b8(), m0_sv, 1);\n+    const svuint8_t m2_sv = svlsl_n_u8_x(svptrue_b8(), m0_sv, 2);\n+    const svuint8_t m3_sv = svlsl_n_u8_x(svptrue_b8(), m0_sv, 3);\n+\n+    float sum = 0;\n+\n+    for (int i = 0; i < nb; ++i) {\n+\n+        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+\n+        const uint8_t * GGML_RESTRICT q3_sv = x[i].qs;\n+        const uint8_t * GGML_RESTRICT qh_sv = x[i].hmask;\n+        const int8_t  * GGML_RESTRICT q8_sv = y[i].qs;\n+\n+        // Set up scales\n+        memcpy(aux, x[i].scales, 12);\n+        utmp[3] = ((aux[1] >> 4) & kmask2) | (((aux[2] >> 6) & kmask1) << 4);\n+        utmp[2] = ((aux[0] >> 4) & kmask2) | (((aux[2] >> 4) & kmask1) << 4);\n+        utmp[1] = (aux[1] & kmask2) | (((aux[2] >> 2) & kmask1) << 4);\n+        utmp[0] = (aux[0] & kmask2) | (((aux[2] >> 0) & kmask1) << 4);\n+\n+        int8_t * scale = (int8_t *)utmp;\n+\n+        for (int j = 0; j < 16; ++j) scale[j] -= m32;\n+\n+        switch (vector_length) {\n+            case 128:\n+                {\n+                    svuint8_t qhbits_sv_1 = svld1_u8(svptrue_b8(), qh_sv);\n+                    svuint8_t qhbits_sv_2 = svld1_u8(svptrue_b8(), qh_sv+16);\n+                    svuint8_t q3h_sv;\n+\n+                    svint32_t sumi1_1 = svdup_n_s32(0);\n+                    svint8_t q3bytes_sv;\n+\n+                    for (int j = 0; j < QK_K/128; ++j) {\n+\n+                        const svuint8_t q3bits_sv = svld1_u8(svptrue_b8(), q3_sv); q3_sv += 16;\n+                        const svuint8_t q3bits_sv_1 = svld1_u8(svptrue_b8(), q3_sv); q3_sv += 16;\n+                        svint8_t q8bytes_1_sv_1 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                        svint8_t q8bytes_1_sv_2 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m0_sv, qhbits_sv_1), 2);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), q3bits_sv, m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), svdup_n_s32((int32_t)scale[0]));\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m0_sv, qhbits_sv_2), 2);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), q3bits_sv_1, m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), svdup_n_s32((int32_t)scale[1]));\n+\n+                        q8bytes_1_sv_1 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                        q8bytes_1_sv_2 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m1_sv, qhbits_sv_1), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv, 2), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), svdup_n_s32((int32_t)scale[2]));\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m1_sv, qhbits_sv_2), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv_1, 2), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), svdup_n_s32((int32_t)scale[3]));\n+\n+\n+                        scale += 4;\n+                        q8bytes_1_sv_1 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                        q8bytes_1_sv_2 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                        q3h_sv = svbic_u8_x(svptrue_b8(), m2_sv, qhbits_sv_1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv, 4), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), svdup_n_s32((int32_t)scale[0]));\n+\n+                        q3h_sv = svbic_u8_x(svptrue_b8(), m2_sv, qhbits_sv_2);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv_1, 4), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), svdup_n_s32((int32_t)scale[1]));\n+\n+\n+                        q8bytes_1_sv_1 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+                        q8bytes_1_sv_2 = svld1_s8(svptrue_b8(), q8_sv); q8_sv += 16;\n+\n+                        q3h_sv = svlsr_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m3_sv, qhbits_sv_1), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv, 6), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), svdup_n_s32((int32_t)scale[2]));\n+\n+                        q3h_sv = svlsr_n_u8_x(svptrue_b8(), svbic_u8_x(svptrue_b8(), m3_sv, qhbits_sv_2), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_b8(), svreinterpret_s8_u8(svand_u8_m(svptrue_b8(), svlsr_n_u8_x(svptrue_b8(), q3bits_sv_1, 6), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        sumi1_1 = svmla_s32_m(svptrue_b32(), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), svdup_n_s32((int32_t)scale[3]));\n+\n+                        if (j == 0) {\n+                            qhbits_sv_1 = svlsr_n_u8_x(svptrue_b8(), qhbits_sv_1, 4);\n+                            qhbits_sv_2 = svlsr_n_u8_x(svptrue_b8(), qhbits_sv_2, 4);\n+                        }\n+\n+                        scale += 4;\n+                    }\n+\n+                    sum += d * (svaddv_s32(svptrue_b32(), sumi1_1));\n+                } break;\n+            case 256:\n+            case 512:\n+                {\n+                    svuint8_t qhbits_sv = svld1_u8(svptrue_pat_b8(SV_VL32), qh_sv);\n+                    svuint8_t q3h_sv;\n+\n+                    svint32_t sumi1_1 = svdup_n_s32(0);\n+                    svint8_t q3bytes_sv;\n+\n+                    for (int j = 0; j < QK_K/128; ++j) {\n+\n+                        const svuint8_t q3bits_sv = svld1_u8(svptrue_pat_b8(SV_VL32), q3_sv); q3_sv += 32;\n+                        svint8_t q8bytes_1_sv_1 = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+                        svint8_t q8bytes_1_sv_2 = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_pat_b8(SV_VL32), svbic_u8_x(svptrue_pat_b8(SV_VL32), m0_sv, qhbits_sv), 2);\n+                        q3bytes_sv = svsub_s8_x(svptrue_pat_b8(SV_VL32), svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), q3bits_sv, m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+\n+                        svint32_t scale_1 = svsel_s32(svptrue_pat_b32(SV_VL4), svdup_n_s32((int32_t)scale[0]), svdup_n_s32((int32_t)scale[1]));\n+                        sumi1_1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), scale_1);\n+\n+                        q3h_sv = svlsl_n_u8_x(svptrue_pat_b8(SV_VL32), svbic_u8_x(svptrue_pat_b8(SV_VL32), m1_sv, qhbits_sv), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_pat_b8(SV_VL32), svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q3bits_sv, 2), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        scale_1 = svsel_s32(svptrue_pat_b32(SV_VL4), svdup_n_s32((int32_t)scale[2]), svdup_n_s32((int32_t)scale[3]));\n+                        sumi1_1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), scale_1);\n+\n+                        scale += 4;\n+                        q8bytes_1_sv_1 = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+                        q8bytes_1_sv_2 = svld1_s8(svptrue_pat_b8(SV_VL32), q8_sv); q8_sv += 32;\n+\n+                        q3h_sv = svbic_u8_x(svptrue_pat_b8(SV_VL32), m2_sv, qhbits_sv);\n+                        q3bytes_sv = svsub_s8_x(svptrue_pat_b8(SV_VL32), svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q3bits_sv, 4), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        scale_1 = svsel_s32(svptrue_pat_b32(SV_VL4), svdup_n_s32((int32_t)scale[0]), svdup_n_s32((int32_t)scale[1]));\n+                        sumi1_1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_1), scale_1);\n+\n+                        q3h_sv = svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), svbic_u8_x(svptrue_pat_b8(SV_VL32), m3_sv, qhbits_sv), 1);\n+                        q3bytes_sv = svsub_s8_x(svptrue_pat_b8(SV_VL32), svreinterpret_s8_u8(svand_u8_m(svptrue_pat_b8(SV_VL32), svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), q3bits_sv, 6), m3b_sv)), svreinterpret_s8_u8(q3h_sv));\n+\n+                        scale_1 = svsel_s32(svptrue_pat_b32(SV_VL4), svdup_n_s32((int32_t)scale[2]), svdup_n_s32((int32_t)scale[3]));\n+                        sumi1_1 = svmla_s32_m(svptrue_pat_b32(SV_VL8), sumi1_1, svdot_s32(vzero_sv, q3bytes_sv, q8bytes_1_sv_2), scale_1);\n+\n+                        if (j == 0) {\n+                            qhbits_sv = svlsr_n_u8_x(svptrue_pat_b8(SV_VL32), qhbits_sv, 4);\n+                        }\n+\n+                        scale += 4;\n+                    }\n+\n+                    sum += d * (svaddv_s32(svptrue_pat_b32(SV_VL8), sumi1_1));\n+                } break;\n+            default:\n+                assert(false && \"Unsupported vector length\");\n+                break;\n+        }\n+    }\n+    *s = sum;\n+\n+#elif __ARM_NEON\n+\n+    uint32_t aux[3];\n+    uint32_t utmp[4];\n+\n+    const uint8x16_t m3b = vdupq_n_u8(0x3);\n+    const int32x4_t  vzero = vdupq_n_s32(0);\n+\n+    const uint8x16_t m0 = vdupq_n_u8(1);\n+    const uint8x16_t m1 = vshlq_n_u8(m0, 1);\n+    const uint8x16_t m2 = vshlq_n_u8(m0, 2);\n+    const uint8x16_t m3 = vshlq_n_u8(m0, 3);\n+    const int8_t m32 = 32;\n+\n+    ggml_int8x16x4_t q3bytes;\n+\n+    float sum = 0;\n+\n+    for (int i = 0; i < nb; ++i) {\n+\n+        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n+\n+        const uint8_t * GGML_RESTRICT q3 = x[i].qs;\n+        const uint8_t * GGML_RESTRICT qh = x[i].hmask;\n+        const int8_t  * GGML_RESTRICT q8 = y[i].qs;\n+\n+        ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\n+\n+        ggml_uint8x16x4_t q3h;\n+\n+        int32_t isum = 0;\n+\n+        // Set up scales\n+        memcpy(aux, x[i].scales, 12);\n+        utmp[3] = ((aux[1] >> 4) & kmask2) | (((aux[2] >> 6) & kmask1) << 4);\n+        utmp[2] = ((aux[0] >> 4) & kmask2) | (((aux[2] >> 4) & kmask1) << 4);\n+        utmp[1] = (aux[1] & kmask2) | (((aux[2] >> 2) & kmask1) << 4);\n+        utmp[0] = (aux[0] & kmask2) | (((aux[2] >> 0) & kmask1) << 4);\n+\n+        int8_t * scale = (int8_t *)utmp;\n+        for (int j = 0; j < 16; ++j) scale[j] -= m32;\n+\n+        for (int j = 0; j < QK_K/128; ++j) {\n+\n+            const ggml_uint8x16x2_t q3bits = ggml_vld1q_u8_x2(q3); q3 += 32;\n+            const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\n+            const ggml_int8x16x4_t q8bytes_2 = ggml_vld1q_s8_x4(q8); q8 += 64;\n+\n+            q3h.val[0] = vshlq_n_u8(vbicq_u8(m0, qhbits.val[0]), 2);\n+            q3h.val[1] = vshlq_n_u8(vbicq_u8(m0, qhbits.val[1]), 2);\n+            q3h.val[2] = vshlq_n_u8(vbicq_u8(m1, qhbits.val[0]), 1);\n+            q3h.val[3] = vshlq_n_u8(vbicq_u8(m1, qhbits.val[1]), 1);\n+\n+            q3bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(q3bits.val[0], m3b)), vreinterpretq_s8_u8(q3h.val[0]));\n+            q3bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(q3bits.val[1], m3b)), vreinterpretq_s8_u8(q3h.val[1]));\n+            q3bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 2), m3b)), vreinterpretq_s8_u8(q3h.val[2]));\n+            q3bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 2), m3b)), vreinterpretq_s8_u8(q3h.val[3]));\n+\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[0], q8bytes_1.val[0])) * scale[0];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[1], q8bytes_1.val[1])) * scale[1];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[2], q8bytes_1.val[2])) * scale[2];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[3], q8bytes_1.val[3])) * scale[3];\n+\n+            scale += 4;\n+\n+            q3h.val[0] = vbicq_u8(m2, qhbits.val[0]);\n+            q3h.val[1] = vbicq_u8(m2, qhbits.val[1]);\n+            q3h.val[2] = vshrq_n_u8(vbicq_u8(m3, qhbits.val[0]), 1);\n+            q3h.val[3] = vshrq_n_u8(vbicq_u8(m3, qhbits.val[1]), 1);\n+\n+            q3bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 4), m3b)), vreinterpretq_s8_u8(q3h.val[0]));\n+            q3bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 4), m3b)), vreinterpretq_s8_u8(q3h.val[1]));\n+            q3bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 6), m3b)), vreinterpretq_s8_u8(q3h.val[2]));\n+            q3bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 6), m3b)), vreinterpretq_s8_u8(q3h.val[3]));\n+\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[0], q8bytes_2.val[0])) * scale[0];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[1], q8bytes_2.val[1])) * scale[1];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[2], q8bytes_2.val[2])) * scale[2];\n+            isum += vaddvq_s32(ggml_vdotq_s32(vzero, q3bytes.val[3], q8bytes_2.val[3])) * scale[3];\n+\n+            scale += 4;\n+\n+            if (j == 0) {\n+                qhbits.val[0] = vshrq_n_u8(qhbits.val[0], 4);\n+                qhbits.val[1] = vshrq_n_u8(qhbits.val[1], 4);\n+            }\n+\n+        }\n+        sum += d * isum;\n+\n+    }\n+\n+    *s = sum;\n+\n+#else\n+    // scalar version\n+    // This function is written like this so the compiler can manage to vectorize most of it\n+    // Using -Ofast, GCC and clang manage to produce code that is within a factor of 2 or so from the\n+    // manually vectorized version above. Every other version I tried would run at least 4 times slower.\n+    // The ideal situation would be if we could just write the code once, and the compiler would\n+    // automatically produce the best possible set of machine instructions, instead of us having to manually\n+    // write vectorized versions for AVX, ARM_NEON, etc.\n+\n+    int8_t  aux8[QK_K];\n+    int16_t aux16[8];\n+    float   sums [8];\n+    int32_t aux32[8];\n+    memset(sums, 0, 8*sizeof(float));\n+\n+    uint32_t auxs[4];\n+    const int8_t * scales = (const int8_t*)auxs;\n+\n+    float sumf = 0;\n+    for (int i = 0; i < nb; ++i) {\n+        const uint8_t * GGML_RESTRICT q3 = x[i].qs;\n+        const uint8_t * GGML_RESTRICT hm = x[i].hmask;\n+        const  int8_t * GGML_RESTRICT q8 = y[i].qs;\n+        memset(aux32, 0, 8*sizeof(int32_t));\n+        int8_t * GGML_RESTRICT a = aux8;\n+        uint8_t m = 1;\n+        for (int j = 0; j < QK_K; j += 128) {\n+            for (int l = 0; l < 32; ++l) a[l] = q3[l] & 3;\n+            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n+            a += 32; m <<= 1;\n+            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 2) & 3;\n+            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n+            a += 32; m <<= 1;\n+            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 4) & 3;\n+            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n+            a += 32; m <<= 1;\n+            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 6) & 3;\n+            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n+            a += 32; m <<= 1;\n+            q3 += 32;\n+        }\n+        a = aux8;\n+\n+        memcpy(auxs, x[i].scales, 12);\n+        uint32_t tmp = auxs[2];\n+        auxs[2] = ((auxs[0] >> 4) & kmask2) | (((tmp >> 4) & kmask1) << 4);\n+        auxs[3] = ((auxs[1] >> 4) & kmask2) | (((tmp >> 6) & kmask1) << 4);\n+        auxs[0] = (auxs[0] & kmask2) | (((tmp >> 0) & kmask1) << 4);\n+        auxs[1] = (auxs[1] & kmask2) | (((tmp >> 2) & kmask1) << 4);\n+        for (int j = 0; j < QK_K/16; ++j) {\n+            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n+            for (int l = 0; l < 8; ++l) aux32[l] += (scales[j] - 32) * aux16[l];\n+            q8 += 8; a += 8;\n+            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n+            for (int l = 0; l < 8; ++l) aux32[l] += (scales[j] - 32) * aux16[l];\n+            q8 += 8; a += 8;\n+        }\n+        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n+        for (int l = 0; l < 8; ++l) sums[l] += d * aux32[l];\n+    }\n+    for (int l = 0; l < 8; ++l) sumf += sums[l];\n+    *s = sumf;\n+\n+#endif\n+\n+}\n+\n+void ggml_vec_dot_q4_K_q8_K(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, size_t bx, const void * GGML_RESTRICT vy, size_t by, int nrc) {\n+    assert(n % QK_K == 0);\n+    assert(nrc == 1);",
        "comment_created_at": "2025-06-05T08:50:17+00:00",
        "comment_author": "ggerganov",
        "comment_body": "```suggestion\r\n#ifdef __ARM_FEATURE_MATMUL_INT8\r\n    assert((nrc == 2) || (nrc == 1));\r\n#else\r\n    assert(nrc == 1);\r\n#endif\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2171615811",
    "pr_number": 14366,
    "pr_file": "ggml/src/ggml.c",
    "created_at": "2025-06-27T10:52:41+00:00",
    "commented_code": "(cgraph->order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i :\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) :\n             /* unknown order, just fall back to using i*/ i;\n-        if (node->src[k]) {\n-            ggml_visit_parents(cgraph, node->src[k]);\n+\n+        struct ggml_tensor * s = node->src[k];\n+        if (s) {\n+            ggml_visit_parents(cgraph, s);\n+\n+            // Update the use count for this operand.\n+            // Skip if it's a leaf node\n+            if (!(s->op == GGML_OP_NONE && !(s->flags & GGML_TENSOR_FLAG_PARAM))) {\n+                // the src can be the node itself (happens in ggml_cast)\n+                if (s == node) {\n+                    cgraph->use_counts[cgraph->n_nodes]++;\n+                } else {\n+                    // Search backward to find the src. This usually takes very few\n+                    // (most often one) iteration(s). Probably comparable to hashing\n+                    // on average..\n+                    int j = cgraph->n_nodes - 1;\n+                    for (; j >= 0; --j) {\n+                        if (s == cgraph->nodes[j]) {\n+                            break;\n+                        }\n+                    }",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2171615811",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14366,
        "pr_file": "ggml/src/ggml.c",
        "discussion_id": "2171615811",
        "commented_code": "@@ -5815,8 +5815,31 @@ static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor *\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i :\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) :\n             /* unknown order, just fall back to using i*/ i;\n-        if (node->src[k]) {\n-            ggml_visit_parents(cgraph, node->src[k]);\n+\n+        struct ggml_tensor * s = node->src[k];\n+        if (s) {\n+            ggml_visit_parents(cgraph, s);\n+\n+            // Update the use count for this operand.\n+            // Skip if it's a leaf node\n+            if (!(s->op == GGML_OP_NONE && !(s->flags & GGML_TENSOR_FLAG_PARAM))) {\n+                // the src can be the node itself (happens in ggml_cast)\n+                if (s == node) {\n+                    cgraph->use_counts[cgraph->n_nodes]++;\n+                } else {\n+                    // Search backward to find the src. This usually takes very few\n+                    // (most often one) iteration(s). Probably comparable to hashing\n+                    // on average..\n+                    int j = cgraph->n_nodes - 1;\n+                    for (; j >= 0; --j) {\n+                        if (s == cgraph->nodes[j]) {\n+                            break;\n+                        }\n+                    }",
        "comment_created_at": "2025-06-27T10:52:41+00:00",
        "comment_author": "slaren",
        "comment_body": "Wouldn't it be better to use the hash table that is already in `ggml_cgraph`? It can be done by allocating an array of the same size as the hash table, and using `ggml_hash_find` to find the slot for the node.",
        "pr_file_module": null
      },
      {
        "comment_id": "2171990912",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14366,
        "pr_file": "ggml/src/ggml.c",
        "discussion_id": "2171615811",
        "commented_code": "@@ -5815,8 +5815,31 @@ static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor *\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i :\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) :\n             /* unknown order, just fall back to using i*/ i;\n-        if (node->src[k]) {\n-            ggml_visit_parents(cgraph, node->src[k]);\n+\n+        struct ggml_tensor * s = node->src[k];\n+        if (s) {\n+            ggml_visit_parents(cgraph, s);\n+\n+            // Update the use count for this operand.\n+            // Skip if it's a leaf node\n+            if (!(s->op == GGML_OP_NONE && !(s->flags & GGML_TENSOR_FLAG_PARAM))) {\n+                // the src can be the node itself (happens in ggml_cast)\n+                if (s == node) {\n+                    cgraph->use_counts[cgraph->n_nodes]++;\n+                } else {\n+                    // Search backward to find the src. This usually takes very few\n+                    // (most often one) iteration(s). Probably comparable to hashing\n+                    // on average..\n+                    int j = cgraph->n_nodes - 1;\n+                    for (; j >= 0; --j) {\n+                        if (s == cgraph->nodes[j]) {\n+                            break;\n+                        }\n+                    }",
        "comment_created_at": "2025-06-27T12:54:39+00:00",
        "comment_author": "jeffbolznv",
        "comment_body": "I've made this change. It's a tradeoff of memory vs search time...",
        "pr_file_module": null
      },
      {
        "comment_id": "2172021019",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14366,
        "pr_file": "ggml/src/ggml.c",
        "discussion_id": "2171615811",
        "commented_code": "@@ -5815,8 +5815,31 @@ static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor *\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i :\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) :\n             /* unknown order, just fall back to using i*/ i;\n-        if (node->src[k]) {\n-            ggml_visit_parents(cgraph, node->src[k]);\n+\n+        struct ggml_tensor * s = node->src[k];\n+        if (s) {\n+            ggml_visit_parents(cgraph, s);\n+\n+            // Update the use count for this operand.\n+            // Skip if it's a leaf node\n+            if (!(s->op == GGML_OP_NONE && !(s->flags & GGML_TENSOR_FLAG_PARAM))) {\n+                // the src can be the node itself (happens in ggml_cast)\n+                if (s == node) {\n+                    cgraph->use_counts[cgraph->n_nodes]++;\n+                } else {\n+                    // Search backward to find the src. This usually takes very few\n+                    // (most often one) iteration(s). Probably comparable to hashing\n+                    // on average..\n+                    int j = cgraph->n_nodes - 1;\n+                    for (; j >= 0; --j) {\n+                        if (s == cgraph->nodes[j]) {\n+                            break;\n+                        }\n+                    }",
        "comment_created_at": "2025-06-27T13:12:02+00:00",
        "comment_author": "slaren",
        "comment_body": "The search time can be significant, that's why the hash table was added, previously it scanned the previous nodes to determine if a node had already been visited. This implementation is not exactly what I meant, you can make `use_counts` have size `hash_size` and essentially use it as data attached to the hash table. The `memset` should also be avoided, it has a significant cost, which is also why the hash table has a bit field to determine which slots are used. I will take a look at this later and change it in this way.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172046194",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14366,
        "pr_file": "ggml/src/ggml.c",
        "discussion_id": "2171615811",
        "commented_code": "@@ -5815,8 +5815,31 @@ static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor *\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i :\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) :\n             /* unknown order, just fall back to using i*/ i;\n-        if (node->src[k]) {\n-            ggml_visit_parents(cgraph, node->src[k]);\n+\n+        struct ggml_tensor * s = node->src[k];\n+        if (s) {\n+            ggml_visit_parents(cgraph, s);\n+\n+            // Update the use count for this operand.\n+            // Skip if it's a leaf node\n+            if (!(s->op == GGML_OP_NONE && !(s->flags & GGML_TENSOR_FLAG_PARAM))) {\n+                // the src can be the node itself (happens in ggml_cast)\n+                if (s == node) {\n+                    cgraph->use_counts[cgraph->n_nodes]++;\n+                } else {\n+                    // Search backward to find the src. This usually takes very few\n+                    // (most often one) iteration(s). Probably comparable to hashing\n+                    // on average..\n+                    int j = cgraph->n_nodes - 1;\n+                    for (; j >= 0; --j) {\n+                        if (s == cgraph->nodes[j]) {\n+                            break;\n+                        }\n+                    }",
        "comment_created_at": "2025-06-27T13:25:59+00:00",
        "comment_author": "jeffbolznv",
        "comment_body": "I see what you mean. I'll change it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172090807",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14366,
        "pr_file": "ggml/src/ggml.c",
        "discussion_id": "2171615811",
        "commented_code": "@@ -5815,8 +5815,31 @@ static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor *\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i :\n             (cgraph->order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) :\n             /* unknown order, just fall back to using i*/ i;\n-        if (node->src[k]) {\n-            ggml_visit_parents(cgraph, node->src[k]);\n+\n+        struct ggml_tensor * s = node->src[k];\n+        if (s) {\n+            ggml_visit_parents(cgraph, s);\n+\n+            // Update the use count for this operand.\n+            // Skip if it's a leaf node\n+            if (!(s->op == GGML_OP_NONE && !(s->flags & GGML_TENSOR_FLAG_PARAM))) {\n+                // the src can be the node itself (happens in ggml_cast)\n+                if (s == node) {\n+                    cgraph->use_counts[cgraph->n_nodes]++;\n+                } else {\n+                    // Search backward to find the src. This usually takes very few\n+                    // (most often one) iteration(s). Probably comparable to hashing\n+                    // on average..\n+                    int j = cgraph->n_nodes - 1;\n+                    for (; j >= 0; --j) {\n+                        if (s == cgraph->nodes[j]) {\n+                            break;\n+                        }\n+                    }",
        "comment_created_at": "2025-06-27T13:49:33+00:00",
        "comment_author": "jeffbolznv",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1466661502",
    "pr_number": 2059,
    "pr_file": "ggml.c",
    "created_at": "2024-01-25T16:48:48+00:00",
    "commented_code": "}\n     }\n \n+#ifdef GGML_USE_VULKAN\n+    for (int i = 0; i < cgraph->n_nodes; i++) {\n+        ggml_vk_preallocate_buffers_graph(cgraph->nodes[i]);\n+    }\n+    ggml_vk_preallocate_buffers();\n+\n+    for (int i = 0; i < cgraph->n_nodes; i++) {\n+        ggml_vk_build_graph(cgraph->nodes[i], i == cgraph->n_nodes - 1);\n+    }\n+#endif",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "1466661502",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 2059,
        "pr_file": "ggml.c",
        "discussion_id": "1466661502",
        "commented_code": "@@ -16854,6 +16870,17 @@ int ggml_graph_compute(struct ggml_cgraph * cgraph, struct ggml_cplan * cplan) {\n         }\n     }\n \n+#ifdef GGML_USE_VULKAN\n+    for (int i = 0; i < cgraph->n_nodes; i++) {\n+        ggml_vk_preallocate_buffers_graph(cgraph->nodes[i]);\n+    }\n+    ggml_vk_preallocate_buffers();\n+\n+    for (int i = 0; i < cgraph->n_nodes; i++) {\n+        ggml_vk_build_graph(cgraph->nodes[i], i == cgraph->n_nodes - 1);\n+    }\n+#endif",
        "comment_created_at": "2024-01-25T16:48:48+00:00",
        "comment_author": "slaren",
        "comment_body": "Are these calls still needed after ggml-backend?",
        "pr_file_module": null
      },
      {
        "comment_id": "1466699213",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 2059,
        "pr_file": "ggml.c",
        "discussion_id": "1466661502",
        "commented_code": "@@ -16854,6 +16870,17 @@ int ggml_graph_compute(struct ggml_cgraph * cgraph, struct ggml_cplan * cplan) {\n         }\n     }\n \n+#ifdef GGML_USE_VULKAN\n+    for (int i = 0; i < cgraph->n_nodes; i++) {\n+        ggml_vk_preallocate_buffers_graph(cgraph->nodes[i]);\n+    }\n+    ggml_vk_preallocate_buffers();\n+\n+    for (int i = 0; i < cgraph->n_nodes; i++) {\n+        ggml_vk_build_graph(cgraph->nodes[i], i == cgraph->n_nodes - 1);\n+    }\n+#endif",
        "comment_created_at": "2024-01-25T17:18:41+00:00",
        "comment_author": "0cc4m",
        "comment_body": "They are, for the matrix multiplications. Vulkan needs to prerecord its operations into a command buffer before they can be executed. I can simplify a bunch of stuff once offloading matrix multiplications is handled by the backend.",
        "pr_file_module": null
      }
    ]
  }
]