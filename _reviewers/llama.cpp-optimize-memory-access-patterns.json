[
  {
    "discussion_id": "2229070169",
    "pr_number": 14624,
    "pr_file": "ggml/src/ggml-cuda/mma.cuh",
    "created_at": "2025-07-24T16:59:25+00:00",
    "commented_code": "template <int I, int J, typename T>\n     static __device__ __forceinline__ void load_generic(tile<I, J, T> & t, const T * __restrict__ xs0, const int stride) {\n+#if defined(AMD_MFMA_AVAILABLE)\n+        if constexpr (I == 64 && J == 2) { // Special tile size to load <16, 4> as <16, 8>\n+#pragma unroll\n+            for (int l = 0; l < t.ne; ++l) {\n+                t.x[l] = xs0[t.get_i(l)*stride + t.get_j(l)];\n+            }\n+        } else {\n+            int64_t * xi = (int64_t *) t.x;\n+            const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+            xi[0] = xs[0];\n+        }",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2229070169",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mma.cuh",
        "discussion_id": "2229070169",
        "commented_code": "@@ -148,10 +187,23 @@ namespace ggml_cuda_mma {\n \n     template <int I, int J, typename T>\n     static __device__ __forceinline__ void load_generic(tile<I, J, T> & t, const T * __restrict__ xs0, const int stride) {\n+#if defined(AMD_MFMA_AVAILABLE)\n+        if constexpr (I == 64 && J == 2) { // Special tile size to load <16, 4> as <16, 8>\n+#pragma unroll\n+            for (int l = 0; l < t.ne; ++l) {\n+                t.x[l] = xs0[t.get_i(l)*stride + t.get_j(l)];\n+            }\n+        } else {\n+            int64_t * xi = (int64_t *) t.x;\n+            const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+            xi[0] = xs[0];\n+        }",
        "comment_created_at": "2025-07-24T16:59:25+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n        if constexpr (I != 64 || J != 2) {\r\n            int64_t * xi = (int64_t *) t.x;\r\n            const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\r\n            xi[0] = xs[0];\r\n            return;\r\n        }\r\n```\r\n\r\nI think this would be simpler.",
        "pr_file_module": null
      },
      {
        "comment_id": "2229356841",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mma.cuh",
        "discussion_id": "2229070169",
        "commented_code": "@@ -148,10 +187,23 @@ namespace ggml_cuda_mma {\n \n     template <int I, int J, typename T>\n     static __device__ __forceinline__ void load_generic(tile<I, J, T> & t, const T * __restrict__ xs0, const int stride) {\n+#if defined(AMD_MFMA_AVAILABLE)\n+        if constexpr (I == 64 && J == 2) { // Special tile size to load <16, 4> as <16, 8>\n+#pragma unroll\n+            for (int l = 0; l < t.ne; ++l) {\n+                t.x[l] = xs0[t.get_i(l)*stride + t.get_j(l)];\n+            }\n+        } else {\n+            int64_t * xi = (int64_t *) t.x;\n+            const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+            xi[0] = xs[0];\n+        }",
        "comment_created_at": "2025-07-24T19:12:46+00:00",
        "comment_author": "deepsek",
        "comment_body": "Do you mean without the preprocessor directives?\r\nThis would affect the NV code path when we call load_generic though?  I see some instances where load_generic is called\r\n\r\n```\r\nstatic __device__ __forceinline__ void load_generic(...) {\r\n        if constexpr (I != 64 || J != 2) {\r\n            int64_t * xi = (int64_t *) t.x;\r\n            const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\r\n            xi[0] = xs[0];\r\n            return;\r\n        }\r\n\r\n#pragma unroll\r\n        for (int l = 0; l < t.ne; ++l) {\r\n            t.x[l] = xs0[t.get_i(l)*stride + t.get_j(l)];\r\n        }\r\n}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2229504657",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mma.cuh",
        "discussion_id": "2229070169",
        "commented_code": "@@ -148,10 +187,23 @@ namespace ggml_cuda_mma {\n \n     template <int I, int J, typename T>\n     static __device__ __forceinline__ void load_generic(tile<I, J, T> & t, const T * __restrict__ xs0, const int stride) {\n+#if defined(AMD_MFMA_AVAILABLE)\n+        if constexpr (I == 64 && J == 2) { // Special tile size to load <16, 4> as <16, 8>\n+#pragma unroll\n+            for (int l = 0; l < t.ne; ++l) {\n+                t.x[l] = xs0[t.get_i(l)*stride + t.get_j(l)];\n+            }\n+        } else {\n+            int64_t * xi = (int64_t *) t.x;\n+            const int64_t * xs = (int64_t *) ((const int *) xs0 + (threadIdx.x % t.I) * stride + 2 * (threadIdx.x / t.I));\n+            xi[0] = xs[0];\n+        }",
        "comment_created_at": "2025-07-24T20:25:16+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I basically meant to have the instructions for loading data as 64 bit encapsulated in an `ifdef AMD_MFMA_AVAILABLE ... #endif` and to use the generic implementation if the preconditions aren't met. But if this is going to be refactored anyways it doesn't matter.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2206936778",
    "pr_number": 14659,
    "pr_file": "ggml/src/ggml-cuda/pad_reflect_1d.cu",
    "created_at": "2025-07-15T09:12:27+00:00",
    "commented_code": "+#include \"pad_reflect_1d.cuh\"\n+\n+static __global__ void pad_reflect_1d_kernel_f32(\n+    const void * src0,\n+    void * dst,",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2206936778",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14659,
        "pr_file": "ggml/src/ggml-cuda/pad_reflect_1d.cu",
        "discussion_id": "2206936778",
        "commented_code": "@@ -0,0 +1,82 @@\n+#include \"pad_reflect_1d.cuh\"\n+\n+static __global__ void pad_reflect_1d_kernel_f32(\n+    const void * src0,\n+    void * dst,",
        "comment_created_at": "2025-07-15T09:12:27+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n    const void * __restrict__ src0,\r\n    void * __restrict__ dst,\r\n```\r\n\r\nOn older GPUs especially you can gain some performance by telling the compiler that the pointers aren't aliased.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190178778",
    "pr_number": 14551,
    "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
    "created_at": "2025-07-07T13:48:28+00:00",
    "commented_code": "+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+static __device__ void set_rows_1_f32_f32(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    float * dst_f = (float *) dst;\n+    *dst_f = *src_f;\n+}\n+\n+static __device__ void set_rows_1_f32_f16(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    half * dst_h = (half *) dst;\n+    *dst_h = __float2half(*src_f);\n+}\n+\n+//TODO: consolidate kernels from cpy.cu, get_rows etc to make this function generic\n+template<set_rows_kernel_t set_rows_1>\n+static __global__ void k_set_rows(\n+        const char * __restrict__ src0, const int64_t * __restrict__ src1, char * __restrict__ dst,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size) {\n+\n+    const int i03 = blockIdx.z;\n+    const int i02 = blockIdx.y;\n+    const int i01 = blockIdx.x * blockDim.y + threadIdx.y;  // Row index\n+\n+    if (i01 >= ne01) {\n+        return;\n+    }\n+\n+    const int i12 = i03 % ne12;\n+    const int i11 = i02 % ne11;\n+    const int i10 = i01;\n+\n+    const int64_t dst_row = *(int64_t *)((char *)src1 + i10*nb10 + i11*nb11 + i12*nb12);\n+\n+    const char * src0_row = src0 + i01*nb01 + i02*nb02 + i03*nb03;\n+    char * dst_row_ptr    = dst + dst_row*nb1 + i02*nb2 + i03*nb3;\n+\n+    for (int col = threadIdx.x; col < ne00; col += blockDim.x) {\n+        const char * src_elem = src0_row + col * src_type_size;\n+        char * dst_elem       = dst_row_ptr + col * dst_type_size;\n+        set_rows_1(src_elem, dst_elem);\n+    }",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2190178778",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14551,
        "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
        "discussion_id": "2190178778",
        "commented_code": "@@ -0,0 +1,128 @@\n+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+static __device__ void set_rows_1_f32_f32(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    float * dst_f = (float *) dst;\n+    *dst_f = *src_f;\n+}\n+\n+static __device__ void set_rows_1_f32_f16(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    half * dst_h = (half *) dst;\n+    *dst_h = __float2half(*src_f);\n+}\n+\n+//TODO: consolidate kernels from cpy.cu, get_rows etc to make this function generic\n+template<set_rows_kernel_t set_rows_1>\n+static __global__ void k_set_rows(\n+        const char * __restrict__ src0, const int64_t * __restrict__ src1, char * __restrict__ dst,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size) {\n+\n+    const int i03 = blockIdx.z;\n+    const int i02 = blockIdx.y;\n+    const int i01 = blockIdx.x * blockDim.y + threadIdx.y;  // Row index\n+\n+    if (i01 >= ne01) {\n+        return;\n+    }\n+\n+    const int i12 = i03 % ne12;\n+    const int i11 = i02 % ne11;\n+    const int i10 = i01;\n+\n+    const int64_t dst_row = *(int64_t *)((char *)src1 + i10*nb10 + i11*nb11 + i12*nb12);\n+\n+    const char * src0_row = src0 + i01*nb01 + i02*nb02 + i03*nb03;\n+    char * dst_row_ptr    = dst + dst_row*nb1 + i02*nb2 + i03*nb3;\n+\n+    for (int col = threadIdx.x; col < ne00; col += blockDim.x) {\n+        const char * src_elem = src0_row + col * src_type_size;\n+        char * dst_elem       = dst_row_ptr + col * dst_type_size;\n+        set_rows_1(src_elem, dst_elem);\n+    }",
        "comment_created_at": "2025-07-07T13:48:28+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I didn't look at the generated PTX code for this specific CUDA code but in my experience the CPU code pattern with explicit byte offsets performs comparatively poorly on GPUs. My recommendation would be to have the input and output types as template parameters and to compute the strides in units of the types (e.g. `nb01/ggml_element_size(src0)`) in host code. This is of course assuming that this kernel has a non-negligible contribution to the end-to-end performance in the first place.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190214645",
    "pr_number": 14551,
    "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
    "created_at": "2025-07-07T14:03:40+00:00",
    "commented_code": "+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+static __device__ void set_rows_1_f32_f32(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    float * dst_f = (float *) dst;\n+    *dst_f = *src_f;\n+}\n+\n+static __device__ void set_rows_1_f32_f16(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    half * dst_h = (half *) dst;\n+    *dst_h = __float2half(*src_f);\n+}\n+\n+//TODO: consolidate kernels from cpy.cu, get_rows etc to make this function generic\n+template<set_rows_kernel_t set_rows_1>\n+static __global__ void k_set_rows(\n+        const char * __restrict__ src0, const int64_t * __restrict__ src1, char * __restrict__ dst,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size) {\n+\n+    const int i03 = blockIdx.z;\n+    const int i02 = blockIdx.y;\n+    const int i01 = blockIdx.x * blockDim.y + threadIdx.y;  // Row index\n+\n+    if (i01 >= ne01) {\n+        return;\n+    }\n+\n+    const int i12 = i03 % ne12;\n+    const int i11 = i02 % ne11;\n+    const int i10 = i01;\n+\n+    const int64_t dst_row = *(int64_t *)((char *)src1 + i10*nb10 + i11*nb11 + i12*nb12);\n+\n+    const char * src0_row = src0 + i01*nb01 + i02*nb02 + i03*nb03;\n+    char * dst_row_ptr    = dst + dst_row*nb1 + i02*nb2 + i03*nb3;\n+\n+    for (int col = threadIdx.x; col < ne00; col += blockDim.x) {\n+        const char * src_elem = src0_row + col * src_type_size;\n+        char * dst_elem       = dst_row_ptr + col * dst_type_size;\n+        set_rows_1(src_elem, dst_elem);\n+    }\n+}\n+\n+template<set_rows_kernel_t set_rows_1>\n+static void set_rows_cuda(\n+        const char * src0_d, const int64_t * src1_d, char * dst_d,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size,\n+        cudaStream_t stream) {\n+\n+    const int max_threads_per_row = 256;\n+    const int threads_per_row     = std::min((int)ne00, max_threads_per_row);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2190214645",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14551,
        "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
        "discussion_id": "2190214645",
        "commented_code": "@@ -0,0 +1,128 @@\n+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+static __device__ void set_rows_1_f32_f32(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    float * dst_f = (float *) dst;\n+    *dst_f = *src_f;\n+}\n+\n+static __device__ void set_rows_1_f32_f16(const char * src, char * dst) {\n+    const float * src_f = (const float *) src;\n+    half * dst_h = (half *) dst;\n+    *dst_h = __float2half(*src_f);\n+}\n+\n+//TODO: consolidate kernels from cpy.cu, get_rows etc to make this function generic\n+template<set_rows_kernel_t set_rows_1>\n+static __global__ void k_set_rows(\n+        const char * __restrict__ src0, const int64_t * __restrict__ src1, char * __restrict__ dst,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size) {\n+\n+    const int i03 = blockIdx.z;\n+    const int i02 = blockIdx.y;\n+    const int i01 = blockIdx.x * blockDim.y + threadIdx.y;  // Row index\n+\n+    if (i01 >= ne01) {\n+        return;\n+    }\n+\n+    const int i12 = i03 % ne12;\n+    const int i11 = i02 % ne11;\n+    const int i10 = i01;\n+\n+    const int64_t dst_row = *(int64_t *)((char *)src1 + i10*nb10 + i11*nb11 + i12*nb12);\n+\n+    const char * src0_row = src0 + i01*nb01 + i02*nb02 + i03*nb03;\n+    char * dst_row_ptr    = dst + dst_row*nb1 + i02*nb2 + i03*nb3;\n+\n+    for (int col = threadIdx.x; col < ne00; col += blockDim.x) {\n+        const char * src_elem = src0_row + col * src_type_size;\n+        char * dst_elem       = dst_row_ptr + col * dst_type_size;\n+        set_rows_1(src_elem, dst_elem);\n+    }\n+}\n+\n+template<set_rows_kernel_t set_rows_1>\n+static void set_rows_cuda(\n+        const char * src0_d, const int64_t * src1_d, char * dst_d,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size,\n+        cudaStream_t stream) {\n+\n+    const int max_threads_per_row = 256;\n+    const int threads_per_row     = std::min((int)ne00, max_threads_per_row);",
        "comment_created_at": "2025-07-07T14:03:40+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "I didn't compare the two versions but I think it would be better to use a static block size of e.g. 256 here and to map the thread indices to tensor data indices as if the tensor was flattened. Running CUDA code with fractional warps will always result in wasted GPU resources.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2193057284",
    "pr_number": 14551,
    "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
    "created_at": "2025-07-08T17:20:35+00:00",
    "commented_code": "+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+template<typename src_t, typename dst_t>\n+__device__ void set_rows_1(const src_t * src_f, dst_t * dst_f) {\n+    GGML_ABORT(\"unsupport type for set_rows\");\n+}\n+\n+template<>\n+__device__ __forceinline__ void set_rows_1<float, half>(const float * src_f, half * dst_h) {\n+    *dst_h = __float2half(*src_f);\n+}\n+\n+template<>\n+__device__ __forceinline__ void set_rows_1<float, float>(const float * src_f, float * dst_f) {\n+    *dst_f = *src_f;\n+}\n+\n+//TODO: consolidate kernels from cpy.cu, get_rows etc to make this function generic\n+template<typename src_t, typename dst_t>\n+static __global__ void k_set_rows(\n+        const src_t * __restrict__ src0, const int64_t * __restrict__ src1, dst_t * __restrict__ dst,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size) {\n+\n+    const int i03 = blockIdx.z / ne02;\n+    const int i02 = blockIdx.z % ne02;\n+    const int i01 = blockDim.x * blockIdx.x +  threadIdx.x;\n+    const int i00 = blockIdx.y;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2193057284",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14551,
        "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
        "discussion_id": "2193057284",
        "commented_code": "@@ -0,0 +1,137 @@\n+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+template<typename src_t, typename dst_t>\n+__device__ void set_rows_1(const src_t * src_f, dst_t * dst_f) {\n+    GGML_ABORT(\"unsupport type for set_rows\");\n+}\n+\n+template<>\n+__device__ __forceinline__ void set_rows_1<float, half>(const float * src_f, half * dst_h) {\n+    *dst_h = __float2half(*src_f);\n+}\n+\n+template<>\n+__device__ __forceinline__ void set_rows_1<float, float>(const float * src_f, float * dst_f) {\n+    *dst_f = *src_f;\n+}\n+\n+//TODO: consolidate kernels from cpy.cu, get_rows etc to make this function generic\n+template<typename src_t, typename dst_t>\n+static __global__ void k_set_rows(\n+        const src_t * __restrict__ src0, const int64_t * __restrict__ src1, dst_t * __restrict__ dst,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size) {\n+\n+    const int i03 = blockIdx.z / ne02;\n+    const int i02 = blockIdx.z % ne02;\n+    const int i01 = blockDim.x * blockIdx.x +  threadIdx.x;\n+    const int i00 = blockIdx.y;",
        "comment_created_at": "2025-07-08T17:20:35+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n    const int i01 = blockIdx.x;\r\n    const int i00 = blockIdx.y*blockDim.x + threadIdx.x;\r\n```\r\n\r\nThis is how you should organize the threads, otherwise the memory accesses are uncoalesced for `nb00 == ggml_element_size(src0)`. If `blockIdx` had the same size in all dimensions we could just do:\r\n\r\n```\r\n    const int i01 = blockIdx.y;\r\n    const int i00 = blockIdx.x*blockDim.x + threadIdx.x;\r\n```\r\n\r\nBut then we run into issues for `i01 >= 65536`. So the solution is to just swap `blockIdx.x` and `blockIdx.y` but without touching `blockdim` and `threadIdx` (since they are conceptually different).",
        "pr_file_module": null
      },
      {
        "comment_id": "2194134067",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14551,
        "pr_file": "ggml/src/ggml-cuda/set-rows.cu",
        "discussion_id": "2193057284",
        "commented_code": "@@ -0,0 +1,137 @@\n+#include \"set-rows.cuh\"\n+\n+typedef void (*set_rows_kernel_t)(const char * src, char * dst);\n+\n+template<typename src_t, typename dst_t>\n+__device__ void set_rows_1(const src_t * src_f, dst_t * dst_f) {\n+    GGML_ABORT(\"unsupport type for set_rows\");\n+}\n+\n+template<>\n+__device__ __forceinline__ void set_rows_1<float, half>(const float * src_f, half * dst_h) {\n+    *dst_h = __float2half(*src_f);\n+}\n+\n+template<>\n+__device__ __forceinline__ void set_rows_1<float, float>(const float * src_f, float * dst_f) {\n+    *dst_f = *src_f;\n+}\n+\n+//TODO: consolidate kernels from cpy.cu, get_rows etc to make this function generic\n+template<typename src_t, typename dst_t>\n+static __global__ void k_set_rows(\n+        const src_t * __restrict__ src0, const int64_t * __restrict__ src1, dst_t * __restrict__ dst,\n+        const int64_t ne00, const int64_t ne01, const int64_t ne02, const int64_t ne03,\n+        const int64_t ne10, const int64_t ne11, const int64_t ne12, const int64_t ne13,\n+        const size_t nb01, const size_t nb02, const size_t nb03,\n+        const size_t nb10, const size_t nb11, const size_t nb12,\n+        const size_t nb1, const size_t nb2, const size_t nb3,\n+        const size_t src_type_size, const size_t dst_type_size) {\n+\n+    const int i03 = blockIdx.z / ne02;\n+    const int i02 = blockIdx.z % ne02;\n+    const int i01 = blockDim.x * blockIdx.x +  threadIdx.x;\n+    const int i00 = blockIdx.y;",
        "comment_created_at": "2025-07-09T06:16:44+00:00",
        "comment_author": "am17an",
        "comment_body": "I tried this and this was way worse in pp somehow, I suspect it's something to do the kernel grid params and the input shapes. Converting this to 1-d kernel (like cpy), perhaps unsurprisingly, seems to match performance of `LLAMA_SET_ROWS=0`. \r\n\r\nSince this function can have an impact on performance, it might be worthwhile experimenting with different launch parameters for different shapes. For now, this PR doesn't degrade performance and provides the SET_ROWS operation.",
        "pr_file_module": null
      }
    ]
  }
]