[
  {
    "discussion_id": "2219250286",
    "pr_number": 14624,
    "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
    "created_at": "2025-07-21T13:40:28+00:00",
    "commented_code": "const half2 * y_ds = (const half2 *) y;\n \n // #pragma unroll\n-    for (int k01 = 0; k01 < WARP_SIZE; k01 += VDR_Q8_0_Q8_1_MMQ) {\n+    for (int k01 = 0; k01 < MMQ_TILE_NE_K; k01 += VDR_Q8_0_Q8_1_MMQ) {\n         const int k0 = k00 + k01;\n \n #pragma unroll\n         for (int j0 = 0; j0 < mmq_x; j0 += nwarps) {\n             const int j = j0 + threadIdx.y;\n \n #pragma unroll\n-            for (int i0 = 0; i0 < mmq_y; i0 += WARP_SIZE) {\n+            for (int i0 = 0; i0 < mmq_y; i0 += warp_size) {\n                 const int i = i0 + threadIdx.x;\n \n-                sum[j0/nwarps*mmq_y/WARP_SIZE + i0/WARP_SIZE] += vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>\n-                    (&x_qs[i*(2*WARP_SIZE + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],\n-                    x_dm[i*(WARP_SIZE/QI5_1) + i/QI5_1 + k0/QI8_1], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);\n+                sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>\n+                    (&x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],\n+                    x_dm[i*(MMQ_TILE_NE_K/QI5_1) + i/QI5_1 + k0/QI8_1], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);\n             }\n         }\n     }\n }\n \n-template <int mmq_x, int mmq_y, int nwarps>\n+template <int mmq_x, int mmq_y>\n static __device__ __forceinline__ void vec_dot_q8_1_q8_1_mma(\n     const int * __restrict__ x, const int * __restrict__ y, float * __restrict__ sum, const int k00) {\n+#if defined(AMD_MMA_AVAILABLE)\n+    typedef tile<16,  8, int> tile_A;\n+    typedef tile<16,  8, int> tile_B;\n+    typedef tile<16, 16, int> tile_C;\n \n-    typedef tile<16, 8, int> tile_A;\n-    typedef tile< 8, 8, int> tile_B;\n-    typedef tile<16, 8, int> tile_C;\n+    constexpr int granularity = mmq_get_granularity_device(GGML_TYPE_Q4_K, mmq_x);\n+    constexpr int rows_per_warp = granularity;\n+    constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.\n+\n+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);\n+\n+    const int   * x_qs = (const int   *) x;\n+    const half2 * x_dm = (const half2 *) x_qs + 2*MMQ_TILE_NE_K;\n+    const int   * y_qs = (const int   *) y + 4;\n+    const half2 * y_dm = (const half2 *) y;\n+\n+    const int i0 = (threadIdx.y / ntx) * rows_per_warp;\n \n-    constexpr int granularity = mmq_get_granularity_device(mmq_x);\n+    for (int k01 = 0; k01 < MMQ_TILE_NE_K; k01 += QI8_1) {\n+        const int k0 = k00 + k01;\n+\n+        tile_A A[ntx];\n+#pragma unroll\n+        for (int n = 0; n < ntx; ++n) {\n+            load_ldmatrix(A[n], x_qs + (i0 + n*tile_A::I)*MMQ_MMA_TILE_X_K_Q8_1 + k0, MMQ_MMA_TILE_X_K_Q8_1);\n+        }\n+\n+#pragma unroll\n+        for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {\n+            tile_B B;\n+            load_ldmatrix(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);\n+\n+            float2 dsB;\n+            const int j = j0 + tile_C::get_j(0);\n+            dsB = __half22float2(y_dm[j*MMQ_TILE_Y_K + k01/QI8_1]);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2219250286",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219250286",
        "commented_code": "@@ -742,53 +977,104 @@ static __device__ __forceinline__ void vec_dot_q8_1_q8_1_dp4a(\n     const half2 * y_ds = (const half2 *) y;\n \n // #pragma unroll\n-    for (int k01 = 0; k01 < WARP_SIZE; k01 += VDR_Q8_0_Q8_1_MMQ) {\n+    for (int k01 = 0; k01 < MMQ_TILE_NE_K; k01 += VDR_Q8_0_Q8_1_MMQ) {\n         const int k0 = k00 + k01;\n \n #pragma unroll\n         for (int j0 = 0; j0 < mmq_x; j0 += nwarps) {\n             const int j = j0 + threadIdx.y;\n \n #pragma unroll\n-            for (int i0 = 0; i0 < mmq_y; i0 += WARP_SIZE) {\n+            for (int i0 = 0; i0 < mmq_y; i0 += warp_size) {\n                 const int i = i0 + threadIdx.x;\n \n-                sum[j0/nwarps*mmq_y/WARP_SIZE + i0/WARP_SIZE] += vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>\n-                    (&x_qs[i*(2*WARP_SIZE + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],\n-                    x_dm[i*(WARP_SIZE/QI5_1) + i/QI5_1 + k0/QI8_1], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);\n+                sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>\n+                    (&x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],\n+                    x_dm[i*(MMQ_TILE_NE_K/QI5_1) + i/QI5_1 + k0/QI8_1], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);\n             }\n         }\n     }\n }\n \n-template <int mmq_x, int mmq_y, int nwarps>\n+template <int mmq_x, int mmq_y>\n static __device__ __forceinline__ void vec_dot_q8_1_q8_1_mma(\n     const int * __restrict__ x, const int * __restrict__ y, float * __restrict__ sum, const int k00) {\n+#if defined(AMD_MMA_AVAILABLE)\n+    typedef tile<16,  8, int> tile_A;\n+    typedef tile<16,  8, int> tile_B;\n+    typedef tile<16, 16, int> tile_C;\n \n-    typedef tile<16, 8, int> tile_A;\n-    typedef tile< 8, 8, int> tile_B;\n-    typedef tile<16, 8, int> tile_C;\n+    constexpr int granularity = mmq_get_granularity_device(GGML_TYPE_Q4_K, mmq_x);\n+    constexpr int rows_per_warp = granularity;\n+    constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.\n+\n+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);\n+\n+    const int   * x_qs = (const int   *) x;\n+    const half2 * x_dm = (const half2 *) x_qs + 2*MMQ_TILE_NE_K;\n+    const int   * y_qs = (const int   *) y + 4;\n+    const half2 * y_dm = (const half2 *) y;\n+\n+    const int i0 = (threadIdx.y / ntx) * rows_per_warp;\n \n-    constexpr int granularity = mmq_get_granularity_device(mmq_x);\n+    for (int k01 = 0; k01 < MMQ_TILE_NE_K; k01 += QI8_1) {\n+        const int k0 = k00 + k01;\n+\n+        tile_A A[ntx];\n+#pragma unroll\n+        for (int n = 0; n < ntx; ++n) {\n+            load_ldmatrix(A[n], x_qs + (i0 + n*tile_A::I)*MMQ_MMA_TILE_X_K_Q8_1 + k0, MMQ_MMA_TILE_X_K_Q8_1);\n+        }\n+\n+#pragma unroll\n+        for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {\n+            tile_B B;\n+            load_ldmatrix(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);\n+\n+            float2 dsB;\n+            const int j = j0 + tile_C::get_j(0);\n+            dsB = __half22float2(y_dm[j*MMQ_TILE_Y_K + k01/QI8_1]);",
        "comment_created_at": "2025-07-21T13:40:28+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n            const int j = j0 + tile_C::get_j(0);\r\n            const float2 dsB = __half22float2(y_dm[j*MMQ_TILE_Y_K + k01/QI8_1]);\r\n```\r\n\r\nPreferably use `const` if applicable.",
        "pr_file_module": null
      },
      {
        "comment_id": "2220848703",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14624,
        "pr_file": "ggml/src/ggml-cuda/mmq.cuh",
        "discussion_id": "2219250286",
        "commented_code": "@@ -742,53 +977,104 @@ static __device__ __forceinline__ void vec_dot_q8_1_q8_1_dp4a(\n     const half2 * y_ds = (const half2 *) y;\n \n // #pragma unroll\n-    for (int k01 = 0; k01 < WARP_SIZE; k01 += VDR_Q8_0_Q8_1_MMQ) {\n+    for (int k01 = 0; k01 < MMQ_TILE_NE_K; k01 += VDR_Q8_0_Q8_1_MMQ) {\n         const int k0 = k00 + k01;\n \n #pragma unroll\n         for (int j0 = 0; j0 < mmq_x; j0 += nwarps) {\n             const int j = j0 + threadIdx.y;\n \n #pragma unroll\n-            for (int i0 = 0; i0 < mmq_y; i0 += WARP_SIZE) {\n+            for (int i0 = 0; i0 < mmq_y; i0 += warp_size) {\n                 const int i = i0 + threadIdx.x;\n \n-                sum[j0/nwarps*mmq_y/WARP_SIZE + i0/WARP_SIZE] += vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>\n-                    (&x_qs[i*(2*WARP_SIZE + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],\n-                    x_dm[i*(WARP_SIZE/QI5_1) + i/QI5_1 + k0/QI8_1], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);\n+                sum[j0/nwarps*mmq_y/warp_size + i0/warp_size] += vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>\n+                    (&x_qs[i*(2*MMQ_TILE_NE_K + 1) + k0], &y_qs[j*MMQ_TILE_Y_K + k01],\n+                    x_dm[i*(MMQ_TILE_NE_K/QI5_1) + i/QI5_1 + k0/QI8_1], y_ds[j*MMQ_TILE_Y_K + k01/QI8_1]);\n             }\n         }\n     }\n }\n \n-template <int mmq_x, int mmq_y, int nwarps>\n+template <int mmq_x, int mmq_y>\n static __device__ __forceinline__ void vec_dot_q8_1_q8_1_mma(\n     const int * __restrict__ x, const int * __restrict__ y, float * __restrict__ sum, const int k00) {\n+#if defined(AMD_MMA_AVAILABLE)\n+    typedef tile<16,  8, int> tile_A;\n+    typedef tile<16,  8, int> tile_B;\n+    typedef tile<16, 16, int> tile_C;\n \n-    typedef tile<16, 8, int> tile_A;\n-    typedef tile< 8, 8, int> tile_B;\n-    typedef tile<16, 8, int> tile_C;\n+    constexpr int granularity = mmq_get_granularity_device(GGML_TYPE_Q4_K, mmq_x);\n+    constexpr int rows_per_warp = granularity;\n+    constexpr int ntx = rows_per_warp/tile_C::I; // Number of x minitiles per warp.\n+\n+    y += (threadIdx.y % ntx) * (tile_C::J*MMQ_TILE_Y_K);\n+\n+    const int   * x_qs = (const int   *) x;\n+    const half2 * x_dm = (const half2 *) x_qs + 2*MMQ_TILE_NE_K;\n+    const int   * y_qs = (const int   *) y + 4;\n+    const half2 * y_dm = (const half2 *) y;\n+\n+    const int i0 = (threadIdx.y / ntx) * rows_per_warp;\n \n-    constexpr int granularity = mmq_get_granularity_device(mmq_x);\n+    for (int k01 = 0; k01 < MMQ_TILE_NE_K; k01 += QI8_1) {\n+        const int k0 = k00 + k01;\n+\n+        tile_A A[ntx];\n+#pragma unroll\n+        for (int n = 0; n < ntx; ++n) {\n+            load_ldmatrix(A[n], x_qs + (i0 + n*tile_A::I)*MMQ_MMA_TILE_X_K_Q8_1 + k0, MMQ_MMA_TILE_X_K_Q8_1);\n+        }\n+\n+#pragma unroll\n+        for (int j0 = 0; j0 < mmq_x; j0 += ntx*tile_C::J) {\n+            tile_B B;\n+            load_ldmatrix(B, y_qs + j0*MMQ_TILE_Y_K + k01, MMQ_TILE_Y_K);\n+\n+            float2 dsB;\n+            const int j = j0 + tile_C::get_j(0);\n+            dsB = __half22float2(y_dm[j*MMQ_TILE_Y_K + k01/QI8_1]);",
        "comment_created_at": "2025-07-22T02:25:58+00:00",
        "comment_author": "deepsek",
        "comment_body": "Resolved",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2156555104",
    "pr_number": 14265,
    "pr_file": "ggml/src/ggml-cuda/conv2d-dw.cu",
    "created_at": "2025-06-19T09:23:35+00:00",
    "commented_code": "+#include \"conv2d-dw.cuh\"\n+\n+struct conv_params {\n+    int in_w, in_h;\n+    int out_w, out_h;\n+    int kernel_w, kernel_h;\n+    int stride_x, stride_y;\n+    int padding_x, padding_y;\n+    int dilation_x, dilation_y;\n+    int channels, batches;\n+};\n+\n+struct kernel_bounds {\n+    int y_min, y_max;\n+    int x_min, x_max;\n+};\n+\n+__device__ inline kernel_bounds calculate_kernel_bounds(int out_x, int out_y, const conv_params & params) {\n+    kernel_bounds bounds;\n+    bounds.y_min = max(0, (params.padding_y - out_y * params.stride_y + params.dilation_y - 1) / params.dilation_y);\n+    bounds.y_max =\n+        min(params.kernel_h,\n+            (params.in_h + params.padding_y - out_y * params.stride_y + params.dilation_y - 1) / params.dilation_y);\n+    bounds.x_min = max(0, (params.padding_x - out_x * params.stride_x + params.dilation_x - 1) / params.dilation_x);\n+    bounds.x_max =\n+        min(params.kernel_w,\n+            (params.in_w + params.padding_x - out_x * params.stride_x + params.dilation_x - 1) / params.dilation_x);\n+    return bounds;\n+}\n+\n+__device__ inline int calculate_input_coord(int out_coord, int kern_coord, int stride, int dilation, int padding) {\n+    return out_coord * stride + kern_coord * dilation - padding;\n+}\n+\n+struct whcn_layout {\n+    __device__ static int input_index(int n, int c, int y, int x, const conv_params & params) {\n+        return n * (params.channels * params.in_w * params.in_h) + c * params.in_w * params.in_h + y * params.in_w + x;\n+    }\n+\n+    __device__ static int kernel_index(int c, int ky, int kx, const conv_params & params) {\n+        return c * params.kernel_h * params.kernel_w + ky * params.kernel_w + kx;\n+    }\n+\n+    __device__ static int output_index(int n, int c, int y, int x, const conv_params & params) {\n+        return n * (params.channels * params.out_w * params.out_h) + c * params.out_w * params.out_h +\n+               y * params.out_w + x;\n+    }\n+\n+    __device__ static void unpack_indices(int global_idx, const conv_params & params, int & n, int & c, int & out_y,\n+                                          int & out_x) {\n+        out_x = global_idx % params.out_w;\n+        out_y = (global_idx / params.out_w) % params.out_h;\n+        c     = (global_idx / (params.out_w * params.out_h)) % params.channels;\n+        n     = global_idx / (params.out_w * params.out_h * params.channels);\n+    }\n+};\n+\n+struct cwhn_layout {\n+    __device__ static int input_index(int n, int c, int y, int x, const conv_params & params) {\n+        return n * (params.channels * params.in_w * params.in_h) + (y * params.in_w + x) * params.channels + c;\n+    }\n+\n+    __device__ static int kernel_index(int c, int ky, int kx, const conv_params & params) {\n+        return (ky * params.kernel_w + kx) * params.channels + c;\n+    }\n+\n+    __device__ static int output_index(int n, int c, int y, int x, const conv_params & params) {\n+        return n * (params.channels * params.out_w * params.out_h) + y * (params.out_w * params.channels) +\n+               x * params.channels + c;\n+    }\n+\n+    __device__ static void unpack_indices(int global_idx, const conv_params & params, int & n, int & c, int & out_y,\n+                                          int & out_x) {\n+        c     = global_idx % params.channels;\n+        out_x = (global_idx / params.channels) % params.out_w;\n+        out_y = (global_idx / (params.channels * params.out_w)) % params.out_h;\n+        n     = global_idx / (params.channels * params.out_w * params.out_h);\n+    }\n+};\n+\n+template <typename T, typename Layout>\n+__global__ void conv2d_dw_kernel(const T * __restrict__ input, const T * __restrict__ kernel, T * __restrict__ output,\n+                                 const int in_w, const int in_h, const int out_w, const int out_h,\n+                                 const int kernel_w, const int kernel_h, const int stride_x, const int stride_y,\n+                                 const int padding_x, const int padding_y, const int dilation_x, const int dilation_y,\n+                                 const int channels, const int batches) {\n+    int global_idx     = blockIdx.x * blockDim.x + threadIdx.x;\n+    int total_elements = batches * channels * out_h * out_w;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2156555104",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14265,
        "pr_file": "ggml/src/ggml-cuda/conv2d-dw.cu",
        "discussion_id": "2156555104",
        "commented_code": "@@ -0,0 +1,161 @@\n+#include \"conv2d-dw.cuh\"\n+\n+struct conv_params {\n+    int in_w, in_h;\n+    int out_w, out_h;\n+    int kernel_w, kernel_h;\n+    int stride_x, stride_y;\n+    int padding_x, padding_y;\n+    int dilation_x, dilation_y;\n+    int channels, batches;\n+};\n+\n+struct kernel_bounds {\n+    int y_min, y_max;\n+    int x_min, x_max;\n+};\n+\n+__device__ inline kernel_bounds calculate_kernel_bounds(int out_x, int out_y, const conv_params & params) {\n+    kernel_bounds bounds;\n+    bounds.y_min = max(0, (params.padding_y - out_y * params.stride_y + params.dilation_y - 1) / params.dilation_y);\n+    bounds.y_max =\n+        min(params.kernel_h,\n+            (params.in_h + params.padding_y - out_y * params.stride_y + params.dilation_y - 1) / params.dilation_y);\n+    bounds.x_min = max(0, (params.padding_x - out_x * params.stride_x + params.dilation_x - 1) / params.dilation_x);\n+    bounds.x_max =\n+        min(params.kernel_w,\n+            (params.in_w + params.padding_x - out_x * params.stride_x + params.dilation_x - 1) / params.dilation_x);\n+    return bounds;\n+}\n+\n+__device__ inline int calculate_input_coord(int out_coord, int kern_coord, int stride, int dilation, int padding) {\n+    return out_coord * stride + kern_coord * dilation - padding;\n+}\n+\n+struct whcn_layout {\n+    __device__ static int input_index(int n, int c, int y, int x, const conv_params & params) {\n+        return n * (params.channels * params.in_w * params.in_h) + c * params.in_w * params.in_h + y * params.in_w + x;\n+    }\n+\n+    __device__ static int kernel_index(int c, int ky, int kx, const conv_params & params) {\n+        return c * params.kernel_h * params.kernel_w + ky * params.kernel_w + kx;\n+    }\n+\n+    __device__ static int output_index(int n, int c, int y, int x, const conv_params & params) {\n+        return n * (params.channels * params.out_w * params.out_h) + c * params.out_w * params.out_h +\n+               y * params.out_w + x;\n+    }\n+\n+    __device__ static void unpack_indices(int global_idx, const conv_params & params, int & n, int & c, int & out_y,\n+                                          int & out_x) {\n+        out_x = global_idx % params.out_w;\n+        out_y = (global_idx / params.out_w) % params.out_h;\n+        c     = (global_idx / (params.out_w * params.out_h)) % params.channels;\n+        n     = global_idx / (params.out_w * params.out_h * params.channels);\n+    }\n+};\n+\n+struct cwhn_layout {\n+    __device__ static int input_index(int n, int c, int y, int x, const conv_params & params) {\n+        return n * (params.channels * params.in_w * params.in_h) + (y * params.in_w + x) * params.channels + c;\n+    }\n+\n+    __device__ static int kernel_index(int c, int ky, int kx, const conv_params & params) {\n+        return (ky * params.kernel_w + kx) * params.channels + c;\n+    }\n+\n+    __device__ static int output_index(int n, int c, int y, int x, const conv_params & params) {\n+        return n * (params.channels * params.out_w * params.out_h) + y * (params.out_w * params.channels) +\n+               x * params.channels + c;\n+    }\n+\n+    __device__ static void unpack_indices(int global_idx, const conv_params & params, int & n, int & c, int & out_y,\n+                                          int & out_x) {\n+        c     = global_idx % params.channels;\n+        out_x = (global_idx / params.channels) % params.out_w;\n+        out_y = (global_idx / (params.channels * params.out_w)) % params.out_h;\n+        n     = global_idx / (params.channels * params.out_w * params.out_h);\n+    }\n+};\n+\n+template <typename T, typename Layout>\n+__global__ void conv2d_dw_kernel(const T * __restrict__ input, const T * __restrict__ kernel, T * __restrict__ output,\n+                                 const int in_w, const int in_h, const int out_w, const int out_h,\n+                                 const int kernel_w, const int kernel_h, const int stride_x, const int stride_y,\n+                                 const int padding_x, const int padding_y, const int dilation_x, const int dilation_y,\n+                                 const int channels, const int batches) {\n+    int global_idx     = blockIdx.x * blockDim.x + threadIdx.x;\n+    int total_elements = batches * channels * out_h * out_w;",
        "comment_created_at": "2025-06-19T09:23:35+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n    const int global_idx     = blockIdx.x * blockDim.x + threadIdx.x;\r\n    const int total_elements = batches * channels * out_h * out_w;\r\n```\r\n\r\nPreferably add const to variables unless they are actually intended to be changed later.",
        "pr_file_module": null
      }
    ]
  }
]