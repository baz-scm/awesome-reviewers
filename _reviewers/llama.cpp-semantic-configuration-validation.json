[
  {
    "discussion_id": "2093731085",
    "pr_number": 13196,
    "pr_file": "common/chat.h",
    "created_at": "2025-05-16T22:13:48+00:00",
    "commented_code": "bool parallel_tool_calls = false;\n     bool extract_reasoning     = true;\n     std::chrono::system_clock::time_point now = std::chrono::system_clock::now();\n+    std::map<std::string, std::string> chat_template_kwargs;",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2093731085",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13196,
        "pr_file": "common/chat.h",
        "discussion_id": "2093731085",
        "commented_code": "@@ -73,6 +74,7 @@ struct common_chat_templates_inputs {\n     bool parallel_tool_calls = false;\n     bool extract_reasoning     = true;\n     std::chrono::system_clock::time_point now = std::chrono::system_clock::now();\n+    std::map<std::string, std::string> chat_template_kwargs;",
        "comment_created_at": "2025-05-16T22:13:48+00:00",
        "comment_author": "ochafik",
        "comment_body": "This is a map from string keys to stringified json values. Why not just store the stringified top-level json? (maybe name it `chat_additional_context_json` or `chat_template_kwargs_json`?)",
        "pr_file_module": null
      },
      {
        "comment_id": "2094157132",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13196,
        "pr_file": "common/chat.h",
        "discussion_id": "2093731085",
        "commented_code": "@@ -73,6 +74,7 @@ struct common_chat_templates_inputs {\n     bool parallel_tool_calls = false;\n     bool extract_reasoning     = true;\n     std::chrono::system_clock::time_point now = std::chrono::system_clock::now();\n+    std::map<std::string, std::string> chat_template_kwargs;",
        "comment_created_at": "2025-05-17T16:09:27+00:00",
        "comment_author": "matteoserva",
        "comment_body": "Being a map of strings is on purpose. It allows other code to set a key-value pair or check if a key exists without adding json as a dependency.\r\nFor example I used that to prevent setting enable_thinking when the user also requested an assistant response prefill since the two feature are incompatible.\r\nOf course I'm always open to suggestions.",
        "pr_file_module": null
      },
      {
        "comment_id": "2106124676",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13196,
        "pr_file": "common/chat.h",
        "discussion_id": "2093731085",
        "commented_code": "@@ -73,6 +74,7 @@ struct common_chat_templates_inputs {\n     bool parallel_tool_calls = false;\n     bool extract_reasoning     = true;\n     std::chrono::system_clock::time_point now = std::chrono::system_clock::now();\n+    std::map<std::string, std::string> chat_template_kwargs;",
        "comment_created_at": "2025-05-25T08:21:27+00:00",
        "comment_author": "ochafik",
        "comment_body": "Ah, fair enough, but... if other code needs to set the non-thinking behaviour programmatically, it's probably better to introduce some semantic variable rather than use generic extra context variable. https://github.com/ggml-org/llama.cpp/pull/13771 goes in this direction although maybe it shouldn't reuse the existing reasoning-format concept, tbd.\r\n\r\nThe general direction w/ jinja templates so far has been to prevent users from having to learn about the very many templates available in the wild and all of their idiosyncracies / custom variables, by doing backfill of various behaviours, and unified API surface for common features. I think we should only deviate from this principle when absolutely needed, and disabling the thoughts falls into the category of \"common feature that should be handled generically\" (also, doesn't involve a variable for most thinking models).",
        "pr_file_module": null
      },
      {
        "comment_id": "2106129175",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13196,
        "pr_file": "common/chat.h",
        "discussion_id": "2093731085",
        "commented_code": "@@ -73,6 +74,7 @@ struct common_chat_templates_inputs {\n     bool parallel_tool_calls = false;\n     bool extract_reasoning     = true;\n     std::chrono::system_clock::time_point now = std::chrono::system_clock::now();\n+    std::map<std::string, std::string> chat_template_kwargs;",
        "comment_created_at": "2025-05-25T08:39:56+00:00",
        "comment_author": "matteoserva",
        "comment_body": "I agree with you. The thinking feature of models is becoming standard (IMHO) so it makes sense to have a common way to disable it. This PR should be a generic way to pass variables to the jinja template.\r\n\r\nLet me know if there is something I should modify.",
        "pr_file_module": null
      },
      {
        "comment_id": "2107890711",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13196,
        "pr_file": "common/chat.h",
        "discussion_id": "2093731085",
        "commented_code": "@@ -73,6 +74,7 @@ struct common_chat_templates_inputs {\n     bool parallel_tool_calls = false;\n     bool extract_reasoning     = true;\n     std::chrono::system_clock::time_point now = std::chrono::system_clock::now();\n+    std::map<std::string, std::string> chat_template_kwargs;",
        "comment_created_at": "2025-05-26T22:22:44+00:00",
        "comment_author": "ochafik",
        "comment_body": "Took the liberty to push an update in your branch with support for propagating the extra context for all the chat formats with slightly simpler code.\r\n\r\nCould you please update the PR description mentioning the new alternative way to disable the thinking globally  (https://github.com/ggml-org/llama.cpp/pull/13771, and discussions on upcoming per-request mechanism for it: https://github.com/ggml-org/llama.cpp/issues/13272), and adding a link to the [VLLM API](https://docs.vllm.ai/en/v0.8.3/serving/openai_compatible_server.html#id7) mentioning the new param has the same syntax (I only realized that from your last comment :-))",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2146720296",
    "pr_number": 13196,
    "pr_file": "tools/server/utils.hpp",
    "created_at": "2025-06-14T08:09:54+00:00",
    "commented_code": "/* TODO: test this properly */\n         inputs.reasoning_format = COMMON_REASONING_FORMAT_NONE;\n+\n+        if ( (!inputs.enable_thinking) || inputs.chat_template_kwargs.find(\"enable_thinking\") != inputs.chat_template_kwargs.end()) {\n+            throw std::runtime_error(\"Assistant response prefill is incompatible with enable_thinking.\");",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2146720296",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13196,
        "pr_file": "tools/server/utils.hpp",
        "discussion_id": "2146720296",
        "commented_code": "@@ -763,6 +771,11 @@ static json oaicompat_chat_params_parse(\n \n         /* TODO: test this properly */\n         inputs.reasoning_format = COMMON_REASONING_FORMAT_NONE;\n+\n+        if ( (!inputs.enable_thinking) || inputs.chat_template_kwargs.find(\"enable_thinking\") != inputs.chat_template_kwargs.end()) {\n+            throw std::runtime_error(\"Assistant response prefill is incompatible with enable_thinking.\");",
        "comment_created_at": "2025-06-14T08:09:54+00:00",
        "comment_author": "exxocism",
        "comment_body": "question) I tried `enable_thinking: false` param without this statement, and it seems working well.\r\nIs there a reason for prohibiting response prefill? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2146923038",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13196,
        "pr_file": "tools/server/utils.hpp",
        "discussion_id": "2146720296",
        "commented_code": "@@ -763,6 +771,11 @@ static json oaicompat_chat_params_parse(\n \n         /* TODO: test this properly */\n         inputs.reasoning_format = COMMON_REASONING_FORMAT_NONE;\n+\n+        if ( (!inputs.enable_thinking) || inputs.chat_template_kwargs.find(\"enable_thinking\") != inputs.chat_template_kwargs.end()) {\n+            throw std::runtime_error(\"Assistant response prefill is incompatible with enable_thinking.\");",
        "comment_created_at": "2025-06-14T12:23:21+00:00",
        "comment_author": "matteoserva",
        "comment_body": "Because `enable_thinking: false` is implemented through prefilling the assistant response. These features can be made work together in a future update but right now they are incompatible.",
        "pr_file_module": null
      }
    ]
  }
]