[
  {
    "discussion_id": "2200785702",
    "pr_number": 14622,
    "pr_file": "tools/dataset-converter/README.md",
    "created_at": "2025-07-11T13:42:47+00:00",
    "commented_code": "+`convert-to-train-gguf` Utility\n+===============================\n+\n+This utility is designed to convert text datasets (or pre-tokenized data) into the GGUF format, optimized for training models in `llama.cpp`.\n+\n+Features\n+--------\n+\n+*   **Two-pass processing**: Efficiently handles large datasets that do not fit entirely into RAM, performing a first pass to collect metadata and a second pass to write the actual tensor data.\n+\n+*   **Flexible input**: Supports reading both raw text (with subsequent tokenization using a provided model) and pre-tokenized data (in the format of space-separated token IDs).\n+\n+*   **Modular architecture**: The code is divided into several classes (`llama_gguf_file`, `llama_gguf_writer`, `llama_dataset_reader`, `llama_text_dataset_reader`, `llama_gguf_converter`, `llama_gguf_reader`) to improve modularity, extensibility, and testability.\n+\n+*   **Preview functionality**: Allows you to view metadata and the first few sequences of the generated GGUF file, including optional detokenization.\n+\n+\n+GGUF Structure for Training Data\n+--------------------------------\n+\n+The generated GGUF files follow a specific structure for training data:\n+\n+*   **Metadata (KV-pairs)**: All metadata keys are prefixed with `training.` to avoid conflicts with model metadata.\n+\n+    *   `training.format.version`: `string` (e.g., \"1.0\") - Specification version.",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2200785702",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14622,
        "pr_file": "tools/dataset-converter/README.md",
        "discussion_id": "2200785702",
        "commented_code": "@@ -0,0 +1,148 @@\n+`convert-to-train-gguf` Utility\n+===============================\n+\n+This utility is designed to convert text datasets (or pre-tokenized data) into the GGUF format, optimized for training models in `llama.cpp`.\n+\n+Features\n+--------\n+\n+*   **Two-pass processing**: Efficiently handles large datasets that do not fit entirely into RAM, performing a first pass to collect metadata and a second pass to write the actual tensor data.\n+\n+*   **Flexible input**: Supports reading both raw text (with subsequent tokenization using a provided model) and pre-tokenized data (in the format of space-separated token IDs).\n+\n+*   **Modular architecture**: The code is divided into several classes (`llama_gguf_file`, `llama_gguf_writer`, `llama_dataset_reader`, `llama_text_dataset_reader`, `llama_gguf_converter`, `llama_gguf_reader`) to improve modularity, extensibility, and testability.\n+\n+*   **Preview functionality**: Allows you to view metadata and the first few sequences of the generated GGUF file, including optional detokenization.\n+\n+\n+GGUF Structure for Training Data\n+--------------------------------\n+\n+The generated GGUF files follow a specific structure for training data:\n+\n+*   **Metadata (KV-pairs)**: All metadata keys are prefixed with `training.` to avoid conflicts with model metadata.\n+\n+    *   `training.format.version`: `string` (e.g., \"1.0\") - Specification version.",
        "comment_created_at": "2025-07-11T13:42:47+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "Preferably use an integer for the version. If you want to specify minor versions my suggestion is to either specify multiple integers or to do what NVIDIA does and specify e.g. v12.34 as something like `12034`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2200792274",
    "pr_number": 14622,
    "pr_file": "tools/dataset-converter/README.md",
    "created_at": "2025-07-11T13:46:11+00:00",
    "commented_code": "+`convert-to-train-gguf` Utility\n+===============================\n+\n+This utility is designed to convert text datasets (or pre-tokenized data) into the GGUF format, optimized for training models in `llama.cpp`.\n+\n+Features\n+--------\n+\n+*   **Two-pass processing**: Efficiently handles large datasets that do not fit entirely into RAM, performing a first pass to collect metadata and a second pass to write the actual tensor data.\n+\n+*   **Flexible input**: Supports reading both raw text (with subsequent tokenization using a provided model) and pre-tokenized data (in the format of space-separated token IDs).\n+\n+*   **Modular architecture**: The code is divided into several classes (`llama_gguf_file`, `llama_gguf_writer`, `llama_dataset_reader`, `llama_text_dataset_reader`, `llama_gguf_converter`, `llama_gguf_reader`) to improve modularity, extensibility, and testability.\n+\n+*   **Preview functionality**: Allows you to view metadata and the first few sequences of the generated GGUF file, including optional detokenization.\n+\n+\n+GGUF Structure for Training Data\n+--------------------------------\n+\n+The generated GGUF files follow a specific structure for training data:\n+\n+*   **Metadata (KV-pairs)**: All metadata keys are prefixed with `training.` to avoid conflicts with model metadata.\n+\n+    *   `training.format.version`: `string` (e.g., \"1.0\") - Specification version.\n+\n+    *   `training.dataset.name`: `string` (optional) - Dataset name (e.g., \"OpenWebText-ru\").\n+\n+    *   `training.dataset.source`: `string` (optional) - URL or description of the data source.\n+\n+    *   `training.file.creation_date`: `string` (ISO 8601) - File creation date.\n+\n+    *   `training.tokenizer.gguf.model`: `string` - Tokenizer model name (e.g., \"llama\", \"gpt2\", \"bert\").\n+\n+    *   `training.tokenizer.gguf.vocab`: `array[string]` - Tokenizer vocabulary.\n+\n+    *   `training.tokenizer.gguf.merges`: `array[string]` (optional) - Tokenizer merges (for BPE).\n+\n+    *   `training.tokenizer.gguf.pre`: `string` (optional) - Pre-tokenization architecture.\n+\n+    *   `training.sequence.count`: `uint64` - Total number of sequences in the file.\n+\n+*   **Tensors**: Each training sequence is stored as a separate tensor.\n+\n+    *   **Naming**: `training.tensor.{index}` (e.g., `training.tensor.0`, `training.tensor.1`, ...).",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2200792274",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14622,
        "pr_file": "tools/dataset-converter/README.md",
        "discussion_id": "2200792274",
        "commented_code": "@@ -0,0 +1,148 @@\n+`convert-to-train-gguf` Utility\n+===============================\n+\n+This utility is designed to convert text datasets (or pre-tokenized data) into the GGUF format, optimized for training models in `llama.cpp`.\n+\n+Features\n+--------\n+\n+*   **Two-pass processing**: Efficiently handles large datasets that do not fit entirely into RAM, performing a first pass to collect metadata and a second pass to write the actual tensor data.\n+\n+*   **Flexible input**: Supports reading both raw text (with subsequent tokenization using a provided model) and pre-tokenized data (in the format of space-separated token IDs).\n+\n+*   **Modular architecture**: The code is divided into several classes (`llama_gguf_file`, `llama_gguf_writer`, `llama_dataset_reader`, `llama_text_dataset_reader`, `llama_gguf_converter`, `llama_gguf_reader`) to improve modularity, extensibility, and testability.\n+\n+*   **Preview functionality**: Allows you to view metadata and the first few sequences of the generated GGUF file, including optional detokenization.\n+\n+\n+GGUF Structure for Training Data\n+--------------------------------\n+\n+The generated GGUF files follow a specific structure for training data:\n+\n+*   **Metadata (KV-pairs)**: All metadata keys are prefixed with `training.` to avoid conflicts with model metadata.\n+\n+    *   `training.format.version`: `string` (e.g., \"1.0\") - Specification version.\n+\n+    *   `training.dataset.name`: `string` (optional) - Dataset name (e.g., \"OpenWebText-ru\").\n+\n+    *   `training.dataset.source`: `string` (optional) - URL or description of the data source.\n+\n+    *   `training.file.creation_date`: `string` (ISO 8601) - File creation date.\n+\n+    *   `training.tokenizer.gguf.model`: `string` - Tokenizer model name (e.g., \"llama\", \"gpt2\", \"bert\").\n+\n+    *   `training.tokenizer.gguf.vocab`: `array[string]` - Tokenizer vocabulary.\n+\n+    *   `training.tokenizer.gguf.merges`: `array[string]` (optional) - Tokenizer merges (for BPE).\n+\n+    *   `training.tokenizer.gguf.pre`: `string` (optional) - Pre-tokenization architecture.\n+\n+    *   `training.sequence.count`: `uint64` - Total number of sequences in the file.\n+\n+*   **Tensors**: Each training sequence is stored as a separate tensor.\n+\n+    *   **Naming**: `training.tensor.{index}` (e.g., `training.tensor.0`, `training.tensor.1`, ...).",
        "comment_created_at": "2025-07-11T13:46:11+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "```suggestion\r\n    *   **Naming**: `training.tensor.{index}` (e.g., `training.tensor.0`, `training.tensor.1`, ...). No leading zeros.\r\n```\r\n\r\nWould also be fine to add leading zeros but it should be specified.",
        "pr_file_module": null
      }
    ]
  }
]