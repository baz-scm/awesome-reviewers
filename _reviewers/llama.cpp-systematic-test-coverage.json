[
  {
    "discussion_id": "2231046522",
    "pr_number": 14872,
    "pr_file": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
    "created_at": "2025-07-25T13:09:52+00:00",
    "commented_code": "}\n         return nullptr;\n     case GGML_OP_CONV_2D:\n-        if (src0->type == GGML_TYPE_F32 && src1->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32 &&",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2231046522",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14872,
        "pr_file": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "discussion_id": "2231046522",
        "commented_code": "@@ -6958,9 +6968,13 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n         }\n         return nullptr;\n     case GGML_OP_CONV_2D:\n-        if (src0->type == GGML_TYPE_F32 && src1->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32 &&",
        "comment_created_at": "2025-07-25T13:09:52+00:00",
        "comment_author": "jeffbolznv",
        "comment_body": "Please also update ggml_backend_vk_device_supports_op to report that this combination is supported.\r\n\r\nIt looks like test-backend-ops only tests F32, please also add variants for F16.",
        "pr_file_module": null
      },
      {
        "comment_id": "2231112607",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14872,
        "pr_file": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "discussion_id": "2231046522",
        "commented_code": "@@ -6958,9 +6968,13 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n         }\n         return nullptr;\n     case GGML_OP_CONV_2D:\n-        if (src0->type == GGML_TYPE_F32 && src1->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32 &&",
        "comment_created_at": "2025-07-25T13:38:09+00:00",
        "comment_author": "Green-Sky",
        "comment_body": "Done. Not sure about what cases to test in f16, f32 or both.",
        "pr_file_module": null
      },
      {
        "comment_id": "2231622019",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14872,
        "pr_file": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "discussion_id": "2231046522",
        "commented_code": "@@ -6958,9 +6968,13 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n         }\n         return nullptr;\n     case GGML_OP_CONV_2D:\n-        if (src0->type == GGML_TYPE_F32 && src1->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32 &&",
        "comment_created_at": "2025-07-25T17:01:31+00:00",
        "comment_author": "netrunnereve",
        "comment_body": "I think it's fine to just reuse the f32 cases for now.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2231616959",
    "pr_number": 14872,
    "pr_file": "tests/test-backend-ops.cpp",
    "created_at": "2025-07-25T16:59:28+00:00",
    "commented_code": "for (auto act_case : cases) {\n         test_cases.emplace_back(new test_conv_2d(\n             { act_case[iwh_idx], act_case[iwh_idx], act_case[Cin_idx], act_case[B_idx] },\n-            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] }, 1, 1, 0, 0, 1, 1, false));\n+            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] },\n+            GGML_TYPE_F32, 1, 1, 0, 0, 1, 1, false));\n+        test_cases.emplace_back(new test_conv_2d(\n+            { act_case[iwh_idx], act_case[iwh_idx], act_case[Cin_idx], act_case[B_idx] },\n+            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] },\n+            GGML_TYPE_F16, 1, 1, 0, 0, 1, 1, false));",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2231616959",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14872,
        "pr_file": "tests/test-backend-ops.cpp",
        "discussion_id": "2231616959",
        "commented_code": "@@ -5141,7 +5143,12 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n     for (auto act_case : cases) {\n         test_cases.emplace_back(new test_conv_2d(\n             { act_case[iwh_idx], act_case[iwh_idx], act_case[Cin_idx], act_case[B_idx] },\n-            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] }, 1, 1, 0, 0, 1, 1, false));\n+            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] },\n+            GGML_TYPE_F32, 1, 1, 0, 0, 1, 1, false));\n+        test_cases.emplace_back(new test_conv_2d(\n+            { act_case[iwh_idx], act_case[iwh_idx], act_case[Cin_idx], act_case[B_idx] },\n+            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] },\n+            GGML_TYPE_F16, 1, 1, 0, 0, 1, 1, false));",
        "comment_created_at": "2025-07-25T16:59:28+00:00",
        "comment_author": "netrunnereve",
        "comment_body": "If we repeat the same test for different formats we typically loop through a ggml_type instead.\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/793c0d7f46384001738c337d7afa46b45ae32745/tests/test-backend-ops.cpp#L4989-L4996",
        "pr_file_module": null
      },
      {
        "comment_id": "2231817009",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14872,
        "pr_file": "tests/test-backend-ops.cpp",
        "discussion_id": "2231616959",
        "commented_code": "@@ -5141,7 +5143,12 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n     for (auto act_case : cases) {\n         test_cases.emplace_back(new test_conv_2d(\n             { act_case[iwh_idx], act_case[iwh_idx], act_case[Cin_idx], act_case[B_idx] },\n-            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] }, 1, 1, 0, 0, 1, 1, false));\n+            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] },\n+            GGML_TYPE_F32, 1, 1, 0, 0, 1, 1, false));\n+        test_cases.emplace_back(new test_conv_2d(\n+            { act_case[iwh_idx], act_case[iwh_idx], act_case[Cin_idx], act_case[B_idx] },\n+            { act_case[kwh_idx], act_case[kwh_idx], act_case[Cin_idx], act_case[Cout_idx] },\n+            GGML_TYPE_F16, 1, 1, 0, 0, 1, 1, false));",
        "comment_created_at": "2025-07-25T18:57:47+00:00",
        "comment_author": "Green-Sky",
        "comment_body": "Done. Also added the f16 tests to the normal tests, since they seem to run fast.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2200663021",
    "pr_number": 13873,
    "pr_file": "tests/test-backend-ops.cpp",
    "created_at": "2025-07-11T12:56:33+00:00",
    "commented_code": "}\n     };\n \n+    char const* name = ggml_backend_name(backend);\n+    bool const vulkan = strstr(name, \"ulkan\");\n+    bool const sgd = !vulkan;\n+\n     if (mode == MODE_TEST) {\n-        auto test_cases = make_test_cases_eval();\n+        auto test_cases = make_test_cases_eval(sgd);",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2200663021",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 13873,
        "pr_file": "tests/test-backend-ops.cpp",
        "discussion_id": "2200663021",
        "commented_code": "@@ -5548,8 +5589,12 @@ static bool test_backend(ggml_backend_t backend, test_mode mode, const char * op\n         }\n     };\n \n+    char const* name = ggml_backend_name(backend);\n+    bool const vulkan = strstr(name, \"ulkan\");\n+    bool const sgd = !vulkan;\n+\n     if (mode == MODE_TEST) {\n-        auto test_cases = make_test_cases_eval();\n+        auto test_cases = make_test_cases_eval(sgd);",
        "comment_created_at": "2025-07-11T12:56:33+00:00",
        "comment_author": "JohannesGaessler",
        "comment_body": "You can remove this logic if the Vulkan backend simply returns that `OPT_STEP_SGD` is unsupported, the corresponding test will then simply be skipped.",
        "pr_file_module": null
      }
    ]
  }
]