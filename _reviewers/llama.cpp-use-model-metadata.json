[
  {
    "discussion_id": "2233027231",
    "pr_number": 14771,
    "pr_file": "examples/diffusion/diffusion-cli.cpp",
    "created_at": "2025-07-26T15:03:02+00:00",
    "commented_code": "const llama_vocab * vocab            = llama_model_get_vocab(model);\n     std::string         formatted_prompt = format_input_text(params.prompt, params.enable_chat_template, model);\n \n-    std::vector<llama_token> input_tokens = common_tokenize(vocab, formatted_prompt,\n+    std::vector<llama_token> input_tokens = common_tokenize(vocab,\n+                                                            formatted_prompt,\n                                                             /*add special tokens*/ true,\n                                                             /*parse special*/ true);\n-    int                      n_input      = input_tokens.size();\n+\n+    // For LLaDA models, forcefully add BOS token at the beginning. TODO: check why\n+    if (arch == \"llada\") {\n+        llama_token bos_token = llama_vocab_bos(vocab);\n+        if (bos_token != LLAMA_TOKEN_NULL && (input_tokens.empty() || input_tokens[0] != bos_token)) {\n+            input_tokens.insert(input_tokens.begin(), bos_token);\n+        }\n+    }\n+",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2233027231",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14771,
        "pr_file": "examples/diffusion/diffusion-cli.cpp",
        "discussion_id": "2233027231",
        "commented_code": "@@ -442,10 +753,20 @@ int main(int argc, char ** argv) {\n     const llama_vocab * vocab            = llama_model_get_vocab(model);\n     std::string         formatted_prompt = format_input_text(params.prompt, params.enable_chat_template, model);\n \n-    std::vector<llama_token> input_tokens = common_tokenize(vocab, formatted_prompt,\n+    std::vector<llama_token> input_tokens = common_tokenize(vocab,\n+                                                            formatted_prompt,\n                                                             /*add special tokens*/ true,\n                                                             /*parse special*/ true);\n-    int                      n_input      = input_tokens.size();\n+\n+    // For LLaDA models, forcefully add BOS token at the beginning. TODO: check why\n+    if (arch == \"llada\") {\n+        llama_token bos_token = llama_vocab_bos(vocab);\n+        if (bos_token != LLAMA_TOKEN_NULL && (input_tokens.empty() || input_tokens[0] != bos_token)) {\n+            input_tokens.insert(input_tokens.begin(), bos_token);\n+        }\n+    }\n+",
        "comment_created_at": "2025-07-26T15:03:02+00:00",
        "comment_author": "ggerganov",
        "comment_body": "This should be handled by the meta data in the GGUF model. There is a boolean field for when BOS is needed or not.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2227982392",
    "pr_number": 14750,
    "pr_file": "tools/mtmd/clip.cpp",
    "created_at": "2025-07-24T09:30:51+00:00",
    "commented_code": "int n_embd = clip_n_mmproj_embd(ctx);\n             const int d_head = 128;\n             int n_head = n_embd/d_head;\n+            // Use actual config value if available, otherwise fall back to hardcoded values\n             int num_query = 96;\n-            if (ctx->model.hparams.minicpmv_version == 2) {\n-                num_query = 96;\n-            } else if (ctx->model.hparams.minicpmv_version == 3) {\n-                num_query = 64;\n-            } else if (ctx->model.hparams.minicpmv_version == 4) {\n-                num_query = 64;\n+            if (ctx->model.hparams.minicpmv_query_num > 0) {\n+                num_query = ctx->model.hparams.minicpmv_query_num;\n+            } else {\n+                // Fallback to hardcoded values for legacy models\n+                if (ctx->model.hparams.minicpmv_version == 2) {\n+                    num_query = 96;\n+                } else if (ctx->model.hparams.minicpmv_version == 3) {\n+                    num_query = 64;\n+                } else if (ctx->model.hparams.minicpmv_version == 4) {\n+                    num_query = 64;\n+                }",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2227982392",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14750,
        "pr_file": "tools/mtmd/clip.cpp",
        "discussion_id": "2227982392",
        "commented_code": "@@ -847,13 +849,19 @@ struct clip_graph {\n             int n_embd = clip_n_mmproj_embd(ctx);\n             const int d_head = 128;\n             int n_head = n_embd/d_head;\n+            // Use actual config value if available, otherwise fall back to hardcoded values\n             int num_query = 96;\n-            if (ctx->model.hparams.minicpmv_version == 2) {\n-                num_query = 96;\n-            } else if (ctx->model.hparams.minicpmv_version == 3) {\n-                num_query = 64;\n-            } else if (ctx->model.hparams.minicpmv_version == 4) {\n-                num_query = 64;\n+            if (ctx->model.hparams.minicpmv_query_num > 0) {\n+                num_query = ctx->model.hparams.minicpmv_query_num;\n+            } else {\n+                // Fallback to hardcoded values for legacy models\n+                if (ctx->model.hparams.minicpmv_version == 2) {\n+                    num_query = 96;\n+                } else if (ctx->model.hparams.minicpmv_version == 3) {\n+                    num_query = 64;\n+                } else if (ctx->model.hparams.minicpmv_version == 4) {\n+                    num_query = 64;\n+                }",
        "comment_created_at": "2025-07-24T09:30:51+00:00",
        "comment_author": "ngxson",
        "comment_body": "move this into `clip_model_loader` and populate `ctx->model.hparams.minicpmv_query_num` when loading the model",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2231917163",
    "pr_number": 14878,
    "pr_file": "src/llama-chat.cpp",
    "created_at": "2025-07-25T20:03:06+00:00",
    "commented_code": "if (role == \"system\") {\n                 ss << \"<|startoftext|>\" << message->content << \"<|extra_4|>\";\n             } else if (role == \"assistant\") {\n-                ss << \"<|startoftext|>\" << message->content << \"<|eos|>\";\n+                ss << message->content << \"<|eos|>\";\n             } else {\n                 ss << \"<|startoftext|>\" << message->content << \"<|extra_0|>\";\n             }\n         }\n+    } else if (tmpl == LLM_CHAT_TEMPLATE_HUNYUAN_DENSE) {\n+        // tencent/Hunyuan-4B\n+        for (size_t i = 0; i < chat.size(); i++) {\n+            std::string role(chat[i]->role);\n+            if (i == 0) {\n+                if (role == \"system\") {\n+                    ss << \"<\uff5chy_begin\u2581of\u2581sentence\uff5c>\" << chat[i]->content << \"<\uff5chy_place\u2581holder\u2581no\u25813\uff5c>\";\n+                } else {\n+                    ss << \"<\uff5chy_begin\u2581of\u2581sentence\uff5c>\";",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2231917163",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14878,
        "pr_file": "src/llama-chat.cpp",
        "discussion_id": "2231917163",
        "commented_code": "@@ -698,11 +701,34 @@ int32_t llm_chat_apply_template(\n             if (role == \"system\") {\n                 ss << \"<|startoftext|>\" << message->content << \"<|extra_4|>\";\n             } else if (role == \"assistant\") {\n-                ss << \"<|startoftext|>\" << message->content << \"<|eos|>\";\n+                ss << message->content << \"<|eos|>\";\n             } else {\n                 ss << \"<|startoftext|>\" << message->content << \"<|extra_0|>\";\n             }\n         }\n+    } else if (tmpl == LLM_CHAT_TEMPLATE_HUNYUAN_DENSE) {\n+        // tencent/Hunyuan-4B\n+        for (size_t i = 0; i < chat.size(); i++) {\n+            std::string role(chat[i]->role);\n+            if (i == 0) {\n+                if (role == \"system\") {\n+                    ss << \"<\uff5chy_begin\u2581of\u2581sentence\uff5c>\" << chat[i]->content << \"<\uff5chy_place\u2581holder\u2581no\u25813\uff5c>\";\n+                } else {\n+                    ss << \"<\uff5chy_begin\u2581of\u2581sentence\uff5c>\";",
        "comment_created_at": "2025-07-25T20:03:06+00:00",
        "comment_author": "CISC",
        "comment_body": "Unlike the jinja chat template you should not add `BOS` here, it will get automatically added depending on the `add_bos_token` metadata.",
        "pr_file_module": null
      },
      {
        "comment_id": "2233106908",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14878,
        "pr_file": "src/llama-chat.cpp",
        "discussion_id": "2231917163",
        "commented_code": "@@ -698,11 +701,34 @@ int32_t llm_chat_apply_template(\n             if (role == \"system\") {\n                 ss << \"<|startoftext|>\" << message->content << \"<|extra_4|>\";\n             } else if (role == \"assistant\") {\n-                ss << \"<|startoftext|>\" << message->content << \"<|eos|>\";\n+                ss << message->content << \"<|eos|>\";\n             } else {\n                 ss << \"<|startoftext|>\" << message->content << \"<|extra_0|>\";\n             }\n         }\n+    } else if (tmpl == LLM_CHAT_TEMPLATE_HUNYUAN_DENSE) {\n+        // tencent/Hunyuan-4B\n+        for (size_t i = 0; i < chat.size(); i++) {\n+            std::string role(chat[i]->role);\n+            if (i == 0) {\n+                if (role == \"system\") {\n+                    ss << \"<\uff5chy_begin\u2581of\u2581sentence\uff5c>\" << chat[i]->content << \"<\uff5chy_place\u2581holder\u2581no\u25813\uff5c>\";\n+                } else {\n+                    ss << \"<\uff5chy_begin\u2581of\u2581sentence\uff5c>\";",
        "comment_created_at": "2025-07-26T17:12:15+00:00",
        "comment_author": "stevenkuang-tencent",
        "comment_body": "> Unlike the jinja chat template you should not add `BOS` here, it will get automatically added depending on the `add_bos_token` metadata.\r\n\r\nUpdated.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2227211862",
    "pr_number": 14842,
    "pr_file": "tools/imatrix/imatrix.cpp",
    "created_at": "2025-07-24T02:58:04+00:00",
    "commented_code": "void IMatrixCollector::save_imatrix(int32_t n_chunk) const {\n     auto fname = m_params.out_file;\n+    auto imat_type = m_params.imat_out_type;\n \n-    // TODO: use the new format in more cases\n-    if (!string_ends_with(fname, \".gguf\")) {\n-        LOG_WRN(\"\n%s: saving to legacy imatrix format because output suffix is not .gguf\n\", __func__);\n+    if ((imat_type == COMMON_IMATRIX_FORMAT_AUTO && string_ends_with(fname, \".dat\")) ||",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2227211862",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14842,
        "pr_file": "tools/imatrix/imatrix.cpp",
        "discussion_id": "2227211862",
        "commented_code": "@@ -492,13 +492,15 @@ void IMatrixCollector::save_imatrix_legacy(int32_t ncall) const {\n \n void IMatrixCollector::save_imatrix(int32_t n_chunk) const {\n     auto fname = m_params.out_file;\n+    auto imat_type = m_params.imat_out_type;\n \n-    // TODO: use the new format in more cases\n-    if (!string_ends_with(fname, \".gguf\")) {\n-        LOG_WRN(\"\\n%s: saving to legacy imatrix format because output suffix is not .gguf\\n\", __func__);\n+    if ((imat_type == COMMON_IMATRIX_FORMAT_AUTO && string_ends_with(fname, \".dat\")) ||",
        "comment_created_at": "2025-07-24T02:58:04+00:00",
        "comment_author": "compilade",
        "comment_body": "It might be better to instead simply use GGUF regardless of the file name by default.\r\n\r\nI don't know why I'm hesitating.\r\n\r\nGenerating new `imatrix.dat` has limited uses (however, reading has many uses). The main user who would benefit doesn't really use mainline `llama.cpp` for this anymore (see <https://github.com/ikawrakow/ik_llama.cpp/discussions/15#discussioncomment-13739971>).\r\n\r\nThis simplification could also remove the need for the `common_imatrix_format_type` enum, which could be a `bool` instead.\r\n\r\nEDIT: I've changed this in <https://github.com/ggml-org/llama.cpp/pull/14842/commits/1ef3cc1a87ec9344c7329d9c72c3eb991ceb70d7>, the format is no longer decided with the output filename.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2131909360",
    "pr_number": 14029,
    "pr_file": "src/llama-arch.cpp",
    "created_at": "2025-06-06T10:00:35+00:00",
    "commented_code": "{ LLM_KV_TOKENIZER_HF_JSON,              \"tokenizer.huggingface.json\"              },\n     { LLM_KV_TOKENIZER_RWKV,                 \"tokenizer.rwkv.world\"                    },\n     { LLM_KV_TOKENIZER_CHAT_TEMPLATE,        \"tokenizer.chat_template\"                 },\n-    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.%s\"              },\n+    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.\"                }, // FIXME: cannot add %s because it will be replaced by arch name",
    "repo_full_name": "ggml-org/llama.cpp",
    "discussion_comments": [
      {
        "comment_id": "2131909360",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14029,
        "pr_file": "src/llama-arch.cpp",
        "discussion_id": "2131909360",
        "commented_code": "@@ -200,7 +200,7 @@ static const std::map<llm_kv, const char *> LLM_KV_NAMES = {\n     { LLM_KV_TOKENIZER_HF_JSON,              \"tokenizer.huggingface.json\"              },\n     { LLM_KV_TOKENIZER_RWKV,                 \"tokenizer.rwkv.world\"                    },\n     { LLM_KV_TOKENIZER_CHAT_TEMPLATE,        \"tokenizer.chat_template\"                 },\n-    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.%s\"              },\n+    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.\"                }, // FIXME: cannot add %s because it will be replaced by arch name",
        "comment_created_at": "2025-06-06T10:00:35+00:00",
        "comment_author": "CISC",
        "comment_body": "Actually, you can use LLM_KV_TOKENIZER_CHAT_TEMPLATE with suffix:\r\nhttps://github.com/ggml-org/llama.cpp/blob/c02f53d9ce7832b3d81b8114ca7b966ab9d160c9/src/llama-arch.cpp#L1722-L1725",
        "pr_file_module": null
      },
      {
        "comment_id": "2131924505",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14029,
        "pr_file": "src/llama-arch.cpp",
        "discussion_id": "2131909360",
        "commented_code": "@@ -200,7 +200,7 @@ static const std::map<llm_kv, const char *> LLM_KV_NAMES = {\n     { LLM_KV_TOKENIZER_HF_JSON,              \"tokenizer.huggingface.json\"              },\n     { LLM_KV_TOKENIZER_RWKV,                 \"tokenizer.rwkv.world\"                    },\n     { LLM_KV_TOKENIZER_CHAT_TEMPLATE,        \"tokenizer.chat_template\"                 },\n-    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.%s\"              },\n+    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.\"                }, // FIXME: cannot add %s because it will be replaced by arch name",
        "comment_created_at": "2025-06-06T10:11:43+00:00",
        "comment_author": "ngxson",
        "comment_body": "doing this but it doesn't work, maybe it's buggy somewhere else:\r\n\r\n```cpp\r\n    const auto key = name\r\n        ? LLM_KV(model->arch, name)(LLM_KV_TOKENIZER_CHAT_TEMPLATE)\r\n        : LLM_KV(model->arch)(LLM_KV_TOKENIZER_CHAT_TEMPLATE);\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2132016493",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14029,
        "pr_file": "src/llama-arch.cpp",
        "discussion_id": "2131909360",
        "commented_code": "@@ -200,7 +200,7 @@ static const std::map<llm_kv, const char *> LLM_KV_NAMES = {\n     { LLM_KV_TOKENIZER_HF_JSON,              \"tokenizer.huggingface.json\"              },\n     { LLM_KV_TOKENIZER_RWKV,                 \"tokenizer.rwkv.world\"                    },\n     { LLM_KV_TOKENIZER_CHAT_TEMPLATE,        \"tokenizer.chat_template\"                 },\n-    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.%s\"              },\n+    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.\"                }, // FIXME: cannot add %s because it will be replaced by arch name",
        "comment_created_at": "2025-06-06T11:21:15+00:00",
        "comment_author": "CISC",
        "comment_body": "I was looking in the wrong place, this is where it's broken:\r\nhttps://github.com/ggml-org/llama.cpp/blob/e83ba3e460651b20a594e9f2f0f0bffb998d3ce1/src/llama-arch.cpp#L1709-L1712",
        "pr_file_module": null
      },
      {
        "comment_id": "2132059483",
        "repo_full_name": "ggml-org/llama.cpp",
        "pr_number": 14029,
        "pr_file": "src/llama-arch.cpp",
        "discussion_id": "2131909360",
        "commented_code": "@@ -200,7 +200,7 @@ static const std::map<llm_kv, const char *> LLM_KV_NAMES = {\n     { LLM_KV_TOKENIZER_HF_JSON,              \"tokenizer.huggingface.json\"              },\n     { LLM_KV_TOKENIZER_RWKV,                 \"tokenizer.rwkv.world\"                    },\n     { LLM_KV_TOKENIZER_CHAT_TEMPLATE,        \"tokenizer.chat_template\"                 },\n-    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.%s\"              },\n+    { LLM_KV_TOKENIZER_CHAT_TEMPLATE_N,      \"tokenizer.chat_template.\"                }, // FIXME: cannot add %s because it will be replaced by arch name",
        "comment_created_at": "2025-06-06T11:56:02+00:00",
        "comment_author": "CISC",
        "comment_body": "Fixed in #14050",
        "pr_file_module": null
      }
    ]
  }
]