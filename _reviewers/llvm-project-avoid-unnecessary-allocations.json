[
  {
    "discussion_id": "2230451840",
    "pr_number": 150543,
    "pr_file": "llvm/lib/Analysis/MemorySSAUpdater.cpp",
    "created_at": "2025-07-25T08:05:55+00:00",
    "commented_code": "removeMemoryAccess(Phi);\n   }\n \n-  // We should only end up recursing in case we replaced something, in which\n-  // case, we may have made other Phis trivial.\n-  return recursePhi(Same);\n+  // Continue traversal in a DFS worklist approach, in case we might find\n+  // other trivial Phis.\n+  if (!Same)\n+    return nullptr;\n+\n+  TrackingVH<MemoryAccess> Result(Same);",
    "repo_full_name": "llvm/llvm-project",
    "discussion_comments": [
      {
        "comment_id": "2230451840",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 150543,
        "pr_file": "llvm/lib/Analysis/MemorySSAUpdater.cpp",
        "discussion_id": "2230451840",
        "commented_code": "@@ -230,9 +223,61 @@ MemoryAccess *MemorySSAUpdater::tryRemoveTrivialPhi(MemoryPhi *Phi,\n     removeMemoryAccess(Phi);\n   }\n \n-  // We should only end up recursing in case we replaced something, in which\n-  // case, we may have made other Phis trivial.\n-  return recursePhi(Same);\n+  // Continue traversal in a DFS worklist approach, in case we might find\n+  // other trivial Phis.\n+  if (!Same)\n+    return nullptr;\n+\n+  TrackingVH<MemoryAccess> Result(Same);",
        "comment_created_at": "2025-07-25T08:05:55+00:00",
        "comment_author": "nikic",
        "comment_body": "I think the key issue in this code is not really that we can visit many users (which should be ok as they are only visited if an actual simplification occurs), but that we use TrackingVH for ... everything here, which is going to be very expensive.\r\n\r\nCan we avoid it by using a visited set so that we don't have to worry about revisiting a phi that has been erased in a different iteration?",
        "pr_file_module": null
      },
      {
        "comment_id": "2237583187",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 150543,
        "pr_file": "llvm/lib/Analysis/MemorySSAUpdater.cpp",
        "discussion_id": "2230451840",
        "commented_code": "@@ -230,9 +223,61 @@ MemoryAccess *MemorySSAUpdater::tryRemoveTrivialPhi(MemoryPhi *Phi,\n     removeMemoryAccess(Phi);\n   }\n \n-  // We should only end up recursing in case we replaced something, in which\n-  // case, we may have made other Phis trivial.\n-  return recursePhi(Same);\n+  // Continue traversal in a DFS worklist approach, in case we might find\n+  // other trivial Phis.\n+  if (!Same)\n+    return nullptr;\n+\n+  TrackingVH<MemoryAccess> Result(Same);",
        "comment_created_at": "2025-07-28T18:56:21+00:00",
        "comment_author": "alinas",
        "comment_body": "I fully agree with you on this one. The use of TrackingVH has been really baked into MSSA and its updater since the beginning and this is trying to deal with that (e.g.\r\n[MemorySSAUpdater.cpp:68](https://github.com/llvm/llvm-project/blob/main/llvm/lib/Analysis/MemorySSAUpdater.cpp#L68) passes in a VH, and removing phis [MemorySSAUpdater.cpp:1326](https://github.com/llvm/llvm-project/blob/main/llvm/lib/Analysis/MemorySSAUpdater.cpp#L1326) checks if a VH is passed in, to do RAUW). My initial attempts to use a visited list here and drop the VH resulted in asserting where a VH was expected (in [Value.cpp](https://github.com/llvm/llvm-project/blob/main/llvm/lib/IR/Value.cpp#L1171) ).\r\n\r\nI'm happy to keep trying to address this and I'm open to feedback on what I missed when triggering the assert. The updated profile with this change, for the original test that found this issue, does show too much time spent on creating VH here. But it is a step fwd since with this change I can cap the recursion and number of phis processed.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2214151082",
    "pr_number": 148948,
    "pr_file": "libc/test/integration/src/pthread/pthread_barrier_test.cpp",
    "created_at": "2025-07-17T19:42:26+00:00",
    "commented_code": "+//===-- Tests for pthread_barrier_t ---------------------------------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"src/pthread/pthread_barrier_destroy.h\"\n+#include \"src/pthread/pthread_barrier_init.h\"\n+#include \"src/pthread/pthread_barrier_wait.h\"\n+\n+#include \"src/__support/CPP/atomic.h\"\n+#include \"src/pthread/pthread_create.h\"\n+#include \"src/pthread/pthread_join.h\"\n+#include \"src/pthread/pthread_mutex_destroy.h\"\n+#include \"src/pthread/pthread_mutex_init.h\"\n+#include \"src/pthread/pthread_mutex_lock.h\"\n+#include \"src/pthread/pthread_mutex_unlock.h\"\n+#include \"src/stdio/printf.h\"\n+\n+#include \"test/IntegrationTest/test.h\"\n+\n+#include <pthread.h>\n+\n+pthread_barrier_t barrier;\n+\n+void smoke_test() {\n+  ASSERT_EQ(LIBC_NAMESPACE::pthread_barrier_init(&barrier, nullptr, 1), 0);\n+  ASSERT_EQ(LIBC_NAMESPACE::pthread_barrier_wait(&barrier),\n+            PTHREAD_BARRIER_SERIAL_THREAD);\n+  ASSERT_EQ(LIBC_NAMESPACE::pthread_barrier_destroy(&barrier), 0);\n+}\n+\n+LIBC_NAMESPACE::cpp::Atomic<int> counter;\n+void *increment_counter_and_wait(void *args) {\n+  counter.fetch_add(1);\n+  LIBC_NAMESPACE::pthread_barrier_wait(&barrier);\n+  return 0;\n+}\n+\n+void single_use_barrier() {\n+  counter.set(0);\n+  const int NUM_THREADS = 30;\n+  pthread_t threads[NUM_THREADS];\n+  ASSERT_EQ(\n+      LIBC_NAMESPACE::pthread_barrier_init(&barrier, nullptr, NUM_THREADS + 1),\n+      0);\n+\n+  for (int i = 0; i < NUM_THREADS; ++i)\n+    LIBC_NAMESPACE::pthread_create(&threads[i], nullptr,\n+                                   increment_counter_and_wait, nullptr);\n+\n+  LIBC_NAMESPACE::pthread_barrier_wait(&barrier);\n+  ASSERT_EQ(counter.load(), NUM_THREADS);\n+\n+  for (int i = 0; i < NUM_THREADS; ++i)\n+    LIBC_NAMESPACE::pthread_join(threads[i], nullptr);\n+\n+  LIBC_NAMESPACE::pthread_barrier_destroy(&barrier);\n+}\n+\n+void reusable_barrier() {\n+  counter.set(0);\n+  const int NUM_THREADS = 30;\n+  const int REPEAT = 20;\n+  pthread_t threads[NUM_THREADS * REPEAT];",
    "repo_full_name": "llvm/llvm-project",
    "discussion_comments": [
      {
        "comment_id": "2214151082",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 148948,
        "pr_file": "libc/test/integration/src/pthread/pthread_barrier_test.cpp",
        "discussion_id": "2214151082",
        "commented_code": "@@ -0,0 +1,117 @@\n+//===-- Tests for pthread_barrier_t ---------------------------------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"src/pthread/pthread_barrier_destroy.h\"\n+#include \"src/pthread/pthread_barrier_init.h\"\n+#include \"src/pthread/pthread_barrier_wait.h\"\n+\n+#include \"src/__support/CPP/atomic.h\"\n+#include \"src/pthread/pthread_create.h\"\n+#include \"src/pthread/pthread_join.h\"\n+#include \"src/pthread/pthread_mutex_destroy.h\"\n+#include \"src/pthread/pthread_mutex_init.h\"\n+#include \"src/pthread/pthread_mutex_lock.h\"\n+#include \"src/pthread/pthread_mutex_unlock.h\"\n+#include \"src/stdio/printf.h\"\n+\n+#include \"test/IntegrationTest/test.h\"\n+\n+#include <pthread.h>\n+\n+pthread_barrier_t barrier;\n+\n+void smoke_test() {\n+  ASSERT_EQ(LIBC_NAMESPACE::pthread_barrier_init(&barrier, nullptr, 1), 0);\n+  ASSERT_EQ(LIBC_NAMESPACE::pthread_barrier_wait(&barrier),\n+            PTHREAD_BARRIER_SERIAL_THREAD);\n+  ASSERT_EQ(LIBC_NAMESPACE::pthread_barrier_destroy(&barrier), 0);\n+}\n+\n+LIBC_NAMESPACE::cpp::Atomic<int> counter;\n+void *increment_counter_and_wait(void *args) {\n+  counter.fetch_add(1);\n+  LIBC_NAMESPACE::pthread_barrier_wait(&barrier);\n+  return 0;\n+}\n+\n+void single_use_barrier() {\n+  counter.set(0);\n+  const int NUM_THREADS = 30;\n+  pthread_t threads[NUM_THREADS];\n+  ASSERT_EQ(\n+      LIBC_NAMESPACE::pthread_barrier_init(&barrier, nullptr, NUM_THREADS + 1),\n+      0);\n+\n+  for (int i = 0; i < NUM_THREADS; ++i)\n+    LIBC_NAMESPACE::pthread_create(&threads[i], nullptr,\n+                                   increment_counter_and_wait, nullptr);\n+\n+  LIBC_NAMESPACE::pthread_barrier_wait(&barrier);\n+  ASSERT_EQ(counter.load(), NUM_THREADS);\n+\n+  for (int i = 0; i < NUM_THREADS; ++i)\n+    LIBC_NAMESPACE::pthread_join(threads[i], nullptr);\n+\n+  LIBC_NAMESPACE::pthread_barrier_destroy(&barrier);\n+}\n+\n+void reusable_barrier() {\n+  counter.set(0);\n+  const int NUM_THREADS = 30;\n+  const int REPEAT = 20;\n+  pthread_t threads[NUM_THREADS * REPEAT];",
        "comment_created_at": "2025-07-17T19:42:26+00:00",
        "comment_author": "brooksmoses",
        "comment_body": "Not that it matters, but it seems a little wasteful to create a different set of threads for each repeat, and to create all of them before joining any of them.  Why not just join the threads inside the outer loop, and reuse the `pthread_t` array entries?\r\n\r\nAlso, if you make `REPEAT` a parameter of the function, this becomes the same as the previous function when `REPEAT` is 1, so you can avoid duplication.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2231565772",
    "pr_number": 150572,
    "pr_file": "mlir/lib/Dialect/EmitC/Transforms/AddReflectionMap.cpp",
    "created_at": "2025-07-25T16:30:11+00:00",
    "commented_code": "+//===- AddReflectionMap.cpp - Add a reflection map to a class -------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+#include \"mlir/Dialect/EmitC/IR/EmitC.h\"\n+#include \"mlir/Dialect/EmitC/Transforms/Passes.h\"\n+#include \"mlir/Dialect/EmitC/Transforms/Transforms.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Transforms/WalkPatternRewriteDriver.h\"\n+\n+using namespace mlir;\n+using namespace emitc;\n+\n+namespace mlir {\n+namespace emitc {\n+#define GEN_PASS_DEF_ADDREFLECTIONMAPPASS\n+#include \"mlir/Dialect/EmitC/Transforms/Passes.h.inc\"\n+\n+namespace {\n+constexpr const char *kMapLibraryHeader = \"map\";\n+constexpr const char *kStringLibraryHeader = \"string\";\n+class AddReflectionMapPass\n+    : public impl::AddReflectionMapPassBase<AddReflectionMapPass> {\n+  using AddReflectionMapPassBase::AddReflectionMapPassBase;\n+  void runOnOperation() override {\n+    mlir::ModuleOp module = getOperation();\n+\n+    RewritePatternSet patterns(&getContext());\n+    populateAddReflectionMapPatterns(patterns, namedAttribute);\n+\n+    walkAndApplyPatterns(module, std::move(patterns));\n+    bool hasMap = false;\n+    bool hasString = false;\n+    for (auto &op : *module.getBody()) {\n+      emitc::IncludeOp includeOp = llvm::dyn_cast<mlir::emitc::IncludeOp>(op);\n+      if (!includeOp)\n+        continue;\n+      if (includeOp.getIsStandardInclude()) {\n+        if (includeOp.getInclude() == kMapLibraryHeader)\n+          hasMap = true;\n+        if (includeOp.getInclude() == kStringLibraryHeader)\n+          hasString = true;\n+      }\n+    }\n+\n+    if (hasMap && hasString)\n+      return;\n+\n+    mlir::OpBuilder builder(module.getBody(), module.getBody()->begin());\n+    if (!hasMap) {\n+      StringAttr includeAttr = builder.getStringAttr(kMapLibraryHeader);\n+      builder.create<mlir::emitc::IncludeOp>(\n+          module.getLoc(), includeAttr,\n+          /*is_standard_include=*/builder.getUnitAttr());\n+    }\n+    if (!hasString) {\n+      StringAttr includeAttr = builder.getStringAttr(kStringLibraryHeader);\n+      builder.create<emitc::IncludeOp>(\n+          module.getLoc(), includeAttr,\n+          /*is_standard_include=*/builder.getUnitAttr());\n+    }\n+  }\n+};\n+\n+} // namespace\n+} // namespace emitc\n+} // namespace mlir\n+\n+class AddReflectionMapClass : public OpRewritePattern<emitc::ClassOp> {\n+public:\n+  AddReflectionMapClass(MLIRContext *context, StringRef attrName)\n+      : OpRewritePattern<emitc::ClassOp>(context), attributeName(attrName) {}\n+\n+  LogicalResult matchAndRewrite(mlir::emitc::ClassOp classOp,\n+                                PatternRewriter &rewriter) const override {\n+    mlir::MLIRContext *context = rewriter.getContext();\n+    emitc::OpaqueType stringViewType =\n+        mlir::emitc::OpaqueType::get(rewriter.getContext(), \"std::string_view\");\n+    emitc::OpaqueType charType =\n+        mlir::emitc::OpaqueType::get(rewriter.getContext(), \"char\");\n+    emitc::OpaqueType mapType = mlir::emitc::OpaqueType::get(\n+        rewriter.getContext(), \"const std::map<std::string, char*>\");\n+\n+    FunctionType funcType =\n+        rewriter.getFunctionType({stringViewType}, {charType});\n+    emitc::FuncOp executeFunc =\n+        classOp.lookupSymbol<mlir::emitc::FuncOp>(\"execute\");\n+    if (executeFunc)\n+      rewriter.setInsertionPoint(executeFunc);\n+    else\n+      classOp.emitError() << \"ClassOp must contain a function named 'execute' \"\n+                             \"to add reflection map\";\n+\n+    emitc::FuncOp getBufferFunc = rewriter.create<mlir::emitc::FuncOp>(\n+        classOp.getLoc(), \"getBufferForName\", funcType);\n+\n+    Block *funcBody = getBufferFunc.addEntryBlock();\n+    rewriter.setInsertionPointToStart(funcBody);\n+\n+    // Collect all field names\n+    std::vector<std::pair<std::string, std::string>> fieldNames;\n+    classOp.walk([&](mlir::emitc::FieldOp fieldOp) {\n+      if (mlir::Attribute attrsAttr =\n+              fieldOp->getAttrDictionary().get(\"attrs\")) {\n+        if (DictionaryAttr innerDictAttr =\n+                dyn_cast<mlir::DictionaryAttr>(attrsAttr)) {\n+          ArrayAttr arrayAttr = dyn_cast<mlir::ArrayAttr>(\n+              innerDictAttr.getNamed(attributeName)->getValue());\n+          if (!arrayAttr.empty()) {\n+            StringAttr stringAttr = dyn_cast<mlir::StringAttr>(arrayAttr[0]);\n+            std::string indexPath = stringAttr.getValue().str();\n+            fieldNames.emplace_back(indexPath, fieldOp.getName().str());",
    "repo_full_name": "llvm/llvm-project",
    "discussion_comments": [
      {
        "comment_id": "2231565772",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 150572,
        "pr_file": "mlir/lib/Dialect/EmitC/Transforms/AddReflectionMap.cpp",
        "discussion_id": "2231565772",
        "commented_code": "@@ -0,0 +1,180 @@\n+//===- AddReflectionMap.cpp - Add a reflection map to a class -------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+#include \"mlir/Dialect/EmitC/IR/EmitC.h\"\n+#include \"mlir/Dialect/EmitC/Transforms/Passes.h\"\n+#include \"mlir/Dialect/EmitC/Transforms/Transforms.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Transforms/WalkPatternRewriteDriver.h\"\n+\n+using namespace mlir;\n+using namespace emitc;\n+\n+namespace mlir {\n+namespace emitc {\n+#define GEN_PASS_DEF_ADDREFLECTIONMAPPASS\n+#include \"mlir/Dialect/EmitC/Transforms/Passes.h.inc\"\n+\n+namespace {\n+constexpr const char *kMapLibraryHeader = \"map\";\n+constexpr const char *kStringLibraryHeader = \"string\";\n+class AddReflectionMapPass\n+    : public impl::AddReflectionMapPassBase<AddReflectionMapPass> {\n+  using AddReflectionMapPassBase::AddReflectionMapPassBase;\n+  void runOnOperation() override {\n+    mlir::ModuleOp module = getOperation();\n+\n+    RewritePatternSet patterns(&getContext());\n+    populateAddReflectionMapPatterns(patterns, namedAttribute);\n+\n+    walkAndApplyPatterns(module, std::move(patterns));\n+    bool hasMap = false;\n+    bool hasString = false;\n+    for (auto &op : *module.getBody()) {\n+      emitc::IncludeOp includeOp = llvm::dyn_cast<mlir::emitc::IncludeOp>(op);\n+      if (!includeOp)\n+        continue;\n+      if (includeOp.getIsStandardInclude()) {\n+        if (includeOp.getInclude() == kMapLibraryHeader)\n+          hasMap = true;\n+        if (includeOp.getInclude() == kStringLibraryHeader)\n+          hasString = true;\n+      }\n+    }\n+\n+    if (hasMap && hasString)\n+      return;\n+\n+    mlir::OpBuilder builder(module.getBody(), module.getBody()->begin());\n+    if (!hasMap) {\n+      StringAttr includeAttr = builder.getStringAttr(kMapLibraryHeader);\n+      builder.create<mlir::emitc::IncludeOp>(\n+          module.getLoc(), includeAttr,\n+          /*is_standard_include=*/builder.getUnitAttr());\n+    }\n+    if (!hasString) {\n+      StringAttr includeAttr = builder.getStringAttr(kStringLibraryHeader);\n+      builder.create<emitc::IncludeOp>(\n+          module.getLoc(), includeAttr,\n+          /*is_standard_include=*/builder.getUnitAttr());\n+    }\n+  }\n+};\n+\n+} // namespace\n+} // namespace emitc\n+} // namespace mlir\n+\n+class AddReflectionMapClass : public OpRewritePattern<emitc::ClassOp> {\n+public:\n+  AddReflectionMapClass(MLIRContext *context, StringRef attrName)\n+      : OpRewritePattern<emitc::ClassOp>(context), attributeName(attrName) {}\n+\n+  LogicalResult matchAndRewrite(mlir::emitc::ClassOp classOp,\n+                                PatternRewriter &rewriter) const override {\n+    mlir::MLIRContext *context = rewriter.getContext();\n+    emitc::OpaqueType stringViewType =\n+        mlir::emitc::OpaqueType::get(rewriter.getContext(), \"std::string_view\");\n+    emitc::OpaqueType charType =\n+        mlir::emitc::OpaqueType::get(rewriter.getContext(), \"char\");\n+    emitc::OpaqueType mapType = mlir::emitc::OpaqueType::get(\n+        rewriter.getContext(), \"const std::map<std::string, char*>\");\n+\n+    FunctionType funcType =\n+        rewriter.getFunctionType({stringViewType}, {charType});\n+    emitc::FuncOp executeFunc =\n+        classOp.lookupSymbol<mlir::emitc::FuncOp>(\"execute\");\n+    if (executeFunc)\n+      rewriter.setInsertionPoint(executeFunc);\n+    else\n+      classOp.emitError() << \"ClassOp must contain a function named 'execute' \"\n+                             \"to add reflection map\";\n+\n+    emitc::FuncOp getBufferFunc = rewriter.create<mlir::emitc::FuncOp>(\n+        classOp.getLoc(), \"getBufferForName\", funcType);\n+\n+    Block *funcBody = getBufferFunc.addEntryBlock();\n+    rewriter.setInsertionPointToStart(funcBody);\n+\n+    // Collect all field names\n+    std::vector<std::pair<std::string, std::string>> fieldNames;\n+    classOp.walk([&](mlir::emitc::FieldOp fieldOp) {\n+      if (mlir::Attribute attrsAttr =\n+              fieldOp->getAttrDictionary().get(\"attrs\")) {\n+        if (DictionaryAttr innerDictAttr =\n+                dyn_cast<mlir::DictionaryAttr>(attrsAttr)) {\n+          ArrayAttr arrayAttr = dyn_cast<mlir::ArrayAttr>(\n+              innerDictAttr.getNamed(attributeName)->getValue());\n+          if (!arrayAttr.empty()) {\n+            StringAttr stringAttr = dyn_cast<mlir::StringAttr>(arrayAttr[0]);\n+            std::string indexPath = stringAttr.getValue().str();\n+            fieldNames.emplace_back(indexPath, fieldOp.getName().str());",
        "comment_created_at": "2025-07-25T16:30:11+00:00",
        "comment_author": "ilovepi",
        "comment_body": "```suggestion\r\n            fieldNames.emplace_back(tringAttr.getValue().str(), fieldOp.getName().str());\r\n```\r\nYou can avoid a copy here, otherwise you can probably use `std::move()` to similar effect.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2231575840",
    "pr_number": 150572,
    "pr_file": "mlir/lib/Dialect/EmitC/Transforms/AddReflectionMap.cpp",
    "created_at": "2025-07-25T16:36:05+00:00",
    "commented_code": "+//===- AddReflectionMap.cpp - Add a reflection map to a class -------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+#include \"mlir/Dialect/EmitC/IR/EmitC.h\"\n+#include \"mlir/Dialect/EmitC/Transforms/Passes.h\"\n+#include \"mlir/Dialect/EmitC/Transforms/Transforms.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Transforms/WalkPatternRewriteDriver.h\"\n+\n+using namespace mlir;\n+using namespace emitc;\n+\n+namespace mlir {\n+namespace emitc {\n+#define GEN_PASS_DEF_ADDREFLECTIONMAPPASS\n+#include \"mlir/Dialect/EmitC/Transforms/Passes.h.inc\"\n+\n+namespace {\n+constexpr const char *kMapLibraryHeader = \"map\";\n+constexpr const char *kStringLibraryHeader = \"string\";\n+class AddReflectionMapPass\n+    : public impl::AddReflectionMapPassBase<AddReflectionMapPass> {\n+  using AddReflectionMapPassBase::AddReflectionMapPassBase;\n+  void runOnOperation() override {\n+    mlir::ModuleOp module = getOperation();\n+\n+    RewritePatternSet patterns(&getContext());\n+    populateAddReflectionMapPatterns(patterns, namedAttribute);\n+\n+    walkAndApplyPatterns(module, std::move(patterns));\n+    bool hasMap = false;\n+    bool hasString = false;\n+    for (auto &op : *module.getBody()) {\n+      emitc::IncludeOp includeOp = llvm::dyn_cast<mlir::emitc::IncludeOp>(op);\n+      if (!includeOp)\n+        continue;\n+      if (includeOp.getIsStandardInclude()) {\n+        if (includeOp.getInclude() == kMapLibraryHeader)\n+          hasMap = true;\n+        if (includeOp.getInclude() == kStringLibraryHeader)\n+          hasString = true;\n+      }\n+    }\n+\n+    if (hasMap && hasString)\n+      return;\n+\n+    mlir::OpBuilder builder(module.getBody(), module.getBody()->begin());\n+    if (!hasMap) {\n+      StringAttr includeAttr = builder.getStringAttr(kMapLibraryHeader);\n+      builder.create<mlir::emitc::IncludeOp>(\n+          module.getLoc(), includeAttr,\n+          /*is_standard_include=*/builder.getUnitAttr());\n+    }\n+    if (!hasString) {\n+      StringAttr includeAttr = builder.getStringAttr(kStringLibraryHeader);\n+      builder.create<emitc::IncludeOp>(\n+          module.getLoc(), includeAttr,\n+          /*is_standard_include=*/builder.getUnitAttr());\n+    }\n+  }\n+};\n+\n+} // namespace\n+} // namespace emitc\n+} // namespace mlir\n+\n+class AddReflectionMapClass : public OpRewritePattern<emitc::ClassOp> {\n+public:\n+  AddReflectionMapClass(MLIRContext *context, StringRef attrName)\n+      : OpRewritePattern<emitc::ClassOp>(context), attributeName(attrName) {}\n+\n+  LogicalResult matchAndRewrite(mlir::emitc::ClassOp classOp,\n+                                PatternRewriter &rewriter) const override {\n+    mlir::MLIRContext *context = rewriter.getContext();\n+    emitc::OpaqueType stringViewType =\n+        mlir::emitc::OpaqueType::get(rewriter.getContext(), \"std::string_view\");\n+    emitc::OpaqueType charType =\n+        mlir::emitc::OpaqueType::get(rewriter.getContext(), \"char\");\n+    emitc::OpaqueType mapType = mlir::emitc::OpaqueType::get(\n+        rewriter.getContext(), \"const std::map<std::string, char*>\");\n+\n+    FunctionType funcType =\n+        rewriter.getFunctionType({stringViewType}, {charType});\n+    emitc::FuncOp executeFunc =\n+        classOp.lookupSymbol<mlir::emitc::FuncOp>(\"execute\");\n+    if (executeFunc)\n+      rewriter.setInsertionPoint(executeFunc);\n+    else\n+      classOp.emitError() << \"ClassOp must contain a function named 'execute' \"\n+                             \"to add reflection map\";\n+\n+    emitc::FuncOp getBufferFunc = rewriter.create<mlir::emitc::FuncOp>(\n+        classOp.getLoc(), \"getBufferForName\", funcType);\n+\n+    Block *funcBody = getBufferFunc.addEntryBlock();\n+    rewriter.setInsertionPointToStart(funcBody);\n+\n+    // Collect all field names\n+    std::vector<std::pair<std::string, std::string>> fieldNames;\n+    classOp.walk([&](mlir::emitc::FieldOp fieldOp) {\n+      if (mlir::Attribute attrsAttr =\n+              fieldOp->getAttrDictionary().get(\"attrs\")) {\n+        if (DictionaryAttr innerDictAttr =\n+                dyn_cast<mlir::DictionaryAttr>(attrsAttr)) {\n+          ArrayAttr arrayAttr = dyn_cast<mlir::ArrayAttr>(\n+              innerDictAttr.getNamed(attributeName)->getValue());\n+          if (!arrayAttr.empty()) {\n+            StringAttr stringAttr = dyn_cast<mlir::StringAttr>(arrayAttr[0]);\n+            std::string indexPath = stringAttr.getValue().str();\n+            fieldNames.emplace_back(indexPath, fieldOp.getName().str());\n+          }\n+          if (arrayAttr.size() > 1) {\n+            fieldOp.emitError() << attributeName\n+                                << \" attribute must \"\n+                                   \"contain at most one value, but found \"\n+                                << arrayAttr.size() << \" values.\";\n+            return;\n+          }\n+        }\n+      }\n+    });\n+\n+    std::string mapInitializer = \"{ \";\n+    for (size_t i = 0; i < fieldNames.size(); ++i) {\n+      mapInitializer += \" { \\\"\" + fieldNames[i].first + \"\\\", \" +\n+                        \"reinterpret_cast<char*>(&\" + fieldNames[i].second +\n+                        \")\",\n+          mapInitializer += \" }\";\n+      if (i < fieldNames.size() - 1)\n+        mapInitializer += \", \";\n+    }\n+    mapInitializer += \" }\";",
    "repo_full_name": "llvm/llvm-project",
    "discussion_comments": [
      {
        "comment_id": "2231575840",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 150572,
        "pr_file": "mlir/lib/Dialect/EmitC/Transforms/AddReflectionMap.cpp",
        "discussion_id": "2231575840",
        "commented_code": "@@ -0,0 +1,180 @@\n+//===- AddReflectionMap.cpp - Add a reflection map to a class -------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+#include \"mlir/Dialect/EmitC/IR/EmitC.h\"\n+#include \"mlir/Dialect/EmitC/Transforms/Passes.h\"\n+#include \"mlir/Dialect/EmitC/Transforms/Transforms.h\"\n+#include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Transforms/WalkPatternRewriteDriver.h\"\n+\n+using namespace mlir;\n+using namespace emitc;\n+\n+namespace mlir {\n+namespace emitc {\n+#define GEN_PASS_DEF_ADDREFLECTIONMAPPASS\n+#include \"mlir/Dialect/EmitC/Transforms/Passes.h.inc\"\n+\n+namespace {\n+constexpr const char *kMapLibraryHeader = \"map\";\n+constexpr const char *kStringLibraryHeader = \"string\";\n+class AddReflectionMapPass\n+    : public impl::AddReflectionMapPassBase<AddReflectionMapPass> {\n+  using AddReflectionMapPassBase::AddReflectionMapPassBase;\n+  void runOnOperation() override {\n+    mlir::ModuleOp module = getOperation();\n+\n+    RewritePatternSet patterns(&getContext());\n+    populateAddReflectionMapPatterns(patterns, namedAttribute);\n+\n+    walkAndApplyPatterns(module, std::move(patterns));\n+    bool hasMap = false;\n+    bool hasString = false;\n+    for (auto &op : *module.getBody()) {\n+      emitc::IncludeOp includeOp = llvm::dyn_cast<mlir::emitc::IncludeOp>(op);\n+      if (!includeOp)\n+        continue;\n+      if (includeOp.getIsStandardInclude()) {\n+        if (includeOp.getInclude() == kMapLibraryHeader)\n+          hasMap = true;\n+        if (includeOp.getInclude() == kStringLibraryHeader)\n+          hasString = true;\n+      }\n+    }\n+\n+    if (hasMap && hasString)\n+      return;\n+\n+    mlir::OpBuilder builder(module.getBody(), module.getBody()->begin());\n+    if (!hasMap) {\n+      StringAttr includeAttr = builder.getStringAttr(kMapLibraryHeader);\n+      builder.create<mlir::emitc::IncludeOp>(\n+          module.getLoc(), includeAttr,\n+          /*is_standard_include=*/builder.getUnitAttr());\n+    }\n+    if (!hasString) {\n+      StringAttr includeAttr = builder.getStringAttr(kStringLibraryHeader);\n+      builder.create<emitc::IncludeOp>(\n+          module.getLoc(), includeAttr,\n+          /*is_standard_include=*/builder.getUnitAttr());\n+    }\n+  }\n+};\n+\n+} // namespace\n+} // namespace emitc\n+} // namespace mlir\n+\n+class AddReflectionMapClass : public OpRewritePattern<emitc::ClassOp> {\n+public:\n+  AddReflectionMapClass(MLIRContext *context, StringRef attrName)\n+      : OpRewritePattern<emitc::ClassOp>(context), attributeName(attrName) {}\n+\n+  LogicalResult matchAndRewrite(mlir::emitc::ClassOp classOp,\n+                                PatternRewriter &rewriter) const override {\n+    mlir::MLIRContext *context = rewriter.getContext();\n+    emitc::OpaqueType stringViewType =\n+        mlir::emitc::OpaqueType::get(rewriter.getContext(), \"std::string_view\");\n+    emitc::OpaqueType charType =\n+        mlir::emitc::OpaqueType::get(rewriter.getContext(), \"char\");\n+    emitc::OpaqueType mapType = mlir::emitc::OpaqueType::get(\n+        rewriter.getContext(), \"const std::map<std::string, char*>\");\n+\n+    FunctionType funcType =\n+        rewriter.getFunctionType({stringViewType}, {charType});\n+    emitc::FuncOp executeFunc =\n+        classOp.lookupSymbol<mlir::emitc::FuncOp>(\"execute\");\n+    if (executeFunc)\n+      rewriter.setInsertionPoint(executeFunc);\n+    else\n+      classOp.emitError() << \"ClassOp must contain a function named 'execute' \"\n+                             \"to add reflection map\";\n+\n+    emitc::FuncOp getBufferFunc = rewriter.create<mlir::emitc::FuncOp>(\n+        classOp.getLoc(), \"getBufferForName\", funcType);\n+\n+    Block *funcBody = getBufferFunc.addEntryBlock();\n+    rewriter.setInsertionPointToStart(funcBody);\n+\n+    // Collect all field names\n+    std::vector<std::pair<std::string, std::string>> fieldNames;\n+    classOp.walk([&](mlir::emitc::FieldOp fieldOp) {\n+      if (mlir::Attribute attrsAttr =\n+              fieldOp->getAttrDictionary().get(\"attrs\")) {\n+        if (DictionaryAttr innerDictAttr =\n+                dyn_cast<mlir::DictionaryAttr>(attrsAttr)) {\n+          ArrayAttr arrayAttr = dyn_cast<mlir::ArrayAttr>(\n+              innerDictAttr.getNamed(attributeName)->getValue());\n+          if (!arrayAttr.empty()) {\n+            StringAttr stringAttr = dyn_cast<mlir::StringAttr>(arrayAttr[0]);\n+            std::string indexPath = stringAttr.getValue().str();\n+            fieldNames.emplace_back(indexPath, fieldOp.getName().str());\n+          }\n+          if (arrayAttr.size() > 1) {\n+            fieldOp.emitError() << attributeName\n+                                << \" attribute must \"\n+                                   \"contain at most one value, but found \"\n+                                << arrayAttr.size() << \" values.\";\n+            return;\n+          }\n+        }\n+      }\n+    });\n+\n+    std::string mapInitializer = \"{ \";\n+    for (size_t i = 0; i < fieldNames.size(); ++i) {\n+      mapInitializer += \" { \\\"\" + fieldNames[i].first + \"\\\", \" +\n+                        \"reinterpret_cast<char*>(&\" + fieldNames[i].second +\n+                        \")\",\n+          mapInitializer += \" }\";\n+      if (i < fieldNames.size() - 1)\n+        mapInitializer += \", \";\n+    }\n+    mapInitializer += \" }\";",
        "comment_created_at": "2025-07-25T16:36:05+00:00",
        "comment_author": "ilovepi",
        "comment_body": "I'd recommend doing this w/ a string_stream. You can avoid a lot of copies and you can use things like `formatv()` to make the code easier to read.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2237127816",
    "pr_number": 149619,
    "pr_file": "llvm/lib/Analysis/ConstantFolding.cpp",
    "created_at": "2025-07-28T16:13:04+00:00",
    "commented_code": "}\n     return ConstantVector::get(Result);\n   }\n+  case Intrinsic::wasm_dot: {\n+    unsigned NumElements =\n+        cast<FixedVectorType>(Operands[0]->getType())->getNumElements();\n+\n+    assert(NumElements == 8 && Result.size() == 4 &&\n+           \"wasm dot takes i16x8 and produces i32x4\");\n+    assert(Ty->isIntegerTy());\n+    SmallVector<APInt, 8> MulVector;",
    "repo_full_name": "llvm/llvm-project",
    "discussion_comments": [
      {
        "comment_id": "2237127816",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 149619,
        "pr_file": "llvm/lib/Analysis/ConstantFolding.cpp",
        "discussion_id": "2237127816",
        "commented_code": "@@ -3826,6 +3827,37 @@ static Constant *ConstantFoldFixedVectorCall(\n     }\n     return ConstantVector::get(Result);\n   }\n+  case Intrinsic::wasm_dot: {\n+    unsigned NumElements =\n+        cast<FixedVectorType>(Operands[0]->getType())->getNumElements();\n+\n+    assert(NumElements == 8 && Result.size() == 4 &&\n+           \"wasm dot takes i16x8 and produces i32x4\");\n+    assert(Ty->isIntegerTy());\n+    SmallVector<APInt, 8> MulVector;",
        "comment_created_at": "2025-07-28T16:13:04+00:00",
        "comment_author": "lukel97",
        "comment_body": "Nit, if we don't need the arbitrary precisioness of APInt or the dynamicness of a vector, we can just use a plain old int32_t array:\n\n\n```suggestion\n    int32_t MulVector[8];\n```\n\nAnd then you can use `ConstantInt::getSExtValue` to get the values out. It should be a little bit faster than using an APInt.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192509521",
    "pr_number": 145382,
    "pr_file": "lldb/source/Plugins/Platform/Android/AdbClient.cpp",
    "created_at": "2025-07-08T13:18:28+00:00",
    "commented_code": "return m_conn && m_conn->IsConnected();\n }\n \n-AdbClient::SyncService::SyncService(std::unique_ptr<Connection> &&conn)\n-    : m_conn(std::move(conn)) {}\n+AdbClient::SyncService::SyncService(std::unique_ptr<Connection> conn, const std::string &device_id)\n+    : m_conn(std::move(conn)), m_device_id(device_id) {}",
    "repo_full_name": "llvm/llvm-project",
    "discussion_comments": [
      {
        "comment_id": "2192509521",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 145382,
        "pr_file": "lldb/source/Plugins/Platform/Android/AdbClient.cpp",
        "discussion_id": "2192509521",
        "commented_code": "@@ -580,17 +564,23 @@ bool AdbClient::SyncService::IsConnected() const {\n   return m_conn && m_conn->IsConnected();\n }\n \n-AdbClient::SyncService::SyncService(std::unique_ptr<Connection> &&conn)\n-    : m_conn(std::move(conn)) {}\n+AdbClient::SyncService::SyncService(std::unique_ptr<Connection> conn, const std::string &device_id)\n+    : m_conn(std::move(conn)), m_device_id(device_id) {}",
        "comment_created_at": "2025-07-08T13:18:28+00:00",
        "comment_author": "labath",
        "comment_body": "Use StringRef or move the string.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2223926874",
    "pr_number": 125556,
    "pr_file": "clang/lib/Driver/Driver.cpp",
    "created_at": "2025-07-22T22:13:37+00:00",
    "commented_code": "return true;\n }\n \n-void Driver::CreateOffloadingDeviceToolChains(Compilation &C,\n-                                              InputList &Inputs) {\n-\n-  //\n-  // CUDA/HIP\n-  //\n-  // We need to generate a CUDA/HIP toolchain if any of the inputs has a CUDA\n-  // or HIP type. However, mixed CUDA/HIP compilation is not supported.\n-  bool IsCuda =\n-      llvm::any_of(Inputs, [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-        return types::isCuda(I.first);\n-      });\n-  bool IsHIP =\n-      llvm::any_of(Inputs,\n-                   [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-                     return types::isHIP(I.first);\n-                   }) ||\n-      C.getInputArgs().hasArg(options::OPT_hip_link) ||\n-      C.getInputArgs().hasArg(options::OPT_hipstdpar);\n-  bool UseLLVMOffload = C.getInputArgs().hasArg(\n-      options::OPT_foffload_via_llvm, options::OPT_fno_offload_via_llvm, false);\n-  if (IsCuda && IsHIP) {\n-    Diag(clang::diag::err_drv_mix_cuda_hip);\n-    return;\n+// Handles `native` offload architectures by using the 'offload-arch' utility.\n+static llvm::SmallVector<std::string>\n+getSystemOffloadArchs(Compilation &C, Action::OffloadKind Kind) {\n+  StringRef Program = C.getArgs().getLastArgValue(\n+      options::OPT_offload_arch_tool_EQ, \"offload-arch\");\n+\n+  SmallVector<std::string, 1> GPUArchs;\n+  if (llvm::ErrorOr<std::string> Executable =\n+          llvm::sys::findProgramByName(Program)) {\n+    llvm::SmallVector<StringRef> Args{*Executable};\n+    if (Kind == Action::OFK_HIP)\n+      Args.push_back(\"--only=amdgpu\");\n+    else if (Kind == Action::OFK_Cuda)\n+      Args.push_back(\"--only=nvptx\");\n+    auto StdoutOrErr = C.getDriver().executeProgram(Args);\n+\n+    if (!StdoutOrErr) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << StdoutOrErr.takeError()\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    } else if ((*StdoutOrErr)->getBuffer().empty()) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << \"No GPU detected in the system\"\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    }\n+\n+    for (StringRef Arch : llvm::split((*StdoutOrErr)->getBuffer(), \"\n\"))\n+      if (!Arch.empty())\n+        GPUArchs.push_back(Arch.str());\n+  } else {\n+    C.getDriver().Diag(diag::err_drv_command_failure) << \"offload-arch\";\n   }\n-  if (IsCuda && !UseLLVMOffload) {\n-    auto CudaTriple = getNVIDIAOffloadTargetTriple(\n-        *this, C.getInputArgs(), C.getDefaultToolChain().getTriple());\n-    if (!CudaTriple)\n-      return;\n+  return GPUArchs;\n+}\n \n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_Cuda, *CudaTriple,\n-                            C.getDefaultToolChain().getTriple());\n-\n-    // Emit a warning if the detected CUDA version is too new.\n-    const CudaInstallationDetector &CudaInstallation =\n-        static_cast<const toolchains::CudaToolChain &>(TC).CudaInstallation;\n-    if (CudaInstallation.isValid())\n-      CudaInstallation.WarnIfUnsupportedVersion();\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_Cuda);\n-    OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_Cuda, &TC,\n-                                        /*SpecificToolchain=*/true);\n-  } else if (IsHIP && !UseLLVMOffload) {\n-    if (auto *OMPTargetArg =\n-            C.getInputArgs().getLastArg(options::OPT_offload_targets_EQ)) {\n-      Diag(clang::diag::err_drv_unsupported_opt_for_language_mode)\n-          << OMPTargetArg->getSpelling() << \"HIP\";\n-      return;\n+// Attempts to infer the correct offloading toolchain triple by looking at the\n+// requested offloading kind and architectures.\n+static llvm::DenseSet<llvm::StringRef>\n+inferOffloadToolchains(Compilation &C, Action::OffloadKind Kind) {\n+  std::set<std::string> Archs;\n+  for (Arg *A : C.getInputArgs()) {\n+    for (StringRef Arch : A->getValues()) {\n+      if (A->getOption().matches(options::OPT_offload_arch_EQ)) {\n+        if (Arch == \"native\") {\n+          for (StringRef Str : getSystemOffloadArchs(C, Kind))\n+            Archs.insert(Str.str());\n+        } else {\n+          Archs.insert(Arch.str());\n+        }\n+      } else if (A->getOption().matches(options::OPT_no_offload_arch_EQ)) {\n+        if (Arch == \"all\")\n+          Archs.clear();\n+        else\n+          Archs.erase(Arch.str());\n+      }\n     }\n+  }\n \n-    auto HIPTriple = getHIPOffloadTargetTriple(*this, C.getInputArgs());\n-    if (!HIPTriple)\n-      return;\n-\n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_HIP, *HIPTriple,\n-                            C.getDefaultToolChain().getTriple());\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_HIP);\n+  llvm::DenseSet<llvm::StringRef> Triples;\n+  for (llvm::StringRef Arch : Archs) {\n+    OffloadArch ID = StringToOffloadArch(Arch);\n+    if (ID == OffloadArch::UNKNOWN)\n+      ID = StringToOffloadArch(\n+          getProcessorFromTargetID(llvm::Triple(\"amdgcn-amd-amdhsa\"), Arch));\n \n-    // TODO: Fix 'amdgcnspirv' handling with the new driver.\n-    if (C.getInputArgs().hasFlag(options::OPT_offload_new_driver,\n-                                 options::OPT_no_offload_new_driver, false))\n-      OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_HIP, &TC,\n-                                          /*SpecificToolchain=*/true);\n-  }\n+    if (Kind == Action::OFK_HIP && !IsAMDOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"HIP\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_Cuda && !IsNVIDIAOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"CUDA\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_OpenMP &&\n+        (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED)) {\n+      C.getDriver().Diag(clang::diag::err_drv_failed_to_deduce_target_from_arch)\n+          << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"offload\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+\n+    StringRef Triple;\n+    if (ID == OffloadArch::AMDGCNSPIRV)\n+      Triple = \"spirv64-amd-amdhsa\";\n+    else if (IsNVIDIAOffloadArch(ID))\n+      Triple = C.getDefaultToolChain().getTriple().isArch64Bit()\n+                   ? \"nvptx64-nvidia-cuda\"\n+                   : \"nvptx-nvidia-cuda\";\n+    else if (IsAMDOffloadArch(ID))\n+      Triple = \"amdgcn-amd-amdhsa\";\n+    else\n+      continue;\n \n-  if (IsCuda || IsHIP)\n-    CUIDOpts = CUIDOptions(C.getArgs(), *this);\n+    // Make a new argument that dispatches this argument to the appropriate\n+    // toolchain. This is required when we infer it and create potentially\n+    // incompatible toolchains from the global option.\n+    Option Opt = C.getDriver().getOpts().getOption(options::OPT_Xarch__);\n+    unsigned Index = C.getArgs().getBaseArgs().MakeIndex(\"-Xarch_\");\n+    Arg *A = new Arg(Opt, C.getArgs().getArgString(Index), Index,",
    "repo_full_name": "llvm/llvm-project",
    "discussion_comments": [
      {
        "comment_id": "2223926874",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 125556,
        "pr_file": "clang/lib/Driver/Driver.cpp",
        "discussion_id": "2223926874",
        "commented_code": "@@ -950,221 +930,264 @@ static bool addSYCLDefaultTriple(Compilation &C,\n   return true;\n }\n \n-void Driver::CreateOffloadingDeviceToolChains(Compilation &C,\n-                                              InputList &Inputs) {\n-\n-  //\n-  // CUDA/HIP\n-  //\n-  // We need to generate a CUDA/HIP toolchain if any of the inputs has a CUDA\n-  // or HIP type. However, mixed CUDA/HIP compilation is not supported.\n-  bool IsCuda =\n-      llvm::any_of(Inputs, [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-        return types::isCuda(I.first);\n-      });\n-  bool IsHIP =\n-      llvm::any_of(Inputs,\n-                   [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-                     return types::isHIP(I.first);\n-                   }) ||\n-      C.getInputArgs().hasArg(options::OPT_hip_link) ||\n-      C.getInputArgs().hasArg(options::OPT_hipstdpar);\n-  bool UseLLVMOffload = C.getInputArgs().hasArg(\n-      options::OPT_foffload_via_llvm, options::OPT_fno_offload_via_llvm, false);\n-  if (IsCuda && IsHIP) {\n-    Diag(clang::diag::err_drv_mix_cuda_hip);\n-    return;\n+// Handles `native` offload architectures by using the 'offload-arch' utility.\n+static llvm::SmallVector<std::string>\n+getSystemOffloadArchs(Compilation &C, Action::OffloadKind Kind) {\n+  StringRef Program = C.getArgs().getLastArgValue(\n+      options::OPT_offload_arch_tool_EQ, \"offload-arch\");\n+\n+  SmallVector<std::string, 1> GPUArchs;\n+  if (llvm::ErrorOr<std::string> Executable =\n+          llvm::sys::findProgramByName(Program)) {\n+    llvm::SmallVector<StringRef> Args{*Executable};\n+    if (Kind == Action::OFK_HIP)\n+      Args.push_back(\"--only=amdgpu\");\n+    else if (Kind == Action::OFK_Cuda)\n+      Args.push_back(\"--only=nvptx\");\n+    auto StdoutOrErr = C.getDriver().executeProgram(Args);\n+\n+    if (!StdoutOrErr) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << StdoutOrErr.takeError()\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    } else if ((*StdoutOrErr)->getBuffer().empty()) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << \"No GPU detected in the system\"\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    }\n+\n+    for (StringRef Arch : llvm::split((*StdoutOrErr)->getBuffer(), \"\\n\"))\n+      if (!Arch.empty())\n+        GPUArchs.push_back(Arch.str());\n+  } else {\n+    C.getDriver().Diag(diag::err_drv_command_failure) << \"offload-arch\";\n   }\n-  if (IsCuda && !UseLLVMOffload) {\n-    auto CudaTriple = getNVIDIAOffloadTargetTriple(\n-        *this, C.getInputArgs(), C.getDefaultToolChain().getTriple());\n-    if (!CudaTriple)\n-      return;\n+  return GPUArchs;\n+}\n \n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_Cuda, *CudaTriple,\n-                            C.getDefaultToolChain().getTriple());\n-\n-    // Emit a warning if the detected CUDA version is too new.\n-    const CudaInstallationDetector &CudaInstallation =\n-        static_cast<const toolchains::CudaToolChain &>(TC).CudaInstallation;\n-    if (CudaInstallation.isValid())\n-      CudaInstallation.WarnIfUnsupportedVersion();\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_Cuda);\n-    OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_Cuda, &TC,\n-                                        /*SpecificToolchain=*/true);\n-  } else if (IsHIP && !UseLLVMOffload) {\n-    if (auto *OMPTargetArg =\n-            C.getInputArgs().getLastArg(options::OPT_offload_targets_EQ)) {\n-      Diag(clang::diag::err_drv_unsupported_opt_for_language_mode)\n-          << OMPTargetArg->getSpelling() << \"HIP\";\n-      return;\n+// Attempts to infer the correct offloading toolchain triple by looking at the\n+// requested offloading kind and architectures.\n+static llvm::DenseSet<llvm::StringRef>\n+inferOffloadToolchains(Compilation &C, Action::OffloadKind Kind) {\n+  std::set<std::string> Archs;\n+  for (Arg *A : C.getInputArgs()) {\n+    for (StringRef Arch : A->getValues()) {\n+      if (A->getOption().matches(options::OPT_offload_arch_EQ)) {\n+        if (Arch == \"native\") {\n+          for (StringRef Str : getSystemOffloadArchs(C, Kind))\n+            Archs.insert(Str.str());\n+        } else {\n+          Archs.insert(Arch.str());\n+        }\n+      } else if (A->getOption().matches(options::OPT_no_offload_arch_EQ)) {\n+        if (Arch == \"all\")\n+          Archs.clear();\n+        else\n+          Archs.erase(Arch.str());\n+      }\n     }\n+  }\n \n-    auto HIPTriple = getHIPOffloadTargetTriple(*this, C.getInputArgs());\n-    if (!HIPTriple)\n-      return;\n-\n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_HIP, *HIPTriple,\n-                            C.getDefaultToolChain().getTriple());\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_HIP);\n+  llvm::DenseSet<llvm::StringRef> Triples;\n+  for (llvm::StringRef Arch : Archs) {\n+    OffloadArch ID = StringToOffloadArch(Arch);\n+    if (ID == OffloadArch::UNKNOWN)\n+      ID = StringToOffloadArch(\n+          getProcessorFromTargetID(llvm::Triple(\"amdgcn-amd-amdhsa\"), Arch));\n \n-    // TODO: Fix 'amdgcnspirv' handling with the new driver.\n-    if (C.getInputArgs().hasFlag(options::OPT_offload_new_driver,\n-                                 options::OPT_no_offload_new_driver, false))\n-      OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_HIP, &TC,\n-                                          /*SpecificToolchain=*/true);\n-  }\n+    if (Kind == Action::OFK_HIP && !IsAMDOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"HIP\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_Cuda && !IsNVIDIAOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"CUDA\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_OpenMP &&\n+        (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED)) {\n+      C.getDriver().Diag(clang::diag::err_drv_failed_to_deduce_target_from_arch)\n+          << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"offload\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+\n+    StringRef Triple;\n+    if (ID == OffloadArch::AMDGCNSPIRV)\n+      Triple = \"spirv64-amd-amdhsa\";\n+    else if (IsNVIDIAOffloadArch(ID))\n+      Triple = C.getDefaultToolChain().getTriple().isArch64Bit()\n+                   ? \"nvptx64-nvidia-cuda\"\n+                   : \"nvptx-nvidia-cuda\";\n+    else if (IsAMDOffloadArch(ID))\n+      Triple = \"amdgcn-amd-amdhsa\";\n+    else\n+      continue;\n \n-  if (IsCuda || IsHIP)\n-    CUIDOpts = CUIDOptions(C.getArgs(), *this);\n+    // Make a new argument that dispatches this argument to the appropriate\n+    // toolchain. This is required when we infer it and create potentially\n+    // incompatible toolchains from the global option.\n+    Option Opt = C.getDriver().getOpts().getOption(options::OPT_Xarch__);\n+    unsigned Index = C.getArgs().getBaseArgs().MakeIndex(\"-Xarch_\");\n+    Arg *A = new Arg(Opt, C.getArgs().getArgString(Index), Index,",
        "comment_created_at": "2025-07-22T22:13:37+00:00",
        "comment_author": "jsji",
        "comment_body": "ASAN build detected memory leaks in ~75 Driver tests.\r\n\r\nbin/clang -### --target=x86_64-linux-gnu --cuda-gpu-arch=gfx900 --rocm-path=clang/test/Driver/Inputs/rocm --cuda-device-only clang/test/Driver/hip-wavefront-size.hip\r\n\r\n=================================================================\r\n==845==ERROR: LeakSanitizer: detected memory leaks\r\n\r\nDirect leak of 88 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7f7b99cdb698 in operator new(unsigned long) ../../../../src/libsanitizer/asan/asan_new_delete.cpp:95\r\n    #1 0x7f7b894eee20 in inferOffloadToolchains(clang::driver::Compilation&, clang::driver::Action::OffloadKind) \r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2223932006",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 125556,
        "pr_file": "clang/lib/Driver/Driver.cpp",
        "discussion_id": "2223926874",
        "commented_code": "@@ -950,221 +930,264 @@ static bool addSYCLDefaultTriple(Compilation &C,\n   return true;\n }\n \n-void Driver::CreateOffloadingDeviceToolChains(Compilation &C,\n-                                              InputList &Inputs) {\n-\n-  //\n-  // CUDA/HIP\n-  //\n-  // We need to generate a CUDA/HIP toolchain if any of the inputs has a CUDA\n-  // or HIP type. However, mixed CUDA/HIP compilation is not supported.\n-  bool IsCuda =\n-      llvm::any_of(Inputs, [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-        return types::isCuda(I.first);\n-      });\n-  bool IsHIP =\n-      llvm::any_of(Inputs,\n-                   [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-                     return types::isHIP(I.first);\n-                   }) ||\n-      C.getInputArgs().hasArg(options::OPT_hip_link) ||\n-      C.getInputArgs().hasArg(options::OPT_hipstdpar);\n-  bool UseLLVMOffload = C.getInputArgs().hasArg(\n-      options::OPT_foffload_via_llvm, options::OPT_fno_offload_via_llvm, false);\n-  if (IsCuda && IsHIP) {\n-    Diag(clang::diag::err_drv_mix_cuda_hip);\n-    return;\n+// Handles `native` offload architectures by using the 'offload-arch' utility.\n+static llvm::SmallVector<std::string>\n+getSystemOffloadArchs(Compilation &C, Action::OffloadKind Kind) {\n+  StringRef Program = C.getArgs().getLastArgValue(\n+      options::OPT_offload_arch_tool_EQ, \"offload-arch\");\n+\n+  SmallVector<std::string, 1> GPUArchs;\n+  if (llvm::ErrorOr<std::string> Executable =\n+          llvm::sys::findProgramByName(Program)) {\n+    llvm::SmallVector<StringRef> Args{*Executable};\n+    if (Kind == Action::OFK_HIP)\n+      Args.push_back(\"--only=amdgpu\");\n+    else if (Kind == Action::OFK_Cuda)\n+      Args.push_back(\"--only=nvptx\");\n+    auto StdoutOrErr = C.getDriver().executeProgram(Args);\n+\n+    if (!StdoutOrErr) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << StdoutOrErr.takeError()\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    } else if ((*StdoutOrErr)->getBuffer().empty()) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << \"No GPU detected in the system\"\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    }\n+\n+    for (StringRef Arch : llvm::split((*StdoutOrErr)->getBuffer(), \"\\n\"))\n+      if (!Arch.empty())\n+        GPUArchs.push_back(Arch.str());\n+  } else {\n+    C.getDriver().Diag(diag::err_drv_command_failure) << \"offload-arch\";\n   }\n-  if (IsCuda && !UseLLVMOffload) {\n-    auto CudaTriple = getNVIDIAOffloadTargetTriple(\n-        *this, C.getInputArgs(), C.getDefaultToolChain().getTriple());\n-    if (!CudaTriple)\n-      return;\n+  return GPUArchs;\n+}\n \n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_Cuda, *CudaTriple,\n-                            C.getDefaultToolChain().getTriple());\n-\n-    // Emit a warning if the detected CUDA version is too new.\n-    const CudaInstallationDetector &CudaInstallation =\n-        static_cast<const toolchains::CudaToolChain &>(TC).CudaInstallation;\n-    if (CudaInstallation.isValid())\n-      CudaInstallation.WarnIfUnsupportedVersion();\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_Cuda);\n-    OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_Cuda, &TC,\n-                                        /*SpecificToolchain=*/true);\n-  } else if (IsHIP && !UseLLVMOffload) {\n-    if (auto *OMPTargetArg =\n-            C.getInputArgs().getLastArg(options::OPT_offload_targets_EQ)) {\n-      Diag(clang::diag::err_drv_unsupported_opt_for_language_mode)\n-          << OMPTargetArg->getSpelling() << \"HIP\";\n-      return;\n+// Attempts to infer the correct offloading toolchain triple by looking at the\n+// requested offloading kind and architectures.\n+static llvm::DenseSet<llvm::StringRef>\n+inferOffloadToolchains(Compilation &C, Action::OffloadKind Kind) {\n+  std::set<std::string> Archs;\n+  for (Arg *A : C.getInputArgs()) {\n+    for (StringRef Arch : A->getValues()) {\n+      if (A->getOption().matches(options::OPT_offload_arch_EQ)) {\n+        if (Arch == \"native\") {\n+          for (StringRef Str : getSystemOffloadArchs(C, Kind))\n+            Archs.insert(Str.str());\n+        } else {\n+          Archs.insert(Arch.str());\n+        }\n+      } else if (A->getOption().matches(options::OPT_no_offload_arch_EQ)) {\n+        if (Arch == \"all\")\n+          Archs.clear();\n+        else\n+          Archs.erase(Arch.str());\n+      }\n     }\n+  }\n \n-    auto HIPTriple = getHIPOffloadTargetTriple(*this, C.getInputArgs());\n-    if (!HIPTriple)\n-      return;\n-\n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_HIP, *HIPTriple,\n-                            C.getDefaultToolChain().getTriple());\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_HIP);\n+  llvm::DenseSet<llvm::StringRef> Triples;\n+  for (llvm::StringRef Arch : Archs) {\n+    OffloadArch ID = StringToOffloadArch(Arch);\n+    if (ID == OffloadArch::UNKNOWN)\n+      ID = StringToOffloadArch(\n+          getProcessorFromTargetID(llvm::Triple(\"amdgcn-amd-amdhsa\"), Arch));\n \n-    // TODO: Fix 'amdgcnspirv' handling with the new driver.\n-    if (C.getInputArgs().hasFlag(options::OPT_offload_new_driver,\n-                                 options::OPT_no_offload_new_driver, false))\n-      OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_HIP, &TC,\n-                                          /*SpecificToolchain=*/true);\n-  }\n+    if (Kind == Action::OFK_HIP && !IsAMDOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"HIP\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_Cuda && !IsNVIDIAOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"CUDA\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_OpenMP &&\n+        (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED)) {\n+      C.getDriver().Diag(clang::diag::err_drv_failed_to_deduce_target_from_arch)\n+          << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"offload\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+\n+    StringRef Triple;\n+    if (ID == OffloadArch::AMDGCNSPIRV)\n+      Triple = \"spirv64-amd-amdhsa\";\n+    else if (IsNVIDIAOffloadArch(ID))\n+      Triple = C.getDefaultToolChain().getTriple().isArch64Bit()\n+                   ? \"nvptx64-nvidia-cuda\"\n+                   : \"nvptx-nvidia-cuda\";\n+    else if (IsAMDOffloadArch(ID))\n+      Triple = \"amdgcn-amd-amdhsa\";\n+    else\n+      continue;\n \n-  if (IsCuda || IsHIP)\n-    CUIDOpts = CUIDOptions(C.getArgs(), *this);\n+    // Make a new argument that dispatches this argument to the appropriate\n+    // toolchain. This is required when we infer it and create potentially\n+    // incompatible toolchains from the global option.\n+    Option Opt = C.getDriver().getOpts().getOption(options::OPT_Xarch__);\n+    unsigned Index = C.getArgs().getBaseArgs().MakeIndex(\"-Xarch_\");\n+    Arg *A = new Arg(Opt, C.getArgs().getArgString(Index), Index,",
        "comment_created_at": "2025-07-22T22:17:20+00:00",
        "comment_author": "jhuber6",
        "comment_body": "Any guess why? We call `MakeArgString` here which creates permanent string storage. The argument string and index should be managed by the argument list as well. The new argument we get out is appended, we do similar things elsewhere. Do you know what could be problematic here?",
        "pr_file_module": null
      },
      {
        "comment_id": "2224001795",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 125556,
        "pr_file": "clang/lib/Driver/Driver.cpp",
        "discussion_id": "2223926874",
        "commented_code": "@@ -950,221 +930,264 @@ static bool addSYCLDefaultTriple(Compilation &C,\n   return true;\n }\n \n-void Driver::CreateOffloadingDeviceToolChains(Compilation &C,\n-                                              InputList &Inputs) {\n-\n-  //\n-  // CUDA/HIP\n-  //\n-  // We need to generate a CUDA/HIP toolchain if any of the inputs has a CUDA\n-  // or HIP type. However, mixed CUDA/HIP compilation is not supported.\n-  bool IsCuda =\n-      llvm::any_of(Inputs, [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-        return types::isCuda(I.first);\n-      });\n-  bool IsHIP =\n-      llvm::any_of(Inputs,\n-                   [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-                     return types::isHIP(I.first);\n-                   }) ||\n-      C.getInputArgs().hasArg(options::OPT_hip_link) ||\n-      C.getInputArgs().hasArg(options::OPT_hipstdpar);\n-  bool UseLLVMOffload = C.getInputArgs().hasArg(\n-      options::OPT_foffload_via_llvm, options::OPT_fno_offload_via_llvm, false);\n-  if (IsCuda && IsHIP) {\n-    Diag(clang::diag::err_drv_mix_cuda_hip);\n-    return;\n+// Handles `native` offload architectures by using the 'offload-arch' utility.\n+static llvm::SmallVector<std::string>\n+getSystemOffloadArchs(Compilation &C, Action::OffloadKind Kind) {\n+  StringRef Program = C.getArgs().getLastArgValue(\n+      options::OPT_offload_arch_tool_EQ, \"offload-arch\");\n+\n+  SmallVector<std::string, 1> GPUArchs;\n+  if (llvm::ErrorOr<std::string> Executable =\n+          llvm::sys::findProgramByName(Program)) {\n+    llvm::SmallVector<StringRef> Args{*Executable};\n+    if (Kind == Action::OFK_HIP)\n+      Args.push_back(\"--only=amdgpu\");\n+    else if (Kind == Action::OFK_Cuda)\n+      Args.push_back(\"--only=nvptx\");\n+    auto StdoutOrErr = C.getDriver().executeProgram(Args);\n+\n+    if (!StdoutOrErr) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << StdoutOrErr.takeError()\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    } else if ((*StdoutOrErr)->getBuffer().empty()) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << \"No GPU detected in the system\"\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    }\n+\n+    for (StringRef Arch : llvm::split((*StdoutOrErr)->getBuffer(), \"\\n\"))\n+      if (!Arch.empty())\n+        GPUArchs.push_back(Arch.str());\n+  } else {\n+    C.getDriver().Diag(diag::err_drv_command_failure) << \"offload-arch\";\n   }\n-  if (IsCuda && !UseLLVMOffload) {\n-    auto CudaTriple = getNVIDIAOffloadTargetTriple(\n-        *this, C.getInputArgs(), C.getDefaultToolChain().getTriple());\n-    if (!CudaTriple)\n-      return;\n+  return GPUArchs;\n+}\n \n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_Cuda, *CudaTriple,\n-                            C.getDefaultToolChain().getTriple());\n-\n-    // Emit a warning if the detected CUDA version is too new.\n-    const CudaInstallationDetector &CudaInstallation =\n-        static_cast<const toolchains::CudaToolChain &>(TC).CudaInstallation;\n-    if (CudaInstallation.isValid())\n-      CudaInstallation.WarnIfUnsupportedVersion();\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_Cuda);\n-    OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_Cuda, &TC,\n-                                        /*SpecificToolchain=*/true);\n-  } else if (IsHIP && !UseLLVMOffload) {\n-    if (auto *OMPTargetArg =\n-            C.getInputArgs().getLastArg(options::OPT_offload_targets_EQ)) {\n-      Diag(clang::diag::err_drv_unsupported_opt_for_language_mode)\n-          << OMPTargetArg->getSpelling() << \"HIP\";\n-      return;\n+// Attempts to infer the correct offloading toolchain triple by looking at the\n+// requested offloading kind and architectures.\n+static llvm::DenseSet<llvm::StringRef>\n+inferOffloadToolchains(Compilation &C, Action::OffloadKind Kind) {\n+  std::set<std::string> Archs;\n+  for (Arg *A : C.getInputArgs()) {\n+    for (StringRef Arch : A->getValues()) {\n+      if (A->getOption().matches(options::OPT_offload_arch_EQ)) {\n+        if (Arch == \"native\") {\n+          for (StringRef Str : getSystemOffloadArchs(C, Kind))\n+            Archs.insert(Str.str());\n+        } else {\n+          Archs.insert(Arch.str());\n+        }\n+      } else if (A->getOption().matches(options::OPT_no_offload_arch_EQ)) {\n+        if (Arch == \"all\")\n+          Archs.clear();\n+        else\n+          Archs.erase(Arch.str());\n+      }\n     }\n+  }\n \n-    auto HIPTriple = getHIPOffloadTargetTriple(*this, C.getInputArgs());\n-    if (!HIPTriple)\n-      return;\n-\n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_HIP, *HIPTriple,\n-                            C.getDefaultToolChain().getTriple());\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_HIP);\n+  llvm::DenseSet<llvm::StringRef> Triples;\n+  for (llvm::StringRef Arch : Archs) {\n+    OffloadArch ID = StringToOffloadArch(Arch);\n+    if (ID == OffloadArch::UNKNOWN)\n+      ID = StringToOffloadArch(\n+          getProcessorFromTargetID(llvm::Triple(\"amdgcn-amd-amdhsa\"), Arch));\n \n-    // TODO: Fix 'amdgcnspirv' handling with the new driver.\n-    if (C.getInputArgs().hasFlag(options::OPT_offload_new_driver,\n-                                 options::OPT_no_offload_new_driver, false))\n-      OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_HIP, &TC,\n-                                          /*SpecificToolchain=*/true);\n-  }\n+    if (Kind == Action::OFK_HIP && !IsAMDOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"HIP\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_Cuda && !IsNVIDIAOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"CUDA\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_OpenMP &&\n+        (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED)) {\n+      C.getDriver().Diag(clang::diag::err_drv_failed_to_deduce_target_from_arch)\n+          << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"offload\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+\n+    StringRef Triple;\n+    if (ID == OffloadArch::AMDGCNSPIRV)\n+      Triple = \"spirv64-amd-amdhsa\";\n+    else if (IsNVIDIAOffloadArch(ID))\n+      Triple = C.getDefaultToolChain().getTriple().isArch64Bit()\n+                   ? \"nvptx64-nvidia-cuda\"\n+                   : \"nvptx-nvidia-cuda\";\n+    else if (IsAMDOffloadArch(ID))\n+      Triple = \"amdgcn-amd-amdhsa\";\n+    else\n+      continue;\n \n-  if (IsCuda || IsHIP)\n-    CUIDOpts = CUIDOptions(C.getArgs(), *this);\n+    // Make a new argument that dispatches this argument to the appropriate\n+    // toolchain. This is required when we infer it and create potentially\n+    // incompatible toolchains from the global option.\n+    Option Opt = C.getDriver().getOpts().getOption(options::OPT_Xarch__);\n+    unsigned Index = C.getArgs().getBaseArgs().MakeIndex(\"-Xarch_\");\n+    Arg *A = new Arg(Opt, C.getArgs().getArgString(Index), Index,",
        "comment_created_at": "2025-07-22T23:20:11+00:00",
        "comment_author": "jhuber6",
        "comment_body": "Let me know if https://github.com/llvm/llvm-project/pull/150142 fixes it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2224117393",
        "repo_full_name": "llvm/llvm-project",
        "pr_number": 125556,
        "pr_file": "clang/lib/Driver/Driver.cpp",
        "discussion_id": "2223926874",
        "commented_code": "@@ -950,221 +930,264 @@ static bool addSYCLDefaultTriple(Compilation &C,\n   return true;\n }\n \n-void Driver::CreateOffloadingDeviceToolChains(Compilation &C,\n-                                              InputList &Inputs) {\n-\n-  //\n-  // CUDA/HIP\n-  //\n-  // We need to generate a CUDA/HIP toolchain if any of the inputs has a CUDA\n-  // or HIP type. However, mixed CUDA/HIP compilation is not supported.\n-  bool IsCuda =\n-      llvm::any_of(Inputs, [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-        return types::isCuda(I.first);\n-      });\n-  bool IsHIP =\n-      llvm::any_of(Inputs,\n-                   [](std::pair<types::ID, const llvm::opt::Arg *> &I) {\n-                     return types::isHIP(I.first);\n-                   }) ||\n-      C.getInputArgs().hasArg(options::OPT_hip_link) ||\n-      C.getInputArgs().hasArg(options::OPT_hipstdpar);\n-  bool UseLLVMOffload = C.getInputArgs().hasArg(\n-      options::OPT_foffload_via_llvm, options::OPT_fno_offload_via_llvm, false);\n-  if (IsCuda && IsHIP) {\n-    Diag(clang::diag::err_drv_mix_cuda_hip);\n-    return;\n+// Handles `native` offload architectures by using the 'offload-arch' utility.\n+static llvm::SmallVector<std::string>\n+getSystemOffloadArchs(Compilation &C, Action::OffloadKind Kind) {\n+  StringRef Program = C.getArgs().getLastArgValue(\n+      options::OPT_offload_arch_tool_EQ, \"offload-arch\");\n+\n+  SmallVector<std::string, 1> GPUArchs;\n+  if (llvm::ErrorOr<std::string> Executable =\n+          llvm::sys::findProgramByName(Program)) {\n+    llvm::SmallVector<StringRef> Args{*Executable};\n+    if (Kind == Action::OFK_HIP)\n+      Args.push_back(\"--only=amdgpu\");\n+    else if (Kind == Action::OFK_Cuda)\n+      Args.push_back(\"--only=nvptx\");\n+    auto StdoutOrErr = C.getDriver().executeProgram(Args);\n+\n+    if (!StdoutOrErr) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << StdoutOrErr.takeError()\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    } else if ((*StdoutOrErr)->getBuffer().empty()) {\n+      C.getDriver().Diag(diag::err_drv_undetermined_gpu_arch)\n+          << Action::GetOffloadKindName(Kind) << \"No GPU detected in the system\"\n+          << \"--offload-arch\";\n+      return GPUArchs;\n+    }\n+\n+    for (StringRef Arch : llvm::split((*StdoutOrErr)->getBuffer(), \"\\n\"))\n+      if (!Arch.empty())\n+        GPUArchs.push_back(Arch.str());\n+  } else {\n+    C.getDriver().Diag(diag::err_drv_command_failure) << \"offload-arch\";\n   }\n-  if (IsCuda && !UseLLVMOffload) {\n-    auto CudaTriple = getNVIDIAOffloadTargetTriple(\n-        *this, C.getInputArgs(), C.getDefaultToolChain().getTriple());\n-    if (!CudaTriple)\n-      return;\n+  return GPUArchs;\n+}\n \n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_Cuda, *CudaTriple,\n-                            C.getDefaultToolChain().getTriple());\n-\n-    // Emit a warning if the detected CUDA version is too new.\n-    const CudaInstallationDetector &CudaInstallation =\n-        static_cast<const toolchains::CudaToolChain &>(TC).CudaInstallation;\n-    if (CudaInstallation.isValid())\n-      CudaInstallation.WarnIfUnsupportedVersion();\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_Cuda);\n-    OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_Cuda, &TC,\n-                                        /*SpecificToolchain=*/true);\n-  } else if (IsHIP && !UseLLVMOffload) {\n-    if (auto *OMPTargetArg =\n-            C.getInputArgs().getLastArg(options::OPT_offload_targets_EQ)) {\n-      Diag(clang::diag::err_drv_unsupported_opt_for_language_mode)\n-          << OMPTargetArg->getSpelling() << \"HIP\";\n-      return;\n+// Attempts to infer the correct offloading toolchain triple by looking at the\n+// requested offloading kind and architectures.\n+static llvm::DenseSet<llvm::StringRef>\n+inferOffloadToolchains(Compilation &C, Action::OffloadKind Kind) {\n+  std::set<std::string> Archs;\n+  for (Arg *A : C.getInputArgs()) {\n+    for (StringRef Arch : A->getValues()) {\n+      if (A->getOption().matches(options::OPT_offload_arch_EQ)) {\n+        if (Arch == \"native\") {\n+          for (StringRef Str : getSystemOffloadArchs(C, Kind))\n+            Archs.insert(Str.str());\n+        } else {\n+          Archs.insert(Arch.str());\n+        }\n+      } else if (A->getOption().matches(options::OPT_no_offload_arch_EQ)) {\n+        if (Arch == \"all\")\n+          Archs.clear();\n+        else\n+          Archs.erase(Arch.str());\n+      }\n     }\n+  }\n \n-    auto HIPTriple = getHIPOffloadTargetTriple(*this, C.getInputArgs());\n-    if (!HIPTriple)\n-      return;\n-\n-    auto &TC =\n-        getOffloadToolChain(C.getInputArgs(), Action::OFK_HIP, *HIPTriple,\n-                            C.getDefaultToolChain().getTriple());\n-    C.addOffloadDeviceToolChain(&TC, Action::OFK_HIP);\n+  llvm::DenseSet<llvm::StringRef> Triples;\n+  for (llvm::StringRef Arch : Archs) {\n+    OffloadArch ID = StringToOffloadArch(Arch);\n+    if (ID == OffloadArch::UNKNOWN)\n+      ID = StringToOffloadArch(\n+          getProcessorFromTargetID(llvm::Triple(\"amdgcn-amd-amdhsa\"), Arch));\n \n-    // TODO: Fix 'amdgcnspirv' handling with the new driver.\n-    if (C.getInputArgs().hasFlag(options::OPT_offload_new_driver,\n-                                 options::OPT_no_offload_new_driver, false))\n-      OffloadArchs[&TC] = getOffloadArchs(C, C.getArgs(), Action::OFK_HIP, &TC,\n-                                          /*SpecificToolchain=*/true);\n-  }\n+    if (Kind == Action::OFK_HIP && !IsAMDOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"HIP\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_Cuda && !IsNVIDIAOffloadArch(ID)) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"CUDA\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (Kind == Action::OFK_OpenMP &&\n+        (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED)) {\n+      C.getDriver().Diag(clang::diag::err_drv_failed_to_deduce_target_from_arch)\n+          << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+    if (ID == OffloadArch::UNKNOWN || ID == OffloadArch::UNUSED) {\n+      C.getDriver().Diag(clang::diag::err_drv_offload_bad_gpu_arch)\n+          << \"offload\" << Arch;\n+      return llvm::DenseSet<llvm::StringRef>();\n+    }\n+\n+    StringRef Triple;\n+    if (ID == OffloadArch::AMDGCNSPIRV)\n+      Triple = \"spirv64-amd-amdhsa\";\n+    else if (IsNVIDIAOffloadArch(ID))\n+      Triple = C.getDefaultToolChain().getTriple().isArch64Bit()\n+                   ? \"nvptx64-nvidia-cuda\"\n+                   : \"nvptx-nvidia-cuda\";\n+    else if (IsAMDOffloadArch(ID))\n+      Triple = \"amdgcn-amd-amdhsa\";\n+    else\n+      continue;\n \n-  if (IsCuda || IsHIP)\n-    CUIDOpts = CUIDOptions(C.getArgs(), *this);\n+    // Make a new argument that dispatches this argument to the appropriate\n+    // toolchain. This is required when we infer it and create potentially\n+    // incompatible toolchains from the global option.\n+    Option Opt = C.getDriver().getOpts().getOption(options::OPT_Xarch__);\n+    unsigned Index = C.getArgs().getBaseArgs().MakeIndex(\"-Xarch_\");\n+    Arg *A = new Arg(Opt, C.getArgs().getArgString(Index), Index,",
        "comment_created_at": "2025-07-23T01:21:57+00:00",
        "comment_author": "jsji",
        "comment_body": "Yes, #150142 fixed them. Thanks!",
        "pr_file_module": null
      }
    ]
  }
]