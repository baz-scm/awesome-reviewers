[
  {
    "discussion_id": "2226306655",
    "pr_number": 1107,
    "pr_file": "lmcache/v1/storage_backend/connector/base_connector.py",
    "created_at": "2025-07-23T18:11:48+00:00",
    "commented_code": "Interface for remote connector\n     \"\"\"\n \n+    save_chunk_meta: bool = True\n+    meta_shape: Optional[torch.Size] = None\n+    meta_dtype: Optional[torch.dtype] = None\n+    meta_fmt: Optional[MemoryFormat] = None\n+    full_chunk_size: Optional[int] = None\n+    single_token_size: Optional[int] = None\n+\n+    def init_chunk_meta(\n+        self,\n+        config: Optional[LMCacheEngineConfig],\n+        metadata: Optional[LMCacheEngineMetadata],\n+    ) -> None:\n+        if (\n+            config is None\n+            or metadata is None\n+            or config.extra_config is None\n+            or config.extra_config.get(\"save_chunk_meta\", True)\n+        ):\n+            return\n+\n+        self.save_chunk_meta = False\n+        self.meta_shape = torch.Size(",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2226306655",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 1107,
        "pr_file": "lmcache/v1/storage_backend/connector/base_connector.py",
        "discussion_id": "2226306655",
        "commented_code": "@@ -29,6 +34,86 @@ class RemoteConnector(metaclass=abc.ABCMeta):\n     Interface for remote connector\n     \"\"\"\n \n+    save_chunk_meta: bool = True\n+    meta_shape: Optional[torch.Size] = None\n+    meta_dtype: Optional[torch.dtype] = None\n+    meta_fmt: Optional[MemoryFormat] = None\n+    full_chunk_size: Optional[int] = None\n+    single_token_size: Optional[int] = None\n+\n+    def init_chunk_meta(\n+        self,\n+        config: Optional[LMCacheEngineConfig],\n+        metadata: Optional[LMCacheEngineMetadata],\n+    ) -> None:\n+        if (\n+            config is None\n+            or metadata is None\n+            or config.extra_config is None\n+            or config.extra_config.get(\"save_chunk_meta\", True)\n+        ):\n+            return\n+\n+        self.save_chunk_meta = False\n+        self.meta_shape = torch.Size(",
        "comment_created_at": "2025-07-23T18:11:48+00:00",
        "comment_author": "Shaoting-Feng",
        "comment_body": "Will `self.meta_shape` be different with different `self.meta_fmt`?\r\nFrom `VLLMPagedMemGPUConnectorV2`:\r\n```python\r\nif self.use_mla:\r\n    # kv_caches[0].shape: [num_pages, page_size, head_size]\r\n    assert kv_caches[0].dim() == 3\r\n    self.page_buffer_size = kv_caches[0].shape[0] * kv_caches[0].shape[1]\r\nelse:\r\n    # kv_caches[0].shape: [2, num_pages, page_size, num_heads, head_size]\r\n    assert kv_caches[0].dim() == 5\r\n    self.page_buffer_size = kv_caches[0].shape[1] * kv_caches[0].shape[2]\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2230482425",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 1107,
        "pr_file": "lmcache/v1/storage_backend/connector/base_connector.py",
        "discussion_id": "2226306655",
        "commented_code": "@@ -29,6 +34,86 @@ class RemoteConnector(metaclass=abc.ABCMeta):\n     Interface for remote connector\n     \"\"\"\n \n+    save_chunk_meta: bool = True\n+    meta_shape: Optional[torch.Size] = None\n+    meta_dtype: Optional[torch.dtype] = None\n+    meta_fmt: Optional[MemoryFormat] = None\n+    full_chunk_size: Optional[int] = None\n+    single_token_size: Optional[int] = None\n+\n+    def init_chunk_meta(\n+        self,\n+        config: Optional[LMCacheEngineConfig],\n+        metadata: Optional[LMCacheEngineMetadata],\n+    ) -> None:\n+        if (\n+            config is None\n+            or metadata is None\n+            or config.extra_config is None\n+            or config.extra_config.get(\"save_chunk_meta\", True)\n+        ):\n+            return\n+\n+        self.save_chunk_meta = False\n+        self.meta_shape = torch.Size(",
        "comment_created_at": "2025-07-25T08:20:04+00:00",
        "comment_author": "chunxiaozheng",
        "comment_body": "> Will `self.meta_shape` be different with different `self.meta_fmt`? From `VLLMPagedMemGPUConnectorV2`:\r\n> \r\n> ```python\r\n> if self.use_mla:\r\n>     # kv_caches[0].shape: [num_pages, page_size, head_size]\r\n>     assert kv_caches[0].dim() == 3\r\n>     self.page_buffer_size = kv_caches[0].shape[0] * kv_caches[0].shape[1]\r\n> else:\r\n>     # kv_caches[0].shape: [2, num_pages, page_size, num_heads, head_size]\r\n>     assert kv_caches[0].dim() == 5\r\n>     self.page_buffer_size = kv_caches[0].shape[1] * kv_caches[0].shape[2]\r\n> ```\r\n\r\nSure, the content in `self.meta_shape` is related to whether `MLA` is enabled or not. But, when calculate `kv_shape` of `metadata`, `MLA` and other factors has already been taken into consideration, so here we can directly use the content of `kv_shape` to calculate `self.meta_shape`.\r\n\r\nthe `metadata` and `kv_shape` is calculated by the following code that in `init_lmcache_engine`:\r\n```\r\n    # construct kv shape (for mem pool)\r\n    num_layer = model_config.get_num_layers(parallel_config)\r\n    chunk_size = config.chunk_size\r\n    num_kv_head = model_config.get_num_kv_heads(parallel_config)\r\n    head_size = model_config.get_head_size()\r\n    kv_shape = (num_layer, 1 if use_mla else 2, chunk_size, num_kv_head, head_size)\r\n    logger.info(f\"use mla: {use_mla}, kv shape: {kv_shape}\")\r\n\r\n    # Change current device.\r\n    torch.cuda.device(parallel_config.rank)\r\n    device = torch.device(f\"cuda:{parallel_config.rank}\")\r\n    metadata = LMCacheEngineMetadata(\r\n        model_config.model,\r\n        parallel_config.world_size,\r\n        parallel_config.rank,\r\n        \"vllm\",\r\n        kv_dtype,\r\n        kv_shape,\r\n        use_mla,\r\n    )\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1809868695",
    "pr_number": 156,
    "pr_file": "lmcache/storage_backend/serde/cachegen_basics.py",
    "created_at": "2024-10-22T04:27:55+00:00",
    "commented_code": "family_9b = [\"THUDM/glm-4-9b-chat\"]\n         if model_name in family_7b:\n             return CacheGenConfig(\n-                key_first_layers=10,\n-                key_second_layers=20,\n-                key_third_layers=32,  # total layers\n-                key_first_bins=32,\n-                key_second_bins=16,\n-                key_third_bins=16,\n-                value_first_layers=2,\n-                value_first_bins=32,\n-                value_second_bins=16,\n+                nlayers=32,\n+                kspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=10, bins=32),\n+                    QuantizationSpec(start_layer=10, end_layer=20, bins=16),\n+                    QuantizationSpec(start_layer=20, end_layer=32, bins=16),\n+                ],\n+                vspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=2, bins=32),\n+                    QuantizationSpec(start_layer=2, end_layer=32, bins=16),\n+                ],\n             )\n         elif model_name in family_8b:\n             return CacheGenConfig(\n-                key_first_layers=10,\n-                key_second_layers=20,\n-                key_third_layers=32,  # total layers\n-                key_first_bins=32,\n-                key_second_bins=16,\n-                key_third_bins=16,\n-                value_first_layers=2,\n-                value_first_bins=32,\n-                value_second_bins=16,\n+                nlayers=32,\n+                kspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=10, bins=32),\n+                    QuantizationSpec(start_layer=10, end_layer=20, bins=16),\n+                    QuantizationSpec(start_layer=20, end_layer=32, bins=16),\n+                ],\n+                vspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=2, bins=32),\n+                    QuantizationSpec(start_layer=2, end_layer=32, bins=16),\n+                ],\n             )\n         # TODO(Jiayi): needs tuning for better quality\n         elif model_name in family_9b:\n             return CacheGenConfig(\n-                key_first_layers=10,\n-                key_second_layers=20,\n-                key_third_layers=40,\n-                key_first_bins=32,\n-                key_second_bins=16,\n-                key_third_bins=16,\n-                value_first_layers=2,\n-                value_first_bins=32,\n-                value_second_bins=16,\n+                nlayers=40,\n+                kspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=10, bins=32),\n+                    QuantizationSpec(start_layer=10, end_layer=20, bins=16),\n+                    QuantizationSpec(start_layer=20, end_layer=40, bins=16),\n+                ],\n+                vspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=2, bins=32),\n+                    QuantizationSpec(start_layer=2, end_layer=40, bins=16),\n+                ],",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1809868695",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 156,
        "pr_file": "lmcache/storage_backend/serde/cachegen_basics.py",
        "discussion_id": "1809868695",
        "commented_code": "@@ -39,40 +43,43 @@ def from_model_name(model_name: str) -> \"CacheGenConfig\":\n         family_9b = [\"THUDM/glm-4-9b-chat\"]\n         if model_name in family_7b:\n             return CacheGenConfig(\n-                key_first_layers=10,\n-                key_second_layers=20,\n-                key_third_layers=32,  # total layers\n-                key_first_bins=32,\n-                key_second_bins=16,\n-                key_third_bins=16,\n-                value_first_layers=2,\n-                value_first_bins=32,\n-                value_second_bins=16,\n+                nlayers=32,\n+                kspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=10, bins=32),\n+                    QuantizationSpec(start_layer=10, end_layer=20, bins=16),\n+                    QuantizationSpec(start_layer=20, end_layer=32, bins=16),\n+                ],\n+                vspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=2, bins=32),\n+                    QuantizationSpec(start_layer=2, end_layer=32, bins=16),\n+                ],\n             )\n         elif model_name in family_8b:\n             return CacheGenConfig(\n-                key_first_layers=10,\n-                key_second_layers=20,\n-                key_third_layers=32,  # total layers\n-                key_first_bins=32,\n-                key_second_bins=16,\n-                key_third_bins=16,\n-                value_first_layers=2,\n-                value_first_bins=32,\n-                value_second_bins=16,\n+                nlayers=32,\n+                kspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=10, bins=32),\n+                    QuantizationSpec(start_layer=10, end_layer=20, bins=16),\n+                    QuantizationSpec(start_layer=20, end_layer=32, bins=16),\n+                ],\n+                vspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=2, bins=32),\n+                    QuantizationSpec(start_layer=2, end_layer=32, bins=16),\n+                ],\n             )\n         # TODO(Jiayi): needs tuning for better quality\n         elif model_name in family_9b:\n             return CacheGenConfig(\n-                key_first_layers=10,\n-                key_second_layers=20,\n-                key_third_layers=40,\n-                key_first_bins=32,\n-                key_second_bins=16,\n-                key_third_bins=16,\n-                value_first_layers=2,\n-                value_first_bins=32,\n-                value_second_bins=16,\n+                nlayers=40,\n+                kspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=10, bins=32),\n+                    QuantizationSpec(start_layer=10, end_layer=20, bins=16),\n+                    QuantizationSpec(start_layer=20, end_layer=40, bins=16),\n+                ],\n+                vspecs=[\n+                    QuantizationSpec(start_layer=0, end_layer=2, bins=32),\n+                    QuantizationSpec(start_layer=2, end_layer=40, bins=16),\n+                ],",
        "comment_created_at": "2024-10-22T04:27:55+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "Is there any support for the default setting (for models other than these five models)? For exmple, we can pass the model_config in vllm (which contains info about the number of layers) or the number of layers directly as the function param, and configure the `CacheGenConfig` dynamically based on this input.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1791565687",
    "pr_number": 108,
    "pr_file": "lmcache/config.py",
    "created_at": "2024-10-08T09:48:47+00:00",
    "commented_code": "@dataclass\n class LMCacheEngineMetadata:\n-    \"\"\"name of the LLM model\"\"\"\n-\n+    \"\"\" name of the LLM model \"\"\"\n     model_name: str\n+    \n     \"\"\" world size when running under a distributed setting \"\"\"\n     world_size: int\n+    \n     \"\"\" worker id when running under a distributed setting \"\"\"\n     worker_id: int\n+    \n     \"\"\" the format of kv tensors \"\"\"\n     fmt: str\n \n+    \"\"\" the data type of kv tensors \"\"\"\n+    dtype: str",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1791565687",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 108,
        "pr_file": "lmcache/config.py",
        "discussion_id": "1791565687",
        "commented_code": "@@ -7,16 +7,20 @@\n \n @dataclass\n class LMCacheEngineMetadata:\n-    \"\"\"name of the LLM model\"\"\"\n-\n+    \"\"\" name of the LLM model \"\"\"\n     model_name: str\n+    \n     \"\"\" world size when running under a distributed setting \"\"\"\n     world_size: int\n+    \n     \"\"\" worker id when running under a distributed setting \"\"\"\n     worker_id: int\n+    \n     \"\"\" the format of kv tensors \"\"\"\n     fmt: str\n \n+    \"\"\" the data type of kv tensors \"\"\"\n+    dtype: str",
        "comment_created_at": "2024-10-08T09:48:47+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Is there any reason to support `dtype = \"auto\"` in LMCache? \r\nI think in any case, the LMCache engine needs to know an explicit dtype instead of auto?\r\n@KuntaiDu @YaoJiayi Please chime in if you have any opinion about this",
        "pr_file_module": null
      },
      {
        "comment_id": "1792532442",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 108,
        "pr_file": "lmcache/config.py",
        "discussion_id": "1791565687",
        "commented_code": "@@ -7,16 +7,20 @@\n \n @dataclass\n class LMCacheEngineMetadata:\n-    \"\"\"name of the LLM model\"\"\"\n-\n+    \"\"\" name of the LLM model \"\"\"\n     model_name: str\n+    \n     \"\"\" world size when running under a distributed setting \"\"\"\n     world_size: int\n+    \n     \"\"\" worker id when running under a distributed setting \"\"\"\n     worker_id: int\n+    \n     \"\"\" the format of kv tensors \"\"\"\n     fmt: str\n \n+    \"\"\" the data type of kv tensors \"\"\"\n+    dtype: str",
        "comment_created_at": "2024-10-08T21:28:47+00:00",
        "comment_author": "Alex-q-z",
        "comment_body": "@ApostaC \r\n\r\nYes, what we agreed on last time is to make sure there is no \"auto\" option for dtype in lmcache metadata, i.e. dtype must be an explicit type. Currently, this is achieved by overwriting any \"auto\" value of dtype in lmcache metadata with the dtype of the model in lmcache_vllm/vllm_injection.py. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1769032408",
    "pr_number": 102,
    "pr_file": "lmcache/storage_backend/serde/fast_serde.py",
    "created_at": "2024-09-20T18:30:56+00:00",
    "commented_code": "+import torch\n+import io\n+import time\n+\n+from lmcache.storage_backend.serde.serde import Serializer, Deserializer\n+from lmcache.logging import init_logger\n+from lmcache.config import GlobalConfig\n+\n+logger = init_logger(__name__)\n+\n+class FastSerializer(Serializer):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def to_bytes(self, t: torch.Tensor) -> bytes:\n+        # FIXME: only support fp16 for now\n+        assert t.dtype == torch.float16",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1769032408",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 102,
        "pr_file": "lmcache/storage_backend/serde/fast_serde.py",
        "discussion_id": "1769032408",
        "commented_code": "@@ -0,0 +1,28 @@\n+import torch\n+import io\n+import time\n+\n+from lmcache.storage_backend.serde.serde import Serializer, Deserializer\n+from lmcache.logging import init_logger\n+from lmcache.config import GlobalConfig\n+\n+logger = init_logger(__name__)\n+\n+class FastSerializer(Serializer):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def to_bytes(self, t: torch.Tensor) -> bytes:\n+        # FIXME: only support fp16 for now\n+        assert t.dtype == torch.float16",
        "comment_created_at": "2024-09-20T18:30:56+00:00",
        "comment_author": "ApostaC",
        "comment_body": "vLLM will use bfloat16 on devices supporting compute compatibility >= 8.0 (including A-series, H-series, L40, etc).\r\nTherefore, it would be very helpful to support flexible data types. \r\nFor instance, maybe we can add a separate tag in the byte stream for the data type?",
        "pr_file_module": null
      },
      {
        "comment_id": "1769063235",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 102,
        "pr_file": "lmcache/storage_backend/serde/fast_serde.py",
        "discussion_id": "1769032408",
        "commented_code": "@@ -0,0 +1,28 @@\n+import torch\n+import io\n+import time\n+\n+from lmcache.storage_backend.serde.serde import Serializer, Deserializer\n+from lmcache.logging import init_logger\n+from lmcache.config import GlobalConfig\n+\n+logger = init_logger(__name__)\n+\n+class FastSerializer(Serializer):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def to_bytes(self, t: torch.Tensor) -> bytes:\n+        # FIXME: only support fp16 for now\n+        assert t.dtype == torch.float16",
        "comment_created_at": "2024-09-20T18:53:12+00:00",
        "comment_author": "xenshinu",
        "comment_body": "Totally agree. Just to give a rough example here. Let me think how to make it more flexible. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1769641587",
    "pr_number": 102,
    "pr_file": "lmcache/storage_backend/serde/fast_serde.py",
    "created_at": "2024-09-21T19:20:26+00:00",
    "commented_code": "+import torch\n+import io\n+import time\n+import numpy as np\n+\n+from lmcache.storage_backend.serde.serde import Serializer, Deserializer\n+from lmcache.logging import init_logger\n+from lmcache.config import GlobalConfig\n+\n+logger = init_logger(__name__)\n+\n+DTYPE_TO_TAG = {\n+    torch.bfloat16: 0,\n+    torch.float16: 1,\n+    torch.float32: 2,\n+    torch.float64: 3,\n+}\n+\n+TAG_TO_DTYPE = {v: k for k, v in DTYPE_TO_TAG.items()}\n+\n+class FastSerializer(Serializer):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def to_bytes(self, t: torch.Tensor) -> bytes:\n+        # make dtype into bit stream\n+        tag = DTYPE_TO_TAG[t.dtype]\n+        # make tensor into bit stream\n+        buf = t.contiguous().cpu().view(torch.uint8).numpy().tobytes()\n+        return tag.to_bytes(1, byteorder='big') + buf",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1769641587",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 102,
        "pr_file": "lmcache/storage_backend/serde/fast_serde.py",
        "discussion_id": "1769641587",
        "commented_code": "@@ -0,0 +1,42 @@\n+import torch\n+import io\n+import time\n+import numpy as np\n+\n+from lmcache.storage_backend.serde.serde import Serializer, Deserializer\n+from lmcache.logging import init_logger\n+from lmcache.config import GlobalConfig\n+\n+logger = init_logger(__name__)\n+\n+DTYPE_TO_TAG = {\n+    torch.bfloat16: 0,\n+    torch.float16: 1,\n+    torch.float32: 2,\n+    torch.float64: 3,\n+}\n+\n+TAG_TO_DTYPE = {v: k for k, v in DTYPE_TO_TAG.items()}\n+\n+class FastSerializer(Serializer):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def to_bytes(self, t: torch.Tensor) -> bytes:\n+        # make dtype into bit stream\n+        tag = DTYPE_TO_TAG[t.dtype]\n+        # make tensor into bit stream\n+        buf = t.contiguous().cpu().view(torch.uint8).numpy().tobytes()\n+        return tag.to_bytes(1, byteorder='big') + buf",
        "comment_created_at": "2024-09-21T19:20:26+00:00",
        "comment_author": "xenshinu",
        "comment_body": "Add `dtype` tag. But this will trigger memory copy on deser, maybe attach the dtype to meta data is a better solution.",
        "pr_file_module": null
      }
    ]
  }
]