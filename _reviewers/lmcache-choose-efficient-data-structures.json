[
  {
    "discussion_id": "2237778573",
    "pr_number": 1134,
    "pr_file": "lmcache/v1/cache_engine.py",
    "created_at": "2025-07-28T20:15:25+00:00",
    "commented_code": "ret_mask = torch.zeros_like(tokens, dtype=torch.bool, device=\"cpu\")\n \n-        key_mapping: Dict[str, List[CacheEngineKey]] = {}\n-        start_mapping: Dict[str, List[int]] = {}\n-        end_mapping: Dict[str, List[int]] = {}\n+        block_mapping: Dict[str, List[Tuple[CacheEngineKey, int, int]]] = {}",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2237778573",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 1134,
        "pr_file": "lmcache/v1/cache_engine.py",
        "discussion_id": "2237778573",
        "commented_code": "@@ -421,14 +422,9 @@ def retrieve(\n \n         ret_mask = torch.zeros_like(tokens, dtype=torch.bool, device=\"cpu\")\n \n-        key_mapping: Dict[str, List[CacheEngineKey]] = {}\n-        start_mapping: Dict[str, List[int]] = {}\n-        end_mapping: Dict[str, List[int]] = {}\n+        block_mapping: Dict[str, List[Tuple[CacheEngineKey, int, int]]] = {}",
        "comment_created_at": "2025-07-28T20:15:25+00:00",
        "comment_author": "sammshen",
        "comment_body": "let's use a `defaultdict(list)`",
        "pr_file_module": null
      },
      {
        "comment_id": "2237792106",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 1134,
        "pr_file": "lmcache/v1/cache_engine.py",
        "discussion_id": "2237778573",
        "commented_code": "@@ -421,14 +422,9 @@ def retrieve(\n \n         ret_mask = torch.zeros_like(tokens, dtype=torch.bool, device=\"cpu\")\n \n-        key_mapping: Dict[str, List[CacheEngineKey]] = {}\n-        start_mapping: Dict[str, List[int]] = {}\n-        end_mapping: Dict[str, List[int]] = {}\n+        block_mapping: Dict[str, List[Tuple[CacheEngineKey, int, int]]] = {}",
        "comment_created_at": "2025-07-28T20:21:54+00:00",
        "comment_author": "sammshen",
        "comment_body": "the types are obvious but maybe just for future readability\r\n```python\r\n        # location -> [CacheEngineKey, start, end]\r\n        block_mapping: Dict[str, List[Tuple[CacheEngineKey, int, int]]] = {}\r\n        # [(CacheEngineKey, MemoryObj, start, end)]\r\n        reordered_blocks: List[Tuple[CacheEngineKey, MemoryObj, int, int]] = []\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2237779071",
    "pr_number": 1134,
    "pr_file": "lmcache/v1/cache_engine.py",
    "created_at": "2025-07-28T20:15:41+00:00",
    "commented_code": "# object is already pinned in the storage backend.\n                 ret_mask[start:end] = True\n \n-                if location not in key_mapping:\n-                    key_mapping[location] = [key]\n-                    start_mapping[location] = [start]\n-                    end_mapping[location] = [end]\n-                    continue\n+                if location not in block_mapping:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2237779071",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 1134,
        "pr_file": "lmcache/v1/cache_engine.py",
        "discussion_id": "2237779071",
        "commented_code": "@@ -469,40 +462,59 @@ def retrieve(\n                 # object is already pinned in the storage backend.\n                 ret_mask[start:end] = True\n \n-                if location not in key_mapping:\n-                    key_mapping[location] = [key]\n-                    start_mapping[location] = [start]\n-                    end_mapping[location] = [end]\n-                    continue\n+                if location not in block_mapping:",
        "comment_created_at": "2025-07-28T20:15:41+00:00",
        "comment_author": "sammshen",
        "comment_body": "delete if choose to use `defaultdict(list)`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2064148105",
    "pr_number": 514,
    "pr_file": "lmcache/experimental/storage_backend/local_cpu_backend.py",
    "created_at": "2025-04-28T17:25:54+00:00",
    "commented_code": "+import threading\n+from collections import OrderedDict\n+from concurrent.futures import Future\n+from typing import TYPE_CHECKING, List, Optional\n+\n+import torch\n+\n+from lmcache.experimental.cache_controller.message import (KVAdmitMsg,\n+                                                           KVEvictMsg)\n+from lmcache.experimental.config import LMCacheEngineConfig\n+from lmcache.experimental.lookup_server import LookupServerInterface\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj,\n+                                                    MixedMemoryAllocator)\n+from lmcache.experimental.storage_backend.abstract_backend import \\\n+    StorageBackendInterface\n+from lmcache.logging import init_logger\n+from lmcache.observability import LMCStatsMonitor\n+from lmcache.utils import CacheEngineKey\n+\n+if TYPE_CHECKING:\n+    from lmcache.experimental.cache_controller.worker import LMCacheWorker\n+\n+logger = init_logger(__name__)\n+\n+\n+class LocalCPUBackend(StorageBackendInterface):\n+    \"\"\"\n+    The local cpu backend size is variable depending on how much free space is\n+    left in the allocator so we cannot use LRUEvictor().\n+    (max_local_cpu_size > 0 initializes the memory_allocator)\n+    \"\"\"\n+\n+    def __init__(self,\n+                 config: LMCacheEngineConfig,\n+                 memory_allocator: MemoryAllocatorInterface,\n+                 lookup_server: Optional[LookupServerInterface] = None,\n+                 lmcache_worker: Optional[\"LMCacheWorker\"] = None,\n+                 dst_device: str = \"cpu\"):\n+        assert config.local_cpu is not None\n+        self.dict: OrderedDict[CacheEngineKey, MemoryObj] = OrderedDict()\n+        self.lookup_server = lookup_server\n+        self.memory_allocator = memory_allocator\n+        assert isinstance(self.memory_allocator, MixedMemoryAllocator), \\\n+            \"LocalCPUBackend must be used with a MixedMemoryAllocator\"\n+        self.dst_device = dst_device\n+        self.lmcache_worker = lmcache_worker\n+        self.instance_id = config.lmcache_instance_id\n+        self.cpu_lock = threading.Lock()\n+\n+        self.stats_monitor = LMCStatsMonitor.GetOrCreate()\n+        self.usage = 0\n+\n+    def contains(self, key: CacheEngineKey) -> bool:\n+        with self.cpu_lock:\n+            return key in self.dict\n+\n+    def exists_in_put_tasks(self, key: CacheEngineKey) -> bool:\n+        \"\"\"\n+        contains() and exists_in_put_tasks() should be checked together\n+        \"\"\"\n+        return False\n+\n+    def insert_key(self, key: CacheEngineKey, obj: MemoryObj) -> None:\n+        \"\"\"\n+        synchronously (immediately) write to cpu memory\n+        ref count stays up because the memory object stays in cpu memory\n+        \"\"\"\n+        with self.cpu_lock:\n+            if key in self.dict:\n+                old_memory_obj = self.dict.pop(key)\n+                self.memory_allocator.ref_count_down(old_memory_obj)\n+            self.dict[key] = obj\n+            self.memory_allocator.ref_count_up(obj)\n+\n+            self.usage += obj.get_size()\n+            self.stats_monitor.update_local_cache_usage(self.usage)\n+\n+            # push kv admit msg\n+            if self.lmcache_worker is not None:\n+                self.lmcache_worker.put_msg(\n+                    KVAdmitMsg(self.instance_id, key.worker_id, key.chunk_hash,\n+                               \"cpu\"))\n+\n+    def submit_put_task(self, key: CacheEngineKey,\n+                        obj: MemoryObj) -> Optional[Future]:\n+        pass\n+\n+    def submit_prefetch_task(\n+        self,\n+        key: CacheEngineKey,\n+    ) -> Optional[Future]:\n+        pass\n+\n+    def get_blocking(\n+        self,\n+        key: CacheEngineKey,\n+    ) -> Optional[MemoryObj]:\n+        with self.cpu_lock:\n+            if key not in self.dict:\n+                return None\n+            memory_obj = self.dict[key]\n+            # ref count up for caller to avoid situation where the memory_obj\n+            # is evicted from the local cpu backend before the caller calls\n+            # ref count up themselves\n+            self.memory_allocator.ref_count_up(memory_obj)\n+            return memory_obj\n+\n+    def remove(self, key: CacheEngineKey) -> None:\n+        with self.cpu_lock:\n+            if key in self.dict:\n+                memory_obj = self.dict.pop(key)\n+                self.memory_allocator.ref_count_down(memory_obj)\n+\n+                self.usage -= memory_obj.get_size()\n+                self.stats_monitor.update_local_cache_usage(self.usage)\n+\n+                if self.lmcache_worker is not None:\n+                    self.lmcache_worker.put_msg(\n+                        KVEvictMsg(self.instance_id, key.worker_id,\n+                                   key.chunk_hash, \"cpu\"))\n+\n+    def touch(self, key: CacheEngineKey) -> None:\n+        \"\"\"\n+        Touch a key in the local cpu cache (maximize recency)\n+        \"\"\"\n+        with self.cpu_lock:\n+            if key in self.dict:\n+                self.dict.move_to_end(key)\n+\n+    def allocate(self, shape: torch.Size,\n+                 dtype: torch.dtype) -> Optional[MemoryObj]:\n+        \"\"\"\n+        allocate a memory object in the cpu backend by evicting LRU policy\n+        takes in the shape and dtype of the memory object to be allocated\n+        returns:\n+        - None if we could not make space for the memory object in cpu memory\n+        - the allocated memory object otherwise\n+        \"\"\"\n+        memory_obj = None\n+        evict_keys = []\n+        with self.cpu_lock:\n+            for evict_key in self.dict:\n+                # If the ref_count > 1, we cannot evict it as the cpu memory\n+                # might be used as buffers by other storage backends\n+                if self.memory_allocator.get_ref_count(\n+                        self.dict[evict_key]) > 1:\n+                    continue\n+                evict_keys.append(evict_key)\n+\n+                self.memory_allocator.ref_count_down(self.dict[evict_key])\n+                memory_obj = self.memory_allocator.allocate(shape, dtype)\n+                logger.debug(\"Evicting 1 chunk from cpu memory\")\n+                if memory_obj is not None:\n+                    break\n+        for evict_key in evict_keys:\n+            self.remove(evict_key)\n+        if self.lookup_server is not None:\n+            self.lookup_server.batched_remove(evict_keys)\n+        return memory_obj",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2064148105",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 514,
        "pr_file": "lmcache/experimental/storage_backend/local_cpu_backend.py",
        "discussion_id": "2064148105",
        "commented_code": "@@ -0,0 +1,191 @@\n+import threading\n+from collections import OrderedDict\n+from concurrent.futures import Future\n+from typing import TYPE_CHECKING, List, Optional\n+\n+import torch\n+\n+from lmcache.experimental.cache_controller.message import (KVAdmitMsg,\n+                                                           KVEvictMsg)\n+from lmcache.experimental.config import LMCacheEngineConfig\n+from lmcache.experimental.lookup_server import LookupServerInterface\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj,\n+                                                    MixedMemoryAllocator)\n+from lmcache.experimental.storage_backend.abstract_backend import \\\n+    StorageBackendInterface\n+from lmcache.logging import init_logger\n+from lmcache.observability import LMCStatsMonitor\n+from lmcache.utils import CacheEngineKey\n+\n+if TYPE_CHECKING:\n+    from lmcache.experimental.cache_controller.worker import LMCacheWorker\n+\n+logger = init_logger(__name__)\n+\n+\n+class LocalCPUBackend(StorageBackendInterface):\n+    \"\"\"\n+    The local cpu backend size is variable depending on how much free space is\n+    left in the allocator so we cannot use LRUEvictor().\n+    (max_local_cpu_size > 0 initializes the memory_allocator)\n+    \"\"\"\n+\n+    def __init__(self,\n+                 config: LMCacheEngineConfig,\n+                 memory_allocator: MemoryAllocatorInterface,\n+                 lookup_server: Optional[LookupServerInterface] = None,\n+                 lmcache_worker: Optional[\"LMCacheWorker\"] = None,\n+                 dst_device: str = \"cpu\"):\n+        assert config.local_cpu is not None\n+        self.dict: OrderedDict[CacheEngineKey, MemoryObj] = OrderedDict()\n+        self.lookup_server = lookup_server\n+        self.memory_allocator = memory_allocator\n+        assert isinstance(self.memory_allocator, MixedMemoryAllocator), \\\n+            \"LocalCPUBackend must be used with a MixedMemoryAllocator\"\n+        self.dst_device = dst_device\n+        self.lmcache_worker = lmcache_worker\n+        self.instance_id = config.lmcache_instance_id\n+        self.cpu_lock = threading.Lock()\n+\n+        self.stats_monitor = LMCStatsMonitor.GetOrCreate()\n+        self.usage = 0\n+\n+    def contains(self, key: CacheEngineKey) -> bool:\n+        with self.cpu_lock:\n+            return key in self.dict\n+\n+    def exists_in_put_tasks(self, key: CacheEngineKey) -> bool:\n+        \"\"\"\n+        contains() and exists_in_put_tasks() should be checked together\n+        \"\"\"\n+        return False\n+\n+    def insert_key(self, key: CacheEngineKey, obj: MemoryObj) -> None:\n+        \"\"\"\n+        synchronously (immediately) write to cpu memory\n+        ref count stays up because the memory object stays in cpu memory\n+        \"\"\"\n+        with self.cpu_lock:\n+            if key in self.dict:\n+                old_memory_obj = self.dict.pop(key)\n+                self.memory_allocator.ref_count_down(old_memory_obj)\n+            self.dict[key] = obj\n+            self.memory_allocator.ref_count_up(obj)\n+\n+            self.usage += obj.get_size()\n+            self.stats_monitor.update_local_cache_usage(self.usage)\n+\n+            # push kv admit msg\n+            if self.lmcache_worker is not None:\n+                self.lmcache_worker.put_msg(\n+                    KVAdmitMsg(self.instance_id, key.worker_id, key.chunk_hash,\n+                               \"cpu\"))\n+\n+    def submit_put_task(self, key: CacheEngineKey,\n+                        obj: MemoryObj) -> Optional[Future]:\n+        pass\n+\n+    def submit_prefetch_task(\n+        self,\n+        key: CacheEngineKey,\n+    ) -> Optional[Future]:\n+        pass\n+\n+    def get_blocking(\n+        self,\n+        key: CacheEngineKey,\n+    ) -> Optional[MemoryObj]:\n+        with self.cpu_lock:\n+            if key not in self.dict:\n+                return None\n+            memory_obj = self.dict[key]\n+            # ref count up for caller to avoid situation where the memory_obj\n+            # is evicted from the local cpu backend before the caller calls\n+            # ref count up themselves\n+            self.memory_allocator.ref_count_up(memory_obj)\n+            return memory_obj\n+\n+    def remove(self, key: CacheEngineKey) -> None:\n+        with self.cpu_lock:\n+            if key in self.dict:\n+                memory_obj = self.dict.pop(key)\n+                self.memory_allocator.ref_count_down(memory_obj)\n+\n+                self.usage -= memory_obj.get_size()\n+                self.stats_monitor.update_local_cache_usage(self.usage)\n+\n+                if self.lmcache_worker is not None:\n+                    self.lmcache_worker.put_msg(\n+                        KVEvictMsg(self.instance_id, key.worker_id,\n+                                   key.chunk_hash, \"cpu\"))\n+\n+    def touch(self, key: CacheEngineKey) -> None:\n+        \"\"\"\n+        Touch a key in the local cpu cache (maximize recency)\n+        \"\"\"\n+        with self.cpu_lock:\n+            if key in self.dict:\n+                self.dict.move_to_end(key)\n+\n+    def allocate(self, shape: torch.Size,\n+                 dtype: torch.dtype) -> Optional[MemoryObj]:\n+        \"\"\"\n+        allocate a memory object in the cpu backend by evicting LRU policy\n+        takes in the shape and dtype of the memory object to be allocated\n+        returns:\n+        - None if we could not make space for the memory object in cpu memory\n+        - the allocated memory object otherwise\n+        \"\"\"\n+        memory_obj = None\n+        evict_keys = []\n+        with self.cpu_lock:\n+            for evict_key in self.dict:\n+                # If the ref_count > 1, we cannot evict it as the cpu memory\n+                # might be used as buffers by other storage backends\n+                if self.memory_allocator.get_ref_count(\n+                        self.dict[evict_key]) > 1:\n+                    continue\n+                evict_keys.append(evict_key)\n+\n+                self.memory_allocator.ref_count_down(self.dict[evict_key])\n+                memory_obj = self.memory_allocator.allocate(shape, dtype)\n+                logger.debug(\"Evicting 1 chunk from cpu memory\")\n+                if memory_obj is not None:\n+                    break\n+        for evict_key in evict_keys:\n+            self.remove(evict_key)\n+        if self.lookup_server is not None:\n+            self.lookup_server.batched_remove(evict_keys)\n+        return memory_obj",
        "comment_created_at": "2025-04-28T17:25:54+00:00",
        "comment_author": "ApostaC",
        "comment_body": "IIUC, the eviction logic should only be triggered when the first allocation fails. The \"first allocation\" is missing in this function.\r\ncc @YaoJiayi ",
        "pr_file_module": null
      }
    ]
  }
]