[
  {
    "discussion_id": "2105873863",
    "pr_number": 699,
    "pr_file": "lmcache/experimental/storage_backend/weka_gds_backend.py",
    "created_at": "2025-05-24T16:26:02+00:00",
    "commented_code": "+# Copyright 2025 Ilya Yanok, Serapheim Dimitropoulos.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import asyncio\n+import ctypes\n+import os\n+import random\n+import string\n+import struct\n+import threading\n+import time\n+from collections import OrderedDict\n+from concurrent.futures import Future\n+from typing import Optional\n+\n+import aiofile\n+import cufile\n+import torch\n+\n+from lmcache.experimental.config import LMCacheEngineConfig\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj)\n+from lmcache.experimental.storage_backend.abstract_backend import \\\n+    StorageBackendInterface\n+from lmcache.logging import init_logger\n+from lmcache.utils import (CacheEngineKey, DiskCacheMetadata,\n+                           _lmcache_nvtx_annotate)\n+\n+logger = init_logger(__name__)\n+\n+_METADATA_FILE_SUFFIX = \".metadata\"\n+_DATA_FILE_SUFFIX = \".weka1\"\n+_METADATA_VERSION = 1\n+_METADATA_MAX_SIZE = 4096  # reserve 4K for metadata\n+\n+\n+class UnsupportedMetadataVersion(Exception):\n+    pass\n+\n+\n+torch_dtypes = [\n+    torch.half,\n+    torch.float16,\n+    torch.bfloat16,\n+    torch.float,\n+    torch.float32,\n+    torch.float64,\n+    torch.double,\n+    torch.uint8,\n+    torch.float8_e4m3fn,\n+    torch.float8_e5m2,\n+]\n+dtype_to_idx = {dtype: idx for idx, dtype in enumerate(torch_dtypes)}\n+\n+\n+def pack_metadata(shape, dtype, size) -> bytes:\n+    metadata_desc = \"<QQQQ\" + len(shape) * \"Q\"\n+    if struct.calcsize(metadata_desc) > _METADATA_MAX_SIZE:\n+        # TODO(Serapheim/Ilya): support variable offset for data\n+        raise ValueError(f\"Metadata size {struct.calcsize(metadata_desc)} \"\n+                         f\"exceeds max size {_METADATA_MAX_SIZE}\")\n+    return struct.pack(metadata_desc, _METADATA_VERSION, dtype_to_idx[dtype],\n+                       size, len(shape), *shape)\n+\n+\n+def unpack_metadata(buffer):\n+    version, dt_idx, size, ndim = struct.unpack_from(\"<QQQQ\", buffer)\n+    shape_offset = struct.calcsize(\"<QQQQ\")\n+    if version != _METADATA_VERSION:\n+        # TODO(Serapheim): When we bump the _METADATA_VERSION for\n+        # the first time, we need to ensure that we can still\n+        # read older versions.\n+        raise UnsupportedMetadataVersion(\n+            f\"Unsupported metadata version: {version}\")\n+    shape = struct.unpack_from(\"<\" + ndim * \"Q\", buffer, offset=shape_offset)\n+    return torch.Size(shape), torch_dtypes[dt_idx], size\n+\n+\n+def rand_suffix(rand, n: int):\n+    return ''.join(\n+        rand.choice(string.ascii_uppercase + string.digits) for _ in range(n))\n+\n+\n+async def save_metadata(path: str, tmp: str, metadata: bytes):\n+    tmp_path = path + tmp\n+    async with aiofile.async_open(tmp_path, \"wb\") as f:\n+        await f.write(metadata)\n+    os.rename(tmp_path, path)\n+\n+\n+@_lmcache_nvtx_annotate\n+@torch.inference_mode()\n+def save_gds_cufile(\n+    path: str,\n+    tmp: str,\n+    kv_chunk: torch.Tensor,\n+    base_pointer: int,\n+    device_offset: int,\n+):\n+    if base_pointer is None:\n+        addr = ctypes.c_void_p(kv_chunk.data_ptr())\n+        dev_offset = 0\n+    else:\n+        addr = ctypes.c_void_p(base_pointer)\n+        dev_offset = device_offset\n+    tmp_path = path + tmp\n+    offset = _METADATA_MAX_SIZE\n+    metadata = pack_metadata(kv_chunk.shape, kv_chunk.dtype, kv_chunk.nbytes)\n+    try:\n+        with open(tmp_path, \"wb\") as f:\n+            f.write(metadata)\n+        with cufile.CuFile(tmp_path, \"r+\") as f:\n+            f.write(addr,\n+                    kv_chunk.nbytes,\n+                    file_offset=offset,\n+                    dev_offset=dev_offset)\n+    except Exception as e:\n+        logger.error(f\"Error saving {tmp_path}: {e}\", exc_info=True)\n+        raise e\n+    os.rename(tmp_path, path)\n+    return metadata\n+\n+\n+def load_gds_cufile(file_path: str, file_offset: int,\n+                    gpu_pointer: ctypes.c_void_p, size_in_bytes: int,\n+                    dev_offset: int) -> int:\n+    # Read data from disk into a GPU buffer\n+    with cufile.CuFile(file_path, \"r\") as f:\n+        return f.read(gpu_pointer,\n+                      size_in_bytes,\n+                      file_offset=file_offset,\n+                      dev_offset=dev_offset)\n+\n+\n+class WekaGdsBackend(StorageBackendInterface):\n+    \"\"\"\n+    This is a backend that leverages NVIDIA's cuFile API to issue GDS requests\n+    directly to the Weka Filesystem.  In order to use it, users need to specify\n+    `weka_path` and `cufile_buffer_size` in their LMCache config.\n+\n+    Cache Directory Structure created by this Backend:\n+    /{weka_path}/{first_level}/{second_level}/{data & metadata}\n+    This structure is semi-arbitrary. WekaFS can handle/scale many small files\n+    into a single directory so we could just put all the data/metadata directly\n+    under the weka_path, but we create two levels in the directory hierarchy to\n+    parallelize loading the data during initialization in the Python code.\n+\n+    NOTE: The `weka_path` does not strictly need to be a WekaFS mount so if you\n+    want to test the backend without Weka you are free to do so for testing\n+    purposes. For production though it wouldn't scale as this backend is\n+    tailored to the performance characteristics of WekaFS. More specifically if\n+    used with non-Weka filesystems performance will suffer potentially for two\n+    reasons:\n+    (1) If GPUDirect is not supported on that other filesystem, then CuFile will\n+        fall back to POSIX I/O.\n+    (2) Our cache directory structure creates a lot of small files within a\n+        single directory and uses 4K block/buffer sizes. These align very well\n+        with Weka but not other filesystems.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 config: LMCacheEngineConfig,\n+                 loop: asyncio.AbstractEventLoop,\n+                 memory_allocator: MemoryAllocatorInterface,\n+                 dst_device: str = \"cuda\"):\n+        assert dst_device.startswith(\"cuda\")\n+        super().__init__(dst_device)\n+\n+        self.config = config\n+        self.loop = loop\n+        self.memory_allocator = memory_allocator\n+        self.dst_device = dst_device\n+\n+        assert config.weka_path is not None, (\n+            \"Need to specify weka_path for WekaGdsBackend\")\n+        self.weka_path = config.weka_path\n+        if not os.path.exists(self.weka_path):\n+            os.makedirs(self.weka_path, exist_ok=True)\n+\n+        self.use_thread_pool = False  # TODO(Serapheim): add thread pool paths\n+        self.stats = None  # TODO(Serapheim): plug into LMCache Statistics\n+\n+        self.hot_lock = threading.Lock()\n+        self.hot_cache: OrderedDict[CacheEngineKey,\n+                                    DiskCacheMetadata] = (OrderedDict())\n+        self.metadata_dirs: set[str] = set()\n+\n+        self.put_lock = threading.Lock()\n+        self.put_tasks: set[CacheEngineKey] = set()\n+\n+        self.rand = random.Random(self.dst_device)\n+\n+        self._cufile_driver = cufile.CuFileDriver()\n+        if hasattr(self.memory_allocator, \"base_pointer\"):\n+            logger.debug(\n+                f\"Using base pointer {self.memory_allocator.base_pointer}\")\n+            self.cufile_base_pointer = self.memory_allocator.base_pointer\n+        else:\n+            logger.info(\n+                \"No base pointer found, cufile will use bounce buffers\")\n+            self.cufile_base_pointer = None\n+        asyncio.run_coroutine_threadsafe(self._scan_metadata(), self.loop)\n+        self.save_metadata_tasks: set[asyncio.Task] = set()\n+\n+    async def _scan_metadata(self):\n+        # TODO(Serapheim): even though we only run it once on startup,\n+        # this is still not super scalable maybe we need to add metadata\n+        # snapshotting later.\n+        tasks = []\n+        start = time.perf_counter()\n+        with os.scandir(self.weka_path) as it:\n+            for entry in it:\n+                if not entry.is_dir():\n+                    continue\n+                l1_dir = os.path.basename(entry.name)\n+                if len(l1_dir) != 2:\n+                    continue\n+                tasks.append(\n+                    asyncio.to_thread(self._scan_metadata_subdir,\n+                                      os.path.join(self.weka_path, l1_dir),\n+                                      l1_dir))\n+        # TODO(Serapheim): If Python 3.11+, can we use TaskGroup instead?\n+        await asyncio.gather(*tasks)\n+        end = time.perf_counter()\n+        logger.info(\n+            f\"Read {len(self.hot_cache)} cache entries from persistent \"\n+            f\"storage in {end - start:.2f} seconds\")\n+\n+    def _scan_metadata_subdir(self, path, l1_dir):\n+        target_suffix = _DATA_FILE_SUFFIX + _METADATA_FILE_SUFFIX\n+        with os.scandir(path) as it:\n+            for entry in it:\n+                if not entry.is_dir():\n+                    continue\n+                l2_dir = os.path.basename(entry.name)\n+                if len(l2_dir) != 2:\n+                    continue\n+                with os.scandir(os.path.join(path, l2_dir)) as it2:\n+                    for fentry in it2:\n+                        if not fentry.is_file():\n+                            continue\n+                        if not fentry.name.endswith(target_suffix):\n+                            continue\n+                        filename = os.path.basename(fentry.name)\n+                        key_str = filename[:-14].replace('_', '/')\n+                        try:\n+                            key = CacheEngineKey.from_string(key_str)\n+                        except ValueError as e:\n+                            logger.error(\n+                                f\"Filename {filename} can't be converted \"\n+                                f\"back into cache key: {e}\")\n+                            continue\n+                        try:\n+                            self._read_metadata(key, fentry.path,\n+                                                l1_dir + l2_dir)\n+                        except UnsupportedMetadataVersion:\n+                            logger.error(\"Unsupported metadata version for \"\n+                                         f\"{fentry.path}, ignoring\")\n+\n+    def _read_metadata(self, key, filename, subdir_key):\n+        with open(filename, 'rb') as f:\n+            buf = f.read(_METADATA_MAX_SIZE)\n+        shape, dtype, size = unpack_metadata(buf)\n+        metadata = DiskCacheMetadata(\n+            filename.removesuffix(_METADATA_FILE_SUFFIX), size, shape, dtype)\n+        with self.hot_lock:\n+            self.metadata_dirs.add(subdir_key)\n+            self.hot_cache[key] = metadata\n+        return metadata\n+\n+    def __str__(self):\n+        return self.__class__.__name__\n+\n+    def contains(self, key: CacheEngineKey, pin: bool = False) -> bool:\n+        # TODO(Serapheim): implement pin() semantics\n+        with self.hot_lock:\n+            res = key in self.hot_cache\n+        if res:\n+            return True\n+        if self._try_to_read_metadata(key):",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2105873863",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 699,
        "pr_file": "lmcache/experimental/storage_backend/weka_gds_backend.py",
        "discussion_id": "2105873863",
        "commented_code": "@@ -0,0 +1,480 @@\n+# Copyright 2025 Ilya Yanok, Serapheim Dimitropoulos.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import asyncio\n+import ctypes\n+import os\n+import random\n+import string\n+import struct\n+import threading\n+import time\n+from collections import OrderedDict\n+from concurrent.futures import Future\n+from typing import Optional\n+\n+import aiofile\n+import cufile\n+import torch\n+\n+from lmcache.experimental.config import LMCacheEngineConfig\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj)\n+from lmcache.experimental.storage_backend.abstract_backend import \\\n+    StorageBackendInterface\n+from lmcache.logging import init_logger\n+from lmcache.utils import (CacheEngineKey, DiskCacheMetadata,\n+                           _lmcache_nvtx_annotate)\n+\n+logger = init_logger(__name__)\n+\n+_METADATA_FILE_SUFFIX = \".metadata\"\n+_DATA_FILE_SUFFIX = \".weka1\"\n+_METADATA_VERSION = 1\n+_METADATA_MAX_SIZE = 4096  # reserve 4K for metadata\n+\n+\n+class UnsupportedMetadataVersion(Exception):\n+    pass\n+\n+\n+torch_dtypes = [\n+    torch.half,\n+    torch.float16,\n+    torch.bfloat16,\n+    torch.float,\n+    torch.float32,\n+    torch.float64,\n+    torch.double,\n+    torch.uint8,\n+    torch.float8_e4m3fn,\n+    torch.float8_e5m2,\n+]\n+dtype_to_idx = {dtype: idx for idx, dtype in enumerate(torch_dtypes)}\n+\n+\n+def pack_metadata(shape, dtype, size) -> bytes:\n+    metadata_desc = \"<QQQQ\" + len(shape) * \"Q\"\n+    if struct.calcsize(metadata_desc) > _METADATA_MAX_SIZE:\n+        # TODO(Serapheim/Ilya): support variable offset for data\n+        raise ValueError(f\"Metadata size {struct.calcsize(metadata_desc)} \"\n+                         f\"exceeds max size {_METADATA_MAX_SIZE}\")\n+    return struct.pack(metadata_desc, _METADATA_VERSION, dtype_to_idx[dtype],\n+                       size, len(shape), *shape)\n+\n+\n+def unpack_metadata(buffer):\n+    version, dt_idx, size, ndim = struct.unpack_from(\"<QQQQ\", buffer)\n+    shape_offset = struct.calcsize(\"<QQQQ\")\n+    if version != _METADATA_VERSION:\n+        # TODO(Serapheim): When we bump the _METADATA_VERSION for\n+        # the first time, we need to ensure that we can still\n+        # read older versions.\n+        raise UnsupportedMetadataVersion(\n+            f\"Unsupported metadata version: {version}\")\n+    shape = struct.unpack_from(\"<\" + ndim * \"Q\", buffer, offset=shape_offset)\n+    return torch.Size(shape), torch_dtypes[dt_idx], size\n+\n+\n+def rand_suffix(rand, n: int):\n+    return ''.join(\n+        rand.choice(string.ascii_uppercase + string.digits) for _ in range(n))\n+\n+\n+async def save_metadata(path: str, tmp: str, metadata: bytes):\n+    tmp_path = path + tmp\n+    async with aiofile.async_open(tmp_path, \"wb\") as f:\n+        await f.write(metadata)\n+    os.rename(tmp_path, path)\n+\n+\n+@_lmcache_nvtx_annotate\n+@torch.inference_mode()\n+def save_gds_cufile(\n+    path: str,\n+    tmp: str,\n+    kv_chunk: torch.Tensor,\n+    base_pointer: int,\n+    device_offset: int,\n+):\n+    if base_pointer is None:\n+        addr = ctypes.c_void_p(kv_chunk.data_ptr())\n+        dev_offset = 0\n+    else:\n+        addr = ctypes.c_void_p(base_pointer)\n+        dev_offset = device_offset\n+    tmp_path = path + tmp\n+    offset = _METADATA_MAX_SIZE\n+    metadata = pack_metadata(kv_chunk.shape, kv_chunk.dtype, kv_chunk.nbytes)\n+    try:\n+        with open(tmp_path, \"wb\") as f:\n+            f.write(metadata)\n+        with cufile.CuFile(tmp_path, \"r+\") as f:\n+            f.write(addr,\n+                    kv_chunk.nbytes,\n+                    file_offset=offset,\n+                    dev_offset=dev_offset)\n+    except Exception as e:\n+        logger.error(f\"Error saving {tmp_path}: {e}\", exc_info=True)\n+        raise e\n+    os.rename(tmp_path, path)\n+    return metadata\n+\n+\n+def load_gds_cufile(file_path: str, file_offset: int,\n+                    gpu_pointer: ctypes.c_void_p, size_in_bytes: int,\n+                    dev_offset: int) -> int:\n+    # Read data from disk into a GPU buffer\n+    with cufile.CuFile(file_path, \"r\") as f:\n+        return f.read(gpu_pointer,\n+                      size_in_bytes,\n+                      file_offset=file_offset,\n+                      dev_offset=dev_offset)\n+\n+\n+class WekaGdsBackend(StorageBackendInterface):\n+    \"\"\"\n+    This is a backend that leverages NVIDIA's cuFile API to issue GDS requests\n+    directly to the Weka Filesystem.  In order to use it, users need to specify\n+    `weka_path` and `cufile_buffer_size` in their LMCache config.\n+\n+    Cache Directory Structure created by this Backend:\n+    /{weka_path}/{first_level}/{second_level}/{data & metadata}\n+    This structure is semi-arbitrary. WekaFS can handle/scale many small files\n+    into a single directory so we could just put all the data/metadata directly\n+    under the weka_path, but we create two levels in the directory hierarchy to\n+    parallelize loading the data during initialization in the Python code.\n+\n+    NOTE: The `weka_path` does not strictly need to be a WekaFS mount so if you\n+    want to test the backend without Weka you are free to do so for testing\n+    purposes. For production though it wouldn't scale as this backend is\n+    tailored to the performance characteristics of WekaFS. More specifically if\n+    used with non-Weka filesystems performance will suffer potentially for two\n+    reasons:\n+    (1) If GPUDirect is not supported on that other filesystem, then CuFile will\n+        fall back to POSIX I/O.\n+    (2) Our cache directory structure creates a lot of small files within a\n+        single directory and uses 4K block/buffer sizes. These align very well\n+        with Weka but not other filesystems.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 config: LMCacheEngineConfig,\n+                 loop: asyncio.AbstractEventLoop,\n+                 memory_allocator: MemoryAllocatorInterface,\n+                 dst_device: str = \"cuda\"):\n+        assert dst_device.startswith(\"cuda\")\n+        super().__init__(dst_device)\n+\n+        self.config = config\n+        self.loop = loop\n+        self.memory_allocator = memory_allocator\n+        self.dst_device = dst_device\n+\n+        assert config.weka_path is not None, (\n+            \"Need to specify weka_path for WekaGdsBackend\")\n+        self.weka_path = config.weka_path\n+        if not os.path.exists(self.weka_path):\n+            os.makedirs(self.weka_path, exist_ok=True)\n+\n+        self.use_thread_pool = False  # TODO(Serapheim): add thread pool paths\n+        self.stats = None  # TODO(Serapheim): plug into LMCache Statistics\n+\n+        self.hot_lock = threading.Lock()\n+        self.hot_cache: OrderedDict[CacheEngineKey,\n+                                    DiskCacheMetadata] = (OrderedDict())\n+        self.metadata_dirs: set[str] = set()\n+\n+        self.put_lock = threading.Lock()\n+        self.put_tasks: set[CacheEngineKey] = set()\n+\n+        self.rand = random.Random(self.dst_device)\n+\n+        self._cufile_driver = cufile.CuFileDriver()\n+        if hasattr(self.memory_allocator, \"base_pointer\"):\n+            logger.debug(\n+                f\"Using base pointer {self.memory_allocator.base_pointer}\")\n+            self.cufile_base_pointer = self.memory_allocator.base_pointer\n+        else:\n+            logger.info(\n+                \"No base pointer found, cufile will use bounce buffers\")\n+            self.cufile_base_pointer = None\n+        asyncio.run_coroutine_threadsafe(self._scan_metadata(), self.loop)\n+        self.save_metadata_tasks: set[asyncio.Task] = set()\n+\n+    async def _scan_metadata(self):\n+        # TODO(Serapheim): even though we only run it once on startup,\n+        # this is still not super scalable maybe we need to add metadata\n+        # snapshotting later.\n+        tasks = []\n+        start = time.perf_counter()\n+        with os.scandir(self.weka_path) as it:\n+            for entry in it:\n+                if not entry.is_dir():\n+                    continue\n+                l1_dir = os.path.basename(entry.name)\n+                if len(l1_dir) != 2:\n+                    continue\n+                tasks.append(\n+                    asyncio.to_thread(self._scan_metadata_subdir,\n+                                      os.path.join(self.weka_path, l1_dir),\n+                                      l1_dir))\n+        # TODO(Serapheim): If Python 3.11+, can we use TaskGroup instead?\n+        await asyncio.gather(*tasks)\n+        end = time.perf_counter()\n+        logger.info(\n+            f\"Read {len(self.hot_cache)} cache entries from persistent \"\n+            f\"storage in {end - start:.2f} seconds\")\n+\n+    def _scan_metadata_subdir(self, path, l1_dir):\n+        target_suffix = _DATA_FILE_SUFFIX + _METADATA_FILE_SUFFIX\n+        with os.scandir(path) as it:\n+            for entry in it:\n+                if not entry.is_dir():\n+                    continue\n+                l2_dir = os.path.basename(entry.name)\n+                if len(l2_dir) != 2:\n+                    continue\n+                with os.scandir(os.path.join(path, l2_dir)) as it2:\n+                    for fentry in it2:\n+                        if not fentry.is_file():\n+                            continue\n+                        if not fentry.name.endswith(target_suffix):\n+                            continue\n+                        filename = os.path.basename(fentry.name)\n+                        key_str = filename[:-14].replace('_', '/')\n+                        try:\n+                            key = CacheEngineKey.from_string(key_str)\n+                        except ValueError as e:\n+                            logger.error(\n+                                f\"Filename {filename} can't be converted \"\n+                                f\"back into cache key: {e}\")\n+                            continue\n+                        try:\n+                            self._read_metadata(key, fentry.path,\n+                                                l1_dir + l2_dir)\n+                        except UnsupportedMetadataVersion:\n+                            logger.error(\"Unsupported metadata version for \"\n+                                         f\"{fentry.path}, ignoring\")\n+\n+    def _read_metadata(self, key, filename, subdir_key):\n+        with open(filename, 'rb') as f:\n+            buf = f.read(_METADATA_MAX_SIZE)\n+        shape, dtype, size = unpack_metadata(buf)\n+        metadata = DiskCacheMetadata(\n+            filename.removesuffix(_METADATA_FILE_SUFFIX), size, shape, dtype)\n+        with self.hot_lock:\n+            self.metadata_dirs.add(subdir_key)\n+            self.hot_cache[key] = metadata\n+        return metadata\n+\n+    def __str__(self):\n+        return self.__class__.__name__\n+\n+    def contains(self, key: CacheEngineKey, pin: bool = False) -> bool:\n+        # TODO(Serapheim): implement pin() semantics\n+        with self.hot_lock:\n+            res = key in self.hot_cache\n+        if res:\n+            return True\n+        if self._try_to_read_metadata(key):",
        "comment_created_at": "2025-05-24T16:26:02+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "When will metadata be on disk?",
        "pr_file_module": null
      },
      {
        "comment_id": "2107602749",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 699,
        "pr_file": "lmcache/experimental/storage_backend/weka_gds_backend.py",
        "discussion_id": "2105873863",
        "commented_code": "@@ -0,0 +1,480 @@\n+# Copyright 2025 Ilya Yanok, Serapheim Dimitropoulos.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import asyncio\n+import ctypes\n+import os\n+import random\n+import string\n+import struct\n+import threading\n+import time\n+from collections import OrderedDict\n+from concurrent.futures import Future\n+from typing import Optional\n+\n+import aiofile\n+import cufile\n+import torch\n+\n+from lmcache.experimental.config import LMCacheEngineConfig\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj)\n+from lmcache.experimental.storage_backend.abstract_backend import \\\n+    StorageBackendInterface\n+from lmcache.logging import init_logger\n+from lmcache.utils import (CacheEngineKey, DiskCacheMetadata,\n+                           _lmcache_nvtx_annotate)\n+\n+logger = init_logger(__name__)\n+\n+_METADATA_FILE_SUFFIX = \".metadata\"\n+_DATA_FILE_SUFFIX = \".weka1\"\n+_METADATA_VERSION = 1\n+_METADATA_MAX_SIZE = 4096  # reserve 4K for metadata\n+\n+\n+class UnsupportedMetadataVersion(Exception):\n+    pass\n+\n+\n+torch_dtypes = [\n+    torch.half,\n+    torch.float16,\n+    torch.bfloat16,\n+    torch.float,\n+    torch.float32,\n+    torch.float64,\n+    torch.double,\n+    torch.uint8,\n+    torch.float8_e4m3fn,\n+    torch.float8_e5m2,\n+]\n+dtype_to_idx = {dtype: idx for idx, dtype in enumerate(torch_dtypes)}\n+\n+\n+def pack_metadata(shape, dtype, size) -> bytes:\n+    metadata_desc = \"<QQQQ\" + len(shape) * \"Q\"\n+    if struct.calcsize(metadata_desc) > _METADATA_MAX_SIZE:\n+        # TODO(Serapheim/Ilya): support variable offset for data\n+        raise ValueError(f\"Metadata size {struct.calcsize(metadata_desc)} \"\n+                         f\"exceeds max size {_METADATA_MAX_SIZE}\")\n+    return struct.pack(metadata_desc, _METADATA_VERSION, dtype_to_idx[dtype],\n+                       size, len(shape), *shape)\n+\n+\n+def unpack_metadata(buffer):\n+    version, dt_idx, size, ndim = struct.unpack_from(\"<QQQQ\", buffer)\n+    shape_offset = struct.calcsize(\"<QQQQ\")\n+    if version != _METADATA_VERSION:\n+        # TODO(Serapheim): When we bump the _METADATA_VERSION for\n+        # the first time, we need to ensure that we can still\n+        # read older versions.\n+        raise UnsupportedMetadataVersion(\n+            f\"Unsupported metadata version: {version}\")\n+    shape = struct.unpack_from(\"<\" + ndim * \"Q\", buffer, offset=shape_offset)\n+    return torch.Size(shape), torch_dtypes[dt_idx], size\n+\n+\n+def rand_suffix(rand, n: int):\n+    return ''.join(\n+        rand.choice(string.ascii_uppercase + string.digits) for _ in range(n))\n+\n+\n+async def save_metadata(path: str, tmp: str, metadata: bytes):\n+    tmp_path = path + tmp\n+    async with aiofile.async_open(tmp_path, \"wb\") as f:\n+        await f.write(metadata)\n+    os.rename(tmp_path, path)\n+\n+\n+@_lmcache_nvtx_annotate\n+@torch.inference_mode()\n+def save_gds_cufile(\n+    path: str,\n+    tmp: str,\n+    kv_chunk: torch.Tensor,\n+    base_pointer: int,\n+    device_offset: int,\n+):\n+    if base_pointer is None:\n+        addr = ctypes.c_void_p(kv_chunk.data_ptr())\n+        dev_offset = 0\n+    else:\n+        addr = ctypes.c_void_p(base_pointer)\n+        dev_offset = device_offset\n+    tmp_path = path + tmp\n+    offset = _METADATA_MAX_SIZE\n+    metadata = pack_metadata(kv_chunk.shape, kv_chunk.dtype, kv_chunk.nbytes)\n+    try:\n+        with open(tmp_path, \"wb\") as f:\n+            f.write(metadata)\n+        with cufile.CuFile(tmp_path, \"r+\") as f:\n+            f.write(addr,\n+                    kv_chunk.nbytes,\n+                    file_offset=offset,\n+                    dev_offset=dev_offset)\n+    except Exception as e:\n+        logger.error(f\"Error saving {tmp_path}: {e}\", exc_info=True)\n+        raise e\n+    os.rename(tmp_path, path)\n+    return metadata\n+\n+\n+def load_gds_cufile(file_path: str, file_offset: int,\n+                    gpu_pointer: ctypes.c_void_p, size_in_bytes: int,\n+                    dev_offset: int) -> int:\n+    # Read data from disk into a GPU buffer\n+    with cufile.CuFile(file_path, \"r\") as f:\n+        return f.read(gpu_pointer,\n+                      size_in_bytes,\n+                      file_offset=file_offset,\n+                      dev_offset=dev_offset)\n+\n+\n+class WekaGdsBackend(StorageBackendInterface):\n+    \"\"\"\n+    This is a backend that leverages NVIDIA's cuFile API to issue GDS requests\n+    directly to the Weka Filesystem.  In order to use it, users need to specify\n+    `weka_path` and `cufile_buffer_size` in their LMCache config.\n+\n+    Cache Directory Structure created by this Backend:\n+    /{weka_path}/{first_level}/{second_level}/{data & metadata}\n+    This structure is semi-arbitrary. WekaFS can handle/scale many small files\n+    into a single directory so we could just put all the data/metadata directly\n+    under the weka_path, but we create two levels in the directory hierarchy to\n+    parallelize loading the data during initialization in the Python code.\n+\n+    NOTE: The `weka_path` does not strictly need to be a WekaFS mount so if you\n+    want to test the backend without Weka you are free to do so for testing\n+    purposes. For production though it wouldn't scale as this backend is\n+    tailored to the performance characteristics of WekaFS. More specifically if\n+    used with non-Weka filesystems performance will suffer potentially for two\n+    reasons:\n+    (1) If GPUDirect is not supported on that other filesystem, then CuFile will\n+        fall back to POSIX I/O.\n+    (2) Our cache directory structure creates a lot of small files within a\n+        single directory and uses 4K block/buffer sizes. These align very well\n+        with Weka but not other filesystems.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 config: LMCacheEngineConfig,\n+                 loop: asyncio.AbstractEventLoop,\n+                 memory_allocator: MemoryAllocatorInterface,\n+                 dst_device: str = \"cuda\"):\n+        assert dst_device.startswith(\"cuda\")\n+        super().__init__(dst_device)\n+\n+        self.config = config\n+        self.loop = loop\n+        self.memory_allocator = memory_allocator\n+        self.dst_device = dst_device\n+\n+        assert config.weka_path is not None, (\n+            \"Need to specify weka_path for WekaGdsBackend\")\n+        self.weka_path = config.weka_path\n+        if not os.path.exists(self.weka_path):\n+            os.makedirs(self.weka_path, exist_ok=True)\n+\n+        self.use_thread_pool = False  # TODO(Serapheim): add thread pool paths\n+        self.stats = None  # TODO(Serapheim): plug into LMCache Statistics\n+\n+        self.hot_lock = threading.Lock()\n+        self.hot_cache: OrderedDict[CacheEngineKey,\n+                                    DiskCacheMetadata] = (OrderedDict())\n+        self.metadata_dirs: set[str] = set()\n+\n+        self.put_lock = threading.Lock()\n+        self.put_tasks: set[CacheEngineKey] = set()\n+\n+        self.rand = random.Random(self.dst_device)\n+\n+        self._cufile_driver = cufile.CuFileDriver()\n+        if hasattr(self.memory_allocator, \"base_pointer\"):\n+            logger.debug(\n+                f\"Using base pointer {self.memory_allocator.base_pointer}\")\n+            self.cufile_base_pointer = self.memory_allocator.base_pointer\n+        else:\n+            logger.info(\n+                \"No base pointer found, cufile will use bounce buffers\")\n+            self.cufile_base_pointer = None\n+        asyncio.run_coroutine_threadsafe(self._scan_metadata(), self.loop)\n+        self.save_metadata_tasks: set[asyncio.Task] = set()\n+\n+    async def _scan_metadata(self):\n+        # TODO(Serapheim): even though we only run it once on startup,\n+        # this is still not super scalable maybe we need to add metadata\n+        # snapshotting later.\n+        tasks = []\n+        start = time.perf_counter()\n+        with os.scandir(self.weka_path) as it:\n+            for entry in it:\n+                if not entry.is_dir():\n+                    continue\n+                l1_dir = os.path.basename(entry.name)\n+                if len(l1_dir) != 2:\n+                    continue\n+                tasks.append(\n+                    asyncio.to_thread(self._scan_metadata_subdir,\n+                                      os.path.join(self.weka_path, l1_dir),\n+                                      l1_dir))\n+        # TODO(Serapheim): If Python 3.11+, can we use TaskGroup instead?\n+        await asyncio.gather(*tasks)\n+        end = time.perf_counter()\n+        logger.info(\n+            f\"Read {len(self.hot_cache)} cache entries from persistent \"\n+            f\"storage in {end - start:.2f} seconds\")\n+\n+    def _scan_metadata_subdir(self, path, l1_dir):\n+        target_suffix = _DATA_FILE_SUFFIX + _METADATA_FILE_SUFFIX\n+        with os.scandir(path) as it:\n+            for entry in it:\n+                if not entry.is_dir():\n+                    continue\n+                l2_dir = os.path.basename(entry.name)\n+                if len(l2_dir) != 2:\n+                    continue\n+                with os.scandir(os.path.join(path, l2_dir)) as it2:\n+                    for fentry in it2:\n+                        if not fentry.is_file():\n+                            continue\n+                        if not fentry.name.endswith(target_suffix):\n+                            continue\n+                        filename = os.path.basename(fentry.name)\n+                        key_str = filename[:-14].replace('_', '/')\n+                        try:\n+                            key = CacheEngineKey.from_string(key_str)\n+                        except ValueError as e:\n+                            logger.error(\n+                                f\"Filename {filename} can't be converted \"\n+                                f\"back into cache key: {e}\")\n+                            continue\n+                        try:\n+                            self._read_metadata(key, fentry.path,\n+                                                l1_dir + l2_dir)\n+                        except UnsupportedMetadataVersion:\n+                            logger.error(\"Unsupported metadata version for \"\n+                                         f\"{fentry.path}, ignoring\")\n+\n+    def _read_metadata(self, key, filename, subdir_key):\n+        with open(filename, 'rb') as f:\n+            buf = f.read(_METADATA_MAX_SIZE)\n+        shape, dtype, size = unpack_metadata(buf)\n+        metadata = DiskCacheMetadata(\n+            filename.removesuffix(_METADATA_FILE_SUFFIX), size, shape, dtype)\n+        with self.hot_lock:\n+            self.metadata_dirs.add(subdir_key)\n+            self.hot_cache[key] = metadata\n+        return metadata\n+\n+    def __str__(self):\n+        return self.__class__.__name__\n+\n+    def contains(self, key: CacheEngineKey, pin: bool = False) -> bool:\n+        # TODO(Serapheim): implement pin() semantics\n+        with self.hot_lock:\n+            res = key in self.hot_cache\n+        if res:\n+            return True\n+        if self._try_to_read_metadata(key):",
        "comment_created_at": "2025-05-26T15:59:29+00:00",
        "comment_author": "sdimitro",
        "comment_body": "Every time we write something on disk we write its metadata too. For every write we also add it to the `hot_cache`.\r\n\r\nThat said given that Weka is a distributed filesystem, some other host/node could have added the key that we are looking for (and thus the key is not in our `hot_cache`) so we need to check on the filesystem with `_try_to_read_metadata`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2107940276",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 699,
        "pr_file": "lmcache/experimental/storage_backend/weka_gds_backend.py",
        "discussion_id": "2105873863",
        "commented_code": "@@ -0,0 +1,480 @@\n+# Copyright 2025 Ilya Yanok, Serapheim Dimitropoulos.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import asyncio\n+import ctypes\n+import os\n+import random\n+import string\n+import struct\n+import threading\n+import time\n+from collections import OrderedDict\n+from concurrent.futures import Future\n+from typing import Optional\n+\n+import aiofile\n+import cufile\n+import torch\n+\n+from lmcache.experimental.config import LMCacheEngineConfig\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj)\n+from lmcache.experimental.storage_backend.abstract_backend import \\\n+    StorageBackendInterface\n+from lmcache.logging import init_logger\n+from lmcache.utils import (CacheEngineKey, DiskCacheMetadata,\n+                           _lmcache_nvtx_annotate)\n+\n+logger = init_logger(__name__)\n+\n+_METADATA_FILE_SUFFIX = \".metadata\"\n+_DATA_FILE_SUFFIX = \".weka1\"\n+_METADATA_VERSION = 1\n+_METADATA_MAX_SIZE = 4096  # reserve 4K for metadata\n+\n+\n+class UnsupportedMetadataVersion(Exception):\n+    pass\n+\n+\n+torch_dtypes = [\n+    torch.half,\n+    torch.float16,\n+    torch.bfloat16,\n+    torch.float,\n+    torch.float32,\n+    torch.float64,\n+    torch.double,\n+    torch.uint8,\n+    torch.float8_e4m3fn,\n+    torch.float8_e5m2,\n+]\n+dtype_to_idx = {dtype: idx for idx, dtype in enumerate(torch_dtypes)}\n+\n+\n+def pack_metadata(shape, dtype, size) -> bytes:\n+    metadata_desc = \"<QQQQ\" + len(shape) * \"Q\"\n+    if struct.calcsize(metadata_desc) > _METADATA_MAX_SIZE:\n+        # TODO(Serapheim/Ilya): support variable offset for data\n+        raise ValueError(f\"Metadata size {struct.calcsize(metadata_desc)} \"\n+                         f\"exceeds max size {_METADATA_MAX_SIZE}\")\n+    return struct.pack(metadata_desc, _METADATA_VERSION, dtype_to_idx[dtype],\n+                       size, len(shape), *shape)\n+\n+\n+def unpack_metadata(buffer):\n+    version, dt_idx, size, ndim = struct.unpack_from(\"<QQQQ\", buffer)\n+    shape_offset = struct.calcsize(\"<QQQQ\")\n+    if version != _METADATA_VERSION:\n+        # TODO(Serapheim): When we bump the _METADATA_VERSION for\n+        # the first time, we need to ensure that we can still\n+        # read older versions.\n+        raise UnsupportedMetadataVersion(\n+            f\"Unsupported metadata version: {version}\")\n+    shape = struct.unpack_from(\"<\" + ndim * \"Q\", buffer, offset=shape_offset)\n+    return torch.Size(shape), torch_dtypes[dt_idx], size\n+\n+\n+def rand_suffix(rand, n: int):\n+    return ''.join(\n+        rand.choice(string.ascii_uppercase + string.digits) for _ in range(n))\n+\n+\n+async def save_metadata(path: str, tmp: str, metadata: bytes):\n+    tmp_path = path + tmp\n+    async with aiofile.async_open(tmp_path, \"wb\") as f:\n+        await f.write(metadata)\n+    os.rename(tmp_path, path)\n+\n+\n+@_lmcache_nvtx_annotate\n+@torch.inference_mode()\n+def save_gds_cufile(\n+    path: str,\n+    tmp: str,\n+    kv_chunk: torch.Tensor,\n+    base_pointer: int,\n+    device_offset: int,\n+):\n+    if base_pointer is None:\n+        addr = ctypes.c_void_p(kv_chunk.data_ptr())\n+        dev_offset = 0\n+    else:\n+        addr = ctypes.c_void_p(base_pointer)\n+        dev_offset = device_offset\n+    tmp_path = path + tmp\n+    offset = _METADATA_MAX_SIZE\n+    metadata = pack_metadata(kv_chunk.shape, kv_chunk.dtype, kv_chunk.nbytes)\n+    try:\n+        with open(tmp_path, \"wb\") as f:\n+            f.write(metadata)\n+        with cufile.CuFile(tmp_path, \"r+\") as f:\n+            f.write(addr,\n+                    kv_chunk.nbytes,\n+                    file_offset=offset,\n+                    dev_offset=dev_offset)\n+    except Exception as e:\n+        logger.error(f\"Error saving {tmp_path}: {e}\", exc_info=True)\n+        raise e\n+    os.rename(tmp_path, path)\n+    return metadata\n+\n+\n+def load_gds_cufile(file_path: str, file_offset: int,\n+                    gpu_pointer: ctypes.c_void_p, size_in_bytes: int,\n+                    dev_offset: int) -> int:\n+    # Read data from disk into a GPU buffer\n+    with cufile.CuFile(file_path, \"r\") as f:\n+        return f.read(gpu_pointer,\n+                      size_in_bytes,\n+                      file_offset=file_offset,\n+                      dev_offset=dev_offset)\n+\n+\n+class WekaGdsBackend(StorageBackendInterface):\n+    \"\"\"\n+    This is a backend that leverages NVIDIA's cuFile API to issue GDS requests\n+    directly to the Weka Filesystem.  In order to use it, users need to specify\n+    `weka_path` and `cufile_buffer_size` in their LMCache config.\n+\n+    Cache Directory Structure created by this Backend:\n+    /{weka_path}/{first_level}/{second_level}/{data & metadata}\n+    This structure is semi-arbitrary. WekaFS can handle/scale many small files\n+    into a single directory so we could just put all the data/metadata directly\n+    under the weka_path, but we create two levels in the directory hierarchy to\n+    parallelize loading the data during initialization in the Python code.\n+\n+    NOTE: The `weka_path` does not strictly need to be a WekaFS mount so if you\n+    want to test the backend without Weka you are free to do so for testing\n+    purposes. For production though it wouldn't scale as this backend is\n+    tailored to the performance characteristics of WekaFS. More specifically if\n+    used with non-Weka filesystems performance will suffer potentially for two\n+    reasons:\n+    (1) If GPUDirect is not supported on that other filesystem, then CuFile will\n+        fall back to POSIX I/O.\n+    (2) Our cache directory structure creates a lot of small files within a\n+        single directory and uses 4K block/buffer sizes. These align very well\n+        with Weka but not other filesystems.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 config: LMCacheEngineConfig,\n+                 loop: asyncio.AbstractEventLoop,\n+                 memory_allocator: MemoryAllocatorInterface,\n+                 dst_device: str = \"cuda\"):\n+        assert dst_device.startswith(\"cuda\")\n+        super().__init__(dst_device)\n+\n+        self.config = config\n+        self.loop = loop\n+        self.memory_allocator = memory_allocator\n+        self.dst_device = dst_device\n+\n+        assert config.weka_path is not None, (\n+            \"Need to specify weka_path for WekaGdsBackend\")\n+        self.weka_path = config.weka_path\n+        if not os.path.exists(self.weka_path):\n+            os.makedirs(self.weka_path, exist_ok=True)\n+\n+        self.use_thread_pool = False  # TODO(Serapheim): add thread pool paths\n+        self.stats = None  # TODO(Serapheim): plug into LMCache Statistics\n+\n+        self.hot_lock = threading.Lock()\n+        self.hot_cache: OrderedDict[CacheEngineKey,\n+                                    DiskCacheMetadata] = (OrderedDict())\n+        self.metadata_dirs: set[str] = set()\n+\n+        self.put_lock = threading.Lock()\n+        self.put_tasks: set[CacheEngineKey] = set()\n+\n+        self.rand = random.Random(self.dst_device)\n+\n+        self._cufile_driver = cufile.CuFileDriver()\n+        if hasattr(self.memory_allocator, \"base_pointer\"):\n+            logger.debug(\n+                f\"Using base pointer {self.memory_allocator.base_pointer}\")\n+            self.cufile_base_pointer = self.memory_allocator.base_pointer\n+        else:\n+            logger.info(\n+                \"No base pointer found, cufile will use bounce buffers\")\n+            self.cufile_base_pointer = None\n+        asyncio.run_coroutine_threadsafe(self._scan_metadata(), self.loop)\n+        self.save_metadata_tasks: set[asyncio.Task] = set()\n+\n+    async def _scan_metadata(self):\n+        # TODO(Serapheim): even though we only run it once on startup,\n+        # this is still not super scalable maybe we need to add metadata\n+        # snapshotting later.\n+        tasks = []\n+        start = time.perf_counter()\n+        with os.scandir(self.weka_path) as it:\n+            for entry in it:\n+                if not entry.is_dir():\n+                    continue\n+                l1_dir = os.path.basename(entry.name)\n+                if len(l1_dir) != 2:\n+                    continue\n+                tasks.append(\n+                    asyncio.to_thread(self._scan_metadata_subdir,\n+                                      os.path.join(self.weka_path, l1_dir),\n+                                      l1_dir))\n+        # TODO(Serapheim): If Python 3.11+, can we use TaskGroup instead?\n+        await asyncio.gather(*tasks)\n+        end = time.perf_counter()\n+        logger.info(\n+            f\"Read {len(self.hot_cache)} cache entries from persistent \"\n+            f\"storage in {end - start:.2f} seconds\")\n+\n+    def _scan_metadata_subdir(self, path, l1_dir):\n+        target_suffix = _DATA_FILE_SUFFIX + _METADATA_FILE_SUFFIX\n+        with os.scandir(path) as it:\n+            for entry in it:\n+                if not entry.is_dir():\n+                    continue\n+                l2_dir = os.path.basename(entry.name)\n+                if len(l2_dir) != 2:\n+                    continue\n+                with os.scandir(os.path.join(path, l2_dir)) as it2:\n+                    for fentry in it2:\n+                        if not fentry.is_file():\n+                            continue\n+                        if not fentry.name.endswith(target_suffix):\n+                            continue\n+                        filename = os.path.basename(fentry.name)\n+                        key_str = filename[:-14].replace('_', '/')\n+                        try:\n+                            key = CacheEngineKey.from_string(key_str)\n+                        except ValueError as e:\n+                            logger.error(\n+                                f\"Filename {filename} can't be converted \"\n+                                f\"back into cache key: {e}\")\n+                            continue\n+                        try:\n+                            self._read_metadata(key, fentry.path,\n+                                                l1_dir + l2_dir)\n+                        except UnsupportedMetadataVersion:\n+                            logger.error(\"Unsupported metadata version for \"\n+                                         f\"{fentry.path}, ignoring\")\n+\n+    def _read_metadata(self, key, filename, subdir_key):\n+        with open(filename, 'rb') as f:\n+            buf = f.read(_METADATA_MAX_SIZE)\n+        shape, dtype, size = unpack_metadata(buf)\n+        metadata = DiskCacheMetadata(\n+            filename.removesuffix(_METADATA_FILE_SUFFIX), size, shape, dtype)\n+        with self.hot_lock:\n+            self.metadata_dirs.add(subdir_key)\n+            self.hot_cache[key] = metadata\n+        return metadata\n+\n+    def __str__(self):\n+        return self.__class__.__name__\n+\n+    def contains(self, key: CacheEngineKey, pin: bool = False) -> bool:\n+        # TODO(Serapheim): implement pin() semantics\n+        with self.hot_lock:\n+            res = key in self.hot_cache\n+        if res:\n+            return True\n+        if self._try_to_read_metadata(key):",
        "comment_created_at": "2025-05-27T00:19:52+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "Got it:)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2085233966",
    "pr_number": 625,
    "pr_file": "lmcache/experimental/cache_engine.py",
    "created_at": "2025-05-12T18:32:04+00:00",
    "commented_code": "logger.info(\"LMCacheEngine closed.\")\n \n \n+# TODO(Jiayi): Using a separate class here.\n+# Should use the same class once the code is stable.\n+class LayerwiseLMCacheEngine(LMCacheEngine):\n+    \"\"\"A specialized LMCacheEngine for layerwise cache engine.\n+    \n+    This class is used to store the layerwise cache engine. It is a\n+    subclass of LMCacheEngine and inherits all the methods and attributes\n+    from it. The only difference is that it uses a different token database\n+    and memory allocator.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: LMCacheEngineConfig,\n+        metadata: LMCacheEngineMetadata,\n+        memory_allocator: MemoryAllocatorInterface,\n+        token_database: TokenDatabase,\n+        layerwise_gpu_connector: GPUConnectorInterface,\n+    ):\n+        super().__init__(config, metadata, memory_allocator, token_database,\n+                         layerwise_gpu_connector)\n+        assert isinstance(self.gpu_connector,\n+                          VLLMPagedMemLayerwiseGPUConnector)\n+\n+        self.num_layers = metadata.kv_shape[0]\n+\n+    @_lmcache_nvtx_annotate\n+    @torch.inference_mode()\n+    def store_layer(self,\n+                    tokens: torch.Tensor,\n+                    mask: Optional[torch.Tensor] = None,\n+                    **kwargs) -> Generator[None, None, None]:\n+        \"\"\"\n+        Store the KV cache in a layerwise manner.\n+        \"\"\"\n+\n+        if mask is not None:\n+            num_stored_tokens = torch.sum(mask).item()\n+        else:\n+            num_stored_tokens = len(tokens)\n+        monitor_req_id = self.stats_monitor.on_store_request(num_stored_tokens)\n+\n+        starts = []\n+        ends = []\n+        keys = []\n+        memory_objs = []\n+        kv_dtype = self.metadata.kv_dtype\n+        for start, end, key in self.token_database.process_tokens(\n+                tokens, mask):\n+            assert isinstance(key, CacheEngineKey)\n+\n+            keys_multi_layer = key.split_layers(self.num_layers)\n+\n+            # Only check the first layer\n+            if self.storage_manager.contains(keys_multi_layer[0]):",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2085233966",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 625,
        "pr_file": "lmcache/experimental/cache_engine.py",
        "discussion_id": "2085233966",
        "commented_code": "@@ -422,6 +424,244 @@ def close(self) -> None:\n         logger.info(\"LMCacheEngine closed.\")\n \n \n+# TODO(Jiayi): Using a separate class here.\n+# Should use the same class once the code is stable.\n+class LayerwiseLMCacheEngine(LMCacheEngine):\n+    \"\"\"A specialized LMCacheEngine for layerwise cache engine.\n+    \n+    This class is used to store the layerwise cache engine. It is a\n+    subclass of LMCacheEngine and inherits all the methods and attributes\n+    from it. The only difference is that it uses a different token database\n+    and memory allocator.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: LMCacheEngineConfig,\n+        metadata: LMCacheEngineMetadata,\n+        memory_allocator: MemoryAllocatorInterface,\n+        token_database: TokenDatabase,\n+        layerwise_gpu_connector: GPUConnectorInterface,\n+    ):\n+        super().__init__(config, metadata, memory_allocator, token_database,\n+                         layerwise_gpu_connector)\n+        assert isinstance(self.gpu_connector,\n+                          VLLMPagedMemLayerwiseGPUConnector)\n+\n+        self.num_layers = metadata.kv_shape[0]\n+\n+    @_lmcache_nvtx_annotate\n+    @torch.inference_mode()\n+    def store_layer(self,\n+                    tokens: torch.Tensor,\n+                    mask: Optional[torch.Tensor] = None,\n+                    **kwargs) -> Generator[None, None, None]:\n+        \"\"\"\n+        Store the KV cache in a layerwise manner.\n+        \"\"\"\n+\n+        if mask is not None:\n+            num_stored_tokens = torch.sum(mask).item()\n+        else:\n+            num_stored_tokens = len(tokens)\n+        monitor_req_id = self.stats_monitor.on_store_request(num_stored_tokens)\n+\n+        starts = []\n+        ends = []\n+        keys = []\n+        memory_objs = []\n+        kv_dtype = self.metadata.kv_dtype\n+        for start, end, key in self.token_database.process_tokens(\n+                tokens, mask):\n+            assert isinstance(key, CacheEngineKey)\n+\n+            keys_multi_layer = key.split_layers(self.num_layers)\n+\n+            # Only check the first layer\n+            if self.storage_manager.contains(keys_multi_layer[0]):",
        "comment_created_at": "2025-05-12T18:32:04+00:00",
        "comment_author": "ApostaC",
        "comment_body": "(Not directly related to this PR) Can you remind me how the storage manager deals with the keys that already exist in the put tasks? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2085633712",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 625,
        "pr_file": "lmcache/experimental/cache_engine.py",
        "discussion_id": "2085233966",
        "commented_code": "@@ -422,6 +424,244 @@ def close(self) -> None:\n         logger.info(\"LMCacheEngine closed.\")\n \n \n+# TODO(Jiayi): Using a separate class here.\n+# Should use the same class once the code is stable.\n+class LayerwiseLMCacheEngine(LMCacheEngine):\n+    \"\"\"A specialized LMCacheEngine for layerwise cache engine.\n+    \n+    This class is used to store the layerwise cache engine. It is a\n+    subclass of LMCacheEngine and inherits all the methods and attributes\n+    from it. The only difference is that it uses a different token database\n+    and memory allocator.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: LMCacheEngineConfig,\n+        metadata: LMCacheEngineMetadata,\n+        memory_allocator: MemoryAllocatorInterface,\n+        token_database: TokenDatabase,\n+        layerwise_gpu_connector: GPUConnectorInterface,\n+    ):\n+        super().__init__(config, metadata, memory_allocator, token_database,\n+                         layerwise_gpu_connector)\n+        assert isinstance(self.gpu_connector,\n+                          VLLMPagedMemLayerwiseGPUConnector)\n+\n+        self.num_layers = metadata.kv_shape[0]\n+\n+    @_lmcache_nvtx_annotate\n+    @torch.inference_mode()\n+    def store_layer(self,\n+                    tokens: torch.Tensor,\n+                    mask: Optional[torch.Tensor] = None,\n+                    **kwargs) -> Generator[None, None, None]:\n+        \"\"\"\n+        Store the KV cache in a layerwise manner.\n+        \"\"\"\n+\n+        if mask is not None:\n+            num_stored_tokens = torch.sum(mask).item()\n+        else:\n+            num_stored_tokens = len(tokens)\n+        monitor_req_id = self.stats_monitor.on_store_request(num_stored_tokens)\n+\n+        starts = []\n+        ends = []\n+        keys = []\n+        memory_objs = []\n+        kv_dtype = self.metadata.kv_dtype\n+        for start, end, key in self.token_database.process_tokens(\n+                tokens, mask):\n+            assert isinstance(key, CacheEngineKey)\n+\n+            keys_multi_layer = key.split_layers(self.num_layers)\n+\n+            # Only check the first layer\n+            if self.storage_manager.contains(keys_multi_layer[0]):",
        "comment_created_at": "2025-05-12T22:42:09+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "`contains` does not check put tasks but `store` check put tasks so that the same KV cache is not stored repeatedly",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2053003704",
    "pr_number": 482,
    "pr_file": "lmcache/experimental/storage_backend/connector/valkey_connector.py",
    "created_at": "2025-04-21T21:14:25+00:00",
    "commented_code": "+# Copyright 2024-2025 LMCache Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import asyncio\n+import inspect\n+import os\n+from typing import List, Optional, Tuple, Union, no_type_check\n+\n+import valkey\n+from valkey import Valkey\n+\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj)\n+from lmcache.experimental.protocol import METADATA_BYTES_LEN, RedisMetadata\n+from lmcache.experimental.storage_backend.connector.base_connector import \\\n+    RemoteConnector\n+from lmcache.logging import init_logger\n+from lmcache.utils import CacheEngineKey\n+\n+logger = init_logger(__name__)\n+\n+\n+class BaseValkeyConnector(RemoteConnector):\n+    \"\"\"Base Valkey connector with common operations\"\"\"\n+\n+    def __init__(self, memory_allocator: MemoryAllocatorInterface):\n+        self.memory_allocator = memory_allocator\n+\n+    @property\n+    def read_client(self) -> Valkey:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2053003704",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 482,
        "pr_file": "lmcache/experimental/storage_backend/connector/valkey_connector.py",
        "discussion_id": "2053003704",
        "commented_code": "@@ -0,0 +1,184 @@\n+# Copyright 2024-2025 LMCache Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import asyncio\n+import inspect\n+import os\n+from typing import List, Optional, Tuple, Union, no_type_check\n+\n+import valkey\n+from valkey import Valkey\n+\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj)\n+from lmcache.experimental.protocol import METADATA_BYTES_LEN, RedisMetadata\n+from lmcache.experimental.storage_backend.connector.base_connector import \\\n+    RemoteConnector\n+from lmcache.logging import init_logger\n+from lmcache.utils import CacheEngineKey\n+\n+logger = init_logger(__name__)\n+\n+\n+class BaseValkeyConnector(RemoteConnector):\n+    \"\"\"Base Valkey connector with common operations\"\"\"\n+\n+    def __init__(self, memory_allocator: MemoryAllocatorInterface):\n+        self.memory_allocator = memory_allocator\n+\n+    @property\n+    def read_client(self) -> Valkey:",
        "comment_created_at": "2025-04-21T21:14:25+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "What is this?",
        "pr_file_module": null
      },
      {
        "comment_id": "2053165273",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 482,
        "pr_file": "lmcache/experimental/storage_backend/connector/valkey_connector.py",
        "discussion_id": "2053003704",
        "commented_code": "@@ -0,0 +1,184 @@\n+# Copyright 2024-2025 LMCache Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import asyncio\n+import inspect\n+import os\n+from typing import List, Optional, Tuple, Union, no_type_check\n+\n+import valkey\n+from valkey import Valkey\n+\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj)\n+from lmcache.experimental.protocol import METADATA_BYTES_LEN, RedisMetadata\n+from lmcache.experimental.storage_backend.connector.base_connector import \\\n+    RemoteConnector\n+from lmcache.logging import init_logger\n+from lmcache.utils import CacheEngineKey\n+\n+logger = init_logger(__name__)\n+\n+\n+class BaseValkeyConnector(RemoteConnector):\n+    \"\"\"Base Valkey connector with common operations\"\"\"\n+\n+    def __init__(self, memory_allocator: MemoryAllocatorInterface):\n+        self.memory_allocator = memory_allocator\n+\n+    @property\n+    def read_client(self) -> Valkey:",
        "comment_created_at": "2025-04-22T01:06:16+00:00",
        "comment_author": "maobaolong",
        "comment_body": "This is a property for `read_client`, meanwhile, there is a `write_client` also, for the `ValkeyConnector`, it can be same, but for the `ValkeySentinelConnector`, we can use read_client for read operation which will access the `slave` role `valkey server`, and for write operation, we can use the `write_client` to access to `master` role `valkey server`",
        "pr_file_module": null
      }
    ]
  }
]