[
  {
    "discussion_id": "2149027783",
    "pr_number": 828,
    "pr_file": "lmcache/v1/storage_backend/remote_backend.py",
    "created_at": "2025-06-16T04:37:27+00:00",
    "commented_code": "logger.warning(\"Connection is None in contains, returning False\")\n             return False\n \n-        future = asyncio.run_coroutine_threadsafe(\n-            self.connection.exists(key), self.loop\n-        )\n-        try:\n-            res = future.result()\n-            return res\n-        except Exception as e:\n-            with self.lock:\n-                self.connection = None\n-                self.failure_time = time.time()\n-            logger.warning(f\"Remote connection failed in contains: {e}\")\n-            logger.warning(\"Returning False\")\n-            return False\n+        return self.connection.exists_sync(key)",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2149027783",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 828,
        "pr_file": "lmcache/v1/storage_backend/remote_backend.py",
        "discussion_id": "2149027783",
        "commented_code": "@@ -126,19 +126,7 @@ def contains(self, key: CacheEngineKey, pin: bool = False) -> bool:\n             logger.warning(\"Connection is None in contains, returning False\")\n             return False\n \n-        future = asyncio.run_coroutine_threadsafe(\n-            self.connection.exists(key), self.loop\n-        )\n-        try:\n-            res = future.result()\n-            return res\n-        except Exception as e:\n-            with self.lock:\n-                self.connection = None\n-                self.failure_time = time.time()\n-            logger.warning(f\"Remote connection failed in contains: {e}\")\n-            logger.warning(\"Returning False\")\n-            return False\n+        return self.connection.exists_sync(key)",
        "comment_created_at": "2025-06-16T04:37:27+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "Why do we remove the error handling here?",
        "pr_file_module": null
      },
      {
        "comment_id": "2150241733",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 828,
        "pr_file": "lmcache/v1/storage_backend/remote_backend.py",
        "discussion_id": "2149027783",
        "commented_code": "@@ -126,19 +126,7 @@ def contains(self, key: CacheEngineKey, pin: bool = False) -> bool:\n             logger.warning(\"Connection is None in contains, returning False\")\n             return False\n \n-        future = asyncio.run_coroutine_threadsafe(\n-            self.connection.exists(key), self.loop\n-        )\n-        try:\n-            res = future.result()\n-            return res\n-        except Exception as e:\n-            with self.lock:\n-                self.connection = None\n-                self.failure_time = time.time()\n-            logger.warning(f\"Remote connection failed in contains: {e}\")\n-            logger.warning(\"Returning False\")\n-            return False\n+        return self.connection.exists_sync(key)",
        "comment_created_at": "2025-06-16T15:01:51+00:00",
        "comment_author": "maobaolong",
        "comment_body": "```suggestion\r\n        try:\r\n            return self.connection.exists_sync(key)\r\n        except Exception as e:\r\n            with self.lock:\r\n                self.connection = None\r\n                self.failure_time = time.time()\r\n            logger.warning(f\"Remote connection failed in contains: {e}\")\r\n            logger.warning(\"Returning False\")\r\n            return False\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2151205637",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 828,
        "pr_file": "lmcache/v1/storage_backend/remote_backend.py",
        "discussion_id": "2149027783",
        "commented_code": "@@ -126,19 +126,7 @@ def contains(self, key: CacheEngineKey, pin: bool = False) -> bool:\n             logger.warning(\"Connection is None in contains, returning False\")\n             return False\n \n-        future = asyncio.run_coroutine_threadsafe(\n-            self.connection.exists(key), self.loop\n-        )\n-        try:\n-            res = future.result()\n-            return res\n-        except Exception as e:\n-            with self.lock:\n-                self.connection = None\n-                self.failure_time = time.time()\n-            logger.warning(f\"Remote connection failed in contains: {e}\")\n-            logger.warning(\"Returning False\")\n-            return False\n+        return self.connection.exists_sync(key)",
        "comment_created_at": "2025-06-17T02:37:55+00:00",
        "comment_author": "chunxiaozheng",
        "comment_body": "Updated! Thanks.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2085238355",
    "pr_number": 625,
    "pr_file": "lmcache/experimental/cache_engine.py",
    "created_at": "2025-05-12T18:33:51+00:00",
    "commented_code": "logger.info(\"LMCacheEngine closed.\")\n \n \n+# TODO(Jiayi): Using a separate class here.\n+# Should use the same class once the code is stable.\n+class LayerwiseLMCacheEngine(LMCacheEngine):\n+    \"\"\"A specialized LMCacheEngine for layerwise cache engine.\n+    \n+    This class is used to store the layerwise cache engine. It is a\n+    subclass of LMCacheEngine and inherits all the methods and attributes\n+    from it. The only difference is that it uses a different token database\n+    and memory allocator.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: LMCacheEngineConfig,\n+        metadata: LMCacheEngineMetadata,\n+        memory_allocator: MemoryAllocatorInterface,\n+        token_database: TokenDatabase,\n+        layerwise_gpu_connector: GPUConnectorInterface,\n+    ):\n+        super().__init__(config, metadata, memory_allocator, token_database,\n+                         layerwise_gpu_connector)\n+        assert isinstance(self.gpu_connector,\n+                          VLLMPagedMemLayerwiseGPUConnector)\n+\n+        self.num_layers = metadata.kv_shape[0]\n+\n+    @_lmcache_nvtx_annotate\n+    @torch.inference_mode()\n+    def store_layer(self,\n+                    tokens: torch.Tensor,\n+                    mask: Optional[torch.Tensor] = None,\n+                    **kwargs) -> Generator[None, None, None]:\n+        \"\"\"\n+        Store the KV cache in a layerwise manner.\n+        \"\"\"\n+\n+        if mask is not None:\n+            num_stored_tokens = torch.sum(mask).item()\n+        else:\n+            num_stored_tokens = len(tokens)\n+        monitor_req_id = self.stats_monitor.on_store_request(num_stored_tokens)\n+\n+        starts = []\n+        ends = []\n+        keys = []\n+        memory_objs = []\n+        kv_dtype = self.metadata.kv_dtype\n+        for start, end, key in self.token_database.process_tokens(\n+                tokens, mask):\n+            assert isinstance(key, CacheEngineKey)\n+\n+            keys_multi_layer = key.split_layers(self.num_layers)\n+\n+            # Only check the first layer\n+            if self.storage_manager.contains(keys_multi_layer[0]):\n+                continue\n+\n+            # Allocate the memory object\n+            num_tokens = end - start\n+            kv_shape_single_layer = self.gpu_connector.get_shape(num_tokens)\n+\n+            # TODO(Jiayi): Optimize with batched allocation\n+            memory_objs_multi_layer = []\n+            no_space_left = False\n+            for layer_id in range(self.num_layers):\n+                mem_obj_single_layer = self.storage_manager.allocate(\n+                    kv_shape_single_layer, kv_dtype)\n+\n+                if mem_obj_single_layer is None:\n+                    logger.warning(\n+                        \"Failed to allocate memory for the KV cache.\n\"\n+                        \"The KV cache will not be stored.\")\n+                    no_space_left = True\n+                    break\n+\n+                memory_objs_multi_layer.append(mem_obj_single_layer)\n+\n+            if no_space_left:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2085238355",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 625,
        "pr_file": "lmcache/experimental/cache_engine.py",
        "discussion_id": "2085238355",
        "commented_code": "@@ -422,6 +424,244 @@ def close(self) -> None:\n         logger.info(\"LMCacheEngine closed.\")\n \n \n+# TODO(Jiayi): Using a separate class here.\n+# Should use the same class once the code is stable.\n+class LayerwiseLMCacheEngine(LMCacheEngine):\n+    \"\"\"A specialized LMCacheEngine for layerwise cache engine.\n+    \n+    This class is used to store the layerwise cache engine. It is a\n+    subclass of LMCacheEngine and inherits all the methods and attributes\n+    from it. The only difference is that it uses a different token database\n+    and memory allocator.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: LMCacheEngineConfig,\n+        metadata: LMCacheEngineMetadata,\n+        memory_allocator: MemoryAllocatorInterface,\n+        token_database: TokenDatabase,\n+        layerwise_gpu_connector: GPUConnectorInterface,\n+    ):\n+        super().__init__(config, metadata, memory_allocator, token_database,\n+                         layerwise_gpu_connector)\n+        assert isinstance(self.gpu_connector,\n+                          VLLMPagedMemLayerwiseGPUConnector)\n+\n+        self.num_layers = metadata.kv_shape[0]\n+\n+    @_lmcache_nvtx_annotate\n+    @torch.inference_mode()\n+    def store_layer(self,\n+                    tokens: torch.Tensor,\n+                    mask: Optional[torch.Tensor] = None,\n+                    **kwargs) -> Generator[None, None, None]:\n+        \"\"\"\n+        Store the KV cache in a layerwise manner.\n+        \"\"\"\n+\n+        if mask is not None:\n+            num_stored_tokens = torch.sum(mask).item()\n+        else:\n+            num_stored_tokens = len(tokens)\n+        monitor_req_id = self.stats_monitor.on_store_request(num_stored_tokens)\n+\n+        starts = []\n+        ends = []\n+        keys = []\n+        memory_objs = []\n+        kv_dtype = self.metadata.kv_dtype\n+        for start, end, key in self.token_database.process_tokens(\n+                tokens, mask):\n+            assert isinstance(key, CacheEngineKey)\n+\n+            keys_multi_layer = key.split_layers(self.num_layers)\n+\n+            # Only check the first layer\n+            if self.storage_manager.contains(keys_multi_layer[0]):\n+                continue\n+\n+            # Allocate the memory object\n+            num_tokens = end - start\n+            kv_shape_single_layer = self.gpu_connector.get_shape(num_tokens)\n+\n+            # TODO(Jiayi): Optimize with batched allocation\n+            memory_objs_multi_layer = []\n+            no_space_left = False\n+            for layer_id in range(self.num_layers):\n+                mem_obj_single_layer = self.storage_manager.allocate(\n+                    kv_shape_single_layer, kv_dtype)\n+\n+                if mem_obj_single_layer is None:\n+                    logger.warning(\n+                        \"Failed to allocate memory for the KV cache.\\n\"\n+                        \"The KV cache will not be stored.\")\n+                    no_space_left = True\n+                    break\n+\n+                memory_objs_multi_layer.append(mem_obj_single_layer)\n+\n+            if no_space_left:",
        "comment_created_at": "2025-05-12T18:33:51+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Do we need to free the allocated memory objs if `no_space_left` is true?",
        "pr_file_module": null
      },
      {
        "comment_id": "2085635254",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 625,
        "pr_file": "lmcache/experimental/cache_engine.py",
        "discussion_id": "2085238355",
        "commented_code": "@@ -422,6 +424,244 @@ def close(self) -> None:\n         logger.info(\"LMCacheEngine closed.\")\n \n \n+# TODO(Jiayi): Using a separate class here.\n+# Should use the same class once the code is stable.\n+class LayerwiseLMCacheEngine(LMCacheEngine):\n+    \"\"\"A specialized LMCacheEngine for layerwise cache engine.\n+    \n+    This class is used to store the layerwise cache engine. It is a\n+    subclass of LMCacheEngine and inherits all the methods and attributes\n+    from it. The only difference is that it uses a different token database\n+    and memory allocator.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: LMCacheEngineConfig,\n+        metadata: LMCacheEngineMetadata,\n+        memory_allocator: MemoryAllocatorInterface,\n+        token_database: TokenDatabase,\n+        layerwise_gpu_connector: GPUConnectorInterface,\n+    ):\n+        super().__init__(config, metadata, memory_allocator, token_database,\n+                         layerwise_gpu_connector)\n+        assert isinstance(self.gpu_connector,\n+                          VLLMPagedMemLayerwiseGPUConnector)\n+\n+        self.num_layers = metadata.kv_shape[0]\n+\n+    @_lmcache_nvtx_annotate\n+    @torch.inference_mode()\n+    def store_layer(self,\n+                    tokens: torch.Tensor,\n+                    mask: Optional[torch.Tensor] = None,\n+                    **kwargs) -> Generator[None, None, None]:\n+        \"\"\"\n+        Store the KV cache in a layerwise manner.\n+        \"\"\"\n+\n+        if mask is not None:\n+            num_stored_tokens = torch.sum(mask).item()\n+        else:\n+            num_stored_tokens = len(tokens)\n+        monitor_req_id = self.stats_monitor.on_store_request(num_stored_tokens)\n+\n+        starts = []\n+        ends = []\n+        keys = []\n+        memory_objs = []\n+        kv_dtype = self.metadata.kv_dtype\n+        for start, end, key in self.token_database.process_tokens(\n+                tokens, mask):\n+            assert isinstance(key, CacheEngineKey)\n+\n+            keys_multi_layer = key.split_layers(self.num_layers)\n+\n+            # Only check the first layer\n+            if self.storage_manager.contains(keys_multi_layer[0]):\n+                continue\n+\n+            # Allocate the memory object\n+            num_tokens = end - start\n+            kv_shape_single_layer = self.gpu_connector.get_shape(num_tokens)\n+\n+            # TODO(Jiayi): Optimize with batched allocation\n+            memory_objs_multi_layer = []\n+            no_space_left = False\n+            for layer_id in range(self.num_layers):\n+                mem_obj_single_layer = self.storage_manager.allocate(\n+                    kv_shape_single_layer, kv_dtype)\n+\n+                if mem_obj_single_layer is None:\n+                    logger.warning(\n+                        \"Failed to allocate memory for the KV cache.\\n\"\n+                        \"The KV cache will not be stored.\")\n+                    no_space_left = True\n+                    break\n+\n+                memory_objs_multi_layer.append(mem_obj_single_layer)\n+\n+            if no_space_left:",
        "comment_created_at": "2025-05-12T22:44:19+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "https://github.com/LMCache/LMCache/blob/5c6eeca506243896951c73dc0cf81b3d3bd185df/lmcache/experimental/storage_backend/storage_manager.py#L472  Eviction is handled here. However, there are other edge cases where failed allocation can still happen with eviction (e.g., KV cache is bigger than storage size)",
        "pr_file_module": null
      },
      {
        "comment_id": "2085917188",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 625,
        "pr_file": "lmcache/experimental/cache_engine.py",
        "discussion_id": "2085238355",
        "commented_code": "@@ -422,6 +424,244 @@ def close(self) -> None:\n         logger.info(\"LMCacheEngine closed.\")\n \n \n+# TODO(Jiayi): Using a separate class here.\n+# Should use the same class once the code is stable.\n+class LayerwiseLMCacheEngine(LMCacheEngine):\n+    \"\"\"A specialized LMCacheEngine for layerwise cache engine.\n+    \n+    This class is used to store the layerwise cache engine. It is a\n+    subclass of LMCacheEngine and inherits all the methods and attributes\n+    from it. The only difference is that it uses a different token database\n+    and memory allocator.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: LMCacheEngineConfig,\n+        metadata: LMCacheEngineMetadata,\n+        memory_allocator: MemoryAllocatorInterface,\n+        token_database: TokenDatabase,\n+        layerwise_gpu_connector: GPUConnectorInterface,\n+    ):\n+        super().__init__(config, metadata, memory_allocator, token_database,\n+                         layerwise_gpu_connector)\n+        assert isinstance(self.gpu_connector,\n+                          VLLMPagedMemLayerwiseGPUConnector)\n+\n+        self.num_layers = metadata.kv_shape[0]\n+\n+    @_lmcache_nvtx_annotate\n+    @torch.inference_mode()\n+    def store_layer(self,\n+                    tokens: torch.Tensor,\n+                    mask: Optional[torch.Tensor] = None,\n+                    **kwargs) -> Generator[None, None, None]:\n+        \"\"\"\n+        Store the KV cache in a layerwise manner.\n+        \"\"\"\n+\n+        if mask is not None:\n+            num_stored_tokens = torch.sum(mask).item()\n+        else:\n+            num_stored_tokens = len(tokens)\n+        monitor_req_id = self.stats_monitor.on_store_request(num_stored_tokens)\n+\n+        starts = []\n+        ends = []\n+        keys = []\n+        memory_objs = []\n+        kv_dtype = self.metadata.kv_dtype\n+        for start, end, key in self.token_database.process_tokens(\n+                tokens, mask):\n+            assert isinstance(key, CacheEngineKey)\n+\n+            keys_multi_layer = key.split_layers(self.num_layers)\n+\n+            # Only check the first layer\n+            if self.storage_manager.contains(keys_multi_layer[0]):\n+                continue\n+\n+            # Allocate the memory object\n+            num_tokens = end - start\n+            kv_shape_single_layer = self.gpu_connector.get_shape(num_tokens)\n+\n+            # TODO(Jiayi): Optimize with batched allocation\n+            memory_objs_multi_layer = []\n+            no_space_left = False\n+            for layer_id in range(self.num_layers):\n+                mem_obj_single_layer = self.storage_manager.allocate(\n+                    kv_shape_single_layer, kv_dtype)\n+\n+                if mem_obj_single_layer is None:\n+                    logger.warning(\n+                        \"Failed to allocate memory for the KV cache.\\n\"\n+                        \"The KV cache will not be stored.\")\n+                    no_space_left = True\n+                    break\n+\n+                memory_objs_multi_layer.append(mem_obj_single_layer)\n+\n+            if no_space_left:",
        "comment_created_at": "2025-05-13T04:55:15+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "Yes, you are right. Fixed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2053313708",
    "pr_number": 496,
    "pr_file": "lmcache/experimental/storage_backend/local_cpu_backend.py",
    "created_at": "2025-04-22T04:36:30+00:00",
    "commented_code": "+import threading\n+from collections import OrderedDict\n+from concurrent.futures import Future\n+from typing import List, Optional, cast\n+\n+import torch\n+\n+from lmcache.experimental.lookup_server import LookupServerInterface\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj,\n+                                                    MixedMemoryAllocator)\n+from lmcache.experimental.storage_backend.abstract_backend import \\\n+    StorageBackendInterface\n+from lmcache.logging import init_logger\n+from lmcache.utils import CacheEngineKey\n+\n+logger = init_logger(__name__)\n+\n+\n+class LocalCPUBackend(StorageBackendInterface):\n+    \"\"\"\n+    The local CPU backend is primarily used for hot cache, thinly wrapping\n+    an ordered dictionary and tightly coupled with the memory allocator.\n+\n+    It can not use the LRUEvictor() helper because its size is variable\n+    depending on how much free space is left in the allocator.\n+\n+    R/W from RAM is synchronous and does not need an event loop or use futures.\n+\n+    NOTE(sam): make sure no methods call each other or else we will deadlock\n+\n+    QUESTION(sam): four methods raise NotImplementedError. Is this the best\n+    implementation and/or inheritance structure?\n+    \"\"\"\n+\n+    def __init__(self,\n+                 memory_allocator: MemoryAllocatorInterface,\n+                 lookup_server: Optional[LookupServerInterface] = None,\n+                 real_allocator: bool = True):\n+        # rely on ordered dict to manage LRU\n+        self.hot_cache_: OrderedDict[CacheEngineKey, MemoryObj] = OrderedDict()\n+        self.lookup_server = lookup_server\n+        self.memory_allocator = memory_allocator\n+        self.real_allocator = real_allocator\n+        if self.real_allocator:  # turn off real_allocator for testing\n+            assert isinstance(self.memory_allocator, MixedMemoryAllocator), \\\n+                \"LocalCPUBackend must be used with a MixedMemoryAllocator\"\n+\n+        # multiple threads can access the hot cache (protects self.hot_cache_)\n+        self.hot_cache_lock = threading.Lock()\n+\n+    def exists_in_put_tasks(self, key: CacheEngineKey) -> bool:\n+        \"\"\"\n+        please do not check asynchronous futures for cpu backend\n+        \"\"\"\n+        raise NotImplementedError",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2053313708",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 496,
        "pr_file": "lmcache/experimental/storage_backend/local_cpu_backend.py",
        "discussion_id": "2053313708",
        "commented_code": "@@ -0,0 +1,260 @@\n+import threading\n+from collections import OrderedDict\n+from concurrent.futures import Future\n+from typing import List, Optional, cast\n+\n+import torch\n+\n+from lmcache.experimental.lookup_server import LookupServerInterface\n+from lmcache.experimental.memory_management import (MemoryAllocatorInterface,\n+                                                    MemoryObj,\n+                                                    MixedMemoryAllocator)\n+from lmcache.experimental.storage_backend.abstract_backend import \\\n+    StorageBackendInterface\n+from lmcache.logging import init_logger\n+from lmcache.utils import CacheEngineKey\n+\n+logger = init_logger(__name__)\n+\n+\n+class LocalCPUBackend(StorageBackendInterface):\n+    \"\"\"\n+    The local CPU backend is primarily used for hot cache, thinly wrapping\n+    an ordered dictionary and tightly coupled with the memory allocator.\n+\n+    It can not use the LRUEvictor() helper because its size is variable\n+    depending on how much free space is left in the allocator.\n+\n+    R/W from RAM is synchronous and does not need an event loop or use futures.\n+\n+    NOTE(sam): make sure no methods call each other or else we will deadlock\n+\n+    QUESTION(sam): four methods raise NotImplementedError. Is this the best\n+    implementation and/or inheritance structure?\n+    \"\"\"\n+\n+    def __init__(self,\n+                 memory_allocator: MemoryAllocatorInterface,\n+                 lookup_server: Optional[LookupServerInterface] = None,\n+                 real_allocator: bool = True):\n+        # rely on ordered dict to manage LRU\n+        self.hot_cache_: OrderedDict[CacheEngineKey, MemoryObj] = OrderedDict()\n+        self.lookup_server = lookup_server\n+        self.memory_allocator = memory_allocator\n+        self.real_allocator = real_allocator\n+        if self.real_allocator:  # turn off real_allocator for testing\n+            assert isinstance(self.memory_allocator, MixedMemoryAllocator), \\\n+                \"LocalCPUBackend must be used with a MixedMemoryAllocator\"\n+\n+        # multiple threads can access the hot cache (protects self.hot_cache_)\n+        self.hot_cache_lock = threading.Lock()\n+\n+    def exists_in_put_tasks(self, key: CacheEngineKey) -> bool:\n+        \"\"\"\n+        please do not check asynchronous futures for cpu backend\n+        \"\"\"\n+        raise NotImplementedError",
        "comment_created_at": "2025-04-22T04:36:30+00:00",
        "comment_author": "ApostaC",
        "comment_body": "We can do `return False` here. \r\nLet's avoid throwing an error because the storage manager may directly call it without checking the type of the StorageBackend",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2059637998",
    "pr_number": 525,
    "pr_file": "lmcache/experimental/storage_backend/connector/mooncakestore_connector.py",
    "created_at": "2025-04-25T06:29:28+00:00",
    "commented_code": "async def get(self, key: CacheEngineKey) -> Optional[MemoryObj]:\n         key_str = key.to_string()\n \n-        metadata_bytes = self.store.get(key_str + \"metadata\")\n+        try:\n+            buffer = self.store.get(key_str) # buffer is pybind11 buffer type\n+        except Exception as e:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2059637998",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 525,
        "pr_file": "lmcache/experimental/storage_backend/connector/mooncakestore_connector.py",
        "discussion_id": "2059637998",
        "commented_code": "@@ -115,11 +116,22 @@ async def exists(self, key: CacheEngineKey) -> bool:\n     async def get(self, key: CacheEngineKey) -> Optional[MemoryObj]:\n         key_str = key.to_string()\n \n-        metadata_bytes = self.store.get(key_str + \"metadata\")\n+        try:\n+            buffer = self.store.get(key_str) # buffer is pybind11 buffer type\n+        except Exception as e:",
        "comment_created_at": "2025-04-25T06:29:28+00:00",
        "comment_author": "maobaolong",
        "comment_body": "I guess you want to log the exception? And what is the expected behavior if `key_str` absent? Return None?",
        "pr_file_module": null
      }
    ]
  }
]