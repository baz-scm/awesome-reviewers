[
  {
    "discussion_id": "1919328231",
    "pr_number": 312,
    "pr_file": "benchmarks/README.md",
    "created_at": "2025-01-16T22:36:10+00:00",
    "commented_code": "--base-url http://localhost:8000/v1 \\\n         --sharegpt\n     ```\n+\n+# Benchmarking LLM Performance: RAG Use Case\n+## Overview\n+\n+This repository contains benchmarking tools for evaluating the performance of language models in various scenarios. The initial focus of this benchmark is on the RAG (Retrieval-augmented generation) use case. The script `rag.py` simulates RAG workloads, allowing you to analyze the serving engine's throughput and latency.  \n+\n+### Current Workloads\n+\n+- **RAG Benchmark**: Simulates a real RAG dataset to evaluate key metrics such as token throughput, average time to first token, and average quality.\n+\n+## Setup\n+\n+1. Install the required dependencies:\n+   ```bash\n+   pip install -r requirements.txt\n+   ```\n+## Running the RAG Benchmark\n+\n+To run the RAG benchmark, use the following command:\n+\n+```bash\n+python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\nPassages:\n\" --query-prompt \"\n\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \nQuestion:\" --separator \"\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1919328231",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 312,
        "pr_file": "benchmarks/README.md",
        "discussion_id": "1919328231",
        "commented_code": "@@ -118,3 +116,117 @@ The `multi-round-qa.py` script works by:\n         --base-url http://localhost:8000/v1 \\\n         --sharegpt\n     ```\n+\n+# Benchmarking LLM Performance: RAG Use Case\n+## Overview\n+\n+This repository contains benchmarking tools for evaluating the performance of language models in various scenarios. The initial focus of this benchmark is on the RAG (Retrieval-augmented generation) use case. The script `rag.py` simulates RAG workloads, allowing you to analyze the serving engine's throughput and latency.  \n+\n+### Current Workloads\n+\n+- **RAG Benchmark**: Simulates a real RAG dataset to evaluate key metrics such as token throughput, average time to first token, and average quality.\n+\n+## Setup\n+\n+1. Install the required dependencies:\n+   ```bash\n+   pip install -r requirements.txt\n+   ```\n+## Running the RAG Benchmark\n+\n+To run the RAG benchmark, use the following command:\n+\n+```bash\n+python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\\nPassages:\\n\" --query-prompt \"\\n\\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \\nQuestion:\" --separator \"\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv",
        "comment_created_at": "2025-01-16T22:36:10+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Can we simplify the example command by the following:\r\n- Remove the system prompt and query prompt text (maybe have a default prompt in the script when users don't provide them)\r\n- Skip the command line args if there is a default value \r\n- Split the command into multiple lines with `\\`",
        "pr_file_module": null
      },
      {
        "comment_id": "1919503851",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 312,
        "pr_file": "benchmarks/README.md",
        "discussion_id": "1919328231",
        "commented_code": "@@ -118,3 +116,117 @@ The `multi-round-qa.py` script works by:\n         --base-url http://localhost:8000/v1 \\\n         --sharegpt\n     ```\n+\n+# Benchmarking LLM Performance: RAG Use Case\n+## Overview\n+\n+This repository contains benchmarking tools for evaluating the performance of language models in various scenarios. The initial focus of this benchmark is on the RAG (Retrieval-augmented generation) use case. The script `rag.py` simulates RAG workloads, allowing you to analyze the serving engine's throughput and latency.  \n+\n+### Current Workloads\n+\n+- **RAG Benchmark**: Simulates a real RAG dataset to evaluate key metrics such as token throughput, average time to first token, and average quality.\n+\n+## Setup\n+\n+1. Install the required dependencies:\n+   ```bash\n+   pip install -r requirements.txt\n+   ```\n+## Running the RAG Benchmark\n+\n+To run the RAG benchmark, use the following command:\n+\n+```bash\n+python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\\nPassages:\\n\" --query-prompt \"\\n\\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \\nQuestion:\" --separator \"\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv",
        "comment_created_at": "2025-01-17T03:30:35+00:00",
        "comment_author": "XbzOnGit",
        "comment_body": "We can decide those prompts by prompt-build-method if not provided. That makes sense.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1919341420",
    "pr_number": 312,
    "pr_file": "benchmarks/README.md",
    "created_at": "2025-01-16T22:53:26+00:00",
    "commented_code": "--base-url http://localhost:8000/v1 \\\n         --sharegpt\n     ```\n+\n+# Benchmarking LLM Performance: RAG Use Case\n+## Overview\n+\n+This repository contains benchmarking tools for evaluating the performance of language models in various scenarios. The initial focus of this benchmark is on the RAG (Retrieval-augmented generation) use case. The script `rag.py` simulates RAG workloads, allowing you to analyze the serving engine's throughput and latency.  \n+\n+### Current Workloads\n+\n+- **RAG Benchmark**: Simulates a real RAG dataset to evaluate key metrics such as token throughput, average time to first token, and average quality.\n+\n+## Setup\n+\n+1. Install the required dependencies:\n+   ```bash\n+   pip install -r requirements.txt\n+   ```\n+## Running the RAG Benchmark\n+\n+To run the RAG benchmark, use the following command:\n+\n+```bash\n+python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\nPassages:\n\" --query-prompt \"\n\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \nQuestion:\" --separator \"\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv\n+```\n+\n+Use ctrl-C to terminate the benchmark at any time, and the the script will write each request's detailed stats to `summary.csv`.\n+\n+\n+*Note:* the above command requires there is a serving engine with the `mistralai/Mistral-7B-Instruct-v0.2` model served locally at `http://localhost:8000/v1`. Here's an example command to launch the serving engine:\n+\n+```bash\n+vllm serve mistralai/Mistral-7B-Instruct-v0.2 --disable-log-requests\n+```\n+\n+## Running the RAG benchmark on LMCache\n+For benchmarking LMCache with CacheBlend:  \n+```bash\n+LMCACHE_CONFIG_FILE=example_blending.yaml python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\nPassages:\n\" --query-prompt \"\n\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \nQuestion:\" --separator \"[BLEND_SEP]\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv\n+```\n+Here's an example command to launch the serving engine with LMCache+CacheBlend:  \n+\n+```bash\n+LMCACHE_CONFIG_FILE=example_blending.yaml python3 -m lmcache_vllm.vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --gpu-memory-utilization 0.7 --port 8000\n+```\n+\n+\n+\n+\n+### What does precompute.py do\n+--skip-precompute will skip the precomputation, while by default to will call precompute functions on documents.  \n+If no --end-index provided, it will check kv-storage-size and try to precompute the documents that can be held in this size.  \n+Later RAG benchmark will only run these requests.  \n+\n+### Arguments\n+#### Configure the workload",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1919341420",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 312,
        "pr_file": "benchmarks/README.md",
        "discussion_id": "1919341420",
        "commented_code": "@@ -118,3 +116,117 @@ The `multi-round-qa.py` script works by:\n         --base-url http://localhost:8000/v1 \\\n         --sharegpt\n     ```\n+\n+# Benchmarking LLM Performance: RAG Use Case\n+## Overview\n+\n+This repository contains benchmarking tools for evaluating the performance of language models in various scenarios. The initial focus of this benchmark is on the RAG (Retrieval-augmented generation) use case. The script `rag.py` simulates RAG workloads, allowing you to analyze the serving engine's throughput and latency.  \n+\n+### Current Workloads\n+\n+- **RAG Benchmark**: Simulates a real RAG dataset to evaluate key metrics such as token throughput, average time to first token, and average quality.\n+\n+## Setup\n+\n+1. Install the required dependencies:\n+   ```bash\n+   pip install -r requirements.txt\n+   ```\n+## Running the RAG Benchmark\n+\n+To run the RAG benchmark, use the following command:\n+\n+```bash\n+python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\\nPassages:\\n\" --query-prompt \"\\n\\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \\nQuestion:\" --separator \"\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv\n+```\n+\n+Use ctrl-C to terminate the benchmark at any time, and the the script will write each request's detailed stats to `summary.csv`.\n+\n+\n+*Note:* the above command requires there is a serving engine with the `mistralai/Mistral-7B-Instruct-v0.2` model served locally at `http://localhost:8000/v1`. Here's an example command to launch the serving engine:\n+\n+```bash\n+vllm serve mistralai/Mistral-7B-Instruct-v0.2 --disable-log-requests\n+```\n+\n+## Running the RAG benchmark on LMCache\n+For benchmarking LMCache with CacheBlend:  \n+```bash\n+LMCACHE_CONFIG_FILE=example_blending.yaml python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\\nPassages:\\n\" --query-prompt \"\\n\\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \\nQuestion:\" --separator \"[BLEND_SEP]\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv\n+```\n+Here's an example command to launch the serving engine with LMCache+CacheBlend:  \n+\n+```bash\n+LMCACHE_CONFIG_FILE=example_blending.yaml python3 -m lmcache_vllm.vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --gpu-memory-utilization 0.7 --port 8000\n+```\n+\n+\n+\n+\n+### What does precompute.py do\n+--skip-precompute will skip the precomputation, while by default to will call precompute functions on documents.  \n+If no --end-index provided, it will check kv-storage-size and try to precompute the documents that can be held in this size.  \n+Later RAG benchmark will only run these requests.  \n+\n+### Arguments\n+#### Configure the workload",
        "comment_created_at": "2025-01-16T22:53:26+00:00",
        "comment_author": "ApostaC",
        "comment_body": "It's great to have such flexible configurations, but as a user, I found it a bit difficult to understand all the arguments. Here are few items to potentially improve it:\r\n- Try to hide the LMCache-related details. IIUC, there are quite a few args related to LMCache, including `-separator`, `--model`, `--skip-precompute`, `--kv-storage-size`, `--kv-storage-token-bit`, `--kv-precision-bit`. \r\n  - One potential solution is to use another command to run all the LMCache precompute related stuff. And rag.py only provides a single argument called `--enable-lmcache` (so the script knows it needs to add separators). \r\n- Try to avoid ambiguous naming. For example, at first glance, I thought `--model` is for specifying the model's name, but it seems like the correct configuration is `--model-api-name`.\r\n- It's okay to hard-code some configurations that won't have too much impact on end-to-end results. E.g., `--step-interval`, `--separator`.\r\n- Better clarification of the arg's meaning. I didn't quite get the meaning of `--doc-token-cnt`\r\n\r\nI'm happy to have a chat to discuss the details of those configurations. Just let me know if you want.",
        "pr_file_module": null
      },
      {
        "comment_id": "1919508079",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 312,
        "pr_file": "benchmarks/README.md",
        "discussion_id": "1919341420",
        "commented_code": "@@ -118,3 +116,117 @@ The `multi-round-qa.py` script works by:\n         --base-url http://localhost:8000/v1 \\\n         --sharegpt\n     ```\n+\n+# Benchmarking LLM Performance: RAG Use Case\n+## Overview\n+\n+This repository contains benchmarking tools for evaluating the performance of language models in various scenarios. The initial focus of this benchmark is on the RAG (Retrieval-augmented generation) use case. The script `rag.py` simulates RAG workloads, allowing you to analyze the serving engine's throughput and latency.  \n+\n+### Current Workloads\n+\n+- **RAG Benchmark**: Simulates a real RAG dataset to evaluate key metrics such as token throughput, average time to first token, and average quality.\n+\n+## Setup\n+\n+1. Install the required dependencies:\n+   ```bash\n+   pip install -r requirements.txt\n+   ```\n+## Running the RAG Benchmark\n+\n+To run the RAG benchmark, use the following command:\n+\n+```bash\n+python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\\nPassages:\\n\" --query-prompt \"\\n\\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \\nQuestion:\" --separator \"\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv\n+```\n+\n+Use ctrl-C to terminate the benchmark at any time, and the the script will write each request's detailed stats to `summary.csv`.\n+\n+\n+*Note:* the above command requires there is a serving engine with the `mistralai/Mistral-7B-Instruct-v0.2` model served locally at `http://localhost:8000/v1`. Here's an example command to launch the serving engine:\n+\n+```bash\n+vllm serve mistralai/Mistral-7B-Instruct-v0.2 --disable-log-requests\n+```\n+\n+## Running the RAG benchmark on LMCache\n+For benchmarking LMCache with CacheBlend:  \n+```bash\n+LMCACHE_CONFIG_FILE=example_blending.yaml python3 rag.py --qps 3.5 --model mistralai/Mistral-7B-Instruct-v0.2 --dataset ~/CacheBlend/inputs/musique_s.json --system-prompt \"You will be asked a question after reading several passages. Please directly answer the question based on the given passages. Do NOT repeat the question. The answer should be within 5 words..\\nPassages:\\n\" --query-prompt \"\\n\\nAnswer the question directly based on the given passages. Do NOT repeat the question. The answer should be within 5 words. \\nQuestion:\" --separator \"[BLEND_SEP]\" --prompt-build-method QA --base-url \"http://localhost:8000/v1\" --kv-storage-size 30GB --kv-storage-token-unit 256 --max-tokens 32 --output summary.csv\n+```\n+Here's an example command to launch the serving engine with LMCache+CacheBlend:  \n+\n+```bash\n+LMCACHE_CONFIG_FILE=example_blending.yaml python3 -m lmcache_vllm.vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2 --gpu-memory-utilization 0.7 --port 8000\n+```\n+\n+\n+\n+\n+### What does precompute.py do\n+--skip-precompute will skip the precomputation, while by default to will call precompute functions on documents.  \n+If no --end-index provided, it will check kv-storage-size and try to precompute the documents that can be held in this size.  \n+Later RAG benchmark will only run these requests.  \n+\n+### Arguments\n+#### Configure the workload",
        "comment_created_at": "2025-01-17T03:39:03+00:00",
        "comment_author": "XbzOnGit",
        "comment_body": "We can remove --doc-token-cnt. It is used for getting a sense that how much of text is documents(something like reuse ratio).\r\n--separator cannot be hard-coded because there's a difference between blending and non-blending/vllm ones.\r\n--step-interval is just in case someone wants a higher QPS. We can keep it default most of the time.\r\nThe precompute part in README is related with LMCache. Previously I used this design just for convenience. I can run one command to launch one benchmark. It would be logically better to divide them into two commands. ",
        "pr_file_module": null
      }
    ]
  }
]