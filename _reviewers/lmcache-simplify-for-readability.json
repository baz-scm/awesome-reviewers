[
  {
    "discussion_id": "2248507202",
    "pr_number": 1128,
    "pr_file": "lmcache/v1/cache_engine.py",
    "created_at": "2025-08-01T17:32:15+00:00",
    "commented_code": ":raises: ValueError if the number of Falses in the mask is not a\n             multiple of the chunk size.\n         \"\"\"\n+        if self.save_only_first_rank and not self.metadata.is_first_rank():",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2248507202",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 1128,
        "pr_file": "lmcache/v1/cache_engine.py",
        "discussion_id": "2248507202",
        "commented_code": "@@ -178,6 +192,9 @@ def store(\n         :raises: ValueError if the number of Falses in the mask is not a\n             multiple of the chunk size.\n         \"\"\"\n+        if self.save_only_first_rank and not self.metadata.is_first_rank():",
        "comment_created_at": "2025-08-01T17:32:15+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Can we add a helper function in CacheEngine with a name like `is_passive`:\r\n```\r\n    def is_passive(self):\r\n        \"\"\"A 'passive' CacheEngine means that the node itself will not store/retrieve \r\n        the data directly, but from the \"active\" worker (i.e., rank 0 in MLA)\r\n        \"\"\"\r\n        return self.save_only_first_rank and not self.metadata.is_first_rank()\r\n```\r\n\r\nThen the if condition can be more readable:\r\n\r\n```suggestion\r\n        if self.is_passive():\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2232134319",
    "pr_number": 1144,
    "pr_file": "lmcache/v1/lookup_client/lmcache_lookup_client.py",
    "created_at": "2025-07-25T22:46:33+00:00",
    "commented_code": "class LMCacheLookupClient(LookupClientInterface):\n     \"\"\"ZMQ-based lookup client that communicates with a lookup server.\"\"\"\n \n-    def __init__(self, vllm_config: \"VllmConfig\"):\n+    def __init__(self, vllm_config: \"VllmConfig\", config: LMCacheEngineConfig):\n         self.encoder = MsgpackEncoder()\n         self.ctx = zmq.Context()  # type: ignore[attr-defined]\n         rpc_port = vllm_config.kv_transfer_config.get_from_extra_config(\n             \"lmcache_rpc_port\", 0\n         )\n         self.tensor_parallel_size = vllm_config.parallel_config.tensor_parallel_size\n+        self.create_lookup_server_only_on_worker_0 = (\n+            config.extra_config\n+            and config.extra_config.get(\"create_lookup_server_only_on_worker_0\", True)\n+        )\n+        if self.create_lookup_server_only_on_worker_0:\n+            socket_path = get_zmq_rpc_path_lmcache(vllm_config, rpc_port)\n+            self.socket = make_zmq_socket(\n+                self.ctx,\n+                socket_path,\n+                zmq.REQ,  # type: ignore[attr-defined]\n+                bind=False,\n+            )\n+            return\n         for tp_rank in range(self.tensor_parallel_size):",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2232134319",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 1144,
        "pr_file": "lmcache/v1/lookup_client/lmcache_lookup_client.py",
        "discussion_id": "2232134319",
        "commented_code": "@@ -38,13 +39,26 @@\n class LMCacheLookupClient(LookupClientInterface):\n     \"\"\"ZMQ-based lookup client that communicates with a lookup server.\"\"\"\n \n-    def __init__(self, vllm_config: \"VllmConfig\"):\n+    def __init__(self, vllm_config: \"VllmConfig\", config: LMCacheEngineConfig):\n         self.encoder = MsgpackEncoder()\n         self.ctx = zmq.Context()  # type: ignore[attr-defined]\n         rpc_port = vllm_config.kv_transfer_config.get_from_extra_config(\n             \"lmcache_rpc_port\", 0\n         )\n         self.tensor_parallel_size = vllm_config.parallel_config.tensor_parallel_size\n+        self.create_lookup_server_only_on_worker_0 = (\n+            config.extra_config\n+            and config.extra_config.get(\"create_lookup_server_only_on_worker_0\", True)\n+        )\n+        if self.create_lookup_server_only_on_worker_0:\n+            socket_path = get_zmq_rpc_path_lmcache(vllm_config, rpc_port)\n+            self.socket = make_zmq_socket(\n+                self.ctx,\n+                socket_path,\n+                zmq.REQ,  # type: ignore[attr-defined]\n+                bind=False,\n+            )\n+            return\n         for tp_rank in range(self.tensor_parallel_size):",
        "comment_created_at": "2025-07-25T22:46:33+00:00",
        "comment_author": "ApostaC",
        "comment_body": "We can simplify the code here to avoid copy-pasting:\r\n\r\n```suggestion\r\n        ranks = self.tensor_parallel_size\r\n        if self.create_lookup_server_only_on_worker_0:\r\n            ranks = 1\r\n        for tp_rank in range(ranks):\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2187037635",
    "pr_number": 982,
    "pr_file": "lmcache/v1/cache_engine.py",
    "created_at": "2025-07-05T09:57:45+00:00",
    "commented_code": "memory_objs.append(memory_obj)\n \n             tot_kv_size += memory_obj.get_size()\n-\n+        # memory_objs might be empty, directly return to avoid sending tokens\n+        if memory_objs == []:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2187037635",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 982,
        "pr_file": "lmcache/v1/cache_engine.py",
        "discussion_id": "2187037635",
        "commented_code": "@@ -212,7 +212,9 @@ def store(\n             memory_objs.append(memory_obj)\n \n             tot_kv_size += memory_obj.get_size()\n-\n+        # memory_objs might be empty, directly return to avoid sending tokens\n+        if memory_objs == []:",
        "comment_created_at": "2025-07-05T09:57:45+00:00",
        "comment_author": "maobaolong",
        "comment_body": "`if not memory_objs:` is the most `Pythonic` way compared to `memory_objs == []` and `len(memory_objs)==0`, and `if not memory_objs:` is also the suggest style is PEP 8.",
        "pr_file_module": null
      },
      {
        "comment_id": "2187849323",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 982,
        "pr_file": "lmcache/v1/cache_engine.py",
        "discussion_id": "2187037635",
        "commented_code": "@@ -212,7 +212,9 @@ def store(\n             memory_objs.append(memory_obj)\n \n             tot_kv_size += memory_obj.get_size()\n-\n+        # memory_objs might be empty, directly return to avoid sending tokens\n+        if memory_objs == []:",
        "comment_created_at": "2025-07-06T00:01:37+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "Good catch:)",
        "pr_file_module": null
      },
      {
        "comment_id": "2188040119",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 982,
        "pr_file": "lmcache/v1/cache_engine.py",
        "discussion_id": "2187037635",
        "commented_code": "@@ -212,7 +212,9 @@ def store(\n             memory_objs.append(memory_obj)\n \n             tot_kv_size += memory_obj.get_size()\n-\n+        # memory_objs might be empty, directly return to avoid sending tokens\n+        if memory_objs == []:",
        "comment_created_at": "2025-07-06T05:48:53+00:00",
        "comment_author": "lrq619",
        "comment_body": "Sure, updated to `if not memory_objs:`, thanks for your suggestion!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2192966102",
    "pr_number": 997,
    "pr_file": "lmcache/integration/vllm/vllm_v1_adapter.py",
    "created_at": "2025-07-08T16:32:51+00:00",
    "commented_code": "cached_reqs = scheduler_output.scheduled_cached_reqs\n         for i, req_id in enumerate(cached_reqs.req_ids):\n             request_tracker = self._request_trackers[req_id]\n-            new_token_ids = cached_reqs.new_token_ids[i]\n+            num_new_tokens = scheduler_output.num_scheduled_tokens[req_id]\n+            if req_id in self._requests_in_step:\n+                request = self._requests_in_step[req_id]",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2192966102",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 997,
        "pr_file": "lmcache/integration/vllm/vllm_v1_adapter.py",
        "discussion_id": "2192966102",
        "commented_code": "@@ -941,7 +944,18 @@ def build_connector_meta(\n         cached_reqs = scheduler_output.scheduled_cached_reqs\n         for i, req_id in enumerate(cached_reqs.req_ids):\n             request_tracker = self._request_trackers[req_id]\n-            new_token_ids = cached_reqs.new_token_ids[i]\n+            num_new_tokens = scheduler_output.num_scheduled_tokens[req_id]\n+            if req_id in self._requests_in_step:\n+                request = self._requests_in_step[req_id]",
        "comment_created_at": "2025-07-08T16:32:51+00:00",
        "comment_author": "ApostaC",
        "comment_body": "Grammar sugar to reduce one dict lookup:\r\n\r\n```python\r\nif request := self._requests_in_step.get(req_id):\r\n    num_current_tokens = len(......)\r\n    ......\r\nelse:\r\n    raise ......\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2025721556",
    "pr_number": 406,
    "pr_file": "lmcache/experimental/cache_engine.py",
    "created_at": "2025-04-02T23:14:18+00:00",
    "commented_code": "# cpu tensor for the sake of performance.\n             # For example, disk->gpu is faster than disk->cpu->gpu.\n             # RDMA is another example.\n-\n-            self.gpu_connector.to_gpu(memory_obj, start, end, **kwargs)\n+            if self.gpu_connector is not None:\n+                self.gpu_connector.to_gpu(memory_obj, start, end, **kwargs)\n+            elif self.dram_connector is not None:\n+                self.dram_connector.to_dram(memory_obj, start, end, **kwargs)",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2025721556",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 406,
        "pr_file": "lmcache/experimental/cache_engine.py",
        "discussion_id": "2025721556",
        "commented_code": "@@ -197,14 +216,81 @@ def retrieve(self,\n             # cpu tensor for the sake of performance.\n             # For example, disk->gpu is faster than disk->cpu->gpu.\n             # RDMA is another example.\n-\n-            self.gpu_connector.to_gpu(memory_obj, start, end, **kwargs)\n+            if self.gpu_connector is not None:\n+                self.gpu_connector.to_gpu(memory_obj, start, end, **kwargs)\n+            elif self.dram_connector is not None:\n+                self.dram_connector.to_dram(memory_obj, start, end, **kwargs)",
        "comment_created_at": "2025-04-02T23:14:18+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "We can use a single variable name here. No need to check `gpu_connector` or `dram_connector` everytime.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1750876202",
    "pr_number": 91,
    "pr_file": "lmcache/cache_engine.py",
    "created_at": "2024-09-09T20:15:27+00:00",
    "commented_code": "def _chunk_tokens(\n             self, \n             tokens: torch.Tensor, \n-            device\n+            #device",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1750876202",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 91,
        "pr_file": "lmcache/cache_engine.py",
        "discussion_id": "1750876202",
        "commented_code": "@@ -74,7 +66,7 @@ def _hash(\n     def _chunk_tokens(\n             self, \n             tokens: torch.Tensor, \n-            device\n+            #device",
        "comment_created_at": "2024-09-09T20:15:27+00:00",
        "comment_author": "ApostaC",
        "comment_body": "A general comment: just feel free to remove the useless code instead of commenting it out -- git can be used to track history anyway.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2059635990",
    "pr_number": 525,
    "pr_file": "lmcache/experimental/storage_backend/connector/mooncakestore_connector.py",
    "created_at": "2025-04-25T06:27:30+00:00",
    "commented_code": "import os\n from dataclasses import dataclass\n from typing import List, Optional, no_type_check\n+import ctypes",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2059635990",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 525,
        "pr_file": "lmcache/experimental/storage_backend/connector/mooncakestore_connector.py",
        "discussion_id": "2059635990",
        "commented_code": "@@ -18,6 +18,7 @@\n import os\n from dataclasses import dataclass\n from typing import List, Optional, no_type_check\n+import ctypes ",
        "comment_created_at": "2025-04-25T06:27:30+00:00",
        "comment_author": "maobaolong",
        "comment_body": "It seems defined but never used",
        "pr_file_module": null
      }
    ]
  }
]