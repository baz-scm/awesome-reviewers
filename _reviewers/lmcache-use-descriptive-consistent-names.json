[
  {
    "discussion_id": "2209064451",
    "pr_number": 988,
    "pr_file": "lmcache/v1/storage_backend/abstract_backend.py",
    "created_at": "2025-07-16T02:15:45+00:00",
    "commented_code": "\"\"\"\n         raise NotImplementedError\n \n+    def batch_contains(",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2209064451",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 988,
        "pr_file": "lmcache/v1/storage_backend/abstract_backend.py",
        "discussion_id": "2209064451",
        "commented_code": "@@ -60,6 +60,27 @@ def contains(self, key: CacheEngineKey, pin: bool = False) -> bool:\n         \"\"\"\n         raise NotImplementedError\n \n+    def batch_contains(",
        "comment_created_at": "2025-07-16T02:15:45+00:00",
        "comment_author": "chunxiaozheng",
        "comment_body": "one minor suggestion, unified with other batched interfaces, starting with `batched_xxx`, such as `batched_contains`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2088018454",
    "pr_number": 528,
    "pr_file": "lmcache/experimental/storage_backend/connector/nixl_connector.py",
    "created_at": "2025-05-14T04:42:45+00:00",
    "commented_code": "if nixl_config.role == NixlRole.SENDER:\n             self._side_channel.connect(\"tcp://{}:{}\".format(\n-                nixl_config.peer_host_name, nixl_config.peer_port))\n+                nixl_config.receiver_host, nixl_config.receiver_port))\n             self._side_channel.setsockopt(zmq.LINGER, 0)  # type: ignore\n         else:\n             self._side_channel.bind(\"tcp://{}:{}\".format(\n-                nixl_config.peer_host_name, nixl_config.peer_port))\n+                nixl_config.receiver_host, nixl_config.receiver_port))",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2088018454",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 528,
        "pr_file": "lmcache/experimental/storage_backend/connector/nixl_connector.py",
        "discussion_id": "2088018454",
        "commented_code": "@@ -307,11 +307,11 @@ def __init__(self, nixl_config: NixlConfig):\n \n         if nixl_config.role == NixlRole.SENDER:\n             self._side_channel.connect(\"tcp://{}:{}\".format(\n-                nixl_config.peer_host_name, nixl_config.peer_port))\n+                nixl_config.receiver_host, nixl_config.receiver_port))\n             self._side_channel.setsockopt(zmq.LINGER, 0)  # type: ignore\n         else:\n             self._side_channel.bind(\"tcp://{}:{}\".format(\n-                nixl_config.peer_host_name, nixl_config.peer_port))\n+                nixl_config.receiver_host, nixl_config.receiver_port))",
        "comment_created_at": "2025-05-14T04:42:45+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "Why renaming all `peer` to `receiver`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2089772017",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 528,
        "pr_file": "lmcache/experimental/storage_backend/connector/nixl_connector.py",
        "discussion_id": "2088018454",
        "commented_code": "@@ -307,11 +307,11 @@ def __init__(self, nixl_config: NixlConfig):\n \n         if nixl_config.role == NixlRole.SENDER:\n             self._side_channel.connect(\"tcp://{}:{}\".format(\n-                nixl_config.peer_host_name, nixl_config.peer_port))\n+                nixl_config.receiver_host, nixl_config.receiver_port))\n             self._side_channel.setsockopt(zmq.LINGER, 0)  # type: ignore\n         else:\n             self._side_channel.bind(\"tcp://{}:{}\".format(\n-                nixl_config.peer_host_name, nixl_config.peer_port))\n+                nixl_config.receiver_host, nixl_config.receiver_port))",
        "comment_created_at": "2025-05-14T21:19:27+00:00",
        "comment_author": "ApostaC",
        "comment_body": "The current architecture is a receiver listening on a specific URL, and senders connect to that URL. Using \"peer\" may create confusion about that the receiver is going to use a \"sender-side URL\" rather than its listening URL.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2194395337",
    "pr_number": 936,
    "pr_file": "lmcache/v1/config.py",
    "created_at": "2025-07-09T08:27:18+00:00",
    "commented_code": "extra_config: Optional[dict] = None,\n         save_unfull_chunk: bool = True,\n         blocking_timeout_secs: int = 10,\n+        mooncake_lookup_client: Optional[str] = None,",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2194395337",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 936,
        "pr_file": "lmcache/v1/config.py",
        "discussion_id": "2194395337",
        "commented_code": "@@ -157,6 +161,7 @@ def from_defaults(\n         extra_config: Optional[dict] = None,\n         save_unfull_chunk: bool = True,\n         blocking_timeout_secs: int = 10,\n+        mooncake_lookup_client: Optional[str] = None,",
        "comment_created_at": "2025-07-09T08:27:18+00:00",
        "comment_author": "maobaolong",
        "comment_body": "Can you naming this more abstract so that other backend can implement other `xxx_lookup_client` and can reuse this config?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1878997292",
    "pr_number": 256,
    "pr_file": "lmcache/cache_engine.py",
    "created_at": "2024-12-10T22:22:46+00:00",
    "commented_code": "num_skip_prefix_chunk)\n         # With num_skip_chunks, the following is relative to\n         # the new start after skip.\n-        num_tokens: int = self._num_tokens_in_kv(kv_tensors, fmt)\n \n         start_token_idx = None\n         start_chunk_idx = 0\n-        for chunk_hash, idx in zip(chunk_hashes,\n-                                   range(0, num_tokens, self.chunk_size)):\n-            if not self.engine_.contains(self._make_key(chunk_hash, fmt)):\n-                start_token_idx = idx\n+        keys = []\n+        for chunk_hash in chunk_hashes:\n+            keys.append(self._make_key(chunk_hash, fmt))\n+        anws = self.engine_.batched_contains(keys,\n+                                             total_size=_get_size(kv_tensors))\n+        for anw in anws:\n+            if not anw:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1878997292",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 256,
        "pr_file": "lmcache/cache_engine.py",
        "discussion_id": "1878997292",
        "commented_code": "@@ -194,14 +195,17 @@ def _make_chunks_skip_existing(\n                                          num_skip_prefix_chunk)\n         # With num_skip_chunks, the following is relative to\n         # the new start after skip.\n-        num_tokens: int = self._num_tokens_in_kv(kv_tensors, fmt)\n \n         start_token_idx = None\n         start_chunk_idx = 0\n-        for chunk_hash, idx in zip(chunk_hashes,\n-                                   range(0, num_tokens, self.chunk_size)):\n-            if not self.engine_.contains(self._make_key(chunk_hash, fmt)):\n-                start_token_idx = idx\n+        keys = []\n+        for chunk_hash in chunk_hashes:\n+            keys.append(self._make_key(chunk_hash, fmt))\n+        anws = self.engine_.batched_contains(keys,\n+                                             total_size=_get_size(kv_tensors))\n+        for anw in anws:\n+            if not anw:",
        "comment_created_at": "2024-12-10T22:22:46+00:00",
        "comment_author": "ApostaC",
        "comment_body": "The loop is easy to understand. But what does `anw` and `anws` mean? \r\n\r\nNote that the code might be changed by other people in the future, it's better to have a clear variable name, thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2141763941",
    "pr_number": 812,
    "pr_file": "lmcache/v1/storage_backend/connector/base_connector.py",
    "created_at": "2025-06-12T05:46:05+00:00",
    "commented_code": "\"\"\"\n         raise NotImplementedError\n+\n+    # New layerwise methods with default implementations for backwards compatibility\n+    async def layerwise_exists(self, keys: List[List[CacheEngineKey]]) -> List[List[bool]]:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2141763941",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 812,
        "pr_file": "lmcache/v1/storage_backend/connector/base_connector.py",
        "discussion_id": "2141763941",
        "commented_code": "@@ -86,3 +86,72 @@ async def close(self):\n \n         \"\"\"\n         raise NotImplementedError\n+\n+    # New layerwise methods with default implementations for backwards compatibility\n+    async def layerwise_exists(self, keys: List[List[CacheEngineKey]]) -> List[List[bool]]:",
        "comment_created_at": "2025-06-12T05:46:05+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "To me, these functions starting with `layerwise` should be renamed to `batched`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2144153902",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 812,
        "pr_file": "lmcache/v1/storage_backend/connector/base_connector.py",
        "discussion_id": "2141763941",
        "commented_code": "@@ -86,3 +86,72 @@ async def close(self):\n \n         \"\"\"\n         raise NotImplementedError\n+\n+    # New layerwise methods with default implementations for backwards compatibility\n+    async def layerwise_exists(self, keys: List[List[CacheEngineKey]]) -> List[List[bool]]:",
        "comment_created_at": "2025-06-13T03:54:17+00:00",
        "comment_author": "prashant182",
        "comment_body": "Acknowledged, updated in commit will publish",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2138871623",
    "pr_number": 809,
    "pr_file": "examples/disagg_prefill/xp1d/disagg_proxy_server_original.py",
    "created_at": "2025-06-10T22:36:17+00:00",
    "commented_code": "+# SPDX-License-Identifier: Apache-2.0",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2138871623",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 809,
        "pr_file": "examples/disagg_prefill/xp1d/disagg_proxy_server_original.py",
        "discussion_id": "2138871623",
        "commented_code": "@@ -0,0 +1,223 @@\n+# SPDX-License-Identifier: Apache-2.0",
        "comment_created_at": "2025-06-10T22:36:17+00:00",
        "comment_author": "KuntaiDu",
        "comment_body": "Naming-wise, it is a bit difficult to figure out the difference between `disagg_proxy_server.py` and `disagg_proxy_server_original.py`. Maybe call them`disagg_proxy_first_token_from_prefiller.py` and `disagg_proxy_first_token_from_decoder.py`? I know this name is a bit long --- feel free to propose another one.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2085491934",
    "pr_number": 625,
    "pr_file": "lmcache/experimental/storage_backend/local_cpu_backend.py",
    "created_at": "2025-05-12T21:02:57+00:00",
    "commented_code": "def __str__(self):\n         return self.__class__.__name__\n \n-    def contains(self, key: CacheEngineKey) -> bool:\n+    def contains(self, key: CacheEngineKey, touch: bool = False) -> bool:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2085491934",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 625,
        "pr_file": "lmcache/experimental/storage_backend/local_cpu_backend.py",
        "discussion_id": "2085491934",
        "commented_code": "@@ -69,9 +69,13 @@ def __init__(self,\n     def __str__(self):\n         return self.__class__.__name__\n \n-    def contains(self, key: CacheEngineKey) -> bool:\n+    def contains(self, key: CacheEngineKey, touch: bool = False) -> bool:",
        "comment_created_at": "2025-05-12T21:02:57+00:00",
        "comment_author": "ApostaC",
        "comment_body": "How about changing the \"touch\" to \"pin\"? \"Touch\" sounds like marking something as recently used in LRU.",
        "pr_file_module": null
      },
      {
        "comment_id": "2085859473",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 625,
        "pr_file": "lmcache/experimental/storage_backend/local_cpu_backend.py",
        "discussion_id": "2085491934",
        "commented_code": "@@ -69,9 +69,13 @@ def __init__(self,\n     def __str__(self):\n         return self.__class__.__name__\n \n-    def contains(self, key: CacheEngineKey) -> bool:\n+    def contains(self, key: CacheEngineKey, touch: bool = False) -> bool:",
        "comment_created_at": "2025-05-13T03:33:06+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2067824857",
    "pr_number": 511,
    "pr_file": "lmcache/experimental/protocol.py",
    "created_at": "2025-04-30T04:12:18+00:00",
    "commented_code": "@dataclass\n-class RedisMetadata:\n+class CommonMetadata:",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "2067824857",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 511,
        "pr_file": "lmcache/experimental/protocol.py",
        "discussion_id": "2067824857",
        "commented_code": "@@ -48,7 +48,7 @@ class Constants:\n \n \n @dataclass\n-class RedisMetadata:\n+class CommonMetadata:",
        "comment_created_at": "2025-04-30T04:12:18+00:00",
        "comment_author": "YaoJiayi",
        "comment_body": "Maybe `RemoteMetadata` is a better name?",
        "pr_file_module": null
      },
      {
        "comment_id": "2068225193",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 511,
        "pr_file": "lmcache/experimental/protocol.py",
        "discussion_id": "2067824857",
        "commented_code": "@@ -48,7 +48,7 @@ class Constants:\n \n \n @dataclass\n-class RedisMetadata:\n+class CommonMetadata:",
        "comment_created_at": "2025-04-30T09:09:16+00:00",
        "comment_author": "maobaolong",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1815485242",
    "pr_number": 164,
    "pr_file": "lmcache/cache_engine.py",
    "created_at": "2024-10-24T17:47:41+00:00",
    "commented_code": "tokens: torch.Tensor,\n         kv_tensors: torch.Tensor,\n         fmt: str,\n+        num_skip_chunk=0,",
    "repo_full_name": "LMCache/LMCache",
    "discussion_comments": [
      {
        "comment_id": "1815485242",
        "repo_full_name": "LMCache/LMCache",
        "pr_number": 164,
        "pr_file": "lmcache/cache_engine.py",
        "discussion_id": "1815485242",
        "commented_code": "@@ -185,11 +185,15 @@ def _make_chunks_skip_existing(\n         tokens: torch.Tensor,\n         kv_tensors: torch.Tensor,\n         fmt: str,\n+        num_skip_chunk=0,",
        "comment_created_at": "2024-10-24T17:47:41+00:00",
        "comment_author": "ApostaC",
        "comment_body": "The \"physical meaning\" of `num_skip_chunk` does not match its name. IIUC, the physical meaning here is \"number of prefix chunks to skip\". Can we have a better variable name?\r\n@YaoJiayi feel free to chime in if you have any thoughts on this.",
        "pr_file_module": null
      }
    ]
  }
]