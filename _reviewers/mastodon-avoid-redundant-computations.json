[
  {
    "discussion_id": "1770862319",
    "pr_number": 30475,
    "pr_file": "app/javascript/mastodon/actions/compose.js",
    "created_at": "2024-09-23T07:06:30+00:00",
    "commented_code": "const uploadLimit = getState().getIn(['server', 'server', 'configuration', 'statuses', 'max_media_attachments']);\n     const media = getState().getIn(['compose', 'media_attachments']);\n     const pending = getState().getIn(['compose', 'pending_media_attachments']);\n+    const serverConfiguration = getState().getIn(['server', 'server', 'configuration']);\n+    const maxMediaAttachments = serverConfiguration.getIn(['statuses', 'max_media_attachments']);\n+    const videoSizeLimit = serverConfiguration.getIn(['media_attachments', 'video_size_limit']);\n+    const imageSizeLimit = serverConfiguration.getIn(['media_attachments', 'image_size_limit']);\n+    const filesArray = Array.from(files);\n     const progress = new Array(files.length).fill(0);\n-\n-    let total = Array.from(files).reduce((a, v) => a + v.size, 0);\n-\n-    if (files.length + media.size + pending > uploadLimit) {\n+    let total = filesArray.reduce((a, v) => a + v.size, 0);\n+    if (\n+      filesArray.some(file => (file.type.match(/video\\/.*/) && file.size > videoSizeLimit)\n+        || (file.type.match(/image\\/.*/) && file.size > imageSizeLimit)\n+        || (files.length + media.size + pending > maxMediaAttachments))) {",
    "repo_full_name": "mastodon/mastodon",
    "discussion_comments": [
      {
        "comment_id": "1770862319",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 30475,
        "pr_file": "app/javascript/mastodon/actions/compose.js",
        "discussion_id": "1770862319",
        "commented_code": "@@ -301,11 +301,17 @@ export function uploadCompose(files) {\n     const uploadLimit = getState().getIn(['server', 'server', 'configuration', 'statuses', 'max_media_attachments']);\n     const media = getState().getIn(['compose', 'media_attachments']);\n     const pending = getState().getIn(['compose', 'pending_media_attachments']);\n+    const serverConfiguration = getState().getIn(['server', 'server', 'configuration']);\n+    const maxMediaAttachments = serverConfiguration.getIn(['statuses', 'max_media_attachments']);\n+    const videoSizeLimit = serverConfiguration.getIn(['media_attachments', 'video_size_limit']);\n+    const imageSizeLimit = serverConfiguration.getIn(['media_attachments', 'image_size_limit']);\n+    const filesArray = Array.from(files);\n     const progress = new Array(files.length).fill(0);\n-\n-    let total = Array.from(files).reduce((a, v) => a + v.size, 0);\n-\n-    if (files.length + media.size + pending > uploadLimit) {\n+    let total = filesArray.reduce((a, v) => a + v.size, 0);\n+    if (\n+      filesArray.some(file => (file.type.match(/video\\/.*/) && file.size > videoSizeLimit)\n+        || (file.type.match(/image\\/.*/) && file.size > imageSizeLimit)\n+        || (files.length + media.size + pending > maxMediaAttachments))) {",
        "comment_created_at": "2024-09-23T07:06:30+00:00",
        "comment_author": "oneiros",
        "comment_body": "This last condition does not rely on any single file. As such it does not need to be repeated for every single file. Also, as this is probably cheaper than looping over all the files, you might want to check this first.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1671316488",
    "pr_number": 30979,
    "pr_file": "streaming/index.js",
    "created_at": "2024-07-09T22:33:47+00:00",
    "commented_code": "* @param {any} req\n    * @returns {Promise.<void>}\n    */\n-  const authorizeListAccess = (listId, req) => new Promise((resolve, reject) => {\n+  const authorizeListAccess = async (listId, req) => {\n     const { accountId } = req;\n \n-    pgPool.connect((err, client, done) => {\n-      if (err) {\n-        reject();\n-        return;\n-      }\n-\n-      // @ts-ignore\n-      client.query('SELECT id, account_id FROM lists WHERE id = $1 LIMIT 1', [listId], (err, result) => {\n-        done();\n-\n-        if (err || result.rows.length === 0 || result.rows[0].account_id !== accountId) {\n-          reject();\n-          return;\n-        }\n+    const result = await pgPool.query('SELECT id, account_id FROM lists WHERE id = $1 AND account_id = $2 LIMIT 1', [listId, accountId]);\n \n-        resolve();\n-      });\n-    });\n-  });\n+    if (result.rows.length === 0) {\n+      throw new AuthenticationError('List not found');\n+    }\n+  };\n \n   /**\n    * @param {string[]} channelIds\n    * @param {http.IncomingMessage & ResolvedAccount} req\n    * @param {import('pino').Logger} log\n    * @param {function(string, string): void} output\n-   * @param {undefined | function(string[], SubscriptionListener): void} attachCloseHandler\n    * @param {'websocket' | 'eventsource'} destinationType\n    * @param {boolean=} needsFiltering\n-   * @returns {SubscriptionListener}\n+   * @returns {import('./pubsub_manager.js').MessageCallback}\n    */\n-  const streamFrom = (channelIds, req, log, output, attachCloseHandler, destinationType, needsFiltering = false) => {\n+  const streamFrom = (channelIds, req, log, output, destinationType, needsFiltering = false) => {",
    "repo_full_name": "mastodon/mastodon",
    "discussion_comments": [
      {
        "comment_id": "1671316488",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 30979,
        "pr_file": "streaming/index.js",
        "discussion_id": "1671316488",
        "commented_code": "@@ -771,40 +583,26 @@ const startServer = async () => {\n    * @param {any} req\n    * @returns {Promise.<void>}\n    */\n-  const authorizeListAccess = (listId, req) => new Promise((resolve, reject) => {\n+  const authorizeListAccess = async (listId, req) => {\n     const { accountId } = req;\n \n-    pgPool.connect((err, client, done) => {\n-      if (err) {\n-        reject();\n-        return;\n-      }\n-\n-      // @ts-ignore\n-      client.query('SELECT id, account_id FROM lists WHERE id = $1 LIMIT 1', [listId], (err, result) => {\n-        done();\n-\n-        if (err || result.rows.length === 0 || result.rows[0].account_id !== accountId) {\n-          reject();\n-          return;\n-        }\n+    const result = await pgPool.query('SELECT id, account_id FROM lists WHERE id = $1 AND account_id = $2 LIMIT 1', [listId, accountId]);\n \n-        resolve();\n-      });\n-    });\n-  });\n+    if (result.rows.length === 0) {\n+      throw new AuthenticationError('List not found');\n+    }\n+  };\n \n   /**\n    * @param {string[]} channelIds\n    * @param {http.IncomingMessage & ResolvedAccount} req\n    * @param {import('pino').Logger} log\n    * @param {function(string, string): void} output\n-   * @param {undefined | function(string[], SubscriptionListener): void} attachCloseHandler\n    * @param {'websocket' | 'eventsource'} destinationType\n    * @param {boolean=} needsFiltering\n-   * @returns {SubscriptionListener}\n+   * @returns {import('./pubsub_manager.js').MessageCallback}\n    */\n-  const streamFrom = (channelIds, req, log, output, attachCloseHandler, destinationType, needsFiltering = false) => {\n+  const streamFrom = (channelIds, req, log, output, destinationType, needsFiltering = false) => {",
        "comment_created_at": "2024-07-09T22:33:47+00:00",
        "comment_author": "ThisIsMissEm",
        "comment_body": "This method previously did multiple things:\r\n- created the listener for the messages from Redis\r\n- subscribed to the topics in redis\r\n- attached a close handler in the case of http EventSource connections\r\n\r\nThis made the code difficult to reason about. It is still currently responsible for sending the message on to the output destination, but that can't really be avoided.\r\n\r\nThis does mean that for both WebSockets and EventSource connections we need to manage the pubsub manager subscriptions independently, but eventually we'll be able to replace that with a \"subscription manager\", much like WebSocket's already has, but abstracted for all connection types.\r\n\r\nThis will mean that processing a message from redis won't need N callbacks, but instead just the one that grabs the subscribed connections and sends the messages, making the streaming server lighter overall (currently we've some GC churn by constantly declaring complex functions such as this.",
        "pr_file_module": null
      }
    ]
  }
]