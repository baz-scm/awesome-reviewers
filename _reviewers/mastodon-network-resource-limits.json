[
  {
    "discussion_id": "1947348548",
    "pr_number": 33236,
    "pr_file": "app/services/fan_out_on_write_service.rb",
    "created_at": "2025-02-08T00:21:31+00:00",
    "commented_code": "end\n   end\n \n+  def fan_out_to_profile_streams!\n+    redis.publish(\"timeline:profile:#{@status.account_id}:public\", anonymous_payload)\n+  end",
    "repo_full_name": "mastodon/mastodon",
    "discussion_comments": [
      {
        "comment_id": "1947348548",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 33236,
        "pr_file": "app/services/fan_out_on_write_service.rb",
        "discussion_id": "1947348548",
        "commented_code": "@@ -145,6 +146,10 @@ def broadcast_to_public_streams!\n     end\n   end\n \n+  def fan_out_to_profile_streams!\n+    redis.publish(\"timeline:profile:#{@status.account_id}:public\", anonymous_payload)\n+  end",
        "comment_created_at": "2025-02-08T00:21:31+00:00",
        "comment_author": "ThisIsMissEm",
        "comment_body": "It'd be better to probably use just a single topic, since the churn on subscribe/unsubscribe is going to be high otherwise \u2014 one typically isn't staying on a profile for a long period of time, but there are a significant number of \"view file, go back to notifications\" type interactions.\r\n\r\nBy using a per-account redis channel, we'd be encountering fairly significant subscribe/unsubscribe load on redis, so I'd advise against it. As it is, we've a proliferation of redis topics unnecessarily for other things (e.g., access token revocations), we should probably let the streaming server just do the filtering it needs.\r\n\r\nThe only case where we'd decide to do something different is if we were to use `pubsub numsub` to check if we should publish to redis at all in the first place.",
        "pr_file_module": null
      },
      {
        "comment_id": "1960138157",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 33236,
        "pr_file": "app/services/fan_out_on_write_service.rb",
        "discussion_id": "1947348548",
        "commented_code": "@@ -145,6 +146,10 @@ def broadcast_to_public_streams!\n     end\n   end\n \n+  def fan_out_to_profile_streams!\n+    redis.publish(\"timeline:profile:#{@status.account_id}:public\", anonymous_payload)\n+  end",
        "comment_created_at": "2025-02-18T16:53:25+00:00",
        "comment_author": "Gargron",
        "comment_body": "The alternative is to have one channel for every public or unlisted post by every account, do I understand that correctly?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1819315662",
    "pr_number": 32615,
    "pr_file": "app/services/activitypub/fetch_replies_service.rb",
    "created_at": "2024-10-28T15:49:44+00:00",
    "commented_code": "collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
    "repo_full_name": "mastodon/mastodon",
    "discussion_comments": [
      {
        "comment_id": "1819315662",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-28T15:49:44+00:00",
        "comment_author": "ClearlyClaire",
        "comment_body": "I think an infinite loop is possible if an attacker builds a collection of infinitely many empty pages.",
        "pr_file_module": null
      },
      {
        "comment_id": "1819389261",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-28T16:35:57+00:00",
        "comment_author": "sneakers-the-rat",
        "comment_body": "Ah good point. Should I add a global loop limit here as a classvar? Or should it be a method argument with a default?\r\n\r\nOr should we quit if we get an empty collection?",
        "pr_file_module": null
      },
      {
        "comment_id": "1819445543",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-28T17:13:20+00:00",
        "comment_author": "sneakers-the-rat",
        "comment_body": "[ActivityStreams](https://www.w3.org/TR/activitystreams-core/#collections) doesn't have a recommendation for malicious paging, just says to page until you get to the end\r\n\r\n[ActivityPub](https://www.w3.org/TR/activitypub/#security-c2s-response-dos) has a comment re: clients expecting to handle DoS from broken/malicious servers.\r\n\r\nso doesn't look like we have a \"standards-compliant\" call to make here, i think quitting on an empty page makes sense? if someone is feeding us an empty page that's not the `last` page, then that is incorrect behavior and we shoudn't expect objects in future pages. \r\n\r\nthe edge case would be a malicious server that wants to serve us 1000 pages of single objects, so should we have a separate page limit aside from the max status limit?\r\n\r\nedit: if we resolve this here, then in https://github.com/mastodon/mastodon/pull/32634 where the paging gets centralized into JsonLDHelper (or wherever it ends up living), we only need to answer this question once",
        "pr_file_module": null
      },
      {
        "comment_id": "1820062613",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-29T04:22:22+00:00",
        "comment_author": "trwnh",
        "comment_body": "> i think quitting on an empty page makes sense? if someone is feeding us an empty page that's not the last page, then that is incorrect behavior and we shoudn't expect objects in future pages.\r\n\r\nthere is the possibility that an empty page can be served if the server in question uses \"stable paging\" -- in this case, imagine the following scenario:\r\n\r\n- the server has an OrderedCollection sorted reverse chronologically\r\n- the server has a page size of 10\r\n- 27 items get added to the collection\r\n\r\nthe representation of the collection and its pages now looks something like this:\r\n\r\n```\r\n{\r\n3: [ 27, 26, 25, 24, 23, 22, 21 ],\r\n2: [ 20, 19, 18, 17, 16, 15, 14, 13, 12, 11],\r\n1: [ 10, 09, 08, 07, 06, 05, 04, 03, 02, 01 ]\r\n}\r\n\r\nOrderedCollection.first = 3\r\nOrderedCollection.last = 1\r\n3.next = 2\r\n2.next = 1\r\n1.prev = 2\r\n2.prev = 3\r\n```\r\n\r\nnow say you Remove item 7:\r\n\r\n```\r\n{\r\n3: [ 27, 26, 25, 24, 23, 22, 21 ],\r\n2: [ 20, 19, 18, 17, 16, 15, 14, 13, 12, 11],\r\n1: [ 10, 09, 08, 06, 05, 04, 03, 02, 01 ]\r\n}\r\n\r\nOrderedCollection.first = 3\r\nOrderedCollection.last = 1\r\n3.next = 2\r\n2.next = 1\r\n1.prev = 2\r\n2.prev = 3\r\n```\r\n\r\ninstead of reflowing the pages so that the 1st page contains 10 items, the use of \"stable paging\" means that the 1st page is now 9 items instead of 10 items. but in exchange for that, pages 2 and 3 are unchanged. you can surely imagine how problematic it would be for caching if every removal or deletion invalidated the cache of every single page after it, in a cascade.\r\n\r\nif all items are removed from a \"stable page\", you have the chance of ending up with a \"degenerate page\". consider what happens when you remove items 11-20:\r\n\r\n```\r\n{\r\n3: [ 27, 26, 25, 24, 23, 22, 21 ],\r\n2: [ ],\r\n1: [ 10, 09, 08, 06, 05, 04, 03, 02, 01 ]\r\n}\r\n\r\nOrderedCollection.first = 3\r\nOrderedCollection.last = 1\r\n3.next = 2\r\n2.next = 1\r\n1.prev = 2\r\n2.prev = 3\r\n```\r\n\r\nnow, page 1 contains 9 items, page 2 contains 0 items, and page 3 contains 7 items.\r\n\r\n> the edge case would be a malicious server that wants to serve us 1000 pages of single objects, so should we have a separate page limit aside from the max status limit?\r\n\r\nthe \"out\" here is that you need to:\r\n\r\n- keep track of visited page ids, so that you never revisit a page that you've already \"seen\". this prevents loops.\r\n- set a \"max recursion limit\" for how many pages you visit before you stop. this prevents unbounded recursion.\r\n\r\none other thing to consider is that perhaps this doesn't need to happen automatically; perhaps it could be triggered by a user action, so that the user can request \"show me more replies\" and the user-agent (the instance, acting as an activitypub client) can go fetch those replies on demand. the parameters of this action would be:\r\n\r\n- depth. reddit for example i think uses a parameter `?context` which defaults to 3, so by default it will show 3 replies upward or downward. any more than that, and the \"see more replies\" link will be shown below as a child.\r\n- width. this is how many direct descendants to show. i don't know what a good limit is, but using reddit as an example again, it seems to be anywhere from 3-100 depending on the depth in the tree. when the limit gets exceeded, you see a different \"see more replies\" button\", which is placed at the same level alongside the replies.\r\n\r\na final thing to consider is that it is probably simpler to introduce the concept of a \"conversation\" entity (or broaden the existing one from being narrowly scoped to mentions-only statuses) which contains all statuses in the conversation (ideally decided by a conversation's owner), then show this as a flat chronological list, with the `inReplyTo` being metadata shown above each status. but this is something that should happen more generally and broadly outside of this PR.",
        "pr_file_module": null
      },
      {
        "comment_id": "1820102496",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-29T05:19:13+00:00",
        "comment_author": "sneakers-the-rat",
        "comment_body": "Alright good point, so we can't just quit on empty pages. Maybe we can have some lower threshold for `n` empty pages in a row though. \r\n\r\n> keep track of visited page ids, so that you never revisit a page that you've already \"seen\". this prevents loops.\r\n\r\nthat would be easy to do here, and a good idea too\r\n\r\n> set a \"max recursion limit\" for how many pages you visit before you stop. this prevents unbounded recursion.\r\n\r\nya basically this is what i was asking about above. so maybe some combination of the two values of `n` blank pages and `m` max pages.\r\n\r\n> one other thing to consider is that perhaps this doesn't need to happen automatically; perhaps it could be triggered by a user action\r\n\r\nha yes [i wish `context` was paginated too](https://github.com/mastodon/mastodon/issues/11029#issuecomment-2423630034). Since this is a backend service, it's fine to fetch all the replies that need fetching, the decision is basically how often to do that. if user intent is to look closer at a status, maybe they also want to eg. search among the replies, so having a reasonable approximation of \"all\" the replies matches that, and also meets the stated needs that people want to see the same thing as other people on different instances, modulo blocks/mutes/filters/etc. \r\n\r\nwhat gets sent to the client would ideally be paged tho and i think you're right a shallow broad paging mechanism for context trees would be the right call.\r\n\r\n> a final thing to consider is that it is probably simpler to introduce the concept of a \"conversation\" entity\r\n\r\nya agreed seems unrelated to this pr",
        "pr_file_module": null
      },
      {
        "comment_id": "1821186580",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-29T16:38:47+00:00",
        "comment_author": "ClearlyClaire",
        "comment_body": "I think having a low recursion limit makes sense. Every request is costly (up to a few seconds of waiting on a remote server), and the more we do in the service, the longer we will keep a worker busy. This can easily become a DoS vector.\r\n\r\nAs for paginating the context, yes, that would be extremely useful, and it could be useful in this specific case as well, but as you may have noticed this is a massively complex issue that also involves a lot of UX considerations (for instance, Mastodon's display of replies is \u201cflattened\u201d so the pagination cutoffs are far less obvious to represent than on say, reddit)",
        "pr_file_module": null
      },
      {
        "comment_id": "1821385581",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-29T18:57:40+00:00",
        "comment_author": "sneakers-the-rat",
        "comment_body": "> I think having a low recursion limit makes sense.\r\n\r\nChecking on how we are meaning recursion here, just because activitystreams also refers to just iterating through collection pages as recursion: do we mean a limit on paging or do we mean limiting the recursion depth within a reply tree (only get e.g. `depth<n` replies)?\r\n\r\nTo me a limit on paging makes perfect sense as a complementary limit to the max statuses limit, but a depth in reply tree recursion is in my opinion undesirable: In practice i have been seeing a pretty even distribution between deep and broad trees, and since there is no paging in the context endpoint, the experience for both being \"i go to the status, and a few seconds later I refresh, and all the replies are there\" is ... great! Since the action is implicit on loading the context endpoint, and we don't have the ux pieces in place for \"see more replies\" and recursively expanding trees, I would think that some invisible barrier where someone finds out later 'oh i wasn't actually seeing all the replies, i needed to click around more' would be pretty frustrating.\r\n\r\nI think the max page limit handles concerns re: runtime/resource usage in either case of deep and broad trees well, and does it more directly - we care about the number of requests/pages, not the depth of trees per se, the depth of trees is just a proxy for that.\r\n\r\n> Every request is costly (up to a few seconds of waiting on a remote server), and the more we do in the service, the longer we will keep a worker busy\r\n\r\nFor the sake of having it in thread (assuming you have read it, but for the benefit of passers-by the haven't), the way the service works is that there is one long-running `FetchAllRepliesWorker`. That is just getting a list of URIs to fetch from the replies collections. It then hands off those URIs in batches to sets of `FetchReplyWorker`s. So the long-running worker should be mostly I/O bound as you say. We have found it necessary to [modify the sidekiq queues](https://wiki.neuromatch.social/Performance#Sidekiq) when we were as small as a few dozen users, so that might be a thing worth exploring making default separately if we are concerned about long running workers clogging up slots. Otherwise, it makes sense to me for this worker to be one of the main things that occupies the `pull` queue, as most of the rest of the things that use it are even more background-oriented tasks than this is - the one that we see running most commonly are the `SynchronizeFeatured*Collection` workers.\r\n\r\nI think this makes sense to make configurable rather than setting a hard limit - for small instances, they probably want to have a higher limit because they are the ones most impacted by not seeing replies, don't have a million users requesting threads, and can let a worker linger for longer. for large instances, they probably want to set the page limit lower, etc. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1822390745",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-30T11:08:04+00:00",
        "comment_author": "ClearlyClaire",
        "comment_body": "> Checking on how we are meaning recursion here, just because activitystreams also refers to just iterating through collection pages as recursion: do we mean a limit on paging or do we mean limiting the recursion depth within a reply tree (only get e.g. `depth<n` replies)?\r\n\r\nI meant a pagination limit, not specifically a reply tree depth limit.\r\n\r\n> I would think that some invisible barrier where someone finds out later 'oh i wasn't actually seeing all the replies, i needed to click around more' would be pretty frustrating.\r\n\r\nThis is always a risk, whatever limits we have, unfortunately.",
        "pr_file_module": null
      },
      {
        "comment_id": "1822422853",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-10-30T11:28:18+00:00",
        "comment_author": "sneakers-the-rat",
        "comment_body": "ok yes i'll put in the limits tmrw :)\r\n\r\nalso yes definitely lmk if there is a better way to do scheduling and how to avoid clogging smaller sidekiq queues. i looked into some other async strategies that seemed like they would allow a worker to pause while awaiting from a batch of jobs, but it looks like its sidekiq pro only. if there is another long-running worker that i should mimic also lmk.",
        "pr_file_module": null
      },
      {
        "comment_id": "1843556569",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32615,
        "pr_file": "app/services/activitypub/fetch_replies_service.rb",
        "discussion_id": "1819315662",
        "commented_code": "@@ -24,18 +28,29 @@ def collection_items(collection_or_uri)\n     collection = fetch_collection(collection['first']) if collection['first'].present?\n     return unless collection.is_a?(Hash)\n \n-    case collection['type']\n-    when 'Collection', 'CollectionPage'\n-      as_array(collection['items'])\n-    when 'OrderedCollection', 'OrderedCollectionPage'\n-      as_array(collection['orderedItems'])\n+    all_items = []\n+    while collection.is_a?(Hash)\n+      items = case collection['type']\n+              when 'Collection', 'CollectionPage'\n+                collection['items']\n+              when 'OrderedCollection', 'OrderedCollectionPage'\n+                collection['orderedItems']\n+              end\n+\n+      all_items.concat(as_array(items))\n+\n+      break if all_items.size > MAX_REPLIES\n+\n+      collection = collection['next'].present? ? fetch_collection(collection['next']) : nil",
        "comment_created_at": "2024-11-15T10:28:14+00:00",
        "comment_author": "sneakers-the-rat",
        "comment_body": "completed in https://github.com/mastodon/mastodon/pull/32615/commits/e991fb890cebcc0b33450e76d6decbf6023df4c4 i think",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1827392569",
    "pr_number": 32729,
    "pr_file": "app/lib/request.rb",
    "created_at": "2024-11-04T08:58:15+00:00",
    "commented_code": "end\n \n     begin\n-      # If we are using a persistent connection, we have to\n-      # read every response to be able to move forward at all.\n-      # However, simply calling #to_s or #flush may not be safe,\n-      # as the response body, if malicious, could be too big\n-      # for our memory. So we use the #body_with_limit method\n-      response.body_with_limit if http_client.persistent?",
    "repo_full_name": "mastodon/mastodon",
    "discussion_comments": [
      {
        "comment_id": "1827392569",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32729,
        "pr_file": "app/lib/request.rb",
        "discussion_id": "1827392569",
        "commented_code": "@@ -111,16 +111,9 @@ def perform\n     end\n \n     begin\n-      # If we are using a persistent connection, we have to\n-      # read every response to be able to move forward at all.\n-      # However, simply calling #to_s or #flush may not be safe,\n-      # as the response body, if malicious, could be too big\n-      # for our memory. So we use the #body_with_limit method\n-      response.body_with_limit if http_client.persistent?",
        "comment_created_at": "2024-11-04T08:58:15+00:00",
        "comment_author": "ClearlyClaire",
        "comment_body": "Wouldn't this change require that every caller that uses persistent connection fully consume the response? Is that something we're sure we're currently doing?",
        "pr_file_module": null
      },
      {
        "comment_id": "1829877815",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32729,
        "pr_file": "app/lib/request.rb",
        "discussion_id": "1827392569",
        "commented_code": "@@ -111,16 +111,9 @@ def perform\n     end\n \n     begin\n-      # If we are using a persistent connection, we have to\n-      # read every response to be able to move forward at all.\n-      # However, simply calling #to_s or #flush may not be safe,\n-      # as the response body, if malicious, could be too big\n-      # for our memory. So we use the #body_with_limit method\n-      response.body_with_limit if http_client.persistent?",
        "comment_created_at": "2024-11-05T19:12:27+00:00",
        "comment_author": "c960657",
        "comment_body": "True. Even though we ignore the body of the POST response, we need to consume anything returned by the server.\r\n\r\nI have  now changed `#perform` to always consume up to 1 MB. This is plenty for the regular case. Only if a server sends more than that, the persistent connection will be closed.",
        "pr_file_module": null
      },
      {
        "comment_id": "1907409356",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32729,
        "pr_file": "app/lib/request.rb",
        "discussion_id": "1827392569",
        "commented_code": "@@ -111,16 +111,9 @@ def perform\n     end\n \n     begin\n-      # If we are using a persistent connection, we have to\n-      # read every response to be able to move forward at all.\n-      # However, simply calling #to_s or #flush may not be safe,\n-      # as the response body, if malicious, could be too big\n-      # for our memory. So we use the #body_with_limit method\n-      response.body_with_limit if http_client.persistent?",
        "comment_created_at": "2025-01-08T15:51:49+00:00",
        "comment_author": "ClearlyClaire",
        "comment_body": "So this changes persistent connections from never being closed by `Request` to being closed by `Request` if they return too large of a body. I wonder if this is safe. What about only closing if it's not a persistent connection?",
        "pr_file_module": null
      },
      {
        "comment_id": "1907572722",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32729,
        "pr_file": "app/lib/request.rb",
        "discussion_id": "1827392569",
        "commented_code": "@@ -111,16 +111,9 @@ def perform\n     end\n \n     begin\n-      # If we are using a persistent connection, we have to\n-      # read every response to be able to move forward at all.\n-      # However, simply calling #to_s or #flush may not be safe,\n-      # as the response body, if malicious, could be too big\n-      # for our memory. So we use the #body_with_limit method\n-      response.body_with_limit if http_client.persistent?",
        "comment_created_at": "2025-01-08T17:49:40+00:00",
        "comment_author": "c960657",
        "comment_body": "Why would closing a connection not be safe?\r\n\r\nIn order to use a persistent connection for further requests, we _must_ consume the entire body. If this turns out to be >1 MB, I think it is better to close the connection (and reconnect for further requests) rather than consume potentially gigabytes of data.",
        "pr_file_module": null
      },
      {
        "comment_id": "1908351334",
        "repo_full_name": "mastodon/mastodon",
        "pr_number": 32729,
        "pr_file": "app/lib/request.rb",
        "discussion_id": "1827392569",
        "commented_code": "@@ -111,16 +111,9 @@ def perform\n     end\n \n     begin\n-      # If we are using a persistent connection, we have to\n-      # read every response to be able to move forward at all.\n-      # However, simply calling #to_s or #flush may not be safe,\n-      # as the response body, if malicious, could be too big\n-      # for our memory. So we use the #body_with_limit method\n-      response.body_with_limit if http_client.persistent?",
        "comment_created_at": "2025-01-09T08:35:28+00:00",
        "comment_author": "ClearlyClaire",
        "comment_body": "I was not completely sure how HTTP.rb handled closed connections. It seems it silently reopens a connection, which seems safe in this case.",
        "pr_file_module": null
      }
    ]
  }
]