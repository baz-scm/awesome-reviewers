[
  {
    "discussion_id": "764007192",
    "pr_number": 20757,
    "pr_file": "src/operator/nn/dnnl/dnnl_split.cc",
    "created_at": "2021-12-07T13:46:24+00:00",
    "commented_code": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\n/*!\n * \\file dnnl_split.cc\n */\n\n#if MXNET_USE_ONEDNN == 1\n\n#include \"../../tensor/matrix_op-inl.h\"\n#include \"./dnnl_split-inl.h\"\n\nnamespace mxnet {\nnamespace op {\n\nbool SupportDNNLSplit(const NDArray& input) {\n  static const std::set<int> supported_dtypes = {\n      mshadow::kFloat32, mshadow::kBfloat16, mshadow::kInt32, mshadow::kInt8, mshadow::kUint8};\n  return supported_dtypes.count(input.dtype());\n}\n\nvoid DNNLSplitForward(const nnvm::NodeAttrs& attrs,\n                      const OpContext& ctx,\n                      const std::vector<NDArray>& inputs,\n                      const std::vector<OpReqType>& req,\n                      const std::vector<NDArray>& outputs) {\n  const SplitParam& param = dmlc::get<SplitParam>(attrs.parsed);\n  const auto tensors      = DNNLSplitFwd::Tensors(inputs[0], outputs);\n\n  const auto& ishape   = tensors.input.shape();\n  const int split_axis = param.axis >= 0 ? param.axis : param.axis + ishape.ndim();\n  const mxnet::TShape split_pts =\n      (param.sections > 0) ? GetSplitIndices(tensors.input.shape(), split_axis, param.sections) :\n                             param.indices;\n\n  const auto fwd = DNNLSplitFwd::GetCached(param, tensors, split_pts, split_axis);\n  fwd.Execute(tensors, split_pts, split_axis, req);\n}\n\nDNNLSplitFwd::Tensors::Tensors(const NDArray& input, const std::vector<NDArray>& outputs)\n    : input(input), outputs(outputs) {}\n\ntypedef ParamOpSign<SplitParam> DNNLSplitSignature;\n\nDNNLSplitFwd DNNLSplitFwd::GetCached(const SplitParam& param,\n                                     const Tensors& tensors,\n                                     const TShape& split_pts,\n                                     const int split_axis) {\n#if DMLC_CXX11_THREAD_LOCAL\n  static thread_local std::unordered_map<DNNLSplitSignature, DNNLSplitFwd, OpHash> fwds;\n#else\n  static MX_THREAD_LOCAL std::unordered_map<DNNLSplitSignature, DNNLSplitFwd, OpHash> fwds;\n#endif\n\n  DNNLSplitSignature key(param);\n  key.AddSign(tensors.input);\n  key.AddSign(tensors.outputs);\n  key.AddSign(split_pts);\n  key.AddSign(split_axis);\n  auto it = fwds.find(key);\n  if (it == fwds.end()) {\n    DNNLSplitFwd fwd(tensors, split_pts, split_axis);\n    it = AddToCache(&fwds, key, fwd);\n  }\n  return it->second;\n}\n\nDNNLSplitFwd::DNNLSplitFwd(const Tensors& tensors, const TShape& split_pts, const int split_axis) {\n  const auto cpu_engine = CpuEngine::Get()->get_engine();\n  const auto input      = tensors.input.Reorder2Default();\n  const auto& ishape    = input.shape();\n  const auto& dtype     = get_dnnl_type(input.dtype());\n  const auto format_tag = static_cast<dnnl::memory::format_tag>(GetDefaultFormat(ishape.ndim()));\n\n  dnnl::memory::dims strides(ishape.ndim(), 1);\n  // last dim stride = 1, start loop from the penultimate\n  for (int i = ishape.ndim() - 2; i >= 0; --i) {\n    strides[i] = strides[i + 1] * ishape[i + 1];",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "764007192",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20757,
        "pr_file": "src/operator/nn/dnnl/dnnl_split.cc",
        "discussion_id": "764007192",
        "commented_code": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/*!\n+ * \\file dnnl_split.cc\n+ */\n+\n+#if MXNET_USE_ONEDNN == 1\n+\n+#include \"../../tensor/matrix_op-inl.h\"\n+#include \"./dnnl_split-inl.h\"\n+\n+namespace mxnet {\n+namespace op {\n+\n+bool SupportDNNLSplit(const NDArray& input) {\n+  static const std::set<int> supported_dtypes = {\n+      mshadow::kFloat32, mshadow::kBfloat16, mshadow::kInt32, mshadow::kInt8, mshadow::kUint8};\n+  return supported_dtypes.count(input.dtype());\n+}\n+\n+void DNNLSplitForward(const nnvm::NodeAttrs& attrs,\n+                      const OpContext& ctx,\n+                      const std::vector<NDArray>& inputs,\n+                      const std::vector<OpReqType>& req,\n+                      const std::vector<NDArray>& outputs) {\n+  const SplitParam& param = dmlc::get<SplitParam>(attrs.parsed);\n+  const auto tensors      = DNNLSplitFwd::Tensors(inputs[0], outputs);\n+\n+  const auto& ishape   = tensors.input.shape();\n+  const int split_axis = param.axis >= 0 ? param.axis : param.axis + ishape.ndim();\n+  const mxnet::TShape split_pts =\n+      (param.sections > 0) ? GetSplitIndices(tensors.input.shape(), split_axis, param.sections) :\n+                             param.indices;\n+\n+  const auto fwd = DNNLSplitFwd::GetCached(param, tensors, split_pts, split_axis);\n+  fwd.Execute(tensors, split_pts, split_axis, req);\n+}\n+\n+DNNLSplitFwd::Tensors::Tensors(const NDArray& input, const std::vector<NDArray>& outputs)\n+    : input(input), outputs(outputs) {}\n+\n+typedef ParamOpSign<SplitParam> DNNLSplitSignature;\n+\n+DNNLSplitFwd DNNLSplitFwd::GetCached(const SplitParam& param,\n+                                     const Tensors& tensors,\n+                                     const TShape& split_pts,\n+                                     const int split_axis) {\n+#if DMLC_CXX11_THREAD_LOCAL\n+  static thread_local std::unordered_map<DNNLSplitSignature, DNNLSplitFwd, OpHash> fwds;\n+#else\n+  static MX_THREAD_LOCAL std::unordered_map<DNNLSplitSignature, DNNLSplitFwd, OpHash> fwds;\n+#endif\n+\n+  DNNLSplitSignature key(param);\n+  key.AddSign(tensors.input);\n+  key.AddSign(tensors.outputs);\n+  key.AddSign(split_pts);\n+  key.AddSign(split_axis);\n+  auto it = fwds.find(key);\n+  if (it == fwds.end()) {\n+    DNNLSplitFwd fwd(tensors, split_pts, split_axis);\n+    it = AddToCache(&fwds, key, fwd);\n+  }\n+  return it->second;\n+}\n+\n+DNNLSplitFwd::DNNLSplitFwd(const Tensors& tensors, const TShape& split_pts, const int split_axis) {\n+  const auto cpu_engine = CpuEngine::Get()->get_engine();\n+  const auto input      = tensors.input.Reorder2Default();\n+  const auto& ishape    = input.shape();\n+  const auto& dtype     = get_dnnl_type(input.dtype());\n+  const auto format_tag = static_cast<dnnl::memory::format_tag>(GetDefaultFormat(ishape.ndim()));\n+\n+  dnnl::memory::dims strides(ishape.ndim(), 1);\n+  // last dim stride = 1, start loop from the penultimate\n+  for (int i = ishape.ndim() - 2; i >= 0; --i) {\n+    strides[i] = strides[i + 1] * ishape[i + 1];",
        "comment_created_at": "2021-12-07T13:46:24+00:00",
        "comment_author": "agrabows",
        "comment_body": "This vector could be an attribute of DNNLSplitFwd class to avoid duplicate loop in Execute() function.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "746524936",
    "pr_number": 20724,
    "pr_file": "src/operator/subgraph/dnnl/dnnl_post_quantize_property.h",
    "created_at": "2021-11-10T12:06:59+00:00",
    "commented_code": "#define MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_POST_QUANTIZE_PROPERTY_H_\n#if MXNET_USE_ONEDNN == 1\n\n#include <memory>\n#include <set>\n#include <string>\n#include <vector>\n\n#include \"../../nn/dnnl/dnnl_convolution-inl.h\"\n#include \"../../nn/fully_connected-inl.h\"\n#include \"../../quantization/requantize-inl.h\"\n#include \"../../tensor/elemwise_binary_op-inl.h\"\n#include \"../common.h\"\n#include \"dnnl_conv-inl.h\"\n#include \"dnnl_subgraph_base-inl.h\"\n\nnamespace mxnet {\nnamespace op {\n\nclass SgDNNLPostQuantizeSelector : public SubgraphSelector {\nconst std::set<std::string> support_req_fusion_op = {\"_contrib_quantized_elemwise_add\",\n                                                     \"_contrib_quantized_elemwise_mul\",\n                                                     \"_contrib_quantized_npi_add\",\n                                                     \"_sg_onednn_conv\",\n                                                     \"_sg_onednn_fully_connected\",\n                                                     \"_sg_onednn_selfatt_qk\",\n                                                     \"_sg_onednn_selfatt_valatt\",\n                                                     \"_sg_onednn_batch_dot\"};\n\nclass SgDNNLPostQuantizeSelector : public SubgraphSelectorV2 {\n public:\n  /*! \\brief pattern match status */\n  enum SelectStatus {\n    kFail = 0,\n    kStart,\n    kRequantize,\n    kSuccess,\n  };\n\n private:\n  bool disable_fuse_all;\n  bool disable_float_output;\n  SelectStatus status;\n  std::vector<const nnvm::Node*> matched_list;\n  std::vector<const BiDirectedNode*> matched_list;\n  std::set<std::string> support_requantize_fusion_op_name;\n\n public:\n  SgDNNLPostQuantizeSelector() {\n    support_requantize_fusion_op_name.insert(\"_sg_onednn_conv\");\n    support_requantize_fusion_op_name.insert(\"_contrib_quantized_elemwise_add\");\n    support_requantize_fusion_op_name.insert(\"_contrib_quantized_npi_add\");\n  explicit SgDNNLPostQuantizeSelector(const bool dis_fuse_all, const bool dis_float_output)\n      : disable_fuse_all(dis_fuse_all), disable_float_output(dis_float_output) {\n    support_requantize_fusion_op_name = support_req_fusion_op;\n  }\n\n  bool Select(const nnvm::Node& n) override {\n    if (n.op() && support_requantize_fusion_op_name.count(n.op()->name)) {\n      if (n.op() == Op::Get(\"_sg_onednn_conv\")) {\n        auto const& param = nnvm::get<DNNLConvFusionParam>(n.attrs.parsed);\n        if (param.full_conv_param.dnnl_param.quantized) {\n          status = kStart;\n          matched_list.clear();\n          matched_list.push_back(&n);\n          return true;\n        }\n      } else if (n.op()->name == \"_contrib_quantized_elemwise_add\" ||\n                 n.op()->name == \"_contrib_quantized_npi_add\") {\n        status = kStart;\n        matched_list.clear();\n        matched_list.push_back(&n);\n        return true;\n      }\n  bool Select(const BiDirectedNode& n) override {\n    const nnvm::Node* raw_node = n.node;\n    if ((!disable_fuse_all) && raw_node->op() &&\n        support_requantize_fusion_op_name.count(raw_node->op()->name)) {\n      status = kStart;\n      matched_list.clear();\n      matched_list.push_back(&n);",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "746524936",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20724,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_post_quantize_property.h",
        "discussion_id": "746524936",
        "commented_code": "@@ -20,146 +20,209 @@\n #define MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_POST_QUANTIZE_PROPERTY_H_\n #if MXNET_USE_ONEDNN == 1\n \n+#include <memory>\n #include <set>\n #include <string>\n #include <vector>\n \n #include \"../../nn/dnnl/dnnl_convolution-inl.h\"\n+#include \"../../nn/fully_connected-inl.h\"\n #include \"../../quantization/requantize-inl.h\"\n+#include \"../../tensor/elemwise_binary_op-inl.h\"\n #include \"../common.h\"\n #include \"dnnl_conv-inl.h\"\n #include \"dnnl_subgraph_base-inl.h\"\n \n namespace mxnet {\n namespace op {\n \n-class SgDNNLPostQuantizeSelector : public SubgraphSelector {\n+const std::set<std::string> support_req_fusion_op = {\"_contrib_quantized_elemwise_add\",\n+                                                     \"_contrib_quantized_elemwise_mul\",\n+                                                     \"_contrib_quantized_npi_add\",\n+                                                     \"_sg_onednn_conv\",\n+                                                     \"_sg_onednn_fully_connected\",\n+                                                     \"_sg_onednn_selfatt_qk\",\n+                                                     \"_sg_onednn_selfatt_valatt\",\n+                                                     \"_sg_onednn_batch_dot\"};\n+\n+class SgDNNLPostQuantizeSelector : public SubgraphSelectorV2 {\n  public:\n   /*! \\brief pattern match status */\n   enum SelectStatus {\n     kFail = 0,\n     kStart,\n+    kRequantize,\n     kSuccess,\n   };\n \n  private:\n+  bool disable_fuse_all;\n+  bool disable_float_output;\n   SelectStatus status;\n-  std::vector<const nnvm::Node*> matched_list;\n+  std::vector<const BiDirectedNode*> matched_list;\n   std::set<std::string> support_requantize_fusion_op_name;\n \n  public:\n-  SgDNNLPostQuantizeSelector() {\n-    support_requantize_fusion_op_name.insert(\"_sg_onednn_conv\");\n-    support_requantize_fusion_op_name.insert(\"_contrib_quantized_elemwise_add\");\n-    support_requantize_fusion_op_name.insert(\"_contrib_quantized_npi_add\");\n+  explicit SgDNNLPostQuantizeSelector(const bool dis_fuse_all, const bool dis_float_output)\n+      : disable_fuse_all(dis_fuse_all), disable_float_output(dis_float_output) {\n+    support_requantize_fusion_op_name = support_req_fusion_op;\n   }\n \n-  bool Select(const nnvm::Node& n) override {\n-    if (n.op() && support_requantize_fusion_op_name.count(n.op()->name)) {\n-      if (n.op() == Op::Get(\"_sg_onednn_conv\")) {\n-        auto const& param = nnvm::get<DNNLConvFusionParam>(n.attrs.parsed);\n-        if (param.full_conv_param.dnnl_param.quantized) {\n-          status = kStart;\n-          matched_list.clear();\n-          matched_list.push_back(&n);\n-          return true;\n-        }\n-      } else if (n.op()->name == \"_contrib_quantized_elemwise_add\" ||\n-                 n.op()->name == \"_contrib_quantized_npi_add\") {\n-        status = kStart;\n-        matched_list.clear();\n-        matched_list.push_back(&n);\n-        return true;\n-      }\n+  bool Select(const BiDirectedNode& n) override {\n+    const nnvm::Node* raw_node = n.node;\n+    if ((!disable_fuse_all) && raw_node->op() &&\n+        support_requantize_fusion_op_name.count(raw_node->op()->name)) {\n+      status = kStart;\n+      matched_list.clear();\n+      matched_list.push_back(&n);",
        "comment_created_at": "2021-11-10T12:06:59+00:00",
        "comment_author": "mozga-intel",
        "comment_body": "If we clear the vector here, the capacity is the same as before; Then it might be better to use `emplace_back(&n)` instead of `push_back(n)` ~ I hope to avoid creating temp object before.\r\nHow about using?\r\n```suggestion\r\n      matched_list.emplace_back(&n);\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "744752804",
    "pr_number": 20621,
    "pr_file": "src/operator/nn/dnnl/dnnl_concat-inl.h",
    "created_at": "2021-11-08T14:03:21+00:00",
    "commented_code": "static DNNLConcatFwd& GetConcatForward(int concat_dim,\n                                       const std::vector<NDArray>& in_data,\n                                       const std::vector<dnnl::memory::desc>& data_md) {\n                                       const std::vector<dnnl::memory::desc>& data_md,\n                                       int cache_dim = -1) {\n#if DMLC_CXX11_THREAD_LOCAL\n  static thread_local std::unordered_map<OpSignature, DNNLConcatFwd, OpHash> fwds;\n#else\n  static MX_THREAD_LOCAL std::unordered_map<OpSignature, DNNLConcatFwd, OpHash> fwds;\n#endif\n  if (cache_dim == -1) {\n    cache_dim = concat_dim;",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "744752804",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20621,
        "pr_file": "src/operator/nn/dnnl/dnnl_concat-inl.h",
        "discussion_id": "744752804",
        "commented_code": "@@ -52,13 +52,18 @@ class DNNLConcatFwd {\n \n static DNNLConcatFwd& GetConcatForward(int concat_dim,\n                                        const std::vector<NDArray>& in_data,\n-                                       const std::vector<dnnl::memory::desc>& data_md) {\n+                                       const std::vector<dnnl::memory::desc>& data_md,\n+                                       int cache_dim = -1) {\n #if DMLC_CXX11_THREAD_LOCAL\n   static thread_local std::unordered_map<OpSignature, DNNLConcatFwd, OpHash> fwds;\n #else\n   static MX_THREAD_LOCAL std::unordered_map<OpSignature, DNNLConcatFwd, OpHash> fwds;\n #endif\n+  if (cache_dim == -1) {\n+    cache_dim = concat_dim;",
        "comment_created_at": "2021-11-08T14:03:21+00:00",
        "comment_author": "mozga-intel",
        "comment_body": "What is the advantage of adding the same value here (twofold: cache_dim if cache_dim == 1)?",
        "pr_file_module": null
      },
      {
        "comment_id": "745458419",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20621,
        "pr_file": "src/operator/nn/dnnl/dnnl_concat-inl.h",
        "discussion_id": "744752804",
        "commented_code": "@@ -52,13 +52,18 @@ class DNNLConcatFwd {\n \n static DNNLConcatFwd& GetConcatForward(int concat_dim,\n                                        const std::vector<NDArray>& in_data,\n-                                       const std::vector<dnnl::memory::desc>& data_md) {\n+                                       const std::vector<dnnl::memory::desc>& data_md,\n+                                       int cache_dim = -1) {\n #if DMLC_CXX11_THREAD_LOCAL\n   static thread_local std::unordered_map<OpSignature, DNNLConcatFwd, OpHash> fwds;\n #else\n   static MX_THREAD_LOCAL std::unordered_map<OpSignature, DNNLConcatFwd, OpHash> fwds;\n #endif\n+  if (cache_dim == -1) {\n+    cache_dim = concat_dim;",
        "comment_created_at": "2021-11-09T09:59:18+00:00",
        "comment_author": "bgawrych",
        "comment_body": "Changed to adding -1 value to key. It's necessary for stack op as there is used trick creating always 3 dimensional memory descriptor [leading_dim, mid_dim, trailing_dim]. For the same input shapes stack op can be performed on different axis. Without adding new key returned primitive would be the same for both scenarios which is wrong.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "249252790",
    "pr_number": 13811,
    "pr_file": "src/operator/contrib/erfinv-inl.h",
    "created_at": "2019-01-19T18:14:11+00:00",
    "commented_code": "/*\n * Copyright (c) 2014 Indiana University\n * All rights reserved.\n * Written by Prof. Gary L. Pavlis, Dept. of Geol. Sci.,\n *           Indiana University, Bloomington, IN\n * This software is licensed under the New BSD license:\n * Redistribution and use in source and binary forms,\n * with or without modification, are permitted provided\n * that the following conditions are met:\n * Redistributions of source code must retain the above\n * copyright notice, this list of conditions and the\n * following disclaimer.\n * Redistributions in binary form must reproduce the\n * above copyright notice, this list of conditions and\n * the following disclaimer in the documentation and/or\n * other materials provided with the distribution.\n * Neither the name of Indiana University nor\n * the names of its contributors may be used to endorse\n * or promote products derived from this software without\n * specific prior written permission.\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\n * CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED\n * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n * PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL\n * THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY\n * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF\n * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER\n * IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n */\n/*\n * The next function is taken from\n * https://github.com/antelopeusersgroup/antelope_contrib/blob/master/lib/location/libgenloc/erfinv.c.\n * Output was modified to be inf or -inf when input is 1 or -1.\n */\n#ifndef MXNET_OPERATOR_CONTRIB_ERFINV_INL_H_\n#define MXNET_OPERATOR_CONTRIB_ERFINV_INL_H_\n\n#define _USE_MATH_DEFINES\n#define CENTRAL_RANGE 0.7\n\n#include <mxnet/base.h>\n#include <limits>\n#include \"math.h\"\n\nnamespace mxnet {\nnamespace op {\nnamespace mshadow_op {\n\n/*! \\brief inverse gauss error function */\nstruct erfinv : public mxnet_op::tunable {\n  template<typename DType>\n  MSHADOW_XINLINE static DType Map(DType v) {\n    /* Function to calculate inverse error function.  Rational approximation\n    is used to generate an initial approximation, which is then improved to\n    full accuracy by two steps of Newton's method.  Code is a direct\n    translation of the erfinv m file in matlab version 2.0.\n    Author:  Gary L. Pavlis, Indiana University\n    Date:  February 1996\n    */\n    double y = static_cast<double>(v);\n    /*working variables */\n    double x = 0.0;\n    double z, num, dem;\n    /* coefficients in rational expansion */\n    double a[4]={ 0.886226899, -1.645349621,  0.914624893, -0.140543331};\n    double b[4]={-2.118377725,  1.442710462, -0.329097515,  0.012229801};\n    double c[4]={-1.970840454, -1.624906493,  3.429567803,  1.641345311};\n    double d[2]={ 3.543889200,  1.637067800};\n    if (fabs(y) > 1.0) {",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "249252790",
        "repo_full_name": "apache/mxnet",
        "pr_number": 13811,
        "pr_file": "src/operator/contrib/erfinv-inl.h",
        "discussion_id": "249252790",
        "commented_code": "@@ -0,0 +1,105 @@\n+/*\n+ * Copyright (c) 2014 Indiana University\n+ * All rights reserved.\n+ * Written by Prof. Gary L. Pavlis, Dept. of Geol. Sci.,\n+ *           Indiana University, Bloomington, IN\n+ * This software is licensed under the New BSD license:\n+ * Redistribution and use in source and binary forms,\n+ * with or without modification, are permitted provided\n+ * that the following conditions are met:\n+ * Redistributions of source code must retain the above\n+ * copyright notice, this list of conditions and the\n+ * following disclaimer.\n+ * Redistributions in binary form must reproduce the\n+ * above copyright notice, this list of conditions and\n+ * the following disclaimer in the documentation and/or\n+ * other materials provided with the distribution.\n+ * Neither the name of Indiana University nor\n+ * the names of its contributors may be used to endorse\n+ * or promote products derived from this software without\n+ * specific prior written permission.\n+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\n+ * CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED\n+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n+ * PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL\n+ * THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY\n+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF\n+ * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER\n+ * IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n+ * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n+ * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n+ * POSSIBILITY OF SUCH DAMAGE.\n+ */\n+/*\n+ * The next function is taken from\n+ * https://github.com/antelopeusersgroup/antelope_contrib/blob/master/lib/location/libgenloc/erfinv.c.\n+ * Output was modified to be inf or -inf when input is 1 or -1.\n+ */\n+#ifndef MXNET_OPERATOR_CONTRIB_ERFINV_INL_H_\n+#define MXNET_OPERATOR_CONTRIB_ERFINV_INL_H_\n+\n+#define _USE_MATH_DEFINES\n+#define CENTRAL_RANGE 0.7\n+\n+#include <mxnet/base.h>\n+#include <limits>\n+#include \"math.h\"\n+\n+namespace mxnet {\n+namespace op {\n+namespace mshadow_op {\n+\n+/*! \\brief inverse gauss error function */\n+struct erfinv : public mxnet_op::tunable {\n+  template<typename DType>\n+  MSHADOW_XINLINE static DType Map(DType v) {\n+    /* Function to calculate inverse error function.  Rational approximation\n+    is used to generate an initial approximation, which is then improved to\n+    full accuracy by two steps of Newton's method.  Code is a direct\n+    translation of the erfinv m file in matlab version 2.0.\n+    Author:  Gary L. Pavlis, Indiana University\n+    Date:  February 1996\n+    */\n+    double y = static_cast<double>(v);\n+    /*working variables */\n+    double x = 0.0;\n+    double z, num, dem;\n+    /* coefficients in rational expansion */\n+    double a[4]={ 0.886226899, -1.645349621,  0.914624893, -0.140543331};\n+    double b[4]={-2.118377725,  1.442710462, -0.329097515,  0.012229801};\n+    double c[4]={-1.970840454, -1.624906493,  3.429567803,  1.641345311};\n+    double d[2]={ 3.543889200,  1.637067800};\n+    if (fabs(y) > 1.0) {",
        "comment_created_at": "2019-01-19T18:14:11+00:00",
        "comment_author": "eric-haibin-lin",
        "comment_body": "Can we cache the value of fabs(y) instead of calculating it multiple times? ",
        "pr_file_module": null
      }
    ]
  }
]