---
title: Consistent logging format
description: Use consistent string formatting in logging statements throughout the
  codebase. Prefer `%` style placeholders over f-strings or `.format()` in logging
  calls as this is more efficient when logs are filtered by level (placeholders are
  only evaluated if the log is actually emitted).
repository: apache/mxnet
label: Logging
language: Python
comments_count: 3
repository_stars: 20801
---

Use consistent string formatting in logging statements throughout the codebase. Prefer `%` style placeholders over f-strings or `.format()` in logging calls as this is more efficient when logs are filtered by level (placeholders are only evaluated if the log is actually emitted).

For good logging practices:

1. Use placeholder style consistently:
```python
# Recommended
logging.info('%s benchmark running.', operation_type)

# Avoid mixing styles in the same codebase
logging.error('No models found in S3 bucket: {}'.format(bucket_name))
logging.warning(f'Failed to process {item_name}') # Avoid f-strings in logging
```

2. Reduce duplicate logging logic:
```python
# Instead of:
if opt.train:
    logging.info('%s training benchmark.', cell)
else:
    logging.info('%s inference benchmark.', cell)

# Prefer:
mode = 'training' if opt.train else 'inference'
logging.info('%s %s benchmark.', cell, mode)
```

3. Choose appropriate log levels based on severity:
   - Use `logging.error()` for failures that prevent normal operation
   - Use `logging.warning()` for potential issues that don't stop execution
   - Use assertions only for developer-facing invariants that should never be violated
   - Consider warnings instead of assertions in user-facing code


[
  {
    "discussion_id": "237215006",
    "pr_number": 12893,
    "pr_file": "benchmark/python/gluon/benchmark_gluon_rnn.py",
    "created_at": "2018-11-28T18:51:38+00:00",
    "commented_code": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport mxnet as mx\nimport mxnet.gluon as gluon\nimport time\nimport logging\nimport sys\nimport argparse\nfrom mxnet.gluon import nn, rnn\n\nparser = argparse.ArgumentParser(description='Gluon RNN Benchmarking.')\nparser.add_argument('--num-layer', type=int, default=1,\n                    help='The number of layers of the RNN model')\nparser.add_argument('--layout', type=str, default='TNC',\n                    help='The layout of the input shape, can be either TNC or NTC.')\nparser.add_argument('--specify-shape', type=str,\n                    help='Specify the input shape, format batchsize, time-step, embed-size, hidden-size.')\nparser.add_argument('--cell-type', type=str, default='lstm',\n                    help='RNN cell type, can be either lstm, gru or all to cover both.')\nparser.add_argument('--unfuse', action='store_true', default=False,\n                    help='Unfuse the RNN layer to stacked RNN cell instead.') \nparser.add_argument('--latency', action='store_true', default=False,\n                    help='Measursing the latency, batchsize will be set to 1.')\nparser.add_argument('--train', action='store_true', default=False,\n                    help='Run backward benchmark.')\nparser.add_argument('--dropout', type=float, default=0, \n                    help='float, default 0; Fraction of the input that gets dropped out during training time.')\nparser.add_argument('--gpu', action='store_true', default=False,\n                    help='bool, default Flase; set True to run benchmarks on GPU.')\nparser.add_argument('--no-hybridize', action='store_true', default=False,\n                    help='bool, default False; hybridize will introduce a better perf.')\nparser.add_argument('--bidirection', action='store_true', default=False,\n                    help='bool, default False; Whether to use bidirectional unroll.')\n\nopt = parser.parse_args()\nlogging.basicConfig(level=logging.INFO)\n\n#[bs, sequence length, embedding size, hidden size]\ninput_shape_list = [[64,15,500,500],\n   [64,20,500,500],\n   [64,25,500,500],\n   [64,30,500,500],\n   [64,35,500,500],\n   [64,40,500,500],\n   [64,45,500,500],\n   [64,50,500,500],\n   [16,25,512,512],\n   [32,25,512,512],\n   [64,25,512,512],\n   [128,25,512,512],\n   [16,25,1024,1024],\n   [32,25,1024,1024],\n   [64,25,1024,1024],\n   [128,25,1024,1024],\n   [16,25,2048,2048],\n   [32,25,2048,2048],\n   [64,25,2048,2048],\n   [128,25,2048,2048],\n   [16,25,4096,4096],\n   [32,25,4096,4096],\n   [64,25,4096,4096],\n   [128,25,4096,4096]]\n\nrnncell_type = ['lstm', 'gru', 'all']\ninput_layout = ['TNC', 'NTC']\n\nif not opt.gpu:\n    ctx = mx.cpu()\nelse:\n    ctx = mx.gpu(0)\n\ndropout = opt.dropout\nbidirection = opt.bidirection\nunfuse = opt.unfuse\ncelltype = opt.cell_type\n\ndry_run = 20\nnum_iter = 100\n\ndef get_rnn_layer(input_shape, num_layer, cell_type, dropout=0, bidirection=False):\n    hidden_size = input_shape[3]\n    embedding_size = input_shape[2]\n    if cell_type == 'lstm':\n        rnn_layer = rnn.LSTM(hidden_size, num_layer, dropout=dropout,\n                             bidirectional=bidirection, input_size=embedding_size,\n                             prefix='_lstm_layer')\n    elif cell_type == 'gru':\n        rnn_layer = rnn.GRU(hidden_size, num_layer, dropout=dropout,\n                            bidirectional=bidirection, input_size=embedding_size,\n                            prefix='_gru_layer')\n    return rnn_layer\n\n\ndef rnn_cell_score(input_shape, cell_type, ctx, num_layer, dropout=0, bidirection=False, layout='TNC', unfuse=False, hybridize=True, is_train=False):\n    bs = input_shape[0]\n    seq_len = input_shape[1]\n    embedding_size = input_shape[2]\n    hidden_size = input_shape[3]\n    rnn_layer = get_rnn_layer(input_shape, num_layer, cell_type, dropout, bidirection)\n    input_data = mx.sym.Variable('data')\n\n    if unfuse:\n        rnn_cell = rnn_layer._unfuse()\n        if hybridize:\n            rnn_cell.hybridize()\n        out, _ = rnn_cell.unroll(length=seq_len, inputs = input_data, layout=layout, merge_outputs=True)\n    else:\n        if hybridize:\n            rnn_layer.hybridize()\n        out = rnn_layer(input_data)\n\n    if is_train: \n        #out = mx.sym.slice(out, begin=(0, None), end=(bs, None))\n        hidden = mx.sym.Reshape(data = out, shape=(-1, hidden_size))\n        pred = mx.sym.FullyConnected(data=hidden, num_hidden=embedding_size, name='pred')\n        if layout == 'TNC':\n            pred = mx.sym.Reshape(data=pred, shape=(seq_len, -1, embedding_size))\n        elif layout == 'NTC':\n            pred = mx.sym.Reshape(data=pred, shape=(-1, seq_len, embedding_size))\n        softmax_output = mx.sym.SoftmaxOutput(data=pred, name='softmax')\n\n    if layout == 'NTC':\n        dshape = (bs, seq_len, embedding_size)\n    elif layout == 'TNC':\n        dshape = (seq_len, bs, embedding_size)\n    \n    if is_train:\n        mod = mx.mod.Module(softmax_output, label_names=('softmax_label',), context=ctx)\n    else:\n        mod = mx.mod.Module(out, label_names=None, context=ctx)\n    \n    if is_train:\n        if layout == 'TNC':\n            mod.bind(for_training = True, data_shapes=[('data', dshape)],\n                label_shapes=[('softmax_label', (seq_len, bs, embedding_size))])\n        elif layout == 'NTC':\n            mod.bind(for_training = True, data_shapes=[('data', dshape)],\n                label_shapes=[('softmax_label', (bs, seq_len, embedding_size))])\n        \n    else:\n        mod.bind(data_shapes=[('data', dshape)], label_shapes=None)\n\n    batch = mx.io.DataBatch(data=[mx.random.uniform(shape=dshape)], label=[])\n    mod.init_params(initializer=mx.init.Xavier(magnitude=2.))\n    if is_train:\n        mod.init_optimizer(optimizer='sgd')\n        mod.forward(batch, is_train=True)\n        if unfuse:\n            for o in mod.get_outputs():\n                o.wait_to_read()\n        else:\n            o = mod.get_outputs()[0]\n            o.wait_to_read()\n        mod.backward()\n        mod.update()\n    else:\n        mod.forward(batch, is_train=False)\n        if unfuse:\n            for o in mod.get_outputs():\n                o.wait_to_read()\n        else:\n            o = mod.get_outputs()[0]\n            o.wait_to_read()\n\nif __name__ == '__main__':\n\n    num_layer = opt.num_layer\n    layout = opt.layout\n    latency = opt.latency\n    hybridize = not(opt.no_hybridize)\n    \n    if layout not in input_layout:\n        logging.warning('Only TNC or NTC are supported!')\n        sys.exit(0)\n\n    if celltype not in rnncell_type:\n        logging.warning('Only LSTM and GRU cell are supported!')\n        sys.exit(0)\n    \n    if celltype == 'all':\n        cell_lst = ['lstm', 'gru']\n    else:\n        cell_lst = [celltype]\n\n    if opt.specify_shape != None:\n        input_shape_list = [[int(x) for x in opt.specify_shape.split(',')]]\n\n    for cell in cell_lst:\n        if opt.train:\n            logging.info('%s training benchmark.', cell)\n        else:\n            logging.info('%s inference benchmark.', cell)",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "237215006",
        "repo_full_name": "apache/mxnet",
        "pr_number": 12893,
        "pr_file": "benchmark/python/gluon/benchmark_gluon_rnn.py",
        "discussion_id": "237215006",
        "commented_code": "@@ -0,0 +1,225 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+import mxnet as mx\n+import mxnet.gluon as gluon\n+import time\n+import logging\n+import sys\n+import argparse\n+from mxnet.gluon import nn, rnn\n+\n+parser = argparse.ArgumentParser(description='Gluon RNN Benchmarking.')\n+parser.add_argument('--num-layer', type=int, default=1,\n+                    help='The number of layers of the RNN model')\n+parser.add_argument('--layout', type=str, default='TNC',\n+                    help='The layout of the input shape, can be either TNC or NTC.')\n+parser.add_argument('--specify-shape', type=str,\n+                    help='Specify the input shape, format batchsize, time-step, embed-size, hidden-size.')\n+parser.add_argument('--cell-type', type=str, default='lstm',\n+                    help='RNN cell type, can be either lstm, gru or all to cover both.')\n+parser.add_argument('--unfuse', action='store_true', default=False,\n+                    help='Unfuse the RNN layer to stacked RNN cell instead.') \n+parser.add_argument('--latency', action='store_true', default=False,\n+                    help='Measursing the latency, batchsize will be set to 1.')\n+parser.add_argument('--train', action='store_true', default=False,\n+                    help='Run backward benchmark.')\n+parser.add_argument('--dropout', type=float, default=0, \n+                    help='float, default 0; Fraction of the input that gets dropped out during training time.')\n+parser.add_argument('--gpu', action='store_true', default=False,\n+                    help='bool, default Flase; set True to run benchmarks on GPU.')\n+parser.add_argument('--no-hybridize', action='store_true', default=False,\n+                    help='bool, default False; hybridize will introduce a better perf.')\n+parser.add_argument('--bidirection', action='store_true', default=False,\n+                    help='bool, default False; Whether to use bidirectional unroll.')\n+\n+opt = parser.parse_args()\n+logging.basicConfig(level=logging.INFO)\n+\n+#[bs, sequence length, embedding size, hidden size]\n+input_shape_list = [[64,15,500,500],\n+   [64,20,500,500],\n+   [64,25,500,500],\n+   [64,30,500,500],\n+   [64,35,500,500],\n+   [64,40,500,500],\n+   [64,45,500,500],\n+   [64,50,500,500],\n+   [16,25,512,512],\n+   [32,25,512,512],\n+   [64,25,512,512],\n+   [128,25,512,512],\n+   [16,25,1024,1024],\n+   [32,25,1024,1024],\n+   [64,25,1024,1024],\n+   [128,25,1024,1024],\n+   [16,25,2048,2048],\n+   [32,25,2048,2048],\n+   [64,25,2048,2048],\n+   [128,25,2048,2048],\n+   [16,25,4096,4096],\n+   [32,25,4096,4096],\n+   [64,25,4096,4096],\n+   [128,25,4096,4096]]\n+\n+rnncell_type = ['lstm', 'gru', 'all']\n+input_layout = ['TNC', 'NTC']\n+\n+if not opt.gpu:\n+    ctx = mx.cpu()\n+else:\n+    ctx = mx.gpu(0)\n+\n+dropout = opt.dropout\n+bidirection = opt.bidirection\n+unfuse = opt.unfuse\n+celltype = opt.cell_type\n+\n+dry_run = 20\n+num_iter = 100\n+\n+def get_rnn_layer(input_shape, num_layer, cell_type, dropout=0, bidirection=False):\n+    hidden_size = input_shape[3]\n+    embedding_size = input_shape[2]\n+    if cell_type == 'lstm':\n+        rnn_layer = rnn.LSTM(hidden_size, num_layer, dropout=dropout,\n+                             bidirectional=bidirection, input_size=embedding_size,\n+                             prefix='_lstm_layer')\n+    elif cell_type == 'gru':\n+        rnn_layer = rnn.GRU(hidden_size, num_layer, dropout=dropout,\n+                            bidirectional=bidirection, input_size=embedding_size,\n+                            prefix='_gru_layer')\n+    return rnn_layer\n+\n+\n+def rnn_cell_score(input_shape, cell_type, ctx, num_layer, dropout=0, bidirection=False, layout='TNC', unfuse=False, hybridize=True, is_train=False):\n+    bs = input_shape[0]\n+    seq_len = input_shape[1]\n+    embedding_size = input_shape[2]\n+    hidden_size = input_shape[3]\n+    rnn_layer = get_rnn_layer(input_shape, num_layer, cell_type, dropout, bidirection)\n+    input_data = mx.sym.Variable('data')\n+\n+    if unfuse:\n+        rnn_cell = rnn_layer._unfuse()\n+        if hybridize:\n+            rnn_cell.hybridize()\n+        out, _ = rnn_cell.unroll(length=seq_len, inputs = input_data, layout=layout, merge_outputs=True)\n+    else:\n+        if hybridize:\n+            rnn_layer.hybridize()\n+        out = rnn_layer(input_data)\n+\n+    if is_train: \n+        #out = mx.sym.slice(out, begin=(0, None), end=(bs, None))\n+        hidden = mx.sym.Reshape(data = out, shape=(-1, hidden_size))\n+        pred = mx.sym.FullyConnected(data=hidden, num_hidden=embedding_size, name='pred')\n+        if layout == 'TNC':\n+            pred = mx.sym.Reshape(data=pred, shape=(seq_len, -1, embedding_size))\n+        elif layout == 'NTC':\n+            pred = mx.sym.Reshape(data=pred, shape=(-1, seq_len, embedding_size))\n+        softmax_output = mx.sym.SoftmaxOutput(data=pred, name='softmax')\n+\n+    if layout == 'NTC':\n+        dshape = (bs, seq_len, embedding_size)\n+    elif layout == 'TNC':\n+        dshape = (seq_len, bs, embedding_size)\n+    \n+    if is_train:\n+        mod = mx.mod.Module(softmax_output, label_names=('softmax_label',), context=ctx)\n+    else:\n+        mod = mx.mod.Module(out, label_names=None, context=ctx)\n+    \n+    if is_train:\n+        if layout == 'TNC':\n+            mod.bind(for_training = True, data_shapes=[('data', dshape)],\n+                label_shapes=[('softmax_label', (seq_len, bs, embedding_size))])\n+        elif layout == 'NTC':\n+            mod.bind(for_training = True, data_shapes=[('data', dshape)],\n+                label_shapes=[('softmax_label', (bs, seq_len, embedding_size))])\n+        \n+    else:\n+        mod.bind(data_shapes=[('data', dshape)], label_shapes=None)\n+\n+    batch = mx.io.DataBatch(data=[mx.random.uniform(shape=dshape)], label=[])\n+    mod.init_params(initializer=mx.init.Xavier(magnitude=2.))\n+    if is_train:\n+        mod.init_optimizer(optimizer='sgd')\n+        mod.forward(batch, is_train=True)\n+        if unfuse:\n+            for o in mod.get_outputs():\n+                o.wait_to_read()\n+        else:\n+            o = mod.get_outputs()[0]\n+            o.wait_to_read()\n+        mod.backward()\n+        mod.update()\n+    else:\n+        mod.forward(batch, is_train=False)\n+        if unfuse:\n+            for o in mod.get_outputs():\n+                o.wait_to_read()\n+        else:\n+            o = mod.get_outputs()[0]\n+            o.wait_to_read()\n+\n+if __name__ == '__main__':\n+\n+    num_layer = opt.num_layer\n+    layout = opt.layout\n+    latency = opt.latency\n+    hybridize = not(opt.no_hybridize)\n+    \n+    if layout not in input_layout:\n+        logging.warning('Only TNC or NTC are supported!')\n+        sys.exit(0)\n+\n+    if celltype not in rnncell_type:\n+        logging.warning('Only LSTM and GRU cell are supported!')\n+        sys.exit(0)\n+    \n+    if celltype == 'all':\n+        cell_lst = ['lstm', 'gru']\n+    else:\n+        cell_lst = [celltype]\n+\n+    if opt.specify_shape != None:\n+        input_shape_list = [[int(x) for x in opt.specify_shape.split(',')]]\n+\n+    for cell in cell_lst:\n+        if opt.train:\n+            logging.info('%s training benchmark.', cell)\n+        else:\n+            logging.info('%s inference benchmark.', cell)",
        "comment_created_at": "2018-11-28T18:51:38+00:00",
        "comment_author": "vandanavk",
        "comment_body": "can this be changed to check for opt.train before the for loop?\r\nsomething like\r\n`print_string = 'training' if opt.train else 'inference'`\r\nand inside the for loop it would be `logging.info('%s %s benchmark.', cell, print_string)`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "968208176",
    "pr_number": 21136,
    "pr_file": "tests/nightly/model_backwards_compatibility_check/common.py",
    "created_at": "2022-09-12T09:47:28+00:00",
    "commented_code": "result = bucket.meta.client.list_objects(Bucket=bucket.name, Delimiter=backslash)\n    folder_list = list()\n    if 'CommonPrefixes' not in result:\n        logging.error('No trained models found in S3 bucket : %s for this file. '\n                      'Please train the models and run inference again' % bucket_name)\n        raise Exception(\"No trained models found in S3 bucket : %s for this file. \"\n                        \"Please train the models and run inference again\" % bucket_name)\n        logging.error('No trained models found in S3 bucket : {} for this file. '",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "968208176",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21136,
        "pr_file": "tests/nightly/model_backwards_compatibility_check/common.py",
        "discussion_id": "968208176",
        "commented_code": "@@ -105,10 +105,10 @@ def get_top_level_folders_in_bucket(s3client, bucket_name):\n     result = bucket.meta.client.list_objects(Bucket=bucket.name, Delimiter=backslash)\n     folder_list = list()\n     if 'CommonPrefixes' not in result:\n-        logging.error('No trained models found in S3 bucket : %s for this file. '\n-                      'Please train the models and run inference again' % bucket_name)\n-        raise Exception(\"No trained models found in S3 bucket : %s for this file. \"\n-                        \"Please train the models and run inference again\" % bucket_name)\n+        logging.error('No trained models found in S3 bucket : {} for this file. '",
        "comment_created_at": "2022-09-12T09:47:28+00:00",
        "comment_author": "anko-intel",
        "comment_body": "What is the reason to have to different approaches than in line 110?\r\nhere {bucket_name} seems to be more readable",
        "pr_file_module": null
      },
      {
        "comment_id": "968317987",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21136,
        "pr_file": "tests/nightly/model_backwards_compatibility_check/common.py",
        "discussion_id": "968208176",
        "commented_code": "@@ -105,10 +105,10 @@ def get_top_level_folders_in_bucket(s3client, bucket_name):\n     result = bucket.meta.client.list_objects(Bucket=bucket.name, Delimiter=backslash)\n     folder_list = list()\n     if 'CommonPrefixes' not in result:\n-        logging.error('No trained models found in S3 bucket : %s for this file. '\n-                      'Please train the models and run inference again' % bucket_name)\n-        raise Exception(\"No trained models found in S3 bucket : %s for this file. \"\n-                        \"Please train the models and run inference again\" % bucket_name)\n+        logging.error('No trained models found in S3 bucket : {} for this file. '",
        "comment_created_at": "2022-09-12T11:52:22+00:00",
        "comment_author": "hankaj",
        "comment_body": "In CI I got the error that you should not use f-strings in messages of logging functions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "823646148",
    "pr_number": 20753,
    "pr_file": "python/mxnet/amp/amp.py",
    "created_at": "2022-03-10T12:01:50+00:00",
    "commented_code": "from being casted to LP16 or FP32.\n    data_names : list of strs, optional\n        A list of strings that represent input data tensor names to the model\n    cast_optional_params : bool, default False\n    cast_params_offline : bool, default False\n        Whether to cast the arg_params and aux_params that don't require to be in LP16\n        because of a cast layer following it, but will reduce the computation and memory\n        overhead of the model if casted.\n    \"\"\"\n    assert isinstance(sym, Symbol), \"First argument to convert_symbol should be Symbol\"\n    import json\n\n    assert target_dtype in ['float16', 'bfloat16'], \\\n               \"Only target_dtype float16 and bfloat16 are supported currently\"\n    assert isinstance(sym, Symbol), \"First argument to convert_symbol should be a Symbol\"\n    assert target_dtype_ops is None or isinstance(target_dtype_ops, list), \\\n        \"target_dtype_ops should be a list of strs\"\n    assert fp32_ops is None or isinstance(fp32_ops, list), \\\n        \"fp32_ops should be a list of strs\"\n    assert conditional_fp32_ops is None or isinstance(conditional_fp32_ops, list), \\\n        \"conditional_fp32_ops should be a list\"\n\n    if target_dtype == 'bfloat16':\n        target_dtype = bfloat16\n    target_dtype = get_dtype_name(target_dtype)\n    assert target_dtype in ['float16', *bfloat16.names], \\\n        \"Only float16 and bfloat16 types are currently supported as target_dtype\"\n\n    if target_dtype_ops is not None:\n        assert isinstance(target_dtype_ops, list), \"target_dtype_ops should be a list of strs\"\n    else:\n    if target_dtype_ops is None:\n        target_dtype_ops = list_lp16_ops(target_dtype)\n\n    if fp32_ops is not None:\n        assert isinstance(fp32_ops, list), \"fp32_ops should be a list of strs\"\n    else:\n    if fp32_ops is None:\n        fp32_ops = list_fp32_ops(target_dtype)\n\n    if conditional_fp32_ops is not None:\n        assert isinstance(conditional_fp32_ops, list), \"conditional_fp32_ops should be a list\"\n    else:\n    # conditional ops\n    if conditional_fp32_ops is None:\n        conditional_fp32_ops = list_conditional_fp32_ops(target_dtype)\n    cond_ops = {cond_op[0]: {} for cond_op in conditional_fp32_ops}\n    for cond_op in conditional_fp32_ops:\n        op_name, attr_name, attr_vals = cond_op\n        assert isinstance(op_name, str) and isinstance(attr_name, str) and isinstance(attr_vals, list), \\\n            \"conditional_fp32_ops should be a list of (str, str, list of str)\"\n        cond_ops[op_name].setdefault(attr_name, []).extend(attr_vals)\n\n    nodes_attr = sym.attr_dict()\n    nodes_op = {n['name']: n['op'] for n in json.loads(sym.tojson())['nodes']}\n    assert set(excluded_sym_names).issubset(set(nodes_op.keys())), \\",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "823646148",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20753,
        "pr_file": "python/mxnet/amp/amp.py",
        "discussion_id": "823646148",
        "commented_code": "@@ -459,73 +461,73 @@ def convert_symbol(sym, target_dtype=\"float16\", target_dtype_ops=None,\n         from being casted to LP16 or FP32.\n     data_names : list of strs, optional\n         A list of strings that represent input data tensor names to the model\n-    cast_optional_params : bool, default False\n+    cast_params_offline : bool, default False\n         Whether to cast the arg_params and aux_params that don't require to be in LP16\n         because of a cast layer following it, but will reduce the computation and memory\n         overhead of the model if casted.\n     \"\"\"\n-    assert isinstance(sym, Symbol), \"First argument to convert_symbol should be Symbol\"\n+    import json\n \n-    assert target_dtype in ['float16', 'bfloat16'], \\\n-               \"Only target_dtype float16 and bfloat16 are supported currently\"\n+    assert isinstance(sym, Symbol), \"First argument to convert_symbol should be a Symbol\"\n+    assert target_dtype_ops is None or isinstance(target_dtype_ops, list), \\\n+        \"target_dtype_ops should be a list of strs\"\n+    assert fp32_ops is None or isinstance(fp32_ops, list), \\\n+        \"fp32_ops should be a list of strs\"\n+    assert conditional_fp32_ops is None or isinstance(conditional_fp32_ops, list), \\\n+        \"conditional_fp32_ops should be a list\"\n \n-    if target_dtype == 'bfloat16':\n-        target_dtype = bfloat16\n+    target_dtype = get_dtype_name(target_dtype)\n+    assert target_dtype in ['float16', *bfloat16.names], \\\n+        \"Only float16 and bfloat16 types are currently supported as target_dtype\"\n \n-    if target_dtype_ops is not None:\n-        assert isinstance(target_dtype_ops, list), \"target_dtype_ops should be a list of strs\"\n-    else:\n+    if target_dtype_ops is None:\n         target_dtype_ops = list_lp16_ops(target_dtype)\n-\n-    if fp32_ops is not None:\n-        assert isinstance(fp32_ops, list), \"fp32_ops should be a list of strs\"\n-    else:\n+    if fp32_ops is None:\n         fp32_ops = list_fp32_ops(target_dtype)\n \n-    if conditional_fp32_ops is not None:\n-        assert isinstance(conditional_fp32_ops, list), \"conditional_fp32_ops should be a list\"\n-    else:\n+    # conditional ops\n+    if conditional_fp32_ops is None:\n         conditional_fp32_ops = list_conditional_fp32_ops(target_dtype)\n+    cond_ops = {cond_op[0]: {} for cond_op in conditional_fp32_ops}\n+    for cond_op in conditional_fp32_ops:\n+        op_name, attr_name, attr_vals = cond_op\n+        assert isinstance(op_name, str) and isinstance(attr_name, str) and isinstance(attr_vals, list), \\\n+            \"conditional_fp32_ops should be a list of (str, str, list of str)\"\n+        cond_ops[op_name].setdefault(attr_name, []).extend(attr_vals)\n+\n+    nodes_attr = sym.attr_dict()\n+    nodes_op = {n['name']: n['op'] for n in json.loads(sym.tojson())['nodes']}\n+    assert set(excluded_sym_names).issubset(set(nodes_op.keys())), \\",
        "comment_created_at": "2022-03-10T12:01:50+00:00",
        "comment_author": "bgawrych",
        "comment_body": "should it be assert? maybe warning is enough?",
        "pr_file_module": null
      },
      {
        "comment_id": "824647707",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20753,
        "pr_file": "python/mxnet/amp/amp.py",
        "discussion_id": "823646148",
        "commented_code": "@@ -459,73 +461,73 @@ def convert_symbol(sym, target_dtype=\"float16\", target_dtype_ops=None,\n         from being casted to LP16 or FP32.\n     data_names : list of strs, optional\n         A list of strings that represent input data tensor names to the model\n-    cast_optional_params : bool, default False\n+    cast_params_offline : bool, default False\n         Whether to cast the arg_params and aux_params that don't require to be in LP16\n         because of a cast layer following it, but will reduce the computation and memory\n         overhead of the model if casted.\n     \"\"\"\n-    assert isinstance(sym, Symbol), \"First argument to convert_symbol should be Symbol\"\n+    import json\n \n-    assert target_dtype in ['float16', 'bfloat16'], \\\n-               \"Only target_dtype float16 and bfloat16 are supported currently\"\n+    assert isinstance(sym, Symbol), \"First argument to convert_symbol should be a Symbol\"\n+    assert target_dtype_ops is None or isinstance(target_dtype_ops, list), \\\n+        \"target_dtype_ops should be a list of strs\"\n+    assert fp32_ops is None or isinstance(fp32_ops, list), \\\n+        \"fp32_ops should be a list of strs\"\n+    assert conditional_fp32_ops is None or isinstance(conditional_fp32_ops, list), \\\n+        \"conditional_fp32_ops should be a list\"\n \n-    if target_dtype == 'bfloat16':\n-        target_dtype = bfloat16\n+    target_dtype = get_dtype_name(target_dtype)\n+    assert target_dtype in ['float16', *bfloat16.names], \\\n+        \"Only float16 and bfloat16 types are currently supported as target_dtype\"\n \n-    if target_dtype_ops is not None:\n-        assert isinstance(target_dtype_ops, list), \"target_dtype_ops should be a list of strs\"\n-    else:\n+    if target_dtype_ops is None:\n         target_dtype_ops = list_lp16_ops(target_dtype)\n-\n-    if fp32_ops is not None:\n-        assert isinstance(fp32_ops, list), \"fp32_ops should be a list of strs\"\n-    else:\n+    if fp32_ops is None:\n         fp32_ops = list_fp32_ops(target_dtype)\n \n-    if conditional_fp32_ops is not None:\n-        assert isinstance(conditional_fp32_ops, list), \"conditional_fp32_ops should be a list\"\n-    else:\n+    # conditional ops\n+    if conditional_fp32_ops is None:\n         conditional_fp32_ops = list_conditional_fp32_ops(target_dtype)\n+    cond_ops = {cond_op[0]: {} for cond_op in conditional_fp32_ops}\n+    for cond_op in conditional_fp32_ops:\n+        op_name, attr_name, attr_vals = cond_op\n+        assert isinstance(op_name, str) and isinstance(attr_name, str) and isinstance(attr_vals, list), \\\n+            \"conditional_fp32_ops should be a list of (str, str, list of str)\"\n+        cond_ops[op_name].setdefault(attr_name, []).extend(attr_vals)\n+\n+    nodes_attr = sym.attr_dict()\n+    nodes_op = {n['name']: n['op'] for n in json.loads(sym.tojson())['nodes']}\n+    assert set(excluded_sym_names).issubset(set(nodes_op.keys())), \\",
        "comment_created_at": "2022-03-11T11:55:22+00:00",
        "comment_author": "PawelGlomski-Intel",
        "comment_body": "I changed it to use `logging.warning(...)` since this is how logging is handled in this file. Do you think we should instead add a logger as an argument to the conversion functions?",
        "pr_file_module": null
      },
      {
        "comment_id": "827244414",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20753,
        "pr_file": "python/mxnet/amp/amp.py",
        "discussion_id": "823646148",
        "commented_code": "@@ -459,73 +461,73 @@ def convert_symbol(sym, target_dtype=\"float16\", target_dtype_ops=None,\n         from being casted to LP16 or FP32.\n     data_names : list of strs, optional\n         A list of strings that represent input data tensor names to the model\n-    cast_optional_params : bool, default False\n+    cast_params_offline : bool, default False\n         Whether to cast the arg_params and aux_params that don't require to be in LP16\n         because of a cast layer following it, but will reduce the computation and memory\n         overhead of the model if casted.\n     \"\"\"\n-    assert isinstance(sym, Symbol), \"First argument to convert_symbol should be Symbol\"\n+    import json\n \n-    assert target_dtype in ['float16', 'bfloat16'], \\\n-               \"Only target_dtype float16 and bfloat16 are supported currently\"\n+    assert isinstance(sym, Symbol), \"First argument to convert_symbol should be a Symbol\"\n+    assert target_dtype_ops is None or isinstance(target_dtype_ops, list), \\\n+        \"target_dtype_ops should be a list of strs\"\n+    assert fp32_ops is None or isinstance(fp32_ops, list), \\\n+        \"fp32_ops should be a list of strs\"\n+    assert conditional_fp32_ops is None or isinstance(conditional_fp32_ops, list), \\\n+        \"conditional_fp32_ops should be a list\"\n \n-    if target_dtype == 'bfloat16':\n-        target_dtype = bfloat16\n+    target_dtype = get_dtype_name(target_dtype)\n+    assert target_dtype in ['float16', *bfloat16.names], \\\n+        \"Only float16 and bfloat16 types are currently supported as target_dtype\"\n \n-    if target_dtype_ops is not None:\n-        assert isinstance(target_dtype_ops, list), \"target_dtype_ops should be a list of strs\"\n-    else:\n+    if target_dtype_ops is None:\n         target_dtype_ops = list_lp16_ops(target_dtype)\n-\n-    if fp32_ops is not None:\n-        assert isinstance(fp32_ops, list), \"fp32_ops should be a list of strs\"\n-    else:\n+    if fp32_ops is None:\n         fp32_ops = list_fp32_ops(target_dtype)\n \n-    if conditional_fp32_ops is not None:\n-        assert isinstance(conditional_fp32_ops, list), \"conditional_fp32_ops should be a list\"\n-    else:\n+    # conditional ops\n+    if conditional_fp32_ops is None:\n         conditional_fp32_ops = list_conditional_fp32_ops(target_dtype)\n+    cond_ops = {cond_op[0]: {} for cond_op in conditional_fp32_ops}\n+    for cond_op in conditional_fp32_ops:\n+        op_name, attr_name, attr_vals = cond_op\n+        assert isinstance(op_name, str) and isinstance(attr_name, str) and isinstance(attr_vals, list), \\\n+            \"conditional_fp32_ops should be a list of (str, str, list of str)\"\n+        cond_ops[op_name].setdefault(attr_name, []).extend(attr_vals)\n+\n+    nodes_attr = sym.attr_dict()\n+    nodes_op = {n['name']: n['op'] for n in json.loads(sym.tojson())['nodes']}\n+    assert set(excluded_sym_names).issubset(set(nodes_op.keys())), \\",
        "comment_created_at": "2022-03-15T17:33:16+00:00",
        "comment_author": "bgawrych",
        "comment_body": "Other functions are just using logging module, so I don't think adding argument is necessary",
        "pr_file_module": null
      }
    ]
  }
]
