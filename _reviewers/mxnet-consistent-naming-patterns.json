[
  {
    "discussion_id": "722364711",
    "pr_number": 20633,
    "pr_file": "python/mxnet/numpy/linalg.py",
    "created_at": "2021-10-05T15:33:59+00:00",
    "commented_code": "return _mx_nd_np.linalg.matrix_rank(M, tol, hermitian)\n\n\ndef vecdot(a: ndarray, b, ndarray, axis: Optional[int] =None) -> ndarray:",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "722364711",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20633,
        "pr_file": "python/mxnet/numpy/linalg.py",
        "discussion_id": "722364711",
        "commented_code": "@@ -67,6 +70,53 @@ def matrix_rank(M, tol=None, hermitian=False):\n     return _mx_nd_np.linalg.matrix_rank(M, tol, hermitian)\n \n \n+def vecdot(a: ndarray, b, ndarray, axis: Optional[int] =None) -> ndarray:",
        "comment_created_at": "2021-10-05T15:33:59+00:00",
        "comment_author": "mozga-intel",
        "comment_body": "```def vecdot(a: ndarray, b, ndarray, axis: Optional[int] =None) -> ndarray:```\r\nHow about adding the same function style in the entire file by giving the type-name explicitly?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "756375086",
    "pr_number": 20633,
    "pr_file": "python/mxnet/numpy/multiarray.py",
    "created_at": "2021-11-24T19:36:54+00:00",
    "commented_code": "\"\"\"\n    from_dlpack = ndarray_from_dlpack(ndarray)\n    return from_dlpack(x)\n\n\n@set_module('mxnet.numpy')\ndef matrix_transpose(x: ndarray, /) -> ndarray:\n    \"\"\"\n    Transposes a matrix (or a stack of matrices) x ..\n\n    Parameters\n    ----------\n    x : ndarray\n        input array having shape (..., M, N) and whose innermost two dimensions form MxN matrices\n\n    Returns\n    -------\n    out : ndarray\n        an array containing the transpose for each matrix and having shape (..., N, M) . The\n        returned array must have the same data type as x .\n    \"\"\"\n    if x.ndim < 2:\n        raise ValueError(\"x must be at least 2-dimensional for matrix_transpose\")\n    return _mx_nd_np.swapaxes(x, -1, -2)\n\n\ndef astype(\n        self: ndarray,",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "756375086",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20633,
        "pr_file": "python/mxnet/numpy/multiarray.py",
        "discussion_id": "756375086",
        "commented_code": "@@ -13363,3 +13965,96 @@ def from_dlpack(x):\n     \"\"\"\n     from_dlpack = ndarray_from_dlpack(ndarray)\n     return from_dlpack(x)\n+\n+\n+@set_module('mxnet.numpy')\n+def matrix_transpose(x: ndarray, /) -> ndarray:\n+    \"\"\"\n+    Transposes a matrix (or a stack of matrices) x ..\n+\n+    Parameters\n+    ----------\n+    x : ndarray\n+        input array having shape (..., M, N) and whose innermost two dimensions form MxN matrices\n+\n+    Returns\n+    -------\n+    out : ndarray\n+        an array containing the transpose for each matrix and having shape (..., N, M) . The\n+        returned array must have the same data type as x .\n+    \"\"\"\n+    if x.ndim < 2:\n+        raise ValueError(\"x must be at least 2-dimensional for matrix_transpose\")\n+    return _mx_nd_np.swapaxes(x, -1, -2)\n+\n+\n+def astype(\n+        self: ndarray,",
        "comment_created_at": "2021-11-24T19:36:54+00:00",
        "comment_author": "barry-jin",
        "comment_body": "Please use x or array as argument here instead of self. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "797614635",
    "pr_number": 20846,
    "pr_file": "tests/python/dnnl/test_bf16_operator.py",
    "created_at": "2022-02-02T13:38:27+00:00",
    "commented_code": "etol: float\n        The error rate threshold, allow a small amount of value not consistent between bf16 and fp32\n    \"\"\"\n    if not isinstance(data_shape, tuple):\n        data_shape = tuple(data_shape)\n    if isinstance(data_shape, tuple):\n        data_shape = {\"data\": data_shape}\n    data_range = (0.0, 10.0)\n    data_list_fp32 = list()\n    data_list_bf16 = list()\n    for i in range(num_input_data):\n        data_list_fp32.append(mx.nd.random.uniform(low=data_range[0], high=data_range[1], shape=data_shape))\n    for i, obj in enumerate(data_shape):",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "797614635",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20846,
        "pr_file": "tests/python/dnnl/test_bf16_operator.py",
        "discussion_id": "797614635",
        "commented_code": "@@ -55,20 +55,22 @@ def check_operator_accuracy(sym_fp32, sym_bf16, data_shape, num_input_data=1, bf\n     etol: float\n         The error rate threshold, allow a small amount of value not consistent between bf16 and fp32\n     \"\"\"\n-    if not isinstance(data_shape, tuple):\n-        data_shape = tuple(data_shape)\n+    if isinstance(data_shape, tuple):\n+        data_shape = {\"data\": data_shape}\n     data_range = (0.0, 10.0)\n     data_list_fp32 = list()\n     data_list_bf16 = list()\n-    for i in range(num_input_data):\n-        data_list_fp32.append(mx.nd.random.uniform(low=data_range[0], high=data_range[1], shape=data_shape))\n+    for i, obj in enumerate(data_shape):",
        "comment_created_at": "2022-02-02T13:38:27+00:00",
        "comment_author": "bgawrych",
        "comment_body": "replace obj with name/key or use items() to iterate over values in dict",
        "pr_file_module": null
      },
      {
        "comment_id": "797733350",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20846,
        "pr_file": "tests/python/dnnl/test_bf16_operator.py",
        "discussion_id": "797614635",
        "commented_code": "@@ -55,20 +55,22 @@ def check_operator_accuracy(sym_fp32, sym_bf16, data_shape, num_input_data=1, bf\n     etol: float\n         The error rate threshold, allow a small amount of value not consistent between bf16 and fp32\n     \"\"\"\n-    if not isinstance(data_shape, tuple):\n-        data_shape = tuple(data_shape)\n+    if isinstance(data_shape, tuple):\n+        data_shape = {\"data\": data_shape}\n     data_range = (0.0, 10.0)\n     data_list_fp32 = list()\n     data_list_bf16 = list()\n-    for i in range(num_input_data):\n-        data_list_fp32.append(mx.nd.random.uniform(low=data_range[0], high=data_range[1], shape=data_shape))\n+    for i, obj in enumerate(data_shape):",
        "comment_created_at": "2022-02-02T15:34:35+00:00",
        "comment_author": "agrabows",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "646155037",
    "pr_number": 20262,
    "pr_file": "tests/python/unittest/test_gluon.py",
    "created_at": "2021-06-06T16:21:48+00:00",
    "commented_code": "from mxnet.util import is_np_array\nfrom mxnet.ndarray.ndarray import _STORAGE_TYPE_STR_TO_ID\nfrom mxnet.test_utils import use_np\nimport mxnet.numpy as _mx_np\nfrom common import assertRaises, assert_raises_cudnn_not_satisfied, \\\n    xfail_when_nonstandard_decimal_separator, environment\nimport numpy as np\nimport numpy as _np",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "646155037",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20262,
        "pr_file": "tests/python/unittest/test_gluon.py",
        "discussion_id": "646155037",
        "commented_code": "@@ -27,10 +27,9 @@\n from mxnet.util import is_np_array\n from mxnet.ndarray.ndarray import _STORAGE_TYPE_STR_TO_ID\n from mxnet.test_utils import use_np\n-import mxnet.numpy as _mx_np\n from common import assertRaises, assert_raises_cudnn_not_satisfied, \\\n     xfail_when_nonstandard_decimal_separator, environment\n-import numpy as np\n+import numpy as _np",
        "comment_created_at": "2021-06-06T16:21:48+00:00",
        "comment_author": "szha",
        "comment_body": "use `onp` as the name to be consistent",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "646157100",
    "pr_number": 20262,
    "pr_file": "tests/python/unittest/test_loss.py",
    "created_at": "2021-06-06T16:39:10+00:00",
    "commented_code": "# under the License.\n\nimport mxnet as mx\nimport numpy as np\nimport numpy as _np",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "646157100",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20262,
        "pr_file": "tests/python/unittest/test_loss.py",
        "discussion_id": "646157100",
        "commented_code": "@@ -16,91 +16,98 @@\n # under the License.\n \n import mxnet as mx\n-import numpy as np\n+import numpy as _np",
        "comment_created_at": "2021-06-06T16:39:10+00:00",
        "comment_author": "szha",
        "comment_body": "use `onp` as the name for consistency",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "654714998",
    "pr_number": 20262,
    "pr_file": "benchmark/python/control_flow/rnn.py",
    "created_at": "2021-06-18T23:32:28+00:00",
    "commented_code": "from time import time\n\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import gluon\nimport numpy as onp",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "654714998",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20262,
        "pr_file": "benchmark/python/control_flow/rnn.py",
        "discussion_id": "654714998",
        "commented_code": "@@ -24,8 +24,8 @@\n from time import time\n \n import mxnet as mx\n-import numpy as np\n-from mxnet import gluon\n+import numpy as onp",
        "comment_created_at": "2021-06-18T23:32:28+00:00",
        "comment_author": "TristonC",
        "comment_body": "Is the 'o' in onp as original as this is original numpy? It will be confusing as np being well known as numpy for short.",
        "pr_file_module": null
      },
      {
        "comment_id": "654727506",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20262,
        "pr_file": "benchmark/python/control_flow/rnn.py",
        "discussion_id": "654714998",
        "commented_code": "@@ -24,8 +24,8 @@\n from time import time\n \n import mxnet as mx\n-import numpy as np\n-from mxnet import gluon\n+import numpy as onp",
        "comment_created_at": "2021-06-19T01:03:34+00:00",
        "comment_author": "barry-jin",
        "comment_body": "Yes, 'o' in onp is 'official', which is used to distinguish between official numpy and MXNet numpy. Usually, user will do \r\n`from mxnet import np` and build their models with numpy operators from MXNet. This will provide numpy-compatible coding experience in MXNet for users. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "638717908",
    "pr_number": 20227,
    "pr_file": "tests/python/quantization/test_quantization.py",
    "created_at": "2021-05-25T11:55:26+00:00",
    "commented_code": "data_high = 127.0\n\n        # run fp32 bn\n        data_sym = mx.sym.Variable(name='data', shape=data_shape, dtype='float32')\n        bn_fp32 = mx.sym.BatchNorm(data=data_sym, name='bn', use_global_stats=True, fix_gamma=False)\n        arg_shapes, out_shapes, aux_shapes = bn_fp32.infer_shape(data=data_shape)\n        arg_names = bn_fp32.list_arguments()\n        aux_names = bn_fp32.list_auxiliary_states()\n        bn_fp32 = mx.gluon.nn.BatchNorm(use_global_stats=True, scale=True)\n        data = mx.nd.random.uniform(low=data_low, high=data_high, shape=data_shape)\n        bn_fp32.initialize()\n        bn_fp32.hybridize()\n        bn_fp32(data)\n        fp32_params = bn_fp32.collect_params()\n        \n        data = mx.nd.random.uniform(low=data_low, high=data_high, shape=data_shape)\n        gamma = mx.nd.random.uniform(low=data_low, high=data_high, shape=arg_shapes[1])\n        beta = mx.nd.random.uniform(low=data_low, high=data_high, shape=arg_shapes[2])\n        gamma = mx.nd.random.uniform(low=data_low, high=data_high, shape=fp32_params['gamma'].shape)\n        beta = mx.nd.random.uniform(low=data_low, high=data_high, shape=fp32_params['beta'].shape)\n        moving_mean, moving_var = get_mean_var(data)\n        new_params = {\n            'gamma':gamma,\n            'beta':beta,\n            'running_mean': moving_mean,",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "638717908",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20227,
        "pr_file": "tests/python/quantization/test_quantization.py",
        "discussion_id": "638717908",
        "commented_code": "@@ -760,48 +864,38 @@ def check_quantized_bn(data_shape, qdtype):\n             data_high = 127.0\n \n         # run fp32 bn\n-        data_sym = mx.sym.Variable(name='data', shape=data_shape, dtype='float32')\n-        bn_fp32 = mx.sym.BatchNorm(data=data_sym, name='bn', use_global_stats=True, fix_gamma=False)\n-        arg_shapes, out_shapes, aux_shapes = bn_fp32.infer_shape(data=data_shape)\n-        arg_names = bn_fp32.list_arguments()\n-        aux_names = bn_fp32.list_auxiliary_states()\n+        bn_fp32 = mx.gluon.nn.BatchNorm(use_global_stats=True, scale=True)\n+        data = mx.nd.random.uniform(low=data_low, high=data_high, shape=data_shape)\n+        bn_fp32.initialize()\n+        bn_fp32.hybridize()\n+        bn_fp32(data)\n+        fp32_params = bn_fp32.collect_params()\n+        \n         data = mx.nd.random.uniform(low=data_low, high=data_high, shape=data_shape)\n-        gamma = mx.nd.random.uniform(low=data_low, high=data_high, shape=arg_shapes[1])\n-        beta = mx.nd.random.uniform(low=data_low, high=data_high, shape=arg_shapes[2])\n+        gamma = mx.nd.random.uniform(low=data_low, high=data_high, shape=fp32_params['gamma'].shape)\n+        beta = mx.nd.random.uniform(low=data_low, high=data_high, shape=fp32_params['beta'].shape)\n         moving_mean, moving_var = get_mean_var(data)\n+        new_params = {\n+            'gamma':gamma,\n+            'beta':beta,\n+            'running_mean': moving_mean,",
        "comment_created_at": "2021-05-25T11:55:26+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "Maybe rename 'moving_*' vars to 'running_*' vars for consistency?",
        "pr_file_module": null
      }
    ]
  }
]