[
  {
    "discussion_id": "954872677",
    "pr_number": 21127,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization_inc.md",
    "created_at": "2022-08-25T11:55:17+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n\n# Improving accuracy with Intel® Neural Compressor\n\nThe accuracy of a model can decrease as a result of quantization. When the accuracy drop is significant, we can try to manually find a better quantization configuration (exclude some layers, try different calibration methods, etc.), but for bigger models this might prove to be a difficult and time consuming task. [Intel® Neural Compressor](https://github.com/intel/neural-compressor) (INC) tries to automate this process using several tuning heuristics, which aim to find the quantization configuration that satisfies the specified accuracy requirement.\n\n**NOTE:**\n\nMost tuning strategies will try different configurations on an evaluation dataset in order to find out how each layer affects the accuracy of the model. This means that for larger models, it may take a long time to find a solution (as the tuning space is usually larger and the evaluation itself takes longer).\n\n## Installation and Prerequisites\n\n- Install MXNet with oneDNN enabled as described in the [Get started](https://mxnet.apache.org/versions/master/get_started?platform=linux&language=python&processor=cpu&environ=pip&). (Until the 2.0 release you can use the nightly build version: `pip install --pre mxnet -f https://dist.mxnet.io/python`)\n\n- Install Intel® Neural Compressor:\n\n  Use one of the commands below to install INC (supported python versions are: 3.6, 3.7, 3.8, 3.9):\n\n  ```bash\n  # install stable version from pip\n  pip install neural-compressor\n\n  # install nightly version from pip\n  pip install -i https://test.pypi.org/simple/ neural-compressor\n\n  # install stable version from conda\n  conda install neural-compressor -c conda-forge -c intel\n  ```\n  If you come into trouble with dependencies on `cv2` library you can run: `apt-get update && apt-get install -y python3-opencv`\n\n## Configuration file\n\nQuantization tuning process can be customized in the yaml configuration file. Below is a simple example:\n\n```yaml\n# cnn.yaml\n\nversion: 1.0\n\nmodel:\n  name: cnn\n  framework: mxnet\n\nquantization:\n  calibration:\n    sampling_size: 160 # number of samples for calibration\n\ntuning:\n  strategy:\n    name: basic\n  accuracy_criterion:\n    relative: 0.01\n  exit_policy:\n    timeout: 0\n  random_seed: 9527\n```\n\nWe are using the `basic` strategy, but you could also try out different ones. [Here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md) you can find a list of strategies available in INC and details of how they work. You can also add your own strategy if the existing ones do not suit your needs.\n\nSince the value of `timeout` is 0, INC will run until it finds a configuration that satisfies the accuracy criterion and then exit. Depending on the strategy this may not be ideal, as sometimes it would be better to further explore the tuning space to find a superior configuration both in terms of accuracy and speed. To achieve this, we can set a specific `timeout` value, which will tell INC how long (in seconds) it should run.\n\nFor more information about the configuration file, see the [template](https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml) from the official INC repo. Keep in mind that only the `post training quantization` is currently supported for MXNet.\n\n## Model quantization and tuning\n\nIn general, Intel® Neural Compressor requires 4 elements in order to run:  \n1. Config file - like the example above  \n2. Model to be quantized  \n3. Calibration dataloader  \n4. Evaluation function - a function that takes a model as an argument and returns the accuracy it achieves on a certain evaluation dataset. \n\n### Quantizing ResNet\n\nThe [quantization](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/performance/backend/dnnl/dnnl_quantization.html#Quantization) sections described how to quantize ResNet using the native MXNet quantization. This example shows how we can achieve the similar results (with the auto-tuning) using INC.\n\n1. Get the model\n\n```python\nimport logging\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo import vision\n\nlogging.basicConfig()\nlogger = logging.getLogger('logger')\nlogger.setLevel(logging.INFO)\n\nbatch_shape = (1, 3, 224, 224)\nresnet18 = vision.resnet18_v1(pretrained=True)\n```\n\n2. Prepare the dataset:\n\n```python\nmx.test_utils.download('http://data.mxnet.io/data/val_256_q90.rec', 'data/val_256_q90.rec')\n\nbatch_size = 16\nmean_std = {'mean_r': 123.68, 'mean_g': 116.779, 'mean_b': 103.939,\n            'std_r': 58.393, 'std_g': 57.12, 'std_b': 57.375}\n\ndata = mx.io.ImageRecordIter(path_imgrec='data/val_256_q90.rec',\n                             batch_size=batch_size,\n                             data_shape=batch_shape[1:],\n                             rand_crop=False,\n                             rand_mirror=False,\n                             shuffle=False,\n                             **mean_std)\ndata.batch_size = batch_size\n```\n\n3. Prepare the evaluation function:\n\n```python\neval_samples = batch_size*10\n\ndef eval_func(model):\n    data.reset()\n    metric = mx.metric.Accuracy()\n    for i, batch in enumerate(data):\n        if i * batch_size >= eval_samples:\n            break\n        x = batch.data[0].as_in_context(mx.cpu())\n        label = batch.label[0].as_in_context(mx.cpu())\n        outputs = model.forward(x)\n        metric.update(label, outputs)\n    return metric.get()[1]\n```\n\n4. Run Intel® Neural Compressor:\n\n```python\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./cnn.yaml\")\nquantizer.model = resnet18\nquantizer.calib_dataloader = data\nquantizer.eval_func = eval_func\nqnet = quantizer.fit().model\n```\n\nSince this model already achieves good accuracy using native quantization (less than 1% accuracy drop), for the given configuration file, INC will end on the first configuration, quantizing all layers using `naive` calibration mode for each. To see the true potential of INC, we need a model which suffers from a larger accuracy drop after quantization.\n\n### Quantizing ResNet50v2\n\nThis example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows automatically find better solution.\n\nThis is the (TODO link to INC configuration file) for this example: \n```yaml\nversion: 1.0\n\nmodel:\n  name: resnet50_v2\n  framework: mxnet\n\nquantization:\n  calibration:\n    sampling_size: 192 # number of samples for calibration\n\ntuning:\n  strategy:\n    name: mse\n  accuracy_criterion:\n    relative: 0.015\n  exit_policy:\n    timeout: 0\n    max_trials: 500\n  random_seed: 9527\n```\n\nIt could be used with script below \n(TODO link to resnet_mse.py)\nto find operator which mostly influence accuracy drops and disable it from quantization. \nYou can find description of MSE strategy \n[here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md#user-content-mse).\n\n```python\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo.vision import resnet50_v2\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.contrib.quantization import quantize_net\n\n# Preparing input data\nrgb_mean = (0.485, 0.456, 0.406)\nrgb_std = (0.229, 0.224, 0.225)\nbatch_size = 64\nnum_calib_batches = 9\n# set below proper path to ImageNet data set\ndataset = mx.gluon.data.vision.ImageRecordDataset('../imagenet/rec/val.rec')\n# Tuning in INC on whole data set takes too long time so we take only part of the whole data set\n# as representative part of it:\ndataset = dataset.take(num_calib_batches * batch_size)\ntransformer = transforms.Compose([transforms.Resize(256),\n                                  transforms.CenterCrop(224),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n# Note: as input data are used many times during tuning it is better to prepared data earlier,\n#       so lazy parameter for transform_first is set to False",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "954872677",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21127,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization_inc.md",
        "discussion_id": "954872677",
        "commented_code": "@@ -0,0 +1,290 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+\n+# Improving accuracy with Intel® Neural Compressor\n+\n+The accuracy of a model can decrease as a result of quantization. When the accuracy drop is significant, we can try to manually find a better quantization configuration (exclude some layers, try different calibration methods, etc.), but for bigger models this might prove to be a difficult and time consuming task. [Intel® Neural Compressor](https://github.com/intel/neural-compressor) (INC) tries to automate this process using several tuning heuristics, which aim to find the quantization configuration that satisfies the specified accuracy requirement.\n+\n+**NOTE:**\n+\n+Most tuning strategies will try different configurations on an evaluation dataset in order to find out how each layer affects the accuracy of the model. This means that for larger models, it may take a long time to find a solution (as the tuning space is usually larger and the evaluation itself takes longer).\n+\n+## Installation and Prerequisites\n+\n+- Install MXNet with oneDNN enabled as described in the [Get started](https://mxnet.apache.org/versions/master/get_started?platform=linux&language=python&processor=cpu&environ=pip&). (Until the 2.0 release you can use the nightly build version: `pip install --pre mxnet -f https://dist.mxnet.io/python`)\n+\n+- Install Intel® Neural Compressor:\n+\n+  Use one of the commands below to install INC (supported python versions are: 3.6, 3.7, 3.8, 3.9):\n+\n+  ```bash\n+  # install stable version from pip\n+  pip install neural-compressor\n+\n+  # install nightly version from pip\n+  pip install -i https://test.pypi.org/simple/ neural-compressor\n+\n+  # install stable version from conda\n+  conda install neural-compressor -c conda-forge -c intel\n+  ```\n+  If you come into trouble with dependencies on `cv2` library you can run: `apt-get update && apt-get install -y python3-opencv`\n+\n+## Configuration file\n+\n+Quantization tuning process can be customized in the yaml configuration file. Below is a simple example:\n+\n+```yaml\n+# cnn.yaml\n+\n+version: 1.0\n+\n+model:\n+  name: cnn\n+  framework: mxnet\n+\n+quantization:\n+  calibration:\n+    sampling_size: 160 # number of samples for calibration\n+\n+tuning:\n+  strategy:\n+    name: basic\n+  accuracy_criterion:\n+    relative: 0.01\n+  exit_policy:\n+    timeout: 0\n+  random_seed: 9527\n+```\n+\n+We are using the `basic` strategy, but you could also try out different ones. [Here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md) you can find a list of strategies available in INC and details of how they work. You can also add your own strategy if the existing ones do not suit your needs.\n+\n+Since the value of `timeout` is 0, INC will run until it finds a configuration that satisfies the accuracy criterion and then exit. Depending on the strategy this may not be ideal, as sometimes it would be better to further explore the tuning space to find a superior configuration both in terms of accuracy and speed. To achieve this, we can set a specific `timeout` value, which will tell INC how long (in seconds) it should run.\n+\n+For more information about the configuration file, see the [template](https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml) from the official INC repo. Keep in mind that only the `post training quantization` is currently supported for MXNet.\n+\n+## Model quantization and tuning\n+\n+In general, Intel® Neural Compressor requires 4 elements in order to run:  \n+1. Config file - like the example above  \n+2. Model to be quantized  \n+3. Calibration dataloader  \n+4. Evaluation function - a function that takes a model as an argument and returns the accuracy it achieves on a certain evaluation dataset. \n+\n+### Quantizing ResNet\n+\n+The [quantization](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/performance/backend/dnnl/dnnl_quantization.html#Quantization) sections described how to quantize ResNet using the native MXNet quantization. This example shows how we can achieve the similar results (with the auto-tuning) using INC.\n+\n+1. Get the model\n+\n+```python\n+import logging\n+import mxnet as mx\n+from mxnet.gluon.model_zoo import vision\n+\n+logging.basicConfig()\n+logger = logging.getLogger('logger')\n+logger.setLevel(logging.INFO)\n+\n+batch_shape = (1, 3, 224, 224)\n+resnet18 = vision.resnet18_v1(pretrained=True)\n+```\n+\n+2. Prepare the dataset:\n+\n+```python\n+mx.test_utils.download('http://data.mxnet.io/data/val_256_q90.rec', 'data/val_256_q90.rec')\n+\n+batch_size = 16\n+mean_std = {'mean_r': 123.68, 'mean_g': 116.779, 'mean_b': 103.939,\n+            'std_r': 58.393, 'std_g': 57.12, 'std_b': 57.375}\n+\n+data = mx.io.ImageRecordIter(path_imgrec='data/val_256_q90.rec',\n+                             batch_size=batch_size,\n+                             data_shape=batch_shape[1:],\n+                             rand_crop=False,\n+                             rand_mirror=False,\n+                             shuffle=False,\n+                             **mean_std)\n+data.batch_size = batch_size\n+```\n+\n+3. Prepare the evaluation function:\n+\n+```python\n+eval_samples = batch_size*10\n+\n+def eval_func(model):\n+    data.reset()\n+    metric = mx.metric.Accuracy()\n+    for i, batch in enumerate(data):\n+        if i * batch_size >= eval_samples:\n+            break\n+        x = batch.data[0].as_in_context(mx.cpu())\n+        label = batch.label[0].as_in_context(mx.cpu())\n+        outputs = model.forward(x)\n+        metric.update(label, outputs)\n+    return metric.get()[1]\n+```\n+\n+4. Run Intel® Neural Compressor:\n+\n+```python\n+from neural_compressor.experimental import Quantization\n+quantizer = Quantization(\"./cnn.yaml\")\n+quantizer.model = resnet18\n+quantizer.calib_dataloader = data\n+quantizer.eval_func = eval_func\n+qnet = quantizer.fit().model\n+```\n+\n+Since this model already achieves good accuracy using native quantization (less than 1% accuracy drop), for the given configuration file, INC will end on the first configuration, quantizing all layers using `naive` calibration mode for each. To see the true potential of INC, we need a model which suffers from a larger accuracy drop after quantization.\n+\n+### Quantizing ResNet50v2\n+\n+This example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows automatically find better solution.\n+\n+This is the (TODO link to INC configuration file) for this example: \n+```yaml\n+version: 1.0\n+\n+model:\n+  name: resnet50_v2\n+  framework: mxnet\n+\n+quantization:\n+  calibration:\n+    sampling_size: 192 # number of samples for calibration\n+\n+tuning:\n+  strategy:\n+    name: mse\n+  accuracy_criterion:\n+    relative: 0.015\n+  exit_policy:\n+    timeout: 0\n+    max_trials: 500\n+  random_seed: 9527\n+```\n+\n+It could be used with script below \n+(TODO link to resnet_mse.py)\n+to find operator which mostly influence accuracy drops and disable it from quantization. \n+You can find description of MSE strategy \n+[here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md#user-content-mse).\n+\n+```python\n+import mxnet as mx\n+from mxnet.gluon.model_zoo.vision import resnet50_v2\n+from mxnet.gluon.data.vision import transforms\n+from mxnet.contrib.quantization import quantize_net\n+\n+# Preparing input data\n+rgb_mean = (0.485, 0.456, 0.406)\n+rgb_std = (0.229, 0.224, 0.225)\n+batch_size = 64\n+num_calib_batches = 9\n+# set below proper path to ImageNet data set\n+dataset = mx.gluon.data.vision.ImageRecordDataset('../imagenet/rec/val.rec')\n+# Tuning in INC on whole data set takes too long time so we take only part of the whole data set\n+# as representative part of it:\n+dataset = dataset.take(num_calib_batches * batch_size)\n+transformer = transforms.Compose([transforms.Resize(256),\n+                                  transforms.CenterCrop(224),\n+                                  transforms.ToTensor(),\n+                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n+# Note: as input data are used many times during tuning it is better to prepared data earlier,\n+#       so lazy parameter for transform_first is set to False",
        "comment_created_at": "2022-08-25T11:55:17+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\n# Note: as input data is used many times during tuning, it is better to have it prepared earlier.\r\n#       Therefore, lazy parameter for transform_first is set to False.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "796512474",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-01T11:42:30+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining sequence of operations which can be performed one after another immediately (example: ReLU activation)",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "796512474",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "796512474",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining sequence of operations which can be performed one after another immediately (example: ReLU activation)",
        "comment_created_at": "2022-02-01T11:42:30+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\n- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "796567112",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-01T12:55:09+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining sequence of operations which can be performed one after another immediately (example: ReLU activation)\n- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization\n\nIn version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n\n## Operator Fusion\n\nModels are often represented as directed graph of operations (represented by nodes) and data flow (representad as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n\n\nThe simplest way to explain what fusion is and how it works is to present an example. On the image above is shown a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar block called residual blocks. Some possible fusion patterns are:\n\n- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n- Conv2D + Add => even simpler idea than the previous one - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "796567112",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "796567112",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining sequence of operations which can be performed one after another immediately (example: ReLU activation)\n+- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization\n+\n+In version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n+\n+## Operator Fusion\n+\n+Models are often represented as directed graph of operations (represented by nodes) and data flow (representad as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n+![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n+\n+\n+The simplest way to explain what fusion is and how it works is to present an example. On the image above is shown a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar block called residual blocks. Some possible fusion patterns are:\n+\n+- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n+- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n+- Conv2D + Add => even simpler idea than the previous one - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`",
        "comment_created_at": "2022-02-01T12:55:09+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\n- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "800439402",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-07T09:02:47+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "800439402",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "800439402",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)",
        "comment_created_at": "2022-02-07T09:02:47+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\n- memory-bound optimizations - main objective of these optimizations is to reduce the amount of memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution),\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "804494315",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-11T09:47:22+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers simultaneously. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce the amount of memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution),\n- compute-bound optimizations - these optimizations are mainly made on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution. One of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization.\n\nIn version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n\n## Operator Fusion\n\nModels are often represented as a directed graph of operations (represented by nodes) and data flow (represented as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n\n\nThe simplest way to explain what fusion is and how it works is to present an example. Image above depicts a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar blocks called residual blocks. Some possible fusion patterns are:\n\n- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "804494315",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "804494315",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers simultaneously. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce the amount of memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution),\n+- compute-bound optimizations - these optimizations are mainly made on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution. One of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization.\n+\n+In version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n+\n+## Operator Fusion\n+\n+Models are often represented as a directed graph of operations (represented by nodes) and data flow (represented as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n+![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n+\n+\n+The simplest way to explain what fusion is and how it works is to present an example. Image above depicts a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar blocks called residual blocks. Some possible fusion patterns are:\n+\n+- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n+- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n+- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.",
        "comment_created_at": "2022-02-11T09:47:22+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\n- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting the output memory, results are added to it. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.\r\n```",
        "pr_file_module": null
      }
    ]
  }
]