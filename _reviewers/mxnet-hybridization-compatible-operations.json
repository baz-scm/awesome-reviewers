[
  {
    "discussion_id": "250017909",
    "pr_number": 13735,
    "pr_file": "example/gluon/wavenet/models.py",
    "created_at": "2019-01-23T00:44:31+00:00",
    "commented_code": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n\"\"\"\nModule: WaveNet network modulep\n\"\"\"\nfrom mxnet import nd\nfrom mxnet.gluon import nn\nimport mxnet.ndarray as F\n# pylint: disable=invalid-name, too-many-arguments, arguments-differ, attribute-defined-outside-init, too-many-instance-attributes, invalid-sequence-index, no-self-use\nclass One_Hot(nn.Block):\n    \"\"\"\n    Description : generate one hot result\n    \"\"\"\n    def __init__(self, depth):\n        super(One_Hot, self).__init__()\n        self.depth = depth\n\n    def forward(self, X_in):\n        with X_in.context:\n            X_in = X_in\n            self.ones = nd.one_hot(nd.arange(self.depth), self.depth)\n            return self.ones[X_in, :]\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"({})\".format(self.depth)\n\nclass WaveNet(nn.Block):",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "250017909",
        "repo_full_name": "apache/mxnet",
        "pr_number": 13735,
        "pr_file": "example/gluon/wavenet/models.py",
        "discussion_id": "250017909",
        "commented_code": "@@ -0,0 +1,118 @@\n+\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\"\"\"\n+Module: WaveNet network modulep\n+\"\"\"\n+from mxnet import nd\n+from mxnet.gluon import nn\n+import mxnet.ndarray as F\n+# pylint: disable=invalid-name, too-many-arguments, arguments-differ, attribute-defined-outside-init, too-many-instance-attributes, invalid-sequence-index, no-self-use\n+class One_Hot(nn.Block):\n+    \"\"\"\n+    Description : generate one hot result\n+    \"\"\"\n+    def __init__(self, depth):\n+        super(One_Hot, self).__init__()\n+        self.depth = depth\n+\n+    def forward(self, X_in):\n+        with X_in.context:\n+            X_in = X_in\n+            self.ones = nd.one_hot(nd.arange(self.depth), self.depth)\n+            return self.ones[X_in, :]\n+\n+    def __repr__(self):\n+        return self.__class__.__name__ + \"({})\".format(self.depth)\n+\n+class WaveNet(nn.Block):",
        "comment_created_at": "2019-01-23T00:44:31+00:00",
        "comment_author": "ThomasDelteil",
        "comment_body": "Could we consider a hybridizable implementation?",
        "pr_file_module": null
      },
      {
        "comment_id": "252269415",
        "repo_full_name": "apache/mxnet",
        "pr_number": 13735,
        "pr_file": "example/gluon/wavenet/models.py",
        "discussion_id": "250017909",
        "commented_code": "@@ -0,0 +1,118 @@\n+\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\"\"\"\n+Module: WaveNet network modulep\n+\"\"\"\n+from mxnet import nd\n+from mxnet.gluon import nn\n+import mxnet.ndarray as F\n+# pylint: disable=invalid-name, too-many-arguments, arguments-differ, attribute-defined-outside-init, too-many-instance-attributes, invalid-sequence-index, no-self-use\n+class One_Hot(nn.Block):\n+    \"\"\"\n+    Description : generate one hot result\n+    \"\"\"\n+    def __init__(self, depth):\n+        super(One_Hot, self).__init__()\n+        self.depth = depth\n+\n+    def forward(self, X_in):\n+        with X_in.context:\n+            X_in = X_in\n+            self.ones = nd.one_hot(nd.arange(self.depth), self.depth)\n+            return self.ones[X_in, :]\n+\n+    def __repr__(self):\n+        return self.__class__.__name__ + \"({})\".format(self.depth)\n+\n+class WaveNet(nn.Block):",
        "comment_created_at": "2019-01-30T14:07:24+00:00",
        "comment_author": "seujung",
        "comment_body": "When I change Block to HybridBlock, some problem occurs in the following code.\r\n``\r\noutput = sum([s[:, :, -output.shape[2]:] for s in skip_connections])\r\n``\r\nCould you tell me how to fix the problem?",
        "pr_file_module": null
      },
      {
        "comment_id": "252419590",
        "repo_full_name": "apache/mxnet",
        "pr_number": 13735,
        "pr_file": "example/gluon/wavenet/models.py",
        "discussion_id": "250017909",
        "commented_code": "@@ -0,0 +1,118 @@\n+\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\"\"\"\n+Module: WaveNet network modulep\n+\"\"\"\n+from mxnet import nd\n+from mxnet.gluon import nn\n+import mxnet.ndarray as F\n+# pylint: disable=invalid-name, too-many-arguments, arguments-differ, attribute-defined-outside-init, too-many-instance-attributes, invalid-sequence-index, no-self-use\n+class One_Hot(nn.Block):\n+    \"\"\"\n+    Description : generate one hot result\n+    \"\"\"\n+    def __init__(self, depth):\n+        super(One_Hot, self).__init__()\n+        self.depth = depth\n+\n+    def forward(self, X_in):\n+        with X_in.context:\n+            X_in = X_in\n+            self.ones = nd.one_hot(nd.arange(self.depth), self.depth)\n+            return self.ones[X_in, :]\n+\n+    def __repr__(self):\n+        return self.__class__.__name__ + \"({})\".format(self.depth)\n+\n+class WaveNet(nn.Block):",
        "comment_created_at": "2019-01-30T20:11:28+00:00",
        "comment_author": "safrooze",
        "comment_body": "You cannot use index operator with hybridization. You need to use F.slice(). Also shape is not available with hybridized networks. You have three options:\r\n1. use padding so that output is the same size as input\r\n2. calculate the amount to slice by doing the math (a function of kernel size and dilation)\r\n3. use `sym.infer_shape()` to infer shape of the output in hybridized mode.\r\n\r\nI recommend option 2.",
        "pr_file_module": null
      },
      {
        "comment_id": "252942411",
        "repo_full_name": "apache/mxnet",
        "pr_number": 13735,
        "pr_file": "example/gluon/wavenet/models.py",
        "discussion_id": "250017909",
        "commented_code": "@@ -0,0 +1,118 @@\n+\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\"\"\"\n+Module: WaveNet network modulep\n+\"\"\"\n+from mxnet import nd\n+from mxnet.gluon import nn\n+import mxnet.ndarray as F\n+# pylint: disable=invalid-name, too-many-arguments, arguments-differ, attribute-defined-outside-init, too-many-instance-attributes, invalid-sequence-index, no-self-use\n+class One_Hot(nn.Block):\n+    \"\"\"\n+    Description : generate one hot result\n+    \"\"\"\n+    def __init__(self, depth):\n+        super(One_Hot, self).__init__()\n+        self.depth = depth\n+\n+    def forward(self, X_in):\n+        with X_in.context:\n+            X_in = X_in\n+            self.ones = nd.one_hot(nd.arange(self.depth), self.depth)\n+            return self.ones[X_in, :]\n+\n+    def __repr__(self):\n+        return self.__class__.__name__ + \"({})\".format(self.depth)\n+\n+class WaveNet(nn.Block):",
        "comment_created_at": "2019-02-01T06:13:20+00:00",
        "comment_author": "seujung",
        "comment_body": "I try to change code using HybridBlock. But, I need too much time to apply it. Is it essential to reflect this? If I have to do it, I will try to change this module.",
        "pr_file_module": null
      },
      {
        "comment_id": "265847269",
        "repo_full_name": "apache/mxnet",
        "pr_number": 13735,
        "pr_file": "example/gluon/wavenet/models.py",
        "discussion_id": "250017909",
        "commented_code": "@@ -0,0 +1,118 @@\n+\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\"\"\"\n+Module: WaveNet network modulep\n+\"\"\"\n+from mxnet import nd\n+from mxnet.gluon import nn\n+import mxnet.ndarray as F\n+# pylint: disable=invalid-name, too-many-arguments, arguments-differ, attribute-defined-outside-init, too-many-instance-attributes, invalid-sequence-index, no-self-use\n+class One_Hot(nn.Block):\n+    \"\"\"\n+    Description : generate one hot result\n+    \"\"\"\n+    def __init__(self, depth):\n+        super(One_Hot, self).__init__()\n+        self.depth = depth\n+\n+    def forward(self, X_in):\n+        with X_in.context:\n+            X_in = X_in\n+            self.ones = nd.one_hot(nd.arange(self.depth), self.depth)\n+            return self.ones[X_in, :]\n+\n+    def __repr__(self):\n+        return self.__class__.__name__ + \"({})\".format(self.depth)\n+\n+class WaveNet(nn.Block):",
        "comment_created_at": "2019-03-15T04:31:55+00:00",
        "comment_author": "seujung",
        "comment_body": "update hybrid module for wavenet model ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "697558544",
    "pr_number": 20559,
    "pr_file": "tests/python/unittest/test_autograd.py",
    "created_at": "2021-08-27T16:08:00+00:00",
    "commented_code": "dx.backward()\n    assert abs(x.grad.asscalar() - 2.71828175) < 1e-7\n\ndef test_retain_grad_drop_grad():\n    x = nd.array([1,2,3,4])\n    x.attach_grad()\n    y = nd.array([5,6,7,8])\n    y.attach_grad()\n\n    with mx.autograd.record():\n        u = x * y\n        z = u * x\n\n    u.attach_grad()\n    z.attach_grad()\n    out_grad = nd.array([10, 10, 10, 10])\n    z.backward(out_grad, retain_graph=True)\n    \n    assert (u.grad == out_grad * x).asnumpy().all()\n    assert (z.grad == out_grad).asnumpy().all()\n    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n    assert (y.grad == out_grad * x*x).asnumpy().all()\n\n    u.drop_grad()\n    z.drop_grad()\n    y.drop_grad()\n    out_grad = nd.array([0.1, 0.1, 0.1, 0.1])\n    z.backward(out_grad)\n\n    assert u.grad is None and z.grad is None and y.grad is None\n    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n\ndef test_retain_grad_drop_grad_gluon():\n    class CompBlock(mx.gluon.HybridBlock):\n        def __init__(self):\n            super().__init__()\n            self.marked_var = None\n        def forward(self, a, b):\n            out1 = a*b\n            out2 = out1 * a\n            self.marked_var = out1\n            return out2\n    x = mx.np.array([1,2,3,4])\n    y = mx.np.array([5,6,7,8])\n    x.attach_grad()\n    y.attach_grad()\n    block2 = CompBlock()\n    block2.initialize()\n    # block2.hybridize()\n    with mx.autograd.record():\n        z = block2(x, y)\n    u = block2.marked_var\n    u.attach_grad()\n    z.attach_grad()\n    z.backward(retain_graph=True)",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "697558544",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20559,
        "pr_file": "tests/python/unittest/test_autograd.py",
        "discussion_id": "697558544",
        "commented_code": "@@ -519,3 +519,68 @@ def test_gradient():\n     dx.backward()\n     assert abs(x.grad.asscalar() - 2.71828175) < 1e-7\n \n+def test_retain_grad_drop_grad():\n+    x = nd.array([1,2,3,4])\n+    x.attach_grad()\n+    y = nd.array([5,6,7,8])\n+    y.attach_grad()\n+\n+    with mx.autograd.record():\n+        u = x * y\n+        z = u * x\n+\n+    u.attach_grad()\n+    z.attach_grad()\n+    out_grad = nd.array([10, 10, 10, 10])\n+    z.backward(out_grad, retain_graph=True)\n+    \n+    assert (u.grad == out_grad * x).asnumpy().all()\n+    assert (z.grad == out_grad).asnumpy().all()\n+    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n+    assert (y.grad == out_grad * x*x).asnumpy().all()\n+\n+    u.drop_grad()\n+    z.drop_grad()\n+    y.drop_grad()\n+    out_grad = nd.array([0.1, 0.1, 0.1, 0.1])\n+    z.backward(out_grad)\n+\n+    assert u.grad is None and z.grad is None and y.grad is None\n+    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n+\n+def test_retain_grad_drop_grad_gluon():\n+    class CompBlock(mx.gluon.HybridBlock):\n+        def __init__(self):\n+            super().__init__()\n+            self.marked_var = None\n+        def forward(self, a, b):\n+            out1 = a*b\n+            out2 = out1 * a\n+            self.marked_var = out1\n+            return out2\n+    x = mx.np.array([1,2,3,4])\n+    y = mx.np.array([5,6,7,8])\n+    x.attach_grad()\n+    y.attach_grad()\n+    block2 = CompBlock()\n+    block2.initialize()\n+    # block2.hybridize()\n+    with mx.autograd.record():\n+        z = block2(x, y)\n+    u = block2.marked_var\n+    u.attach_grad()\n+    z.attach_grad()\n+    z.backward(retain_graph=True)",
        "comment_created_at": "2021-08-27T16:08:00+00:00",
        "comment_author": "leezu",
        "comment_body": "Accessing the `marked_var` outside of the Block wouldn't work for hybridized Blocks. That's because the ndarray (`out1`) is just a temporary object used in the forward function for tracing the computation. Afterwards it's discarded and only the underlying symbolic graph is kept.\r\n\r\nThus I think you'll need to support something like `attach_grad` inside of the forward function. WDYT?\r\nFor that, I'd recommend you take a look at the CachedOp and how it handles gradients. You'll need a way to express that it should return a gradient buffer for the intermediate variable.\r\n\r\nAlso keep in mind that `autograd` API currently only targets the imperative use-case (without `hybridize`) and the deferred compute tracing used during hybridization are can't be enabled at the same time as `autograd` currently:\r\n\r\nhttps://github.com/apache/incubator-mxnet/blob/71526851d857fde97660aa79ccdb77f2b1cf963f/src/imperative/imperative.cc#L301-L305",
        "pr_file_module": null
      },
      {
        "comment_id": "697632922",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20559,
        "pr_file": "tests/python/unittest/test_autograd.py",
        "discussion_id": "697558544",
        "commented_code": "@@ -519,3 +519,68 @@ def test_gradient():\n     dx.backward()\n     assert abs(x.grad.asscalar() - 2.71828175) < 1e-7\n \n+def test_retain_grad_drop_grad():\n+    x = nd.array([1,2,3,4])\n+    x.attach_grad()\n+    y = nd.array([5,6,7,8])\n+    y.attach_grad()\n+\n+    with mx.autograd.record():\n+        u = x * y\n+        z = u * x\n+\n+    u.attach_grad()\n+    z.attach_grad()\n+    out_grad = nd.array([10, 10, 10, 10])\n+    z.backward(out_grad, retain_graph=True)\n+    \n+    assert (u.grad == out_grad * x).asnumpy().all()\n+    assert (z.grad == out_grad).asnumpy().all()\n+    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n+    assert (y.grad == out_grad * x*x).asnumpy().all()\n+\n+    u.drop_grad()\n+    z.drop_grad()\n+    y.drop_grad()\n+    out_grad = nd.array([0.1, 0.1, 0.1, 0.1])\n+    z.backward(out_grad)\n+\n+    assert u.grad is None and z.grad is None and y.grad is None\n+    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n+\n+def test_retain_grad_drop_grad_gluon():\n+    class CompBlock(mx.gluon.HybridBlock):\n+        def __init__(self):\n+            super().__init__()\n+            self.marked_var = None\n+        def forward(self, a, b):\n+            out1 = a*b\n+            out2 = out1 * a\n+            self.marked_var = out1\n+            return out2\n+    x = mx.np.array([1,2,3,4])\n+    y = mx.np.array([5,6,7,8])\n+    x.attach_grad()\n+    y.attach_grad()\n+    block2 = CompBlock()\n+    block2.initialize()\n+    # block2.hybridize()\n+    with mx.autograd.record():\n+        z = block2(x, y)\n+    u = block2.marked_var\n+    u.attach_grad()\n+    z.attach_grad()\n+    z.backward(retain_graph=True)",
        "comment_created_at": "2021-08-27T18:14:10+00:00",
        "comment_author": "KexinFeng",
        "comment_body": "Hi Leo,\r\nThanks for pointing out the relation between deferred compute and autograd API.  I just committed my first implementation of `attach_grad` on hybridize mode. Indeed, the major implementation is in the forward pass, ie the invoke of `CachedOp`. \r\n\r\nThe main idea is to utilize the handle of deferred ndarray (`out1`) to mark the nonleaf node which are still deferredcompute node. And then, when invoking `CachedOp`, the autograd computation nodes are retained and linked to the ndarrays like 'out1'. Thus these ndarrays can further call `attach_grad` which will the the same as imperative mode.\r\n\r\nBut it is true that there is an incompatibility of hybridize computation with the imperative style of `attach_grad`. This incompatibility may lead to inconvenience when designing the python frontend use of this feature. This could be done next.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "701074106",
    "pr_number": 20559,
    "pr_file": "tests/python/unittest/test_autograd.py",
    "created_at": "2021-09-02T13:18:39+00:00",
    "commented_code": "dx.backward()\n    assert abs(x.grad.asscalar() - 2.71828175) < 1e-7\n\ndef test_retain_grad_drop_grad():\n    x = nd.array([1,2,3,4])\n    x.attach_grad()\n    y = nd.array([5,6,7,8])\n    y.attach_grad()\n\n    with mx.autograd.record():\n        u = x * y\n        z = u * x\n\n    u.attach_grad()\n    z.attach_grad()\n    out_grad = nd.array([10, 10, 10, 10])\n    z.backward(out_grad, retain_graph=True)\n\n    assert (u.grad == out_grad * x).asnumpy().all()\n    assert (z.grad == out_grad).asnumpy().all()\n    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n    assert (y.grad == out_grad * x*x).asnumpy().all()\n\n    u.drop_grad()\n    z.drop_grad()\n    y.drop_grad()\n    out_grad = nd.array([0.1, 0.1, 0.1, 0.1])\n    z.backward(out_grad)\n\n    assert u.grad is None and z.grad is None and y.grad is None\n    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n\ndef test_retain_grad_drop_grad_gluon():\n    class CompBlock(mx.gluon.HybridBlock):\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, a, b):\n            out1 = a*b\n            out2 = out1 * a\n            self.mark_vars(out1)\n            return out2\n\n    x = mx.np.array([1,2,3,4])\n    y = mx.np.array([5,6,7,8])\n    x.attach_grad()\n    y.attach_grad()\n    block2 = CompBlock()\n    block2.initialize()\n    # block2.hybridize()",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "701074106",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20559,
        "pr_file": "tests/python/unittest/test_autograd.py",
        "discussion_id": "701074106",
        "commented_code": "@@ -519,3 +519,110 @@ def test_gradient():\n     dx.backward()\n     assert abs(x.grad.asscalar() - 2.71828175) < 1e-7\n \n+def test_retain_grad_drop_grad():\n+    x = nd.array([1,2,3,4])\n+    x.attach_grad()\n+    y = nd.array([5,6,7,8])\n+    y.attach_grad()\n+\n+    with mx.autograd.record():\n+        u = x * y\n+        z = u * x\n+\n+    u.attach_grad()\n+    z.attach_grad()\n+    out_grad = nd.array([10, 10, 10, 10])\n+    z.backward(out_grad, retain_graph=True)\n+\n+    assert (u.grad == out_grad * x).asnumpy().all()\n+    assert (z.grad == out_grad).asnumpy().all()\n+    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n+    assert (y.grad == out_grad * x*x).asnumpy().all()\n+\n+    u.drop_grad()\n+    z.drop_grad()\n+    y.drop_grad()\n+    out_grad = nd.array([0.1, 0.1, 0.1, 0.1])\n+    z.backward(out_grad)\n+\n+    assert u.grad is None and z.grad is None and y.grad is None\n+    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n+\n+def test_retain_grad_drop_grad_gluon():\n+    class CompBlock(mx.gluon.HybridBlock):\n+        def __init__(self):\n+            super().__init__()\n+\n+        def forward(self, a, b):\n+            out1 = a*b\n+            out2 = out1 * a\n+            self.mark_vars(out1)\n+            return out2\n+\n+    x = mx.np.array([1,2,3,4])\n+    y = mx.np.array([5,6,7,8])\n+    x.attach_grad()\n+    y.attach_grad()\n+    block2 = CompBlock()\n+    block2.initialize()\n+    # block2.hybridize()",
        "comment_created_at": "2021-09-02T13:18:39+00:00",
        "comment_author": "leezu",
        "comment_body": "Should this test be run both with and without hybridize? You can parameterize the test function https://docs.pytest.org/en/6.2.x/parametrize.html",
        "pr_file_module": null
      },
      {
        "comment_id": "701944170",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20559,
        "pr_file": "tests/python/unittest/test_autograd.py",
        "discussion_id": "701074106",
        "commented_code": "@@ -519,3 +519,110 @@ def test_gradient():\n     dx.backward()\n     assert abs(x.grad.asscalar() - 2.71828175) < 1e-7\n \n+def test_retain_grad_drop_grad():\n+    x = nd.array([1,2,3,4])\n+    x.attach_grad()\n+    y = nd.array([5,6,7,8])\n+    y.attach_grad()\n+\n+    with mx.autograd.record():\n+        u = x * y\n+        z = u * x\n+\n+    u.attach_grad()\n+    z.attach_grad()\n+    out_grad = nd.array([10, 10, 10, 10])\n+    z.backward(out_grad, retain_graph=True)\n+\n+    assert (u.grad == out_grad * x).asnumpy().all()\n+    assert (z.grad == out_grad).asnumpy().all()\n+    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n+    assert (y.grad == out_grad * x*x).asnumpy().all()\n+\n+    u.drop_grad()\n+    z.drop_grad()\n+    y.drop_grad()\n+    out_grad = nd.array([0.1, 0.1, 0.1, 0.1])\n+    z.backward(out_grad)\n+\n+    assert u.grad is None and z.grad is None and y.grad is None\n+    assert (x.grad == out_grad * 2 * x * y).asnumpy().all()\n+\n+def test_retain_grad_drop_grad_gluon():\n+    class CompBlock(mx.gluon.HybridBlock):\n+        def __init__(self):\n+            super().__init__()\n+\n+        def forward(self, a, b):\n+            out1 = a*b\n+            out2 = out1 * a\n+            self.mark_vars(out1)\n+            return out2\n+\n+    x = mx.np.array([1,2,3,4])\n+    y = mx.np.array([5,6,7,8])\n+    x.attach_grad()\n+    y.attach_grad()\n+    block2 = CompBlock()\n+    block2.initialize()\n+    # block2.hybridize()",
        "comment_created_at": "2021-09-03T14:40:15+00:00",
        "comment_author": "KexinFeng",
        "comment_body": "Done.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "325399680",
    "pr_number": 15839,
    "pr_file": "python/mxnet/gluon/block.py",
    "created_at": "2019-09-17T21:45:03+00:00",
    "commented_code": "def _call_cached_op(self, *args):\n        if self._cached_op is None:\n            self._build_cache(*args)\n        assert self._cached_op, \"cached op is not None\"\n        if self._callback:\n            self._cached_op._register_op_hook(self._callback, self._monitor_all)\n            if len(self._flags) >= 2 and self._flags[1]:\n                warnings.warn(\"Callback is not supported when static_shape=True \"",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "325399680",
        "repo_full_name": "apache/mxnet",
        "pr_number": 15839,
        "pr_file": "python/mxnet/gluon/block.py",
        "discussion_id": "325399680",
        "commented_code": "@@ -833,6 +848,12 @@ def _deferred_infer_shape(self, *args):\n     def _call_cached_op(self, *args):\n         if self._cached_op is None:\n             self._build_cache(*args)\n+        assert self._cached_op, \"cached op is not None\"\n+        if self._callback:\n+            self._cached_op._register_op_hook(self._callback, self._monitor_all)\n+            if len(self._flags) >= 2 and self._flags[1]:\n+                warnings.warn(\"Callback is not supported when static_shape=True \"",
        "comment_created_at": "2019-09-17T21:45:03+00:00",
        "comment_author": "rahul003",
        "comment_body": "Why's this? If it's not supported, do not register the callback?",
        "pr_file_module": null
      },
      {
        "comment_id": "325399851",
        "repo_full_name": "apache/mxnet",
        "pr_number": 15839,
        "pr_file": "python/mxnet/gluon/block.py",
        "discussion_id": "325399680",
        "commented_code": "@@ -833,6 +848,12 @@ def _deferred_infer_shape(self, *args):\n     def _call_cached_op(self, *args):\n         if self._cached_op is None:\n             self._build_cache(*args)\n+        assert self._cached_op, \"cached op is not None\"\n+        if self._callback:\n+            self._cached_op._register_op_hook(self._callback, self._monitor_all)\n+            if len(self._flags) >= 2 and self._flags[1]:\n+                warnings.warn(\"Callback is not supported when static_shape=True \"",
        "comment_created_at": "2019-09-17T21:45:33+00:00",
        "comment_author": "rahul003",
        "comment_body": "else say something like experimental for static_shape=True if you are not confident, but it might work",
        "pr_file_module": null
      },
      {
        "comment_id": "325407901",
        "repo_full_name": "apache/mxnet",
        "pr_number": 15839,
        "pr_file": "python/mxnet/gluon/block.py",
        "discussion_id": "325399680",
        "commented_code": "@@ -833,6 +848,12 @@ def _deferred_infer_shape(self, *args):\n     def _call_cached_op(self, *args):\n         if self._cached_op is None:\n             self._build_cache(*args)\n+        assert self._cached_op, \"cached op is not None\"\n+        if self._callback:\n+            self._cached_op._register_op_hook(self._callback, self._monitor_all)\n+            if len(self._flags) >= 2 and self._flags[1]:\n+                warnings.warn(\"Callback is not supported when static_shape=True \"",
        "comment_created_at": "2019-09-17T22:10:50+00:00",
        "comment_author": "anirudh2290",
        "comment_body": "Yes I will say experimental, good point thanks. The reason this is experimental is because for certain code path the cached op directly calls engine push instead of calling InvokeOp. adding the callbacks in such cases was much more difficult since we just had the opr and the engine vars not the ndarray references. Please see line: https://github.com/apache/incubator-mxnet/blob/master/src/imperative/cached_op.cc#L689\r\n\r\nFor this case, it is not supported.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "646152752",
    "pr_number": 20262,
    "pr_file": "python/mxnet/gluon/rnn/rnn_layer.py",
    "created_at": "2021-06-06T16:02:42+00:00",
    "commented_code": "else:\n            return super(_RNNLayer, self).__call__(inputs, states, **kwargs)\n\n    def hybrid_forward(self, F, inputs, states, sequence_length=None, **kwargs):\n        if F is ndarray:\n            batch_size = inputs.shape[self._layout.find('N')]\n    def forward(self, inputs, states, sequence_length=None):\n        batch_size = inputs.shape[self._layout.find('N')]\n\n        if F is ndarray:\n            for state, info in zip(states, self.state_info(batch_size)):\n                if state.shape != info['shape']:\n                    raise ValueError(\n                        \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n                            str(info['shape']), str(state.shape)))\n        out = self._forward_kernel(F, inputs, states, sequence_length, **kwargs)\n        for state, info in zip(states, self.state_info(batch_size)):\n            if state.shape != info['shape']:\n                raise ValueError(\n                    \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n                        str(info['shape']), str(state.shape)))\n        out = self._forward_kernel(inputs, states, sequence_length)\n\n        # out is (output, state)\n        return out[0] if self.skip_states else out\n\n    def _forward_kernel(self, F, inputs, states, sequence_length, **kwargs):\n    def infer_shape(self, inputs, *args):\n        assert inputs.ndim == 3, \\\n            \"Input data should be rank-3 tensor of dim [sequence length, batch size, input size]\"\n        if not self._projection_size:\n            step = self._hidden_size\n        else:\n            step = self._projection_size\n        ni = inputs.shape[2]\n        for i in range(self._num_layers):\n            for j in ['l', 'r'][:self._dir]:\n                name = '{}{}_i2h_weight'.format(j, i)\n                getattr(self, name).shape = (self._gates*self._hidden_size, ni)\n            ni = step * self._dir\n\n    def _forward_kernel(self, inputs, states, sequence_length):\n        \"\"\" forward using CUDNN or CPU kenrel\"\"\"\n        swapaxes = F.np.swapaxes if is_np_array() else F.swapaxes\n        ctx = inputs.ctx\n        if self._layout == 'NTC':\n            inputs = swapaxes(inputs, 0, 1)\n            inputs = np.swapaxes(inputs, 0, 1)\n        if self._projection_size is None:\n            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n            params = (getattr(self, '{}{}_{}_{}'.format(d, l, g, t)).data(ctx).reshape(-1)\n                      for t in ['weight', 'bias']\n                      for l in range(self._num_layers)\n                      for d in ['l', 'r'][:self._dir]\n                      for g in ['i2h', 'h2h'])\n        else:\n            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n            params = (getattr(self, '{}{}_{}_{}'.format(d, l, g, t)).data(ctx).reshape(-1)\n                      for t in ['weight', 'bias']\n                      for l in range(self._num_layers)\n                      for d in ['l', 'r'][:self._dir]\n                      for g in ['i2h', 'h2h', 'h2r']\n                      if g != 'h2r' or t != 'bias')\n\n        rnn_param_concat = F.np._internal.rnn_param_concat if is_np_array()\\\n            else F._internal._rnn_param_concat\n        params = rnn_param_concat(*params, dim=0)\n        params = ndarray.np._internal.rnn_param_concat(*params, dim=0)",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "646152752",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20262,
        "pr_file": "python/mxnet/gluon/rnn/rnn_layer.py",
        "discussion_id": "646152752",
        "commented_code": "@@ -182,65 +180,79 @@ def __call__(self, inputs, states=None, sequence_length=None, **kwargs):\n         else:\n             return super(_RNNLayer, self).__call__(inputs, states, **kwargs)\n \n-    def hybrid_forward(self, F, inputs, states, sequence_length=None, **kwargs):\n-        if F is ndarray:\n-            batch_size = inputs.shape[self._layout.find('N')]\n+    def forward(self, inputs, states, sequence_length=None):\n+        batch_size = inputs.shape[self._layout.find('N')]\n \n-        if F is ndarray:\n-            for state, info in zip(states, self.state_info(batch_size)):\n-                if state.shape != info['shape']:\n-                    raise ValueError(\n-                        \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n-                            str(info['shape']), str(state.shape)))\n-        out = self._forward_kernel(F, inputs, states, sequence_length, **kwargs)\n+        for state, info in zip(states, self.state_info(batch_size)):\n+            if state.shape != info['shape']:\n+                raise ValueError(\n+                    \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n+                        str(info['shape']), str(state.shape)))\n+        out = self._forward_kernel(inputs, states, sequence_length)\n \n         # out is (output, state)\n         return out[0] if self.skip_states else out\n \n-    def _forward_kernel(self, F, inputs, states, sequence_length, **kwargs):\n+    def infer_shape(self, inputs, *args):\n+        assert inputs.ndim == 3, \\\n+            \"Input data should be rank-3 tensor of dim [sequence length, batch size, input size]\"\n+        if not self._projection_size:\n+            step = self._hidden_size\n+        else:\n+            step = self._projection_size\n+        ni = inputs.shape[2]\n+        for i in range(self._num_layers):\n+            for j in ['l', 'r'][:self._dir]:\n+                name = '{}{}_i2h_weight'.format(j, i)\n+                getattr(self, name).shape = (self._gates*self._hidden_size, ni)\n+            ni = step * self._dir\n+\n+    def _forward_kernel(self, inputs, states, sequence_length):\n         \"\"\" forward using CUDNN or CPU kenrel\"\"\"\n-        swapaxes = F.np.swapaxes if is_np_array() else F.swapaxes\n+        ctx = inputs.ctx\n         if self._layout == 'NTC':\n-            inputs = swapaxes(inputs, 0, 1)\n+            inputs = np.swapaxes(inputs, 0, 1)\n         if self._projection_size is None:\n-            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n+            params = (getattr(self, '{}{}_{}_{}'.format(d, l, g, t)).data(ctx).reshape(-1)\n                       for t in ['weight', 'bias']\n                       for l in range(self._num_layers)\n                       for d in ['l', 'r'][:self._dir]\n                       for g in ['i2h', 'h2h'])\n         else:\n-            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n+            params = (getattr(self, '{}{}_{}_{}'.format(d, l, g, t)).data(ctx).reshape(-1)\n                       for t in ['weight', 'bias']\n                       for l in range(self._num_layers)\n                       for d in ['l', 'r'][:self._dir]\n                       for g in ['i2h', 'h2h', 'h2r']\n                       if g != 'h2r' or t != 'bias')\n \n-        rnn_param_concat = F.np._internal.rnn_param_concat if is_np_array()\\\n-            else F._internal._rnn_param_concat\n-        params = rnn_param_concat(*params, dim=0)\n+        params = ndarray.np._internal.rnn_param_concat(*params, dim=0)",
        "comment_created_at": "2021-06-06T16:02:42+00:00",
        "comment_author": "szha",
        "comment_body": "I think we should get rid of `rnn_param_concat` by\r\n1. registering only a fused parameter for the RNN layer\r\n2. add a utility for converting the fused parameter to split parameters for consumption in RNN cells only as needed.",
        "pr_file_module": null
      },
      {
        "comment_id": "646846433",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20262,
        "pr_file": "python/mxnet/gluon/rnn/rnn_layer.py",
        "discussion_id": "646152752",
        "commented_code": "@@ -182,65 +180,79 @@ def __call__(self, inputs, states=None, sequence_length=None, **kwargs):\n         else:\n             return super(_RNNLayer, self).__call__(inputs, states, **kwargs)\n \n-    def hybrid_forward(self, F, inputs, states, sequence_length=None, **kwargs):\n-        if F is ndarray:\n-            batch_size = inputs.shape[self._layout.find('N')]\n+    def forward(self, inputs, states, sequence_length=None):\n+        batch_size = inputs.shape[self._layout.find('N')]\n \n-        if F is ndarray:\n-            for state, info in zip(states, self.state_info(batch_size)):\n-                if state.shape != info['shape']:\n-                    raise ValueError(\n-                        \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n-                            str(info['shape']), str(state.shape)))\n-        out = self._forward_kernel(F, inputs, states, sequence_length, **kwargs)\n+        for state, info in zip(states, self.state_info(batch_size)):\n+            if state.shape != info['shape']:\n+                raise ValueError(\n+                    \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n+                        str(info['shape']), str(state.shape)))\n+        out = self._forward_kernel(inputs, states, sequence_length)\n \n         # out is (output, state)\n         return out[0] if self.skip_states else out\n \n-    def _forward_kernel(self, F, inputs, states, sequence_length, **kwargs):\n+    def infer_shape(self, inputs, *args):\n+        assert inputs.ndim == 3, \\\n+            \"Input data should be rank-3 tensor of dim [sequence length, batch size, input size]\"\n+        if not self._projection_size:\n+            step = self._hidden_size\n+        else:\n+            step = self._projection_size\n+        ni = inputs.shape[2]\n+        for i in range(self._num_layers):\n+            for j in ['l', 'r'][:self._dir]:\n+                name = '{}{}_i2h_weight'.format(j, i)\n+                getattr(self, name).shape = (self._gates*self._hidden_size, ni)\n+            ni = step * self._dir\n+\n+    def _forward_kernel(self, inputs, states, sequence_length):\n         \"\"\" forward using CUDNN or CPU kenrel\"\"\"\n-        swapaxes = F.np.swapaxes if is_np_array() else F.swapaxes\n+        ctx = inputs.ctx\n         if self._layout == 'NTC':\n-            inputs = swapaxes(inputs, 0, 1)\n+            inputs = np.swapaxes(inputs, 0, 1)\n         if self._projection_size is None:\n-            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n+            params = (getattr(self, '{}{}_{}_{}'.format(d, l, g, t)).data(ctx).reshape(-1)\n                       for t in ['weight', 'bias']\n                       for l in range(self._num_layers)\n                       for d in ['l', 'r'][:self._dir]\n                       for g in ['i2h', 'h2h'])\n         else:\n-            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n+            params = (getattr(self, '{}{}_{}_{}'.format(d, l, g, t)).data(ctx).reshape(-1)\n                       for t in ['weight', 'bias']\n                       for l in range(self._num_layers)\n                       for d in ['l', 'r'][:self._dir]\n                       for g in ['i2h', 'h2h', 'h2r']\n                       if g != 'h2r' or t != 'bias')\n \n-        rnn_param_concat = F.np._internal.rnn_param_concat if is_np_array()\\\n-            else F._internal._rnn_param_concat\n-        params = rnn_param_concat(*params, dim=0)\n+        params = ndarray.np._internal.rnn_param_concat(*params, dim=0)",
        "comment_created_at": "2021-06-07T18:34:34+00:00",
        "comment_author": "barry-jin",
        "comment_body": "I think another way is to use `np.concatenate` here. Because the only difference between rnn_param_cancat and np.concatenate is [InferShape](https://github.com/apache/incubator-mxnet/blob/a6fdc7ae11ab1590f2b2a9a47379e7ab41479c72/src/operator/nn/concat.cc#L436-L440). Currently in deferred compute mode, infer_shape method is defined on python side, so we can just use `np.concatenate` here. ",
        "pr_file_module": null
      }
    ]
  }
]