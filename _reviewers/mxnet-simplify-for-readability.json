[
  {
    "discussion_id": "954763427",
    "pr_number": 21132,
    "pr_file": "src/operator/tensor/elemwise_binary_broadcast_op_basic.cc",
    "created_at": "2022-08-25T09:54:33+00:00",
    "commented_code": "const std::vector<NDArray>& inputs,\n                         const std::vector<OpReqType>& req,\n                         const std::vector<NDArray>& outputs) {\n  mxnet::TShape new_lshape, new_rshape, new_oshape;\n  int ndim_diff = BinaryBroadcastShapeCompact(inputs[0].shape(),\n                                              inputs[1].shape(),\n                                              outputs[0].shape(),\n                                              &new_lshape,\n                                              &new_rshape,\n                                              &new_oshape);\n  std::vector<NDArray> new_inputs;\n  std::vector<NDArray> new_outputs;\n  if (ndim_diff) {\n    new_inputs  = {inputs[0].Reshape(new_lshape), inputs[1].Reshape(new_rshape)};\n    new_outputs = {outputs[0].Reshape(new_oshape)};\n  } else if (inputs[0].shape().Size() == 1 && inputs[1].shape().Size() == 1) {\n    // BinaryBroadcastShapeCompact function doesn't reshape tensors of size (1,1,...,1)\n    // into shape (1). It is mandatory for oneDNN primitive to have this reshape done.\n    mxnet::TShape one_shape = mxnet::TShape(1, 1);\n    new_inputs              = {inputs[0].Reshape(one_shape), inputs[1].Reshape(one_shape)};\n    new_outputs             = {outputs[0].Reshape(one_shape)};\n  // We can use more efficient sum kernel when there is no broadcast - when shapes are the same\n  const bool same_shape = (inputs[0].shape() == inputs[1].shape());",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "954763427",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21132,
        "pr_file": "src/operator/tensor/elemwise_binary_broadcast_op_basic.cc",
        "discussion_id": "954763427",
        "commented_code": "@@ -38,31 +39,39 @@ void DNNLBinaryOpForward(const nnvm::NodeAttrs& attrs,\n                          const std::vector<NDArray>& inputs,\n                          const std::vector<OpReqType>& req,\n                          const std::vector<NDArray>& outputs) {\n-  mxnet::TShape new_lshape, new_rshape, new_oshape;\n-  int ndim_diff = BinaryBroadcastShapeCompact(inputs[0].shape(),\n-                                              inputs[1].shape(),\n-                                              outputs[0].shape(),\n-                                              &new_lshape,\n-                                              &new_rshape,\n-                                              &new_oshape);\n-  std::vector<NDArray> new_inputs;\n-  std::vector<NDArray> new_outputs;\n-  if (ndim_diff) {\n-    new_inputs  = {inputs[0].Reshape(new_lshape), inputs[1].Reshape(new_rshape)};\n-    new_outputs = {outputs[0].Reshape(new_oshape)};\n-  } else if (inputs[0].shape().Size() == 1 && inputs[1].shape().Size() == 1) {\n-    // BinaryBroadcastShapeCompact function doesn't reshape tensors of size (1,1,...,1)\n-    // into shape (1). It is mandatory for oneDNN primitive to have this reshape done.\n-    mxnet::TShape one_shape = mxnet::TShape(1, 1);\n-    new_inputs              = {inputs[0].Reshape(one_shape), inputs[1].Reshape(one_shape)};\n-    new_outputs             = {outputs[0].Reshape(one_shape)};\n+  // We can use more efficient sum kernel when there is no broadcast - when shapes are the same\n+  const bool same_shape = (inputs[0].shape() == inputs[1].shape());",
        "comment_created_at": "2022-08-25T09:54:33+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\n  const bool same_shape = inputs[0].shape() == inputs[1].shape();\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "954778291",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21132,
        "pr_file": "src/operator/tensor/elemwise_binary_broadcast_op_basic.cc",
        "discussion_id": "954763427",
        "commented_code": "@@ -38,31 +39,39 @@ void DNNLBinaryOpForward(const nnvm::NodeAttrs& attrs,\n                          const std::vector<NDArray>& inputs,\n                          const std::vector<OpReqType>& req,\n                          const std::vector<NDArray>& outputs) {\n-  mxnet::TShape new_lshape, new_rshape, new_oshape;\n-  int ndim_diff = BinaryBroadcastShapeCompact(inputs[0].shape(),\n-                                              inputs[1].shape(),\n-                                              outputs[0].shape(),\n-                                              &new_lshape,\n-                                              &new_rshape,\n-                                              &new_oshape);\n-  std::vector<NDArray> new_inputs;\n-  std::vector<NDArray> new_outputs;\n-  if (ndim_diff) {\n-    new_inputs  = {inputs[0].Reshape(new_lshape), inputs[1].Reshape(new_rshape)};\n-    new_outputs = {outputs[0].Reshape(new_oshape)};\n-  } else if (inputs[0].shape().Size() == 1 && inputs[1].shape().Size() == 1) {\n-    // BinaryBroadcastShapeCompact function doesn't reshape tensors of size (1,1,...,1)\n-    // into shape (1). It is mandatory for oneDNN primitive to have this reshape done.\n-    mxnet::TShape one_shape = mxnet::TShape(1, 1);\n-    new_inputs              = {inputs[0].Reshape(one_shape), inputs[1].Reshape(one_shape)};\n-    new_outputs             = {outputs[0].Reshape(one_shape)};\n+  // We can use more efficient sum kernel when there is no broadcast - when shapes are the same\n+  const bool same_shape = (inputs[0].shape() == inputs[1].shape());",
        "comment_created_at": "2022-08-25T10:10:31+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "Maybe put these shapes in variables as they are referred to multiple times?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "941155133",
    "pr_number": 21115,
    "pr_file": "src/operator/subgraph/dnnl/dnnl_transformer_qk_common.h",
    "created_at": "2022-08-09T10:10:26+00:00",
    "commented_code": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\n#ifndef MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_TRANSFORMER_QK_COMMON_H_\n#define MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_TRANSFORMER_QK_COMMON_H_\n\n#if MXNET_USE_ONEDNN == 1\n\n#include <string>\n#include <vector>\n\n#include \"operator/contrib/transformer-inl.h\"\n#include \"operator/numpy/np_matrix_op-inl.h\"\n#include \"operator/tensor/matrix_op-inl.h\"\n#include \"operator/subgraph/common.h\"\n#include \"dnnl_common.h\"\n#include \"dnnl_subgraph_base-inl.h\"\n#include \"dnnl_transformer-inl.h\"\n\nnamespace mxnet {\nnamespace op {\nnamespace qk_common {\n\nenum SelectStatusTransformerQK {\n  kFail = 0,\n  kStart,\n  kFirstSwapAx,\n  kSecondSwapAx,\n  kFirstReshape,\n  kSecondReshape,\n  kSuccess\n};\n\n// /*\n// kStart ---> kFirstSwapAx ---> kSecondSwapAx ---> kFirstReshape ---> kSecondReshape ---> kSuccess\n// OR\n// kStart ---> kFirstSwapAx ---> kSecondSwapAx ---> kFirstReshape ---> kSuccess\n// Each status except kStart is connected with kFail\n// */\n\nenum mode { include_split = 0, without_split };\n\ninline bool CheckSwapAxisConditionsQK(const BiDirectedNode& input_node) {\n  if (input_node.outputs.size() != 1)\n    return false;\n  return CheckSwapAxisConditions(*input_node.node);\n}\n\ninline bool CheckReshapeConditionsQK(const BiDirectedNode& input_node, const index_t out_index) {\n  if (input_node.outputs.size() != 1)\n    return false;\n  return CheckReshapeConditions(*input_node.node, out_index);\n}\n\ninline bool CheckSplitConditions(const std::vector<const BiDirectedNode*>& matched_list,\n                                 const BiDirectedNode& node) {\n  const SplitParam& param = dmlc::get<SplitParam>(node.node->attrs.parsed);\n\n  if (param.axis != -1 || param.sections != 3 || param.squeeze_axis)\n    return false;\n\n  const auto first_reshape  = (*(matched_list.end() - 2))->node;\n  const auto second_reshape = (*(matched_list.end() - 1))->node;\n  if (first_reshape->op() != Op::Get(\"_npx_reshape\") ||\n      second_reshape->op() != Op::Get(\"_npx_reshape\")) {\n    return false;\n  }\n  // 3 sections - ensure that every output is used only once\n  if (node.outputs.size() == 3 && node.outputs.count(first_reshape) &&\n      node.outputs.count(second_reshape)) {\n    return true;\n  }\n\n  return false;\n}\n\ninline bool Select(SelectStatusTransformerQK* status,",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "941155133",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21115,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_transformer_qk_common.h",
        "discussion_id": "941155133",
        "commented_code": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+#ifndef MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_TRANSFORMER_QK_COMMON_H_\n+#define MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_TRANSFORMER_QK_COMMON_H_\n+\n+#if MXNET_USE_ONEDNN == 1\n+\n+#include <string>\n+#include <vector>\n+\n+#include \"operator/contrib/transformer-inl.h\"\n+#include \"operator/numpy/np_matrix_op-inl.h\"\n+#include \"operator/tensor/matrix_op-inl.h\"\n+#include \"operator/subgraph/common.h\"\n+#include \"dnnl_common.h\"\n+#include \"dnnl_subgraph_base-inl.h\"\n+#include \"dnnl_transformer-inl.h\"\n+\n+namespace mxnet {\n+namespace op {\n+namespace qk_common {\n+\n+enum SelectStatusTransformerQK {\n+  kFail = 0,\n+  kStart,\n+  kFirstSwapAx,\n+  kSecondSwapAx,\n+  kFirstReshape,\n+  kSecondReshape,\n+  kSuccess\n+};\n+\n+// /*\n+// kStart ---> kFirstSwapAx ---> kSecondSwapAx ---> kFirstReshape ---> kSecondReshape ---> kSuccess\n+// OR\n+// kStart ---> kFirstSwapAx ---> kSecondSwapAx ---> kFirstReshape ---> kSuccess\n+// Each status except kStart is connected with kFail\n+// */\n+\n+enum mode { include_split = 0, without_split };\n+\n+inline bool CheckSwapAxisConditionsQK(const BiDirectedNode& input_node) {\n+  if (input_node.outputs.size() != 1)\n+    return false;\n+  return CheckSwapAxisConditions(*input_node.node);\n+}\n+\n+inline bool CheckReshapeConditionsQK(const BiDirectedNode& input_node, const index_t out_index) {\n+  if (input_node.outputs.size() != 1)\n+    return false;\n+  return CheckReshapeConditions(*input_node.node, out_index);\n+}\n+\n+inline bool CheckSplitConditions(const std::vector<const BiDirectedNode*>& matched_list,\n+                                 const BiDirectedNode& node) {\n+  const SplitParam& param = dmlc::get<SplitParam>(node.node->attrs.parsed);\n+\n+  if (param.axis != -1 || param.sections != 3 || param.squeeze_axis)\n+    return false;\n+\n+  const auto first_reshape  = (*(matched_list.end() - 2))->node;\n+  const auto second_reshape = (*(matched_list.end() - 1))->node;\n+  if (first_reshape->op() != Op::Get(\"_npx_reshape\") ||\n+      second_reshape->op() != Op::Get(\"_npx_reshape\")) {\n+    return false;\n+  }\n+  // 3 sections - ensure that every output is used only once\n+  if (node.outputs.size() == 3 && node.outputs.count(first_reshape) &&\n+      node.outputs.count(second_reshape)) {\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+inline bool Select(SelectStatusTransformerQK* status,",
        "comment_created_at": "2022-08-09T10:10:26+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "Why pointers instead of references?",
        "pr_file_module": null
      },
      {
        "comment_id": "942626592",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21115,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_transformer_qk_common.h",
        "discussion_id": "941155133",
        "commented_code": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+#ifndef MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_TRANSFORMER_QK_COMMON_H_\n+#define MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_TRANSFORMER_QK_COMMON_H_\n+\n+#if MXNET_USE_ONEDNN == 1\n+\n+#include <string>\n+#include <vector>\n+\n+#include \"operator/contrib/transformer-inl.h\"\n+#include \"operator/numpy/np_matrix_op-inl.h\"\n+#include \"operator/tensor/matrix_op-inl.h\"\n+#include \"operator/subgraph/common.h\"\n+#include \"dnnl_common.h\"\n+#include \"dnnl_subgraph_base-inl.h\"\n+#include \"dnnl_transformer-inl.h\"\n+\n+namespace mxnet {\n+namespace op {\n+namespace qk_common {\n+\n+enum SelectStatusTransformerQK {\n+  kFail = 0,\n+  kStart,\n+  kFirstSwapAx,\n+  kSecondSwapAx,\n+  kFirstReshape,\n+  kSecondReshape,\n+  kSuccess\n+};\n+\n+// /*\n+// kStart ---> kFirstSwapAx ---> kSecondSwapAx ---> kFirstReshape ---> kSecondReshape ---> kSuccess\n+// OR\n+// kStart ---> kFirstSwapAx ---> kSecondSwapAx ---> kFirstReshape ---> kSuccess\n+// Each status except kStart is connected with kFail\n+// */\n+\n+enum mode { include_split = 0, without_split };\n+\n+inline bool CheckSwapAxisConditionsQK(const BiDirectedNode& input_node) {\n+  if (input_node.outputs.size() != 1)\n+    return false;\n+  return CheckSwapAxisConditions(*input_node.node);\n+}\n+\n+inline bool CheckReshapeConditionsQK(const BiDirectedNode& input_node, const index_t out_index) {\n+  if (input_node.outputs.size() != 1)\n+    return false;\n+  return CheckReshapeConditions(*input_node.node, out_index);\n+}\n+\n+inline bool CheckSplitConditions(const std::vector<const BiDirectedNode*>& matched_list,\n+                                 const BiDirectedNode& node) {\n+  const SplitParam& param = dmlc::get<SplitParam>(node.node->attrs.parsed);\n+\n+  if (param.axis != -1 || param.sections != 3 || param.squeeze_axis)\n+    return false;\n+\n+  const auto first_reshape  = (*(matched_list.end() - 2))->node;\n+  const auto second_reshape = (*(matched_list.end() - 1))->node;\n+  if (first_reshape->op() != Op::Get(\"_npx_reshape\") ||\n+      second_reshape->op() != Op::Get(\"_npx_reshape\")) {\n+    return false;\n+  }\n+  // 3 sections - ensure that every output is used only once\n+  if (node.outputs.size() == 3 && node.outputs.count(first_reshape) &&\n+      node.outputs.count(second_reshape)) {\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+inline bool Select(SelectStatusTransformerQK* status,",
        "comment_created_at": "2022-08-10T15:55:42+00:00",
        "comment_author": "agrabows",
        "comment_body": "using references causes error:\r\nsrc/operator/subgraph/dnnl/dnnl_transformer_qk_common.h:93:  Is this a non-const reference? If so, make const or use a pointer: SelectStatusTransformerQK& status  [runtime/references] [2]",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "904833817",
    "pr_number": 21077,
    "pr_file": "src/operator/subgraph/dnnl/dnnl_fc_sum_fuse_property.h",
    "created_at": "2022-06-23T10:00:41+00:00",
    "commented_code": "bool SelectOutput(const BiDirectedNode& cur_node, const BiDirectedNode& output_node) override {\n    const auto cur_n    = cur_node.node;\n    const auto output_n = output_node.node;\n    if (status_ == kFail || status_ == kSuccess || output_n->is_variable()) {\n    if (status_ != kStart || output_n->is_variable()) {",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "904833817",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21077,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_fc_sum_fuse_property.h",
        "discussion_id": "904833817",
        "commented_code": "@@ -95,42 +90,27 @@ class SgDNNLFCSumFuseSelector : public SubgraphSelectorV2 {\n   bool SelectOutput(const BiDirectedNode& cur_node, const BiDirectedNode& output_node) override {\n     const auto cur_n    = cur_node.node;\n     const auto output_n = output_node.node;\n-    if (status_ == kFail || status_ == kSuccess || output_n->is_variable()) {\n+    if (status_ != kStart || output_n->is_variable()) {",
        "comment_created_at": "2022-06-23T10:00:41+00:00",
        "comment_author": "anko-intel",
        "comment_body": "The same state machine process was done on other operators. Evee it is not fully utilized here yet, I think it is better to do it in the common way. (status_ == kFail || status_ == kSuccess ||)",
        "pr_file_module": null
      },
      {
        "comment_id": "904894874",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21077,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_fc_sum_fuse_property.h",
        "discussion_id": "904833817",
        "commented_code": "@@ -95,42 +90,27 @@ class SgDNNLFCSumFuseSelector : public SubgraphSelectorV2 {\n   bool SelectOutput(const BiDirectedNode& cur_node, const BiDirectedNode& output_node) override {\n     const auto cur_n    = cur_node.node;\n     const auto output_n = output_node.node;\n-    if (status_ == kFail || status_ == kSuccess || output_n->is_variable()) {\n+    if (status_ != kStart || output_n->is_variable()) {",
        "comment_created_at": "2022-06-23T11:13:20+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "In my opinion it is a basic change and there is no need to keep this condition any longer than it needs to be especially that now it is way easier to read. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "906009360",
    "pr_number": 20976,
    "pr_file": "src/operator/subgraph/dnnl/dnnl_pow_mul_scalar.cc",
    "created_at": "2022-06-24T12:20:50+00:00",
    "commented_code": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\n/*!\n * \\file dnnl_pow_mul_scalar.cc\n * \\brief DNNL pow_mul_scalar operator based on subgraph\n */\n\n#if MXNET_USE_ONEDNN == 1\n\n#include <string>\n#include <utility>\n#include <vector>\n\n#include \"operator/nn/dnnl/dnnl_base-inl.h\"\n#include \"operator/nn/dnnl/dnnl_pow_mul_scalar-inl.h\"\n#include \"operator/subgraph/common.h\"\n\nnamespace mxnet {\nnamespace op {\nbool DNNLPowMulScalarType(const nnvm::NodeAttrs& attrs,\n                          std::vector<int>* in_attrs,\n                          std::vector<int>* out_attrs) {\n  CHECK_EQ(in_attrs->size(), 1U);\n  CHECK_EQ(out_attrs->size(), 1U);\n  const DNNLPowMulScalarParam& param = nnvm::get<DNNLPowMulScalarParam>(attrs.parsed);\n  bool scalar_is_int                 = param.exp_is_int && param.mul_is_int;\n  if (common::is_int(in_attrs->at(0)) && !scalar_is_int) {\n    TYPE_ASSIGN_CHECK(*out_attrs, 0, mshadow::kFloat64);\n  } else if (in_attrs->at(0) == mshadow::kBool) {\n    TYPE_ASSIGN_CHECK(*out_attrs, 0, scalar_is_int ? mshadow::kInt64 : mshadow::kFloat64);\n  } else {\n    TYPE_ASSIGN_CHECK(*out_attrs, 0, in_attrs->at(0));\n  }\n  return out_attrs->at(0) != -1;\n}\n\ninline static bool DNNLPowMulScalarStorageType(const nnvm::NodeAttrs& attrs,\n                                               const int dev_mask,\n                                               DispatchMode* dispatch_mode,\n                                               std::vector<int>* in_attrs,\n                                               std::vector<int>* out_attrs) {\n  return DNNLStorageType(attrs, dev_mask, true, dispatch_mode, in_attrs, out_attrs);\n}\n\ntemplate <typename OP>\nstatic void ComputeOP(const nnvm::NodeAttrs& attrs,\n                      const OpContext& ctx,\n                      mshadow::Stream<cpu>* s,\n                      const TBlob& input,\n                      const TBlob& output,\n                      const double scalar) {\n  using namespace mshadow;\n  using namespace mshadow::expr;\n  MSHADOW_TYPE_SWITCH(output.type_flag_, DType, {\n    auto temp_req    = input.dptr_ == output.dptr_ ? kWriteInplace : kWriteTo;\n    TBlob temp_tblob = input;\n    if (input.type_flag_ != output.type_flag_) {\n      temp_tblob = TBlob(ctx.requested[0].get_space_typed<cpu, 1, DType>(Shape1(output.Size()), s));\n      CastCompute<cpu>(attrs, ctx, {input}, {kWriteTo}, {temp_tblob});\n    }\n    MXNET_ASSIGN_REQ_SWITCH(temp_req, Req, {\n      mxnet_op::Kernel<mxnet_op::op_with_req<OP, Req>, cpu>::Launch(\n          s, input.Size(), output.dptr<DType>(), temp_tblob.dptr<DType>(), DType(scalar));\n    });\n  });\n}\n\nstatic void PowMulScalarCompute(const nnvm::NodeAttrs& attrs,\n                                const OpContext& ctx,\n                                const std::vector<TBlob>& inputs,\n                                const std::vector<OpReqType>& req,\n                                const std::vector<TBlob>& outputs) {\n  mshadow::Stream<cpu>* s = ctx.get_stream<cpu>();\n  DCHECK_EQ(inputs.size(), 1);\n  DCHECK_EQ(outputs.size(), 1);\n  using namespace mshadow;\n  using namespace mshadow::expr;\n  const DNNLPowMulScalarParam& param = nnvm::get<DNNLPowMulScalarParam>(attrs.parsed);\n  // temp_mid_tblob is output of power operation and input of multiplication.\n  // Its dtype depends on input dtype and scalar type.\n  TBlob temp_mid_tblob =\n      ((common::is_int(inputs[0].type_flag_) || inputs[0].type_flag_ == kBool) &&\n       !param.exp_is_int) ?\n          outputs[0] :\n          inputs[0].type_flag_ == kBool ?\n          TBlob(ctx.requested[0].get_space_typed<cpu, 1, int64_t>(Shape1(inputs[0].Size()), s)) :\n          inputs[0];",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "906009360",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20976,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_pow_mul_scalar.cc",
        "discussion_id": "906009360",
        "commented_code": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/*!\n+ * \\file dnnl_pow_mul_scalar.cc\n+ * \\brief DNNL pow_mul_scalar operator based on subgraph\n+ */\n+\n+#if MXNET_USE_ONEDNN == 1\n+\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"operator/nn/dnnl/dnnl_base-inl.h\"\n+#include \"operator/nn/dnnl/dnnl_pow_mul_scalar-inl.h\"\n+#include \"operator/subgraph/common.h\"\n+\n+namespace mxnet {\n+namespace op {\n+bool DNNLPowMulScalarType(const nnvm::NodeAttrs& attrs,\n+                          std::vector<int>* in_attrs,\n+                          std::vector<int>* out_attrs) {\n+  CHECK_EQ(in_attrs->size(), 1U);\n+  CHECK_EQ(out_attrs->size(), 1U);\n+  const DNNLPowMulScalarParam& param = nnvm::get<DNNLPowMulScalarParam>(attrs.parsed);\n+  bool scalar_is_int                 = param.exp_is_int && param.mul_is_int;\n+  if (common::is_int(in_attrs->at(0)) && !scalar_is_int) {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, mshadow::kFloat64);\n+  } else if (in_attrs->at(0) == mshadow::kBool) {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, scalar_is_int ? mshadow::kInt64 : mshadow::kFloat64);\n+  } else {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, in_attrs->at(0));\n+  }\n+  return out_attrs->at(0) != -1;\n+}\n+\n+inline static bool DNNLPowMulScalarStorageType(const nnvm::NodeAttrs& attrs,\n+                                               const int dev_mask,\n+                                               DispatchMode* dispatch_mode,\n+                                               std::vector<int>* in_attrs,\n+                                               std::vector<int>* out_attrs) {\n+  return DNNLStorageType(attrs, dev_mask, true, dispatch_mode, in_attrs, out_attrs);\n+}\n+\n+template <typename OP>\n+static void ComputeOP(const nnvm::NodeAttrs& attrs,\n+                      const OpContext& ctx,\n+                      mshadow::Stream<cpu>* s,\n+                      const TBlob& input,\n+                      const TBlob& output,\n+                      const double scalar) {\n+  using namespace mshadow;\n+  using namespace mshadow::expr;\n+  MSHADOW_TYPE_SWITCH(output.type_flag_, DType, {\n+    auto temp_req    = input.dptr_ == output.dptr_ ? kWriteInplace : kWriteTo;\n+    TBlob temp_tblob = input;\n+    if (input.type_flag_ != output.type_flag_) {\n+      temp_tblob = TBlob(ctx.requested[0].get_space_typed<cpu, 1, DType>(Shape1(output.Size()), s));\n+      CastCompute<cpu>(attrs, ctx, {input}, {kWriteTo}, {temp_tblob});\n+    }\n+    MXNET_ASSIGN_REQ_SWITCH(temp_req, Req, {\n+      mxnet_op::Kernel<mxnet_op::op_with_req<OP, Req>, cpu>::Launch(\n+          s, input.Size(), output.dptr<DType>(), temp_tblob.dptr<DType>(), DType(scalar));\n+    });\n+  });\n+}\n+\n+static void PowMulScalarCompute(const nnvm::NodeAttrs& attrs,\n+                                const OpContext& ctx,\n+                                const std::vector<TBlob>& inputs,\n+                                const std::vector<OpReqType>& req,\n+                                const std::vector<TBlob>& outputs) {\n+  mshadow::Stream<cpu>* s = ctx.get_stream<cpu>();\n+  DCHECK_EQ(inputs.size(), 1);\n+  DCHECK_EQ(outputs.size(), 1);\n+  using namespace mshadow;\n+  using namespace mshadow::expr;\n+  const DNNLPowMulScalarParam& param = nnvm::get<DNNLPowMulScalarParam>(attrs.parsed);\n+  // temp_mid_tblob is output of power operation and input of multiplication.\n+  // Its dtype depends on input dtype and scalar type.\n+  TBlob temp_mid_tblob =\n+      ((common::is_int(inputs[0].type_flag_) || inputs[0].type_flag_ == kBool) &&\n+       !param.exp_is_int) ?\n+          outputs[0] :\n+          inputs[0].type_flag_ == kBool ?\n+          TBlob(ctx.requested[0].get_space_typed<cpu, 1, int64_t>(Shape1(inputs[0].Size()), s)) :\n+          inputs[0];",
        "comment_created_at": "2022-06-24T12:20:50+00:00",
        "comment_author": "bgawrych",
        "comment_body": "please use regular if else statements, it is not readable at all",
        "pr_file_module": null
      },
      {
        "comment_id": "906030146",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20976,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_pow_mul_scalar.cc",
        "discussion_id": "906009360",
        "commented_code": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/*!\n+ * \\file dnnl_pow_mul_scalar.cc\n+ * \\brief DNNL pow_mul_scalar operator based on subgraph\n+ */\n+\n+#if MXNET_USE_ONEDNN == 1\n+\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"operator/nn/dnnl/dnnl_base-inl.h\"\n+#include \"operator/nn/dnnl/dnnl_pow_mul_scalar-inl.h\"\n+#include \"operator/subgraph/common.h\"\n+\n+namespace mxnet {\n+namespace op {\n+bool DNNLPowMulScalarType(const nnvm::NodeAttrs& attrs,\n+                          std::vector<int>* in_attrs,\n+                          std::vector<int>* out_attrs) {\n+  CHECK_EQ(in_attrs->size(), 1U);\n+  CHECK_EQ(out_attrs->size(), 1U);\n+  const DNNLPowMulScalarParam& param = nnvm::get<DNNLPowMulScalarParam>(attrs.parsed);\n+  bool scalar_is_int                 = param.exp_is_int && param.mul_is_int;\n+  if (common::is_int(in_attrs->at(0)) && !scalar_is_int) {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, mshadow::kFloat64);\n+  } else if (in_attrs->at(0) == mshadow::kBool) {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, scalar_is_int ? mshadow::kInt64 : mshadow::kFloat64);\n+  } else {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, in_attrs->at(0));\n+  }\n+  return out_attrs->at(0) != -1;\n+}\n+\n+inline static bool DNNLPowMulScalarStorageType(const nnvm::NodeAttrs& attrs,\n+                                               const int dev_mask,\n+                                               DispatchMode* dispatch_mode,\n+                                               std::vector<int>* in_attrs,\n+                                               std::vector<int>* out_attrs) {\n+  return DNNLStorageType(attrs, dev_mask, true, dispatch_mode, in_attrs, out_attrs);\n+}\n+\n+template <typename OP>\n+static void ComputeOP(const nnvm::NodeAttrs& attrs,\n+                      const OpContext& ctx,\n+                      mshadow::Stream<cpu>* s,\n+                      const TBlob& input,\n+                      const TBlob& output,\n+                      const double scalar) {\n+  using namespace mshadow;\n+  using namespace mshadow::expr;\n+  MSHADOW_TYPE_SWITCH(output.type_flag_, DType, {\n+    auto temp_req    = input.dptr_ == output.dptr_ ? kWriteInplace : kWriteTo;\n+    TBlob temp_tblob = input;\n+    if (input.type_flag_ != output.type_flag_) {\n+      temp_tblob = TBlob(ctx.requested[0].get_space_typed<cpu, 1, DType>(Shape1(output.Size()), s));\n+      CastCompute<cpu>(attrs, ctx, {input}, {kWriteTo}, {temp_tblob});\n+    }\n+    MXNET_ASSIGN_REQ_SWITCH(temp_req, Req, {\n+      mxnet_op::Kernel<mxnet_op::op_with_req<OP, Req>, cpu>::Launch(\n+          s, input.Size(), output.dptr<DType>(), temp_tblob.dptr<DType>(), DType(scalar));\n+    });\n+  });\n+}\n+\n+static void PowMulScalarCompute(const nnvm::NodeAttrs& attrs,\n+                                const OpContext& ctx,\n+                                const std::vector<TBlob>& inputs,\n+                                const std::vector<OpReqType>& req,\n+                                const std::vector<TBlob>& outputs) {\n+  mshadow::Stream<cpu>* s = ctx.get_stream<cpu>();\n+  DCHECK_EQ(inputs.size(), 1);\n+  DCHECK_EQ(outputs.size(), 1);\n+  using namespace mshadow;\n+  using namespace mshadow::expr;\n+  const DNNLPowMulScalarParam& param = nnvm::get<DNNLPowMulScalarParam>(attrs.parsed);\n+  // temp_mid_tblob is output of power operation and input of multiplication.\n+  // Its dtype depends on input dtype and scalar type.\n+  TBlob temp_mid_tblob =\n+      ((common::is_int(inputs[0].type_flag_) || inputs[0].type_flag_ == kBool) &&\n+       !param.exp_is_int) ?\n+          outputs[0] :\n+          inputs[0].type_flag_ == kBool ?\n+          TBlob(ctx.requested[0].get_space_typed<cpu, 1, int64_t>(Shape1(inputs[0].Size()), s)) :\n+          inputs[0];",
        "comment_created_at": "2022-06-24T12:49:21+00:00",
        "comment_author": "bgawrych",
        "comment_body": "also are you sure that inputs[0] should be here as temp_mid_tblob?",
        "pr_file_module": null
      },
      {
        "comment_id": "906108836",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20976,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_pow_mul_scalar.cc",
        "discussion_id": "906009360",
        "commented_code": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/*!\n+ * \\file dnnl_pow_mul_scalar.cc\n+ * \\brief DNNL pow_mul_scalar operator based on subgraph\n+ */\n+\n+#if MXNET_USE_ONEDNN == 1\n+\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"operator/nn/dnnl/dnnl_base-inl.h\"\n+#include \"operator/nn/dnnl/dnnl_pow_mul_scalar-inl.h\"\n+#include \"operator/subgraph/common.h\"\n+\n+namespace mxnet {\n+namespace op {\n+bool DNNLPowMulScalarType(const nnvm::NodeAttrs& attrs,\n+                          std::vector<int>* in_attrs,\n+                          std::vector<int>* out_attrs) {\n+  CHECK_EQ(in_attrs->size(), 1U);\n+  CHECK_EQ(out_attrs->size(), 1U);\n+  const DNNLPowMulScalarParam& param = nnvm::get<DNNLPowMulScalarParam>(attrs.parsed);\n+  bool scalar_is_int                 = param.exp_is_int && param.mul_is_int;\n+  if (common::is_int(in_attrs->at(0)) && !scalar_is_int) {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, mshadow::kFloat64);\n+  } else if (in_attrs->at(0) == mshadow::kBool) {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, scalar_is_int ? mshadow::kInt64 : mshadow::kFloat64);\n+  } else {\n+    TYPE_ASSIGN_CHECK(*out_attrs, 0, in_attrs->at(0));\n+  }\n+  return out_attrs->at(0) != -1;\n+}\n+\n+inline static bool DNNLPowMulScalarStorageType(const nnvm::NodeAttrs& attrs,\n+                                               const int dev_mask,\n+                                               DispatchMode* dispatch_mode,\n+                                               std::vector<int>* in_attrs,\n+                                               std::vector<int>* out_attrs) {\n+  return DNNLStorageType(attrs, dev_mask, true, dispatch_mode, in_attrs, out_attrs);\n+}\n+\n+template <typename OP>\n+static void ComputeOP(const nnvm::NodeAttrs& attrs,\n+                      const OpContext& ctx,\n+                      mshadow::Stream<cpu>* s,\n+                      const TBlob& input,\n+                      const TBlob& output,\n+                      const double scalar) {\n+  using namespace mshadow;\n+  using namespace mshadow::expr;\n+  MSHADOW_TYPE_SWITCH(output.type_flag_, DType, {\n+    auto temp_req    = input.dptr_ == output.dptr_ ? kWriteInplace : kWriteTo;\n+    TBlob temp_tblob = input;\n+    if (input.type_flag_ != output.type_flag_) {\n+      temp_tblob = TBlob(ctx.requested[0].get_space_typed<cpu, 1, DType>(Shape1(output.Size()), s));\n+      CastCompute<cpu>(attrs, ctx, {input}, {kWriteTo}, {temp_tblob});\n+    }\n+    MXNET_ASSIGN_REQ_SWITCH(temp_req, Req, {\n+      mxnet_op::Kernel<mxnet_op::op_with_req<OP, Req>, cpu>::Launch(\n+          s, input.Size(), output.dptr<DType>(), temp_tblob.dptr<DType>(), DType(scalar));\n+    });\n+  });\n+}\n+\n+static void PowMulScalarCompute(const nnvm::NodeAttrs& attrs,\n+                                const OpContext& ctx,\n+                                const std::vector<TBlob>& inputs,\n+                                const std::vector<OpReqType>& req,\n+                                const std::vector<TBlob>& outputs) {\n+  mshadow::Stream<cpu>* s = ctx.get_stream<cpu>();\n+  DCHECK_EQ(inputs.size(), 1);\n+  DCHECK_EQ(outputs.size(), 1);\n+  using namespace mshadow;\n+  using namespace mshadow::expr;\n+  const DNNLPowMulScalarParam& param = nnvm::get<DNNLPowMulScalarParam>(attrs.parsed);\n+  // temp_mid_tblob is output of power operation and input of multiplication.\n+  // Its dtype depends on input dtype and scalar type.\n+  TBlob temp_mid_tblob =\n+      ((common::is_int(inputs[0].type_flag_) || inputs[0].type_flag_ == kBool) &&\n+       !param.exp_is_int) ?\n+          outputs[0] :\n+          inputs[0].type_flag_ == kBool ?\n+          TBlob(ctx.requested[0].get_space_typed<cpu, 1, int64_t>(Shape1(inputs[0].Size()), s)) :\n+          inputs[0];",
        "comment_created_at": "2022-06-24T14:14:58+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "To me there is no \"should\". If the parameters of the operation are right (output of the power op is of the same dtype as the input) I believe it can be used as a temporary buffer.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "903418937",
    "pr_number": 21034,
    "pr_file": "src/operator/mshadow_op.h",
    "created_at": "2022-06-22T08:02:08+00:00",
    "commented_code": "(1.0f + math::erf(static_cast<float>(a) / SQRT_2))));\n\nMXNET_BINARY_MATH_OP_NC(gelu_grad,\n                        DType(0.5f * (1.0f + math::erf(static_cast<float>(a) / SQRT_2) +\n                                      static_cast<float>(a) *\n                                          erf_grad::Map(static_cast<float>(a) / SQRT_2) / SQRT_2)));\n                        DType(static_cast<float>(b) / static_cast<float>(a) +\n                              0.5f * static_cast<float>(a) *\n                                  erf_grad::Map(static_cast<float>(a) / SQRT_2) / SQRT_2));\n\nMXNET_UNARY_MATH_OP(gelu_tanh,\n                    DType(0.5f * static_cast<float>(a) *\n                          (1.0f + math::tanh(math::sqrt(2.0f / PI) *\n                                             (static_cast<float>(a) +\n                                              0.044715 * math::pow(static_cast<float>(a), 3))))));\n\nMXNET_BINARY_MATH_OP_NC(\n    gelu_tanh_grad,\n    DType(static_cast<float>(b) *\n          (1.0f / static_cast<float>(a) +\n           (1.0f -\n            math::tanh(math::sqrt(2.0f / PI) *\n                       (static_cast<float>(a) + 0.044715 * math::pow(static_cast<float>(a), 3))) *\n                (math::sqrt(2.0f / PI) *\n                 (1.0f + 0.134145 * math::pow(static_cast<float>(a), 2)))))));",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "903418937",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21034,
        "pr_file": "src/operator/mshadow_op.h",
        "discussion_id": "903418937",
        "commented_code": "@@ -617,9 +617,25 @@ MXNET_UNARY_MATH_OP(gelu,\n                           (1.0f + math::erf(static_cast<float>(a) / SQRT_2))));\n \n MXNET_BINARY_MATH_OP_NC(gelu_grad,\n-                        DType(0.5f * (1.0f + math::erf(static_cast<float>(a) / SQRT_2) +\n-                                      static_cast<float>(a) *\n-                                          erf_grad::Map(static_cast<float>(a) / SQRT_2) / SQRT_2)));\n+                        DType(static_cast<float>(b) / static_cast<float>(a) +\n+                              0.5f * static_cast<float>(a) *\n+                                  erf_grad::Map(static_cast<float>(a) / SQRT_2) / SQRT_2));\n+\n+MXNET_UNARY_MATH_OP(gelu_tanh,\n+                    DType(0.5f * static_cast<float>(a) *\n+                          (1.0f + math::tanh(math::sqrt(2.0f / PI) *\n+                                             (static_cast<float>(a) +\n+                                              0.044715 * math::pow(static_cast<float>(a), 3))))));\n+\n+MXNET_BINARY_MATH_OP_NC(\n+    gelu_tanh_grad,\n+    DType(static_cast<float>(b) *\n+          (1.0f / static_cast<float>(a) +\n+           (1.0f -\n+            math::tanh(math::sqrt(2.0f / PI) *\n+                       (static_cast<float>(a) + 0.044715 * math::pow(static_cast<float>(a), 3))) *\n+                (math::sqrt(2.0f / PI) *\n+                 (1.0f + 0.134145 * math::pow(static_cast<float>(a), 2)))))));",
        "comment_created_at": "2022-06-22T08:02:08+00:00",
        "comment_author": "RafLit",
        "comment_body": "It would be cleaner to define 0.044715 and 0.134145 as constants.",
        "pr_file_module": null
      },
      {
        "comment_id": "904916624",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21034,
        "pr_file": "src/operator/mshadow_op.h",
        "discussion_id": "903418937",
        "commented_code": "@@ -617,9 +617,25 @@ MXNET_UNARY_MATH_OP(gelu,\n                           (1.0f + math::erf(static_cast<float>(a) / SQRT_2))));\n \n MXNET_BINARY_MATH_OP_NC(gelu_grad,\n-                        DType(0.5f * (1.0f + math::erf(static_cast<float>(a) / SQRT_2) +\n-                                      static_cast<float>(a) *\n-                                          erf_grad::Map(static_cast<float>(a) / SQRT_2) / SQRT_2)));\n+                        DType(static_cast<float>(b) / static_cast<float>(a) +\n+                              0.5f * static_cast<float>(a) *\n+                                  erf_grad::Map(static_cast<float>(a) / SQRT_2) / SQRT_2));\n+\n+MXNET_UNARY_MATH_OP(gelu_tanh,\n+                    DType(0.5f * static_cast<float>(a) *\n+                          (1.0f + math::tanh(math::sqrt(2.0f / PI) *\n+                                             (static_cast<float>(a) +\n+                                              0.044715 * math::pow(static_cast<float>(a), 3))))));\n+\n+MXNET_BINARY_MATH_OP_NC(\n+    gelu_tanh_grad,\n+    DType(static_cast<float>(b) *\n+          (1.0f / static_cast<float>(a) +\n+           (1.0f -\n+            math::tanh(math::sqrt(2.0f / PI) *\n+                       (static_cast<float>(a) + 0.044715 * math::pow(static_cast<float>(a), 3))) *\n+                (math::sqrt(2.0f / PI) *\n+                 (1.0f + 0.134145 * math::pow(static_cast<float>(a), 2)))))));",
        "comment_created_at": "2022-06-23T11:40:54+00:00",
        "comment_author": "bgawrych",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "746508919",
    "pr_number": 20724,
    "pr_file": "src/operator/subgraph/dnnl/dnnl_post_quantize_property.h",
    "created_at": "2021-11-10T11:43:44+00:00",
    "commented_code": "#define MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_POST_QUANTIZE_PROPERTY_H_\n#if MXNET_USE_ONEDNN == 1\n\n#include <memory>\n#include <set>\n#include <string>\n#include <vector>\n\n#include \"../../nn/dnnl/dnnl_convolution-inl.h\"\n#include \"../../nn/fully_connected-inl.h\"\n#include \"../../quantization/requantize-inl.h\"\n#include \"../../tensor/elemwise_binary_op-inl.h\"\n#include \"../common.h\"\n#include \"dnnl_conv-inl.h\"\n#include \"dnnl_subgraph_base-inl.h\"\n\nnamespace mxnet {\nnamespace op {\n\nclass SgDNNLPostQuantizeSelector : public SubgraphSelector {\nconst std::set<std::string> support_req_fusion_op = {\"_contrib_quantized_elemwise_add\",\n                                                     \"_contrib_quantized_elemwise_mul\",\n                                                     \"_contrib_quantized_npi_add\",\n                                                     \"_sg_onednn_conv\",\n                                                     \"_sg_onednn_fully_connected\",\n                                                     \"_sg_onednn_selfatt_qk\",\n                                                     \"_sg_onednn_selfatt_valatt\",\n                                                     \"_sg_onednn_batch_dot\"};\n\nclass SgDNNLPostQuantizeSelector : public SubgraphSelectorV2 {\n public:\n  /*! \\brief pattern match status */\n  enum SelectStatus {\n    kFail = 0,\n    kStart,\n    kRequantize,\n    kSuccess,\n  };\n\n private:\n  bool disable_fuse_all;\n  bool disable_float_output;\n  SelectStatus status;\n  std::vector<const nnvm::Node*> matched_list;\n  std::vector<const BiDirectedNode*> matched_list;\n  std::set<std::string> support_requantize_fusion_op_name;\n\n public:\n  SgDNNLPostQuantizeSelector() {\n    support_requantize_fusion_op_name.insert(\"_sg_onednn_conv\");\n    support_requantize_fusion_op_name.insert(\"_contrib_quantized_elemwise_add\");\n    support_requantize_fusion_op_name.insert(\"_contrib_quantized_npi_add\");\n  explicit SgDNNLPostQuantizeSelector(const bool dis_fuse_all, const bool dis_float_output)\n      : disable_fuse_all(dis_fuse_all), disable_float_output(dis_float_output) {\n    support_requantize_fusion_op_name = support_req_fusion_op;\n  }\n\n  bool Select(const nnvm::Node& n) override {\n    if (n.op() && support_requantize_fusion_op_name.count(n.op()->name)) {\n      if (n.op() == Op::Get(\"_sg_onednn_conv\")) {\n        auto const& param = nnvm::get<DNNLConvFusionParam>(n.attrs.parsed);\n        if (param.full_conv_param.dnnl_param.quantized) {\n          status = kStart;\n          matched_list.clear();\n          matched_list.push_back(&n);\n          return true;\n        }\n      } else if (n.op()->name == \"_contrib_quantized_elemwise_add\" ||\n                 n.op()->name == \"_contrib_quantized_npi_add\") {\n        status = kStart;\n        matched_list.clear();\n        matched_list.push_back(&n);\n        return true;\n      }\n  bool Select(const BiDirectedNode& n) override {\n    const nnvm::Node* raw_node = n.node;\n    if ((!disable_fuse_all) && raw_node->op() &&\n        support_requantize_fusion_op_name.count(raw_node->op()->name)) {\n      status = kStart;\n      matched_list.clear();\n      matched_list.push_back(&n);\n      return true;\n    }\n    return false;\n  }\n\n  bool SelectInput(const nnvm::Node& n, const nnvm::Node& new_node) override {\n  bool SelectInput(const BiDirectedNode& n, const BiDirectedNode& new_node) override {\n    return false;\n  }\n\n  bool SelectOutput(const nnvm::Node& n, const nnvm::Node& new_node) override {\n    if (status == kFail || status == kSuccess || new_node.is_variable())\n  bool SelectOutput(const BiDirectedNode& n, const BiDirectedNode& new_node) override {\n    const nnvm::Node* raw_node     = n.node;\n    const nnvm::Node* raw_new_node = new_node.node;\n    if (status == kFail || status == kSuccess || raw_new_node->is_variable())\n      return false;\n    // If n isn't the last matched node, then we encoutered a internal\n    // branch, we should pop out the node behind n and stop fusion.\n    if (matched_list.back() != &n) {\n      status = kFail;\n      if (std::find(matched_list.begin(), matched_list.end(), &n) != matched_list.end()) {\n        while (matched_list.back() != &n) {\n          matched_list.pop_back();\n        }\n      }\n      status = kSuccess;\n      return false;\n    }\n    if (new_node.op()->name == \"_contrib_requantize\") {\n      auto const& param = nnvm::get<RequantizeParam>(new_node.attrs.parsed);\n      if (param.min_calib_range.has_value() && param.max_calib_range.has_value()) {\n        matched_list.push_back(&new_node);\n\n    switch (status) {\n      case kStart:\n        if (raw_new_node->op() == Op::Get(\"_contrib_requantize\")) {\n          auto const& param = nnvm::get<RequantizeParam>(raw_new_node->attrs.parsed);\n          if (param.min_calib_range.has_value() && param.max_calib_range.has_value()) {\n            matched_list.push_back(&new_node);\n            status = kRequantize;\n            if (raw_node->op() == Op::Get(\"_sg_onednn_conv\")) {\n              status = kSuccess;\n            }\n            return true;\n          }\n        }\n      case kRequantize:\n        if (!disable_float_output && raw_new_node->op() == Op::Get(\"_contrib_dequantize\")) {\n          CHECK(raw_node->op() == Op::Get(\"_contrib_requantize\"));\n          if (n.outputs.size() > 1) {\n            // check if requantize have other outputs than dequantize\n            // if it has we can't fuse dequantize\n            for (auto kv : n.outputs) {",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "746508919",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20724,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_post_quantize_property.h",
        "discussion_id": "746508919",
        "commented_code": "@@ -20,146 +20,209 @@\n #define MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_POST_QUANTIZE_PROPERTY_H_\n #if MXNET_USE_ONEDNN == 1\n \n+#include <memory>\n #include <set>\n #include <string>\n #include <vector>\n \n #include \"../../nn/dnnl/dnnl_convolution-inl.h\"\n+#include \"../../nn/fully_connected-inl.h\"\n #include \"../../quantization/requantize-inl.h\"\n+#include \"../../tensor/elemwise_binary_op-inl.h\"\n #include \"../common.h\"\n #include \"dnnl_conv-inl.h\"\n #include \"dnnl_subgraph_base-inl.h\"\n \n namespace mxnet {\n namespace op {\n \n-class SgDNNLPostQuantizeSelector : public SubgraphSelector {\n+const std::set<std::string> support_req_fusion_op = {\"_contrib_quantized_elemwise_add\",\n+                                                     \"_contrib_quantized_elemwise_mul\",\n+                                                     \"_contrib_quantized_npi_add\",\n+                                                     \"_sg_onednn_conv\",\n+                                                     \"_sg_onednn_fully_connected\",\n+                                                     \"_sg_onednn_selfatt_qk\",\n+                                                     \"_sg_onednn_selfatt_valatt\",\n+                                                     \"_sg_onednn_batch_dot\"};\n+\n+class SgDNNLPostQuantizeSelector : public SubgraphSelectorV2 {\n  public:\n   /*! \\brief pattern match status */\n   enum SelectStatus {\n     kFail = 0,\n     kStart,\n+    kRequantize,\n     kSuccess,\n   };\n \n  private:\n+  bool disable_fuse_all;\n+  bool disable_float_output;\n   SelectStatus status;\n-  std::vector<const nnvm::Node*> matched_list;\n+  std::vector<const BiDirectedNode*> matched_list;\n   std::set<std::string> support_requantize_fusion_op_name;\n \n  public:\n-  SgDNNLPostQuantizeSelector() {\n-    support_requantize_fusion_op_name.insert(\"_sg_onednn_conv\");\n-    support_requantize_fusion_op_name.insert(\"_contrib_quantized_elemwise_add\");\n-    support_requantize_fusion_op_name.insert(\"_contrib_quantized_npi_add\");\n+  explicit SgDNNLPostQuantizeSelector(const bool dis_fuse_all, const bool dis_float_output)\n+      : disable_fuse_all(dis_fuse_all), disable_float_output(dis_float_output) {\n+    support_requantize_fusion_op_name = support_req_fusion_op;\n   }\n \n-  bool Select(const nnvm::Node& n) override {\n-    if (n.op() && support_requantize_fusion_op_name.count(n.op()->name)) {\n-      if (n.op() == Op::Get(\"_sg_onednn_conv\")) {\n-        auto const& param = nnvm::get<DNNLConvFusionParam>(n.attrs.parsed);\n-        if (param.full_conv_param.dnnl_param.quantized) {\n-          status = kStart;\n-          matched_list.clear();\n-          matched_list.push_back(&n);\n-          return true;\n-        }\n-      } else if (n.op()->name == \"_contrib_quantized_elemwise_add\" ||\n-                 n.op()->name == \"_contrib_quantized_npi_add\") {\n-        status = kStart;\n-        matched_list.clear();\n-        matched_list.push_back(&n);\n-        return true;\n-      }\n+  bool Select(const BiDirectedNode& n) override {\n+    const nnvm::Node* raw_node = n.node;\n+    if ((!disable_fuse_all) && raw_node->op() &&\n+        support_requantize_fusion_op_name.count(raw_node->op()->name)) {\n+      status = kStart;\n+      matched_list.clear();\n+      matched_list.push_back(&n);\n+      return true;\n     }\n     return false;\n   }\n \n-  bool SelectInput(const nnvm::Node& n, const nnvm::Node& new_node) override {\n+  bool SelectInput(const BiDirectedNode& n, const BiDirectedNode& new_node) override {\n     return false;\n   }\n \n-  bool SelectOutput(const nnvm::Node& n, const nnvm::Node& new_node) override {\n-    if (status == kFail || status == kSuccess || new_node.is_variable())\n+  bool SelectOutput(const BiDirectedNode& n, const BiDirectedNode& new_node) override {\n+    const nnvm::Node* raw_node     = n.node;\n+    const nnvm::Node* raw_new_node = new_node.node;\n+    if (status == kFail || status == kSuccess || raw_new_node->is_variable())\n       return false;\n     // If n isn't the last matched node, then we encoutered a internal\n     // branch, we should pop out the node behind n and stop fusion.\n     if (matched_list.back() != &n) {\n-      status = kFail;\n+      if (std::find(matched_list.begin(), matched_list.end(), &n) != matched_list.end()) {\n+        while (matched_list.back() != &n) {\n+          matched_list.pop_back();\n+        }\n+      }\n+      status = kSuccess;\n       return false;\n     }\n-    if (new_node.op()->name == \"_contrib_requantize\") {\n-      auto const& param = nnvm::get<RequantizeParam>(new_node.attrs.parsed);\n-      if (param.min_calib_range.has_value() && param.max_calib_range.has_value()) {\n-        matched_list.push_back(&new_node);\n+\n+    switch (status) {\n+      case kStart:\n+        if (raw_new_node->op() == Op::Get(\"_contrib_requantize\")) {\n+          auto const& param = nnvm::get<RequantizeParam>(raw_new_node->attrs.parsed);\n+          if (param.min_calib_range.has_value() && param.max_calib_range.has_value()) {\n+            matched_list.push_back(&new_node);\n+            status = kRequantize;\n+            if (raw_node->op() == Op::Get(\"_sg_onednn_conv\")) {\n+              status = kSuccess;\n+            }\n+            return true;\n+          }\n+        }\n+      case kRequantize:\n+        if (!disable_float_output && raw_new_node->op() == Op::Get(\"_contrib_dequantize\")) {\n+          CHECK(raw_node->op() == Op::Get(\"_contrib_requantize\"));\n+          if (n.outputs.size() > 1) {\n+            // check if requantize have other outputs than dequantize\n+            // if it has we can't fuse dequantize\n+            for (auto kv : n.outputs) {",
        "comment_created_at": "2021-11-10T11:43:44+00:00",
        "comment_author": "mozga-intel",
        "comment_body": "How about? If an object is const-> then `kv` is const, otherwise, the `kv` might be non-const.\r\n```suggestion\r\n            for (const auto kv : n.outputs) {\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "750712116",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20724,
        "pr_file": "src/operator/subgraph/dnnl/dnnl_post_quantize_property.h",
        "discussion_id": "746508919",
        "commented_code": "@@ -20,146 +20,209 @@\n #define MXNET_OPERATOR_SUBGRAPH_DNNL_DNNL_POST_QUANTIZE_PROPERTY_H_\n #if MXNET_USE_ONEDNN == 1\n \n+#include <memory>\n #include <set>\n #include <string>\n #include <vector>\n \n #include \"../../nn/dnnl/dnnl_convolution-inl.h\"\n+#include \"../../nn/fully_connected-inl.h\"\n #include \"../../quantization/requantize-inl.h\"\n+#include \"../../tensor/elemwise_binary_op-inl.h\"\n #include \"../common.h\"\n #include \"dnnl_conv-inl.h\"\n #include \"dnnl_subgraph_base-inl.h\"\n \n namespace mxnet {\n namespace op {\n \n-class SgDNNLPostQuantizeSelector : public SubgraphSelector {\n+const std::set<std::string> support_req_fusion_op = {\"_contrib_quantized_elemwise_add\",\n+                                                     \"_contrib_quantized_elemwise_mul\",\n+                                                     \"_contrib_quantized_npi_add\",\n+                                                     \"_sg_onednn_conv\",\n+                                                     \"_sg_onednn_fully_connected\",\n+                                                     \"_sg_onednn_selfatt_qk\",\n+                                                     \"_sg_onednn_selfatt_valatt\",\n+                                                     \"_sg_onednn_batch_dot\"};\n+\n+class SgDNNLPostQuantizeSelector : public SubgraphSelectorV2 {\n  public:\n   /*! \\brief pattern match status */\n   enum SelectStatus {\n     kFail = 0,\n     kStart,\n+    kRequantize,\n     kSuccess,\n   };\n \n  private:\n+  bool disable_fuse_all;\n+  bool disable_float_output;\n   SelectStatus status;\n-  std::vector<const nnvm::Node*> matched_list;\n+  std::vector<const BiDirectedNode*> matched_list;\n   std::set<std::string> support_requantize_fusion_op_name;\n \n  public:\n-  SgDNNLPostQuantizeSelector() {\n-    support_requantize_fusion_op_name.insert(\"_sg_onednn_conv\");\n-    support_requantize_fusion_op_name.insert(\"_contrib_quantized_elemwise_add\");\n-    support_requantize_fusion_op_name.insert(\"_contrib_quantized_npi_add\");\n+  explicit SgDNNLPostQuantizeSelector(const bool dis_fuse_all, const bool dis_float_output)\n+      : disable_fuse_all(dis_fuse_all), disable_float_output(dis_float_output) {\n+    support_requantize_fusion_op_name = support_req_fusion_op;\n   }\n \n-  bool Select(const nnvm::Node& n) override {\n-    if (n.op() && support_requantize_fusion_op_name.count(n.op()->name)) {\n-      if (n.op() == Op::Get(\"_sg_onednn_conv\")) {\n-        auto const& param = nnvm::get<DNNLConvFusionParam>(n.attrs.parsed);\n-        if (param.full_conv_param.dnnl_param.quantized) {\n-          status = kStart;\n-          matched_list.clear();\n-          matched_list.push_back(&n);\n-          return true;\n-        }\n-      } else if (n.op()->name == \"_contrib_quantized_elemwise_add\" ||\n-                 n.op()->name == \"_contrib_quantized_npi_add\") {\n-        status = kStart;\n-        matched_list.clear();\n-        matched_list.push_back(&n);\n-        return true;\n-      }\n+  bool Select(const BiDirectedNode& n) override {\n+    const nnvm::Node* raw_node = n.node;\n+    if ((!disable_fuse_all) && raw_node->op() &&\n+        support_requantize_fusion_op_name.count(raw_node->op()->name)) {\n+      status = kStart;\n+      matched_list.clear();\n+      matched_list.push_back(&n);\n+      return true;\n     }\n     return false;\n   }\n \n-  bool SelectInput(const nnvm::Node& n, const nnvm::Node& new_node) override {\n+  bool SelectInput(const BiDirectedNode& n, const BiDirectedNode& new_node) override {\n     return false;\n   }\n \n-  bool SelectOutput(const nnvm::Node& n, const nnvm::Node& new_node) override {\n-    if (status == kFail || status == kSuccess || new_node.is_variable())\n+  bool SelectOutput(const BiDirectedNode& n, const BiDirectedNode& new_node) override {\n+    const nnvm::Node* raw_node     = n.node;\n+    const nnvm::Node* raw_new_node = new_node.node;\n+    if (status == kFail || status == kSuccess || raw_new_node->is_variable())\n       return false;\n     // If n isn't the last matched node, then we encoutered a internal\n     // branch, we should pop out the node behind n and stop fusion.\n     if (matched_list.back() != &n) {\n-      status = kFail;\n+      if (std::find(matched_list.begin(), matched_list.end(), &n) != matched_list.end()) {\n+        while (matched_list.back() != &n) {\n+          matched_list.pop_back();\n+        }\n+      }\n+      status = kSuccess;\n       return false;\n     }\n-    if (new_node.op()->name == \"_contrib_requantize\") {\n-      auto const& param = nnvm::get<RequantizeParam>(new_node.attrs.parsed);\n-      if (param.min_calib_range.has_value() && param.max_calib_range.has_value()) {\n-        matched_list.push_back(&new_node);\n+\n+    switch (status) {\n+      case kStart:\n+        if (raw_new_node->op() == Op::Get(\"_contrib_requantize\")) {\n+          auto const& param = nnvm::get<RequantizeParam>(raw_new_node->attrs.parsed);\n+          if (param.min_calib_range.has_value() && param.max_calib_range.has_value()) {\n+            matched_list.push_back(&new_node);\n+            status = kRequantize;\n+            if (raw_node->op() == Op::Get(\"_sg_onednn_conv\")) {\n+              status = kSuccess;\n+            }\n+            return true;\n+          }\n+        }\n+      case kRequantize:\n+        if (!disable_float_output && raw_new_node->op() == Op::Get(\"_contrib_dequantize\")) {\n+          CHECK(raw_node->op() == Op::Get(\"_contrib_requantize\"));\n+          if (n.outputs.size() > 1) {\n+            // check if requantize have other outputs than dequantize\n+            // if it has we can't fuse dequantize\n+            for (auto kv : n.outputs) {",
        "comment_created_at": "2021-11-16T22:27:00+00:00",
        "comment_author": "DominikaJedynak",
        "comment_body": "kv.first, which is the only place where kv is used, is marked const, but I can make kv const as well for clarity",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "670199194",
    "pr_number": 20430,
    "pr_file": "src/operator/quantization/quantize_graph_pass.h",
    "created_at": "2021-07-15T07:18:28+00:00",
    "commented_code": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\n/*!\n *  Copyright (c) 2021 by Contributors\n * \\file quantize_graph_pass.h\n * \\brief\n */\n#ifndef MXNET_OPERATOR_QUANTIZATION_QUANTIZE_GRAPH_PASS_H_\n#define MXNET_OPERATOR_QUANTIZATION_QUANTIZE_GRAPH_PASS_H_\n\n#include <mxnet/op_attr_types.h>\n#include <nnvm/graph.h>\n#include <nnvm/pass.h>\n#include <queue>\n#include <unordered_map>\n#include <unordered_set>\n#include <vector>\n#include <string>\n#include \"quantize_v2-inl.h\"\n#include \"../nn/mkldnn/mkldnn_fully_connected-inl.h\"\n#include \"../../common/utils.h\"\n\nnamespace mxnet {\nnamespace op {\n\nusing nnvm::Symbol;\nusing nnvm::Node;\nusing nnvm::ObjectPtr;\nusing nnvm::NodeEntry;\nusing nnvm::Graph;\n\ninline ObjectPtr CreateNode(std::string op_name, std::string node_name) {\n  ObjectPtr node = Node::Create();\n  node->attrs.name = node_name;\n  if (op_name == \"nullptr\") {\n    node->attrs.op = nullptr;\n    // ugly workaround because VariableParam is not exposed\n    node->attrs.parsed =\n      nnvm::Symbol::CreateVariable(node->attrs.name).outputs[0].node->attrs.parsed;\n  } else {\n    node->attrs.op = Op::Get(op_name);\n  }\n  return node;\n}\n\ntemplate <bool require_bias>\nstatic inline bool IsOneDNNFullyConnected(const ObjectPtr& n) {\n#if MXNET_USE_MKLDNN == 1\n  if (n->op() == Op::Get(\"_sg_mkldnn_fully_connected\")) {\n    auto const& param = nnvm::get<MKLDNNFCFullParam>(n->attrs.parsed);\n    if (!(param.mkldnn_param.channel_wise_quantize.has_value() &&\n          param.mkldnn_param.channel_wise_quantize.value())) {\n      return !require_bias || (param.default_param.no_bias == false &&\n                               n->inputs[2].node->is_variable());\n    }\n  }\n#endif\n  return false;\n}\n\nstatic inline bool IsQuantize(const ObjectPtr& n) {\n  if (n->op() == Op::Get(\"_contrib_quantize_v2\")) {\n    auto const &param = nnvm::get<QuantizeV2Param>(n->attrs.parsed);\n    if (param.min_calib_range.has_value() &&\n        param.min_calib_range.value() < 0.0f) {\n      return true;\n    }\n  }\n  return false;\n}\n\nstatic NDArray* FindInArgByName(const Graph &g, const std::string& name) {\n  const std::vector<std::string>& in_arg_names =\n      g.GetAttr<std::vector<std::string>>(\"in_arg_names\");\n  size_t i = std::distance(in_arg_names.begin(),\n                           std::find(in_arg_names.begin(), in_arg_names.end(), name));\n  if (i == in_arg_names.size()) {\n    LOG(FATAL) << name << \" not found in in_arg_names\";\n  }\n  return g.GetAttr<NDArray **>(\"in_args\")[i];\n}\n\n// Rescales weights, min_weight and max_weight. Returns bias_int32_rescale.\nstatic inline float RescaleWeights(const Graph &g, const ObjectPtr &fc, NDArray* weight_tensor) {\n  ObjectPtr &quantize = fc->inputs[0].node;\n  auto min_data = std::stof(quantize->attrs.dict.at(\"min_calib_range\"));\n  auto max_data = std::stof(quantize->attrs.dict.at(\"max_calib_range\"));\n\n  FCInputIndex id(nnvm::get<MKLDNNFCFullParam>(fc->attrs.parsed));\n  auto in = fc->inputs;\n\n  float *min_weight = FindInArgByName(g, in[id.weight_min].node->attrs.name)->data().dptr<float>();",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "670199194",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20430,
        "pr_file": "src/operator/quantization/quantize_graph_pass.h",
        "discussion_id": "670199194",
        "commented_code": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/*!\n+ *  Copyright (c) 2021 by Contributors\n+ * \\file quantize_graph_pass.h\n+ * \\brief\n+ */\n+#ifndef MXNET_OPERATOR_QUANTIZATION_QUANTIZE_GRAPH_PASS_H_\n+#define MXNET_OPERATOR_QUANTIZATION_QUANTIZE_GRAPH_PASS_H_\n+\n+#include <mxnet/op_attr_types.h>\n+#include <nnvm/graph.h>\n+#include <nnvm/pass.h>\n+#include <queue>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <string>\n+#include \"quantize_v2-inl.h\"\n+#include \"../nn/mkldnn/mkldnn_fully_connected-inl.h\"\n+#include \"../../common/utils.h\"\n+\n+namespace mxnet {\n+namespace op {\n+\n+using nnvm::Symbol;\n+using nnvm::Node;\n+using nnvm::ObjectPtr;\n+using nnvm::NodeEntry;\n+using nnvm::Graph;\n+\n+inline ObjectPtr CreateNode(std::string op_name, std::string node_name) {\n+  ObjectPtr node = Node::Create();\n+  node->attrs.name = node_name;\n+  if (op_name == \"nullptr\") {\n+    node->attrs.op = nullptr;\n+    // ugly workaround because VariableParam is not exposed\n+    node->attrs.parsed =\n+      nnvm::Symbol::CreateVariable(node->attrs.name).outputs[0].node->attrs.parsed;\n+  } else {\n+    node->attrs.op = Op::Get(op_name);\n+  }\n+  return node;\n+}\n+\n+template <bool require_bias>\n+static inline bool IsOneDNNFullyConnected(const ObjectPtr& n) {\n+#if MXNET_USE_MKLDNN == 1\n+  if (n->op() == Op::Get(\"_sg_mkldnn_fully_connected\")) {\n+    auto const& param = nnvm::get<MKLDNNFCFullParam>(n->attrs.parsed);\n+    if (!(param.mkldnn_param.channel_wise_quantize.has_value() &&\n+          param.mkldnn_param.channel_wise_quantize.value())) {\n+      return !require_bias || (param.default_param.no_bias == false &&\n+                               n->inputs[2].node->is_variable());\n+    }\n+  }\n+#endif\n+  return false;\n+}\n+\n+static inline bool IsQuantize(const ObjectPtr& n) {\n+  if (n->op() == Op::Get(\"_contrib_quantize_v2\")) {\n+    auto const &param = nnvm::get<QuantizeV2Param>(n->attrs.parsed);\n+    if (param.min_calib_range.has_value() &&\n+        param.min_calib_range.value() < 0.0f) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+static NDArray* FindInArgByName(const Graph &g, const std::string& name) {\n+  const std::vector<std::string>& in_arg_names =\n+      g.GetAttr<std::vector<std::string>>(\"in_arg_names\");\n+  size_t i = std::distance(in_arg_names.begin(),\n+                           std::find(in_arg_names.begin(), in_arg_names.end(), name));\n+  if (i == in_arg_names.size()) {\n+    LOG(FATAL) << name << \" not found in in_arg_names\";\n+  }\n+  return g.GetAttr<NDArray **>(\"in_args\")[i];\n+}\n+\n+// Rescales weights, min_weight and max_weight. Returns bias_int32_rescale.\n+static inline float RescaleWeights(const Graph &g, const ObjectPtr &fc, NDArray* weight_tensor) {\n+  ObjectPtr &quantize = fc->inputs[0].node;\n+  auto min_data = std::stof(quantize->attrs.dict.at(\"min_calib_range\"));\n+  auto max_data = std::stof(quantize->attrs.dict.at(\"max_calib_range\"));\n+\n+  FCInputIndex id(nnvm::get<MKLDNNFCFullParam>(fc->attrs.parsed));\n+  auto in = fc->inputs;\n+\n+  float *min_weight = FindInArgByName(g, in[id.weight_min].node->attrs.name)->data().dptr<float>();",
        "comment_created_at": "2021-07-15T07:18:28+00:00",
        "comment_author": "anko-intel",
        "comment_body": "One suggestion below:\r\n```suggestion\r\n  float *min_weight = \r\n    FindInArgByName(g, in[idx.weight_min].node->attrs.name)->data().dptr<float>();\r\n```\r\nor maybe add another helper function which already return pointer to float to avoid \"->data().dptr<float>()\" in the end of each line \r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "670413549",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20430,
        "pr_file": "src/operator/quantization/quantize_graph_pass.h",
        "discussion_id": "670199194",
        "commented_code": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/*!\n+ *  Copyright (c) 2021 by Contributors\n+ * \\file quantize_graph_pass.h\n+ * \\brief\n+ */\n+#ifndef MXNET_OPERATOR_QUANTIZATION_QUANTIZE_GRAPH_PASS_H_\n+#define MXNET_OPERATOR_QUANTIZATION_QUANTIZE_GRAPH_PASS_H_\n+\n+#include <mxnet/op_attr_types.h>\n+#include <nnvm/graph.h>\n+#include <nnvm/pass.h>\n+#include <queue>\n+#include <unordered_map>\n+#include <unordered_set>\n+#include <vector>\n+#include <string>\n+#include \"quantize_v2-inl.h\"\n+#include \"../nn/mkldnn/mkldnn_fully_connected-inl.h\"\n+#include \"../../common/utils.h\"\n+\n+namespace mxnet {\n+namespace op {\n+\n+using nnvm::Symbol;\n+using nnvm::Node;\n+using nnvm::ObjectPtr;\n+using nnvm::NodeEntry;\n+using nnvm::Graph;\n+\n+inline ObjectPtr CreateNode(std::string op_name, std::string node_name) {\n+  ObjectPtr node = Node::Create();\n+  node->attrs.name = node_name;\n+  if (op_name == \"nullptr\") {\n+    node->attrs.op = nullptr;\n+    // ugly workaround because VariableParam is not exposed\n+    node->attrs.parsed =\n+      nnvm::Symbol::CreateVariable(node->attrs.name).outputs[0].node->attrs.parsed;\n+  } else {\n+    node->attrs.op = Op::Get(op_name);\n+  }\n+  return node;\n+}\n+\n+template <bool require_bias>\n+static inline bool IsOneDNNFullyConnected(const ObjectPtr& n) {\n+#if MXNET_USE_MKLDNN == 1\n+  if (n->op() == Op::Get(\"_sg_mkldnn_fully_connected\")) {\n+    auto const& param = nnvm::get<MKLDNNFCFullParam>(n->attrs.parsed);\n+    if (!(param.mkldnn_param.channel_wise_quantize.has_value() &&\n+          param.mkldnn_param.channel_wise_quantize.value())) {\n+      return !require_bias || (param.default_param.no_bias == false &&\n+                               n->inputs[2].node->is_variable());\n+    }\n+  }\n+#endif\n+  return false;\n+}\n+\n+static inline bool IsQuantize(const ObjectPtr& n) {\n+  if (n->op() == Op::Get(\"_contrib_quantize_v2\")) {\n+    auto const &param = nnvm::get<QuantizeV2Param>(n->attrs.parsed);\n+    if (param.min_calib_range.has_value() &&\n+        param.min_calib_range.value() < 0.0f) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+static NDArray* FindInArgByName(const Graph &g, const std::string& name) {\n+  const std::vector<std::string>& in_arg_names =\n+      g.GetAttr<std::vector<std::string>>(\"in_arg_names\");\n+  size_t i = std::distance(in_arg_names.begin(),\n+                           std::find(in_arg_names.begin(), in_arg_names.end(), name));\n+  if (i == in_arg_names.size()) {\n+    LOG(FATAL) << name << \" not found in in_arg_names\";\n+  }\n+  return g.GetAttr<NDArray **>(\"in_args\")[i];\n+}\n+\n+// Rescales weights, min_weight and max_weight. Returns bias_int32_rescale.\n+static inline float RescaleWeights(const Graph &g, const ObjectPtr &fc, NDArray* weight_tensor) {\n+  ObjectPtr &quantize = fc->inputs[0].node;\n+  auto min_data = std::stof(quantize->attrs.dict.at(\"min_calib_range\"));\n+  auto max_data = std::stof(quantize->attrs.dict.at(\"max_calib_range\"));\n+\n+  FCInputIndex id(nnvm::get<MKLDNNFCFullParam>(fc->attrs.parsed));\n+  auto in = fc->inputs;\n+\n+  float *min_weight = FindInArgByName(g, in[id.weight_min].node->attrs.name)->data().dptr<float>();",
        "comment_created_at": "2021-07-15T12:26:40+00:00",
        "comment_author": "sfraczek",
        "comment_body": "thank you",
        "pr_file_module": null
      }
    ]
  }
]