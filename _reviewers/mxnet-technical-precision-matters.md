---
title: Technical precision matters
description: When documenting AI models, frameworks, and optimization techniques,
  precision in language is as important as precision in your algorithms. Technical
  inaccuracies or unclear explanations can lead to implementation errors and confusion.
repository: apache/mxnet
label: AI
language: Markdown
comments_count: 8
repository_stars: 20801
---

When documenting AI models, frameworks, and optimization techniques, precision in language is as important as precision in your algorithms. Technical inaccuracies or unclear explanations can lead to implementation errors and confusion.

Key practices to follow:

1. Use precise terminology when describing AI operations like quantization, operator fusion, and model optimization:
   - Be specific about performance impacts: "With quantized model there is a tiny accuracy drop, however this is the cost of great performance optimization and memory footprint reduction."
   - Clearly explain technical processes: "Last stage of quantization flow is to perform additional operator fusion."

2. Ensure grammatical correctness, especially when explaining causality in AI systems:
   - Incorrect: "find operator which mostly influence accuracy drops"
   - Correct: "find operator, which caused the most significant accuracy drop"

3. Maintain consistency in technical descriptions:
   - Use correct library names and conventions (e.g., oneDNN not ONEDNN)
   - Be consistent with framework terminology across documentation

4. Use appropriate articles and prepositions in technical explanations:
   - Incorrect: "INC allows automatically find better solution"
   - Correct: "INC allows to automatically find better solution"

Clear documentation directly impacts how effectively developers can implement and optimize AI models, particularly for critical operations like quantization that balance accuracy and performance.


[
  {
    "discussion_id": "954861509",
    "pr_number": 21127,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization_inc.md",
    "created_at": "2022-08-25T11:42:50+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n\n# Improving accuracy with Intel\u00ae Neural Compressor\n\nThe accuracy of a model can decrease as a result of quantization. When the accuracy drop is significant, we can try to manually find a better quantization configuration (exclude some layers, try different calibration methods, etc.), but for bigger models this might prove to be a difficult and time consuming task. [Intel\u00ae Neural Compressor](https://github.com/intel/neural-compressor) (INC) tries to automate this process using several tuning heuristics, which aim to find the quantization configuration that satisfies the specified accuracy requirement.\n\n**NOTE:**\n\nMost tuning strategies will try different configurations on an evaluation dataset in order to find out how each layer affects the accuracy of the model. This means that for larger models, it may take a long time to find a solution (as the tuning space is usually larger and the evaluation itself takes longer).\n\n## Installation and Prerequisites\n\n- Install MXNet with oneDNN enabled as described in the [Get started](https://mxnet.apache.org/versions/master/get_started?platform=linux&language=python&processor=cpu&environ=pip&). (Until the 2.0 release you can use the nightly build version: `pip install --pre mxnet -f https://dist.mxnet.io/python`)\n\n- Install Intel\u00ae Neural Compressor:\n\n  Use one of the commands below to install INC (supported python versions are: 3.6, 3.7, 3.8, 3.9):\n\n  ```bash\n  # install stable version from pip\n  pip install neural-compressor\n\n  # install nightly version from pip\n  pip install -i https://test.pypi.org/simple/ neural-compressor\n\n  # install stable version from conda\n  conda install neural-compressor -c conda-forge -c intel\n  ```\n  If you come into trouble with dependencies on `cv2` library you can run: `apt-get update && apt-get install -y python3-opencv`\n\n## Configuration file\n\nQuantization tuning process can be customized in the yaml configuration file. Below is a simple example:\n\n```yaml\n# cnn.yaml\n\nversion: 1.0\n\nmodel:\n  name: cnn\n  framework: mxnet\n\nquantization:\n  calibration:\n    sampling_size: 160 # number of samples for calibration\n\ntuning:\n  strategy:\n    name: basic\n  accuracy_criterion:\n    relative: 0.01\n  exit_policy:\n    timeout: 0\n  random_seed: 9527\n```\n\nWe are using the `basic` strategy, but you could also try out different ones. [Here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md) you can find a list of strategies available in INC and details of how they work. You can also add your own strategy if the existing ones do not suit your needs.\n\nSince the value of `timeout` is 0, INC will run until it finds a configuration that satisfies the accuracy criterion and then exit. Depending on the strategy this may not be ideal, as sometimes it would be better to further explore the tuning space to find a superior configuration both in terms of accuracy and speed. To achieve this, we can set a specific `timeout` value, which will tell INC how long (in seconds) it should run.\n\nFor more information about the configuration file, see the [template](https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml) from the official INC repo. Keep in mind that only the `post training quantization` is currently supported for MXNet.\n\n## Model quantization and tuning\n\nIn general, Intel\u00ae Neural Compressor requires 4 elements in order to run:  \n1. Config file - like the example above  \n2. Model to be quantized  \n3. Calibration dataloader  \n4. Evaluation function - a function that takes a model as an argument and returns the accuracy it achieves on a certain evaluation dataset. \n\n### Quantizing ResNet\n\nThe [quantization](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/performance/backend/dnnl/dnnl_quantization.html#Quantization) sections described how to quantize ResNet using the native MXNet quantization. This example shows how we can achieve the similar results (with the auto-tuning) using INC.\n\n1. Get the model\n\n```python\nimport logging\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo import vision\n\nlogging.basicConfig()\nlogger = logging.getLogger('logger')\nlogger.setLevel(logging.INFO)\n\nbatch_shape = (1, 3, 224, 224)\nresnet18 = vision.resnet18_v1(pretrained=True)\n```\n\n2. Prepare the dataset:\n\n```python\nmx.test_utils.download('http://data.mxnet.io/data/val_256_q90.rec', 'data/val_256_q90.rec')\n\nbatch_size = 16\nmean_std = {'mean_r': 123.68, 'mean_g': 116.779, 'mean_b': 103.939,\n            'std_r': 58.393, 'std_g': 57.12, 'std_b': 57.375}\n\ndata = mx.io.ImageRecordIter(path_imgrec='data/val_256_q90.rec',\n                             batch_size=batch_size,\n                             data_shape=batch_shape[1:],\n                             rand_crop=False,\n                             rand_mirror=False,\n                             shuffle=False,\n                             **mean_std)\ndata.batch_size = batch_size\n```\n\n3. Prepare the evaluation function:\n\n```python\neval_samples = batch_size*10\n\ndef eval_func(model):\n    data.reset()\n    metric = mx.metric.Accuracy()\n    for i, batch in enumerate(data):\n        if i * batch_size >= eval_samples:\n            break\n        x = batch.data[0].as_in_context(mx.cpu())\n        label = batch.label[0].as_in_context(mx.cpu())\n        outputs = model.forward(x)\n        metric.update(label, outputs)\n    return metric.get()[1]\n```\n\n4. Run Intel\u00ae Neural Compressor:\n\n```python\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./cnn.yaml\")\nquantizer.model = resnet18\nquantizer.calib_dataloader = data\nquantizer.eval_func = eval_func\nqnet = quantizer.fit().model\n```\n\nSince this model already achieves good accuracy using native quantization (less than 1% accuracy drop), for the given configuration file, INC will end on the first configuration, quantizing all layers using `naive` calibration mode for each. To see the true potential of INC, we need a model which suffers from a larger accuracy drop after quantization.\n\n### Quantizing ResNet50v2\n\nThis example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows automatically find better solution.",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "954861509",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21127,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization_inc.md",
        "discussion_id": "954861509",
        "commented_code": "@@ -0,0 +1,290 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+\n+# Improving accuracy with Intel\u00ae Neural Compressor\n+\n+The accuracy of a model can decrease as a result of quantization. When the accuracy drop is significant, we can try to manually find a better quantization configuration (exclude some layers, try different calibration methods, etc.), but for bigger models this might prove to be a difficult and time consuming task. [Intel\u00ae Neural Compressor](https://github.com/intel/neural-compressor) (INC) tries to automate this process using several tuning heuristics, which aim to find the quantization configuration that satisfies the specified accuracy requirement.\n+\n+**NOTE:**\n+\n+Most tuning strategies will try different configurations on an evaluation dataset in order to find out how each layer affects the accuracy of the model. This means that for larger models, it may take a long time to find a solution (as the tuning space is usually larger and the evaluation itself takes longer).\n+\n+## Installation and Prerequisites\n+\n+- Install MXNet with oneDNN enabled as described in the [Get started](https://mxnet.apache.org/versions/master/get_started?platform=linux&language=python&processor=cpu&environ=pip&). (Until the 2.0 release you can use the nightly build version: `pip install --pre mxnet -f https://dist.mxnet.io/python`)\n+\n+- Install Intel\u00ae Neural Compressor:\n+\n+  Use one of the commands below to install INC (supported python versions are: 3.6, 3.7, 3.8, 3.9):\n+\n+  ```bash\n+  # install stable version from pip\n+  pip install neural-compressor\n+\n+  # install nightly version from pip\n+  pip install -i https://test.pypi.org/simple/ neural-compressor\n+\n+  # install stable version from conda\n+  conda install neural-compressor -c conda-forge -c intel\n+  ```\n+  If you come into trouble with dependencies on `cv2` library you can run: `apt-get update && apt-get install -y python3-opencv`\n+\n+## Configuration file\n+\n+Quantization tuning process can be customized in the yaml configuration file. Below is a simple example:\n+\n+```yaml\n+# cnn.yaml\n+\n+version: 1.0\n+\n+model:\n+  name: cnn\n+  framework: mxnet\n+\n+quantization:\n+  calibration:\n+    sampling_size: 160 # number of samples for calibration\n+\n+tuning:\n+  strategy:\n+    name: basic\n+  accuracy_criterion:\n+    relative: 0.01\n+  exit_policy:\n+    timeout: 0\n+  random_seed: 9527\n+```\n+\n+We are using the `basic` strategy, but you could also try out different ones. [Here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md) you can find a list of strategies available in INC and details of how they work. You can also add your own strategy if the existing ones do not suit your needs.\n+\n+Since the value of `timeout` is 0, INC will run until it finds a configuration that satisfies the accuracy criterion and then exit. Depending on the strategy this may not be ideal, as sometimes it would be better to further explore the tuning space to find a superior configuration both in terms of accuracy and speed. To achieve this, we can set a specific `timeout` value, which will tell INC how long (in seconds) it should run.\n+\n+For more information about the configuration file, see the [template](https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml) from the official INC repo. Keep in mind that only the `post training quantization` is currently supported for MXNet.\n+\n+## Model quantization and tuning\n+\n+In general, Intel\u00ae Neural Compressor requires 4 elements in order to run:  \n+1. Config file - like the example above  \n+2. Model to be quantized  \n+3. Calibration dataloader  \n+4. Evaluation function - a function that takes a model as an argument and returns the accuracy it achieves on a certain evaluation dataset. \n+\n+### Quantizing ResNet\n+\n+The [quantization](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/performance/backend/dnnl/dnnl_quantization.html#Quantization) sections described how to quantize ResNet using the native MXNet quantization. This example shows how we can achieve the similar results (with the auto-tuning) using INC.\n+\n+1. Get the model\n+\n+```python\n+import logging\n+import mxnet as mx\n+from mxnet.gluon.model_zoo import vision\n+\n+logging.basicConfig()\n+logger = logging.getLogger('logger')\n+logger.setLevel(logging.INFO)\n+\n+batch_shape = (1, 3, 224, 224)\n+resnet18 = vision.resnet18_v1(pretrained=True)\n+```\n+\n+2. Prepare the dataset:\n+\n+```python\n+mx.test_utils.download('http://data.mxnet.io/data/val_256_q90.rec', 'data/val_256_q90.rec')\n+\n+batch_size = 16\n+mean_std = {'mean_r': 123.68, 'mean_g': 116.779, 'mean_b': 103.939,\n+            'std_r': 58.393, 'std_g': 57.12, 'std_b': 57.375}\n+\n+data = mx.io.ImageRecordIter(path_imgrec='data/val_256_q90.rec',\n+                             batch_size=batch_size,\n+                             data_shape=batch_shape[1:],\n+                             rand_crop=False,\n+                             rand_mirror=False,\n+                             shuffle=False,\n+                             **mean_std)\n+data.batch_size = batch_size\n+```\n+\n+3. Prepare the evaluation function:\n+\n+```python\n+eval_samples = batch_size*10\n+\n+def eval_func(model):\n+    data.reset()\n+    metric = mx.metric.Accuracy()\n+    for i, batch in enumerate(data):\n+        if i * batch_size >= eval_samples:\n+            break\n+        x = batch.data[0].as_in_context(mx.cpu())\n+        label = batch.label[0].as_in_context(mx.cpu())\n+        outputs = model.forward(x)\n+        metric.update(label, outputs)\n+    return metric.get()[1]\n+```\n+\n+4. Run Intel\u00ae Neural Compressor:\n+\n+```python\n+from neural_compressor.experimental import Quantization\n+quantizer = Quantization(\"./cnn.yaml\")\n+quantizer.model = resnet18\n+quantizer.calib_dataloader = data\n+quantizer.eval_func = eval_func\n+qnet = quantizer.fit().model\n+```\n+\n+Since this model already achieves good accuracy using native quantization (less than 1% accuracy drop), for the given configuration file, INC will end on the first configuration, quantizing all layers using `naive` calibration mode for each. To see the true potential of INC, we need a model which suffers from a larger accuracy drop after quantization.\n+\n+### Quantizing ResNet50v2\n+\n+This example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows automatically find better solution.",
        "comment_created_at": "2022-08-25T11:42:50+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\nThis example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows to automatically find better solution.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "954863542",
    "pr_number": 21127,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization_inc.md",
    "created_at": "2022-08-25T11:45:02+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n\n# Improving accuracy with Intel\u00ae Neural Compressor\n\nThe accuracy of a model can decrease as a result of quantization. When the accuracy drop is significant, we can try to manually find a better quantization configuration (exclude some layers, try different calibration methods, etc.), but for bigger models this might prove to be a difficult and time consuming task. [Intel\u00ae Neural Compressor](https://github.com/intel/neural-compressor) (INC) tries to automate this process using several tuning heuristics, which aim to find the quantization configuration that satisfies the specified accuracy requirement.\n\n**NOTE:**\n\nMost tuning strategies will try different configurations on an evaluation dataset in order to find out how each layer affects the accuracy of the model. This means that for larger models, it may take a long time to find a solution (as the tuning space is usually larger and the evaluation itself takes longer).\n\n## Installation and Prerequisites\n\n- Install MXNet with oneDNN enabled as described in the [Get started](https://mxnet.apache.org/versions/master/get_started?platform=linux&language=python&processor=cpu&environ=pip&). (Until the 2.0 release you can use the nightly build version: `pip install --pre mxnet -f https://dist.mxnet.io/python`)\n\n- Install Intel\u00ae Neural Compressor:\n\n  Use one of the commands below to install INC (supported python versions are: 3.6, 3.7, 3.8, 3.9):\n\n  ```bash\n  # install stable version from pip\n  pip install neural-compressor\n\n  # install nightly version from pip\n  pip install -i https://test.pypi.org/simple/ neural-compressor\n\n  # install stable version from conda\n  conda install neural-compressor -c conda-forge -c intel\n  ```\n  If you come into trouble with dependencies on `cv2` library you can run: `apt-get update && apt-get install -y python3-opencv`\n\n## Configuration file\n\nQuantization tuning process can be customized in the yaml configuration file. Below is a simple example:\n\n```yaml\n# cnn.yaml\n\nversion: 1.0\n\nmodel:\n  name: cnn\n  framework: mxnet\n\nquantization:\n  calibration:\n    sampling_size: 160 # number of samples for calibration\n\ntuning:\n  strategy:\n    name: basic\n  accuracy_criterion:\n    relative: 0.01\n  exit_policy:\n    timeout: 0\n  random_seed: 9527\n```\n\nWe are using the `basic` strategy, but you could also try out different ones. [Here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md) you can find a list of strategies available in INC and details of how they work. You can also add your own strategy if the existing ones do not suit your needs.\n\nSince the value of `timeout` is 0, INC will run until it finds a configuration that satisfies the accuracy criterion and then exit. Depending on the strategy this may not be ideal, as sometimes it would be better to further explore the tuning space to find a superior configuration both in terms of accuracy and speed. To achieve this, we can set a specific `timeout` value, which will tell INC how long (in seconds) it should run.\n\nFor more information about the configuration file, see the [template](https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml) from the official INC repo. Keep in mind that only the `post training quantization` is currently supported for MXNet.\n\n## Model quantization and tuning\n\nIn general, Intel\u00ae Neural Compressor requires 4 elements in order to run:  \n1. Config file - like the example above  \n2. Model to be quantized  \n3. Calibration dataloader  \n4. Evaluation function - a function that takes a model as an argument and returns the accuracy it achieves on a certain evaluation dataset. \n\n### Quantizing ResNet\n\nThe [quantization](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/performance/backend/dnnl/dnnl_quantization.html#Quantization) sections described how to quantize ResNet using the native MXNet quantization. This example shows how we can achieve the similar results (with the auto-tuning) using INC.\n\n1. Get the model\n\n```python\nimport logging\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo import vision\n\nlogging.basicConfig()\nlogger = logging.getLogger('logger')\nlogger.setLevel(logging.INFO)\n\nbatch_shape = (1, 3, 224, 224)\nresnet18 = vision.resnet18_v1(pretrained=True)\n```\n\n2. Prepare the dataset:\n\n```python\nmx.test_utils.download('http://data.mxnet.io/data/val_256_q90.rec', 'data/val_256_q90.rec')\n\nbatch_size = 16\nmean_std = {'mean_r': 123.68, 'mean_g': 116.779, 'mean_b': 103.939,\n            'std_r': 58.393, 'std_g': 57.12, 'std_b': 57.375}\n\ndata = mx.io.ImageRecordIter(path_imgrec='data/val_256_q90.rec',\n                             batch_size=batch_size,\n                             data_shape=batch_shape[1:],\n                             rand_crop=False,\n                             rand_mirror=False,\n                             shuffle=False,\n                             **mean_std)\ndata.batch_size = batch_size\n```\n\n3. Prepare the evaluation function:\n\n```python\neval_samples = batch_size*10\n\ndef eval_func(model):\n    data.reset()\n    metric = mx.metric.Accuracy()\n    for i, batch in enumerate(data):\n        if i * batch_size >= eval_samples:\n            break\n        x = batch.data[0].as_in_context(mx.cpu())\n        label = batch.label[0].as_in_context(mx.cpu())\n        outputs = model.forward(x)\n        metric.update(label, outputs)\n    return metric.get()[1]\n```\n\n4. Run Intel\u00ae Neural Compressor:\n\n```python\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./cnn.yaml\")\nquantizer.model = resnet18\nquantizer.calib_dataloader = data\nquantizer.eval_func = eval_func\nqnet = quantizer.fit().model\n```\n\nSince this model already achieves good accuracy using native quantization (less than 1% accuracy drop), for the given configuration file, INC will end on the first configuration, quantizing all layers using `naive` calibration mode for each. To see the true potential of INC, we need a model which suffers from a larger accuracy drop after quantization.\n\n### Quantizing ResNet50v2\n\nThis example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows automatically find better solution.\n\nThis is the (TODO link to INC configuration file) for this example: \n```yaml\nversion: 1.0\n\nmodel:\n  name: resnet50_v2\n  framework: mxnet\n\nquantization:\n  calibration:\n    sampling_size: 192 # number of samples for calibration\n\ntuning:\n  strategy:\n    name: mse\n  accuracy_criterion:\n    relative: 0.015\n  exit_policy:\n    timeout: 0\n    max_trials: 500\n  random_seed: 9527\n```\n\nIt could be used with script below \n(TODO link to resnet_mse.py)\nto find operator which mostly influence accuracy drops and disable it from quantization.",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "954863542",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21127,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization_inc.md",
        "discussion_id": "954863542",
        "commented_code": "@@ -0,0 +1,290 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+\n+# Improving accuracy with Intel\u00ae Neural Compressor\n+\n+The accuracy of a model can decrease as a result of quantization. When the accuracy drop is significant, we can try to manually find a better quantization configuration (exclude some layers, try different calibration methods, etc.), but for bigger models this might prove to be a difficult and time consuming task. [Intel\u00ae Neural Compressor](https://github.com/intel/neural-compressor) (INC) tries to automate this process using several tuning heuristics, which aim to find the quantization configuration that satisfies the specified accuracy requirement.\n+\n+**NOTE:**\n+\n+Most tuning strategies will try different configurations on an evaluation dataset in order to find out how each layer affects the accuracy of the model. This means that for larger models, it may take a long time to find a solution (as the tuning space is usually larger and the evaluation itself takes longer).\n+\n+## Installation and Prerequisites\n+\n+- Install MXNet with oneDNN enabled as described in the [Get started](https://mxnet.apache.org/versions/master/get_started?platform=linux&language=python&processor=cpu&environ=pip&). (Until the 2.0 release you can use the nightly build version: `pip install --pre mxnet -f https://dist.mxnet.io/python`)\n+\n+- Install Intel\u00ae Neural Compressor:\n+\n+  Use one of the commands below to install INC (supported python versions are: 3.6, 3.7, 3.8, 3.9):\n+\n+  ```bash\n+  # install stable version from pip\n+  pip install neural-compressor\n+\n+  # install nightly version from pip\n+  pip install -i https://test.pypi.org/simple/ neural-compressor\n+\n+  # install stable version from conda\n+  conda install neural-compressor -c conda-forge -c intel\n+  ```\n+  If you come into trouble with dependencies on `cv2` library you can run: `apt-get update && apt-get install -y python3-opencv`\n+\n+## Configuration file\n+\n+Quantization tuning process can be customized in the yaml configuration file. Below is a simple example:\n+\n+```yaml\n+# cnn.yaml\n+\n+version: 1.0\n+\n+model:\n+  name: cnn\n+  framework: mxnet\n+\n+quantization:\n+  calibration:\n+    sampling_size: 160 # number of samples for calibration\n+\n+tuning:\n+  strategy:\n+    name: basic\n+  accuracy_criterion:\n+    relative: 0.01\n+  exit_policy:\n+    timeout: 0\n+  random_seed: 9527\n+```\n+\n+We are using the `basic` strategy, but you could also try out different ones. [Here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md) you can find a list of strategies available in INC and details of how they work. You can also add your own strategy if the existing ones do not suit your needs.\n+\n+Since the value of `timeout` is 0, INC will run until it finds a configuration that satisfies the accuracy criterion and then exit. Depending on the strategy this may not be ideal, as sometimes it would be better to further explore the tuning space to find a superior configuration both in terms of accuracy and speed. To achieve this, we can set a specific `timeout` value, which will tell INC how long (in seconds) it should run.\n+\n+For more information about the configuration file, see the [template](https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml) from the official INC repo. Keep in mind that only the `post training quantization` is currently supported for MXNet.\n+\n+## Model quantization and tuning\n+\n+In general, Intel\u00ae Neural Compressor requires 4 elements in order to run:  \n+1. Config file - like the example above  \n+2. Model to be quantized  \n+3. Calibration dataloader  \n+4. Evaluation function - a function that takes a model as an argument and returns the accuracy it achieves on a certain evaluation dataset. \n+\n+### Quantizing ResNet\n+\n+The [quantization](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/performance/backend/dnnl/dnnl_quantization.html#Quantization) sections described how to quantize ResNet using the native MXNet quantization. This example shows how we can achieve the similar results (with the auto-tuning) using INC.\n+\n+1. Get the model\n+\n+```python\n+import logging\n+import mxnet as mx\n+from mxnet.gluon.model_zoo import vision\n+\n+logging.basicConfig()\n+logger = logging.getLogger('logger')\n+logger.setLevel(logging.INFO)\n+\n+batch_shape = (1, 3, 224, 224)\n+resnet18 = vision.resnet18_v1(pretrained=True)\n+```\n+\n+2. Prepare the dataset:\n+\n+```python\n+mx.test_utils.download('http://data.mxnet.io/data/val_256_q90.rec', 'data/val_256_q90.rec')\n+\n+batch_size = 16\n+mean_std = {'mean_r': 123.68, 'mean_g': 116.779, 'mean_b': 103.939,\n+            'std_r': 58.393, 'std_g': 57.12, 'std_b': 57.375}\n+\n+data = mx.io.ImageRecordIter(path_imgrec='data/val_256_q90.rec',\n+                             batch_size=batch_size,\n+                             data_shape=batch_shape[1:],\n+                             rand_crop=False,\n+                             rand_mirror=False,\n+                             shuffle=False,\n+                             **mean_std)\n+data.batch_size = batch_size\n+```\n+\n+3. Prepare the evaluation function:\n+\n+```python\n+eval_samples = batch_size*10\n+\n+def eval_func(model):\n+    data.reset()\n+    metric = mx.metric.Accuracy()\n+    for i, batch in enumerate(data):\n+        if i * batch_size >= eval_samples:\n+            break\n+        x = batch.data[0].as_in_context(mx.cpu())\n+        label = batch.label[0].as_in_context(mx.cpu())\n+        outputs = model.forward(x)\n+        metric.update(label, outputs)\n+    return metric.get()[1]\n+```\n+\n+4. Run Intel\u00ae Neural Compressor:\n+\n+```python\n+from neural_compressor.experimental import Quantization\n+quantizer = Quantization(\"./cnn.yaml\")\n+quantizer.model = resnet18\n+quantizer.calib_dataloader = data\n+quantizer.eval_func = eval_func\n+qnet = quantizer.fit().model\n+```\n+\n+Since this model already achieves good accuracy using native quantization (less than 1% accuracy drop), for the given configuration file, INC will end on the first configuration, quantizing all layers using `naive` calibration mode for each. To see the true potential of INC, we need a model which suffers from a larger accuracy drop after quantization.\n+\n+### Quantizing ResNet50v2\n+\n+This example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows automatically find better solution.\n+\n+This is the (TODO link to INC configuration file) for this example: \n+```yaml\n+version: 1.0\n+\n+model:\n+  name: resnet50_v2\n+  framework: mxnet\n+\n+quantization:\n+  calibration:\n+    sampling_size: 192 # number of samples for calibration\n+\n+tuning:\n+  strategy:\n+    name: mse\n+  accuracy_criterion:\n+    relative: 0.015\n+  exit_policy:\n+    timeout: 0\n+    max_trials: 500\n+  random_seed: 9527\n+```\n+\n+It could be used with script below \n+(TODO link to resnet_mse.py)\n+to find operator which mostly influence accuracy drops and disable it from quantization. ",
        "comment_created_at": "2022-08-25T11:45:02+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\nto find operator, which caused the most significant accuracy drop and disable it from quantization. \r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "954874866",
    "pr_number": 21127,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization_inc.md",
    "created_at": "2022-08-25T11:57:43+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n\n# Improving accuracy with Intel\u00ae Neural Compressor\n\nThe accuracy of a model can decrease as a result of quantization. When the accuracy drop is significant, we can try to manually find a better quantization configuration (exclude some layers, try different calibration methods, etc.), but for bigger models this might prove to be a difficult and time consuming task. [Intel\u00ae Neural Compressor](https://github.com/intel/neural-compressor) (INC) tries to automate this process using several tuning heuristics, which aim to find the quantization configuration that satisfies the specified accuracy requirement.\n\n**NOTE:**\n\nMost tuning strategies will try different configurations on an evaluation dataset in order to find out how each layer affects the accuracy of the model. This means that for larger models, it may take a long time to find a solution (as the tuning space is usually larger and the evaluation itself takes longer).\n\n## Installation and Prerequisites\n\n- Install MXNet with oneDNN enabled as described in the [Get started](https://mxnet.apache.org/versions/master/get_started?platform=linux&language=python&processor=cpu&environ=pip&). (Until the 2.0 release you can use the nightly build version: `pip install --pre mxnet -f https://dist.mxnet.io/python`)\n\n- Install Intel\u00ae Neural Compressor:\n\n  Use one of the commands below to install INC (supported python versions are: 3.6, 3.7, 3.8, 3.9):\n\n  ```bash\n  # install stable version from pip\n  pip install neural-compressor\n\n  # install nightly version from pip\n  pip install -i https://test.pypi.org/simple/ neural-compressor\n\n  # install stable version from conda\n  conda install neural-compressor -c conda-forge -c intel\n  ```\n  If you come into trouble with dependencies on `cv2` library you can run: `apt-get update && apt-get install -y python3-opencv`\n\n## Configuration file\n\nQuantization tuning process can be customized in the yaml configuration file. Below is a simple example:\n\n```yaml\n# cnn.yaml\n\nversion: 1.0\n\nmodel:\n  name: cnn\n  framework: mxnet\n\nquantization:\n  calibration:\n    sampling_size: 160 # number of samples for calibration\n\ntuning:\n  strategy:\n    name: basic\n  accuracy_criterion:\n    relative: 0.01\n  exit_policy:\n    timeout: 0\n  random_seed: 9527\n```\n\nWe are using the `basic` strategy, but you could also try out different ones. [Here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md) you can find a list of strategies available in INC and details of how they work. You can also add your own strategy if the existing ones do not suit your needs.\n\nSince the value of `timeout` is 0, INC will run until it finds a configuration that satisfies the accuracy criterion and then exit. Depending on the strategy this may not be ideal, as sometimes it would be better to further explore the tuning space to find a superior configuration both in terms of accuracy and speed. To achieve this, we can set a specific `timeout` value, which will tell INC how long (in seconds) it should run.\n\nFor more information about the configuration file, see the [template](https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml) from the official INC repo. Keep in mind that only the `post training quantization` is currently supported for MXNet.\n\n## Model quantization and tuning\n\nIn general, Intel\u00ae Neural Compressor requires 4 elements in order to run:  \n1. Config file - like the example above  \n2. Model to be quantized  \n3. Calibration dataloader  \n4. Evaluation function - a function that takes a model as an argument and returns the accuracy it achieves on a certain evaluation dataset. \n\n### Quantizing ResNet\n\nThe [quantization](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/performance/backend/dnnl/dnnl_quantization.html#Quantization) sections described how to quantize ResNet using the native MXNet quantization. This example shows how we can achieve the similar results (with the auto-tuning) using INC.\n\n1. Get the model\n\n```python\nimport logging\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo import vision\n\nlogging.basicConfig()\nlogger = logging.getLogger('logger')\nlogger.setLevel(logging.INFO)\n\nbatch_shape = (1, 3, 224, 224)\nresnet18 = vision.resnet18_v1(pretrained=True)\n```\n\n2. Prepare the dataset:\n\n```python\nmx.test_utils.download('http://data.mxnet.io/data/val_256_q90.rec', 'data/val_256_q90.rec')\n\nbatch_size = 16\nmean_std = {'mean_r': 123.68, 'mean_g': 116.779, 'mean_b': 103.939,\n            'std_r': 58.393, 'std_g': 57.12, 'std_b': 57.375}\n\ndata = mx.io.ImageRecordIter(path_imgrec='data/val_256_q90.rec',\n                             batch_size=batch_size,\n                             data_shape=batch_shape[1:],\n                             rand_crop=False,\n                             rand_mirror=False,\n                             shuffle=False,\n                             **mean_std)\ndata.batch_size = batch_size\n```\n\n3. Prepare the evaluation function:\n\n```python\neval_samples = batch_size*10\n\ndef eval_func(model):\n    data.reset()\n    metric = mx.metric.Accuracy()\n    for i, batch in enumerate(data):\n        if i * batch_size >= eval_samples:\n            break\n        x = batch.data[0].as_in_context(mx.cpu())\n        label = batch.label[0].as_in_context(mx.cpu())\n        outputs = model.forward(x)\n        metric.update(label, outputs)\n    return metric.get()[1]\n```\n\n4. Run Intel\u00ae Neural Compressor:\n\n```python\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./cnn.yaml\")\nquantizer.model = resnet18\nquantizer.calib_dataloader = data\nquantizer.eval_func = eval_func\nqnet = quantizer.fit().model\n```\n\nSince this model already achieves good accuracy using native quantization (less than 1% accuracy drop), for the given configuration file, INC will end on the first configuration, quantizing all layers using `naive` calibration mode for each. To see the true potential of INC, we need a model which suffers from a larger accuracy drop after quantization.\n\n### Quantizing ResNet50v2\n\nThis example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows automatically find better solution.\n\nThis is the (TODO link to INC configuration file) for this example: \n```yaml\nversion: 1.0\n\nmodel:\n  name: resnet50_v2\n  framework: mxnet\n\nquantization:\n  calibration:\n    sampling_size: 192 # number of samples for calibration\n\ntuning:\n  strategy:\n    name: mse\n  accuracy_criterion:\n    relative: 0.015\n  exit_policy:\n    timeout: 0\n    max_trials: 500\n  random_seed: 9527\n```\n\nIt could be used with script below \n(TODO link to resnet_mse.py)\nto find operator which mostly influence accuracy drops and disable it from quantization. \nYou can find description of MSE strategy \n[here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md#user-content-mse).\n\n```python\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo.vision import resnet50_v2\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.contrib.quantization import quantize_net\n\n# Preparing input data\nrgb_mean = (0.485, 0.456, 0.406)\nrgb_std = (0.229, 0.224, 0.225)\nbatch_size = 64\nnum_calib_batches = 9\n# set below proper path to ImageNet data set\ndataset = mx.gluon.data.vision.ImageRecordDataset('../imagenet/rec/val.rec')\n# Tuning in INC on whole data set takes too long time so we take only part of the whole data set\n# as representative part of it:\ndataset = dataset.take(num_calib_batches * batch_size)\ntransformer = transforms.Compose([transforms.Resize(256),\n                                  transforms.CenterCrop(224),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n# Note: as input data are used many times during tuning it is better to prepared data earlier,\n#       so lazy parameter for transform_first is set to False\nval_data = mx.gluon.data.DataLoader(\n    dataset.transform_first(transformer, lazy=False), batch_size, shuffle=False)\nval_data.batch_size = batch_size\n\nnet = resnet50_v2(pretrained=True)\n\ndef eval_func(model):\n  metric = mx.gluon.metric.Accuracy()\n  for x, label in val_data:\n    output = model(x)\n    metric.update(label, output)\n  accuracy = metric.get()[1]\n  return accuracy\n\n\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"resnet50v2_mse.yaml\")\nquantizer.model = net\nquantizer.calib_dataloader = val_data\nquantizer.eval_func = eval_func\nqnet_inc = quantizer.fit().model\nprint(\"INC finished\")\n# You can save optimized model for the later use:\nqnet_inc.export(\"__quantized_with_inc\")\n# You can see what configurations was applied aby INC and which nodes was excluded from quantization\n# to achieve given accuracy lost against floating point calculation",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "954874866",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21127,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization_inc.md",
        "discussion_id": "954874866",
        "commented_code": "@@ -0,0 +1,290 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+\n+# Improving accuracy with Intel\u00ae Neural Compressor\n+\n+The accuracy of a model can decrease as a result of quantization. When the accuracy drop is significant, we can try to manually find a better quantization configuration (exclude some layers, try different calibration methods, etc.), but for bigger models this might prove to be a difficult and time consuming task. [Intel\u00ae Neural Compressor](https://github.com/intel/neural-compressor) (INC) tries to automate this process using several tuning heuristics, which aim to find the quantization configuration that satisfies the specified accuracy requirement.\n+\n+**NOTE:**\n+\n+Most tuning strategies will try different configurations on an evaluation dataset in order to find out how each layer affects the accuracy of the model. This means that for larger models, it may take a long time to find a solution (as the tuning space is usually larger and the evaluation itself takes longer).\n+\n+## Installation and Prerequisites\n+\n+- Install MXNet with oneDNN enabled as described in the [Get started](https://mxnet.apache.org/versions/master/get_started?platform=linux&language=python&processor=cpu&environ=pip&). (Until the 2.0 release you can use the nightly build version: `pip install --pre mxnet -f https://dist.mxnet.io/python`)\n+\n+- Install Intel\u00ae Neural Compressor:\n+\n+  Use one of the commands below to install INC (supported python versions are: 3.6, 3.7, 3.8, 3.9):\n+\n+  ```bash\n+  # install stable version from pip\n+  pip install neural-compressor\n+\n+  # install nightly version from pip\n+  pip install -i https://test.pypi.org/simple/ neural-compressor\n+\n+  # install stable version from conda\n+  conda install neural-compressor -c conda-forge -c intel\n+  ```\n+  If you come into trouble with dependencies on `cv2` library you can run: `apt-get update && apt-get install -y python3-opencv`\n+\n+## Configuration file\n+\n+Quantization tuning process can be customized in the yaml configuration file. Below is a simple example:\n+\n+```yaml\n+# cnn.yaml\n+\n+version: 1.0\n+\n+model:\n+  name: cnn\n+  framework: mxnet\n+\n+quantization:\n+  calibration:\n+    sampling_size: 160 # number of samples for calibration\n+\n+tuning:\n+  strategy:\n+    name: basic\n+  accuracy_criterion:\n+    relative: 0.01\n+  exit_policy:\n+    timeout: 0\n+  random_seed: 9527\n+```\n+\n+We are using the `basic` strategy, but you could also try out different ones. [Here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md) you can find a list of strategies available in INC and details of how they work. You can also add your own strategy if the existing ones do not suit your needs.\n+\n+Since the value of `timeout` is 0, INC will run until it finds a configuration that satisfies the accuracy criterion and then exit. Depending on the strategy this may not be ideal, as sometimes it would be better to further explore the tuning space to find a superior configuration both in terms of accuracy and speed. To achieve this, we can set a specific `timeout` value, which will tell INC how long (in seconds) it should run.\n+\n+For more information about the configuration file, see the [template](https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml) from the official INC repo. Keep in mind that only the `post training quantization` is currently supported for MXNet.\n+\n+## Model quantization and tuning\n+\n+In general, Intel\u00ae Neural Compressor requires 4 elements in order to run:  \n+1. Config file - like the example above  \n+2. Model to be quantized  \n+3. Calibration dataloader  \n+4. Evaluation function - a function that takes a model as an argument and returns the accuracy it achieves on a certain evaluation dataset. \n+\n+### Quantizing ResNet\n+\n+The [quantization](https://mxnet.apache.org/versions/master/api/python/docs/tutorials/performance/backend/dnnl/dnnl_quantization.html#Quantization) sections described how to quantize ResNet using the native MXNet quantization. This example shows how we can achieve the similar results (with the auto-tuning) using INC.\n+\n+1. Get the model\n+\n+```python\n+import logging\n+import mxnet as mx\n+from mxnet.gluon.model_zoo import vision\n+\n+logging.basicConfig()\n+logger = logging.getLogger('logger')\n+logger.setLevel(logging.INFO)\n+\n+batch_shape = (1, 3, 224, 224)\n+resnet18 = vision.resnet18_v1(pretrained=True)\n+```\n+\n+2. Prepare the dataset:\n+\n+```python\n+mx.test_utils.download('http://data.mxnet.io/data/val_256_q90.rec', 'data/val_256_q90.rec')\n+\n+batch_size = 16\n+mean_std = {'mean_r': 123.68, 'mean_g': 116.779, 'mean_b': 103.939,\n+            'std_r': 58.393, 'std_g': 57.12, 'std_b': 57.375}\n+\n+data = mx.io.ImageRecordIter(path_imgrec='data/val_256_q90.rec',\n+                             batch_size=batch_size,\n+                             data_shape=batch_shape[1:],\n+                             rand_crop=False,\n+                             rand_mirror=False,\n+                             shuffle=False,\n+                             **mean_std)\n+data.batch_size = batch_size\n+```\n+\n+3. Prepare the evaluation function:\n+\n+```python\n+eval_samples = batch_size*10\n+\n+def eval_func(model):\n+    data.reset()\n+    metric = mx.metric.Accuracy()\n+    for i, batch in enumerate(data):\n+        if i * batch_size >= eval_samples:\n+            break\n+        x = batch.data[0].as_in_context(mx.cpu())\n+        label = batch.label[0].as_in_context(mx.cpu())\n+        outputs = model.forward(x)\n+        metric.update(label, outputs)\n+    return metric.get()[1]\n+```\n+\n+4. Run Intel\u00ae Neural Compressor:\n+\n+```python\n+from neural_compressor.experimental import Quantization\n+quantizer = Quantization(\"./cnn.yaml\")\n+quantizer.model = resnet18\n+quantizer.calib_dataloader = data\n+quantizer.eval_func = eval_func\n+qnet = quantizer.fit().model\n+```\n+\n+Since this model already achieves good accuracy using native quantization (less than 1% accuracy drop), for the given configuration file, INC will end on the first configuration, quantizing all layers using `naive` calibration mode for each. To see the true potential of INC, we need a model which suffers from a larger accuracy drop after quantization.\n+\n+### Quantizing ResNet50v2\n+\n+This example shows how to use INC to quantize ResNet50 v2. In this case, the native MXNet quantization introduce a huge accuracy drop (70% using `naive` calibration mode) and INC allows automatically find better solution.\n+\n+This is the (TODO link to INC configuration file) for this example: \n+```yaml\n+version: 1.0\n+\n+model:\n+  name: resnet50_v2\n+  framework: mxnet\n+\n+quantization:\n+  calibration:\n+    sampling_size: 192 # number of samples for calibration\n+\n+tuning:\n+  strategy:\n+    name: mse\n+  accuracy_criterion:\n+    relative: 0.015\n+  exit_policy:\n+    timeout: 0\n+    max_trials: 500\n+  random_seed: 9527\n+```\n+\n+It could be used with script below \n+(TODO link to resnet_mse.py)\n+to find operator which mostly influence accuracy drops and disable it from quantization. \n+You can find description of MSE strategy \n+[here](https://github.com/intel/neural-compressor/blob/master/docs/tuning_strategies.md#user-content-mse).\n+\n+```python\n+import mxnet as mx\n+from mxnet.gluon.model_zoo.vision import resnet50_v2\n+from mxnet.gluon.data.vision import transforms\n+from mxnet.contrib.quantization import quantize_net\n+\n+# Preparing input data\n+rgb_mean = (0.485, 0.456, 0.406)\n+rgb_std = (0.229, 0.224, 0.225)\n+batch_size = 64\n+num_calib_batches = 9\n+# set below proper path to ImageNet data set\n+dataset = mx.gluon.data.vision.ImageRecordDataset('../imagenet/rec/val.rec')\n+# Tuning in INC on whole data set takes too long time so we take only part of the whole data set\n+# as representative part of it:\n+dataset = dataset.take(num_calib_batches * batch_size)\n+transformer = transforms.Compose([transforms.Resize(256),\n+                                  transforms.CenterCrop(224),\n+                                  transforms.ToTensor(),\n+                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n+# Note: as input data are used many times during tuning it is better to prepared data earlier,\n+#       so lazy parameter for transform_first is set to False\n+val_data = mx.gluon.data.DataLoader(\n+    dataset.transform_first(transformer, lazy=False), batch_size, shuffle=False)\n+val_data.batch_size = batch_size\n+\n+net = resnet50_v2(pretrained=True)\n+\n+def eval_func(model):\n+  metric = mx.gluon.metric.Accuracy()\n+  for x, label in val_data:\n+    output = model(x)\n+    metric.update(label, output)\n+  accuracy = metric.get()[1]\n+  return accuracy\n+\n+\n+from neural_compressor.experimental import Quantization\n+quantizer = Quantization(\"resnet50v2_mse.yaml\")\n+quantizer.model = net\n+quantizer.calib_dataloader = val_data\n+quantizer.eval_func = eval_func\n+qnet_inc = quantizer.fit().model\n+print(\"INC finished\")\n+# You can save optimized model for the later use:\n+qnet_inc.export(\"__quantized_with_inc\")\n+# You can see what configurations was applied aby INC and which nodes was excluded from quantization\n+# to achieve given accuracy lost against floating point calculation",
        "comment_created_at": "2022-08-25T11:57:43+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\n# You can see which configuration was applied by INC and which nodes were excluded from quantization,\r\n# to achieve given accuracy loss against floating point calculation.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "796581572",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-01T13:12:13+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining sequence of operations which can be performed one after another immediately (example: ReLU activation)\n- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization\n\nIn version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n\n## Operator Fusion\n\nModels are often represented as directed graph of operations (represented by nodes) and data flow (representad as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n\n\nThe simplest way to explain what fusion is and how it works is to present an example. On the image above is shown a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar block called residual blocks. Some possible fusion patterns are:\n\n- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n- Conv2D + Add => even simpler idea than the previous one - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`\n\nAbove examples are presented as atomic ones, but often they can be combined together, thus two patterns can be fused in above example:\n- Conv2D + BatchNorm + ReLU\n- Conv2D + BatchNorm + Add + ReLU\n\nAfter fusing all patterns, computational graph will be changed to the following one:\n![fused_fp32_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/fused_f32.png?raw=true)\n\n\n\n### Operator fusion in MXNet\nSince the version 1.6 of MXNet built with oneDNN support, operator fusion had been enabled by default if executing model with Module API, however in version 2.0 it has been decided to remove setting this feature by environment flag and replace it by aware user API call.\n\nTo fuse model in MXNet 2.0 there are two requirements:\n- the model must be defined as a subclass of HybridBlock or Symbol,\n- the model must have specific operator patterns which can be fused.\n\nAs an example we define example network (sample block from ResNet architecture):\n\n```\nimport mxnet as mx\nfrom mxnet.gluon import nn\n\nclass SampleBlock(nn.HybridBlock):\n    def __init__(self):\n        super(SampleBlock, self).__init__()\n        self.conv1 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n                               use_bias=False, in_channels=64)\n        self.bn1 = nn.BatchNorm()\n        self.conv2 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n                               use_bias=False, in_channels=64)\n        self.bn2 = nn.BatchNorm()\n\n    def forward(self, x):\n        out = mx.npx.activation(self.bn1(self.conv1(x)), 'relu')\n        out = self.bn2(self.conv2(out))\n        out = mx.npx.activation(out + x, 'relu')\n        return out\n        \nnet = SampleBlock()\nnet.initialize()\n\ndata = mx.np.zeros(shape=(1,64,224,224))\n# run fusion\nnet.optimize_for(data, backend='ONEDNN')\n\n# We can check fusion by plotting current symbol of our optimized network\nsym, _ = net.export(None)\ngraph = mx.viz.plot_network(sym, save_format='jpg')\ngraph.view()\n```\nBoth HybridBlock and Symbol classes provide API to easily run fusion of operators. Single line of code is enabling fusion passes on model:\n```\nnet.optimize_for(data, backend='ONEDNN')\n```\n\n*optimize_for* function is available also as Symbol class method. Example call to this API is shown below. Notice that Symbol\u2019s *optimize_for* method is not done in-place, so assigning it to a new variable is required:\n\n```\noptimized_symbol = sym.optimize_for(backend='ONEDNN')\n```\n\nFor the above model definition in a naive benchmark with artificial data, we can gain up to 1.75x speedup without any accuracy loss on our testing machine with Intel(R) Core(TM) i9-9940X.\n\n\n## Quantization\nAs mentioned in the introduction, precision reduction is another very popular method of improving performance of workloads and, what is important, in most cases is combined together with operator fusion which improves performance even more. In training precision reduction utilizes 16 bit data types like bfloat or float16, but for inference great results can be achieved using int8. \n\nModel quantization helps on both memory-bound and compute-bound operations. In quantized model IO operations are reduced as int8 data type is 4x smaller than float32, and also computational throughput is increased as more data can be SIMD'ed. On modern Intel architectures using int8 data type can bring even more speedup by utilizing special VNNI instruction set. \n\n![before_quant](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/before_quant.png?raw=true)\n\nFirstly quantization performs operator fusion on floating-point model as mentioned in paragraph earlier. Next, all operators which support int8 data type are marked as quantized and if needed additional operators are injected into graph surrounding quantizable operator - the goal of this additional operators is to quantize, dequantize or requantize data to keep data type between operators compatible.\n \n![quant_not_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_not_calib.png?raw=true)\n\n\nAfter injection step it is important to perform calibration of the model, however this step is optional. Quantizing without calibration is not recommended in terms of performance. It will result in calculating data minimum and maximum in quantize and requantize nodes during each inference pass. Calibrating a model greatly improves performance as minimum and maximum values are collected offline and are saved inside node - this way there is no need to search for these values during inference pass. \n\n![quant_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib.png?raw=true)\n\n\nCurrently, there are three supported calibration methods:\n- naive \u2014 min/max values from the calibration run,\n- entropy \u2014 uses KL divergence to determine the best symmetrical quantization thresholds for a given histogram of values.\n- custom \u2014 uses user-defined CalibrationCollector to control the calibration process.\n\nLast operation in quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "796581572",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "796581572",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining sequence of operations which can be performed one after another immediately (example: ReLU activation)\n+- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization\n+\n+In version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n+\n+## Operator Fusion\n+\n+Models are often represented as directed graph of operations (represented by nodes) and data flow (representad as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n+![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n+\n+\n+The simplest way to explain what fusion is and how it works is to present an example. On the image above is shown a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar block called residual blocks. Some possible fusion patterns are:\n+\n+- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n+- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n+- Conv2D + Add => even simpler idea than the previous one - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`\n+\n+Above examples are presented as atomic ones, but often they can be combined together, thus two patterns can be fused in above example:\n+- Conv2D + BatchNorm + ReLU\n+- Conv2D + BatchNorm + Add + ReLU\n+\n+After fusing all patterns, computational graph will be changed to the following one:\n+![fused_fp32_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/fused_f32.png?raw=true)\n+\n+\n+\n+### Operator fusion in MXNet\n+Since the version 1.6 of MXNet built with oneDNN support, operator fusion had been enabled by default if executing model with Module API, however in version 2.0 it has been decided to remove setting this feature by environment flag and replace it by aware user API call.\n+\n+To fuse model in MXNet 2.0 there are two requirements:\n+- the model must be defined as a subclass of HybridBlock or Symbol,\n+- the model must have specific operator patterns which can be fused.\n+\n+As an example we define example network (sample block from ResNet architecture):\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon import nn\n+\n+class SampleBlock(nn.HybridBlock):\n+    def __init__(self):\n+        super(SampleBlock, self).__init__()\n+        self.conv1 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n+                               use_bias=False, in_channels=64)\n+        self.bn1 = nn.BatchNorm()\n+        self.conv2 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n+                               use_bias=False, in_channels=64)\n+        self.bn2 = nn.BatchNorm()\n+\n+    def forward(self, x):\n+        out = mx.npx.activation(self.bn1(self.conv1(x)), 'relu')\n+        out = self.bn2(self.conv2(out))\n+        out = mx.npx.activation(out + x, 'relu')\n+        return out\n+        \n+net = SampleBlock()\n+net.initialize()\n+\n+data = mx.np.zeros(shape=(1,64,224,224))\n+# run fusion\n+net.optimize_for(data, backend='ONEDNN')\n+\n+# We can check fusion by plotting current symbol of our optimized network\n+sym, _ = net.export(None)\n+graph = mx.viz.plot_network(sym, save_format='jpg')\n+graph.view()\n+```\n+Both HybridBlock and Symbol classes provide API to easily run fusion of operators. Single line of code is enabling fusion passes on model:\n+```\n+net.optimize_for(data, backend='ONEDNN')\n+```\n+\n+*optimize_for* function is available also as Symbol class method. Example call to this API is shown below. Notice that Symbol\u2019s *optimize_for* method is not done in-place, so assigning it to a new variable is required:\n+\n+```\n+optimized_symbol = sym.optimize_for(backend='ONEDNN')\n+```\n+\n+For the above model definition in a naive benchmark with artificial data, we can gain up to 1.75x speedup without any accuracy loss on our testing machine with Intel(R) Core(TM) i9-9940X.\n+\n+\n+## Quantization\n+As mentioned in the introduction, precision reduction is another very popular method of improving performance of workloads and, what is important, in most cases is combined together with operator fusion which improves performance even more. In training precision reduction utilizes 16 bit data types like bfloat or float16, but for inference great results can be achieved using int8. \n+\n+Model quantization helps on both memory-bound and compute-bound operations. In quantized model IO operations are reduced as int8 data type is 4x smaller than float32, and also computational throughput is increased as more data can be SIMD'ed. On modern Intel architectures using int8 data type can bring even more speedup by utilizing special VNNI instruction set. \n+\n+![before_quant](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/before_quant.png?raw=true)\n+\n+Firstly quantization performs operator fusion on floating-point model as mentioned in paragraph earlier. Next, all operators which support int8 data type are marked as quantized and if needed additional operators are injected into graph surrounding quantizable operator - the goal of this additional operators is to quantize, dequantize or requantize data to keep data type between operators compatible.\n+ \n+![quant_not_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_not_calib.png?raw=true)\n+\n+\n+After injection step it is important to perform calibration of the model, however this step is optional. Quantizing without calibration is not recommended in terms of performance. It will result in calculating data minimum and maximum in quantize and requantize nodes during each inference pass. Calibrating a model greatly improves performance as minimum and maximum values are collected offline and are saved inside node - this way there is no need to search for these values during inference pass. \n+\n+![quant_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib.png?raw=true)\n+\n+\n+Currently, there are three supported calibration methods:\n+- naive \u2014 min/max values from the calibration run,\n+- entropy \u2014 uses KL divergence to determine the best symmetrical quantization thresholds for a given histogram of values.\n+- custom \u2014 uses user-defined CalibrationCollector to control the calibration process.\n+\n+Last operation in quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.",
        "comment_created_at": "2022-02-01T13:12:13+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\nLast stage of quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "800441603",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-07T09:05:28+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)\n- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "800441603",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "800441603",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)\n+- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization",
        "comment_created_at": "2022-02-07T09:05:28+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\n- compute-bound optimizations - these optimizations are mainly made on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution. One of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "800492034",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-07T10:03:03+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)\n- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization\n\nIn version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n\n## Operator Fusion\n\nModels are often represented as a directed graph of operations (represented by nodes) and data flow (represented as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n\n\nThe simplest way to explain what fusion is and how it works is to present an example. Image above depicts a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar blocks called residual blocks. Some possible fusion patterns are:\n\n- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.\n\nAbove examples are presented as atomic ones, but often they can be combined together, thus two patterns that can be fused in above example are:\n- Conv2D + BatchNorm + ReLU\n- Conv2D + BatchNorm + Add + ReLU\n\nAfter fusing all patterns, computational graph will be changed to the following one:\n![fused_fp32_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/fused_f32.png?raw=true)\n\n\n\n### Operator fusion in MXNet\nSince the version 1.6 of MXNet built with oneDNN support, operator fusion had been enabled by default for executing model with Module API, however in version 2.0 it has been decided to remove setting this feature by environment flag and replace it by aware user API call.\n\nTo fuse model in MXNet 2.0 there are two requirements:\n- the model must be defined as a subclass of HybridBlock or Symbol,\n- the model must have specific operator patterns which can be fused.\n\nAs an example we define example network (sample block from ResNet architecture):\n\n```\nimport mxnet as mx\nfrom mxnet.gluon import nn\n\nclass SampleBlock(nn.HybridBlock):\n    def __init__(self):\n        super(SampleBlock, self).__init__()\n        self.conv1 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n                               use_bias=False, in_channels=64)\n        self.bn1 = nn.BatchNorm()\n        self.conv2 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n                               use_bias=False, in_channels=64)\n        self.bn2 = nn.BatchNorm()\n\n    def forward(self, x):\n        out = mx.npx.activation(self.bn1(self.conv1(x)), 'relu')\n        out = self.bn2(self.conv2(out))\n        out = mx.npx.activation(out + x, 'relu')\n        return out\n        \nnet = SampleBlock()\nnet.initialize()\n\ndata = mx.np.zeros(shape=(1,64,224,224))\n# run fusion\nnet.optimize_for(data, backend='ONEDNN')\n\n# We can check fusion by plotting current symbol of our optimized network\nsym, _ = net.export(None)\ngraph = mx.viz.plot_network(sym, save_format='jpg')\ngraph.view()\n```\nBoth HybridBlock and Symbol classes provide API to easily run fusion of operators. Single line of code is enabling fusion passes on model:\n```\nnet.optimize_for(data, backend='ONEDNN')\n```\n\n*optimize_for* function is available also as Symbol class method. Example call to this API is shown below. Notice that Symbol\u2019s *optimize_for* method is not done in-place, so assigning it to a new variable is required:\n\n```\noptimized_symbol = sym.optimize_for(backend='ONEDNN')\n```\n\nFor the above model definition in a naive benchmark with artificial data, we can gain up to 1.75x speedup without any accuracy loss on our testing machine with Intel(R) Core(TM) i9-9940X.\n\n\n## Quantization\nAs mentioned in the introduction, precision reduction is another very popular method of improving performance of workloads and, what is important, in most cases is combined together with operator fusion which improves performance even more. In training precision reduction utilizes 16 bit data types like bfloat or float16, but for inference great results can be achieved using int8. \n\nModel quantization helps on both memory-bound and compute-bound operations. In quantized model IO operations are reduced as int8 data type is 4x smaller than float32, and also computational throughput is increased as more data can be SIMD'ed. On modern Intel architectures using int8 data type can bring even more speedup by utilizing special VNNI instruction set. \n\n![before_quant](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/before_quant.png?raw=true)\n\nFirstly quantization performs operator fusion on floating-point model as mentioned in paragraph earlier. Next, all operators which support int8 data type are marked as quantized and if needed additional operators are injected into graph surrounding quantizable operator - the goal of this additional operators is to quantize, dequantize or requantize data to keep data type between operators compatible.\n \n![quant_not_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_not_calib.png?raw=true)\n\n\nAfter injection step it is important to perform calibration of the model, however this step is optional. Quantizing without calibration is not recommended in terms of performance. It will result in calculating data minimum and maximum in quantize and requantize nodes during each inference pass. Calibrating a model greatly improves performance as minimum and maximum values are collected offline and are saved inside node - this way there is no need to search for these values during inference pass. \n\n![quant_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib.png?raw=true)\n\n\nCurrently, there are three supported calibration methods:\n- naive \u2014 min/max values from the calibration run,\n- entropy \u2014 uses KL divergence to determine the best symmetrical quantization thresholds for a given histogram of values,\n- custom \u2014 uses user-defined CalibrationCollector to control the calibration process.\n\nLast stage of quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.\n\n![quant_calib_fused](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib_fused.png?raw=true)\n\nIn MXNet 2.0, the quantization procedure has been adjusted to work well with Gluon models since it is the main API now. The goal was to allow the user to quantize fp32 HybridBlock model in just a few lines of code.\n\n### Quantization in MXNet\n\nAs an example of a quantization procedure, pretrained *resnet50_v1* from *model_zoo.vision* package can be used. To get it simply run the following code:\n\n```\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo.vision import resnet50_v1\n\nnet = resnet50_v1(pretrained=True)\n```\n\nNow, to get a ready-to-deploy quantized model two steps are required:\n1. Prepare data loader with calibration data - this data will be used as input to the network. All necessary layers will be observed with layer collector to calculate minimum and maximum value of that layer. This flow is internal mechanism and all what user needs to do is to provide data loader.\n2. Call `quantize_net` function from `contrib.quantize` package - both operator fusion calls will be called inside this API.\n\n```\ncalib_data_loader = mx.gluon.data.DataLoader(dummy_data, batch_size=batch_size)\nqnet = quantize_net(net, calib_mode='naive', calib_data=calib_data_loader)\n```\n\nFollowing function, which calculates total inference time on the model with an artificial data, can be used to compare the performance:\n\n```\ndef benchmark_net(net, batch_size=32, batches=100, warmup_batches=5):\n  import time\n  data = mx.np.random.uniform(-1.0, 1.0, (batch_size, 3, 224, 224))\n  \n  for i in range(batches + warmup_batches):\n    if i == warmup_batches:\n      tic = time.time()\n    out = net(data)\n    out.wait_to_read()\n    \n  total_time = time.time()\u200a-\u200atic\n  return total_time\n```\n\n\nComparing fused float32 network to quantized network on CLX8280 shows 4.29x speedup - measurment was done on 28 cores and this machine utilizes VNNI instruction set.\n\n\nThe other aspect of lowering the precision of a model is a difference in its accuracy. We will check that on previously tested resnet50_v1 with ImageNet dataset. To run this example you will need ImageNet dataset prepared with this tutorial and stored in path_to_imagenet. Let\u2019s compare top1 and top5 accuracy of standard fp32 model with quantized int8 model calibrated using naive and entropy calibration mode. We will use only 10 batches of the validation dataset to calibrate quantized model.\n\n```\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo.vision import resnet50_v1\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.contrib.quantization import quantize_net\n\ndef test_accuracy(net, data_loader):\n  acc_top1 = mx.gluon.metric.Accuracy()\n  acc_top5 = mx.gluon.metric.TopKAccuracy(5)\n  \n  for x, label in data_loader:\n    output = net(x)\n    acc_top1.update(label, output)\n    acc_top5.update(label, output)\n\n  _, top1 = acc_top1.get()\n  _, top5 = acc_top5.get()\n\n  return top1, top5\n  \nrgb_mean = (0.485, 0.456, 0.406)\nrgb_std = (0.229, 0.224, 0.225)\nbatch_size = 64\n \ndataset = mx.gluon.data.vision.ImageRecordDataset('path_to_imagenet/val.rec')\ntransformer = transforms.Compose([transforms.Resize(256),\n                                  transforms.CenterCrop(224),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\nval_data = mx.gluon.data.DataLoader(dataset.transform_first(transformer), batch_size, shuffle=True)\n\nnet = resnet50_v1(pretrained=True)\nnet.hybridize(backend='ONEDNN', static_alloc=True, static_shape=True)\n\ntop1, top5 = test_accuracy(net, val_data)\nprint('FP32 Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n\nqnet = quantize_net(net, calib_mode='naive', calib_data=val_data, num_calib_batches=10)\nqnet.hybridize(static_alloc=True, static_shape=True)\ntop1, top5 = test_accuracy(qnet, val_data)\nprint('INT8Naive Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n\nqnet = quantize_net(net, calib_mode='entropy', calib_data=val_data, num_calib_batches=10)\nqnet.hybridize(static_alloc=True, static_shape=True)\ntop1, top5 = test_accuracy(qnet, val_data)\nprint('INT8Entropy Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n```\n\n#### Output:\n> FP32 Top1 Accuracy: 0.76364 Top5 Accuracy: 0.93094\n> INT8Naive Top1 Accuracy: 0.76028 Top5 Accuracy: 0.92796\n> INT8Entropy Top1 Accuracy: 0.76404 Top5 Accuracy: 0.93042\n\nWith quantized model there is tiny accuracy drop, however this is cost of great performance optimization and memory footprint. The difference between calibration method is dependent on the model itself, used activation layers and the size of calibration data.",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "800492034",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "800492034",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)\n+- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization\n+\n+In version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n+\n+## Operator Fusion\n+\n+Models are often represented as a directed graph of operations (represented by nodes) and data flow (represented as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n+![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n+\n+\n+The simplest way to explain what fusion is and how it works is to present an example. Image above depicts a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar blocks called residual blocks. Some possible fusion patterns are:\n+\n+- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n+- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n+- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.\n+\n+Above examples are presented as atomic ones, but often they can be combined together, thus two patterns that can be fused in above example are:\n+- Conv2D + BatchNorm + ReLU\n+- Conv2D + BatchNorm + Add + ReLU\n+\n+After fusing all patterns, computational graph will be changed to the following one:\n+![fused_fp32_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/fused_f32.png?raw=true)\n+\n+\n+\n+### Operator fusion in MXNet\n+Since the version 1.6 of MXNet built with oneDNN support, operator fusion had been enabled by default for executing model with Module API, however in version 2.0 it has been decided to remove setting this feature by environment flag and replace it by aware user API call.\n+\n+To fuse model in MXNet 2.0 there are two requirements:\n+- the model must be defined as a subclass of HybridBlock or Symbol,\n+- the model must have specific operator patterns which can be fused.\n+\n+As an example we define example network (sample block from ResNet architecture):\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon import nn\n+\n+class SampleBlock(nn.HybridBlock):\n+    def __init__(self):\n+        super(SampleBlock, self).__init__()\n+        self.conv1 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n+                               use_bias=False, in_channels=64)\n+        self.bn1 = nn.BatchNorm()\n+        self.conv2 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n+                               use_bias=False, in_channels=64)\n+        self.bn2 = nn.BatchNorm()\n+\n+    def forward(self, x):\n+        out = mx.npx.activation(self.bn1(self.conv1(x)), 'relu')\n+        out = self.bn2(self.conv2(out))\n+        out = mx.npx.activation(out + x, 'relu')\n+        return out\n+        \n+net = SampleBlock()\n+net.initialize()\n+\n+data = mx.np.zeros(shape=(1,64,224,224))\n+# run fusion\n+net.optimize_for(data, backend='ONEDNN')\n+\n+# We can check fusion by plotting current symbol of our optimized network\n+sym, _ = net.export(None)\n+graph = mx.viz.plot_network(sym, save_format='jpg')\n+graph.view()\n+```\n+Both HybridBlock and Symbol classes provide API to easily run fusion of operators. Single line of code is enabling fusion passes on model:\n+```\n+net.optimize_for(data, backend='ONEDNN')\n+```\n+\n+*optimize_for* function is available also as Symbol class method. Example call to this API is shown below. Notice that Symbol\u2019s *optimize_for* method is not done in-place, so assigning it to a new variable is required:\n+\n+```\n+optimized_symbol = sym.optimize_for(backend='ONEDNN')\n+```\n+\n+For the above model definition in a naive benchmark with artificial data, we can gain up to 1.75x speedup without any accuracy loss on our testing machine with Intel(R) Core(TM) i9-9940X.\n+\n+\n+## Quantization\n+As mentioned in the introduction, precision reduction is another very popular method of improving performance of workloads and, what is important, in most cases is combined together with operator fusion which improves performance even more. In training precision reduction utilizes 16 bit data types like bfloat or float16, but for inference great results can be achieved using int8. \n+\n+Model quantization helps on both memory-bound and compute-bound operations. In quantized model IO operations are reduced as int8 data type is 4x smaller than float32, and also computational throughput is increased as more data can be SIMD'ed. On modern Intel architectures using int8 data type can bring even more speedup by utilizing special VNNI instruction set. \n+\n+![before_quant](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/before_quant.png?raw=true)\n+\n+Firstly quantization performs operator fusion on floating-point model as mentioned in paragraph earlier. Next, all operators which support int8 data type are marked as quantized and if needed additional operators are injected into graph surrounding quantizable operator - the goal of this additional operators is to quantize, dequantize or requantize data to keep data type between operators compatible.\n+ \n+![quant_not_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_not_calib.png?raw=true)\n+\n+\n+After injection step it is important to perform calibration of the model, however this step is optional. Quantizing without calibration is not recommended in terms of performance. It will result in calculating data minimum and maximum in quantize and requantize nodes during each inference pass. Calibrating a model greatly improves performance as minimum and maximum values are collected offline and are saved inside node - this way there is no need to search for these values during inference pass. \n+\n+![quant_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib.png?raw=true)\n+\n+\n+Currently, there are three supported calibration methods:\n+- naive \u2014 min/max values from the calibration run,\n+- entropy \u2014 uses KL divergence to determine the best symmetrical quantization thresholds for a given histogram of values,\n+- custom \u2014 uses user-defined CalibrationCollector to control the calibration process.\n+\n+Last stage of quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.\n+\n+![quant_calib_fused](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib_fused.png?raw=true)\n+\n+In MXNet 2.0, the quantization procedure has been adjusted to work well with Gluon models since it is the main API now. The goal was to allow the user to quantize fp32 HybridBlock model in just a few lines of code.\n+\n+### Quantization in MXNet\n+\n+As an example of a quantization procedure, pretrained *resnet50_v1* from *model_zoo.vision* package can be used. To get it simply run the following code:\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon.model_zoo.vision import resnet50_v1\n+\n+net = resnet50_v1(pretrained=True)\n+```\n+\n+Now, to get a ready-to-deploy quantized model two steps are required:\n+1. Prepare data loader with calibration data - this data will be used as input to the network. All necessary layers will be observed with layer collector to calculate minimum and maximum value of that layer. This flow is internal mechanism and all what user needs to do is to provide data loader.\n+2. Call `quantize_net` function from `contrib.quantize` package - both operator fusion calls will be called inside this API.\n+\n+```\n+calib_data_loader = mx.gluon.data.DataLoader(dummy_data, batch_size=batch_size)\n+qnet = quantize_net(net, calib_mode='naive', calib_data=calib_data_loader)\n+```\n+\n+Following function, which calculates total inference time on the model with an artificial data, can be used to compare the performance:\n+\n+```\n+def benchmark_net(net, batch_size=32, batches=100, warmup_batches=5):\n+  import time\n+  data = mx.np.random.uniform(-1.0, 1.0, (batch_size, 3, 224, 224))\n+  \n+  for i in range(batches + warmup_batches):\n+    if i == warmup_batches:\n+      tic = time.time()\n+    out = net(data)\n+    out.wait_to_read()\n+    \n+  total_time = time.time()\u200a-\u200atic\n+  return total_time\n+```\n+\n+\n+Comparing fused float32 network to quantized network on CLX8280 shows 4.29x speedup - measurment was done on 28 cores and this machine utilizes VNNI instruction set.\n+\n+\n+The other aspect of lowering the precision of a model is a difference in its accuracy. We will check that on previously tested resnet50_v1 with ImageNet dataset. To run this example you will need ImageNet dataset prepared with this tutorial and stored in path_to_imagenet. Let\u2019s compare top1 and top5 accuracy of standard fp32 model with quantized int8 model calibrated using naive and entropy calibration mode. We will use only 10 batches of the validation dataset to calibrate quantized model.\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon.model_zoo.vision import resnet50_v1\n+from mxnet.gluon.data.vision import transforms\n+from mxnet.contrib.quantization import quantize_net\n+\n+def test_accuracy(net, data_loader):\n+  acc_top1 = mx.gluon.metric.Accuracy()\n+  acc_top5 = mx.gluon.metric.TopKAccuracy(5)\n+  \n+  for x, label in data_loader:\n+    output = net(x)\n+    acc_top1.update(label, output)\n+    acc_top5.update(label, output)\n+\n+  _, top1 = acc_top1.get()\n+  _, top5 = acc_top5.get()\n+\n+  return top1, top5\n+  \n+rgb_mean = (0.485, 0.456, 0.406)\n+rgb_std = (0.229, 0.224, 0.225)\n+batch_size = 64\n+ \n+dataset = mx.gluon.data.vision.ImageRecordDataset('path_to_imagenet/val.rec')\n+transformer = transforms.Compose([transforms.Resize(256),\n+                                  transforms.CenterCrop(224),\n+                                  transforms.ToTensor(),\n+                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n+val_data = mx.gluon.data.DataLoader(dataset.transform_first(transformer), batch_size, shuffle=True)\n+\n+net = resnet50_v1(pretrained=True)\n+net.hybridize(backend='ONEDNN', static_alloc=True, static_shape=True)\n+\n+top1, top5 = test_accuracy(net, val_data)\n+print('FP32 Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+\n+qnet = quantize_net(net, calib_mode='naive', calib_data=val_data, num_calib_batches=10)\n+qnet.hybridize(static_alloc=True, static_shape=True)\n+top1, top5 = test_accuracy(qnet, val_data)\n+print('INT8Naive Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+\n+qnet = quantize_net(net, calib_mode='entropy', calib_data=val_data, num_calib_batches=10)\n+qnet.hybridize(static_alloc=True, static_shape=True)\n+top1, top5 = test_accuracy(qnet, val_data)\n+print('INT8Entropy Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+```\n+\n+#### Output:\n+> FP32 Top1 Accuracy: 0.76364 Top5 Accuracy: 0.93094\n+> INT8Naive Top1 Accuracy: 0.76028 Top5 Accuracy: 0.92796\n+> INT8Entropy Top1 Accuracy: 0.76404 Top5 Accuracy: 0.93042\n+\n+With quantized model there is tiny accuracy drop, however this is cost of great performance optimization and memory footprint. The difference between calibration method is dependent on the model itself, used activation layers and the size of calibration data.",
        "comment_created_at": "2022-02-07T10:03:03+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\nWith quantized model there is tiny accuracy drop, however this is the cost of great performance optimization and memory footprint reduction. The difference between calibration methods is dependent on the model itself, used activation layers and the size of calibration data.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "800492895",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-07T10:04:07+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)\n- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization\n\nIn version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n\n## Operator Fusion\n\nModels are often represented as a directed graph of operations (represented by nodes) and data flow (represented as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n\n\nThe simplest way to explain what fusion is and how it works is to present an example. Image above depicts a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar blocks called residual blocks. Some possible fusion patterns are:\n\n- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.\n\nAbove examples are presented as atomic ones, but often they can be combined together, thus two patterns that can be fused in above example are:\n- Conv2D + BatchNorm + ReLU\n- Conv2D + BatchNorm + Add + ReLU\n\nAfter fusing all patterns, computational graph will be changed to the following one:\n![fused_fp32_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/fused_f32.png?raw=true)\n\n\n\n### Operator fusion in MXNet\nSince the version 1.6 of MXNet built with oneDNN support, operator fusion had been enabled by default for executing model with Module API, however in version 2.0 it has been decided to remove setting this feature by environment flag and replace it by aware user API call.\n\nTo fuse model in MXNet 2.0 there are two requirements:\n- the model must be defined as a subclass of HybridBlock or Symbol,\n- the model must have specific operator patterns which can be fused.\n\nAs an example we define example network (sample block from ResNet architecture):\n\n```\nimport mxnet as mx\nfrom mxnet.gluon import nn\n\nclass SampleBlock(nn.HybridBlock):\n    def __init__(self):\n        super(SampleBlock, self).__init__()\n        self.conv1 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n                               use_bias=False, in_channels=64)\n        self.bn1 = nn.BatchNorm()\n        self.conv2 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n                               use_bias=False, in_channels=64)\n        self.bn2 = nn.BatchNorm()\n\n    def forward(self, x):\n        out = mx.npx.activation(self.bn1(self.conv1(x)), 'relu')\n        out = self.bn2(self.conv2(out))\n        out = mx.npx.activation(out + x, 'relu')\n        return out\n        \nnet = SampleBlock()\nnet.initialize()\n\ndata = mx.np.zeros(shape=(1,64,224,224))\n# run fusion\nnet.optimize_for(data, backend='ONEDNN')\n\n# We can check fusion by plotting current symbol of our optimized network\nsym, _ = net.export(None)\ngraph = mx.viz.plot_network(sym, save_format='jpg')\ngraph.view()\n```\nBoth HybridBlock and Symbol classes provide API to easily run fusion of operators. Single line of code is enabling fusion passes on model:\n```\nnet.optimize_for(data, backend='ONEDNN')\n```\n\n*optimize_for* function is available also as Symbol class method. Example call to this API is shown below. Notice that Symbol\u2019s *optimize_for* method is not done in-place, so assigning it to a new variable is required:\n\n```\noptimized_symbol = sym.optimize_for(backend='ONEDNN')\n```\n\nFor the above model definition in a naive benchmark with artificial data, we can gain up to 1.75x speedup without any accuracy loss on our testing machine with Intel(R) Core(TM) i9-9940X.\n\n\n## Quantization\nAs mentioned in the introduction, precision reduction is another very popular method of improving performance of workloads and, what is important, in most cases is combined together with operator fusion which improves performance even more. In training precision reduction utilizes 16 bit data types like bfloat or float16, but for inference great results can be achieved using int8. \n\nModel quantization helps on both memory-bound and compute-bound operations. In quantized model IO operations are reduced as int8 data type is 4x smaller than float32, and also computational throughput is increased as more data can be SIMD'ed. On modern Intel architectures using int8 data type can bring even more speedup by utilizing special VNNI instruction set. \n\n![before_quant](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/before_quant.png?raw=true)\n\nFirstly quantization performs operator fusion on floating-point model as mentioned in paragraph earlier. Next, all operators which support int8 data type are marked as quantized and if needed additional operators are injected into graph surrounding quantizable operator - the goal of this additional operators is to quantize, dequantize or requantize data to keep data type between operators compatible.\n \n![quant_not_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_not_calib.png?raw=true)\n\n\nAfter injection step it is important to perform calibration of the model, however this step is optional. Quantizing without calibration is not recommended in terms of performance. It will result in calculating data minimum and maximum in quantize and requantize nodes during each inference pass. Calibrating a model greatly improves performance as minimum and maximum values are collected offline and are saved inside node - this way there is no need to search for these values during inference pass. \n\n![quant_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib.png?raw=true)\n\n\nCurrently, there are three supported calibration methods:\n- naive \u2014 min/max values from the calibration run,\n- entropy \u2014 uses KL divergence to determine the best symmetrical quantization thresholds for a given histogram of values,\n- custom \u2014 uses user-defined CalibrationCollector to control the calibration process.\n\nLast stage of quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.\n\n![quant_calib_fused](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib_fused.png?raw=true)\n\nIn MXNet 2.0, the quantization procedure has been adjusted to work well with Gluon models since it is the main API now. The goal was to allow the user to quantize fp32 HybridBlock model in just a few lines of code.\n\n### Quantization in MXNet\n\nAs an example of a quantization procedure, pretrained *resnet50_v1* from *model_zoo.vision* package can be used. To get it simply run the following code:\n\n```\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo.vision import resnet50_v1\n\nnet = resnet50_v1(pretrained=True)\n```\n\nNow, to get a ready-to-deploy quantized model two steps are required:\n1. Prepare data loader with calibration data - this data will be used as input to the network. All necessary layers will be observed with layer collector to calculate minimum and maximum value of that layer. This flow is internal mechanism and all what user needs to do is to provide data loader.\n2. Call `quantize_net` function from `contrib.quantize` package - both operator fusion calls will be called inside this API.\n\n```\ncalib_data_loader = mx.gluon.data.DataLoader(dummy_data, batch_size=batch_size)\nqnet = quantize_net(net, calib_mode='naive', calib_data=calib_data_loader)\n```\n\nFollowing function, which calculates total inference time on the model with an artificial data, can be used to compare the performance:\n\n```\ndef benchmark_net(net, batch_size=32, batches=100, warmup_batches=5):\n  import time\n  data = mx.np.random.uniform(-1.0, 1.0, (batch_size, 3, 224, 224))\n  \n  for i in range(batches + warmup_batches):\n    if i == warmup_batches:\n      tic = time.time()\n    out = net(data)\n    out.wait_to_read()\n    \n  total_time = time.time()\u200a-\u200atic\n  return total_time\n```\n\n\nComparing fused float32 network to quantized network on CLX8280 shows 4.29x speedup - measurment was done on 28 cores and this machine utilizes VNNI instruction set.\n\n\nThe other aspect of lowering the precision of a model is a difference in its accuracy. We will check that on previously tested resnet50_v1 with ImageNet dataset. To run this example you will need ImageNet dataset prepared with this tutorial and stored in path_to_imagenet. Let\u2019s compare top1 and top5 accuracy of standard fp32 model with quantized int8 model calibrated using naive and entropy calibration mode. We will use only 10 batches of the validation dataset to calibrate quantized model.\n\n```\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo.vision import resnet50_v1\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.contrib.quantization import quantize_net\n\ndef test_accuracy(net, data_loader):\n  acc_top1 = mx.gluon.metric.Accuracy()\n  acc_top5 = mx.gluon.metric.TopKAccuracy(5)\n  \n  for x, label in data_loader:\n    output = net(x)\n    acc_top1.update(label, output)\n    acc_top5.update(label, output)\n\n  _, top1 = acc_top1.get()\n  _, top5 = acc_top5.get()\n\n  return top1, top5\n  \nrgb_mean = (0.485, 0.456, 0.406)\nrgb_std = (0.229, 0.224, 0.225)\nbatch_size = 64\n \ndataset = mx.gluon.data.vision.ImageRecordDataset('path_to_imagenet/val.rec')\ntransformer = transforms.Compose([transforms.Resize(256),\n                                  transforms.CenterCrop(224),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\nval_data = mx.gluon.data.DataLoader(dataset.transform_first(transformer), batch_size, shuffle=True)\n\nnet = resnet50_v1(pretrained=True)\nnet.hybridize(backend='ONEDNN', static_alloc=True, static_shape=True)\n\ntop1, top5 = test_accuracy(net, val_data)\nprint('FP32 Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n\nqnet = quantize_net(net, calib_mode='naive', calib_data=val_data, num_calib_batches=10)\nqnet.hybridize(static_alloc=True, static_shape=True)\ntop1, top5 = test_accuracy(qnet, val_data)\nprint('INT8Naive Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n\nqnet = quantize_net(net, calib_mode='entropy', calib_data=val_data, num_calib_batches=10)\nqnet.hybridize(static_alloc=True, static_shape=True)\ntop1, top5 = test_accuracy(qnet, val_data)\nprint('INT8Entropy Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n```\n\n#### Output:\n> FP32 Top1 Accuracy: 0.76364 Top5 Accuracy: 0.93094\n> INT8Naive Top1 Accuracy: 0.76028 Top5 Accuracy: 0.92796\n> INT8Entropy Top1 Accuracy: 0.76404 Top5 Accuracy: 0.93042\n\nWith quantized model there is tiny accuracy drop, however this is cost of great performance optimization and memory footprint. The difference between calibration method is dependent on the model itself, used activation layers and the size of calibration data.\n\n### Custom layer collectors and calibrating the model\nIn MXNet 2.0 new interface for creating custom calibration collector has been added. Main goal of this interface is to give the user as much flexibility as possible in almost every step of quantization. Creating own layer collector is pretty easy, however computing effective min/max values can be not a trivial task. \n\nLayer collectors are responsible for collecting statistics of each node in the graph \u2014 it means that the input/output data of every operator executed can be observed. Collector utilize the register_op_hook method of HybridBlock class.",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "800492895",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "800492895",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers at the same time. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution)\n+- compute-bound optimizations - these optimizations are mainly done on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution - one of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization\n+\n+In version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n+\n+## Operator Fusion\n+\n+Models are often represented as a directed graph of operations (represented by nodes) and data flow (represented as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n+![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n+\n+\n+The simplest way to explain what fusion is and how it works is to present an example. Image above depicts a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar blocks called residual blocks. Some possible fusion patterns are:\n+\n+- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n+- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n+- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.\n+\n+Above examples are presented as atomic ones, but often they can be combined together, thus two patterns that can be fused in above example are:\n+- Conv2D + BatchNorm + ReLU\n+- Conv2D + BatchNorm + Add + ReLU\n+\n+After fusing all patterns, computational graph will be changed to the following one:\n+![fused_fp32_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/fused_f32.png?raw=true)\n+\n+\n+\n+### Operator fusion in MXNet\n+Since the version 1.6 of MXNet built with oneDNN support, operator fusion had been enabled by default for executing model with Module API, however in version 2.0 it has been decided to remove setting this feature by environment flag and replace it by aware user API call.\n+\n+To fuse model in MXNet 2.0 there are two requirements:\n+- the model must be defined as a subclass of HybridBlock or Symbol,\n+- the model must have specific operator patterns which can be fused.\n+\n+As an example we define example network (sample block from ResNet architecture):\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon import nn\n+\n+class SampleBlock(nn.HybridBlock):\n+    def __init__(self):\n+        super(SampleBlock, self).__init__()\n+        self.conv1 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n+                               use_bias=False, in_channels=64)\n+        self.bn1 = nn.BatchNorm()\n+        self.conv2 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n+                               use_bias=False, in_channels=64)\n+        self.bn2 = nn.BatchNorm()\n+\n+    def forward(self, x):\n+        out = mx.npx.activation(self.bn1(self.conv1(x)), 'relu')\n+        out = self.bn2(self.conv2(out))\n+        out = mx.npx.activation(out + x, 'relu')\n+        return out\n+        \n+net = SampleBlock()\n+net.initialize()\n+\n+data = mx.np.zeros(shape=(1,64,224,224))\n+# run fusion\n+net.optimize_for(data, backend='ONEDNN')\n+\n+# We can check fusion by plotting current symbol of our optimized network\n+sym, _ = net.export(None)\n+graph = mx.viz.plot_network(sym, save_format='jpg')\n+graph.view()\n+```\n+Both HybridBlock and Symbol classes provide API to easily run fusion of operators. Single line of code is enabling fusion passes on model:\n+```\n+net.optimize_for(data, backend='ONEDNN')\n+```\n+\n+*optimize_for* function is available also as Symbol class method. Example call to this API is shown below. Notice that Symbol\u2019s *optimize_for* method is not done in-place, so assigning it to a new variable is required:\n+\n+```\n+optimized_symbol = sym.optimize_for(backend='ONEDNN')\n+```\n+\n+For the above model definition in a naive benchmark with artificial data, we can gain up to 1.75x speedup without any accuracy loss on our testing machine with Intel(R) Core(TM) i9-9940X.\n+\n+\n+## Quantization\n+As mentioned in the introduction, precision reduction is another very popular method of improving performance of workloads and, what is important, in most cases is combined together with operator fusion which improves performance even more. In training precision reduction utilizes 16 bit data types like bfloat or float16, but for inference great results can be achieved using int8. \n+\n+Model quantization helps on both memory-bound and compute-bound operations. In quantized model IO operations are reduced as int8 data type is 4x smaller than float32, and also computational throughput is increased as more data can be SIMD'ed. On modern Intel architectures using int8 data type can bring even more speedup by utilizing special VNNI instruction set. \n+\n+![before_quant](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/before_quant.png?raw=true)\n+\n+Firstly quantization performs operator fusion on floating-point model as mentioned in paragraph earlier. Next, all operators which support int8 data type are marked as quantized and if needed additional operators are injected into graph surrounding quantizable operator - the goal of this additional operators is to quantize, dequantize or requantize data to keep data type between operators compatible.\n+ \n+![quant_not_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_not_calib.png?raw=true)\n+\n+\n+After injection step it is important to perform calibration of the model, however this step is optional. Quantizing without calibration is not recommended in terms of performance. It will result in calculating data minimum and maximum in quantize and requantize nodes during each inference pass. Calibrating a model greatly improves performance as minimum and maximum values are collected offline and are saved inside node - this way there is no need to search for these values during inference pass. \n+\n+![quant_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib.png?raw=true)\n+\n+\n+Currently, there are three supported calibration methods:\n+- naive \u2014 min/max values from the calibration run,\n+- entropy \u2014 uses KL divergence to determine the best symmetrical quantization thresholds for a given histogram of values,\n+- custom \u2014 uses user-defined CalibrationCollector to control the calibration process.\n+\n+Last stage of quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.\n+\n+![quant_calib_fused](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib_fused.png?raw=true)\n+\n+In MXNet 2.0, the quantization procedure has been adjusted to work well with Gluon models since it is the main API now. The goal was to allow the user to quantize fp32 HybridBlock model in just a few lines of code.\n+\n+### Quantization in MXNet\n+\n+As an example of a quantization procedure, pretrained *resnet50_v1* from *model_zoo.vision* package can be used. To get it simply run the following code:\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon.model_zoo.vision import resnet50_v1\n+\n+net = resnet50_v1(pretrained=True)\n+```\n+\n+Now, to get a ready-to-deploy quantized model two steps are required:\n+1. Prepare data loader with calibration data - this data will be used as input to the network. All necessary layers will be observed with layer collector to calculate minimum and maximum value of that layer. This flow is internal mechanism and all what user needs to do is to provide data loader.\n+2. Call `quantize_net` function from `contrib.quantize` package - both operator fusion calls will be called inside this API.\n+\n+```\n+calib_data_loader = mx.gluon.data.DataLoader(dummy_data, batch_size=batch_size)\n+qnet = quantize_net(net, calib_mode='naive', calib_data=calib_data_loader)\n+```\n+\n+Following function, which calculates total inference time on the model with an artificial data, can be used to compare the performance:\n+\n+```\n+def benchmark_net(net, batch_size=32, batches=100, warmup_batches=5):\n+  import time\n+  data = mx.np.random.uniform(-1.0, 1.0, (batch_size, 3, 224, 224))\n+  \n+  for i in range(batches + warmup_batches):\n+    if i == warmup_batches:\n+      tic = time.time()\n+    out = net(data)\n+    out.wait_to_read()\n+    \n+  total_time = time.time()\u200a-\u200atic\n+  return total_time\n+```\n+\n+\n+Comparing fused float32 network to quantized network on CLX8280 shows 4.29x speedup - measurment was done on 28 cores and this machine utilizes VNNI instruction set.\n+\n+\n+The other aspect of lowering the precision of a model is a difference in its accuracy. We will check that on previously tested resnet50_v1 with ImageNet dataset. To run this example you will need ImageNet dataset prepared with this tutorial and stored in path_to_imagenet. Let\u2019s compare top1 and top5 accuracy of standard fp32 model with quantized int8 model calibrated using naive and entropy calibration mode. We will use only 10 batches of the validation dataset to calibrate quantized model.\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon.model_zoo.vision import resnet50_v1\n+from mxnet.gluon.data.vision import transforms\n+from mxnet.contrib.quantization import quantize_net\n+\n+def test_accuracy(net, data_loader):\n+  acc_top1 = mx.gluon.metric.Accuracy()\n+  acc_top5 = mx.gluon.metric.TopKAccuracy(5)\n+  \n+  for x, label in data_loader:\n+    output = net(x)\n+    acc_top1.update(label, output)\n+    acc_top5.update(label, output)\n+\n+  _, top1 = acc_top1.get()\n+  _, top5 = acc_top5.get()\n+\n+  return top1, top5\n+  \n+rgb_mean = (0.485, 0.456, 0.406)\n+rgb_std = (0.229, 0.224, 0.225)\n+batch_size = 64\n+ \n+dataset = mx.gluon.data.vision.ImageRecordDataset('path_to_imagenet/val.rec')\n+transformer = transforms.Compose([transforms.Resize(256),\n+                                  transforms.CenterCrop(224),\n+                                  transforms.ToTensor(),\n+                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n+val_data = mx.gluon.data.DataLoader(dataset.transform_first(transformer), batch_size, shuffle=True)\n+\n+net = resnet50_v1(pretrained=True)\n+net.hybridize(backend='ONEDNN', static_alloc=True, static_shape=True)\n+\n+top1, top5 = test_accuracy(net, val_data)\n+print('FP32 Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+\n+qnet = quantize_net(net, calib_mode='naive', calib_data=val_data, num_calib_batches=10)\n+qnet.hybridize(static_alloc=True, static_shape=True)\n+top1, top5 = test_accuracy(qnet, val_data)\n+print('INT8Naive Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+\n+qnet = quantize_net(net, calib_mode='entropy', calib_data=val_data, num_calib_batches=10)\n+qnet.hybridize(static_alloc=True, static_shape=True)\n+top1, top5 = test_accuracy(qnet, val_data)\n+print('INT8Entropy Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+```\n+\n+#### Output:\n+> FP32 Top1 Accuracy: 0.76364 Top5 Accuracy: 0.93094\n+> INT8Naive Top1 Accuracy: 0.76028 Top5 Accuracy: 0.92796\n+> INT8Entropy Top1 Accuracy: 0.76404 Top5 Accuracy: 0.93042\n+\n+With quantized model there is tiny accuracy drop, however this is cost of great performance optimization and memory footprint. The difference between calibration method is dependent on the model itself, used activation layers and the size of calibration data.\n+\n+### Custom layer collectors and calibrating the model\n+In MXNet 2.0 new interface for creating custom calibration collector has been added. Main goal of this interface is to give the user as much flexibility as possible in almost every step of quantization. Creating own layer collector is pretty easy, however computing effective min/max values can be not a trivial task. \n+\n+Layer collectors are responsible for collecting statistics of each node in the graph \u2014 it means that the input/output data of every operator executed can be observed. Collector utilize the register_op_hook method of HybridBlock class.",
        "comment_created_at": "2022-02-07T10:04:07+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\nLayer collectors are responsible for collecting statistics of each node in the graph \u2014 it means that the input/output data of every operator executed can be observed. Collector utilizes the register_op_hook method of HybridBlock class.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "804509742",
    "pr_number": 20856,
    "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
    "created_at": "2022-02-11T10:08:02+00:00",
    "commented_code": "<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n<!--- or more contributor license agreements.  See the NOTICE file -->\n<!--- distributed with this work for additional information -->\n<!--- regarding copyright ownership.  The ASF licenses this file -->\n<!--- to you under the Apache License, Version 2.0 (the -->\n<!--- \"License\"); you may not use this file except in compliance -->\n<!--- with the License.  You may obtain a copy of the License at -->\n\n<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n\n<!--- Unless required by applicable law or agreed to in writing, -->\n<!--- software distributed under the License is distributed on an -->\n<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n<!--- KIND, either express or implied.  See the License for the -->\n<!--- specific language governing permissions and limitations -->\n<!--- under the License. -->\n\n## Introduction\n\nAfter successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers simultaneously. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n\nTwo main types of software optimizations can be characerized as:\n- memory-bound optimizations - main objective of these optimizations is to reduce the amount of memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution),\n- compute-bound optimizations - these optimizations are mainly made on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution. One of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization.\n\nIn version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n\n## Operator Fusion\n\nModels are often represented as a directed graph of operations (represented by nodes) and data flow (represented as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n\n\nThe simplest way to explain what fusion is and how it works is to present an example. Image above depicts a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar blocks called residual blocks. Some possible fusion patterns are:\n\n- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.\n\nAbove examples are presented as atomic ones, but often they can be combined together, thus two patterns that can be fused in above example are:\n- Conv2D + BatchNorm + ReLU\n- Conv2D + BatchNorm + Add + ReLU\n\nAfter fusing all patterns, computational graph will be changed to the following one:\n![fused_fp32_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/fused_f32.png?raw=true)\n\n\n\n### Operator fusion in MXNet\nSince the version 1.6 of MXNet built with oneDNN support, operator fusion had been enabled by default for executing model with Module API, however in version 2.0 it has been decided to remove setting this feature by environment flag and replace it by aware user API call.\n\nTo fuse model in MXNet 2.0 there are two requirements:\n- the model must be defined as a subclass of HybridBlock or Symbol,\n- the model must have specific operator patterns which can be fused.\n\nAs an example we define example network (sample block from ResNet architecture):\n\n```\nimport mxnet as mx\nfrom mxnet.gluon import nn\n\nclass SampleBlock(nn.HybridBlock):\n    def __init__(self):\n        super(SampleBlock, self).__init__()\n        self.conv1 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n                               use_bias=False, in_channels=64)\n        self.bn1 = nn.BatchNorm()\n        self.conv2 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n                               use_bias=False, in_channels=64)\n        self.bn2 = nn.BatchNorm()\n\n    def forward(self, x):\n        out = mx.npx.activation(self.bn1(self.conv1(x)), 'relu')\n        out = self.bn2(self.conv2(out))\n        out = mx.npx.activation(out + x, 'relu')\n        return out\n        \nnet = SampleBlock()\nnet.initialize()\n\ndata = mx.np.zeros(shape=(1,64,224,224))\n# run fusion\nnet.optimize_for(data, backend='ONEDNN')\n\n# We can check fusion by plotting current symbol of our optimized network\nsym, _ = net.export(None)\ngraph = mx.viz.plot_network(sym, save_format='jpg')\ngraph.view()\n```\nBoth HybridBlock and Symbol classes provide API to easily run fusion of operators. Single line of code is enabling fusion passes on model:\n```\nnet.optimize_for(data, backend='ONEDNN')\n```\n\n*optimize_for* function is available also as Symbol class method. Example call to this API is shown below. Notice that Symbol\u2019s *optimize_for* method is not done in-place, so assigning it to a new variable is required:\n\n```\noptimized_symbol = sym.optimize_for(backend='ONEDNN')\n```\n\nFor the above model definition in a naive benchmark with artificial data, we can gain up to 1.75x speedup without any accuracy loss on our testing machine with Intel(R) Core(TM) i9-9940X.\n\n\n## Quantization\nAs mentioned in the introduction, precision reduction is another very popular method of improving performance of workloads and, what is important, in most cases is combined together with operator fusion which improves performance even more. In training precision reduction utilizes 16 bit data types like bfloat or float16, but for inference great results can be achieved using int8. \n\nModel quantization helps on both memory-bound and compute-bound operations. In quantized model IO operations are reduced as int8 data type is 4x smaller than float32, and also computational throughput is increased as more data can be SIMD'ed. On modern Intel architectures using int8 data type can bring even more speedup by utilizing special VNNI instruction set. \n\n![before_quant](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/before_quant.png?raw=true)\n\nFirstly quantization performs operator fusion on floating-point model as mentioned in paragraph earlier. Next, all operators which support int8 data type are marked as quantized and if needed additional operators are injected into graph surrounding quantizable operator - the goal of this additional operators is to quantize, dequantize or requantize data to keep data type between operators compatible.\n \n![quant_not_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_not_calib.png?raw=true)\n\n\nAfter injection step it is important to perform calibration of the model, however this step is optional. Quantizing without calibration is not recommended in terms of performance. It will result in calculating data minimum and maximum in quantize and requantize nodes during each inference pass. Calibrating a model greatly improves performance as minimum and maximum values are collected offline and are saved inside node - this way there is no need to search for these values during inference pass. \n\n![quant_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib.png?raw=true)\n\n\nCurrently, there are three supported calibration methods:\n- naive \u2014 min/max values from the calibration run,\n- entropy \u2014 uses KL divergence to determine the best symmetrical quantization thresholds for a given histogram of values,\n- custom \u2014 uses user-defined CalibrationCollector to control the calibration process.\n\nLast stage of quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.\n\n![quant_calib_fused](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib_fused.png?raw=true)\n\nIn MXNet 2.0, the quantization procedure has been adjusted to work well with Gluon models since it is the main API now. The goal was to allow the user to quantize fp32 HybridBlock model in just a few lines of code.\n\n### Quantization in MXNet\n\nAs an example of a quantization procedure, pretrained *resnet50_v1* from *model_zoo.vision* package can be used. To get it simply run the following code:\n\n```\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo.vision import resnet50_v1\n\nnet = resnet50_v1(pretrained=True)\n```\n\nNow, to get a ready-to-deploy quantized model two steps are required:\n1. Prepare data loader with calibration data - this data will be used as input to the network. All necessary layers will be observed with layer collector to calculate minimum and maximum value of that layer. This flow is internal mechanism and all what user needs to do is to provide data loader.\n2. Call `quantize_net` function from `contrib.quantize` package - both operator fusion calls will be called inside this API.\n\n```\ncalib_data_loader = mx.gluon.data.DataLoader(dummy_data, batch_size=batch_size)\nqnet = quantize_net(net, calib_mode='naive', calib_data=calib_data_loader)\n```\n\nFollowing function, which calculates total inference time on the model with an artificial data, can be used to compare the performance:\n\n```\ndef benchmark_net(net, batch_size=32, batches=100, warmup_batches=5):\n  import time\n  data = mx.np.random.uniform(-1.0, 1.0, (batch_size, 3, 224, 224))\n  \n  for i in range(batches + warmup_batches):\n    if i == warmup_batches:\n      tic = time.time()\n    out = net(data)\n    out.wait_to_read()\n    \n  total_time = time.time()\u200a-\u200atic\n  return total_time\n```\n\n\nComparing fused float32 network to quantized network on CLX8280 shows 4.29x speedup - measurment was done on 28 cores and this machine utilizes VNNI instruction set.\n\n\nThe other aspect of lowering the precision of a model is a difference in its accuracy. We will check that on previously tested resnet50_v1 with ImageNet dataset. To run this example you will need ImageNet dataset prepared with this tutorial and stored in path_to_imagenet. Let\u2019s compare top1 and top5 accuracy of standard fp32 model with quantized int8 model calibrated using naive and entropy calibration mode. We will use only 10 batches of the validation dataset to calibrate quantized model.\n\n```\nimport mxnet as mx\nfrom mxnet.gluon.model_zoo.vision import resnet50_v1\nfrom mxnet.gluon.data.vision import transforms\nfrom mxnet.contrib.quantization import quantize_net\n\ndef test_accuracy(net, data_loader):\n  acc_top1 = mx.gluon.metric.Accuracy()\n  acc_top5 = mx.gluon.metric.TopKAccuracy(5)\n  \n  for x, label in data_loader:\n    output = net(x)\n    acc_top1.update(label, output)\n    acc_top5.update(label, output)\n\n  _, top1 = acc_top1.get()\n  _, top5 = acc_top5.get()\n\n  return top1, top5\n  \nrgb_mean = (0.485, 0.456, 0.406)\nrgb_std = (0.229, 0.224, 0.225)\nbatch_size = 64\n \ndataset = mx.gluon.data.vision.ImageRecordDataset('path_to_imagenet/val.rec')\ntransformer = transforms.Compose([transforms.Resize(256),\n                                  transforms.CenterCrop(224),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\nval_data = mx.gluon.data.DataLoader(dataset.transform_first(transformer), batch_size, shuffle=True)\n\nnet = resnet50_v1(pretrained=True)\nnet.hybridize(backend='ONEDNN', static_alloc=True, static_shape=True)\n\ntop1, top5 = test_accuracy(net, val_data)\nprint('FP32 Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n\nqnet = quantize_net(net, calib_mode='naive', calib_data=val_data, num_calib_batches=10)\nqnet.hybridize(static_alloc=True, static_shape=True)\ntop1, top5 = test_accuracy(qnet, val_data)\nprint('INT8Naive Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n\nqnet = quantize_net(net, calib_mode='entropy', calib_data=val_data, num_calib_batches=10)\nqnet.hybridize(static_alloc=True, static_shape=True)\ntop1, top5 = test_accuracy(qnet, val_data)\nprint('INT8Entropy Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n```\n\n#### Output:\n> FP32 Top1 Accuracy: 0.76364 Top5 Accuracy: 0.93094\n> INT8Naive Top1 Accuracy: 0.76028 Top5 Accuracy: 0.92796\n> INT8Entropy Top1 Accuracy: 0.76404 Top5 Accuracy: 0.93042\n\nWith quantized model there is tiny accuracy drop, however this is the cost of great performance optimization and memory footprint reduction. The difference between calibration methods is dependent on the model itself, used activation layers and the size of calibration data.",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "804509742",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20856,
        "pr_file": "docs/python_docs/python/tutorials/performance/backend/dnnl/dnnl_quantization.md",
        "discussion_id": "804509742",
        "commented_code": "@@ -0,0 +1,304 @@\n+<!--- Licensed to the Apache Software Foundation (ASF) under one -->\n+<!--- or more contributor license agreements.  See the NOTICE file -->\n+<!--- distributed with this work for additional information -->\n+<!--- regarding copyright ownership.  The ASF licenses this file -->\n+<!--- to you under the Apache License, Version 2.0 (the -->\n+<!--- \"License\"); you may not use this file except in compliance -->\n+<!--- with the License.  You may obtain a copy of the License at -->\n+\n+<!---   http://www.apache.org/licenses/LICENSE-2.0 -->\n+\n+<!--- Unless required by applicable law or agreed to in writing, -->\n+<!--- software distributed under the License is distributed on an -->\n+<!--- \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->\n+<!--- KIND, either express or implied.  See the License for the -->\n+<!--- specific language governing permissions and limitations -->\n+<!--- under the License. -->\n+\n+## Introduction\n+\n+After successful model building and achieving desired accuracy on the test data, often the next step is to optimize inference to deploy the model to production. One of the key features of usable model is to have as small latency as possible to be able to provide services to large number of customers simultaneously. In addition to customer satisfaction, with well optimized model, hardware load is reduced which also reduces energy costs needed to perform inference.\n+\n+Two main types of software optimizations can be characerized as:\n+- memory-bound optimizations - main objective of these optimizations is to reduce the amount of memory operations (reads and writes) - it is done by e.g. chaining operations which can be performed one after another immediately, where input of every subsequent operation is the output of the previous one (example: ReLU activation after convolution),\n+- compute-bound optimizations - these optimizations are mainly made on operations which require large number of CPU cycles to complete, like FullyConnected and Convolution. One of the methods to speedup compute-bound operations is to lower computation precision - this type of optimization is called quantization.\n+\n+In version 2.0 of the Apache MXNet (incubating) GluonAPI2.0 replaced Symbolic API known from versions 1.x, thus there are some differences between API to perform graph fusion and quantization.\n+\n+## Operator Fusion\n+\n+Models are often represented as a directed graph of operations (represented by nodes) and data flow (represented as edges). This way of visualizing helps a lot when searching for common patterns in whole model which can be optimized by fusion. Example:\n+![base_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/sample_net.png?raw=true)\n+\n+\n+The simplest way to explain what fusion is and how it works is to present an example. Image above depicts a sequence of popular operations taken from ResNet architecture. This type of architecture is built with many similar blocks called residual blocks. Some possible fusion patterns are:\n+\n+- Conv2D + BatchNorm => Fusing BatchNorm with Convolution can be performed by modifing weights and bias of Convolution - this way BatchNorm is completely contained within Convolution which makes BatchNorm zero time operation. Only cost of fusing is time needed to prepare weights and bias in Convolution based on BatchNorm parameters.\n+- Conv2D + ReLU => this type of fusion is very popular also with other layers (e.g. FullyConnected + Activation). It is very simple idea where before writing data to output, activation is performed on that data. Main benefit of this fusion is that, there is no need to read and write back data in other layer only to perform simple activation function. \n+- Conv2D + Add => even simpler idea than the previous ones - instead of overwriting output memory, results are added to the output memory. In the simplest terms: `out_mem = conv_result` is replaced by `out_mem += conv_result`.\n+\n+Above examples are presented as atomic ones, but often they can be combined together, thus two patterns that can be fused in above example are:\n+- Conv2D + BatchNorm + ReLU\n+- Conv2D + BatchNorm + Add + ReLU\n+\n+After fusing all patterns, computational graph will be changed to the following one:\n+![fused_fp32_model](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/fused_f32.png?raw=true)\n+\n+\n+\n+### Operator fusion in MXNet\n+Since the version 1.6 of MXNet built with oneDNN support, operator fusion had been enabled by default for executing model with Module API, however in version 2.0 it has been decided to remove setting this feature by environment flag and replace it by aware user API call.\n+\n+To fuse model in MXNet 2.0 there are two requirements:\n+- the model must be defined as a subclass of HybridBlock or Symbol,\n+- the model must have specific operator patterns which can be fused.\n+\n+As an example we define example network (sample block from ResNet architecture):\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon import nn\n+\n+class SampleBlock(nn.HybridBlock):\n+    def __init__(self):\n+        super(SampleBlock, self).__init__()\n+        self.conv1 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n+                               use_bias=False, in_channels=64)\n+        self.bn1 = nn.BatchNorm()\n+        self.conv2 = nn.Conv2D(channels=64, kernel_size=3, strides=1, padding=1,\n+                               use_bias=False, in_channels=64)\n+        self.bn2 = nn.BatchNorm()\n+\n+    def forward(self, x):\n+        out = mx.npx.activation(self.bn1(self.conv1(x)), 'relu')\n+        out = self.bn2(self.conv2(out))\n+        out = mx.npx.activation(out + x, 'relu')\n+        return out\n+        \n+net = SampleBlock()\n+net.initialize()\n+\n+data = mx.np.zeros(shape=(1,64,224,224))\n+# run fusion\n+net.optimize_for(data, backend='ONEDNN')\n+\n+# We can check fusion by plotting current symbol of our optimized network\n+sym, _ = net.export(None)\n+graph = mx.viz.plot_network(sym, save_format='jpg')\n+graph.view()\n+```\n+Both HybridBlock and Symbol classes provide API to easily run fusion of operators. Single line of code is enabling fusion passes on model:\n+```\n+net.optimize_for(data, backend='ONEDNN')\n+```\n+\n+*optimize_for* function is available also as Symbol class method. Example call to this API is shown below. Notice that Symbol\u2019s *optimize_for* method is not done in-place, so assigning it to a new variable is required:\n+\n+```\n+optimized_symbol = sym.optimize_for(backend='ONEDNN')\n+```\n+\n+For the above model definition in a naive benchmark with artificial data, we can gain up to 1.75x speedup without any accuracy loss on our testing machine with Intel(R) Core(TM) i9-9940X.\n+\n+\n+## Quantization\n+As mentioned in the introduction, precision reduction is another very popular method of improving performance of workloads and, what is important, in most cases is combined together with operator fusion which improves performance even more. In training precision reduction utilizes 16 bit data types like bfloat or float16, but for inference great results can be achieved using int8. \n+\n+Model quantization helps on both memory-bound and compute-bound operations. In quantized model IO operations are reduced as int8 data type is 4x smaller than float32, and also computational throughput is increased as more data can be SIMD'ed. On modern Intel architectures using int8 data type can bring even more speedup by utilizing special VNNI instruction set. \n+\n+![before_quant](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/before_quant.png?raw=true)\n+\n+Firstly quantization performs operator fusion on floating-point model as mentioned in paragraph earlier. Next, all operators which support int8 data type are marked as quantized and if needed additional operators are injected into graph surrounding quantizable operator - the goal of this additional operators is to quantize, dequantize or requantize data to keep data type between operators compatible.\n+ \n+![quant_not_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_not_calib.png?raw=true)\n+\n+\n+After injection step it is important to perform calibration of the model, however this step is optional. Quantizing without calibration is not recommended in terms of performance. It will result in calculating data minimum and maximum in quantize and requantize nodes during each inference pass. Calibrating a model greatly improves performance as minimum and maximum values are collected offline and are saved inside node - this way there is no need to search for these values during inference pass. \n+\n+![quant_calib](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib.png?raw=true)\n+\n+\n+Currently, there are three supported calibration methods:\n+- naive \u2014 min/max values from the calibration run,\n+- entropy \u2014 uses KL divergence to determine the best symmetrical quantization thresholds for a given histogram of values,\n+- custom \u2014 uses user-defined CalibrationCollector to control the calibration process.\n+\n+Last stage of quantization flow is to perform additional operator fusion. Second fusion is about merging requantize and dequantize operators into preceding node - oneDNN kernels can perform needed scaling before writing result to output which results in model execution speed-up. Notice that last Convolution does not need minimum and maximum values as it is not requantizing int32 to int8, but dequantizing directly to float32 and scale is calculated basing on minimum and maximum of input and weights.\n+\n+![quant_calib_fused](https://github.com/dmlc/web-data/blob/main/mxnet/tutorials/onednn/quantization_2_0/quant_calib_fused.png?raw=true)\n+\n+In MXNet 2.0, the quantization procedure has been adjusted to work well with Gluon models since it is the main API now. The goal was to allow the user to quantize fp32 HybridBlock model in just a few lines of code.\n+\n+### Quantization in MXNet\n+\n+As an example of a quantization procedure, pretrained *resnet50_v1* from *model_zoo.vision* package can be used. To get it simply run the following code:\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon.model_zoo.vision import resnet50_v1\n+\n+net = resnet50_v1(pretrained=True)\n+```\n+\n+Now, to get a ready-to-deploy quantized model two steps are required:\n+1. Prepare data loader with calibration data - this data will be used as input to the network. All necessary layers will be observed with layer collector to calculate minimum and maximum value of that layer. This flow is internal mechanism and all what user needs to do is to provide data loader.\n+2. Call `quantize_net` function from `contrib.quantize` package - both operator fusion calls will be called inside this API.\n+\n+```\n+calib_data_loader = mx.gluon.data.DataLoader(dummy_data, batch_size=batch_size)\n+qnet = quantize_net(net, calib_mode='naive', calib_data=calib_data_loader)\n+```\n+\n+Following function, which calculates total inference time on the model with an artificial data, can be used to compare the performance:\n+\n+```\n+def benchmark_net(net, batch_size=32, batches=100, warmup_batches=5):\n+  import time\n+  data = mx.np.random.uniform(-1.0, 1.0, (batch_size, 3, 224, 224))\n+  \n+  for i in range(batches + warmup_batches):\n+    if i == warmup_batches:\n+      tic = time.time()\n+    out = net(data)\n+    out.wait_to_read()\n+    \n+  total_time = time.time()\u200a-\u200atic\n+  return total_time\n+```\n+\n+\n+Comparing fused float32 network to quantized network on CLX8280 shows 4.29x speedup - measurment was done on 28 cores and this machine utilizes VNNI instruction set.\n+\n+\n+The other aspect of lowering the precision of a model is a difference in its accuracy. We will check that on previously tested resnet50_v1 with ImageNet dataset. To run this example you will need ImageNet dataset prepared with this tutorial and stored in path_to_imagenet. Let\u2019s compare top1 and top5 accuracy of standard fp32 model with quantized int8 model calibrated using naive and entropy calibration mode. We will use only 10 batches of the validation dataset to calibrate quantized model.\n+\n+```\n+import mxnet as mx\n+from mxnet.gluon.model_zoo.vision import resnet50_v1\n+from mxnet.gluon.data.vision import transforms\n+from mxnet.contrib.quantization import quantize_net\n+\n+def test_accuracy(net, data_loader):\n+  acc_top1 = mx.gluon.metric.Accuracy()\n+  acc_top5 = mx.gluon.metric.TopKAccuracy(5)\n+  \n+  for x, label in data_loader:\n+    output = net(x)\n+    acc_top1.update(label, output)\n+    acc_top5.update(label, output)\n+\n+  _, top1 = acc_top1.get()\n+  _, top5 = acc_top5.get()\n+\n+  return top1, top5\n+  \n+rgb_mean = (0.485, 0.456, 0.406)\n+rgb_std = (0.229, 0.224, 0.225)\n+batch_size = 64\n+ \n+dataset = mx.gluon.data.vision.ImageRecordDataset('path_to_imagenet/val.rec')\n+transformer = transforms.Compose([transforms.Resize(256),\n+                                  transforms.CenterCrop(224),\n+                                  transforms.ToTensor(),\n+                                  transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n+val_data = mx.gluon.data.DataLoader(dataset.transform_first(transformer), batch_size, shuffle=True)\n+\n+net = resnet50_v1(pretrained=True)\n+net.hybridize(backend='ONEDNN', static_alloc=True, static_shape=True)\n+\n+top1, top5 = test_accuracy(net, val_data)\n+print('FP32 Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+\n+qnet = quantize_net(net, calib_mode='naive', calib_data=val_data, num_calib_batches=10)\n+qnet.hybridize(static_alloc=True, static_shape=True)\n+top1, top5 = test_accuracy(qnet, val_data)\n+print('INT8Naive Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+\n+qnet = quantize_net(net, calib_mode='entropy', calib_data=val_data, num_calib_batches=10)\n+qnet.hybridize(static_alloc=True, static_shape=True)\n+top1, top5 = test_accuracy(qnet, val_data)\n+print('INT8Entropy Top1 Accuracy: {} Top5 Accuracy: {}'.format(top1, top5))\n+```\n+\n+#### Output:\n+> FP32 Top1 Accuracy: 0.76364 Top5 Accuracy: 0.93094\n+> INT8Naive Top1 Accuracy: 0.76028 Top5 Accuracy: 0.92796\n+> INT8Entropy Top1 Accuracy: 0.76404 Top5 Accuracy: 0.93042\n+\n+With quantized model there is tiny accuracy drop, however this is the cost of great performance optimization and memory footprint reduction. The difference between calibration methods is dependent on the model itself, used activation layers and the size of calibration data.",
        "comment_created_at": "2022-02-11T10:08:02+00:00",
        "comment_author": "bartekkuncer",
        "comment_body": "```suggestion\r\nWith quantized model there is a tiny accuracy drop, however this is the cost of great performance optimization and memory footprint reduction. The difference between calibration methods is dependent on the model itself, used activation layers and the size of calibration data.\r\n```",
        "pr_file_module": null
      }
    ]
  }
]
