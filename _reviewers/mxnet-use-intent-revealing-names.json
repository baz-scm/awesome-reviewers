[
  {
    "discussion_id": "918911666",
    "pr_number": 21002,
    "pr_file": "src/common/utils.h",
    "created_at": "2022-07-12T12:31:46+00:00",
    "commented_code": "}\n\ninline bool is_float(const int dtype) {",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "918911666",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21002,
        "pr_file": "src/common/utils.h",
        "discussion_id": "918911666",
        "commented_code": "@@ -929,11 +929,8 @@ inline mxnet::TShape CanonicalizeAxes(const mxnet::TShape& src) {\n }\n \n inline bool is_float(const int dtype) {",
        "comment_created_at": "2022-07-12T12:31:46+00:00",
        "comment_author": "anko-intel",
        "comment_body": "maybe we can change the name to is_floating to emphase that it is not an answer for particular type like float but common floating point group of data representation\r\n```suggestion\r\ninline bool is_floating(const int dtype) {\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "921936461",
        "repo_full_name": "apache/mxnet",
        "pr_number": 21002,
        "pr_file": "src/common/utils.h",
        "discussion_id": "918911666",
        "commented_code": "@@ -929,11 +929,8 @@ inline mxnet::TShape CanonicalizeAxes(const mxnet::TShape& src) {\n }\n \n inline bool is_float(const int dtype) {",
        "comment_created_at": "2022-07-15T08:25:13+00:00",
        "comment_author": "bgawrych",
        "comment_body": "can be done in other PR to keep this one simple to track relevant changes",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "751251873",
    "pr_number": 20669,
    "pr_file": "src/operator/nn/dnnl/dnnl_reduce-inl.h",
    "created_at": "2021-11-17T13:42:39+00:00",
    "commented_code": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\n/*!\n * \\file dnnl_reduce-inl.h\n */\n\n#ifndef MXNET_OPERATOR_NN_DNNL_DNNL_REDUCE_INL_H_\n#define MXNET_OPERATOR_NN_DNNL_DNNL_REDUCE_INL_H_\n\n#if MXNET_USE_ONEDNN == 1\n#include <vector>\n\n#include \"./dnnl_base-inl.h\"\n#include \"./dnnl_ops-inl.h\"\n\nnamespace mxnet {\nnamespace op {\n\nusing reduce_fwd_t    = dnnl::reduction;\nusing reduce_fwd_pd_t = dnnl::reduction::primitive_desc;\nstruct NumpyReduceAxesParam;\nstruct ReduceAxesParam;\nclass DNNLReduceFwd {\n public:\n  struct Tensors {\n    Tensors(const NDArray& data, const NDArray& out);\n\n    const NDArray& data;\n    const NDArray& out;\n  };\n\n  static DNNLReduceFwd GetCached(const NumpyReduceAxesParam& param,\n                                 const Tensors& tensors,\n                                 const bool is_train,\n                                 const dnnl::algorithm reduction_alg);\n\n  static reduce_fwd_pd_t GetReduceFwdPd(const dnnl::memory::desc& input_md,\n                                        const dnnl::memory::desc& output_md,\n                                        const dnnl::algorithm reduction_alg);\n\n  DNNLReduceFwd(const NumpyReduceAxesParam& param,\n                const Tensors& tensors,\n                const bool is_train,\n                const dnnl::algorithm reduction_alg);\n  void Execute(const Tensors& tensors) const;\n\n private:\n  std::shared_ptr<reduce_fwd_pd_t> reduce_pd;\n  std::shared_ptr<reduce_fwd_t> reduce_fwd;\n};\n\ntemplate <class T>\nNumpyReduceAxesParam ConvertParamsToNumpy(const T& original_param,",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "751251873",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20669,
        "pr_file": "src/operator/nn/dnnl/dnnl_reduce-inl.h",
        "discussion_id": "751251873",
        "commented_code": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/*!\n+ * \\file dnnl_reduce-inl.h\n+ */\n+\n+#ifndef MXNET_OPERATOR_NN_DNNL_DNNL_REDUCE_INL_H_\n+#define MXNET_OPERATOR_NN_DNNL_DNNL_REDUCE_INL_H_\n+\n+#if MXNET_USE_ONEDNN == 1\n+#include <vector>\n+\n+#include \"./dnnl_base-inl.h\"\n+#include \"./dnnl_ops-inl.h\"\n+\n+namespace mxnet {\n+namespace op {\n+\n+using reduce_fwd_t    = dnnl::reduction;\n+using reduce_fwd_pd_t = dnnl::reduction::primitive_desc;\n+struct NumpyReduceAxesParam;\n+struct ReduceAxesParam;\n+class DNNLReduceFwd {\n+ public:\n+  struct Tensors {\n+    Tensors(const NDArray& data, const NDArray& out);\n+\n+    const NDArray& data;\n+    const NDArray& out;\n+  };\n+\n+  static DNNLReduceFwd GetCached(const NumpyReduceAxesParam& param,\n+                                 const Tensors& tensors,\n+                                 const bool is_train,\n+                                 const dnnl::algorithm reduction_alg);\n+\n+  static reduce_fwd_pd_t GetReduceFwdPd(const dnnl::memory::desc& input_md,\n+                                        const dnnl::memory::desc& output_md,\n+                                        const dnnl::algorithm reduction_alg);\n+\n+  DNNLReduceFwd(const NumpyReduceAxesParam& param,\n+                const Tensors& tensors,\n+                const bool is_train,\n+                const dnnl::algorithm reduction_alg);\n+  void Execute(const Tensors& tensors) const;\n+\n+ private:\n+  std::shared_ptr<reduce_fwd_pd_t> reduce_pd;\n+  std::shared_ptr<reduce_fwd_t> reduce_fwd;\n+};\n+\n+template <class T>\n+NumpyReduceAxesParam ConvertParamsToNumpy(const T& original_param,",
        "comment_created_at": "2021-11-17T13:42:39+00:00",
        "comment_author": "anko-intel",
        "comment_body": "is it possible that different  class has the same functionality? So maybe the function name should be different depending what is returned?",
        "pr_file_module": null
      },
      {
        "comment_id": "752982322",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20669,
        "pr_file": "src/operator/nn/dnnl/dnnl_reduce-inl.h",
        "discussion_id": "751251873",
        "commented_code": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/*!\n+ * \\file dnnl_reduce-inl.h\n+ */\n+\n+#ifndef MXNET_OPERATOR_NN_DNNL_DNNL_REDUCE_INL_H_\n+#define MXNET_OPERATOR_NN_DNNL_DNNL_REDUCE_INL_H_\n+\n+#if MXNET_USE_ONEDNN == 1\n+#include <vector>\n+\n+#include \"./dnnl_base-inl.h\"\n+#include \"./dnnl_ops-inl.h\"\n+\n+namespace mxnet {\n+namespace op {\n+\n+using reduce_fwd_t    = dnnl::reduction;\n+using reduce_fwd_pd_t = dnnl::reduction::primitive_desc;\n+struct NumpyReduceAxesParam;\n+struct ReduceAxesParam;\n+class DNNLReduceFwd {\n+ public:\n+  struct Tensors {\n+    Tensors(const NDArray& data, const NDArray& out);\n+\n+    const NDArray& data;\n+    const NDArray& out;\n+  };\n+\n+  static DNNLReduceFwd GetCached(const NumpyReduceAxesParam& param,\n+                                 const Tensors& tensors,\n+                                 const bool is_train,\n+                                 const dnnl::algorithm reduction_alg);\n+\n+  static reduce_fwd_pd_t GetReduceFwdPd(const dnnl::memory::desc& input_md,\n+                                        const dnnl::memory::desc& output_md,\n+                                        const dnnl::algorithm reduction_alg);\n+\n+  DNNLReduceFwd(const NumpyReduceAxesParam& param,\n+                const Tensors& tensors,\n+                const bool is_train,\n+                const dnnl::algorithm reduction_alg);\n+  void Execute(const Tensors& tensors) const;\n+\n+ private:\n+  std::shared_ptr<reduce_fwd_pd_t> reduce_pd;\n+  std::shared_ptr<reduce_fwd_t> reduce_fwd;\n+};\n+\n+template <class T>\n+NumpyReduceAxesParam ConvertParamsToNumpy(const T& original_param,",
        "comment_created_at": "2021-11-19T08:57:10+00:00",
        "comment_author": "bgawrych",
        "comment_body": "I've changed name to ConvertReduceParamsToNumpy and also in other file ConvertTransposeParamsToNumpy",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "693907785",
    "pr_number": 20474,
    "pr_file": "src/initialize.cc",
    "created_at": "2021-08-23T12:00:11+00:00",
    "commented_code": "cpu_worker_nthreads_(dmlc::GetEnv(\"MXNET_CPU_WORKER_NTHREADS\", 1)),\n    mp_cv_num_threads_(dmlc::GetEnv(\"MXNET_MP_OPENCV_NUM_THREADS\", 0)) {\n  dmlc::InitLogging(\"mxnet\");\n\n#if !(defined(_WIN32) || defined(_WIN64) || defined(__WINDOWS__))",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "693907785",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20474,
        "pr_file": "src/initialize.cc",
        "discussion_id": "693907785",
        "commented_code": "@@ -93,6 +95,24 @@ LibraryInitializer::LibraryInitializer()\n     cpu_worker_nthreads_(dmlc::GetEnv(\"MXNET_CPU_WORKER_NTHREADS\", 1)),\n     mp_cv_num_threads_(dmlc::GetEnv(\"MXNET_MP_OPENCV_NUM_THREADS\", 0)) {\n   dmlc::InitLogging(\"mxnet\");\n+\n+#if !(defined(_WIN32) || defined(_WIN64) || defined(__WINDOWS__))",
        "comment_created_at": "2021-08-23T12:00:11+00:00",
        "comment_author": "bgawrych",
        "comment_body": "maybe it should be moved to a function like install_pthread_atfork_handlers - it probably will be more descriptive on what's happening here",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "643227554",
    "pr_number": 19896,
    "pr_file": "src/operator/contrib/adaptive_avg_pooling.cc",
    "created_at": "2021-06-01T15:45:02+00:00",
    "commented_code": "}\n}\n\n#if MXNET_USE_MKLDNN == 1\nbool SupportMKLDNNAveragePooling(const NDArray &in_data,\n                                    const NDArray &out_data) {\n  for (int64_t idx = 2; idx < in_data.shape().ndim(); ++idx) {\n    const int s1 = in_data.shape()[idx];\n    const int s2 = out_data.shape()[idx];\n    if (s2 == 0) {\n      return false;\n    }\n    if (s1 % s2 != 0) {\n      return false;\n    }\n  }\n  const int IH = in_data.shape()[2];\n  const int IW = in_data.shape()[3];\n  const int OH = out_data.shape()[2];\n  const int OW = out_data.shape()[3];\n\n  const int strides_1 = floor((IH << 1) / OH) - floor(IH / OH);\n  const int strides_2 = floor((IW << 1) / OW) - floor(IW / OW);",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "643227554",
        "repo_full_name": "apache/mxnet",
        "pr_number": 19896,
        "pr_file": "src/operator/contrib/adaptive_avg_pooling.cc",
        "discussion_id": "643227554",
        "commented_code": "@@ -197,6 +198,81 @@ num_threads(engine::OpenMP::Get()->GetRecommendedOMPThreadCount())\n   }\n }\n \n+#if MXNET_USE_MKLDNN == 1\n+bool SupportMKLDNNAveragePooling(const NDArray &in_data,\n+                                    const NDArray &out_data) {\n+  for (int64_t idx = 2; idx < in_data.shape().ndim(); ++idx) {\n+    const int s1 = in_data.shape()[idx];\n+    const int s2 = out_data.shape()[idx];\n+    if (s2 == 0) {\n+      return false;\n+    }\n+    if (s1 % s2 != 0) {\n+      return false;\n+    }\n+  }\n+  const int IH = in_data.shape()[2];\n+  const int IW = in_data.shape()[3];\n+  const int OH = out_data.shape()[2];\n+  const int OW = out_data.shape()[3];\n+\n+  const int strides_1 = floor((IH << 1) / OH) - floor(IH / OH);\n+  const int strides_2 = floor((IW << 1) / OW) - floor(IW / OW);",
        "comment_created_at": "2021-06-01T15:45:02+00:00",
        "comment_author": "akarbown",
        "comment_body": "Maybe worth considering to rename 'strides_1, strides_2' to 'strides_H/strides_W'?",
        "pr_file_module": null
      },
      {
        "comment_id": "650936307",
        "repo_full_name": "apache/mxnet",
        "pr_number": 19896,
        "pr_file": "src/operator/contrib/adaptive_avg_pooling.cc",
        "discussion_id": "643227554",
        "commented_code": "@@ -197,6 +198,81 @@ num_threads(engine::OpenMP::Get()->GetRecommendedOMPThreadCount())\n   }\n }\n \n+#if MXNET_USE_MKLDNN == 1\n+bool SupportMKLDNNAveragePooling(const NDArray &in_data,\n+                                    const NDArray &out_data) {\n+  for (int64_t idx = 2; idx < in_data.shape().ndim(); ++idx) {\n+    const int s1 = in_data.shape()[idx];\n+    const int s2 = out_data.shape()[idx];\n+    if (s2 == 0) {\n+      return false;\n+    }\n+    if (s1 % s2 != 0) {\n+      return false;\n+    }\n+  }\n+  const int IH = in_data.shape()[2];\n+  const int IW = in_data.shape()[3];\n+  const int OH = out_data.shape()[2];\n+  const int OW = out_data.shape()[3];\n+\n+  const int strides_1 = floor((IH << 1) / OH) - floor(IH / OH);\n+  const int strides_2 = floor((IW << 1) / OW) - floor(IW / OW);",
        "comment_created_at": "2021-06-14T13:15:33+00:00",
        "comment_author": "mozga-intel",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "663079191",
    "pr_number": 20302,
    "pr_file": "src/operator/quantization/quantization_utils.h",
    "created_at": "2021-07-02T15:05:08+00:00",
    "commented_code": "return broadcast::ReduceWorkspaceSize<NDim, DType>(s, *dst_shape, kWriteTo, *src_shape);\n}\n\nenum QuantizeOutType { kAuto = 0, kInt8, kUint8 };\nenum QuantizeOutType { qAuto = 0, qInt8, qUint8 };",
    "repo_full_name": "apache/mxnet",
    "discussion_comments": [
      {
        "comment_id": "663079191",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20302,
        "pr_file": "src/operator/quantization/quantization_utils.h",
        "discussion_id": "663079191",
        "commented_code": "@@ -187,22 +187,22 @@ inline size_t ConfigReduce(mshadow::Stream<xpu>* s,\n   return broadcast::ReduceWorkspaceSize<NDim, DType>(s, *dst_shape, kWriteTo, *src_shape);\n }\n \n-enum QuantizeOutType { kAuto = 0, kInt8, kUint8 };\n+enum QuantizeOutType { qAuto = 0, qInt8, qUint8 };",
        "comment_created_at": "2021-07-02T15:05:08+00:00",
        "comment_author": "szha",
        "comment_body": "just curious about the naming, why the q prefix?",
        "pr_file_module": null
      },
      {
        "comment_id": "663082456",
        "repo_full_name": "apache/mxnet",
        "pr_number": 20302,
        "pr_file": "src/operator/quantization/quantization_utils.h",
        "discussion_id": "663079191",
        "commented_code": "@@ -187,22 +187,22 @@ inline size_t ConfigReduce(mshadow::Stream<xpu>* s,\n   return broadcast::ReduceWorkspaceSize<NDim, DType>(s, *dst_shape, kWriteTo, *src_shape);\n }\n \n-enum QuantizeOutType { kAuto = 0, kInt8, kUint8 };\n+enum QuantizeOutType { qAuto = 0, qInt8, qUint8 };",
        "comment_created_at": "2021-07-02T15:09:57+00:00",
        "comment_author": "sfraczek",
        "comment_body": "Because there is already name mshadow::kInt8 and mshadow::kUint8 which I was intending to use but when I wrote just kUint8 without prefix `mshadow::` in front of it this enum got used instead and it was hard to detect error.Adding q makes a clear distinction at low cost. I think it's for the better since this enum is scarcely used anyway.",
        "pr_file_module": null
      }
    ]
  }
]