[
  {
    "discussion_id": "2216324020",
    "pr_number": 17430,
    "pr_file": "packages/frontend/editor-ui/src/stores/builder.store.ts",
    "created_at": "2025-07-18T15:21:09+00:00",
    "commented_code": "}, ASK_AI_SLIDE_OUT_DURATION_MS + 50);\n \t}\n \n-\tfunction clearMessages() {\n-\t\tchatMessages.value = [];\n-\t}\n-\n+\t/**\n+\t * Updates chat panel width with enforced boundaries.\n+\t * Width is clamped between MIN_CHAT_WIDTH (330px) and MAX_CHAT_WIDTH (650px)\n+\t * to ensure usability on various screen sizes.\n+\t */\n \tfunction updateWindowWidth(width: number) {\n \t\tchatWidth.value = Math.min(Math.max(width, MIN_CHAT_WIDTH), MAX_CHAT_WIDTH);\n \t}\n \n \t// Message handling functions\n-\tfunction addAssistantMessages(newMessages: ChatRequest.MessageResponse[], id: string) {\n-\t\tconst read = true; // Always mark as read in builder\n-\t\tconst messages = [...chatMessages.value].filter(\n-\t\t\t(msg) => !(msg.id === id && msg.role === 'assistant'),\n-\t\t);\n-\t\tassistantThinkingMessage.value = undefined;\n-\n-\t\tnewMessages.forEach((msg) => {\n-\t\t\tif (msg.type === 'message') {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'text',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tcontent: msg.text,\n-\t\t\t\t\tquickReplies: msg.quickReplies,\n-\t\t\t\t\tcodeSnippet: msg.codeSnippet,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'workflow-step' && 'steps' in msg) {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'workflow-step',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tsteps: msg.steps,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'prompt-validation' && !msg.isWorkflowPrompt) {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\ttype: 'error',\n-\t\t\t\t\tcontent: locale.baseText('aiAssistant.builder.invalidPrompt'),\n-\t\t\t\t\tread: true,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'workflow-node' && 'nodes' in msg) {\n-\t\t\t\tconst mappedNodes = msg.nodes.map(\n-\t\t\t\t\t(node) => nodeTypesStore.getNodeType(node)?.displayName ?? node,\n-\t\t\t\t);\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'workflow-node',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tnodes: mappedNodes,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'workflow-composed' && 'nodes' in msg) {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'workflow-composed',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tnodes: msg.nodes,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'workflow-generated' && 'codeSnippet' in msg) {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'workflow-generated',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tcodeSnippet: msg.codeSnippet,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'rate-workflow') {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'rate-workflow',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tcontent: msg.content,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t}\n-\t\t});\n-\t\tchatMessages.value = messages;\n-\t}\n-\n-\tfunction addAssistantError(content: string, id: string, retry?: () => Promise<void>) {\n-\t\tchatMessages.value.push({\n-\t\t\tid,\n-\t\t\trole: 'assistant',\n-\t\t\ttype: 'error',\n-\t\t\tcontent,\n-\t\t\tread: true,\n-\t\t\tretry,\n-\t\t});\n-\t}\n-\n \tfunction addLoadingAssistantMessage(message: string) {\n \t\tassistantThinkingMessage.value = message;\n \t}\n \n-\tfunction addUserMessage(content: string, id: string) {\n-\t\tchatMessages.value.push({\n-\t\t\tid,\n-\t\t\trole: 'user',\n-\t\t\ttype: 'text',\n-\t\t\tcontent,\n-\t\t\tread: true,\n-\t\t});\n-\t}\n-\n \tfunction stopStreaming() {\n \t\tstreaming.value = false;\n \t}\n \n \t// Error handling\n+\t/**\n+\t * Handles streaming errors by creating an error message with optional retry capability.\n+\t * Cleans up streaming state and removes the thinking indicator.\n+\t * The retry function, if provided, will remove the error message before retrying.\n+\t * Tracks error telemetry\n+\t */\n \tfunction handleServiceError(e: unknown, id: string, retry?: () => Promise<void>) {\n \t\tassert(e instanceof Error);\n+\n \t\tstopStreaming();\n \t\tassistantThinkingMessage.value = undefined;\n-\t\taddAssistantError(\n+\n+\t\tconst errorMessage = createErrorMessage(\n \t\t\tlocale.baseText('aiAssistant.serviceError.message', { interpolate: { message: e.message } }),\n \t\t\tid,\n \t\t\tretry,\n \t\t);\n+\t\tchatMessages.value = [...chatMessages.value, errorMessage];\n+\n \t\ttelemetry.track('Workflow generation errored', {\n \t\t\terror: e.message,\n-\t\t\tprompt: workflowPrompt.value,\n+\t\t\tworkflow_id: workflowsStore.workflowId,\n \t\t});\n \t}\n \n-\t// API interaction\n-\tfunction getRandomId() {\n-\t\treturn `${Math.floor(Math.random() * 100000000)}`;\n+\t// Helper functions\n+\t/**\n+\t * Prepares UI for incoming streaming response.\n+\t * Adds user message immediately for visual feedback, shows thinking indicator,\n+\t * and ensures chat is open. Called before initiating API request to minimize\n+\t * perceived latency.\n+\t */\n+\tfunction prepareForStreaming(userMessage: string, messageId: string) {\n+\t\tconst userMsg = createUserMessage(userMessage, messageId);\n+\t\tchatMessages.value = [...chatMessages.value, userMsg];\n+\t\taddLoadingAssistantMessage(locale.baseText('aiAssistant.thinkingSteps.thinking'));\n+\t\tstreaming.value = true;\n+\t}\n+\n+\t/**\n+\t * Creates a retry function that removes the associated error message before retrying.\n+\t * This ensures the chat doesn't accumulate multiple error messages for the same failure.\n+\t * The messageId parameter refers to the error message to remove, not the original user message.\n+\t */\n+\tfunction createRetryHandler(messageId: string, retryFn: () => Promise<void>) {\n+\t\treturn async () => {\n+\t\t\t// Remove the error message before retrying\n+\t\t\tchatMessages.value = chatMessages.value.filter((msg) => msg.id !== messageId);\n+\t\t\tawait retryFn();\n+\t\t};\n \t}\n \n-\tfunction onEachStreamingMessage(response: ChatRequest.ResponsePayload, id: string) {\n-\t\tif (response.sessionId && !currentSessionId.value) {\n-\t\t\tcurrentSessionId.value = response.sessionId;\n-\t\t\ttelemetry.track('Assistant session started', {\n-\t\t\t\tchat_session_id: currentSessionId.value,\n-\t\t\t\ttask: 'workflow-generation',\n-\t\t\t});\n-\t\t} else if (currentSessionId.value !== response.sessionId) {\n-\t\t\t// Ignore messages from other sessions\n+\t// Core API functions\n+\t/**\n+\t * Sends a message to the AI builder service and handles the streaming response.\n+\t * Prevents concurrent requests by checking streaming state.\n+\t * Captures workflow state before sending for comparison in telemetry.\n+\t * Creates a retry handler that preserves the original message context.\n+\t * Note: This function is NOT async - streaming happens via callbacks.\n+\t */\n+\tfunction sendChatMessage(options: {\n+\t\ttext: string;\n+\t\tsource?: 'chat' | 'canvas';\n+\t\tquickReplyType?: string;\n+\t}) {\n+\t\tif (streaming.value) {\n \t\t\treturn;\n \t\t}\n-\t\taddAssistantMessages(response.messages, id);\n-\t}\n \n-\tfunction onDoneStreaming() {\n-\t\tstopStreaming();\n-\t}\n+\t\tconst { text, source = 'chat', quickReplyType } = options;\n+\t\tconst messageId = generateMessageId();\n \n-\t// Core API functions\n-\tasync function initBuilderChat(userMessage: string, source: 'chat' | 'canvas') {\n-\t\ttelemetry.track('User submitted workflow prompt', {\n+\t\tconst currentWorkflowJson = getWorkflowSnapshot();\n+\t\ttelemetry.track('User submitted builder message', {\n \t\t\tsource,\n-\t\t\tprompt: userMessage,\n+\t\t\tmessage: text,\n+\t\t\tstart_workflow_json: currentWorkflowJson,\n+\t\t\tworkflow_id: workflowsStore.workflowId,\n \t\t});\n-\t\tresetBuilderChat();\n-\t\tconst id = getRandomId();\n \n-\t\taddUserMessage(userMessage, id);\n-\t\taddLoadingAssistantMessage(locale.baseText('aiAssistant.thinkingSteps.thinking'));\n-\t\topenChat();\n-\t\tstreaming.value = true;\n+\t\tprepareForStreaming(text, messageId);\n \n-\t\tconst payload: ChatRequest.InitBuilderChat = {\n-\t\t\trole: 'user',\n-\t\t\ttype: 'init-builder-chat',\n-\t\t\tuser: {\n-\t\t\t\tfirstName: usersStore.currentUser?.firstName ?? '',\n-\t\t\t},\n-\t\t\tquestion: userMessage,\n-\t\t};\n+\t\tconst executionResult = workflowsStore.workflowExecutionData?.data?.resultData;\n+\t\tconst payload = createBuilderPayload(text, {\n+\t\t\tquickReplyType,\n+\t\t\tworkflow: workflowsStore.workflow,\n+\t\t\texecutionData: executionResult,\n+\t\t\tnodesForSchema: Object.keys(workflowsStore.nodesByName),\n+\t\t});\n+\t\tconst retry = createRetryHandler(messageId, async () => sendChatMessage(options));\n \n-\t\tchatWithBuilder(\n-\t\t\trootStore.restApiContext,\n-\t\t\t{\n-\t\t\t\tpayload,\n-\t\t\t},\n-\t\t\t(msg) => onEachStreamingMessage(msg, id),\n-\t\t\t() => onDoneStreaming(),\n-\t\t\t(e) => handleServiceError(e, id, async () => await initBuilderChat(userMessage, 'chat')),\n-\t\t);\n+\t\ttry {\n+\t\t\tchatWithBuilder(\n+\t\t\t\trootStore.restApiContext,\n+\t\t\t\t{ payload },\n+\t\t\t\t(response) => {\n+\t\t\t\t\tconst result = processAssistantMessages(\n+\t\t\t\t\t\tchatMessages.value,\n+\t\t\t\t\t\tresponse.messages,\n+\t\t\t\t\t\tgenerateMessageId(),\n+\t\t\t\t\t);\n+\t\t\t\t\tchatMessages.value = result.messages;\n+\n+\t\t\t\t\tif (result.shouldClearThinking) {\n+\t\t\t\t\t\tassistantThinkingMessage.value = undefined;\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tif (result.thinkingMessage) {\n+\t\t\t\t\t\tassistantThinkingMessage.value = result.thinkingMessage;\n+\t\t\t\t\t}\n+\t\t\t\t},\n+\t\t\t\t() => stopStreaming(),\n+\t\t\t\t(e) => handleServiceError(e, messageId, retry),\n+\t\t\t);\n+\t\t} catch (e: unknown) {\n+\t\t\thandleServiceError(e, messageId, retry);\n+\t\t}\n \t}\n \n-\tasync function sendMessage(\n-\t\tchatMessage: Pick<ChatRequest.UserChatMessage, 'text' | 'quickReplyType'>,\n-\t) {\n-\t\tif (streaming.value) {\n-\t\t\treturn;\n+\t/**\n+\t * Loads the most recent chat session for the current workflow.\n+\t * Only loads if a workflow ID exists (not for new unsaved workflows).\n+\t * Replaces current chat messages entirely - does NOT merge with existing messages.\n+\t * Sessions are ordered by recency, so sessions[0] is always the latest.\n+\t * Silently fails and returns empty array on error to prevent UI disruption.\n+\t */\n+\tasync function loadSessions() {\n+\t\tconst workflowId = workflowsStore.workflowId;\n+\t\tif (!workflowId) {\n+\t\t\treturn [];\n \t\t}\n \n-\t\tconst id = getRandomId();\n+\t\ttry {\n+\t\t\tconst response = await getAiSessions(rootStore.restApiContext, workflowId);\n+\t\t\tconst sessions = response.sessions || [];\n+\n+\t\t\t// Load the most recent session if available\n+\t\t\tif (sessions.length > 0) {\n+\t\t\t\tconst latestSession = sessions[0];\n+\n+\t\t\t\t// Clear existing messages\n+\t\t\t\tchatMessages.value = clearMessages();\n+\n+\t\t\t\t// Convert and add messages from the session\n+\t\t\t\tconst convertedMessages = latestSession.messages\n+\t\t\t\t\t.map((msg) => {\n+\t\t\t\t\t\tconst id = generateMessageId();\n+\t\t\t\t\t\treturn mapAssistantMessageToUI(msg, id);\n+\t\t\t\t\t})\n+\t\t\t\t\t// Do not include wf updated messages from session\n+\t\t\t\t\t.filter((msg) => msg.type !== 'workflow-updated');\n+\n+\t\t\t\tchatMessages.value = convertedMessages;\n+\t\t\t}\n \n-\t\tconst retry = async () => {\n-\t\t\tchatMessages.value = chatMessages.value.filter((msg) => msg.id !== id);\n-\t\t\tawait sendMessage(chatMessage);\n+\t\t\treturn sessions;\n+\t\t} catch (error) {\n+\t\t\tconsole.error('Failed to load AI sessions:', error);\n+\t\t\treturn [];\n+\t\t}\n+\t}\n+\n+\tfunction captureCurrentWorkflowState() {\n+\t\tconst nodePositions = new Map<string, [number, number]>();\n+\t\tconst existingNodeIds = new Set<string>();\n+\n+\t\tworkflowsStore.allNodes.forEach((node) => {\n+\t\t\tnodePositions.set(node.id, [...node.position]);\n+\t\t\texistingNodeIds.add(node.id);\n+\t\t});\n+\n+\t\treturn {\n+\t\t\tnodePositions,\n+\t\t\texistingNodeIds,\n+\t\t\tcurrentWorkflowJson: JSON.stringify(pick(workflowsStore.workflow, ['nodes', 'connections'])),\n \t\t};\n+\t}\n \n+\tfunction applyWorkflowUpdate(workflowJson: string) {\n+\t\tlet workflowData: WorkflowDataUpdate;\n \t\ttry {\n-\t\t\taddUserMessage(chatMessage.text, id);\n-\t\t\taddLoadingAssistantMessage(locale.baseText('aiAssistant.thinkingSteps.thinking'));\n-\n-\t\t\tstreaming.value = true;\n-\t\t\tassert(currentSessionId.value);\n+\t\t\tworkflowData = jsonParse<WorkflowDataUpdate>(workflowJson);\n+\t\t} catch (error) {\n+\t\t\tconsole.error('Error parsing workflow data', error);",
    "repo_full_name": "n8n-io/n8n",
    "discussion_comments": [
      {
        "comment_id": "2216324020",
        "repo_full_name": "n8n-io/n8n",
        "pr_number": 17430,
        "pr_file": "packages/frontend/editor-ui/src/stores/builder.store.ts",
        "discussion_id": "2216324020",
        "commented_code": "@@ -106,236 +132,240 @@ export const useBuilderStore = defineStore(STORES.BUILDER, () => {\n \t\t}, ASK_AI_SLIDE_OUT_DURATION_MS + 50);\n \t}\n \n-\tfunction clearMessages() {\n-\t\tchatMessages.value = [];\n-\t}\n-\n+\t/**\n+\t * Updates chat panel width with enforced boundaries.\n+\t * Width is clamped between MIN_CHAT_WIDTH (330px) and MAX_CHAT_WIDTH (650px)\n+\t * to ensure usability on various screen sizes.\n+\t */\n \tfunction updateWindowWidth(width: number) {\n \t\tchatWidth.value = Math.min(Math.max(width, MIN_CHAT_WIDTH), MAX_CHAT_WIDTH);\n \t}\n \n \t// Message handling functions\n-\tfunction addAssistantMessages(newMessages: ChatRequest.MessageResponse[], id: string) {\n-\t\tconst read = true; // Always mark as read in builder\n-\t\tconst messages = [...chatMessages.value].filter(\n-\t\t\t(msg) => !(msg.id === id && msg.role === 'assistant'),\n-\t\t);\n-\t\tassistantThinkingMessage.value = undefined;\n-\n-\t\tnewMessages.forEach((msg) => {\n-\t\t\tif (msg.type === 'message') {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'text',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tcontent: msg.text,\n-\t\t\t\t\tquickReplies: msg.quickReplies,\n-\t\t\t\t\tcodeSnippet: msg.codeSnippet,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'workflow-step' && 'steps' in msg) {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'workflow-step',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tsteps: msg.steps,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'prompt-validation' && !msg.isWorkflowPrompt) {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\ttype: 'error',\n-\t\t\t\t\tcontent: locale.baseText('aiAssistant.builder.invalidPrompt'),\n-\t\t\t\t\tread: true,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'workflow-node' && 'nodes' in msg) {\n-\t\t\t\tconst mappedNodes = msg.nodes.map(\n-\t\t\t\t\t(node) => nodeTypesStore.getNodeType(node)?.displayName ?? node,\n-\t\t\t\t);\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'workflow-node',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tnodes: mappedNodes,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'workflow-composed' && 'nodes' in msg) {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'workflow-composed',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tnodes: msg.nodes,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'workflow-generated' && 'codeSnippet' in msg) {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'workflow-generated',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tcodeSnippet: msg.codeSnippet,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t} else if (msg.type === 'rate-workflow') {\n-\t\t\t\tmessages.push({\n-\t\t\t\t\tid,\n-\t\t\t\t\ttype: 'rate-workflow',\n-\t\t\t\t\trole: 'assistant',\n-\t\t\t\t\tcontent: msg.content,\n-\t\t\t\t\tread,\n-\t\t\t\t});\n-\t\t\t}\n-\t\t});\n-\t\tchatMessages.value = messages;\n-\t}\n-\n-\tfunction addAssistantError(content: string, id: string, retry?: () => Promise<void>) {\n-\t\tchatMessages.value.push({\n-\t\t\tid,\n-\t\t\trole: 'assistant',\n-\t\t\ttype: 'error',\n-\t\t\tcontent,\n-\t\t\tread: true,\n-\t\t\tretry,\n-\t\t});\n-\t}\n-\n \tfunction addLoadingAssistantMessage(message: string) {\n \t\tassistantThinkingMessage.value = message;\n \t}\n \n-\tfunction addUserMessage(content: string, id: string) {\n-\t\tchatMessages.value.push({\n-\t\t\tid,\n-\t\t\trole: 'user',\n-\t\t\ttype: 'text',\n-\t\t\tcontent,\n-\t\t\tread: true,\n-\t\t});\n-\t}\n-\n \tfunction stopStreaming() {\n \t\tstreaming.value = false;\n \t}\n \n \t// Error handling\n+\t/**\n+\t * Handles streaming errors by creating an error message with optional retry capability.\n+\t * Cleans up streaming state and removes the thinking indicator.\n+\t * The retry function, if provided, will remove the error message before retrying.\n+\t * Tracks error telemetry\n+\t */\n \tfunction handleServiceError(e: unknown, id: string, retry?: () => Promise<void>) {\n \t\tassert(e instanceof Error);\n+\n \t\tstopStreaming();\n \t\tassistantThinkingMessage.value = undefined;\n-\t\taddAssistantError(\n+\n+\t\tconst errorMessage = createErrorMessage(\n \t\t\tlocale.baseText('aiAssistant.serviceError.message', { interpolate: { message: e.message } }),\n \t\t\tid,\n \t\t\tretry,\n \t\t);\n+\t\tchatMessages.value = [...chatMessages.value, errorMessage];\n+\n \t\ttelemetry.track('Workflow generation errored', {\n \t\t\terror: e.message,\n-\t\t\tprompt: workflowPrompt.value,\n+\t\t\tworkflow_id: workflowsStore.workflowId,\n \t\t});\n \t}\n \n-\t// API interaction\n-\tfunction getRandomId() {\n-\t\treturn `${Math.floor(Math.random() * 100000000)}`;\n+\t// Helper functions\n+\t/**\n+\t * Prepares UI for incoming streaming response.\n+\t * Adds user message immediately for visual feedback, shows thinking indicator,\n+\t * and ensures chat is open. Called before initiating API request to minimize\n+\t * perceived latency.\n+\t */\n+\tfunction prepareForStreaming(userMessage: string, messageId: string) {\n+\t\tconst userMsg = createUserMessage(userMessage, messageId);\n+\t\tchatMessages.value = [...chatMessages.value, userMsg];\n+\t\taddLoadingAssistantMessage(locale.baseText('aiAssistant.thinkingSteps.thinking'));\n+\t\tstreaming.value = true;\n+\t}\n+\n+\t/**\n+\t * Creates a retry function that removes the associated error message before retrying.\n+\t * This ensures the chat doesn't accumulate multiple error messages for the same failure.\n+\t * The messageId parameter refers to the error message to remove, not the original user message.\n+\t */\n+\tfunction createRetryHandler(messageId: string, retryFn: () => Promise<void>) {\n+\t\treturn async () => {\n+\t\t\t// Remove the error message before retrying\n+\t\t\tchatMessages.value = chatMessages.value.filter((msg) => msg.id !== messageId);\n+\t\t\tawait retryFn();\n+\t\t};\n \t}\n \n-\tfunction onEachStreamingMessage(response: ChatRequest.ResponsePayload, id: string) {\n-\t\tif (response.sessionId && !currentSessionId.value) {\n-\t\t\tcurrentSessionId.value = response.sessionId;\n-\t\t\ttelemetry.track('Assistant session started', {\n-\t\t\t\tchat_session_id: currentSessionId.value,\n-\t\t\t\ttask: 'workflow-generation',\n-\t\t\t});\n-\t\t} else if (currentSessionId.value !== response.sessionId) {\n-\t\t\t// Ignore messages from other sessions\n+\t// Core API functions\n+\t/**\n+\t * Sends a message to the AI builder service and handles the streaming response.\n+\t * Prevents concurrent requests by checking streaming state.\n+\t * Captures workflow state before sending for comparison in telemetry.\n+\t * Creates a retry handler that preserves the original message context.\n+\t * Note: This function is NOT async - streaming happens via callbacks.\n+\t */\n+\tfunction sendChatMessage(options: {\n+\t\ttext: string;\n+\t\tsource?: 'chat' | 'canvas';\n+\t\tquickReplyType?: string;\n+\t}) {\n+\t\tif (streaming.value) {\n \t\t\treturn;\n \t\t}\n-\t\taddAssistantMessages(response.messages, id);\n-\t}\n \n-\tfunction onDoneStreaming() {\n-\t\tstopStreaming();\n-\t}\n+\t\tconst { text, source = 'chat', quickReplyType } = options;\n+\t\tconst messageId = generateMessageId();\n \n-\t// Core API functions\n-\tasync function initBuilderChat(userMessage: string, source: 'chat' | 'canvas') {\n-\t\ttelemetry.track('User submitted workflow prompt', {\n+\t\tconst currentWorkflowJson = getWorkflowSnapshot();\n+\t\ttelemetry.track('User submitted builder message', {\n \t\t\tsource,\n-\t\t\tprompt: userMessage,\n+\t\t\tmessage: text,\n+\t\t\tstart_workflow_json: currentWorkflowJson,\n+\t\t\tworkflow_id: workflowsStore.workflowId,\n \t\t});\n-\t\tresetBuilderChat();\n-\t\tconst id = getRandomId();\n \n-\t\taddUserMessage(userMessage, id);\n-\t\taddLoadingAssistantMessage(locale.baseText('aiAssistant.thinkingSteps.thinking'));\n-\t\topenChat();\n-\t\tstreaming.value = true;\n+\t\tprepareForStreaming(text, messageId);\n \n-\t\tconst payload: ChatRequest.InitBuilderChat = {\n-\t\t\trole: 'user',\n-\t\t\ttype: 'init-builder-chat',\n-\t\t\tuser: {\n-\t\t\t\tfirstName: usersStore.currentUser?.firstName ?? '',\n-\t\t\t},\n-\t\t\tquestion: userMessage,\n-\t\t};\n+\t\tconst executionResult = workflowsStore.workflowExecutionData?.data?.resultData;\n+\t\tconst payload = createBuilderPayload(text, {\n+\t\t\tquickReplyType,\n+\t\t\tworkflow: workflowsStore.workflow,\n+\t\t\texecutionData: executionResult,\n+\t\t\tnodesForSchema: Object.keys(workflowsStore.nodesByName),\n+\t\t});\n+\t\tconst retry = createRetryHandler(messageId, async () => sendChatMessage(options));\n \n-\t\tchatWithBuilder(\n-\t\t\trootStore.restApiContext,\n-\t\t\t{\n-\t\t\t\tpayload,\n-\t\t\t},\n-\t\t\t(msg) => onEachStreamingMessage(msg, id),\n-\t\t\t() => onDoneStreaming(),\n-\t\t\t(e) => handleServiceError(e, id, async () => await initBuilderChat(userMessage, 'chat')),\n-\t\t);\n+\t\ttry {\n+\t\t\tchatWithBuilder(\n+\t\t\t\trootStore.restApiContext,\n+\t\t\t\t{ payload },\n+\t\t\t\t(response) => {\n+\t\t\t\t\tconst result = processAssistantMessages(\n+\t\t\t\t\t\tchatMessages.value,\n+\t\t\t\t\t\tresponse.messages,\n+\t\t\t\t\t\tgenerateMessageId(),\n+\t\t\t\t\t);\n+\t\t\t\t\tchatMessages.value = result.messages;\n+\n+\t\t\t\t\tif (result.shouldClearThinking) {\n+\t\t\t\t\t\tassistantThinkingMessage.value = undefined;\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tif (result.thinkingMessage) {\n+\t\t\t\t\t\tassistantThinkingMessage.value = result.thinkingMessage;\n+\t\t\t\t\t}\n+\t\t\t\t},\n+\t\t\t\t() => stopStreaming(),\n+\t\t\t\t(e) => handleServiceError(e, messageId, retry),\n+\t\t\t);\n+\t\t} catch (e: unknown) {\n+\t\t\thandleServiceError(e, messageId, retry);\n+\t\t}\n \t}\n \n-\tasync function sendMessage(\n-\t\tchatMessage: Pick<ChatRequest.UserChatMessage, 'text' | 'quickReplyType'>,\n-\t) {\n-\t\tif (streaming.value) {\n-\t\t\treturn;\n+\t/**\n+\t * Loads the most recent chat session for the current workflow.\n+\t * Only loads if a workflow ID exists (not for new unsaved workflows).\n+\t * Replaces current chat messages entirely - does NOT merge with existing messages.\n+\t * Sessions are ordered by recency, so sessions[0] is always the latest.\n+\t * Silently fails and returns empty array on error to prevent UI disruption.\n+\t */\n+\tasync function loadSessions() {\n+\t\tconst workflowId = workflowsStore.workflowId;\n+\t\tif (!workflowId) {\n+\t\t\treturn [];\n \t\t}\n \n-\t\tconst id = getRandomId();\n+\t\ttry {\n+\t\t\tconst response = await getAiSessions(rootStore.restApiContext, workflowId);\n+\t\t\tconst sessions = response.sessions || [];\n+\n+\t\t\t// Load the most recent session if available\n+\t\t\tif (sessions.length > 0) {\n+\t\t\t\tconst latestSession = sessions[0];\n+\n+\t\t\t\t// Clear existing messages\n+\t\t\t\tchatMessages.value = clearMessages();\n+\n+\t\t\t\t// Convert and add messages from the session\n+\t\t\t\tconst convertedMessages = latestSession.messages\n+\t\t\t\t\t.map((msg) => {\n+\t\t\t\t\t\tconst id = generateMessageId();\n+\t\t\t\t\t\treturn mapAssistantMessageToUI(msg, id);\n+\t\t\t\t\t})\n+\t\t\t\t\t// Do not include wf updated messages from session\n+\t\t\t\t\t.filter((msg) => msg.type !== 'workflow-updated');\n+\n+\t\t\t\tchatMessages.value = convertedMessages;\n+\t\t\t}\n \n-\t\tconst retry = async () => {\n-\t\t\tchatMessages.value = chatMessages.value.filter((msg) => msg.id !== id);\n-\t\t\tawait sendMessage(chatMessage);\n+\t\t\treturn sessions;\n+\t\t} catch (error) {\n+\t\t\tconsole.error('Failed to load AI sessions:', error);\n+\t\t\treturn [];\n+\t\t}\n+\t}\n+\n+\tfunction captureCurrentWorkflowState() {\n+\t\tconst nodePositions = new Map<string, [number, number]>();\n+\t\tconst existingNodeIds = new Set<string>();\n+\n+\t\tworkflowsStore.allNodes.forEach((node) => {\n+\t\t\tnodePositions.set(node.id, [...node.position]);\n+\t\t\texistingNodeIds.add(node.id);\n+\t\t});\n+\n+\t\treturn {\n+\t\t\tnodePositions,\n+\t\t\texistingNodeIds,\n+\t\t\tcurrentWorkflowJson: JSON.stringify(pick(workflowsStore.workflow, ['nodes', 'connections'])),\n \t\t};\n+\t}\n \n+\tfunction applyWorkflowUpdate(workflowJson: string) {\n+\t\tlet workflowData: WorkflowDataUpdate;\n \t\ttry {\n-\t\t\taddUserMessage(chatMessage.text, id);\n-\t\t\taddLoadingAssistantMessage(locale.baseText('aiAssistant.thinkingSteps.thinking'));\n-\n-\t\t\tstreaming.value = true;\n-\t\t\tassert(currentSessionId.value);\n+\t\t\tworkflowData = jsonParse<WorkflowDataUpdate>(workflowJson);\n+\t\t} catch (error) {\n+\t\t\tconsole.error('Error parsing workflow data', error);",
        "comment_created_at": "2025-07-18T15:21:09+00:00",
        "comment_author": "mutdmour",
        "comment_body": "We should avoid `console.error` as we expect users to use the browser's console for code node and http request node. Could we surface those in the UI somehow instead to help with debugging?",
        "pr_file_module": null
      }
    ]
  }
]