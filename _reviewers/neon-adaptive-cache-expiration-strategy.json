[
  {
    "discussion_id": "2005190467",
    "pr_number": 11294,
    "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
    "created_at": "2025-03-20T09:44:45+00:00",
    "commented_code": "+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2005190467",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2005190467",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.",
        "comment_created_at": "2025-03-20T09:44:45+00:00",
        "comment_author": "VladLazar",
        "comment_body": "The compute is available during pre-warm, correct? Wondering if there's any interactions between the user workload and prewarm that are worth considering.",
        "pr_file_module": null
      },
      {
        "comment_id": "2005309605",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2005190467",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.",
        "comment_created_at": "2025-03-20T10:51:37+00:00",
        "comment_author": "mtyazici",
        "comment_body": "Yes, I was wondering this as well. Will we try to pre-warm computes when not restarting a compute; for example compute start due to a proxy connection. \r\n\r\nIf compute is not available during pre-warm the answer is definitely no but if it's available then the LFC cache might have become stale over time. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2006500157",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2005190467",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.",
        "comment_created_at": "2025-03-20T21:41:48+00:00",
        "comment_author": "ololobus",
        "comment_body": "Replied here https://github.com/neondatabase/neon/pull/11294#discussion_r2006276800 and here https://github.com/neondatabase/neon/pull/11294#discussion_r2006283261 as well\r\n\r\n> Wondering if there's any interactions between the user workload and prewarm that are worth considering.\r\n> if it's available then the LFC cache might have become stale over time.\r\n\r\nThis PR https://github.com/neondatabase/neon/pull/10442 introduced additional locking when accessing LFC, so it's now considered safe to write there concurrently, so that's the base for all this work.\r\n\r\nDuring prefetch, we always request the latest pages from the pageserver. If, after loading page gets modified, then it will be either updated in LFC (in case of primary) or evicted from LFC after receiving the corresponding WAL record (in case of replica). In other words, if pages is not present in the LFC, we will fetch it from the pageserver; if someone (backend, normal client workload) tries to write it concurrently, then the access will be synchronized, and we should still get a freshness guarantee. @knizhnik or @MMeent can correct me, as I'm not fluent in the underlying mechanism, I consider it as given here\r\n\r\nThus, it should be safe to prewarm LFC concurrently with user load. The only problem is performance, I wrote about it in other comments, but anyway. Yes, if it's highly intensive workload, then prewarm can compete for storage resources with user workload, so we can consider auto-prewarm to be user-togglable feature, I wrote about it in the section about auto-prewarm concerns\r\n\r\n@VladLazar @mtyazici let me know if it makes it clearer",
        "pr_file_module": null
      },
      {
        "comment_id": "2012172123",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2005190467",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.",
        "comment_created_at": "2025-03-25T13:57:42+00:00",
        "comment_author": "MMeent",
        "comment_body": "> or evicted from LFC after receiving the corresponding WAL record (in case of replica).\r\n\r\nThis is incorrect: WAL will be replayed for every page that's currently in the LFC or shared buffers.\r\n\r\n> Wondering if there's any interactions between the user workload and prewarm that are worth considering.\r\n> if it's available then the LFC cache might have become stale over time.\r\n\r\nThe LFC can become stale, but only if the main page is still in shared buffers. The (modified) page from buffers will at some point be written, which will make the LFC lose its stale-ness.\r\n\r\nIn any case, the stale-ness of a page in LFC doesn't (shouldn't) matter for this RFC.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2084933689",
    "pr_number": 11294,
    "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
    "created_at": "2025-05-12T15:25:15+00:00",
    "commented_code": "+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.\n+\n+6. Any unexpected failure during auto-prewarm. This **failure mustn't be fatal**,\n+    `compute_ctl` has to report the failure, but do not crash the compute.\n+\n+7. Control plane failed to confirm that old primary has terminated. This can happen, especially\n+    in the future HA setup. In this case, control plane has to ensure that it sent VM deletion\n+    and pod termination requests to k8s, so long-term we do not have two running primaries\n+    on the same timeline.\n+\n+### Security implications\n+\n+There are two security implications to consider:\n+\n+1. Access to `compute_ctl` API. It has to be accessible from the outside of compute, so all\n+    new API methods have to be exposed on the **external** HTTP port and **must** be authenticated\n+    with JWT.\n+\n+2. Read/write only your own LFC state data in S3. Although it's not really a security concern,\n+    since LFC state is just a mapping of blocks present in LFC at certain moment in time;\n+    it still has to be highly restricted, so that i) only computes on the same timeline can\n+    read S3 state; ii) each compute can only write to the path that contains it's `endpoint_id`.\n+    Both of this must be validated by S3 proxy and JWT token used by `compute_ctl`.\n+\n+### Unresolved questions\n+\n+#### Billing, metrics and monitoring\n+\n+Currently, we only label computes with `endpoint_id` after attaching them to the endpoint.\n+In this proposal, this means that temporary replica will remain unlabelled until it's promoted\n+to primary. We can also hide it from users in the control plane API, but what to do with\n+billing and monitoring is still unclear.\n+\n+We can probably mark it as 'billable' and tag with `project_id`, so it will be billed, but\n+not interfere in any way with the current primary monitoring.\n+\n+Another thing to consider is how logs and metrics export will switch to the new compute.\n+It's expected that OpenTelemetry collector will auto-discover the new compute and start\n+scraping metrics from it.\n+\n+#### Auto-prewarm\n+\n+It's still an open question whether we need auto-prewarm at all. The author's gut-feeling is\n+that yes, we need it, but might be not for all workloads, so it could end up exposed as a\n+user-controllable knob on the endpoint. There are two arguments for that:\n+\n+1. Auto-prewarm existing in upstream's `pg_prewarm`, _probably for a reason_.\n+\n+2. There are still could be 2 flows when we cannot perform the rolling restart via the warm\n+    replica: i) any failure or interruption during promotion; ii) wake up after scale-to-zero.\n+    The latter might be challenged as well, i.e. one can argue that auto-prewarm may and will\n+    compete with user-workload for storage resources. This is correct, but it might as well\n+    reduce the time to get warm LFC and good performance.\n+\n+#### Low-level details of the replica promotion\n+\n+There are many things to consider here, but three items just off the top of my head:\n+\n+1. How to properly start the `walproposer` inside Postgres.\n+\n+2. What to do with logical replication. Currently, we do not include logical replication slots\n+    inside basebackup, because nobody advances them at replica, so they just prevent the WAL\n+    deletion. Yet, we do need to have them at primary after promotion. Starting with Postgres 17,\n+    there is a new feature called\n+    [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html)\n+    and `synchronized_standby_slots` setting, but we need a plan for the older versions. Should we\n+    request a new basebackup during promotion?\n+\n+3. How do we guarantee that replica will receive all the latest WAL from safekeepers? Do some\n+    'shallow' version of sync safekeepers without data copying? Or just a standard version of\n+    sync safekeepers?\n+\n+## Alternative implementation\n+\n+The proposal already assumes one of the alternatives -- do not have any persistent storage for\n+LFC state. This is possible to implement faster with the proposed API, but it means that\n+we do not implement auto-prewarm yet.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2084933689",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2084933689",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.\n+\n+6. Any unexpected failure during auto-prewarm. This **failure mustn't be fatal**,\n+    `compute_ctl` has to report the failure, but do not crash the compute.\n+\n+7. Control plane failed to confirm that old primary has terminated. This can happen, especially\n+    in the future HA setup. In this case, control plane has to ensure that it sent VM deletion\n+    and pod termination requests to k8s, so long-term we do not have two running primaries\n+    on the same timeline.\n+\n+### Security implications\n+\n+There are two security implications to consider:\n+\n+1. Access to `compute_ctl` API. It has to be accessible from the outside of compute, so all\n+    new API methods have to be exposed on the **external** HTTP port and **must** be authenticated\n+    with JWT.\n+\n+2. Read/write only your own LFC state data in S3. Although it's not really a security concern,\n+    since LFC state is just a mapping of blocks present in LFC at certain moment in time;\n+    it still has to be highly restricted, so that i) only computes on the same timeline can\n+    read S3 state; ii) each compute can only write to the path that contains it's `endpoint_id`.\n+    Both of this must be validated by S3 proxy and JWT token used by `compute_ctl`.\n+\n+### Unresolved questions\n+\n+#### Billing, metrics and monitoring\n+\n+Currently, we only label computes with `endpoint_id` after attaching them to the endpoint.\n+In this proposal, this means that temporary replica will remain unlabelled until it's promoted\n+to primary. We can also hide it from users in the control plane API, but what to do with\n+billing and monitoring is still unclear.\n+\n+We can probably mark it as 'billable' and tag with `project_id`, so it will be billed, but\n+not interfere in any way with the current primary monitoring.\n+\n+Another thing to consider is how logs and metrics export will switch to the new compute.\n+It's expected that OpenTelemetry collector will auto-discover the new compute and start\n+scraping metrics from it.\n+\n+#### Auto-prewarm\n+\n+It's still an open question whether we need auto-prewarm at all. The author's gut-feeling is\n+that yes, we need it, but might be not for all workloads, so it could end up exposed as a\n+user-controllable knob on the endpoint. There are two arguments for that:\n+\n+1. Auto-prewarm existing in upstream's `pg_prewarm`, _probably for a reason_.\n+\n+2. There are still could be 2 flows when we cannot perform the rolling restart via the warm\n+    replica: i) any failure or interruption during promotion; ii) wake up after scale-to-zero.\n+    The latter might be challenged as well, i.e. one can argue that auto-prewarm may and will\n+    compete with user-workload for storage resources. This is correct, but it might as well\n+    reduce the time to get warm LFC and good performance.\n+\n+#### Low-level details of the replica promotion\n+\n+There are many things to consider here, but three items just off the top of my head:\n+\n+1. How to properly start the `walproposer` inside Postgres.\n+\n+2. What to do with logical replication. Currently, we do not include logical replication slots\n+    inside basebackup, because nobody advances them at replica, so they just prevent the WAL\n+    deletion. Yet, we do need to have them at primary after promotion. Starting with Postgres 17,\n+    there is a new feature called\n+    [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html)\n+    and `synchronized_standby_slots` setting, but we need a plan for the older versions. Should we\n+    request a new basebackup during promotion?\n+\n+3. How do we guarantee that replica will receive all the latest WAL from safekeepers? Do some\n+    'shallow' version of sync safekeepers without data copying? Or just a standard version of\n+    sync safekeepers?\n+\n+## Alternative implementation\n+\n+The proposal already assumes one of the alternatives -- do not have any persistent storage for\n+LFC state. This is possible to implement faster with the proposed API, but it means that\n+we do not implement auto-prewarm yet.",
        "comment_created_at": "2025-05-12T15:25:15+00:00",
        "comment_author": "dimitri",
        "comment_body": "Two alternative approaches for prewarming LFC:\r\n\r\n  1. Logical Replication custom messages and replay\r\n  2. Standby-driven sync of cache\r\n\r\nSome details now.\r\n\r\n### Logical Replication\r\n\r\nThe idea is to use the function https://pgpedia.info/p/pg_logical_emit_message.html to send messages using our own textual representation of the LFC cache down to the logical replication stream. We can receive and replay these messages from a Rust thread in compute_ctl, the replay action consists of driving the local LFC contents on the standby. It could probably also be implemented as a Postgres extension that consumes and replays messages from the Logical Replication stream.\r\n\r\n### Client driven LFC sync maintenance\r\n\r\nThe idea here is to expose some LFC primitives at the SQL level on the primary. The API should allow for fetching the current state of the LFC in a way that it can be reproduced, and deriving a diff from the previous state/snapshot, in a way that allows applying the diff once retrieved.\r\n\r\n### Common aspects: LFC contents representation, computing diffs\r\n\r\nIn both cases it seems to me that it would be a good thing to have a new Postgres data-type included in the Neon extension to represent the LFC state/snapshot, implement some operations on it, and expose a basic API including the ability to compute a diff for incremental maintenance.\r\n\r\nData type names could be `neonlfc` and and `neonlfcdiff` (think timestamp and interval) would have an internal binary representation and a JSON based textual representation (I suppose that's the most useful here), and expose some operators such as `+` for an union operation, `-` for computing a diff, and functions such as `neonlfc_apply()` that could accept either data type and act accordingly.\r\n\r\n### Bottom-up approach\r\n\r\nEquiped with this low-level functionality it's easier to reason about LFC maintenance and multi-nodes syncing and how to drive it. Could now be done in a single SQL statement where a part runs on the primary (thanks to remote function execution of some kind, I used to like PL/proxy, I think Postgres should have FOREIGN FUNCTION but it does not at the moment) and another part runs on the local node ; even though it would still probably require client code (PL/pgSQL could do) to implement fetching the data separately from applying it locally, as two SQL queries now.",
        "pr_file_module": null
      },
      {
        "comment_id": "2124375600",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2084933689",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.\n+\n+6. Any unexpected failure during auto-prewarm. This **failure mustn't be fatal**,\n+    `compute_ctl` has to report the failure, but do not crash the compute.\n+\n+7. Control plane failed to confirm that old primary has terminated. This can happen, especially\n+    in the future HA setup. In this case, control plane has to ensure that it sent VM deletion\n+    and pod termination requests to k8s, so long-term we do not have two running primaries\n+    on the same timeline.\n+\n+### Security implications\n+\n+There are two security implications to consider:\n+\n+1. Access to `compute_ctl` API. It has to be accessible from the outside of compute, so all\n+    new API methods have to be exposed on the **external** HTTP port and **must** be authenticated\n+    with JWT.\n+\n+2. Read/write only your own LFC state data in S3. Although it's not really a security concern,\n+    since LFC state is just a mapping of blocks present in LFC at certain moment in time;\n+    it still has to be highly restricted, so that i) only computes on the same timeline can\n+    read S3 state; ii) each compute can only write to the path that contains it's `endpoint_id`.\n+    Both of this must be validated by S3 proxy and JWT token used by `compute_ctl`.\n+\n+### Unresolved questions\n+\n+#### Billing, metrics and monitoring\n+\n+Currently, we only label computes with `endpoint_id` after attaching them to the endpoint.\n+In this proposal, this means that temporary replica will remain unlabelled until it's promoted\n+to primary. We can also hide it from users in the control plane API, but what to do with\n+billing and monitoring is still unclear.\n+\n+We can probably mark it as 'billable' and tag with `project_id`, so it will be billed, but\n+not interfere in any way with the current primary monitoring.\n+\n+Another thing to consider is how logs and metrics export will switch to the new compute.\n+It's expected that OpenTelemetry collector will auto-discover the new compute and start\n+scraping metrics from it.\n+\n+#### Auto-prewarm\n+\n+It's still an open question whether we need auto-prewarm at all. The author's gut-feeling is\n+that yes, we need it, but might be not for all workloads, so it could end up exposed as a\n+user-controllable knob on the endpoint. There are two arguments for that:\n+\n+1. Auto-prewarm existing in upstream's `pg_prewarm`, _probably for a reason_.\n+\n+2. There are still could be 2 flows when we cannot perform the rolling restart via the warm\n+    replica: i) any failure or interruption during promotion; ii) wake up after scale-to-zero.\n+    The latter might be challenged as well, i.e. one can argue that auto-prewarm may and will\n+    compete with user-workload for storage resources. This is correct, but it might as well\n+    reduce the time to get warm LFC and good performance.\n+\n+#### Low-level details of the replica promotion\n+\n+There are many things to consider here, but three items just off the top of my head:\n+\n+1. How to properly start the `walproposer` inside Postgres.\n+\n+2. What to do with logical replication. Currently, we do not include logical replication slots\n+    inside basebackup, because nobody advances them at replica, so they just prevent the WAL\n+    deletion. Yet, we do need to have them at primary after promotion. Starting with Postgres 17,\n+    there is a new feature called\n+    [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html)\n+    and `synchronized_standby_slots` setting, but we need a plan for the older versions. Should we\n+    request a new basebackup during promotion?\n+\n+3. How do we guarantee that replica will receive all the latest WAL from safekeepers? Do some\n+    'shallow' version of sync safekeepers without data copying? Or just a standard version of\n+    sync safekeepers?\n+\n+## Alternative implementation\n+\n+The proposal already assumes one of the alternatives -- do not have any persistent storage for\n+LFC state. This is possible to implement faster with the proposed API, but it means that\n+we do not implement auto-prewarm yet.",
        "comment_created_at": "2025-06-03T16:26:52+00:00",
        "comment_author": "myrrc",
        "comment_body": "We already have a service for prewarm (endpoint_storage) in neon/",
        "pr_file_module": null
      },
      {
        "comment_id": "2178231888",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2084933689",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid\n+  sequenceDiagram\n+\n+  autonumber\n+\n+  participant proxy as Neon proxy\n+\n+  participant cplane as Control plane\n+\n+  participant primary as Compute (primary)\n+  box Compute (replica)\n+    participant ctl as compute_ctl\n+    participant pg as Postgres\n+  end\n+\n+  box Endpoint unlogged storage\n+    participant s3proxy as Object storage proxy\n+    participant s3 as S3/ABS/etc\n+  end\n+\n+\n+  cplane ->> primary: POST /store_lfc_state\n+  primary -->> cplane: 200 OK\n+\n+  cplane ->> ctl: POST /restore_lfc_state\n+  activate ctl\n+  ctl -->> cplane: 202 Accepted\n+\n+  activate cplane\n+  cplane ->> ctl: GET /status: poll prewarm status\n+  ctl ->> s3proxy: GET /read_file\n+  s3proxy ->> s3: read file\n+  s3 -->> s3proxy: file content\n+  s3proxy -->> ctl: 200 OK: file content\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+  cplane -->> proxy: 200 OK: old primary conninfo\n+\n+  ctl ->> pg: prewarm LFC\n+  activate pg\n+  pg -->> ctl: prewarm is completed\n+  deactivate pg\n+\n+  ctl -->> cplane: 200 OK: prewarm is completed\n+  deactivate ctl\n+  deactivate cplane\n+\n+  cplane -->> cplane: reassign replica compute to endpoint,<br>start terminating the old primary compute\n+  activate cplane\n+  cplane ->> proxy: invalidate caches\n+\n+  proxy ->> cplane: GET /proxy_wake_compute\n+\n+  cplane -x primary: POST /terminate\n+  primary -->> cplane: 200 OK\n+  note over primary: old primary<br>compute terminated\n+\n+  cplane ->> ctl: POST /promote\n+  activate ctl\n+  ctl ->> pg: pg_ctl promote\n+  activate pg\n+  pg -->> ctl: done\n+  deactivate pg\n+  ctl -->> cplane: 200 OK\n+  deactivate ctl\n+\n+  cplane -->> cplane: finalize operation\n+  cplane -->> proxy: 200 OK: new primary conninfo\n+  deactivate cplane\n+```\n+\n+### Reliability, failure modes and corner cases\n+\n+We consider following failures while implementing this RFC:\n+\n+1. Compute got interrupted/crashed/restarted during prewarm. The caller -- control plane -- should\n+    detect that and start prewarm from the beginning.\n+\n+2. Control plane promotion request timed out or hit network issues. If it never reached the\n+    compute, control plane should just repeat it. If it did reach the compute, then during\n+    retry control plane can hit `409` as previous request triggered the promotion already.\n+    In this case, control plane need to retry until either `200` or\n+    permanent error `500` is returned.\n+\n+3. Compute got interrupted/crashed/restarted during promotion. At restart it will ask for\n+    a spec from control plane, and its content should signal compute to start as **primary**,\n+    so it's expected that control plane will continue polling for certain period of time and\n+    will discover that compute is ready to accept connections if restart is fast enough.\n+\n+4. Any other unexpected failure or timeout during prewarming. This **failure mustn't be fatal**,\n+    control plane has to report failure, terminate replica and keep primary running.\n+\n+5. Any other unexpected failure or timeout during promotion. Unfortunately, at this moment\n+    we already have the primary node stopped, so the only option is to start primary again\n+    and proceed with auto-prewarm.\n+\n+6. Any unexpected failure during auto-prewarm. This **failure mustn't be fatal**,\n+    `compute_ctl` has to report the failure, but do not crash the compute.\n+\n+7. Control plane failed to confirm that old primary has terminated. This can happen, especially\n+    in the future HA setup. In this case, control plane has to ensure that it sent VM deletion\n+    and pod termination requests to k8s, so long-term we do not have two running primaries\n+    on the same timeline.\n+\n+### Security implications\n+\n+There are two security implications to consider:\n+\n+1. Access to `compute_ctl` API. It has to be accessible from the outside of compute, so all\n+    new API methods have to be exposed on the **external** HTTP port and **must** be authenticated\n+    with JWT.\n+\n+2. Read/write only your own LFC state data in S3. Although it's not really a security concern,\n+    since LFC state is just a mapping of blocks present in LFC at certain moment in time;\n+    it still has to be highly restricted, so that i) only computes on the same timeline can\n+    read S3 state; ii) each compute can only write to the path that contains it's `endpoint_id`.\n+    Both of this must be validated by S3 proxy and JWT token used by `compute_ctl`.\n+\n+### Unresolved questions\n+\n+#### Billing, metrics and monitoring\n+\n+Currently, we only label computes with `endpoint_id` after attaching them to the endpoint.\n+In this proposal, this means that temporary replica will remain unlabelled until it's promoted\n+to primary. We can also hide it from users in the control plane API, but what to do with\n+billing and monitoring is still unclear.\n+\n+We can probably mark it as 'billable' and tag with `project_id`, so it will be billed, but\n+not interfere in any way with the current primary monitoring.\n+\n+Another thing to consider is how logs and metrics export will switch to the new compute.\n+It's expected that OpenTelemetry collector will auto-discover the new compute and start\n+scraping metrics from it.\n+\n+#### Auto-prewarm\n+\n+It's still an open question whether we need auto-prewarm at all. The author's gut-feeling is\n+that yes, we need it, but might be not for all workloads, so it could end up exposed as a\n+user-controllable knob on the endpoint. There are two arguments for that:\n+\n+1. Auto-prewarm existing in upstream's `pg_prewarm`, _probably for a reason_.\n+\n+2. There are still could be 2 flows when we cannot perform the rolling restart via the warm\n+    replica: i) any failure or interruption during promotion; ii) wake up after scale-to-zero.\n+    The latter might be challenged as well, i.e. one can argue that auto-prewarm may and will\n+    compete with user-workload for storage resources. This is correct, but it might as well\n+    reduce the time to get warm LFC and good performance.\n+\n+#### Low-level details of the replica promotion\n+\n+There are many things to consider here, but three items just off the top of my head:\n+\n+1. How to properly start the `walproposer` inside Postgres.\n+\n+2. What to do with logical replication. Currently, we do not include logical replication slots\n+    inside basebackup, because nobody advances them at replica, so they just prevent the WAL\n+    deletion. Yet, we do need to have them at primary after promotion. Starting with Postgres 17,\n+    there is a new feature called\n+    [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html)\n+    and `synchronized_standby_slots` setting, but we need a plan for the older versions. Should we\n+    request a new basebackup during promotion?\n+\n+3. How do we guarantee that replica will receive all the latest WAL from safekeepers? Do some\n+    'shallow' version of sync safekeepers without data copying? Or just a standard version of\n+    sync safekeepers?\n+\n+## Alternative implementation\n+\n+The proposal already assumes one of the alternatives -- do not have any persistent storage for\n+LFC state. This is possible to implement faster with the proposed API, but it means that\n+we do not implement auto-prewarm yet.",
        "comment_created_at": "2025-07-01T17:56:37+00:00",
        "comment_author": "ololobus",
        "comment_body": "> Logical Replication\r\n\r\nI recall that Konstantin did a POC like that. We discarded that because it only helps with keeping a warm replica, and it's not possible to implement autoprewarm with that + it bloats the WAL on safekeepers and eats the network bandwidth. It's not a big problem, as Pageservers should discard such records during ingestion, so it wont bloat the data files, but it's still nice to avoid\r\n\r\n> The idea here is to expose some LFC primitives at the SQL level on the primary. The API should allow for fetching the current state of the LFC in a way that it can be reproduced\r\n\r\nI don't remember that we considered any diffing, but otherwise it's pretty much how it works -- we have SQL funcs to dump/load caches state",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1837129591",
    "pr_number": 9661,
    "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
    "created_at": "2024-11-11T19:53:25+00:00",
    "commented_code": "+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+- \n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3, and on the deletion of the\n+Endpoint this ephemeral data is dropped, too.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as easiest method.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as S3\n+\n+    co->>+ep: Store Unlogged Persistent File\n+    opt is authenticated\n+        ep->>s3: Write UPF to S3\n+    end\n+    ep->>-co: OK / Failure / Auth Failure\n+```\n+\n+Read data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Storage\n+\n+    co->>+ep: Read Unlogged Persistent File\n+    opt is authenticated\n+        ep->>+s3: Request UPF from storage\n+        s3->>-ep: Receive UPF from storage\n+    end\n+    ep->>-co: OK(response) / Failure(storage, auth, ...)\n+```\n+\n+Compute Startup:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ps as Pageserver\n+    participant ep as EPUFS\n+    participant es as Extension server\n+\n+    note over co: Bind endpoint ep-xxx\n+    par Get basebackup\n+        co->>+ps: Request basebackup @ LSN\n+        ps-)ps: Construct basebackup\n+        ps->>-co: Receive basebackup TAR @ LSN\n+    and Get startup-critical Unlogged Persistent Files\n+        co->>+ep: Get all UPFs of endpoint ep-xxx\n+        ep-)ep: Retrieve and gather all UPFs\n+        ep->>-co: TAR of UPFs\n+    and Get startup-critical extensions\n+        loop For every startup-critical extension\n+            co->>es: Get critical extension\n+            es->>co: Receive critical extension\n+        end\n+    end\n+    note over co: Start compute\n+```\n+\n+CPlane ops:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant cp as Control Plane\n+    participant ep as EPUFS\n+    participant s3 as Storage\n+\n+    alt Tenant deleted",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "1837129591",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 9661,
        "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
        "discussion_id": "1837129591",
        "commented_code": "@@ -0,0 +1,177 @@\n+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+- \n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3, and on the deletion of the\n+Endpoint this ephemeral data is dropped, too.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as easiest method.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as S3\n+\n+    co->>+ep: Store Unlogged Persistent File\n+    opt is authenticated\n+        ep->>s3: Write UPF to S3\n+    end\n+    ep->>-co: OK / Failure / Auth Failure\n+```\n+\n+Read data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Storage\n+\n+    co->>+ep: Read Unlogged Persistent File\n+    opt is authenticated\n+        ep->>+s3: Request UPF from storage\n+        s3->>-ep: Receive UPF from storage\n+    end\n+    ep->>-co: OK(response) / Failure(storage, auth, ...)\n+```\n+\n+Compute Startup:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ps as Pageserver\n+    participant ep as EPUFS\n+    participant es as Extension server\n+\n+    note over co: Bind endpoint ep-xxx\n+    par Get basebackup\n+        co->>+ps: Request basebackup @ LSN\n+        ps-)ps: Construct basebackup\n+        ps->>-co: Receive basebackup TAR @ LSN\n+    and Get startup-critical Unlogged Persistent Files\n+        co->>+ep: Get all UPFs of endpoint ep-xxx\n+        ep-)ep: Retrieve and gather all UPFs\n+        ep->>-co: TAR of UPFs\n+    and Get startup-critical extensions\n+        loop For every startup-critical extension\n+            co->>es: Get critical extension\n+            es->>co: Receive critical extension\n+        end\n+    end\n+    note over co: Start compute\n+```\n+\n+CPlane ops:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant cp as Control Plane\n+    participant ep as EPUFS\n+    participant s3 as Storage\n+\n+    alt Tenant deleted",
        "comment_created_at": "2024-11-11T19:53:25+00:00",
        "comment_author": "ololobus",
        "comment_body": "If we consider this compute data as non-critical, could we avoid explicit deletion completely? I was thinking about setting a TTL for perfix/bucket https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-set-lifecycle-configuration-intro.html (never used it personally, though)\r\n\r\nThat should most likely work for prewarm/caches content. Assuming we set it to a high enough value (like 7d or 30d), if one doesn't start endpoint for that long, they likely don't care about prewarming much. For `pg_stat_statements` it's pretty much the same -- well, your perf data will expire after N days -- sounds fair. For stats it could be a bit more annoying, but again should be not critical at all\r\n\r\nAt the same time, with TTL we avoid implementing a huge piece of deletion orchestration.\r\n\r\nWhat do you think?",
        "pr_file_module": null
      },
      {
        "comment_id": "1848260534",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 9661,
        "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
        "discussion_id": "1837129591",
        "commented_code": "@@ -0,0 +1,177 @@\n+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+- \n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3, and on the deletion of the\n+Endpoint this ephemeral data is dropped, too.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as easiest method.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as S3\n+\n+    co->>+ep: Store Unlogged Persistent File\n+    opt is authenticated\n+        ep->>s3: Write UPF to S3\n+    end\n+    ep->>-co: OK / Failure / Auth Failure\n+```\n+\n+Read data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Storage\n+\n+    co->>+ep: Read Unlogged Persistent File\n+    opt is authenticated\n+        ep->>+s3: Request UPF from storage\n+        s3->>-ep: Receive UPF from storage\n+    end\n+    ep->>-co: OK(response) / Failure(storage, auth, ...)\n+```\n+\n+Compute Startup:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ps as Pageserver\n+    participant ep as EPUFS\n+    participant es as Extension server\n+\n+    note over co: Bind endpoint ep-xxx\n+    par Get basebackup\n+        co->>+ps: Request basebackup @ LSN\n+        ps-)ps: Construct basebackup\n+        ps->>-co: Receive basebackup TAR @ LSN\n+    and Get startup-critical Unlogged Persistent Files\n+        co->>+ep: Get all UPFs of endpoint ep-xxx\n+        ep-)ep: Retrieve and gather all UPFs\n+        ep->>-co: TAR of UPFs\n+    and Get startup-critical extensions\n+        loop For every startup-critical extension\n+            co->>es: Get critical extension\n+            es->>co: Receive critical extension\n+        end\n+    end\n+    note over co: Start compute\n+```\n+\n+CPlane ops:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant cp as Control Plane\n+    participant ep as EPUFS\n+    participant s3 as Storage\n+\n+    alt Tenant deleted",
        "comment_created_at": "2024-11-19T12:26:19+00:00",
        "comment_author": "MMeent",
        "comment_body": "I'm a bit concerned about issues that would arise from deletion on a weekly schedule while using the endpoint exclusively for monthly tasks - you'd want that endpoint to have good performance.",
        "pr_file_module": null
      },
      {
        "comment_id": "1987716553",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 9661,
        "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
        "discussion_id": "1837129591",
        "commented_code": "@@ -0,0 +1,177 @@\n+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+- \n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3, and on the deletion of the\n+Endpoint this ephemeral data is dropped, too.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as easiest method.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as S3\n+\n+    co->>+ep: Store Unlogged Persistent File\n+    opt is authenticated\n+        ep->>s3: Write UPF to S3\n+    end\n+    ep->>-co: OK / Failure / Auth Failure\n+```\n+\n+Read data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Storage\n+\n+    co->>+ep: Read Unlogged Persistent File\n+    opt is authenticated\n+        ep->>+s3: Request UPF from storage\n+        s3->>-ep: Receive UPF from storage\n+    end\n+    ep->>-co: OK(response) / Failure(storage, auth, ...)\n+```\n+\n+Compute Startup:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ps as Pageserver\n+    participant ep as EPUFS\n+    participant es as Extension server\n+\n+    note over co: Bind endpoint ep-xxx\n+    par Get basebackup\n+        co->>+ps: Request basebackup @ LSN\n+        ps-)ps: Construct basebackup\n+        ps->>-co: Receive basebackup TAR @ LSN\n+    and Get startup-critical Unlogged Persistent Files\n+        co->>+ep: Get all UPFs of endpoint ep-xxx\n+        ep-)ep: Retrieve and gather all UPFs\n+        ep->>-co: TAR of UPFs\n+    and Get startup-critical extensions\n+        loop For every startup-critical extension\n+            co->>es: Get critical extension\n+            es->>co: Receive critical extension\n+        end\n+    end\n+    note over co: Start compute\n+```\n+\n+CPlane ops:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant cp as Control Plane\n+    participant ep as EPUFS\n+    participant s3 as Storage\n+\n+    alt Tenant deleted",
        "comment_created_at": "2025-03-10T17:16:49+00:00",
        "comment_author": "ololobus",
        "comment_body": "Explicit deletion would work as well, I think, just more work on the control plane side",
        "pr_file_module": null
      }
    ]
  }
]