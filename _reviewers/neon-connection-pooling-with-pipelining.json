[
  {
    "discussion_id": "2192579006",
    "pr_number": 12475,
    "pr_file": "pageserver/client_grpc/src/pool.rs",
    "created_at": "2025-07-08T13:44:13+00:00",
    "commented_code": "// Acquire a client from the pool and create a stream.\n         let mut client = client_pool.get().await?;\n \n-        let (req_tx, req_rx) = mpsc::channel(STREAM_QUEUE_DEPTH);\n+        let (req_tx, req_rx) = mpsc::channel(1);",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2192579006",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12475,
        "pr_file": "pageserver/client_grpc/src/pool.rs",
        "discussion_id": "2192579006",
        "commented_code": "@@ -484,19 +493,16 @@ impl StreamPool {\n         // Acquire a client from the pool and create a stream.\n         let mut client = client_pool.get().await?;\n \n-        let (req_tx, req_rx) = mpsc::channel(STREAM_QUEUE_DEPTH);\n+        let (req_tx, req_rx) = mpsc::channel(1);",
        "comment_created_at": "2025-07-08T13:44:13+00:00",
        "comment_author": "VladLazar",
        "comment_body": "What's the point of this extra channel now? Can't we just plug `caller_rx` into `client.get_pages`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2192808749",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12475,
        "pr_file": "pageserver/client_grpc/src/pool.rs",
        "discussion_id": "2192579006",
        "commented_code": "@@ -484,19 +493,16 @@ impl StreamPool {\n         // Acquire a client from the pool and create a stream.\n         let mut client = client_pool.get().await?;\n \n-        let (req_tx, req_rx) = mpsc::channel(STREAM_QUEUE_DEPTH);\n+        let (req_tx, req_rx) = mpsc::channel(1);",
        "comment_created_at": "2025-07-08T15:16:07+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "`caller_rx` is a stream of `(page_api::GetPageRequest, ResponseSender)`. We have to pick out the `ResponseSender` and keep it around by request ID so we know where to send each response.",
        "pr_file_module": null
      },
      {
        "comment_id": "2193199160",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12475,
        "pr_file": "pageserver/client_grpc/src/pool.rs",
        "discussion_id": "2192579006",
        "commented_code": "@@ -484,19 +493,16 @@ impl StreamPool {\n         // Acquire a client from the pool and create a stream.\n         let mut client = client_pool.get().await?;\n \n-        let (req_tx, req_rx) = mpsc::channel(STREAM_QUEUE_DEPTH);\n+        let (req_tx, req_rx) = mpsc::channel(1);",
        "comment_created_at": "2025-07-08T18:33:10+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Makes sense. The extra stream still allows for pipelining as long as the previous request was pushed down the tcp pipe (response need not be received by that point).",
        "pr_file_module": null
      },
      {
        "comment_id": "2193319922",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12475,
        "pr_file": "pageserver/client_grpc/src/pool.rs",
        "discussion_id": "2192579006",
        "commented_code": "@@ -484,19 +493,16 @@ impl StreamPool {\n         // Acquire a client from the pool and create a stream.\n         let mut client = client_pool.get().await?;\n \n-        let (req_tx, req_rx) = mpsc::channel(STREAM_QUEUE_DEPTH);\n+        let (req_tx, req_rx) = mpsc::channel(1);",
        "comment_created_at": "2025-07-08T19:45:36+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "Ah, yes, exactly. For pipelining to be effective the request must be sent onwards to the TCP stack and server. If the request isn't passed on to TCP it doesn't matter if we block here or buffer them in the channel -- they're not getting sent across the network any faster regardless, and the caller is just going to sit around and wait for the response no matter what.\r\n\r\nI'll add a comment to that effect.",
        "pr_file_module": null
      },
      {
        "comment_id": "2193328685",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12475,
        "pr_file": "pageserver/client_grpc/src/pool.rs",
        "discussion_id": "2192579006",
        "commented_code": "@@ -484,19 +493,16 @@ impl StreamPool {\n         // Acquire a client from the pool and create a stream.\n         let mut client = client_pool.get().await?;\n \n-        let (req_tx, req_rx) = mpsc::channel(STREAM_QUEUE_DEPTH);\n+        let (req_tx, req_rx) = mpsc::channel(1);",
        "comment_created_at": "2025-07-08T19:51:18+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "Hm, actually, there might be a hazard here. If the gRPC stream send blocks, we won't be reading responses (that send is nested below the select) -- so with a large enough queue depth and with TCP/gRPC backpressure, we could end up in a similar deadlock as we've seen with the current libpq implementation (both client and server are blocked on sends).\r\n\r\nI don't think that's likely given that gRPC/TCP buffers are large and queue depths are small, but it's probably prudent to use the queue depth as a buffer size here -- it's cheap insurance.",
        "pr_file_module": null
      },
      {
        "comment_id": "2193363508",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12475,
        "pr_file": "pageserver/client_grpc/src/pool.rs",
        "discussion_id": "2192579006",
        "commented_code": "@@ -484,19 +493,16 @@ impl StreamPool {\n         // Acquire a client from the pool and create a stream.\n         let mut client = client_pool.get().await?;\n \n-        let (req_tx, req_rx) = mpsc::channel(STREAM_QUEUE_DEPTH);\n+        let (req_tx, req_rx) = mpsc::channel(1);",
        "comment_created_at": "2025-07-08T20:14:22+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "Even better, we can just use an unbounded channel. https://github.com/neondatabase/neon/pull/12475/commits/ba1d816bc79d2020c03953bb544d499fec11a4ac",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172136411",
    "pr_number": 12244,
    "pr_file": "compute_tools/src/compute.rs",
    "created_at": "2025-06-27T14:12:01+00:00",
    "commented_code": "Ok(())\n     }\n \n-    // Get basebackup from the libpq connection to pageserver using `connstr` and\n-    // unarchive it to `pgdata` directory overriding all its previous content.\n+    /// Fetches a basebackup from the Pageserver using the compute state's Pageserver connstring and\n+    /// unarchives it to `pgdata` directory, replacing any existing contents.\n     #[instrument(skip_all, fields(%lsn))]\n     fn try_get_basebackup(&self, compute_state: &ComputeState, lsn: Lsn) -> Result<()> {\n         let spec = compute_state.pspec.as_ref().expect(\"spec must be set\");\n-        let start_time = Instant::now();\n \n+        // Detect the protocol scheme. If the URL doesn't have a scheme, assume libpq.\n+        let shard0_connstr = spec.pageserver_connstr.split(',').next().unwrap();\n+        let scheme = match Url::parse(shard0_connstr) {\n+            Ok(url) => url.scheme().to_lowercase().to_string(),\n+            Err(url::ParseError::RelativeUrlWithoutBase) => \"postgresql\".to_string(),\n+            Err(err) => return Err(anyhow!(\"invalid connstring URL: {err}\")),\n+        };\n+\n+        let started = Instant::now();\n+        let (connected, size) = match scheme.as_str() {\n+            \"postgresql\" | \"postgres\" => self.try_get_basebackup_libpq(spec, lsn)?,\n+            \"grpc\" => self.try_get_basebackup_grpc(spec, lsn)?,\n+            scheme => return Err(anyhow!(\"unknown URL scheme {scheme}\")),\n+        };\n+\n+        let mut state = self.state.lock().unwrap();\n+        state.metrics.pageserver_connect_micros =\n+            connected.duration_since(started).as_micros() as u64;\n+        state.metrics.basebackup_bytes = size as u64;\n+        state.metrics.basebackup_ms = started.elapsed().as_millis() as u64;\n+\n+        Ok(())\n+    }\n+\n+    /// Fetches a basebackup via gRPC. The connstring must use grpc://. Returns the connect time\n+    /// and base backup size.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2172136411",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12244,
        "pr_file": "compute_tools/src/compute.rs",
        "discussion_id": "2172136411",
        "commented_code": "@@ -998,13 +1000,88 @@ impl ComputeNode {\n         Ok(())\n     }\n \n-    // Get basebackup from the libpq connection to pageserver using `connstr` and\n-    // unarchive it to `pgdata` directory overriding all its previous content.\n+    /// Fetches a basebackup from the Pageserver using the compute state's Pageserver connstring and\n+    /// unarchives it to `pgdata` directory, replacing any existing contents.\n     #[instrument(skip_all, fields(%lsn))]\n     fn try_get_basebackup(&self, compute_state: &ComputeState, lsn: Lsn) -> Result<()> {\n         let spec = compute_state.pspec.as_ref().expect(\"spec must be set\");\n-        let start_time = Instant::now();\n \n+        // Detect the protocol scheme. If the URL doesn't have a scheme, assume libpq.\n+        let shard0_connstr = spec.pageserver_connstr.split(',').next().unwrap();\n+        let scheme = match Url::parse(shard0_connstr) {\n+            Ok(url) => url.scheme().to_lowercase().to_string(),\n+            Err(url::ParseError::RelativeUrlWithoutBase) => \"postgresql\".to_string(),\n+            Err(err) => return Err(anyhow!(\"invalid connstring URL: {err}\")),\n+        };\n+\n+        let started = Instant::now();\n+        let (connected, size) = match scheme.as_str() {\n+            \"postgresql\" | \"postgres\" => self.try_get_basebackup_libpq(spec, lsn)?,\n+            \"grpc\" => self.try_get_basebackup_grpc(spec, lsn)?,\n+            scheme => return Err(anyhow!(\"unknown URL scheme {scheme}\")),\n+        };\n+\n+        let mut state = self.state.lock().unwrap();\n+        state.metrics.pageserver_connect_micros =\n+            connected.duration_since(started).as_micros() as u64;\n+        state.metrics.basebackup_bytes = size as u64;\n+        state.metrics.basebackup_ms = started.elapsed().as_millis() as u64;\n+\n+        Ok(())\n+    }\n+\n+    /// Fetches a basebackup via gRPC. The connstring must use grpc://. Returns the connect time\n+    /// and base backup size.",
        "comment_created_at": "2025-06-27T14:12:01+00:00",
        "comment_author": "hlinnaka",
        "comment_body": "If compression is used, does it return the compressed or uncompressed size?",
        "pr_file_module": null
      },
      {
        "comment_id": "2172139320",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12244,
        "pr_file": "compute_tools/src/compute.rs",
        "discussion_id": "2172136411",
        "commented_code": "@@ -998,13 +1000,88 @@ impl ComputeNode {\n         Ok(())\n     }\n \n-    // Get basebackup from the libpq connection to pageserver using `connstr` and\n-    // unarchive it to `pgdata` directory overriding all its previous content.\n+    /// Fetches a basebackup from the Pageserver using the compute state's Pageserver connstring and\n+    /// unarchives it to `pgdata` directory, replacing any existing contents.\n     #[instrument(skip_all, fields(%lsn))]\n     fn try_get_basebackup(&self, compute_state: &ComputeState, lsn: Lsn) -> Result<()> {\n         let spec = compute_state.pspec.as_ref().expect(\"spec must be set\");\n-        let start_time = Instant::now();\n \n+        // Detect the protocol scheme. If the URL doesn't have a scheme, assume libpq.\n+        let shard0_connstr = spec.pageserver_connstr.split(',').next().unwrap();\n+        let scheme = match Url::parse(shard0_connstr) {\n+            Ok(url) => url.scheme().to_lowercase().to_string(),\n+            Err(url::ParseError::RelativeUrlWithoutBase) => \"postgresql\".to_string(),\n+            Err(err) => return Err(anyhow!(\"invalid connstring URL: {err}\")),\n+        };\n+\n+        let started = Instant::now();\n+        let (connected, size) = match scheme.as_str() {\n+            \"postgresql\" | \"postgres\" => self.try_get_basebackup_libpq(spec, lsn)?,\n+            \"grpc\" => self.try_get_basebackup_grpc(spec, lsn)?,\n+            scheme => return Err(anyhow!(\"unknown URL scheme {scheme}\")),\n+        };\n+\n+        let mut state = self.state.lock().unwrap();\n+        state.metrics.pageserver_connect_micros =\n+            connected.duration_since(started).as_micros() as u64;\n+        state.metrics.basebackup_bytes = size as u64;\n+        state.metrics.basebackup_ms = started.elapsed().as_millis() as u64;\n+\n+        Ok(())\n+    }\n+\n+    /// Fetches a basebackup via gRPC. The connstring must use grpc://. Returns the connect time\n+    /// and base backup size.",
        "comment_created_at": "2025-06-27T14:13:21+00:00",
        "comment_author": "hlinnaka",
        "comment_body": "Is the time returned just the time it took to establish the connection, or the full time it took to process and extract the basebackup? \"the connect time\" is a bit vague",
        "pr_file_module": null
      },
      {
        "comment_id": "2172151152",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12244,
        "pr_file": "compute_tools/src/compute.rs",
        "discussion_id": "2172136411",
        "commented_code": "@@ -998,13 +1000,88 @@ impl ComputeNode {\n         Ok(())\n     }\n \n-    // Get basebackup from the libpq connection to pageserver using `connstr` and\n-    // unarchive it to `pgdata` directory overriding all its previous content.\n+    /// Fetches a basebackup from the Pageserver using the compute state's Pageserver connstring and\n+    /// unarchives it to `pgdata` directory, replacing any existing contents.\n     #[instrument(skip_all, fields(%lsn))]\n     fn try_get_basebackup(&self, compute_state: &ComputeState, lsn: Lsn) -> Result<()> {\n         let spec = compute_state.pspec.as_ref().expect(\"spec must be set\");\n-        let start_time = Instant::now();\n \n+        // Detect the protocol scheme. If the URL doesn't have a scheme, assume libpq.\n+        let shard0_connstr = spec.pageserver_connstr.split(',').next().unwrap();\n+        let scheme = match Url::parse(shard0_connstr) {\n+            Ok(url) => url.scheme().to_lowercase().to_string(),\n+            Err(url::ParseError::RelativeUrlWithoutBase) => \"postgresql\".to_string(),\n+            Err(err) => return Err(anyhow!(\"invalid connstring URL: {err}\")),\n+        };\n+\n+        let started = Instant::now();\n+        let (connected, size) = match scheme.as_str() {\n+            \"postgresql\" | \"postgres\" => self.try_get_basebackup_libpq(spec, lsn)?,\n+            \"grpc\" => self.try_get_basebackup_grpc(spec, lsn)?,\n+            scheme => return Err(anyhow!(\"unknown URL scheme {scheme}\")),\n+        };\n+\n+        let mut state = self.state.lock().unwrap();\n+        state.metrics.pageserver_connect_micros =\n+            connected.duration_since(started).as_micros() as u64;\n+        state.metrics.basebackup_bytes = size as u64;\n+        state.metrics.basebackup_ms = started.elapsed().as_millis() as u64;\n+\n+        Ok(())\n+    }\n+\n+    /// Fetches a basebackup via gRPC. The connstring must use grpc://. Returns the connect time\n+    /// and base backup size.",
        "comment_created_at": "2025-06-27T14:18:46+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "> If compression is used, does it return the compressed or uncompressed size?\r\n\r\nTurns out this was buggy! libpq would report the compressed size, gRPC the uncompressed size. Fixed.\r\n\r\n> Is the time returned just the time it took to establish the connection, or the full time it took to process and extract the basebackup?\r\n\r\nNeither, it's the timestamp when the connection to the server was established.\r\n\r\nUpdated the comments to address both concerns.",
        "pr_file_module": null
      },
      {
        "comment_id": "2172178395",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12244,
        "pr_file": "compute_tools/src/compute.rs",
        "discussion_id": "2172136411",
        "commented_code": "@@ -998,13 +1000,88 @@ impl ComputeNode {\n         Ok(())\n     }\n \n-    // Get basebackup from the libpq connection to pageserver using `connstr` and\n-    // unarchive it to `pgdata` directory overriding all its previous content.\n+    /// Fetches a basebackup from the Pageserver using the compute state's Pageserver connstring and\n+    /// unarchives it to `pgdata` directory, replacing any existing contents.\n     #[instrument(skip_all, fields(%lsn))]\n     fn try_get_basebackup(&self, compute_state: &ComputeState, lsn: Lsn) -> Result<()> {\n         let spec = compute_state.pspec.as_ref().expect(\"spec must be set\");\n-        let start_time = Instant::now();\n \n+        // Detect the protocol scheme. If the URL doesn't have a scheme, assume libpq.\n+        let shard0_connstr = spec.pageserver_connstr.split(',').next().unwrap();\n+        let scheme = match Url::parse(shard0_connstr) {\n+            Ok(url) => url.scheme().to_lowercase().to_string(),\n+            Err(url::ParseError::RelativeUrlWithoutBase) => \"postgresql\".to_string(),\n+            Err(err) => return Err(anyhow!(\"invalid connstring URL: {err}\")),\n+        };\n+\n+        let started = Instant::now();\n+        let (connected, size) = match scheme.as_str() {\n+            \"postgresql\" | \"postgres\" => self.try_get_basebackup_libpq(spec, lsn)?,\n+            \"grpc\" => self.try_get_basebackup_grpc(spec, lsn)?,\n+            scheme => return Err(anyhow!(\"unknown URL scheme {scheme}\")),\n+        };\n+\n+        let mut state = self.state.lock().unwrap();\n+        state.metrics.pageserver_connect_micros =\n+            connected.duration_since(started).as_micros() as u64;\n+        state.metrics.basebackup_bytes = size as u64;\n+        state.metrics.basebackup_ms = started.elapsed().as_millis() as u64;\n+\n+        Ok(())\n+    }\n+\n+    /// Fetches a basebackup via gRPC. The connstring must use grpc://. Returns the connect time\n+    /// and base backup size.",
        "comment_created_at": "2025-06-27T14:32:07+00:00",
        "comment_author": "hlinnaka",
        "comment_body": "Ah, it's a timestamp rather than a duration, gotcha!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2129228207",
    "pr_number": 12146,
    "pr_file": "pageserver/src/tenant/mgr.rs",
    "created_at": "2025-06-05T16:21:38+00:00",
    "commented_code": "shutdown_all_tenants0(self.tenants).await\n     }\n \n+    /// Detaches a tenant, and removes its local files asynchronously.\n+    ///\n+    /// File removal is idempotent: even if the tenant has already been removed, this will still\n+    /// remove any local files. This is used during shard splits, where we leave the parent shard's\n+    /// files around in case we have to roll back the split.\n     pub(crate) async fn detach_tenant(\n         &self,\n         conf: &'static PageServerConf,\n         tenant_shard_id: TenantShardId,\n         deletion_queue_client: &DeletionQueueClient,\n     ) -> Result<(), TenantStateError> {\n-        let tmp_path = self\n+        if let Some(tmp_path) = self\n             .detach_tenant0(conf, tenant_shard_id, deletion_queue_client)\n-            .await?;\n-        self.background_purges.spawn(tmp_path);\n+            .await?\n+        {\n+            self.background_purges.spawn(tmp_path);\n+        }\n \n         Ok(())\n     }\n \n+    /// Detaches a tenant. This renames the tenant directory to a temporary path and returns it,\n+    /// allowing the caller to delete it asynchronously. Returns None if the dir is already removed.\n     async fn detach_tenant0(\n         &self,\n         conf: &'static PageServerConf,\n         tenant_shard_id: TenantShardId,\n         deletion_queue_client: &DeletionQueueClient,\n-    ) -> Result<Utf8PathBuf, TenantStateError> {\n+    ) -> Result<Option<Utf8PathBuf>, TenantStateError> {\n         let tenant_dir_rename_operation = |tenant_id_to_clean: TenantShardId| async move {\n             let local_tenant_directory = conf.tenant_path(&tenant_id_to_clean);\n+            if !Path::new(&local_tenant_directory).exists() {",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2129228207",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12146,
        "pr_file": "pageserver/src/tenant/mgr.rs",
        "discussion_id": "2129228207",
        "commented_code": "@@ -1846,42 +1847,70 @@ impl TenantManager {\n         shutdown_all_tenants0(self.tenants).await\n     }\n \n+    /// Detaches a tenant, and removes its local files asynchronously.\n+    ///\n+    /// File removal is idempotent: even if the tenant has already been removed, this will still\n+    /// remove any local files. This is used during shard splits, where we leave the parent shard's\n+    /// files around in case we have to roll back the split.\n     pub(crate) async fn detach_tenant(\n         &self,\n         conf: &'static PageServerConf,\n         tenant_shard_id: TenantShardId,\n         deletion_queue_client: &DeletionQueueClient,\n     ) -> Result<(), TenantStateError> {\n-        let tmp_path = self\n+        if let Some(tmp_path) = self\n             .detach_tenant0(conf, tenant_shard_id, deletion_queue_client)\n-            .await?;\n-        self.background_purges.spawn(tmp_path);\n+            .await?\n+        {\n+            self.background_purges.spawn(tmp_path);\n+        }\n \n         Ok(())\n     }\n \n+    /// Detaches a tenant. This renames the tenant directory to a temporary path and returns it,\n+    /// allowing the caller to delete it asynchronously. Returns None if the dir is already removed.\n     async fn detach_tenant0(\n         &self,\n         conf: &'static PageServerConf,\n         tenant_shard_id: TenantShardId,\n         deletion_queue_client: &DeletionQueueClient,\n-    ) -> Result<Utf8PathBuf, TenantStateError> {\n+    ) -> Result<Option<Utf8PathBuf>, TenantStateError> {\n         let tenant_dir_rename_operation = |tenant_id_to_clean: TenantShardId| async move {\n             let local_tenant_directory = conf.tenant_path(&tenant_id_to_clean);\n+            if !Path::new(&local_tenant_directory).exists() {",
        "comment_created_at": "2025-06-05T16:21:38+00:00",
        "comment_author": "jcsp",
        "comment_body": "Sync .exists() is usually fast, but this should probably be the tokio::fs async equivalent",
        "pr_file_module": null
      },
      {
        "comment_id": "2131957317",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12146,
        "pr_file": "pageserver/src/tenant/mgr.rs",
        "discussion_id": "2129228207",
        "commented_code": "@@ -1846,42 +1847,70 @@ impl TenantManager {\n         shutdown_all_tenants0(self.tenants).await\n     }\n \n+    /// Detaches a tenant, and removes its local files asynchronously.\n+    ///\n+    /// File removal is idempotent: even if the tenant has already been removed, this will still\n+    /// remove any local files. This is used during shard splits, where we leave the parent shard's\n+    /// files around in case we have to roll back the split.\n     pub(crate) async fn detach_tenant(\n         &self,\n         conf: &'static PageServerConf,\n         tenant_shard_id: TenantShardId,\n         deletion_queue_client: &DeletionQueueClient,\n     ) -> Result<(), TenantStateError> {\n-        let tmp_path = self\n+        if let Some(tmp_path) = self\n             .detach_tenant0(conf, tenant_shard_id, deletion_queue_client)\n-            .await?;\n-        self.background_purges.spawn(tmp_path);\n+            .await?\n+        {\n+            self.background_purges.spawn(tmp_path);\n+        }\n \n         Ok(())\n     }\n \n+    /// Detaches a tenant. This renames the tenant directory to a temporary path and returns it,\n+    /// allowing the caller to delete it asynchronously. Returns None if the dir is already removed.\n     async fn detach_tenant0(\n         &self,\n         conf: &'static PageServerConf,\n         tenant_shard_id: TenantShardId,\n         deletion_queue_client: &DeletionQueueClient,\n-    ) -> Result<Utf8PathBuf, TenantStateError> {\n+    ) -> Result<Option<Utf8PathBuf>, TenantStateError> {\n         let tenant_dir_rename_operation = |tenant_id_to_clean: TenantShardId| async move {\n             let local_tenant_directory = conf.tenant_path(&tenant_id_to_clean);\n+            if !Path::new(&local_tenant_directory).exists() {",
        "comment_created_at": "2025-06-06T10:35:43+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "Good call, thanks. `Path::exists()` would also discard errors.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2113668490",
    "pr_number": 12044,
    "pr_file": "pageserver/client_grpc/src/lib.rs",
    "created_at": "2025-05-29T10:28:50+00:00",
    "commented_code": "+//!\n+//! Pageserver gRPC client library\n+//!\n+//! This library provides a gRPC client for the pageserver for the\n+//! communicator project.\n+//!\n+//! This library is a work in progress.\n+//!\n+//!\n+\n+use std::collections::HashMap;\n+use bytes::Bytes;\n+use futures::{StreamExt};\n+use thiserror::Error;\n+use tonic::metadata::AsciiMetadataValue;\n+use pageserver_page_api::proto;\n+use pageserver_page_api::proto::PageServiceClient;\n+use utils::shard::ShardIndex;\n+use std::fmt::Debug;\n+use tracing::error;\n+use tokio::sync::RwLock;\n+use tonic::transport::{Channel, Endpoint};\n+\n+#[derive(Error, Debug)]\n+pub enum PageserverClientError {\n+    #[error(\"could not connect to service: {0}\")]\n+    ConnectError(#[from] tonic::transport::Error),\n+    #[error(\"could not perform request: {0}`\")]\n+    RequestError(#[from] tonic::Status),\n+    #[error(\"protocol error: {0}\")]\n+    ProtocolError(#[from] pageserver_page_api::ProtocolError),\n+    #[error(\"could not perform request: {0}`\")]\n+    InvalidUri(#[from] http::uri::InvalidUri),\n+    #[error(\"could not perform request: {0}`\")]\n+    Other(String),\n+}\n+\n+pub struct PageserverClient {\n+    endpoint_map: HashMap<ShardIndex, Endpoint>,\n+    channels: tokio::sync::RwLock<HashMap<ShardIndex, Channel>>,\n+    auth_interceptor: AuthInterceptor,\n+}\n+\n+impl PageserverClient {\n+    /// TODO: this doesn't currently react to changes in the shard map.\n+    pub fn new(\n+        tenant_id: AsciiMetadataValue,\n+        timeline_id: AsciiMetadataValue,\n+        auth_token: Option<String>,\n+        shard_map: HashMap<ShardIndex, String>,\n+    ) -> Result<Self, PageserverClientError> {\n+        let endpoint_map: HashMap<ShardIndex, Endpoint> = shard_map\n+            .into_iter()\n+            .map(|(shard, url)| {\n+                let endpoint = Endpoint::from_shared(url)\n+                    .map_err(|_e| PageserverClientError::Other(\"Unable to parse endpoint {url}\".to_string()))?;\n+                Ok::<(ShardIndex, Endpoint), PageserverClientError>((shard, endpoint))\n+            })\n+            .collect::<Result<_, _>>()?;\n+        Ok(Self {\n+            endpoint_map,\n+            channels: RwLock::new(HashMap::new()),\n+            auth_interceptor: AuthInterceptor::new(\n+                tenant_id,\n+                timeline_id,\n+                auth_token,\n+            ),\n+        })\n+    }\n+    //\n+    // TODO: This opens a new gRPC stream for every request, which is extremely inefficient",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2113668490",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12044,
        "pr_file": "pageserver/client_grpc/src/lib.rs",
        "discussion_id": "2113668490",
        "commented_code": "@@ -0,0 +1,192 @@\n+//!\n+//! Pageserver gRPC client library\n+//!\n+//! This library provides a gRPC client for the pageserver for the\n+//! communicator project.\n+//!\n+//! This library is a work in progress.\n+//!\n+//!\n+\n+use std::collections::HashMap;\n+use bytes::Bytes;\n+use futures::{StreamExt};\n+use thiserror::Error;\n+use tonic::metadata::AsciiMetadataValue;\n+use pageserver_page_api::proto;\n+use pageserver_page_api::proto::PageServiceClient;\n+use utils::shard::ShardIndex;\n+use std::fmt::Debug;\n+use tracing::error;\n+use tokio::sync::RwLock;\n+use tonic::transport::{Channel, Endpoint};\n+\n+#[derive(Error, Debug)]\n+pub enum PageserverClientError {\n+    #[error(\"could not connect to service: {0}\")]\n+    ConnectError(#[from] tonic::transport::Error),\n+    #[error(\"could not perform request: {0}`\")]\n+    RequestError(#[from] tonic::Status),\n+    #[error(\"protocol error: {0}\")]\n+    ProtocolError(#[from] pageserver_page_api::ProtocolError),\n+    #[error(\"could not perform request: {0}`\")]\n+    InvalidUri(#[from] http::uri::InvalidUri),\n+    #[error(\"could not perform request: {0}`\")]\n+    Other(String),\n+}\n+\n+pub struct PageserverClient {\n+    endpoint_map: HashMap<ShardIndex, Endpoint>,\n+    channels: tokio::sync::RwLock<HashMap<ShardIndex, Channel>>,\n+    auth_interceptor: AuthInterceptor,\n+}\n+\n+impl PageserverClient {\n+    /// TODO: this doesn't currently react to changes in the shard map.\n+    pub fn new(\n+        tenant_id: AsciiMetadataValue,\n+        timeline_id: AsciiMetadataValue,\n+        auth_token: Option<String>,\n+        shard_map: HashMap<ShardIndex, String>,\n+    ) -> Result<Self, PageserverClientError> {\n+        let endpoint_map: HashMap<ShardIndex, Endpoint> = shard_map\n+            .into_iter()\n+            .map(|(shard, url)| {\n+                let endpoint = Endpoint::from_shared(url)\n+                    .map_err(|_e| PageserverClientError::Other(\"Unable to parse endpoint {url}\".to_string()))?;\n+                Ok::<(ShardIndex, Endpoint), PageserverClientError>((shard, endpoint))\n+            })\n+            .collect::<Result<_, _>>()?;\n+        Ok(Self {\n+            endpoint_map,\n+            channels: RwLock::new(HashMap::new()),\n+            auth_interceptor: AuthInterceptor::new(\n+                tenant_id,\n+                timeline_id,\n+                auth_token,\n+            ),\n+        })\n+    }\n+    //\n+    // TODO: This opens a new gRPC stream for every request, which is extremely inefficient",
        "comment_created_at": "2025-05-29T10:28:50+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "We'll need to figure out stream management here. It's kind of pointless to run Pagebench without stream reuse, so we may as well do that now. Two points:\r\n\r\n1. It should be possible to use this client both with a stream/connection pool (for the communicator) and without it (for tests and benchmarks). We can either pass in a single connection and stream (tying the client's lifetime to it and preventing concurrent use), or pass in a trait that can be implemented both by the pools and a trivial raw gRPC client. This will have implications for where stream/connection multiplexing should happen. What's your take on this?\r\n\r\n2. The API here should probably be a bidirectional stream that implements the `futures::Stream` trait: return an object that can be used both to send requests and receive responses. This is flexible enough to support both pipelining (i.e. queue depth > 1), multiple responses per request (e.g. eager page emission and informational responses about layer downloads), and out-of-order responses (if we ever want them), while keeping request tracking at a higher level. The `futures::stream` module has helpers that might come in handy for implementing this. We can also keep this `get_page()` as a convenience method that sends a single request without exposing a stream, for non-performance-critical code such as tests.\r\n\r\nIt may be premature to make these decisions before we've prototyped the higher layers (pools and trackers). If we think it is, then we can punt this and implement a Pagebench transport using the raw gRPC client. But if we're reasonably confident about the final shape of this client API then we may as well build and merge it now.",
        "pr_file_module": null
      },
      {
        "comment_id": "2114078382",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12044,
        "pr_file": "pageserver/client_grpc/src/lib.rs",
        "discussion_id": "2113668490",
        "commented_code": "@@ -0,0 +1,192 @@\n+//!\n+//! Pageserver gRPC client library\n+//!\n+//! This library provides a gRPC client for the pageserver for the\n+//! communicator project.\n+//!\n+//! This library is a work in progress.\n+//!\n+//!\n+\n+use std::collections::HashMap;\n+use bytes::Bytes;\n+use futures::{StreamExt};\n+use thiserror::Error;\n+use tonic::metadata::AsciiMetadataValue;\n+use pageserver_page_api::proto;\n+use pageserver_page_api::proto::PageServiceClient;\n+use utils::shard::ShardIndex;\n+use std::fmt::Debug;\n+use tracing::error;\n+use tokio::sync::RwLock;\n+use tonic::transport::{Channel, Endpoint};\n+\n+#[derive(Error, Debug)]\n+pub enum PageserverClientError {\n+    #[error(\"could not connect to service: {0}\")]\n+    ConnectError(#[from] tonic::transport::Error),\n+    #[error(\"could not perform request: {0}`\")]\n+    RequestError(#[from] tonic::Status),\n+    #[error(\"protocol error: {0}\")]\n+    ProtocolError(#[from] pageserver_page_api::ProtocolError),\n+    #[error(\"could not perform request: {0}`\")]\n+    InvalidUri(#[from] http::uri::InvalidUri),\n+    #[error(\"could not perform request: {0}`\")]\n+    Other(String),\n+}\n+\n+pub struct PageserverClient {\n+    endpoint_map: HashMap<ShardIndex, Endpoint>,\n+    channels: tokio::sync::RwLock<HashMap<ShardIndex, Channel>>,\n+    auth_interceptor: AuthInterceptor,\n+}\n+\n+impl PageserverClient {\n+    /// TODO: this doesn't currently react to changes in the shard map.\n+    pub fn new(\n+        tenant_id: AsciiMetadataValue,\n+        timeline_id: AsciiMetadataValue,\n+        auth_token: Option<String>,\n+        shard_map: HashMap<ShardIndex, String>,\n+    ) -> Result<Self, PageserverClientError> {\n+        let endpoint_map: HashMap<ShardIndex, Endpoint> = shard_map\n+            .into_iter()\n+            .map(|(shard, url)| {\n+                let endpoint = Endpoint::from_shared(url)\n+                    .map_err(|_e| PageserverClientError::Other(\"Unable to parse endpoint {url}\".to_string()))?;\n+                Ok::<(ShardIndex, Endpoint), PageserverClientError>((shard, endpoint))\n+            })\n+            .collect::<Result<_, _>>()?;\n+        Ok(Self {\n+            endpoint_map,\n+            channels: RwLock::new(HashMap::new()),\n+            auth_interceptor: AuthInterceptor::new(\n+                tenant_id,\n+                timeline_id,\n+                auth_token,\n+            ),\n+        })\n+    }\n+    //\n+    // TODO: This opens a new gRPC stream for every request, which is extremely inefficient",
        "comment_created_at": "2025-05-29T14:19:32+00:00",
        "comment_author": "bizwark",
        "comment_body": "Maybe it makes sense to have this client take a trait that has a \"get connection\" and \"get stream\", and pagebench has a basic implementation that either reuses a stream, or creates a new one for every request. So there would be a \"send_get_page\" that returns a receiving stream, and a \"recv_get_page\" that receives from that stream. It's up to pagebench to have the trait return either a unique stream every time, or reuse a stream and pagebench matches up responses to requests. I think this is pretty basic and there will have to be some logic like this no matter what, even if it moves around a bit in the future.",
        "pr_file_module": null
      },
      {
        "comment_id": "2114125916",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12044,
        "pr_file": "pageserver/client_grpc/src/lib.rs",
        "discussion_id": "2113668490",
        "commented_code": "@@ -0,0 +1,192 @@\n+//!\n+//! Pageserver gRPC client library\n+//!\n+//! This library provides a gRPC client for the pageserver for the\n+//! communicator project.\n+//!\n+//! This library is a work in progress.\n+//!\n+//!\n+\n+use std::collections::HashMap;\n+use bytes::Bytes;\n+use futures::{StreamExt};\n+use thiserror::Error;\n+use tonic::metadata::AsciiMetadataValue;\n+use pageserver_page_api::proto;\n+use pageserver_page_api::proto::PageServiceClient;\n+use utils::shard::ShardIndex;\n+use std::fmt::Debug;\n+use tracing::error;\n+use tokio::sync::RwLock;\n+use tonic::transport::{Channel, Endpoint};\n+\n+#[derive(Error, Debug)]\n+pub enum PageserverClientError {\n+    #[error(\"could not connect to service: {0}\")]\n+    ConnectError(#[from] tonic::transport::Error),\n+    #[error(\"could not perform request: {0}`\")]\n+    RequestError(#[from] tonic::Status),\n+    #[error(\"protocol error: {0}\")]\n+    ProtocolError(#[from] pageserver_page_api::ProtocolError),\n+    #[error(\"could not perform request: {0}`\")]\n+    InvalidUri(#[from] http::uri::InvalidUri),\n+    #[error(\"could not perform request: {0}`\")]\n+    Other(String),\n+}\n+\n+pub struct PageserverClient {\n+    endpoint_map: HashMap<ShardIndex, Endpoint>,\n+    channels: tokio::sync::RwLock<HashMap<ShardIndex, Channel>>,\n+    auth_interceptor: AuthInterceptor,\n+}\n+\n+impl PageserverClient {\n+    /// TODO: this doesn't currently react to changes in the shard map.\n+    pub fn new(\n+        tenant_id: AsciiMetadataValue,\n+        timeline_id: AsciiMetadataValue,\n+        auth_token: Option<String>,\n+        shard_map: HashMap<ShardIndex, String>,\n+    ) -> Result<Self, PageserverClientError> {\n+        let endpoint_map: HashMap<ShardIndex, Endpoint> = shard_map\n+            .into_iter()\n+            .map(|(shard, url)| {\n+                let endpoint = Endpoint::from_shared(url)\n+                    .map_err(|_e| PageserverClientError::Other(\"Unable to parse endpoint {url}\".to_string()))?;\n+                Ok::<(ShardIndex, Endpoint), PageserverClientError>((shard, endpoint))\n+            })\n+            .collect::<Result<_, _>>()?;\n+        Ok(Self {\n+            endpoint_map,\n+            channels: RwLock::new(HashMap::new()),\n+            auth_interceptor: AuthInterceptor::new(\n+                tenant_id,\n+                timeline_id,\n+                auth_token,\n+            ),\n+        })\n+    }\n+    //\n+    // TODO: This opens a new gRPC stream for every request, which is extremely inefficient",
        "comment_created_at": "2025-05-29T14:44:59+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "> have this client take a trait that has a \"get connection\" and \"get stream\"\r\n\r\nYeah, that's probably the way to go.\r\n\r\n> pagebench has a basic implementation that either reuses a stream, or creates a new one for every request\r\n\r\nLet's generalize the basic implementation -- this will also be needed by e.g. `compute_ctl` when fetching base backups via gRPC, and in other tests.\r\n\r\n> So there would be a \"send_get_page\" that returns a receiving stream, and a \"recv_get_page\" that receives from that stream.\r\n\r\nI think we should just return both the send and receive streams from `get_pages()`, since their lifetimes are linked. This is similar to how most other stream handles are returned e.g. in Tokio.",
        "pr_file_module": null
      }
    ]
  }
]