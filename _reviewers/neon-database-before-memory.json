[
  {
    "discussion_id": "2174823847",
    "pr_number": 12320,
    "pr_file": "storage_controller/src/service.rs",
    "created_at": "2025-06-30T11:12:13+00:00",
    "commented_code": "Ok(())\n     }\n \n+    pub(crate) async fn delete_node(\n+        self: &Arc<Self>,\n+        node_id: NodeId,\n+        last_policy: NodeSchedulingPolicy,\n+        cancel: CancellationToken,\n+    ) -> Result<(), OperationError> {\n+        const SECONDARY_WARMUP_TIMEOUT: Duration = Duration::from_secs(30);\n+        const SECONDARY_DOWNLOAD_REQUEST_TIMEOUT: Duration = Duration::from_secs(5);\n+        let reconciler_config = ReconcilerConfigBuilder::new(ReconcilerPriority::Normal)\n+            .secondary_warmup_timeout(SECONDARY_WARMUP_TIMEOUT)\n+            .secondary_download_request_timeout(SECONDARY_DOWNLOAD_REQUEST_TIMEOUT)\n+            .build();\n+\n+        let mut waiters: Vec<ReconcilerWaiter> = Vec::new();\n+\n+        let mut tid_iter = self.create_tenant_shard_iterator().await;\n+\n+        while !tid_iter.finished() {\n+            if cancel.is_cancelled() {\n+                match self.node_configure(node_id, None, Some(last_policy)).await {\n+                    Ok(()) => return Err(OperationError::Cancelled),\n+                    Err(err) => {\n+                        return Err(OperationError::FinalizeError(\n+                            format!(\n+                                \"Failed to finalise delete cancel of {} by setting scheduling policy to {}: {}\",\n+                                node_id, String::from(last_policy), err\n+                            )\n+                            .into(),\n+                        ));\n+                    }\n+                }\n+            }\n+\n+            operation_utils::validate_node_state(\n+                &node_id,\n+                self.inner.read().unwrap().nodes.clone(),\n+                NodeSchedulingPolicy::Deleting,\n+            )?;\n+\n+            while waiters.len() < MAX_RECONCILES_PER_OPERATION {\n+                let tid = match tid_iter.next() {\n+                    Some(tid) => tid,\n+                    None => {\n+                        break;\n+                    }\n+                };\n+\n+                let mut locked = self.inner.write().unwrap();\n+                let (nodes, tenants, scheduler) = locked.parts_mut();\n+\n+                let tenant_shard = match tenants.get_mut(&tid) {\n+                    Some(tenant_shard) => tenant_shard,\n+                    None => {\n+                        tracing::warn!(\"Skip tenant shard {} during delete\", tid);\n+                        continue;\n+                    }\n+                };\n+\n+                match tenant_shard.get_scheduling_policy() {\n+                    ShardSchedulingPolicy::Active | ShardSchedulingPolicy::Essential => {\n+                        // A migration during drain is classed as 'essential' because it is required to\n+                        // uphold our availability goals for the tenant: this shard is elegible for migration.\n+                    }\n+                    ShardSchedulingPolicy::Pause | ShardSchedulingPolicy::Stop => {\n+                        // If we have been asked to avoid rescheduling this shard, then do not migrate it during a drain\n+                        continue;\n+                    }\n+                }\n+\n+                let mut schedule_context = ScheduleContext::new(ScheduleMode::Normal);\n+                schedule_context.avoid(&tenant_shard.intent.all_pageservers());\n+\n+                if tenant_shard.deref_node(node_id) {\n+                    if let Err(e) = tenant_shard.schedule(scheduler, &mut schedule_context) {\n+                        tracing::error!(\n+                            \"Refusing to delete node, shard {} can't be rescheduled: {e}\",\n+                            tenant_shard.tenant_shard_id\n+                        );\n+                        return Err(OperationError::ImpossibleConstraint(e.to_string().into()));\n+                    } else {\n+                        tracing::info!(\n+                            \"Rescheduled shard {} away from node during deletion\",\n+                            tenant_shard.tenant_shard_id\n+                        )\n+                    }\n+\n+                    let waiter = self.maybe_configured_reconcile_shard(\n+                        tenant_shard,\n+                        nodes,\n+                        reconciler_config,\n+                    );\n+                    if let Some(some) = waiter {\n+                        waiters.push(some);\n+                    }\n+                }\n+\n+                // For review purposes: discuss if we want for this line to remain or not.\n+                //\n+                // tenant_shard.observed.locations.remove(&node_id);\n+            }\n+\n+            waiters = self\n+                .await_waiters_remainder(waiters, WAITER_OPERATION_POLL_TIMEOUT)\n+                .await;\n+\n+            failpoint_support::sleep_millis_async!(\"sleepy-delete-loop\", &cancel);\n+        }\n+\n+        while !waiters.is_empty() {\n+            if cancel.is_cancelled() {\n+                match self.node_configure(node_id, None, Some(last_policy)).await {\n+                    Ok(()) => return Err(OperationError::Cancelled),\n+                    Err(err) => {\n+                        return Err(OperationError::FinalizeError(\n+                            format!(\n+                                \"Failed to finalise drain cancel of {} by setting scheduling policy to {}: {}\",\n+                                node_id, String::from(last_policy), err\n+                            )\n+                            .into(),\n+                        ));\n+                    }\n+                }\n+            }\n+\n+            tracing::info!(\"Awaiting {} pending delete reconciliations\", waiters.len());\n+\n+            waiters = self\n+                .await_waiters_remainder(waiters, SHORT_RECONCILE_TIMEOUT)\n+                .await;\n+        }\n+\n+        {\n+            let mut locked = self.inner.write().unwrap();\n+            let (nodes, _, scheduler) = locked.parts_mut();\n+\n+            scheduler.node_remove(node_id);\n+\n+            let mut nodes_mut = (**nodes).clone();\n+            if let Some(mut removed_node) = nodes_mut.remove(&node_id) {\n+                // Ensure that any reconciler holding an Arc<> to this node will\n+                // drop out when trying to RPC to it (setting Offline state sets the\n+                // cancellation token on the Node object).\n+                removed_node.set_availability(NodeAvailability::Offline);\n+            }\n+            *nodes = Arc::new(nodes_mut);\n+\n+            metrics::METRICS_REGISTRY\n+                .metrics_group\n+                .storage_controller_pageserver_nodes\n+                .set(nodes.len() as i64);\n+            metrics::METRICS_REGISTRY\n+                .metrics_group\n+                .storage_controller_https_pageserver_nodes\n+                .set(nodes.values().filter(|n| n.has_https_port()).count() as i64);\n+        }\n+\n+        self.persistence\n+            .set_tombstone(node_id)\n+            .await\n+            .map_err(|e| OperationError::FinalizeError(e.to_string().into()))?;",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2174823847",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12320,
        "pr_file": "storage_controller/src/service.rs",
        "discussion_id": "2174823847",
        "commented_code": "@@ -7085,6 +7088,170 @@ impl Service {\n         Ok(())\n     }\n \n+    pub(crate) async fn delete_node(\n+        self: &Arc<Self>,\n+        node_id: NodeId,\n+        last_policy: NodeSchedulingPolicy,\n+        cancel: CancellationToken,\n+    ) -> Result<(), OperationError> {\n+        const SECONDARY_WARMUP_TIMEOUT: Duration = Duration::from_secs(30);\n+        const SECONDARY_DOWNLOAD_REQUEST_TIMEOUT: Duration = Duration::from_secs(5);\n+        let reconciler_config = ReconcilerConfigBuilder::new(ReconcilerPriority::Normal)\n+            .secondary_warmup_timeout(SECONDARY_WARMUP_TIMEOUT)\n+            .secondary_download_request_timeout(SECONDARY_DOWNLOAD_REQUEST_TIMEOUT)\n+            .build();\n+\n+        let mut waiters: Vec<ReconcilerWaiter> = Vec::new();\n+\n+        let mut tid_iter = self.create_tenant_shard_iterator().await;\n+\n+        while !tid_iter.finished() {\n+            if cancel.is_cancelled() {\n+                match self.node_configure(node_id, None, Some(last_policy)).await {\n+                    Ok(()) => return Err(OperationError::Cancelled),\n+                    Err(err) => {\n+                        return Err(OperationError::FinalizeError(\n+                            format!(\n+                                \"Failed to finalise delete cancel of {} by setting scheduling policy to {}: {}\",\n+                                node_id, String::from(last_policy), err\n+                            )\n+                            .into(),\n+                        ));\n+                    }\n+                }\n+            }\n+\n+            operation_utils::validate_node_state(\n+                &node_id,\n+                self.inner.read().unwrap().nodes.clone(),\n+                NodeSchedulingPolicy::Deleting,\n+            )?;\n+\n+            while waiters.len() < MAX_RECONCILES_PER_OPERATION {\n+                let tid = match tid_iter.next() {\n+                    Some(tid) => tid,\n+                    None => {\n+                        break;\n+                    }\n+                };\n+\n+                let mut locked = self.inner.write().unwrap();\n+                let (nodes, tenants, scheduler) = locked.parts_mut();\n+\n+                let tenant_shard = match tenants.get_mut(&tid) {\n+                    Some(tenant_shard) => tenant_shard,\n+                    None => {\n+                        tracing::warn!(\"Skip tenant shard {} during delete\", tid);\n+                        continue;\n+                    }\n+                };\n+\n+                match tenant_shard.get_scheduling_policy() {\n+                    ShardSchedulingPolicy::Active | ShardSchedulingPolicy::Essential => {\n+                        // A migration during drain is classed as 'essential' because it is required to\n+                        // uphold our availability goals for the tenant: this shard is elegible for migration.\n+                    }\n+                    ShardSchedulingPolicy::Pause | ShardSchedulingPolicy::Stop => {\n+                        // If we have been asked to avoid rescheduling this shard, then do not migrate it during a drain\n+                        continue;\n+                    }\n+                }\n+\n+                let mut schedule_context = ScheduleContext::new(ScheduleMode::Normal);\n+                schedule_context.avoid(&tenant_shard.intent.all_pageservers());\n+\n+                if tenant_shard.deref_node(node_id) {\n+                    if let Err(e) = tenant_shard.schedule(scheduler, &mut schedule_context) {\n+                        tracing::error!(\n+                            \"Refusing to delete node, shard {} can't be rescheduled: {e}\",\n+                            tenant_shard.tenant_shard_id\n+                        );\n+                        return Err(OperationError::ImpossibleConstraint(e.to_string().into()));\n+                    } else {\n+                        tracing::info!(\n+                            \"Rescheduled shard {} away from node during deletion\",\n+                            tenant_shard.tenant_shard_id\n+                        )\n+                    }\n+\n+                    let waiter = self.maybe_configured_reconcile_shard(\n+                        tenant_shard,\n+                        nodes,\n+                        reconciler_config,\n+                    );\n+                    if let Some(some) = waiter {\n+                        waiters.push(some);\n+                    }\n+                }\n+\n+                // For review purposes: discuss if we want for this line to remain or not.\n+                //\n+                // tenant_shard.observed.locations.remove(&node_id);\n+            }\n+\n+            waiters = self\n+                .await_waiters_remainder(waiters, WAITER_OPERATION_POLL_TIMEOUT)\n+                .await;\n+\n+            failpoint_support::sleep_millis_async!(\"sleepy-delete-loop\", &cancel);\n+        }\n+\n+        while !waiters.is_empty() {\n+            if cancel.is_cancelled() {\n+                match self.node_configure(node_id, None, Some(last_policy)).await {\n+                    Ok(()) => return Err(OperationError::Cancelled),\n+                    Err(err) => {\n+                        return Err(OperationError::FinalizeError(\n+                            format!(\n+                                \"Failed to finalise drain cancel of {} by setting scheduling policy to {}: {}\",\n+                                node_id, String::from(last_policy), err\n+                            )\n+                            .into(),\n+                        ));\n+                    }\n+                }\n+            }\n+\n+            tracing::info!(\"Awaiting {} pending delete reconciliations\", waiters.len());\n+\n+            waiters = self\n+                .await_waiters_remainder(waiters, SHORT_RECONCILE_TIMEOUT)\n+                .await;\n+        }\n+\n+        {\n+            let mut locked = self.inner.write().unwrap();\n+            let (nodes, _, scheduler) = locked.parts_mut();\n+\n+            scheduler.node_remove(node_id);\n+\n+            let mut nodes_mut = (**nodes).clone();\n+            if let Some(mut removed_node) = nodes_mut.remove(&node_id) {\n+                // Ensure that any reconciler holding an Arc<> to this node will\n+                // drop out when trying to RPC to it (setting Offline state sets the\n+                // cancellation token on the Node object).\n+                removed_node.set_availability(NodeAvailability::Offline);\n+            }\n+            *nodes = Arc::new(nodes_mut);\n+\n+            metrics::METRICS_REGISTRY\n+                .metrics_group\n+                .storage_controller_pageserver_nodes\n+                .set(nodes.len() as i64);\n+            metrics::METRICS_REGISTRY\n+                .metrics_group\n+                .storage_controller_https_pageserver_nodes\n+                .set(nodes.values().filter(|n| n.has_https_port()).count() as i64);\n+        }\n+\n+        self.persistence\n+            .set_tombstone(node_id)\n+            .await\n+            .map_err(|e| OperationError::FinalizeError(e.to_string().into()))?;",
        "comment_created_at": "2025-06-30T11:12:13+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Another general guideline: aim to update the database state __before__ the in-memory state. Otherwise, if you crash in between it will look like you time travelled since storcon rebuilds the world state from the db.",
        "pr_file_module": null
      },
      {
        "comment_id": "2176519496",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12320,
        "pr_file": "storage_controller/src/service.rs",
        "discussion_id": "2174823847",
        "commented_code": "@@ -7085,6 +7088,170 @@ impl Service {\n         Ok(())\n     }\n \n+    pub(crate) async fn delete_node(\n+        self: &Arc<Self>,\n+        node_id: NodeId,\n+        last_policy: NodeSchedulingPolicy,\n+        cancel: CancellationToken,\n+    ) -> Result<(), OperationError> {\n+        const SECONDARY_WARMUP_TIMEOUT: Duration = Duration::from_secs(30);\n+        const SECONDARY_DOWNLOAD_REQUEST_TIMEOUT: Duration = Duration::from_secs(5);\n+        let reconciler_config = ReconcilerConfigBuilder::new(ReconcilerPriority::Normal)\n+            .secondary_warmup_timeout(SECONDARY_WARMUP_TIMEOUT)\n+            .secondary_download_request_timeout(SECONDARY_DOWNLOAD_REQUEST_TIMEOUT)\n+            .build();\n+\n+        let mut waiters: Vec<ReconcilerWaiter> = Vec::new();\n+\n+        let mut tid_iter = self.create_tenant_shard_iterator().await;\n+\n+        while !tid_iter.finished() {\n+            if cancel.is_cancelled() {\n+                match self.node_configure(node_id, None, Some(last_policy)).await {\n+                    Ok(()) => return Err(OperationError::Cancelled),\n+                    Err(err) => {\n+                        return Err(OperationError::FinalizeError(\n+                            format!(\n+                                \"Failed to finalise delete cancel of {} by setting scheduling policy to {}: {}\",\n+                                node_id, String::from(last_policy), err\n+                            )\n+                            .into(),\n+                        ));\n+                    }\n+                }\n+            }\n+\n+            operation_utils::validate_node_state(\n+                &node_id,\n+                self.inner.read().unwrap().nodes.clone(),\n+                NodeSchedulingPolicy::Deleting,\n+            )?;\n+\n+            while waiters.len() < MAX_RECONCILES_PER_OPERATION {\n+                let tid = match tid_iter.next() {\n+                    Some(tid) => tid,\n+                    None => {\n+                        break;\n+                    }\n+                };\n+\n+                let mut locked = self.inner.write().unwrap();\n+                let (nodes, tenants, scheduler) = locked.parts_mut();\n+\n+                let tenant_shard = match tenants.get_mut(&tid) {\n+                    Some(tenant_shard) => tenant_shard,\n+                    None => {\n+                        tracing::warn!(\"Skip tenant shard {} during delete\", tid);\n+                        continue;\n+                    }\n+                };\n+\n+                match tenant_shard.get_scheduling_policy() {\n+                    ShardSchedulingPolicy::Active | ShardSchedulingPolicy::Essential => {\n+                        // A migration during drain is classed as 'essential' because it is required to\n+                        // uphold our availability goals for the tenant: this shard is elegible for migration.\n+                    }\n+                    ShardSchedulingPolicy::Pause | ShardSchedulingPolicy::Stop => {\n+                        // If we have been asked to avoid rescheduling this shard, then do not migrate it during a drain\n+                        continue;\n+                    }\n+                }\n+\n+                let mut schedule_context = ScheduleContext::new(ScheduleMode::Normal);\n+                schedule_context.avoid(&tenant_shard.intent.all_pageservers());\n+\n+                if tenant_shard.deref_node(node_id) {\n+                    if let Err(e) = tenant_shard.schedule(scheduler, &mut schedule_context) {\n+                        tracing::error!(\n+                            \"Refusing to delete node, shard {} can't be rescheduled: {e}\",\n+                            tenant_shard.tenant_shard_id\n+                        );\n+                        return Err(OperationError::ImpossibleConstraint(e.to_string().into()));\n+                    } else {\n+                        tracing::info!(\n+                            \"Rescheduled shard {} away from node during deletion\",\n+                            tenant_shard.tenant_shard_id\n+                        )\n+                    }\n+\n+                    let waiter = self.maybe_configured_reconcile_shard(\n+                        tenant_shard,\n+                        nodes,\n+                        reconciler_config,\n+                    );\n+                    if let Some(some) = waiter {\n+                        waiters.push(some);\n+                    }\n+                }\n+\n+                // For review purposes: discuss if we want for this line to remain or not.\n+                //\n+                // tenant_shard.observed.locations.remove(&node_id);\n+            }\n+\n+            waiters = self\n+                .await_waiters_remainder(waiters, WAITER_OPERATION_POLL_TIMEOUT)\n+                .await;\n+\n+            failpoint_support::sleep_millis_async!(\"sleepy-delete-loop\", &cancel);\n+        }\n+\n+        while !waiters.is_empty() {\n+            if cancel.is_cancelled() {\n+                match self.node_configure(node_id, None, Some(last_policy)).await {\n+                    Ok(()) => return Err(OperationError::Cancelled),\n+                    Err(err) => {\n+                        return Err(OperationError::FinalizeError(\n+                            format!(\n+                                \"Failed to finalise drain cancel of {} by setting scheduling policy to {}: {}\",\n+                                node_id, String::from(last_policy), err\n+                            )\n+                            .into(),\n+                        ));\n+                    }\n+                }\n+            }\n+\n+            tracing::info!(\"Awaiting {} pending delete reconciliations\", waiters.len());\n+\n+            waiters = self\n+                .await_waiters_remainder(waiters, SHORT_RECONCILE_TIMEOUT)\n+                .await;\n+        }\n+\n+        {\n+            let mut locked = self.inner.write().unwrap();\n+            let (nodes, _, scheduler) = locked.parts_mut();\n+\n+            scheduler.node_remove(node_id);\n+\n+            let mut nodes_mut = (**nodes).clone();\n+            if let Some(mut removed_node) = nodes_mut.remove(&node_id) {\n+                // Ensure that any reconciler holding an Arc<> to this node will\n+                // drop out when trying to RPC to it (setting Offline state sets the\n+                // cancellation token on the Node object).\n+                removed_node.set_availability(NodeAvailability::Offline);\n+            }\n+            *nodes = Arc::new(nodes_mut);\n+\n+            metrics::METRICS_REGISTRY\n+                .metrics_group\n+                .storage_controller_pageserver_nodes\n+                .set(nodes.len() as i64);\n+            metrics::METRICS_REGISTRY\n+                .metrics_group\n+                .storage_controller_https_pageserver_nodes\n+                .set(nodes.values().filter(|n| n.has_https_port()).count() as i64);\n+        }\n+\n+        self.persistence\n+            .set_tombstone(node_id)\n+            .await\n+            .map_err(|e| OperationError::FinalizeError(e.to_string().into()))?;",
        "comment_created_at": "2025-07-01T06:18:05+00:00",
        "comment_author": "ephemeralsad",
        "comment_body": "It was copied from the old function, but fully agreed",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2112288397",
    "pr_number": 12038,
    "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
    "created_at": "2025-05-28T16:17:48+00:00",
    "commented_code": ".cloned();\n         match existing_layer {\n             Some(existing) => {\n+                // Schedule the deletion of the remote layer before removing it from the layer map.\n+                // When `existing_layer` drops [`LayerInner::drop`] will schedule its deletion from\n+                // remote storage, but that assumes that the layer was unlinked from the index first.\n+                timeline\n+                    .remote_client\n+                    .schedule_layer_file_deletion(&[existing.layer_desc().layer_name()])?;",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2112288397",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12038,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2112288397",
        "commented_code": "@@ -964,6 +964,13 @@ impl ChunkProcessingJob {\n             .cloned();\n         match existing_layer {\n             Some(existing) => {\n+                // Schedule the deletion of the remote layer before removing it from the layer map.\n+                // When `existing_layer` drops [`LayerInner::drop`] will schedule its deletion from\n+                // remote storage, but that assumes that the layer was unlinked from the index first.\n+                timeline\n+                    .remote_client\n+                    .schedule_layer_file_deletion(&[existing.layer_desc().layer_name()])?;",
        "comment_created_at": "2025-05-28T16:17:48+00:00",
        "comment_author": "problame",
        "comment_body": "`schedule_layer_file_deletion` schedules both unlink from index and deletion of the object in S3.\n\nBut deletion of the objection in S3 is scheduled by `LayerInner::drop`.\n\nIsn't there a method just for unlinking from index?\nIt saves one S3 DELETE call, plus it doesn't leave me wondering why the heck we do it twice and what can go wrong about it happening twice.",
        "pr_file_module": null
      },
      {
        "comment_id": "2112291341",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12038,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2112288397",
        "commented_code": "@@ -964,6 +964,13 @@ impl ChunkProcessingJob {\n             .cloned();\n         match existing_layer {\n             Some(existing) => {\n+                // Schedule the deletion of the remote layer before removing it from the layer map.\n+                // When `existing_layer` drops [`LayerInner::drop`] will schedule its deletion from\n+                // remote storage, but that assumes that the layer was unlinked from the index first.\n+                timeline\n+                    .remote_client\n+                    .schedule_layer_file_deletion(&[existing.layer_desc().layer_name()])?;",
        "comment_created_at": "2025-05-28T16:19:24+00:00",
        "comment_author": "problame",
        "comment_body": "Maybe `schedule_compaction_update`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2112317830",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12038,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2112288397",
        "commented_code": "@@ -964,6 +964,13 @@ impl ChunkProcessingJob {\n             .cloned();\n         match existing_layer {\n             Some(existing) => {\n+                // Schedule the deletion of the remote layer before removing it from the layer map.\n+                // When `existing_layer` drops [`LayerInner::drop`] will schedule its deletion from\n+                // remote storage, but that assumes that the layer was unlinked from the index first.\n+                timeline\n+                    .remote_client\n+                    .schedule_layer_file_deletion(&[existing.layer_desc().layer_name()])?;",
        "comment_created_at": "2025-05-28T16:33:36+00:00",
        "comment_author": "VladLazar",
        "comment_body": "`schedule_compaction_update`, yeah that should, but the name is bad :laughing: \r\n\r\nFair point. I'll just unlink here.",
        "pr_file_module": null
      },
      {
        "comment_id": "2113514305",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12038,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2112288397",
        "commented_code": "@@ -964,6 +964,13 @@ impl ChunkProcessingJob {\n             .cloned();\n         match existing_layer {\n             Some(existing) => {\n+                // Schedule the deletion of the remote layer before removing it from the layer map.\n+                // When `existing_layer` drops [`LayerInner::drop`] will schedule its deletion from\n+                // remote storage, but that assumes that the layer was unlinked from the index first.\n+                timeline\n+                    .remote_client\n+                    .schedule_layer_file_deletion(&[existing.layer_desc().layer_name()])?;",
        "comment_created_at": "2025-05-29T08:51:17+00:00",
        "comment_author": "VladLazar",
        "comment_body": "https://github.com/neondatabase/neon/pull/12038/commits/2053e7f4a4b8f838159008d0e7841c454bda4459",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2106712991",
    "pr_number": 12015,
    "pr_file": "storage_controller/src/service/safekeeper_service.rs",
    "created_at": "2025-05-26T07:18:58+00:00",
    "commented_code": ".chain(tl.sk_set.iter())\n             .collect::<HashSet<_>>();\n \n+        // The timeline has no safekeepers: we need to delete it from the db manually,\n+        // as no safekeeper reconciler will get to it\n+        if all_sks.is_empty() {",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2106712991",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12015,
        "pr_file": "storage_controller/src/service/safekeeper_service.rs",
        "discussion_id": "2106712991",
        "commented_code": "@@ -410,6 +422,18 @@ impl Service {\n             .chain(tl.sk_set.iter())\n             .collect::<HashSet<_>>();\n \n+        // The timeline has no safekeepers: we need to delete it from the db manually,\n+        // as no safekeeper reconciler will get to it\n+        if all_sks.is_empty() {",
        "comment_created_at": "2025-05-26T07:18:58+00:00",
        "comment_author": "DimasKovas",
        "comment_body": "Can we have a tenant only with read-only timelines?\r\nIn this case similar logic is needed in `tenant_delete_safekeepers`",
        "pr_file_module": null
      },
      {
        "comment_id": "2107245353",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12015,
        "pr_file": "storage_controller/src/service/safekeeper_service.rs",
        "discussion_id": "2106712991",
        "commented_code": "@@ -410,6 +422,18 @@ impl Service {\n             .chain(tl.sk_set.iter())\n             .collect::<HashSet<_>>();\n \n+        // The timeline has no safekeepers: we need to delete it from the db manually,\n+        // as no safekeeper reconciler will get to it\n+        if all_sks.is_empty() {",
        "comment_created_at": "2025-05-26T12:34:40+00:00",
        "comment_author": "arpad-m",
        "comment_body": "We can't have that, because root timelines can't be read-only. Only non-root timelines, and there is always either at least one root timeline, or the tenant is empty.",
        "pr_file_module": null
      }
    ]
  }
]