[
  {
    "discussion_id": "2189615558",
    "pr_number": 12467,
    "pr_file": "pageserver/src/metrics.rs",
    "created_at": "2025-07-07T10:26:13+00:00",
    "commented_code": ".expect(\"failed to define a metric\")\n });\n \n-// Buckets for background operation duration in seconds, like compaction, GC, size calculation.\n+/* BEGIN_HADRON */\n+pub(crate) static STORAGE_ACTIVE_COUNT_PER_TIMELINE: Lazy<IntGaugeVec> = Lazy::new(|| {\n+    register_int_gauge_vec!(\n+        \"pageserver_active_storage_operations_count\",\n+        \"Count of active storage operations with operation, tenant and timeline dimensions\",\n+        &[\"operation\", \"tenant_id\", \"shard_id\", \"timeline_id\"],\n+    )\n+    .expect(\"failed to define a metric\")\n+});",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2189615558",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12467,
        "pr_file": "pageserver/src/metrics.rs",
        "discussion_id": "2189615558",
        "commented_code": "@@ -102,7 +103,18 @@ pub(crate) static STORAGE_TIME_COUNT_PER_TIMELINE: Lazy<IntCounterVec> = Lazy::n\n     .expect(\"failed to define a metric\")\n });\n \n-// Buckets for background operation duration in seconds, like compaction, GC, size calculation.\n+/* BEGIN_HADRON */\n+pub(crate) static STORAGE_ACTIVE_COUNT_PER_TIMELINE: Lazy<IntGaugeVec> = Lazy::new(|| {\n+    register_int_gauge_vec!(\n+        \"pageserver_active_storage_operations_count\",\n+        \"Count of active storage operations with operation, tenant and timeline dimensions\",\n+        &[\"operation\", \"tenant_id\", \"shard_id\", \"timeline_id\"],\n+    )\n+    .expect(\"failed to define a metric\")\n+});",
        "comment_created_at": "2025-07-07T10:26:13+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Is this metric useful at timeline granularity? If the goal is to get a sense of the overall PS background operations, you can look at `rate(pageserver_storage_operations_seconds_count)`, but that's a per-pageserver view.\r\n\r\nAsking, since this is pretty high cardinality.",
        "pr_file_module": null
      },
      {
        "comment_id": "2193079337",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12467,
        "pr_file": "pageserver/src/metrics.rs",
        "discussion_id": "2189615558",
        "commented_code": "@@ -102,7 +103,18 @@ pub(crate) static STORAGE_TIME_COUNT_PER_TIMELINE: Lazy<IntCounterVec> = Lazy::n\n     .expect(\"failed to define a metric\")\n });\n \n-// Buckets for background operation duration in seconds, like compaction, GC, size calculation.\n+/* BEGIN_HADRON */\n+pub(crate) static STORAGE_ACTIVE_COUNT_PER_TIMELINE: Lazy<IntGaugeVec> = Lazy::new(|| {\n+    register_int_gauge_vec!(\n+        \"pageserver_active_storage_operations_count\",\n+        \"Count of active storage operations with operation, tenant and timeline dimensions\",\n+        &[\"operation\", \"tenant_id\", \"shard_id\", \"timeline_id\"],\n+    )\n+    .expect(\"failed to define a metric\")\n+});",
        "comment_created_at": "2025-07-08T17:33:45+00:00",
        "comment_author": "HaoyuHuang",
        "comment_body": "yes, it's useful when we need to troubleshoot individual timelines. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189733469",
    "pr_number": 12467,
    "pr_file": "pageserver/src/page_service.rs",
    "created_at": "2025-07-07T11:05:20+00:00",
    "commented_code": "// Closing the connection by returning ``::Reconnect` has the side effect of rate-limiting above message, via\n                                 // client's reconnect backoff, as well as hopefully prompting the client to load its updated configuration\n                                 // and talk to a different pageserver.\n+                                MISROUTED_PAGESTREAM_REQUESTS.inc();",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2189733469",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12467,
        "pr_file": "pageserver/src/page_service.rs",
        "discussion_id": "2189733469",
        "commented_code": "@@ -1128,6 +1129,7 @@ impl PageServerHandler {\n                                 // Closing the connection by returning ``::Reconnect` has the side effect of rate-limiting above message, via\n                                 // client's reconnect backoff, as well as hopefully prompting the client to load its updated configuration\n                                 // and talk to a different pageserver.\n+                                MISROUTED_PAGESTREAM_REQUESTS.inc();",
        "comment_created_at": "2025-07-07T11:05:20+00:00",
        "comment_author": "VladLazar",
        "comment_body": "This metric is being bumped multiple times for the same request. I think it makes sense to remove the increment from resolve and keep just this one (this is when we actually tell the compute to reconnect).",
        "pr_file_module": null
      },
      {
        "comment_id": "2189912165",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12467,
        "pr_file": "pageserver/src/page_service.rs",
        "discussion_id": "2189733469",
        "commented_code": "@@ -1128,6 +1129,7 @@ impl PageServerHandler {\n                                 // Closing the connection by returning ``::Reconnect` has the side effect of rate-limiting above message, via\n                                 // client's reconnect backoff, as well as hopefully prompting the client to load its updated configuration\n                                 // and talk to a different pageserver.\n+                                MISROUTED_PAGESTREAM_REQUESTS.inc();",
        "comment_created_at": "2025-07-07T12:21:21+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Pushed [b01579a](https://github.com/neondatabase/neon/pull/12467/commits/b01579afad6b604597c95234d59a031c8b5f2329). PTAL",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2185848723",
    "pr_number": 12447,
    "pr_file": "compute_tools/src/metrics.rs",
    "created_at": "2025-07-04T17:38:23+00:00",
    "commented_code": "metrics.extend(AUDIT_LOG_DIR_SIZE.collect());\n     metrics.extend(PG_CURR_DOWNTIME_MS.collect());\n     metrics.extend(PG_TOTAL_DOWNTIME_MS.collect());\n-    metrics.extend(LFC_PREWARM_REQUESTS.collect());\n-    metrics.extend(LFC_OFFLOAD_REQUESTS.collect());\n+    metrics.extend(LFC_PREWARMS.collect());\n+    metrics.extend(LFC_OFFLOADS.collect());",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2185848723",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12447,
        "pr_file": "compute_tools/src/metrics.rs",
        "discussion_id": "2185848723",
        "commented_code": "@@ -124,7 +122,7 @@ pub fn collect() -> Vec<MetricFamily> {\n     metrics.extend(AUDIT_LOG_DIR_SIZE.collect());\n     metrics.extend(PG_CURR_DOWNTIME_MS.collect());\n     metrics.extend(PG_TOTAL_DOWNTIME_MS.collect());\n-    metrics.extend(LFC_PREWARM_REQUESTS.collect());\n-    metrics.extend(LFC_OFFLOAD_REQUESTS.collect());\n+    metrics.extend(LFC_PREWARMS.collect());\n+    metrics.extend(LFC_OFFLOADS.collect());",
        "comment_created_at": "2025-07-04T17:38:23+00:00",
        "comment_author": "ololobus",
        "comment_body": "It usually makes sense to track 2 parameters out of 3: total, failed, success, so that you can always reconstruct all 3. Because just number of requests doesn't tell us much, we care more about success/error rates. Could be a separate PR, up to you",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2182720203",
    "pr_number": 12448,
    "pr_file": "safekeeper/src/wal_backup.rs",
    "created_at": "2025-07-03T12:57:31+00:00",
    "commented_code": "}\n }\n \n+/* BEGIN_HADRON */\n+// On top of the neon determine_offloader, we also check if the current offloader is lagging behind too much.\n+// If it is, we re-elect a new offloader. This mitigates the below issue. It also helps distribute the load across SKs.\n+//\n+// We observe that the offloader fails to upload a segment due to race conditions on XLOG SWITCH and PG start streaming WALs.\n+// wal_backup task continously failing to upload a full segment while the segment remains partial on the disk.\n+// The consequence is that commit_lsn for all SKs move forward but backup_lsn stays the same. Then, all SKs run out of disk space.\n+// See go/sk-ood-xlog-switch for more details.\n+//\n+// To mitigate this issue, we will re-elect a new offloader if the current offloader is lagging behind too much.\n+// Each SK makes the decision locally but they are aware of each other's commit and backup lsns.\n+//\n+// determine_offloader will pick a SK. say SK-1.\n+// Each SK checks\n+// -- if commit_lsn - back_lsn > threshold,\n+// -- -- remove SK-1 from the candidate and call determine_offloader again.\n+// SK-1 will step down and all SKs will elect the same leader again.\n+// After the backup is caught up, the leader will become SK-1 again.\n+fn hadron_determine_offloader(mgr: &Manager, state: &StateSnapshot) -> (Option<NodeId>, String) {\n+    let mut offloader: Option<NodeId>;\n+    let mut election_dbg_str: String;\n+    let caughtup_peers_count: usize;\n+    (offloader, election_dbg_str, caughtup_peers_count) =\n+        determine_offloader(&state.peers, state.backup_lsn, mgr.tli.ttid, &mgr.conf);\n+\n+    if offloader.is_none() || caughtup_peers_count <= 1 {\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    let offloader_sk_id = offloader.unwrap();\n+\n+    let backup_lag = state.commit_lsn.checked_sub(state.backup_lsn);\n+    if backup_lag.is_none() {\n+        info!(\"Backup lag is None. Skipping re-election.\");\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    let backup_lag = backup_lag.unwrap().0;\n+\n+    if backup_lag < mgr.conf.max_reelect_offloader_lag_bytes {\n+        info!(\n+            \"Backup lag {} is lower than the threshold {}. Skipping re-election.\",\n+            backup_lag, mgr.conf.max_reelect_offloader_lag_bytes\n+        );\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    info!(\n+        \"Electing a new leader: Backup lag is too high backup lsn lag {} threshold {}: {}\",\n+        backup_lag, mgr.conf.max_reelect_offloader_lag_bytes, election_dbg_str\n+    );\n+    BACKUP_REELECT_LEADER_COUNT.inc();\n+    // Remove the current offloader if lag is too high.\n+    let new_peers: Vec<_> = state\n+        .peers\n+        .iter()\n+        .filter(|p| p.sk_id != offloader_sk_id)\n+        .cloned()\n+        .collect();",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2182720203",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12448,
        "pr_file": "safekeeper/src/wal_backup.rs",
        "discussion_id": "2182720203",
        "commented_code": "@@ -127,6 +125,71 @@ async fn shut_down_task(entry: &mut Option<WalBackupTaskHandle>) {\n     }\n }\n \n+/* BEGIN_HADRON */\n+// On top of the neon determine_offloader, we also check if the current offloader is lagging behind too much.\n+// If it is, we re-elect a new offloader. This mitigates the below issue. It also helps distribute the load across SKs.\n+//\n+// We observe that the offloader fails to upload a segment due to race conditions on XLOG SWITCH and PG start streaming WALs.\n+// wal_backup task continously failing to upload a full segment while the segment remains partial on the disk.\n+// The consequence is that commit_lsn for all SKs move forward but backup_lsn stays the same. Then, all SKs run out of disk space.\n+// See go/sk-ood-xlog-switch for more details.\n+//\n+// To mitigate this issue, we will re-elect a new offloader if the current offloader is lagging behind too much.\n+// Each SK makes the decision locally but they are aware of each other's commit and backup lsns.\n+//\n+// determine_offloader will pick a SK. say SK-1.\n+// Each SK checks\n+// -- if commit_lsn - back_lsn > threshold,\n+// -- -- remove SK-1 from the candidate and call determine_offloader again.\n+// SK-1 will step down and all SKs will elect the same leader again.\n+// After the backup is caught up, the leader will become SK-1 again.\n+fn hadron_determine_offloader(mgr: &Manager, state: &StateSnapshot) -> (Option<NodeId>, String) {\n+    let mut offloader: Option<NodeId>;\n+    let mut election_dbg_str: String;\n+    let caughtup_peers_count: usize;\n+    (offloader, election_dbg_str, caughtup_peers_count) =\n+        determine_offloader(&state.peers, state.backup_lsn, mgr.tli.ttid, &mgr.conf);\n+\n+    if offloader.is_none() || caughtup_peers_count <= 1 {\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    let offloader_sk_id = offloader.unwrap();\n+\n+    let backup_lag = state.commit_lsn.checked_sub(state.backup_lsn);\n+    if backup_lag.is_none() {\n+        info!(\"Backup lag is None. Skipping re-election.\");\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    let backup_lag = backup_lag.unwrap().0;\n+\n+    if backup_lag < mgr.conf.max_reelect_offloader_lag_bytes {\n+        info!(\n+            \"Backup lag {} is lower than the threshold {}. Skipping re-election.\",\n+            backup_lag, mgr.conf.max_reelect_offloader_lag_bytes\n+        );\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    info!(\n+        \"Electing a new leader: Backup lag is too high backup lsn lag {} threshold {}: {}\",\n+        backup_lag, mgr.conf.max_reelect_offloader_lag_bytes, election_dbg_str\n+    );\n+    BACKUP_REELECT_LEADER_COUNT.inc();\n+    // Remove the current offloader if lag is too high.\n+    let new_peers: Vec<_> = state\n+        .peers\n+        .iter()\n+        .filter(|p| p.sk_id != offloader_sk_id)\n+        .cloned()\n+        .collect();",
        "comment_created_at": "2025-07-03T12:57:31+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Not sure I understand this. If the current offloader is indeed lagging, then the call to `determine_offloader` would have already picked a different offloader. I mean `offloader_sk_id` might not actually be the current offloader.",
        "pr_file_module": null
      },
      {
        "comment_id": "2182949056",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12448,
        "pr_file": "safekeeper/src/wal_backup.rs",
        "discussion_id": "2182720203",
        "commented_code": "@@ -127,6 +125,71 @@ async fn shut_down_task(entry: &mut Option<WalBackupTaskHandle>) {\n     }\n }\n \n+/* BEGIN_HADRON */\n+// On top of the neon determine_offloader, we also check if the current offloader is lagging behind too much.\n+// If it is, we re-elect a new offloader. This mitigates the below issue. It also helps distribute the load across SKs.\n+//\n+// We observe that the offloader fails to upload a segment due to race conditions on XLOG SWITCH and PG start streaming WALs.\n+// wal_backup task continously failing to upload a full segment while the segment remains partial on the disk.\n+// The consequence is that commit_lsn for all SKs move forward but backup_lsn stays the same. Then, all SKs run out of disk space.\n+// See go/sk-ood-xlog-switch for more details.\n+//\n+// To mitigate this issue, we will re-elect a new offloader if the current offloader is lagging behind too much.\n+// Each SK makes the decision locally but they are aware of each other's commit and backup lsns.\n+//\n+// determine_offloader will pick a SK. say SK-1.\n+// Each SK checks\n+// -- if commit_lsn - back_lsn > threshold,\n+// -- -- remove SK-1 from the candidate and call determine_offloader again.\n+// SK-1 will step down and all SKs will elect the same leader again.\n+// After the backup is caught up, the leader will become SK-1 again.\n+fn hadron_determine_offloader(mgr: &Manager, state: &StateSnapshot) -> (Option<NodeId>, String) {\n+    let mut offloader: Option<NodeId>;\n+    let mut election_dbg_str: String;\n+    let caughtup_peers_count: usize;\n+    (offloader, election_dbg_str, caughtup_peers_count) =\n+        determine_offloader(&state.peers, state.backup_lsn, mgr.tli.ttid, &mgr.conf);\n+\n+    if offloader.is_none() || caughtup_peers_count <= 1 {\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    let offloader_sk_id = offloader.unwrap();\n+\n+    let backup_lag = state.commit_lsn.checked_sub(state.backup_lsn);\n+    if backup_lag.is_none() {\n+        info!(\"Backup lag is None. Skipping re-election.\");\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    let backup_lag = backup_lag.unwrap().0;\n+\n+    if backup_lag < mgr.conf.max_reelect_offloader_lag_bytes {\n+        info!(\n+            \"Backup lag {} is lower than the threshold {}. Skipping re-election.\",\n+            backup_lag, mgr.conf.max_reelect_offloader_lag_bytes\n+        );\n+        return (offloader, election_dbg_str);\n+    }\n+\n+    info!(\n+        \"Electing a new leader: Backup lag is too high backup lsn lag {} threshold {}: {}\",\n+        backup_lag, mgr.conf.max_reelect_offloader_lag_bytes, election_dbg_str\n+    );\n+    BACKUP_REELECT_LEADER_COUNT.inc();\n+    // Remove the current offloader if lag is too high.\n+    let new_peers: Vec<_> = state\n+        .peers\n+        .iter()\n+        .filter(|p| p.sk_id != offloader_sk_id)\n+        .cloned()\n+        .collect();",
        "comment_created_at": "2025-07-03T14:31:38+00:00",
        "comment_author": "HaoyuHuang",
        "comment_body": "The problem we saw earlier is that the backup leader keeps failing to backup due to a bug. XLOG SWITCH switched the WAL segment file. However, the SK still keeps it as .partial but commit lsn went ahead. \r\n\r\nWAL backup then computes the LSN -> segment file name as a full seg file but it couldn't find the file since it's still partial. \r\n\r\nI was worried about changing the rename logic so instead just re-elect another leader. \r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2095936036",
    "pr_number": 11756,
    "pr_file": "pageserver/src/consumption_metrics/metrics.rs",
    "created_at": "2025-05-19T15:10:17+00:00",
    "commented_code": "});\n         }\n \n+        // Compute the PITR cutoff. We have to make sure this doesn't regress:\n+        // on restart, GC may not run for a while, and the PITR cutoff will be\n+        // reported as 0 (which would bill the user for the entire history from\n+        // 0 to last_record_lsn). We therefore clamp this to the cached value",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2095936036",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11756,
        "pr_file": "pageserver/src/consumption_metrics/metrics.rs",
        "discussion_id": "2095936036",
        "commented_code": "@@ -441,6 +464,25 @@ impl TimelineSnapshot {\n             });\n         }\n \n+        // Compute the PITR cutoff. We have to make sure this doesn't regress:\n+        // on restart, GC may not run for a while, and the PITR cutoff will be\n+        // reported as 0 (which would bill the user for the entire history from\n+        // 0 to last_record_lsn). We therefore clamp this to the cached value",
        "comment_created_at": "2025-05-19T15:10:17+00:00",
        "comment_author": "jcsp",
        "comment_body": "This doesn't sound totally safe: the value is only cached across restarts when restarting on the same physical pageserver -- if the tenant was just attached on a different pageserver then it'll start from scratch.\r\n\r\nShould we just avoid emitting this when we haven't initialized GC cutoffs yet?",
        "pr_file_module": null
      },
      {
        "comment_id": "2098077052",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11756,
        "pr_file": "pageserver/src/consumption_metrics/metrics.rs",
        "discussion_id": "2095936036",
        "commented_code": "@@ -441,6 +464,25 @@ impl TimelineSnapshot {\n             });\n         }\n \n+        // Compute the PITR cutoff. We have to make sure this doesn't regress:\n+        // on restart, GC may not run for a while, and the PITR cutoff will be\n+        // reported as 0 (which would bill the user for the entire history from\n+        // 0 to last_record_lsn). We therefore clamp this to the cached value",
        "comment_created_at": "2025-05-20T14:05:33+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "I don't know if we can reliably do this with the current `gc_info`.\r\n\r\n`GcCutoffs.time` (PITR cutoff) will be 0 when the start of the timeline is within the PITR window (e.g. for the first 7 days of the timeline). However, `GcCutoffs.space` will be initialized with a non-zero value during tenant activation, before the PITR cutoff is computed. This means that we can't disambiguate an uninitialized PITR cutoff from one that falls below the timeline creation. As a consequence, once a timeline is e.g. 7 days old, it's PITR history size will suddenly jump from 0 to some large number.\r\n\r\nTo disambiguate the uninitialized case from the new timeline case, we'll need to change `GcCutoffs.time` from an `Lsn` to an `Option<Lsn>`. Does that sound good?\r\n\r\nFurthermore, if we can't rely on the cache for PITR cutoff monotonicity, then any PITR window increase by the user will have a retroactive effect on billing (in the case where it actually takes effect for GC, which is currently only in the case of a tenant restart/migration). I just want to confirm that we're okay with that.",
        "pr_file_module": null
      },
      {
        "comment_id": "2098084711",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11756,
        "pr_file": "pageserver/src/consumption_metrics/metrics.rs",
        "discussion_id": "2095936036",
        "commented_code": "@@ -441,6 +464,25 @@ impl TimelineSnapshot {\n             });\n         }\n \n+        // Compute the PITR cutoff. We have to make sure this doesn't regress:\n+        // on restart, GC may not run for a while, and the PITR cutoff will be\n+        // reported as 0 (which would bill the user for the entire history from\n+        // 0 to last_record_lsn). We therefore clamp this to the cached value",
        "comment_created_at": "2025-05-20T14:08:26+00:00",
        "comment_author": "jcsp",
        "comment_body": ">change GcCutoffs.time from an Lsn to an Option<Lsn>. Does that sound good?\r\n\r\nYes, I never liked use of Lsn::Invalid anyway, would prefer an Option.\r\n\r\n(I'm still a bit unsure about under which circumstances we need pitr_cutoff reported externally, vs. using the history size metric from the other PR)",
        "pr_file_module": null
      },
      {
        "comment_id": "2098092396",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11756,
        "pr_file": "pageserver/src/consumption_metrics/metrics.rs",
        "discussion_id": "2095936036",
        "commented_code": "@@ -441,6 +464,25 @@ impl TimelineSnapshot {\n             });\n         }\n \n+        // Compute the PITR cutoff. We have to make sure this doesn't regress:\n+        // on restart, GC may not run for a while, and the PITR cutoff will be\n+        // reported as 0 (which would bill the user for the entire history from\n+        // 0 to last_record_lsn). We therefore clamp this to the cached value",
        "comment_created_at": "2025-05-20T14:11:48+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "> I'm still a bit unsure about under which circumstances we need pitr_cutoff reported externally, vs. using the history size metric from the other PR\r\n\r\nIt was needed to guarantee monotonicity, but as you point out it doesn't guarantee that anyway, so we'll have to accept that the PITR history size can retroactively change (sometimes) when the user changes their PITR window. I'll axe it.\r\n\r\nIf we did report it externally, then we could use it to enforce monotonicity in the billing system, which presumably _does_ have access to the previous value. But then we'd have to compute the PITR history size there.",
        "pr_file_module": null
      },
      {
        "comment_id": "2098243536",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11756,
        "pr_file": "pageserver/src/consumption_metrics/metrics.rs",
        "discussion_id": "2095936036",
        "commented_code": "@@ -441,6 +464,25 @@ impl TimelineSnapshot {\n             });\n         }\n \n+        // Compute the PITR cutoff. We have to make sure this doesn't regress:\n+        // on restart, GC may not run for a while, and the PITR cutoff will be\n+        // reported as 0 (which would bill the user for the entire history from\n+        // 0 to last_record_lsn). We therefore clamp this to the cached value",
        "comment_created_at": "2025-05-20T15:14:15+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "https://github.com/neondatabase/neon/pull/11984",
        "pr_file_module": null
      }
    ]
  }
]