[
  {
    "discussion_id": "2193206338",
    "pr_number": 12484,
    "pr_file": "docs/rfcs/2025-07-07-node-deletion-api-improvement.md",
    "created_at": "2025-07-08T18:37:39+00:00",
    "commented_code": "+# Node deletion API improvement\n+\n+Created on 2025-07-07\n+Implemented on _TBD_\n+\n+## Summary\n+\n+This RFC describes improvements to the storage controller API for gracefully deleting pageserver\n+nodes.\n+\n+## Motivation\n+\n+The basic node deletion API introduced in [#8226](https://github.com/neondatabase/neon/issues/8333)\n+has several limitations:\n+\n+- Deleted nodes can re-add themselves if they restart (e.g., a flaky node that keeps restarting and\n+we cannot reach via SSH to stop the pageserver). This issue has been resolved by tombstone\n+mechanism in [#12036](https://github.com/neondatabase/neon/issues/12036)\n+- Process of node deletion is not graceful, i.e. it just imitates a node failure, which can cause\n+issues for user processes\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+\n+- storage controller\n+- pageserver (indirectly)\n+\n+## Proposed implementation\n+\n+### Tombstones\n+\n+To resolve the problem of deleted nodes re-adding themselves, a tombstone mechanism was introduced\n+as part of the node stored information. Each node has a separate `NodeLifecycle` field with two\n+possible states: `Active` and `Deleted`. When node deletion completes, the database row is not\n+deleted but instead has its `NodeLifecycle` column switched to `Deleted`. Nodes with `Deleted`\n+lifecycle are treated as if the row is absent for most handlers, with several exceptions: reattach\n+and register functionality must be aware of tombstones. Additionally, new debug handlers are\n+available for listing and deleting tombstones via the `/debug/v1/tombstone` path.\n+\n+### Gracefulness\n+\n+The problem of making node deletion graceful is complex and involves several challenges:\n+\n+- **Cancellable**: Since graceful deletion can be time-consuming, the operation must be cancellable\n+to allow administrators to abort the process if needed, e.g. if run by mistake.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2193206338",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12484,
        "pr_file": "docs/rfcs/2025-07-07-node-deletion-api-improvement.md",
        "discussion_id": "2193206338",
        "commented_code": "@@ -0,0 +1,149 @@\n+# Node deletion API improvement\n+\n+Created on 2025-07-07\n+Implemented on _TBD_\n+\n+## Summary\n+\n+This RFC describes improvements to the storage controller API for gracefully deleting pageserver\n+nodes.\n+\n+## Motivation\n+\n+The basic node deletion API introduced in [#8226](https://github.com/neondatabase/neon/issues/8333)\n+has several limitations:\n+\n+- Deleted nodes can re-add themselves if they restart (e.g., a flaky node that keeps restarting and\n+we cannot reach via SSH to stop the pageserver). This issue has been resolved by tombstone\n+mechanism in [#12036](https://github.com/neondatabase/neon/issues/12036)\n+- Process of node deletion is not graceful, i.e. it just imitates a node failure, which can cause\n+issues for user processes\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+\n+- storage controller\n+- pageserver (indirectly)\n+\n+## Proposed implementation\n+\n+### Tombstones\n+\n+To resolve the problem of deleted nodes re-adding themselves, a tombstone mechanism was introduced\n+as part of the node stored information. Each node has a separate `NodeLifecycle` field with two\n+possible states: `Active` and `Deleted`. When node deletion completes, the database row is not\n+deleted but instead has its `NodeLifecycle` column switched to `Deleted`. Nodes with `Deleted`\n+lifecycle are treated as if the row is absent for most handlers, with several exceptions: reattach\n+and register functionality must be aware of tombstones. Additionally, new debug handlers are\n+available for listing and deleting tombstones via the `/debug/v1/tombstone` path.\n+\n+### Gracefulness\n+\n+The problem of making node deletion graceful is complex and involves several challenges:\n+\n+- **Cancellable**: Since graceful deletion can be time-consuming, the operation must be cancellable\n+to allow administrators to abort the process if needed, e.g. if run by mistake.",
        "comment_created_at": "2025-07-08T18:37:39+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Let's be kind to future readers and mention why it's time consuming.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1987750964",
    "pr_number": 9661,
    "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
    "created_at": "2025-03-10T17:40:31+00:00",
    "commented_code": "+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint, without granting users access to an unlimited\n+storage device.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3 or any blob storage of choice.\n+\n+Upon deletion of the Endpoint, or reassignment of the endpoint to a different\n+branch, this ephemeral data is dropped: the data stored may not match the\n+state of the branch's data after reassignment, and on endpoint deletion the\n+data won't have any use to the user.\n+\n+Compute gets credentials which it can use to authenticate to this new service\n+and retrieve and store data associated with that endpoint.\n+\n+The path of this endpoint data in S3 is initially as follows:\n+\n+    s3://<regional-epufs-bucket>/\n+        tenant-<hex-tenant-id>/\n+            tl-<hex-timeline-id>/\n+                <endpoint-id>/\n+                    pgdata/<file_path_in_pgdatadir>`\n+\n+For other blob storages an equivalent or similar path can be constructed.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+Most, if not all, blob storage services have sufficiently high persistence\n+guarantees to cater our need for persistence and uptime. The only concern with\n+blob storages is that the access latency is generally higher than local disk,\n+but for the object types stored (stats files, cache state, ...) I don't think \n+this will be much of an issue.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as well-known name of AWS' blob storage - Azure Blob\n+Storage should work too, but is a much longer name.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    co-->ep: Connect with credentials\n+    co->>+ep: Store Unlogged Persistent File",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "1987750964",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 9661,
        "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
        "discussion_id": "1987750964",
        "commented_code": "@@ -0,0 +1,288 @@\n+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint, without granting users access to an unlimited\n+storage device.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3 or any blob storage of choice.\n+\n+Upon deletion of the Endpoint, or reassignment of the endpoint to a different\n+branch, this ephemeral data is dropped: the data stored may not match the\n+state of the branch's data after reassignment, and on endpoint deletion the\n+data won't have any use to the user.\n+\n+Compute gets credentials which it can use to authenticate to this new service\n+and retrieve and store data associated with that endpoint.\n+\n+The path of this endpoint data in S3 is initially as follows:\n+\n+    s3://<regional-epufs-bucket>/\n+        tenant-<hex-tenant-id>/\n+            tl-<hex-timeline-id>/\n+                <endpoint-id>/\n+                    pgdata/<file_path_in_pgdatadir>`\n+\n+For other blob storages an equivalent or similar path can be constructed.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+Most, if not all, blob storage services have sufficiently high persistence\n+guarantees to cater our need for persistence and uptime. The only concern with\n+blob storages is that the access latency is generally higher than local disk,\n+but for the object types stored (stats files, cache state, ...) I don't think \n+this will be much of an issue.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as well-known name of AWS' blob storage - Azure Blob\n+Storage should work too, but is a much longer name.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    co-->ep: Connect with credentials\n+    co->>+ep: Store Unlogged Persistent File",
        "comment_created_at": "2025-03-10T17:40:31+00:00",
        "comment_author": "ololobus",
        "comment_body": "Let's provide a brief API spec for this EPUFS service, i.e. what are the methods and parameters we are going to have:\r\n- PUT: tenant, timeline, endpoint, relative path, data -> json response\r\n- GET: tenant, timeline, endpoint, relative path -> file content response\r\n- DELETE: tenant [ timeline [ endpoint ] ] -> json response",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1989786636",
    "pr_number": 9661,
    "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
    "created_at": "2025-03-11T17:15:46+00:00",
    "commented_code": "+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint, without granting users access to an unlimited\n+storage device.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3 or any blob storage of choice.\n+\n+Upon deletion of the Endpoint, or reassignment of the endpoint to a different\n+branch, this ephemeral data is dropped: the data stored may not match the\n+state of the branch's data after reassignment, and on endpoint deletion the\n+data won't have any use to the user.\n+\n+Compute gets credentials which it can use to authenticate to this new service\n+and retrieve and store data associated with that endpoint.\n+\n+The path of this endpoint data in S3 is initially as follows:\n+\n+    s3://<regional-epufs-bucket>/\n+        tenant-<hex-tenant-id>/\n+            tl-<hex-timeline-id>/\n+                <endpoint-id>/\n+                    pgdata/<file_path_in_pgdatadir>`\n+\n+For other blob storages an equivalent or similar path can be constructed.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+Most, if not all, blob storage services have sufficiently high persistence\n+guarantees to cater our need for persistence and uptime. The only concern with\n+blob storages is that the access latency is generally higher than local disk,\n+but for the object types stored (stats files, cache state, ...) I don't think \n+this will be much of an issue.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as well-known name of AWS' blob storage - Azure Blob\n+Storage should work too, but is a much longer name.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    co-->ep: Connect with credentials\n+    co->>+ep: Store Unlogged Persistent File\n+    opt is authenticated\n+        ep->>s3: Write UPF to S3\n+    end\n+    ep->>-co: OK / Failure / Auth Failure\n+    co-->ep: Cancel connection\n+```\n+\n+Read data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    co->>+ep: Read Unlogged Persistent File\n+    opt is authenticated\n+        ep->>+s3: Request UPF from storage\n+        s3->>-ep: Receive UPF from storage\n+    end\n+    ep->>-co: OK(response) / Failure(storage, auth, ...)\n+```\n+\n+Compute Startup:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ps as Pageserver\n+    participant ep as EPUFS\n+    participant es as Extension server\n+\n+    note over co: Bind endpoint ep-xxx\n+    par Get basebackup\n+        co->>+ps: Request basebackup @ LSN\n+        ps-)ps: Construct basebackup\n+        ps->>-co: Receive basebackup TAR @ LSN\n+    and Get startup-critical Unlogged Persistent Files\n+        co->>+ep: Get all UPFs of endpoint ep-xxx\n+        ep-)ep: Retrieve and gather all UPFs\n+        ep->>-co: TAR of UPFs\n+    and Get startup-critical extensions\n+        loop For every startup-critical extension\n+            co->>es: Get critical extension\n+            es->>co: Receive critical extension\n+        end\n+    end\n+    note over co: Start compute\n+```\n+\n+CPlane ops:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant cp as Control Plane\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    alt Tenant deleted\n+        cp-)ep: Tenant deleted\n+        loop\n+            ep->>s3: Remove data of deleted tenant from Storage\n+        end\n+        opt\n+            ep-)cp: Tenant cleanup complete\n+        end\n+    else Endpoint reassigned\n+        cp->>+ep: Endpoint reassigned\n+        loop For every object associated with reassigned endpoint\n+            ep->>s3: Remove data from Storage\n+        end\n+        ep->>-cp: Cleanup complete\n+    else Endpoint removed",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "1989786636",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 9661,
        "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
        "discussion_id": "1989786636",
        "commented_code": "@@ -0,0 +1,288 @@\n+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint, without granting users access to an unlimited\n+storage device.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3 or any blob storage of choice.\n+\n+Upon deletion of the Endpoint, or reassignment of the endpoint to a different\n+branch, this ephemeral data is dropped: the data stored may not match the\n+state of the branch's data after reassignment, and on endpoint deletion the\n+data won't have any use to the user.\n+\n+Compute gets credentials which it can use to authenticate to this new service\n+and retrieve and store data associated with that endpoint.\n+\n+The path of this endpoint data in S3 is initially as follows:\n+\n+    s3://<regional-epufs-bucket>/\n+        tenant-<hex-tenant-id>/\n+            tl-<hex-timeline-id>/\n+                <endpoint-id>/\n+                    pgdata/<file_path_in_pgdatadir>`\n+\n+For other blob storages an equivalent or similar path can be constructed.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+Most, if not all, blob storage services have sufficiently high persistence\n+guarantees to cater our need for persistence and uptime. The only concern with\n+blob storages is that the access latency is generally higher than local disk,\n+but for the object types stored (stats files, cache state, ...) I don't think \n+this will be much of an issue.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as well-known name of AWS' blob storage - Azure Blob\n+Storage should work too, but is a much longer name.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    co-->ep: Connect with credentials\n+    co->>+ep: Store Unlogged Persistent File\n+    opt is authenticated\n+        ep->>s3: Write UPF to S3\n+    end\n+    ep->>-co: OK / Failure / Auth Failure\n+    co-->ep: Cancel connection\n+```\n+\n+Read data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    co->>+ep: Read Unlogged Persistent File\n+    opt is authenticated\n+        ep->>+s3: Request UPF from storage\n+        s3->>-ep: Receive UPF from storage\n+    end\n+    ep->>-co: OK(response) / Failure(storage, auth, ...)\n+```\n+\n+Compute Startup:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ps as Pageserver\n+    participant ep as EPUFS\n+    participant es as Extension server\n+\n+    note over co: Bind endpoint ep-xxx\n+    par Get basebackup\n+        co->>+ps: Request basebackup @ LSN\n+        ps-)ps: Construct basebackup\n+        ps->>-co: Receive basebackup TAR @ LSN\n+    and Get startup-critical Unlogged Persistent Files\n+        co->>+ep: Get all UPFs of endpoint ep-xxx\n+        ep-)ep: Retrieve and gather all UPFs\n+        ep->>-co: TAR of UPFs\n+    and Get startup-critical extensions\n+        loop For every startup-critical extension\n+            co->>es: Get critical extension\n+            es->>co: Receive critical extension\n+        end\n+    end\n+    note over co: Start compute\n+```\n+\n+CPlane ops:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant cp as Control Plane\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    alt Tenant deleted\n+        cp-)ep: Tenant deleted\n+        loop\n+            ep->>s3: Remove data of deleted tenant from Storage\n+        end\n+        opt\n+            ep-)cp: Tenant cleanup complete\n+        end\n+    else Endpoint reassigned\n+        cp->>+ep: Endpoint reassigned\n+        loop For every object associated with reassigned endpoint\n+            ep->>s3: Remove data from Storage\n+        end\n+        ep->>-cp: Cleanup complete\n+    else Endpoint removed",
        "comment_created_at": "2025-03-11T17:15:46+00:00",
        "comment_author": "jcsp",
        "comment_body": "Does this mean that deleting an endpoint requires starting a compute, or would control plane be calling directly into the S3 proxy service?",
        "pr_file_module": null
      },
      {
        "comment_id": "1989789532",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 9661,
        "pr_file": "docs/rfcs/040-Endpoint-Persistent-Unlogged-Files-Storage.md",
        "discussion_id": "1989786636",
        "commented_code": "@@ -0,0 +1,288 @@\n+# Memo: Endpoint Persistent Unlogged Files Storage\n+Created on 2024-11-05\n+Implemented on N/A\n+\n+## Summary\n+A design for a storage system that allows storage of files required to make\n+Neon's Endpoints have a better experience at or after a reboot.\n+\n+## Motivation\n+Several systems inside PostgreSQL (and Neon) need some persistent storage for\n+optimal workings across reboots and restarts, but still work without.\n+Examples are the cumulative statistics file in `pg_stat/global.stat`,\n+`pg_stat_statements`' `pg_stat/pg_stat_statements.stat`, and `pg_prewarm`'s\n+`autoprewarm.blocks`.  We need a storage system that can store and manage\n+these files for each Endpoint, without granting users access to an unlimited\n+storage device.\n+\n+## Goals\n+- Store known files for Endpoints with reasonable persistence.  \n+  _Data loss in this service, while annoying and bad for UX, won't lose any\n+  customer's data._\n+\n+## Non Goals (if relevant)\n+- This storage system does not need branching, file versioning, or other such\n+  features. The files are as ephemeral to the timeline of the data as the\n+  Endpoints that host the data.\n+- This storage system does not need to store _all_ user files, only 'known'\n+  user files.\n+- This storage system does not need to be hosted fully inside Computes.  \n+  _Instead, this will be a separate component similar to Pageserver,\n+  SafeKeeper, the S3 proxy used for dynamically loaded extensions, etc._\n+\n+## Impacted components (e.g. pageserver, safekeeper, console, etc)\n+- Compute needs new code to load and store these files in its lifetime.\n+- Console or Control Plane needs to consider this new storage system when\n+  signalling the deletion of an Endpoint, Timeline, or Tenant.\n+\n+A new service is created: the Endpoint Persistent Unlogged Files Storage\n+service.  This could be integrated in e.g. Pageserver or Control Plane, or a\n+separately hosted service.\n+\n+## Proposed implementation\n+Endpoint-related data files are managed by a newly designed service (which\n+optionally is integrated in an existing service like Pageserver or Control\n+Plane), which stores data directly into S3 or any blob storage of choice.\n+\n+Upon deletion of the Endpoint, or reassignment of the endpoint to a different\n+branch, this ephemeral data is dropped: the data stored may not match the\n+state of the branch's data after reassignment, and on endpoint deletion the\n+data won't have any use to the user.\n+\n+Compute gets credentials which it can use to authenticate to this new service\n+and retrieve and store data associated with that endpoint.\n+\n+The path of this endpoint data in S3 is initially as follows:\n+\n+    s3://<regional-epufs-bucket>/\n+        tenant-<hex-tenant-id>/\n+            tl-<hex-timeline-id>/\n+                <endpoint-id>/\n+                    pgdata/<file_path_in_pgdatadir>`\n+\n+For other blob storages an equivalent or similar path can be constructed.\n+\n+### Reliability, failure modes and corner cases (if relevant)\n+Reliability is important, but not critical to the workings of Neon.  The data\n+stored in this service will, when lost, reduce performance, but won't be a\n+cause of permanent data loss - only operational metadata is stored.\n+\n+Most, if not all, blob storage services have sufficiently high persistence\n+guarantees to cater our need for persistence and uptime. The only concern with\n+blob storages is that the access latency is generally higher than local disk,\n+but for the object types stored (stats files, cache state, ...) I don't think \n+this will be much of an issue.\n+\n+### Interaction/Sequence diagram (if relevant)\n+\n+In these diagrams you can replace S3 with any persistent storage device of\n+choice, but S3 is chosen as well-known name of AWS' blob storage - Azure Blob\n+Storage should work too, but is a much longer name.\n+\n+Write data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    co-->ep: Connect with credentials\n+    co->>+ep: Store Unlogged Persistent File\n+    opt is authenticated\n+        ep->>s3: Write UPF to S3\n+    end\n+    ep->>-co: OK / Failure / Auth Failure\n+    co-->ep: Cancel connection\n+```\n+\n+Read data:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    co->>+ep: Read Unlogged Persistent File\n+    opt is authenticated\n+        ep->>+s3: Request UPF from storage\n+        s3->>-ep: Receive UPF from storage\n+    end\n+    ep->>-co: OK(response) / Failure(storage, auth, ...)\n+```\n+\n+Compute Startup:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant co as Compute\n+    participant ps as Pageserver\n+    participant ep as EPUFS\n+    participant es as Extension server\n+\n+    note over co: Bind endpoint ep-xxx\n+    par Get basebackup\n+        co->>+ps: Request basebackup @ LSN\n+        ps-)ps: Construct basebackup\n+        ps->>-co: Receive basebackup TAR @ LSN\n+    and Get startup-critical Unlogged Persistent Files\n+        co->>+ep: Get all UPFs of endpoint ep-xxx\n+        ep-)ep: Retrieve and gather all UPFs\n+        ep->>-co: TAR of UPFs\n+    and Get startup-critical extensions\n+        loop For every startup-critical extension\n+            co->>es: Get critical extension\n+            es->>co: Receive critical extension\n+        end\n+    end\n+    note over co: Start compute\n+```\n+\n+CPlane ops:\n+```mermaid\n+sequenceDiagram\n+    autonumber\n+    participant cp as Control Plane\n+    participant ep as EPUFS\n+    participant s3 as Blob Storage\n+\n+    alt Tenant deleted\n+        cp-)ep: Tenant deleted\n+        loop\n+            ep->>s3: Remove data of deleted tenant from Storage\n+        end\n+        opt\n+            ep-)cp: Tenant cleanup complete\n+        end\n+    else Endpoint reassigned\n+        cp->>+ep: Endpoint reassigned\n+        loop For every object associated with reassigned endpoint\n+            ep->>s3: Remove data from Storage\n+        end\n+        ep->>-cp: Cleanup complete\n+    else Endpoint removed",
        "comment_created_at": "2025-03-11T17:17:29+00:00",
        "comment_author": "MMeent",
        "comment_body": "CPlane would be hitting this service.\r\n\r\n(Note that we're not necessarily using S3; any blob storage will do)",
        "pr_file_module": null
      }
    ]
  }
]