[
  {
    "discussion_id": "2092802901",
    "pr_number": 11937,
    "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
    "created_at": "2025-05-16T10:37:54+00:00",
    "commented_code": "opened_at: Instant,\n \n-    /// The above fields never change, except for `end_lsn`, which is only set once.\n+    /// All versions of all pages in the layer are kept here. Indexed\n+    /// by block number and LSN. The [`IndexEntry`] is an offset into the\n+    /// ephemeral file where the page version is stored.\n+    ///\n+    /// We use a separate lock for the index to reduce the critical section\n+    /// during which reads cannot be planned.\n+    index: RwLock<BTreeMap<CompactKey, VecMap<Lsn, IndexEntry>>>,\n+\n+    /// The above fields never change, except for `end_lsn`, which is only set once,\n+    /// and `index` (see rationale there).\n     /// All other changing parts are in `inner`, and protected by a mutex.\n     inner: RwLock<InMemoryLayerInner>,",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2092802901",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092802901",
        "commented_code": "@@ -63,7 +63,16 @@ pub struct InMemoryLayer {\n \n     opened_at: Instant,\n \n-    /// The above fields never change, except for `end_lsn`, which is only set once.\n+    /// All versions of all pages in the layer are kept here. Indexed\n+    /// by block number and LSN. The [`IndexEntry`] is an offset into the\n+    /// ephemeral file where the page version is stored.\n+    ///\n+    /// We use a separate lock for the index to reduce the critical section\n+    /// during which reads cannot be planned.\n+    index: RwLock<BTreeMap<CompactKey, VecMap<Lsn, IndexEntry>>>,\n+\n+    /// The above fields never change, except for `end_lsn`, which is only set once,\n+    /// and `index` (see rationale there).\n     /// All other changing parts are in `inner`, and protected by a mutex.\n     inner: RwLock<InMemoryLayerInner>,",
        "comment_created_at": "2025-05-16T10:37:54+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "This should specify the locking order between `index` and `inner`, to avoid deadlocks.",
        "pr_file_module": null
      },
      {
        "comment_id": "2096122351",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092802901",
        "commented_code": "@@ -63,7 +63,16 @@ pub struct InMemoryLayer {\n \n     opened_at: Instant,\n \n-    /// The above fields never change, except for `end_lsn`, which is only set once.\n+    /// All versions of all pages in the layer are kept here. Indexed\n+    /// by block number and LSN. The [`IndexEntry`] is an offset into the\n+    /// ephemeral file where the page version is stored.\n+    ///\n+    /// We use a separate lock for the index to reduce the critical section\n+    /// during which reads cannot be planned.\n+    index: RwLock<BTreeMap<CompactKey, VecMap<Lsn, IndexEntry>>>,\n+\n+    /// The above fields never change, except for `end_lsn`, which is only set once,\n+    /// and `index` (see rationale there).\n     /// All other changing parts are in `inner`, and protected by a mutex.\n     inner: RwLock<InMemoryLayerInner>,",
        "comment_created_at": "2025-05-19T16:37:33+00:00",
        "comment_author": "VladLazar",
        "comment_body": "https://github.com/neondatabase/neon/pull/11937/commits/9377e9af65921e06e25e4cec5607150faad56a1a\r\n\r\nI don't think a deadlock is possible here. The only place where we grab both locks at the same time is `write_to_disk` and in that case they're read locks.",
        "pr_file_module": null
      },
      {
        "comment_id": "2099842436",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092802901",
        "commented_code": "@@ -63,7 +63,16 @@ pub struct InMemoryLayer {\n \n     opened_at: Instant,\n \n-    /// The above fields never change, except for `end_lsn`, which is only set once.\n+    /// All versions of all pages in the layer are kept here. Indexed\n+    /// by block number and LSN. The [`IndexEntry`] is an offset into the\n+    /// ephemeral file where the page version is stored.\n+    ///\n+    /// We use a separate lock for the index to reduce the critical section\n+    /// during which reads cannot be planned.\n+    index: RwLock<BTreeMap<CompactKey, VecMap<Lsn, IndexEntry>>>,\n+\n+    /// The above fields never change, except for `end_lsn`, which is only set once,\n+    /// and `index` (see rationale there).\n     /// All other changing parts are in `inner`, and protected by a mutex.\n     inner: RwLock<InMemoryLayerInner>,",
        "comment_created_at": "2025-05-21T09:40:25+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "Yeah, I don't think it's possible either, but let's write it down for our future selves. Thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2092806366",
    "pr_number": 11937,
    "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
    "created_at": "2025-05-16T10:40:26+00:00",
    "commented_code": "}\n             }\n         }\n-        drop(inner); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below\n+        drop(index); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2092806366",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092806366",
        "commented_code": "@@ -466,7 +467,7 @@ impl InMemoryLayer {\n                 }\n             }\n         }\n-        drop(inner); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below\n+        drop(index); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below",
        "comment_created_at": "2025-05-16T10:40:26+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "We're relying on `inner` being append-only for this to be safe, yeah? Let's specify that as part of the locking protocol.",
        "pr_file_module": null
      },
      {
        "comment_id": "2096093598",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092806366",
        "commented_code": "@@ -466,7 +467,7 @@ impl InMemoryLayer {\n                 }\n             }\n         }\n-        drop(inner); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below\n+        drop(index); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below",
        "comment_created_at": "2025-05-19T16:20:43+00:00",
        "comment_author": "VladLazar",
        "comment_body": "> We're relying on inner being append-only for this to be safe, yeah?\r\n\r\nCorrect.\r\n\r\n> Let's specify that as part of the locking protocol.\r\n\r\nDon't see how you'd express that via the locking protocol. Can you elaborate?",
        "pr_file_module": null
      },
      {
        "comment_id": "2099858840",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092806366",
        "commented_code": "@@ -466,7 +467,7 @@ impl InMemoryLayer {\n                 }\n             }\n         }\n-        drop(inner); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below\n+        drop(index); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below",
        "comment_created_at": "2025-05-21T09:48:38+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "I'd just add something like this to the comment you added in https://github.com/neondatabase/neon/commit/9377e9af65921e06e25e4cec5607150faad56a1a:\r\n\r\n> Note that `inner` is append-only, so it is not necessary to hold simultaneous locks on `index`. In particular:\r\n>\r\n> * It is safe to read and release `index` before locking and reading from `inner`.\r\n> * It is safe to write and release `inner` before locking and updating `index`.\r\n>\r\n> This avoids holding `index` locks across IO, and is crucial for avoiding read tail latency.",
        "pr_file_module": null
      },
      {
        "comment_id": "2099911689",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092806366",
        "commented_code": "@@ -466,7 +467,7 @@ impl InMemoryLayer {\n                 }\n             }\n         }\n-        drop(inner); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below\n+        drop(index); // release the lock before we spawn the IO; if it's serial-mode IO we will deadlock on the read().await below",
        "comment_created_at": "2025-05-21T10:15:53+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Makes sense. Done in https://github.com/neondatabase/neon/pull/11937/commits/721e8989b0968023d06f8b52e6b559fc6af2cded",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2092832377",
    "pr_number": 11937,
    "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
    "created_at": "2025-05-16T10:59:33+00:00",
    "commented_code": "serialized_batch: SerializedValueBatch,\n         ctx: &RequestContext,\n     ) -> anyhow::Result<()> {\n-        let mut inner = self.inner.write().await;\n-        self.assert_writable();\n-\n-        let base_offset = inner.file.len();\n-\n-        let SerializedValueBatch {\n-            raw,\n-            metadata,\n-            max_lsn: _,\n-            len: _,\n-        } = serialized_batch;\n-\n-        // Write the batch to the file\n-        inner.file.write_raw(&raw, ctx).await?;\n-        let new_size = inner.file.len();\n-\n-        let expected_new_len = base_offset\n-            .checked_add(raw.len().into_u64())\n-            // write_raw would error if we were to overflow u64.\n-            // also IndexEntry and higher levels in\n-            //the code don't allow the file to grow that large\n-            .unwrap();\n-        assert_eq!(new_size, expected_new_len);\n+        let (base_offset, metadata) = {\n+            let mut inner = self.inner.write().await;\n+            self.assert_writable();",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2092832377",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092832377",
        "commented_code": "@@ -592,31 +593,39 @@ impl InMemoryLayer {\n         serialized_batch: SerializedValueBatch,\n         ctx: &RequestContext,\n     ) -> anyhow::Result<()> {\n-        let mut inner = self.inner.write().await;\n-        self.assert_writable();\n-\n-        let base_offset = inner.file.len();\n-\n-        let SerializedValueBatch {\n-            raw,\n-            metadata,\n-            max_lsn: _,\n-            len: _,\n-        } = serialized_batch;\n-\n-        // Write the batch to the file\n-        inner.file.write_raw(&raw, ctx).await?;\n-        let new_size = inner.file.len();\n-\n-        let expected_new_len = base_offset\n-            .checked_add(raw.len().into_u64())\n-            // write_raw would error if we were to overflow u64.\n-            // also IndexEntry and higher levels in\n-            //the code don't allow the file to grow that large\n-            .unwrap();\n-        assert_eq!(new_size, expected_new_len);\n+        let (base_offset, metadata) = {\n+            let mut inner = self.inner.write().await;\n+            self.assert_writable();",
        "comment_created_at": "2025-05-16T10:59:33+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "Is this properly synchronized with `freeze()`? It's possible that the callers avoid races here by synchronizing at a higher level, but this sequence is not prevented by the API (for threads A and B):\r\n\r\n1. A: `put_batch` takes out lock on `inner` and writes.\r\n2. B: `freeze` sets `end_lsn`.\r\n3. B: `write_to_disk` blocks on `inner`.\r\n4. A: `put_batch` releases `inner`.\r\n5. B: `write_to_disk` acquires lock on `inner` and `index`.\r\n6. A: `put_batch` blocks on `index`.\r\n7. B: `write_to_disk` writes an incomplete layer to disk from a stale index.\r\n\r\nWe can avoid this by taking out a lock on `inner` and `index` in `freeze`, and mandating that `end_lsn` must either be accessed under lock or specifying the access ordering of `end_lsn` wrt. to the locks.",
        "pr_file_module": null
      },
      {
        "comment_id": "2096090850",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092832377",
        "commented_code": "@@ -592,31 +593,39 @@ impl InMemoryLayer {\n         serialized_batch: SerializedValueBatch,\n         ctx: &RequestContext,\n     ) -> anyhow::Result<()> {\n-        let mut inner = self.inner.write().await;\n-        self.assert_writable();\n-\n-        let base_offset = inner.file.len();\n-\n-        let SerializedValueBatch {\n-            raw,\n-            metadata,\n-            max_lsn: _,\n-            len: _,\n-        } = serialized_batch;\n-\n-        // Write the batch to the file\n-        inner.file.write_raw(&raw, ctx).await?;\n-        let new_size = inner.file.len();\n-\n-        let expected_new_len = base_offset\n-            .checked_add(raw.len().into_u64())\n-            // write_raw would error if we were to overflow u64.\n-            // also IndexEntry and higher levels in\n-            //the code don't allow the file to grow that large\n-            .unwrap();\n-        assert_eq!(new_size, expected_new_len);\n+        let (base_offset, metadata) = {\n+            let mut inner = self.inner.write().await;\n+            self.assert_writable();",
        "comment_created_at": "2025-05-19T16:18:55+00:00",
        "comment_author": "VladLazar",
        "comment_body": "The API sucks.\r\n\r\n`freeze` and `put_batch` are synchronized via `Timeline::write_lock`.\r\nWe only `freeze` when there's no ongoing writes. There's two paths on which we freeze:\r\n1. Via the active `TimelineWriter`. This holds the `Timeline::write_lock` for its lifetime. The rolling is handled in `TimelineWriter::put_batch`. It's a `&mut self` function so can't be called from different threads.\r\n2. In the background via `Timeline::maybe_freeze_ephemeral_layer`. This only proceeds if `try_lock` on `Timeline::write_lock` succeeds (i.e. there's no active writer), hence there can be no concurrent writes.",
        "pr_file_module": null
      },
      {
        "comment_id": "2099862921",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11937,
        "pr_file": "pageserver/src/tenant/storage_layer/inmemory_layer.rs",
        "discussion_id": "2092832377",
        "commented_code": "@@ -592,31 +593,39 @@ impl InMemoryLayer {\n         serialized_batch: SerializedValueBatch,\n         ctx: &RequestContext,\n     ) -> anyhow::Result<()> {\n-        let mut inner = self.inner.write().await;\n-        self.assert_writable();\n-\n-        let base_offset = inner.file.len();\n-\n-        let SerializedValueBatch {\n-            raw,\n-            metadata,\n-            max_lsn: _,\n-            len: _,\n-        } = serialized_batch;\n-\n-        // Write the batch to the file\n-        inner.file.write_raw(&raw, ctx).await?;\n-        let new_size = inner.file.len();\n-\n-        let expected_new_len = base_offset\n-            .checked_add(raw.len().into_u64())\n-            // write_raw would error if we were to overflow u64.\n-            // also IndexEntry and higher levels in\n-            //the code don't allow the file to grow that large\n-            .unwrap();\n-        assert_eq!(new_size, expected_new_len);\n+        let (base_offset, metadata) = {\n+            let mut inner = self.inner.write().await;\n+            self.assert_writable();",
        "comment_created_at": "2025-05-21T09:50:27+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "Thanks for the clarification and comment.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2090600687",
    "pr_number": 11929,
    "pr_file": "libs/neon-shmem/src/lib.rs",
    "created_at": "2025-05-15T08:29:51+00:00",
    "commented_code": "+//! Shared memory utilities for neon communicator\n+\n+use std::num::NonZeroUsize;\n+use std::os::fd::OwnedFd;\n+use std::ptr::NonNull;\n+use std::sync::atomic::{AtomicUsize, Ordering};\n+\n+use nix::errno::Errno;\n+use nix::fcntl::posix_fallocate as nix_posix_fallocate;\n+use nix::sys::memfd::MFdFlags;\n+use nix::sys::memfd::memfd_create as nix_memfd_create;\n+use nix::sys::mman::MapFlags;\n+use nix::sys::mman::ProtFlags;\n+use nix::sys::mman::mmap as nix_mmap;\n+use nix::sys::mman::munmap as nix_munmap;\n+use nix::unistd::ftruncate as nix_ftruncate;\n+\n+/// ShmemHandle represents a shared memory area that can be shared by processes over fork().\n+/// Unlike shared memory allocated by Postgres, this area is resizable, up to 'max_size' that's\n+/// specified at creation.\n+///\n+/// The area is backed by an anonymous file created with memfd_create(). The full address space for\n+/// 'max_size' is reserved up-front with mmap(), but whenever you call [`ShmemHandle::set_size`],\n+/// the underlying file is resized. Do not access the area beyond the current size. Currently, that\n+/// will cause the file to be expanded, but we might use mprotect() etc. to enforce that in the\n+/// future.\n+pub struct ShmemHandle {\n+    /// memfd file descriptor\n+    fd: OwnedFd,\n+\n+    max_size: usize,\n+\n+    // Pointer to the beginning of the shared memory area. The header is stored there.\n+    shared_ptr: NonNull<SharedStruct>,\n+\n+    // Pointer to the beginning of the user data\n+    pub data_ptr: NonNull<u8>,\n+}\n+\n+/// This is stored at the beginning in the shared memory area.\n+struct SharedStruct {\n+    max_size: usize,\n+\n+    /// Current size of the backing file. The high-order bit is used for the RESIZE_IN_PROGRESS flag\n+    current_size: AtomicUsize,\n+}\n+\n+const RESIZE_IN_PROGRESS: usize = 1 << 63;\n+\n+const HEADER_SIZE: usize = std::mem::size_of::<SharedStruct>();\n+\n+/// Error type returned by the ShmemHandle functions.\n+#[derive(thiserror::Error, Debug)]\n+#[error(\"{msg}: {errno}\")]\n+pub struct Error {\n+    pub msg: String,\n+    pub errno: Errno,\n+}\n+\n+impl Error {\n+    fn new(msg: &str, errno: Errno) -> Error {\n+        Error {\n+            msg: msg.to_string(),\n+            errno,\n+        }\n+    }\n+}\n+\n+impl ShmemHandle {\n+    /// Create a new shared memory area. To communicate between processes, the processes need to be\n+    /// fork()'d after calling this, so that the ShmemHandle is inherited by all processes.\n+    ///\n+    /// If the ShmemHandle is dropped, the memory is unmapped from the current process. Other\n+    /// processes can continue using it, however.\n+    pub fn new(name: &str, initial_size: usize, max_size: usize) -> Result<ShmemHandle, Error> {\n+        let fd = nix_memfd_create(name, MFdFlags::empty())\n+            .map_err(|e| Error::new(\"memfd_create failed: {e}\", e))?;\n+        Self::new_with_fd(fd, initial_size, max_size)\n+    }\n+\n+    fn new_with_fd(\n+        fd: OwnedFd,\n+        initial_size: usize,\n+        max_size: usize,\n+    ) -> Result<ShmemHandle, Error> {\n+        // We reserve the high-order bit for the RESIZE_IN_PROGRESS flag, and the actual size\n+        // is a little larger than this because of the SharedStruct header. Make the upper limit\n+        // somewhat smaller than that, because with anything close to that, you'll run out of\n+        // memory anyway.\n+        if max_size >= 1 << 48 {\n+            panic!(\"max size {} too large\", max_size);\n+        }\n+        if initial_size > max_size {\n+            panic!(\"initial size {initial_size} larger than max size {max_size}\");\n+        }\n+\n+        // The actual initial / max size is the one given by the caller, plus the size of\n+        // 'SharedStruct'.\n+        let initial_size = HEADER_SIZE + initial_size;\n+        let max_size = NonZeroUsize::new(HEADER_SIZE + max_size).unwrap();\n+\n+        // Reserve address space for it with mmap\n+        //\n+        // TODO: Use MAP_HUGELB if possible\n+        let start_ptr = unsafe {\n+            nix_mmap(\n+                None,\n+                max_size,\n+                ProtFlags::PROT_READ | ProtFlags::PROT_WRITE,\n+                MapFlags::MAP_SHARED,\n+                &fd,\n+                0,\n+            )\n+        }\n+        .map_err(|e| Error::new(\"mmap failed: {e}\", e))?;\n+\n+        // Reserve space for the initial size\n+        nix_posix_fallocate(&fd, 0, initial_size as i64).map_err(|e| {\n+            Error::new(\n+                \"could not reserve space for the shmem segment, posix_fallcate failed: {e}\",\n+                e,\n+            )\n+        })?;\n+\n+        // Initialize the header\n+        let shared: NonNull<SharedStruct> = start_ptr.cast();\n+        unsafe {\n+            shared.write(SharedStruct {\n+                max_size: max_size.into(),\n+                current_size: AtomicUsize::new(initial_size),\n+            })\n+        };\n+\n+        // The user data begins after the header\n+        let data_ptr = unsafe { start_ptr.cast().add(HEADER_SIZE) };\n+\n+        Ok(ShmemHandle {\n+            fd,\n+            max_size: max_size.into(),\n+            shared_ptr: shared,\n+            data_ptr,\n+        })\n+    }\n+\n+    // return reference to the header\n+    fn shared(&self) -> &SharedStruct {\n+        unsafe { self.shared_ptr.as_ref() }\n+    }\n+\n+    /// Resize the shared memory area. 'new_size' must not be larger than the 'max_size' specified\n+    /// when creating the area.\n+    ///\n+    /// This may only be called from one process/thread concurrently. We detect that case\n+    /// and return an Error.\n+    pub fn set_size(&self, new_size: usize) -> Result<(), Error> {\n+        let new_size = new_size + HEADER_SIZE;\n+        let shared = self.shared();\n+\n+        if new_size > self.max_size {\n+            panic!(\"size is greater than max_size\");\n+        }\n+        assert_eq!(self.max_size, shared.max_size);\n+\n+        // \"Lock\" the area by setting the bit in 'current_size'\n+        let mut old_size = shared.current_size.load(Ordering::Relaxed);",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2090600687",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11929,
        "pr_file": "libs/neon-shmem/src/lib.rs",
        "discussion_id": "2090600687",
        "commented_code": "@@ -0,0 +1,376 @@\n+//! Shared memory utilities for neon communicator\n+\n+use std::num::NonZeroUsize;\n+use std::os::fd::OwnedFd;\n+use std::ptr::NonNull;\n+use std::sync::atomic::{AtomicUsize, Ordering};\n+\n+use nix::errno::Errno;\n+use nix::fcntl::posix_fallocate as nix_posix_fallocate;\n+use nix::sys::memfd::MFdFlags;\n+use nix::sys::memfd::memfd_create as nix_memfd_create;\n+use nix::sys::mman::MapFlags;\n+use nix::sys::mman::ProtFlags;\n+use nix::sys::mman::mmap as nix_mmap;\n+use nix::sys::mman::munmap as nix_munmap;\n+use nix::unistd::ftruncate as nix_ftruncate;\n+\n+/// ShmemHandle represents a shared memory area that can be shared by processes over fork().\n+/// Unlike shared memory allocated by Postgres, this area is resizable, up to 'max_size' that's\n+/// specified at creation.\n+///\n+/// The area is backed by an anonymous file created with memfd_create(). The full address space for\n+/// 'max_size' is reserved up-front with mmap(), but whenever you call [`ShmemHandle::set_size`],\n+/// the underlying file is resized. Do not access the area beyond the current size. Currently, that\n+/// will cause the file to be expanded, but we might use mprotect() etc. to enforce that in the\n+/// future.\n+pub struct ShmemHandle {\n+    /// memfd file descriptor\n+    fd: OwnedFd,\n+\n+    max_size: usize,\n+\n+    // Pointer to the beginning of the shared memory area. The header is stored there.\n+    shared_ptr: NonNull<SharedStruct>,\n+\n+    // Pointer to the beginning of the user data\n+    pub data_ptr: NonNull<u8>,\n+}\n+\n+/// This is stored at the beginning in the shared memory area.\n+struct SharedStruct {\n+    max_size: usize,\n+\n+    /// Current size of the backing file. The high-order bit is used for the RESIZE_IN_PROGRESS flag\n+    current_size: AtomicUsize,\n+}\n+\n+const RESIZE_IN_PROGRESS: usize = 1 << 63;\n+\n+const HEADER_SIZE: usize = std::mem::size_of::<SharedStruct>();\n+\n+/// Error type returned by the ShmemHandle functions.\n+#[derive(thiserror::Error, Debug)]\n+#[error(\"{msg}: {errno}\")]\n+pub struct Error {\n+    pub msg: String,\n+    pub errno: Errno,\n+}\n+\n+impl Error {\n+    fn new(msg: &str, errno: Errno) -> Error {\n+        Error {\n+            msg: msg.to_string(),\n+            errno,\n+        }\n+    }\n+}\n+\n+impl ShmemHandle {\n+    /// Create a new shared memory area. To communicate between processes, the processes need to be\n+    /// fork()'d after calling this, so that the ShmemHandle is inherited by all processes.\n+    ///\n+    /// If the ShmemHandle is dropped, the memory is unmapped from the current process. Other\n+    /// processes can continue using it, however.\n+    pub fn new(name: &str, initial_size: usize, max_size: usize) -> Result<ShmemHandle, Error> {\n+        let fd = nix_memfd_create(name, MFdFlags::empty())\n+            .map_err(|e| Error::new(\"memfd_create failed: {e}\", e))?;\n+        Self::new_with_fd(fd, initial_size, max_size)\n+    }\n+\n+    fn new_with_fd(\n+        fd: OwnedFd,\n+        initial_size: usize,\n+        max_size: usize,\n+    ) -> Result<ShmemHandle, Error> {\n+        // We reserve the high-order bit for the RESIZE_IN_PROGRESS flag, and the actual size\n+        // is a little larger than this because of the SharedStruct header. Make the upper limit\n+        // somewhat smaller than that, because with anything close to that, you'll run out of\n+        // memory anyway.\n+        if max_size >= 1 << 48 {\n+            panic!(\"max size {} too large\", max_size);\n+        }\n+        if initial_size > max_size {\n+            panic!(\"initial size {initial_size} larger than max size {max_size}\");\n+        }\n+\n+        // The actual initial / max size is the one given by the caller, plus the size of\n+        // 'SharedStruct'.\n+        let initial_size = HEADER_SIZE + initial_size;\n+        let max_size = NonZeroUsize::new(HEADER_SIZE + max_size).unwrap();\n+\n+        // Reserve address space for it with mmap\n+        //\n+        // TODO: Use MAP_HUGELB if possible\n+        let start_ptr = unsafe {\n+            nix_mmap(\n+                None,\n+                max_size,\n+                ProtFlags::PROT_READ | ProtFlags::PROT_WRITE,\n+                MapFlags::MAP_SHARED,\n+                &fd,\n+                0,\n+            )\n+        }\n+        .map_err(|e| Error::new(\"mmap failed: {e}\", e))?;\n+\n+        // Reserve space for the initial size\n+        nix_posix_fallocate(&fd, 0, initial_size as i64).map_err(|e| {\n+            Error::new(\n+                \"could not reserve space for the shmem segment, posix_fallcate failed: {e}\",\n+                e,\n+            )\n+        })?;\n+\n+        // Initialize the header\n+        let shared: NonNull<SharedStruct> = start_ptr.cast();\n+        unsafe {\n+            shared.write(SharedStruct {\n+                max_size: max_size.into(),\n+                current_size: AtomicUsize::new(initial_size),\n+            })\n+        };\n+\n+        // The user data begins after the header\n+        let data_ptr = unsafe { start_ptr.cast().add(HEADER_SIZE) };\n+\n+        Ok(ShmemHandle {\n+            fd,\n+            max_size: max_size.into(),\n+            shared_ptr: shared,\n+            data_ptr,\n+        })\n+    }\n+\n+    // return reference to the header\n+    fn shared(&self) -> &SharedStruct {\n+        unsafe { self.shared_ptr.as_ref() }\n+    }\n+\n+    /// Resize the shared memory area. 'new_size' must not be larger than the 'max_size' specified\n+    /// when creating the area.\n+    ///\n+    /// This may only be called from one process/thread concurrently. We detect that case\n+    /// and return an Error.\n+    pub fn set_size(&self, new_size: usize) -> Result<(), Error> {\n+        let new_size = new_size + HEADER_SIZE;\n+        let shared = self.shared();\n+\n+        if new_size > self.max_size {\n+            panic!(\"size is greater than max_size\");\n+        }\n+        assert_eq!(self.max_size, shared.max_size);\n+\n+        // \"Lock\" the area by setting the bit in 'current_size'\n+        let mut old_size = shared.current_size.load(Ordering::Relaxed);",
        "comment_created_at": "2025-05-15T08:29:51+00:00",
        "comment_author": "erikgrinaker",
        "comment_body": "I think `Ordering::Relaxed` is too lax here, given this is a lock. I think we'll need `Acquire` on the `load` and `AcqRel` on the CaS or something similar. Otherwise, it's possible that subsequent operations (e.g. `ftruncate`) are reordered wrt. the lock acquisition.\n\nNot critical, since we only ever expect one caller to modify this, but if we're going to have a lock we may as well properly lock it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2090771199",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11929,
        "pr_file": "libs/neon-shmem/src/lib.rs",
        "discussion_id": "2090600687",
        "commented_code": "@@ -0,0 +1,376 @@\n+//! Shared memory utilities for neon communicator\n+\n+use std::num::NonZeroUsize;\n+use std::os::fd::OwnedFd;\n+use std::ptr::NonNull;\n+use std::sync::atomic::{AtomicUsize, Ordering};\n+\n+use nix::errno::Errno;\n+use nix::fcntl::posix_fallocate as nix_posix_fallocate;\n+use nix::sys::memfd::MFdFlags;\n+use nix::sys::memfd::memfd_create as nix_memfd_create;\n+use nix::sys::mman::MapFlags;\n+use nix::sys::mman::ProtFlags;\n+use nix::sys::mman::mmap as nix_mmap;\n+use nix::sys::mman::munmap as nix_munmap;\n+use nix::unistd::ftruncate as nix_ftruncate;\n+\n+/// ShmemHandle represents a shared memory area that can be shared by processes over fork().\n+/// Unlike shared memory allocated by Postgres, this area is resizable, up to 'max_size' that's\n+/// specified at creation.\n+///\n+/// The area is backed by an anonymous file created with memfd_create(). The full address space for\n+/// 'max_size' is reserved up-front with mmap(), but whenever you call [`ShmemHandle::set_size`],\n+/// the underlying file is resized. Do not access the area beyond the current size. Currently, that\n+/// will cause the file to be expanded, but we might use mprotect() etc. to enforce that in the\n+/// future.\n+pub struct ShmemHandle {\n+    /// memfd file descriptor\n+    fd: OwnedFd,\n+\n+    max_size: usize,\n+\n+    // Pointer to the beginning of the shared memory area. The header is stored there.\n+    shared_ptr: NonNull<SharedStruct>,\n+\n+    // Pointer to the beginning of the user data\n+    pub data_ptr: NonNull<u8>,\n+}\n+\n+/// This is stored at the beginning in the shared memory area.\n+struct SharedStruct {\n+    max_size: usize,\n+\n+    /// Current size of the backing file. The high-order bit is used for the RESIZE_IN_PROGRESS flag\n+    current_size: AtomicUsize,\n+}\n+\n+const RESIZE_IN_PROGRESS: usize = 1 << 63;\n+\n+const HEADER_SIZE: usize = std::mem::size_of::<SharedStruct>();\n+\n+/// Error type returned by the ShmemHandle functions.\n+#[derive(thiserror::Error, Debug)]\n+#[error(\"{msg}: {errno}\")]\n+pub struct Error {\n+    pub msg: String,\n+    pub errno: Errno,\n+}\n+\n+impl Error {\n+    fn new(msg: &str, errno: Errno) -> Error {\n+        Error {\n+            msg: msg.to_string(),\n+            errno,\n+        }\n+    }\n+}\n+\n+impl ShmemHandle {\n+    /// Create a new shared memory area. To communicate between processes, the processes need to be\n+    /// fork()'d after calling this, so that the ShmemHandle is inherited by all processes.\n+    ///\n+    /// If the ShmemHandle is dropped, the memory is unmapped from the current process. Other\n+    /// processes can continue using it, however.\n+    pub fn new(name: &str, initial_size: usize, max_size: usize) -> Result<ShmemHandle, Error> {\n+        let fd = nix_memfd_create(name, MFdFlags::empty())\n+            .map_err(|e| Error::new(\"memfd_create failed: {e}\", e))?;\n+        Self::new_with_fd(fd, initial_size, max_size)\n+    }\n+\n+    fn new_with_fd(\n+        fd: OwnedFd,\n+        initial_size: usize,\n+        max_size: usize,\n+    ) -> Result<ShmemHandle, Error> {\n+        // We reserve the high-order bit for the RESIZE_IN_PROGRESS flag, and the actual size\n+        // is a little larger than this because of the SharedStruct header. Make the upper limit\n+        // somewhat smaller than that, because with anything close to that, you'll run out of\n+        // memory anyway.\n+        if max_size >= 1 << 48 {\n+            panic!(\"max size {} too large\", max_size);\n+        }\n+        if initial_size > max_size {\n+            panic!(\"initial size {initial_size} larger than max size {max_size}\");\n+        }\n+\n+        // The actual initial / max size is the one given by the caller, plus the size of\n+        // 'SharedStruct'.\n+        let initial_size = HEADER_SIZE + initial_size;\n+        let max_size = NonZeroUsize::new(HEADER_SIZE + max_size).unwrap();\n+\n+        // Reserve address space for it with mmap\n+        //\n+        // TODO: Use MAP_HUGELB if possible\n+        let start_ptr = unsafe {\n+            nix_mmap(\n+                None,\n+                max_size,\n+                ProtFlags::PROT_READ | ProtFlags::PROT_WRITE,\n+                MapFlags::MAP_SHARED,\n+                &fd,\n+                0,\n+            )\n+        }\n+        .map_err(|e| Error::new(\"mmap failed: {e}\", e))?;\n+\n+        // Reserve space for the initial size\n+        nix_posix_fallocate(&fd, 0, initial_size as i64).map_err(|e| {\n+            Error::new(\n+                \"could not reserve space for the shmem segment, posix_fallcate failed: {e}\",\n+                e,\n+            )\n+        })?;\n+\n+        // Initialize the header\n+        let shared: NonNull<SharedStruct> = start_ptr.cast();\n+        unsafe {\n+            shared.write(SharedStruct {\n+                max_size: max_size.into(),\n+                current_size: AtomicUsize::new(initial_size),\n+            })\n+        };\n+\n+        // The user data begins after the header\n+        let data_ptr = unsafe { start_ptr.cast().add(HEADER_SIZE) };\n+\n+        Ok(ShmemHandle {\n+            fd,\n+            max_size: max_size.into(),\n+            shared_ptr: shared,\n+            data_ptr,\n+        })\n+    }\n+\n+    // return reference to the header\n+    fn shared(&self) -> &SharedStruct {\n+        unsafe { self.shared_ptr.as_ref() }\n+    }\n+\n+    /// Resize the shared memory area. 'new_size' must not be larger than the 'max_size' specified\n+    /// when creating the area.\n+    ///\n+    /// This may only be called from one process/thread concurrently. We detect that case\n+    /// and return an Error.\n+    pub fn set_size(&self, new_size: usize) -> Result<(), Error> {\n+        let new_size = new_size + HEADER_SIZE;\n+        let shared = self.shared();\n+\n+        if new_size > self.max_size {\n+            panic!(\"size is greater than max_size\");\n+        }\n+        assert_eq!(self.max_size, shared.max_size);\n+\n+        // \"Lock\" the area by setting the bit in 'current_size'\n+        let mut old_size = shared.current_size.load(Ordering::Relaxed);",
        "comment_created_at": "2025-05-15T09:50:15+00:00",
        "comment_author": "hlinnaka",
        "comment_body": "System calls are surely a synchronization point, and this doesn't read or write any other memory.\r\n\r\nThat said, this is not performance-critical so there's no harm in making it more strict. I'll do that for the sake of safety if more code gets added here in the future.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2086434361",
    "pr_number": 11855,
    "pr_file": "pageserver/src/http/routes.rs",
    "created_at": "2025-05-13T10:05:44+00:00",
    "commented_code": "}.instrument(span).await\n }\n \n+/// Activate a timeline after its import has completed\n+///\n+/// The endpoint is idempotent and callers are expected to retry all\n+/// errors until a successful response.\n+async fn activate_post_import_handler(\n+    request: Request<Body>,\n+    _cancel: CancellationToken,\n+) -> Result<Response<Body>, ApiError> {\n+    let tenant_shard_id: TenantShardId = parse_request_param(&request, \"tenant_shard_id\")?;\n+    check_permission(&request, Some(tenant_shard_id.tenant_id))?;\n+\n+    let timeline_id: TimelineId = parse_request_param(&request, \"timeline_id\")?;\n+\n+    let span = info_span!(\n+        \"activate_post_import_handler\",\n+        tenant_id=%tenant_shard_id.tenant_id,\n+        timeline_id=%timeline_id,\n+        shard_id=%tenant_shard_id.shard_slug()\n+    );\n+\n+    async move {\n+        let state = get_state(&request);\n+        let tenant = state\n+            .tenant_manager\n+            .get_attached_tenant_shard(tenant_shard_id)?;\n+\n+        tenant.wait_to_become_active(ACTIVE_TENANT_TIMEOUT).await?;\n+\n+        tenant\n+            .finalize_importing_timeline(timeline_id)\n+            .await\n+            .map_err(ApiError::InternalServerError)?;\n+\n+        match tenant.get_timeline(timeline_id, false) {\n+            Ok(_timeline) => {\n+                // Timeline is already visible. Reset not required: fall through.\n+            }\n+            Err(GetTimelineError::NotFound { .. }) => {\n+                // This is crude: we reset the whole tenant such that the new timeline is detected\n+                // and activated. We can come up with something more granular in the future.\n+                //\n+                // Note that we only reset the tenant if required: when the timeline is\n+                // not present in [`Tenant::timelines`].\n+                let ctx = RequestContext::new(TaskKind::MgmtRequest, DownloadBehavior::Warn);\n+                state\n+                    .tenant_manager\n+                    .reset_tenant(tenant_shard_id, false, &ctx)\n+                    .await\n+                    .map_err(ApiError::InternalServerError)?;\n+            }\n+            Err(GetTimelineError::ShuttingDown) => {\n+                return Err(ApiError::ShuttingDown);\n+            }\n+            Err(GetTimelineError::NotActive { .. }) => {\n+                unreachable!(\"Called get_timeline with active_only=false\");\n+            }\n+        }\n+\n+        // At this point the timeline should become eventually active.\n+        // If the timeline is not active yet, then the caller will retry.\n+        // TODO(vlad): if the tenant is broken, return a permananet error\n+        let timeline = tenant.get_timeline(timeline_id, true)?;",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2086434361",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11855,
        "pr_file": "pageserver/src/http/routes.rs",
        "discussion_id": "2086434361",
        "commented_code": "@@ -3500,6 +3500,87 @@ async fn put_tenant_timeline_import_wal(\n     }.instrument(span).await\n }\n \n+/// Activate a timeline after its import has completed\n+///\n+/// The endpoint is idempotent and callers are expected to retry all\n+/// errors until a successful response.\n+async fn activate_post_import_handler(\n+    request: Request<Body>,\n+    _cancel: CancellationToken,\n+) -> Result<Response<Body>, ApiError> {\n+    let tenant_shard_id: TenantShardId = parse_request_param(&request, \"tenant_shard_id\")?;\n+    check_permission(&request, Some(tenant_shard_id.tenant_id))?;\n+\n+    let timeline_id: TimelineId = parse_request_param(&request, \"timeline_id\")?;\n+\n+    let span = info_span!(\n+        \"activate_post_import_handler\",\n+        tenant_id=%tenant_shard_id.tenant_id,\n+        timeline_id=%timeline_id,\n+        shard_id=%tenant_shard_id.shard_slug()\n+    );\n+\n+    async move {\n+        let state = get_state(&request);\n+        let tenant = state\n+            .tenant_manager\n+            .get_attached_tenant_shard(tenant_shard_id)?;\n+\n+        tenant.wait_to_become_active(ACTIVE_TENANT_TIMEOUT).await?;\n+\n+        tenant\n+            .finalize_importing_timeline(timeline_id)\n+            .await\n+            .map_err(ApiError::InternalServerError)?;\n+\n+        match tenant.get_timeline(timeline_id, false) {\n+            Ok(_timeline) => {\n+                // Timeline is already visible. Reset not required: fall through.\n+            }\n+            Err(GetTimelineError::NotFound { .. }) => {\n+                // This is crude: we reset the whole tenant such that the new timeline is detected\n+                // and activated. We can come up with something more granular in the future.\n+                //\n+                // Note that we only reset the tenant if required: when the timeline is\n+                // not present in [`Tenant::timelines`].\n+                let ctx = RequestContext::new(TaskKind::MgmtRequest, DownloadBehavior::Warn);\n+                state\n+                    .tenant_manager\n+                    .reset_tenant(tenant_shard_id, false, &ctx)\n+                    .await\n+                    .map_err(ApiError::InternalServerError)?;\n+            }\n+            Err(GetTimelineError::ShuttingDown) => {\n+                return Err(ApiError::ShuttingDown);\n+            }\n+            Err(GetTimelineError::NotActive { .. }) => {\n+                unreachable!(\"Called get_timeline with active_only=false\");\n+            }\n+        }\n+\n+        // At this point the timeline should become eventually active.\n+        // If the timeline is not active yet, then the caller will retry.\n+        // TODO(vlad): if the tenant is broken, return a permananet error\n+        let timeline = tenant.get_timeline(timeline_id, true)?;",
        "comment_created_at": "2025-05-13T10:05:44+00:00",
        "comment_author": "problame",
        "comment_body": "It's not guaranteed to be active at this point, just that it will.\nLikely that will happen within milliseconds, but it's a race that will lead to occasional (flaky) bails from here with the `?`.\n\nSure, caller is obliged to retry, but it adds noise, flakiness, and needless latency because of the retry.\n\nElesewhere in the codebase there are abstractions to wait for a timeline to become active. Let's use it.\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2086708334",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11855,
        "pr_file": "pageserver/src/http/routes.rs",
        "discussion_id": "2086434361",
        "commented_code": "@@ -3500,6 +3500,87 @@ async fn put_tenant_timeline_import_wal(\n     }.instrument(span).await\n }\n \n+/// Activate a timeline after its import has completed\n+///\n+/// The endpoint is idempotent and callers are expected to retry all\n+/// errors until a successful response.\n+async fn activate_post_import_handler(\n+    request: Request<Body>,\n+    _cancel: CancellationToken,\n+) -> Result<Response<Body>, ApiError> {\n+    let tenant_shard_id: TenantShardId = parse_request_param(&request, \"tenant_shard_id\")?;\n+    check_permission(&request, Some(tenant_shard_id.tenant_id))?;\n+\n+    let timeline_id: TimelineId = parse_request_param(&request, \"timeline_id\")?;\n+\n+    let span = info_span!(\n+        \"activate_post_import_handler\",\n+        tenant_id=%tenant_shard_id.tenant_id,\n+        timeline_id=%timeline_id,\n+        shard_id=%tenant_shard_id.shard_slug()\n+    );\n+\n+    async move {\n+        let state = get_state(&request);\n+        let tenant = state\n+            .tenant_manager\n+            .get_attached_tenant_shard(tenant_shard_id)?;\n+\n+        tenant.wait_to_become_active(ACTIVE_TENANT_TIMEOUT).await?;\n+\n+        tenant\n+            .finalize_importing_timeline(timeline_id)\n+            .await\n+            .map_err(ApiError::InternalServerError)?;\n+\n+        match tenant.get_timeline(timeline_id, false) {\n+            Ok(_timeline) => {\n+                // Timeline is already visible. Reset not required: fall through.\n+            }\n+            Err(GetTimelineError::NotFound { .. }) => {\n+                // This is crude: we reset the whole tenant such that the new timeline is detected\n+                // and activated. We can come up with something more granular in the future.\n+                //\n+                // Note that we only reset the tenant if required: when the timeline is\n+                // not present in [`Tenant::timelines`].\n+                let ctx = RequestContext::new(TaskKind::MgmtRequest, DownloadBehavior::Warn);\n+                state\n+                    .tenant_manager\n+                    .reset_tenant(tenant_shard_id, false, &ctx)\n+                    .await\n+                    .map_err(ApiError::InternalServerError)?;\n+            }\n+            Err(GetTimelineError::ShuttingDown) => {\n+                return Err(ApiError::ShuttingDown);\n+            }\n+            Err(GetTimelineError::NotActive { .. }) => {\n+                unreachable!(\"Called get_timeline with active_only=false\");\n+            }\n+        }\n+\n+        // At this point the timeline should become eventually active.\n+        // If the timeline is not active yet, then the caller will retry.\n+        // TODO(vlad): if the tenant is broken, return a permananet error\n+        let timeline = tenant.get_timeline(timeline_id, true)?;",
        "comment_created_at": "2025-05-13T12:31:14+00:00",
        "comment_author": "VladLazar",
        "comment_body": "https://github.com/neondatabase/neon/pull/11855/commits/af641fe64ac386a712952f531076d476eae6555e",
        "pr_file_module": null
      }
    ]
  }
]