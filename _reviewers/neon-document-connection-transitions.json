[
  {
    "discussion_id": "2003190924",
    "pr_number": 11294,
    "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
    "created_at": "2025-03-19T12:15:47+00:00",
    "commented_code": "+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2003190924",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003190924",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid",
        "comment_created_at": "2025-03-19T12:15:47+00:00",
        "comment_author": "MMeent",
        "comment_body": "This is missing the important element where the Primary that has to be shut down first, but the (old) primary does not show up in this diagram.",
        "pr_file_module": null
      },
      {
        "comment_id": "2003876358",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003190924",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid",
        "comment_created_at": "2025-03-19T17:21:33+00:00",
        "comment_author": "ololobus",
        "comment_body": "Primary in the diagram is the old primary, and it's shut down first, it's also mentioned in text. Or what do you mean?",
        "pr_file_module": null
      },
      {
        "comment_id": "2003919725",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003190924",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid",
        "comment_created_at": "2025-03-19T17:47:03+00:00",
        "comment_author": "MMeent",
        "comment_body": "Oh, it didn't have the same actor labeling as those of the secondary, so I'd failed to notice this.\r\n\r\nEither way, that needs additional details, as there are some compute_ctl-postgres interactions which we need to detail on the primary as well for this to work correctly and consistently.",
        "pr_file_module": null
      },
      {
        "comment_id": "2004109942",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003190924",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid",
        "comment_created_at": "2025-03-19T19:14:19+00:00",
        "comment_author": "ololobus",
        "comment_body": "My intent was to keep it reasonably high-level. Do you see some important interactions missing here?",
        "pr_file_module": null
      },
      {
        "comment_id": "2010490848",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003190924",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details.\n+\n+    3.3. Attach warm replica compute to the primary endpoint inside control plane metadata\n+        database.\n+\n+    3.4. Promote replica to primary.\n+\n+    3.5. When everything is done, finalize the endpoint state to be just `active`.\n+\n+### Complete rolling restart flow\n+\n+```mermaid",
        "comment_created_at": "2025-03-24T16:08:26+00:00",
        "comment_author": "MMeent",
        "comment_body": "Not \"missing\" per se, but I do find it confusing that the RFC does go into detail for one side (the promoting replica) but not the other (the primary), while both sides are critial for correct functioning of the system. I'd expected both sides to have the same amount of detail.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2005369080",
    "pr_number": 11294,
    "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
    "created_at": "2025-03-20T11:14:51+00:00",
    "commented_code": "+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2005369080",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2005369080",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.",
        "comment_created_at": "2025-03-20T11:14:51+00:00",
        "comment_author": "conradludgate",
        "comment_body": "Question: Is it possible to make the old primary close the connections instead? \r\n\r\nIt's easy enough to prevent proxy from starting a new connection (assuming at-least-once-delivery) but closing existing connections is not so easy at the moment, and with the upcoming pglb work this likely would not be practical. It's not a major blocker, but I want to explore the options.",
        "pr_file_module": null
      },
      {
        "comment_id": "2006508817",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2005369080",
        "commented_code": "@@ -0,0 +1,379 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute loses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, Object storage proxy for unlogged storage of compute files.\n+For the latter, we will need to implement a uniform abstraction layer on top of S3, ABS, etc., but\n+S3 is used in text interchangeably with 'object storage' for simplicity.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart. For enabling periodic dumping, we should consider the following value\n+`lfc_dump_interval_sec=300` (5 minutes), same as in the upstream's `pg_prewarm.autoprewarm_interval`.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    /// in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Total number of pages to prewarm\n+        pub pages_total: i64\n+        /// Number of pages prewarmed so far\n+        pub pages_processed: i64\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    This API should be very similar to the existing `POST /configure` API, i.e. accept the\n+    spec (primary spec, because originally compute was started as replica). It's a distinct\n+    API method because semantics and response codes are different:\n+\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.\n+\n+    3.2. Send cache invalidation message to all proxies, notifying them that all new connections\n+        should request and wait for the new connection details. At this stage, proxy has to also\n+        drop any existing connections to the old primary, so they didn't do stale reads.",
        "comment_created_at": "2025-03-20T21:49:48+00:00",
        "comment_author": "ololobus",
        "comment_body": "This step is right after we terminate the primary, so yes, during normal termination, we can expect that at this moment primary will be already terminated and all connections to it will be closed.\r\n\r\nI added this item after talking to Stas, as he had a fair point that the old primary could be unresponsive during this promotion flow, so we will send termination and k8s resources deletion requests, but we cannot generally guarantee that it will be dead by this time. So this step is more to protect from the situation, when old connections will still be connected to the old primary\r\n\r\nSee also item 7 in failure modes. I'm not quite sure how big the problem is. Safekeepers will guarantee that there is only one running writer at a time, so it's more like a nice-to-have, than must-have feature, just to prevent unnecessary interference and side effects (I worried about some stale reads from the old primary and failing writes because safekeepers should reject them)",
        "pr_file_module": null
      }
    ]
  }
]