[
  {
    "discussion_id": "2189689434",
    "pr_number": 12467,
    "pr_file": "pageserver/src/tenant/debug.rs",
    "created_at": "2025-07-07T10:49:07+00:00",
    "commented_code": "+use std::{ops::Range, str::FromStr, sync::Arc};\n+\n+use crate::walredo::RedoAttemptType;\n+use base64::{Engine as _, engine::general_purpose::STANDARD};\n+use bytes::{Bytes, BytesMut};\n+use camino::Utf8PathBuf;\n+use clap::Parser;\n+use itertools::Itertools;\n+use pageserver_api::{\n+    key::Key,\n+    keyspace::KeySpace,\n+    shard::{ShardIdentity, ShardStripeSize},\n+};\n+use postgres_ffi::PgMajorVersion;\n+use postgres_ffi::{BLCKSZ, page_is_new, page_set_lsn};\n+use tracing::Instrument;\n+use utils::{\n+    generation::Generation,\n+    id::{TenantId, TimelineId},\n+    lsn::Lsn,\n+    shard::{ShardCount, ShardIndex, ShardNumber},\n+};\n+use wal_decoder::models::record::NeonWalRecord;\n+\n+use crate::{\n+    context::{DownloadBehavior, RequestContext},\n+    task_mgr::TaskKind,\n+    tenant::storage_layer::ValueReconstructState,\n+    walredo::harness::RedoHarness,\n+};\n+\n+use super::{\n+    WalRedoManager, WalredoManagerId,\n+    harness::TenantHarness,\n+    remote_timeline_client::LayerFileMetadata,\n+    storage_layer::{AsLayerDesc, IoConcurrency, Layer, LayerName, ValuesReconstructState},\n+};\n+\n+fn process_page_image(next_record_lsn: Lsn, is_fpw: bool, img_bytes: Bytes) -> Bytes {\n+    // To match the logic in libs/wal_decoder/src/serialized_batch.rs\n+    let mut new_image: BytesMut = img_bytes.into();\n+    if is_fpw && !page_is_new(&new_image) {\n+        page_set_lsn(&mut new_image, next_record_lsn);\n+    }\n+    assert_eq!(new_image.len(), BLCKSZ as usize);\n+    new_image.freeze()\n+}\n+\n+async fn redo_wals(input: &str, key: Key) -> anyhow::Result<()> {\n+    let tenant_id = TenantId::generate();\n+    let timeline_id = TimelineId::generate();\n+    let redo_harness = RedoHarness::new()?;\n+    let span = redo_harness.span();\n+    let tenant_conf = pageserver_api::models::TenantConfig {\n+        ..Default::default()\n+    };\n+\n+    let ctx = RequestContext::new(TaskKind::DebugTool, DownloadBehavior::Error);\n+    let tenant = TenantHarness::create_custom(\n+        \"search_key\",\n+        tenant_conf,\n+        tenant_id,\n+        ShardIdentity::unsharded(),\n+        Generation::new(1),\n+    )\n+    .await?\n+    .do_try_load_with_redo(\n+        Arc::new(WalRedoManager::Prod(\n+            WalredoManagerId::next(),\n+            redo_harness.manager,\n+        )),\n+        &ctx,\n+    )\n+    .await\n+    .unwrap();\n+    let timeline = tenant\n+        .create_test_timeline(timeline_id, Lsn(0x10), PgMajorVersion::PG16, &ctx)\n+        .await?;\n+    let contents = tokio::fs::read_to_string(input)\n+        .await\n+        .map_err(|e| anyhow::Error::msg(format!(\"Failed to read input file {input}: {e}\")))\n+        .unwrap();\n+    let lines = contents.lines();\n+    let mut last_wal_lsn: Option<Lsn> = None;\n+    let state = {\n+        let mut state = ValueReconstructState::default();\n+        let mut is_fpw = false;\n+        let mut is_first_line = true;\n+        for line in lines {\n+            if is_first_line {\n+                is_first_line = false;\n+                if line.trim() == \"FPW\" {\n+                    is_fpw = true;\n+                }\n+                continue; // Skip the first line.\n+            }\n+            // Each input line is in the \"<next_record_lsn>,<base64>\" format.\n+            let (lsn_str, payload_b64) = line\n+                .split_once(',')\n+                .expect(\"Invalid input format: expected '<lsn>,<base64>'\");\n+\n+            // Parse the LSN and decode the payload.\n+            let lsn = Lsn::from_str(lsn_str.trim()).expect(\"Invalid LSN format\");\n+            let bytes = Bytes::from(\n+                STANDARD\n+                    .decode(payload_b64.trim())\n+                    .expect(\"Invalid base64 payload\"),\n+            );\n+\n+            // The first line is considered the base image, the rest are WAL records.\n+            if state.img.is_none() {\n+                state.img = Some((lsn, process_page_image(lsn, is_fpw, bytes)));\n+            } else {\n+                let wal_record = NeonWalRecord::Postgres {\n+                    will_init: false,\n+                    rec: bytes,\n+                };\n+                state.records.push((lsn, wal_record));\n+                last_wal_lsn.replace(lsn);\n+            }\n+        }\n+        state\n+    };\n+\n+    assert!(state.img.is_some(), \"No base image found\");\n+    assert!(!state.records.is_empty(), \"No WAL records found\");\n+    let result = timeline\n+        .reconstruct_value(key, last_wal_lsn.unwrap(), state, RedoAttemptType::ReadPage)\n+        .instrument(span.clone())\n+        .await?;\n+\n+    eprintln!(\"final image: {:?}\", STANDARD.encode(result));\n+\n+    Ok(())\n+}\n+\n+async fn search_key(",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2189689434",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12467,
        "pr_file": "pageserver/src/tenant/debug.rs",
        "discussion_id": "2189689434",
        "commented_code": "@@ -0,0 +1,366 @@\n+use std::{ops::Range, str::FromStr, sync::Arc};\n+\n+use crate::walredo::RedoAttemptType;\n+use base64::{Engine as _, engine::general_purpose::STANDARD};\n+use bytes::{Bytes, BytesMut};\n+use camino::Utf8PathBuf;\n+use clap::Parser;\n+use itertools::Itertools;\n+use pageserver_api::{\n+    key::Key,\n+    keyspace::KeySpace,\n+    shard::{ShardIdentity, ShardStripeSize},\n+};\n+use postgres_ffi::PgMajorVersion;\n+use postgres_ffi::{BLCKSZ, page_is_new, page_set_lsn};\n+use tracing::Instrument;\n+use utils::{\n+    generation::Generation,\n+    id::{TenantId, TimelineId},\n+    lsn::Lsn,\n+    shard::{ShardCount, ShardIndex, ShardNumber},\n+};\n+use wal_decoder::models::record::NeonWalRecord;\n+\n+use crate::{\n+    context::{DownloadBehavior, RequestContext},\n+    task_mgr::TaskKind,\n+    tenant::storage_layer::ValueReconstructState,\n+    walredo::harness::RedoHarness,\n+};\n+\n+use super::{\n+    WalRedoManager, WalredoManagerId,\n+    harness::TenantHarness,\n+    remote_timeline_client::LayerFileMetadata,\n+    storage_layer::{AsLayerDesc, IoConcurrency, Layer, LayerName, ValuesReconstructState},\n+};\n+\n+fn process_page_image(next_record_lsn: Lsn, is_fpw: bool, img_bytes: Bytes) -> Bytes {\n+    // To match the logic in libs/wal_decoder/src/serialized_batch.rs\n+    let mut new_image: BytesMut = img_bytes.into();\n+    if is_fpw && !page_is_new(&new_image) {\n+        page_set_lsn(&mut new_image, next_record_lsn);\n+    }\n+    assert_eq!(new_image.len(), BLCKSZ as usize);\n+    new_image.freeze()\n+}\n+\n+async fn redo_wals(input: &str, key: Key) -> anyhow::Result<()> {\n+    let tenant_id = TenantId::generate();\n+    let timeline_id = TimelineId::generate();\n+    let redo_harness = RedoHarness::new()?;\n+    let span = redo_harness.span();\n+    let tenant_conf = pageserver_api::models::TenantConfig {\n+        ..Default::default()\n+    };\n+\n+    let ctx = RequestContext::new(TaskKind::DebugTool, DownloadBehavior::Error);\n+    let tenant = TenantHarness::create_custom(\n+        \"search_key\",\n+        tenant_conf,\n+        tenant_id,\n+        ShardIdentity::unsharded(),\n+        Generation::new(1),\n+    )\n+    .await?\n+    .do_try_load_with_redo(\n+        Arc::new(WalRedoManager::Prod(\n+            WalredoManagerId::next(),\n+            redo_harness.manager,\n+        )),\n+        &ctx,\n+    )\n+    .await\n+    .unwrap();\n+    let timeline = tenant\n+        .create_test_timeline(timeline_id, Lsn(0x10), PgMajorVersion::PG16, &ctx)\n+        .await?;\n+    let contents = tokio::fs::read_to_string(input)\n+        .await\n+        .map_err(|e| anyhow::Error::msg(format!(\"Failed to read input file {input}: {e}\")))\n+        .unwrap();\n+    let lines = contents.lines();\n+    let mut last_wal_lsn: Option<Lsn> = None;\n+    let state = {\n+        let mut state = ValueReconstructState::default();\n+        let mut is_fpw = false;\n+        let mut is_first_line = true;\n+        for line in lines {\n+            if is_first_line {\n+                is_first_line = false;\n+                if line.trim() == \"FPW\" {\n+                    is_fpw = true;\n+                }\n+                continue; // Skip the first line.\n+            }\n+            // Each input line is in the \"<next_record_lsn>,<base64>\" format.\n+            let (lsn_str, payload_b64) = line\n+                .split_once(',')\n+                .expect(\"Invalid input format: expected '<lsn>,<base64>'\");\n+\n+            // Parse the LSN and decode the payload.\n+            let lsn = Lsn::from_str(lsn_str.trim()).expect(\"Invalid LSN format\");\n+            let bytes = Bytes::from(\n+                STANDARD\n+                    .decode(payload_b64.trim())\n+                    .expect(\"Invalid base64 payload\"),\n+            );\n+\n+            // The first line is considered the base image, the rest are WAL records.\n+            if state.img.is_none() {\n+                state.img = Some((lsn, process_page_image(lsn, is_fpw, bytes)));\n+            } else {\n+                let wal_record = NeonWalRecord::Postgres {\n+                    will_init: false,\n+                    rec: bytes,\n+                };\n+                state.records.push((lsn, wal_record));\n+                last_wal_lsn.replace(lsn);\n+            }\n+        }\n+        state\n+    };\n+\n+    assert!(state.img.is_some(), \"No base image found\");\n+    assert!(!state.records.is_empty(), \"No WAL records found\");\n+    let result = timeline\n+        .reconstruct_value(key, last_wal_lsn.unwrap(), state, RedoAttemptType::ReadPage)\n+        .instrument(span.clone())\n+        .await?;\n+\n+    eprintln!(\"final image: {:?}\", STANDARD.encode(result));\n+\n+    Ok(())\n+}\n+\n+async fn search_key(",
        "comment_created_at": "2025-07-07T10:49:07+00:00",
        "comment_author": "VladLazar",
        "comment_body": "This isn't entirely correct. It doesn't handle:\r\n* overlapping layers\r\n* delta layers containing a full page image\r\n\r\nIn some cases it would panic and in others it would just yield an incorrect result.\r\nThe correct way of doing this is to use a neon local setup, import the tenant state from s3 and then use the HTTP endpoints to get the reconstructed page: `GET v1/tenant/<tid>-<shard-id>/timeline/<tlid>/getpage?lsn=<lsn>`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2193074252",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12467,
        "pr_file": "pageserver/src/tenant/debug.rs",
        "discussion_id": "2189689434",
        "commented_code": "@@ -0,0 +1,366 @@\n+use std::{ops::Range, str::FromStr, sync::Arc};\n+\n+use crate::walredo::RedoAttemptType;\n+use base64::{Engine as _, engine::general_purpose::STANDARD};\n+use bytes::{Bytes, BytesMut};\n+use camino::Utf8PathBuf;\n+use clap::Parser;\n+use itertools::Itertools;\n+use pageserver_api::{\n+    key::Key,\n+    keyspace::KeySpace,\n+    shard::{ShardIdentity, ShardStripeSize},\n+};\n+use postgres_ffi::PgMajorVersion;\n+use postgres_ffi::{BLCKSZ, page_is_new, page_set_lsn};\n+use tracing::Instrument;\n+use utils::{\n+    generation::Generation,\n+    id::{TenantId, TimelineId},\n+    lsn::Lsn,\n+    shard::{ShardCount, ShardIndex, ShardNumber},\n+};\n+use wal_decoder::models::record::NeonWalRecord;\n+\n+use crate::{\n+    context::{DownloadBehavior, RequestContext},\n+    task_mgr::TaskKind,\n+    tenant::storage_layer::ValueReconstructState,\n+    walredo::harness::RedoHarness,\n+};\n+\n+use super::{\n+    WalRedoManager, WalredoManagerId,\n+    harness::TenantHarness,\n+    remote_timeline_client::LayerFileMetadata,\n+    storage_layer::{AsLayerDesc, IoConcurrency, Layer, LayerName, ValuesReconstructState},\n+};\n+\n+fn process_page_image(next_record_lsn: Lsn, is_fpw: bool, img_bytes: Bytes) -> Bytes {\n+    // To match the logic in libs/wal_decoder/src/serialized_batch.rs\n+    let mut new_image: BytesMut = img_bytes.into();\n+    if is_fpw && !page_is_new(&new_image) {\n+        page_set_lsn(&mut new_image, next_record_lsn);\n+    }\n+    assert_eq!(new_image.len(), BLCKSZ as usize);\n+    new_image.freeze()\n+}\n+\n+async fn redo_wals(input: &str, key: Key) -> anyhow::Result<()> {\n+    let tenant_id = TenantId::generate();\n+    let timeline_id = TimelineId::generate();\n+    let redo_harness = RedoHarness::new()?;\n+    let span = redo_harness.span();\n+    let tenant_conf = pageserver_api::models::TenantConfig {\n+        ..Default::default()\n+    };\n+\n+    let ctx = RequestContext::new(TaskKind::DebugTool, DownloadBehavior::Error);\n+    let tenant = TenantHarness::create_custom(\n+        \"search_key\",\n+        tenant_conf,\n+        tenant_id,\n+        ShardIdentity::unsharded(),\n+        Generation::new(1),\n+    )\n+    .await?\n+    .do_try_load_with_redo(\n+        Arc::new(WalRedoManager::Prod(\n+            WalredoManagerId::next(),\n+            redo_harness.manager,\n+        )),\n+        &ctx,\n+    )\n+    .await\n+    .unwrap();\n+    let timeline = tenant\n+        .create_test_timeline(timeline_id, Lsn(0x10), PgMajorVersion::PG16, &ctx)\n+        .await?;\n+    let contents = tokio::fs::read_to_string(input)\n+        .await\n+        .map_err(|e| anyhow::Error::msg(format!(\"Failed to read input file {input}: {e}\")))\n+        .unwrap();\n+    let lines = contents.lines();\n+    let mut last_wal_lsn: Option<Lsn> = None;\n+    let state = {\n+        let mut state = ValueReconstructState::default();\n+        let mut is_fpw = false;\n+        let mut is_first_line = true;\n+        for line in lines {\n+            if is_first_line {\n+                is_first_line = false;\n+                if line.trim() == \"FPW\" {\n+                    is_fpw = true;\n+                }\n+                continue; // Skip the first line.\n+            }\n+            // Each input line is in the \"<next_record_lsn>,<base64>\" format.\n+            let (lsn_str, payload_b64) = line\n+                .split_once(',')\n+                .expect(\"Invalid input format: expected '<lsn>,<base64>'\");\n+\n+            // Parse the LSN and decode the payload.\n+            let lsn = Lsn::from_str(lsn_str.trim()).expect(\"Invalid LSN format\");\n+            let bytes = Bytes::from(\n+                STANDARD\n+                    .decode(payload_b64.trim())\n+                    .expect(\"Invalid base64 payload\"),\n+            );\n+\n+            // The first line is considered the base image, the rest are WAL records.\n+            if state.img.is_none() {\n+                state.img = Some((lsn, process_page_image(lsn, is_fpw, bytes)));\n+            } else {\n+                let wal_record = NeonWalRecord::Postgres {\n+                    will_init: false,\n+                    rec: bytes,\n+                };\n+                state.records.push((lsn, wal_record));\n+                last_wal_lsn.replace(lsn);\n+            }\n+        }\n+        state\n+    };\n+\n+    assert!(state.img.is_some(), \"No base image found\");\n+    assert!(!state.records.is_empty(), \"No WAL records found\");\n+    let result = timeline\n+        .reconstruct_value(key, last_wal_lsn.unwrap(), state, RedoAttemptType::ReadPage)\n+        .instrument(span.clone())\n+        .await?;\n+\n+    eprintln!(\"final image: {:?}\", STANDARD.encode(result));\n+\n+    Ok(())\n+}\n+\n+async fn search_key(",
        "comment_created_at": "2025-07-08T17:30:40+00:00",
        "comment_author": "HaoyuHuang",
        "comment_body": "yes, this is not entirely correct. We only used it once to debug an earlier PS corruption (fixed now). \r\n\r\nLet's merge this in and improve/remove this later. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089443932",
    "pr_number": 11862,
    "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
    "created_at": "2025-05-14T17:45:48+00:00",
    "commented_code": "let import_config = &timeline.conf.timeline_import_config;\n     let plan = planner.plan(import_config).await?;\n \n+    // Hash the plan and compare with the hash of the plan we got back from the storage controller.\n+    // If the two match, it means that the planning stage had the same output.\n+    //\n+    // This is not intended to be a cryptographically secure hash.\n+    const SEED: u64 = 42;\n+    let mut hasher = twox_hash::XxHash64::with_seed(SEED);\n+    plan.hash(&mut hasher);\n+    let plan_hash = hasher.finish();\n+\n+    if let Some(progress) = &import_progress {\n+        if !(plan_hash == progress.import_plan_hash || progress.jobs != plan.jobs.len()) {\n+            anyhow::bail!(\"Import plan does not match storcon metadata\");\n+        }\n+    }",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2089443932",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11862,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2089443932",
        "commented_code": "@@ -81,9 +85,26 @@ pub async fn run(\n     let import_config = &timeline.conf.timeline_import_config;\n     let plan = planner.plan(import_config).await?;\n \n+    // Hash the plan and compare with the hash of the plan we got back from the storage controller.\n+    // If the two match, it means that the planning stage had the same output.\n+    //\n+    // This is not intended to be a cryptographically secure hash.\n+    const SEED: u64 = 42;\n+    let mut hasher = twox_hash::XxHash64::with_seed(SEED);\n+    plan.hash(&mut hasher);\n+    let plan_hash = hasher.finish();\n+\n+    if let Some(progress) = &import_progress {\n+        if !(plan_hash == progress.import_plan_hash || progress.jobs != plan.jobs.len()) {\n+            anyhow::bail!(\"Import plan does not match storcon metadata\");\n+        }\n+    }",
        "comment_created_at": "2025-05-14T17:45:48+00:00",
        "comment_author": "problame",
        "comment_body": "For my understanding, why are we checking for lengths? Probably because we index into it and want to make sure we'll stay in bounds?\n\nAlso, I'm not sure the `!(== || !=)` is right?\n\n```suggestion\n    const SEED: u64 = 42;\n    let mut hasher = twox_hash::XxHash64::with_seed(SEED);\n    plan.hash(&mut hasher);\n    let plan_hash = hasher.finish();\n\n    if let Some(progress) = &import_progress {\n        if plan_hash != progress.import_plan_hash {\n            anyhow::bail!(\"Import plan hash does not match storcon metadata hash\");\n        }\n        // Make absolutely sure we stay within bounds; xxhash collision isn't worth an out-of-bounds panic\n        if progress.jobs != plan.jobs.len() {\n            anyhow::bail!(\"Import plan job length does not match storcon metadata hash)\n        }\n    }\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2090617182",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11862,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2089443932",
        "commented_code": "@@ -81,9 +85,26 @@ pub async fn run(\n     let import_config = &timeline.conf.timeline_import_config;\n     let plan = planner.plan(import_config).await?;\n \n+    // Hash the plan and compare with the hash of the plan we got back from the storage controller.\n+    // If the two match, it means that the planning stage had the same output.\n+    //\n+    // This is not intended to be a cryptographically secure hash.\n+    const SEED: u64 = 42;\n+    let mut hasher = twox_hash::XxHash64::with_seed(SEED);\n+    plan.hash(&mut hasher);\n+    let plan_hash = hasher.finish();\n+\n+    if let Some(progress) = &import_progress {\n+        if !(plan_hash == progress.import_plan_hash || progress.jobs != plan.jobs.len()) {\n+            anyhow::bail!(\"Import plan does not match storcon metadata\");\n+        }\n+    }",
        "comment_created_at": "2025-05-15T08:39:01+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Indeed the check was wrong. Good catch.\r\n\r\nIt's not about bound checking since we don't blindly index into the jobs.\r\nIt narrows collisions down to collisions on plans with the same length.\r\n\r\n> Wonder why we wouldn't go all the way and compute a cryptographic checksum\r\n\r\nI don't think we'll have enough on-going imports at any given time to worry about collisions here.\r\nFor 1 million the collision probability is ~2.71e-08. This is without accounting for imports of the same length check.",
        "pr_file_module": null
      },
      {
        "comment_id": "2090998576",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11862,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2089443932",
        "commented_code": "@@ -81,9 +85,26 @@ pub async fn run(\n     let import_config = &timeline.conf.timeline_import_config;\n     let plan = planner.plan(import_config).await?;\n \n+    // Hash the plan and compare with the hash of the plan we got back from the storage controller.\n+    // If the two match, it means that the planning stage had the same output.\n+    //\n+    // This is not intended to be a cryptographically secure hash.\n+    const SEED: u64 = 42;\n+    let mut hasher = twox_hash::XxHash64::with_seed(SEED);\n+    plan.hash(&mut hasher);\n+    let plan_hash = hasher.finish();\n+\n+    if let Some(progress) = &import_progress {\n+        if !(plan_hash == progress.import_plan_hash || progress.jobs != plan.jobs.len()) {\n+            anyhow::bail!(\"Import plan does not match storcon metadata\");\n+        }\n+    }",
        "comment_created_at": "2025-05-15T11:53:59+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Fixed in https://github.com/neondatabase/neon/pull/11862/commits/5ec58a723b11e0370dab9c8b6cd0d64ceab414e2",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089458640",
    "pr_number": 11862,
    "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
    "created_at": "2025-05-14T17:54:33+00:00",
    "commented_code": "storage: RemoteStorageWrapper,\n }\n \n+impl Hash for ImportRelBlocksTask {\n+    fn hash<H: Hasher>(&self, state: &mut H) {\n+        let ImportRelBlocksTask {\n+            shard_identity: _,\n+            key_range,\n+            path,\n+            storage: _,\n+        } = self;\n+\n+        key_range.hash(state);\n+        path.hash(state);",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2089458640",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11862,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2089458640",
        "commented_code": "@@ -581,6 +634,20 @@ struct ImportRelBlocksTask {\n     storage: RemoteStorageWrapper,\n }\n \n+impl Hash for ImportRelBlocksTask {\n+    fn hash<H: Hasher>(&self, state: &mut H) {\n+        let ImportRelBlocksTask {\n+            shard_identity: _,\n+            key_range,\n+            path,\n+            storage: _,\n+        } = self;\n+\n+        key_range.hash(state);\n+        path.hash(state);",
        "comment_created_at": "2025-05-14T17:54:33+00:00",
        "comment_author": "problame",
        "comment_body": "Hmm, why is shard_identity not part of the hash? It's cheap to include it and it adds some robustness against failure by storcon to ensure shard_split XOR import",
        "pr_file_module": null
      },
      {
        "comment_id": "2091000717",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11862,
        "pr_file": "pageserver/src/tenant/timeline/import_pgdata/flow.rs",
        "discussion_id": "2089458640",
        "commented_code": "@@ -581,6 +634,20 @@ struct ImportRelBlocksTask {\n     storage: RemoteStorageWrapper,\n }\n \n+impl Hash for ImportRelBlocksTask {\n+    fn hash<H: Hasher>(&self, state: &mut H) {\n+        let ImportRelBlocksTask {\n+            shard_identity: _,\n+            key_range,\n+            path,\n+            storage: _,\n+        } = self;\n+\n+        key_range.hash(state);\n+        path.hash(state);",
        "comment_created_at": "2025-05-15T11:55:14+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Done: https://github.com/neondatabase/neon/pull/11862/commits/27458a203b18510e20c57bf6feb229a171f59cb1",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2078010511",
    "pr_number": 11821,
    "pr_file": "libs/posthog_client_lite/src/lib.rs",
    "created_at": "2025-05-07T16:13:12+00:00",
    "commented_code": "+//! A lite version of the PostHog client that only supports local evaluation of feature flags.\n+\n+use std::{collections::HashMap, sync::Arc};\n+\n+use serde::{Deserialize, Serialize};\n+use serde_json::json;\n+use sha2::Digest;\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationResponse {\n+    #[allow(dead_code)]\n+    flags: Vec<PostHogLocalEvaluationFlag>,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlag {\n+    key: String,\n+    filters: PostHogLocalEvaluationFlagFilters,\n+    active: bool,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilters {\n+    groups: Vec<PostHogLocalEvaluationFlagFilterGroup>,\n+    multivariate: PostHogLocalEvaluationFlagMultivariate,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilterGroup {\n+    variant: Option<String>,\n+    properties: Option<Vec<PostHogLocalEvaluationFlagFilterProperty>>,\n+    rollout_percentage: f64,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilterProperty {\n+    key: String,\n+    value: PostHogFlagFilterPropertyValue,\n+    operator: String,\n+}\n+\n+#[derive(Serialize, Deserialize)]\n+#[serde(untagged)]\n+pub enum PostHogFlagFilterPropertyValue {\n+    String(String),\n+    Number(f64),\n+    Boolean(bool),\n+    List(Vec<String>),\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagMultivariate {\n+    variants: Vec<PostHogLocalEvaluationFlagMultivariateVariant>,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagMultivariateVariant {\n+    key: String,\n+    rollout_percentage: f64,\n+}\n+\n+pub struct PostHogClient {\n+    server_api_key: String,\n+    client_api_key: String,\n+    project_id: String,\n+    private_api_url: String,\n+    public_api_url: String,\n+    client: Arc<reqwest::Client>,\n+}\n+\n+pub struct FeatureStore {\n+    flags: HashMap<String, PostHogLocalEvaluationFlag>,\n+}\n+\n+impl Default for FeatureStore {\n+    fn default() -> Self {\n+        Self::new()\n+    }\n+}\n+\n+impl FeatureStore {\n+    pub fn new() -> Self {\n+        Self {\n+            flags: HashMap::new(),\n+        }\n+    }\n+\n+    pub fn update_flags(&mut self, flags: Vec<PostHogLocalEvaluationFlag>) {\n+        self.flags.clear();\n+        for flag in flags {\n+            if flag.active {\n+                self.flags.insert(flag.key.clone(), flag);\n+            }\n+        }\n+    }\n+\n+    /// Generate a consistent hash for a user ID (e.g., tenant ID).\n+    ///\n+    /// The implementation is different from PostHog SDK. In PostHog SDK, it is sha1 of `user_id.distinct_id.salt`.\n+    /// However, as we do not upload all of our tenant IDs to PostHog, we do not have the PostHog distinct_id for a\n+    /// tenant.\n+    fn consistent_hash(user_id: &str) -> f64 {\n+        let mut hasher = sha2::Sha256::new();\n+        hasher.update(user_id);\n+        let hash = hasher.finalize();\n+        let hash_int = u64::from_le_bytes(hash[..8].try_into().unwrap());\n+        hash_int as f64 / u64::MAX as f64\n+    }\n+\n+    /// Evaluate a condition. Returns `None` if the condition cannot be evaluated due to parsing error or missing\n+    /// property.\n+    fn evaluate_condition(\n+        &self,\n+        operator: &str,\n+        provided: &PostHogFlagFilterPropertyValue,\n+        requested: &PostHogFlagFilterPropertyValue,\n+    ) -> Option<bool> {\n+        match operator {\n+            \"exact\" => {\n+                let PostHogFlagFilterPropertyValue::String(provided) = provided else {\n+                    // Left should be a string\n+                    return None;\n+                };\n+                let PostHogFlagFilterPropertyValue::List(requested) = requested else {\n+                    // Right should be a list of string\n+                    return None;\n+                };\n+                Some(requested.contains(provided))\n+            }\n+            \"lt\" | \"gt\" => {\n+                let PostHogFlagFilterPropertyValue::String(requested) = requested else {\n+                    // Right should be a string\n+                    return None;\n+                };\n+                let Ok(requested) = requested.parse::<f64>() else {\n+                    return None;\n+                };\n+                // Left can either be a number or a string\n+                let provided = match provided {\n+                    PostHogFlagFilterPropertyValue::Number(provided) => *provided,\n+                    PostHogFlagFilterPropertyValue::String(provided) => {\n+                        let Ok(provided) = provided.parse::<f64>() else {\n+                            return None;\n+                        };\n+                        provided\n+                    }\n+                    _ => return None,\n+                };\n+                match operator {\n+                    \"lt\" => Some(provided < requested),\n+                    \"gt\" => Some(provided > requested),\n+                    _ => None,\n+                }\n+            }\n+            _ => None,\n+        }\n+    }\n+\n+    /// Evaluate a percentage.\n+    fn evaluate_percentage(&self, mapped_user_id: f64, percentage: f64) -> bool {\n+        mapped_user_id <= percentage\n+    }\n+\n+    /// Evaluate a filter group for a feature flag. Returns `None` if the group is not matched or if there are errors\n+    /// during the evaluation.\n+    fn evaluate_group(\n+        &self,\n+        group: &PostHogLocalEvaluationFlagFilterGroup,\n+        mapped_user_id: f64,\n+        provided_properties: &HashMap<String, PostHogFlagFilterPropertyValue>,\n+    ) -> Option<String> {\n+        if let Some(ref properties) = group.properties {\n+            for property in properties {\n+                if let Some(value) = provided_properties.get(&property.key) {\n+                    // The user provided the property value\n+                    if self.evaluate_condition(property.operator.as_ref(), value, &property.value)\n+                        != Some(true)\n+                    {\n+                        return None;\n+                    }\n+                } else {\n+                    // We cannot evaluate, the property is not available\n+                    return None;\n+                }\n+            }\n+            // If all conditions are met, return the variant\n+            if self.evaluate_percentage(mapped_user_id, group.rollout_percentage / 100.0) {\n+                group.variant.clone()\n+            } else {\n+                None\n+            }\n+        } else {\n+            // No matchers, apply to all users\n+            if self.evaluate_percentage(mapped_user_id, group.rollout_percentage / 100.0) {\n+                group.variant.clone()\n+            } else {\n+                None\n+            }\n+        }\n+    }\n+\n+    /// Evaluate a multivariate feature flag. Returns `None` if the flag is not available or if there are errors\n+    /// during the evaluation.\n+    ///\n+    /// The parsing logic is as follows:\n+    ///\n+    /// * Match each filter group. If any group is matched, check the rollout percentage against the precomputed\n+    ///   user ID on the consistent hash ring. If the user ID is in the range of the group's rollout percentage,\n+    ///   return the variant.\n+    /// * Otherwise, continue with the next group until all groups are evaluated and no group is within the\n+    ///   rollout percentage.\n+    /// * If there are no matching groups, evaluate by the global rollout percentage.\n+    pub fn evaluate_multivariate(&self, flag_key: &str, user_id: &str) -> Option<String> {\n+        let mapped_user_id = Self::consistent_hash(user_id);\n+        self.evaluate_multivariate_inner(flag_key, mapped_user_id, &HashMap::new())\n+    }\n+\n+    /// Evaluate a multivariate feature flag. Returns `None` if the flag is not available or if there are errors\n+    /// during the evaluation. Note that we directly take the mapped user ID (a consistent hash ranging from 0 to 1)\n+    /// so that it is easier to use it in the tests and avoid duplicate computations.\n+    pub fn evaluate_multivariate_inner(\n+        &self,\n+        flag_key: &str,\n+        mapped_user_id: f64,\n+        properties: &HashMap<String, PostHogFlagFilterPropertyValue>,\n+    ) -> Option<String> {\n+        if let Some(flag_config) = self.flags.get(flag_key) {\n+            // First, try to evaluate each group\n+            for group in &flag_config.filters.groups {\n+                if let Some(variant) = self.evaluate_group(group, mapped_user_id, properties) {\n+                    return Some(variant);\n+                }\n+            }\n+            // If no group is matched, evaluate by the global rollout percentage\n+            let mut percentage = 0.0;\n+            for variant in &flag_config.filters.multivariate.variants {\n+                percentage += variant.rollout_percentage;\n+                if self.evaluate_percentage(mapped_user_id, percentage / 100.0) {\n+                    return Some(variant.key.clone());\n+                }\n+            }\n+            // This should not happen because the rollout percentage always adds up to 100, but just in case,",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2078010511",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11821,
        "pr_file": "libs/posthog_client_lite/src/lib.rs",
        "discussion_id": "2078010511",
        "commented_code": "@@ -0,0 +1,433 @@\n+//! A lite version of the PostHog client that only supports local evaluation of feature flags.\n+\n+use std::{collections::HashMap, sync::Arc};\n+\n+use serde::{Deserialize, Serialize};\n+use serde_json::json;\n+use sha2::Digest;\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationResponse {\n+    #[allow(dead_code)]\n+    flags: Vec<PostHogLocalEvaluationFlag>,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlag {\n+    key: String,\n+    filters: PostHogLocalEvaluationFlagFilters,\n+    active: bool,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilters {\n+    groups: Vec<PostHogLocalEvaluationFlagFilterGroup>,\n+    multivariate: PostHogLocalEvaluationFlagMultivariate,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilterGroup {\n+    variant: Option<String>,\n+    properties: Option<Vec<PostHogLocalEvaluationFlagFilterProperty>>,\n+    rollout_percentage: f64,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilterProperty {\n+    key: String,\n+    value: PostHogFlagFilterPropertyValue,\n+    operator: String,\n+}\n+\n+#[derive(Serialize, Deserialize)]\n+#[serde(untagged)]\n+pub enum PostHogFlagFilterPropertyValue {\n+    String(String),\n+    Number(f64),\n+    Boolean(bool),\n+    List(Vec<String>),\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagMultivariate {\n+    variants: Vec<PostHogLocalEvaluationFlagMultivariateVariant>,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagMultivariateVariant {\n+    key: String,\n+    rollout_percentage: f64,\n+}\n+\n+pub struct PostHogClient {\n+    server_api_key: String,\n+    client_api_key: String,\n+    project_id: String,\n+    private_api_url: String,\n+    public_api_url: String,\n+    client: Arc<reqwest::Client>,\n+}\n+\n+pub struct FeatureStore {\n+    flags: HashMap<String, PostHogLocalEvaluationFlag>,\n+}\n+\n+impl Default for FeatureStore {\n+    fn default() -> Self {\n+        Self::new()\n+    }\n+}\n+\n+impl FeatureStore {\n+    pub fn new() -> Self {\n+        Self {\n+            flags: HashMap::new(),\n+        }\n+    }\n+\n+    pub fn update_flags(&mut self, flags: Vec<PostHogLocalEvaluationFlag>) {\n+        self.flags.clear();\n+        for flag in flags {\n+            if flag.active {\n+                self.flags.insert(flag.key.clone(), flag);\n+            }\n+        }\n+    }\n+\n+    /// Generate a consistent hash for a user ID (e.g., tenant ID).\n+    ///\n+    /// The implementation is different from PostHog SDK. In PostHog SDK, it is sha1 of `user_id.distinct_id.salt`.\n+    /// However, as we do not upload all of our tenant IDs to PostHog, we do not have the PostHog distinct_id for a\n+    /// tenant.\n+    fn consistent_hash(user_id: &str) -> f64 {\n+        let mut hasher = sha2::Sha256::new();\n+        hasher.update(user_id);\n+        let hash = hasher.finalize();\n+        let hash_int = u64::from_le_bytes(hash[..8].try_into().unwrap());\n+        hash_int as f64 / u64::MAX as f64\n+    }\n+\n+    /// Evaluate a condition. Returns `None` if the condition cannot be evaluated due to parsing error or missing\n+    /// property.\n+    fn evaluate_condition(\n+        &self,\n+        operator: &str,\n+        provided: &PostHogFlagFilterPropertyValue,\n+        requested: &PostHogFlagFilterPropertyValue,\n+    ) -> Option<bool> {\n+        match operator {\n+            \"exact\" => {\n+                let PostHogFlagFilterPropertyValue::String(provided) = provided else {\n+                    // Left should be a string\n+                    return None;\n+                };\n+                let PostHogFlagFilterPropertyValue::List(requested) = requested else {\n+                    // Right should be a list of string\n+                    return None;\n+                };\n+                Some(requested.contains(provided))\n+            }\n+            \"lt\" | \"gt\" => {\n+                let PostHogFlagFilterPropertyValue::String(requested) = requested else {\n+                    // Right should be a string\n+                    return None;\n+                };\n+                let Ok(requested) = requested.parse::<f64>() else {\n+                    return None;\n+                };\n+                // Left can either be a number or a string\n+                let provided = match provided {\n+                    PostHogFlagFilterPropertyValue::Number(provided) => *provided,\n+                    PostHogFlagFilterPropertyValue::String(provided) => {\n+                        let Ok(provided) = provided.parse::<f64>() else {\n+                            return None;\n+                        };\n+                        provided\n+                    }\n+                    _ => return None,\n+                };\n+                match operator {\n+                    \"lt\" => Some(provided < requested),\n+                    \"gt\" => Some(provided > requested),\n+                    _ => None,\n+                }\n+            }\n+            _ => None,\n+        }\n+    }\n+\n+    /// Evaluate a percentage.\n+    fn evaluate_percentage(&self, mapped_user_id: f64, percentage: f64) -> bool {\n+        mapped_user_id <= percentage\n+    }\n+\n+    /// Evaluate a filter group for a feature flag. Returns `None` if the group is not matched or if there are errors\n+    /// during the evaluation.\n+    fn evaluate_group(\n+        &self,\n+        group: &PostHogLocalEvaluationFlagFilterGroup,\n+        mapped_user_id: f64,\n+        provided_properties: &HashMap<String, PostHogFlagFilterPropertyValue>,\n+    ) -> Option<String> {\n+        if let Some(ref properties) = group.properties {\n+            for property in properties {\n+                if let Some(value) = provided_properties.get(&property.key) {\n+                    // The user provided the property value\n+                    if self.evaluate_condition(property.operator.as_ref(), value, &property.value)\n+                        != Some(true)\n+                    {\n+                        return None;\n+                    }\n+                } else {\n+                    // We cannot evaluate, the property is not available\n+                    return None;\n+                }\n+            }\n+            // If all conditions are met, return the variant\n+            if self.evaluate_percentage(mapped_user_id, group.rollout_percentage / 100.0) {\n+                group.variant.clone()\n+            } else {\n+                None\n+            }\n+        } else {\n+            // No matchers, apply to all users\n+            if self.evaluate_percentage(mapped_user_id, group.rollout_percentage / 100.0) {\n+                group.variant.clone()\n+            } else {\n+                None\n+            }\n+        }\n+    }\n+\n+    /// Evaluate a multivariate feature flag. Returns `None` if the flag is not available or if there are errors\n+    /// during the evaluation.\n+    ///\n+    /// The parsing logic is as follows:\n+    ///\n+    /// * Match each filter group. If any group is matched, check the rollout percentage against the precomputed\n+    ///   user ID on the consistent hash ring. If the user ID is in the range of the group's rollout percentage,\n+    ///   return the variant.\n+    /// * Otherwise, continue with the next group until all groups are evaluated and no group is within the\n+    ///   rollout percentage.\n+    /// * If there are no matching groups, evaluate by the global rollout percentage.\n+    pub fn evaluate_multivariate(&self, flag_key: &str, user_id: &str) -> Option<String> {\n+        let mapped_user_id = Self::consistent_hash(user_id);\n+        self.evaluate_multivariate_inner(flag_key, mapped_user_id, &HashMap::new())\n+    }\n+\n+    /// Evaluate a multivariate feature flag. Returns `None` if the flag is not available or if there are errors\n+    /// during the evaluation. Note that we directly take the mapped user ID (a consistent hash ranging from 0 to 1)\n+    /// so that it is easier to use it in the tests and avoid duplicate computations.\n+    pub fn evaluate_multivariate_inner(\n+        &self,\n+        flag_key: &str,\n+        mapped_user_id: f64,\n+        properties: &HashMap<String, PostHogFlagFilterPropertyValue>,\n+    ) -> Option<String> {\n+        if let Some(flag_config) = self.flags.get(flag_key) {\n+            // First, try to evaluate each group\n+            for group in &flag_config.filters.groups {\n+                if let Some(variant) = self.evaluate_group(group, mapped_user_id, properties) {\n+                    return Some(variant);\n+                }\n+            }\n+            // If no group is matched, evaluate by the global rollout percentage\n+            let mut percentage = 0.0;\n+            for variant in &flag_config.filters.multivariate.variants {\n+                percentage += variant.rollout_percentage;\n+                if self.evaluate_percentage(mapped_user_id, percentage / 100.0) {\n+                    return Some(variant.key.clone());\n+                }\n+            }\n+            // This should not happen because the rollout percentage always adds up to 100, but just in case,",
        "comment_created_at": "2025-05-07T16:13:12+00:00",
        "comment_author": "DimasKovas",
        "comment_body": "Well, I guess it's very unlikely to happen, but working with floating numbers, I wouldn't assume that some numbers add up to 100. There is always a precision error. Maybe returning the last value would be a better option?",
        "pr_file_module": null
      },
      {
        "comment_id": "2078762892",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11821,
        "pr_file": "libs/posthog_client_lite/src/lib.rs",
        "discussion_id": "2078010511",
        "commented_code": "@@ -0,0 +1,433 @@\n+//! A lite version of the PostHog client that only supports local evaluation of feature flags.\n+\n+use std::{collections::HashMap, sync::Arc};\n+\n+use serde::{Deserialize, Serialize};\n+use serde_json::json;\n+use sha2::Digest;\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationResponse {\n+    #[allow(dead_code)]\n+    flags: Vec<PostHogLocalEvaluationFlag>,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlag {\n+    key: String,\n+    filters: PostHogLocalEvaluationFlagFilters,\n+    active: bool,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilters {\n+    groups: Vec<PostHogLocalEvaluationFlagFilterGroup>,\n+    multivariate: PostHogLocalEvaluationFlagMultivariate,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilterGroup {\n+    variant: Option<String>,\n+    properties: Option<Vec<PostHogLocalEvaluationFlagFilterProperty>>,\n+    rollout_percentage: f64,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagFilterProperty {\n+    key: String,\n+    value: PostHogFlagFilterPropertyValue,\n+    operator: String,\n+}\n+\n+#[derive(Serialize, Deserialize)]\n+#[serde(untagged)]\n+pub enum PostHogFlagFilterPropertyValue {\n+    String(String),\n+    Number(f64),\n+    Boolean(bool),\n+    List(Vec<String>),\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagMultivariate {\n+    variants: Vec<PostHogLocalEvaluationFlagMultivariateVariant>,\n+}\n+\n+#[derive(Deserialize)]\n+pub struct PostHogLocalEvaluationFlagMultivariateVariant {\n+    key: String,\n+    rollout_percentage: f64,\n+}\n+\n+pub struct PostHogClient {\n+    server_api_key: String,\n+    client_api_key: String,\n+    project_id: String,\n+    private_api_url: String,\n+    public_api_url: String,\n+    client: Arc<reqwest::Client>,\n+}\n+\n+pub struct FeatureStore {\n+    flags: HashMap<String, PostHogLocalEvaluationFlag>,\n+}\n+\n+impl Default for FeatureStore {\n+    fn default() -> Self {\n+        Self::new()\n+    }\n+}\n+\n+impl FeatureStore {\n+    pub fn new() -> Self {\n+        Self {\n+            flags: HashMap::new(),\n+        }\n+    }\n+\n+    pub fn update_flags(&mut self, flags: Vec<PostHogLocalEvaluationFlag>) {\n+        self.flags.clear();\n+        for flag in flags {\n+            if flag.active {\n+                self.flags.insert(flag.key.clone(), flag);\n+            }\n+        }\n+    }\n+\n+    /// Generate a consistent hash for a user ID (e.g., tenant ID).\n+    ///\n+    /// The implementation is different from PostHog SDK. In PostHog SDK, it is sha1 of `user_id.distinct_id.salt`.\n+    /// However, as we do not upload all of our tenant IDs to PostHog, we do not have the PostHog distinct_id for a\n+    /// tenant.\n+    fn consistent_hash(user_id: &str) -> f64 {\n+        let mut hasher = sha2::Sha256::new();\n+        hasher.update(user_id);\n+        let hash = hasher.finalize();\n+        let hash_int = u64::from_le_bytes(hash[..8].try_into().unwrap());\n+        hash_int as f64 / u64::MAX as f64\n+    }\n+\n+    /// Evaluate a condition. Returns `None` if the condition cannot be evaluated due to parsing error or missing\n+    /// property.\n+    fn evaluate_condition(\n+        &self,\n+        operator: &str,\n+        provided: &PostHogFlagFilterPropertyValue,\n+        requested: &PostHogFlagFilterPropertyValue,\n+    ) -> Option<bool> {\n+        match operator {\n+            \"exact\" => {\n+                let PostHogFlagFilterPropertyValue::String(provided) = provided else {\n+                    // Left should be a string\n+                    return None;\n+                };\n+                let PostHogFlagFilterPropertyValue::List(requested) = requested else {\n+                    // Right should be a list of string\n+                    return None;\n+                };\n+                Some(requested.contains(provided))\n+            }\n+            \"lt\" | \"gt\" => {\n+                let PostHogFlagFilterPropertyValue::String(requested) = requested else {\n+                    // Right should be a string\n+                    return None;\n+                };\n+                let Ok(requested) = requested.parse::<f64>() else {\n+                    return None;\n+                };\n+                // Left can either be a number or a string\n+                let provided = match provided {\n+                    PostHogFlagFilterPropertyValue::Number(provided) => *provided,\n+                    PostHogFlagFilterPropertyValue::String(provided) => {\n+                        let Ok(provided) = provided.parse::<f64>() else {\n+                            return None;\n+                        };\n+                        provided\n+                    }\n+                    _ => return None,\n+                };\n+                match operator {\n+                    \"lt\" => Some(provided < requested),\n+                    \"gt\" => Some(provided > requested),\n+                    _ => None,\n+                }\n+            }\n+            _ => None,\n+        }\n+    }\n+\n+    /// Evaluate a percentage.\n+    fn evaluate_percentage(&self, mapped_user_id: f64, percentage: f64) -> bool {\n+        mapped_user_id <= percentage\n+    }\n+\n+    /// Evaluate a filter group for a feature flag. Returns `None` if the group is not matched or if there are errors\n+    /// during the evaluation.\n+    fn evaluate_group(\n+        &self,\n+        group: &PostHogLocalEvaluationFlagFilterGroup,\n+        mapped_user_id: f64,\n+        provided_properties: &HashMap<String, PostHogFlagFilterPropertyValue>,\n+    ) -> Option<String> {\n+        if let Some(ref properties) = group.properties {\n+            for property in properties {\n+                if let Some(value) = provided_properties.get(&property.key) {\n+                    // The user provided the property value\n+                    if self.evaluate_condition(property.operator.as_ref(), value, &property.value)\n+                        != Some(true)\n+                    {\n+                        return None;\n+                    }\n+                } else {\n+                    // We cannot evaluate, the property is not available\n+                    return None;\n+                }\n+            }\n+            // If all conditions are met, return the variant\n+            if self.evaluate_percentage(mapped_user_id, group.rollout_percentage / 100.0) {\n+                group.variant.clone()\n+            } else {\n+                None\n+            }\n+        } else {\n+            // No matchers, apply to all users\n+            if self.evaluate_percentage(mapped_user_id, group.rollout_percentage / 100.0) {\n+                group.variant.clone()\n+            } else {\n+                None\n+            }\n+        }\n+    }\n+\n+    /// Evaluate a multivariate feature flag. Returns `None` if the flag is not available or if there are errors\n+    /// during the evaluation.\n+    ///\n+    /// The parsing logic is as follows:\n+    ///\n+    /// * Match each filter group. If any group is matched, check the rollout percentage against the precomputed\n+    ///   user ID on the consistent hash ring. If the user ID is in the range of the group's rollout percentage,\n+    ///   return the variant.\n+    /// * Otherwise, continue with the next group until all groups are evaluated and no group is within the\n+    ///   rollout percentage.\n+    /// * If there are no matching groups, evaluate by the global rollout percentage.\n+    pub fn evaluate_multivariate(&self, flag_key: &str, user_id: &str) -> Option<String> {\n+        let mapped_user_id = Self::consistent_hash(user_id);\n+        self.evaluate_multivariate_inner(flag_key, mapped_user_id, &HashMap::new())\n+    }\n+\n+    /// Evaluate a multivariate feature flag. Returns `None` if the flag is not available or if there are errors\n+    /// during the evaluation. Note that we directly take the mapped user ID (a consistent hash ranging from 0 to 1)\n+    /// so that it is easier to use it in the tests and avoid duplicate computations.\n+    pub fn evaluate_multivariate_inner(\n+        &self,\n+        flag_key: &str,\n+        mapped_user_id: f64,\n+        properties: &HashMap<String, PostHogFlagFilterPropertyValue>,\n+    ) -> Option<String> {\n+        if let Some(flag_config) = self.flags.get(flag_key) {\n+            // First, try to evaluate each group\n+            for group in &flag_config.filters.groups {\n+                if let Some(variant) = self.evaluate_group(group, mapped_user_id, properties) {\n+                    return Some(variant);\n+                }\n+            }\n+            // If no group is matched, evaluate by the global rollout percentage\n+            let mut percentage = 0.0;\n+            for variant in &flag_config.filters.multivariate.variants {\n+                percentage += variant.rollout_percentage;\n+                if self.evaluate_percentage(mapped_user_id, percentage / 100.0) {\n+                    return Some(variant.key.clone());\n+                }\n+            }\n+            // This should not happen because the rollout percentage always adds up to 100, but just in case,",
        "comment_created_at": "2025-05-08T01:54:02+00:00",
        "comment_author": "skyzh",
        "comment_body": "PostHog always gives integer as percentage so actually we can also sum using i64 here. I will switch to i64 instead to ensure there're no precision issues.",
        "pr_file_module": null
      }
    ]
  }
]