[
  {
    "discussion_id": "2082145075",
    "pr_number": 10466,
    "pr_file": "test_runner/performance/pageserver/test_page_service_batching.py",
    "created_at": "2025-05-09T17:25:52+00:00",
    "commented_code": ")\n \n \n-PRECISION_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n+LATENCY_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n for max_batch_size in [1, 32]:\n     for execution in EXECUTION:\n         for batching in BATCHING:\n-            PRECISION_CONFIGS.append(\n+            LATENCY_CONFIGS.append(\n                 PageServicePipeliningConfigPipelined(max_batch_size, execution, batching)\n             )\n \n \n @pytest.mark.parametrize(\n-    \"pipelining_config,name\",\n-    [(config, f\"{dataclasses.asdict(config)}\") for config in PRECISION_CONFIGS],\n+    \"pipelining_config,ps_io_concurrency,l0_stack_height,queue_depth,name\",\n+    [\n+        (config, ps_io_concurrency, l0_stack_height, queue_depth, f\"{dataclasses.asdict(config)}\")\n+        for config in LATENCY_CONFIGS\n+        for ps_io_concurrency in PS_IO_CONCURRENCY\n+        for queue_depth in [1, 2, 3, 4, 16, 32]\n+        for l0_stack_height in [0, 3, 10]\n+    ],\n )\n-def test_latency(\n+def test_random_reads(",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2082145075",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 10466,
        "pr_file": "test_runner/performance/pageserver/test_page_service_batching.py",
        "discussion_id": "2082145075",
        "commented_code": "@@ -318,77 +338,74 @@ def disruptor(disruptor_started: threading.Event, stop_disruptor: threading.Even\n     )\n \n \n-PRECISION_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n+LATENCY_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n for max_batch_size in [1, 32]:\n     for execution in EXECUTION:\n         for batching in BATCHING:\n-            PRECISION_CONFIGS.append(\n+            LATENCY_CONFIGS.append(\n                 PageServicePipeliningConfigPipelined(max_batch_size, execution, batching)\n             )\n \n \n @pytest.mark.parametrize(\n-    \"pipelining_config,name\",\n-    [(config, f\"{dataclasses.asdict(config)}\") for config in PRECISION_CONFIGS],\n+    \"pipelining_config,ps_io_concurrency,l0_stack_height,queue_depth,name\",\n+    [\n+        (config, ps_io_concurrency, l0_stack_height, queue_depth, f\"{dataclasses.asdict(config)}\")\n+        for config in LATENCY_CONFIGS\n+        for ps_io_concurrency in PS_IO_CONCURRENCY\n+        for queue_depth in [1, 2, 3, 4, 16, 32]\n+        for l0_stack_height in [0, 3, 10]\n+    ],\n )\n-def test_latency(\n+def test_random_reads(",
        "comment_created_at": "2025-05-09T17:25:52+00:00",
        "comment_author": "problame",
        "comment_body": "The cardinality of this is 180\r\nTakes about 1h to run on my devbox.\r\n\r\nWhat can we trim from the parameter set?\r\nI spent some time browsing through the results, I recommend you do it, too to get an idea of what's in there.\r\n\r\nHere's the results from my local run (SSD is a tad lower latency than the im4gn.2xlarge ones, so, impact of concurrent in prod should be (even) higher than what we see here).\r\n\r\n[test_random_reads.xlsx](https://github.com/user-attachments/files/20126306/test_random_reads.xlsx)\r\n\r\nNB on how efficient the test is:\r\n- the build_and_use_snapshot helps a lot\r\n- Storcon startup from snapshot takes ~5s. Why?!\r\n- Pagebench itself only runs for 10s",
        "pr_file_module": null
      },
      {
        "comment_id": "2086083589",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 10466,
        "pr_file": "test_runner/performance/pageserver/test_page_service_batching.py",
        "discussion_id": "2082145075",
        "commented_code": "@@ -318,77 +338,74 @@ def disruptor(disruptor_started: threading.Event, stop_disruptor: threading.Even\n     )\n \n \n-PRECISION_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n+LATENCY_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n for max_batch_size in [1, 32]:\n     for execution in EXECUTION:\n         for batching in BATCHING:\n-            PRECISION_CONFIGS.append(\n+            LATENCY_CONFIGS.append(\n                 PageServicePipeliningConfigPipelined(max_batch_size, execution, batching)\n             )\n \n \n @pytest.mark.parametrize(\n-    \"pipelining_config,name\",\n-    [(config, f\"{dataclasses.asdict(config)}\") for config in PRECISION_CONFIGS],\n+    \"pipelining_config,ps_io_concurrency,l0_stack_height,queue_depth,name\",\n+    [\n+        (config, ps_io_concurrency, l0_stack_height, queue_depth, f\"{dataclasses.asdict(config)}\")\n+        for config in LATENCY_CONFIGS\n+        for ps_io_concurrency in PS_IO_CONCURRENCY\n+        for queue_depth in [1, 2, 3, 4, 16, 32]\n+        for l0_stack_height in [0, 3, 10]\n+    ],\n )\n-def test_latency(\n+def test_random_reads(",
        "comment_created_at": "2025-05-13T07:09:26+00:00",
        "comment_author": "problame",
        "comment_body": "@Bodobolero  do you have opinions on this? Or should we just blow up the runtime of this bench to 1h? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2086130631",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 10466,
        "pr_file": "test_runner/performance/pageserver/test_page_service_batching.py",
        "discussion_id": "2082145075",
        "commented_code": "@@ -318,77 +338,74 @@ def disruptor(disruptor_started: threading.Event, stop_disruptor: threading.Even\n     )\n \n \n-PRECISION_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n+LATENCY_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n for max_batch_size in [1, 32]:\n     for execution in EXECUTION:\n         for batching in BATCHING:\n-            PRECISION_CONFIGS.append(\n+            LATENCY_CONFIGS.append(\n                 PageServicePipeliningConfigPipelined(max_batch_size, execution, batching)\n             )\n \n \n @pytest.mark.parametrize(\n-    \"pipelining_config,name\",\n-    [(config, f\"{dataclasses.asdict(config)}\") for config in PRECISION_CONFIGS],\n+    \"pipelining_config,ps_io_concurrency,l0_stack_height,queue_depth,name\",\n+    [\n+        (config, ps_io_concurrency, l0_stack_height, queue_depth, f\"{dataclasses.asdict(config)}\")\n+        for config in LATENCY_CONFIGS\n+        for ps_io_concurrency in PS_IO_CONCURRENCY\n+        for queue_depth in [1, 2, 3, 4, 16, 32]\n+        for l0_stack_height in [0, 3, 10]\n+    ],\n )\n-def test_latency(\n+def test_random_reads(",
        "comment_created_at": "2025-05-13T07:35:11+00:00",
        "comment_author": "Bodobolero",
        "comment_body": "> @Bodobolero do you have opinions on this? Or should we just blow up the runtime of this bench to 1h?\r\n\r\nThis single test is dominating the benchmarking runs, see example runs below.\r\nI don't know who will ever look at all these variations, to be honest.\r\nIt would be good if you can decide on a subset of combinations that you consider important (and enumerate them) instead of run the full 180 combinations.\r\n\r\n- https://github.com/neondatabase/neon/actions/runs/14969819577/job/42049963911#step:7:15537\r\n- https://github.com/neondatabase/neon/actions/runs/14969819577/job/42049963974#step:7:15497",
        "pr_file_module": null
      },
      {
        "comment_id": "2086369459",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 10466,
        "pr_file": "test_runner/performance/pageserver/test_page_service_batching.py",
        "discussion_id": "2082145075",
        "commented_code": "@@ -318,77 +338,74 @@ def disruptor(disruptor_started: threading.Event, stop_disruptor: threading.Even\n     )\n \n \n-PRECISION_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n+LATENCY_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n for max_batch_size in [1, 32]:\n     for execution in EXECUTION:\n         for batching in BATCHING:\n-            PRECISION_CONFIGS.append(\n+            LATENCY_CONFIGS.append(\n                 PageServicePipeliningConfigPipelined(max_batch_size, execution, batching)\n             )\n \n \n @pytest.mark.parametrize(\n-    \"pipelining_config,name\",\n-    [(config, f\"{dataclasses.asdict(config)}\") for config in PRECISION_CONFIGS],\n+    \"pipelining_config,ps_io_concurrency,l0_stack_height,queue_depth,name\",\n+    [\n+        (config, ps_io_concurrency, l0_stack_height, queue_depth, f\"{dataclasses.asdict(config)}\")\n+        for config in LATENCY_CONFIGS\n+        for ps_io_concurrency in PS_IO_CONCURRENCY\n+        for queue_depth in [1, 2, 3, 4, 16, 32]\n+        for l0_stack_height in [0, 3, 10]\n+    ],\n )\n-def test_latency(\n+def test_random_reads(",
        "comment_created_at": "2025-05-13T09:34:42+00:00",
        "comment_author": "problame",
        "comment_body": "Yeah, that's what I'm asking about. Are _you_ interested in any specific combination? \r\n\r\nI personally would only every run the production settings for each post-merge-to-`main`  commit.\r\n\r\nThe parametrization is very useful during development for making before-after comparisons / to show off speedup for a new feature.\r\nBut once it's developed and shipped, we don't really look back.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2086375673",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 10466,
        "pr_file": "test_runner/performance/pageserver/test_page_service_batching.py",
        "discussion_id": "2082145075",
        "commented_code": "@@ -318,77 +338,74 @@ def disruptor(disruptor_started: threading.Event, stop_disruptor: threading.Even\n     )\n \n \n-PRECISION_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n+LATENCY_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n for max_batch_size in [1, 32]:\n     for execution in EXECUTION:\n         for batching in BATCHING:\n-            PRECISION_CONFIGS.append(\n+            LATENCY_CONFIGS.append(\n                 PageServicePipeliningConfigPipelined(max_batch_size, execution, batching)\n             )\n \n \n @pytest.mark.parametrize(\n-    \"pipelining_config,name\",\n-    [(config, f\"{dataclasses.asdict(config)}\") for config in PRECISION_CONFIGS],\n+    \"pipelining_config,ps_io_concurrency,l0_stack_height,queue_depth,name\",\n+    [\n+        (config, ps_io_concurrency, l0_stack_height, queue_depth, f\"{dataclasses.asdict(config)}\")\n+        for config in LATENCY_CONFIGS\n+        for ps_io_concurrency in PS_IO_CONCURRENCY\n+        for queue_depth in [1, 2, 3, 4, 16, 32]\n+        for l0_stack_height in [0, 3, 10]\n+    ],\n )\n-def test_latency(\n+def test_random_reads(",
        "comment_created_at": "2025-05-13T09:38:03+00:00",
        "comment_author": "problame",
        "comment_body": "The other takeaway is that maybe we should switch / parametrize `test_runner/performance/pageserver/pagebench/test_pageserver_max_throughput_getpage_at_latest_lsn.py` to use the l0stack that is introduced in this PR, since the characteristics of that workload are better understood than the couple of `pgbench -N` iterations that we do there right now.\r\n\r\nThen again, pgbench workload is probably closer to reality than the l0 stack?\r\nSo it comes down to what we actually want to measure there.\r\n\r\nMaybe it's a good subject for the monthly perf sync meeting.",
        "pr_file_module": null
      },
      {
        "comment_id": "2086400823",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 10466,
        "pr_file": "test_runner/performance/pageserver/test_page_service_batching.py",
        "discussion_id": "2082145075",
        "commented_code": "@@ -318,77 +338,74 @@ def disruptor(disruptor_started: threading.Event, stop_disruptor: threading.Even\n     )\n \n \n-PRECISION_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n+LATENCY_CONFIGS: list[PageServicePipeliningConfig] = [PageServicePipeliningConfigSerial()]\n for max_batch_size in [1, 32]:\n     for execution in EXECUTION:\n         for batching in BATCHING:\n-            PRECISION_CONFIGS.append(\n+            LATENCY_CONFIGS.append(\n                 PageServicePipeliningConfigPipelined(max_batch_size, execution, batching)\n             )\n \n \n @pytest.mark.parametrize(\n-    \"pipelining_config,name\",\n-    [(config, f\"{dataclasses.asdict(config)}\") for config in PRECISION_CONFIGS],\n+    \"pipelining_config,ps_io_concurrency,l0_stack_height,queue_depth,name\",\n+    [\n+        (config, ps_io_concurrency, l0_stack_height, queue_depth, f\"{dataclasses.asdict(config)}\")\n+        for config in LATENCY_CONFIGS\n+        for ps_io_concurrency in PS_IO_CONCURRENCY\n+        for queue_depth in [1, 2, 3, 4, 16, 32]\n+        for l0_stack_height in [0, 3, 10]\n+    ],\n )\n-def test_latency(\n+def test_random_reads(",
        "comment_created_at": "2025-05-13T09:50:39+00:00",
        "comment_author": "Bodobolero",
        "comment_body": "\r\n> I personally would only every run the production settings for each post-merge-to-`main` commit.\r\n\r\nfine with me\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2071156499",
    "pr_number": 11083,
    "pr_file": "test_runner/performance/test_lfc_prefetch.py",
    "created_at": "2025-05-02T06:25:47+00:00",
    "commented_code": "+from __future__ import annotations\n+\n+import random\n+import threading\n+import time\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+from fixtures.log_helper import log\n+\n+if TYPE_CHECKING:\n+    from fixtures.neon_fixtures import NeonEnv\n+from fixtures.utils import USE_LFC\n+\n+\n+@pytest.mark.remote_cluster\n+@pytest.mark.timeout(100000)\n+@pytest.mark.parametrize(\"n_readers\", [1, 2, 4, 8])\n+@pytest.mark.parametrize(\"n_writers\", [0, 1, 2, 4, 8])\n+@pytest.mark.parametrize(\"chunk_size\", [1, 8, 16])",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2071156499",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11083,
        "pr_file": "test_runner/performance/test_lfc_prefetch.py",
        "discussion_id": "2071156499",
        "commented_code": "@@ -0,0 +1,147 @@\n+from __future__ import annotations\n+\n+import random\n+import threading\n+import time\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+from fixtures.log_helper import log\n+\n+if TYPE_CHECKING:\n+    from fixtures.neon_fixtures import NeonEnv\n+from fixtures.utils import USE_LFC\n+\n+\n+@pytest.mark.remote_cluster\n+@pytest.mark.timeout(100000)\n+@pytest.mark.parametrize(\"n_readers\", [1, 2, 4, 8])\n+@pytest.mark.parametrize(\"n_writers\", [0, 1, 2, 4, 8])\n+@pytest.mark.parametrize(\"chunk_size\", [1, 8, 16])",
        "comment_created_at": "2025-05-02T06:25:47+00:00",
        "comment_author": "alexanderlaw",
        "comment_body": "With the chunk_size parameter added, we have 60 combinations and each one runs for 100 seconds, so the test would take 6000 seconds of CPU time. Isn't it too expensive for such a simple (exercising just a couple of operations) test?\r\n\r\nWouldn't it be better to choose the parameters randomly and just print chosen values in the log to ease reproduction?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2073669916",
    "pr_number": 11083,
    "pr_file": "test_runner/performance/test_lfc_prefetch.py",
    "created_at": "2025-05-05T15:28:39+00:00",
    "commented_code": "+from __future__ import annotations\n+\n+import random\n+import threading\n+import time\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+from fixtures.log_helper import log\n+\n+if TYPE_CHECKING:\n+    from fixtures.neon_fixtures import NeonEnv\n+from fixtures.utils import USE_LFC\n+\n+\n+@pytest.mark.remote_cluster\n+@pytest.mark.timeout(100000)\n+@pytest.mark.parametrize(\"n_readers\", [1, 2, 4, 8])\n+@pytest.mark.parametrize(\"n_writers\", [0, 1, 2, 4, 8])\n+@pytest.mark.parametrize(\"chunk_size\", [1, 8, 16])\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_prefetch(neon_simple_env: NeonEnv, n_readers: int, n_writers: int, chunk_size: int):\n+    \"\"\"\n+    Test prefetch under different kinds of workload\n+    \"\"\"\n+    env = neon_simple_env\n+    endpoint = env.endpoints.create_start(\n+        \"main\",\n+        config_lines=[\n+            \"neon.max_file_cache_size=100MB\",\n+            \"neon.file_cache_size_limit=100MB\",\n+            \"effective_io_concurrency=100\",\n+            \"shared_buffers=128MB\",\n+            \"enable_bitmapscan=off\",\n+            \"enable_seqscan=off\",\n+            f\"neon.file_cache_chunk_size={chunk_size}\",\n+            \"neon.store_prefetch_result_in_lfc=on\",\n+        ],\n+    )\n+    n_records = 100000  # 800Mb table\n+    top_n = n_records // 4  # 200Mb - should be larger than LFC size\n+    test_time = 100.0  # seconds\n+\n+    conn = endpoint.connect()\n+    cur = conn.cursor()\n+    cur.execute(\n+        \"create table account(id integer primary key, balance integer default 0, filler text default repeat('?',1000)) with (fillfactor=10)\"\n+    )\n+    cur.execute(f\"insert into account values (generate_series(1,{n_records}))\")\n+    cur.execute(\"vacuum account\")\n+\n+    def reader():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            cur.execute(\n+                f\"select sum(balance) from (select balance from account order by id limit {top_n}) s\"\n+            )\n+            sum = cur.fetchall()[0][0]\n+            assert sum == 0  # check consistency\n+            i += 1\n+        log.info(f\"Did {i} index scans\")\n+\n+    def writer():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            r1 = random.randint(1, top_n)\n+            r2 = random.randint(1, top_n)\n+            # avoid deadlock by ordering src and dst\n+            src = min(r1, r2)\n+            dst = max(r1, r2)\n+            cur.execute(\n+                f\"update account set balance=balance-1 where id={src}; update account set balance=balance+1 where id={dst}\"\n+            )\n+            i += 1\n+        log.info(f\"Did {i} updates\")\n+\n+    readers = [threading.Thread(target=reader) for _ in range(n_readers)]\n+    writers = [threading.Thread(target=writer) for _ in range(n_writers)]\n+\n+    running = True\n+    for t in readers:\n+        t.start()\n+    for t in writers:\n+        t.start()\n+\n+    time.sleep(test_time)\n+    running = False\n+    for t in readers:\n+        t.join()\n+    for t in writers:\n+        t.join()\n+\n+\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_async_prefetch_performance(neon_simple_env: NeonEnv, zenbenchmark):",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2073669916",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11083,
        "pr_file": "test_runner/performance/test_lfc_prefetch.py",
        "discussion_id": "2073669916",
        "commented_code": "@@ -0,0 +1,147 @@\n+from __future__ import annotations\n+\n+import random\n+import threading\n+import time\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+from fixtures.log_helper import log\n+\n+if TYPE_CHECKING:\n+    from fixtures.neon_fixtures import NeonEnv\n+from fixtures.utils import USE_LFC\n+\n+\n+@pytest.mark.remote_cluster\n+@pytest.mark.timeout(100000)\n+@pytest.mark.parametrize(\"n_readers\", [1, 2, 4, 8])\n+@pytest.mark.parametrize(\"n_writers\", [0, 1, 2, 4, 8])\n+@pytest.mark.parametrize(\"chunk_size\", [1, 8, 16])\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_prefetch(neon_simple_env: NeonEnv, n_readers: int, n_writers: int, chunk_size: int):\n+    \"\"\"\n+    Test prefetch under different kinds of workload\n+    \"\"\"\n+    env = neon_simple_env\n+    endpoint = env.endpoints.create_start(\n+        \"main\",\n+        config_lines=[\n+            \"neon.max_file_cache_size=100MB\",\n+            \"neon.file_cache_size_limit=100MB\",\n+            \"effective_io_concurrency=100\",\n+            \"shared_buffers=128MB\",\n+            \"enable_bitmapscan=off\",\n+            \"enable_seqscan=off\",\n+            f\"neon.file_cache_chunk_size={chunk_size}\",\n+            \"neon.store_prefetch_result_in_lfc=on\",\n+        ],\n+    )\n+    n_records = 100000  # 800Mb table\n+    top_n = n_records // 4  # 200Mb - should be larger than LFC size\n+    test_time = 100.0  # seconds\n+\n+    conn = endpoint.connect()\n+    cur = conn.cursor()\n+    cur.execute(\n+        \"create table account(id integer primary key, balance integer default 0, filler text default repeat('?',1000)) with (fillfactor=10)\"\n+    )\n+    cur.execute(f\"insert into account values (generate_series(1,{n_records}))\")\n+    cur.execute(\"vacuum account\")\n+\n+    def reader():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            cur.execute(\n+                f\"select sum(balance) from (select balance from account order by id limit {top_n}) s\"\n+            )\n+            sum = cur.fetchall()[0][0]\n+            assert sum == 0  # check consistency\n+            i += 1\n+        log.info(f\"Did {i} index scans\")\n+\n+    def writer():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            r1 = random.randint(1, top_n)\n+            r2 = random.randint(1, top_n)\n+            # avoid deadlock by ordering src and dst\n+            src = min(r1, r2)\n+            dst = max(r1, r2)\n+            cur.execute(\n+                f\"update account set balance=balance-1 where id={src}; update account set balance=balance+1 where id={dst}\"\n+            )\n+            i += 1\n+        log.info(f\"Did {i} updates\")\n+\n+    readers = [threading.Thread(target=reader) for _ in range(n_readers)]\n+    writers = [threading.Thread(target=writer) for _ in range(n_writers)]\n+\n+    running = True\n+    for t in readers:\n+        t.start()\n+    for t in writers:\n+        t.start()\n+\n+    time.sleep(test_time)\n+    running = False\n+    for t in readers:\n+        t.join()\n+    for t in writers:\n+        t.join()\n+\n+\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_async_prefetch_performance(neon_simple_env: NeonEnv, zenbenchmark):",
        "comment_created_at": "2025-05-05T15:28:39+00:00",
        "comment_author": "Bodobolero",
        "comment_body": "should measure the total time for executing with and without storing prefetch pages in LFC and should then assert that the performance improvement is at least x times.\r\nSo this is perf test - but it can verify perf quickly (no long execution time needed) but it is missing the perf validation by comparing the two execution durations and asserting that storing prefetch results in LFC is x times faster.",
        "pr_file_module": null
      },
      {
        "comment_id": "2074893926",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11083,
        "pr_file": "test_runner/performance/test_lfc_prefetch.py",
        "discussion_id": "2073669916",
        "commented_code": "@@ -0,0 +1,147 @@\n+from __future__ import annotations\n+\n+import random\n+import threading\n+import time\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+from fixtures.log_helper import log\n+\n+if TYPE_CHECKING:\n+    from fixtures.neon_fixtures import NeonEnv\n+from fixtures.utils import USE_LFC\n+\n+\n+@pytest.mark.remote_cluster\n+@pytest.mark.timeout(100000)\n+@pytest.mark.parametrize(\"n_readers\", [1, 2, 4, 8])\n+@pytest.mark.parametrize(\"n_writers\", [0, 1, 2, 4, 8])\n+@pytest.mark.parametrize(\"chunk_size\", [1, 8, 16])\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_prefetch(neon_simple_env: NeonEnv, n_readers: int, n_writers: int, chunk_size: int):\n+    \"\"\"\n+    Test prefetch under different kinds of workload\n+    \"\"\"\n+    env = neon_simple_env\n+    endpoint = env.endpoints.create_start(\n+        \"main\",\n+        config_lines=[\n+            \"neon.max_file_cache_size=100MB\",\n+            \"neon.file_cache_size_limit=100MB\",\n+            \"effective_io_concurrency=100\",\n+            \"shared_buffers=128MB\",\n+            \"enable_bitmapscan=off\",\n+            \"enable_seqscan=off\",\n+            f\"neon.file_cache_chunk_size={chunk_size}\",\n+            \"neon.store_prefetch_result_in_lfc=on\",\n+        ],\n+    )\n+    n_records = 100000  # 800Mb table\n+    top_n = n_records // 4  # 200Mb - should be larger than LFC size\n+    test_time = 100.0  # seconds\n+\n+    conn = endpoint.connect()\n+    cur = conn.cursor()\n+    cur.execute(\n+        \"create table account(id integer primary key, balance integer default 0, filler text default repeat('?',1000)) with (fillfactor=10)\"\n+    )\n+    cur.execute(f\"insert into account values (generate_series(1,{n_records}))\")\n+    cur.execute(\"vacuum account\")\n+\n+    def reader():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            cur.execute(\n+                f\"select sum(balance) from (select balance from account order by id limit {top_n}) s\"\n+            )\n+            sum = cur.fetchall()[0][0]\n+            assert sum == 0  # check consistency\n+            i += 1\n+        log.info(f\"Did {i} index scans\")\n+\n+    def writer():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            r1 = random.randint(1, top_n)\n+            r2 = random.randint(1, top_n)\n+            # avoid deadlock by ordering src and dst\n+            src = min(r1, r2)\n+            dst = max(r1, r2)\n+            cur.execute(\n+                f\"update account set balance=balance-1 where id={src}; update account set balance=balance+1 where id={dst}\"\n+            )\n+            i += 1\n+        log.info(f\"Did {i} updates\")\n+\n+    readers = [threading.Thread(target=reader) for _ in range(n_readers)]\n+    writers = [threading.Thread(target=writer) for _ in range(n_writers)]\n+\n+    running = True\n+    for t in readers:\n+        t.start()\n+    for t in writers:\n+        t.start()\n+\n+    time.sleep(test_time)\n+    running = False\n+    for t in readers:\n+        t.join()\n+    for t in writers:\n+        t.join()\n+\n+\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_async_prefetch_performance(neon_simple_env: NeonEnv, zenbenchmark):",
        "comment_created_at": "2025-05-06T07:37:06+00:00",
        "comment_author": "knizhnik",
        "comment_body": "Linux is not real time OS. And we are running tests in cloud. \r\nSo, especially if query execution time is relatively small (as in this case < 1 second), it is not possible to expect that execution time with optimization always be smaller than without it. Adding such assert is just a direct road to yet another fluky test. Certainly we can repeat query several times and calculate average time.  But IMHO it is overkill.\r\nBut may be such flukyness for performance tests is not so critical as for regression tests...\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2074960788",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11083,
        "pr_file": "test_runner/performance/test_lfc_prefetch.py",
        "discussion_id": "2073669916",
        "commented_code": "@@ -0,0 +1,147 @@\n+from __future__ import annotations\n+\n+import random\n+import threading\n+import time\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+from fixtures.log_helper import log\n+\n+if TYPE_CHECKING:\n+    from fixtures.neon_fixtures import NeonEnv\n+from fixtures.utils import USE_LFC\n+\n+\n+@pytest.mark.remote_cluster\n+@pytest.mark.timeout(100000)\n+@pytest.mark.parametrize(\"n_readers\", [1, 2, 4, 8])\n+@pytest.mark.parametrize(\"n_writers\", [0, 1, 2, 4, 8])\n+@pytest.mark.parametrize(\"chunk_size\", [1, 8, 16])\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_prefetch(neon_simple_env: NeonEnv, n_readers: int, n_writers: int, chunk_size: int):\n+    \"\"\"\n+    Test prefetch under different kinds of workload\n+    \"\"\"\n+    env = neon_simple_env\n+    endpoint = env.endpoints.create_start(\n+        \"main\",\n+        config_lines=[\n+            \"neon.max_file_cache_size=100MB\",\n+            \"neon.file_cache_size_limit=100MB\",\n+            \"effective_io_concurrency=100\",\n+            \"shared_buffers=128MB\",\n+            \"enable_bitmapscan=off\",\n+            \"enable_seqscan=off\",\n+            f\"neon.file_cache_chunk_size={chunk_size}\",\n+            \"neon.store_prefetch_result_in_lfc=on\",\n+        ],\n+    )\n+    n_records = 100000  # 800Mb table\n+    top_n = n_records // 4  # 200Mb - should be larger than LFC size\n+    test_time = 100.0  # seconds\n+\n+    conn = endpoint.connect()\n+    cur = conn.cursor()\n+    cur.execute(\n+        \"create table account(id integer primary key, balance integer default 0, filler text default repeat('?',1000)) with (fillfactor=10)\"\n+    )\n+    cur.execute(f\"insert into account values (generate_series(1,{n_records}))\")\n+    cur.execute(\"vacuum account\")\n+\n+    def reader():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            cur.execute(\n+                f\"select sum(balance) from (select balance from account order by id limit {top_n}) s\"\n+            )\n+            sum = cur.fetchall()[0][0]\n+            assert sum == 0  # check consistency\n+            i += 1\n+        log.info(f\"Did {i} index scans\")\n+\n+    def writer():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            r1 = random.randint(1, top_n)\n+            r2 = random.randint(1, top_n)\n+            # avoid deadlock by ordering src and dst\n+            src = min(r1, r2)\n+            dst = max(r1, r2)\n+            cur.execute(\n+                f\"update account set balance=balance-1 where id={src}; update account set balance=balance+1 where id={dst}\"\n+            )\n+            i += 1\n+        log.info(f\"Did {i} updates\")\n+\n+    readers = [threading.Thread(target=reader) for _ in range(n_readers)]\n+    writers = [threading.Thread(target=writer) for _ in range(n_writers)]\n+\n+    running = True\n+    for t in readers:\n+        t.start()\n+    for t in writers:\n+        t.start()\n+\n+    time.sleep(test_time)\n+    running = False\n+    for t in readers:\n+        t.join()\n+    for t in writers:\n+        t.join()\n+\n+\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_async_prefetch_performance(neon_simple_env: NeonEnv, zenbenchmark):",
        "comment_created_at": "2025-05-06T08:16:39+00:00",
        "comment_author": "Bodobolero",
        "comment_body": "We now have dedicated runner for perf tests with fixed CPU speed, so storage team already has some tests where they verify certain perf with assertions. If you average out the runtime over multiple query executions I think it should be possible to compare with and without LFC activation during prefetch - let's give it a try - if it is flaky we can remove it again.",
        "pr_file_module": null
      },
      {
        "comment_id": "2080984224",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11083,
        "pr_file": "test_runner/performance/test_lfc_prefetch.py",
        "discussion_id": "2073669916",
        "commented_code": "@@ -0,0 +1,147 @@\n+from __future__ import annotations\n+\n+import random\n+import threading\n+import time\n+from typing import TYPE_CHECKING\n+\n+import pytest\n+from fixtures.log_helper import log\n+\n+if TYPE_CHECKING:\n+    from fixtures.neon_fixtures import NeonEnv\n+from fixtures.utils import USE_LFC\n+\n+\n+@pytest.mark.remote_cluster\n+@pytest.mark.timeout(100000)\n+@pytest.mark.parametrize(\"n_readers\", [1, 2, 4, 8])\n+@pytest.mark.parametrize(\"n_writers\", [0, 1, 2, 4, 8])\n+@pytest.mark.parametrize(\"chunk_size\", [1, 8, 16])\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_prefetch(neon_simple_env: NeonEnv, n_readers: int, n_writers: int, chunk_size: int):\n+    \"\"\"\n+    Test prefetch under different kinds of workload\n+    \"\"\"\n+    env = neon_simple_env\n+    endpoint = env.endpoints.create_start(\n+        \"main\",\n+        config_lines=[\n+            \"neon.max_file_cache_size=100MB\",\n+            \"neon.file_cache_size_limit=100MB\",\n+            \"effective_io_concurrency=100\",\n+            \"shared_buffers=128MB\",\n+            \"enable_bitmapscan=off\",\n+            \"enable_seqscan=off\",\n+            f\"neon.file_cache_chunk_size={chunk_size}\",\n+            \"neon.store_prefetch_result_in_lfc=on\",\n+        ],\n+    )\n+    n_records = 100000  # 800Mb table\n+    top_n = n_records // 4  # 200Mb - should be larger than LFC size\n+    test_time = 100.0  # seconds\n+\n+    conn = endpoint.connect()\n+    cur = conn.cursor()\n+    cur.execute(\n+        \"create table account(id integer primary key, balance integer default 0, filler text default repeat('?',1000)) with (fillfactor=10)\"\n+    )\n+    cur.execute(f\"insert into account values (generate_series(1,{n_records}))\")\n+    cur.execute(\"vacuum account\")\n+\n+    def reader():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            cur.execute(\n+                f\"select sum(balance) from (select balance from account order by id limit {top_n}) s\"\n+            )\n+            sum = cur.fetchall()[0][0]\n+            assert sum == 0  # check consistency\n+            i += 1\n+        log.info(f\"Did {i} index scans\")\n+\n+    def writer():\n+        conn = endpoint.connect()\n+        cur = conn.cursor()\n+        i = 0\n+        cur.execute(\"set statement_timeout=0\")\n+        while running:\n+            r1 = random.randint(1, top_n)\n+            r2 = random.randint(1, top_n)\n+            # avoid deadlock by ordering src and dst\n+            src = min(r1, r2)\n+            dst = max(r1, r2)\n+            cur.execute(\n+                f\"update account set balance=balance-1 where id={src}; update account set balance=balance+1 where id={dst}\"\n+            )\n+            i += 1\n+        log.info(f\"Did {i} updates\")\n+\n+    readers = [threading.Thread(target=reader) for _ in range(n_readers)]\n+    writers = [threading.Thread(target=writer) for _ in range(n_writers)]\n+\n+    running = True\n+    for t in readers:\n+        t.start()\n+    for t in writers:\n+        t.start()\n+\n+    time.sleep(test_time)\n+    running = False\n+    for t in readers:\n+        t.join()\n+    for t in writers:\n+        t.join()\n+\n+\n+@pytest.mark.skipif(not USE_LFC, reason=\"LFC is disabled, skipping\")\n+def test_lfc_async_prefetch_performance(neon_simple_env: NeonEnv, zenbenchmark):",
        "comment_created_at": "2025-05-09T05:55:25+00:00",
        "comment_author": "knizhnik",
        "comment_body": "Ok, done",
        "pr_file_module": null
      }
    ]
  }
]