[
  {
    "discussion_id": "1696325867",
    "pr_number": 8425,
    "pr_file": "docs/rfcs/036-bottom-most-gc-compaction.md",
    "created_at": "2024-07-30T05:23:39+00:00",
    "commented_code": "+# Bottommost Garbage-Collection Compaction\n+\n+## Summary\n+\n+The goal of this doc is to propose a way to reliably collect garbages below the GC horizon. This process is called bottom-most garbage-collect-compaction, and is part of the broader legacy-enhanced compaction that we plan to implement in the future.\n+\n+## Motivation\n+\n+The current GC algorithm will wait until the covering image layers being created before collecting the garbages of a key region. Relying on image layer generation to generate covering images is not reliable. There are prior arts to generate feedbacks from the GC algorithm to the image generation process to accelerate garbage collection, but it slows down the system and creates write amplification.\n+\n+# Basic Idea\n+\n+![](images/036-bottom-most-gc-compaction/01-basic-idea.svg)\n+\n+The idea of bottom-most compaction is simple: we rewrite all layers that is below or intersect with the GC horizon to produce a flat level of image layers at the GC horizon and deltas above the GC horizon. In this process,\n+\n+- All images and deltas ≤ GC horizon LSN will be dropped. This process collects garbages.\n+- We produce images for all keys involved in the compaction process at the GC horizon.\n+\n+Therefore, it can precisely collect all garbages below the horizon, and reduce the space amplification, i.e., in the staircase pattern (test_gc_feedback).\n+\n+![The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.](images/036-bottom-most-gc-compaction/12-staircase-test-gc-feedback.png)\n+\n+The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.\n+\n+# Branches\n+\n+With branches, the bottom-most compaction should retain a snapshot of the keyspace at the `retain_lsn` so that the child branch can access data at the branch point. This requires some modifications to the basic bottom-most compaction algorithm that we sketched above. \n+\n+![](images/036-bottom-most-gc-compaction/03-retain-lsn.svg)\n+\n+## Single Timeline w/ Snapshots: handle `retain_lsn`\n+\n+First let’s look into the case where we create branches over the main branch but don’t write any data to them (aka “snapshots”).\n+\n+The bottom-most compaction algorithm collects all deltas and images of a key and can make decisions on what data to retain. Given that we have a single key’s history as below:\n+\n+```\n+LSN 0x10 -> A\n+LSN 0x20 -> append B\n+retain_lsn: 0x20\n+LSN 0x30 -> append C\n+LSN 0x40 -> append D\n+retain_lsn: 0x40\n+LSN 0x50 -> append E\n+GC horizon: 0x50\n+LSN 0x60 -> append F\n+```\n+\n+The algorithm will produce:\n+\n+```\n+LSN 0x20 -> AB\n+(drop all history below the earliest retain_lsn)\n+LSN 0x40 -> ABCD\n+(assume the cost of replaying 2 deltas is higher than storing the full image, we generate an image here)\n+LSN 0x50 -> append E\n+(replay one delta is cheap)\n+LSN 0x60 -> append F\n+(keep everything as-is above the GC horizon)\n+```\n+\n+![](images/036-bottom-most-gc-compaction/05-btmgc-parent.svg)\n+\n+What happens is that we balance the space taken by each retain_lsn and the cost of replaying deltas during the bottom-most compaction process. This is controlled by a threshold. If `sum(deltas) < $threshold`, the deltas will be retained. Otherwise, an image will be generated and the deltas will be dropped.\n+\n+In the example above, the `$threshold` is 2.\n+\n+## Child Branches with data: pull + partial images\n+\n+In the previous section we have shown how bottom-most compaction respects `retain_lsn` so that all data that was readable at branch creation remains readable. But branches can have data on their own, and that data can fall out of the branch’s PITR window. So, this section explains how we deal with that.\n+\n+We will run the same bottom-most compaction for these branches, to ensure the space amplification on the child branch is reasonable. \n+\n+```\n+branch_lsn: 0x20\n+LSN 0x30 -> append P\n+LSN 0x40 -> append Q\n+LSN 0x50 -> append R\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+Note that bottom-most compaction happens on a per-timeline basis. When it processes this key, it only reads the history from LSN 0x30 without a base image. Therefore, on child branches, the bottom-most compaction process will make image creation decisions based on the same `sum(deltas) < $threshold` criteria, and if it decides to create an image, the base image will be retrieved from the ancestor branch.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "1696325867",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 8425,
        "pr_file": "docs/rfcs/036-bottom-most-gc-compaction.md",
        "discussion_id": "1696325867",
        "commented_code": "@@ -0,0 +1,182 @@\n+# Bottommost Garbage-Collection Compaction\n+\n+## Summary\n+\n+The goal of this doc is to propose a way to reliably collect garbages below the GC horizon. This process is called bottom-most garbage-collect-compaction, and is part of the broader legacy-enhanced compaction that we plan to implement in the future.\n+\n+## Motivation\n+\n+The current GC algorithm will wait until the covering image layers being created before collecting the garbages of a key region. Relying on image layer generation to generate covering images is not reliable. There are prior arts to generate feedbacks from the GC algorithm to the image generation process to accelerate garbage collection, but it slows down the system and creates write amplification.\n+\n+# Basic Idea\n+\n+![](images/036-bottom-most-gc-compaction/01-basic-idea.svg)\n+\n+The idea of bottom-most compaction is simple: we rewrite all layers that is below or intersect with the GC horizon to produce a flat level of image layers at the GC horizon and deltas above the GC horizon. In this process,\n+\n+- All images and deltas ≤ GC horizon LSN will be dropped. This process collects garbages.\n+- We produce images for all keys involved in the compaction process at the GC horizon.\n+\n+Therefore, it can precisely collect all garbages below the horizon, and reduce the space amplification, i.e., in the staircase pattern (test_gc_feedback).\n+\n+![The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.](images/036-bottom-most-gc-compaction/12-staircase-test-gc-feedback.png)\n+\n+The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.\n+\n+# Branches\n+\n+With branches, the bottom-most compaction should retain a snapshot of the keyspace at the `retain_lsn` so that the child branch can access data at the branch point. This requires some modifications to the basic bottom-most compaction algorithm that we sketched above. \n+\n+![](images/036-bottom-most-gc-compaction/03-retain-lsn.svg)\n+\n+## Single Timeline w/ Snapshots: handle `retain_lsn`\n+\n+First let’s look into the case where we create branches over the main branch but don’t write any data to them (aka “snapshots”).\n+\n+The bottom-most compaction algorithm collects all deltas and images of a key and can make decisions on what data to retain. Given that we have a single key’s history as below:\n+\n+```\n+LSN 0x10 -> A\n+LSN 0x20 -> append B\n+retain_lsn: 0x20\n+LSN 0x30 -> append C\n+LSN 0x40 -> append D\n+retain_lsn: 0x40\n+LSN 0x50 -> append E\n+GC horizon: 0x50\n+LSN 0x60 -> append F\n+```\n+\n+The algorithm will produce:\n+\n+```\n+LSN 0x20 -> AB\n+(drop all history below the earliest retain_lsn)\n+LSN 0x40 -> ABCD\n+(assume the cost of replaying 2 deltas is higher than storing the full image, we generate an image here)\n+LSN 0x50 -> append E\n+(replay one delta is cheap)\n+LSN 0x60 -> append F\n+(keep everything as-is above the GC horizon)\n+```\n+\n+![](images/036-bottom-most-gc-compaction/05-btmgc-parent.svg)\n+\n+What happens is that we balance the space taken by each retain_lsn and the cost of replaying deltas during the bottom-most compaction process. This is controlled by a threshold. If `sum(deltas) < $threshold`, the deltas will be retained. Otherwise, an image will be generated and the deltas will be dropped.\n+\n+In the example above, the `$threshold` is 2.\n+\n+## Child Branches with data: pull + partial images\n+\n+In the previous section we have shown how bottom-most compaction respects `retain_lsn` so that all data that was readable at branch creation remains readable. But branches can have data on their own, and that data can fall out of the branch’s PITR window. So, this section explains how we deal with that.\n+\n+We will run the same bottom-most compaction for these branches, to ensure the space amplification on the child branch is reasonable. \n+\n+```\n+branch_lsn: 0x20\n+LSN 0x30 -> append P\n+LSN 0x40 -> append Q\n+LSN 0x50 -> append R\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+Note that bottom-most compaction happens on a per-timeline basis. When it processes this key, it only reads the history from LSN 0x30 without a base image. Therefore, on child branches, the bottom-most compaction process will make image creation decisions based on the same `sum(deltas) < $threshold` criteria, and if it decides to create an image, the base image will be retrieved from the ancestor branch.",
        "comment_created_at": "2024-07-30T05:23:39+00:00",
        "comment_author": "arpad-m",
        "comment_body": "```suggestion\r\nNote that bottom-most compaction happens on a per-timeline basis. When it processes this key, it only reads the history from LSN 0x30 without a base image. Therefore, on child branches, the bottom-most compaction process will make image creation decisions based on the same `count(deltas) < $threshold` criteria, and if it decides to create an image, the base image will be retrieved from the ancestor branch.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1696328821",
    "pr_number": 8425,
    "pr_file": "docs/rfcs/036-bottom-most-gc-compaction.md",
    "created_at": "2024-07-30T05:27:48+00:00",
    "commented_code": "+# Bottommost Garbage-Collection Compaction\n+\n+## Summary\n+\n+The goal of this doc is to propose a way to reliably collect garbages below the GC horizon. This process is called bottom-most garbage-collect-compaction, and is part of the broader legacy-enhanced compaction that we plan to implement in the future.\n+\n+## Motivation\n+\n+The current GC algorithm will wait until the covering image layers being created before collecting the garbages of a key region. Relying on image layer generation to generate covering images is not reliable. There are prior arts to generate feedbacks from the GC algorithm to the image generation process to accelerate garbage collection, but it slows down the system and creates write amplification.\n+\n+# Basic Idea\n+\n+![](images/036-bottom-most-gc-compaction/01-basic-idea.svg)\n+\n+The idea of bottom-most compaction is simple: we rewrite all layers that is below or intersect with the GC horizon to produce a flat level of image layers at the GC horizon and deltas above the GC horizon. In this process,\n+\n+- All images and deltas ≤ GC horizon LSN will be dropped. This process collects garbages.\n+- We produce images for all keys involved in the compaction process at the GC horizon.\n+\n+Therefore, it can precisely collect all garbages below the horizon, and reduce the space amplification, i.e., in the staircase pattern (test_gc_feedback).\n+\n+![The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.](images/036-bottom-most-gc-compaction/12-staircase-test-gc-feedback.png)\n+\n+The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.\n+\n+# Branches\n+\n+With branches, the bottom-most compaction should retain a snapshot of the keyspace at the `retain_lsn` so that the child branch can access data at the branch point. This requires some modifications to the basic bottom-most compaction algorithm that we sketched above. \n+\n+![](images/036-bottom-most-gc-compaction/03-retain-lsn.svg)\n+\n+## Single Timeline w/ Snapshots: handle `retain_lsn`\n+\n+First let’s look into the case where we create branches over the main branch but don’t write any data to them (aka “snapshots”).\n+\n+The bottom-most compaction algorithm collects all deltas and images of a key and can make decisions on what data to retain. Given that we have a single key’s history as below:\n+\n+```\n+LSN 0x10 -> A\n+LSN 0x20 -> append B\n+retain_lsn: 0x20\n+LSN 0x30 -> append C\n+LSN 0x40 -> append D\n+retain_lsn: 0x40\n+LSN 0x50 -> append E\n+GC horizon: 0x50\n+LSN 0x60 -> append F\n+```\n+\n+The algorithm will produce:\n+\n+```\n+LSN 0x20 -> AB\n+(drop all history below the earliest retain_lsn)\n+LSN 0x40 -> ABCD\n+(assume the cost of replaying 2 deltas is higher than storing the full image, we generate an image here)\n+LSN 0x50 -> append E\n+(replay one delta is cheap)\n+LSN 0x60 -> append F\n+(keep everything as-is above the GC horizon)\n+```\n+\n+![](images/036-bottom-most-gc-compaction/05-btmgc-parent.svg)\n+\n+What happens is that we balance the space taken by each retain_lsn and the cost of replaying deltas during the bottom-most compaction process. This is controlled by a threshold. If `sum(deltas) < $threshold`, the deltas will be retained. Otherwise, an image will be generated and the deltas will be dropped.\n+\n+In the example above, the `$threshold` is 2.\n+\n+## Child Branches with data: pull + partial images\n+\n+In the previous section we have shown how bottom-most compaction respects `retain_lsn` so that all data that was readable at branch creation remains readable. But branches can have data on their own, and that data can fall out of the branch’s PITR window. So, this section explains how we deal with that.\n+\n+We will run the same bottom-most compaction for these branches, to ensure the space amplification on the child branch is reasonable. \n+\n+```\n+branch_lsn: 0x20\n+LSN 0x30 -> append P\n+LSN 0x40 -> append Q\n+LSN 0x50 -> append R\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+Note that bottom-most compaction happens on a per-timeline basis. When it processes this key, it only reads the history from LSN 0x30 without a base image. Therefore, on child branches, the bottom-most compaction process will make image creation decisions based on the same `sum(deltas) < $threshold` criteria, and if it decides to create an image, the base image will be retrieved from the ancestor branch.\n+\n+```\n+branch_lsn: 0x20\n+LSN 0x50 -> ABPQR\n+(we pull the image at LSN 0x20 from the ancestor branch to get AB, and then apply append PQ to the page; we replace the record at 0x40 with an image and drop the delta)\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+![](images/036-bottom-most-gc-compaction/06-btmgc-child.svg)\n+\n+Note that for child branches, we do not create image layers for the images when bottom-most compaction runs. Instead, we drop the 0x30/0x40/0x50 delta records and directly place the image ABPQR@0x50 into the delta layer, which serves a partial image layer. For child branches, if we create image layers, we will need to put all keys in the range into the image layer. This causes space bloat and slow compactions. In this proposal, the compaction process will only compact and process keys modified inside the child branch.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "1696328821",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 8425,
        "pr_file": "docs/rfcs/036-bottom-most-gc-compaction.md",
        "discussion_id": "1696328821",
        "commented_code": "@@ -0,0 +1,182 @@\n+# Bottommost Garbage-Collection Compaction\n+\n+## Summary\n+\n+The goal of this doc is to propose a way to reliably collect garbages below the GC horizon. This process is called bottom-most garbage-collect-compaction, and is part of the broader legacy-enhanced compaction that we plan to implement in the future.\n+\n+## Motivation\n+\n+The current GC algorithm will wait until the covering image layers being created before collecting the garbages of a key region. Relying on image layer generation to generate covering images is not reliable. There are prior arts to generate feedbacks from the GC algorithm to the image generation process to accelerate garbage collection, but it slows down the system and creates write amplification.\n+\n+# Basic Idea\n+\n+![](images/036-bottom-most-gc-compaction/01-basic-idea.svg)\n+\n+The idea of bottom-most compaction is simple: we rewrite all layers that is below or intersect with the GC horizon to produce a flat level of image layers at the GC horizon and deltas above the GC horizon. In this process,\n+\n+- All images and deltas ≤ GC horizon LSN will be dropped. This process collects garbages.\n+- We produce images for all keys involved in the compaction process at the GC horizon.\n+\n+Therefore, it can precisely collect all garbages below the horizon, and reduce the space amplification, i.e., in the staircase pattern (test_gc_feedback).\n+\n+![The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.](images/036-bottom-most-gc-compaction/12-staircase-test-gc-feedback.png)\n+\n+The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.\n+\n+# Branches\n+\n+With branches, the bottom-most compaction should retain a snapshot of the keyspace at the `retain_lsn` so that the child branch can access data at the branch point. This requires some modifications to the basic bottom-most compaction algorithm that we sketched above. \n+\n+![](images/036-bottom-most-gc-compaction/03-retain-lsn.svg)\n+\n+## Single Timeline w/ Snapshots: handle `retain_lsn`\n+\n+First let’s look into the case where we create branches over the main branch but don’t write any data to them (aka “snapshots”).\n+\n+The bottom-most compaction algorithm collects all deltas and images of a key and can make decisions on what data to retain. Given that we have a single key’s history as below:\n+\n+```\n+LSN 0x10 -> A\n+LSN 0x20 -> append B\n+retain_lsn: 0x20\n+LSN 0x30 -> append C\n+LSN 0x40 -> append D\n+retain_lsn: 0x40\n+LSN 0x50 -> append E\n+GC horizon: 0x50\n+LSN 0x60 -> append F\n+```\n+\n+The algorithm will produce:\n+\n+```\n+LSN 0x20 -> AB\n+(drop all history below the earliest retain_lsn)\n+LSN 0x40 -> ABCD\n+(assume the cost of replaying 2 deltas is higher than storing the full image, we generate an image here)\n+LSN 0x50 -> append E\n+(replay one delta is cheap)\n+LSN 0x60 -> append F\n+(keep everything as-is above the GC horizon)\n+```\n+\n+![](images/036-bottom-most-gc-compaction/05-btmgc-parent.svg)\n+\n+What happens is that we balance the space taken by each retain_lsn and the cost of replaying deltas during the bottom-most compaction process. This is controlled by a threshold. If `sum(deltas) < $threshold`, the deltas will be retained. Otherwise, an image will be generated and the deltas will be dropped.\n+\n+In the example above, the `$threshold` is 2.\n+\n+## Child Branches with data: pull + partial images\n+\n+In the previous section we have shown how bottom-most compaction respects `retain_lsn` so that all data that was readable at branch creation remains readable. But branches can have data on their own, and that data can fall out of the branch’s PITR window. So, this section explains how we deal with that.\n+\n+We will run the same bottom-most compaction for these branches, to ensure the space amplification on the child branch is reasonable. \n+\n+```\n+branch_lsn: 0x20\n+LSN 0x30 -> append P\n+LSN 0x40 -> append Q\n+LSN 0x50 -> append R\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+Note that bottom-most compaction happens on a per-timeline basis. When it processes this key, it only reads the history from LSN 0x30 without a base image. Therefore, on child branches, the bottom-most compaction process will make image creation decisions based on the same `sum(deltas) < $threshold` criteria, and if it decides to create an image, the base image will be retrieved from the ancestor branch.\n+\n+```\n+branch_lsn: 0x20\n+LSN 0x50 -> ABPQR\n+(we pull the image at LSN 0x20 from the ancestor branch to get AB, and then apply append PQ to the page; we replace the record at 0x40 with an image and drop the delta)\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+![](images/036-bottom-most-gc-compaction/06-btmgc-child.svg)\n+\n+Note that for child branches, we do not create image layers for the images when bottom-most compaction runs. Instead, we drop the 0x30/0x40/0x50 delta records and directly place the image ABPQR@0x50 into the delta layer, which serves a partial image layer. For child branches, if we create image layers, we will need to put all keys in the range into the image layer. This causes space bloat and slow compactions. In this proposal, the compaction process will only compact and process keys modified inside the child branch.",
        "comment_created_at": "2024-07-30T05:27:48+00:00",
        "comment_author": "arpad-m",
        "comment_body": "```suggestion\r\nNote that for child branches, we do not create image layers for the images when bottom-most compaction runs. Instead, we drop the 0x30/0x40/0x50 delta records and directly place the image ABPQR@0x50 into the delta layer, which serves as a sparse image layer. For child branches, if we create image layers, we will need to put all keys in the range into the image layer. This causes space bloat and slow compactions. In this proposal, the compaction process will only compact and process keys modified inside the child branch.\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1696332725",
    "pr_number": 8425,
    "pr_file": "docs/rfcs/036-bottom-most-gc-compaction.md",
    "created_at": "2024-07-30T05:33:06+00:00",
    "commented_code": "+# Bottommost Garbage-Collection Compaction\n+\n+## Summary\n+\n+The goal of this doc is to propose a way to reliably collect garbages below the GC horizon. This process is called bottom-most garbage-collect-compaction, and is part of the broader legacy-enhanced compaction that we plan to implement in the future.\n+\n+## Motivation\n+\n+The current GC algorithm will wait until the covering image layers being created before collecting the garbages of a key region. Relying on image layer generation to generate covering images is not reliable. There are prior arts to generate feedbacks from the GC algorithm to the image generation process to accelerate garbage collection, but it slows down the system and creates write amplification.\n+\n+# Basic Idea\n+\n+![](images/036-bottom-most-gc-compaction/01-basic-idea.svg)\n+\n+The idea of bottom-most compaction is simple: we rewrite all layers that is below or intersect with the GC horizon to produce a flat level of image layers at the GC horizon and deltas above the GC horizon. In this process,\n+\n+- All images and deltas ≤ GC horizon LSN will be dropped. This process collects garbages.\n+- We produce images for all keys involved in the compaction process at the GC horizon.\n+\n+Therefore, it can precisely collect all garbages below the horizon, and reduce the space amplification, i.e., in the staircase pattern (test_gc_feedback).\n+\n+![The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.](images/036-bottom-most-gc-compaction/12-staircase-test-gc-feedback.png)\n+\n+The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.\n+\n+# Branches\n+\n+With branches, the bottom-most compaction should retain a snapshot of the keyspace at the `retain_lsn` so that the child branch can access data at the branch point. This requires some modifications to the basic bottom-most compaction algorithm that we sketched above. \n+\n+![](images/036-bottom-most-gc-compaction/03-retain-lsn.svg)\n+\n+## Single Timeline w/ Snapshots: handle `retain_lsn`\n+\n+First let’s look into the case where we create branches over the main branch but don’t write any data to them (aka “snapshots”).\n+\n+The bottom-most compaction algorithm collects all deltas and images of a key and can make decisions on what data to retain. Given that we have a single key’s history as below:\n+\n+```\n+LSN 0x10 -> A\n+LSN 0x20 -> append B\n+retain_lsn: 0x20\n+LSN 0x30 -> append C\n+LSN 0x40 -> append D\n+retain_lsn: 0x40\n+LSN 0x50 -> append E\n+GC horizon: 0x50\n+LSN 0x60 -> append F\n+```\n+\n+The algorithm will produce:\n+\n+```\n+LSN 0x20 -> AB\n+(drop all history below the earliest retain_lsn)\n+LSN 0x40 -> ABCD\n+(assume the cost of replaying 2 deltas is higher than storing the full image, we generate an image here)\n+LSN 0x50 -> append E\n+(replay one delta is cheap)\n+LSN 0x60 -> append F\n+(keep everything as-is above the GC horizon)\n+```\n+\n+![](images/036-bottom-most-gc-compaction/05-btmgc-parent.svg)\n+\n+What happens is that we balance the space taken by each retain_lsn and the cost of replaying deltas during the bottom-most compaction process. This is controlled by a threshold. If `sum(deltas) < $threshold`, the deltas will be retained. Otherwise, an image will be generated and the deltas will be dropped.\n+\n+In the example above, the `$threshold` is 2.\n+\n+## Child Branches with data: pull + partial images\n+\n+In the previous section we have shown how bottom-most compaction respects `retain_lsn` so that all data that was readable at branch creation remains readable. But branches can have data on their own, and that data can fall out of the branch’s PITR window. So, this section explains how we deal with that.\n+\n+We will run the same bottom-most compaction for these branches, to ensure the space amplification on the child branch is reasonable. \n+\n+```\n+branch_lsn: 0x20\n+LSN 0x30 -> append P\n+LSN 0x40 -> append Q\n+LSN 0x50 -> append R\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+Note that bottom-most compaction happens on a per-timeline basis. When it processes this key, it only reads the history from LSN 0x30 without a base image. Therefore, on child branches, the bottom-most compaction process will make image creation decisions based on the same `sum(deltas) < $threshold` criteria, and if it decides to create an image, the base image will be retrieved from the ancestor branch.\n+\n+```\n+branch_lsn: 0x20\n+LSN 0x50 -> ABPQR\n+(we pull the image at LSN 0x20 from the ancestor branch to get AB, and then apply append PQ to the page; we replace the record at 0x40 with an image and drop the delta)\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+![](images/036-bottom-most-gc-compaction/06-btmgc-child.svg)\n+\n+Note that for child branches, we do not create image layers for the images when bottom-most compaction runs. Instead, we drop the 0x30/0x40/0x50 delta records and directly place the image ABPQR@0x50 into the delta layer, which serves a partial image layer. For child branches, if we create image layers, we will need to put all keys in the range into the image layer. This causes space bloat and slow compactions. In this proposal, the compaction process will only compact and process keys modified inside the child branch.\n+\n+## Optimization: Layer Selection for Compaction\n+\n+In the basic bottom-most compaction, we select all layers that intersect with or are below the GC horizon. With retain_lsn and child branches taken into consideration, we do not need to run bottom-most compaction for all layers. Instead, only a subset of the layers need to be picked for compaction.\n+\n+![](images/036-bottom-most-gc-compaction/08-optimization.svg)\n+\n+Consider the case that the system finishes bottom-most compaction at GC horizon 0x60. Now GC horizon grows to 0x70 and the branch at 0x50 is deleted. We only need to pick the layers between 0x40 and 0x70 for compaction (the red layers). As other lower layers will not change after the compaction process, it does not need to be picked.\n+\n+# Result\n+\n+Bottom-most compaction ensures all garbages under the GC horizon gets collected right away (compared with “eventually” in the current algorithm). Meanwhile, it generates images at each of the retain_lsn to ensure branch reads are fast. As we make per-key decision on whether to generate an image or not, the theoretical lower bound of the storage space we need to retain a branch is lower than before.\n+\n+Before: min(sum(logs for each key), sum(image for each key)), for each partition — we always generate image layers on a key range\n+\n+After: sum(min(logs for each key, image for each key))\n+\n+# Dealing with Large Input\n+\n+Bottom-most compaction does a full compaction below the GC horizon, and the process is currently designed as non-resumable. The process should take a fixed amount of memory and be able to run for a few minutes.\n+\n+We made two design choices for the compaction algorithm:\n+\n+- Use k-merge to collect data required for the compaction. The memory consumption is `<buffered key-values per file> x <num of layers involved in compaction>`.\n+    - Key history of 1 million → OOM?\n+- Parallel compaction to make full use of the CPU resources.\n+\n+The image creation is integrated into the k-merge, i.e., reconstruct data naturally becomes available as part of k-merge instead of calling into get_values_reconstruct_data. This is significantly more CPU efficient than the image layer creation of legacy compaction.\n+\n+# Compaction Trigger\n+\n+The bottom-most compaction should be automatically triggered. The goal of the trigger is that it should ensure a constant factor for write amplification. Say that the user write 1GB of WAL into the system, we should write 1GB x C data to S3. The legacy compaction algorithm does not has such a constant factor C. The data we write to S3 is quadratic to the logical size of the database (see [A Theoretical View of Neon Storage](https://www.notion.so/A-Theoretical-View-of-Neon-Storage-8d7ad7555b0c41b2a3597fa780911194?pvs=21)).",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "1696332725",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 8425,
        "pr_file": "docs/rfcs/036-bottom-most-gc-compaction.md",
        "discussion_id": "1696332725",
        "commented_code": "@@ -0,0 +1,182 @@\n+# Bottommost Garbage-Collection Compaction\n+\n+## Summary\n+\n+The goal of this doc is to propose a way to reliably collect garbages below the GC horizon. This process is called bottom-most garbage-collect-compaction, and is part of the broader legacy-enhanced compaction that we plan to implement in the future.\n+\n+## Motivation\n+\n+The current GC algorithm will wait until the covering image layers being created before collecting the garbages of a key region. Relying on image layer generation to generate covering images is not reliable. There are prior arts to generate feedbacks from the GC algorithm to the image generation process to accelerate garbage collection, but it slows down the system and creates write amplification.\n+\n+# Basic Idea\n+\n+![](images/036-bottom-most-gc-compaction/01-basic-idea.svg)\n+\n+The idea of bottom-most compaction is simple: we rewrite all layers that is below or intersect with the GC horizon to produce a flat level of image layers at the GC horizon and deltas above the GC horizon. In this process,\n+\n+- All images and deltas ≤ GC horizon LSN will be dropped. This process collects garbages.\n+- We produce images for all keys involved in the compaction process at the GC horizon.\n+\n+Therefore, it can precisely collect all garbages below the horizon, and reduce the space amplification, i.e., in the staircase pattern (test_gc_feedback).\n+\n+![The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.](images/036-bottom-most-gc-compaction/12-staircase-test-gc-feedback.png)\n+\n+The staircase pattern in test_gc_feedback in the original compaction algorithm. The goal is to collect garbage below the red horizontal line.\n+\n+# Branches\n+\n+With branches, the bottom-most compaction should retain a snapshot of the keyspace at the `retain_lsn` so that the child branch can access data at the branch point. This requires some modifications to the basic bottom-most compaction algorithm that we sketched above. \n+\n+![](images/036-bottom-most-gc-compaction/03-retain-lsn.svg)\n+\n+## Single Timeline w/ Snapshots: handle `retain_lsn`\n+\n+First let’s look into the case where we create branches over the main branch but don’t write any data to them (aka “snapshots”).\n+\n+The bottom-most compaction algorithm collects all deltas and images of a key and can make decisions on what data to retain. Given that we have a single key’s history as below:\n+\n+```\n+LSN 0x10 -> A\n+LSN 0x20 -> append B\n+retain_lsn: 0x20\n+LSN 0x30 -> append C\n+LSN 0x40 -> append D\n+retain_lsn: 0x40\n+LSN 0x50 -> append E\n+GC horizon: 0x50\n+LSN 0x60 -> append F\n+```\n+\n+The algorithm will produce:\n+\n+```\n+LSN 0x20 -> AB\n+(drop all history below the earliest retain_lsn)\n+LSN 0x40 -> ABCD\n+(assume the cost of replaying 2 deltas is higher than storing the full image, we generate an image here)\n+LSN 0x50 -> append E\n+(replay one delta is cheap)\n+LSN 0x60 -> append F\n+(keep everything as-is above the GC horizon)\n+```\n+\n+![](images/036-bottom-most-gc-compaction/05-btmgc-parent.svg)\n+\n+What happens is that we balance the space taken by each retain_lsn and the cost of replaying deltas during the bottom-most compaction process. This is controlled by a threshold. If `sum(deltas) < $threshold`, the deltas will be retained. Otherwise, an image will be generated and the deltas will be dropped.\n+\n+In the example above, the `$threshold` is 2.\n+\n+## Child Branches with data: pull + partial images\n+\n+In the previous section we have shown how bottom-most compaction respects `retain_lsn` so that all data that was readable at branch creation remains readable. But branches can have data on their own, and that data can fall out of the branch’s PITR window. So, this section explains how we deal with that.\n+\n+We will run the same bottom-most compaction for these branches, to ensure the space amplification on the child branch is reasonable. \n+\n+```\n+branch_lsn: 0x20\n+LSN 0x30 -> append P\n+LSN 0x40 -> append Q\n+LSN 0x50 -> append R\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+Note that bottom-most compaction happens on a per-timeline basis. When it processes this key, it only reads the history from LSN 0x30 without a base image. Therefore, on child branches, the bottom-most compaction process will make image creation decisions based on the same `sum(deltas) < $threshold` criteria, and if it decides to create an image, the base image will be retrieved from the ancestor branch.\n+\n+```\n+branch_lsn: 0x20\n+LSN 0x50 -> ABPQR\n+(we pull the image at LSN 0x20 from the ancestor branch to get AB, and then apply append PQ to the page; we replace the record at 0x40 with an image and drop the delta)\n+GC horizon: 0x50\n+LSN 0x60 -> append S\n+```\n+\n+![](images/036-bottom-most-gc-compaction/06-btmgc-child.svg)\n+\n+Note that for child branches, we do not create image layers for the images when bottom-most compaction runs. Instead, we drop the 0x30/0x40/0x50 delta records and directly place the image ABPQR@0x50 into the delta layer, which serves a partial image layer. For child branches, if we create image layers, we will need to put all keys in the range into the image layer. This causes space bloat and slow compactions. In this proposal, the compaction process will only compact and process keys modified inside the child branch.\n+\n+## Optimization: Layer Selection for Compaction\n+\n+In the basic bottom-most compaction, we select all layers that intersect with or are below the GC horizon. With retain_lsn and child branches taken into consideration, we do not need to run bottom-most compaction for all layers. Instead, only a subset of the layers need to be picked for compaction.\n+\n+![](images/036-bottom-most-gc-compaction/08-optimization.svg)\n+\n+Consider the case that the system finishes bottom-most compaction at GC horizon 0x60. Now GC horizon grows to 0x70 and the branch at 0x50 is deleted. We only need to pick the layers between 0x40 and 0x70 for compaction (the red layers). As other lower layers will not change after the compaction process, it does not need to be picked.\n+\n+# Result\n+\n+Bottom-most compaction ensures all garbages under the GC horizon gets collected right away (compared with “eventually” in the current algorithm). Meanwhile, it generates images at each of the retain_lsn to ensure branch reads are fast. As we make per-key decision on whether to generate an image or not, the theoretical lower bound of the storage space we need to retain a branch is lower than before.\n+\n+Before: min(sum(logs for each key), sum(image for each key)), for each partition — we always generate image layers on a key range\n+\n+After: sum(min(logs for each key, image for each key))\n+\n+# Dealing with Large Input\n+\n+Bottom-most compaction does a full compaction below the GC horizon, and the process is currently designed as non-resumable. The process should take a fixed amount of memory and be able to run for a few minutes.\n+\n+We made two design choices for the compaction algorithm:\n+\n+- Use k-merge to collect data required for the compaction. The memory consumption is `<buffered key-values per file> x <num of layers involved in compaction>`.\n+    - Key history of 1 million → OOM?\n+- Parallel compaction to make full use of the CPU resources.\n+\n+The image creation is integrated into the k-merge, i.e., reconstruct data naturally becomes available as part of k-merge instead of calling into get_values_reconstruct_data. This is significantly more CPU efficient than the image layer creation of legacy compaction.\n+\n+# Compaction Trigger\n+\n+The bottom-most compaction should be automatically triggered. The goal of the trigger is that it should ensure a constant factor for write amplification. Say that the user write 1GB of WAL into the system, we should write 1GB x C data to S3. The legacy compaction algorithm does not has such a constant factor C. The data we write to S3 is quadratic to the logical size of the database (see [A Theoretical View of Neon Storage](https://www.notion.so/A-Theoretical-View-of-Neon-Storage-8d7ad7555b0c41b2a3597fa780911194?pvs=21)).",
        "comment_created_at": "2024-07-30T05:33:06+00:00",
        "comment_author": "arpad-m",
        "comment_body": "```suggestion\r\nThe bottom-most compaction should be automatically triggered. The goal of the trigger is that it should ensure a constant factor for write amplification. Say that the user write 1GB of WAL into the system, we should write 1GB x C data to S3. The legacy compaction algorithm does not have such a constant factor C. The data we write to S3 is quadratic to the logical size of the database (see [A Theoretical View of Neon Storage](https://www.notion.so/A-Theoretical-View-of-Neon-Storage-8d7ad7555b0c41b2a3597fa780911194?pvs=21)).\r\n```",
        "pr_file_module": null
      }
    ]
  }
]