[
  {
    "discussion_id": "2003177687",
    "pr_number": 11294,
    "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
    "created_at": "2025-03-19T12:07:50+00:00",
    "commented_code": "+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2003177687",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003177687",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32",
        "comment_created_at": "2025-03-19T12:07:50+00:00",
        "comment_author": "MMeent",
        "comment_body": "I really don't like this. Let's expose plain numbers, not rounded and divided values. It'll also allow (when used as metric) to compare absolute prewarm speed between projects and endpoints even when their sizes are very different.",
        "pr_file_module": null
      },
      {
        "comment_id": "2003867878",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003177687",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32",
        "comment_created_at": "2025-03-19T17:16:35+00:00",
        "comment_author": "ololobus",
        "comment_body": "Makes sense, @MMeent can you suggest what the raw numbers should be? Like `target_lfc_size_pages` and `processed_lfc_pages`? Will it work?",
        "pr_file_module": null
      },
      {
        "comment_id": "2003930802",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003177687",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32",
        "comment_created_at": "2025-03-19T17:52:56+00:00",
        "comment_author": "MMeent",
        "comment_body": "`pages_total` + `pages_processed`, probably?\r\nIIRC, we currently also have internal counters for \"ignored\" and \"dropped\" somewhere in the metrics, to cover those pages that we don't have to load because concurrent workloads already fetched the pages or that we can't load because there are no more LFC entries available, respectively.",
        "pr_file_module": null
      },
      {
        "comment_id": "2004134458",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003177687",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32",
        "comment_created_at": "2025-03-19T19:24:53+00:00",
        "comment_author": "ololobus",
        "comment_body": "Added, thanks",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2003213720",
    "pr_number": 11294,
    "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
    "created_at": "2025-03-19T12:29:24+00:00",
    "commented_code": "+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2003213720",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003213720",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.",
        "comment_created_at": "2025-03-19T12:29:24+00:00",
        "comment_author": "mtyazici",
        "comment_body": "What do you mean by proceed with auto-prewarm? Does it mean primary will start without pre-warming but will continue doing the automatic pre-warm. In this case I don't see how we use auto-prewarm in restart code path. Do we have any usage for it outside of this one? ",
        "pr_file_module": null
      },
      {
        "comment_id": "2003850010",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003213720",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.",
        "comment_created_at": "2025-03-19T17:07:39+00:00",
        "comment_author": "ololobus",
        "comment_body": "I mean that we just start primary from scratch with empty caches, the only option to improve the situation is to do async auto-prewarm, while already accepting new connections. So from cplane perspective, it's just a normal start from with auto-prewarm enabled",
        "pr_file_module": null
      },
      {
        "comment_id": "2005331730",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003213720",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.",
        "comment_created_at": "2025-03-20T10:59:48+00:00",
        "comment_author": "mtyazici",
        "comment_body": ">the only option to improve the situation is to do async auto-prewarm\r\n\r\nI see, how does this improve the situation if the compute is already starting with empty caches? When would we use the auto-prewarmed LFC if we always try to trigger prewarm on restarts triggered from the cplane?",
        "pr_file_module": null
      },
      {
        "comment_id": "2006283261",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003213720",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.\n+\n+When `lfc_auto_prewarm` is set to `true`, `compute_ctl` will start prewarming the LFC upon restart\n+iif some of the previous states is present in S3.\n+\n+### compute_ctl API\n+\n+1. `POST /store_lfc_state` -- dump LFC state using Postgres SQL interface and store result in S3.\n+    This has to be a blocking call, i.e. it will return only after the state is stored in S3.\n+    If there is any concurrent request in progress, we should return `429 Too Many Requests`,\n+    and let the caller to retry.\n+\n+2. `GET /dump_lfc_state` -- dump LFC state using Postgres SQL interface and return it as is\n+    in text format suitable for the future restore/prewarm. This API is not strictly needed at\n+    the end state, but could be useful for a faster prototyping of a complete rolling restart flow\n+    with prewarm, as it doesn't require persistent for LFC state storage.\n+\n+3. `POST /restore_lfc_state` -- restore/prewarm LFC state with request\n+\n+    ```yaml\n+    RestoreLFCStateRequest:\n+      oneOf:\n+        - type: object\n+          required:\n+            - lfc_state\n+          properties:\n+            lfc_state:\n+              type: string\n+              description: Raw LFC content dumped with GET `/dump_lfc_state`\n+        - type: object\n+          required:\n+            - lfc_cache_key\n+          properties:\n+            lfc_cache_key:\n+              type: string\n+              description: |\n+                endpoint_id of the source endpoint on the same branch\n+                to use as a 'donor' for LFC content. Compute will look up\n+                LFC content dump in S3 using this key and do prewarm.\n+    ```\n+\n+    where `lfc_state` and `lfc_cache_key` are mutually exclusive.\n+\n+    The actual prewarming will happen asynchronously, so the caller need to check the\n+    prewarm status using the compute's standard `GET /status` API.\n+\n+4. `GET /status` -- extend existing API with following attributes\n+\n+    ```rust\n+    struct ComputeStatusResponse {\n+        // [All existing attributes]\n+        ...\n+        pub prewarm_state: PrewarmState\n+    }\n+\n+    /// Compute prewarm state. Will be stored in the shared Compute state\n+    //. in compute_ctl\n+    struct PrewarmState {\n+        pub status: PrewarmStatus\n+        /// Prewarm progress in the range [0, 1]\n+        pub progress: f32\n+        /// Optional prewarm error\n+        pub error: Option<String>\n+    }\n+\n+    pub enum PrewarmStatus {\n+        /// Prewarming was never requested on this compute\n+        Off,\n+        /// Prewarming was requested, but not started yet\n+        Pending,\n+        /// Prewarming is in progress. The caller should follow\n+        /// `PrewarmState::progress`.\n+        InProgress,\n+        /// Prewarming has been successfully completed\n+        Completed,\n+        /// Prewarming failed. The caller should look at\n+        /// `PrewarmState::error` for the reason.\n+        Failed,\n+        /// It is intended to be used by auto-prewarm if none of\n+        /// the previous LFC states is available in S3.\n+        /// This is a distinct state from the `Failed` because\n+        /// technically it's not a failure and could happen if\n+        /// compute was restart before it dumped anything into S3,\n+        /// or just after the initial rollout of the feature.\n+        Skipped,\n+    }\n+    ```\n+\n+5. `POST /promote` -- this is a **blocking** API call to promote compute replica into primary.\n+    - If promotion is done successfully, it will return `200 OK`.\n+    - If compute is already primary, the call will be no-op and `compute_ctl`\n+      will return `412 Precondition Failed`.\n+    - If, for some reason, second request reaches compute that is in progress of promotion,\n+      it will respond with `429 Too Many Requests`.\n+    - If compute hit any permanent failure during promotion `500 Internal Server Error`\n+      will be returned.\n+\n+### Control plane operations\n+\n+The complete flow will be present as a sequence diagram in the next section, but here\n+we just want to list some important steps that have to be done by control plane during\n+the rolling restart via warm replica, but without much of low-level implementation details.\n+\n+1. Register the 'intent' of the instance restart, but not yet interrupt any workload at\n+    primary and also accept new connections. This may require some endpoint state machine\n+    changes, e.g. introduction of the `pending_restart` state. Being in this state also\n+    **mustn't prevent any other operations except restart**: suspend, live-reconfiguration\n+    (e.g. due to notify-attach call from the storage controller), deletion.\n+\n+2. Start new replica compute on the same timeline and start prewarming it. This process\n+    may take quite a while, so the same concurrency considerations as in 1. should be applied\n+    here as well.\n+\n+3. When warm replica is ready, control plane should:\n+\n+    3.1. Terminate the primary compute. Starting from here, **this is a critical section**,\n+        if anything goes off, the only option is to start the primary normally and proceed\n+        with auto-prewarm.",
        "comment_created_at": "2025-03-20T19:05:28+00:00",
        "comment_author": "ololobus",
        "comment_body": "Well, that's debatable whether we need auto-prewarm or not at all. It exists in vanilla Postgres. The idea is that we can prewarm caches faster when we do it intentionally vs. when user tries to prewarm by just doing their normal workload\r\n\r\nImagine cplane, it eventually accesses all non-deleted projects/endpoint/branches. If we just restart it at Neon, it will take some time to visit all objects. Yet, if we actively prewarm caches in the brackground, the chance that next project read will hit the cache will be higher, as it will be already there, even though cplane hasn't read it explicitly\r\n\r\nIn practice, it may not suite all workloads, but we cannot answer for sure until we implement it and battle-test, but imo it exists in vanilla Postgres for a reason, so there are use-cases",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2003155214",
    "pr_number": 11294,
    "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
    "created_at": "2025-03-19T11:54:37+00:00",
    "commented_code": "+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2003155214",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003155214",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
        "comment_created_at": "2025-03-19T11:54:37+00:00",
        "comment_author": "MMeent",
        "comment_body": "I don't like this. S3 writes are expensive to scale, and if we're about to write every N seconds it's going to be expensive to host read replicas.\r\n\r\nI'd _expected_ CPlane-triggered writes; either during shutdown (for warming up after start) or before shutdown (for hot restart); not systematic writes.",
        "pr_file_module": null
      },
      {
        "comment_id": "2003482724",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003155214",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
        "comment_created_at": "2025-03-19T14:28:21+00:00",
        "comment_author": "knizhnik",
        "comment_body": "+1 for  CPlane-triggered writes.\r\nAlternatively it can done as part of checkpoint. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2003487682",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003155214",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
        "comment_created_at": "2025-03-19T14:30:33+00:00",
        "comment_author": "MMeent",
        "comment_body": "I don't think checkpoint is a good place for this; it's much too frequent on highly loaded systems.",
        "pr_file_module": null
      },
      {
        "comment_id": "2003839065",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003155214",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
        "comment_created_at": "2025-03-19T17:02:14+00:00",
        "comment_author": "ololobus",
        "comment_body": "Well, seconds is just a unit, it can be set to 5, 15 minutes. For cplane-orchestrated there is a separate API, I imagined periodic dumping to be useful for\r\ni) auto-prewarm, i.e. compute periodically dumps LFC content, so later it can be used at restart. In theory, we can only dump at graceful shutdown, but then it won't help with accidental compute crash/restart, as there might be no LFC state to warm up from\r\nii) later for having a hot standby, i.e. it can periodically fetch fresh content from S3 and do prewarming\r\n\r\nI don't want to wire too complex logic via cplane, so TBH, I don't see other options to have a robust auto-prewarm without periodic dumping of the LFC state, pg_prewarm does the same via `pg_prewarm.autoprewarm_interval`\r\n\r\n@MMeent @knizhnik do you have any specific suggestions of how we can implement it without periodic dumping?",
        "pr_file_module": null
      },
      {
        "comment_id": "2003851746",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003155214",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
        "comment_created_at": "2025-03-19T17:08:09+00:00",
        "comment_author": "MMeent",
        "comment_body": "We can make it part of endpoint shutdown procedures?\r\n\r\nNote that \"i.e. it can periodically fetch fresh content from S3 and do prewarming\" won't work as you seem to expect it to, as prewarming is explicitly designed to never evict existing pages from LFC, and thus won't do much if the size of LFC doesn't change for the hot standby. It's prewarming by replacing unused pages with potentially useful pages, and explicitly not LFC state synchronization.",
        "pr_file_module": null
      },
      {
        "comment_id": "2003890005",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003155214",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
        "comment_created_at": "2025-03-19T17:28:51+00:00",
        "comment_author": "MMeent",
        "comment_body": "As for hot standby, it's probably good enough to \"just\" disable the replay filter and require replay of all WAL (rather than only those records which we already have in our caches)",
        "pr_file_module": null
      },
      {
        "comment_id": "2003966381",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003155214",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
        "comment_created_at": "2025-03-19T18:13:22+00:00",
        "comment_author": "ololobus",
        "comment_body": "Thanks for the comment about hot standby\r\n\r\n> We can make it part of endpoint shutdown procedures?\r\n\r\nYes, this is what I meant by 'In theory, we can only dump at graceful shutdown'. That'd work in most of the cases, but what I don't like is that it doesn't cover any abnormal termination like OOM, VM restarts, etc.\r\n\r\nWith your cost estimation, dumping every 5 minutes is completely reasonable",
        "pr_file_module": null
      },
      {
        "comment_id": "2178478227",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003155214",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>\n+}\n+```\n+\n+When `lfc_dump_interval_sec` is set to `N`, `compute_ctl` will periodically dump the LFC state\n+and store it in S3, so that it could be used either for auto-prewarm after restart or by replica\n+during the rolling restart.",
        "comment_created_at": "2025-07-01T20:42:11+00:00",
        "comment_author": "ololobus",
        "comment_body": "> Note that \"i.e. it can periodically fetch fresh content from S3 and do prewarming\" won't work as you seem to expect it to, as prewarming is explicitly designed to never evict existing pages from LFC, and thus won't do much if the size of LFC doesn't change for the hot standby. It's prewarming by replacing unused pages with potentially useful pages, and explicitly not LFC state synchronization.\r\n\r\nRe-reading it after a long time and now it still looks like it should work. Like\r\n\r\n1. We did prewarm once\r\n2. After some time, we fetch LFC content again and iterate over blocks to check if they are present in the LFC\r\n2.1. If block is in LFC -- good, WAL replay should keep it up-to-date\r\n2.2. If it's not in LFC -- we will fetch it from pageserver\r\n\r\nThat way, we do not need any eviction explicitly, and it will help with keeping the LFC relatively warm. Not saying that we need to do it exactly like that, I like you suggestion with switching the replay mode and replaying all pages, it's just this could be a viable alternative\r\n\r\nOr do I miss something?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2003343578",
    "pr_number": 11294,
    "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
    "created_at": "2025-03-19T13:33:37+00:00",
    "commented_code": "+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2003343578",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003343578",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>",
        "comment_created_at": "2025-03-19T13:33:37+00:00",
        "comment_author": "MMeent",
        "comment_body": "A calculation (S3 us-east-1) based on a setting of 60s:\r\n\r\n> 1 req/min * 60 min/hr * 730 hr/mon * $0.005 /1000req (S3 POST) = $0.219 per concurrently active compute per month, or $219 per 1000 concurrently active computes per month (annualized).\r\n\r\nI'd rather keep this frequency much lower than that even, as this seems like a significant potential cost for free tier operations: It'd be 7% of the monthly cost of a tiny general-purpose compute instance (t4g.micro) at AWS.\r\n\r\nI'm also a bit concerned about dumping LFC state every so often. Dumping that state is not free, and while it isn't all _that_ expensive it does block all IO operations to the LFC for a short while. Doing the dump regularly will likely cause some (small?) amount of degraded performance.\r\n\r\nIf instead of frequently dumping this state, we should dump the state only when needed (i.e. with pending shutdown or pending restart), so that we'll only degrade performance if and when needed.",
        "pr_file_module": null
      },
      {
        "comment_id": "2003513959",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003343578",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>",
        "comment_created_at": "2025-03-19T14:41:54+00:00",
        "comment_author": "knizhnik",
        "comment_body": "I think dumping it in checkpoint is the best choice.",
        "pr_file_module": null
      },
      {
        "comment_id": "2003906248",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003343578",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>",
        "comment_created_at": "2025-03-19T17:39:02+00:00",
        "comment_author": "ololobus",
        "comment_body": "I was actually thinking about using the default that pg_prewarm uses -- 300s. I think it's frequent enough for this purpose. This will lower it it to ~$40 per 1k computes per month, which is good enough, imo",
        "pr_file_module": null
      },
      {
        "comment_id": "2004112358",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 11294,
        "pr_file": "docs/rfcs/2025-03-17-compute-prewarm.md",
        "discussion_id": "2003343578",
        "commented_code": "@@ -0,0 +1,345 @@\n+# Compute rolling restart with prewarm\n+\n+Created on 2025-03-17\n+Implemented on _TBD_\n+Author: Alexey Kondratov (@ololobus)\n+\n+## Summary\n+\n+This RFC describes an approach to reduce performance degradation due to missing caches after compute node restart, i.e.:\n+\n+1. Rolling restart of the running instance via 'warm' replica.\n+2. Auto-prewarm compute caches after unplanned restart or scale-to-zero.\n+\n+## Motivation\n+\n+Neon currently implements several features that guarantee high uptime of compute nodes:\n+\n+1. Storage high-availability (HA), i.e. each tenant shard has a secondary pageserver location, so we can quickly switch over compute to it in case of primary pageserver failure.\n+2. Fast compute provisioning, i.e. we have a fleet of pre-created empty computes, that are ready to serve workload, so restarting unresponsive compute is very fast.\n+3. Preemptive NeonVM compute provisioning in case of k8s node unavailability.\n+\n+This helps us to be well-within the uptime SLO of 99.95% most of the time. Problems begin when we go up to multi-TB workloads and 32-64 CU computes.\n+During restart, compute looses all caches: LFC, shared buffers, file system cache. Depending on the workload, it can take a lot of time to warm up the caches,\n+so that performance could be degraded and might be even unacceptable for certain workloads. The latter means that although current approach works well for small to\n+medium workloads, we still have to do some additional work to avoid performance degradation after restart of large instances.\n+\n+## Non Goals\n+\n+- Details of the persistence storage for prewarm data are out of scope, there is a separate RFC for that: <https://github.com/neondatabase/neon/pull/9661>.\n+- Complete compute/Postgres HA setup and flow. Although it was originally in scope of this RFC, during preliminary research it appeared to be a rabbit hole, so it's worth of a separate RFC.\n+- Low-level implementation details for Postgres replica-to-primary promotion. There are a lot of things to think and care about: how to start walproposer, [logical replication failover](https://www.postgresql.org/docs/current/logical-replication-failover.html), and so on, but it's worth of at least a separate one-pager design document if not RFC.\n+\n+## Impacted components\n+\n+Postgres, compute_ctl, Control plane, S3 proxy for unlogged storage of compute files.\n+\n+## Proposed implementation\n+\n+### compute_ctl spec changes and auto-prewarm\n+\n+We are going to extend the current compute spec with the following attributes\n+\n+```rust\n+struct ComputeSpec {\n+    /// [All existing attributes]\n+    ...\n+    /// Whether to do auto-prewarm at start or not.\n+    /// Default to `false`.\n+    pub lfc_auto_prewarm: bool\n+    /// Interval in seconds between automatic dumps of\n+    /// LFC state into S3. Default `None`, which means 'off'.\n+    pub lfc_dump_interval_sec: Option<i32>",
        "comment_created_at": "2025-03-19T19:15:39+00:00",
        "comment_author": "ololobus",
        "comment_body": "Mentioned this default explicitly, thanks for the estimation",
        "pr_file_module": null
      }
    ]
  }
]