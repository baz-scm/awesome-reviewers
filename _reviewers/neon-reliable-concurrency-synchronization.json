[
  {
    "discussion_id": "2124037771",
    "pr_number": 12116,
    "pr_file": "test_runner/regress/test_sharding.py",
    "created_at": "2025-06-03T14:29:59+00:00",
    "commented_code": "# The split should appear to be rolled back from the point of view of all pageservers\n         # apart from the one that is offline\n-        wait_until(lambda: assert_rolled_back(exclude_ps_id=failure.pageserver_id))\n+        wait_until(lambda: assert_rolled_back(exclude_ps_id=failure.pageserver_id), timeout=120)",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2124037771",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12116,
        "pr_file": "test_runner/regress/test_sharding.py",
        "discussion_id": "2124037771",
        "commented_code": "@@ -1468,7 +1454,7 @@ def finish_split():\n \n         # The split should appear to be rolled back from the point of view of all pageservers\n         # apart from the one that is offline\n-        wait_until(lambda: assert_rolled_back(exclude_ps_id=failure.pageserver_id))\n+        wait_until(lambda: assert_rolled_back(exclude_ps_id=failure.pageserver_id), timeout=120)",
        "comment_created_at": "2025-06-03T14:29:59+00:00",
        "comment_author": "VladLazar",
        "comment_body": "Would calling `env.storage_controller.reconcile_until_idle()` to force the storage controller to run all pending reconciliations work?",
        "pr_file_module": null
      },
      {
        "comment_id": "2124165067",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12116,
        "pr_file": "test_runner/regress/test_sharding.py",
        "discussion_id": "2124037771",
        "commented_code": "@@ -1468,7 +1454,7 @@ def finish_split():\n \n         # The split should appear to be rolled back from the point of view of all pageservers\n         # apart from the one that is offline\n-        wait_until(lambda: assert_rolled_back(exclude_ps_id=failure.pageserver_id))\n+        wait_until(lambda: assert_rolled_back(exclude_ps_id=failure.pageserver_id), timeout=120)",
        "comment_created_at": "2025-06-03T15:05:27+00:00",
        "comment_author": "ephemeralsad",
        "comment_body": "It looks great, added `reconcile_until_idle` instead of timeouts. I think it resolves the problem with long BACKGROUND_RECONCILE_PERIOD.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2128457123",
    "pr_number": 12096,
    "pr_file": "test_runner/regress/test_storage_controller.py",
    "created_at": "2025-06-05T09:58:08+00:00",
    "commented_code": "wait_until(reconfigure_node_again)\n \n \n+def test_ps_unavailable_after_delete(neon_env_builder: NeonEnvBuilder):\n+    neon_env_builder.num_pageservers = 3\n+\n+    env = neon_env_builder.init_start()\n+\n+    def assert_nodes_count(n: int):\n+        nodes = env.storage_controller.node_list()\n+        assert len(nodes) == n\n+\n+    # Nodes count must remain the same before deletion\n+    assert_nodes_count(3)\n+\n+    ps = env.pageservers[0]\n+    env.storage_controller.node_delete(ps.id)\n+\n+    # After deletion, the node count must be reduced\n+    assert_nodes_count(2)\n+\n+    log.info(\"Restarting tombstoned pageserver...\")\n+    ps.restart()",
    "repo_full_name": "neondatabase/neon",
    "discussion_comments": [
      {
        "comment_id": "2128457123",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12096,
        "pr_file": "test_runner/regress/test_storage_controller.py",
        "discussion_id": "2128457123",
        "commented_code": "@@ -3093,6 +3093,48 @@ def reconfigure_node_again():\n     wait_until(reconfigure_node_again)\n \n \n+def test_ps_unavailable_after_delete(neon_env_builder: NeonEnvBuilder):\n+    neon_env_builder.num_pageservers = 3\n+\n+    env = neon_env_builder.init_start()\n+\n+    def assert_nodes_count(n: int):\n+        nodes = env.storage_controller.node_list()\n+        assert len(nodes) == n\n+\n+    # Nodes count must remain the same before deletion\n+    assert_nodes_count(3)\n+\n+    ps = env.pageservers[0]\n+    env.storage_controller.node_delete(ps.id)\n+\n+    # After deletion, the node count must be reduced\n+    assert_nodes_count(2)\n+\n+    log.info(\"Restarting tombstoned pageserver...\")\n+    ps.restart()",
        "comment_created_at": "2025-06-05T09:58:08+00:00",
        "comment_author": "VladLazar",
        "comment_body": "This will hang since `NeonPageserver.start` waits for the node to be marked as active in the storage controller by default. Re-attaching is a pre-condition for that. To avoid this, you can do:\r\n```\r\nps.stop()\r\nps.start(await_active=False)\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2128689772",
        "repo_full_name": "neondatabase/neon",
        "pr_number": 12096,
        "pr_file": "test_runner/regress/test_storage_controller.py",
        "discussion_id": "2128457123",
        "commented_code": "@@ -3093,6 +3093,48 @@ def reconfigure_node_again():\n     wait_until(reconfigure_node_again)\n \n \n+def test_ps_unavailable_after_delete(neon_env_builder: NeonEnvBuilder):\n+    neon_env_builder.num_pageservers = 3\n+\n+    env = neon_env_builder.init_start()\n+\n+    def assert_nodes_count(n: int):\n+        nodes = env.storage_controller.node_list()\n+        assert len(nodes) == n\n+\n+    # Nodes count must remain the same before deletion\n+    assert_nodes_count(3)\n+\n+    ps = env.pageservers[0]\n+    env.storage_controller.node_delete(ps.id)\n+\n+    # After deletion, the node count must be reduced\n+    assert_nodes_count(2)\n+\n+    log.info(\"Restarting tombstoned pageserver...\")\n+    ps.restart()",
        "comment_created_at": "2025-06-05T12:05:22+00:00",
        "comment_author": "ephemeralsad",
        "comment_body": "Had a quick call - discussed that `await_active=False` doesn't help. Agreed to proceed with the `ThreadPoolExecutor` approach.",
        "pr_file_module": null
      }
    ]
  }
]