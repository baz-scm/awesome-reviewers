[
  {
    "discussion_id": "2262324951",
    "pr_number": 6598,
    "pr_file": "app/client/platforms/openai.ts",
    "created_at": "2025-08-08T11:49:09+00:00",
    "commented_code": "messages,\n         stream: options.config.stream,\n         model: modelConfig.model,\n-        temperature: !isO1OrO3 ? modelConfig.temperature : 1,\n+        temperature: !isO1OrO3 && !isGPT5 ? modelConfig.temperature : 1,",
    "repo_full_name": "ChatGPTNextWeb/NextChat",
    "discussion_comments": [
      {
        "comment_id": "2262735680",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 6598,
        "pr_file": "app/client/platforms/openai.ts",
        "discussion_id": "2262324951",
        "commented_code": "@@ -230,7 +231,7 @@ export class ChatGPTApi implements LLMApi {\n         messages,\n         stream: options.config.stream,\n         model: modelConfig.model,\n-        temperature: !isO1OrO3 ? modelConfig.temperature : 1,\n+        temperature: !isO1OrO3 && !isGPT5 ? modelConfig.temperature : 1,",
        "comment_created_at": "2025-08-08T11:49:09+00:00",
        "comment_author": "tiomfree",
        "comment_body": "good point!, and should add the definition for isGPT5: \r\nconst isGPT5 = options.config.model.startsWith(\"gpt-5\");\r\n    const isO1OrO3 = \r\n      options.config.model.startsWith(\"o1\") ||\r\n      options.config.model.startsWith(\"o3\") ||\r\n      options.config.model.startsWith(\"o4-mini\");",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1703964692",
    "pr_number": 5194,
    "pr_file": "app/client/platforms/baidu.ts",
    "created_at": "2024-08-05T11:22:21+00:00",
    "commented_code": "async chat(options: ChatOptions) {\n     const messages = options.messages.map((v) => ({\n-      role: v.role,\n+      // \"error_code\": 336006, \"error_msg\": \"the role of message with odd index in the messages must be assistant\",\n+      role: v.role === \"system\" ? \"assistant\" : v.role,",
    "repo_full_name": "ChatGPTNextWeb/NextChat",
    "discussion_comments": [
      {
        "comment_id": "1703964692",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 5194,
        "pr_file": "app/client/platforms/baidu.ts",
        "discussion_id": "1703964692",
        "commented_code": "@@ -77,7 +77,8 @@ export class ErnieApi implements LLMApi {\n \n   async chat(options: ChatOptions) {\n     const messages = options.messages.map((v) => ({\n-      role: v.role,\n+      // \"error_code\": 336006, \"error_msg\": \"the role of message with odd index in the messages must be assistant\",\n+      role: v.role === \"system\" ? \"assistant\" : v.role,",
        "comment_created_at": "2024-08-05T11:22:21+00:00",
        "comment_author": "lloydzhou",
        "comment_body": "\u8fd9\u91cc\u6216\u8bb8\u5e94\u8be5 `user` \u800c\u4e0d\u662f `assistant`",
        "pr_file_module": null
      },
      {
        "comment_id": "1703970662",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 5194,
        "pr_file": "app/client/platforms/baidu.ts",
        "discussion_id": "1703964692",
        "commented_code": "@@ -77,7 +77,8 @@ export class ErnieApi implements LLMApi {\n \n   async chat(options: ChatOptions) {\n     const messages = options.messages.map((v) => ({\n-      role: v.role,\n+      // \"error_code\": 336006, \"error_msg\": \"the role of message with odd index in the messages must be assistant\",\n+      role: v.role === \"system\" ? \"assistant\" : v.role,",
        "comment_created_at": "2024-08-05T11:27:49+00:00",
        "comment_author": "lloydzhou",
        "comment_body": "\u53e6\u5916\uff0c\u817e\u8baf\u6df7\u5143\u4ee5\u53cagoogle Gemini \u5e94\u8be5\u90fd\u4e0d\u652f\u6301 system message\r\n\u53ef\u4ee5\u4e00\u8d77\u5904\u7406",
        "pr_file_module": null
      },
      {
        "comment_id": "1704085843",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 5194,
        "pr_file": "app/client/platforms/baidu.ts",
        "discussion_id": "1703964692",
        "commented_code": "@@ -77,7 +77,8 @@ export class ErnieApi implements LLMApi {\n \n   async chat(options: ChatOptions) {\n     const messages = options.messages.map((v) => ({\n-      role: v.role,\n+      // \"error_code\": 336006, \"error_msg\": \"the role of message with odd index in the messages must be assistant\",\n+      role: v.role === \"system\" ? \"assistant\" : v.role,",
        "comment_created_at": "2024-08-05T13:01:59+00:00",
        "comment_author": "HyiKi",
        "comment_body": "> \u8fd9\u91cc\u6216\u8bb8\u5e94\u8be5 `user` \u800c\u4e0d\u662f `assistant`\r\n\r\n\u767e\u5ea6\u7684\u6821\u9a8c\u89c4\u5219\u4e25\u683c\uff0c\u8981\u6c42\u662f\u5355\u6570messages\uff0c\u53cc\u6570`user`\uff0c\u5355\u6570`assistant`\uff0c\u5bf9\u8bdd\u662f`user`\u548c`assistant`\u4ea4\u66ff\u8fdb\u884c\uff0c\u8fd9\u91cc\u7684\u5904\u7406\u5c06\u6458\u8981`system`\u8f6c\u6210`assistant`\uff0c\u6765\u627f\u63a5\u539f\u6709\u903b\u8f91\u662f\u53cc\u6570messages\u8865\u4e00\u4e2a\u7a7a`user`\u7ed9\u7b2c\u4e00\u6761\u3002\r\n\r\n\u63a5\u4e0b\u6765\u6211\u8c03\u6574\u6458\u8981`system`\u8f6c\u6210`user`\uff0c\u5982\u679c\u7b2c\u4e00\u6761\u662f`\u001duser`\uff0c\u5c06\u5728\u7d22\u5f15\u4e3a1\u7684\u5730\u65b9\u63d2\u5165\u7a7a`assistant`\u6765\u6ee1\u8db3\u6821\u9a8c\u89c4\u5219\u3002",
        "pr_file_module": null
      },
      {
        "comment_id": "1704086238",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 5194,
        "pr_file": "app/client/platforms/baidu.ts",
        "discussion_id": "1703964692",
        "commented_code": "@@ -77,7 +77,8 @@ export class ErnieApi implements LLMApi {\n \n   async chat(options: ChatOptions) {\n     const messages = options.messages.map((v) => ({\n-      role: v.role,\n+      // \"error_code\": 336006, \"error_msg\": \"the role of message with odd index in the messages must be assistant\",\n+      role: v.role === \"system\" ? \"assistant\" : v.role,",
        "comment_created_at": "2024-08-05T13:02:17+00:00",
        "comment_author": "HyiKi",
        "comment_body": "> \u53e6\u5916\uff0c\u817e\u8baf\u6df7\u5143\u4ee5\u53cagoogle Gemini \u5e94\u8be5\u90fd\u4e0d\u652f\u6301 system message \u53ef\u4ee5\u4e00\u8d77\u5904\u7406\r\n\r\ngoogle.ts \u524d\u4eba\u5df2\u505a\u5904\u7406\uff1a\r\n\r\n```typescript\r\nrole: v.role.replace(\"assistant\", \"model\").replace(\"system\", \"user\"),\r\n```\r\n\r\n\u817e\u8baf\u6df7\u5143\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u6ca1\u6709\u53d1\u73b0\u9519\u8bef\uff0c\u652f\u6301 system message\u3002\r\n\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1704844552",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 5194,
        "pr_file": "app/client/platforms/baidu.ts",
        "discussion_id": "1703964692",
        "commented_code": "@@ -77,7 +77,8 @@ export class ErnieApi implements LLMApi {\n \n   async chat(options: ChatOptions) {\n     const messages = options.messages.map((v) => ({\n-      role: v.role,\n+      // \"error_code\": 336006, \"error_msg\": \"the role of message with odd index in the messages must be assistant\",\n+      role: v.role === \"system\" ? \"assistant\" : v.role,",
        "comment_created_at": "2024-08-06T02:57:54+00:00",
        "comment_author": "HyiKi",
        "comment_body": "> \u53e6\u5916\uff0c\u817e\u8baf\u6df7\u5143\u4ee5\u53cagoogle Gemini \u5e94\u8be5\u90fd\u4e0d\u652f\u6301 system message \u53ef\u4ee5\u4e00\u8d77\u5904\u7406\r\n\r\n@lloydzhou ok\uff0c\u4fee\u590d\u4e86\u804a\u5929\u4e2d\u817e\u8baf\u6df7\u5143\u4e0d\u652f\u6301 system message\u7684\u9519\u8bef\uff1a\r\n\r\n```json\r\n{\r\n  \"Response\": {\r\n    \"RequestId\": \"fb668c8e-c5db-4abd-9cec-8b066a3c06e1\",\r\n    \"Error\": {\r\n      \"Code\": \"InvalidParameter\",\r\n      \"Message\": \"Messages \u4e2d system \u89d2\u8272\u5fc5\u987b\u4f4d\u4e8e\u5217\u8868\u7684\u6700\u5f00\u59cb\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n\u5904\u7406\uff1a\u975e\u6700\u5f00\u59cb\u7684`system` message\uff0c`system`\u8f6c\u6210`user`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1666754141",
    "pr_number": 4930,
    "pr_file": "app/client/platforms/openai.ts",
    "created_at": "2024-07-05T12:33:26+00:00",
    "commented_code": "options.onController?.(controller);\n \n     try {\n-      const chatPath = this.path(OpenaiPath.ChatPath);\n+      let chatPath = \"\";\n+      if (modelConfig.providerName == ServiceProvider.Azure) {",
    "repo_full_name": "ChatGPTNextWeb/NextChat",
    "discussion_comments": [
      {
        "comment_id": "1666754141",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4930,
        "pr_file": "app/client/platforms/openai.ts",
        "discussion_id": "1666754141",
        "commented_code": "@@ -140,7 +141,33 @@ export class ChatGPTApi implements LLMApi {\n     options.onController?.(controller);\n \n     try {\n-      const chatPath = this.path(OpenaiPath.ChatPath);\n+      let chatPath = \"\";\n+      if (modelConfig.providerName == ServiceProvider.Azure) {",
        "comment_created_at": "2024-07-05T12:33:26+00:00",
        "comment_author": "lloydzhou",
        "comment_body": "azure\u548copenai\u4f7f\u7528\u7684\u6a21\u578b\u90fd\u662fGPT\uff0c\u6240\u4ee5\u90fd\u4f7f\u7528ChatGPTApi\r\n\r\n\u8fd9\u91cc\uff0c\u6309modelConfig.providerName\u5224\u65ad\u662f\u5426\u662fAzure\u3002\r\n\u540c\u65f6\u5c1d\u8bd5\u4eceallModels\u91cc\u9762\u83b7\u53d6displayName\u4f5c\u4e3adeploymentName\u4f7f\u7528\u3002\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1666754943",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4930,
        "pr_file": "app/client/platforms/openai.ts",
        "discussion_id": "1666754141",
        "commented_code": "@@ -140,7 +141,33 @@ export class ChatGPTApi implements LLMApi {\n     options.onController?.(controller);\n \n     try {\n-      const chatPath = this.path(OpenaiPath.ChatPath);\n+      let chatPath = \"\";\n+      if (modelConfig.providerName == ServiceProvider.Azure) {",
        "comment_created_at": "2024-07-05T12:34:16+00:00",
        "comment_author": "lloydzhou",
        "comment_body": "deploymentName\u4f1a\u7ec4\u5408\u5728url\u91cc\u9762",
        "pr_file_module": null
      },
      {
        "comment_id": "1666821226",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4930,
        "pr_file": "app/client/platforms/openai.ts",
        "discussion_id": "1666754141",
        "commented_code": "@@ -140,7 +141,33 @@ export class ChatGPTApi implements LLMApi {\n     options.onController?.(controller);\n \n     try {\n-      const chatPath = this.path(OpenaiPath.ChatPath);\n+      let chatPath = \"\";\n+      if (modelConfig.providerName == ServiceProvider.Azure) {",
        "comment_created_at": "2024-07-05T13:37:57+00:00",
        "comment_author": "Dogtiti",
        "comment_body": "\u90a3\u4e48\u8fd9\u91cc\u6700\u65b0\u7684azure \u914d\u7f6e\u662f\u4ec0\u4e48\u6837 \u9700\u8981\u66f4\u65b0\u4e00\u4e0breadme",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1669683914",
    "pr_number": 4936,
    "pr_file": "app/client/platforms/baidu.ts",
    "created_at": "2024-07-09T04:37:48+00:00",
    "commented_code": "+\"use client\";\n+import {\n+  ApiPath,\n+  Baidu,\n+  BAIDU_BASE_URL,\n+  REQUEST_TIMEOUT_MS,\n+} from \"@/app/constant\";\n+import { useAccessStore, useAppConfig, useChatStore } from \"@/app/store\";\n+\n+import {\n+  ChatOptions,\n+  getHeaders,\n+  LLMApi,\n+  LLMModel,\n+  MultimodalContent,\n+} from \"../api\";\n+import Locale from \"../../locales\";\n+import {\n+  EventStreamContentType,\n+  fetchEventSource,\n+} from \"@fortaine/fetch-event-source\";\n+import { prettyObject } from \"@/app/utils/format\";\n+import { getClientConfig } from \"@/app/config/client\";\n+import { getMessageTextContent } from \"@/app/utils\";\n+\n+export interface OpenAIListModelResponse {\n+  object: string;\n+  data: Array<{\n+    id: string;\n+    object: string;\n+    root: string;\n+  }>;\n+}\n+\n+interface RequestPayload {\n+  messages: {\n+    role: \"system\" | \"user\" | \"assistant\";\n+    content: string | MultimodalContent[];\n+  }[];\n+  stream?: boolean;\n+  model: string;\n+  temperature: number;\n+  presence_penalty: number;\n+  frequency_penalty: number;\n+  top_p: number;\n+  max_tokens?: number;\n+}\n+\n+export class ErnieApi implements LLMApi {\n+  path(path: string): string {\n+    const accessStore = useAccessStore.getState();\n+\n+    let baseUrl = \"\";\n+\n+    if (accessStore.useCustomConfig) {\n+      baseUrl = accessStore.baiduUrl;\n+    }\n+\n+    if (baseUrl.length === 0) {\n+      const isApp = !!getClientConfig()?.isApp;\n+      // do not use proxy for baidubce api\n+      baseUrl = isApp ? BAIDU_BASE_URL : ApiPath.Baidu;\n+    }\n+\n+    if (baseUrl.endsWith(\"/\")) {\n+      baseUrl = baseUrl.slice(0, baseUrl.length - 1);\n+    }\n+    if (!baseUrl.startsWith(\"http\") && !baseUrl.startsWith(ApiPath.Baidu)) {\n+      baseUrl = \"https://\" + baseUrl;\n+    }\n+\n+    console.log(\"[Proxy Endpoint] \", baseUrl, path);\n+\n+    return [baseUrl, path].join(\"/\");\n+  }\n+\n+  extractMessage(res: any) {\n+    return res.choices?.at(0)?.message?.content ?? \"\";\n+  }\n+\n+  async chat(options: ChatOptions) {\n+    const messages = options.messages.map((v) => ({\n+      role: v.role,\n+      content: getMessageTextContent(v),\n+    }));\n+\n+    const modelConfig = {\n+      ...useAppConfig.getState().modelConfig,\n+      ...useChatStore.getState().currentSession().mask.modelConfig,\n+      ...{\n+        model: options.config.model,\n+      },\n+    };\n+\n+    const requestPayload: RequestPayload = {\n+      messages,",
    "repo_full_name": "ChatGPTNextWeb/NextChat",
    "discussion_comments": [
      {
        "comment_id": "1669683914",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4936,
        "pr_file": "app/client/platforms/baidu.ts",
        "discussion_id": "1669683914",
        "commented_code": "@@ -0,0 +1,252 @@\n+\"use client\";\n+import {\n+  ApiPath,\n+  Baidu,\n+  BAIDU_BASE_URL,\n+  REQUEST_TIMEOUT_MS,\n+} from \"@/app/constant\";\n+import { useAccessStore, useAppConfig, useChatStore } from \"@/app/store\";\n+\n+import {\n+  ChatOptions,\n+  getHeaders,\n+  LLMApi,\n+  LLMModel,\n+  MultimodalContent,\n+} from \"../api\";\n+import Locale from \"../../locales\";\n+import {\n+  EventStreamContentType,\n+  fetchEventSource,\n+} from \"@fortaine/fetch-event-source\";\n+import { prettyObject } from \"@/app/utils/format\";\n+import { getClientConfig } from \"@/app/config/client\";\n+import { getMessageTextContent } from \"@/app/utils\";\n+\n+export interface OpenAIListModelResponse {\n+  object: string;\n+  data: Array<{\n+    id: string;\n+    object: string;\n+    root: string;\n+  }>;\n+}\n+\n+interface RequestPayload {\n+  messages: {\n+    role: \"system\" | \"user\" | \"assistant\";\n+    content: string | MultimodalContent[];\n+  }[];\n+  stream?: boolean;\n+  model: string;\n+  temperature: number;\n+  presence_penalty: number;\n+  frequency_penalty: number;\n+  top_p: number;\n+  max_tokens?: number;\n+}\n+\n+export class ErnieApi implements LLMApi {\n+  path(path: string): string {\n+    const accessStore = useAccessStore.getState();\n+\n+    let baseUrl = \"\";\n+\n+    if (accessStore.useCustomConfig) {\n+      baseUrl = accessStore.baiduUrl;\n+    }\n+\n+    if (baseUrl.length === 0) {\n+      const isApp = !!getClientConfig()?.isApp;\n+      // do not use proxy for baidubce api\n+      baseUrl = isApp ? BAIDU_BASE_URL : ApiPath.Baidu;\n+    }\n+\n+    if (baseUrl.endsWith(\"/\")) {\n+      baseUrl = baseUrl.slice(0, baseUrl.length - 1);\n+    }\n+    if (!baseUrl.startsWith(\"http\") && !baseUrl.startsWith(ApiPath.Baidu)) {\n+      baseUrl = \"https://\" + baseUrl;\n+    }\n+\n+    console.log(\"[Proxy Endpoint] \", baseUrl, path);\n+\n+    return [baseUrl, path].join(\"/\");\n+  }\n+\n+  extractMessage(res: any) {\n+    return res.choices?.at(0)?.message?.content ?? \"\";\n+  }\n+\n+  async chat(options: ChatOptions) {\n+    const messages = options.messages.map((v) => ({\n+      role: v.role,\n+      content: getMessageTextContent(v),\n+    }));\n+\n+    const modelConfig = {\n+      ...useAppConfig.getState().modelConfig,\n+      ...useChatStore.getState().currentSession().mask.modelConfig,\n+      ...{\n+        model: options.config.model,\n+      },\n+    };\n+\n+    const requestPayload: RequestPayload = {\n+      messages,",
        "comment_created_at": "2024-07-09T04:37:48+00:00",
        "comment_author": "lloydzhou",
        "comment_body": "\u53d1\u8bf7\u6c42\u51fa\u53bb\u4e4b\u524d\uff0c\u9700\u8981\u68c0\u67e5\u4e00\u4e0b\uff0c\u6587\u5fc3\u4e00\u8a00\u7684messages\u6570\u91cf\u6709\u9650\u5236\uff0c\u5e94\u8be5\u662f\u5947\u6570\uff0c\u5982\u679c\u4e0d\u662f\u5947\u6570\u6761\u6d88\u606f\uff0c\u4f1a\u62a5\u9519",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1536746813",
    "pr_number": 4244,
    "pr_file": "app/constant.ts",
    "created_at": "2024-03-24T06:25:33+00:00",
    "commented_code": "export const Google = {\n   ExampleEndpoint: \"https://generativelanguage.googleapis.com/\",\n-  ChatPath: \"v1beta/models/gemini-pro:generateContent\",\n+  ChatPath: \"v1beta/models/gemini-pro:generateContent\",",
    "repo_full_name": "ChatGPTNextWeb/NextChat",
    "discussion_comments": [
      {
        "comment_id": "1536746813",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4244,
        "pr_file": "app/constant.ts",
        "discussion_id": "1536746813",
        "commented_code": "@@ -87,7 +87,7 @@ export const Azure = {\n \n export const Google = {\n   ExampleEndpoint: \"https://generativelanguage.googleapis.com/\",\n-  ChatPath: \"v1beta/models/gemini-pro:generateContent\",\n+  ChatPath: \"v1beta/models/gemini-pro:generateContent\",  ",
        "comment_created_at": "2024-03-24T06:25:33+00:00",
        "comment_author": "fred-bf",
        "comment_body": "Gemini's model is dynamically spliced to the path, and it needs to be modified to be dynamic",
        "pr_file_module": null
      },
      {
        "comment_id": "1536973120",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4244,
        "pr_file": "app/constant.ts",
        "discussion_id": "1536746813",
        "commented_code": "@@ -87,7 +87,7 @@ export const Azure = {\n \n export const Google = {\n   ExampleEndpoint: \"https://generativelanguage.googleapis.com/\",\n-  ChatPath: \"v1beta/models/gemini-pro:generateContent\",\n+  ChatPath: \"v1beta/models/gemini-pro:generateContent\",  ",
        "comment_created_at": "2024-03-25T01:39:59+00:00",
        "comment_author": "mountainguan",
        "comment_body": "> Gemini's model is dynamically spliced to the path, and it needs to be modified to be dynamic\r\n\r\nso, if I want to use gemini-1.5-pro\uff0ci should change the _ChatPath_ as \u201cv1beta/models/models/gemini-1.5-pro:generateContent\u201d?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1495066006",
    "pr_number": 4076,
    "pr_file": "app/store/chat.ts",
    "created_at": "2024-02-19T22:42:57+00:00",
    "commented_code": "function getSummarizeModel(currentModel: string) {\n   // if it is using gpt-* models, force to use 3.5 to summarize\n-  return currentModel.startsWith(\"gpt\") ? SUMMARIZE_MODEL : currentModel;\n+  if (currentModel.startsWith(\"gpt\")) {\n+    return SUMMARIZE_MODEL;\n+  }\n+  if (currentModel.startsWith(\"gemini-pro\")) {",
    "repo_full_name": "ChatGPTNextWeb/NextChat",
    "discussion_comments": [
      {
        "comment_id": "1495066006",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4076,
        "pr_file": "app/store/chat.ts",
        "discussion_id": "1495066006",
        "commented_code": "@@ -84,11 +85,20 @@ function createEmptySession(): ChatSession {\n \n function getSummarizeModel(currentModel: string) {\n   // if it is using gpt-* models, force to use 3.5 to summarize\n-  return currentModel.startsWith(\"gpt\") ? SUMMARIZE_MODEL : currentModel;\n+  if (currentModel.startsWith(\"gpt\")) {\n+    return SUMMARIZE_MODEL;\n+  }\n+  if (currentModel.startsWith(\"gemini-pro\")) {",
        "comment_created_at": "2024-02-19T22:42:57+00:00",
        "comment_author": "H0llyW00dzZ",
        "comment_body": "> [!TIP]\r\n> It's more effective to use `Summarize` with Gemini's own models. For example, `gemini-pro` refers to the Gemini Pro model, and `gemini-pro-vision` refers to the Gemini Pro Vision model. These models are more affordable compared to `OpenAI` hahaha and more efficient, as each model has its own specific token limit. For instance, the token input limit for the `gemini-pro-vision` model is around `12288 tokens`.\r\n\r\nProof:\r\n\r\n![image](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/assets/17626300/ad735b1b-f630-43fd-a3ad-49d6067a58bc)\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1497313012",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4076,
        "pr_file": "app/store/chat.ts",
        "discussion_id": "1495066006",
        "commented_code": "@@ -84,11 +85,20 @@ function createEmptySession(): ChatSession {\n \n function getSummarizeModel(currentModel: string) {\n   // if it is using gpt-* models, force to use 3.5 to summarize\n-  return currentModel.startsWith(\"gpt\") ? SUMMARIZE_MODEL : currentModel;\n+  if (currentModel.startsWith(\"gpt\")) {\n+    return SUMMARIZE_MODEL;\n+  }\n+  if (currentModel.startsWith(\"gemini-pro\")) {",
        "comment_created_at": "2024-02-21T10:52:30+00:00",
        "comment_author": "TheRamU",
        "comment_body": "Gemini Pro Vision does not enable multiturn chat, which limits its role as a \"Summarize Model\"",
        "pr_file_module": null
      },
      {
        "comment_id": "1497909187",
        "repo_full_name": "ChatGPTNextWeb/NextChat",
        "pr_number": 4076,
        "pr_file": "app/store/chat.ts",
        "discussion_id": "1495066006",
        "commented_code": "@@ -84,11 +85,20 @@ function createEmptySession(): ChatSession {\n \n function getSummarizeModel(currentModel: string) {\n   // if it is using gpt-* models, force to use 3.5 to summarize\n-  return currentModel.startsWith(\"gpt\") ? SUMMARIZE_MODEL : currentModel;\n+  if (currentModel.startsWith(\"gpt\")) {\n+    return SUMMARIZE_MODEL;\n+  }\n+  if (currentModel.startsWith(\"gemini-pro\")) {",
        "comment_created_at": "2024-02-21T16:43:46+00:00",
        "comment_author": "H0llyW00dzZ",
        "comment_body": "> Gemini Pro Vision does not enable multiturn chat, which limits its role as a \"Summarize Model\"\r\n\r\nrip, its better model, unlike openai `gpt-4-vision-preview`",
        "pr_file_module": null
      }
    ]
  }
]