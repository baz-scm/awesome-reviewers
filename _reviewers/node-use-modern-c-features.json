[
  {
    "discussion_id": "2076547557",
    "pr_number": 58077,
    "pr_file": "src/inspector/io_agent.cc",
    "created_at": "2025-05-06T23:41:34+00:00",
    "commented_code": "+#include \"io_agent.h\"\n+#include \"crdtp/dispatch.h\"\n+\n+namespace node {\n+namespace inspector {\n+namespace protocol {",
    "repo_full_name": "nodejs/node",
    "discussion_comments": [
      {
        "comment_id": "2076547557",
        "repo_full_name": "nodejs/node",
        "pr_number": 58077,
        "pr_file": "src/inspector/io_agent.cc",
        "discussion_id": "2076547557",
        "commented_code": "@@ -0,0 +1,50 @@\n+#include \"io_agent.h\"\n+#include \"crdtp/dispatch.h\"\n+\n+namespace node {\n+namespace inspector {\n+namespace protocol {",
        "comment_created_at": "2025-05-06T23:41:34+00:00",
        "comment_author": "jasnell",
        "comment_body": "Nit... since we're finally up on c++20, we can start using the more condensed `namespace node::inspector::protocol {` syntax where appropriate.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2138789565",
    "pr_number": 58666,
    "pr_file": "src/node_locks.h",
    "created_at": "2025-06-10T21:22:46+00:00",
    "commented_code": "+#ifndef SRC_NODE_LOCKS_H_\n+#define SRC_NODE_LOCKS_H_\n+\n+#if defined(NODE_WANT_INTERNALS) && NODE_WANT_INTERNALS\n+\n+#include <deque>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+\n+#include \"env.h\"\n+#include \"node_mutex.h\"\n+#include \"v8.h\"\n+\n+namespace node {\n+namespace worker {\n+namespace locks {\n+\n+class Lock final {\n+ public:\n+  enum Mode { kShared, kExclusive };",
    "repo_full_name": "nodejs/node",
    "discussion_comments": [
      {
        "comment_id": "2138789565",
        "repo_full_name": "nodejs/node",
        "pr_number": 58666,
        "pr_file": "src/node_locks.h",
        "discussion_id": "2138789565",
        "commented_code": "@@ -0,0 +1,142 @@\n+#ifndef SRC_NODE_LOCKS_H_\n+#define SRC_NODE_LOCKS_H_\n+\n+#if defined(NODE_WANT_INTERNALS) && NODE_WANT_INTERNALS\n+\n+#include <deque>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+\n+#include \"env.h\"\n+#include \"node_mutex.h\"\n+#include \"v8.h\"\n+\n+namespace node {\n+namespace worker {\n+namespace locks {\n+\n+class Lock final {\n+ public:\n+  enum Mode { kShared, kExclusive };",
        "comment_created_at": "2025-06-10T21:22:46+00:00",
        "comment_author": "jasnell",
        "comment_body": "```suggestion\r\n  enum class Mode { Shared, Exclusive };\r\n```\r\n\r\nOptional style nit.",
        "pr_file_module": null
      },
      {
        "comment_id": "2140903727",
        "repo_full_name": "nodejs/node",
        "pr_number": 58666,
        "pr_file": "src/node_locks.h",
        "discussion_id": "2138789565",
        "commented_code": "@@ -0,0 +1,142 @@\n+#ifndef SRC_NODE_LOCKS_H_\n+#define SRC_NODE_LOCKS_H_\n+\n+#if defined(NODE_WANT_INTERNALS) && NODE_WANT_INTERNALS\n+\n+#include <deque>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+\n+#include \"env.h\"\n+#include \"node_mutex.h\"\n+#include \"v8.h\"\n+\n+namespace node {\n+namespace worker {\n+namespace locks {\n+\n+class Lock final {\n+ public:\n+  enum Mode { kShared, kExclusive };",
        "comment_created_at": "2025-06-11T19:19:57+00:00",
        "comment_author": "IlyasShabi",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2138793305",
    "pr_number": 58666,
    "pr_file": "src/node_locks.h",
    "created_at": "2025-06-10T21:25:42+00:00",
    "commented_code": "+#ifndef SRC_NODE_LOCKS_H_\n+#define SRC_NODE_LOCKS_H_\n+\n+#if defined(NODE_WANT_INTERNALS) && NODE_WANT_INTERNALS\n+\n+#include <deque>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+\n+#include \"env.h\"\n+#include \"node_mutex.h\"\n+#include \"v8.h\"\n+\n+namespace node {\n+namespace worker {\n+namespace locks {",
    "repo_full_name": "nodejs/node",
    "discussion_comments": [
      {
        "comment_id": "2138793305",
        "repo_full_name": "nodejs/node",
        "pr_number": 58666,
        "pr_file": "src/node_locks.h",
        "discussion_id": "2138793305",
        "commented_code": "@@ -0,0 +1,142 @@\n+#ifndef SRC_NODE_LOCKS_H_\n+#define SRC_NODE_LOCKS_H_\n+\n+#if defined(NODE_WANT_INTERNALS) && NODE_WANT_INTERNALS\n+\n+#include <deque>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+\n+#include \"env.h\"\n+#include \"node_mutex.h\"\n+#include \"v8.h\"\n+\n+namespace node {\n+namespace worker {\n+namespace locks {",
        "comment_created_at": "2025-06-10T21:25:42+00:00",
        "comment_author": "jasnell",
        "comment_body": "Since we are standardized on c++20 now, this can be condensed to `namespace node::worker::locks {`",
        "pr_file_module": null
      },
      {
        "comment_id": "2140904325",
        "repo_full_name": "nodejs/node",
        "pr_number": 58666,
        "pr_file": "src/node_locks.h",
        "discussion_id": "2138793305",
        "commented_code": "@@ -0,0 +1,142 @@\n+#ifndef SRC_NODE_LOCKS_H_\n+#define SRC_NODE_LOCKS_H_\n+\n+#if defined(NODE_WANT_INTERNALS) && NODE_WANT_INTERNALS\n+\n+#include <deque>\n+#include <string>\n+#include <unordered_map>\n+#include <unordered_set>\n+\n+#include \"env.h\"\n+#include \"node_mutex.h\"\n+#include \"v8.h\"\n+\n+namespace node {\n+namespace worker {\n+namespace locks {",
        "comment_created_at": "2025-06-11T19:20:20+00:00",
        "comment_author": "IlyasShabi",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2173765284",
    "pr_number": 58666,
    "pr_file": "src/node_locks.cc",
    "created_at": "2025-06-29T13:40:54+00:00",
    "commented_code": "+#include \"node_locks.h\"\n+\n+#include \"env-inl.h\"\n+#include \"node_errors.h\"\n+#include \"node_external_reference.h\"\n+#include \"node_internals.h\"\n+#include \"util-inl.h\"\n+#include \"v8.h\"\n+\n+namespace node::worker::locks {\n+\n+using node::errors::TryCatchScope;\n+using v8::Array;\n+using v8::Context;\n+using v8::Exception;\n+using v8::External;\n+using v8::Function;\n+using v8::FunctionCallbackInfo;\n+using v8::HandleScope;\n+using v8::Isolate;\n+using v8::Local;\n+using v8::MaybeLocal;\n+using v8::Object;\n+using v8::ObjectTemplate;\n+using v8::Promise;\n+using v8::String;\n+using v8::Value;\n+\n+static constexpr const char* kSharedMode = \"shared\";\n+static constexpr const char* kExclusiveMode = \"exclusive\";\n+static constexpr const char* kLockStolenError = \"LOCK_STOLEN\";\n+\n+static MaybeLocal<Object> CreateLockInfoObject(Isolate* isolate,\n+                                               Local<Context> context,\n+                                               const std::u16string& name,\n+                                               Lock::Mode mode,\n+                                               const std::string& client_id);\n+\n+Lock::Lock(Environment* env,\n+           const std::u16string& name,\n+           Mode mode,\n+           const std::string& client_id,\n+           Local<Promise::Resolver> waiting,\n+           Local<Promise::Resolver> released)\n+    : env_(env), name_(name), mode_(mode), client_id_(client_id) {\n+  waiting_promise_.Reset(env_->isolate(), waiting);\n+  released_promise_.Reset(env_->isolate(), released);\n+}\n+\n+LockRequest::LockRequest(Environment* env,\n+                         Local<Promise::Resolver> waiting,\n+                         Local<Promise::Resolver> released,\n+                         Local<Function> callback,\n+                         const std::u16string& name,\n+                         Lock::Mode mode,\n+                         const std::string& client_id,\n+                         bool steal,\n+                         bool if_available)\n+    : env_(env),\n+      name_(name),\n+      mode_(mode),\n+      client_id_(client_id),\n+      steal_(steal),\n+      if_available_(if_available) {\n+  waiting_promise_.Reset(env_->isolate(), waiting);\n+  released_promise_.Reset(env_->isolate(), released);\n+  callback_.Reset(env_->isolate(), callback);\n+}\n+\n+bool LockManager::IsGrantable(const LockRequest* request) const {\n+  // Steal requests bypass all normal granting rules\n+  if (request->steal()) return true;\n+\n+  auto held_locks_iter = held_locks_.find(request->name());\n+  // No existing locks for this resource name\n+  if (held_locks_iter == held_locks_.end()) return true;\n+\n+  // Exclusive requests cannot coexist with any existing locks\n+  if (request->mode() == Lock::Mode::Exclusive) return false;\n+\n+  // For shared requests, check if any existing lock is exclusive\n+  for (const auto& existing_lock : held_locks_iter->second) {\n+    if (existing_lock->mode() == Lock::Mode::Exclusive) return false;\n+  }\n+  // All existing locks are shared, so this shared request can be granted\n+  return true;\n+}\n+\n+// Called when the user callback promise fulfills\n+static void OnLockCallbackFulfilled(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  Environment* env = Environment::GetCurrent(info);\n+\n+  // Extract the LockHolder from V8 External data\n+  auto* lock_holder =\n+      static_cast<LockHolder*>(info.Data().As<External>()->Value());\n+  std::shared_ptr<Lock> lock = lock_holder->lock();\n+  delete lock_holder;\n+\n+  // Release the lock and continue processing the queue.\n+  LockManager::GetCurrent()->ReleaseLockAndProcessQueue(\n+      env, lock, info[0], false);\n+}\n+\n+// Called when the user callback promise rejects\n+static void OnLockCallbackRejected(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  Environment* env = Environment::GetCurrent(info);\n+\n+  auto* lock_holder =\n+      static_cast<LockHolder*>(info.Data().As<External>()->Value());\n+  std::shared_ptr<Lock> lock = lock_holder->lock();\n+  delete lock_holder;\n+\n+  LockManager::GetCurrent()->ReleaseLockAndProcessQueue(\n+      env, lock, info[0], true);\n+}\n+\n+// Called when the promise returned from the user's callback resolves\n+static void OnIfAvailableFulfill(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  auto* holder = static_cast<v8::Global<v8::Promise::Resolver>*>(\n+      info.Data().As<External>()->Value());\n+  USE(holder->Get(info.GetIsolate())\n+          ->Resolve(info.GetIsolate()->GetCurrentContext(), info[0]));\n+\n+  delete holder;\n+}\n+\n+// Called when the promise returned from the user's callback rejects\n+static void OnIfAvailableReject(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  auto* holder = static_cast<v8::Global<v8::Promise::Resolver>*>(\n+      info.Data().As<External>()->Value());\n+  USE(holder->Get(info.GetIsolate())\n+          ->Reject(info.GetIsolate()->GetCurrentContext(), info[0]));\n+\n+  delete holder;\n+}\n+\n+void LockManager::CleanupStolenLocks(Environment* env) {\n+  std::vector<std::u16string> resources_to_clean;\n+\n+  // Iterate held locks and remove entries that were stolen from other envs.\n+  {\n+    Mutex::ScopedLock scoped_lock(mutex_);\n+\n+    for (auto resource_iter = held_locks_.begin();\n+         resource_iter != held_locks_.end();\n+         ++resource_iter) {\n+      auto& resource_locks = resource_iter->second;\n+      bool has_stolen_from_other_env = false;\n+\n+      // Check if this resource has stolen locks from other environments\n+      for (const auto& lock_ptr : resource_locks) {\n+        if (lock_ptr->is_stolen() && lock_ptr->env() != env) {\n+          has_stolen_from_other_env = true;\n+          break;\n+        }\n+      }\n+\n+      if (has_stolen_from_other_env) {\n+        resources_to_clean.push_back(resource_iter->first);\n+      }\n+    }\n+  }\n+\n+  // Clean up resources\n+  for (const auto& resource_name : resources_to_clean) {\n+    Mutex::ScopedLock scoped_lock(mutex_);\n+\n+    auto resource_iter = held_locks_.find(resource_name);\n+    if (resource_iter != held_locks_.end()) {\n+      auto& resource_locks = resource_iter->second;\n+\n+      // Remove stolen locks from other environments\n+      for (auto lock_iter = resource_locks.begin();\n+           lock_iter != resource_locks.end();) {\n+        if ((*lock_iter)->is_stolen() && (*lock_iter)->env() != env) {\n+          lock_iter = resource_locks.erase(lock_iter);\n+        } else {\n+          ++lock_iter;\n+        }\n+      }\n+\n+      if (resource_locks.empty()) {\n+        held_locks_.erase(resource_iter);\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Web Locks algorithm implementation\n+ * https://w3c.github.io/web-locks/#algorithms\n+ */\n+void LockManager::ProcessQueue(Environment* env) {\n+  Isolate* isolate = env->isolate();\n+  HandleScope handle_scope(isolate);\n+  Local<Context> context = env->context();\n+\n+  // Remove locks that were stolen from this Environment first\n+  CleanupStolenLocks(env);\n+\n+  while (true) {\n+    std::unique_ptr<LockRequest> grantable_request;\n+    std::unique_ptr<LockRequest> if_available_request;\n+    std::unordered_set<Environment*> other_envs_to_wake;\n+\n+    /**\n+     * First pass over pending_queue_\n+     * 1- Build first_seen_for_resource: the oldest pending request\n+     *   for every resource name we encounter\n+     * 2- Decide what to do with each entry:\n+     *     – If it belongs to another Environment, remember that env so we\n+     *       can wake it later\n+     *     – For our Environment, pick one of:\n+     *         * grantable_request  – can be granted now\n+     *         * if_available_request – user asked for ifAvailable and the\n+     *           resource is currently busy\n+     *         * otherwise we skip and keep scanning\n+     */\n+\n+    {\n+      std::unordered_map<std::u16string, LockRequest*> first_seen_for_resource;\n+\n+      Mutex::ScopedLock scoped_lock(mutex_);\n+      for (auto queue_iter = pending_queue_.begin();\n+           queue_iter != pending_queue_.end();\n+           ++queue_iter) {\n+        LockRequest* request = queue_iter->get();\n+\n+        // Collect unique environments to wake up later\n+        if (request->env() != env) {\n+          other_envs_to_wake.insert(request->env());\n+        }\n+\n+        // During a single pass, the first time we see a resource name is the\n+        // earliest pending request\n+        auto& first_for_resource = first_seen_for_resource[request->name()];\n+        if (first_for_resource == nullptr) {\n+          first_for_resource = request;  // Mark as first seen for this resource\n+        }\n+\n+        bool has_earlier_request_for_same_resource =\n+            (first_for_resource != request);\n+\n+        bool should_wait_for_earlier_requests = false;\n+\n+        if (has_earlier_request_for_same_resource) {\n+          // Check if this request is compatible with the earliest pending\n+          // request first_for_resource\n+          if (request->mode() == Lock::Mode::Exclusive ||\n+              first_for_resource->mode() == Lock::Mode::Exclusive) {\n+            // Exclusive locks are incompatible with everything\n+            should_wait_for_earlier_requests = true;\n+          }\n+          // If both are shared, they're compatible and can proceed\n+        }\n+\n+        // Only process requests from the current environment\n+        if (request->env() != env) {\n+          continue;\n+        }\n+\n+        if (should_wait_for_earlier_requests || !IsGrantable(request)) {\n+          if (request->if_available()) {\n+            // ifAvailable request when resource not available: grant with null\n+            if_available_request = std::move(*queue_iter);\n+            pending_queue_.erase(queue_iter);\n+            break;\n+          }\n+          continue;\n+        }\n+\n+        // Found a request that can be granted normally\n+        grantable_request = std::move(*queue_iter);\n+        pending_queue_.erase(queue_iter);\n+        break;\n+      }\n+    }\n+\n+    // Wake each environment only once\n+    for (Environment* target_env : other_envs_to_wake) {\n+      WakeEnvironment(target_env);\n+    }\n+\n+    /**\n+     * 1- We call the user callback immediately with `null` to signal\n+     *    that the lock was not granted - Check wrapCallback function in\n+     * locks.js 2- Depending on what the callback returns we settle the two\n+     *    internal promises\n+     * 3- No lock is added to held_locks_ in this path, so nothing to\n+     *    remove later\n+     */\n+    if (if_available_request) {\n+      Local<Value> null_arg = Null(isolate);\n+      Local<Value> callback_result;\n+      {\n+        TryCatchScope try_catch_scope(env);\n+        if (!if_available_request->callback()\n+                 ->Call(context, Undefined(isolate), 1, &null_arg)\n+                 .ToLocal(&callback_result)) {\n+          USE(if_available_request->waiting_promise()->Reject(\n+              context, try_catch_scope.Exception()));\n+          USE(if_available_request->released_promise()->Reject(\n+              context, try_catch_scope.Exception()));\n+          return;\n+        }\n+      }\n+      if (callback_result->IsPromise()) {\n+        Local<Promise> p = callback_result.As<Promise>();\n+\n+        // Use a Global holder so the resolver survives until the promise\n+        // settles.\n+        auto* fulf_holder = new v8::Global<v8::Promise::Resolver>(\n+            isolate, if_available_request->released_promise());\n+        auto* rej_holder = new v8::Global<v8::Promise::Resolver>(\n+            isolate, if_available_request->released_promise());",
    "repo_full_name": "nodejs/node",
    "discussion_comments": [
      {
        "comment_id": "2173765284",
        "repo_full_name": "nodejs/node",
        "pr_number": 58666,
        "pr_file": "src/node_locks.cc",
        "discussion_id": "2173765284",
        "commented_code": "@@ -0,0 +1,880 @@\n+#include \"node_locks.h\"\n+\n+#include \"env-inl.h\"\n+#include \"node_errors.h\"\n+#include \"node_external_reference.h\"\n+#include \"node_internals.h\"\n+#include \"util-inl.h\"\n+#include \"v8.h\"\n+\n+namespace node::worker::locks {\n+\n+using node::errors::TryCatchScope;\n+using v8::Array;\n+using v8::Context;\n+using v8::Exception;\n+using v8::External;\n+using v8::Function;\n+using v8::FunctionCallbackInfo;\n+using v8::HandleScope;\n+using v8::Isolate;\n+using v8::Local;\n+using v8::MaybeLocal;\n+using v8::Object;\n+using v8::ObjectTemplate;\n+using v8::Promise;\n+using v8::String;\n+using v8::Value;\n+\n+static constexpr const char* kSharedMode = \"shared\";\n+static constexpr const char* kExclusiveMode = \"exclusive\";\n+static constexpr const char* kLockStolenError = \"LOCK_STOLEN\";\n+\n+static MaybeLocal<Object> CreateLockInfoObject(Isolate* isolate,\n+                                               Local<Context> context,\n+                                               const std::u16string& name,\n+                                               Lock::Mode mode,\n+                                               const std::string& client_id);\n+\n+Lock::Lock(Environment* env,\n+           const std::u16string& name,\n+           Mode mode,\n+           const std::string& client_id,\n+           Local<Promise::Resolver> waiting,\n+           Local<Promise::Resolver> released)\n+    : env_(env), name_(name), mode_(mode), client_id_(client_id) {\n+  waiting_promise_.Reset(env_->isolate(), waiting);\n+  released_promise_.Reset(env_->isolate(), released);\n+}\n+\n+LockRequest::LockRequest(Environment* env,\n+                         Local<Promise::Resolver> waiting,\n+                         Local<Promise::Resolver> released,\n+                         Local<Function> callback,\n+                         const std::u16string& name,\n+                         Lock::Mode mode,\n+                         const std::string& client_id,\n+                         bool steal,\n+                         bool if_available)\n+    : env_(env),\n+      name_(name),\n+      mode_(mode),\n+      client_id_(client_id),\n+      steal_(steal),\n+      if_available_(if_available) {\n+  waiting_promise_.Reset(env_->isolate(), waiting);\n+  released_promise_.Reset(env_->isolate(), released);\n+  callback_.Reset(env_->isolate(), callback);\n+}\n+\n+bool LockManager::IsGrantable(const LockRequest* request) const {\n+  // Steal requests bypass all normal granting rules\n+  if (request->steal()) return true;\n+\n+  auto held_locks_iter = held_locks_.find(request->name());\n+  // No existing locks for this resource name\n+  if (held_locks_iter == held_locks_.end()) return true;\n+\n+  // Exclusive requests cannot coexist with any existing locks\n+  if (request->mode() == Lock::Mode::Exclusive) return false;\n+\n+  // For shared requests, check if any existing lock is exclusive\n+  for (const auto& existing_lock : held_locks_iter->second) {\n+    if (existing_lock->mode() == Lock::Mode::Exclusive) return false;\n+  }\n+  // All existing locks are shared, so this shared request can be granted\n+  return true;\n+}\n+\n+// Called when the user callback promise fulfills\n+static void OnLockCallbackFulfilled(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  Environment* env = Environment::GetCurrent(info);\n+\n+  // Extract the LockHolder from V8 External data\n+  auto* lock_holder =\n+      static_cast<LockHolder*>(info.Data().As<External>()->Value());\n+  std::shared_ptr<Lock> lock = lock_holder->lock();\n+  delete lock_holder;\n+\n+  // Release the lock and continue processing the queue.\n+  LockManager::GetCurrent()->ReleaseLockAndProcessQueue(\n+      env, lock, info[0], false);\n+}\n+\n+// Called when the user callback promise rejects\n+static void OnLockCallbackRejected(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  Environment* env = Environment::GetCurrent(info);\n+\n+  auto* lock_holder =\n+      static_cast<LockHolder*>(info.Data().As<External>()->Value());\n+  std::shared_ptr<Lock> lock = lock_holder->lock();\n+  delete lock_holder;\n+\n+  LockManager::GetCurrent()->ReleaseLockAndProcessQueue(\n+      env, lock, info[0], true);\n+}\n+\n+// Called when the promise returned from the user's callback resolves\n+static void OnIfAvailableFulfill(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  auto* holder = static_cast<v8::Global<v8::Promise::Resolver>*>(\n+      info.Data().As<External>()->Value());\n+  USE(holder->Get(info.GetIsolate())\n+          ->Resolve(info.GetIsolate()->GetCurrentContext(), info[0]));\n+\n+  delete holder;\n+}\n+\n+// Called when the promise returned from the user's callback rejects\n+static void OnIfAvailableReject(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  auto* holder = static_cast<v8::Global<v8::Promise::Resolver>*>(\n+      info.Data().As<External>()->Value());\n+  USE(holder->Get(info.GetIsolate())\n+          ->Reject(info.GetIsolate()->GetCurrentContext(), info[0]));\n+\n+  delete holder;\n+}\n+\n+void LockManager::CleanupStolenLocks(Environment* env) {\n+  std::vector<std::u16string> resources_to_clean;\n+\n+  // Iterate held locks and remove entries that were stolen from other envs.\n+  {\n+    Mutex::ScopedLock scoped_lock(mutex_);\n+\n+    for (auto resource_iter = held_locks_.begin();\n+         resource_iter != held_locks_.end();\n+         ++resource_iter) {\n+      auto& resource_locks = resource_iter->second;\n+      bool has_stolen_from_other_env = false;\n+\n+      // Check if this resource has stolen locks from other environments\n+      for (const auto& lock_ptr : resource_locks) {\n+        if (lock_ptr->is_stolen() && lock_ptr->env() != env) {\n+          has_stolen_from_other_env = true;\n+          break;\n+        }\n+      }\n+\n+      if (has_stolen_from_other_env) {\n+        resources_to_clean.push_back(resource_iter->first);\n+      }\n+    }\n+  }\n+\n+  // Clean up resources\n+  for (const auto& resource_name : resources_to_clean) {\n+    Mutex::ScopedLock scoped_lock(mutex_);\n+\n+    auto resource_iter = held_locks_.find(resource_name);\n+    if (resource_iter != held_locks_.end()) {\n+      auto& resource_locks = resource_iter->second;\n+\n+      // Remove stolen locks from other environments\n+      for (auto lock_iter = resource_locks.begin();\n+           lock_iter != resource_locks.end();) {\n+        if ((*lock_iter)->is_stolen() && (*lock_iter)->env() != env) {\n+          lock_iter = resource_locks.erase(lock_iter);\n+        } else {\n+          ++lock_iter;\n+        }\n+      }\n+\n+      if (resource_locks.empty()) {\n+        held_locks_.erase(resource_iter);\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Web Locks algorithm implementation\n+ * https://w3c.github.io/web-locks/#algorithms\n+ */\n+void LockManager::ProcessQueue(Environment* env) {\n+  Isolate* isolate = env->isolate();\n+  HandleScope handle_scope(isolate);\n+  Local<Context> context = env->context();\n+\n+  // Remove locks that were stolen from this Environment first\n+  CleanupStolenLocks(env);\n+\n+  while (true) {\n+    std::unique_ptr<LockRequest> grantable_request;\n+    std::unique_ptr<LockRequest> if_available_request;\n+    std::unordered_set<Environment*> other_envs_to_wake;\n+\n+    /**\n+     * First pass over pending_queue_\n+     * 1- Build first_seen_for_resource: the oldest pending request\n+     *   for every resource name we encounter\n+     * 2- Decide what to do with each entry:\n+     *     – If it belongs to another Environment, remember that env so we\n+     *       can wake it later\n+     *     – For our Environment, pick one of:\n+     *         * grantable_request  – can be granted now\n+     *         * if_available_request – user asked for ifAvailable and the\n+     *           resource is currently busy\n+     *         * otherwise we skip and keep scanning\n+     */\n+\n+    {\n+      std::unordered_map<std::u16string, LockRequest*> first_seen_for_resource;\n+\n+      Mutex::ScopedLock scoped_lock(mutex_);\n+      for (auto queue_iter = pending_queue_.begin();\n+           queue_iter != pending_queue_.end();\n+           ++queue_iter) {\n+        LockRequest* request = queue_iter->get();\n+\n+        // Collect unique environments to wake up later\n+        if (request->env() != env) {\n+          other_envs_to_wake.insert(request->env());\n+        }\n+\n+        // During a single pass, the first time we see a resource name is the\n+        // earliest pending request\n+        auto& first_for_resource = first_seen_for_resource[request->name()];\n+        if (first_for_resource == nullptr) {\n+          first_for_resource = request;  // Mark as first seen for this resource\n+        }\n+\n+        bool has_earlier_request_for_same_resource =\n+            (first_for_resource != request);\n+\n+        bool should_wait_for_earlier_requests = false;\n+\n+        if (has_earlier_request_for_same_resource) {\n+          // Check if this request is compatible with the earliest pending\n+          // request first_for_resource\n+          if (request->mode() == Lock::Mode::Exclusive ||\n+              first_for_resource->mode() == Lock::Mode::Exclusive) {\n+            // Exclusive locks are incompatible with everything\n+            should_wait_for_earlier_requests = true;\n+          }\n+          // If both are shared, they're compatible and can proceed\n+        }\n+\n+        // Only process requests from the current environment\n+        if (request->env() != env) {\n+          continue;\n+        }\n+\n+        if (should_wait_for_earlier_requests || !IsGrantable(request)) {\n+          if (request->if_available()) {\n+            // ifAvailable request when resource not available: grant with null\n+            if_available_request = std::move(*queue_iter);\n+            pending_queue_.erase(queue_iter);\n+            break;\n+          }\n+          continue;\n+        }\n+\n+        // Found a request that can be granted normally\n+        grantable_request = std::move(*queue_iter);\n+        pending_queue_.erase(queue_iter);\n+        break;\n+      }\n+    }\n+\n+    // Wake each environment only once\n+    for (Environment* target_env : other_envs_to_wake) {\n+      WakeEnvironment(target_env);\n+    }\n+\n+    /**\n+     * 1- We call the user callback immediately with `null` to signal\n+     *    that the lock was not granted - Check wrapCallback function in\n+     * locks.js 2- Depending on what the callback returns we settle the two\n+     *    internal promises\n+     * 3- No lock is added to held_locks_ in this path, so nothing to\n+     *    remove later\n+     */\n+    if (if_available_request) {\n+      Local<Value> null_arg = Null(isolate);\n+      Local<Value> callback_result;\n+      {\n+        TryCatchScope try_catch_scope(env);\n+        if (!if_available_request->callback()\n+                 ->Call(context, Undefined(isolate), 1, &null_arg)\n+                 .ToLocal(&callback_result)) {\n+          USE(if_available_request->waiting_promise()->Reject(\n+              context, try_catch_scope.Exception()));\n+          USE(if_available_request->released_promise()->Reject(\n+              context, try_catch_scope.Exception()));\n+          return;\n+        }\n+      }\n+      if (callback_result->IsPromise()) {\n+        Local<Promise> p = callback_result.As<Promise>();\n+\n+        // Use a Global holder so the resolver survives until the promise\n+        // settles.\n+        auto* fulf_holder = new v8::Global<v8::Promise::Resolver>(\n+            isolate, if_available_request->released_promise());\n+        auto* rej_holder = new v8::Global<v8::Promise::Resolver>(\n+            isolate, if_available_request->released_promise());",
        "comment_created_at": "2025-06-29T13:40:54+00:00",
        "comment_author": "jasnell",
        "comment_body": "We really shouldn't use `new` with `v8::Global`. These can just be `v8::Global` and we make use of move semantics. Instead of using `delete` with `v8::Global`, the correct way to clear them is to use their `reset()` method",
        "pr_file_module": null
      },
      {
        "comment_id": "2173893496",
        "repo_full_name": "nodejs/node",
        "pr_number": 58666,
        "pr_file": "src/node_locks.cc",
        "discussion_id": "2173765284",
        "commented_code": "@@ -0,0 +1,880 @@\n+#include \"node_locks.h\"\n+\n+#include \"env-inl.h\"\n+#include \"node_errors.h\"\n+#include \"node_external_reference.h\"\n+#include \"node_internals.h\"\n+#include \"util-inl.h\"\n+#include \"v8.h\"\n+\n+namespace node::worker::locks {\n+\n+using node::errors::TryCatchScope;\n+using v8::Array;\n+using v8::Context;\n+using v8::Exception;\n+using v8::External;\n+using v8::Function;\n+using v8::FunctionCallbackInfo;\n+using v8::HandleScope;\n+using v8::Isolate;\n+using v8::Local;\n+using v8::MaybeLocal;\n+using v8::Object;\n+using v8::ObjectTemplate;\n+using v8::Promise;\n+using v8::String;\n+using v8::Value;\n+\n+static constexpr const char* kSharedMode = \"shared\";\n+static constexpr const char* kExclusiveMode = \"exclusive\";\n+static constexpr const char* kLockStolenError = \"LOCK_STOLEN\";\n+\n+static MaybeLocal<Object> CreateLockInfoObject(Isolate* isolate,\n+                                               Local<Context> context,\n+                                               const std::u16string& name,\n+                                               Lock::Mode mode,\n+                                               const std::string& client_id);\n+\n+Lock::Lock(Environment* env,\n+           const std::u16string& name,\n+           Mode mode,\n+           const std::string& client_id,\n+           Local<Promise::Resolver> waiting,\n+           Local<Promise::Resolver> released)\n+    : env_(env), name_(name), mode_(mode), client_id_(client_id) {\n+  waiting_promise_.Reset(env_->isolate(), waiting);\n+  released_promise_.Reset(env_->isolate(), released);\n+}\n+\n+LockRequest::LockRequest(Environment* env,\n+                         Local<Promise::Resolver> waiting,\n+                         Local<Promise::Resolver> released,\n+                         Local<Function> callback,\n+                         const std::u16string& name,\n+                         Lock::Mode mode,\n+                         const std::string& client_id,\n+                         bool steal,\n+                         bool if_available)\n+    : env_(env),\n+      name_(name),\n+      mode_(mode),\n+      client_id_(client_id),\n+      steal_(steal),\n+      if_available_(if_available) {\n+  waiting_promise_.Reset(env_->isolate(), waiting);\n+  released_promise_.Reset(env_->isolate(), released);\n+  callback_.Reset(env_->isolate(), callback);\n+}\n+\n+bool LockManager::IsGrantable(const LockRequest* request) const {\n+  // Steal requests bypass all normal granting rules\n+  if (request->steal()) return true;\n+\n+  auto held_locks_iter = held_locks_.find(request->name());\n+  // No existing locks for this resource name\n+  if (held_locks_iter == held_locks_.end()) return true;\n+\n+  // Exclusive requests cannot coexist with any existing locks\n+  if (request->mode() == Lock::Mode::Exclusive) return false;\n+\n+  // For shared requests, check if any existing lock is exclusive\n+  for (const auto& existing_lock : held_locks_iter->second) {\n+    if (existing_lock->mode() == Lock::Mode::Exclusive) return false;\n+  }\n+  // All existing locks are shared, so this shared request can be granted\n+  return true;\n+}\n+\n+// Called when the user callback promise fulfills\n+static void OnLockCallbackFulfilled(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  Environment* env = Environment::GetCurrent(info);\n+\n+  // Extract the LockHolder from V8 External data\n+  auto* lock_holder =\n+      static_cast<LockHolder*>(info.Data().As<External>()->Value());\n+  std::shared_ptr<Lock> lock = lock_holder->lock();\n+  delete lock_holder;\n+\n+  // Release the lock and continue processing the queue.\n+  LockManager::GetCurrent()->ReleaseLockAndProcessQueue(\n+      env, lock, info[0], false);\n+}\n+\n+// Called when the user callback promise rejects\n+static void OnLockCallbackRejected(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  Environment* env = Environment::GetCurrent(info);\n+\n+  auto* lock_holder =\n+      static_cast<LockHolder*>(info.Data().As<External>()->Value());\n+  std::shared_ptr<Lock> lock = lock_holder->lock();\n+  delete lock_holder;\n+\n+  LockManager::GetCurrent()->ReleaseLockAndProcessQueue(\n+      env, lock, info[0], true);\n+}\n+\n+// Called when the promise returned from the user's callback resolves\n+static void OnIfAvailableFulfill(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  auto* holder = static_cast<v8::Global<v8::Promise::Resolver>*>(\n+      info.Data().As<External>()->Value());\n+  USE(holder->Get(info.GetIsolate())\n+          ->Resolve(info.GetIsolate()->GetCurrentContext(), info[0]));\n+\n+  delete holder;\n+}\n+\n+// Called when the promise returned from the user's callback rejects\n+static void OnIfAvailableReject(\n+    const v8::FunctionCallbackInfo<v8::Value>& info) {\n+  HandleScope handle_scope(info.GetIsolate());\n+  auto* holder = static_cast<v8::Global<v8::Promise::Resolver>*>(\n+      info.Data().As<External>()->Value());\n+  USE(holder->Get(info.GetIsolate())\n+          ->Reject(info.GetIsolate()->GetCurrentContext(), info[0]));\n+\n+  delete holder;\n+}\n+\n+void LockManager::CleanupStolenLocks(Environment* env) {\n+  std::vector<std::u16string> resources_to_clean;\n+\n+  // Iterate held locks and remove entries that were stolen from other envs.\n+  {\n+    Mutex::ScopedLock scoped_lock(mutex_);\n+\n+    for (auto resource_iter = held_locks_.begin();\n+         resource_iter != held_locks_.end();\n+         ++resource_iter) {\n+      auto& resource_locks = resource_iter->second;\n+      bool has_stolen_from_other_env = false;\n+\n+      // Check if this resource has stolen locks from other environments\n+      for (const auto& lock_ptr : resource_locks) {\n+        if (lock_ptr->is_stolen() && lock_ptr->env() != env) {\n+          has_stolen_from_other_env = true;\n+          break;\n+        }\n+      }\n+\n+      if (has_stolen_from_other_env) {\n+        resources_to_clean.push_back(resource_iter->first);\n+      }\n+    }\n+  }\n+\n+  // Clean up resources\n+  for (const auto& resource_name : resources_to_clean) {\n+    Mutex::ScopedLock scoped_lock(mutex_);\n+\n+    auto resource_iter = held_locks_.find(resource_name);\n+    if (resource_iter != held_locks_.end()) {\n+      auto& resource_locks = resource_iter->second;\n+\n+      // Remove stolen locks from other environments\n+      for (auto lock_iter = resource_locks.begin();\n+           lock_iter != resource_locks.end();) {\n+        if ((*lock_iter)->is_stolen() && (*lock_iter)->env() != env) {\n+          lock_iter = resource_locks.erase(lock_iter);\n+        } else {\n+          ++lock_iter;\n+        }\n+      }\n+\n+      if (resource_locks.empty()) {\n+        held_locks_.erase(resource_iter);\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Web Locks algorithm implementation\n+ * https://w3c.github.io/web-locks/#algorithms\n+ */\n+void LockManager::ProcessQueue(Environment* env) {\n+  Isolate* isolate = env->isolate();\n+  HandleScope handle_scope(isolate);\n+  Local<Context> context = env->context();\n+\n+  // Remove locks that were stolen from this Environment first\n+  CleanupStolenLocks(env);\n+\n+  while (true) {\n+    std::unique_ptr<LockRequest> grantable_request;\n+    std::unique_ptr<LockRequest> if_available_request;\n+    std::unordered_set<Environment*> other_envs_to_wake;\n+\n+    /**\n+     * First pass over pending_queue_\n+     * 1- Build first_seen_for_resource: the oldest pending request\n+     *   for every resource name we encounter\n+     * 2- Decide what to do with each entry:\n+     *     – If it belongs to another Environment, remember that env so we\n+     *       can wake it later\n+     *     – For our Environment, pick one of:\n+     *         * grantable_request  – can be granted now\n+     *         * if_available_request – user asked for ifAvailable and the\n+     *           resource is currently busy\n+     *         * otherwise we skip and keep scanning\n+     */\n+\n+    {\n+      std::unordered_map<std::u16string, LockRequest*> first_seen_for_resource;\n+\n+      Mutex::ScopedLock scoped_lock(mutex_);\n+      for (auto queue_iter = pending_queue_.begin();\n+           queue_iter != pending_queue_.end();\n+           ++queue_iter) {\n+        LockRequest* request = queue_iter->get();\n+\n+        // Collect unique environments to wake up later\n+        if (request->env() != env) {\n+          other_envs_to_wake.insert(request->env());\n+        }\n+\n+        // During a single pass, the first time we see a resource name is the\n+        // earliest pending request\n+        auto& first_for_resource = first_seen_for_resource[request->name()];\n+        if (first_for_resource == nullptr) {\n+          first_for_resource = request;  // Mark as first seen for this resource\n+        }\n+\n+        bool has_earlier_request_for_same_resource =\n+            (first_for_resource != request);\n+\n+        bool should_wait_for_earlier_requests = false;\n+\n+        if (has_earlier_request_for_same_resource) {\n+          // Check if this request is compatible with the earliest pending\n+          // request first_for_resource\n+          if (request->mode() == Lock::Mode::Exclusive ||\n+              first_for_resource->mode() == Lock::Mode::Exclusive) {\n+            // Exclusive locks are incompatible with everything\n+            should_wait_for_earlier_requests = true;\n+          }\n+          // If both are shared, they're compatible and can proceed\n+        }\n+\n+        // Only process requests from the current environment\n+        if (request->env() != env) {\n+          continue;\n+        }\n+\n+        if (should_wait_for_earlier_requests || !IsGrantable(request)) {\n+          if (request->if_available()) {\n+            // ifAvailable request when resource not available: grant with null\n+            if_available_request = std::move(*queue_iter);\n+            pending_queue_.erase(queue_iter);\n+            break;\n+          }\n+          continue;\n+        }\n+\n+        // Found a request that can be granted normally\n+        grantable_request = std::move(*queue_iter);\n+        pending_queue_.erase(queue_iter);\n+        break;\n+      }\n+    }\n+\n+    // Wake each environment only once\n+    for (Environment* target_env : other_envs_to_wake) {\n+      WakeEnvironment(target_env);\n+    }\n+\n+    /**\n+     * 1- We call the user callback immediately with `null` to signal\n+     *    that the lock was not granted - Check wrapCallback function in\n+     * locks.js 2- Depending on what the callback returns we settle the two\n+     *    internal promises\n+     * 3- No lock is added to held_locks_ in this path, so nothing to\n+     *    remove later\n+     */\n+    if (if_available_request) {\n+      Local<Value> null_arg = Null(isolate);\n+      Local<Value> callback_result;\n+      {\n+        TryCatchScope try_catch_scope(env);\n+        if (!if_available_request->callback()\n+                 ->Call(context, Undefined(isolate), 1, &null_arg)\n+                 .ToLocal(&callback_result)) {\n+          USE(if_available_request->waiting_promise()->Reject(\n+              context, try_catch_scope.Exception()));\n+          USE(if_available_request->released_promise()->Reject(\n+              context, try_catch_scope.Exception()));\n+          return;\n+        }\n+      }\n+      if (callback_result->IsPromise()) {\n+        Local<Promise> p = callback_result.As<Promise>();\n+\n+        // Use a Global holder so the resolver survives until the promise\n+        // settles.\n+        auto* fulf_holder = new v8::Global<v8::Promise::Resolver>(\n+            isolate, if_available_request->released_promise());\n+        auto* rej_holder = new v8::Global<v8::Promise::Resolver>(\n+            isolate, if_available_request->released_promise());",
        "comment_created_at": "2025-06-29T19:48:24+00:00",
        "comment_author": "IlyasShabi",
        "comment_body": "I agree this is better than using new with Global. Done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160520306",
    "pr_number": 58782,
    "pr_file": "src/node_dotenv.cc",
    "created_at": "2025-06-22T23:07:06+00:00",
    "commented_code": "arg.starts_with(\"--env-file-if-exists=\");\n   };\n \n+  const auto get_sections = [](const std::string& path) {",
    "repo_full_name": "nodejs/node",
    "discussion_comments": [
      {
        "comment_id": "2160520306",
        "repo_full_name": "nodejs/node",
        "pr_number": 58782,
        "pr_file": "src/node_dotenv.cc",
        "discussion_id": "2160520306",
        "commented_code": "@@ -26,6 +27,33 @@ std::vector<Dotenv::env_file_data> Dotenv::GetDataFromArgs(\n            arg.starts_with(\"--env-file-if-exists=\");\n   };\n \n+  const auto get_sections = [](const std::string& path) {",
        "comment_created_at": "2025-06-22T23:07:06+00:00",
        "comment_author": "anonrig",
        "comment_body": "I recommend always using std::string_view since you'll always know when you'll copy (by calling std::string(val))\r\n\r\n```suggestion\r\n  const auto get_sections = [](std::string_view path) {\r\n```",
        "pr_file_module": null
      }
    ]
  }
]