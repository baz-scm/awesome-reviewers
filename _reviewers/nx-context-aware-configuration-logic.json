[
  {
    "discussion_id": "2429743471",
    "pr_number": 32946,
    "pr_file": "packages/nx/src/native/metrics/collector.rs",
    "created_at": "2025-10-14T16:15:36+00:00",
    "commented_code": "+use std::collections::{HashMap, HashSet};\n+use std::sync::Arc;\n+use std::sync::atomic::{AtomicBool, Ordering};\n+use std::thread::JoinHandle;\n+\n+use anyhow::Result;\n+use dashmap::DashMap;\n+use parking_lot::Mutex;\n+use sysinfo::{Pid, ProcessRefreshKind, System, UpdateKind};\n+\n+use crate::native::metrics::types::{\n+    BatchMetricsSnapshot, BatchRegistration, CollectorConfig, DaemonMetrics,\n+    IndividualTaskRegistration, MetricsError, MetricsUpdate, ProcessMetadata, ProcessMetrics,\n+    ProcessMetricsSnapshot, SystemInfo,\n+};\n+use napi::threadsafe_function::{\n+    ErrorStrategy::CalleeHandled, ThreadsafeFunction, ThreadsafeFunctionCallMode,\n+};\n+\n+type ParentToChildrenMap = std::collections::HashMap<i32, Vec<i32>>;\n+\n+/// Result from metrics collection containing all data needed for cleanup and notification\n+struct MetricsCollectionResult {\n+    /// The collected metrics snapshot\n+    metrics_snapshot: ProcessMetricsSnapshot,\n+    /// New metadata discovered during collection\n+    new_metadata: HashMap<i32, ProcessMetadata>,\n+    /// All PIDs that we collected metrics for\n+    live_pids: HashSet<i32>,\n+}\n+\n+/// Metadata store for process metadata\n+/// Stores latest metadata for each known process\n+pub struct MetadataStore {\n+    /// Metadata store: latest metadata for each known process\n+    pub(crate) metadata_store: DashMap<i32, ProcessMetadata>,\n+}\n+\n+impl MetadataStore {\n+    /// Create a new metadata store\n+    pub fn new() -> Self {\n+        Self {\n+            metadata_store: DashMap::new(),\n+        }\n+    }\n+\n+    /// Check if we already have metadata for a PID\n+    pub fn has_metadata(&self, pid: i32) -> bool {\n+        self.metadata_store.contains_key(&pid)\n+    }\n+\n+    /// Insert metadata for a process\n+    pub fn insert_metadata(&self, pid: i32, metadata: ProcessMetadata) {\n+        self.metadata_store.insert(pid, metadata);\n+    }\n+\n+    /// Clean up metadata for dead processes\n+    pub fn cleanup_dead_metadata(&self, live_pids: &HashSet<i32>) {\n+        self.metadata_store.retain(|pid, _| live_pids.contains(pid));\n+    }\n+}\n+\n+/// Subscriber state for tracking metadata delivery\n+pub(crate) struct SubscriberState {\n+    pub callback: ThreadsafeFunction<MetricsUpdate, CalleeHandled>,\n+    pub needs_full_metadata: bool,\n+}\n+\n+/// High-performance metrics collector for Nx tasks\n+/// Thread-safe and designed for minimal overhead\n+pub struct MetricsCollector {\n+    /// Configuration for the collector\n+    config: CollectorConfig,\n+    /// Individual tasks with one or more anchor processes\n+    /// Using DashMap for concurrent access\n+    individual_tasks: Arc<DashMap<String, IndividualTaskRegistration>>,\n+    /// Batch executions with multiple tasks sharing a worker\n+    /// Using DashMap for concurrent access\n+    batches: Arc<DashMap<String, BatchRegistration>>,\n+    /// Main CLI process PID (set once at initialization, uses Mutex for &self methods)\n+    main_cli_pid: Arc<Mutex<Option<i32>>>,\n+    /// Daemon process PID (can be updated when daemon connects)\n+    daemon_pid: Arc<Mutex<Option<i32>>>,\n+    /// System info instance (shared for process querying)\n+    system: Arc<Mutex<System>>,\n+    /// Subscribers for metrics events (thread-safe)\n+    subscribers: Arc<Mutex<Vec<SubscriberState>>>,\n+    /// Metadata store for process metadata\n+    pub(crate) metadata_store: Arc<MetadataStore>,\n+    /// Collection thread state\n+    collection_thread: Option<JoinHandle<()>>,\n+    /// Atomic flag to control collection thread\n+    should_collect: Arc<AtomicBool>,\n+    /// Atomic flag to indicate if collection is running\n+    is_collecting: Arc<AtomicBool>,\n+}\n+\n+impl MetricsCollector {\n+    /// Create a new MetricsCollector with default configuration\n+    pub fn new() -> Self {\n+        Self::with_config(CollectorConfig::default())\n+    }\n+\n+    /// Create a new MetricsCollector with custom configuration\n+    pub fn with_config(config: CollectorConfig) -> Self {\n+        Self {\n+            config,\n+            individual_tasks: Arc::new(DashMap::new()),\n+            batches: Arc::new(DashMap::new()),\n+            main_cli_pid: Arc::new(Mutex::new(None)),\n+            daemon_pid: Arc::new(Mutex::new(None)),\n+            system: Arc::new(Mutex::new(System::new_all())),\n+            subscribers: Arc::new(Mutex::new(Vec::new())),\n+            metadata_store: Arc::new(MetadataStore::new()),\n+            collection_thread: None,\n+            should_collect: Arc::new(AtomicBool::new(false)),\n+            is_collecting: Arc::new(AtomicBool::new(false)),\n+        }\n+    }\n+\n+    /// Start metrics collection\n+    /// Idempotent - safe to call multiple times, subsequent calls are no-ops\n+    pub fn start_collection(&mut self) -> Result<(), MetricsError> {\n+        // Atomically check if collection is not running and start it\n+        // If already running, this is a no-op (idempotent behavior)\n+        if self\n+            .is_collecting\n+            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)\n+            .is_err()\n+        {\n+            // Already started - this is fine, just return success (idempotent)\n+            return Ok(());\n+        }\n+\n+        self.should_collect.store(true, Ordering::Release);\n+\n+        // Clone necessary Arc references for the thread\n+        let should_collect = Arc::clone(&self.should_collect);\n+        let is_collecting = Arc::clone(&self.is_collecting);\n+        let individual_tasks = Arc::clone(&self.individual_tasks);\n+        let batches = Arc::clone(&self.batches);\n+        let main_cli_pid = Arc::clone(&self.main_cli_pid);\n+        let daemon_pid = Arc::clone(&self.daemon_pid);\n+        let system = Arc::clone(&self.system);\n+        let config = self.config.clone();\n+        let subscribers = Arc::clone(&self.subscribers);\n+        let metadata_store = Arc::clone(&self.metadata_store);\n+\n+        // Spawn the collection thread\n+        let collection_thread = std::thread::Builder::new()\n+            .name(\"nx-metrics-collector\".to_string())\n+            .spawn(move || {\n+                Self::collection_loop(\n+                    should_collect,\n+                    is_collecting,\n+                    individual_tasks,\n+                    batches,\n+                    main_cli_pid,\n+                    daemon_pid,\n+                    system,\n+                    config,\n+                    subscribers,\n+                    metadata_store,\n+                );\n+            })\n+            .map_err(|e| {\n+                // Failed to spawn thread - reset flag so future attempts can try again\n+                self.is_collecting.store(false, Ordering::Release);\n+                self.should_collect.store(false, Ordering::Release);\n+                MetricsError::SystemError(format!(\"Failed to start collection thread: {}\", e))\n+            })?;\n+\n+        self.collection_thread = Some(collection_thread);\n+\n+        Ok(())\n+    }\n+\n+    /// Stop metrics collection\n+    pub fn stop_collection(&mut self) -> Result<(), MetricsError> {\n+        if !self.is_collecting.load(Ordering::Acquire) {\n+            return Err(MetricsError::CollectionNotStarted);\n+        }\n+\n+        // Signal the collection thread to stop\n+        self.should_collect.store(false, Ordering::Release);\n+\n+        // Wait for the collection thread to finish\n+        if let Some(thread) = self.collection_thread.take() {\n+            if let Err(e) = thread.join() {\n+                tracing::error!(\"Collection thread panicked: {:?}\", e);\n+            }\n+        }\n+\n+        self.is_collecting.store(false, Ordering::Release);\n+        Ok(())\n+    }\n+\n+    /// Check if collection is currently running\n+    pub fn is_collecting(&self) -> bool {\n+        self.is_collecting.load(Ordering::Acquire)\n+    }\n+\n+    /// Get system information (CPU cores and total memory)\n+    pub fn get_system_info(&self) -> SystemInfo {\n+        let sys = self.system.lock();\n+        SystemInfo {\n+            cpu_cores: sys.cpus().len() as u32,\n+            total_memory: sys.total_memory() as i64,\n+        }\n+    }\n+\n+    /// Register the main CLI process\n+    /// Idempotent - safe to call multiple times with same PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_main_cli_process(&self, pid: i32) {\n+        let mut cli_pid = self.main_cli_pid.lock();\n+        *cli_pid = Some(pid);\n+    }\n+\n+    /// Register the daemon process\n+    /// Idempotent - safe to call multiple times, overwrites with new PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_daemon_process(&self, pid: i32) {\n+        let mut daemon_pid = self.daemon_pid.lock();\n+        *daemon_pid = Some(pid);\n+    }\n+\n+    /// Register a process for an individual task (called when process spawns)\n+    /// Can be called multiple times with same task_id to add more processes\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_task_process(&self, task_id: String, pid: i32) {\n+        self.individual_tasks\n+            .entry(task_id.clone())\n+            .or_insert_with(|| IndividualTaskRegistration::new(task_id))\n+            .anchor_pids\n+            .insert(pid);\n+\n+        // Establish baseline immediately for this task process\n+        // This ensures accurate CPU data from the first collection cycle after spawn\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// Register a batch with multiple tasks sharing a worker\n+    pub fn register_batch(&self, batch_id: String, task_ids: Vec<String>, pid: i32) {\n+        self.batches.insert(\n+            batch_id.clone(),\n+            BatchRegistration::new(batch_id, task_ids, pid),\n+        );\n+\n+        // Establish baseline immediately for the batch worker\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// The main collection loop that runs in a separate thread\n+    /// Discovers current metrics and directly notifies subscribers (true push-based)\n+    fn collection_loop(\n+        should_collect: Arc<AtomicBool>,\n+        is_collecting: Arc<AtomicBool>,\n+        individual_tasks: Arc<DashMap<String, IndividualTaskRegistration>>,\n+        batches: Arc<DashMap<String, BatchRegistration>>,\n+        main_cli_pid: Arc<Mutex<Option<i32>>>,\n+        daemon_pid: Arc<Mutex<Option<i32>>>,\n+        system: Arc<Mutex<System>>,\n+        config: CollectorConfig,\n+        subscribers: Arc<Mutex<Vec<SubscriberState>>>,\n+        metadata_store: Arc<MetadataStore>,\n+    ) {\n+        let interval = std::time::Duration::from_millis(config.collection_interval_ms);\n+\n+        while should_collect.load(Ordering::Acquire) {\n+            // Collect current metrics\n+            match Self::discover_and_collect_current_metrics(\n+                &system,\n+                &individual_tasks,\n+                &batches,\n+                &main_cli_pid,\n+                &daemon_pid,\n+                &metadata_store,\n+            ) {\n+                Ok(result) => {\n+                    let snapshot = result.metrics_snapshot;\n+                    let new_metadata_map = result.new_metadata;\n+\n+                    // Update metadata store with new processes\n+                    for (pid, metadata) in &new_metadata_map {\n+                        metadata_store.insert_metadata(*pid, metadata.clone());\n+                    }\n+\n+                    // Clean up dead metadata and task registrations\n+                    metadata_store.cleanup_dead_metadata(&result.live_pids);\n+\n+                    // Wrap data in Arc for efficient sharing across subscribers\n+                    let snapshot = Arc::new(snapshot);\n+\n+                    // Build notifications inside lock, then release before calling JS\n+                    let notifications = {\n+                        let mut subscribers = subscribers.lock();\n+\n+                        // Pre-compute metadata variants\n+                        let needs_full = subscribers.iter().any(|s| s.needs_full_metadata);\n+\n+                        let full_metadata = if needs_full {\n+                            Some(\n+                                metadata_store\n+                                    .metadata_store\n+                                    .iter()\n+                                    .map(|entry| (entry.key().to_string(), entry.value().clone()))\n+                                    .collect::<std::collections::HashMap<_, _>>(),\n+                            )\n+                        } else {\n+                            None\n+                        };\n+                        let full_metadata = full_metadata.map(Arc::new);\n+\n+                        let new_metadata = if !new_metadata_map.is_empty() {\n+                            Some(Arc::new(\n+                                new_metadata_map\n+                                    .into_iter()\n+                                    .map(|(pid, meta)| (pid.to_string(), meta))\n+                                    .collect::<HashMap<_, _>>(),\n+                            ))\n+                        } else {\n+                            None\n+                        };\n+\n+                        // Build notification list with Arc clones (cheap - just pointer copies)\n+                        subscribers\n+                            .iter_mut()\n+                            .map(|state| {\n+                                let metadata = if state.needs_full_metadata {\n+                                    state.needs_full_metadata = false;\n+                                    full_metadata.clone()\n+                                } else {\n+                                    new_metadata.clone()\n+                                };\n+\n+                                let update = MetricsUpdate {\n+                                    metrics: Arc::clone(&snapshot),\n+                                    metadata,\n+                                };\n+\n+                                // Clone ThreadsafeFunction (cheap - internally Arc-based)\n+                                (state.callback.clone(), update)\n+                            })\n+                            .collect::<Vec<_>>()\n+                    }; // Lock released here!\n+\n+                    // Notify subscribers WITHOUT holding lock (fixes RUST-8)\n+                    for (callback, update) in notifications {\n+                        let status =\n+                            callback.call(Ok(update), ThreadsafeFunctionCallMode::NonBlocking);\n+                        if !matches!(status, napi::Status::Ok) {\n+                            tracing::debug!(\"Failed to notify subscriber: {:?}\", status);\n+                        }\n+                    }\n+                }\n+                Err(e) => {\n+                    tracing::debug!(\"Metrics collection error: {}\", e);\n+                }\n+            }\n+\n+            // Sleep in small chunks so thread can exit quickly on shutdown\n+            // This allows the thread to respond to shutdown signals within 50ms\n+            // instead of waiting for the full collection interval (500ms default)\n+            let wake_interval = std::time::Duration::from_millis(50);\n+            let sleep_iterations = (interval.as_millis() / 50).max(1) as usize;\n+\n+            for _ in 0..sleep_iterations {\n+                if !should_collect.load(Ordering::Acquire) {\n+                    break; // Exit early if shutdown requested\n+                }\n+                std::thread::sleep(wake_interval);\n+            }\n+        }\n+\n+        is_collecting.store(false, Ordering::Release);\n+    }\n+\n+    fn build_parent_child_map(sys: &System) -> ParentToChildrenMap {\n+        let capacity = sys.processes().len();\n+        sys.processes()\n+            .iter()\n+            .filter_map(|(pid, process)| {\n+                process\n+                    .parent()\n+                    .map(|p| (p.as_u32() as i32, pid.as_u32() as i32))\n+            })\n+            .fold(\n+                std::collections::HashMap::with_capacity(capacity),\n+                |mut map, (parent, child)| {\n+                    map.entry(parent).or_insert_with(Vec::new).push(child);\n+                    map\n+                },\n+            )\n+    }\n+\n+    fn collect_tree_pids_from_map(children_map: &ParentToChildrenMap, root_pid: i32) -> Vec<i32> {\n+        let mut all_pids = Vec::new();\n+        let mut to_process = vec![root_pid];\n+\n+        while let Some(current_pid) = to_process.pop() {\n+            all_pids.push(current_pid);\n+\n+            if let Some(children) = children_map.get(&current_pid) {\n+                to_process.extend(children);\n+            }\n+        }\n+\n+        all_pids\n+    }\n+\n+    /// Collect process metrics and conditionally populate metadata if not cached\n+    /// Single-pass collection that checks metadata cache to avoid unnecessary string allocations\n+    /// Returns None if process no longer exists\n+    fn collect_process_info(\n+        sys: &System,\n+        pid: i32,\n+        metadata_store: &MetadataStore,\n+        new_metadata: &mut HashMap<i32, ProcessMetadata>,\n+    ) -> Option<ProcessMetrics> {\n+        let process = sys.process(Pid::from(pid as usize))?;\n+\n+        // Always collect metrics (cheap - 3 numbers)\n+        let metrics = ProcessMetrics {\n+            pid,\n+            cpu: process.cpu_usage() as f64,\n+            memory: process.memory() as i64,\n+        };\n+\n+        // Conditionally collect metadata only if not cached (expensive - 4 strings)\n+        if !metadata_store.has_metadata(pid) {\n+            let metadata = ProcessMetadata {\n+                ppid: process.parent().map(|p| p.as_u32() as i32).unwrap_or(0),\n+                name: process.name().to_string_lossy().into_owned(),\n+                command: process\n+                    .cmd()\n+                    .iter()\n+                    .map(|s| s.to_string_lossy())\n+                    .collect::<Vec<_>>()\n+                    .join(\" \"),\n+                exe_path: process\n+                    .exe()\n+                    .map(|p| p.to_string_lossy().into_owned())\n+                    .unwrap_or_default(),\n+                cwd: process\n+                    .cwd()\n+                    .map(|p| p.to_string_lossy().into_owned())\n+                    .unwrap_or_default(),\n+            };\n+            new_metadata.insert(pid, metadata);\n+        }\n+\n+        Some(metrics)\n+    }\n+\n+    /// Discover all current processes and collect their metrics\n+    /// Single system refresh for accurate timestamp and optimal performance\n+    /// Returns ephemeral data structure for immediate emission\n+    fn discover_and_collect_current_metrics(\n+        system: &Mutex<System>,\n+        individual_tasks: &DashMap<String, IndividualTaskRegistration>,\n+        batches: &DashMap<String, BatchRegistration>,\n+        main_cli_pid: &Mutex<Option<i32>>,\n+        daemon_pid: &Mutex<Option<i32>>,\n+        metadata_store: &MetadataStore,\n+    ) -> Result<MetricsCollectionResult, Box<dyn std::error::Error>> {\n+        // Capture timestamp FIRST for accuracy\n+        let timestamp = std::time::SystemTime::now()\n+            .duration_since(std::time::UNIX_EPOCH)\n+            .unwrap_or_else(|e| {\n+                tracing::warn!(\"System time before UNIX epoch: {}\", e);\n+                std::time::Duration::ZERO\n+            })\n+            .as_millis() as i64;\n+\n+        // Single system refresh for ALL processes (one scan, all data)\n+        // Remove dead processes to prevent stale metrics from terminated processes\n+        let mut sys = system.lock();\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::All,\n+            true, // remove_dead_processes\n+            ProcessRefreshKind::nothing()\n+                .with_memory()\n+                .with_cpu()\n+                .with_exe(UpdateKind::OnlyIfNotSet)\n+                .with_cmd(UpdateKind::OnlyIfNotSet)\n+                .with_cwd(UpdateKind::OnlyIfNotSet),\n+        );\n+\n+        let children_map = Self::build_parent_child_map(&sys);\n+\n+        // Clean up dead individual tasks - retain only tasks with at least one live anchor\n+        individual_tasks.retain(|_, task_reg| {\n+            task_reg\n+                .anchor_pids\n+                .iter()\n+                .any(|pid| sys.process(Pid::from(*pid as usize)).is_some())\n+        });\n+\n+        // Clean up dead batches - retain only batches with live anchor\n+        batches.retain(|_, batch_reg| {\n+            sys.process(Pid::from(batch_reg.anchor_pid as usize))\n+                .is_some()\n+        });\n+\n+        // Pre-allocate HashSet for tracking all live PIDs to reduce allocations\n+        let mut live_pids = HashSet::with_capacity(\n+            1 + // main_cli\n+            10 + // daemon + estimated subprocesses\n+            individual_tasks.len() * 5 + // individual tasks \u00d7 avg processes per task\n+            batches.len() * 5, // batch tasks \u00d7 avg processes per batch\n+        );\n+\n+        // Track new metadata discovered during collection (conditionally populated)\n+        let mut new_metadata = HashMap::new();\n+\n+        // Collect main CLI process\n+        let main_cli_metrics = {\n+            let cli_pid = main_cli_pid.lock();\n+            if let Some(pid) = *cli_pid {",
    "repo_full_name": "nrwl/nx",
    "discussion_comments": [
      {
        "comment_id": "2429743471",
        "repo_full_name": "nrwl/nx",
        "pr_number": 32946,
        "pr_file": "packages/nx/src/native/metrics/collector.rs",
        "discussion_id": "2429743471",
        "commented_code": "@@ -0,0 +1,663 @@\n+use std::collections::{HashMap, HashSet};\n+use std::sync::Arc;\n+use std::sync::atomic::{AtomicBool, Ordering};\n+use std::thread::JoinHandle;\n+\n+use anyhow::Result;\n+use dashmap::DashMap;\n+use parking_lot::Mutex;\n+use sysinfo::{Pid, ProcessRefreshKind, System, UpdateKind};\n+\n+use crate::native::metrics::types::{\n+    BatchMetricsSnapshot, BatchRegistration, CollectorConfig, DaemonMetrics,\n+    IndividualTaskRegistration, MetricsError, MetricsUpdate, ProcessMetadata, ProcessMetrics,\n+    ProcessMetricsSnapshot, SystemInfo,\n+};\n+use napi::threadsafe_function::{\n+    ErrorStrategy::CalleeHandled, ThreadsafeFunction, ThreadsafeFunctionCallMode,\n+};\n+\n+type ParentToChildrenMap = std::collections::HashMap<i32, Vec<i32>>;\n+\n+/// Result from metrics collection containing all data needed for cleanup and notification\n+struct MetricsCollectionResult {\n+    /// The collected metrics snapshot\n+    metrics_snapshot: ProcessMetricsSnapshot,\n+    /// New metadata discovered during collection\n+    new_metadata: HashMap<i32, ProcessMetadata>,\n+    /// All PIDs that we collected metrics for\n+    live_pids: HashSet<i32>,\n+}\n+\n+/// Metadata store for process metadata\n+/// Stores latest metadata for each known process\n+pub struct MetadataStore {\n+    /// Metadata store: latest metadata for each known process\n+    pub(crate) metadata_store: DashMap<i32, ProcessMetadata>,\n+}\n+\n+impl MetadataStore {\n+    /// Create a new metadata store\n+    pub fn new() -> Self {\n+        Self {\n+            metadata_store: DashMap::new(),\n+        }\n+    }\n+\n+    /// Check if we already have metadata for a PID\n+    pub fn has_metadata(&self, pid: i32) -> bool {\n+        self.metadata_store.contains_key(&pid)\n+    }\n+\n+    /// Insert metadata for a process\n+    pub fn insert_metadata(&self, pid: i32, metadata: ProcessMetadata) {\n+        self.metadata_store.insert(pid, metadata);\n+    }\n+\n+    /// Clean up metadata for dead processes\n+    pub fn cleanup_dead_metadata(&self, live_pids: &HashSet<i32>) {\n+        self.metadata_store.retain(|pid, _| live_pids.contains(pid));\n+    }\n+}\n+\n+/// Subscriber state for tracking metadata delivery\n+pub(crate) struct SubscriberState {\n+    pub callback: ThreadsafeFunction<MetricsUpdate, CalleeHandled>,\n+    pub needs_full_metadata: bool,\n+}\n+\n+/// High-performance metrics collector for Nx tasks\n+/// Thread-safe and designed for minimal overhead\n+pub struct MetricsCollector {\n+    /// Configuration for the collector\n+    config: CollectorConfig,\n+    /// Individual tasks with one or more anchor processes\n+    /// Using DashMap for concurrent access\n+    individual_tasks: Arc<DashMap<String, IndividualTaskRegistration>>,\n+    /// Batch executions with multiple tasks sharing a worker\n+    /// Using DashMap for concurrent access\n+    batches: Arc<DashMap<String, BatchRegistration>>,\n+    /// Main CLI process PID (set once at initialization, uses Mutex for &self methods)\n+    main_cli_pid: Arc<Mutex<Option<i32>>>,\n+    /// Daemon process PID (can be updated when daemon connects)\n+    daemon_pid: Arc<Mutex<Option<i32>>>,\n+    /// System info instance (shared for process querying)\n+    system: Arc<Mutex<System>>,\n+    /// Subscribers for metrics events (thread-safe)\n+    subscribers: Arc<Mutex<Vec<SubscriberState>>>,\n+    /// Metadata store for process metadata\n+    pub(crate) metadata_store: Arc<MetadataStore>,\n+    /// Collection thread state\n+    collection_thread: Option<JoinHandle<()>>,\n+    /// Atomic flag to control collection thread\n+    should_collect: Arc<AtomicBool>,\n+    /// Atomic flag to indicate if collection is running\n+    is_collecting: Arc<AtomicBool>,\n+}\n+\n+impl MetricsCollector {\n+    /// Create a new MetricsCollector with default configuration\n+    pub fn new() -> Self {\n+        Self::with_config(CollectorConfig::default())\n+    }\n+\n+    /// Create a new MetricsCollector with custom configuration\n+    pub fn with_config(config: CollectorConfig) -> Self {\n+        Self {\n+            config,\n+            individual_tasks: Arc::new(DashMap::new()),\n+            batches: Arc::new(DashMap::new()),\n+            main_cli_pid: Arc::new(Mutex::new(None)),\n+            daemon_pid: Arc::new(Mutex::new(None)),\n+            system: Arc::new(Mutex::new(System::new_all())),\n+            subscribers: Arc::new(Mutex::new(Vec::new())),\n+            metadata_store: Arc::new(MetadataStore::new()),\n+            collection_thread: None,\n+            should_collect: Arc::new(AtomicBool::new(false)),\n+            is_collecting: Arc::new(AtomicBool::new(false)),\n+        }\n+    }\n+\n+    /// Start metrics collection\n+    /// Idempotent - safe to call multiple times, subsequent calls are no-ops\n+    pub fn start_collection(&mut self) -> Result<(), MetricsError> {\n+        // Atomically check if collection is not running and start it\n+        // If already running, this is a no-op (idempotent behavior)\n+        if self\n+            .is_collecting\n+            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)\n+            .is_err()\n+        {\n+            // Already started - this is fine, just return success (idempotent)\n+            return Ok(());\n+        }\n+\n+        self.should_collect.store(true, Ordering::Release);\n+\n+        // Clone necessary Arc references for the thread\n+        let should_collect = Arc::clone(&self.should_collect);\n+        let is_collecting = Arc::clone(&self.is_collecting);\n+        let individual_tasks = Arc::clone(&self.individual_tasks);\n+        let batches = Arc::clone(&self.batches);\n+        let main_cli_pid = Arc::clone(&self.main_cli_pid);\n+        let daemon_pid = Arc::clone(&self.daemon_pid);\n+        let system = Arc::clone(&self.system);\n+        let config = self.config.clone();\n+        let subscribers = Arc::clone(&self.subscribers);\n+        let metadata_store = Arc::clone(&self.metadata_store);\n+\n+        // Spawn the collection thread\n+        let collection_thread = std::thread::Builder::new()\n+            .name(\"nx-metrics-collector\".to_string())\n+            .spawn(move || {\n+                Self::collection_loop(\n+                    should_collect,\n+                    is_collecting,\n+                    individual_tasks,\n+                    batches,\n+                    main_cli_pid,\n+                    daemon_pid,\n+                    system,\n+                    config,\n+                    subscribers,\n+                    metadata_store,\n+                );\n+            })\n+            .map_err(|e| {\n+                // Failed to spawn thread - reset flag so future attempts can try again\n+                self.is_collecting.store(false, Ordering::Release);\n+                self.should_collect.store(false, Ordering::Release);\n+                MetricsError::SystemError(format!(\"Failed to start collection thread: {}\", e))\n+            })?;\n+\n+        self.collection_thread = Some(collection_thread);\n+\n+        Ok(())\n+    }\n+\n+    /// Stop metrics collection\n+    pub fn stop_collection(&mut self) -> Result<(), MetricsError> {\n+        if !self.is_collecting.load(Ordering::Acquire) {\n+            return Err(MetricsError::CollectionNotStarted);\n+        }\n+\n+        // Signal the collection thread to stop\n+        self.should_collect.store(false, Ordering::Release);\n+\n+        // Wait for the collection thread to finish\n+        if let Some(thread) = self.collection_thread.take() {\n+            if let Err(e) = thread.join() {\n+                tracing::error!(\"Collection thread panicked: {:?}\", e);\n+            }\n+        }\n+\n+        self.is_collecting.store(false, Ordering::Release);\n+        Ok(())\n+    }\n+\n+    /// Check if collection is currently running\n+    pub fn is_collecting(&self) -> bool {\n+        self.is_collecting.load(Ordering::Acquire)\n+    }\n+\n+    /// Get system information (CPU cores and total memory)\n+    pub fn get_system_info(&self) -> SystemInfo {\n+        let sys = self.system.lock();\n+        SystemInfo {\n+            cpu_cores: sys.cpus().len() as u32,\n+            total_memory: sys.total_memory() as i64,\n+        }\n+    }\n+\n+    /// Register the main CLI process\n+    /// Idempotent - safe to call multiple times with same PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_main_cli_process(&self, pid: i32) {\n+        let mut cli_pid = self.main_cli_pid.lock();\n+        *cli_pid = Some(pid);\n+    }\n+\n+    /// Register the daemon process\n+    /// Idempotent - safe to call multiple times, overwrites with new PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_daemon_process(&self, pid: i32) {\n+        let mut daemon_pid = self.daemon_pid.lock();\n+        *daemon_pid = Some(pid);\n+    }\n+\n+    /// Register a process for an individual task (called when process spawns)\n+    /// Can be called multiple times with same task_id to add more processes\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_task_process(&self, task_id: String, pid: i32) {\n+        self.individual_tasks\n+            .entry(task_id.clone())\n+            .or_insert_with(|| IndividualTaskRegistration::new(task_id))\n+            .anchor_pids\n+            .insert(pid);\n+\n+        // Establish baseline immediately for this task process\n+        // This ensures accurate CPU data from the first collection cycle after spawn\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// Register a batch with multiple tasks sharing a worker\n+    pub fn register_batch(&self, batch_id: String, task_ids: Vec<String>, pid: i32) {\n+        self.batches.insert(\n+            batch_id.clone(),\n+            BatchRegistration::new(batch_id, task_ids, pid),\n+        );\n+\n+        // Establish baseline immediately for the batch worker\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// The main collection loop that runs in a separate thread\n+    /// Discovers current metrics and directly notifies subscribers (true push-based)\n+    fn collection_loop(\n+        should_collect: Arc<AtomicBool>,\n+        is_collecting: Arc<AtomicBool>,\n+        individual_tasks: Arc<DashMap<String, IndividualTaskRegistration>>,\n+        batches: Arc<DashMap<String, BatchRegistration>>,\n+        main_cli_pid: Arc<Mutex<Option<i32>>>,\n+        daemon_pid: Arc<Mutex<Option<i32>>>,\n+        system: Arc<Mutex<System>>,\n+        config: CollectorConfig,\n+        subscribers: Arc<Mutex<Vec<SubscriberState>>>,\n+        metadata_store: Arc<MetadataStore>,\n+    ) {\n+        let interval = std::time::Duration::from_millis(config.collection_interval_ms);\n+\n+        while should_collect.load(Ordering::Acquire) {\n+            // Collect current metrics\n+            match Self::discover_and_collect_current_metrics(\n+                &system,\n+                &individual_tasks,\n+                &batches,\n+                &main_cli_pid,\n+                &daemon_pid,\n+                &metadata_store,\n+            ) {\n+                Ok(result) => {\n+                    let snapshot = result.metrics_snapshot;\n+                    let new_metadata_map = result.new_metadata;\n+\n+                    // Update metadata store with new processes\n+                    for (pid, metadata) in &new_metadata_map {\n+                        metadata_store.insert_metadata(*pid, metadata.clone());\n+                    }\n+\n+                    // Clean up dead metadata and task registrations\n+                    metadata_store.cleanup_dead_metadata(&result.live_pids);\n+\n+                    // Wrap data in Arc for efficient sharing across subscribers\n+                    let snapshot = Arc::new(snapshot);\n+\n+                    // Build notifications inside lock, then release before calling JS\n+                    let notifications = {\n+                        let mut subscribers = subscribers.lock();\n+\n+                        // Pre-compute metadata variants\n+                        let needs_full = subscribers.iter().any(|s| s.needs_full_metadata);\n+\n+                        let full_metadata = if needs_full {\n+                            Some(\n+                                metadata_store\n+                                    .metadata_store\n+                                    .iter()\n+                                    .map(|entry| (entry.key().to_string(), entry.value().clone()))\n+                                    .collect::<std::collections::HashMap<_, _>>(),\n+                            )\n+                        } else {\n+                            None\n+                        };\n+                        let full_metadata = full_metadata.map(Arc::new);\n+\n+                        let new_metadata = if !new_metadata_map.is_empty() {\n+                            Some(Arc::new(\n+                                new_metadata_map\n+                                    .into_iter()\n+                                    .map(|(pid, meta)| (pid.to_string(), meta))\n+                                    .collect::<HashMap<_, _>>(),\n+                            ))\n+                        } else {\n+                            None\n+                        };\n+\n+                        // Build notification list with Arc clones (cheap - just pointer copies)\n+                        subscribers\n+                            .iter_mut()\n+                            .map(|state| {\n+                                let metadata = if state.needs_full_metadata {\n+                                    state.needs_full_metadata = false;\n+                                    full_metadata.clone()\n+                                } else {\n+                                    new_metadata.clone()\n+                                };\n+\n+                                let update = MetricsUpdate {\n+                                    metrics: Arc::clone(&snapshot),\n+                                    metadata,\n+                                };\n+\n+                                // Clone ThreadsafeFunction (cheap - internally Arc-based)\n+                                (state.callback.clone(), update)\n+                            })\n+                            .collect::<Vec<_>>()\n+                    }; // Lock released here!\n+\n+                    // Notify subscribers WITHOUT holding lock (fixes RUST-8)\n+                    for (callback, update) in notifications {\n+                        let status =\n+                            callback.call(Ok(update), ThreadsafeFunctionCallMode::NonBlocking);\n+                        if !matches!(status, napi::Status::Ok) {\n+                            tracing::debug!(\"Failed to notify subscriber: {:?}\", status);\n+                        }\n+                    }\n+                }\n+                Err(e) => {\n+                    tracing::debug!(\"Metrics collection error: {}\", e);\n+                }\n+            }\n+\n+            // Sleep in small chunks so thread can exit quickly on shutdown\n+            // This allows the thread to respond to shutdown signals within 50ms\n+            // instead of waiting for the full collection interval (500ms default)\n+            let wake_interval = std::time::Duration::from_millis(50);\n+            let sleep_iterations = (interval.as_millis() / 50).max(1) as usize;\n+\n+            for _ in 0..sleep_iterations {\n+                if !should_collect.load(Ordering::Acquire) {\n+                    break; // Exit early if shutdown requested\n+                }\n+                std::thread::sleep(wake_interval);\n+            }\n+        }\n+\n+        is_collecting.store(false, Ordering::Release);\n+    }\n+\n+    fn build_parent_child_map(sys: &System) -> ParentToChildrenMap {\n+        let capacity = sys.processes().len();\n+        sys.processes()\n+            .iter()\n+            .filter_map(|(pid, process)| {\n+                process\n+                    .parent()\n+                    .map(|p| (p.as_u32() as i32, pid.as_u32() as i32))\n+            })\n+            .fold(\n+                std::collections::HashMap::with_capacity(capacity),\n+                |mut map, (parent, child)| {\n+                    map.entry(parent).or_insert_with(Vec::new).push(child);\n+                    map\n+                },\n+            )\n+    }\n+\n+    fn collect_tree_pids_from_map(children_map: &ParentToChildrenMap, root_pid: i32) -> Vec<i32> {\n+        let mut all_pids = Vec::new();\n+        let mut to_process = vec![root_pid];\n+\n+        while let Some(current_pid) = to_process.pop() {\n+            all_pids.push(current_pid);\n+\n+            if let Some(children) = children_map.get(&current_pid) {\n+                to_process.extend(children);\n+            }\n+        }\n+\n+        all_pids\n+    }\n+\n+    /// Collect process metrics and conditionally populate metadata if not cached\n+    /// Single-pass collection that checks metadata cache to avoid unnecessary string allocations\n+    /// Returns None if process no longer exists\n+    fn collect_process_info(\n+        sys: &System,\n+        pid: i32,\n+        metadata_store: &MetadataStore,\n+        new_metadata: &mut HashMap<i32, ProcessMetadata>,\n+    ) -> Option<ProcessMetrics> {\n+        let process = sys.process(Pid::from(pid as usize))?;\n+\n+        // Always collect metrics (cheap - 3 numbers)\n+        let metrics = ProcessMetrics {\n+            pid,\n+            cpu: process.cpu_usage() as f64,\n+            memory: process.memory() as i64,\n+        };\n+\n+        // Conditionally collect metadata only if not cached (expensive - 4 strings)\n+        if !metadata_store.has_metadata(pid) {\n+            let metadata = ProcessMetadata {\n+                ppid: process.parent().map(|p| p.as_u32() as i32).unwrap_or(0),\n+                name: process.name().to_string_lossy().into_owned(),\n+                command: process\n+                    .cmd()\n+                    .iter()\n+                    .map(|s| s.to_string_lossy())\n+                    .collect::<Vec<_>>()\n+                    .join(\" \"),\n+                exe_path: process\n+                    .exe()\n+                    .map(|p| p.to_string_lossy().into_owned())\n+                    .unwrap_or_default(),\n+                cwd: process\n+                    .cwd()\n+                    .map(|p| p.to_string_lossy().into_owned())\n+                    .unwrap_or_default(),\n+            };\n+            new_metadata.insert(pid, metadata);\n+        }\n+\n+        Some(metrics)\n+    }\n+\n+    /// Discover all current processes and collect their metrics\n+    /// Single system refresh for accurate timestamp and optimal performance\n+    /// Returns ephemeral data structure for immediate emission\n+    fn discover_and_collect_current_metrics(\n+        system: &Mutex<System>,\n+        individual_tasks: &DashMap<String, IndividualTaskRegistration>,\n+        batches: &DashMap<String, BatchRegistration>,\n+        main_cli_pid: &Mutex<Option<i32>>,\n+        daemon_pid: &Mutex<Option<i32>>,\n+        metadata_store: &MetadataStore,\n+    ) -> Result<MetricsCollectionResult, Box<dyn std::error::Error>> {\n+        // Capture timestamp FIRST for accuracy\n+        let timestamp = std::time::SystemTime::now()\n+            .duration_since(std::time::UNIX_EPOCH)\n+            .unwrap_or_else(|e| {\n+                tracing::warn!(\"System time before UNIX epoch: {}\", e);\n+                std::time::Duration::ZERO\n+            })\n+            .as_millis() as i64;\n+\n+        // Single system refresh for ALL processes (one scan, all data)\n+        // Remove dead processes to prevent stale metrics from terminated processes\n+        let mut sys = system.lock();\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::All,\n+            true, // remove_dead_processes\n+            ProcessRefreshKind::nothing()\n+                .with_memory()\n+                .with_cpu()\n+                .with_exe(UpdateKind::OnlyIfNotSet)\n+                .with_cmd(UpdateKind::OnlyIfNotSet)\n+                .with_cwd(UpdateKind::OnlyIfNotSet),\n+        );\n+\n+        let children_map = Self::build_parent_child_map(&sys);\n+\n+        // Clean up dead individual tasks - retain only tasks with at least one live anchor\n+        individual_tasks.retain(|_, task_reg| {\n+            task_reg\n+                .anchor_pids\n+                .iter()\n+                .any(|pid| sys.process(Pid::from(*pid as usize)).is_some())\n+        });\n+\n+        // Clean up dead batches - retain only batches with live anchor\n+        batches.retain(|_, batch_reg| {\n+            sys.process(Pid::from(batch_reg.anchor_pid as usize))\n+                .is_some()\n+        });\n+\n+        // Pre-allocate HashSet for tracking all live PIDs to reduce allocations\n+        let mut live_pids = HashSet::with_capacity(\n+            1 + // main_cli\n+            10 + // daemon + estimated subprocesses\n+            individual_tasks.len() * 5 + // individual tasks \u00d7 avg processes per task\n+            batches.len() * 5, // batch tasks \u00d7 avg processes per batch\n+        );\n+\n+        // Track new metadata discovered during collection (conditionally populated)\n+        let mut new_metadata = HashMap::new();\n+\n+        // Collect main CLI process\n+        let main_cli_metrics = {\n+            let cli_pid = main_cli_pid.lock();\n+            if let Some(pid) = *cli_pid {",
        "comment_created_at": "2025-10-14T16:15:36+00:00",
        "comment_author": "FrozenPandaz",
        "comment_body": "If `NX_ISOLATE_PLUGINS=true NX_DAEMON=false` then the main process will have the plugin workers as child processes.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1936593193",
    "pr_number": 29245,
    "pr_file": "packages/nx/src/native/walker.rs",
    "created_at": "2025-01-31T02:22:55+00:00",
    "commented_code": "let mut walker = WalkBuilder::new(&directory);\n     walker.require_git(false);\n     walker.hidden(false);\n+    walker.parents(false);\n     walker.git_ignore(use_ignores);\n     if use_ignores {\n         walker.add_custom_ignore_filename(\".nxignore\");",
    "repo_full_name": "nrwl/nx",
    "discussion_comments": [
      {
        "comment_id": "1936593193",
        "repo_full_name": "nrwl/nx",
        "pr_number": 29245,
        "pr_file": "packages/nx/src/native/walker.rs",
        "discussion_id": "1936593193",
        "commented_code": "@@ -176,6 +176,7 @@ where\n     let mut walker = WalkBuilder::new(&directory);\n     walker.require_git(false);\n     walker.hidden(false);\n+    walker.parents(false);\n     walker.git_ignore(use_ignores);\n     if use_ignores {\n         walker.add_custom_ignore_filename(\".nxignore\");",
        "comment_created_at": "2025-01-31T02:22:55+00:00",
        "comment_author": "Cammisuli",
        "comment_body": "```suggestion\r\n    walker.git_ignore(use_ignores);\r\n    if use_ignores {\r\n        // disable the parent gitignore if the directory is a git repo, otherwise use the parent gitignore\r\n        walker.parents(!directory.join(\".git\").exists());\r\n        walker.add_custom_ignore_filename(\".nxignore\");\r\n```",
        "pr_file_module": null
      }
    ]
  }
]