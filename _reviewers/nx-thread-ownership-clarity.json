[
  {
    "discussion_id": "2429697271",
    "pr_number": 32946,
    "pr_file": "packages/nx/src/native/metrics/collector.rs",
    "created_at": "2025-10-14T15:59:33+00:00",
    "commented_code": "+use std::collections::{HashMap, HashSet};\n+use std::sync::Arc;\n+use std::sync::atomic::{AtomicBool, Ordering};\n+use std::thread::JoinHandle;\n+\n+use anyhow::Result;\n+use dashmap::DashMap;\n+use parking_lot::Mutex;\n+use sysinfo::{Pid, ProcessRefreshKind, System, UpdateKind};\n+\n+use crate::native::metrics::types::{\n+    BatchMetricsSnapshot, BatchRegistration, CollectorConfig, DaemonMetrics,\n+    IndividualTaskRegistration, MetricsError, MetricsUpdate, ProcessMetadata, ProcessMetrics,\n+    ProcessMetricsSnapshot, SystemInfo,\n+};\n+use napi::threadsafe_function::{\n+    ErrorStrategy::CalleeHandled, ThreadsafeFunction, ThreadsafeFunctionCallMode,\n+};\n+\n+type ParentToChildrenMap = std::collections::HashMap<i32, Vec<i32>>;\n+\n+/// Result from metrics collection containing all data needed for cleanup and notification\n+struct MetricsCollectionResult {\n+    /// The collected metrics snapshot\n+    metrics_snapshot: ProcessMetricsSnapshot,\n+    /// New metadata discovered during collection\n+    new_metadata: HashMap<i32, ProcessMetadata>,\n+    /// All PIDs that we collected metrics for\n+    live_pids: HashSet<i32>,\n+}\n+\n+/// Metadata store for process metadata\n+/// Stores latest metadata for each known process\n+pub struct MetadataStore {\n+    /// Metadata store: latest metadata for each known process\n+    pub(crate) metadata_store: DashMap<i32, ProcessMetadata>,\n+}\n+\n+impl MetadataStore {\n+    /// Create a new metadata store\n+    pub fn new() -> Self {\n+        Self {\n+            metadata_store: DashMap::new(),\n+        }\n+    }\n+\n+    /// Check if we already have metadata for a PID\n+    pub fn has_metadata(&self, pid: i32) -> bool {\n+        self.metadata_store.contains_key(&pid)\n+    }\n+\n+    /// Insert metadata for a process\n+    pub fn insert_metadata(&self, pid: i32, metadata: ProcessMetadata) {\n+        self.metadata_store.insert(pid, metadata);\n+    }\n+\n+    /// Clean up metadata for dead processes\n+    pub fn cleanup_dead_metadata(&self, live_pids: &HashSet<i32>) {\n+        self.metadata_store.retain(|pid, _| live_pids.contains(pid));\n+    }\n+}\n+\n+/// Subscriber state for tracking metadata delivery\n+pub(crate) struct SubscriberState {\n+    pub callback: ThreadsafeFunction<MetricsUpdate, CalleeHandled>,\n+    pub needs_full_metadata: bool,\n+}\n+\n+/// High-performance metrics collector for Nx tasks\n+/// Thread-safe and designed for minimal overhead\n+pub struct MetricsCollector {\n+    /// Configuration for the collector\n+    config: CollectorConfig,\n+    /// Individual tasks with one or more anchor processes\n+    /// Using DashMap for concurrent access\n+    individual_tasks: Arc<DashMap<String, IndividualTaskRegistration>>,\n+    /// Batch executions with multiple tasks sharing a worker\n+    /// Using DashMap for concurrent access\n+    batches: Arc<DashMap<String, BatchRegistration>>,\n+    /// Main CLI process PID (set once at initialization, uses Mutex for &self methods)\n+    main_cli_pid: Arc<Mutex<Option<i32>>>,\n+    /// Daemon process PID (can be updated when daemon connects)\n+    daemon_pid: Arc<Mutex<Option<i32>>>,\n+    /// System info instance (shared for process querying)\n+    system: Arc<Mutex<System>>,\n+    /// Subscribers for metrics events (thread-safe)\n+    subscribers: Arc<Mutex<Vec<SubscriberState>>>,\n+    /// Metadata store for process metadata\n+    pub(crate) metadata_store: Arc<MetadataStore>,\n+    /// Collection thread state\n+    collection_thread: Option<JoinHandle<()>>,\n+    /// Atomic flag to control collection thread\n+    should_collect: Arc<AtomicBool>,\n+    /// Atomic flag to indicate if collection is running\n+    is_collecting: Arc<AtomicBool>,\n+}\n+\n+impl MetricsCollector {\n+    /// Create a new MetricsCollector with default configuration\n+    pub fn new() -> Self {\n+        Self::with_config(CollectorConfig::default())\n+    }\n+\n+    /// Create a new MetricsCollector with custom configuration\n+    pub fn with_config(config: CollectorConfig) -> Self {\n+        Self {\n+            config,\n+            individual_tasks: Arc::new(DashMap::new()),\n+            batches: Arc::new(DashMap::new()),\n+            main_cli_pid: Arc::new(Mutex::new(None)),\n+            daemon_pid: Arc::new(Mutex::new(None)),\n+            system: Arc::new(Mutex::new(System::new_all())),\n+            subscribers: Arc::new(Mutex::new(Vec::new())),\n+            metadata_store: Arc::new(MetadataStore::new()),\n+            collection_thread: None,\n+            should_collect: Arc::new(AtomicBool::new(false)),\n+            is_collecting: Arc::new(AtomicBool::new(false)),\n+        }\n+    }\n+\n+    /// Start metrics collection\n+    /// Idempotent - safe to call multiple times, subsequent calls are no-ops\n+    pub fn start_collection(&mut self) -> Result<(), MetricsError> {\n+        // Atomically check if collection is not running and start it\n+        // If already running, this is a no-op (idempotent behavior)\n+        if self\n+            .is_collecting\n+            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)\n+            .is_err()\n+        {\n+            // Already started - this is fine, just return success (idempotent)\n+            return Ok(());\n+        }\n+\n+        self.should_collect.store(true, Ordering::Release);\n+\n+        // Clone necessary Arc references for the thread\n+        let should_collect = Arc::clone(&self.should_collect);\n+        let is_collecting = Arc::clone(&self.is_collecting);\n+        let individual_tasks = Arc::clone(&self.individual_tasks);\n+        let batches = Arc::clone(&self.batches);\n+        let main_cli_pid = Arc::clone(&self.main_cli_pid);\n+        let daemon_pid = Arc::clone(&self.daemon_pid);\n+        let system = Arc::clone(&self.system);\n+        let config = self.config.clone();\n+        let subscribers = Arc::clone(&self.subscribers);\n+        let metadata_store = Arc::clone(&self.metadata_store);\n+\n+        // Spawn the collection thread\n+        let collection_thread = std::thread::Builder::new()\n+            .name(\"nx-metrics-collector\".to_string())\n+            .spawn(move || {\n+                Self::collection_loop(\n+                    should_collect,\n+                    is_collecting,\n+                    individual_tasks,\n+                    batches,\n+                    main_cli_pid,\n+                    daemon_pid,\n+                    system,\n+                    config,\n+                    subscribers,\n+                    metadata_store,\n+                );\n+            })\n+            .map_err(|e| {\n+                // Failed to spawn thread - reset flag so future attempts can try again\n+                self.is_collecting.store(false, Ordering::Release);\n+                self.should_collect.store(false, Ordering::Release);\n+                MetricsError::SystemError(format!(\"Failed to start collection thread: {}\", e))\n+            })?;\n+\n+        self.collection_thread = Some(collection_thread);\n+\n+        Ok(())\n+    }\n+\n+    /// Stop metrics collection\n+    pub fn stop_collection(&mut self) -> Result<(), MetricsError> {\n+        if !self.is_collecting.load(Ordering::Acquire) {\n+            return Err(MetricsError::CollectionNotStarted);\n+        }\n+\n+        // Signal the collection thread to stop\n+        self.should_collect.store(false, Ordering::Release);\n+\n+        // Wait for the collection thread to finish\n+        if let Some(thread) = self.collection_thread.take() {\n+            if let Err(e) = thread.join() {\n+                tracing::error!(\"Collection thread panicked: {:?}\", e);\n+            }\n+        }\n+\n+        self.is_collecting.store(false, Ordering::Release);\n+        Ok(())\n+    }\n+\n+    /// Check if collection is currently running\n+    pub fn is_collecting(&self) -> bool {\n+        self.is_collecting.load(Ordering::Acquire)\n+    }\n+\n+    /// Get system information (CPU cores and total memory)\n+    pub fn get_system_info(&self) -> SystemInfo {\n+        let sys = self.system.lock();\n+        SystemInfo {\n+            cpu_cores: sys.cpus().len() as u32,\n+            total_memory: sys.total_memory() as i64,\n+        }\n+    }\n+\n+    /// Register the main CLI process\n+    /// Idempotent - safe to call multiple times with same PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_main_cli_process(&self, pid: i32) {\n+        let mut cli_pid = self.main_cli_pid.lock();\n+        *cli_pid = Some(pid);\n+    }\n+\n+    /// Register the daemon process\n+    /// Idempotent - safe to call multiple times, overwrites with new PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_daemon_process(&self, pid: i32) {\n+        let mut daemon_pid = self.daemon_pid.lock();\n+        *daemon_pid = Some(pid);\n+    }\n+\n+    /// Register a process for an individual task (called when process spawns)\n+    /// Can be called multiple times with same task_id to add more processes\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_task_process(&self, task_id: String, pid: i32) {\n+        self.individual_tasks\n+            .entry(task_id.clone())\n+            .or_insert_with(|| IndividualTaskRegistration::new(task_id))\n+            .anchor_pids\n+            .insert(pid);\n+\n+        // Establish baseline immediately for this task process\n+        // This ensures accurate CPU data from the first collection cycle after spawn\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// Register a batch with multiple tasks sharing a worker\n+    pub fn register_batch(&self, batch_id: String, task_ids: Vec<String>, pid: i32) {\n+        self.batches.insert(\n+            batch_id.clone(),\n+            BatchRegistration::new(batch_id, task_ids, pid),\n+        );\n+\n+        // Establish baseline immediately for the batch worker\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// The main collection loop that runs in a separate thread\n+    /// Discovers current metrics and directly notifies subscribers (true push-based)\n+    fn collection_loop(",
    "repo_full_name": "nrwl/nx",
    "discussion_comments": [
      {
        "comment_id": "2429697271",
        "repo_full_name": "nrwl/nx",
        "pr_number": 32946,
        "pr_file": "packages/nx/src/native/metrics/collector.rs",
        "discussion_id": "2429697271",
        "commented_code": "@@ -0,0 +1,663 @@\n+use std::collections::{HashMap, HashSet};\n+use std::sync::Arc;\n+use std::sync::atomic::{AtomicBool, Ordering};\n+use std::thread::JoinHandle;\n+\n+use anyhow::Result;\n+use dashmap::DashMap;\n+use parking_lot::Mutex;\n+use sysinfo::{Pid, ProcessRefreshKind, System, UpdateKind};\n+\n+use crate::native::metrics::types::{\n+    BatchMetricsSnapshot, BatchRegistration, CollectorConfig, DaemonMetrics,\n+    IndividualTaskRegistration, MetricsError, MetricsUpdate, ProcessMetadata, ProcessMetrics,\n+    ProcessMetricsSnapshot, SystemInfo,\n+};\n+use napi::threadsafe_function::{\n+    ErrorStrategy::CalleeHandled, ThreadsafeFunction, ThreadsafeFunctionCallMode,\n+};\n+\n+type ParentToChildrenMap = std::collections::HashMap<i32, Vec<i32>>;\n+\n+/// Result from metrics collection containing all data needed for cleanup and notification\n+struct MetricsCollectionResult {\n+    /// The collected metrics snapshot\n+    metrics_snapshot: ProcessMetricsSnapshot,\n+    /// New metadata discovered during collection\n+    new_metadata: HashMap<i32, ProcessMetadata>,\n+    /// All PIDs that we collected metrics for\n+    live_pids: HashSet<i32>,\n+}\n+\n+/// Metadata store for process metadata\n+/// Stores latest metadata for each known process\n+pub struct MetadataStore {\n+    /// Metadata store: latest metadata for each known process\n+    pub(crate) metadata_store: DashMap<i32, ProcessMetadata>,\n+}\n+\n+impl MetadataStore {\n+    /// Create a new metadata store\n+    pub fn new() -> Self {\n+        Self {\n+            metadata_store: DashMap::new(),\n+        }\n+    }\n+\n+    /// Check if we already have metadata for a PID\n+    pub fn has_metadata(&self, pid: i32) -> bool {\n+        self.metadata_store.contains_key(&pid)\n+    }\n+\n+    /// Insert metadata for a process\n+    pub fn insert_metadata(&self, pid: i32, metadata: ProcessMetadata) {\n+        self.metadata_store.insert(pid, metadata);\n+    }\n+\n+    /// Clean up metadata for dead processes\n+    pub fn cleanup_dead_metadata(&self, live_pids: &HashSet<i32>) {\n+        self.metadata_store.retain(|pid, _| live_pids.contains(pid));\n+    }\n+}\n+\n+/// Subscriber state for tracking metadata delivery\n+pub(crate) struct SubscriberState {\n+    pub callback: ThreadsafeFunction<MetricsUpdate, CalleeHandled>,\n+    pub needs_full_metadata: bool,\n+}\n+\n+/// High-performance metrics collector for Nx tasks\n+/// Thread-safe and designed for minimal overhead\n+pub struct MetricsCollector {\n+    /// Configuration for the collector\n+    config: CollectorConfig,\n+    /// Individual tasks with one or more anchor processes\n+    /// Using DashMap for concurrent access\n+    individual_tasks: Arc<DashMap<String, IndividualTaskRegistration>>,\n+    /// Batch executions with multiple tasks sharing a worker\n+    /// Using DashMap for concurrent access\n+    batches: Arc<DashMap<String, BatchRegistration>>,\n+    /// Main CLI process PID (set once at initialization, uses Mutex for &self methods)\n+    main_cli_pid: Arc<Mutex<Option<i32>>>,\n+    /// Daemon process PID (can be updated when daemon connects)\n+    daemon_pid: Arc<Mutex<Option<i32>>>,\n+    /// System info instance (shared for process querying)\n+    system: Arc<Mutex<System>>,\n+    /// Subscribers for metrics events (thread-safe)\n+    subscribers: Arc<Mutex<Vec<SubscriberState>>>,\n+    /// Metadata store for process metadata\n+    pub(crate) metadata_store: Arc<MetadataStore>,\n+    /// Collection thread state\n+    collection_thread: Option<JoinHandle<()>>,\n+    /// Atomic flag to control collection thread\n+    should_collect: Arc<AtomicBool>,\n+    /// Atomic flag to indicate if collection is running\n+    is_collecting: Arc<AtomicBool>,\n+}\n+\n+impl MetricsCollector {\n+    /// Create a new MetricsCollector with default configuration\n+    pub fn new() -> Self {\n+        Self::with_config(CollectorConfig::default())\n+    }\n+\n+    /// Create a new MetricsCollector with custom configuration\n+    pub fn with_config(config: CollectorConfig) -> Self {\n+        Self {\n+            config,\n+            individual_tasks: Arc::new(DashMap::new()),\n+            batches: Arc::new(DashMap::new()),\n+            main_cli_pid: Arc::new(Mutex::new(None)),\n+            daemon_pid: Arc::new(Mutex::new(None)),\n+            system: Arc::new(Mutex::new(System::new_all())),\n+            subscribers: Arc::new(Mutex::new(Vec::new())),\n+            metadata_store: Arc::new(MetadataStore::new()),\n+            collection_thread: None,\n+            should_collect: Arc::new(AtomicBool::new(false)),\n+            is_collecting: Arc::new(AtomicBool::new(false)),\n+        }\n+    }\n+\n+    /// Start metrics collection\n+    /// Idempotent - safe to call multiple times, subsequent calls are no-ops\n+    pub fn start_collection(&mut self) -> Result<(), MetricsError> {\n+        // Atomically check if collection is not running and start it\n+        // If already running, this is a no-op (idempotent behavior)\n+        if self\n+            .is_collecting\n+            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)\n+            .is_err()\n+        {\n+            // Already started - this is fine, just return success (idempotent)\n+            return Ok(());\n+        }\n+\n+        self.should_collect.store(true, Ordering::Release);\n+\n+        // Clone necessary Arc references for the thread\n+        let should_collect = Arc::clone(&self.should_collect);\n+        let is_collecting = Arc::clone(&self.is_collecting);\n+        let individual_tasks = Arc::clone(&self.individual_tasks);\n+        let batches = Arc::clone(&self.batches);\n+        let main_cli_pid = Arc::clone(&self.main_cli_pid);\n+        let daemon_pid = Arc::clone(&self.daemon_pid);\n+        let system = Arc::clone(&self.system);\n+        let config = self.config.clone();\n+        let subscribers = Arc::clone(&self.subscribers);\n+        let metadata_store = Arc::clone(&self.metadata_store);\n+\n+        // Spawn the collection thread\n+        let collection_thread = std::thread::Builder::new()\n+            .name(\"nx-metrics-collector\".to_string())\n+            .spawn(move || {\n+                Self::collection_loop(\n+                    should_collect,\n+                    is_collecting,\n+                    individual_tasks,\n+                    batches,\n+                    main_cli_pid,\n+                    daemon_pid,\n+                    system,\n+                    config,\n+                    subscribers,\n+                    metadata_store,\n+                );\n+            })\n+            .map_err(|e| {\n+                // Failed to spawn thread - reset flag so future attempts can try again\n+                self.is_collecting.store(false, Ordering::Release);\n+                self.should_collect.store(false, Ordering::Release);\n+                MetricsError::SystemError(format!(\"Failed to start collection thread: {}\", e))\n+            })?;\n+\n+        self.collection_thread = Some(collection_thread);\n+\n+        Ok(())\n+    }\n+\n+    /// Stop metrics collection\n+    pub fn stop_collection(&mut self) -> Result<(), MetricsError> {\n+        if !self.is_collecting.load(Ordering::Acquire) {\n+            return Err(MetricsError::CollectionNotStarted);\n+        }\n+\n+        // Signal the collection thread to stop\n+        self.should_collect.store(false, Ordering::Release);\n+\n+        // Wait for the collection thread to finish\n+        if let Some(thread) = self.collection_thread.take() {\n+            if let Err(e) = thread.join() {\n+                tracing::error!(\"Collection thread panicked: {:?}\", e);\n+            }\n+        }\n+\n+        self.is_collecting.store(false, Ordering::Release);\n+        Ok(())\n+    }\n+\n+    /// Check if collection is currently running\n+    pub fn is_collecting(&self) -> bool {\n+        self.is_collecting.load(Ordering::Acquire)\n+    }\n+\n+    /// Get system information (CPU cores and total memory)\n+    pub fn get_system_info(&self) -> SystemInfo {\n+        let sys = self.system.lock();\n+        SystemInfo {\n+            cpu_cores: sys.cpus().len() as u32,\n+            total_memory: sys.total_memory() as i64,\n+        }\n+    }\n+\n+    /// Register the main CLI process\n+    /// Idempotent - safe to call multiple times with same PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_main_cli_process(&self, pid: i32) {\n+        let mut cli_pid = self.main_cli_pid.lock();\n+        *cli_pid = Some(pid);\n+    }\n+\n+    /// Register the daemon process\n+    /// Idempotent - safe to call multiple times, overwrites with new PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_daemon_process(&self, pid: i32) {\n+        let mut daemon_pid = self.daemon_pid.lock();\n+        *daemon_pid = Some(pid);\n+    }\n+\n+    /// Register a process for an individual task (called when process spawns)\n+    /// Can be called multiple times with same task_id to add more processes\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_task_process(&self, task_id: String, pid: i32) {\n+        self.individual_tasks\n+            .entry(task_id.clone())\n+            .or_insert_with(|| IndividualTaskRegistration::new(task_id))\n+            .anchor_pids\n+            .insert(pid);\n+\n+        // Establish baseline immediately for this task process\n+        // This ensures accurate CPU data from the first collection cycle after spawn\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// Register a batch with multiple tasks sharing a worker\n+    pub fn register_batch(&self, batch_id: String, task_ids: Vec<String>, pid: i32) {\n+        self.batches.insert(\n+            batch_id.clone(),\n+            BatchRegistration::new(batch_id, task_ids, pid),\n+        );\n+\n+        // Establish baseline immediately for the batch worker\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// The main collection loop that runs in a separate thread\n+    /// Discovers current metrics and directly notifies subscribers (true push-based)\n+    fn collection_loop(",
        "comment_created_at": "2025-10-14T15:59:33+00:00",
        "comment_author": "FrozenPandaz",
        "comment_body": "Check if `&mut self` can be used here",
        "pr_file_module": null
      },
      {
        "comment_id": "2459399041",
        "repo_full_name": "nrwl/nx",
        "pr_number": 32946,
        "pr_file": "packages/nx/src/native/metrics/collector.rs",
        "discussion_id": "2429697271",
        "commented_code": "@@ -0,0 +1,663 @@\n+use std::collections::{HashMap, HashSet};\n+use std::sync::Arc;\n+use std::sync::atomic::{AtomicBool, Ordering};\n+use std::thread::JoinHandle;\n+\n+use anyhow::Result;\n+use dashmap::DashMap;\n+use parking_lot::Mutex;\n+use sysinfo::{Pid, ProcessRefreshKind, System, UpdateKind};\n+\n+use crate::native::metrics::types::{\n+    BatchMetricsSnapshot, BatchRegistration, CollectorConfig, DaemonMetrics,\n+    IndividualTaskRegistration, MetricsError, MetricsUpdate, ProcessMetadata, ProcessMetrics,\n+    ProcessMetricsSnapshot, SystemInfo,\n+};\n+use napi::threadsafe_function::{\n+    ErrorStrategy::CalleeHandled, ThreadsafeFunction, ThreadsafeFunctionCallMode,\n+};\n+\n+type ParentToChildrenMap = std::collections::HashMap<i32, Vec<i32>>;\n+\n+/// Result from metrics collection containing all data needed for cleanup and notification\n+struct MetricsCollectionResult {\n+    /// The collected metrics snapshot\n+    metrics_snapshot: ProcessMetricsSnapshot,\n+    /// New metadata discovered during collection\n+    new_metadata: HashMap<i32, ProcessMetadata>,\n+    /// All PIDs that we collected metrics for\n+    live_pids: HashSet<i32>,\n+}\n+\n+/// Metadata store for process metadata\n+/// Stores latest metadata for each known process\n+pub struct MetadataStore {\n+    /// Metadata store: latest metadata for each known process\n+    pub(crate) metadata_store: DashMap<i32, ProcessMetadata>,\n+}\n+\n+impl MetadataStore {\n+    /// Create a new metadata store\n+    pub fn new() -> Self {\n+        Self {\n+            metadata_store: DashMap::new(),\n+        }\n+    }\n+\n+    /// Check if we already have metadata for a PID\n+    pub fn has_metadata(&self, pid: i32) -> bool {\n+        self.metadata_store.contains_key(&pid)\n+    }\n+\n+    /// Insert metadata for a process\n+    pub fn insert_metadata(&self, pid: i32, metadata: ProcessMetadata) {\n+        self.metadata_store.insert(pid, metadata);\n+    }\n+\n+    /// Clean up metadata for dead processes\n+    pub fn cleanup_dead_metadata(&self, live_pids: &HashSet<i32>) {\n+        self.metadata_store.retain(|pid, _| live_pids.contains(pid));\n+    }\n+}\n+\n+/// Subscriber state for tracking metadata delivery\n+pub(crate) struct SubscriberState {\n+    pub callback: ThreadsafeFunction<MetricsUpdate, CalleeHandled>,\n+    pub needs_full_metadata: bool,\n+}\n+\n+/// High-performance metrics collector for Nx tasks\n+/// Thread-safe and designed for minimal overhead\n+pub struct MetricsCollector {\n+    /// Configuration for the collector\n+    config: CollectorConfig,\n+    /// Individual tasks with one or more anchor processes\n+    /// Using DashMap for concurrent access\n+    individual_tasks: Arc<DashMap<String, IndividualTaskRegistration>>,\n+    /// Batch executions with multiple tasks sharing a worker\n+    /// Using DashMap for concurrent access\n+    batches: Arc<DashMap<String, BatchRegistration>>,\n+    /// Main CLI process PID (set once at initialization, uses Mutex for &self methods)\n+    main_cli_pid: Arc<Mutex<Option<i32>>>,\n+    /// Daemon process PID (can be updated when daemon connects)\n+    daemon_pid: Arc<Mutex<Option<i32>>>,\n+    /// System info instance (shared for process querying)\n+    system: Arc<Mutex<System>>,\n+    /// Subscribers for metrics events (thread-safe)\n+    subscribers: Arc<Mutex<Vec<SubscriberState>>>,\n+    /// Metadata store for process metadata\n+    pub(crate) metadata_store: Arc<MetadataStore>,\n+    /// Collection thread state\n+    collection_thread: Option<JoinHandle<()>>,\n+    /// Atomic flag to control collection thread\n+    should_collect: Arc<AtomicBool>,\n+    /// Atomic flag to indicate if collection is running\n+    is_collecting: Arc<AtomicBool>,\n+}\n+\n+impl MetricsCollector {\n+    /// Create a new MetricsCollector with default configuration\n+    pub fn new() -> Self {\n+        Self::with_config(CollectorConfig::default())\n+    }\n+\n+    /// Create a new MetricsCollector with custom configuration\n+    pub fn with_config(config: CollectorConfig) -> Self {\n+        Self {\n+            config,\n+            individual_tasks: Arc::new(DashMap::new()),\n+            batches: Arc::new(DashMap::new()),\n+            main_cli_pid: Arc::new(Mutex::new(None)),\n+            daemon_pid: Arc::new(Mutex::new(None)),\n+            system: Arc::new(Mutex::new(System::new_all())),\n+            subscribers: Arc::new(Mutex::new(Vec::new())),\n+            metadata_store: Arc::new(MetadataStore::new()),\n+            collection_thread: None,\n+            should_collect: Arc::new(AtomicBool::new(false)),\n+            is_collecting: Arc::new(AtomicBool::new(false)),\n+        }\n+    }\n+\n+    /// Start metrics collection\n+    /// Idempotent - safe to call multiple times, subsequent calls are no-ops\n+    pub fn start_collection(&mut self) -> Result<(), MetricsError> {\n+        // Atomically check if collection is not running and start it\n+        // If already running, this is a no-op (idempotent behavior)\n+        if self\n+            .is_collecting\n+            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)\n+            .is_err()\n+        {\n+            // Already started - this is fine, just return success (idempotent)\n+            return Ok(());\n+        }\n+\n+        self.should_collect.store(true, Ordering::Release);\n+\n+        // Clone necessary Arc references for the thread\n+        let should_collect = Arc::clone(&self.should_collect);\n+        let is_collecting = Arc::clone(&self.is_collecting);\n+        let individual_tasks = Arc::clone(&self.individual_tasks);\n+        let batches = Arc::clone(&self.batches);\n+        let main_cli_pid = Arc::clone(&self.main_cli_pid);\n+        let daemon_pid = Arc::clone(&self.daemon_pid);\n+        let system = Arc::clone(&self.system);\n+        let config = self.config.clone();\n+        let subscribers = Arc::clone(&self.subscribers);\n+        let metadata_store = Arc::clone(&self.metadata_store);\n+\n+        // Spawn the collection thread\n+        let collection_thread = std::thread::Builder::new()\n+            .name(\"nx-metrics-collector\".to_string())\n+            .spawn(move || {\n+                Self::collection_loop(\n+                    should_collect,\n+                    is_collecting,\n+                    individual_tasks,\n+                    batches,\n+                    main_cli_pid,\n+                    daemon_pid,\n+                    system,\n+                    config,\n+                    subscribers,\n+                    metadata_store,\n+                );\n+            })\n+            .map_err(|e| {\n+                // Failed to spawn thread - reset flag so future attempts can try again\n+                self.is_collecting.store(false, Ordering::Release);\n+                self.should_collect.store(false, Ordering::Release);\n+                MetricsError::SystemError(format!(\"Failed to start collection thread: {}\", e))\n+            })?;\n+\n+        self.collection_thread = Some(collection_thread);\n+\n+        Ok(())\n+    }\n+\n+    /// Stop metrics collection\n+    pub fn stop_collection(&mut self) -> Result<(), MetricsError> {\n+        if !self.is_collecting.load(Ordering::Acquire) {\n+            return Err(MetricsError::CollectionNotStarted);\n+        }\n+\n+        // Signal the collection thread to stop\n+        self.should_collect.store(false, Ordering::Release);\n+\n+        // Wait for the collection thread to finish\n+        if let Some(thread) = self.collection_thread.take() {\n+            if let Err(e) = thread.join() {\n+                tracing::error!(\"Collection thread panicked: {:?}\", e);\n+            }\n+        }\n+\n+        self.is_collecting.store(false, Ordering::Release);\n+        Ok(())\n+    }\n+\n+    /// Check if collection is currently running\n+    pub fn is_collecting(&self) -> bool {\n+        self.is_collecting.load(Ordering::Acquire)\n+    }\n+\n+    /// Get system information (CPU cores and total memory)\n+    pub fn get_system_info(&self) -> SystemInfo {\n+        let sys = self.system.lock();\n+        SystemInfo {\n+            cpu_cores: sys.cpus().len() as u32,\n+            total_memory: sys.total_memory() as i64,\n+        }\n+    }\n+\n+    /// Register the main CLI process\n+    /// Idempotent - safe to call multiple times with same PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_main_cli_process(&self, pid: i32) {\n+        let mut cli_pid = self.main_cli_pid.lock();\n+        *cli_pid = Some(pid);\n+    }\n+\n+    /// Register the daemon process\n+    /// Idempotent - safe to call multiple times, overwrites with new PID\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_daemon_process(&self, pid: i32) {\n+        let mut daemon_pid = self.daemon_pid.lock();\n+        *daemon_pid = Some(pid);\n+    }\n+\n+    /// Register a process for an individual task (called when process spawns)\n+    /// Can be called multiple times with same task_id to add more processes\n+    /// No validation at registration time - validation happens during collection\n+    pub fn register_task_process(&self, task_id: String, pid: i32) {\n+        self.individual_tasks\n+            .entry(task_id.clone())\n+            .or_insert_with(|| IndividualTaskRegistration::new(task_id))\n+            .anchor_pids\n+            .insert(pid);\n+\n+        // Establish baseline immediately for this task process\n+        // This ensures accurate CPU data from the first collection cycle after spawn\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// Register a batch with multiple tasks sharing a worker\n+    pub fn register_batch(&self, batch_id: String, task_ids: Vec<String>, pid: i32) {\n+        self.batches.insert(\n+            batch_id.clone(),\n+            BatchRegistration::new(batch_id, task_ids, pid),\n+        );\n+\n+        // Establish baseline immediately for the batch worker\n+        let mut sys = self.system.lock();\n+        let target_pid = Pid::from(pid as usize);\n+        sys.refresh_processes_specifics(\n+            sysinfo::ProcessesToUpdate::Some(&[target_pid]),\n+            false, // don't remove dead processes\n+            ProcessRefreshKind::nothing().with_cpu(),\n+        );\n+    }\n+\n+    /// The main collection loop that runs in a separate thread\n+    /// Discovers current metrics and directly notifies subscribers (true push-based)\n+    fn collection_loop(",
        "comment_created_at": "2025-10-24T08:53:15+00:00",
        "comment_author": "leosvelperez",
        "comment_body": "We can't use `&mut self` here because the function runs in a separate thread that needs to own its data. We can't pass references across thread boundaries due to Rust's ownership rules.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2429771402",
    "pr_number": 32946,
    "pr_file": "packages/nx/src/native/pseudo_terminal/child_process.rs",
    "created_at": "2025-10-14T16:24:58+00:00",
    "commented_code": "External::new((self.parser.clone(), self.writer_arc.clone()))\n     }\n \n+    #[napi]\n+    pub fn get_pid(&self) -> i32 {\n+        self.process_killer.get_pid()",
    "repo_full_name": "nrwl/nx",
    "discussion_comments": [
      {
        "comment_id": "2429771402",
        "repo_full_name": "nrwl/nx",
        "pr_number": 32946,
        "pr_file": "packages/nx/src/native/pseudo_terminal/child_process.rs",
        "discussion_id": "2429771402",
        "commented_code": "@@ -52,6 +52,11 @@ impl ChildProcess {\n         External::new((self.parser.clone(), self.writer_arc.clone()))\n     }\n \n+    #[napi]\n+    pub fn get_pid(&self) -> i32 {\n+        self.process_killer.get_pid()",
        "comment_created_at": "2025-10-14T16:24:58+00:00",
        "comment_author": "FrozenPandaz",
        "comment_body": "This is weird to get the pid from the killer. Whatever instantiates this should also pass in a PID",
        "pr_file_module": null
      }
    ]
  }
]