[
  {
    "discussion_id": "1962271907",
    "pr_number": 9204,
    "pr_file": "ml/nn/attention.go",
    "created_at": "2025-02-19T19:43:08+00:00",
    "commented_code": "+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1962271907",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "ml/nn/attention.go",
        "discussion_id": "1962271907",
        "commented_code": "@@ -0,0 +1,22 @@\n+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {",
        "comment_created_at": "2025-02-19T19:43:08+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "```suggestion\r\n// Attention implements scaled dot-product attention for transformer models:\r\n// Attention(Q, K, V) = softmax(QK^T/\u221ad_k)V\r\n//\r\n// Parameters:\r\n//   - ctx: Context for tensor operations\r\n//   - query: Query tensor (Q) with shape [batch, heads, seq_len_q, d_k]\r\n//   - key: Key tensor (K) with shape [batch, heads, seq_len_k, d_k]\r\n//   - value: Value tensor (V) with shape [batch, heads, seq_len_k, d_v]\r\n//   - mask: Optional attention mask. If provided, should broadcast to [batch, heads, seq_len_q, seq_len_k]\r\n//   - scale: Scaling factor, typically 1/\u221ad_k where d_k is the key dimension\r\n//\r\n// Returns:\r\n//   Attention output with shape [batch, seq_len_q, heads, d_v]\r\nfunc Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1962273177",
    "pr_number": 9204,
    "pr_file": "ml/nn/attention.go",
    "created_at": "2025-02-19T19:44:07+00:00",
    "commented_code": "+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\n+\tif sdpa, ok := query.(ml.ScaledDotProductAttention); ok {\n+\t\treturn sdpa.ScaledDotProductAttention(ctx, key, value, mask, scale)",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1962273177",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "ml/nn/attention.go",
        "discussion_id": "1962273177",
        "commented_code": "@@ -0,0 +1,22 @@\n+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\n+\tif sdpa, ok := query.(ml.ScaledDotProductAttention); ok {\n+\t\treturn sdpa.ScaledDotProductAttention(ctx, key, value, mask, scale)",
        "comment_created_at": "2025-02-19T19:44:07+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "When will the query implement scaled dot product attention? Is this something that should handled here or in the caller?",
        "pr_file_module": null
      },
      {
        "comment_id": "1962574720",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "ml/nn/attention.go",
        "discussion_id": "1962273177",
        "commented_code": "@@ -0,0 +1,22 @@\n+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\n+\tif sdpa, ok := query.(ml.ScaledDotProductAttention); ok {\n+\t\treturn sdpa.ScaledDotProductAttention(ctx, key, value, mask, scale)",
        "comment_created_at": "2025-02-20T00:29:40+00:00",
        "comment_author": "jessegross",
        "comment_body": "It's effectively a property of the backend since that's what defines the implementation of Tensor and whether there is a fused implementation of attention.\r\n\r\nIt should definitely be handled here - one of the main goals of this is to abstract away different implementations of attention from the model definition so we can drop in optimized versions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1962275194",
    "pr_number": 9204,
    "pr_file": "ml/nn/attention.go",
    "created_at": "2025-02-19T19:45:14+00:00",
    "commented_code": "+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\n+\tif sdpa, ok := query.(ml.ScaledDotProductAttention); ok {\n+\t\treturn sdpa.ScaledDotProductAttention(ctx, key, value, mask, scale)\n+\t} else {\n+\t\tkq := key.MulmatFullPrec(ctx, query)",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1962275194",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "ml/nn/attention.go",
        "discussion_id": "1962275194",
        "commented_code": "@@ -0,0 +1,22 @@\n+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\n+\tif sdpa, ok := query.(ml.ScaledDotProductAttention); ok {\n+\t\treturn sdpa.ScaledDotProductAttention(ctx, key, value, mask, scale)\n+\t} else {\n+\t\tkq := key.MulmatFullPrec(ctx, query)",
        "comment_created_at": "2025-02-19T19:45:14+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "Should we be validating the tensor shapes here?",
        "pr_file_module": null
      },
      {
        "comment_id": "1964601432",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "ml/nn/attention.go",
        "discussion_id": "1962275194",
        "commented_code": "@@ -0,0 +1,22 @@\n+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\n+\tif sdpa, ok := query.(ml.ScaledDotProductAttention); ok {\n+\t\treturn sdpa.ScaledDotProductAttention(ctx, key, value, mask, scale)\n+\t} else {\n+\t\tkq := key.MulmatFullPrec(ctx, query)",
        "comment_created_at": "2025-02-21T01:16:39+00:00",
        "comment_author": "jessegross",
        "comment_body": "Based on the information that we have here, I think the main thing that we could do is check that the corresponding dimensions of the input tensors match. For example, the sequence length of K matches V and the mask. Is that the kind of thing that you were thinking?",
        "pr_file_module": null
      },
      {
        "comment_id": "1965929279",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "ml/nn/attention.go",
        "discussion_id": "1962275194",
        "commented_code": "@@ -0,0 +1,22 @@\n+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\n+\tif sdpa, ok := query.(ml.ScaledDotProductAttention); ok {\n+\t\treturn sdpa.ScaledDotProductAttention(ctx, key, value, mask, scale)\n+\t} else {\n+\t\tkq := key.MulmatFullPrec(ctx, query)",
        "comment_created_at": "2025-02-21T17:38:49+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "That's pretty much what I was thinking, not a requirement by any means but maybe nice to have",
        "pr_file_module": null
      },
      {
        "comment_id": "1966163999",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "ml/nn/attention.go",
        "discussion_id": "1962275194",
        "commented_code": "@@ -0,0 +1,22 @@\n+package nn\n+\n+import (\n+\t\"github.com/ollama/ollama/ml\"\n+)\n+\n+func Attention(ctx ml.Context, query, key, value, mask ml.Tensor, scale float64) ml.Tensor {\n+\tif sdpa, ok := query.(ml.ScaledDotProductAttention); ok {\n+\t\treturn sdpa.ScaledDotProductAttention(ctx, key, value, mask, scale)\n+\t} else {\n+\t\tkq := key.MulmatFullPrec(ctx, query)",
        "comment_created_at": "2025-02-21T21:01:32+00:00",
        "comment_author": "jessegross",
        "comment_body": "I think it's a good idea, it certainly will make debugging new models or backends easier compared to the eventual GGML error.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1962278379",
    "pr_number": 9204,
    "pr_file": "model/models/llama/model.go",
    "created_at": "2025-02-19T19:46:44+00:00",
    "commented_code": "k = k.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n \tv = v.Permute(ctx, 1, 2, 0, 3).Contiguous(ctx)\n \n-\tkq := k.MulmatFullPrec(ctx, q)\n-\tkq = kq.Scale(ctx, 1.0/math.Sqrt(float64(headDim)))\n-\tkq = kq.Add(ctx, mask)\n-\tkq = kq.Softmax(ctx)\n-\n-\tkqv := v.Mulmat(ctx, kq)\n-\tkqv = kqv.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n+\tkqv := nn.Attention(ctx, q, k, v, mask, 1.0/math.Sqrt(float64(headDim)))",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1962278379",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "model/models/llama/model.go",
        "discussion_id": "1962278379",
        "commented_code": "@@ -86,13 +86,7 @@ func (sa *SelfAttention) Forward(ctx ml.Context, hiddenState, positionIDs ml.Ten\n \tk = k.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n \tv = v.Permute(ctx, 1, 2, 0, 3).Contiguous(ctx)\n \n-\tkq := k.MulmatFullPrec(ctx, q)\n-\tkq = kq.Scale(ctx, 1.0/math.Sqrt(float64(headDim)))\n-\tkq = kq.Add(ctx, mask)\n-\tkq = kq.Softmax(ctx)\n-\n-\tkqv := v.Mulmat(ctx, kq)\n-\tkqv = kqv.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n+\tkqv := nn.Attention(ctx, q, k, v, mask, 1.0/math.Sqrt(float64(headDim)))",
        "comment_created_at": "2025-02-19T19:46:44+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "It feels less clear to me how the scaling factor is set now and how its used, maybe we should move it to a constant somewhere in the model definition",
        "pr_file_module": null
      },
      {
        "comment_id": "1962577040",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "model/models/llama/model.go",
        "discussion_id": "1962278379",
        "commented_code": "@@ -86,13 +86,7 @@ func (sa *SelfAttention) Forward(ctx ml.Context, hiddenState, positionIDs ml.Ten\n \tk = k.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n \tv = v.Permute(ctx, 1, 2, 0, 3).Contiguous(ctx)\n \n-\tkq := k.MulmatFullPrec(ctx, q)\n-\tkq = kq.Scale(ctx, 1.0/math.Sqrt(float64(headDim)))\n-\tkq = kq.Add(ctx, mask)\n-\tkq = kq.Softmax(ctx)\n-\n-\tkqv := v.Mulmat(ctx, kq)\n-\tkqv = kqv.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n+\tkqv := nn.Attention(ctx, q, k, v, mask, 1.0/math.Sqrt(float64(headDim)))",
        "comment_created_at": "2025-02-20T00:33:01+00:00",
        "comment_author": "jessegross",
        "comment_body": "Can you elaborate a little on why this is less clear now? It's the same calculation of the scaling factor as before, just passed as a parameter to Attention rather than explicit call to Scale.",
        "pr_file_module": null
      },
      {
        "comment_id": "1962585855",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "model/models/llama/model.go",
        "discussion_id": "1962278379",
        "commented_code": "@@ -86,13 +86,7 @@ func (sa *SelfAttention) Forward(ctx ml.Context, hiddenState, positionIDs ml.Ten\n \tk = k.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n \tv = v.Permute(ctx, 1, 2, 0, 3).Contiguous(ctx)\n \n-\tkq := k.MulmatFullPrec(ctx, q)\n-\tkq = kq.Scale(ctx, 1.0/math.Sqrt(float64(headDim)))\n-\tkq = kq.Add(ctx, mask)\n-\tkq = kq.Softmax(ctx)\n-\n-\tkqv := v.Mulmat(ctx, kq)\n-\tkqv = kqv.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n+\tkqv := nn.Attention(ctx, q, k, v, mask, 1.0/math.Sqrt(float64(headDim)))",
        "comment_created_at": "2025-02-20T00:41:11+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "I find it easier to eye-ball when looking at the model forward pass implementation when its directly associated with scaling:\r\n```go\r\nkq = kq.Scale(ctx, 1.0/math.Sqrt(float64(headDim)))\r\n```\r\nThe fact it is being passed to scale make it apparent that this is probably the scaling factor.\r\n\r\n```go\r\nkqv := nn.Attention(ctx, q, k, v, mask, 1.0/math.Sqrt(float64(headDim)))\r\n```\r\nNow I can't tell what this mathematical operation is while reading the code, I have to follow through and see what it is actually used for in the attention implementation.\r\n\r\n```go\r\nvar scaleFactor = 1.0/math.Sqrt(float64(headDim))\r\nkqv := nn.Attention(ctx, q, k, v, mask, scaleFactor)\r\n```\r\nI personally like something like this, so I can understand what is going on without leaving the current file. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1962593081",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9204,
        "pr_file": "model/models/llama/model.go",
        "discussion_id": "1962278379",
        "commented_code": "@@ -86,13 +86,7 @@ func (sa *SelfAttention) Forward(ctx ml.Context, hiddenState, positionIDs ml.Ten\n \tk = k.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n \tv = v.Permute(ctx, 1, 2, 0, 3).Contiguous(ctx)\n \n-\tkq := k.MulmatFullPrec(ctx, q)\n-\tkq = kq.Scale(ctx, 1.0/math.Sqrt(float64(headDim)))\n-\tkq = kq.Add(ctx, mask)\n-\tkq = kq.Softmax(ctx)\n-\n-\tkqv := v.Mulmat(ctx, kq)\n-\tkqv = kqv.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n+\tkqv := nn.Attention(ctx, q, k, v, mask, 1.0/math.Sqrt(float64(headDim)))",
        "comment_created_at": "2025-02-20T00:51:52+00:00",
        "comment_author": "jessegross",
        "comment_body": "That makes sense - I can pull it out as a named variable.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1932580281",
    "pr_number": 8490,
    "pr_file": "model/llama/model.go",
    "created_at": "2025-01-28T17:23:13+00:00",
    "commented_code": "Output *nn.Linear `ggml:\"attn_output\"`\n }\n \n-func (sa *SelfAttention) Forward(ctx ml.Context, hiddenState, positionIDs ml.Tensor, cache model.Cache, opts *Options) ml.Tensor {\n-\tbatchSize := hiddenState.Dim(0)\n-\theadDim := opts.hiddenSize / opts.numHeads\n-\n-\tq := sa.Query.Forward(ctx, hiddenState)\n-\tq = q.Reshape(ctx, headDim, opts.numHeads, batchSize)\n-\t// q = q.Rope(ctx, positionIDs, opts.RopeFactors, opts.ropeDim, opts.ropeBase, opts.ropeScale)\n-\n-\tk := sa.Key.Forward(ctx, hiddenState)\n-\tk = k.Reshape(ctx, headDim, opts.numKVHeads, batchSize)\n-\t// k = k.Rope(ctx, positionIDs, opts.RopeFactors, opts.ropeDim, opts.ropeBase, opts.ropeScale)\n-\n-\tv := sa.Value.Forward(ctx, hiddenState)\n-\tv = v.Reshape(ctx, headDim, opts.numKVHeads, batchSize)\n-\n-\tk, v = cache.Put(ctx, k, v, cache.Options)\n-\n-\tq = q.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n-\tk = k.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n-\tv = v.Permute(ctx, 1, 2, 0, 3).Contiguous(ctx)\n+func (sa *SelfAttention) Forward(ctx ml.Context, hiddenState ml.Tensor, offset int32, cache model.Cache, opts *Options) ml.Tensor {\n+\t// Ref: https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/llama.py\n+\tshape := hiddenState.Shape()\n+\tB := shape[0]\n+\tL := shape[1]\n+\tn_heads := opts.numHeads\n+\tn_kv_heads := opts.numKVHeads\n+\n+\thead_dim := opts.hiddenSize / opts.numHeads\n+\tscale := math.Pow(float64(head_dim), -0.5)\n+\n+\tqueries := sa.Query.Forward(ctx, hiddenState)\n+\tkeys := sa.Key.Forward(ctx, hiddenState)\n+\tvalues := sa.Value.Forward(ctx, hiddenState)\n+\n+\tqueries = queries.Reshape(ctx, B, L, n_heads, -1).Permute(ctx, 0, 2, 1, 3)\n+\tkeys = keys.Reshape(ctx, B, L, n_kv_heads, -1).Permute(ctx, 0, 2, 1, 3)\n+\tvalues = values.Reshape(ctx, B, L, n_kv_heads, -1).Permute(ctx, 0, 2, 1, 3)\n+\n+\tqueries = sa.Rope(ctx, queries, offset, opts)\n+\tkeys = sa.Rope(ctx, keys, offset, opts)\n+\n+\t// TODO - The current cache algorithm doesn't work properly on MLX due to stride values being onconsistent with\n+\t// the GGML backend.  When this caching support comes back, the input should be truncated to just the latest token\n+\t// keys, values = cache.Put(ctx, keys, values, cache.Options)\n+\n+\t// TODO - Some backends will support various \"fast\" variations of common operations, but not all will\n+\t// Either we need to build the \"slow\" version in the backend, or the model has to be able to discover\n+\t// if the backend supports the fast operation and if not fall back to the slow version.\n+\t// GGML does not have a fast scaled dot product attention, so this block is currently conditional\n+\tvar output ml.Tensor\n+\tif opts.GGML { // TODO - should be based on an abstracted lookup for \"fast scaled dot product attention\" support",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1932580281",
        "repo_full_name": "ollama/ollama",
        "pr_number": 8490,
        "pr_file": "model/llama/model.go",
        "discussion_id": "1932580281",
        "commented_code": "@@ -52,38 +57,177 @@ type SelfAttention struct {\n \tOutput *nn.Linear `ggml:\"attn_output\"`\n }\n \n-func (sa *SelfAttention) Forward(ctx ml.Context, hiddenState, positionIDs ml.Tensor, cache model.Cache, opts *Options) ml.Tensor {\n-\tbatchSize := hiddenState.Dim(0)\n-\theadDim := opts.hiddenSize / opts.numHeads\n-\n-\tq := sa.Query.Forward(ctx, hiddenState)\n-\tq = q.Reshape(ctx, headDim, opts.numHeads, batchSize)\n-\t// q = q.Rope(ctx, positionIDs, opts.RopeFactors, opts.ropeDim, opts.ropeBase, opts.ropeScale)\n-\n-\tk := sa.Key.Forward(ctx, hiddenState)\n-\tk = k.Reshape(ctx, headDim, opts.numKVHeads, batchSize)\n-\t// k = k.Rope(ctx, positionIDs, opts.RopeFactors, opts.ropeDim, opts.ropeBase, opts.ropeScale)\n-\n-\tv := sa.Value.Forward(ctx, hiddenState)\n-\tv = v.Reshape(ctx, headDim, opts.numKVHeads, batchSize)\n-\n-\tk, v = cache.Put(ctx, k, v, cache.Options)\n-\n-\tq = q.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n-\tk = k.Permute(ctx, 0, 2, 1, 3).Contiguous(ctx)\n-\tv = v.Permute(ctx, 1, 2, 0, 3).Contiguous(ctx)\n+func (sa *SelfAttention) Forward(ctx ml.Context, hiddenState ml.Tensor, offset int32, cache model.Cache, opts *Options) ml.Tensor {\n+\t// Ref: https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/models/llama.py\n+\tshape := hiddenState.Shape()\n+\tB := shape[0]\n+\tL := shape[1]\n+\tn_heads := opts.numHeads\n+\tn_kv_heads := opts.numKVHeads\n+\n+\thead_dim := opts.hiddenSize / opts.numHeads\n+\tscale := math.Pow(float64(head_dim), -0.5)\n+\n+\tqueries := sa.Query.Forward(ctx, hiddenState)\n+\tkeys := sa.Key.Forward(ctx, hiddenState)\n+\tvalues := sa.Value.Forward(ctx, hiddenState)\n+\n+\tqueries = queries.Reshape(ctx, B, L, n_heads, -1).Permute(ctx, 0, 2, 1, 3)\n+\tkeys = keys.Reshape(ctx, B, L, n_kv_heads, -1).Permute(ctx, 0, 2, 1, 3)\n+\tvalues = values.Reshape(ctx, B, L, n_kv_heads, -1).Permute(ctx, 0, 2, 1, 3)\n+\n+\tqueries = sa.Rope(ctx, queries, offset, opts)\n+\tkeys = sa.Rope(ctx, keys, offset, opts)\n+\n+\t// TODO - The current cache algorithm doesn't work properly on MLX due to stride values being onconsistent with\n+\t// the GGML backend.  When this caching support comes back, the input should be truncated to just the latest token\n+\t// keys, values = cache.Put(ctx, keys, values, cache.Options)\n+\n+\t// TODO - Some backends will support various \"fast\" variations of common operations, but not all will\n+\t// Either we need to build the \"slow\" version in the backend, or the model has to be able to discover\n+\t// if the backend supports the fast operation and if not fall back to the slow version.\n+\t// GGML does not have a fast scaled dot product attention, so this block is currently conditional\n+\tvar output ml.Tensor\n+\tif opts.GGML { // TODO - should be based on an abstracted lookup for \"fast scaled dot product attention\" support",
        "comment_created_at": "2025-01-28T17:23:13+00:00",
        "comment_author": "mxyng",
        "comment_body": "You can create an interface `FastScaledDotProductAttention` and fallback to manual sdpa if the context doesn't implement it. something like\r\n\r\n```go\r\nif sdpa, ok := ctx.(ml.FastScaledDotProductAttention); ok {\r\n  hiddenState = sdpa.Forward(...)\r\n} else {\r\n  // manual sdpa\r\n}\r\n```",
        "pr_file_module": null
      }
    ]
  }
]