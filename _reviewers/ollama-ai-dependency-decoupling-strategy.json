[
  {
    "discussion_id": "1890836664",
    "pr_number": 8134,
    "pr_file": "llama/speculative.cpp",
    "created_at": "2024-12-18T21:06:33+00:00",
    "commented_code": "+/**\n+ * llama.cpp - commit ba1cb19cdd0d92e012e0f6e009e0620f854b6afd - do not edit this file",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1890836664",
        "repo_full_name": "ollama/ollama",
        "pr_number": 8134,
        "pr_file": "llama/speculative.cpp",
        "discussion_id": "1890836664",
        "commented_code": "@@ -0,0 +1,300 @@\n+/**\n+ * llama.cpp - commit ba1cb19cdd0d92e012e0f6e009e0620f854b6afd - do not edit this file",
        "comment_created_at": "2024-12-18T21:06:33+00:00",
        "comment_author": "sammcj",
        "comment_body": "Does this have to manually copied over and be kept up to date with llama.cpp?",
        "pr_file_module": null
      },
      {
        "comment_id": "1891318408",
        "repo_full_name": "ollama/ollama",
        "pr_number": 8134,
        "pr_file": "llama/speculative.cpp",
        "discussion_id": "1890836664",
        "commented_code": "@@ -0,0 +1,300 @@\n+/**\n+ * llama.cpp - commit ba1cb19cdd0d92e012e0f6e009e0620f854b6afd - do not edit this file",
        "comment_created_at": "2024-12-19T08:10:33+00:00",
        "comment_author": "bfroemel",
        "comment_body": "This is done when following the recently changed vendoring process, see make target `sync`, https://github.com/ollama/ollama/blob/4c066179b0f8604fb1ce1c95e45cdd0332bee240/make/Makefile.sync#L192 and https://github.com/ollama/ollama/blob/main/llama/README.md#vendoring . \r\n\r\nWhile taking a second look I noticed that I should keep vendoring regarding speculative decoding separated from the llava vendoring stuff. FIXED: https://github.com/ollama/ollama/pull/8134/commits/d539e92eb66cc86047de5bd3e1906349cf9083c4",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1986224389",
    "pr_number": 9586,
    "pr_file": "llama/sampling_ext.cpp",
    "created_at": "2025-03-09T05:43:20+00:00",
    "commented_code": "return 0;\n     }\n }\n+\n+struct llama_vocab * llama_load_vocab_from_file(const char * fname) {",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1986224389",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9586,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "1986224389",
        "commented_code": "@@ -64,3 +67,22 @@ int schema_to_grammar(const char *json_schema, char *grammar, size_t max_len)\n         return 0;\n     }\n }\n+\n+struct llama_vocab * llama_load_vocab_from_file(const char * fname) {",
        "comment_created_at": "2025-03-09T05:43:20+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "Nice, simpler than where I was going with this",
        "pr_file_module": null
      },
      {
        "comment_id": "1986548720",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9586,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "1986224389",
        "commented_code": "@@ -64,3 +67,22 @@ int schema_to_grammar(const char *json_schema, char *grammar, size_t max_len)\n         return 0;\n     }\n }\n+\n+struct llama_vocab * llama_load_vocab_from_file(const char * fname) {",
        "comment_created_at": "2025-03-10T03:22:47+00:00",
        "comment_author": "jmorganca",
        "comment_body": "Yeah! Turned out to be little code, however the tradeoff is we have to keep using `llama` code for now. It's very important we work to remove this to use new engine tokenizers instead as this restricts us to loading GGUF files with specific KV keys. cc @ParthSareen ",
        "pr_file_module": null
      }
    ]
  }
]