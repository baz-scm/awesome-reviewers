[
  {
    "discussion_id": "1663091677",
    "pr_number": 5426,
    "pr_file": "docs/tutorials/amd-igpu-780m.md",
    "created_at": "2024-07-02T20:00:09+00:00",
    "commented_code": "+\n+The iGPU 780M of AMD Ryzen CPU could run on ROCm at Linux with extral settings. \n+\n+Available CPU: Ryzen 7000s/8000s CPU with iGPU 780M\n+\n+## Test Platform\n+\n+| **Platform ** | **miniPC**                                                      |\n+| ------------- | --------------------------------------------------------------- |\n+| **HW**        | AMD Ryzen R8845HS + Radeon780M(iGPU, set 16GB VRAM of 64GB DDR) |\n+| **OS**        | Ubuntu22.04                                                     |\n+| **SW**        | ROCm6.0+PyTorch                                                 |\n+\n+\n+## Prerequisites\n+0. Set UMA for iGPU in BIOS.",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1663091677",
        "repo_full_name": "ollama/ollama",
        "pr_number": 5426,
        "pr_file": "docs/tutorials/amd-igpu-780m.md",
        "discussion_id": "1663091677",
        "commented_code": "@@ -0,0 +1,116 @@\n+\n+The iGPU 780M of AMD Ryzen CPU could run on ROCm at Linux with extral settings. \n+\n+Available CPU: Ryzen 7000s/8000s CPU with iGPU 780M\n+\n+## Test Platform\n+\n+| **Platform ** | **miniPC**                                                      |\n+| ------------- | --------------------------------------------------------------- |\n+| **HW**        | AMD Ryzen R8845HS + Radeon780M(iGPU, set 16GB VRAM of 64GB DDR) |\n+| **OS**        | Ubuntu22.04                                                     |\n+| **SW**        | ROCm6.0+PyTorch                                                 |\n+\n+\n+## Prerequisites\n+0. Set UMA for iGPU in BIOS.",
        "comment_created_at": "2024-07-02T20:00:09+00:00",
        "comment_author": "dhiltgen",
        "comment_body": "I believe this is the step where the user needs to dedicate a certain amount of system memory to the iGPU, correct?   We should note that leaving 1G wont work as our current code will skip the iGPU as a result, so at least > 1G has to be assigned, but really we should recommend more I think.",
        "pr_file_module": null
      },
      {
        "comment_id": "1663516730",
        "repo_full_name": "ollama/ollama",
        "pr_number": 5426,
        "pr_file": "docs/tutorials/amd-igpu-780m.md",
        "discussion_id": "1663091677",
        "commented_code": "@@ -0,0 +1,116 @@\n+\n+The iGPU 780M of AMD Ryzen CPU could run on ROCm at Linux with extral settings. \n+\n+Available CPU: Ryzen 7000s/8000s CPU with iGPU 780M\n+\n+## Test Platform\n+\n+| **Platform ** | **miniPC**                                                      |\n+| ------------- | --------------------------------------------------------------- |\n+| **HW**        | AMD Ryzen R8845HS + Radeon780M(iGPU, set 16GB VRAM of 64GB DDR) |\n+| **OS**        | Ubuntu22.04                                                     |\n+| **SW**        | ROCm6.0+PyTorch                                                 |\n+\n+\n+## Prerequisites\n+0. Set UMA for iGPU in BIOS.",
        "comment_created_at": "2024-07-03T05:35:45+00:00",
        "comment_author": "alexhegit",
        "comment_body": "OK. I will modify this part according your comments.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1987781554",
    "pr_number": 9545,
    "pr_file": "docs/faq.md",
    "created_at": "2025-03-10T18:02:28+00:00",
    "commented_code": "## How can I specify the context window size?\n \n-By default, Ollama uses a context window size of 2048 tokens.\n+By default, Ollama uses a context window size of 2048 tokens.  This can be overridden with the `OLLAMA_CONTEXT_LENGTH` environment variable. For example, to set the default context length to 8K, use: `OLLAMA_CONTEXT_LENGTH=8192 ollama serve`.",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1987781554",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9545,
        "pr_file": "docs/faq.md",
        "discussion_id": "1987781554",
        "commented_code": "@@ -20,7 +20,7 @@ Please refer to the [GPU docs](./gpu.md).\n \n ## How can I specify the context window size?\n \n-By default, Ollama uses a context window size of 2048 tokens.\n+By default, Ollama uses a context window size of 2048 tokens.  This can be overridden with the `OLLAMA_CONTEXT_LENGTH` environment variable. For example, to set the default context length to 8K, use: `OLLAMA_CONTEXT_LENGTH=8192 ollama serve`.",
        "comment_created_at": "2025-03-10T18:02:28+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "```suggestion\r\nBy default, Ollama uses a context window size of 2048 tokens. This can be overridden with the `OLLAMA_CONTEXT_LENGTH` environment variable. For example, to set the default context length to 8K, use: `OLLAMA_CONTEXT_LENGTH=8192 ollama serve`.\r\n```\r\n\r\nFixing the double space here.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1960435593",
    "pr_number": 9073,
    "pr_file": "docs/faq.md",
    "created_at": "2025-02-18T19:22:39+00:00",
    "commented_code": "How much the cache quantization impacts the model's response quality will depend on the model and the task.  Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.\n \n You may need to experiment with different quantization types to find the best balance between memory usage and quality.\n+\n+## How can I prevent OOMs?\n+\n+Memory calculations depend on the architecture of the model and sometimes it's not correct.  This can lead to the runner terminating from an OOM.  This may be mitigated by one or more of the following:\n+\n+1. Set [`OLLAMA_GPU_OVERHEAD`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L237) to give llama.cpp a buffer to grow in to (eg, `OLLAMA_GPU_OVERHEAD=536870912` to reserve 512M).  The exact value depends on model/GPU.\n+2. Enable flash attention by setting [`OLLAMA_FLASH_ATTENTION=1`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L236) in the server environment.  Flash attention is a more efficient use of memory and may reduce memory pressure.  See above.\n+3. Reduce the number layers that ollama thinks it can offload to the GPU, see [here](https://github.com/ollama/ollama/issues/6950#issuecomment-2373663650).\n+4. In Linux, set `GGML_CUDA_ENABLE_UNIFIED_MEMORY=1`.  This will allow the GPU to offload to CPU memory if VRAM is exhausted.  This is only useful for small amounts of memory as there is a [performance penalty](https://github.com/ollama/ollama/issues/7584#issuecomment-2466715900).  However, in the case where the goal is to reduce OOMs, the amount offloaded will be small and the impact minimal. \n+5. Set [`OLLAMA_NUM_PARALLEL`](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/envconfig/config.go#L249) to 1.\n+6. Using a smaller context buffer by reducing [`num_ctx`](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/docs/modelfile.md#valid-parameters-and-values:~:text=mirostat_tau%205.0-,num_ctx,-Sets%20the%20size).",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1960435593",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9073,
        "pr_file": "docs/faq.md",
        "discussion_id": "1960435593",
        "commented_code": "@@ -310,3 +310,93 @@ The currently available K/V cache quantization types are:\n How much the cache quantization impacts the model's response quality will depend on the model and the task.  Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.\n \n You may need to experiment with different quantization types to find the best balance between memory usage and quality.\n+\n+## How can I prevent OOMs?\n+\n+Memory calculations depend on the architecture of the model and sometimes it's not correct.  This can lead to the runner terminating from an OOM.  This may be mitigated by one or more of the following:\n+\n+1. Set [`OLLAMA_GPU_OVERHEAD`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L237) to give llama.cpp a buffer to grow in to (eg, `OLLAMA_GPU_OVERHEAD=536870912` to reserve 512M).  The exact value depends on model/GPU.\n+2. Enable flash attention by setting [`OLLAMA_FLASH_ATTENTION=1`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L236) in the server environment.  Flash attention is a more efficient use of memory and may reduce memory pressure.  See above.\n+3. Reduce the number layers that ollama thinks it can offload to the GPU, see [here](https://github.com/ollama/ollama/issues/6950#issuecomment-2373663650).\n+4. In Linux, set `GGML_CUDA_ENABLE_UNIFIED_MEMORY=1`.  This will allow the GPU to offload to CPU memory if VRAM is exhausted.  This is only useful for small amounts of memory as there is a [performance penalty](https://github.com/ollama/ollama/issues/7584#issuecomment-2466715900).  However, in the case where the goal is to reduce OOMs, the amount offloaded will be small and the impact minimal. \n+5. Set [`OLLAMA_NUM_PARALLEL`](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/envconfig/config.go#L249) to 1.\n+6. Using a smaller context buffer by reducing [`num_ctx`](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/docs/modelfile.md#valid-parameters-and-values:~:text=mirostat_tau%205.0-,num_ctx,-Sets%20the%20size).",
        "comment_created_at": "2025-02-18T19:22:39+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "```suggestion\r\n# Preventing Out of Memory (OOM) Errors\r\n\r\nOllama is designed to automatically manage memory usage and prevent out-of-memory errors. However, in some cases, these errors might still occur. Here are solutions to help prevent these crashes:\r\n\r\n## Basic Solutions\r\n\r\n1. Decrease Context Size\r\n   - [Lower the `num_ctx` parameter](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/docs/modelfile.md#valid-parameters-and-values:~:text=mirostat_tau%205.0-,num_ctx,-Sets%20the%20size)\r\n   - This reduces how much text the model can process at once\r\n   - Results in lower memory usage\r\n\r\n2. Limit GPU Layer Usage\r\n   - [Reduce the number of model layers that run on your GPU](https://github.com/ollama/ollama/issues/6950#issuecomment-2373663650).\r\n   - This helps balance the workload between GPU and CPU\r\n\r\n## Advanced Solutions\r\n\r\nFor users who need additional control, the following environment variables can be configured:\r\n\r\n- [`OLLAMA_GPU_OVERHEAD`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L237): Reserves additional GPU memory (e.g., for 512MB buffer)\r\n- [`OLLAMA_FLASH_ATTENTION`](https://github.com/ollama/ollama/blob/5f8051180e3b9aeafc153f6b5056e7358a939c88/envconfig/config.go#L236): Enables more efficient memory usage\r\n- `GGML_CUDA_ENABLE_UNIFIED_MEMORY`: Allows GPU to use CPU memory (Linux only)\r\n- [`OLLAMA_NUM_PARALLEL`](https://github.com/ollama/ollama/blob/a4f69a0191b304c204ef074ccd6523f121bfddfe/envconfig/config.go#L249): Controls parallel processing\r\n\r\nNote: Environment variable support may vary depending on your setup and system configuration.\r\n```",
        "pr_file_module": null
      }
    ]
  }
]