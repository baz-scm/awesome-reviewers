[
  {
    "discussion_id": "2103478979",
    "pr_number": 10747,
    "pr_file": "llama/sampling_ext.cpp",
    "created_at": "2025-05-22T22:22:10+00:00",
    "commented_code": "return common_sampler_sample(sampler, ctx, idx);\n }\n \n-int schema_to_grammar(const char *json_schema, char *grammar, size_t max_len)\n+struct parsed_grammar * schema_to_grammar(const char *json_schema)\n {\n+    struct parsed_grammar *result = new parsed_grammar();\n     try\n     {\n         nlohmann::ordered_json schema = nlohmann::ordered_json::parse(json_schema);\n         std::string grammar_str = json_schema_to_grammar(schema);\n-        size_t len = grammar_str.length();\n-        if (len >= max_len)\n-        {\n-            len = max_len - 1;\n-        }\n-        strncpy(grammar, grammar_str.c_str(), len);\n-        return len;\n+        result->length = grammar_str.length();\n+        result->grammar = new char[result->length];",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "2103478979",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10747,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "2103478979",
        "commented_code": "@@ -45,25 +46,22 @@ llama_token common_sampler_csample(struct common_sampler *sampler, struct llama_\n     return common_sampler_sample(sampler, ctx, idx);\n }\n \n-int schema_to_grammar(const char *json_schema, char *grammar, size_t max_len)\n+struct parsed_grammar * schema_to_grammar(const char *json_schema)\n {\n+    struct parsed_grammar *result = new parsed_grammar();\n     try\n     {\n         nlohmann::ordered_json schema = nlohmann::ordered_json::parse(json_schema);\n         std::string grammar_str = json_schema_to_grammar(schema);\n-        size_t len = grammar_str.length();\n-        if (len >= max_len)\n-        {\n-            len = max_len - 1;\n-        }\n-        strncpy(grammar, grammar_str.c_str(), len);\n-        return len;\n+        result->length = grammar_str.length();\n+        result->grammar = new char[result->length];",
        "comment_created_at": "2025-05-22T22:22:10+00:00",
        "comment_author": "ParthSareen",
        "comment_body": "We'd have to manage the memory in this as well which as grammar or the result struct would not be getting freed. Probably better to maintain the signature from before ~~(we'd also have to update the sampling code otherwise)~~",
        "pr_file_module": null
      },
      {
        "comment_id": "2107749574",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10747,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "2103478979",
        "commented_code": "@@ -45,25 +46,22 @@ llama_token common_sampler_csample(struct common_sampler *sampler, struct llama_\n     return common_sampler_sample(sampler, ctx, idx);\n }\n \n-int schema_to_grammar(const char *json_schema, char *grammar, size_t max_len)\n+struct parsed_grammar * schema_to_grammar(const char *json_schema)\n {\n+    struct parsed_grammar *result = new parsed_grammar();\n     try\n     {\n         nlohmann::ordered_json schema = nlohmann::ordered_json::parse(json_schema);\n         std::string grammar_str = json_schema_to_grammar(schema);\n-        size_t len = grammar_str.length();\n-        if (len >= max_len)\n-        {\n-            len = max_len - 1;\n-        }\n-        strncpy(grammar, grammar_str.c_str(), len);\n-        return len;\n+        result->length = grammar_str.length();\n+        result->grammar = new char[result->length];",
        "comment_created_at": "2025-05-26T18:54:37+00:00",
        "comment_author": "SchoofsKelvin",
        "comment_body": "Both `result` and `result->grammar` are freed in `SchemaToGrammar`, lines 584 and (in case `length>0` which is when we have a grammar) line 593:\r\nhttps://github.com/ollama/ollama/blob/21a4811c9c69e7a9d4c36af94729717129371ad9/llama/llama.go#L578-L595",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2087635544",
    "pr_number": 10696,
    "pr_file": "llama/sampling_ext.cpp",
    "created_at": "2025-05-13T21:07:29+00:00",
    "commented_code": "if (g->vocab != nullptr) {\n             delete g->vocab;\n         }\n-        llama_grammar_free_impl(g);\n     }\n+    if (g->o_vocab != nullptr) {",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "2087635544",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10696,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "2087635544",
        "commented_code": "@@ -114,8 +114,11 @@ void grammar_free(struct llama_grammar *g) {\n         if (g->vocab != nullptr) {\n             delete g->vocab;\n         }\n-        llama_grammar_free_impl(g);\n     }\n+    if (g->o_vocab != nullptr) {",
        "comment_created_at": "2025-05-13T21:07:29+00:00",
        "comment_author": "rick-github",
        "comment_body": "This assumes g is never nil.  If that's true, does it need to be tested in the statement further up?  Or should the nil test be done here as well?",
        "pr_file_module": null
      },
      {
        "comment_id": "2087639837",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10696,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "2087635544",
        "commented_code": "@@ -114,8 +114,11 @@ void grammar_free(struct llama_grammar *g) {\n         if (g->vocab != nullptr) {\n             delete g->vocab;\n         }\n-        llama_grammar_free_impl(g);\n     }\n+    if (g->o_vocab != nullptr) {",
        "comment_created_at": "2025-05-13T21:10:51+00:00",
        "comment_author": "ParthSareen",
        "comment_body": "_Should_ be okay - `llama.go` checks the nil before calling the free where `g.c` is the grammar:\r\n```go\r\nfunc (g *Grammar) Free() {\r\n\tg.mu.Lock()\r\n\tdefer g.mu.Unlock()\r\n\tif g.c != nil {\r\n\t\tC.grammar_free(g.c)\r\n\t\tg.c = nil\r\n\t}\r\n}\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2087691435",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10696,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "2087635544",
        "commented_code": "@@ -114,8 +114,11 @@ void grammar_free(struct llama_grammar *g) {\n         if (g->vocab != nullptr) {\n             delete g->vocab;\n         }\n-        llama_grammar_free_impl(g);\n     }\n+    if (g->o_vocab != nullptr) {",
        "comment_created_at": "2025-05-13T21:56:54+00:00",
        "comment_author": "rick-github",
        "comment_body": "So the check on line 113 is not required.",
        "pr_file_module": null
      },
      {
        "comment_id": "2087719654",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10696,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "2087635544",
        "commented_code": "@@ -114,8 +114,11 @@ void grammar_free(struct llama_grammar *g) {\n         if (g->vocab != nullptr) {\n             delete g->vocab;\n         }\n-        llama_grammar_free_impl(g);\n     }\n+    if (g->o_vocab != nullptr) {",
        "comment_created_at": "2025-05-13T22:27:49+00:00",
        "comment_author": "jmorganca",
        "comment_body": "@ParthSareen yes this should be in the `if (g != nullptr) {` block",
        "pr_file_module": null
      },
      {
        "comment_id": "2087724582",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10696,
        "pr_file": "llama/sampling_ext.cpp",
        "discussion_id": "2087635544",
        "commented_code": "@@ -114,8 +114,11 @@ void grammar_free(struct llama_grammar *g) {\n         if (g->vocab != nullptr) {\n             delete g->vocab;\n         }\n-        llama_grammar_free_impl(g);\n     }\n+    if (g->o_vocab != nullptr) {",
        "comment_created_at": "2025-05-13T22:34:09+00:00",
        "comment_author": "ParthSareen",
        "comment_body": "Fixed! Read it a bit wrong initially",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2029496411",
    "pr_number": 10096,
    "pr_file": "llama/llama.cpp/src/llama-grammar.cpp",
    "created_at": "2025-04-04T22:19:38+00:00",
    "commented_code": "throw std::runtime_error(\"Unexpected empty grammar stack after accepting piece: \" + piece);\n     }\n }\n+\n+\n+const std::string & ollama_vocab::token_to_piece(uint32_t token) {",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "2029496411",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10096,
        "pr_file": "llama/llama.cpp/src/llama-grammar.cpp",
        "discussion_id": "2029496411",
        "commented_code": "@@ -1217,3 +1250,20 @@ void llama_grammar_accept_str(struct llama_grammar & grammar, const std::string\n         throw std::runtime_error(\"Unexpected empty grammar stack after accepting piece: \" + piece);\n     }\n }\n+\n+\n+const std::string & ollama_vocab::token_to_piece(uint32_t token) {",
        "comment_created_at": "2025-04-04T22:19:38+00:00",
        "comment_author": "jmorganca",
        "comment_body": "I would return a regular `std::string` here to avoid issues if the string gets deallocated or changed later by the caller\r\n\r\n```suggestion\r\nconst std::string ollama_vocab::token_to_piece(uint32_t token) {\r\n```",
        "pr_file_module": null
      }
    ]
  }
]