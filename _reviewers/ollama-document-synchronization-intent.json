[
  {
    "discussion_id": "2034454700",
    "pr_number": 10135,
    "pr_file": "cmd/cmd.go",
    "created_at": "2025-04-09T05:08:09+00:00",
    "commented_code": "return err\n \t}\n \n-\tif len(req.Files) > 0 {\n-\t\tfileMap := map[string]string{}\n-\t\tfor f, digest := range req.Files {\n+\tvar mu sync.Mutex",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "2034454700",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10135,
        "pr_file": "cmd/cmd.go",
        "discussion_id": "2034454700",
        "commented_code": "@@ -117,26 +119,43 @@ func CreateHandler(cmd *cobra.Command, args []string) error {\n \t\treturn err\n \t}\n \n-\tif len(req.Files) > 0 {\n-\t\tfileMap := map[string]string{}\n-\t\tfor f, digest := range req.Files {\n+\tvar mu sync.Mutex",
        "comment_created_at": "2025-04-09T05:08:09+00:00",
        "comment_author": "bmizerany",
        "comment_body": "Is this guarding the errgroup? I hope not. \r\n\r\nBe more clear about what it is guarding. Usually one defines directly below the mutex what it is guarding, visually grouped by blank lines. Grep stdlib for `sync.Mutex` for examples.\r\n\r\nIf g does not need mu held, then keep them separated, literally. If, in the rare case it is guarding something not declared where it, the mutex, is declared, then please document what it is guarding. For example:\r\n\r\n```\r\nvar mu sync.Mutex // guards req.Files across goroutines below.\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2034462626",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10135,
        "pr_file": "cmd/cmd.go",
        "discussion_id": "2034454700",
        "commented_code": "@@ -117,26 +119,43 @@ func CreateHandler(cmd *cobra.Command, args []string) error {\n \t\treturn err\n \t}\n \n-\tif len(req.Files) > 0 {\n-\t\tfileMap := map[string]string{}\n-\t\tfor f, digest := range req.Files {\n+\tvar mu sync.Mutex",
        "comment_created_at": "2025-04-09T05:18:16+00:00",
        "comment_author": "bmizerany",
        "comment_body": "The rules still holds, however, you don't need this lock. Read below.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1844646680",
    "pr_number": 7262,
    "pr_file": "server/routes.go",
    "created_at": "2024-11-15T22:59:55+00:00",
    "commented_code": "c.JSON(http.StatusOK, api.ProcessResponse{Models: models})\n }\n \n+func (s *Server) InfoHandler(c *gin.Context) {\n+\trunnerMap := runners.GetAvailableServers(s.runnerDir)\n+\trunners := []string{}\n+\tfor k := range runnerMap {\n+\t\trunners = append(runners, k)\n+\t}\n+\tsort.Strings(runners)\n+\n+\tsysInfo := discover.GetSystemInfo()\n+\tms, _ := Manifests()\n+\tfsUsed := uint64(0)\n+\tlayers := map[string]int64{}\n+\tfor _, m := range ms {\n+\t\tfor _, l := range m.Layers {\n+\t\t\tlayers[l.Digest] = l.Size\n+\t\t}\n+\t}\n+\tfor _, sz := range layers {\n+\t\tfsUsed += uint64(sz)\n+\t}\n+\tvramUsed := uint64(0)\n+\tfor _, r := range s.sched.loaded {\n+\t\tvramUsed += r.estimatedVRAM",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1844646680",
        "repo_full_name": "ollama/ollama",
        "pr_number": 7262,
        "pr_file": "server/routes.go",
        "discussion_id": "1844646680",
        "commented_code": "@@ -1343,6 +1348,94 @@ func (s *Server) PsHandler(c *gin.Context) {\n \tc.JSON(http.StatusOK, api.ProcessResponse{Models: models})\n }\n \n+func (s *Server) InfoHandler(c *gin.Context) {\n+\trunnerMap := runners.GetAvailableServers(s.runnerDir)\n+\trunners := []string{}\n+\tfor k := range runnerMap {\n+\t\trunners = append(runners, k)\n+\t}\n+\tsort.Strings(runners)\n+\n+\tsysInfo := discover.GetSystemInfo()\n+\tms, _ := Manifests()\n+\tfsUsed := uint64(0)\n+\tlayers := map[string]int64{}\n+\tfor _, m := range ms {\n+\t\tfor _, l := range m.Layers {\n+\t\t\tlayers[l.Digest] = l.Size\n+\t\t}\n+\t}\n+\tfor _, sz := range layers {\n+\t\tfsUsed += uint64(sz)\n+\t}\n+\tvramUsed := uint64(0)\n+\tfor _, r := range s.sched.loaded {\n+\t\tvramUsed += r.estimatedVRAM",
        "comment_created_at": "2024-11-15T22:59:55+00:00",
        "comment_author": "pdevine",
        "comment_body": "I'm assuming `s.sched.loaded` and `estimatedVRAM` are safe to access.",
        "pr_file_module": null
      },
      {
        "comment_id": "1848743924",
        "repo_full_name": "ollama/ollama",
        "pr_number": 7262,
        "pr_file": "server/routes.go",
        "discussion_id": "1844646680",
        "commented_code": "@@ -1343,6 +1348,94 @@ func (s *Server) PsHandler(c *gin.Context) {\n \tc.JSON(http.StatusOK, api.ProcessResponse{Models: models})\n }\n \n+func (s *Server) InfoHandler(c *gin.Context) {\n+\trunnerMap := runners.GetAvailableServers(s.runnerDir)\n+\trunners := []string{}\n+\tfor k := range runnerMap {\n+\t\trunners = append(runners, k)\n+\t}\n+\tsort.Strings(runners)\n+\n+\tsysInfo := discover.GetSystemInfo()\n+\tms, _ := Manifests()\n+\tfsUsed := uint64(0)\n+\tlayers := map[string]int64{}\n+\tfor _, m := range ms {\n+\t\tfor _, l := range m.Layers {\n+\t\t\tlayers[l.Digest] = l.Size\n+\t\t}\n+\t}\n+\tfor _, sz := range layers {\n+\t\tfsUsed += uint64(sz)\n+\t}\n+\tvramUsed := uint64(0)\n+\tfor _, r := range s.sched.loaded {\n+\t\tvramUsed += r.estimatedVRAM",
        "comment_created_at": "2024-11-19T17:08:38+00:00",
        "comment_author": "dhiltgen",
        "comment_body": "good catch - this should be protected with the lock.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1947380772",
    "pr_number": 8301,
    "pr_file": "runner/newrunner/cache.go",
    "created_at": "2025-02-08T00:44:51+00:00",
    "commented_code": "+package newrunner\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"log/slog\"\n+\t\"math\"\n+\t\"reflect\"\n+\t\"time\"\n+\n+\t\"github.com/ollama/ollama/kvcache\"\n+\t\"github.com/ollama/ollama/ml\"\n+\t\"github.com/ollama/ollama/model\"\n+)\n+\n+type InputCache struct {\n+\t// context window size (per slot)\n+\tnumCtx int32\n+\n+\t// does the cache store data or do we need to always send the full input?\n+\t// note that when enabled is false the underlying cache may either be nil\n+\t// or a non-nil dummy that doesn't actually store anything\n+\tenabled bool\n+\n+\t// individual KV caches\n+\tslots []InputCacheSlot\n+\n+\t// optimize cache eviction for multiple users\n+\tmultiUserCache bool\n+\n+\tcache kvcache.Cache\n+}\n+\n+func NewInputCache(model model.Model, kvCacheType string, kvSize int32, numSlots int, multiUserCache bool) (*InputCache, error) {\n+\tif kvSize/int32(numSlots) < 1 {\n+\t\treturn nil, fmt.Errorf(\"must have at least one kv cache entry per parallel sequence (kv: %v parallel: %v)\", kvSize, numSlots)\n+\t}\n+\n+\tslots := make([]InputCacheSlot, numSlots)\n+\n+\tfor i := range slots {\n+\t\tslots[i] = InputCacheSlot{\n+\t\t\tId:     i,\n+\t\t\tInputs: make([]input, 0),\n+\t\t}\n+\t}\n+\n+\tcache := model.Config().Cache\n+\tif cache != nil {\n+\t\tcache.Init(model.Backend(), kvCacheTypeFromStr(kvCacheType), kvSize)\n+\t}\n+\n+\treturn &InputCache{\n+\t\tnumCtx:         kvSize / int32(numSlots),\n+\t\tenabled:        cache != nil,\n+\t\tslots:          slots,\n+\t\tmultiUserCache: multiUserCache,\n+\t\tcache:          cache,\n+\t}, nil\n+}\n+\n+func kvCacheTypeFromStr(s string) ml.DType {\n+\tswitch s {\n+\tcase \"q8_0\":\n+\t\tpanic(\"kv cache quantization not yet implemented\")\n+\tcase \"q4_0\":\n+\t\tpanic(\"kv cache quantization not yet implemented\")\n+\tdefault:\n+\t\treturn ml.DTypeF32\n+\t}\n+}\n+\n+// Locking: Operations on InputCacheSlot (including finding one\n+// through LoadCacheSlot) require a lock to be be held that serializes\n+// these operations with each other and processBatch",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1947380772",
        "repo_full_name": "ollama/ollama",
        "pr_number": 8301,
        "pr_file": "runner/newrunner/cache.go",
        "discussion_id": "1947380772",
        "commented_code": "@@ -0,0 +1,276 @@\n+package newrunner\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"log/slog\"\n+\t\"math\"\n+\t\"reflect\"\n+\t\"time\"\n+\n+\t\"github.com/ollama/ollama/kvcache\"\n+\t\"github.com/ollama/ollama/ml\"\n+\t\"github.com/ollama/ollama/model\"\n+)\n+\n+type InputCache struct {\n+\t// context window size (per slot)\n+\tnumCtx int32\n+\n+\t// does the cache store data or do we need to always send the full input?\n+\t// note that when enabled is false the underlying cache may either be nil\n+\t// or a non-nil dummy that doesn't actually store anything\n+\tenabled bool\n+\n+\t// individual KV caches\n+\tslots []InputCacheSlot\n+\n+\t// optimize cache eviction for multiple users\n+\tmultiUserCache bool\n+\n+\tcache kvcache.Cache\n+}\n+\n+func NewInputCache(model model.Model, kvCacheType string, kvSize int32, numSlots int, multiUserCache bool) (*InputCache, error) {\n+\tif kvSize/int32(numSlots) < 1 {\n+\t\treturn nil, fmt.Errorf(\"must have at least one kv cache entry per parallel sequence (kv: %v parallel: %v)\", kvSize, numSlots)\n+\t}\n+\n+\tslots := make([]InputCacheSlot, numSlots)\n+\n+\tfor i := range slots {\n+\t\tslots[i] = InputCacheSlot{\n+\t\t\tId:     i,\n+\t\t\tInputs: make([]input, 0),\n+\t\t}\n+\t}\n+\n+\tcache := model.Config().Cache\n+\tif cache != nil {\n+\t\tcache.Init(model.Backend(), kvCacheTypeFromStr(kvCacheType), kvSize)\n+\t}\n+\n+\treturn &InputCache{\n+\t\tnumCtx:         kvSize / int32(numSlots),\n+\t\tenabled:        cache != nil,\n+\t\tslots:          slots,\n+\t\tmultiUserCache: multiUserCache,\n+\t\tcache:          cache,\n+\t}, nil\n+}\n+\n+func kvCacheTypeFromStr(s string) ml.DType {\n+\tswitch s {\n+\tcase \"q8_0\":\n+\t\tpanic(\"kv cache quantization not yet implemented\")\n+\tcase \"q4_0\":\n+\t\tpanic(\"kv cache quantization not yet implemented\")\n+\tdefault:\n+\t\treturn ml.DTypeF32\n+\t}\n+}\n+\n+// Locking: Operations on InputCacheSlot (including finding one\n+// through LoadCacheSlot) require a lock to be be held that serializes\n+// these operations with each other and processBatch",
        "comment_created_at": "2025-02-08T00:44:51+00:00",
        "comment_author": "ParthSareen",
        "comment_body": "I also didn't see the lock here is it implicit?",
        "pr_file_module": null
      },
      {
        "comment_id": "1947384732",
        "repo_full_name": "ollama/ollama",
        "pr_number": 8301,
        "pr_file": "runner/newrunner/cache.go",
        "discussion_id": "1947380772",
        "commented_code": "@@ -0,0 +1,276 @@\n+package newrunner\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"log/slog\"\n+\t\"math\"\n+\t\"reflect\"\n+\t\"time\"\n+\n+\t\"github.com/ollama/ollama/kvcache\"\n+\t\"github.com/ollama/ollama/ml\"\n+\t\"github.com/ollama/ollama/model\"\n+)\n+\n+type InputCache struct {\n+\t// context window size (per slot)\n+\tnumCtx int32\n+\n+\t// does the cache store data or do we need to always send the full input?\n+\t// note that when enabled is false the underlying cache may either be nil\n+\t// or a non-nil dummy that doesn't actually store anything\n+\tenabled bool\n+\n+\t// individual KV caches\n+\tslots []InputCacheSlot\n+\n+\t// optimize cache eviction for multiple users\n+\tmultiUserCache bool\n+\n+\tcache kvcache.Cache\n+}\n+\n+func NewInputCache(model model.Model, kvCacheType string, kvSize int32, numSlots int, multiUserCache bool) (*InputCache, error) {\n+\tif kvSize/int32(numSlots) < 1 {\n+\t\treturn nil, fmt.Errorf(\"must have at least one kv cache entry per parallel sequence (kv: %v parallel: %v)\", kvSize, numSlots)\n+\t}\n+\n+\tslots := make([]InputCacheSlot, numSlots)\n+\n+\tfor i := range slots {\n+\t\tslots[i] = InputCacheSlot{\n+\t\t\tId:     i,\n+\t\t\tInputs: make([]input, 0),\n+\t\t}\n+\t}\n+\n+\tcache := model.Config().Cache\n+\tif cache != nil {\n+\t\tcache.Init(model.Backend(), kvCacheTypeFromStr(kvCacheType), kvSize)\n+\t}\n+\n+\treturn &InputCache{\n+\t\tnumCtx:         kvSize / int32(numSlots),\n+\t\tenabled:        cache != nil,\n+\t\tslots:          slots,\n+\t\tmultiUserCache: multiUserCache,\n+\t\tcache:          cache,\n+\t}, nil\n+}\n+\n+func kvCacheTypeFromStr(s string) ml.DType {\n+\tswitch s {\n+\tcase \"q8_0\":\n+\t\tpanic(\"kv cache quantization not yet implemented\")\n+\tcase \"q4_0\":\n+\t\tpanic(\"kv cache quantization not yet implemented\")\n+\tdefault:\n+\t\treturn ml.DTypeF32\n+\t}\n+}\n+\n+// Locking: Operations on InputCacheSlot (including finding one\n+// through LoadCacheSlot) require a lock to be be held that serializes\n+// these operations with each other and processBatch",
        "comment_created_at": "2025-02-08T00:56:01+00:00",
        "comment_author": "jessegross",
        "comment_body": "It needs to be held by the caller to synchronize with token generation, for example:\r\nhttps://github.com/ollama/ollama/blob/361a8230d02ab509776406fb925137e1992a9a9f/runner/newrunner/runner.go#L306\r\n\r\nAnd slot lookup:\r\nhttps://github.com/ollama/ollama/blob/361a8230d02ab509776406fb925137e1992a9a9f/runner/newrunner/runner.go#L628",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070931284",
    "pr_number": 10135,
    "pr_file": "types/syncmap/syncmap.go",
    "created_at": "2025-05-01T23:40:16+00:00",
    "commented_code": "+package syncmap\n+\n+import (\n+\t\"sync\"\n+)\n+\n+// SyncMap is a simple, generic thread-safe map implementation.\n+type SyncMap[K comparable, V any] struct {\n+\tmu sync.RWMutex\n+\tm  map[K]V\n+}\n+\n+func NewSyncMap[K comparable, V any]() *SyncMap[K, V] {\n+\treturn &SyncMap[K, V]{\n+\t\tm: make(map[K]V),\n+\t}\n+}\n+\n+func (s *SyncMap[K, V]) Load(key K) (V, bool) {\n+\ts.mu.RLock()\n+\tdefer s.mu.RUnlock()\n+\tval, ok := s.m[key]\n+\treturn val, ok\n+}\n+\n+func (s *SyncMap[K, V]) Store(key K, value V) {\n+\ts.mu.Lock()\n+\tdefer s.mu.Unlock()\n+\ts.m[key] = value\n+}\n+\n+func (s *SyncMap[K, V]) Items() map[K]V {",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "2070931284",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10135,
        "pr_file": "types/syncmap/syncmap.go",
        "discussion_id": "2070931284",
        "commented_code": "@@ -0,0 +1,36 @@\n+package syncmap\n+\n+import (\n+\t\"sync\"\n+)\n+\n+// SyncMap is a simple, generic thread-safe map implementation.\n+type SyncMap[K comparable, V any] struct {\n+\tmu sync.RWMutex\n+\tm  map[K]V\n+}\n+\n+func NewSyncMap[K comparable, V any]() *SyncMap[K, V] {\n+\treturn &SyncMap[K, V]{\n+\t\tm: make(map[K]V),\n+\t}\n+}\n+\n+func (s *SyncMap[K, V]) Load(key K) (V, bool) {\n+\ts.mu.RLock()\n+\tdefer s.mu.RUnlock()\n+\tval, ok := s.m[key]\n+\treturn val, ok\n+}\n+\n+func (s *SyncMap[K, V]) Store(key K, value V) {\n+\ts.mu.Lock()\n+\tdefer s.mu.Unlock()\n+\ts.m[key] = value\n+}\n+\n+func (s *SyncMap[K, V]) Items() map[K]V {",
        "comment_created_at": "2025-05-01T23:40:16+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "Looks like `Items` is returning the actual map, which could be unsafely modified by a caller. We should return a copy instead to prevent accidental misuse that creates a race condition.\r\n```\r\n// Items returns a copy of the underlying map.\r\nfunc (s *SyncMap[K, V]) Items() map[K]V {\r\n\ts.mu.RLock()\r\n\tdefer s.mu.RUnlock()\r\n\t\r\n\tresult := make(map[K]V, len(s.m))\r\n\tfor k, v := range s.m {\r\n\t\tresult[k] = v\r\n\t}\r\n\treturn result\r\n}\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1978181947",
    "pr_number": 9113,
    "pr_file": "llm/server.go",
    "created_at": "2025-03-03T20:55:34+00:00",
    "commented_code": "}\n \n func (s *llmServer) Detokenize(ctx context.Context, tokens []int) (string, error) {\n-\ts.modelLock.Lock()\n-\tdefer s.modelLock.Unlock()\n-\tif s.model != nil {\n+\tif s.llamaModel != nil {\n \t\tvar resp string\n \t\tfor _, token := range tokens {\n-\t\t\tresp += s.model.TokenToPiece(token)\n+\t\t\tresp += s.llamaModel.TokenToPiece(token)\n \t\t}\n \t\treturn resp, nil\n \t}\n-\t// Make sure the server is ready\n-\tstatus, err := s.getServerStatus(ctx)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t} else if status != ServerStatusReady && status != ServerStatusNoSlotsAvailable {\n-\t\treturn \"\", fmt.Errorf(\"unexpected server status: %s\", status.ToString())\n-\t}\n-\n-\tdata, err := json.Marshal(DetokenizeRequest{Tokens: tokens})\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"marshaling decode data: %w\", err)\n-\t}\n-\n-\treq, err := http.NewRequestWithContext(ctx, http.MethodPost, fmt.Sprintf(\"http://127.0.0.1:%d/detokenize\", s.port), bytes.NewBuffer(data))\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"decode request: %w\", err)\n-\t}\n-\treq.Header.Set(\"Content-Type\", \"application/json\")\n-\n-\tresp, err := http.DefaultClient.Do(req)\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"do decode request: %w\", err)\n-\t}\n-\tdefer resp.Body.Close()\n-\tif resp.StatusCode == http.StatusNotFound {\n-\t\tif s.model == nil {\n-\t\t\tslog.Debug(\"new runner detected, loading model for cgo tokenization\")\n-\t\t\tm, err := llama.LoadModelFromFile(s.modelPath, llama.ModelParams{VocabOnly: true})\n-\t\t\tif err != nil {\n-\t\t\t\treturn \"\", err\n-\t\t\t}\n-\t\t\ts.model = m\n+\tif s.textProcessor != nil {\n+\t\ttoks := make([]int32, len(tokens))\n+\t\tfor i, t := range tokens {\n+\t\t\ttoks[i] = int32(t)\n \t\t}\n-\t\tvar resp string\n-\t\tfor _, token := range tokens {\n-\t\t\tresp += s.model.TokenToPiece(token)\n+\t\tcontent, err := s.textProcessor.Decode(toks)\n+\t\tif err != nil {\n+\t\t\treturn \"\", err\n \t\t}\n-\t\treturn resp, nil\n-\t}\n-\n-\tbody, err := io.ReadAll(resp.Body)\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"read decode request: %w\", err)\n+\t\treturn content, nil\n \t}\n-\n-\tif resp.StatusCode >= 400 {\n-\t\tlog.Printf(\"llm decode error: %s\", body)\n-\t\treturn \"\", fmt.Errorf(\"%s\", body)\n-\t}\n-\n-\tvar decoded DetokenizeResponse\n-\tif err := json.Unmarshal(body, &decoded); err != nil {\n-\t\treturn \"\", fmt.Errorf(\"unmarshal encode response: %w\", err)\n-\t}\n-\n-\treturn decoded.Content, nil\n+\t// not reached\n+\treturn \"\", fmt.Errorf(\"no tokenizer configured\")\n }\n \n func (s *llmServer) Close() error {\n-\ts.modelLock.Lock()\n-\tif s.model != nil {\n-\t\tllama.FreeModel(s.model)\n-\t\ts.model = nil\n+\tif s.llamaModel != nil {\n+\t\tllama.FreeModel(s.llamaModel)\n+\t\ts.llamaModel = nil\n \t}\n-\ts.modelLock.Unlock()",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1978181947",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9113,
        "pr_file": "llm/server.go",
        "discussion_id": "1978181947",
        "commented_code": "@@ -1002,80 +992,33 @@ type DetokenizeResponse struct {\n }\n \n func (s *llmServer) Detokenize(ctx context.Context, tokens []int) (string, error) {\n-\ts.modelLock.Lock()\n-\tdefer s.modelLock.Unlock()\n-\tif s.model != nil {\n+\tif s.llamaModel != nil {\n \t\tvar resp string\n \t\tfor _, token := range tokens {\n-\t\t\tresp += s.model.TokenToPiece(token)\n+\t\t\tresp += s.llamaModel.TokenToPiece(token)\n \t\t}\n \t\treturn resp, nil\n \t}\n-\t// Make sure the server is ready\n-\tstatus, err := s.getServerStatus(ctx)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t} else if status != ServerStatusReady && status != ServerStatusNoSlotsAvailable {\n-\t\treturn \"\", fmt.Errorf(\"unexpected server status: %s\", status.ToString())\n-\t}\n-\n-\tdata, err := json.Marshal(DetokenizeRequest{Tokens: tokens})\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"marshaling decode data: %w\", err)\n-\t}\n-\n-\treq, err := http.NewRequestWithContext(ctx, http.MethodPost, fmt.Sprintf(\"http://127.0.0.1:%d/detokenize\", s.port), bytes.NewBuffer(data))\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"decode request: %w\", err)\n-\t}\n-\treq.Header.Set(\"Content-Type\", \"application/json\")\n-\n-\tresp, err := http.DefaultClient.Do(req)\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"do decode request: %w\", err)\n-\t}\n-\tdefer resp.Body.Close()\n-\tif resp.StatusCode == http.StatusNotFound {\n-\t\tif s.model == nil {\n-\t\t\tslog.Debug(\"new runner detected, loading model for cgo tokenization\")\n-\t\t\tm, err := llama.LoadModelFromFile(s.modelPath, llama.ModelParams{VocabOnly: true})\n-\t\t\tif err != nil {\n-\t\t\t\treturn \"\", err\n-\t\t\t}\n-\t\t\ts.model = m\n+\tif s.textProcessor != nil {\n+\t\ttoks := make([]int32, len(tokens))\n+\t\tfor i, t := range tokens {\n+\t\t\ttoks[i] = int32(t)\n \t\t}\n-\t\tvar resp string\n-\t\tfor _, token := range tokens {\n-\t\t\tresp += s.model.TokenToPiece(token)\n+\t\tcontent, err := s.textProcessor.Decode(toks)\n+\t\tif err != nil {\n+\t\t\treturn \"\", err\n \t\t}\n-\t\treturn resp, nil\n-\t}\n-\n-\tbody, err := io.ReadAll(resp.Body)\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"read decode request: %w\", err)\n+\t\treturn content, nil\n \t}\n-\n-\tif resp.StatusCode >= 400 {\n-\t\tlog.Printf(\"llm decode error: %s\", body)\n-\t\treturn \"\", fmt.Errorf(\"%s\", body)\n-\t}\n-\n-\tvar decoded DetokenizeResponse\n-\tif err := json.Unmarshal(body, &decoded); err != nil {\n-\t\treturn \"\", fmt.Errorf(\"unmarshal encode response: %w\", err)\n-\t}\n-\n-\treturn decoded.Content, nil\n+\t// not reached\n+\treturn \"\", fmt.Errorf(\"no tokenizer configured\")\n }\n \n func (s *llmServer) Close() error {\n-\ts.modelLock.Lock()\n-\tif s.model != nil {\n-\t\tllama.FreeModel(s.model)\n-\t\ts.model = nil\n+\tif s.llamaModel != nil {\n+\t\tllama.FreeModel(s.llamaModel)\n+\t\ts.llamaModel = nil\n \t}\n-\ts.modelLock.Unlock()",
        "comment_created_at": "2025-03-03T20:55:34+00:00",
        "comment_author": "jessegross",
        "comment_body": "I think there might be a race with this lock removed. If `llmServer.Completion` gets an error, it will directly call `llmServer.Close`. However, there can still be outstanding references to the llmServer, one of which may call Tokenize and crash. With the lock, it won't crash but will still result in an error.",
        "pr_file_module": null
      }
    ]
  }
]