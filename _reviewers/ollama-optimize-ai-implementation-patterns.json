[
  {
    "discussion_id": "1391756974",
    "pr_number": 1104,
    "pr_file": "examples/jupyter-notebook/ollama.ipynb",
    "created_at": "2023-11-13T22:07:20+00:00",
    "commented_code": "+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"38d57674-b3d5-40f3-ab83-9109df3a7821\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Ollama Jupyter Notebook\n\",\n+    \"\n\",\n+    \"Ollama is the easiest way to run large language models (LLMs) locally. You can deploy it to macOS by installing the the macOS application, Linux by running the install script (below), and Docker or Kubernetes by pulling the official Ollama Docker image.\n\",\n+    \"\n\",\n+    \"For best results, this notebook should be run on a Linux node with GPU or an environment like Google Colab.\"",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1391756974",
        "repo_full_name": "ollama/ollama",
        "pr_number": 1104,
        "pr_file": "examples/jupyter-notebook/ollama.ipynb",
        "discussion_id": "1391756974",
        "commented_code": "@@ -0,0 +1,113 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"38d57674-b3d5-40f3-ab83-9109df3a7821\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# Ollama Jupyter Notebook\\n\",\n+    \"\\n\",\n+    \"Ollama is the easiest way to run large language models (LLMs) locally. You can deploy it to macOS by installing the the macOS application, Linux by running the install script (below), and Docker or Kubernetes by pulling the official Ollama Docker image.\\n\",\n+    \"\\n\",\n+    \"For best results, this notebook should be run on a Linux node with GPU or an environment like Google Colab.\"",
        "comment_created_at": "2023-11-13T22:07:20+00:00",
        "comment_author": "BruceMacD",
        "comment_body": "```suggestion\r\n    \"For best results, this notebook should be run on a Linux node with a GPU or an environment like Google Colab.\"\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070760290",
    "pr_number": 10099,
    "pr_file": "llama/patches/0022-metal-add-op_neg.patch",
    "created_at": "2025-05-01T20:23:58+00:00",
    "commented_code": "+From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001\n+From: Michael Yang <git@mxy.ng>\n+Date: Wed, 2 Apr 2025 15:26:15 -0700\n+Subject: [PATCH] metal: add op_neg",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "2070760290",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10099,
        "pr_file": "llama/patches/0022-metal-add-op_neg.patch",
        "discussion_id": "2070760290",
        "commented_code": "@@ -0,0 +1,75 @@\n+From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001\n+From: Michael Yang <git@mxy.ng>\n+Date: Wed, 2 Apr 2025 15:26:15 -0700\n+Subject: [PATCH] metal: add op_neg",
        "comment_created_at": "2025-05-01T20:23:58+00:00",
        "comment_author": "ngxson",
        "comment_body": "AFAIK we usually do `ggml_scale(ctx, cur, -1)` instead of using `ggml_neg`, so that's why we don't implement this op on every backend. The performance and memory should be the same with `ggml_scale_inplace`. Could this allow you to carry one less patch?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2029498210",
    "pr_number": 10096,
    "pr_file": "llama/llama.cpp/src/llama-grammar.h",
    "created_at": "2025-04-04T22:22:49+00:00",
    "commented_code": "#include <vector>\n \n struct llama_vocab;\n+struct ollama_vocab {\n+    std::map<uint32_t, std::string> token_to_piece_map;\n+    uint32_t eog_token;",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "2029498210",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10096,
        "pr_file": "llama/llama.cpp/src/llama-grammar.h",
        "discussion_id": "2029498210",
        "commented_code": "@@ -7,6 +7,17 @@\n #include <vector>\n \n struct llama_vocab;\n+struct ollama_vocab {\n+    std::map<uint32_t, std::string> token_to_piece_map;\n+    uint32_t eog_token;",
        "comment_created_at": "2025-04-04T22:22:49+00:00",
        "comment_author": "jmorganca",
        "comment_body": "There may be multiple eog tokens, specifically EOS or EOT",
        "pr_file_module": null
      }
    ]
  }
]