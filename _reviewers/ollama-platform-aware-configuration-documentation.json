[
  {
    "discussion_id": "1884573850",
    "pr_number": 8087,
    "pr_file": "docs/linux.md",
    "created_at": "2024-12-13T21:42:55+00:00",
    "commented_code": "```shell\n sudo rm $(which ollama)\n ```\n-\n+Remove the ollama libraries from your lib directory (`/usr/lib`):\n+```shell\n+sudo rm /usr/lib/ollama",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1884573850",
        "repo_full_name": "ollama/ollama",
        "pr_number": 8087,
        "pr_file": "docs/linux.md",
        "discussion_id": "1884573850",
        "commented_code": "@@ -178,7 +178,10 @@ Remove the ollama binary from your bin directory (either `/usr/local/bin`, `/usr\n ```shell\n sudo rm $(which ollama)\n ```\n-\n+Remove the ollama libraries from your lib directory (`/usr/lib`):\n+```shell\n+sudo rm /usr/lib/ollama",
        "comment_created_at": "2024-12-13T21:42:55+00:00",
        "comment_author": "dhiltgen",
        "comment_body": "Depending on the system, and if it was manually installed or installed with the official [install script](https://github.com/ollama/ollama/blob/main/scripts/install.sh#L69-L71), the location may vary.  The step above should locate ollama, so perhaps we should adjust the sequence to leverage that.  Something like the following as a step **before** removing the main executable:\r\n```shell\r\nsudo rm -r $(dirname $(which ollama))/../lib/ollama/\r\n```\r\n(also needs the `-r` to handle directories.)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2039438385",
    "pr_number": 10224,
    "pr_file": "docs/import.md",
    "created_at": "2025-04-11T12:05:12+00:00",
    "commented_code": "success\n ```\n \n+**Note:** When quantizing models, Ollama temporarily writes the model files to your system's temp directory, make sure your `TMP` envvar is set to a drive where you have sufficient space to avoid issues.",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "2039438385",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10224,
        "pr_file": "docs/import.md",
        "discussion_id": "2039438385",
        "commented_code": "@@ -130,6 +130,8 @@ writing manifest\n success\n ```\n \n+**Note:** When quantizing models, Ollama temporarily writes the model files to your system's temp directory, make sure your `TMP` envvar is set to a drive where you have sufficient space to avoid issues. ",
        "comment_created_at": "2025-04-11T12:05:12+00:00",
        "comment_author": "rick-github",
        "comment_body": "The variable depends on the OS.  On Windows, it's the first non-empty value from %TMP%, %TEMP%, %USERPROFILE%, or the Windows directory. On Linux it's TMPDIR, I guess it's the same for MacOS but I don't know for sure.  Also include that it needs to be set in the server environment.",
        "pr_file_module": null
      },
      {
        "comment_id": "2040418716",
        "repo_full_name": "ollama/ollama",
        "pr_number": 10224,
        "pr_file": "docs/import.md",
        "discussion_id": "2039438385",
        "commented_code": "@@ -130,6 +130,8 @@ writing manifest\n success\n ```\n \n+**Note:** When quantizing models, Ollama temporarily writes the model files to your system's temp directory, make sure your `TMP` envvar is set to a drive where you have sufficient space to avoid issues. ",
        "comment_created_at": "2025-04-11T23:25:36+00:00",
        "comment_author": "thot-experiment",
        "comment_body": "That appears to be correct. https://osxdaily.com/2018/08/17/where-temp-folder-mac-access/",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1960139509",
    "pr_number": 9120,
    "pr_file": "docs/development.md",
    "created_at": "2025-02-18T16:54:12+00:00",
    "commented_code": "go run . serve\n ```\n \n-## Windows\n+## Windows (x64)",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1960139509",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9120,
        "pr_file": "docs/development.md",
        "discussion_id": "1960139509",
        "commented_code": "@@ -34,7 +34,7 @@ Lastly, run Ollama:\n go run . serve\n ```\n \n-## Windows\n+## Windows (x64)",
        "comment_created_at": "2025-02-18T16:54:12+00:00",
        "comment_author": "jmorganca",
        "comment_body": "It may be simpler to keep this `Windows` but to mention in a warning below acceleration libraries aren't currently supported for ARM (vs two sections)",
        "pr_file_module": null
      },
      {
        "comment_id": "1962595907",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9120,
        "pr_file": "docs/development.md",
        "discussion_id": "1960139509",
        "commented_code": "@@ -34,7 +34,7 @@ Lastly, run Ollama:\n go run . serve\n ```\n \n-## Windows\n+## Windows (x64)",
        "comment_created_at": "2025-02-20T00:55:53+00:00",
        "comment_author": "dhiltgen",
        "comment_body": "Have we found an easier way to get a gcc compatible compiler wrapper on Windows ARM?  Msys2 isn't the easiest, so if we haven't found an easier one, I think we should have a bit more guidance to help users get a working setup installed.",
        "pr_file_module": null
      },
      {
        "comment_id": "1970687562",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9120,
        "pr_file": "docs/development.md",
        "discussion_id": "1960139509",
        "commented_code": "@@ -34,7 +34,7 @@ Lastly, run Ollama:\n go run . serve\n ```\n \n-## Windows\n+## Windows (x64)",
        "comment_created_at": "2025-02-25T23:35:00+00:00",
        "comment_author": "jmorganca",
        "comment_body": "The top of the doc (perhaps a bit too easy to glance over) has a link to TDM-gcc which works pretty well.",
        "pr_file_module": null
      },
      {
        "comment_id": "1970687684",
        "repo_full_name": "ollama/ollama",
        "pr_number": 9120,
        "pr_file": "docs/development.md",
        "discussion_id": "1960139509",
        "commented_code": "@@ -34,7 +34,7 @@ Lastly, run Ollama:\n go run . serve\n ```\n \n-## Windows\n+## Windows (x64)",
        "comment_created_at": "2025-02-25T23:35:14+00:00",
        "comment_author": "jmorganca",
        "comment_body": "(and llvm-mingw for ARM)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1384144711",
    "pr_number": 980,
    "pr_file": "README.md",
    "created_at": "2023-11-06T22:49:45+00:00",
    "commented_code": "The official [Ollama Docker image `ollama/ollama`](https://hub.docker.com/r/ollama/ollama)\n is available on Docker Hub.\n \n+### Autocompletion\n+To enable autocompletion generate autocompletion script for your shell and `source` generated file in your shell config (`~/.bash_profile`, `~/.zshrc`, `~/.profile`, `~/.bashrc`, etc.):\n+```\n+ollama completions [bash|zsh|fish] /path/to/completion/file",
    "repo_full_name": "ollama/ollama",
    "discussion_comments": [
      {
        "comment_id": "1384144711",
        "repo_full_name": "ollama/ollama",
        "pr_number": 980,
        "pr_file": "README.md",
        "discussion_id": "1384144711",
        "commented_code": "@@ -32,6 +32,12 @@ curl https://ollama.ai/install.sh | sh\n The official [Ollama Docker image `ollama/ollama`](https://hub.docker.com/r/ollama/ollama)\n is available on Docker Hub.\n \n+### Autocompletion\n+To enable autocompletion generate autocompletion script for your shell and `source` generated file in your shell config (`~/.bash_profile`, `~/.zshrc`, `~/.profile`, `~/.bashrc`, etc.):\n+```\n+ollama completions [bash|zsh|fish] /path/to/completion/file",
        "comment_created_at": "2023-11-06T22:49:45+00:00",
        "comment_author": "pdevine",
        "comment_body": "We should let the user deal with the file handling here, and go with \"completion\" over \"completions\" since that what docker/kubectl/brew use.\r\n\r\n```\r\nollama completion [bash|zsh|fish] > /path/to/completion/file\r\n```",
        "pr_file_module": null
      }
    ]
  }
]