[
  {
    "discussion_id": "1896466401",
    "pr_number": 25584,
    "pr_file": "modules/videoio/misc/python/pyopencv_videoio.hpp",
    "created_at": "2024-12-24T06:56:18+00:00",
    "commented_code": "return true;\n}\n\nclass IOBaseWrapper : public std::streambuf\n{\npublic:\n    IOBaseWrapper(PyObject* _obj = nullptr) : obj(_obj)\n    {\n        if (obj)\n            Py_INCREF(obj);\n    }\n\n    ~IOBaseWrapper()\n    {\n        if (obj)\n            Py_DECREF(obj);\n    }\n\n    std::streamsize xsgetn(char* buf, std::streamsize n) override\n    {\n        PyObject* ioBase = reinterpret_cast<PyObject*>(obj);\n\n        PyGILState_STATE gstate;\n        gstate = PyGILState_Ensure();\n\n        PyObject* size = pyopencv_from(static_cast<int>(n));\n\n        PyObject* res = PyObject_CallMethodObjArgs(ioBase, PyString_FromString(\"read\"), size, NULL);\n        char* src = PyBytes_AsString(res);\n        size_t len = static_cast<size_t>(PyBytes_Size(res));\n        CV_CheckLE(len, static_cast<size_t>(n), \"Stream chunk size should be less or equal than requested size\");\n        std::memcpy(buf, src, len);\n        Py_DECREF(res);\n        Py_DECREF(size);\n\n        PyGILState_Release(gstate);\n\n        return len;\n    }\n\n    std::streampos seekoff(std::streamoff off, std::ios_base::seekdir way, std::ios_base::openmode = std::ios_base::in | std::ios_base::out) override\n    {\n        PyObject* ioBase = reinterpret_cast<PyObject*>(obj);\n\n        PyGILState_STATE gstate;\n        gstate = PyGILState_Ensure();\n\n        PyObject* size = pyopencv_from(static_cast<int>(off));\n        PyObject* whence = pyopencv_from(way == std::ios_base::beg ? SEEK_SET : (way == std::ios_base::end ? SEEK_END : SEEK_CUR));\n\n        PyObject* res = PyObject_CallMethodObjArgs(ioBase, PyString_FromString(\"seek\"), size, whence, NULL);\n        int pos = PyLong_AsLong(res);\n        Py_DECREF(res);\n        Py_DECREF(size);\n        Py_DECREF(whence);\n\n        PyGILState_Release(gstate);\n\n        return pos;\n    }\n\nprivate:\n    PyObject* obj;\n};\n\ntemplate<>\nbool pyopencv_to(PyObject* obj, Ptr<std::streambuf>& p, const ArgInfo&)\n{\n    if (!obj)\n        return false;\n\n    PyGILState_STATE gstate;\n    gstate = PyGILState_Ensure();\n\n    PyObject* ioModule = PyImport_ImportModule(\"io\");\n    PyObject* type = PyObject_GetAttrString(ioModule, \"BufferedIOBase\");\n    Py_DECREF(ioModule);\n    if (!PyObject_IsInstance(obj, type))\n        CV_Error(cv::Error::StsBadArg, \"Input stream should be derived from io.BufferedIOBase\");",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "1896466401",
        "repo_full_name": "opencv/opencv",
        "pr_number": 25584,
        "pr_file": "modules/videoio/misc/python/pyopencv_videoio.hpp",
        "discussion_id": "1896466401",
        "commented_code": "@@ -31,4 +31,87 @@ template<> bool pyopencv_to(PyObject* obj, cv::VideoCapture& stream, const ArgIn\n     return true;\n }\n \n+class IOBaseWrapper : public std::streambuf\n+{\n+public:\n+    IOBaseWrapper(PyObject* _obj = nullptr) : obj(_obj)\n+    {\n+        if (obj)\n+            Py_INCREF(obj);\n+    }\n+\n+    ~IOBaseWrapper()\n+    {\n+        if (obj)\n+            Py_DECREF(obj);\n+    }\n+\n+    std::streamsize xsgetn(char* buf, std::streamsize n) override\n+    {\n+        PyObject* ioBase = reinterpret_cast<PyObject*>(obj);\n+\n+        PyGILState_STATE gstate;\n+        gstate = PyGILState_Ensure();\n+\n+        PyObject* size = pyopencv_from(static_cast<int>(n));\n+\n+        PyObject* res = PyObject_CallMethodObjArgs(ioBase, PyString_FromString(\"read\"), size, NULL);\n+        char* src = PyBytes_AsString(res);\n+        size_t len = static_cast<size_t>(PyBytes_Size(res));\n+        CV_CheckLE(len, static_cast<size_t>(n), \"Stream chunk size should be less or equal than requested size\");\n+        std::memcpy(buf, src, len);\n+        Py_DECREF(res);\n+        Py_DECREF(size);\n+\n+        PyGILState_Release(gstate);\n+\n+        return len;\n+    }\n+\n+    std::streampos seekoff(std::streamoff off, std::ios_base::seekdir way, std::ios_base::openmode = std::ios_base::in | std::ios_base::out) override\n+    {\n+        PyObject* ioBase = reinterpret_cast<PyObject*>(obj);\n+\n+        PyGILState_STATE gstate;\n+        gstate = PyGILState_Ensure();\n+\n+        PyObject* size = pyopencv_from(static_cast<int>(off));\n+        PyObject* whence = pyopencv_from(way == std::ios_base::beg ? SEEK_SET : (way == std::ios_base::end ? SEEK_END : SEEK_CUR));\n+\n+        PyObject* res = PyObject_CallMethodObjArgs(ioBase, PyString_FromString(\"seek\"), size, whence, NULL);\n+        int pos = PyLong_AsLong(res);\n+        Py_DECREF(res);\n+        Py_DECREF(size);\n+        Py_DECREF(whence);\n+\n+        PyGILState_Release(gstate);\n+\n+        return pos;\n+    }\n+\n+private:\n+    PyObject* obj;\n+};\n+\n+template<>\n+bool pyopencv_to(PyObject* obj, Ptr<std::streambuf>& p, const ArgInfo&)\n+{\n+    if (!obj)\n+        return false;\n+\n+    PyGILState_STATE gstate;\n+    gstate = PyGILState_Ensure();\n+\n+    PyObject* ioModule = PyImport_ImportModule(\"io\");\n+    PyObject* type = PyObject_GetAttrString(ioModule, \"BufferedIOBase\");\n+    Py_DECREF(ioModule);\n+    if (!PyObject_IsInstance(obj, type))\n+        CV_Error(cv::Error::StsBadArg, \"Input stream should be derived from io.BufferedIOBase\");",
        "comment_created_at": "2024-12-24T06:56:18+00:00",
        "comment_author": "opencv-alalek",
        "comment_body": "`CV_Error` doesn't call cleanup (release GIL and release `type`).\r\n\r\n```\r\nif (...)\r\n    has_error = true;\r\n\r\n... cleanup ...\r\n\r\nif (has_error)\r\n    CV_Error(...);\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2010187533",
    "pr_number": 27115,
    "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/norm.hpp",
    "created_at": "2025-03-24T13:30:33+00:00",
    "commented_code": "sizeof(int),     sizeof(float),\n        sizeof(int64_t), 0,\n    };\n    assert(elem_size_tab[depth]);",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "2010187533",
        "repo_full_name": "opencv/opencv",
        "pr_number": 27115,
        "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/norm.hpp",
        "discussion_id": "2010187533",
        "commented_code": "@@ -1008,6 +1008,7 @@ inline int norm(const uchar* src, size_t src_step, const uchar* mask, size_t mas\n         sizeof(int),     sizeof(float),\n         sizeof(int64_t), 0,\n     };\n+    assert(elem_size_tab[depth]);",
        "comment_created_at": "2025-03-24T13:30:33+00:00",
        "comment_author": "asmorkalov",
        "comment_body": "Looks like the assert will be disabled in regular release builds: https://en.cppreference.com/w/cpp/error/assert.\r\nWhy not just CV_Assert? It's defined in `opencv2/core/base.hpp`",
        "pr_file_module": null
      },
      {
        "comment_id": "2010279840",
        "repo_full_name": "opencv/opencv",
        "pr_number": 27115,
        "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/norm.hpp",
        "discussion_id": "2010187533",
        "commented_code": "@@ -1008,6 +1008,7 @@ inline int norm(const uchar* src, size_t src_step, const uchar* mask, size_t mas\n         sizeof(int),     sizeof(float),\n         sizeof(int64_t), 0,\n     };\n+    assert(elem_size_tab[depth]);",
        "comment_created_at": "2025-03-24T14:19:36+00:00",
        "comment_author": "fengyuentau",
        "comment_body": "Ok",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1975939795",
    "pr_number": 26958,
    "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/pyramids.hpp",
    "created_at": "2025-02-28T19:51:05+00:00",
    "commented_code": "// This file is part of OpenCV project.\n// It is subject to the license terms in the LICENSE file found in the top-level directory\n// of this distribution and at http://opencv.org/license.html.\n#ifndef OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n#define OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n\n#include <riscv_vector.h>\n\nnamespace cv { namespace cv_hal_rvv { namespace pyramids {\n\n#undef cv_hal_pyrdown\n#define cv_hal_pyrdown cv::cv_hal_rvv::pyramids::pyrDown\n#undef cv_hal_pyrup\n#define cv_hal_pyrup cv::cv_hal_rvv::pyramids::pyrUp\n\ntemplate<typename T> struct rvv;\n\ntemplate<> struct rvv<uchar>\n{\n    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n    static inline vuint8m1_t vle_T(const uchar* a, size_t b) { return __riscv_vle8_v_u8m1(a, b); }\n    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n    static inline vuint8m1_t vlse_T(const uchar* a, ptrdiff_t b, size_t c) { return __riscv_vlse8_v_u8m1(a, b, c); }\n    static inline vuint8m1_t vloxei_T(const uchar* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_u8m1(a, b, c); }\n    static inline void vse_T(uchar* a, vuint8m1_t b, size_t c) { return __riscv_vse8(a, b, c); }\n    static inline vint32m4_t vcvt_T_WT(vuint8m1_t a, size_t b) { return __riscv_vreinterpret_v_u32m4_i32m4(__riscv_vzext_vf4(a, b)); }\n    static inline vuint8m1_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vncvt_x(__riscv_vreinterpret_v_i32m4_u32m4(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c)), c), c); }\n};\n\ntemplate<> struct rvv<short>\n{\n    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n    static inline vint16m2_t vle_T(const short* a, size_t b) { return __riscv_vle16_v_i16m2(a, b); }\n    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n    static inline vint16m2_t vlse_T(const short* a, ptrdiff_t b, size_t c) { return __riscv_vlse16_v_i16m2(a, b, c); }\n    static inline vint16m2_t vloxei_T(const short* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_i16m2(a, b, c); }\n    static inline void vse_T(short* a, vint16m2_t b, size_t c) { return __riscv_vse16(a, b, c); }\n    static inline vint32m4_t vcvt_T_WT(vint16m2_t a, size_t b) { return __riscv_vsext_vf2(a, b); }\n    static inline vint16m2_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c), c); }\n};\n\ntemplate<> struct rvv<float>\n{\n    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n    static inline vfloat32m4_t vle_T(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n    static inline vfloat32m4_t vle_WT(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n    static inline vfloat32m4_t vlse_T(const float* a, ptrdiff_t b, size_t c) { return __riscv_vlse32_v_f32m4(a, b, c); }\n    static inline vfloat32m4_t vloxei_T(const float* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_f32m4(a, b, c); }\n    static inline void vse_T(float* a, vfloat32m4_t b, size_t c) { return __riscv_vse32(a, b, c); }\n};\n\ntemplate<typename T, typename WT> struct pyrDownVec0\n{\n    void operator()(const T* src, WT* row, const uint* tabM, int start, int end)\n    {\n        int vl;\n        switch (start)\n        {\n        case 1:\n            for( int x = start; x < end; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 2, 2 * sizeof(T), vl), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 1, 2 * sizeof(T), vl), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2, 2 * sizeof(T), vl), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 1, 2 * sizeof(T), vl), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 2, 2 * sizeof(T), vl), vl);\n                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n            break;\n        case 2:\n            for( int x = start / 2; x < end / 2; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end / 2 - x);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(T), vl), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(T), vl), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2, 4 * sizeof(T), vl), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(T), vl), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 2, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n            break;\n        case 3:\n            for( int x = start / 3; x < end / 3; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end / 3 - x);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(T), vl), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(T), vl), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3, 6 * sizeof(T), vl), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(T), vl), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 3, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n            break;\n        case 4:\n            for( int x = start / 4; x < end / 4; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end / 4 - x);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(T), vl), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(T), vl), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4, 8 * sizeof(T), vl), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(T), vl), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 4, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n            break;\n        default:\n            for( int x = start; x < end; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end - x);\n                auto vec_tabM = rvv<T>::vle_M(tabM + x, vl);\n                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(T), vl);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n        }\n    }\n};\ntemplate<> struct pyrDownVec0<float, float>\n{\n    void operator()(const float* src, float* row, const uint* tabM, int start, int end)\n    {\n        int vl;\n        switch (start)\n        {\n        case 1:\n            for( int x = start; x < end; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<float>::vlse_T(src + x * 2 - 2, 2 * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vlse_T(src + x * 2 - 1, 2 * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vlse_T(src + x * 2, 2 * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vlse_T(src + x * 2 + 1, 2 * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vlse_T(src + x * 2 + 2, 2 * sizeof(float), vl);\n                __riscv_vse32(row + x, __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n            }\n            break;\n        case 2:\n            for( int x = start / 2; x < end / 2; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end / 2 - x);\n                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2, 4 * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 2, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n            }\n            break;\n        case 3:\n            for( int x = start / 3; x < end / 3; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end / 3 - x);\n                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3, 6 * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 3, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n            }\n            break;\n        case 4:\n            for( int x = start / 4; x < end / 4; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end / 4 - x);\n                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4, 8 * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 4, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n            }\n            break;\n        default:\n            for( int x = start; x < end; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end - x);\n                auto vec_tabM = rvv<float>::vle_M(tabM + x, vl);\n                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(float), vl);\n                auto vec_src0 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                __riscv_vse32(row + x, __riscv_vfmadd(__riscv_vfadd(__riscv_vfadd(vec_src1, vec_src2, vl), vec_src3, vl), 4,\n                                                      __riscv_vfadd(__riscv_vfadd(vec_src0, vec_src4, vl), __riscv_vfadd(vec_src2, vec_src2, vl), vl), vl), vl);\n            }\n        }\n    }\n};\n\ntemplate<typename T, typename WT> struct pyrDownVec1\n{\n    void operator()(WT* row0, WT* row1, WT* row2, WT* row3, WT* row4, T* dst, int end)\n    {\n        int vl;\n        for( int x = 0 ; x < end; x += vl )\n        {\n            vl = rvv<T>::vsetvl_WT(end - x);\n            auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n            auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n            auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n            auto vec_src3 = rvv<T>::vle_WT(row3 + x, vl);\n            auto vec_src4 = rvv<T>::vle_WT(row4 + x, vl);\n            rvv<T>::vse_T(dst + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                                      __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), 8, vl), vl);\n        }\n    }\n};\ntemplate<> struct pyrDownVec1<float, float>\n{\n    void operator()(float* row0, float* row1, float* row2, float* row3, float* row4, float* dst, int end)\n    {\n        int vl;\n        for( int x = 0 ; x < end; x += vl )\n        {\n            vl = rvv<float>::vsetvl_WT(end - x);\n            auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n            auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n            auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n            auto vec_src3 = rvv<float>::vle_WT(row3 + x, vl);\n            auto vec_src4 = rvv<float>::vle_WT(row4 + x, vl);\n            rvv<float>::vse_T(dst + x, __riscv_vfmul(__riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), 1.f / 256.f, vl), vl);\n        }\n    }\n};\n\ntemplate<typename T, typename WT> struct pyrUpVec0\n{\n    void operator()(const T* src, WT* row, const uint* dtab, int start, int end)\n    {\n        int vl;\n        for( int x = start; x < end; x += vl )\n        {\n            vl = rvv<T>::vsetvl_WT(end - x);\n            auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x - start, vl), vl);\n            auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x, vl), vl);\n            auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x + start, vl), vl);\n\n            auto vec_dtab = rvv<T>::vle_M(dtab + x, vl);\n            vec_dtab = __riscv_vmul(vec_dtab, sizeof(WT), vl);\n            __riscv_vsoxei32(row, vec_dtab, __riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), vl);\n            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(WT), vl), __riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), vl);\n        }\n    }\n};\ntemplate<> struct pyrUpVec0<float, float>\n{\n    void operator()(const float* src, float* row, const uint* dtab, int start, int end)\n    {\n        int vl;\n        for( int x = start; x < end; x += vl )\n        {\n            vl = rvv<float>::vsetvl_WT(end - x);\n            auto vec_src0 = rvv<float>::vle_T(src + x - start, vl);\n            auto vec_src1 = rvv<float>::vle_T(src + x, vl);\n            auto vec_src2 = rvv<float>::vle_T(src + x + start, vl);\n\n            auto vec_dtab = rvv<float>::vle_M(dtab + x, vl);\n            vec_dtab = __riscv_vmul(vec_dtab, sizeof(float), vl);\n            __riscv_vsoxei32(row, vec_dtab, __riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), vl);\n            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(float), vl), __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 4, vl), vl);\n        }\n    }\n};\n\ntemplate<typename T, typename WT> struct pyrUpVec1\n{\n    void operator()(WT* row0, WT* row1, WT* row2, T* dst0, T* dst1, int end)\n    {\n        int vl;\n        if (dst0 != dst1)\n        {\n            for( int x = 0 ; x < end; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n                rvv<T>::vse_T(dst1 + x, rvv<T>::vcvt_WT_T(__riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), 6, vl), vl);\n            }\n        }\n        else\n        {\n            for( int x = 0 ; x < end; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n            }\n        }\n    }\n};\ntemplate<> struct pyrUpVec1<float, float>\n{\n    void operator()(float* row0, float* row1, float* row2, float* dst0, float* dst1, int end)\n    {\n        int vl;\n        if (dst0 != dst1)\n        {\n            for( int x = 0 ; x < end; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n                rvv<float>::vse_T(dst1 + x, __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 1.f / 16.f, vl), vl);\n            }\n        }\n        else\n        {\n            for( int x = 0 ; x < end; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n            }\n        }\n    }\n};\n\ntemplate<typename T, typename WT>\nstruct PyrDownInvoker : ParallelLoopBody\n{\n    PyrDownInvoker(const uchar* _src_data, size_t _src_step, int _src_width, int _src_height, uchar* _dst_data, size_t _dst_step, int _dst_width, int _dst_height, int _cn, int _borderType, int* _tabR, int* _tabM, int* _tabL)\n    {\n        src_data = _src_data;\n        src_step = _src_step;\n        src_width = _src_width;\n        src_height = _src_height;\n        dst_data = _dst_data;\n        dst_step = _dst_step;\n        dst_width = _dst_width;\n        dst_height = _dst_height;\n        cn = _cn;\n        borderType = _borderType;\n        tabR = _tabR;\n        tabM = _tabM;\n        tabL = _tabL;\n    }\n\n    void operator()(const Range& range) const CV_OVERRIDE;\n\n    const uchar* src_data;\n    size_t src_step;\n    int src_width;\n    int src_height;\n    uchar* dst_data;\n    size_t dst_step;\n    int dst_width;\n    int dst_height;\n    int cn;\n    int borderType;\n    int* tabR;\n    int* tabM;\n    int* tabL;\n};\n\n// the algorithm is copied from imgproc/src/pyramids.cpp,\n// in the function template void cv::pyrDown_\ntemplate<typename T, typename WT>\ninline int pyrDown(const uchar* src_data, size_t src_step, int src_width, int src_height, uchar* dst_data, size_t dst_step, int dst_width, int dst_height, int cn, int borderType)\n{\n    const int PD_SZ = 5;\n\n    std::vector<int> _tabM(dst_width * cn), _tabL(cn * (PD_SZ + 2)), _tabR(cn * (PD_SZ + 2));\n    int *tabM = _tabM.data(), *tabL = _tabL.data(), *tabR = _tabR.data();\n\n    CV_Assert( src_width > 0 && src_height > 0 &&\n               std::abs(dst_width*2 - src_width) <= 2 &&\n               std::abs(dst_height*2 - src_height) <= 2 );\n    int width0 = std::min((src_width-PD_SZ/2-1)/2 + 1, dst_width);\n\n    for (int x = 0; x <= PD_SZ+1; x++)\n    {\n        int sx0 = borderInterpolate(x - PD_SZ/2, src_width, borderType)*cn;\n        int sx1 = borderInterpolate(x + width0*2 - PD_SZ/2, src_width, borderType)*cn;\n        for (int k = 0; k < cn; k++)\n        {\n            tabL[x*cn + k] = sx0 + k;\n            tabR[x*cn + k] = sx1 + k;\n        }\n    }\n\n    for (int x = 0; x < dst_width*cn; x++)\n        tabM[x] = (x/cn)*2*cn + x % cn;\n\n    cv::parallel_for_(Range(0,dst_height), PyrDownInvoker<T, WT>(src_data, src_step, src_width, src_height, dst_data, dst_step, dst_width, dst_height, cn, borderType, tabR, tabM, tabL), cv::getNumThreads());\n    return CV_HAL_ERROR_OK;\n}\n\ntemplate<typename T, typename WT>\nvoid PyrDownInvoker<T, WT>::operator()(const Range& range) const\n{\n    const int PD_SZ = 5;\n\n    int bufstep = (dst_width*cn + 15) & -16;\n    std::vector<WT> _buf(bufstep*PD_SZ + 16);\n    WT* buf = (WT*)(((size_t)_buf.data() + 15) & -16);\n    WT* rows[PD_SZ];\n\n    int sy0 = -PD_SZ/2, sy = range.start * 2 + sy0, width0 = std::min((src_width-PD_SZ/2-1)/2 + 1, dst_width);\n\n    int _dst_width = dst_width * cn;\n    width0 *= cn;\n\n    for (int y = range.start; y < range.end; y++)\n    {\n        T* dst = reinterpret_cast<T*>(dst_data + dst_step * y);\n        WT *row0, *row1, *row2, *row3, *row4;\n\n        // fill the ring buffer (horizontal convolution and decimation)\n        int sy_limit = y*2 + 2;\n        for( ; sy <= sy_limit; sy++ )\n        {\n            WT* row = buf + ((sy - sy0) % PD_SZ)*bufstep;\n            int _sy = borderInterpolate(sy, src_height, borderType);\n            const T* src = reinterpret_cast<const T*>(src_data + src_step * _sy);\n\n            do {\n                int x = 0;\n                for( ; x < cn; x++ )\n                {\n                    row[x] = src[tabL[x+cn*2]]*6 + (src[tabL[x+cn]] + src[tabL[x+cn*3]])*4 +\n                        src[tabL[x]] + src[tabL[x+cn*4]];\n                }\n\n                if( x == _dst_width )\n                    break;\n\n                pyrDownVec0<T, WT>()(src, row, reinterpret_cast<const uint*>(tabM), cn, width0);\n                x = width0;\n\n                // tabR\n                for (int x_ = 0; x < _dst_width; x++, x_++)\n                {\n                    row[x] = src[tabR[x_+cn*2]]*6 + (src[tabR[x_+cn]] + src[tabR[x_+cn*3]])*4 +\n                        src[tabR[x_]] + src[tabR[x_+cn*4]];\n                }\n            } while (0);\n        }\n\n        // do vertical convolution and decimation and write the result to the destination image\n        for (int k = 0; k < PD_SZ; k++)\n            rows[k] = buf + ((y*2 - PD_SZ/2 + k - sy0) % PD_SZ)*bufstep;\n        row0 = rows[0]; row1 = rows[1]; row2 = rows[2]; row3 = rows[3]; row4 = rows[4];\n\n        pyrDownVec1<T, WT>()(row0, row1, row2, row3, row4, dst, _dst_width);\n    }\n}\n\n// the algorithm is copied from imgproc/src/pyramids.cpp,\n// in the function template void cv::pyrUp_\ntemplate<typename T, typename WT>\ninline int pyrUp(const uchar* src_data, size_t src_step, int src_width, int src_height, uchar* dst_data, size_t dst_step, int dst_width, int dst_height, int cn)\n{\n    const int PU_SZ = 3;\n\n    int bufstep = ((dst_width+1)*cn + 15) & -16;\n    std::vector<WT> _buf(bufstep*PU_SZ + 16);\n    WT* buf = (WT*)(((size_t)_buf.data() + 15) & -16);\n    std::vector<int> _dtab(src_width*cn);\n    int* dtab = _dtab.data();\n    WT* rows[PU_SZ];\n\n    CV_Assert( std::abs(dst_width - src_width*2) == dst_width % 2 &&",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "1975939795",
        "repo_full_name": "opencv/opencv",
        "pr_number": 26958,
        "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/pyramids.hpp",
        "discussion_id": "1975939795",
        "commented_code": "@@ -0,0 +1,681 @@\n+// This file is part of OpenCV project.\n+// It is subject to the license terms in the LICENSE file found in the top-level directory\n+// of this distribution and at http://opencv.org/license.html.\n+#ifndef OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n+#define OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n+\n+#include <riscv_vector.h>\n+\n+namespace cv { namespace cv_hal_rvv { namespace pyramids {\n+\n+#undef cv_hal_pyrdown\n+#define cv_hal_pyrdown cv::cv_hal_rvv::pyramids::pyrDown\n+#undef cv_hal_pyrup\n+#define cv_hal_pyrup cv::cv_hal_rvv::pyramids::pyrUp\n+\n+template<typename T> struct rvv;\n+\n+template<> struct rvv<uchar>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vuint8m1_t vle_T(const uchar* a, size_t b) { return __riscv_vle8_v_u8m1(a, b); }\n+    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vuint8m1_t vlse_T(const uchar* a, ptrdiff_t b, size_t c) { return __riscv_vlse8_v_u8m1(a, b, c); }\n+    static inline vuint8m1_t vloxei_T(const uchar* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_u8m1(a, b, c); }\n+    static inline void vse_T(uchar* a, vuint8m1_t b, size_t c) { return __riscv_vse8(a, b, c); }\n+    static inline vint32m4_t vcvt_T_WT(vuint8m1_t a, size_t b) { return __riscv_vreinterpret_v_u32m4_i32m4(__riscv_vzext_vf4(a, b)); }\n+    static inline vuint8m1_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vncvt_x(__riscv_vreinterpret_v_i32m4_u32m4(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c)), c), c); }\n+};\n+\n+template<> struct rvv<short>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vint16m2_t vle_T(const short* a, size_t b) { return __riscv_vle16_v_i16m2(a, b); }\n+    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vint16m2_t vlse_T(const short* a, ptrdiff_t b, size_t c) { return __riscv_vlse16_v_i16m2(a, b, c); }\n+    static inline vint16m2_t vloxei_T(const short* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_i16m2(a, b, c); }\n+    static inline void vse_T(short* a, vint16m2_t b, size_t c) { return __riscv_vse16(a, b, c); }\n+    static inline vint32m4_t vcvt_T_WT(vint16m2_t a, size_t b) { return __riscv_vsext_vf2(a, b); }\n+    static inline vint16m2_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c), c); }\n+};\n+\n+template<> struct rvv<float>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vfloat32m4_t vle_T(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n+    static inline vfloat32m4_t vle_WT(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vfloat32m4_t vlse_T(const float* a, ptrdiff_t b, size_t c) { return __riscv_vlse32_v_f32m4(a, b, c); }\n+    static inline vfloat32m4_t vloxei_T(const float* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_f32m4(a, b, c); }\n+    static inline void vse_T(float* a, vfloat32m4_t b, size_t c) { return __riscv_vse32(a, b, c); }\n+};\n+\n+template<typename T, typename WT> struct pyrDownVec0\n+{\n+    void operator()(const T* src, WT* row, const uint* tabM, int start, int end)\n+    {\n+        int vl;\n+        switch (start)\n+        {\n+        case 1:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 2, 2 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 1, 2 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2, 2 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 1, 2 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 2, 2 * sizeof(T), vl), vl);\n+                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 2:\n+            for( int x = start / 2; x < end / 2; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 2 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 2, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 3:\n+            for( int x = start / 3; x < end / 3; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 3 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 4:\n+            for( int x = start / 4; x < end / 4; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 4 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        default:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_tabM = rvv<T>::vle_M(tabM + x, vl);\n+                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(T), vl);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+        }\n+    }\n+};\n+template<> struct pyrDownVec0<float, float>\n+{\n+    void operator()(const float* src, float* row, const uint* tabM, int start, int end)\n+    {\n+        int vl;\n+        switch (start)\n+        {\n+        case 1:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + x * 2 - 2, 2 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + x * 2 - 1, 2 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + x * 2, 2 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + x * 2 + 1, 2 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + x * 2 + 2, 2 * sizeof(float), vl);\n+                __riscv_vse32(row + x, __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 2:\n+            for( int x = start / 2; x < end / 2; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 2 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2, 4 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 2, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 3:\n+            for( int x = start / 3; x < end / 3; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 3 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3, 6 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 4:\n+            for( int x = start / 4; x < end / 4; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 4 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4, 8 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        default:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_tabM = rvv<float>::vle_M(tabM + x, vl);\n+                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(float), vl);\n+                auto vec_src0 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                __riscv_vse32(row + x, __riscv_vfmadd(__riscv_vfadd(__riscv_vfadd(vec_src1, vec_src2, vl), vec_src3, vl), 4,\n+                                                      __riscv_vfadd(__riscv_vfadd(vec_src0, vec_src4, vl), __riscv_vfadd(vec_src2, vec_src2, vl), vl), vl), vl);\n+            }\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrDownVec1\n+{\n+    void operator()(WT* row0, WT* row1, WT* row2, WT* row3, WT* row4, T* dst, int end)\n+    {\n+        int vl;\n+        for( int x = 0 ; x < end; x += vl )\n+        {\n+            vl = rvv<T>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+            auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+            auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+            auto vec_src3 = rvv<T>::vle_WT(row3 + x, vl);\n+            auto vec_src4 = rvv<T>::vle_WT(row4 + x, vl);\n+            rvv<T>::vse_T(dst + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                                      __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), 8, vl), vl);\n+        }\n+    }\n+};\n+template<> struct pyrDownVec1<float, float>\n+{\n+    void operator()(float* row0, float* row1, float* row2, float* row3, float* row4, float* dst, int end)\n+    {\n+        int vl;\n+        for( int x = 0 ; x < end; x += vl )\n+        {\n+            vl = rvv<float>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+            auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+            auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+            auto vec_src3 = rvv<float>::vle_WT(row3 + x, vl);\n+            auto vec_src4 = rvv<float>::vle_WT(row4 + x, vl);\n+            rvv<float>::vse_T(dst + x, __riscv_vfmul(__riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), 1.f / 256.f, vl), vl);\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrUpVec0\n+{\n+    void operator()(const T* src, WT* row, const uint* dtab, int start, int end)\n+    {\n+        int vl;\n+        for( int x = start; x < end; x += vl )\n+        {\n+            vl = rvv<T>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x - start, vl), vl);\n+            auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x, vl), vl);\n+            auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x + start, vl), vl);\n+\n+            auto vec_dtab = rvv<T>::vle_M(dtab + x, vl);\n+            vec_dtab = __riscv_vmul(vec_dtab, sizeof(WT), vl);\n+            __riscv_vsoxei32(row, vec_dtab, __riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), vl);\n+            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(WT), vl), __riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), vl);\n+        }\n+    }\n+};\n+template<> struct pyrUpVec0<float, float>\n+{\n+    void operator()(const float* src, float* row, const uint* dtab, int start, int end)\n+    {\n+        int vl;\n+        for( int x = start; x < end; x += vl )\n+        {\n+            vl = rvv<float>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<float>::vle_T(src + x - start, vl);\n+            auto vec_src1 = rvv<float>::vle_T(src + x, vl);\n+            auto vec_src2 = rvv<float>::vle_T(src + x + start, vl);\n+\n+            auto vec_dtab = rvv<float>::vle_M(dtab + x, vl);\n+            vec_dtab = __riscv_vmul(vec_dtab, sizeof(float), vl);\n+            __riscv_vsoxei32(row, vec_dtab, __riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), vl);\n+            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(float), vl), __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 4, vl), vl);\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrUpVec1\n+{\n+    void operator()(WT* row0, WT* row1, WT* row2, T* dst0, T* dst1, int end)\n+    {\n+        int vl;\n+        if (dst0 != dst1)\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n+                rvv<T>::vse_T(dst1 + x, rvv<T>::vcvt_WT_T(__riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), 6, vl), vl);\n+            }\n+        }\n+        else\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n+            }\n+        }\n+    }\n+};\n+template<> struct pyrUpVec1<float, float>\n+{\n+    void operator()(float* row0, float* row1, float* row2, float* dst0, float* dst1, int end)\n+    {\n+        int vl;\n+        if (dst0 != dst1)\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n+                rvv<float>::vse_T(dst1 + x, __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 1.f / 16.f, vl), vl);\n+            }\n+        }\n+        else\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n+            }\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT>\n+struct PyrDownInvoker : ParallelLoopBody\n+{\n+    PyrDownInvoker(const uchar* _src_data, size_t _src_step, int _src_width, int _src_height, uchar* _dst_data, size_t _dst_step, int _dst_width, int _dst_height, int _cn, int _borderType, int* _tabR, int* _tabM, int* _tabL)\n+    {\n+        src_data = _src_data;\n+        src_step = _src_step;\n+        src_width = _src_width;\n+        src_height = _src_height;\n+        dst_data = _dst_data;\n+        dst_step = _dst_step;\n+        dst_width = _dst_width;\n+        dst_height = _dst_height;\n+        cn = _cn;\n+        borderType = _borderType;\n+        tabR = _tabR;\n+        tabM = _tabM;\n+        tabL = _tabL;\n+    }\n+\n+    void operator()(const Range& range) const CV_OVERRIDE;\n+\n+    const uchar* src_data;\n+    size_t src_step;\n+    int src_width;\n+    int src_height;\n+    uchar* dst_data;\n+    size_t dst_step;\n+    int dst_width;\n+    int dst_height;\n+    int cn;\n+    int borderType;\n+    int* tabR;\n+    int* tabM;\n+    int* tabL;\n+};\n+\n+// the algorithm is copied from imgproc/src/pyramids.cpp,\n+// in the function template void cv::pyrDown_\n+template<typename T, typename WT>\n+inline int pyrDown(const uchar* src_data, size_t src_step, int src_width, int src_height, uchar* dst_data, size_t dst_step, int dst_width, int dst_height, int cn, int borderType)\n+{\n+    const int PD_SZ = 5;\n+\n+    std::vector<int> _tabM(dst_width * cn), _tabL(cn * (PD_SZ + 2)), _tabR(cn * (PD_SZ + 2));\n+    int *tabM = _tabM.data(), *tabL = _tabL.data(), *tabR = _tabR.data();\n+\n+    CV_Assert( src_width > 0 && src_height > 0 &&\n+               std::abs(dst_width*2 - src_width) <= 2 &&\n+               std::abs(dst_height*2 - src_height) <= 2 );\n+    int width0 = std::min((src_width-PD_SZ/2-1)/2 + 1, dst_width);\n+\n+    for (int x = 0; x <= PD_SZ+1; x++)\n+    {\n+        int sx0 = borderInterpolate(x - PD_SZ/2, src_width, borderType)*cn;\n+        int sx1 = borderInterpolate(x + width0*2 - PD_SZ/2, src_width, borderType)*cn;\n+        for (int k = 0; k < cn; k++)\n+        {\n+            tabL[x*cn + k] = sx0 + k;\n+            tabR[x*cn + k] = sx1 + k;\n+        }\n+    }\n+\n+    for (int x = 0; x < dst_width*cn; x++)\n+        tabM[x] = (x/cn)*2*cn + x % cn;\n+\n+    cv::parallel_for_(Range(0,dst_height), PyrDownInvoker<T, WT>(src_data, src_step, src_width, src_height, dst_data, dst_step, dst_width, dst_height, cn, borderType, tabR, tabM, tabL), cv::getNumThreads());\n+    return CV_HAL_ERROR_OK;\n+}\n+\n+template<typename T, typename WT>\n+void PyrDownInvoker<T, WT>::operator()(const Range& range) const\n+{\n+    const int PD_SZ = 5;\n+\n+    int bufstep = (dst_width*cn + 15) & -16;\n+    std::vector<WT> _buf(bufstep*PD_SZ + 16);\n+    WT* buf = (WT*)(((size_t)_buf.data() + 15) & -16);\n+    WT* rows[PD_SZ];\n+\n+    int sy0 = -PD_SZ/2, sy = range.start * 2 + sy0, width0 = std::min((src_width-PD_SZ/2-1)/2 + 1, dst_width);\n+\n+    int _dst_width = dst_width * cn;\n+    width0 *= cn;\n+\n+    for (int y = range.start; y < range.end; y++)\n+    {\n+        T* dst = reinterpret_cast<T*>(dst_data + dst_step * y);\n+        WT *row0, *row1, *row2, *row3, *row4;\n+\n+        // fill the ring buffer (horizontal convolution and decimation)\n+        int sy_limit = y*2 + 2;\n+        for( ; sy <= sy_limit; sy++ )\n+        {\n+            WT* row = buf + ((sy - sy0) % PD_SZ)*bufstep;\n+            int _sy = borderInterpolate(sy, src_height, borderType);\n+            const T* src = reinterpret_cast<const T*>(src_data + src_step * _sy);\n+\n+            do {\n+                int x = 0;\n+                for( ; x < cn; x++ )\n+                {\n+                    row[x] = src[tabL[x+cn*2]]*6 + (src[tabL[x+cn]] + src[tabL[x+cn*3]])*4 +\n+                        src[tabL[x]] + src[tabL[x+cn*4]];\n+                }\n+\n+                if( x == _dst_width )\n+                    break;\n+\n+                pyrDownVec0<T, WT>()(src, row, reinterpret_cast<const uint*>(tabM), cn, width0);\n+                x = width0;\n+\n+                // tabR\n+                for (int x_ = 0; x < _dst_width; x++, x_++)\n+                {\n+                    row[x] = src[tabR[x_+cn*2]]*6 + (src[tabR[x_+cn]] + src[tabR[x_+cn*3]])*4 +\n+                        src[tabR[x_]] + src[tabR[x_+cn*4]];\n+                }\n+            } while (0);\n+        }\n+\n+        // do vertical convolution and decimation and write the result to the destination image\n+        for (int k = 0; k < PD_SZ; k++)\n+            rows[k] = buf + ((y*2 - PD_SZ/2 + k - sy0) % PD_SZ)*bufstep;\n+        row0 = rows[0]; row1 = rows[1]; row2 = rows[2]; row3 = rows[3]; row4 = rows[4];\n+\n+        pyrDownVec1<T, WT>()(row0, row1, row2, row3, row4, dst, _dst_width);\n+    }\n+}\n+\n+// the algorithm is copied from imgproc/src/pyramids.cpp,\n+// in the function template void cv::pyrUp_\n+template<typename T, typename WT>\n+inline int pyrUp(const uchar* src_data, size_t src_step, int src_width, int src_height, uchar* dst_data, size_t dst_step, int dst_width, int dst_height, int cn)\n+{\n+    const int PU_SZ = 3;\n+\n+    int bufstep = ((dst_width+1)*cn + 15) & -16;\n+    std::vector<WT> _buf(bufstep*PU_SZ + 16);\n+    WT* buf = (WT*)(((size_t)_buf.data() + 15) & -16);\n+    std::vector<int> _dtab(src_width*cn);\n+    int* dtab = _dtab.data();\n+    WT* rows[PU_SZ];\n+\n+    CV_Assert( std::abs(dst_width - src_width*2) == dst_width % 2 &&",
        "comment_created_at": "2025-02-28T19:51:05+00:00",
        "comment_author": "mshabunin",
        "comment_body": "Let's avoid `CV_Assert` too. Just return NOT_IMPLEMENTED in this case. Here and in other places.",
        "pr_file_module": null
      }
    ]
  }
]