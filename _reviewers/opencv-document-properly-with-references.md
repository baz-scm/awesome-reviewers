---
title: Document properly with references
description: 'Always separate code from documentation and use proper referencing techniques.
  Documentation should be in markdown files when extensive, while code should be in
  files that are regularly checked for compilation. '
repository: opencv/opencv
label: Documentation
language: Other
comments_count: 4
repository_stars: 82865
---

Always separate code from documentation and use proper referencing techniques. Documentation should be in markdown files when extensive, while code should be in files that are regularly checked for compilation. 

When documenting APIs:
1. Use `CV_EXPORTS_W` for functions that should be included in Java and Python bindings
2. Reference existing documentation rather than duplicating descriptions
3. Use `@snippet` and `@include` directives to embed code examples in documentation
4. Ensure all parameters are fully documented

**Example - Avoid:**
```cpp
/** @brief Enum of the possible types of ccm.
*/
enum CCM_TYPE
{
    CCM_3x3,   ///< The CCM with the shape \f$3\times3\f$ performs linear transformation on color values.
    CCM_4x3,   ///< The CCM with the shape \f$4\times3\f$ performs affine transformation.
};
// [followed by 200+ lines of in-header documentation]
```

**Example - Prefer:**
```cpp
/** @brief Enum of the possible types of ccm.
 *  @see @ref ccm_documentation for detailed explanations
*/
enum CCM_TYPE
{
    CCM_3x3,   ///< The CCM with the shape \f$3\times3\f$ performs linear transformation
    CCM_4x3,   ///< The CCM with the shape \f$4\times3\f$ performs affine transformation
};
```

With detailed documentation in a markdown file referenced by `@ref ccm_documentation` and any code examples included via `@snippet` or `@include` directives.


[
  {
    "discussion_id": "1995027406",
    "pr_number": 27051,
    "pr_file": "modules/photo/include/opencv2/ccm.hpp",
    "created_at": "2025-03-14T07:43:22+00:00",
    "commented_code": "// This file is part of OpenCV project.\n// It is subject to the license terms in the LICENSE file found in the top-level directory\n// of this distribution and at http://opencv.org/license.html.\n//\n//\n//                       License Agreement\n//              For Open Source Computer Vision Library\n//\n// Copyright(C) 2020, Huawei Technologies Co.,Ltd. All rights reserved.\n// Third party copyrights are property of their respective owners.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//             http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// Author: Longbu Wang <wanglongbu@huawei.com.com>\n//         Jinheng Zhang <zhangjinheng1@huawei.com>\n//         Chenqi Shan <shanchenqi@huawei.com>\n\n#ifndef OPENCV_PHOTO_CCM_HPP\n#define OPENCV_PHOTO_CCM_HPP\n\n#include <opencv2/core.hpp>\n#include <opencv2/imgproc.hpp>\n\nnamespace cv\n{\nnamespace ccm\n{\n\n/** @defgroup ccm Color Correction module\n@{\n\nIntroduction\n------------\n\nThe purpose of color correction is to adjust the color response of input\nand output devices to a known state. The device being calibrated is sometimes\ncalled the calibration source; the color space used as the standard is sometimes\ncalled the calibration target. Color calibration has been used in many industries,\nsuch as television production, games, photography, engineering, chemistry,\nmedicine, etc. Due to the manufacturing process of the input and output equipment,\nthe channel response has nonlinear distortion. In order to correct the picture output\nof the equipment, it is nessary to calibrate the captured color and the actual color.\n\n*/\n\n\n\n/** @brief Enum of the possible types of ccm.\n*/\nenum CCM_TYPE\n{\n    CCM_3x3,   ///< The CCM with the shape \\f$3\\times3\\f$ performs linear transformation on color values.\n    CCM_4x3,   ///< The CCM with the shape \\f$4\\times3\\f$ performs affine transformation.\n};\n\n/** @brief Enum of the possible types of initial method.\n*/\nenum INITIAL_METHOD_TYPE\n{\n    INITIAL_METHOD_WHITE_BALANCE,      ///< The white balance method. The initial value is:\\n\n                        /// \\f$\n                        /// M_{CCM}=\n                        /// \\begin{bmatrix}\n                        /// k_R & 0 & 0\\\\\n                        /// 0 & k_G & 0\\\\\n                        /// 0 & 0 & k_B\\\\\n                        /// \\end{bmatrix}\n                        /// \\f$\\n\n                        /// where\\n\n                        /// \\f$\n                        /// k_R=mean(R_{li}')/mean(R_{li})\\\\\n                        /// k_R=mean(G_{li}')/mean(G_{li})\\\\\n                        /// k_R=mean(B_{li}')/mean(B_{li})\n                        /// \\f$\n    INITIAL_METHOD_LEAST_SQUARE,       ///<the least square method is an optimal solution under the linear RGB distance function\n};\n/** @brief  Macbeth and Vinyl ColorChecker with 2deg D50\n*/\nenum CONST_COLOR {\n    COLORCHECKER_Macbeth,                ///< Macbeth ColorChecker\n    COLORCHECKER_Vinyl,                  ///< DKK ColorChecker\n    COLORCHECKER_DigitalSG,              ///< DigitalSG ColorChecker with 140 squares\n};\nenum COLOR_SPACE {\n    COLOR_SPACE_sRGB,                       ///< https://en.wikipedia.org/wiki/SRGB , RGB color space\n    COLOR_SPACE_sRGBL,                      ///< https://en.wikipedia.org/wiki/SRGB , linear RGB color space\n    COLOR_SPACE_AdobeRGB,                   ///< https://en.wikipedia.org/wiki/Adobe_RGB_color_space , RGB color space\n    COLOR_SPACE_AdobeRGBL,                  ///< https://en.wikipedia.org/wiki/Adobe_RGB_color_space , linear RGB color space\n    COLOR_SPACE_WideGamutRGB,               ///< https://en.wikipedia.org/wiki/Wide-gamut_RGB_color_space , RGB color space\n    COLOR_SPACE_WideGamutRGBL,              ///< https://en.wikipedia.org/wiki/Wide-gamut_RGB_color_space , linear RGB color space\n    COLOR_SPACE_ProPhotoRGB,                ///< https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space , RGB color space\n    COLOR_SPACE_ProPhotoRGBL,               ///< https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space , linear RGB color space\n    COLOR_SPACE_DCI_P3_RGB,                 ///< https://en.wikipedia.org/wiki/DCI-P3 , RGB color space\n    COLOR_SPACE_DCI_P3_RGBL,                ///< https://en.wikipedia.org/wiki/DCI-P3 , linear RGB color space\n    COLOR_SPACE_AppleRGB,                   ///< https://en.wikipedia.org/wiki/RGB_color_space , RGB color space\n    COLOR_SPACE_AppleRGBL,                  ///< https://en.wikipedia.org/wiki/RGB_color_space , linear RGB color space\n    COLOR_SPACE_REC_709_RGB,                ///< https://en.wikipedia.org/wiki/Rec._709 , RGB color space\n    COLOR_SPACE_REC_709_RGBL,               ///< https://en.wikipedia.org/wiki/Rec._709 , linear RGB color space\n    COLOR_SPACE_REC_2020_RGB,               ///< https://en.wikipedia.org/wiki/Rec._2020 , RGB color space\n    COLOR_SPACE_REC_2020_RGBL,              ///< https://en.wikipedia.org/wiki/Rec._2020 , linear RGB color space\n    COLOR_SPACE_XYZ_D65_2,                  ///< https://en.wikipedia.org/wiki/CIE_1931_color_space , non-RGB color space\n    COLOR_SPACE_XYZ_D65_10,                 ///< non-RGB color space\n    COLOR_SPACE_XYZ_D50_2,                  ///< non-RGB color space\n    COLOR_SPACE_XYZ_D50_10,                 ///< non-RGB color space\n    COLOR_SPACE_XYZ_A_2,                    ///< non-RGB color space\n    COLOR_SPACE_XYZ_A_10,                   ///< non-RGB color space\n    COLOR_SPACE_XYZ_D55_2,                  ///< non-RGB color space\n    COLOR_SPACE_XYZ_D55_10,                 ///< non-RGB color space\n    COLOR_SPACE_XYZ_D75_2,                  ///< non-RGB color space\n    COLOR_SPACE_XYZ_D75_10,                 ///< non-RGB color space\n    COLOR_SPACE_XYZ_E_2,                    ///< non-RGB color space\n    COLOR_SPACE_XYZ_E_10,                   ///< non-RGB color space\n    COLOR_SPACE_Lab_D65_2,                  ///< https://en.wikipedia.org/wiki/CIELAB_color_space , non-RGB color space\n    COLOR_SPACE_Lab_D65_10,                 ///< non-RGB color space\n    COLOR_SPACE_Lab_D50_2,                  ///< non-RGB color space\n    COLOR_SPACE_Lab_D50_10,                 ///< non-RGB color space\n    COLOR_SPACE_Lab_A_2,                    ///< non-RGB color space\n    COLOR_SPACE_Lab_A_10,                   ///< non-RGB color space\n    COLOR_SPACE_Lab_D55_2,                  ///< non-RGB color space\n    COLOR_SPACE_Lab_D55_10,                 ///< non-RGB color space\n    COLOR_SPACE_Lab_D75_2,                  ///< non-RGB color space\n    COLOR_SPACE_Lab_D75_10,                 ///< non-RGB color space\n    COLOR_SPACE_Lab_E_2,                    ///< non-RGB color space\n    COLOR_SPACE_Lab_E_10,                   ///< non-RGB color space\n};\n\n/** @brief Linearization transformation type\n\nThe first step in color correction is to linearize the detected colors.\nBecause the input color space has not been calibrated, we usually use some empirical methods to linearize.\nThere are several common linearization methods.\nThe first is identical transformation, the second is gamma correction, and the third is polynomial fitting.\n\nLinearization is generally an elementwise function. The mathematical symbols are as follows:\n\n\\f$C\\f$: any channel of a color, could be \\f$R, G\\f$ or \\f$B\\f$.\n\n\\f$R, G,  B\\f$:  \\f$R, G, B\\f$ channels respectively.\n\n\\f$G\\f$: grayscale;\n\n\\f$s,sl\\f$: subscript, which represents the detected data and its linearized value, the former is the input and the latter is the output;\n\n\\f$d,dl\\f$: subscript, which represents the reference data and its linearized value\n\n\n\n### Identical Transformation\n\nNo change is made during the Identical transformation linearization, usually because the tristimulus values of the input RGB image is already proportional to the luminance.\nFor example, if the input measurement data is in RAW format, the measurement data is already linear, so no linearization is required.\n\nThe identity transformation formula is as follows:\n\n\\f[\nC_{sl}=C_s\n\\f]\n\n### Gamma Correction\n\nGamma correction is a means of performing nonlinearity in RGB space, see the Color Space documentation for details.\nIn the linearization part, the value of \\f$\\gamma\\f$ is usually set to 2.2.\nYou can also customize the value.\n\nThe formula for gamma correction linearization is as follows:\n\\f[\nC_{sl}=C_s^{\\gamma},\\qquad C_s\\ge0\\\\\nC_{sl}=-(-C_s)^{\\gamma},\\qquad C_s<0\\\\\\\\\n\\f]\n\n### Polynomial Fitting\n\nPolynomial fitting uses polynomials to linearize.\nProvided the polynomial is:\n\\f[\nf(x)=a_nx^n+a_{n-1}x^{n-1}+... +a_0\n\\f]\nThen:\n\\f[\nC_{sl}=f(C_s)\n\\f]\nIn practice, \\f$n\\le3\\f$ is used to prevent overfitting.\n\nThere are many variants of polynomial fitting, the difference lies in the way of generating \\f$f(x)\\f$.\nIt is usually necessary to use linearized reference colors and corresponding detected colors to calculate the polynomial parameters.\nHowever, not all colors can participate in the calculation. The saturation detected colors needs to be removed. See the algorithm introduction document for details.\n\n#### Fitting Channels Respectively\n\nUse three polynomials, \\f$r(x), g(x), b(x)\\f$,  to linearize each channel of the RGB color space[1-3]:\n\\f[\nR_{sl}=r(R_s)\\\\\nG_{sl}=g(G_s)\\\\\nB_{sl}=b(B_s)\\\\\n\\f]\nThe polynomial is generated by minimizing the residual sum of squares between the detected data and the linearized reference data.\nTake the R-channel as an example:\n\n\\f[\nR=\\arg min_{f}(\\Sigma(R_{dl}-f(R_S)^2)\n\\f]\n\nIt's equivalent to finding the least square regression for below equations:\n\\f[\nf(R_{s1})=R_{dl1}\\\\\nf(R_{s2})=R_{dl2}\\\\\n...\n\\f]\n\nWith a polynomial, the above equations becomes:\n\\f[\n\\begin{bmatrix}\nR_{s1}^{n} & R_{s1}^{n-1} & ... & 1\\\\\nR_{s2}^{n} & R_{s2}^{n-1} & ... & 1\\\\\n... & ... & ... & ...\n\\end{bmatrix}\n\\begin{bmatrix}\na_{n}\\\\\na_{n-1}\\\\\n... \\\\\na_0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nR_{dl1}\\\\\nR_{dl2}\\\\\n...\n\\end{bmatrix}\n\\f]\nIt can be expressed as a system of linear equations:\n\n\\f[\nAX=B\n\\f]\n\nWhen the number of reference colors is not less than the degree of the polynomial, the linear system has a least-squares solution:\n\n\\f[\nX=(A^TA)^{-1}A^TB\n\\f]\n\nOnce we get the polynomial coefficients, we can get the polynomial r.\n\nThis method of finding polynomial coefficients can be implemented by numpy.polyfit in numpy, expressed here as:\n\n\\f[\nR=polyfit(R_S, R_{dl})\n\\f]\n\nNote that, in general, the polynomial that we want to obtain is guaranteed to monotonically increase in the interval [0,1] ,\nbut this means that nonlinear method is needed to generate the polynomials(see [4] for detail).\nThis would greatly increases the complexity of the program.\nConsidering that the monotonicity does not affect the correct operation of the color correction program, polyfit is still used to implement the program.\n\nParameters for other channels can also be derived in a similar way.\n\n#### Grayscale Polynomial Fitting\n\nIn this method[2], single polynomial is used for all channels.\nThe polynomial is still a polyfit result from the detected colors to the linear reference colors.\nHowever, only the gray of the reference colors can participate in the calculation.\n\nSince the detected colors corresponding to the gray of reference colors is not necessarily gray, it needs to be grayed.\nGrayscale refers to the Y channel of the XYZ color space.\nThe color space of the detected data is not determined and cannot be converted into the XYZ space.\nTherefore, the sRGB formula is used to approximate[5].\n\\f[\nG_{s}=0.2126R_{s}+0.7152G_{s}+0.0722B_{s}\n\\f]\nThen the polynomial parameters can be obtained by using the polyfit.\n\\f[\nf=polyfit(G_{s}, G_{dl})\n\\f]\nAfter \\f$f\\f$ is obtained, linearization can be performed.\n\n#### Logarithmic Polynomial Fitting\n\nFor gamma correction formula, we take the logarithm:\n\\f[\nln(C_{sl})={\\gamma}ln(C_s),\\qquad C_s\\ge0\\\n\\f]\nIt can be seen that there is a linear relationship between \\f$ln(C_s)\\f$ and \\f$ln(C_{sl})\\f$. It can be considered that the formula is an approximation of a polynomial relationship, that is, there exists a polynomial \\f$f\\f$, which makes[2]:\n\\f[\nln(C_{sl})=f(ln(C_s)), \\qquad C_s>0\\\\\nC_{sl}=0, \\qquad C_s=0\n\\f]\n\nBecause \\f$exp(ln(0))\\to\\infty \\f$, the channel whose component is 0 is directly mapped to 0 in the formula above.\n\nFor fitting channels respectively, we have:\n\\f[\nr=polyfit(ln(R_s),ln(R_{dl}))\\\\\ng=polyfit(ln(G_s),ln(G_{dl}))\\\\\nb=polyfit(ln(B_s),ln(B_{dl}))\\\\\n\\f]\nNote that the parameter of \\f$ln(*) \\f$ cannot be 0.\nTherefore, we need to delete the channels whose values are 0 from \\f$R_s \\f$ and \\f$R_{dl} \\f$, \\f$G_s\\f$ and \\f$G_{dl}\\f$, \\f$B_s\\f$ and \\f$B_{dl}\\f$.\n\nTherefore:\n\n\\f[\nln(R_{sl})=r(ln(R_s)), \\qquad R_s>0\\\\\nR_{sl}=0, \\qquad R_s=0\\\\\nln(G_{sl})=g(ln(G_s)),\\qquad G_s>0\\\\\nG_{sl}=0, \\qquad G_s=0\\\\\nln(B_{sl})=b(ln(B_s)),\\qquad B_s>0\\\\\nB_{sl}=0, \\qquad B_s=0\\\\\n\\f]",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "1995027406",
        "repo_full_name": "opencv/opencv",
        "pr_number": 27051,
        "pr_file": "modules/photo/include/opencv2/ccm.hpp",
        "discussion_id": "1995027406",
        "commented_code": "@@ -0,0 +1,520 @@\n+// This file is part of OpenCV project.\n+// It is subject to the license terms in the LICENSE file found in the top-level directory\n+// of this distribution and at http://opencv.org/license.html.\n+//\n+//\n+//                       License Agreement\n+//              For Open Source Computer Vision Library\n+//\n+// Copyright(C) 2020, Huawei Technologies Co.,Ltd. All rights reserved.\n+// Third party copyrights are property of their respective owners.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//             http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+//\n+// Author: Longbu Wang <wanglongbu@huawei.com.com>\n+//         Jinheng Zhang <zhangjinheng1@huawei.com>\n+//         Chenqi Shan <shanchenqi@huawei.com>\n+\n+#ifndef OPENCV_PHOTO_CCM_HPP\n+#define OPENCV_PHOTO_CCM_HPP\n+\n+#include <opencv2/core.hpp>\n+#include <opencv2/imgproc.hpp>\n+\n+namespace cv\n+{\n+namespace ccm\n+{\n+\n+/** @defgroup ccm Color Correction module\n+@{\n+\n+Introduction\n+------------\n+\n+The purpose of color correction is to adjust the color response of input\n+and output devices to a known state. The device being calibrated is sometimes\n+called the calibration source; the color space used as the standard is sometimes\n+called the calibration target. Color calibration has been used in many industries,\n+such as television production, games, photography, engineering, chemistry,\n+medicine, etc. Due to the manufacturing process of the input and output equipment,\n+the channel response has nonlinear distortion. In order to correct the picture output\n+of the equipment, it is nessary to calibrate the captured color and the actual color.\n+\n+*/\n+\n+\n+\n+/** @brief Enum of the possible types of ccm.\n+*/\n+enum CCM_TYPE\n+{\n+    CCM_3x3,   ///< The CCM with the shape \\f$3\\times3\\f$ performs linear transformation on color values.\n+    CCM_4x3,   ///< The CCM with the shape \\f$4\\times3\\f$ performs affine transformation.\n+};\n+\n+/** @brief Enum of the possible types of initial method.\n+*/\n+enum INITIAL_METHOD_TYPE\n+{\n+    INITIAL_METHOD_WHITE_BALANCE,      ///< The white balance method. The initial value is:\\n\n+                        /// \\f$\n+                        /// M_{CCM}=\n+                        /// \\begin{bmatrix}\n+                        /// k_R & 0 & 0\\\\\n+                        /// 0 & k_G & 0\\\\\n+                        /// 0 & 0 & k_B\\\\\n+                        /// \\end{bmatrix}\n+                        /// \\f$\\n\n+                        /// where\\n\n+                        /// \\f$\n+                        /// k_R=mean(R_{li}')/mean(R_{li})\\\\\n+                        /// k_R=mean(G_{li}')/mean(G_{li})\\\\\n+                        /// k_R=mean(B_{li}')/mean(B_{li})\n+                        /// \\f$\n+    INITIAL_METHOD_LEAST_SQUARE,       ///<the least square method is an optimal solution under the linear RGB distance function\n+};\n+/** @brief  Macbeth and Vinyl ColorChecker with 2deg D50\n+*/\n+enum CONST_COLOR {\n+    COLORCHECKER_Macbeth,                ///< Macbeth ColorChecker\n+    COLORCHECKER_Vinyl,                  ///< DKK ColorChecker\n+    COLORCHECKER_DigitalSG,              ///< DigitalSG ColorChecker with 140 squares\n+};\n+enum COLOR_SPACE {\n+    COLOR_SPACE_sRGB,                       ///< https://en.wikipedia.org/wiki/SRGB , RGB color space\n+    COLOR_SPACE_sRGBL,                      ///< https://en.wikipedia.org/wiki/SRGB , linear RGB color space\n+    COLOR_SPACE_AdobeRGB,                   ///< https://en.wikipedia.org/wiki/Adobe_RGB_color_space , RGB color space\n+    COLOR_SPACE_AdobeRGBL,                  ///< https://en.wikipedia.org/wiki/Adobe_RGB_color_space , linear RGB color space\n+    COLOR_SPACE_WideGamutRGB,               ///< https://en.wikipedia.org/wiki/Wide-gamut_RGB_color_space , RGB color space\n+    COLOR_SPACE_WideGamutRGBL,              ///< https://en.wikipedia.org/wiki/Wide-gamut_RGB_color_space , linear RGB color space\n+    COLOR_SPACE_ProPhotoRGB,                ///< https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space , RGB color space\n+    COLOR_SPACE_ProPhotoRGBL,               ///< https://en.wikipedia.org/wiki/ProPhoto_RGB_color_space , linear RGB color space\n+    COLOR_SPACE_DCI_P3_RGB,                 ///< https://en.wikipedia.org/wiki/DCI-P3 , RGB color space\n+    COLOR_SPACE_DCI_P3_RGBL,                ///< https://en.wikipedia.org/wiki/DCI-P3 , linear RGB color space\n+    COLOR_SPACE_AppleRGB,                   ///< https://en.wikipedia.org/wiki/RGB_color_space , RGB color space\n+    COLOR_SPACE_AppleRGBL,                  ///< https://en.wikipedia.org/wiki/RGB_color_space , linear RGB color space\n+    COLOR_SPACE_REC_709_RGB,                ///< https://en.wikipedia.org/wiki/Rec._709 , RGB color space\n+    COLOR_SPACE_REC_709_RGBL,               ///< https://en.wikipedia.org/wiki/Rec._709 , linear RGB color space\n+    COLOR_SPACE_REC_2020_RGB,               ///< https://en.wikipedia.org/wiki/Rec._2020 , RGB color space\n+    COLOR_SPACE_REC_2020_RGBL,              ///< https://en.wikipedia.org/wiki/Rec._2020 , linear RGB color space\n+    COLOR_SPACE_XYZ_D65_2,                  ///< https://en.wikipedia.org/wiki/CIE_1931_color_space , non-RGB color space\n+    COLOR_SPACE_XYZ_D65_10,                 ///< non-RGB color space\n+    COLOR_SPACE_XYZ_D50_2,                  ///< non-RGB color space\n+    COLOR_SPACE_XYZ_D50_10,                 ///< non-RGB color space\n+    COLOR_SPACE_XYZ_A_2,                    ///< non-RGB color space\n+    COLOR_SPACE_XYZ_A_10,                   ///< non-RGB color space\n+    COLOR_SPACE_XYZ_D55_2,                  ///< non-RGB color space\n+    COLOR_SPACE_XYZ_D55_10,                 ///< non-RGB color space\n+    COLOR_SPACE_XYZ_D75_2,                  ///< non-RGB color space\n+    COLOR_SPACE_XYZ_D75_10,                 ///< non-RGB color space\n+    COLOR_SPACE_XYZ_E_2,                    ///< non-RGB color space\n+    COLOR_SPACE_XYZ_E_10,                   ///< non-RGB color space\n+    COLOR_SPACE_Lab_D65_2,                  ///< https://en.wikipedia.org/wiki/CIELAB_color_space , non-RGB color space\n+    COLOR_SPACE_Lab_D65_10,                 ///< non-RGB color space\n+    COLOR_SPACE_Lab_D50_2,                  ///< non-RGB color space\n+    COLOR_SPACE_Lab_D50_10,                 ///< non-RGB color space\n+    COLOR_SPACE_Lab_A_2,                    ///< non-RGB color space\n+    COLOR_SPACE_Lab_A_10,                   ///< non-RGB color space\n+    COLOR_SPACE_Lab_D55_2,                  ///< non-RGB color space\n+    COLOR_SPACE_Lab_D55_10,                 ///< non-RGB color space\n+    COLOR_SPACE_Lab_D75_2,                  ///< non-RGB color space\n+    COLOR_SPACE_Lab_D75_10,                 ///< non-RGB color space\n+    COLOR_SPACE_Lab_E_2,                    ///< non-RGB color space\n+    COLOR_SPACE_Lab_E_10,                   ///< non-RGB color space\n+};\n+\n+/** @brief Linearization transformation type\n+\n+The first step in color correction is to linearize the detected colors.\n+Because the input color space has not been calibrated, we usually use some empirical methods to linearize.\n+There are several common linearization methods.\n+The first is identical transformation, the second is gamma correction, and the third is polynomial fitting.\n+\n+Linearization is generally an elementwise function. The mathematical symbols are as follows:\n+\n+\\f$C\\f$: any channel of a color, could be \\f$R, G\\f$ or \\f$B\\f$.\n+\n+\\f$R, G,  B\\f$:  \\f$R, G, B\\f$ channels respectively.\n+\n+\\f$G\\f$: grayscale;\n+\n+\\f$s,sl\\f$: subscript, which represents the detected data and its linearized value, the former is the input and the latter is the output;\n+\n+\\f$d,dl\\f$: subscript, which represents the reference data and its linearized value\n+\n+\n+\n+### Identical Transformation\n+\n+No change is made during the Identical transformation linearization, usually because the tristimulus values of the input RGB image is already proportional to the luminance.\n+For example, if the input measurement data is in RAW format, the measurement data is already linear, so no linearization is required.\n+\n+The identity transformation formula is as follows:\n+\n+\\f[\n+C_{sl}=C_s\n+\\f]\n+\n+### Gamma Correction\n+\n+Gamma correction is a means of performing nonlinearity in RGB space, see the Color Space documentation for details.\n+In the linearization part, the value of \\f$\\gamma\\f$ is usually set to 2.2.\n+You can also customize the value.\n+\n+The formula for gamma correction linearization is as follows:\n+\\f[\n+C_{sl}=C_s^{\\gamma},\\qquad C_s\\ge0\\\\\n+C_{sl}=-(-C_s)^{\\gamma},\\qquad C_s<0\\\\\\\\\n+\\f]\n+\n+### Polynomial Fitting\n+\n+Polynomial fitting uses polynomials to linearize.\n+Provided the polynomial is:\n+\\f[\n+f(x)=a_nx^n+a_{n-1}x^{n-1}+... +a_0\n+\\f]\n+Then:\n+\\f[\n+C_{sl}=f(C_s)\n+\\f]\n+In practice, \\f$n\\le3\\f$ is used to prevent overfitting.\n+\n+There are many variants of polynomial fitting, the difference lies in the way of generating \\f$f(x)\\f$.\n+It is usually necessary to use linearized reference colors and corresponding detected colors to calculate the polynomial parameters.\n+However, not all colors can participate in the calculation. The saturation detected colors needs to be removed. See the algorithm introduction document for details.\n+\n+#### Fitting Channels Respectively\n+\n+Use three polynomials, \\f$r(x), g(x), b(x)\\f$,  to linearize each channel of the RGB color space[1-3]:\n+\\f[\n+R_{sl}=r(R_s)\\\\\n+G_{sl}=g(G_s)\\\\\n+B_{sl}=b(B_s)\\\\\n+\\f]\n+The polynomial is generated by minimizing the residual sum of squares between the detected data and the linearized reference data.\n+Take the R-channel as an example:\n+\n+\\f[\n+R=\\arg min_{f}(\\Sigma(R_{dl}-f(R_S)^2)\n+\\f]\n+\n+It's equivalent to finding the least square regression for below equations:\n+\\f[\n+f(R_{s1})=R_{dl1}\\\\\n+f(R_{s2})=R_{dl2}\\\\\n+...\n+\\f]\n+\n+With a polynomial, the above equations becomes:\n+\\f[\n+\\begin{bmatrix}\n+R_{s1}^{n} & R_{s1}^{n-1} & ... & 1\\\\\n+R_{s2}^{n} & R_{s2}^{n-1} & ... & 1\\\\\n+... & ... & ... & ...\n+\\end{bmatrix}\n+\\begin{bmatrix}\n+a_{n}\\\\\n+a_{n-1}\\\\\n+... \\\\\n+a_0\n+\\end{bmatrix}\n+=\n+\\begin{bmatrix}\n+R_{dl1}\\\\\n+R_{dl2}\\\\\n+...\n+\\end{bmatrix}\n+\\f]\n+It can be expressed as a system of linear equations:\n+\n+\\f[\n+AX=B\n+\\f]\n+\n+When the number of reference colors is not less than the degree of the polynomial, the linear system has a least-squares solution:\n+\n+\\f[\n+X=(A^TA)^{-1}A^TB\n+\\f]\n+\n+Once we get the polynomial coefficients, we can get the polynomial r.\n+\n+This method of finding polynomial coefficients can be implemented by numpy.polyfit in numpy, expressed here as:\n+\n+\\f[\n+R=polyfit(R_S, R_{dl})\n+\\f]\n+\n+Note that, in general, the polynomial that we want to obtain is guaranteed to monotonically increase in the interval [0,1] ,\n+but this means that nonlinear method is needed to generate the polynomials(see [4] for detail).\n+This would greatly increases the complexity of the program.\n+Considering that the monotonicity does not affect the correct operation of the color correction program, polyfit is still used to implement the program.\n+\n+Parameters for other channels can also be derived in a similar way.\n+\n+#### Grayscale Polynomial Fitting\n+\n+In this method[2], single polynomial is used for all channels.\n+The polynomial is still a polyfit result from the detected colors to the linear reference colors.\n+However, only the gray of the reference colors can participate in the calculation.\n+\n+Since the detected colors corresponding to the gray of reference colors is not necessarily gray, it needs to be grayed.\n+Grayscale refers to the Y channel of the XYZ color space.\n+The color space of the detected data is not determined and cannot be converted into the XYZ space.\n+Therefore, the sRGB formula is used to approximate[5].\n+\\f[\n+G_{s}=0.2126R_{s}+0.7152G_{s}+0.0722B_{s}\n+\\f]\n+Then the polynomial parameters can be obtained by using the polyfit.\n+\\f[\n+f=polyfit(G_{s}, G_{dl})\n+\\f]\n+After \\f$f\\f$ is obtained, linearization can be performed.\n+\n+#### Logarithmic Polynomial Fitting\n+\n+For gamma correction formula, we take the logarithm:\n+\\f[\n+ln(C_{sl})={\\gamma}ln(C_s),\\qquad C_s\\ge0\\\n+\\f]\n+It can be seen that there is a linear relationship between \\f$ln(C_s)\\f$ and \\f$ln(C_{sl})\\f$. It can be considered that the formula is an approximation of a polynomial relationship, that is, there exists a polynomial \\f$f\\f$, which makes[2]:\n+\\f[\n+ln(C_{sl})=f(ln(C_s)), \\qquad C_s>0\\\\\n+C_{sl}=0, \\qquad C_s=0\n+\\f]\n+\n+Because \\f$exp(ln(0))\\to\\infty \\f$, the channel whose component is 0 is directly mapped to 0 in the formula above.\n+\n+For fitting channels respectively, we have:\n+\\f[\n+r=polyfit(ln(R_s),ln(R_{dl}))\\\\\n+g=polyfit(ln(G_s),ln(G_{dl}))\\\\\n+b=polyfit(ln(B_s),ln(B_{dl}))\\\\\n+\\f]\n+Note that the parameter of \\f$ln(*) \\f$ cannot be 0.\n+Therefore, we need to delete the channels whose values are 0 from \\f$R_s \\f$ and \\f$R_{dl} \\f$, \\f$G_s\\f$ and \\f$G_{dl}\\f$, \\f$B_s\\f$ and \\f$B_{dl}\\f$.\n+\n+Therefore:\n+\n+\\f[\n+ln(R_{sl})=r(ln(R_s)), \\qquad R_s>0\\\\\n+R_{sl}=0, \\qquad R_s=0\\\\\n+ln(G_{sl})=g(ln(G_s)),\\qquad G_s>0\\\\\n+G_{sl}=0, \\qquad G_s=0\\\\\n+ln(B_{sl})=b(ln(B_s)),\\qquad B_s>0\\\\\n+B_{sl}=0, \\qquad B_s=0\\\\\n+\\f]",
        "comment_created_at": "2025-03-14T07:43:22+00:00",
        "comment_author": "asmorkalov",
        "comment_body": "Please move the text to tutorial (markdown) and add references to the header file.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2073350574",
    "pr_number": 27051,
    "pr_file": "modules/photo/src/ccm/utils.hpp",
    "created_at": "2025-05-05T12:29:45+00:00",
    "commented_code": "@param gamma a constant for gamma correction.\n    @param dst the output array, type of Mat.\n */\nMat gammaCorrection(const Mat& src, double gamma, Mat dst=Mat());\n CV_EXPORTS void gammaCorrection(InputArray src, OutputArray dst, double gamma);",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "2073350574",
        "repo_full_name": "opencv/opencv",
        "pr_number": 27051,
        "pr_file": "modules/photo/src/ccm/utils.hpp",
        "discussion_id": "2073350574",
        "commented_code": "@@ -25,7 +25,7 @@ double gammaCorrection_(double element, double gamma);\n     @param gamma a constant for gamma correction.\n     @param dst the output array, type of Mat.\n  */\n-Mat gammaCorrection(const Mat& src, double gamma, Mat dst=Mat());\n+ CV_EXPORTS void gammaCorrection(InputArray src, OutputArray dst, double gamma);",
        "comment_created_at": "2025-05-05T12:29:45+00:00",
        "comment_author": "asmorkalov",
        "comment_body": "It should go to make ccm header to be included into documentation and bindings. Please CV_EXPORTS_W to generate Java and Python bindings for it too.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "562257420",
    "pr_number": 19305,
    "pr_file": "doc/tutorials/introduction/test_writing/accuracy_test_writing.markdown",
    "created_at": "2021-01-21T23:09:29+00:00",
    "commented_code": "# Writing Accuracy Tests\n\n# {#tutorial_accuracy_test_writing}\n\n`This testing framework is based on Google Tests`\n\nThere are two major types of C++ tests: *accuracy/regression* tests and *performance* tests.  Each module can have two test binaries: `opencv_test_<MODULE_NAME>` and `opencv_perf_<MODULE_NAME>`, and two tests folders: `<opencv(_contrib)/modules/<MODULE_NAME>/test>` and `<opencv(_contrib)/modules/<MODULE_NAME>/perf>`. These applications can be built for every supported platform (Win/Lin/Mac/Android).\n\n## ACCURACY TESTS\n\n### Work directory\n\nAll modules have their own dir for accuracy tests: `opencv_contrib/modules/(moduleName)/test/...`\n\n### Dir Structure\n\n- `test_precomp.hpp` - file for includes\n- `test_main.cpp` - main file of test sample\n- `test_smth1.cpp` - files for tests\n- `test_smth2.cpp`\n- `...`\n\n### Test structure\n\n```c++\n// name of this case is \"name1.name2\"\nTEST(name1, name2)\n{\n    ASSERT_....;\n}\n```\n\n### Tests sample example\n\n**Files:**\n\n- test_precomp.hpp\n- test_main.cpp\n- test_sum.cpp\n- test_sub.cpp\n\n\n\n```c++\n// test_precomp.hpp\n\n#ifndef __OPENCV_TEST_PRECOMP_HPP__\n#define __OPENCV_TEST_PRECOMP_HPP__",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "562257420",
        "repo_full_name": "opencv/opencv",
        "pr_number": 19305,
        "pr_file": "doc/tutorials/introduction/test_writing/accuracy_test_writing.markdown",
        "discussion_id": "562257420",
        "commented_code": "@@ -0,0 +1,269 @@\n+# Writing Accuracy Tests\n+\n+# {#tutorial_accuracy_test_writing}\n+\n+`This testing framework is based on Google Tests`\n+\n+There are two major types of C++ tests: *accuracy/regression* tests and *performance* tests.  Each module can have two test binaries: `opencv_test_<MODULE_NAME>` and `opencv_perf_<MODULE_NAME>`, and two tests folders: `<opencv(_contrib)/modules/<MODULE_NAME>/test>` and `<opencv(_contrib)/modules/<MODULE_NAME>/perf>`. These applications can be built for every supported platform (Win/Lin/Mac/Android).\n+\n+## ACCURACY TESTS\n+\n+### Work directory\n+\n+All modules have their own dir for accuracy tests: `opencv_contrib/modules/(moduleName)/test/...`\n+\n+### Dir Structure\n+\n+- `test_precomp.hpp` - file for includes\n+- `test_main.cpp` - main file of test sample\n+- `test_smth1.cpp` - files for tests\n+- `test_smth2.cpp`\n+- `...`\n+\n+### Test structure\n+\n+```c++\n+// name of this case is \"name1.name2\"\n+TEST(name1, name2)\n+{\n+    ASSERT_....;\n+}\n+```\n+\n+### Tests sample example\n+\n+**Files:**\n+\n+- test_precomp.hpp\n+- test_main.cpp\n+- test_sum.cpp\n+- test_sub.cpp\n+\n+\n+\n+```c++\n+// test_precomp.hpp\n+\n+#ifndef __OPENCV_TEST_PRECOMP_HPP__\n+#define __OPENCV_TEST_PRECOMP_HPP__",
        "comment_created_at": "2021-01-21T23:09:29+00:00",
        "comment_author": "alalek",
        "comment_body": "There is general important rule: avoid code in documentation files/blobs/etc.\r\n\r\nCode must go into files which are regularly checked for compilation.\r\nDocumentation embeds code through `@snippet` and/or `@include`",
        "pr_file_module": null
      },
      {
        "comment_id": "575126009",
        "repo_full_name": "opencv/opencv",
        "pr_number": 19305,
        "pr_file": "doc/tutorials/introduction/test_writing/accuracy_test_writing.markdown",
        "discussion_id": "562257420",
        "commented_code": "@@ -0,0 +1,269 @@\n+# Writing Accuracy Tests\n+\n+# {#tutorial_accuracy_test_writing}\n+\n+`This testing framework is based on Google Tests`\n+\n+There are two major types of C++ tests: *accuracy/regression* tests and *performance* tests.  Each module can have two test binaries: `opencv_test_<MODULE_NAME>` and `opencv_perf_<MODULE_NAME>`, and two tests folders: `<opencv(_contrib)/modules/<MODULE_NAME>/test>` and `<opencv(_contrib)/modules/<MODULE_NAME>/perf>`. These applications can be built for every supported platform (Win/Lin/Mac/Android).\n+\n+## ACCURACY TESTS\n+\n+### Work directory\n+\n+All modules have their own dir for accuracy tests: `opencv_contrib/modules/(moduleName)/test/...`\n+\n+### Dir Structure\n+\n+- `test_precomp.hpp` - file for includes\n+- `test_main.cpp` - main file of test sample\n+- `test_smth1.cpp` - files for tests\n+- `test_smth2.cpp`\n+- `...`\n+\n+### Test structure\n+\n+```c++\n+// name of this case is \"name1.name2\"\n+TEST(name1, name2)\n+{\n+    ASSERT_....;\n+}\n+```\n+\n+### Tests sample example\n+\n+**Files:**\n+\n+- test_precomp.hpp\n+- test_main.cpp\n+- test_sum.cpp\n+- test_sub.cpp\n+\n+\n+\n+```c++\n+// test_precomp.hpp\n+\n+#ifndef __OPENCV_TEST_PRECOMP_HPP__\n+#define __OPENCV_TEST_PRECOMP_HPP__",
        "comment_created_at": "2021-02-12T10:30:05+00:00",
        "comment_author": "DumDereDum",
        "comment_body": "code is deleted",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1926898553",
    "pr_number": 26669,
    "pr_file": "modules/calib3d/include/opencv2/calib3d.hpp",
    "created_at": "2025-01-23T12:27:20+00:00",
    "commented_code": "TermCriteria criteria = TermCriteria(TermCriteria::MAX_ITER + TermCriteria::EPS, 10, 1e-8)\n                              );\n\n    /**\n    @brief Finds an object pose from 3D-2D point correspondences using the RANSAC scheme for fisheye camera moodel.\n\n    @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or\n    1xN/Nx1 3-channel, where N is the number of points. vector\\<Point3d\\> can be also passed here.\n    @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,\n    where N is the number of points. vector\\<Point2d\\> can be also passed here.\n    @param cameraMatrix Input camera intrinsic matrix \\f$\\cameramatrix{A}\\f$ .\n    @param distCoeffs Input vector of distortion coefficients (4x1/1x4).\n    @param rvec Output rotation vector (see @ref Rodrigues ) that, together with tvec, brings points from\n    the model coordinate system to the camera coordinate system.\n    @param tvec Output translation vector.\n    @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses\n    the provided rvec and tvec values as initial approximations of the rotation and translation\n    vectors, respectively, and further optimizes them.\n    @param iterationsCount Number of iterations.\n    @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value\n    is the maximum allowed distance between the observed and computed point projections to consider it\n    an inlier.\n    @param confidence The probability that the algorithm produces a useful result.\n    @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .\n    @param flags Method for solving a PnP problem: see @ref calib3d_solvePnP_flags\n    This function returns the rotation and the translation vectors that transform a 3D point expressed in the object\n    coordinate frame to the camera coordinate frame, using different methods:\n    - P3P methods (@ref SOLVEPNP_P3P, @ref SOLVEPNP_AP3P): need 4 input points to return a unique solution.\n    - @ref SOLVEPNP_IPPE Input points must be >= 4 and object points must be coplanar.\n    - @ref SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.\n    Number of input points must be 4. Object points must be defined in the following order:\n    - point 0: [-squareLength / 2,  squareLength / 2, 0]\n    - point 1: [ squareLength / 2,  squareLength / 2, 0]\n    - point 2: [ squareLength / 2, -squareLength / 2, 0]\n    - point 3: [-squareLength / 2, -squareLength / 2, 0]\n    - for all the other flags, number of input points must be >= 4 and object points can be in any configuration.",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "1926898553",
        "repo_full_name": "opencv/opencv",
        "pr_number": 26669,
        "pr_file": "modules/calib3d/include/opencv2/calib3d.hpp",
        "discussion_id": "1926898553",
        "commented_code": "@@ -4098,6 +4098,53 @@ optimization. It is the \\f$max(width,height)/\\pi\\f$ or the provided \\f$f_x\\f$, \\\n                                 TermCriteria criteria = TermCriteria(TermCriteria::MAX_ITER + TermCriteria::EPS, 10, 1e-8)\n                               );\n \n+    /**\n+    @brief Finds an object pose from 3D-2D point correspondences using the RANSAC scheme for fisheye camera moodel.\n+\n+    @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or\n+    1xN/Nx1 3-channel, where N is the number of points. vector\\<Point3d\\> can be also passed here.\n+    @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,\n+    where N is the number of points. vector\\<Point2d\\> can be also passed here.\n+    @param cameraMatrix Input camera intrinsic matrix \\f$\\cameramatrix{A}\\f$ .\n+    @param distCoeffs Input vector of distortion coefficients (4x1/1x4).\n+    @param rvec Output rotation vector (see @ref Rodrigues ) that, together with tvec, brings points from\n+    the model coordinate system to the camera coordinate system.\n+    @param tvec Output translation vector.\n+    @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses\n+    the provided rvec and tvec values as initial approximations of the rotation and translation\n+    vectors, respectively, and further optimizes them.\n+    @param iterationsCount Number of iterations.\n+    @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value\n+    is the maximum allowed distance between the observed and computed point projections to consider it\n+    an inlier.\n+    @param confidence The probability that the algorithm produces a useful result.\n+    @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .\n+    @param flags Method for solving a PnP problem: see @ref calib3d_solvePnP_flags\n+    This function returns the rotation and the translation vectors that transform a 3D point expressed in the object\n+    coordinate frame to the camera coordinate frame, using different methods:\n+    - P3P methods (@ref SOLVEPNP_P3P, @ref SOLVEPNP_AP3P): need 4 input points to return a unique solution.\n+    - @ref SOLVEPNP_IPPE Input points must be >= 4 and object points must be coplanar.\n+    - @ref SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.\n+    Number of input points must be 4. Object points must be defined in the following order:\n+    - point 0: [-squareLength / 2,  squareLength / 2, 0]\n+    - point 1: [ squareLength / 2,  squareLength / 2, 0]\n+    - point 2: [ squareLength / 2, -squareLength / 2, 0]\n+    - point 3: [-squareLength / 2, -squareLength / 2, 0]\n+    - for all the other flags, number of input points must be >= 4 and object points can be in any configuration.",
        "comment_created_at": "2025-01-23T12:27:20+00:00",
        "comment_author": "asmorkalov",
        "comment_body": "I propose to use the same description as for cv::solvePnpRansac with fisheye model reference and add reference to SOLVEPNP_XXX constants instead of copy.",
        "pr_file_module": null
      },
      {
        "comment_id": "1927319841",
        "repo_full_name": "opencv/opencv",
        "pr_number": 26669,
        "pr_file": "modules/calib3d/include/opencv2/calib3d.hpp",
        "discussion_id": "1926898553",
        "commented_code": "@@ -4098,6 +4098,53 @@ optimization. It is the \\f$max(width,height)/\\pi\\f$ or the provided \\f$f_x\\f$, \\\n                                 TermCriteria criteria = TermCriteria(TermCriteria::MAX_ITER + TermCriteria::EPS, 10, 1e-8)\n                               );\n \n+    /**\n+    @brief Finds an object pose from 3D-2D point correspondences using the RANSAC scheme for fisheye camera moodel.\n+\n+    @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or\n+    1xN/Nx1 3-channel, where N is the number of points. vector\\<Point3d\\> can be also passed here.\n+    @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,\n+    where N is the number of points. vector\\<Point2d\\> can be also passed here.\n+    @param cameraMatrix Input camera intrinsic matrix \\f$\\cameramatrix{A}\\f$ .\n+    @param distCoeffs Input vector of distortion coefficients (4x1/1x4).\n+    @param rvec Output rotation vector (see @ref Rodrigues ) that, together with tvec, brings points from\n+    the model coordinate system to the camera coordinate system.\n+    @param tvec Output translation vector.\n+    @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses\n+    the provided rvec and tvec values as initial approximations of the rotation and translation\n+    vectors, respectively, and further optimizes them.\n+    @param iterationsCount Number of iterations.\n+    @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value\n+    is the maximum allowed distance between the observed and computed point projections to consider it\n+    an inlier.\n+    @param confidence The probability that the algorithm produces a useful result.\n+    @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .\n+    @param flags Method for solving a PnP problem: see @ref calib3d_solvePnP_flags\n+    This function returns the rotation and the translation vectors that transform a 3D point expressed in the object\n+    coordinate frame to the camera coordinate frame, using different methods:\n+    - P3P methods (@ref SOLVEPNP_P3P, @ref SOLVEPNP_AP3P): need 4 input points to return a unique solution.\n+    - @ref SOLVEPNP_IPPE Input points must be >= 4 and object points must be coplanar.\n+    - @ref SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.\n+    Number of input points must be 4. Object points must be defined in the following order:\n+    - point 0: [-squareLength / 2,  squareLength / 2, 0]\n+    - point 1: [ squareLength / 2,  squareLength / 2, 0]\n+    - point 2: [ squareLength / 2, -squareLength / 2, 0]\n+    - point 3: [-squareLength / 2, -squareLength / 2, 0]\n+    - for all the other flags, number of input points must be >= 4 and object points can be in any configuration.",
        "comment_created_at": "2025-01-23T16:45:10+00:00",
        "comment_author": "GouMinghao",
        "comment_body": "fixed",
        "pr_file_module": null
      }
    ]
  }
]
