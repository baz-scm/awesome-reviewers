---
title: Optimize for common cases
description: When implementing algorithms, create specialized versions for common
  cases to improve performance, while maintaining a generic version for completeness.
  For functions that process data with different channel counts, implement separate
  optimized code paths for 1-channel and 3-channel data, which represent the majority
  of image processing workloads.
repository: opencv/opencv
label: Algorithms
language: Other
comments_count: 4
repository_stars: 82865
---

When implementing algorithms, create specialized versions for common cases to improve performance, while maintaining a generic version for completeness. For functions that process data with different channel counts, implement separate optimized code paths for 1-channel and 3-channel data, which represent the majority of image processing workloads.

For example, in histogram-based median filtering:

```cpp
// Specialized version for 1-channel images
if (channels == 1) {
    uint16_t* hist0 = hist256[0].data();
    for (int x = x_start; x != x_end; x += x_step) {
        // Optimized single-channel histogram update
    }
}
// Specialized version for 3-channel images
else if (channels == 3) {
    uint16_t* hist0 = hist256[0].data();
    uint16_t* hist1 = hist256[1].data();
    uint16_t* hist2 = hist256[2].data();
    for (int x = x_start; x != x_end; x += x_step) {
        // RGB-specific histogram update
    }
}
// Generic version for other channel counts
else {
    for (int x = x_start; x != x_end; x += x_step) {
        // Generic multi-channel implementation
    }
}
```

Structure loops efficiently to avoid redundant calculations. When processing arrays, consider vectorization boundaries and ensure proper handling of edge cases with minimum conditional branches. For iterative algorithms with fixed-size vectors, implement a tail-handling pattern:

```cpp
size_t i = 0;
for (; i <= len - vl; i += vl) {
    // Process full vector width
}
if (i < len) {
    size_t tail_len = remaining_elements(len - i);
    // Process remaining elements
}
```

This pattern avoids unnecessary modulo operations and extra variables for tracking remainders. When implementing algorithms with specialized branches, ensure that all cases are handled correctly, including those that don't meet optimization criteria.


[
  {
    "discussion_id": "2086365469",
    "pr_number": 27299,
    "pr_file": "modules/imgproc/src/median_blur.simd.hpp",
    "created_at": "2025-05-13T09:32:49+00:00",
    "commented_code": "#else\n                int nlanes = 1;\n#endif\n                for( ; j <= size.width - nlanes - cn; j += nlanes )\n                if(_dst.cols > 2*nlanes) //minimum size to safely allow extra vectorized store into next row.",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "2086365469",
        "repo_full_name": "opencv/opencv",
        "pr_number": 27299,
        "pr_file": "modules/imgproc/src/median_blur.simd.hpp",
        "discussion_id": "2086365469",
        "commented_code": "@@ -693,18 +694,38 @@ medianBlur_SortNet( const Mat& _src, Mat& _dst, int m )\n #else\n                 int nlanes = 1;\n #endif\n-                for( ; j <= size.width - nlanes - cn; j += nlanes )\n+                if(_dst.cols > 2*nlanes) //minimum size to safely allow extra vectorized store into next row.",
        "comment_created_at": "2025-05-13T09:32:49+00:00",
        "comment_author": "fengyuentau",
        "comment_body": "What if _dst.cols <= 2*nlanes? Just skipped?",
        "pr_file_module": null
      },
      {
        "comment_id": "2086435491",
        "repo_full_name": "opencv/opencv",
        "pr_number": 27299,
        "pr_file": "modules/imgproc/src/median_blur.simd.hpp",
        "discussion_id": "2086365469",
        "commented_code": "@@ -693,18 +694,38 @@ medianBlur_SortNet( const Mat& _src, Mat& _dst, int m )\n #else\n                 int nlanes = 1;\n #endif\n-                for( ; j <= size.width - nlanes - cn; j += nlanes )\n+                if(_dst.cols > 2*nlanes) //minimum size to safely allow extra vectorized store into next row.",
        "comment_created_at": "2025-05-13T10:06:19+00:00",
        "comment_author": "madanm3",
        "comment_body": "this will not be skipped. when cols <= 2* lanes,\r\n- limit gets increased after this loop. \"limit = size.width\"\r\n- then it gets done in \"for( ; j < limit; j++ )\" loop above.\r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1946963394",
    "pr_number": 26887,
    "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/inv_sqrt_f.hpp",
    "created_at": "2025-02-07T18:07:36+00:00",
    "commented_code": "// This file is part of OpenCV project.\n// It is subject to the license terms in the LICENSE file found in the top-level directory\n// of this distribution and at http://opencv.org/license.html.\n#ifndef OPENCV_HAL_RVV_INV_SQRTF32_HPP_INCLUDED\n#define OPENCV_HAL_RVV_INV_SQRTF32_HPP_INCLUDED\n\n#include <riscv_vector.h>\n\nnamespace cv { namespace cv_hal_rvv {\n\n#undef cv_hal_invSqrt32f\n#define cv_hal_invSqrt32f cv::cv_hal_rvv::invSqrt32f\n\ninline int invSqrt32f (const float *src, float *dst, const int len) {\n    const size_t vl = __riscv_vsetvl_e32m8(len);\n    const size_t remainings = len % vl;\n    auto calc_fun = [&](const size_t i, const size_t vl) {\n        vfloat32m8_t vsrc = __riscv_vle32_v_f32m8(&src[i], vl), \n                    vres;\n        vres = __riscv_vfsqrt_v_f32m8(vsrc, vl);\n        vres =  __riscv_vfrdiv_vf_f32m8(vres, 1., vl);\n        __riscv_vse32_v_f32m8(&dst[i], vres, vl);\n    };\n\n    size_t i = 0;\n    for (; i < len - remainings; i += vl)",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "1946963394",
        "repo_full_name": "opencv/opencv",
        "pr_number": 26887,
        "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/inv_sqrt_f.hpp",
        "discussion_id": "1946963394",
        "commented_code": "@@ -0,0 +1,36 @@\n+// This file is part of OpenCV project.\n+// It is subject to the license terms in the LICENSE file found in the top-level directory\n+// of this distribution and at http://opencv.org/license.html.\n+#ifndef OPENCV_HAL_RVV_INV_SQRTF32_HPP_INCLUDED\n+#define OPENCV_HAL_RVV_INV_SQRTF32_HPP_INCLUDED\n+\n+#include <riscv_vector.h>\n+\n+namespace cv { namespace cv_hal_rvv {\n+\n+#undef cv_hal_invSqrt32f\n+#define cv_hal_invSqrt32f cv::cv_hal_rvv::invSqrt32f\n+\n+inline int invSqrt32f (const float *src, float *dst, const int len) {\n+    const size_t vl = __riscv_vsetvl_e32m8(len);\n+    const size_t remainings = len % vl;\n+    auto calc_fun = [&](const size_t i, const size_t vl) {\n+        vfloat32m8_t vsrc = __riscv_vle32_v_f32m8(&src[i], vl), \n+                    vres;\n+        vres = __riscv_vfsqrt_v_f32m8(vsrc, vl);\n+        vres =  __riscv_vfrdiv_vf_f32m8(vres, 1., vl);\n+        __riscv_vse32_v_f32m8(&dst[i], vres, vl);\n+    };\n+\n+    size_t i = 0;\n+    for (; i < len - remainings; i += vl)",
        "comment_created_at": "2025-02-07T18:07:36+00:00",
        "comment_author": "dkurt",
        "comment_body": "We can avoid `len % vl` and `remainings ` variable by using the following loop:\r\n```cpp\r\nfor (; i <= len - vl; i += vl)\r\n{\r\n    calc_fun(i, vl);\r\n}\r\nif (i < len)\r\n{\r\n    size_t tail_len = __riscv_vsetvl_e32m8(len - i);\r\n    calc_fun(i, tail_len);\r\n}\r\n```\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1830973143",
    "pr_number": 26422,
    "pr_file": "modules/imgproc/src/median_blur.simd.hpp",
    "created_at": "2024-11-06T12:54:38+00:00",
    "commented_code": "#undef N\n#undef UPDATE_ACC\n}\n#endif\n\n// Binary search to find the median offset in the histogram\nstatic inline void\nget_median_ofs256(int &ofs, const uint16_t *hist, const int halfsum) {\n    int s = 0, step = 128;\n    uint16_t ds, m;\n\n#if CV_SIMD || CV_SIMD_SCALABLE\n    const int lanes = VTraits<v_uint16>::vlanes();\n#endif\n\n    ofs = 0;\n    while (step) {\n#if CV_SIMD || CV_SIMD_SCALABLE\n        // Use SIMD instructions when the step is larger than or equal to the lane size\n        if (step >= lanes) {\n            v_uint16 v_ds = vx_load(hist + ofs);\n            for (int i = lanes; i <= step - lanes; i += lanes)\n                v_ds = v_add(v_ds, vx_load(hist + ofs + i));\n            ds = v_reduce_sum(v_ds);\n        }\n            // For smaller steps, use scalar accumulation\n        else\n#endif\n        {\n            ds = hist[ofs];\n            for (int i = 1; i < step; i++)\n                ds += hist[ofs + i];\n        }\n\n        // Determine if the cumulative sum has reached or surpassed half the total sum\n        m = (s + ds) <= halfsum;\n        ofs += m * step;\n        s += ds & -m;\n        step = step >> 1;\n    }\n}\n\nstatic void\nmedianBlur_8u(const Mat &_src, Mat &_dst, int ksize) {\n    CV_INSTRUMENT_REGION();\n\n    const int channels = _src.channels();\n    int radius = ksize / 2;\n    CV_Assert(ksize % 2 == 1);\n\n    const int rows = _src.rows;\n    const int cols = _src.cols;\n    const int win_half_size = ksize * ksize / 2;\n\n    const uint8_t *src_data = _src.ptr<uint8_t>();\n    size_t src_step = _src.step;\n    uint8_t *dst_data = _dst.ptr<uint8_t>();\n    size_t dst_step = _dst.step;\n\n    double nstripes = (double) (rows * cols) / 1024.0;\n    parallel_for_(Range(0, rows), [&](const Range &r) {\n        int y_start = r.start;\n        int y_end = r.end;\n\n        std::vector<std::vector<uint16_t>> hist256(channels, std::vector<uint16_t>(256, 0));\n        std::vector<const uint8_t *> win_row_ptrs(ksize);\n\n        // initial hist for the first pixel\n        for (int dy = -radius; dy <= radius; dy++) {\n            int y = std::clamp(y_start + dy, 0, rows - 1);\n            const uint8_t *row_ptr = src_data + y * src_step;\n            win_row_ptrs[dy + radius] = row_ptr;\n\n            // always start from col 0, so need to add radius to first pixel\n            for (int c = 0; c < channels; ++c)\n                hist256[c][row_ptr[c]] += radius + 1;\n            // add the rest of the row\n            for (int dx = 1; dx <= radius; dx++) {\n                int x = std::min(dx, cols - 1);\n                const uint8_t *pixel_ptr = row_ptr + x * channels;\n                for (int c = 0; c < channels; ++c) {\n                    hist256[c][pixel_ptr[c]]++;\n                }\n            }\n        }\n\n        int x_start = 0, x_end = cols, x_step = 1;\n        // slide the window across the row\n        for (int y = y_start; y < y_end; y++) {\n            uint8_t *dst_pixel_ptr = dst_data + y * dst_step + x_start * channels;\n            for (int x = x_start; x != x_end; x += x_step, dst_pixel_ptr += x_step * channels) {\n                if (x != x_start) {\n                    // update hist by removing and adding pixel values as the window moves\n                    int x_remove = std::clamp(x - (radius + 1) * x_step, 0, cols - 1) * channels;\n                    int x_add = std::clamp(x + radius * x_step, 0, cols - 1) * channels;\n                    for (int dy = 0; dy < ksize; dy++) {",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "1830973143",
        "repo_full_name": "opencv/opencv",
        "pr_number": 26422,
        "pr_file": "modules/imgproc/src/median_blur.simd.hpp",
        "discussion_id": "1830973143",
        "commented_code": "@@ -489,7 +490,149 @@ medianBlur_8u_Om( const Mat& _src, Mat& _dst, int m )\n #undef N\n #undef UPDATE_ACC\n }\n+#endif\n+\n+// Binary search to find the median offset in the histogram\n+static inline void\n+get_median_ofs256(int &ofs, const uint16_t *hist, const int halfsum) {\n+    int s = 0, step = 128;\n+    uint16_t ds, m;\n \n+#if CV_SIMD || CV_SIMD_SCALABLE\n+    const int lanes = VTraits<v_uint16>::vlanes();\n+#endif\n+\n+    ofs = 0;\n+    while (step) {\n+#if CV_SIMD || CV_SIMD_SCALABLE\n+        // Use SIMD instructions when the step is larger than or equal to the lane size\n+        if (step >= lanes) {\n+            v_uint16 v_ds = vx_load(hist + ofs);\n+            for (int i = lanes; i <= step - lanes; i += lanes)\n+                v_ds = v_add(v_ds, vx_load(hist + ofs + i));\n+            ds = v_reduce_sum(v_ds);\n+        }\n+            // For smaller steps, use scalar accumulation\n+        else\n+#endif\n+        {\n+            ds = hist[ofs];\n+            for (int i = 1; i < step; i++)\n+                ds += hist[ofs + i];\n+        }\n+\n+        // Determine if the cumulative sum has reached or surpassed half the total sum\n+        m = (s + ds) <= halfsum;\n+        ofs += m * step;\n+        s += ds & -m;\n+        step = step >> 1;\n+    }\n+}\n+\n+static void\n+medianBlur_8u(const Mat &_src, Mat &_dst, int ksize) {\n+    CV_INSTRUMENT_REGION();\n+\n+    const int channels = _src.channels();\n+    int radius = ksize / 2;\n+    CV_Assert(ksize % 2 == 1);\n+\n+    const int rows = _src.rows;\n+    const int cols = _src.cols;\n+    const int win_half_size = ksize * ksize / 2;\n+\n+    const uint8_t *src_data = _src.ptr<uint8_t>();\n+    size_t src_step = _src.step;\n+    uint8_t *dst_data = _dst.ptr<uint8_t>();\n+    size_t dst_step = _dst.step;\n+\n+    double nstripes = (double) (rows * cols) / 1024.0;\n+    parallel_for_(Range(0, rows), [&](const Range &r) {\n+        int y_start = r.start;\n+        int y_end = r.end;\n+\n+        std::vector<std::vector<uint16_t>> hist256(channels, std::vector<uint16_t>(256, 0));\n+        std::vector<const uint8_t *> win_row_ptrs(ksize);\n+\n+        // initial hist for the first pixel\n+        for (int dy = -radius; dy <= radius; dy++) {\n+            int y = std::clamp(y_start + dy, 0, rows - 1);\n+            const uint8_t *row_ptr = src_data + y * src_step;\n+            win_row_ptrs[dy + radius] = row_ptr;\n+\n+            // always start from col 0, so need to add radius to first pixel\n+            for (int c = 0; c < channels; ++c)\n+                hist256[c][row_ptr[c]] += radius + 1;\n+            // add the rest of the row\n+            for (int dx = 1; dx <= radius; dx++) {\n+                int x = std::min(dx, cols - 1);\n+                const uint8_t *pixel_ptr = row_ptr + x * channels;\n+                for (int c = 0; c < channels; ++c) {\n+                    hist256[c][pixel_ptr[c]]++;\n+                }\n+            }\n+        }\n+\n+        int x_start = 0, x_end = cols, x_step = 1;\n+        // slide the window across the row\n+        for (int y = y_start; y < y_end; y++) {\n+            uint8_t *dst_pixel_ptr = dst_data + y * dst_step + x_start * channels;\n+            for (int x = x_start; x != x_end; x += x_step, dst_pixel_ptr += x_step * channels) {\n+                if (x != x_start) {\n+                    // update hist by removing and adding pixel values as the window moves\n+                    int x_remove = std::clamp(x - (radius + 1) * x_step, 0, cols - 1) * channels;\n+                    int x_add = std::clamp(x + radius * x_step, 0, cols - 1) * channels;\n+                    for (int dy = 0; dy < ksize; dy++) {",
        "comment_created_at": "2024-11-06T12:54:38+00:00",
        "comment_author": "vpisarev",
        "comment_body": "please, create separate versions of histogram update loop for `channels == 1`, `channels == 3` and the generic case.\r\nstore separate pointers for 1-channel hist256 (hist0=hist256[0]) and 3-channel hist256 (`hist0 = hist256[0], hist1=hist256[1] and hist2=hist256[2]`)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1975941881",
    "pr_number": 26958,
    "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/pyramids.hpp",
    "created_at": "2025-02-28T19:53:20+00:00",
    "commented_code": "// This file is part of OpenCV project.\n// It is subject to the license terms in the LICENSE file found in the top-level directory\n// of this distribution and at http://opencv.org/license.html.\n#ifndef OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n#define OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n\n#include <riscv_vector.h>\n\nnamespace cv { namespace cv_hal_rvv { namespace pyramids {\n\n#undef cv_hal_pyrdown\n#define cv_hal_pyrdown cv::cv_hal_rvv::pyramids::pyrDown\n#undef cv_hal_pyrup\n#define cv_hal_pyrup cv::cv_hal_rvv::pyramids::pyrUp\n\ntemplate<typename T> struct rvv;\n\ntemplate<> struct rvv<uchar>\n{\n    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n    static inline vuint8m1_t vle_T(const uchar* a, size_t b) { return __riscv_vle8_v_u8m1(a, b); }\n    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n    static inline vuint8m1_t vlse_T(const uchar* a, ptrdiff_t b, size_t c) { return __riscv_vlse8_v_u8m1(a, b, c); }\n    static inline vuint8m1_t vloxei_T(const uchar* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_u8m1(a, b, c); }\n    static inline void vse_T(uchar* a, vuint8m1_t b, size_t c) { return __riscv_vse8(a, b, c); }\n    static inline vint32m4_t vcvt_T_WT(vuint8m1_t a, size_t b) { return __riscv_vreinterpret_v_u32m4_i32m4(__riscv_vzext_vf4(a, b)); }\n    static inline vuint8m1_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vncvt_x(__riscv_vreinterpret_v_i32m4_u32m4(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c)), c), c); }\n};\n\ntemplate<> struct rvv<short>\n{\n    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n    static inline vint16m2_t vle_T(const short* a, size_t b) { return __riscv_vle16_v_i16m2(a, b); }\n    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n    static inline vint16m2_t vlse_T(const short* a, ptrdiff_t b, size_t c) { return __riscv_vlse16_v_i16m2(a, b, c); }\n    static inline vint16m2_t vloxei_T(const short* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_i16m2(a, b, c); }\n    static inline void vse_T(short* a, vint16m2_t b, size_t c) { return __riscv_vse16(a, b, c); }\n    static inline vint32m4_t vcvt_T_WT(vint16m2_t a, size_t b) { return __riscv_vsext_vf2(a, b); }\n    static inline vint16m2_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c), c); }\n};\n\ntemplate<> struct rvv<float>\n{\n    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n    static inline vfloat32m4_t vle_T(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n    static inline vfloat32m4_t vle_WT(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n    static inline vfloat32m4_t vlse_T(const float* a, ptrdiff_t b, size_t c) { return __riscv_vlse32_v_f32m4(a, b, c); }\n    static inline vfloat32m4_t vloxei_T(const float* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_f32m4(a, b, c); }\n    static inline void vse_T(float* a, vfloat32m4_t b, size_t c) { return __riscv_vse32(a, b, c); }\n};\n\ntemplate<typename T, typename WT> struct pyrDownVec0\n{\n    void operator()(const T* src, WT* row, const uint* tabM, int start, int end)\n    {\n        int vl;\n        switch (start)\n        {\n        case 1:\n            for( int x = start; x < end; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 2, 2 * sizeof(T), vl), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 1, 2 * sizeof(T), vl), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2, 2 * sizeof(T), vl), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 1, 2 * sizeof(T), vl), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 2, 2 * sizeof(T), vl), vl);\n                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n            break;\n        case 2:\n            for( int x = start / 2; x < end / 2; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end / 2 - x);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(T), vl), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(T), vl), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2, 4 * sizeof(T), vl), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(T), vl), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 2, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n            break;\n        case 3:\n            for( int x = start / 3; x < end / 3; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end / 3 - x);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(T), vl), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(T), vl), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3, 6 * sizeof(T), vl), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(T), vl), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 3, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n            break;\n        case 4:\n            for( int x = start / 4; x < end / 4; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end / 4 - x);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(T), vl), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(T), vl), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4, 8 * sizeof(T), vl), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(T), vl), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 4, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n            break;\n        default:\n            for( int x = start; x < end; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end - x);\n                auto vec_tabM = rvv<T>::vle_M(tabM + x, vl);\n                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(T), vl);\n                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n            }\n        }\n    }\n};\ntemplate<> struct pyrDownVec0<float, float>\n{\n    void operator()(const float* src, float* row, const uint* tabM, int start, int end)\n    {\n        int vl;\n        switch (start)\n        {\n        case 1:\n            for( int x = start; x < end; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<float>::vlse_T(src + x * 2 - 2, 2 * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vlse_T(src + x * 2 - 1, 2 * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vlse_T(src + x * 2, 2 * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vlse_T(src + x * 2 + 1, 2 * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vlse_T(src + x * 2 + 2, 2 * sizeof(float), vl);\n                __riscv_vse32(row + x, __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n            }\n            break;\n        case 2:\n            for( int x = start / 2; x < end / 2; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end / 2 - x);\n                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2, 4 * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 2, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n            }\n            break;\n        case 3:\n            for( int x = start / 3; x < end / 3; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end / 3 - x);\n                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3, 6 * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 3, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n            }\n            break;\n        case 4:\n            for( int x = start / 4; x < end / 4; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end / 4 - x);\n                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4, 8 * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 4, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(float), vl);\n                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(float), vl);\n                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(float), vl);\n                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(float), vl);\n                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(float), vl);\n                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n            }\n            break;\n        default:\n            for( int x = start; x < end; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end - x);\n                auto vec_tabM = rvv<float>::vle_M(tabM + x, vl);\n                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(float), vl);\n                auto vec_src0 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n                auto vec_src1 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n                auto vec_src2 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n                auto vec_src3 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n                auto vec_src4 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n                __riscv_vse32(row + x, __riscv_vfmadd(__riscv_vfadd(__riscv_vfadd(vec_src1, vec_src2, vl), vec_src3, vl), 4,\n                                                      __riscv_vfadd(__riscv_vfadd(vec_src0, vec_src4, vl), __riscv_vfadd(vec_src2, vec_src2, vl), vl), vl), vl);\n            }\n        }\n    }\n};\n\ntemplate<typename T, typename WT> struct pyrDownVec1\n{\n    void operator()(WT* row0, WT* row1, WT* row2, WT* row3, WT* row4, T* dst, int end)\n    {\n        int vl;\n        for( int x = 0 ; x < end; x += vl )\n        {\n            vl = rvv<T>::vsetvl_WT(end - x);\n            auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n            auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n            auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n            auto vec_src3 = rvv<T>::vle_WT(row3 + x, vl);\n            auto vec_src4 = rvv<T>::vle_WT(row4 + x, vl);\n            rvv<T>::vse_T(dst + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n                                                                                      __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), 8, vl), vl);\n        }\n    }\n};\ntemplate<> struct pyrDownVec1<float, float>\n{\n    void operator()(float* row0, float* row1, float* row2, float* row3, float* row4, float* dst, int end)\n    {\n        int vl;\n        for( int x = 0 ; x < end; x += vl )\n        {\n            vl = rvv<float>::vsetvl_WT(end - x);\n            auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n            auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n            auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n            auto vec_src3 = rvv<float>::vle_WT(row3 + x, vl);\n            auto vec_src4 = rvv<float>::vle_WT(row4 + x, vl);\n            rvv<float>::vse_T(dst + x, __riscv_vfmul(__riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), 1.f / 256.f, vl), vl);\n        }\n    }\n};\n\ntemplate<typename T, typename WT> struct pyrUpVec0\n{\n    void operator()(const T* src, WT* row, const uint* dtab, int start, int end)\n    {\n        int vl;\n        for( int x = start; x < end; x += vl )\n        {\n            vl = rvv<T>::vsetvl_WT(end - x);\n            auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x - start, vl), vl);\n            auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x, vl), vl);\n            auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x + start, vl), vl);\n\n            auto vec_dtab = rvv<T>::vle_M(dtab + x, vl);\n            vec_dtab = __riscv_vmul(vec_dtab, sizeof(WT), vl);\n            __riscv_vsoxei32(row, vec_dtab, __riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), vl);\n            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(WT), vl), __riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), vl);\n        }\n    }\n};\ntemplate<> struct pyrUpVec0<float, float>\n{\n    void operator()(const float* src, float* row, const uint* dtab, int start, int end)\n    {\n        int vl;\n        for( int x = start; x < end; x += vl )\n        {\n            vl = rvv<float>::vsetvl_WT(end - x);\n            auto vec_src0 = rvv<float>::vle_T(src + x - start, vl);\n            auto vec_src1 = rvv<float>::vle_T(src + x, vl);\n            auto vec_src2 = rvv<float>::vle_T(src + x + start, vl);\n\n            auto vec_dtab = rvv<float>::vle_M(dtab + x, vl);\n            vec_dtab = __riscv_vmul(vec_dtab, sizeof(float), vl);\n            __riscv_vsoxei32(row, vec_dtab, __riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), vl);\n            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(float), vl), __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 4, vl), vl);\n        }\n    }\n};\n\ntemplate<typename T, typename WT> struct pyrUpVec1\n{\n    void operator()(WT* row0, WT* row1, WT* row2, T* dst0, T* dst1, int end)\n    {\n        int vl;\n        if (dst0 != dst1)\n        {\n            for( int x = 0 ; x < end; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n                rvv<T>::vse_T(dst1 + x, rvv<T>::vcvt_WT_T(__riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), 6, vl), vl);\n            }\n        }\n        else\n        {\n            for( int x = 0 ; x < end; x += vl )\n            {\n                vl = rvv<T>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n            }\n        }\n    }\n};\ntemplate<> struct pyrUpVec1<float, float>\n{\n    void operator()(float* row0, float* row1, float* row2, float* dst0, float* dst1, int end)\n    {\n        int vl;\n        if (dst0 != dst1)\n        {\n            for( int x = 0 ; x < end; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n                rvv<float>::vse_T(dst1 + x, __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 1.f / 16.f, vl), vl);\n            }\n        }\n        else\n        {\n            for( int x = 0 ; x < end; x += vl )\n            {\n                vl = rvv<float>::vsetvl_WT(end - x);\n                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n            }\n        }\n    }\n};\n\ntemplate<typename T, typename WT>\nstruct PyrDownInvoker : ParallelLoopBody\n{\n    PyrDownInvoker(const uchar* _src_data, size_t _src_step, int _src_width, int _src_height, uchar* _dst_data, size_t _dst_step, int _dst_width, int _dst_height, int _cn, int _borderType, int* _tabR, int* _tabM, int* _tabL)\n    {\n        src_data = _src_data;\n        src_step = _src_step;\n        src_width = _src_width;\n        src_height = _src_height;\n        dst_data = _dst_data;\n        dst_step = _dst_step;\n        dst_width = _dst_width;\n        dst_height = _dst_height;\n        cn = _cn;\n        borderType = _borderType;\n        tabR = _tabR;\n        tabM = _tabM;\n        tabL = _tabL;\n    }\n\n    void operator()(const Range& range) const CV_OVERRIDE;\n\n    const uchar* src_data;\n    size_t src_step;\n    int src_width;\n    int src_height;\n    uchar* dst_data;\n    size_t dst_step;\n    int dst_width;\n    int dst_height;\n    int cn;\n    int borderType;\n    int* tabR;\n    int* tabM;\n    int* tabL;\n};\n\n// the algorithm is copied from imgproc/src/pyramids.cpp,\n// in the function template void cv::pyrDown_\ntemplate<typename T, typename WT>\ninline int pyrDown(const uchar* src_data, size_t src_step, int src_width, int src_height, uchar* dst_data, size_t dst_step, int dst_width, int dst_height, int cn, int borderType)\n{\n    const int PD_SZ = 5;\n\n    std::vector<int> _tabM(dst_width * cn), _tabL(cn * (PD_SZ + 2)), _tabR(cn * (PD_SZ + 2));\n    int *tabM = _tabM.data(), *tabL = _tabL.data(), *tabR = _tabR.data();\n\n    CV_Assert( src_width > 0 && src_height > 0 &&\n               std::abs(dst_width*2 - src_width) <= 2 &&\n               std::abs(dst_height*2 - src_height) <= 2 );\n    int width0 = std::min((src_width-PD_SZ/2-1)/2 + 1, dst_width);\n\n    for (int x = 0; x <= PD_SZ+1; x++)\n    {\n        int sx0 = borderInterpolate(x - PD_SZ/2, src_width, borderType)*cn;",
    "repo_full_name": "opencv/opencv",
    "discussion_comments": [
      {
        "comment_id": "1975941881",
        "repo_full_name": "opencv/opencv",
        "pr_number": 26958,
        "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/pyramids.hpp",
        "discussion_id": "1975941881",
        "commented_code": "@@ -0,0 +1,681 @@\n+// This file is part of OpenCV project.\n+// It is subject to the license terms in the LICENSE file found in the top-level directory\n+// of this distribution and at http://opencv.org/license.html.\n+#ifndef OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n+#define OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n+\n+#include <riscv_vector.h>\n+\n+namespace cv { namespace cv_hal_rvv { namespace pyramids {\n+\n+#undef cv_hal_pyrdown\n+#define cv_hal_pyrdown cv::cv_hal_rvv::pyramids::pyrDown\n+#undef cv_hal_pyrup\n+#define cv_hal_pyrup cv::cv_hal_rvv::pyramids::pyrUp\n+\n+template<typename T> struct rvv;\n+\n+template<> struct rvv<uchar>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vuint8m1_t vle_T(const uchar* a, size_t b) { return __riscv_vle8_v_u8m1(a, b); }\n+    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vuint8m1_t vlse_T(const uchar* a, ptrdiff_t b, size_t c) { return __riscv_vlse8_v_u8m1(a, b, c); }\n+    static inline vuint8m1_t vloxei_T(const uchar* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_u8m1(a, b, c); }\n+    static inline void vse_T(uchar* a, vuint8m1_t b, size_t c) { return __riscv_vse8(a, b, c); }\n+    static inline vint32m4_t vcvt_T_WT(vuint8m1_t a, size_t b) { return __riscv_vreinterpret_v_u32m4_i32m4(__riscv_vzext_vf4(a, b)); }\n+    static inline vuint8m1_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vncvt_x(__riscv_vreinterpret_v_i32m4_u32m4(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c)), c), c); }\n+};\n+\n+template<> struct rvv<short>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vint16m2_t vle_T(const short* a, size_t b) { return __riscv_vle16_v_i16m2(a, b); }\n+    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vint16m2_t vlse_T(const short* a, ptrdiff_t b, size_t c) { return __riscv_vlse16_v_i16m2(a, b, c); }\n+    static inline vint16m2_t vloxei_T(const short* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_i16m2(a, b, c); }\n+    static inline void vse_T(short* a, vint16m2_t b, size_t c) { return __riscv_vse16(a, b, c); }\n+    static inline vint32m4_t vcvt_T_WT(vint16m2_t a, size_t b) { return __riscv_vsext_vf2(a, b); }\n+    static inline vint16m2_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c), c); }\n+};\n+\n+template<> struct rvv<float>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vfloat32m4_t vle_T(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n+    static inline vfloat32m4_t vle_WT(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vfloat32m4_t vlse_T(const float* a, ptrdiff_t b, size_t c) { return __riscv_vlse32_v_f32m4(a, b, c); }\n+    static inline vfloat32m4_t vloxei_T(const float* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_f32m4(a, b, c); }\n+    static inline void vse_T(float* a, vfloat32m4_t b, size_t c) { return __riscv_vse32(a, b, c); }\n+};\n+\n+template<typename T, typename WT> struct pyrDownVec0\n+{\n+    void operator()(const T* src, WT* row, const uint* tabM, int start, int end)\n+    {\n+        int vl;\n+        switch (start)\n+        {\n+        case 1:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 2, 2 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 1, 2 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2, 2 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 1, 2 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 2, 2 * sizeof(T), vl), vl);\n+                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 2:\n+            for( int x = start / 2; x < end / 2; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 2 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 2, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 3:\n+            for( int x = start / 3; x < end / 3; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 3 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 4:\n+            for( int x = start / 4; x < end / 4; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 4 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        default:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_tabM = rvv<T>::vle_M(tabM + x, vl);\n+                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(T), vl);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+        }\n+    }\n+};\n+template<> struct pyrDownVec0<float, float>\n+{\n+    void operator()(const float* src, float* row, const uint* tabM, int start, int end)\n+    {\n+        int vl;\n+        switch (start)\n+        {\n+        case 1:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + x * 2 - 2, 2 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + x * 2 - 1, 2 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + x * 2, 2 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + x * 2 + 1, 2 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + x * 2 + 2, 2 * sizeof(float), vl);\n+                __riscv_vse32(row + x, __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 2:\n+            for( int x = start / 2; x < end / 2; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 2 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2, 4 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 2, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 3:\n+            for( int x = start / 3; x < end / 3; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 3 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3, 6 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 4:\n+            for( int x = start / 4; x < end / 4; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 4 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4, 8 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        default:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_tabM = rvv<float>::vle_M(tabM + x, vl);\n+                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(float), vl);\n+                auto vec_src0 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                __riscv_vse32(row + x, __riscv_vfmadd(__riscv_vfadd(__riscv_vfadd(vec_src1, vec_src2, vl), vec_src3, vl), 4,\n+                                                      __riscv_vfadd(__riscv_vfadd(vec_src0, vec_src4, vl), __riscv_vfadd(vec_src2, vec_src2, vl), vl), vl), vl);\n+            }\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrDownVec1\n+{\n+    void operator()(WT* row0, WT* row1, WT* row2, WT* row3, WT* row4, T* dst, int end)\n+    {\n+        int vl;\n+        for( int x = 0 ; x < end; x += vl )\n+        {\n+            vl = rvv<T>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+            auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+            auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+            auto vec_src3 = rvv<T>::vle_WT(row3 + x, vl);\n+            auto vec_src4 = rvv<T>::vle_WT(row4 + x, vl);\n+            rvv<T>::vse_T(dst + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                                      __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), 8, vl), vl);\n+        }\n+    }\n+};\n+template<> struct pyrDownVec1<float, float>\n+{\n+    void operator()(float* row0, float* row1, float* row2, float* row3, float* row4, float* dst, int end)\n+    {\n+        int vl;\n+        for( int x = 0 ; x < end; x += vl )\n+        {\n+            vl = rvv<float>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+            auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+            auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+            auto vec_src3 = rvv<float>::vle_WT(row3 + x, vl);\n+            auto vec_src4 = rvv<float>::vle_WT(row4 + x, vl);\n+            rvv<float>::vse_T(dst + x, __riscv_vfmul(__riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), 1.f / 256.f, vl), vl);\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrUpVec0\n+{\n+    void operator()(const T* src, WT* row, const uint* dtab, int start, int end)\n+    {\n+        int vl;\n+        for( int x = start; x < end; x += vl )\n+        {\n+            vl = rvv<T>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x - start, vl), vl);\n+            auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x, vl), vl);\n+            auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x + start, vl), vl);\n+\n+            auto vec_dtab = rvv<T>::vle_M(dtab + x, vl);\n+            vec_dtab = __riscv_vmul(vec_dtab, sizeof(WT), vl);\n+            __riscv_vsoxei32(row, vec_dtab, __riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), vl);\n+            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(WT), vl), __riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), vl);\n+        }\n+    }\n+};\n+template<> struct pyrUpVec0<float, float>\n+{\n+    void operator()(const float* src, float* row, const uint* dtab, int start, int end)\n+    {\n+        int vl;\n+        for( int x = start; x < end; x += vl )\n+        {\n+            vl = rvv<float>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<float>::vle_T(src + x - start, vl);\n+            auto vec_src1 = rvv<float>::vle_T(src + x, vl);\n+            auto vec_src2 = rvv<float>::vle_T(src + x + start, vl);\n+\n+            auto vec_dtab = rvv<float>::vle_M(dtab + x, vl);\n+            vec_dtab = __riscv_vmul(vec_dtab, sizeof(float), vl);\n+            __riscv_vsoxei32(row, vec_dtab, __riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), vl);\n+            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(float), vl), __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 4, vl), vl);\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrUpVec1\n+{\n+    void operator()(WT* row0, WT* row1, WT* row2, T* dst0, T* dst1, int end)\n+    {\n+        int vl;\n+        if (dst0 != dst1)\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n+                rvv<T>::vse_T(dst1 + x, rvv<T>::vcvt_WT_T(__riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), 6, vl), vl);\n+            }\n+        }\n+        else\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n+            }\n+        }\n+    }\n+};\n+template<> struct pyrUpVec1<float, float>\n+{\n+    void operator()(float* row0, float* row1, float* row2, float* dst0, float* dst1, int end)\n+    {\n+        int vl;\n+        if (dst0 != dst1)\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n+                rvv<float>::vse_T(dst1 + x, __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 1.f / 16.f, vl), vl);\n+            }\n+        }\n+        else\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n+            }\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT>\n+struct PyrDownInvoker : ParallelLoopBody\n+{\n+    PyrDownInvoker(const uchar* _src_data, size_t _src_step, int _src_width, int _src_height, uchar* _dst_data, size_t _dst_step, int _dst_width, int _dst_height, int _cn, int _borderType, int* _tabR, int* _tabM, int* _tabL)\n+    {\n+        src_data = _src_data;\n+        src_step = _src_step;\n+        src_width = _src_width;\n+        src_height = _src_height;\n+        dst_data = _dst_data;\n+        dst_step = _dst_step;\n+        dst_width = _dst_width;\n+        dst_height = _dst_height;\n+        cn = _cn;\n+        borderType = _borderType;\n+        tabR = _tabR;\n+        tabM = _tabM;\n+        tabL = _tabL;\n+    }\n+\n+    void operator()(const Range& range) const CV_OVERRIDE;\n+\n+    const uchar* src_data;\n+    size_t src_step;\n+    int src_width;\n+    int src_height;\n+    uchar* dst_data;\n+    size_t dst_step;\n+    int dst_width;\n+    int dst_height;\n+    int cn;\n+    int borderType;\n+    int* tabR;\n+    int* tabM;\n+    int* tabL;\n+};\n+\n+// the algorithm is copied from imgproc/src/pyramids.cpp,\n+// in the function template void cv::pyrDown_\n+template<typename T, typename WT>\n+inline int pyrDown(const uchar* src_data, size_t src_step, int src_width, int src_height, uchar* dst_data, size_t dst_step, int dst_width, int dst_height, int cn, int borderType)\n+{\n+    const int PD_SZ = 5;\n+\n+    std::vector<int> _tabM(dst_width * cn), _tabL(cn * (PD_SZ + 2)), _tabR(cn * (PD_SZ + 2));\n+    int *tabM = _tabM.data(), *tabL = _tabL.data(), *tabR = _tabR.data();\n+\n+    CV_Assert( src_width > 0 && src_height > 0 &&\n+               std::abs(dst_width*2 - src_width) <= 2 &&\n+               std::abs(dst_height*2 - src_height) <= 2 );\n+    int width0 = std::min((src_width-PD_SZ/2-1)/2 + 1, dst_width);\n+\n+    for (int x = 0; x <= PD_SZ+1; x++)\n+    {\n+        int sx0 = borderInterpolate(x - PD_SZ/2, src_width, borderType)*cn;",
        "comment_created_at": "2025-02-28T19:53:20+00:00",
        "comment_author": "mshabunin",
        "comment_body": "Not sure what to do with `borderInterpolate`. Perhaps it should be implemented here as well (maybe in limited version only for acceptable border modes).",
        "pr_file_module": null
      },
      {
        "comment_id": "1976248637",
        "repo_full_name": "opencv/opencv",
        "pr_number": 26958,
        "pr_file": "3rdparty/hal_rvv/hal_rvv_1p0/pyramids.hpp",
        "discussion_id": "1975941881",
        "commented_code": "@@ -0,0 +1,681 @@\n+// This file is part of OpenCV project.\n+// It is subject to the license terms in the LICENSE file found in the top-level directory\n+// of this distribution and at http://opencv.org/license.html.\n+#ifndef OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n+#define OPENCV_HAL_RVV_PYRAMIDS_HPP_INCLUDED\n+\n+#include <riscv_vector.h>\n+\n+namespace cv { namespace cv_hal_rvv { namespace pyramids {\n+\n+#undef cv_hal_pyrdown\n+#define cv_hal_pyrdown cv::cv_hal_rvv::pyramids::pyrDown\n+#undef cv_hal_pyrup\n+#define cv_hal_pyrup cv::cv_hal_rvv::pyramids::pyrUp\n+\n+template<typename T> struct rvv;\n+\n+template<> struct rvv<uchar>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vuint8m1_t vle_T(const uchar* a, size_t b) { return __riscv_vle8_v_u8m1(a, b); }\n+    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vuint8m1_t vlse_T(const uchar* a, ptrdiff_t b, size_t c) { return __riscv_vlse8_v_u8m1(a, b, c); }\n+    static inline vuint8m1_t vloxei_T(const uchar* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_u8m1(a, b, c); }\n+    static inline void vse_T(uchar* a, vuint8m1_t b, size_t c) { return __riscv_vse8(a, b, c); }\n+    static inline vint32m4_t vcvt_T_WT(vuint8m1_t a, size_t b) { return __riscv_vreinterpret_v_u32m4_i32m4(__riscv_vzext_vf4(a, b)); }\n+    static inline vuint8m1_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vncvt_x(__riscv_vreinterpret_v_i32m4_u32m4(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c)), c), c); }\n+};\n+\n+template<> struct rvv<short>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vint16m2_t vle_T(const short* a, size_t b) { return __riscv_vle16_v_i16m2(a, b); }\n+    static inline vint32m4_t vle_WT(const int* a, size_t b) { return __riscv_vle32_v_i32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vint16m2_t vlse_T(const short* a, ptrdiff_t b, size_t c) { return __riscv_vlse16_v_i16m2(a, b, c); }\n+    static inline vint16m2_t vloxei_T(const short* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_i16m2(a, b, c); }\n+    static inline void vse_T(short* a, vint16m2_t b, size_t c) { return __riscv_vse16(a, b, c); }\n+    static inline vint32m4_t vcvt_T_WT(vint16m2_t a, size_t b) { return __riscv_vsext_vf2(a, b); }\n+    static inline vint16m2_t vcvt_WT_T(vint32m4_t a, int b, size_t c) { return __riscv_vncvt_x(__riscv_vsra(__riscv_vadd(a, 1 << (b - 1), c), b, c), c); }\n+};\n+\n+template<> struct rvv<float>\n+{\n+    static inline size_t vsetvl_WT(size_t a) { return __riscv_vsetvl_e32m4(a); }\n+    static inline vfloat32m4_t vle_T(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n+    static inline vfloat32m4_t vle_WT(const float* a, size_t b) { return __riscv_vle32_v_f32m4(a, b); }\n+    static inline vuint32m4_t vle_M(const uint* a, size_t b) { return __riscv_vle32_v_u32m4(a, b); }\n+    static inline vfloat32m4_t vlse_T(const float* a, ptrdiff_t b, size_t c) { return __riscv_vlse32_v_f32m4(a, b, c); }\n+    static inline vfloat32m4_t vloxei_T(const float* a, vuint32m4_t b, size_t c) { return __riscv_vloxei32_v_f32m4(a, b, c); }\n+    static inline void vse_T(float* a, vfloat32m4_t b, size_t c) { return __riscv_vse32(a, b, c); }\n+};\n+\n+template<typename T, typename WT> struct pyrDownVec0\n+{\n+    void operator()(const T* src, WT* row, const uint* tabM, int start, int end)\n+    {\n+        int vl;\n+        switch (start)\n+        {\n+        case 1:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 2, 2 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 - 1, 2 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2, 2 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 1, 2 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + x * 2 + 2, 2 * sizeof(T), vl), vl);\n+                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 2:\n+            for( int x = start / 2; x < end / 2; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 2 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 2, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 3:\n+            for( int x = start / 3; x < end / 3; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 3 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        case 4:\n+            for( int x = start / 4; x < end / 4; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end / 4 - x);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(T), vl), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                         __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+                vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(T), vl), vl);\n+                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(WT), __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                             __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+            break;\n+        default:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_tabM = rvv<T>::vle_M(tabM + x, vl);\n+                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(T), vl);\n+                auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src3 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(T), vl);\n+                auto vec_src4 = rvv<T>::vcvt_T_WT(rvv<T>::vloxei_T(src, vec_tabM, vl), vl);\n+                __riscv_vse32(row + x, __riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                    __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), vl);\n+            }\n+        }\n+    }\n+};\n+template<> struct pyrDownVec0<float, float>\n+{\n+    void operator()(const float* src, float* row, const uint* tabM, int start, int end)\n+    {\n+        int vl;\n+        switch (start)\n+        {\n+        case 1:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + x * 2 - 2, 2 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + x * 2 - 1, 2 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + x * 2, 2 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + x * 2 + 1, 2 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + x * 2 + 2, 2 * sizeof(float), vl);\n+                __riscv_vse32(row + x, __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 2:\n+            for( int x = start / 2; x < end / 2; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 2 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2, 4 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2, 4 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2, 4 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2, 4 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2, 4 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 2, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 2 + 1, 4 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 2 + 1, 4 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 2 + 1, 2 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 3:\n+            for( int x = start / 3; x < end / 3; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 3 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3, 6 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3, 6 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3, 6 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3, 6 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 1, 6 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 1, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3 + 1, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 3 + 2, 6 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 3 + 2, 6 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 3 + 2, 3 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        case 4:\n+            for( int x = start / 4; x < end / 4; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end / 4 - x);\n+                auto vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4, 8 * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4, 8 * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4, 8 * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4, 8 * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 1, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 1, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 1, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 2, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 2, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 2, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+                vec_src0 = rvv<float>::vlse_T(src + (x * 2 - 2) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src1 = rvv<float>::vlse_T(src + (x * 2 - 1) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src2 = rvv<float>::vlse_T(src + (x * 2) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src3 = rvv<float>::vlse_T(src + (x * 2 + 1) * 4 + 3, 8 * sizeof(float), vl);\n+                vec_src4 = rvv<float>::vlse_T(src + (x * 2 + 2) * 4 + 3, 8 * sizeof(float), vl);\n+                __riscv_vsse32(row + x * 4 + 3, 4 * sizeof(float), __riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), vl);\n+            }\n+            break;\n+        default:\n+            for( int x = start; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_tabM = rvv<float>::vle_M(tabM + x, vl);\n+                vec_tabM = __riscv_vmul(__riscv_vsub(vec_tabM, start * 2, vl), sizeof(float), vl);\n+                auto vec_src0 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src1 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src2 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src3 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                vec_tabM =  __riscv_vadd(vec_tabM, start * sizeof(float), vl);\n+                auto vec_src4 = rvv<float>::vloxei_T(src, vec_tabM, vl);\n+                __riscv_vse32(row + x, __riscv_vfmadd(__riscv_vfadd(__riscv_vfadd(vec_src1, vec_src2, vl), vec_src3, vl), 4,\n+                                                      __riscv_vfadd(__riscv_vfadd(vec_src0, vec_src4, vl), __riscv_vfadd(vec_src2, vec_src2, vl), vl), vl), vl);\n+            }\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrDownVec1\n+{\n+    void operator()(WT* row0, WT* row1, WT* row2, WT* row3, WT* row4, T* dst, int end)\n+    {\n+        int vl;\n+        for( int x = 0 ; x < end; x += vl )\n+        {\n+            vl = rvv<T>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+            auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+            auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+            auto vec_src3 = rvv<T>::vle_WT(row3 + x, vl);\n+            auto vec_src4 = rvv<T>::vle_WT(row4 + x, vl);\n+            rvv<T>::vse_T(dst + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(__riscv_vadd(vec_src0, vec_src4, vl), __riscv_vadd(vec_src2, vec_src2, vl), vl),\n+                                                                                      __riscv_vsll(__riscv_vadd(__riscv_vadd(vec_src1, vec_src2, vl), vec_src3, vl), 2, vl), vl), 8, vl), vl);\n+        }\n+    }\n+};\n+template<> struct pyrDownVec1<float, float>\n+{\n+    void operator()(float* row0, float* row1, float* row2, float* row3, float* row4, float* dst, int end)\n+    {\n+        int vl;\n+        for( int x = 0 ; x < end; x += vl )\n+        {\n+            vl = rvv<float>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+            auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+            auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+            auto vec_src3 = rvv<float>::vle_WT(row3 + x, vl);\n+            auto vec_src4 = rvv<float>::vle_WT(row4 + x, vl);\n+            rvv<float>::vse_T(dst + x, __riscv_vfmul(__riscv_vfmadd(vec_src2, 6, __riscv_vfmadd(__riscv_vfadd(vec_src1, vec_src3, vl), 4, __riscv_vfadd(vec_src0, vec_src4, vl), vl), vl), 1.f / 256.f, vl), vl);\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrUpVec0\n+{\n+    void operator()(const T* src, WT* row, const uint* dtab, int start, int end)\n+    {\n+        int vl;\n+        for( int x = start; x < end; x += vl )\n+        {\n+            vl = rvv<T>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x - start, vl), vl);\n+            auto vec_src1 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x, vl), vl);\n+            auto vec_src2 = rvv<T>::vcvt_T_WT(rvv<T>::vle_T(src + x + start, vl), vl);\n+\n+            auto vec_dtab = rvv<T>::vle_M(dtab + x, vl);\n+            vec_dtab = __riscv_vmul(vec_dtab, sizeof(WT), vl);\n+            __riscv_vsoxei32(row, vec_dtab, __riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), vl);\n+            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(WT), vl), __riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), vl);\n+        }\n+    }\n+};\n+template<> struct pyrUpVec0<float, float>\n+{\n+    void operator()(const float* src, float* row, const uint* dtab, int start, int end)\n+    {\n+        int vl;\n+        for( int x = start; x < end; x += vl )\n+        {\n+            vl = rvv<float>::vsetvl_WT(end - x);\n+            auto vec_src0 = rvv<float>::vle_T(src + x - start, vl);\n+            auto vec_src1 = rvv<float>::vle_T(src + x, vl);\n+            auto vec_src2 = rvv<float>::vle_T(src + x + start, vl);\n+\n+            auto vec_dtab = rvv<float>::vle_M(dtab + x, vl);\n+            vec_dtab = __riscv_vmul(vec_dtab, sizeof(float), vl);\n+            __riscv_vsoxei32(row, vec_dtab, __riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), vl);\n+            __riscv_vsoxei32(row, __riscv_vadd(vec_dtab, start * sizeof(float), vl), __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 4, vl), vl);\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT> struct pyrUpVec1\n+{\n+    void operator()(WT* row0, WT* row1, WT* row2, T* dst0, T* dst1, int end)\n+    {\n+        int vl;\n+        if (dst0 != dst1)\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n+                rvv<T>::vse_T(dst1 + x, rvv<T>::vcvt_WT_T(__riscv_vsll(__riscv_vadd(vec_src1, vec_src2, vl), 2, vl), 6, vl), vl);\n+            }\n+        }\n+        else\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<T>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<T>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<T>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<T>::vle_WT(row2 + x, vl);\n+                rvv<T>::vse_T(dst0 + x, rvv<T>::vcvt_WT_T(__riscv_vadd(__riscv_vadd(vec_src0, vec_src2, vl), __riscv_vadd(__riscv_vsll(vec_src1, 2, vl), __riscv_vsll(vec_src1, 1, vl), vl), vl), 6, vl), vl);\n+            }\n+        }\n+    }\n+};\n+template<> struct pyrUpVec1<float, float>\n+{\n+    void operator()(float* row0, float* row1, float* row2, float* dst0, float* dst1, int end)\n+    {\n+        int vl;\n+        if (dst0 != dst1)\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n+                rvv<float>::vse_T(dst1 + x, __riscv_vfmul(__riscv_vfadd(vec_src1, vec_src2, vl), 1.f / 16.f, vl), vl);\n+            }\n+        }\n+        else\n+        {\n+            for( int x = 0 ; x < end; x += vl )\n+            {\n+                vl = rvv<float>::vsetvl_WT(end - x);\n+                auto vec_src0 = rvv<float>::vle_WT(row0 + x, vl);\n+                auto vec_src1 = rvv<float>::vle_WT(row1 + x, vl);\n+                auto vec_src2 = rvv<float>::vle_WT(row2 + x, vl);\n+                rvv<float>::vse_T(dst0 + x, __riscv_vfmul(__riscv_vfadd(__riscv_vfmadd(vec_src1, 6, vec_src0, vl), vec_src2, vl), 1.f / 64.f, vl), vl);\n+            }\n+        }\n+    }\n+};\n+\n+template<typename T, typename WT>\n+struct PyrDownInvoker : ParallelLoopBody\n+{\n+    PyrDownInvoker(const uchar* _src_data, size_t _src_step, int _src_width, int _src_height, uchar* _dst_data, size_t _dst_step, int _dst_width, int _dst_height, int _cn, int _borderType, int* _tabR, int* _tabM, int* _tabL)\n+    {\n+        src_data = _src_data;\n+        src_step = _src_step;\n+        src_width = _src_width;\n+        src_height = _src_height;\n+        dst_data = _dst_data;\n+        dst_step = _dst_step;\n+        dst_width = _dst_width;\n+        dst_height = _dst_height;\n+        cn = _cn;\n+        borderType = _borderType;\n+        tabR = _tabR;\n+        tabM = _tabM;\n+        tabL = _tabL;\n+    }\n+\n+    void operator()(const Range& range) const CV_OVERRIDE;\n+\n+    const uchar* src_data;\n+    size_t src_step;\n+    int src_width;\n+    int src_height;\n+    uchar* dst_data;\n+    size_t dst_step;\n+    int dst_width;\n+    int dst_height;\n+    int cn;\n+    int borderType;\n+    int* tabR;\n+    int* tabM;\n+    int* tabL;\n+};\n+\n+// the algorithm is copied from imgproc/src/pyramids.cpp,\n+// in the function template void cv::pyrDown_\n+template<typename T, typename WT>\n+inline int pyrDown(const uchar* src_data, size_t src_step, int src_width, int src_height, uchar* dst_data, size_t dst_step, int dst_width, int dst_height, int cn, int borderType)\n+{\n+    const int PD_SZ = 5;\n+\n+    std::vector<int> _tabM(dst_width * cn), _tabL(cn * (PD_SZ + 2)), _tabR(cn * (PD_SZ + 2));\n+    int *tabM = _tabM.data(), *tabL = _tabL.data(), *tabR = _tabR.data();\n+\n+    CV_Assert( src_width > 0 && src_height > 0 &&\n+               std::abs(dst_width*2 - src_width) <= 2 &&\n+               std::abs(dst_height*2 - src_height) <= 2 );\n+    int width0 = std::min((src_width-PD_SZ/2-1)/2 + 1, dst_width);\n+\n+    for (int x = 0; x <= PD_SZ+1; x++)\n+    {\n+        int sx0 = borderInterpolate(x - PD_SZ/2, src_width, borderType)*cn;",
        "comment_created_at": "2025-03-01T03:11:06+00:00",
        "comment_author": "amane-ame",
        "comment_body": "Carotene(ARM HAL) copied this function from ::cv into its self namespace, I will follow this.",
        "pr_file_module": null
      }
    ]
  }
]
