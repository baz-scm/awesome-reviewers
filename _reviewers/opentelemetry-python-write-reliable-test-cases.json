[
  {
    "discussion_id": "1916862159",
    "pr_number": 4361,
    "pr_file": "opentelemetry-sdk/tests/metrics/test_aggregation.py",
    "created_at": "2025-01-15T15:30:59+00:00",
    "commented_code": ")\n         self.assertIsInstance(aggregation, _ExplicitBucketHistogramAggregation)\n \n+    def test_histogram_with_advisory(self):\n+        boundaries = [randint(0, 1000)]",
    "repo_full_name": "open-telemetry/opentelemetry-python",
    "discussion_comments": [
      {
        "comment_id": "1916862159",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 4361,
        "pr_file": "opentelemetry-sdk/tests/metrics/test_aggregation.py",
        "discussion_id": "1916862159",
        "commented_code": "@@ -628,6 +629,22 @@ def test_histogram(self):\n         )\n         self.assertIsInstance(aggregation, _ExplicitBucketHistogramAggregation)\n \n+    def test_histogram_with_advisory(self):\n+        boundaries = [randint(0, 1000)]",
        "comment_created_at": "2025-01-15T15:30:59+00:00",
        "comment_author": "aabmass",
        "comment_body": "nit can we just hardcode a value to keep the test reproducible",
        "pr_file_module": null
      },
      {
        "comment_id": "1916994798",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 4361,
        "pr_file": "opentelemetry-sdk/tests/metrics/test_aggregation.py",
        "discussion_id": "1916862159",
        "commented_code": "@@ -628,6 +629,22 @@ def test_histogram(self):\n         )\n         self.assertIsInstance(aggregation, _ExplicitBucketHistogramAggregation)\n \n+    def test_histogram_with_advisory(self):\n+        boundaries = [randint(0, 1000)]",
        "comment_created_at": "2025-01-15T16:37:35+00:00",
        "comment_author": "xrmx",
        "comment_body": "Hardcoded",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1622869544",
    "pr_number": 3938,
    "pr_file": "opentelemetry-sdk/tests/trace/export/test_export.py",
    "created_at": "2024-05-31T19:21:26+00:00",
    "commented_code": "self.assertTrue(export_event.wait(2))\n         export_time = time.time()\n         self.assertEqual(len(spans_names_list), 1)\n-        self.assertGreaterEqual((export_time - start_time) * 1e3, 500)\n+        # call round to avoid flaky tests on pypy with delays ending some microseconds earlier",
    "repo_full_name": "open-telemetry/opentelemetry-python",
    "discussion_comments": [
      {
        "comment_id": "1622869544",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 3938,
        "pr_file": "opentelemetry-sdk/tests/trace/export/test_export.py",
        "discussion_id": "1622869544",
        "commented_code": "@@ -482,7 +482,8 @@ def test_batch_span_processor_scheduled_delay(self):\n         self.assertTrue(export_event.wait(2))\n         export_time = time.time()\n         self.assertEqual(len(spans_names_list), 1)\n-        self.assertGreaterEqual((export_time - start_time) * 1e3, 500)\n+        # call round to avoid flaky tests on pypy with delays ending some microseconds earlier",
        "comment_created_at": "2024-05-31T19:21:26+00:00",
        "comment_author": "ocelotl",
        "comment_body": "If this is a `pypy`-specific problem, we should actually skip the test case for `pypy`, or mark it as flaky.",
        "pr_file_module": null
      },
      {
        "comment_id": "1622932755",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 3938,
        "pr_file": "opentelemetry-sdk/tests/trace/export/test_export.py",
        "discussion_id": "1622869544",
        "commented_code": "@@ -482,7 +482,8 @@ def test_batch_span_processor_scheduled_delay(self):\n         self.assertTrue(export_event.wait(2))\n         export_time = time.time()\n         self.assertEqual(len(spans_names_list), 1)\n-        self.assertGreaterEqual((export_time - start_time) * 1e3, 500)\n+        # call round to avoid flaky tests on pypy with delays ending some microseconds earlier",
        "comment_created_at": "2024-05-31T20:41:19+00:00",
        "comment_author": "xrmx",
        "comment_body": "It looks like the version of pypy we are using in ci has some timer precision issues on windows. This issues could have been fixed in 7.3.12 but that release has a baseline of python 3.9.",
        "pr_file_module": null
      },
      {
        "comment_id": "1623018388",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 3938,
        "pr_file": "opentelemetry-sdk/tests/trace/export/test_export.py",
        "discussion_id": "1622869544",
        "commented_code": "@@ -482,7 +482,8 @@ def test_batch_span_processor_scheduled_delay(self):\n         self.assertTrue(export_event.wait(2))\n         export_time = time.time()\n         self.assertEqual(len(spans_names_list), 1)\n-        self.assertGreaterEqual((export_time - start_time) * 1e3, 500)\n+        # call round to avoid flaky tests on pypy with delays ending some microseconds earlier",
        "comment_created_at": "2024-05-31T23:17:45+00:00",
        "comment_author": "ocelotl",
        "comment_body": "> It looks like the version of pypy we are using in ci has some timer precision issues on windows. This issues could have been fixed in 7.3.12 but that release has a baseline of python 3.9.\r\n\r\nEven more reason to skip it if it is being run with pypy in windows, then.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1621499696",
    "pr_number": 3937,
    "pr_file": "opentelemetry-sdk/tests/trace/export/test_export.py",
    "created_at": "2024-05-30T22:54:10+00:00",
    "commented_code": "# give some time for exporter to loop\n             # since wait is mocked it should return immediately\n-            time.sleep(0.05)",
    "repo_full_name": "open-telemetry/opentelemetry-python",
    "discussion_comments": [
      {
        "comment_id": "1621499696",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 3937,
        "pr_file": "opentelemetry-sdk/tests/trace/export/test_export.py",
        "discussion_id": "1621499696",
        "commented_code": "@@ -513,7 +513,7 @@ def test_batch_span_processor_reset_timeout(self):\n \n             # give some time for exporter to loop\n             # since wait is mocked it should return immediately\n-            time.sleep(0.05)",
        "comment_created_at": "2024-05-30T22:54:10+00:00",
        "comment_author": "ocelotl",
        "comment_body": "The test should be marked as [flaky](https://github.com/open-telemetry/opentelemetry-python/blob/main/opentelemetry-sdk/tests/metrics/test_periodic_exporting_metric_reader.py#L193) instead. That way, we actually show that this test is flaky instead of just having a larger sleep.",
        "pr_file_module": null
      },
      {
        "comment_id": "1621986153",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 3937,
        "pr_file": "opentelemetry-sdk/tests/trace/export/test_export.py",
        "discussion_id": "1621499696",
        "commented_code": "@@ -513,7 +513,7 @@ def test_batch_span_processor_reset_timeout(self):\n \n             # give some time for exporter to loop\n             # since wait is mocked it should return immediately\n-            time.sleep(0.05)",
        "comment_created_at": "2024-05-31T08:44:58+00:00",
        "comment_author": "xrmx",
        "comment_body": "In general I agree, but here we are adding the race condition on purpose by mocking the wait condition in order to do asserts on the mock calls. Before marking it as flaky I would like to test if adding more slack in timing is helpful",
        "pr_file_module": null
      },
      {
        "comment_id": "1622761214",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 3937,
        "pr_file": "opentelemetry-sdk/tests/trace/export/test_export.py",
        "discussion_id": "1621499696",
        "commented_code": "@@ -513,7 +513,7 @@ def test_batch_span_processor_reset_timeout(self):\n \n             # give some time for exporter to loop\n             # since wait is mocked it should return immediately\n-            time.sleep(0.05)",
        "comment_created_at": "2024-05-31T17:44:04+00:00",
        "comment_author": "ocelotl",
        "comment_body": "All right, if you prefer so :v:",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2047631117",
    "pr_number": 4535,
    "pr_file": "opentelemetry-sdk/tests/logs/test_export.py",
    "created_at": "2025-04-16T19:41:29+00:00",
    "commented_code": "max_export_batch_size=101,\n         )\n \n-    def test_shutdown(self):\n-        exporter = InMemoryLogExporter()\n-        log_record_processor = BatchLogRecordProcessor(exporter)\n-\n-        provider = LoggerProvider()\n-        provider.add_log_record_processor(log_record_processor)\n-\n-        logger = logging.getLogger(\"shutdown\")\n-        logger.addHandler(LoggingHandler(logger_provider=provider))\n-\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.warning(\"warning message: %s\", \"possible upcoming heatwave\")\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.error(\"Very high rise in temperatures across the globe\")\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.critical(\"Temperature hits high 420 C in Hyderabad\")\n+    def test_logs_exported_once_batch_size_reached(self):\n+        exporter = Mock()\n+        log_record_processor = BatchLogRecordProcessor(\n+            exporter=exporter,\n+            max_queue_size=15,\n+            max_export_batch_size=15,\n+            # Will not reach this during the test, this sleep should be interrupted when batch size is reached.\n+            schedule_delay_millis=30000,\n+        )\n+        before_export = time.time_ns()\n+        for _ in range(15):\n+            log_record_processor.emit(EMPTY_LOG)\n+        # Wait a bit for the worker thread to wake up and call export.\n+        time.sleep(0.1)\n+        exporter.export.assert_called_once()\n+        after_export = time.time_ns()\n+        # Shows the worker's 30 second sleep was interrupted within a second.\n+        self.assertTrue((after_export - before_export) < 1e9)",
    "repo_full_name": "open-telemetry/opentelemetry-python",
    "discussion_comments": [
      {
        "comment_id": "2047631117",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 4535,
        "pr_file": "opentelemetry-sdk/tests/logs/test_export.py",
        "discussion_id": "2047631117",
        "commented_code": "@@ -463,161 +468,129 @@ def test_validation_negative_max_queue_size(self):\n             max_export_batch_size=101,\n         )\n \n-    def test_shutdown(self):\n-        exporter = InMemoryLogExporter()\n-        log_record_processor = BatchLogRecordProcessor(exporter)\n-\n-        provider = LoggerProvider()\n-        provider.add_log_record_processor(log_record_processor)\n-\n-        logger = logging.getLogger(\"shutdown\")\n-        logger.addHandler(LoggingHandler(logger_provider=provider))\n-\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.warning(\"warning message: %s\", \"possible upcoming heatwave\")\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.error(\"Very high rise in temperatures across the globe\")\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.critical(\"Temperature hits high 420 C in Hyderabad\")\n+    def test_logs_exported_once_batch_size_reached(self):\n+        exporter = Mock()\n+        log_record_processor = BatchLogRecordProcessor(\n+            exporter=exporter,\n+            max_queue_size=15,\n+            max_export_batch_size=15,\n+            # Will not reach this during the test, this sleep should be interrupted when batch size is reached.\n+            schedule_delay_millis=30000,\n+        )\n+        before_export = time.time_ns()\n+        for _ in range(15):\n+            log_record_processor.emit(EMPTY_LOG)\n+        # Wait a bit for the worker thread to wake up and call export.\n+        time.sleep(0.1)\n+        exporter.export.assert_called_once()\n+        after_export = time.time_ns()\n+        # Shows the worker's 30 second sleep was interrupted within a second.\n+        self.assertTrue((after_export - before_export) < 1e9)",
        "comment_created_at": "2025-04-16T19:41:29+00:00",
        "comment_author": "aabmass",
        "comment_body": "nit Use `assertLess()` or consider `assertAlmostEqual()` with a delta.",
        "pr_file_module": null
      },
      {
        "comment_id": "2050774673",
        "repo_full_name": "open-telemetry/opentelemetry-python",
        "pr_number": 4535,
        "pr_file": "opentelemetry-sdk/tests/logs/test_export.py",
        "discussion_id": "2047631117",
        "commented_code": "@@ -463,161 +468,129 @@ def test_validation_negative_max_queue_size(self):\n             max_export_batch_size=101,\n         )\n \n-    def test_shutdown(self):\n-        exporter = InMemoryLogExporter()\n-        log_record_processor = BatchLogRecordProcessor(exporter)\n-\n-        provider = LoggerProvider()\n-        provider.add_log_record_processor(log_record_processor)\n-\n-        logger = logging.getLogger(\"shutdown\")\n-        logger.addHandler(LoggingHandler(logger_provider=provider))\n-\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.warning(\"warning message: %s\", \"possible upcoming heatwave\")\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.error(\"Very high rise in temperatures across the globe\")\n-        with self.assertLogs(level=logging.WARNING):\n-            logger.critical(\"Temperature hits high 420 C in Hyderabad\")\n+    def test_logs_exported_once_batch_size_reached(self):\n+        exporter = Mock()\n+        log_record_processor = BatchLogRecordProcessor(\n+            exporter=exporter,\n+            max_queue_size=15,\n+            max_export_batch_size=15,\n+            # Will not reach this during the test, this sleep should be interrupted when batch size is reached.\n+            schedule_delay_millis=30000,\n+        )\n+        before_export = time.time_ns()\n+        for _ in range(15):\n+            log_record_processor.emit(EMPTY_LOG)\n+        # Wait a bit for the worker thread to wake up and call export.\n+        time.sleep(0.1)\n+        exporter.export.assert_called_once()\n+        after_export = time.time_ns()\n+        # Shows the worker's 30 second sleep was interrupted within a second.\n+        self.assertTrue((after_export - before_export) < 1e9)",
        "comment_created_at": "2025-04-18T15:29:53+00:00",
        "comment_author": "DylanRussell",
        "comment_body": "Ack, switched to assertLess",
        "pr_file_module": null
      }
    ]
  }
]