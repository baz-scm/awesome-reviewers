[
  {
    "discussion_id": "2315318678",
    "pr_number": 529,
    "pr_file": "src/parlant/adapters/nlp/snowflake_cortex_service.py",
    "created_at": "2025-09-02T08:25:35+00:00",
    "commented_code": "+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Maintainer: Tao Tang <ttan@habitus.dk>\n+\n+from __future__ import annotations\n+\n+import os\n+import time\n+import json\n+from typing import Any, Mapping, Optional, Type, cast\n+\n+import httpx\n+import tiktoken\n+from typing_extensions import override\n+from pydantic import ValidationError\n+\n+from parlant.adapters.nlp.common import normalize_json_output\n+from parlant.core.engines.alpha.prompt_builder import PromptBuilder\n+from parlant.core.loggers import Logger\n+from parlant.core.nlp.policies import policy, retry\n+from parlant.core.nlp.tokenization import EstimatingTokenizer\n+from parlant.core.nlp.service import NLPService\n+from parlant.core.nlp.embedding import Embedder, EmbeddingResult\n+from parlant.core.nlp.generation import T, SchematicGenerator, SchematicGenerationResult\n+from parlant.core.nlp.generation_info import GenerationInfo, UsageInfo\n+from parlant.core.nlp.moderation import ModerationService, NoModeration\n+\n+HTTPX_TIMEOUT = httpx.Timeout(timeout=60.0, connect=5.0, read=60.0, write=60.0)\n+\n+\n+class CortexEstimatingTokenizer(EstimatingTokenizer):\n+    \"\"\"\n+    Token estimator. Cortex doesn't expose a tokenizer; use tiktoken heuristics.\n+    Default to cl100k_base if the specific model encoding is unknown.\n+    \"\"\"\n+\n+    def __init__(self, model_name: Optional[str] = None) -> None:\n+        self.model_name = model_name or \"cl100k_base\"\n+        try:\n+            self.encoding = tiktoken.encoding_for_model(self.model_name)\n+        except Exception:\n+            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n+\n+    @override\n+    async def estimate_token_count(self, prompt: str) -> int:\n+        \"\"\"Estimate token count for a prompt.\n+\n+        Args:",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "2315318678",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 529,
        "pr_file": "src/parlant/adapters/nlp/snowflake_cortex_service.py",
        "discussion_id": "2315318678",
        "commented_code": "@@ -0,0 +1,434 @@\n+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Maintainer: Tao Tang <ttan@habitus.dk>\n+\n+from __future__ import annotations\n+\n+import os\n+import time\n+import json\n+from typing import Any, Mapping, Optional, Type, cast\n+\n+import httpx\n+import tiktoken\n+from typing_extensions import override\n+from pydantic import ValidationError\n+\n+from parlant.adapters.nlp.common import normalize_json_output\n+from parlant.core.engines.alpha.prompt_builder import PromptBuilder\n+from parlant.core.loggers import Logger\n+from parlant.core.nlp.policies import policy, retry\n+from parlant.core.nlp.tokenization import EstimatingTokenizer\n+from parlant.core.nlp.service import NLPService\n+from parlant.core.nlp.embedding import Embedder, EmbeddingResult\n+from parlant.core.nlp.generation import T, SchematicGenerator, SchematicGenerationResult\n+from parlant.core.nlp.generation_info import GenerationInfo, UsageInfo\n+from parlant.core.nlp.moderation import ModerationService, NoModeration\n+\n+HTTPX_TIMEOUT = httpx.Timeout(timeout=60.0, connect=5.0, read=60.0, write=60.0)\n+\n+\n+class CortexEstimatingTokenizer(EstimatingTokenizer):\n+    \"\"\"\n+    Token estimator. Cortex doesn't expose a tokenizer; use tiktoken heuristics.\n+    Default to cl100k_base if the specific model encoding is unknown.\n+    \"\"\"\n+\n+    def __init__(self, model_name: Optional[str] = None) -> None:\n+        self.model_name = model_name or \"cl100k_base\"\n+        try:\n+            self.encoding = tiktoken.encoding_for_model(self.model_name)\n+        except Exception:\n+            self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n+\n+    @override\n+    async def estimate_token_count(self, prompt: str) -> int:\n+        \"\"\"Estimate token count for a prompt.\n+\n+        Args:",
        "comment_created_at": "2025-09-02T08:25:35+00:00",
        "comment_author": "mc-dorzo",
        "comment_body": "Some of these docstrings are redundant. Please remove class and function comments that don\u2019t add value beyond what the code already makes obvious. Keep documentation only where it clarifies behavior that isn\u2019t trivial to understand from the code itself.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2160045782",
    "pr_number": 428,
    "pr_file": "src/parlant/core/capabilities.py",
    "created_at": "2025-06-21T13:59:59+00:00",
    "commented_code": "+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from abc import abstractmethod\n+from dataclasses import dataclass\n+from datetime import datetime, timezone\n+import heapq\n+from itertools import chain\n+import json\n+import time\n+from typing import Awaitable, Callable, NewType, Optional, Sequence, TypedDict, cast\n+from typing_extensions import override, Self, Required\n+\n+from parlant.core import async_utils\n+from parlant.core.async_utils import ReaderWriterLock\n+from parlant.core.common import ItemNotFoundError, Version, generate_id, UniqueId, md5_checksum\n+from parlant.core.persistence.common import ObjectId, Where\n+from parlant.core.nlp.embedding import Embedder, EmbedderFactory\n+from parlant.core.persistence.vector_database import (\n+    BaseDocument as VectorBaseDocument,\n+    SimilarDocumentResult,\n+    VectorCollection,\n+    VectorDatabase,\n+)\n+from parlant.core.persistence.vector_database_helper import (\n+    VectorDocumentStoreMigrationHelper,\n+)\n+from parlant.core.persistence.document_database import (\n+    DocumentCollection,\n+    DocumentDatabase,\n+    BaseDocument,\n+)\n+from parlant.core.persistence.document_database_helper import DocumentStoreMigrationHelper\n+from parlant.core.tags import TagId\n+\n+\n+CapabilityId = NewType(\"CapabilityId\", str)\n+\n+\n+@dataclass(frozen=True)\n+class Capability:\n+    id: CapabilityId\n+    creation_utc: datetime\n+    title: str\n+    description: str\n+    queries: Sequence[str]\n+    tags: list[TagId]\n+\n+    def __hash__(self) -> int:\n+        return hash(self.id)\n+\n+\n+class CapabilityUpdateParams(TypedDict, total=False):\n+    title: str\n+    description: str\n+    queries: Sequence[str]\n+    tags: Sequence[TagId]\n+\n+\n+class CapabilityStore:\n+    @abstractmethod\n+    async def create_capability(\n+        self,\n+        title: str,\n+        description: str,\n+        creation_utc: Optional[datetime] = None,\n+        queries: Optional[Sequence[str]] = None,\n+        tags: Optional[Sequence[TagId]] = None,\n+    ) -> Capability: ...\n+\n+    @abstractmethod\n+    async def update_capability(\n+        self,\n+        capability_id: CapabilityId,\n+        params: CapabilityUpdateParams,\n+    ) -> Capability: ...\n+\n+    @abstractmethod\n+    async def read_capability(\n+        self,\n+        capability_id: CapabilityId,\n+    ) -> Capability: ...\n+\n+    @abstractmethod\n+    async def list_capabilities(\n+        self,\n+        tags: Optional[Sequence[TagId]] = None,\n+    ) -> Sequence[Capability]: ...\n+\n+    @abstractmethod\n+    async def delete_capability(\n+        self,\n+        capability_id: CapabilityId,\n+    ) -> None: ...\n+\n+    @abstractmethod\n+    async def find_relevant_capabilities(\n+        self,\n+        query: str,\n+        available_capabilities: Sequence[Capability],\n+        max_capabilities: int = 3,\n+    ) -> Sequence[Capability]: ...\n+\n+    @abstractmethod\n+    async def upsert_tag(\n+        self,\n+        capability_id: CapabilityId,\n+        tag_id: TagId,\n+        creation_utc: Optional[datetime] = None,\n+    ) -> bool: ...\n+\n+    @abstractmethod\n+    async def remove_tag(\n+        self,\n+        capability_id: CapabilityId,\n+        tag_id: TagId,\n+    ) -> None: ...\n+\n+\n+class _CapabilityDocument(TypedDict, total=False):\n+    id: ObjectId\n+    capability_id: ObjectId\n+    version: Version.String\n+    creation_utc: str\n+    content: str\n+    checksum: Required[str]\n+    title: str\n+    description: str\n+    queries: str\n+\n+\n+class _CapabilityTagAssociationDocument(TypedDict, total=False):\n+    id: ObjectId\n+    version: Version.String\n+    creation_utc: str\n+    capability_id: CapabilityId\n+    tag_id: TagId\n+\n+\n+class CapabilityVectorStore(CapabilityStore):\n+    VERSION = Version.from_string(\"0.1.0\")\n+\n+    def __init__(\n+        self,\n+        vector_db: VectorDatabase,\n+        document_db: DocumentDatabase,\n+        embedder_type_provider: Callable[[], Awaitable[type[Embedder]]],\n+        embedder_factory: EmbedderFactory,\n+        allow_migration: bool = True,\n+    ):\n+        self._vector_db = vector_db\n+        self._document_db = document_db\n+        self._allow_migration = allow_migration\n+        self._embedder_factory = embedder_factory\n+        self._embedder_type_provider = embedder_type_provider\n+        self._lock = ReaderWriterLock()\n+        self._collection: VectorCollection[_CapabilityDocument]\n+        self._tag_association_collection: DocumentCollection[_CapabilityTagAssociationDocument]\n+        self._embedder: Embedder\n+\n+    async def _document_loader(self, doc: VectorBaseDocument) -> Optional[_CapabilityDocument]:\n+        if doc[\"version\"] == self.VERSION.to_string():\n+            return cast(_CapabilityDocument, doc)\n+        return None\n+\n+    async def _association_document_loader(\n+        self, doc: BaseDocument\n+    ) -> Optional[_CapabilityTagAssociationDocument]:\n+        if doc[\"version\"] == self.VERSION.to_string():\n+            return cast(_CapabilityTagAssociationDocument, doc)\n+        return None\n+\n+    async def __aenter__(self) -> Self:\n+        embedder_type = await self._embedder_type_provider()\n+        self._embedder = self._embedder_factory.create_embedder(embedder_type)\n+\n+        async with VectorDocumentStoreMigrationHelper(\n+            store=self,\n+            database=self._vector_db,\n+            allow_migration=self._allow_migration,\n+        ):\n+            self._collection = await self._vector_db.get_or_create_collection(\n+                name=\"capabilities\",\n+                schema=_CapabilityDocument,\n+                embedder_type=embedder_type,\n+                document_loader=self._document_loader,\n+            )\n+\n+        async with DocumentStoreMigrationHelper(\n+            store=self,\n+            database=self._document_db,\n+            allow_migration=self._allow_migration,\n+        ):\n+            self._tag_association_collection = await self._document_db.get_or_create_collection(\n+                name=\"capability_tags\",\n+                schema=_CapabilityTagAssociationDocument,\n+                document_loader=self._association_document_loader,\n+            )\n+\n+        return self\n+\n+    async def __aexit__(\n+        self,\n+        exc_type: Optional[type[BaseException]],\n+        exc_value: Optional[BaseException],\n+        traceback: Optional[object],\n+    ) -> None:\n+        pass\n+\n+    @staticmethod\n+    def assemble_content(title: str, description: str, queries: Sequence[str]) -> str:\n+        content = f\"{title}: {description}\"\n+        if queries:\n+            content += \"\nQueries: \" + \"; \".join(queries)\n+        return content\n+\n+    def _serialize(\n+        self,\n+        capability: Capability,\n+        content: str,\n+    ) -> _CapabilityDocument:\n+        queries_json = json.dumps(list(capability.queries))\n+\n+        return _CapabilityDocument(\n+            id=ObjectId(generate_id()),\n+            capability_id=ObjectId(capability.id),\n+            version=self.VERSION.to_string(),\n+            creation_utc=capability.creation_utc.isoformat(),\n+            title=capability.title,\n+            description=capability.description,\n+            queries=queries_json,\n+            content=content,\n+            checksum=md5_checksum(content),\n+        )\n+\n+    async def _deserialize(self, doc: _CapabilityDocument) -> Capability:\n+        tags = [\n+            d[\"tag_id\"]\n+            for d in await self._tag_association_collection.find(\n+                {\"capability_id\": {\"$eq\": doc[\"capability_id\"]}}\n+            )\n+        ]\n+\n+        return Capability(\n+            id=CapabilityId(doc[\"capability_id\"]),\n+            creation_utc=datetime.fromisoformat(doc[\"creation_utc\"]),\n+            title=doc[\"title\"],\n+            description=doc[\"description\"],\n+            queries=json.loads(doc[\"queries\"]),\n+            tags=tags,\n+        )\n+\n+    def _list_capability_contents(self, capability: Capability) -> list[str]:\n+        return [f\"{capability.title}: {capability.description}\"] + list(capability.queries)\n+\n+    async def _insert_capability(self, capability: Capability) -> _CapabilityDocument:\n+        insertion_tasks = []\n+\n+        for content in self._list_capability_contents(capability):\n+            doc = self._serialize(capability, content)\n+            insertion_tasks.append(self._collection.insert_one(document=doc))\n+\n+        await async_utils.safe_gather(*insertion_tasks)\n+\n+        return doc\n+\n+    @override\n+    async def create_capability(\n+        self,\n+        title: str,\n+        description: str,\n+        creation_utc: Optional[datetime] = None,\n+        queries: Optional[Sequence[str]] = None,\n+        tags: Optional[Sequence[TagId]] = None,\n+    ) -> Capability:\n+        t_start = time.time()\n+\n+        async with self._lock.writer_lock:\n+            creation_utc = creation_utc or datetime.now(timezone.utc)\n+\n+            queries = list(queries) if queries else []\n+            tags = list(tags) if tags else []\n+\n+            capability_id = CapabilityId(generate_id())\n+            capability = Capability(\n+                id=capability_id,\n+                creation_utc=creation_utc,\n+                title=title,\n+                description=description,\n+                queries=queries,\n+                tags=tags,\n+            )\n+\n+            await self._insert_capability(capability)\n+\n+            for tag in tags:\n+                await self._tag_association_collection.insert_one(\n+                    document={\n+                        \"id\": ObjectId(generate_id()),\n+                        \"version\": self.VERSION.to_string(),\n+                        \"creation_utc\": creation_utc.isoformat(),\n+                        \"capability_id\": capability.id,\n+                        \"tag_id\": tag,\n+                    }\n+                )\n+\n+        t_end = time.time()\n+\n+        print(f\"Insert time: {t_end - t_start}\")\n+\n+        return capability\n+\n+    @override\n+    async def update_capability(\n+        self,\n+        capability_id: CapabilityId,\n+        params: CapabilityUpdateParams,\n+    ) -> Capability:\n+        async with self._lock.writer_lock:\n+            all_docs = await self._collection.find(\n+                filters={\"capability_id\": {\"$eq\": capability_id}}\n+            )\n+\n+            if not all_docs:\n+                raise ItemNotFoundError(item_id=UniqueId(capability_id))\n+\n+            for doc in all_docs:\n+                await self._collection.delete_one(filters={\"id\": {\"$eq\": doc[\"id\"]}})\n+\n+            title = params.get(\"title\", doc[\"title\"])\n+            description = params.get(\"description\", doc[\"description\"])\n+            queries = params.get(\"queries\", json.loads(doc[\"queries\"]))\n+            queries = list(queries)\n+\n+            capability = Capability(\n+                id=capability_id,\n+                creation_utc=datetime.fromisoformat(all_docs[0][\"creation_utc\"]),\n+                title=title,\n+                description=description,\n+                queries=queries,\n+                tags=[],\n+            )\n+\n+            doc = await self._insert_capability(capability)\n+\n+        return await self._deserialize(doc)\n+\n+    @override\n+    async def read_capability(\n+        self,\n+        capability_id: CapabilityId,\n+    ) -> Capability:\n+        async with self._lock.reader_lock:\n+            doc = await self._collection.find_one(filters={\"capability_id\": {\"$eq\": capability_id}})\n+\n+        if not doc:\n+            raise ItemNotFoundError(item_id=UniqueId(capability_id))\n+\n+        return await self._deserialize(doc)\n+\n+    @override\n+    async def list_capabilities(\n+        self,\n+        tags: Optional[Sequence[TagId]] = None,\n+    ) -> Sequence[Capability]:\n+        filters: Where = {}\n+        async with self._lock.reader_lock:\n+            if tags is not None:\n+                if len(tags) == 0:\n+                    capability_ids = {\n+                        doc[\"capability_id\"]\n+                        for doc in await self._tag_association_collection.find(filters={})\n+                    }\n+\n+                    if not capability_ids:\n+                        filters = {}\n+\n+                    elif len(capability_ids) == 1:\n+                        filters = {\"capability_id\": {\"$ne\": capability_ids.pop()}}\n+\n+                    else:\n+                        filters = {\n+                            \"$and\": [{\"capability_id\": {\"$ne\": id}} for id in capability_ids]\n+                        }\n+\n+                else:\n+                    tag_filters: Where = {\"$or\": [{\"tag_id\": {\"$eq\": tag}} for tag in tags]}\n+                    tag_associations = await self._tag_association_collection.find(\n+                        filters=tag_filters\n+                    )\n+\n+                    capability_ids = {assoc[\"capability_id\"] for assoc in tag_associations}\n+                    if not capability_ids:\n+                        return []\n+\n+                    if len(capability_ids) == 1:\n+                        filters = {\"capability_id\": {\"$eq\": capability_ids.pop()}}\n+\n+                    else:\n+                        filters = {\"$or\": [{\"capability_id\": {\"$eq\": id}} for id in capability_ids]}\n+\n+            docs = {}\n+            for d in await self._collection.find(filters=filters):\n+                if d[\"capability_id\"] not in docs:\n+                    docs[d[\"capability_id\"]] = d\n+\n+            return [await self._deserialize(d) for d in docs.values()]\n+\n+    @override\n+    async def delete_capability(\n+        self,\n+        capability_id: CapabilityId,\n+    ) -> None:\n+        async with self._lock.writer_lock:\n+            docs = await self._collection.find(filters={\"capability_id\": {\"$eq\": capability_id}})\n+\n+            tag_associations = await self._tag_association_collection.find(\n+                filters={\"capability_id\": {\"$eq\": capability_id}}\n+            )\n+\n+            if not docs:\n+                raise ItemNotFoundError(item_id=UniqueId(capability_id))\n+\n+            for doc in docs:\n+                await self._collection.delete_one(filters={\"id\": {\"$eq\": doc[\"id\"]}})\n+\n+            for tag_assoc in tag_associations:\n+                await self._tag_association_collection.delete_one(\n+                    filters={\"id\": {\"$eq\": tag_assoc[\"id\"]}}\n+                )\n+\n+    @override\n+    async def find_relevant_capabilities(\n+        self,\n+        query: str,\n+        available_capabilities: Sequence[Capability],\n+        max_capabilities: int = 3,\n+    ) -> Sequence[Capability]:\n+        if not available_capabilities:\n+            return []\n+\n+        n_result = sum(",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "2160045782",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 428,
        "pr_file": "src/parlant/core/capabilities.py",
        "discussion_id": "2160045782",
        "commented_code": "@@ -0,0 +1,538 @@\n+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from abc import abstractmethod\n+from dataclasses import dataclass\n+from datetime import datetime, timezone\n+import heapq\n+from itertools import chain\n+import json\n+import time\n+from typing import Awaitable, Callable, NewType, Optional, Sequence, TypedDict, cast\n+from typing_extensions import override, Self, Required\n+\n+from parlant.core import async_utils\n+from parlant.core.async_utils import ReaderWriterLock\n+from parlant.core.common import ItemNotFoundError, Version, generate_id, UniqueId, md5_checksum\n+from parlant.core.persistence.common import ObjectId, Where\n+from parlant.core.nlp.embedding import Embedder, EmbedderFactory\n+from parlant.core.persistence.vector_database import (\n+    BaseDocument as VectorBaseDocument,\n+    SimilarDocumentResult,\n+    VectorCollection,\n+    VectorDatabase,\n+)\n+from parlant.core.persistence.vector_database_helper import (\n+    VectorDocumentStoreMigrationHelper,\n+)\n+from parlant.core.persistence.document_database import (\n+    DocumentCollection,\n+    DocumentDatabase,\n+    BaseDocument,\n+)\n+from parlant.core.persistence.document_database_helper import DocumentStoreMigrationHelper\n+from parlant.core.tags import TagId\n+\n+\n+CapabilityId = NewType(\"CapabilityId\", str)\n+\n+\n+@dataclass(frozen=True)\n+class Capability:\n+    id: CapabilityId\n+    creation_utc: datetime\n+    title: str\n+    description: str\n+    queries: Sequence[str]\n+    tags: list[TagId]\n+\n+    def __hash__(self) -> int:\n+        return hash(self.id)\n+\n+\n+class CapabilityUpdateParams(TypedDict, total=False):\n+    title: str\n+    description: str\n+    queries: Sequence[str]\n+    tags: Sequence[TagId]\n+\n+\n+class CapabilityStore:\n+    @abstractmethod\n+    async def create_capability(\n+        self,\n+        title: str,\n+        description: str,\n+        creation_utc: Optional[datetime] = None,\n+        queries: Optional[Sequence[str]] = None,\n+        tags: Optional[Sequence[TagId]] = None,\n+    ) -> Capability: ...\n+\n+    @abstractmethod\n+    async def update_capability(\n+        self,\n+        capability_id: CapabilityId,\n+        params: CapabilityUpdateParams,\n+    ) -> Capability: ...\n+\n+    @abstractmethod\n+    async def read_capability(\n+        self,\n+        capability_id: CapabilityId,\n+    ) -> Capability: ...\n+\n+    @abstractmethod\n+    async def list_capabilities(\n+        self,\n+        tags: Optional[Sequence[TagId]] = None,\n+    ) -> Sequence[Capability]: ...\n+\n+    @abstractmethod\n+    async def delete_capability(\n+        self,\n+        capability_id: CapabilityId,\n+    ) -> None: ...\n+\n+    @abstractmethod\n+    async def find_relevant_capabilities(\n+        self,\n+        query: str,\n+        available_capabilities: Sequence[Capability],\n+        max_capabilities: int = 3,\n+    ) -> Sequence[Capability]: ...\n+\n+    @abstractmethod\n+    async def upsert_tag(\n+        self,\n+        capability_id: CapabilityId,\n+        tag_id: TagId,\n+        creation_utc: Optional[datetime] = None,\n+    ) -> bool: ...\n+\n+    @abstractmethod\n+    async def remove_tag(\n+        self,\n+        capability_id: CapabilityId,\n+        tag_id: TagId,\n+    ) -> None: ...\n+\n+\n+class _CapabilityDocument(TypedDict, total=False):\n+    id: ObjectId\n+    capability_id: ObjectId\n+    version: Version.String\n+    creation_utc: str\n+    content: str\n+    checksum: Required[str]\n+    title: str\n+    description: str\n+    queries: str\n+\n+\n+class _CapabilityTagAssociationDocument(TypedDict, total=False):\n+    id: ObjectId\n+    version: Version.String\n+    creation_utc: str\n+    capability_id: CapabilityId\n+    tag_id: TagId\n+\n+\n+class CapabilityVectorStore(CapabilityStore):\n+    VERSION = Version.from_string(\"0.1.0\")\n+\n+    def __init__(\n+        self,\n+        vector_db: VectorDatabase,\n+        document_db: DocumentDatabase,\n+        embedder_type_provider: Callable[[], Awaitable[type[Embedder]]],\n+        embedder_factory: EmbedderFactory,\n+        allow_migration: bool = True,\n+    ):\n+        self._vector_db = vector_db\n+        self._document_db = document_db\n+        self._allow_migration = allow_migration\n+        self._embedder_factory = embedder_factory\n+        self._embedder_type_provider = embedder_type_provider\n+        self._lock = ReaderWriterLock()\n+        self._collection: VectorCollection[_CapabilityDocument]\n+        self._tag_association_collection: DocumentCollection[_CapabilityTagAssociationDocument]\n+        self._embedder: Embedder\n+\n+    async def _document_loader(self, doc: VectorBaseDocument) -> Optional[_CapabilityDocument]:\n+        if doc[\"version\"] == self.VERSION.to_string():\n+            return cast(_CapabilityDocument, doc)\n+        return None\n+\n+    async def _association_document_loader(\n+        self, doc: BaseDocument\n+    ) -> Optional[_CapabilityTagAssociationDocument]:\n+        if doc[\"version\"] == self.VERSION.to_string():\n+            return cast(_CapabilityTagAssociationDocument, doc)\n+        return None\n+\n+    async def __aenter__(self) -> Self:\n+        embedder_type = await self._embedder_type_provider()\n+        self._embedder = self._embedder_factory.create_embedder(embedder_type)\n+\n+        async with VectorDocumentStoreMigrationHelper(\n+            store=self,\n+            database=self._vector_db,\n+            allow_migration=self._allow_migration,\n+        ):\n+            self._collection = await self._vector_db.get_or_create_collection(\n+                name=\"capabilities\",\n+                schema=_CapabilityDocument,\n+                embedder_type=embedder_type,\n+                document_loader=self._document_loader,\n+            )\n+\n+        async with DocumentStoreMigrationHelper(\n+            store=self,\n+            database=self._document_db,\n+            allow_migration=self._allow_migration,\n+        ):\n+            self._tag_association_collection = await self._document_db.get_or_create_collection(\n+                name=\"capability_tags\",\n+                schema=_CapabilityTagAssociationDocument,\n+                document_loader=self._association_document_loader,\n+            )\n+\n+        return self\n+\n+    async def __aexit__(\n+        self,\n+        exc_type: Optional[type[BaseException]],\n+        exc_value: Optional[BaseException],\n+        traceback: Optional[object],\n+    ) -> None:\n+        pass\n+\n+    @staticmethod\n+    def assemble_content(title: str, description: str, queries: Sequence[str]) -> str:\n+        content = f\"{title}: {description}\"\n+        if queries:\n+            content += \"\\nQueries: \" + \"; \".join(queries)\n+        return content\n+\n+    def _serialize(\n+        self,\n+        capability: Capability,\n+        content: str,\n+    ) -> _CapabilityDocument:\n+        queries_json = json.dumps(list(capability.queries))\n+\n+        return _CapabilityDocument(\n+            id=ObjectId(generate_id()),\n+            capability_id=ObjectId(capability.id),\n+            version=self.VERSION.to_string(),\n+            creation_utc=capability.creation_utc.isoformat(),\n+            title=capability.title,\n+            description=capability.description,\n+            queries=queries_json,\n+            content=content,\n+            checksum=md5_checksum(content),\n+        )\n+\n+    async def _deserialize(self, doc: _CapabilityDocument) -> Capability:\n+        tags = [\n+            d[\"tag_id\"]\n+            for d in await self._tag_association_collection.find(\n+                {\"capability_id\": {\"$eq\": doc[\"capability_id\"]}}\n+            )\n+        ]\n+\n+        return Capability(\n+            id=CapabilityId(doc[\"capability_id\"]),\n+            creation_utc=datetime.fromisoformat(doc[\"creation_utc\"]),\n+            title=doc[\"title\"],\n+            description=doc[\"description\"],\n+            queries=json.loads(doc[\"queries\"]),\n+            tags=tags,\n+        )\n+\n+    def _list_capability_contents(self, capability: Capability) -> list[str]:\n+        return [f\"{capability.title}: {capability.description}\"] + list(capability.queries)\n+\n+    async def _insert_capability(self, capability: Capability) -> _CapabilityDocument:\n+        insertion_tasks = []\n+\n+        for content in self._list_capability_contents(capability):\n+            doc = self._serialize(capability, content)\n+            insertion_tasks.append(self._collection.insert_one(document=doc))\n+\n+        await async_utils.safe_gather(*insertion_tasks)\n+\n+        return doc\n+\n+    @override\n+    async def create_capability(\n+        self,\n+        title: str,\n+        description: str,\n+        creation_utc: Optional[datetime] = None,\n+        queries: Optional[Sequence[str]] = None,\n+        tags: Optional[Sequence[TagId]] = None,\n+    ) -> Capability:\n+        t_start = time.time()\n+\n+        async with self._lock.writer_lock:\n+            creation_utc = creation_utc or datetime.now(timezone.utc)\n+\n+            queries = list(queries) if queries else []\n+            tags = list(tags) if tags else []\n+\n+            capability_id = CapabilityId(generate_id())\n+            capability = Capability(\n+                id=capability_id,\n+                creation_utc=creation_utc,\n+                title=title,\n+                description=description,\n+                queries=queries,\n+                tags=tags,\n+            )\n+\n+            await self._insert_capability(capability)\n+\n+            for tag in tags:\n+                await self._tag_association_collection.insert_one(\n+                    document={\n+                        \"id\": ObjectId(generate_id()),\n+                        \"version\": self.VERSION.to_string(),\n+                        \"creation_utc\": creation_utc.isoformat(),\n+                        \"capability_id\": capability.id,\n+                        \"tag_id\": tag,\n+                    }\n+                )\n+\n+        t_end = time.time()\n+\n+        print(f\"Insert time: {t_end - t_start}\")\n+\n+        return capability\n+\n+    @override\n+    async def update_capability(\n+        self,\n+        capability_id: CapabilityId,\n+        params: CapabilityUpdateParams,\n+    ) -> Capability:\n+        async with self._lock.writer_lock:\n+            all_docs = await self._collection.find(\n+                filters={\"capability_id\": {\"$eq\": capability_id}}\n+            )\n+\n+            if not all_docs:\n+                raise ItemNotFoundError(item_id=UniqueId(capability_id))\n+\n+            for doc in all_docs:\n+                await self._collection.delete_one(filters={\"id\": {\"$eq\": doc[\"id\"]}})\n+\n+            title = params.get(\"title\", doc[\"title\"])\n+            description = params.get(\"description\", doc[\"description\"])\n+            queries = params.get(\"queries\", json.loads(doc[\"queries\"]))\n+            queries = list(queries)\n+\n+            capability = Capability(\n+                id=capability_id,\n+                creation_utc=datetime.fromisoformat(all_docs[0][\"creation_utc\"]),\n+                title=title,\n+                description=description,\n+                queries=queries,\n+                tags=[],\n+            )\n+\n+            doc = await self._insert_capability(capability)\n+\n+        return await self._deserialize(doc)\n+\n+    @override\n+    async def read_capability(\n+        self,\n+        capability_id: CapabilityId,\n+    ) -> Capability:\n+        async with self._lock.reader_lock:\n+            doc = await self._collection.find_one(filters={\"capability_id\": {\"$eq\": capability_id}})\n+\n+        if not doc:\n+            raise ItemNotFoundError(item_id=UniqueId(capability_id))\n+\n+        return await self._deserialize(doc)\n+\n+    @override\n+    async def list_capabilities(\n+        self,\n+        tags: Optional[Sequence[TagId]] = None,\n+    ) -> Sequence[Capability]:\n+        filters: Where = {}\n+        async with self._lock.reader_lock:\n+            if tags is not None:\n+                if len(tags) == 0:\n+                    capability_ids = {\n+                        doc[\"capability_id\"]\n+                        for doc in await self._tag_association_collection.find(filters={})\n+                    }\n+\n+                    if not capability_ids:\n+                        filters = {}\n+\n+                    elif len(capability_ids) == 1:\n+                        filters = {\"capability_id\": {\"$ne\": capability_ids.pop()}}\n+\n+                    else:\n+                        filters = {\n+                            \"$and\": [{\"capability_id\": {\"$ne\": id}} for id in capability_ids]\n+                        }\n+\n+                else:\n+                    tag_filters: Where = {\"$or\": [{\"tag_id\": {\"$eq\": tag}} for tag in tags]}\n+                    tag_associations = await self._tag_association_collection.find(\n+                        filters=tag_filters\n+                    )\n+\n+                    capability_ids = {assoc[\"capability_id\"] for assoc in tag_associations}\n+                    if not capability_ids:\n+                        return []\n+\n+                    if len(capability_ids) == 1:\n+                        filters = {\"capability_id\": {\"$eq\": capability_ids.pop()}}\n+\n+                    else:\n+                        filters = {\"$or\": [{\"capability_id\": {\"$eq\": id}} for id in capability_ids]}\n+\n+            docs = {}\n+            for d in await self._collection.find(filters=filters):\n+                if d[\"capability_id\"] not in docs:\n+                    docs[d[\"capability_id\"]] = d\n+\n+            return [await self._deserialize(d) for d in docs.values()]\n+\n+    @override\n+    async def delete_capability(\n+        self,\n+        capability_id: CapabilityId,\n+    ) -> None:\n+        async with self._lock.writer_lock:\n+            docs = await self._collection.find(filters={\"capability_id\": {\"$eq\": capability_id}})\n+\n+            tag_associations = await self._tag_association_collection.find(\n+                filters={\"capability_id\": {\"$eq\": capability_id}}\n+            )\n+\n+            if not docs:\n+                raise ItemNotFoundError(item_id=UniqueId(capability_id))\n+\n+            for doc in docs:\n+                await self._collection.delete_one(filters={\"id\": {\"$eq\": doc[\"id\"]}})\n+\n+            for tag_assoc in tag_associations:\n+                await self._tag_association_collection.delete_one(\n+                    filters={\"id\": {\"$eq\": tag_assoc[\"id\"]}}\n+                )\n+\n+    @override\n+    async def find_relevant_capabilities(\n+        self,\n+        query: str,\n+        available_capabilities: Sequence[Capability],\n+        max_capabilities: int = 3,\n+    ) -> Sequence[Capability]:\n+        if not available_capabilities:\n+            return []\n+\n+        n_result = sum(",
        "comment_created_at": "2025-06-21T13:59:59+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "This is a brilliant approach - but we must document right above it how and why this approach works (compared to just getting ALL the documents and sorting manually)",
        "pr_file_module": null
      }
    ]
  }
]