[
  {
    "discussion_id": "2301423071",
    "pr_number": 515,
    "pr_file": "src/parlant/adapters/nlp/openrouter_service.py",
    "created_at": "2025-08-26T15:45:43+00:00",
    "commented_code": "+# src/parlant/adapters/nlp/openrouter_service.py\n+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+OpenRouter NLP Service Adapter for Parlant.\n+\n+This module provides integration with OpenRouter's API gateway, offering a unified\n+interface to access language models from multiple providers (OpenAI, Anthropic, \n+Google, Meta, and more) through a single API endpoint.\n+\n+Key Benefits:\n+- Single API key for all LLM providers\n+- Centralized billing and usage tracking\n+- Mix models from different vendors for optimal performance\n+- No vendor lock-in - switch models without code changes\n+\n+Example Mixed Model Configuration:\n+    Use different models for different tasks within the same application:\n+    - GPT-4 for tool execution (high accuracy)\n+    - Claude 3.5 for journey reasoning (complex logic)\n+    - Gemini Pro for content generation (creativity)\n+    - Llama 3 for simple selections (cost efficiency)\n+\n+Configuration is managed entirely through environment variables.\n+\n+Required Environment Variables:\n+    OPENROUTER_API_KEY: API key for OpenRouter authentication\n+\n+Optional Environment Variables:\n+    OPENROUTER_BASE_URL: Base URL for OpenRouter API (default: https://openrouter.ai/api/v1)\n+\n+Schema-Specific Model Configuration:\n+    OPENROUTER_SINGLE_TOOL_MODEL: Model for SingleToolBatchSchema (default: openai/gpt-4o)\n+    OPENROUTER_SINGLE_TOOL_MAX_TOKENS: Max tokens for SingleToolBatchSchema (default: 131072)\n+    OPENROUTER_JOURNEY_NODE_MODEL: Model for JourneyNodeSelectionSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_JOURNEY_NODE_MAX_TOKENS: Max tokens for JourneyNodeSelectionSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL: Model for CannedResponseDraftSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS: Max tokens for CannedResponseDraftSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL: Model for CannedResponseSelectionSchema (default: anthropic/claude-3-haiku)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS: Max tokens for CannedResponseSelectionSchema (default: 204800)",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "2301423071",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 515,
        "pr_file": "src/parlant/adapters/nlp/openrouter_service.py",
        "discussion_id": "2301423071",
        "commented_code": "@@ -0,0 +1,457 @@\n+# src/parlant/adapters/nlp/openrouter_service.py\n+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+OpenRouter NLP Service Adapter for Parlant.\n+\n+This module provides integration with OpenRouter's API gateway, offering a unified\n+interface to access language models from multiple providers (OpenAI, Anthropic, \n+Google, Meta, and more) through a single API endpoint.\n+\n+Key Benefits:\n+- Single API key for all LLM providers\n+- Centralized billing and usage tracking\n+- Mix models from different vendors for optimal performance\n+- No vendor lock-in - switch models without code changes\n+\n+Example Mixed Model Configuration:\n+    Use different models for different tasks within the same application:\n+    - GPT-4 for tool execution (high accuracy)\n+    - Claude 3.5 for journey reasoning (complex logic)\n+    - Gemini Pro for content generation (creativity)\n+    - Llama 3 for simple selections (cost efficiency)\n+\n+Configuration is managed entirely through environment variables.\n+\n+Required Environment Variables:\n+    OPENROUTER_API_KEY: API key for OpenRouter authentication\n+\n+Optional Environment Variables:\n+    OPENROUTER_BASE_URL: Base URL for OpenRouter API (default: https://openrouter.ai/api/v1)\n+\n+Schema-Specific Model Configuration:\n+    OPENROUTER_SINGLE_TOOL_MODEL: Model for SingleToolBatchSchema (default: openai/gpt-4o)\n+    OPENROUTER_SINGLE_TOOL_MAX_TOKENS: Max tokens for SingleToolBatchSchema (default: 131072)\n+    OPENROUTER_JOURNEY_NODE_MODEL: Model for JourneyNodeSelectionSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_JOURNEY_NODE_MAX_TOKENS: Max tokens for JourneyNodeSelectionSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL: Model for CannedResponseDraftSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS: Max tokens for CannedResponseDraftSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL: Model for CannedResponseSelectionSchema (default: anthropic/claude-3-haiku)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS: Max tokens for CannedResponseSelectionSchema (default: 204800)",
        "comment_created_at": "2025-08-26T15:45:43+00:00",
        "comment_author": "Agam1997",
        "comment_body": "I don't think specific model configuration needs to done like this - although this adds great flexibility. A fallbackschematic generator is already available to use. Maybe consider using CustomSchematicGenerator like how its there on azure_service.py `CustomAzureSchematicGenerator`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2301429620",
    "pr_number": 515,
    "pr_file": "src/parlant/adapters/nlp/openrouter_service.py",
    "created_at": "2025-08-26T15:48:11+00:00",
    "commented_code": "+# src/parlant/adapters/nlp/openrouter_service.py\n+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+OpenRouter NLP Service Adapter for Parlant.\n+\n+This module provides integration with OpenRouter's API gateway, offering a unified\n+interface to access language models from multiple providers (OpenAI, Anthropic, \n+Google, Meta, and more) through a single API endpoint.\n+\n+Key Benefits:\n+- Single API key for all LLM providers\n+- Centralized billing and usage tracking\n+- Mix models from different vendors for optimal performance\n+- No vendor lock-in - switch models without code changes\n+\n+Example Mixed Model Configuration:\n+    Use different models for different tasks within the same application:\n+    - GPT-4 for tool execution (high accuracy)\n+    - Claude 3.5 for journey reasoning (complex logic)\n+    - Gemini Pro for content generation (creativity)\n+    - Llama 3 for simple selections (cost efficiency)\n+\n+Configuration is managed entirely through environment variables.\n+\n+Required Environment Variables:\n+    OPENROUTER_API_KEY: API key for OpenRouter authentication\n+\n+Optional Environment Variables:\n+    OPENROUTER_BASE_URL: Base URL for OpenRouter API (default: https://openrouter.ai/api/v1)\n+\n+Schema-Specific Model Configuration:\n+    OPENROUTER_SINGLE_TOOL_MODEL: Model for SingleToolBatchSchema (default: openai/gpt-4o)\n+    OPENROUTER_SINGLE_TOOL_MAX_TOKENS: Max tokens for SingleToolBatchSchema (default: 131072)\n+    OPENROUTER_JOURNEY_NODE_MODEL: Model for JourneyNodeSelectionSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_JOURNEY_NODE_MAX_TOKENS: Max tokens for JourneyNodeSelectionSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL: Model for CannedResponseDraftSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS: Max tokens for CannedResponseDraftSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL: Model for CannedResponseSelectionSchema (default: anthropic/claude-3-haiku)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS: Max tokens for CannedResponseSelectionSchema (default: 204800)\n+\n+Embedding Configuration (Third-party provider required):",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "2301429620",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 515,
        "pr_file": "src/parlant/adapters/nlp/openrouter_service.py",
        "discussion_id": "2301429620",
        "commented_code": "@@ -0,0 +1,457 @@\n+# src/parlant/adapters/nlp/openrouter_service.py\n+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+OpenRouter NLP Service Adapter for Parlant.\n+\n+This module provides integration with OpenRouter's API gateway, offering a unified\n+interface to access language models from multiple providers (OpenAI, Anthropic, \n+Google, Meta, and more) through a single API endpoint.\n+\n+Key Benefits:\n+- Single API key for all LLM providers\n+- Centralized billing and usage tracking\n+- Mix models from different vendors for optimal performance\n+- No vendor lock-in - switch models without code changes\n+\n+Example Mixed Model Configuration:\n+    Use different models for different tasks within the same application:\n+    - GPT-4 for tool execution (high accuracy)\n+    - Claude 3.5 for journey reasoning (complex logic)\n+    - Gemini Pro for content generation (creativity)\n+    - Llama 3 for simple selections (cost efficiency)\n+\n+Configuration is managed entirely through environment variables.\n+\n+Required Environment Variables:\n+    OPENROUTER_API_KEY: API key for OpenRouter authentication\n+\n+Optional Environment Variables:\n+    OPENROUTER_BASE_URL: Base URL for OpenRouter API (default: https://openrouter.ai/api/v1)\n+\n+Schema-Specific Model Configuration:\n+    OPENROUTER_SINGLE_TOOL_MODEL: Model for SingleToolBatchSchema (default: openai/gpt-4o)\n+    OPENROUTER_SINGLE_TOOL_MAX_TOKENS: Max tokens for SingleToolBatchSchema (default: 131072)\n+    OPENROUTER_JOURNEY_NODE_MODEL: Model for JourneyNodeSelectionSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_JOURNEY_NODE_MAX_TOKENS: Max tokens for JourneyNodeSelectionSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL: Model for CannedResponseDraftSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS: Max tokens for CannedResponseDraftSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL: Model for CannedResponseSelectionSchema (default: anthropic/claude-3-haiku)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS: Max tokens for CannedResponseSelectionSchema (default: 204800)\n+\n+Embedding Configuration (Third-party provider required):",
        "comment_created_at": "2025-08-26T15:48:11+00:00",
        "comment_author": "Agam1997",
        "comment_body": "This I feel goes against how other built-in adapters are made, although a great flexibility but if a separate service provider is to be used for embeddings IMO a custom NLPService implementation for the specific use case would be better way to go.\r\n\r\n@kichanyurd tagging you as it may be relevant",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1758284180",
    "pr_number": 81,
    "pr_file": "server/src/emcie/server/llm/schematic_generators.py",
    "created_at": "2024-09-13T06:56:03+00:00",
    "commented_code": "+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import json\n+import jsonfinder  # type: ignore\n+import os\n+from typing import Any, Generic, Type, TypeVar\n+\n+from openai import AsyncClient\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.base_models import DefaultBaseModel\n+\n+T = TypeVar(\"T\", bound=DefaultBaseModel)\n+\n+\n+@dataclass(frozen=True)\n+class SchematicGenerationResult(Generic[T]):\n+    content: T\n+\n+\n+class SchematicGenerator(ABC, Generic[T]):\n+    @abstractmethod\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: dict[str, Any],\n+    ) -> SchematicGenerationResult[T]: ...\n+\n+\n+class GPT4o(SchematicGenerator[T]):",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "1758284180",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 81,
        "pr_file": "server/src/emcie/server/llm/schematic_generators.py",
        "discussion_id": "1758284180",
        "commented_code": "@@ -0,0 +1,107 @@\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import json\n+import jsonfinder  # type: ignore\n+import os\n+from typing import Any, Generic, Type, TypeVar\n+\n+from openai import AsyncClient\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.base_models import DefaultBaseModel\n+\n+T = TypeVar(\"T\", bound=DefaultBaseModel)\n+\n+\n+@dataclass(frozen=True)\n+class SchematicGenerationResult(Generic[T]):\n+    content: T\n+\n+\n+class SchematicGenerator(ABC, Generic[T]):\n+    @abstractmethod\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: dict[str, Any],\n+    ) -> SchematicGenerationResult[T]: ...\n+\n+\n+class GPT4o(SchematicGenerator[T]):",
        "comment_created_at": "2024-09-13T06:56:03+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "Make a base class OpenAI and inherit from it.\nThis will allow some places to say, \"I want an OpenAI model\", and be able to make assumptions on the special hint params it can take.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1758286417",
    "pr_number": 81,
    "pr_file": "server/src/emcie/server/llm/schematic_generators.py",
    "created_at": "2024-09-13T06:57:56+00:00",
    "commented_code": "+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import json\n+import jsonfinder  # type: ignore\n+import os\n+from typing import Any, Generic, Type, TypeVar\n+\n+from openai import AsyncClient\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.base_models import DefaultBaseModel\n+\n+T = TypeVar(\"T\", bound=DefaultBaseModel)\n+\n+\n+@dataclass(frozen=True)\n+class SchematicGenerationResult(Generic[T]):\n+    content: T\n+\n+\n+class SchematicGenerator(ABC, Generic[T]):\n+    @abstractmethod\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: dict[str, Any],\n+    ) -> SchematicGenerationResult[T]: ...\n+\n+\n+class GPT4o(SchematicGenerator[T]):\n+    def __init__(self, schema: Type[T]) -> None:\n+        self._llm_client = AsyncClient(api_key=os.environ[\"OPENAI_API_KEY\"])\n+        self._schema = schema\n+\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: dict[str, Any],\n+    ) -> SchematicGenerationResult[T]:\n+        response = await self._llm_client.chat.completions.create(\n+            messages=[{\"role\": \"user\", \"content\": prompt}],\n+            model=\"gpt-4o\",\n+            response_format={\"type\": \"json_object\"},\n+            **hints,",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "1758286417",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 81,
        "pr_file": "server/src/emcie/server/llm/schematic_generators.py",
        "discussion_id": "1758286417",
        "commented_code": "@@ -0,0 +1,107 @@\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import json\n+import jsonfinder  # type: ignore\n+import os\n+from typing import Any, Generic, Type, TypeVar\n+\n+from openai import AsyncClient\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.base_models import DefaultBaseModel\n+\n+T = TypeVar(\"T\", bound=DefaultBaseModel)\n+\n+\n+@dataclass(frozen=True)\n+class SchematicGenerationResult(Generic[T]):\n+    content: T\n+\n+\n+class SchematicGenerator(ABC, Generic[T]):\n+    @abstractmethod\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: dict[str, Any],\n+    ) -> SchematicGenerationResult[T]: ...\n+\n+\n+class GPT4o(SchematicGenerator[T]):\n+    def __init__(self, schema: Type[T]) -> None:\n+        self._llm_client = AsyncClient(api_key=os.environ[\"OPENAI_API_KEY\"])\n+        self._schema = schema\n+\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: dict[str, Any],\n+    ) -> SchematicGenerationResult[T]:\n+        response = await self._llm_client.chat.completions.create(\n+            messages=[{\"role\": \"user\", \"content\": prompt}],\n+            model=\"gpt-4o\",\n+            response_format={\"type\": \"json_object\"},\n+            **hints,",
        "comment_created_at": "2024-09-13T06:57:56+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "In the OpenAI base class, make sure you want take from `hints` the specific args you want to support, and which *are* supported in OpenAI.\nFor example, in the beginning we can support only `temperature`, `max_tokens` and `logit_bias`.\nThe rest of the keys in `hints` should be ignored and not passed to the underlying API.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1759995667",
    "pr_number": 81,
    "pr_file": "server/src/emcie/server/core/generation/schematic_generators.py",
    "created_at": "2024-09-15T08:35:12+00:00",
    "commented_code": "+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import json\n+import os\n+from typing import Any, Generic, Optional, Type, TypeVar\n+\n+import jsonfinder  # type: ignore\n+from openai import AsyncClient\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.base_models import DefaultBaseModel\n+from emcie.server.logger import Logger\n+\n+T = TypeVar(\"T\", bound=DefaultBaseModel)\n+\n+\n+@dataclass(frozen=True)\n+class SchematicGenerationResult(Generic[T]):\n+    content: T\n+\n+\n+class BaseSchematicGenerator(ABC, Generic[T]):\n+    supported_arguments: list[str] = []\n+\n+    def __init__(\n+        self,\n+        logger: Logger,\n+        schema: Type[T],\n+    ) -> None:\n+        self.logger = logger\n+        self._schema = schema\n+\n+    @abstractmethod\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: Optional[dict[str, Any]],\n+    ) -> SchematicGenerationResult[T]: ...\n+\n+\n+class OpenAIBaseSchematicGenerator(BaseSchematicGenerator[T], ABC):\n+    supported_arguments = [\"temperature\", \"logit_bias\", \"max_tokens\"]\n+\n+    def __init__(\n+        self,\n+        logger: Logger,\n+        schema: Type[T],\n+    ) -> None:\n+        super().__init__(logger=logger, schema=schema)\n+        self._client = AsyncClient(api_key=os.environ[\"OPENAI_API_KEY\"])\n+\n+    @abstractmethod\n+    def _get_model_name(self) -> str:\n+        pass\n+\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: Optional[dict[str, Any]] = None,\n+    ) -> SchematicGenerationResult[T]:\n+        filtered_hints = {}\n+        if hints:\n+            for k, v in hints.items():\n+                if k not in self.supported_arguments:\n+                    self.logger.warning(\n+                        f\"Key '{k}' is not supported in the provided model. Skipping...\"\n+                    )\n+                    continue\n+                filtered_hints[k] = v\n+\n+        response = await self._client.chat.completions.create(\n+            messages=[{\"role\": \"user\", \"content\": prompt}],\n+            model=self._get_model_name(),\n+            response_format={\n+                \"type\": \"json_object\",\n+            },\n+            **filtered_hints,\n+        )\n+\n+        raw_content = response.choices[0].message.content or \"{}\"\n+\n+        try:\n+            json_content = json.loads(raw_content)\n+        except json.JSONDecodeError:\n+            json_content = jsonfinder.only_json(raw_content)[2]\n+\n+        content = self._schema.model_validate(json_content)\n+        return SchematicGenerationResult(content=content)\n+\n+\n+class TogetherAIBaseSchematicGenerator(BaseSchematicGenerator[T], ABC):\n+    supported_arguments = [\"temperature\"]\n+\n+    def __init__(\n+        self,\n+        logger: Logger,\n+        schema: Type[T],\n+    ) -> None:\n+        self.logger = logger\n+        self._schema = schema\n+        self._client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n+\n+    @abstractmethod\n+    def _get_model_name(self) -> str:\n+        pass\n+\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: Optional[dict[str, Any]] = None,\n+    ) -> SchematicGenerationResult[T]:\n+        filtered_hints = {}\n+        if hints:\n+            for k, v in hints.items():\n+                if k not in self.supported_arguments:\n+                    self.logger.warning(\n+                        f\"Key '{k}' is not supported in the provided model. Skipping...\"\n+                    )\n+                    continue\n+                filtered_hints[k] = v\n+\n+        response = await self._client.chat.completions.create(\n+            messages=[{\"role\": \"user\", \"content\": prompt}],\n+            model=self._get_model_name(),\n+            **filtered_hints,\n+        )\n+\n+        raw_content = response.choices[0].message.content or \"{}\"\n+\n+        try:\n+            json_content = json.loads(raw_content)",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "1759995667",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 81,
        "pr_file": "server/src/emcie/server/core/generation/schematic_generators.py",
        "discussion_id": "1759995667",
        "commented_code": "@@ -0,0 +1,151 @@\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import json\n+import os\n+from typing import Any, Generic, Optional, Type, TypeVar\n+\n+import jsonfinder  # type: ignore\n+from openai import AsyncClient\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.base_models import DefaultBaseModel\n+from emcie.server.logger import Logger\n+\n+T = TypeVar(\"T\", bound=DefaultBaseModel)\n+\n+\n+@dataclass(frozen=True)\n+class SchematicGenerationResult(Generic[T]):\n+    content: T\n+\n+\n+class BaseSchematicGenerator(ABC, Generic[T]):\n+    supported_arguments: list[str] = []\n+\n+    def __init__(\n+        self,\n+        logger: Logger,\n+        schema: Type[T],\n+    ) -> None:\n+        self.logger = logger\n+        self._schema = schema\n+\n+    @abstractmethod\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: Optional[dict[str, Any]],\n+    ) -> SchematicGenerationResult[T]: ...\n+\n+\n+class OpenAIBaseSchematicGenerator(BaseSchematicGenerator[T], ABC):\n+    supported_arguments = [\"temperature\", \"logit_bias\", \"max_tokens\"]\n+\n+    def __init__(\n+        self,\n+        logger: Logger,\n+        schema: Type[T],\n+    ) -> None:\n+        super().__init__(logger=logger, schema=schema)\n+        self._client = AsyncClient(api_key=os.environ[\"OPENAI_API_KEY\"])\n+\n+    @abstractmethod\n+    def _get_model_name(self) -> str:\n+        pass\n+\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: Optional[dict[str, Any]] = None,\n+    ) -> SchematicGenerationResult[T]:\n+        filtered_hints = {}\n+        if hints:\n+            for k, v in hints.items():\n+                if k not in self.supported_arguments:\n+                    self.logger.warning(\n+                        f\"Key '{k}' is not supported in the provided model. Skipping...\"\n+                    )\n+                    continue\n+                filtered_hints[k] = v\n+\n+        response = await self._client.chat.completions.create(\n+            messages=[{\"role\": \"user\", \"content\": prompt}],\n+            model=self._get_model_name(),\n+            response_format={\n+                \"type\": \"json_object\",\n+            },\n+            **filtered_hints,\n+        )\n+\n+        raw_content = response.choices[0].message.content or \"{}\"\n+\n+        try:\n+            json_content = json.loads(raw_content)\n+        except json.JSONDecodeError:\n+            json_content = jsonfinder.only_json(raw_content)[2]\n+\n+        content = self._schema.model_validate(json_content)\n+        return SchematicGenerationResult(content=content)\n+\n+\n+class TogetherAIBaseSchematicGenerator(BaseSchematicGenerator[T], ABC):\n+    supported_arguments = [\"temperature\"]\n+\n+    def __init__(\n+        self,\n+        logger: Logger,\n+        schema: Type[T],\n+    ) -> None:\n+        self.logger = logger\n+        self._schema = schema\n+        self._client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n+\n+    @abstractmethod\n+    def _get_model_name(self) -> str:\n+        pass\n+\n+    async def generate(\n+        self,\n+        prompt: str,\n+        hints: Optional[dict[str, Any]] = None,\n+    ) -> SchematicGenerationResult[T]:\n+        filtered_hints = {}\n+        if hints:\n+            for k, v in hints.items():\n+                if k not in self.supported_arguments:\n+                    self.logger.warning(\n+                        f\"Key '{k}' is not supported in the provided model. Skipping...\"\n+                    )\n+                    continue\n+                filtered_hints[k] = v\n+\n+        response = await self._client.chat.completions.create(\n+            messages=[{\"role\": \"user\", \"content\": prompt}],\n+            model=self._get_model_name(),\n+            **filtered_hints,\n+        )\n+\n+        raw_content = response.choices[0].message.content or \"{}\"\n+\n+        try:\n+            json_content = json.loads(raw_content)",
        "comment_created_at": "2024-09-15T08:35:12+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "Don't even try it like this with Together :) Go straight to jsonfinder",
        "pr_file_module": null
      }
    ]
  }
]