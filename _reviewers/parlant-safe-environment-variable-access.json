[
  {
    "discussion_id": "2301442795",
    "pr_number": 515,
    "pr_file": "src/parlant/adapters/nlp/openrouter_service.py",
    "created_at": "2025-08-26T15:52:43+00:00",
    "commented_code": "+# src/parlant/adapters/nlp/openrouter_service.py\n+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+OpenRouter NLP Service Adapter for Parlant.\n+\n+This module provides integration with OpenRouter's API gateway, offering a unified\n+interface to access language models from multiple providers (OpenAI, Anthropic, \n+Google, Meta, and more) through a single API endpoint.\n+\n+Key Benefits:\n+- Single API key for all LLM providers\n+- Centralized billing and usage tracking\n+- Mix models from different vendors for optimal performance\n+- No vendor lock-in - switch models without code changes\n+\n+Example Mixed Model Configuration:\n+    Use different models for different tasks within the same application:\n+    - GPT-4 for tool execution (high accuracy)\n+    - Claude 3.5 for journey reasoning (complex logic)\n+    - Gemini Pro for content generation (creativity)\n+    - Llama 3 for simple selections (cost efficiency)\n+\n+Configuration is managed entirely through environment variables.\n+\n+Required Environment Variables:\n+    OPENROUTER_API_KEY: API key for OpenRouter authentication\n+\n+Optional Environment Variables:\n+    OPENROUTER_BASE_URL: Base URL for OpenRouter API (default: https://openrouter.ai/api/v1)\n+\n+Schema-Specific Model Configuration:\n+    OPENROUTER_SINGLE_TOOL_MODEL: Model for SingleToolBatchSchema (default: openai/gpt-4o)\n+    OPENROUTER_SINGLE_TOOL_MAX_TOKENS: Max tokens for SingleToolBatchSchema (default: 131072)\n+    OPENROUTER_JOURNEY_NODE_MODEL: Model for JourneyNodeSelectionSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_JOURNEY_NODE_MAX_TOKENS: Max tokens for JourneyNodeSelectionSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL: Model for CannedResponseDraftSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS: Max tokens for CannedResponseDraftSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL: Model for CannedResponseSelectionSchema (default: anthropic/claude-3-haiku)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS: Max tokens for CannedResponseSelectionSchema (default: 204800)\n+\n+Embedding Configuration (Third-party provider required):\n+    Note: OpenRouter does not provide embedding services. Configure a third-party provider\n+    that follows the OpenAI embedding API format (e.g., OpenAI, Together AI).\n+    \n+    OPENROUTER_EMBEDDING_MODEL: Model name for the embedding provider (default: openai/text-embedding-3-small)\n+    OPENROUTER_EMBEDDING_DIMENSIONS: Embedding vector dimensions (default: 1536)\n+    OPENROUTER_EMBEDDING_BASE_URL: Base URL of your embedding provider (e.g., https://api.openai.com/v1)\n+    OPENROUTER_EMBEDDING_API_KEY: API key for the embedding provider (defaults to OPENROUTER_API_KEY)\n+\n+Example Usage:\n+    # OpenRouter API for LLM\n+    export OPENROUTER_API_KEY=\"your-openrouter-api-key\"\n+    export OPENROUTER_SINGLE_TOOL_MODEL=\"openai/gpt-4o-mini\"\n+    export OPENROUTER_SINGLE_TOOL_MAX_TOKENS=65536\n+    \n+    # Third-party embedding provider (e.g., OpenAI)\n+    export OPENROUTER_EMBEDDING_BASE_URL=\"https://api.openai.com/v1\"\n+    export OPENROUTER_EMBEDDING_API_KEY=\"your-openai-api-key\"\n+    export OPENROUTER_EMBEDDING_MODEL=\"text-embedding-3-small\"\n+\"\"\"\n+\n+from __future__ import annotations\n+import time\n+from typing import Any, Mapping\n+from typing_extensions import override\n+import json\n+import jsonfinder  # type: ignore\n+import os\n+from dataclasses import dataclass\n+\n+from pydantic import ValidationError\n+import tiktoken\n+import openai\n+\n+from parlant.adapters.nlp.common import normalize_json_output\n+from parlant.core.engines.alpha.canned_response_generator import (\n+    CannedResponseDraftSchema,\n+    CannedResponseSelectionSchema,\n+)\n+from parlant.core.engines.alpha.guideline_matching.generic.journey_node_selection_batch import (\n+    JourneyNodeSelectionSchema,\n+)\n+from parlant.core.engines.alpha.prompt_builder import PromptBuilder\n+from parlant.core.engines.alpha.tool_calling.single_tool_batch import SingleToolBatchSchema\n+from parlant.core.loggers import Logger\n+from parlant.core.nlp.tokenization import EstimatingTokenizer\n+from parlant.core.nlp.service import NLPService\n+from parlant.core.nlp.embedding import Embedder, EmbeddingResult\n+from parlant.core.nlp.generation import (\n+    T,\n+    SchematicGenerator,\n+    SchematicGenerationResult,\n+)\n+from parlant.core.nlp.generation_info import GenerationInfo, UsageInfo\n+from parlant.core.nlp.moderation import (\n+    ModerationService,\n+    NoModeration,\n+)\n+from parlant.core.nlp.policies import policy, retry\n+\n+\n+# Environment variable configuration keys\n+ENV_API_KEY = \"OPENROUTER_API_KEY\"\n+ENV_BASE_URL = \"OPENROUTER_BASE_URL\"\n+\n+# Schema-specific model configuration\n+ENV_SINGLE_TOOL_MODEL = \"OPENROUTER_SINGLE_TOOL_MODEL\"\n+ENV_SINGLE_TOOL_MAX_TOKENS = \"OPENROUTER_SINGLE_TOOL_MAX_TOKENS\"\n+ENV_JOURNEY_NODE_MODEL = \"OPENROUTER_JOURNEY_NODE_MODEL\"\n+ENV_JOURNEY_NODE_MAX_TOKENS = \"OPENROUTER_JOURNEY_NODE_MAX_TOKENS\"\n+ENV_CANNED_RESPONSE_DRAFT_MODEL = \"OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL\"\n+ENV_CANNED_RESPONSE_DRAFT_MAX_TOKENS = \"OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS\"\n+ENV_CANNED_RESPONSE_SELECTION_MODEL = \"OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL\"\n+ENV_CANNED_RESPONSE_SELECTION_MAX_TOKENS = \"OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS\"\n+\n+# Embedding configuration\n+ENV_EMBEDDING_MODEL = \"OPENROUTER_EMBEDDING_MODEL\"\n+ENV_EMBEDDING_DIMENSIONS = \"OPENROUTER_EMBEDDING_DIMENSIONS\"\n+ENV_EMBEDDING_BASE_URL = \"OPENROUTER_EMBEDDING_BASE_URL\"\n+ENV_EMBEDDING_API_KEY = \"OPENROUTER_EMBEDDING_API_KEY\"\n+\n+\n+@dataclass\n+class ModelConfig:\n+    \"\"\"Configuration for a specific model.\"\"\"\n+    model: str\n+    max_tokens: int\n+\n+\n+def get_env_int(key: str, default: int) -> int:\n+    \"\"\"Get integer value from environment variable with default.\"\"\"\n+    value = os.environ.get(key)\n+    if value:\n+        try:\n+            return int(value)\n+        except ValueError:\n+            pass\n+    return default\n+\n+\n+def get_model_config_for_schema(schema_type: type) -> ModelConfig:\n+    \"\"\"Get model configuration for a specific schema type from environment variables.\"\"\"\n+    # Default configurations\n+    defaults = {\n+        SingleToolBatchSchema: ModelConfig(\"openai/gpt-4o\", 128 * 1024),\n+        JourneyNodeSelectionSchema: ModelConfig(\"anthropic/claude-3.5-sonnet\", 200 * 1024),\n+        CannedResponseDraftSchema: ModelConfig(\"anthropic/claude-3.5-sonnet\", 200 * 1024),\n+        CannedResponseSelectionSchema: ModelConfig(\"anthropic/claude-3-haiku\", 200 * 1024),\n+    }\n+    \n+    # Environment variable mappings\n+    env_mappings = {\n+        SingleToolBatchSchema: (ENV_SINGLE_TOOL_MODEL, ENV_SINGLE_TOOL_MAX_TOKENS),\n+        JourneyNodeSelectionSchema: (ENV_JOURNEY_NODE_MODEL, ENV_JOURNEY_NODE_MAX_TOKENS),\n+        CannedResponseDraftSchema: (ENV_CANNED_RESPONSE_DRAFT_MODEL, ENV_CANNED_RESPONSE_DRAFT_MAX_TOKENS),\n+        CannedResponseSelectionSchema: (ENV_CANNED_RESPONSE_SELECTION_MODEL, ENV_CANNED_RESPONSE_SELECTION_MAX_TOKENS),\n+    }\n+    \n+    default_config = defaults.get(schema_type, defaults[SingleToolBatchSchema])\n+    \n+    if schema_type in env_mappings:\n+        model_env, tokens_env = env_mappings[schema_type]\n+        model = os.environ.get(model_env, default_config.model)\n+        max_tokens = get_env_int(tokens_env, default_config.max_tokens)\n+        return ModelConfig(model, max_tokens)\n+    \n+    return default_config\n+\n+\n+RATE_LIMIT_ERROR_MESSAGE = (\n+    \"OpenRouter API rate limit exceeded. Possible reasons:\n\"\n+    \"1. Your account may have insufficient API credits.\n\"\n+    \"2. You may be using a free-tier account with limited request capacity.\n\"\n+    \"3. You might have exceeded the requests-per-minute limit for your account.\n\n\"\n+    \"Recommended actions:\n\"\n+    \"- Check your OpenRouter account balance and billing status.\n\"\n+    \"- Review your API usage limits in OpenRouter dashboard.\n\"\n+    \"- For more details on rate limits and usage tiers, visit:\n\"\n+    \"  https://openrouter.ai/docs\"\n+)\n+\n+\n+class OpenRouterEstimatingTokenizer(EstimatingTokenizer):\n+    def __init__(self, model_name: str) -> None:\n+        self.model_name = model_name\n+        # Use GPT-4 tokenizer as fallback for most models\n+        self.encoding = tiktoken.encoding_for_model(\"gpt-4o-2024-08-06\")\n+\n+    @override\n+    async def estimate_token_count(self, prompt: str) -> int:\n+        tokens = self.encoding.encode(prompt)\n+        return len(tokens)\n+\n+\n+class OpenRouterSchematicGenerator(SchematicGenerator[T]):\n+    supported_openrouter_params = [\"temperature\", \"max_tokens\", \"top_p\", \"frequency_penalty\", \"presence_penalty\"]\n+    supported_hints = supported_openrouter_params + [\"strict\"]\n+\n+    def __init__(\n+        self,\n+        model_name: str,\n+        max_tokens: int,\n+        logger: Logger,\n+        api_key: str,\n+        base_url: str = \"https://openrouter.ai/api/v1\",\n+    ) -> None:\n+        self._model_name = model_name\n+        self._max_tokens = max_tokens\n+        self._logger = logger\n+        self._api_key = api_key\n+        self._base_url = base_url\n+        \n+        # Initialize OpenAI client\n+        self._client = openai.AsyncOpenAI(\n+            api_key=api_key,\n+            base_url=base_url,\n+            default_headers={\n+                \"HTTP-Referer\": \"https://parlant.ai\",\n+                \"X-Title\": \"Parlant\",\n+            }\n+        )\n+        self._tokenizer = OpenRouterEstimatingTokenizer(model_name=self._model_name)\n+\n+    @property\n+    @override\n+    def id(self) -> str:\n+        return f\"{self._model_name}\"\n+\n+    @property\n+    @override\n+    def tokenizer(self) -> OpenRouterEstimatingTokenizer:\n+        return self._tokenizer\n+\n+    @property\n+    @override\n+    def max_tokens(self) -> int:\n+        return self._max_tokens\n+\n+    @policy(\n+        [\n+            retry(\n+                exceptions=(\n+                    openai.RateLimitError,\n+                    openai.APITimeoutError,\n+                    openai.APIError,\n+                ),\n+            ),\n+            retry(openai.APIError, max_exceptions=2, wait_times=(1.0, 5.0)),\n+        ]\n+    )\n+    @override\n+    async def generate(\n+        self,\n+        prompt: str | PromptBuilder,\n+        hints: Mapping[str, Any] = {},\n+    ) -> SchematicGenerationResult[T]:\n+        if isinstance(prompt, PromptBuilder):\n+            prompt = prompt.build()\n+\n+        openrouter_api_arguments = {k: v for k, v in hints.items() if k in self.supported_openrouter_params}\n+\n+        t_start = time.time()\n+        try:\n+            response = await self._client.chat.completions.create(\n+                model=self._model_name,\n+                messages=[{\"role\": \"user\", \"content\": prompt}],\n+                response_format={\"type\": \"json_object\"},\n+                **openrouter_api_arguments,\n+            )\n+        except openai.RateLimitError:\n+            self._logger.error(RATE_LIMIT_ERROR_MESSAGE)\n+            raise\n+\n+        t_end = time.time()\n+\n+        self._logger.trace(json.dumps(response.model_dump(), indent=2))\n+\n+        raw_content = response.choices[0].message.content or \"{}\"\n+\n+        try:\n+            json_content = json.loads(normalize_json_output(raw_content))\n+        except json.JSONDecodeError:\n+            self._logger.warning(\n+                f\"Invalid JSON returned by openrouter/{self._model_name}:\n{raw_content}\"\n+            )\n+            json_content = jsonfinder.only_json(raw_content)[2]\n+            self._logger.warning(\"Found JSON content within model response; continuing...\")\n+\n+        try:\n+            content = self.schema.model_validate(json_content)\n+            usage_data = response.usage\n+\n+            return SchematicGenerationResult(\n+                content=content,\n+                info=GenerationInfo(\n+                    schema_name=self.schema.__name__,\n+                    model=self.id,\n+                    duration=(t_end - t_start),\n+                    usage=UsageInfo(\n+                        input_tokens=usage_data.prompt_tokens if usage_data else 0,\n+                        output_tokens=usage_data.completion_tokens if usage_data else 0,\n+                    ),\n+                ),\n+            )\n+        except ValidationError:\n+            self._logger.error(\n+                f\"JSON content returned by openrouter/{self._model_name} does not match expected schema:\n{raw_content}\"\n+            )\n+            raise\n+\n+\n+class OpenRouterEmbedder(Embedder):\n+    \"\"\"\n+    Embedder that connects to third-party embedding services.\n+    \n+    Note: OpenRouter does not provide embedding services. This embedder is designed\n+    to work with any third-party provider that follows the OpenAI embedding API format,\n+    such as OpenAI, Together AI, or other compatible services.\n+    \"\"\"\n+    def __init__(self, logger: Logger) -> None:\n+        self._logger = logger\n+        \n+        # Get embedding configuration from environment variables\n+        self._embedding_model = os.environ.get(ENV_EMBEDDING_MODEL, \"openai/text-embedding-3-small\")\n+        self._embedding_dimensions = get_env_int(ENV_EMBEDDING_DIMENSIONS, 1536)\n+        \n+        # Get base URL and API key from environment\n+        base_url = os.environ.get(ENV_BASE_URL, \"https://openrouter.ai/api/v1\")\n+        api_key = os.environ.get(ENV_API_KEY, \"\")\n+        \n+        self._embedding_base_url = os.environ.get(ENV_EMBEDDING_BASE_URL, base_url)\n+        # Use specific embedding API key if provided, otherwise use main API key\n+        self._embedding_api_key = os.environ.get(ENV_EMBEDDING_API_KEY, api_key)\n+        \n+        if not self._embedding_api_key:\n+            raise ValueError(f\"Either {ENV_API_KEY} or {ENV_EMBEDDING_API_KEY} must be set\")\n+        \n+        # Initialize OpenAI client\n+        self._client = openai.AsyncOpenAI(\n+            api_key=self._embedding_api_key,\n+            base_url=self._embedding_base_url,\n+        )\n+        self._logger.info(f\"Initialized OpenRouterEmbedder with model: {self._embedding_model}\")\n+\n+    @property\n+    @override\n+    def id(self) -> str:\n+        return f\"{self._embedding_model}\"\n+\n+    @property\n+    @override\n+    def max_tokens(self) -> int:\n+        return 8192\n+\n+    @property\n+    @override\n+    def tokenizer(self) -> EstimatingTokenizer:\n+        return OpenRouterEstimatingTokenizer(self._embedding_model)\n+\n+    @property\n+    @override\n+    def dimensions(self) -> int:\n+        return self._embedding_dimensions\n+\n+    @policy(\n+        [\n+            retry(\n+                exceptions=(\n+                    openai.RateLimitError,\n+                    openai.APITimeoutError,\n+                    openai.APIError,\n+                ),\n+            ),\n+            retry(openai.APIError, max_exceptions=2, wait_times=(1.0, 5.0)),\n+        ]\n+    )\n+    @override\n+    async def embed(\n+        self,\n+        texts: list[str],\n+        hints: Mapping[str, Any] = {},\n+    ) -> EmbeddingResult:\n+        _ = hints  # Not used for OpenRouter\n+        \n+        response = await self._client.embeddings.create(\n+            model=self._embedding_model,\n+            input=texts,\n+        )\n+\n+        vectors = [data_point.embedding for data_point in response.data]\n+        return EmbeddingResult(vectors=vectors)\n+\n+\n+# Specific generator classes for different models/use cases\n+class OpenRouter_Default(OpenRouterSchematicGenerator[T]):\n+    \"\"\"Default OpenRouter generator using configuration from environment.\"\"\"\n+    def __init__(self, logger: Logger, api_key: str, base_url: str, model_config: ModelConfig) -> None:\n+        super().__init__(\n+            model_name=model_config.model,\n+            max_tokens=model_config.max_tokens,\n+            logger=logger,\n+            api_key=api_key,\n+            base_url=base_url,\n+        )\n+\n+\n+class OpenRouterService(NLPService):\n+    @staticmethod\n+    def verify_environment() -> str | None:",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "2301442795",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 515,
        "pr_file": "src/parlant/adapters/nlp/openrouter_service.py",
        "discussion_id": "2301442795",
        "commented_code": "@@ -0,0 +1,457 @@\n+# src/parlant/adapters/nlp/openrouter_service.py\n+# Copyright 2025 Emcie Co Ltd.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+OpenRouter NLP Service Adapter for Parlant.\n+\n+This module provides integration with OpenRouter's API gateway, offering a unified\n+interface to access language models from multiple providers (OpenAI, Anthropic, \n+Google, Meta, and more) through a single API endpoint.\n+\n+Key Benefits:\n+- Single API key for all LLM providers\n+- Centralized billing and usage tracking\n+- Mix models from different vendors for optimal performance\n+- No vendor lock-in - switch models without code changes\n+\n+Example Mixed Model Configuration:\n+    Use different models for different tasks within the same application:\n+    - GPT-4 for tool execution (high accuracy)\n+    - Claude 3.5 for journey reasoning (complex logic)\n+    - Gemini Pro for content generation (creativity)\n+    - Llama 3 for simple selections (cost efficiency)\n+\n+Configuration is managed entirely through environment variables.\n+\n+Required Environment Variables:\n+    OPENROUTER_API_KEY: API key for OpenRouter authentication\n+\n+Optional Environment Variables:\n+    OPENROUTER_BASE_URL: Base URL for OpenRouter API (default: https://openrouter.ai/api/v1)\n+\n+Schema-Specific Model Configuration:\n+    OPENROUTER_SINGLE_TOOL_MODEL: Model for SingleToolBatchSchema (default: openai/gpt-4o)\n+    OPENROUTER_SINGLE_TOOL_MAX_TOKENS: Max tokens for SingleToolBatchSchema (default: 131072)\n+    OPENROUTER_JOURNEY_NODE_MODEL: Model for JourneyNodeSelectionSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_JOURNEY_NODE_MAX_TOKENS: Max tokens for JourneyNodeSelectionSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL: Model for CannedResponseDraftSchema (default: anthropic/claude-3.5-sonnet)\n+    OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS: Max tokens for CannedResponseDraftSchema (default: 204800)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL: Model for CannedResponseSelectionSchema (default: anthropic/claude-3-haiku)\n+    OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS: Max tokens for CannedResponseSelectionSchema (default: 204800)\n+\n+Embedding Configuration (Third-party provider required):\n+    Note: OpenRouter does not provide embedding services. Configure a third-party provider\n+    that follows the OpenAI embedding API format (e.g., OpenAI, Together AI).\n+    \n+    OPENROUTER_EMBEDDING_MODEL: Model name for the embedding provider (default: openai/text-embedding-3-small)\n+    OPENROUTER_EMBEDDING_DIMENSIONS: Embedding vector dimensions (default: 1536)\n+    OPENROUTER_EMBEDDING_BASE_URL: Base URL of your embedding provider (e.g., https://api.openai.com/v1)\n+    OPENROUTER_EMBEDDING_API_KEY: API key for the embedding provider (defaults to OPENROUTER_API_KEY)\n+\n+Example Usage:\n+    # OpenRouter API for LLM\n+    export OPENROUTER_API_KEY=\"your-openrouter-api-key\"\n+    export OPENROUTER_SINGLE_TOOL_MODEL=\"openai/gpt-4o-mini\"\n+    export OPENROUTER_SINGLE_TOOL_MAX_TOKENS=65536\n+    \n+    # Third-party embedding provider (e.g., OpenAI)\n+    export OPENROUTER_EMBEDDING_BASE_URL=\"https://api.openai.com/v1\"\n+    export OPENROUTER_EMBEDDING_API_KEY=\"your-openai-api-key\"\n+    export OPENROUTER_EMBEDDING_MODEL=\"text-embedding-3-small\"\n+\"\"\"\n+\n+from __future__ import annotations\n+import time\n+from typing import Any, Mapping\n+from typing_extensions import override\n+import json\n+import jsonfinder  # type: ignore\n+import os\n+from dataclasses import dataclass\n+\n+from pydantic import ValidationError\n+import tiktoken\n+import openai\n+\n+from parlant.adapters.nlp.common import normalize_json_output\n+from parlant.core.engines.alpha.canned_response_generator import (\n+    CannedResponseDraftSchema,\n+    CannedResponseSelectionSchema,\n+)\n+from parlant.core.engines.alpha.guideline_matching.generic.journey_node_selection_batch import (\n+    JourneyNodeSelectionSchema,\n+)\n+from parlant.core.engines.alpha.prompt_builder import PromptBuilder\n+from parlant.core.engines.alpha.tool_calling.single_tool_batch import SingleToolBatchSchema\n+from parlant.core.loggers import Logger\n+from parlant.core.nlp.tokenization import EstimatingTokenizer\n+from parlant.core.nlp.service import NLPService\n+from parlant.core.nlp.embedding import Embedder, EmbeddingResult\n+from parlant.core.nlp.generation import (\n+    T,\n+    SchematicGenerator,\n+    SchematicGenerationResult,\n+)\n+from parlant.core.nlp.generation_info import GenerationInfo, UsageInfo\n+from parlant.core.nlp.moderation import (\n+    ModerationService,\n+    NoModeration,\n+)\n+from parlant.core.nlp.policies import policy, retry\n+\n+\n+# Environment variable configuration keys\n+ENV_API_KEY = \"OPENROUTER_API_KEY\"\n+ENV_BASE_URL = \"OPENROUTER_BASE_URL\"\n+\n+# Schema-specific model configuration\n+ENV_SINGLE_TOOL_MODEL = \"OPENROUTER_SINGLE_TOOL_MODEL\"\n+ENV_SINGLE_TOOL_MAX_TOKENS = \"OPENROUTER_SINGLE_TOOL_MAX_TOKENS\"\n+ENV_JOURNEY_NODE_MODEL = \"OPENROUTER_JOURNEY_NODE_MODEL\"\n+ENV_JOURNEY_NODE_MAX_TOKENS = \"OPENROUTER_JOURNEY_NODE_MAX_TOKENS\"\n+ENV_CANNED_RESPONSE_DRAFT_MODEL = \"OPENROUTER_CANNED_RESPONSE_DRAFT_MODEL\"\n+ENV_CANNED_RESPONSE_DRAFT_MAX_TOKENS = \"OPENROUTER_CANNED_RESPONSE_DRAFT_MAX_TOKENS\"\n+ENV_CANNED_RESPONSE_SELECTION_MODEL = \"OPENROUTER_CANNED_RESPONSE_SELECTION_MODEL\"\n+ENV_CANNED_RESPONSE_SELECTION_MAX_TOKENS = \"OPENROUTER_CANNED_RESPONSE_SELECTION_MAX_TOKENS\"\n+\n+# Embedding configuration\n+ENV_EMBEDDING_MODEL = \"OPENROUTER_EMBEDDING_MODEL\"\n+ENV_EMBEDDING_DIMENSIONS = \"OPENROUTER_EMBEDDING_DIMENSIONS\"\n+ENV_EMBEDDING_BASE_URL = \"OPENROUTER_EMBEDDING_BASE_URL\"\n+ENV_EMBEDDING_API_KEY = \"OPENROUTER_EMBEDDING_API_KEY\"\n+\n+\n+@dataclass\n+class ModelConfig:\n+    \"\"\"Configuration for a specific model.\"\"\"\n+    model: str\n+    max_tokens: int\n+\n+\n+def get_env_int(key: str, default: int) -> int:\n+    \"\"\"Get integer value from environment variable with default.\"\"\"\n+    value = os.environ.get(key)\n+    if value:\n+        try:\n+            return int(value)\n+        except ValueError:\n+            pass\n+    return default\n+\n+\n+def get_model_config_for_schema(schema_type: type) -> ModelConfig:\n+    \"\"\"Get model configuration for a specific schema type from environment variables.\"\"\"\n+    # Default configurations\n+    defaults = {\n+        SingleToolBatchSchema: ModelConfig(\"openai/gpt-4o\", 128 * 1024),\n+        JourneyNodeSelectionSchema: ModelConfig(\"anthropic/claude-3.5-sonnet\", 200 * 1024),\n+        CannedResponseDraftSchema: ModelConfig(\"anthropic/claude-3.5-sonnet\", 200 * 1024),\n+        CannedResponseSelectionSchema: ModelConfig(\"anthropic/claude-3-haiku\", 200 * 1024),\n+    }\n+    \n+    # Environment variable mappings\n+    env_mappings = {\n+        SingleToolBatchSchema: (ENV_SINGLE_TOOL_MODEL, ENV_SINGLE_TOOL_MAX_TOKENS),\n+        JourneyNodeSelectionSchema: (ENV_JOURNEY_NODE_MODEL, ENV_JOURNEY_NODE_MAX_TOKENS),\n+        CannedResponseDraftSchema: (ENV_CANNED_RESPONSE_DRAFT_MODEL, ENV_CANNED_RESPONSE_DRAFT_MAX_TOKENS),\n+        CannedResponseSelectionSchema: (ENV_CANNED_RESPONSE_SELECTION_MODEL, ENV_CANNED_RESPONSE_SELECTION_MAX_TOKENS),\n+    }\n+    \n+    default_config = defaults.get(schema_type, defaults[SingleToolBatchSchema])\n+    \n+    if schema_type in env_mappings:\n+        model_env, tokens_env = env_mappings[schema_type]\n+        model = os.environ.get(model_env, default_config.model)\n+        max_tokens = get_env_int(tokens_env, default_config.max_tokens)\n+        return ModelConfig(model, max_tokens)\n+    \n+    return default_config\n+\n+\n+RATE_LIMIT_ERROR_MESSAGE = (\n+    \"OpenRouter API rate limit exceeded. Possible reasons:\\n\"\n+    \"1. Your account may have insufficient API credits.\\n\"\n+    \"2. You may be using a free-tier account with limited request capacity.\\n\"\n+    \"3. You might have exceeded the requests-per-minute limit for your account.\\n\\n\"\n+    \"Recommended actions:\\n\"\n+    \"- Check your OpenRouter account balance and billing status.\\n\"\n+    \"- Review your API usage limits in OpenRouter dashboard.\\n\"\n+    \"- For more details on rate limits and usage tiers, visit:\\n\"\n+    \"  https://openrouter.ai/docs\"\n+)\n+\n+\n+class OpenRouterEstimatingTokenizer(EstimatingTokenizer):\n+    def __init__(self, model_name: str) -> None:\n+        self.model_name = model_name\n+        # Use GPT-4 tokenizer as fallback for most models\n+        self.encoding = tiktoken.encoding_for_model(\"gpt-4o-2024-08-06\")\n+\n+    @override\n+    async def estimate_token_count(self, prompt: str) -> int:\n+        tokens = self.encoding.encode(prompt)\n+        return len(tokens)\n+\n+\n+class OpenRouterSchematicGenerator(SchematicGenerator[T]):\n+    supported_openrouter_params = [\"temperature\", \"max_tokens\", \"top_p\", \"frequency_penalty\", \"presence_penalty\"]\n+    supported_hints = supported_openrouter_params + [\"strict\"]\n+\n+    def __init__(\n+        self,\n+        model_name: str,\n+        max_tokens: int,\n+        logger: Logger,\n+        api_key: str,\n+        base_url: str = \"https://openrouter.ai/api/v1\",\n+    ) -> None:\n+        self._model_name = model_name\n+        self._max_tokens = max_tokens\n+        self._logger = logger\n+        self._api_key = api_key\n+        self._base_url = base_url\n+        \n+        # Initialize OpenAI client\n+        self._client = openai.AsyncOpenAI(\n+            api_key=api_key,\n+            base_url=base_url,\n+            default_headers={\n+                \"HTTP-Referer\": \"https://parlant.ai\",\n+                \"X-Title\": \"Parlant\",\n+            }\n+        )\n+        self._tokenizer = OpenRouterEstimatingTokenizer(model_name=self._model_name)\n+\n+    @property\n+    @override\n+    def id(self) -> str:\n+        return f\"{self._model_name}\"\n+\n+    @property\n+    @override\n+    def tokenizer(self) -> OpenRouterEstimatingTokenizer:\n+        return self._tokenizer\n+\n+    @property\n+    @override\n+    def max_tokens(self) -> int:\n+        return self._max_tokens\n+\n+    @policy(\n+        [\n+            retry(\n+                exceptions=(\n+                    openai.RateLimitError,\n+                    openai.APITimeoutError,\n+                    openai.APIError,\n+                ),\n+            ),\n+            retry(openai.APIError, max_exceptions=2, wait_times=(1.0, 5.0)),\n+        ]\n+    )\n+    @override\n+    async def generate(\n+        self,\n+        prompt: str | PromptBuilder,\n+        hints: Mapping[str, Any] = {},\n+    ) -> SchematicGenerationResult[T]:\n+        if isinstance(prompt, PromptBuilder):\n+            prompt = prompt.build()\n+\n+        openrouter_api_arguments = {k: v for k, v in hints.items() if k in self.supported_openrouter_params}\n+\n+        t_start = time.time()\n+        try:\n+            response = await self._client.chat.completions.create(\n+                model=self._model_name,\n+                messages=[{\"role\": \"user\", \"content\": prompt}],\n+                response_format={\"type\": \"json_object\"},\n+                **openrouter_api_arguments,\n+            )\n+        except openai.RateLimitError:\n+            self._logger.error(RATE_LIMIT_ERROR_MESSAGE)\n+            raise\n+\n+        t_end = time.time()\n+\n+        self._logger.trace(json.dumps(response.model_dump(), indent=2))\n+\n+        raw_content = response.choices[0].message.content or \"{}\"\n+\n+        try:\n+            json_content = json.loads(normalize_json_output(raw_content))\n+        except json.JSONDecodeError:\n+            self._logger.warning(\n+                f\"Invalid JSON returned by openrouter/{self._model_name}:\\n{raw_content}\"\n+            )\n+            json_content = jsonfinder.only_json(raw_content)[2]\n+            self._logger.warning(\"Found JSON content within model response; continuing...\")\n+\n+        try:\n+            content = self.schema.model_validate(json_content)\n+            usage_data = response.usage\n+\n+            return SchematicGenerationResult(\n+                content=content,\n+                info=GenerationInfo(\n+                    schema_name=self.schema.__name__,\n+                    model=self.id,\n+                    duration=(t_end - t_start),\n+                    usage=UsageInfo(\n+                        input_tokens=usage_data.prompt_tokens if usage_data else 0,\n+                        output_tokens=usage_data.completion_tokens if usage_data else 0,\n+                    ),\n+                ),\n+            )\n+        except ValidationError:\n+            self._logger.error(\n+                f\"JSON content returned by openrouter/{self._model_name} does not match expected schema:\\n{raw_content}\"\n+            )\n+            raise\n+\n+\n+class OpenRouterEmbedder(Embedder):\n+    \"\"\"\n+    Embedder that connects to third-party embedding services.\n+    \n+    Note: OpenRouter does not provide embedding services. This embedder is designed\n+    to work with any third-party provider that follows the OpenAI embedding API format,\n+    such as OpenAI, Together AI, or other compatible services.\n+    \"\"\"\n+    def __init__(self, logger: Logger) -> None:\n+        self._logger = logger\n+        \n+        # Get embedding configuration from environment variables\n+        self._embedding_model = os.environ.get(ENV_EMBEDDING_MODEL, \"openai/text-embedding-3-small\")\n+        self._embedding_dimensions = get_env_int(ENV_EMBEDDING_DIMENSIONS, 1536)\n+        \n+        # Get base URL and API key from environment\n+        base_url = os.environ.get(ENV_BASE_URL, \"https://openrouter.ai/api/v1\")\n+        api_key = os.environ.get(ENV_API_KEY, \"\")\n+        \n+        self._embedding_base_url = os.environ.get(ENV_EMBEDDING_BASE_URL, base_url)\n+        # Use specific embedding API key if provided, otherwise use main API key\n+        self._embedding_api_key = os.environ.get(ENV_EMBEDDING_API_KEY, api_key)\n+        \n+        if not self._embedding_api_key:\n+            raise ValueError(f\"Either {ENV_API_KEY} or {ENV_EMBEDDING_API_KEY} must be set\")\n+        \n+        # Initialize OpenAI client\n+        self._client = openai.AsyncOpenAI(\n+            api_key=self._embedding_api_key,\n+            base_url=self._embedding_base_url,\n+        )\n+        self._logger.info(f\"Initialized OpenRouterEmbedder with model: {self._embedding_model}\")\n+\n+    @property\n+    @override\n+    def id(self) -> str:\n+        return f\"{self._embedding_model}\"\n+\n+    @property\n+    @override\n+    def max_tokens(self) -> int:\n+        return 8192\n+\n+    @property\n+    @override\n+    def tokenizer(self) -> EstimatingTokenizer:\n+        return OpenRouterEstimatingTokenizer(self._embedding_model)\n+\n+    @property\n+    @override\n+    def dimensions(self) -> int:\n+        return self._embedding_dimensions\n+\n+    @policy(\n+        [\n+            retry(\n+                exceptions=(\n+                    openai.RateLimitError,\n+                    openai.APITimeoutError,\n+                    openai.APIError,\n+                ),\n+            ),\n+            retry(openai.APIError, max_exceptions=2, wait_times=(1.0, 5.0)),\n+        ]\n+    )\n+    @override\n+    async def embed(\n+        self,\n+        texts: list[str],\n+        hints: Mapping[str, Any] = {},\n+    ) -> EmbeddingResult:\n+        _ = hints  # Not used for OpenRouter\n+        \n+        response = await self._client.embeddings.create(\n+            model=self._embedding_model,\n+            input=texts,\n+        )\n+\n+        vectors = [data_point.embedding for data_point in response.data]\n+        return EmbeddingResult(vectors=vectors)\n+\n+\n+# Specific generator classes for different models/use cases\n+class OpenRouter_Default(OpenRouterSchematicGenerator[T]):\n+    \"\"\"Default OpenRouter generator using configuration from environment.\"\"\"\n+    def __init__(self, logger: Logger, api_key: str, base_url: str, model_config: ModelConfig) -> None:\n+        super().__init__(\n+            model_name=model_config.model,\n+            max_tokens=model_config.max_tokens,\n+            logger=logger,\n+            api_key=api_key,\n+            base_url=base_url,\n+        )\n+\n+\n+class OpenRouterService(NLPService):\n+    @staticmethod\n+    def verify_environment() -> str | None:",
        "comment_created_at": "2025-08-26T15:52:43+00:00",
        "comment_author": "Agam1997",
        "comment_body": "Consider adding other required ENV VARS as well -> like embedding model providers base url",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2288551439",
    "pr_number": 503,
    "pr_file": "src/parlant/adapters/nlp/azure_service.py",
    "created_at": "2025-08-20T15:32:39+00:00",
    "commented_code": "_client = AsyncAzureOpenAI(\n             api_key=os.environ[\"AZURE_API_KEY\"],\n             azure_endpoint=os.environ[\"AZURE_ENDPOINT\"],\n-            api_version=\"2024-08-01-preview\",\n+            api_version=os.environ[\"AZURE_API_VERSION\"] or \"2024-08-01-preview\",",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "2288551439",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 503,
        "pr_file": "src/parlant/adapters/nlp/azure_service.py",
        "discussion_id": "2288551439",
        "commented_code": "@@ -239,7 +239,7 @@ def __init__(self, logger: Logger) -> None:\n         _client = AsyncAzureOpenAI(\n             api_key=os.environ[\"AZURE_API_KEY\"],\n             azure_endpoint=os.environ[\"AZURE_ENDPOINT\"],\n-            api_version=\"2024-08-01-preview\",\n+            api_version=os.environ[\"AZURE_API_VERSION\"] or \"2024-08-01-preview\",",
        "comment_created_at": "2025-08-20T15:32:39+00:00",
        "comment_author": "mc-dorzo",
        "comment_body": "Please write os.environ.get(\"AZURE_API_VERSION\", \"2024-08-01-preview\") here and in all other places. Otherwise, this will raise a KeyError if the environment variable is not set.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1730261808",
    "pr_number": 57,
    "pr_file": "server/tests/e2e/test_server_cli.py",
    "created_at": "2024-08-25T06:43:01+00:00",
    "commented_code": "config_file=context.config_file,\n         )\n \n-        await asyncio.sleep(REASONABLE_AMOUNT_OF_TIME)\n+        await asyncio.sleep(10)",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "1730261808",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 57,
        "pr_file": "server/tests/e2e/test_server_cli.py",
        "discussion_id": "1730261808",
        "commented_code": "@@ -267,7 +267,7 @@ async def test_that_the_server_detects_and_conforms_to_a_state_change_if_told_to\n             config_file=context.config_file,\n         )\n \n-        await asyncio.sleep(REASONABLE_AMOUNT_OF_TIME)\n+        await asyncio.sleep(10)",
        "comment_created_at": "2024-08-25T06:43:01+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "Please add a global variable `EXTENDED_PERIOD_OF_TIME = 10`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1647151946",
    "pr_number": 5,
    "pr_file": "server/src/emcie/server/engines/alpha/coherence_checker.py",
    "created_at": "2024-06-20T08:10:31+00:00",
    "commented_code": "+from abc import ABC, abstractmethod\n+import asyncio\n+from datetime import datetime, timezone\n+from enum import Enum\n+from itertools import chain\n+import json\n+from typing import Iterable, NewType\n+\n+from more_itertools import chunked\n+from tenacity import retry, stop_after_attempt, wait_fixed\n+\n+from emcie.server.base import DefaultBaseModel\n+from emcie.server.core.guidelines import Guideline, GuidelineId\n+from emcie.server.engines.alpha.utils import duration_logger, make_llm_client\n+\n+CoherenceContradictionId = NewType(\"CoherenceContradictionId\", str)\n+\n+\n+class ContradictionType(Enum):\n+    HIERARCHICAL = \"Hierarchical Contradiction\"\n+    PARALLEL = \"Parallel Contradiction\"\n+    TEMPORAL = \"Temporal Contradiction\"\n+    CONTEXTUAL = \"Contextual Contradiction\"\n+\n+\n+class ContradictionTest(DefaultBaseModel):\n+    contradiction_type: ContradictionType\n+    existing_guideline_id: GuidelineId\n+    proposed_guideline_id: GuidelineId\n+    severity: int\n+    rationale: str\n+    creation_utc: datetime\n+\n+\n+def _remove_duplicate_contradictions(\n+    contradictions: Iterable[ContradictionTest],\n+) -> Iterable[ContradictionTest]:\n+    \"\"\"\n+    Filter unique contradictions based on the combination of existing and proposed guidelines.\n+\n+    Args:\n+        contradictions: Iterable of Contradiction objects to filter.\n+\n+    Returns:\n+        Iterable of unique Contradiction objects.\n+    \"\"\"\n+\n+    def _generate_key(g1_id: GuidelineId, g2_id: GuidelineId) -> tuple[GuidelineId, GuidelineId]:\n+        return tuple(sorted((g1_id, g2_id)))\n+\n+    seen_keys = set()\n+    unique_contradictions = []\n+    for contradiction in contradictions:\n+        key = _generate_key(\n+            contradiction.existing_guideline_id, contradiction.proposed_guideline_id\n+        )\n+        if key not in seen_keys:\n+            seen_keys.add(key)\n+            unique_contradictions.append(contradiction)\n+    return unique_contradictions\n+\n+\n+class ContradictionEvaluator(ABC):\n+    @abstractmethod\n+    async def evaluate(\n+        self,\n+        proposed_guidelines: Iterable[Guideline],\n+        existing_guidelines: Iterable[Guideline] = [],\n+    ) -> Iterable[ContradictionTest]: ...\n+\n+\n+class HierarchicalContradictionEvaluator(ContradictionEvaluator):\n+    def __init__(self) -> None:\n+        self.contradiction_type = ContradictionType.HIERARCHICAL\n+        self._llm_client = make_llm_client(\"openai\")\n+\n+    async def evaluate(\n+        self,\n+        proposed_guidelines: Iterable[Guideline],\n+        existing_guidelines: Iterable[Guideline] = [],\n+    ) -> Iterable[ContradictionTest]:\n+        batch_size = 5\n+        existing_guideline_list = list(existing_guidelines)\n+        proposed_guidelines_list = list(proposed_guidelines)\n+        tasks = []\n+\n+        for proposed_guideline in proposed_guidelines:\n+            filtered_existing_guidelines = [\n+                g\n+                for g in proposed_guidelines_list + existing_guideline_list\n+                if g.id != proposed_guideline.id\n+            ]\n+            guideline_batches = chunked(filtered_existing_guidelines, batch_size)\n+            tasks.extend(\n+                [\n+                    asyncio.create_task(self._process_proposed_guideline(proposed_guideline, batch))\n+                    for batch in guideline_batches\n+                ]\n+            )\n+        with duration_logger(\n+            f\"Evaluate hierarchical coherence contradictions for ({len(tasks)} batches)\"\n+        ):\n+            contradictions = chain.from_iterable(await asyncio.gather(*tasks))\n+\n+        distinct_contradictions = _remove_duplicate_contradictions(contradictions)\n+        return distinct_contradictions\n+\n+    async def _process_proposed_guideline(\n+        self,\n+        proposed_guideline: Guideline,\n+        existing_guidelines: Iterable[Guideline],\n+    ) -> Iterable[ContradictionTest]:\n+        prompt = self._format_contradiction_prompt(\n+            proposed_guideline,\n+            existing_guidelines,\n+        )\n+        contradictions = await self._generate_contradictions(prompt)\n+        return contradictions\n+\n+    def _format_contradiction_prompt(\n+        self,\n+        proposed_guideline: Guideline,\n+        existing_guidelines: Iterable[Guideline],\n+    ) -> str:\n+        existing_guidelines_string = \"\n\".join(\n+            f\"{i}) {{id: {g.id}, guideline: When {g.predicate}, then {g.content}}}\"\n+            for i, g in enumerate(existing_guidelines, start=1)\n+        )\n+        proposed_guideline_string = (\n+            f\"{{id: {proposed_guideline.id}, \"\n+            f\"guideline: When {proposed_guideline.predicate}, then {proposed_guideline.content}}}\"\n+        )\n+        return f\"\"\"\n+### Definition of Hierarchical Coherence Contradiction:\n+\n+Hierarchical Coherence Contradiction arises when there are multiple layers of guidelines, with one being more specific or detailed than the other. This type of Contradiction occurs when the application of a general guideline is contradicted by a more specific guideline under certain conditions, leading to inconsistencies in decision-making.\n+\n+**Objective**: Evaluate potential hierarchical contradictions between the set of existing guidelines and the proposed guideline.\n+\n+**Task Description**:\n+1. **Input**:\n+   - Existing Guidelines: ###\n+   {existing_guidelines_string}\n+   ###\n+    - Proposed Guidelines:###\n+   {proposed_guideline_string}\n+   ###\n+\n+2. **Process**:\n+   - For each guideline in the existing set, compare it with the proposed guideline.\n+   - Determine if there is a hierarchical contradiction, where the proposed guideline is more specific and directly contradicts a more general guideline from the existing set.\n+   - If no contradiction is detected, set the severity_level to 1 to indicate minimal or no contradiction.\n+\n+3. **Output**:\n+   - A list of results, each item detailing a potential contradiction, structured as follows:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"<ID of the existing guideline in the contradiction>\",\n+                 \"proposed_guideline_id\": \"<ID of the proposed guideline in the contradiction>\",\n+                 \"severity_level\": \"<Severity Level (1-10): Indicates the intensity of the contradiction arising from overlapping conditions>\"\n+                 \"rationale\": \"<Brief explanation of why the two guidelines have a hierarchical contradiction>\"\n+             }}\n+         ]\n+     }}\n+     ```\n+\n+### Examples of Evaluations:\n+\n+#### Example #1:\n+- **Foundational Guideline**: {{\"id\": 3, \"guideline\": \"When a customer orders any item, Then prioritize shipping based on customer loyalty level.\"}}\n+- **Proposed Guideline**: {{\"id\": 4, \"guideline\": \"When a customer orders a high-demand item, Then ship immediately, regardless of loyalty level.\"}}\n+- **Expected Result**:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"3\",\n+                 \"proposed_guideline_id\": \"4\",\n+                 \"severity_level\": 9,\n+                 \"rationale\": \"The guideline to immediately ship high-demand items directly contradicts the broader policy of prioritizing based on loyalty, leading to a situation where the specific scenario of high-demand items undermines the general loyalty prioritization.\"\n+             }}\n+         ]\n+     }}\n+     ```\n+\n+#### Example #2:\n+- **Foundational Guideline**: {{\"id\": 1, \"guideline\": \"When an employee qualifies for any reward, Then distribute rewards based on standard performance metrics.\"}}\n+- **Proposed Guideline**: {{\"id\": 2, \"guideline\": \"When an employee excels in a critical project, Then offer additional rewards beyond standard metrics.\"}}\n+- **Expected Result**:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"1\",\n+                 \"proposed_guideline_id\": \"2\",\n+                 \"severity_level\": 8,\n+                 \"rationale\": \"The policy to give additional rewards for critical project performance contradicts the general policy of standard performance metrics, creating a Contradiction where a specific achievement overlaps and supersedes the general reward system.\"\n+             }}\n+         ]\n+     }}\n+     ```\n+\n+#### Example #3:\n+- **Foundational Guideline**: {{\"id\": 5, \"guideline\": \"When a customer subscribes to a yearly plan, Then offer a 10% discount on the subscription fee.\"}}\n+- **Proposed Guideline**: {{\"id\": 6, \"guideline\": \"When a customer subscribes to any plan during a promotional period, Then offer an additional 5% discount on the subscription fee.\"}}\n+- **Expected Result**:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"5\",\n+                 \"proposed_guideline_id\": \"6\",\n+                 \"severity_level\": 1,\n+                 \"rationale\": \"The policies to offer discounts for yearly subscriptions and additional discounts during promotional periods complement each other rather than contradict. Both discounts can be applied simultaneously without undermining one another, enhancing the overall attractiveness of the subscription offers during promotions.\"\n+             }}\n+         ]\n+     }}\n+     ```\n+\n+#### Example #4:\n+- **Foundational Guideline**: {{\"id\": 7, \"guideline\": \"When there is a software update, Then deploy it within 48 hours.\"}}\n+- **Proposed Guideline**: {{\"id\": 8, \"guideline\": \"When a software update includes major changes affecting user interfaces, Then delay deployment for additional user training.\"}}\n+- **Expected Result**:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"7\",\n+                 \"proposed_guideline_id\": \"8\",\n+                 \"severity_level\": 9,\n+                 \"rationale\": \"The requirement for additional training for major UI changes contradicts the general guideline of rapid deployment for security updates, showing how a specific feature of an update (UI changes) can override a general security protocol.\"\n+             }}\n+         ]\n+     }}\n+     ```\n+        \"\"\"  # noqa\n+\n+    @retry(wait=wait_fixed(3.5), stop=stop_after_attempt(100))",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "1647151946",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 5,
        "pr_file": "server/src/emcie/server/engines/alpha/coherence_checker.py",
        "discussion_id": "1647151946",
        "commented_code": "@@ -0,0 +1,894 @@\n+from abc import ABC, abstractmethod\n+import asyncio\n+from datetime import datetime, timezone\n+from enum import Enum\n+from itertools import chain\n+import json\n+from typing import Iterable, NewType\n+\n+from more_itertools import chunked\n+from tenacity import retry, stop_after_attempt, wait_fixed\n+\n+from emcie.server.base import DefaultBaseModel\n+from emcie.server.core.guidelines import Guideline, GuidelineId\n+from emcie.server.engines.alpha.utils import duration_logger, make_llm_client\n+\n+CoherenceContradictionId = NewType(\"CoherenceContradictionId\", str)\n+\n+\n+class ContradictionType(Enum):\n+    HIERARCHICAL = \"Hierarchical Contradiction\"\n+    PARALLEL = \"Parallel Contradiction\"\n+    TEMPORAL = \"Temporal Contradiction\"\n+    CONTEXTUAL = \"Contextual Contradiction\"\n+\n+\n+class ContradictionTest(DefaultBaseModel):\n+    contradiction_type: ContradictionType\n+    existing_guideline_id: GuidelineId\n+    proposed_guideline_id: GuidelineId\n+    severity: int\n+    rationale: str\n+    creation_utc: datetime\n+\n+\n+def _remove_duplicate_contradictions(\n+    contradictions: Iterable[ContradictionTest],\n+) -> Iterable[ContradictionTest]:\n+    \"\"\"\n+    Filter unique contradictions based on the combination of existing and proposed guidelines.\n+\n+    Args:\n+        contradictions: Iterable of Contradiction objects to filter.\n+\n+    Returns:\n+        Iterable of unique Contradiction objects.\n+    \"\"\"\n+\n+    def _generate_key(g1_id: GuidelineId, g2_id: GuidelineId) -> tuple[GuidelineId, GuidelineId]:\n+        return tuple(sorted((g1_id, g2_id)))\n+\n+    seen_keys = set()\n+    unique_contradictions = []\n+    for contradiction in contradictions:\n+        key = _generate_key(\n+            contradiction.existing_guideline_id, contradiction.proposed_guideline_id\n+        )\n+        if key not in seen_keys:\n+            seen_keys.add(key)\n+            unique_contradictions.append(contradiction)\n+    return unique_contradictions\n+\n+\n+class ContradictionEvaluator(ABC):\n+    @abstractmethod\n+    async def evaluate(\n+        self,\n+        proposed_guidelines: Iterable[Guideline],\n+        existing_guidelines: Iterable[Guideline] = [],\n+    ) -> Iterable[ContradictionTest]: ...\n+\n+\n+class HierarchicalContradictionEvaluator(ContradictionEvaluator):\n+    def __init__(self) -> None:\n+        self.contradiction_type = ContradictionType.HIERARCHICAL\n+        self._llm_client = make_llm_client(\"openai\")\n+\n+    async def evaluate(\n+        self,\n+        proposed_guidelines: Iterable[Guideline],\n+        existing_guidelines: Iterable[Guideline] = [],\n+    ) -> Iterable[ContradictionTest]:\n+        batch_size = 5\n+        existing_guideline_list = list(existing_guidelines)\n+        proposed_guidelines_list = list(proposed_guidelines)\n+        tasks = []\n+\n+        for proposed_guideline in proposed_guidelines:\n+            filtered_existing_guidelines = [\n+                g\n+                for g in proposed_guidelines_list + existing_guideline_list\n+                if g.id != proposed_guideline.id\n+            ]\n+            guideline_batches = chunked(filtered_existing_guidelines, batch_size)\n+            tasks.extend(\n+                [\n+                    asyncio.create_task(self._process_proposed_guideline(proposed_guideline, batch))\n+                    for batch in guideline_batches\n+                ]\n+            )\n+        with duration_logger(\n+            f\"Evaluate hierarchical coherence contradictions for ({len(tasks)} batches)\"\n+        ):\n+            contradictions = chain.from_iterable(await asyncio.gather(*tasks))\n+\n+        distinct_contradictions = _remove_duplicate_contradictions(contradictions)\n+        return distinct_contradictions\n+\n+    async def _process_proposed_guideline(\n+        self,\n+        proposed_guideline: Guideline,\n+        existing_guidelines: Iterable[Guideline],\n+    ) -> Iterable[ContradictionTest]:\n+        prompt = self._format_contradiction_prompt(\n+            proposed_guideline,\n+            existing_guidelines,\n+        )\n+        contradictions = await self._generate_contradictions(prompt)\n+        return contradictions\n+\n+    def _format_contradiction_prompt(\n+        self,\n+        proposed_guideline: Guideline,\n+        existing_guidelines: Iterable[Guideline],\n+    ) -> str:\n+        existing_guidelines_string = \"\\n\".join(\n+            f\"{i}) {{id: {g.id}, guideline: When {g.predicate}, then {g.content}}}\"\n+            for i, g in enumerate(existing_guidelines, start=1)\n+        )\n+        proposed_guideline_string = (\n+            f\"{{id: {proposed_guideline.id}, \"\n+            f\"guideline: When {proposed_guideline.predicate}, then {proposed_guideline.content}}}\"\n+        )\n+        return f\"\"\"\n+### Definition of Hierarchical Coherence Contradiction:\n+\n+Hierarchical Coherence Contradiction arises when there are multiple layers of guidelines, with one being more specific or detailed than the other. This type of Contradiction occurs when the application of a general guideline is contradicted by a more specific guideline under certain conditions, leading to inconsistencies in decision-making.\n+\n+**Objective**: Evaluate potential hierarchical contradictions between the set of existing guidelines and the proposed guideline.\n+\n+**Task Description**:\n+1. **Input**:\n+   - Existing Guidelines: ###\n+   {existing_guidelines_string}\n+   ###\n+    - Proposed Guidelines:###\n+   {proposed_guideline_string}\n+   ###\n+\n+2. **Process**:\n+   - For each guideline in the existing set, compare it with the proposed guideline.\n+   - Determine if there is a hierarchical contradiction, where the proposed guideline is more specific and directly contradicts a more general guideline from the existing set.\n+   - If no contradiction is detected, set the severity_level to 1 to indicate minimal or no contradiction.\n+\n+3. **Output**:\n+   - A list of results, each item detailing a potential contradiction, structured as follows:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"<ID of the existing guideline in the contradiction>\",\n+                 \"proposed_guideline_id\": \"<ID of the proposed guideline in the contradiction>\",\n+                 \"severity_level\": \"<Severity Level (1-10): Indicates the intensity of the contradiction arising from overlapping conditions>\"\n+                 \"rationale\": \"<Brief explanation of why the two guidelines have a hierarchical contradiction>\"\n+             }}\n+         ]\n+     }}\n+     ```\n+\n+### Examples of Evaluations:\n+\n+#### Example #1:\n+- **Foundational Guideline**: {{\"id\": 3, \"guideline\": \"When a customer orders any item, Then prioritize shipping based on customer loyalty level.\"}}\n+- **Proposed Guideline**: {{\"id\": 4, \"guideline\": \"When a customer orders a high-demand item, Then ship immediately, regardless of loyalty level.\"}}\n+- **Expected Result**:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"3\",\n+                 \"proposed_guideline_id\": \"4\",\n+                 \"severity_level\": 9,\n+                 \"rationale\": \"The guideline to immediately ship high-demand items directly contradicts the broader policy of prioritizing based on loyalty, leading to a situation where the specific scenario of high-demand items undermines the general loyalty prioritization.\"\n+             }}\n+         ]\n+     }}\n+     ```\n+\n+#### Example #2:\n+- **Foundational Guideline**: {{\"id\": 1, \"guideline\": \"When an employee qualifies for any reward, Then distribute rewards based on standard performance metrics.\"}}\n+- **Proposed Guideline**: {{\"id\": 2, \"guideline\": \"When an employee excels in a critical project, Then offer additional rewards beyond standard metrics.\"}}\n+- **Expected Result**:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"1\",\n+                 \"proposed_guideline_id\": \"2\",\n+                 \"severity_level\": 8,\n+                 \"rationale\": \"The policy to give additional rewards for critical project performance contradicts the general policy of standard performance metrics, creating a Contradiction where a specific achievement overlaps and supersedes the general reward system.\"\n+             }}\n+         ]\n+     }}\n+     ```\n+\n+#### Example #3:\n+- **Foundational Guideline**: {{\"id\": 5, \"guideline\": \"When a customer subscribes to a yearly plan, Then offer a 10% discount on the subscription fee.\"}}\n+- **Proposed Guideline**: {{\"id\": 6, \"guideline\": \"When a customer subscribes to any plan during a promotional period, Then offer an additional 5% discount on the subscription fee.\"}}\n+- **Expected Result**:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"5\",\n+                 \"proposed_guideline_id\": \"6\",\n+                 \"severity_level\": 1,\n+                 \"rationale\": \"The policies to offer discounts for yearly subscriptions and additional discounts during promotional periods complement each other rather than contradict. Both discounts can be applied simultaneously without undermining one another, enhancing the overall attractiveness of the subscription offers during promotions.\"\n+             }}\n+         ]\n+     }}\n+     ```\n+\n+#### Example #4:\n+- **Foundational Guideline**: {{\"id\": 7, \"guideline\": \"When there is a software update, Then deploy it within 48 hours.\"}}\n+- **Proposed Guideline**: {{\"id\": 8, \"guideline\": \"When a software update includes major changes affecting user interfaces, Then delay deployment for additional user training.\"}}\n+- **Expected Result**:\n+     ```json\n+     {{\n+         \"hierarchical_coherence_contradictions\": [\n+             {{\n+                 \"existing_guideline_id\": \"7\",\n+                 \"proposed_guideline_id\": \"8\",\n+                 \"severity_level\": 9,\n+                 \"rationale\": \"The requirement for additional training for major UI changes contradicts the general guideline of rapid deployment for security updates, showing how a specific feature of an update (UI changes) can override a general security protocol.\"\n+             }}\n+         ]\n+     }}\n+     ```\n+        \"\"\"  # noqa\n+\n+    @retry(wait=wait_fixed(3.5), stop=stop_after_attempt(100))",
        "comment_created_at": "2024-06-20T08:10:31+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "For now I also suggest moving these numbers to module variables. They sound like the sort of thing we'd want to change all at once for all evaluators, rather than something that would be individually tailored for each evaluator.",
        "pr_file_module": null
      }
    ]
  }
]