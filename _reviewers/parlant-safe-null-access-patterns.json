[
  {
    "discussion_id": "2290661226",
    "pr_number": 505,
    "pr_file": "src/parlant/adapters/nlp/litellm_service.py",
    "created_at": "2025-08-21T10:50:57+00:00",
    "commented_code": "class LiteLLMService(NLPService):\n+    \n+    @staticmethod\n+    def verify_environment() -> str | None:\n+        \"\"\"Returns an error message if the environment is not set up correctly.\"\"\"\n+\n+        if not os.environ[\"LITELLM_PROVIDER_MODEL_NAME\"]:",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "2290661226",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 505,
        "pr_file": "src/parlant/adapters/nlp/litellm_service.py",
        "discussion_id": "2290661226",
        "commented_code": "@@ -194,6 +194,23 @@ def max_tokens(self) -> int:\n \n \n class LiteLLMService(NLPService):\n+    \n+    @staticmethod\n+    def verify_environment() -> str | None:\n+        \"\"\"Returns an error message if the environment is not set up correctly.\"\"\"\n+\n+        if not os.environ[\"LITELLM_PROVIDER_MODEL_NAME\"]:",
        "comment_created_at": "2025-08-21T10:50:57+00:00",
        "comment_author": "mc-dorzo",
        "comment_body": "Good work man, just please fix a KeyError bug here, since if there is no `\"LITELLM_PROVIDER_MODEL_NAME\" in os.environ` than it will raise error.\r\n\r\nwrite either (I prefer the first) `KEY not in os.environ` or `os.environ.get(KEY)` (do to the other env vars obviously)",
        "pr_file_module": null
      },
      {
        "comment_id": "2290695814",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 505,
        "pr_file": "src/parlant/adapters/nlp/litellm_service.py",
        "discussion_id": "2290661226",
        "commented_code": "@@ -194,6 +194,23 @@ def max_tokens(self) -> int:\n \n \n class LiteLLMService(NLPService):\n+    \n+    @staticmethod\n+    def verify_environment() -> str | None:\n+        \"\"\"Returns an error message if the environment is not set up correctly.\"\"\"\n+\n+        if not os.environ[\"LITELLM_PROVIDER_MODEL_NAME\"]:",
        "comment_created_at": "2025-08-21T11:06:22+00:00",
        "comment_author": "Agam1997",
        "comment_body": "Oh my bad, i see the problem now",
        "pr_file_module": null
      },
      {
        "comment_id": "2290700934",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 505,
        "pr_file": "src/parlant/adapters/nlp/litellm_service.py",
        "discussion_id": "2290661226",
        "commented_code": "@@ -194,6 +194,23 @@ def max_tokens(self) -> int:\n \n \n class LiteLLMService(NLPService):\n+    \n+    @staticmethod\n+    def verify_environment() -> str | None:\n+        \"\"\"Returns an error message if the environment is not set up correctly.\"\"\"\n+\n+        if not os.environ[\"LITELLM_PROVIDER_MODEL_NAME\"]:",
        "comment_created_at": "2025-08-21T11:08:41+00:00",
        "comment_author": "Agam1997",
        "comment_body": "Resolved",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1885733524",
    "pr_number": 181,
    "pr_file": "src/parlant/core/engines/alpha/engine.py",
    "created_at": "2024-12-15T15:04:51+00:00",
    "commented_code": "):\n                 message_generation_inspections.append(\n                     MessageGenerationInspection(\n-                        generation=event_generation_result.generation_info,\n+                        generation=event_generation_result.generations[0],",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "1885733524",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 181,
        "pr_file": "src/parlant/core/engines/alpha/engine.py",
        "discussion_id": "1885733524",
        "commented_code": "@@ -356,7 +356,7 @@ async def _do_process(\n             ):\n                 message_generation_inspections.append(\n                     MessageGenerationInspection(\n-                        generation=event_generation_result.generation_info,\n+                        generation=event_generation_result.generations[0],",
        "comment_created_at": "2024-12-15T15:04:51+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "It's incorrect to do `generations[0]` here. What if there were no generations? What if there was more than one? This is coincidental programming at this point.\r\n\r\nWhat really happened here was that `EventGenerationResult` took on a different contract with respect to tool calls and messages. So, instead of forcing them both still under one heading, maybe we don't need to have a reusable dataclass here anymore.\r\n\r\nRemove `event_generation.py`, add `MessageEventGenerationResult` under the message generation module, and `ToolEventGenerationResult` under the tool call generation module.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1842400784",
    "pr_number": 145,
    "pr_file": "src/parlant/core/engines/alpha/tool_caller.py",
    "created_at": "2024-11-14T15:15:39+00:00",
    "commented_code": "with self._logger.operation(\"Tool classification\"):\n             generation_info, inference_output = await self._run_inference(inference_prompt)\n \n-        tool_calls_that_need_to_run = [\n-            c for c in inference_output if not c.same_call_is_already_staged\n-        ]\n-\n         return generation_info, [\n             ToolCall(\n                 id=ToolCallId(generate_id()),\n                 tool_id=ToolId.from_string(tc.name),\n-                arguments=tc.arguments,\n+                arguments=cast(Mapping[str, JSONSerializable], tc.arguments),",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "1842400784",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 145,
        "pr_file": "src/parlant/core/engines/alpha/tool_caller.py",
        "discussion_id": "1842400784",
        "commented_code": "@@ -102,18 +102,14 @@ async def _get_id_tool_pairs(tool_ids: Sequence[ToolId]) -> Sequence[tuple[ToolI\n         with self._logger.operation(\"Tool classification\"):\n             generation_info, inference_output = await self._run_inference(inference_prompt)\n \n-        tool_calls_that_need_to_run = [\n-            c for c in inference_output if not c.same_call_is_already_staged\n-        ]\n-\n         return generation_info, [\n             ToolCall(\n                 id=ToolCallId(generate_id()),\n                 tool_id=ToolId.from_string(tc.name),\n-                arguments=tc.arguments,\n+                arguments=cast(Mapping[str, JSONSerializable], tc.arguments),",
        "comment_created_at": "2024-11-14T15:15:39+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "Please change this line to `arguments=tc.arguments or {},`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1759989307",
    "pr_number": 81,
    "pr_file": "server/src/emcie/server/core/generation/embedders.py",
    "created_at": "2024-09-15T07:57:51+00:00",
    "commented_code": "+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import os\n+from typing import Any, Optional, Sequence\n+\n+import openai\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.logger import Logger\n+\n+\n+@dataclass(frozen=True)\n+class EmbeddingResult:\n+    vectors: Sequence[Sequence[float]]\n+\n+\n+class Embedder(ABC):\n+    supported_arguments: list[str] = []\n+\n+    def __init__(self, logger: Logger) -> None:\n+        self.logger = logger\n+\n+    @abstractmethod\n+    async def embed(\n+        self,\n+        texts: list[str],\n+        hints: Optional[dict[str, Any]] = None,",
    "repo_full_name": "emcie-co/parlant",
    "discussion_comments": [
      {
        "comment_id": "1759989307",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 81,
        "pr_file": "server/src/emcie/server/core/generation/embedders.py",
        "discussion_id": "1759989307",
        "commented_code": "@@ -0,0 +1,121 @@\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import os\n+from typing import Any, Optional, Sequence\n+\n+import openai\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.logger import Logger\n+\n+\n+@dataclass(frozen=True)\n+class EmbeddingResult:\n+    vectors: Sequence[Sequence[float]]\n+\n+\n+class Embedder(ABC):\n+    supported_arguments: list[str] = []\n+\n+    def __init__(self, logger: Logger) -> None:\n+        self.logger = logger\n+\n+    @abstractmethod\n+    async def embed(\n+        self,\n+        texts: list[str],\n+        hints: Optional[dict[str, Any]] = None,",
        "comment_created_at": "2024-09-15T07:57:51+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "Change hints type to `dict[str, Any]` and set `{}` as default",
        "pr_file_module": null
      },
      {
        "comment_id": "1759990343",
        "repo_full_name": "emcie-co/parlant",
        "pr_number": 81,
        "pr_file": "server/src/emcie/server/core/generation/embedders.py",
        "discussion_id": "1759989307",
        "commented_code": "@@ -0,0 +1,121 @@\n+from abc import ABC, abstractmethod\n+from dataclasses import dataclass\n+import os\n+from typing import Any, Optional, Sequence\n+\n+import openai\n+from together import AsyncTogether  # type: ignore\n+\n+from emcie.server.logger import Logger\n+\n+\n+@dataclass(frozen=True)\n+class EmbeddingResult:\n+    vectors: Sequence[Sequence[float]]\n+\n+\n+class Embedder(ABC):\n+    supported_arguments: list[str] = []\n+\n+    def __init__(self, logger: Logger) -> None:\n+        self.logger = logger\n+\n+    @abstractmethod\n+    async def embed(\n+        self,\n+        texts: list[str],\n+        hints: Optional[dict[str, Any]] = None,",
        "comment_created_at": "2024-09-15T08:03:59+00:00",
        "comment_author": "kichanyurd",
        "comment_body": "And change `dict` to `Mapping`",
        "pr_file_module": null
      }
    ]
  }
]