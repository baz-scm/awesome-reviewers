[
  {
    "discussion_id": "1976526111",
    "pr_number": 21288,
    "pr_file": "py-polars/polars/io/database/_utils.py",
    "created_at": "2025-03-02T02:26:07+00:00",
    "commented_code": "partition_num: int | None = None,\n     protocol: str | None = None,\n     schema_overrides: SchemaDict | None = None,\n+    pre_execution_query: str | list[str] | None = None,\n ) -> DataFrame:\n     cx = import_optional(\"connectorx\")\n+\n+    if parse_version(cx.__version__) < (0, 4, 2) and pre_execution_query:\n+        msg = \"pre_execution_query is only supported in connectorx version 0.4.2 or later.\"\n+        raise ValueError(msg)\n+\n     try:\n-        return_type = \"arrow2\" if parse_version(cx.__version__) < (0, 4, 2) else \"arrow\"\n-        tbl = cx.read_sql(\n-            conn=connection_uri,\n-            query=query,\n-            return_type=return_type,\n-            partition_on=partition_on,\n-            partition_range=partition_range,\n-            partition_num=partition_num,\n-            protocol=protocol,\n-        )\n+        if parse_version(cx.__version__) < (0, 4, 2):\n+            tbl = cx.read_sql(\n+                conn=connection_uri,\n+                query=query,\n+                return_type=\"arrow2\",\n+                partition_on=partition_on,\n+                partition_range=partition_range,\n+                partition_num=partition_num,\n+                protocol=protocol,\n+            )\n+        else:\n+            tbl = cx.read_sql(\n+                conn=connection_uri,\n+                query=query,\n+                return_type=\"arrow\",\n+                partition_on=partition_on,\n+                partition_range=partition_range,\n+                partition_num=partition_num,\n+                protocol=protocol,\n+                pre_execution_query=pre_execution_query,\n+            )",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1976526111",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 21288,
        "pr_file": "py-polars/polars/io/database/_utils.py",
        "discussion_id": "1976526111",
        "commented_code": "@@ -60,19 +60,36 @@ def _read_sql_connectorx(\n     partition_num: int | None = None,\n     protocol: str | None = None,\n     schema_overrides: SchemaDict | None = None,\n+    pre_execution_query: str | list[str] | None = None,\n ) -> DataFrame:\n     cx = import_optional(\"connectorx\")\n+\n+    if parse_version(cx.__version__) < (0, 4, 2) and pre_execution_query:\n+        msg = \"pre_execution_query is only supported in connectorx version 0.4.2 or later.\"\n+        raise ValueError(msg)\n+\n     try:\n-        return_type = \"arrow2\" if parse_version(cx.__version__) < (0, 4, 2) else \"arrow\"\n-        tbl = cx.read_sql(\n-            conn=connection_uri,\n-            query=query,\n-            return_type=return_type,\n-            partition_on=partition_on,\n-            partition_range=partition_range,\n-            partition_num=partition_num,\n-            protocol=protocol,\n-        )\n+        if parse_version(cx.__version__) < (0, 4, 2):\n+            tbl = cx.read_sql(\n+                conn=connection_uri,\n+                query=query,\n+                return_type=\"arrow2\",\n+                partition_on=partition_on,\n+                partition_range=partition_range,\n+                partition_num=partition_num,\n+                protocol=protocol,\n+            )\n+        else:\n+            tbl = cx.read_sql(\n+                conn=connection_uri,\n+                query=query,\n+                return_type=\"arrow\",\n+                partition_on=partition_on,\n+                partition_range=partition_range,\n+                partition_num=partition_num,\n+                protocol=protocol,\n+                pre_execution_query=pre_execution_query,\n+            )",
        "comment_created_at": "2025-03-02T02:26:07+00:00",
        "comment_author": "mcrumiller",
        "comment_body": "Minor refactor, but this section would be a bit cleaner as:\r\n\r\n```python\r\ntry:\r\n    if pre_execution_query:\r\n        if parse_version(cx.__version__) < (0, 4, 2):\r\n            msg = \"pre_execution_query is only supported in connectorx version 0.4.2 or later.\"\r\n            raise ValueError(msg)\r\n        return_type = \"arrow2\"\r\n        pre_execution_args = {\"pre_execution_query\": pre_execution_query}\r\n    else:\r\n        return_type = \"arrow\"\r\n        pre_execution_args = {}\r\n    tbl = cx.read_sql(\r\n        conn=connection_uri,\r\n        query=query,\r\n        return_type=return_type,\r\n        partition_on=partition_on,\r\n        partition_range=partition_range,\r\n        partition_num=partition_num,\r\n        protocol=protocol,\r\n        **pre_execution_args,\r\n    )\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1981699762",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 21288,
        "pr_file": "py-polars/polars/io/database/_utils.py",
        "discussion_id": "1976526111",
        "commented_code": "@@ -60,19 +60,36 @@ def _read_sql_connectorx(\n     partition_num: int | None = None,\n     protocol: str | None = None,\n     schema_overrides: SchemaDict | None = None,\n+    pre_execution_query: str | list[str] | None = None,\n ) -> DataFrame:\n     cx = import_optional(\"connectorx\")\n+\n+    if parse_version(cx.__version__) < (0, 4, 2) and pre_execution_query:\n+        msg = \"pre_execution_query is only supported in connectorx version 0.4.2 or later.\"\n+        raise ValueError(msg)\n+\n     try:\n-        return_type = \"arrow2\" if parse_version(cx.__version__) < (0, 4, 2) else \"arrow\"\n-        tbl = cx.read_sql(\n-            conn=connection_uri,\n-            query=query,\n-            return_type=return_type,\n-            partition_on=partition_on,\n-            partition_range=partition_range,\n-            partition_num=partition_num,\n-            protocol=protocol,\n-        )\n+        if parse_version(cx.__version__) < (0, 4, 2):\n+            tbl = cx.read_sql(\n+                conn=connection_uri,\n+                query=query,\n+                return_type=\"arrow2\",\n+                partition_on=partition_on,\n+                partition_range=partition_range,\n+                partition_num=partition_num,\n+                protocol=protocol,\n+            )\n+        else:\n+            tbl = cx.read_sql(\n+                conn=connection_uri,\n+                query=query,\n+                return_type=\"arrow\",\n+                partition_on=partition_on,\n+                partition_range=partition_range,\n+                partition_num=partition_num,\n+                protocol=protocol,\n+                pre_execution_query=pre_execution_query,\n+            )",
        "comment_created_at": "2025-03-05T15:51:26+00:00",
        "comment_author": "jsjasonseba",
        "comment_body": "Agreed. Appreciate your feedback!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1663153115",
    "pr_number": 15018,
    "pr_file": "py-polars/polars/dataframe/frame.py",
    "created_at": "2024-07-02T20:49:13+00:00",
    "commented_code": "msg = f\"unrecognised connection type {connection!r}\"\n             raise TypeError(msg)\n \n+    def write_iceberg(\n+        self,\n+        table: pyiceberg.table.Table,\n+        mode: Literal[\"append\", \"overwrite\"],\n+    ) -> None:\n+        \"\"\"\n+        Write DataFrame to an Iceberg table.\n+\n+        Parameters\n+        ----------\n+        table\n+            The pyiceberg.table.Table object representing an Iceberg table.\n+        mode : {'append', 'overwrite'}\n+            How to handle existing data.\n+\n+            - If 'append', will add new data.\n+            - If 'overwrite', will replace table with new data.\n+\n+        \"\"\"\n+        data = self.to_arrow()\n+\n+        if mode == \"append\":\n+            table.append(data)\n+        else:\n+            table.overwrite(data)",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1663153115",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 15018,
        "pr_file": "py-polars/polars/dataframe/frame.py",
        "discussion_id": "1663153115",
        "commented_code": "@@ -3834,6 +3835,32 @@ def unpack_table_name(name: str) -> tuple[str | None, str | None, str]:\n             msg = f\"unrecognised connection type {connection!r}\"\n             raise TypeError(msg)\n \n+    def write_iceberg(\n+        self,\n+        table: pyiceberg.table.Table,\n+        mode: Literal[\"append\", \"overwrite\"],\n+    ) -> None:\n+        \"\"\"\n+        Write DataFrame to an Iceberg table.\n+\n+        Parameters\n+        ----------\n+        table\n+            The pyiceberg.table.Table object representing an Iceberg table.\n+        mode : {'append', 'overwrite'}\n+            How to handle existing data.\n+\n+            - If 'append', will add new data.\n+            - If 'overwrite', will replace table with new data.\n+\n+        \"\"\"\n+        data = self.to_arrow()\n+\n+        if mode == \"append\":\n+            table.append(data)\n+        else:\n+            table.overwrite(data)",
        "comment_created_at": "2024-07-02T20:49:13+00:00",
        "comment_author": "stinodego",
        "comment_body": "This logic is so simple that this does not warrant its own method, in my opinion.",
        "pr_file_module": null
      },
      {
        "comment_id": "1663187368",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 15018,
        "pr_file": "py-polars/polars/dataframe/frame.py",
        "discussion_id": "1663153115",
        "commented_code": "@@ -3834,6 +3835,32 @@ def unpack_table_name(name: str) -> tuple[str | None, str | None, str]:\n             msg = f\"unrecognised connection type {connection!r}\"\n             raise TypeError(msg)\n \n+    def write_iceberg(\n+        self,\n+        table: pyiceberg.table.Table,\n+        mode: Literal[\"append\", \"overwrite\"],\n+    ) -> None:\n+        \"\"\"\n+        Write DataFrame to an Iceberg table.\n+\n+        Parameters\n+        ----------\n+        table\n+            The pyiceberg.table.Table object representing an Iceberg table.\n+        mode : {'append', 'overwrite'}\n+            How to handle existing data.\n+\n+            - If 'append', will add new data.\n+            - If 'overwrite', will replace table with new data.\n+\n+        \"\"\"\n+        data = self.to_arrow()\n+\n+        if mode == \"append\":\n+            table.append(data)\n+        else:\n+            table.overwrite(data)",
        "comment_created_at": "2024-07-02T21:24:06+00:00",
        "comment_author": "kevinjqliu",
        "comment_body": "The main goal is to implement the top-level `write_iceberg` function for the dataframe API. The actual implementation code can be moved somewhere else",
        "pr_file_module": null
      },
      {
        "comment_id": "1665255686",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 15018,
        "pr_file": "py-polars/polars/dataframe/frame.py",
        "discussion_id": "1663153115",
        "commented_code": "@@ -3834,6 +3835,32 @@ def unpack_table_name(name: str) -> tuple[str | None, str | None, str]:\n             msg = f\"unrecognised connection type {connection!r}\"\n             raise TypeError(msg)\n \n+    def write_iceberg(\n+        self,\n+        table: pyiceberg.table.Table,\n+        mode: Literal[\"append\", \"overwrite\"],\n+    ) -> None:\n+        \"\"\"\n+        Write DataFrame to an Iceberg table.\n+\n+        Parameters\n+        ----------\n+        table\n+            The pyiceberg.table.Table object representing an Iceberg table.\n+        mode : {'append', 'overwrite'}\n+            How to handle existing data.\n+\n+            - If 'append', will add new data.\n+            - If 'overwrite', will replace table with new data.\n+\n+        \"\"\"\n+        data = self.to_arrow()\n+\n+        if mode == \"append\":\n+            table.append(data)\n+        else:\n+            table.overwrite(data)",
        "comment_created_at": "2024-07-04T07:25:27+00:00",
        "comment_author": "stinodego",
        "comment_body": "The point is that, if users already have an Iceberg table object, they can just write `table.append(df.to_arrow)`. It's even shorter than `df.write_iceberg(table, mode=\"append\")`. So there is not much added value to a `write_iceberg` method.\r\n\r\nIf there is some complex logic required to set up the iceberg table, or if `to_arrow` is not sufficient to handle all data types correctly, we can consider adding a `write_iceberg` to save all users some implementation hassle. But right now it doesn't seem warranted.",
        "pr_file_module": null
      },
      {
        "comment_id": "1666010649",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 15018,
        "pr_file": "py-polars/polars/dataframe/frame.py",
        "discussion_id": "1663153115",
        "commented_code": "@@ -3834,6 +3835,32 @@ def unpack_table_name(name: str) -> tuple[str | None, str | None, str]:\n             msg = f\"unrecognised connection type {connection!r}\"\n             raise TypeError(msg)\n \n+    def write_iceberg(\n+        self,\n+        table: pyiceberg.table.Table,\n+        mode: Literal[\"append\", \"overwrite\"],\n+    ) -> None:\n+        \"\"\"\n+        Write DataFrame to an Iceberg table.\n+\n+        Parameters\n+        ----------\n+        table\n+            The pyiceberg.table.Table object representing an Iceberg table.\n+        mode : {'append', 'overwrite'}\n+            How to handle existing data.\n+\n+            - If 'append', will add new data.\n+            - If 'overwrite', will replace table with new data.\n+\n+        \"\"\"\n+        data = self.to_arrow()\n+\n+        if mode == \"append\":\n+            table.append(data)\n+        else:\n+            table.overwrite(data)",
        "comment_created_at": "2024-07-04T19:05:20+00:00",
        "comment_author": "kevinjqliu",
        "comment_body": "That is true. Pyiceberg is well integrated with arrow. With an arrow dataframe and a PyIceberg table object, one can just invoke the pyiceberg write functions `.append`/`.overwrite`. \r\n\r\nGiven the above, is there still value in implementing a simple `write_iceberg` method? \r\n\r\nLooking at the [`write_delta`](https://github.com/pola-rs/polars/blob/f803053fb73abda45bd11ad25fc0a6e642e1038e/py-polars/polars/dataframe/frame.py#L3859-L4059) method, aside from the `merge` functionality, the function just pass data as arrow into the `write_deltalake` function.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1666011521",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 15018,
        "pr_file": "py-polars/polars/dataframe/frame.py",
        "discussion_id": "1663153115",
        "commented_code": "@@ -3834,6 +3835,32 @@ def unpack_table_name(name: str) -> tuple[str | None, str | None, str]:\n             msg = f\"unrecognised connection type {connection!r}\"\n             raise TypeError(msg)\n \n+    def write_iceberg(\n+        self,\n+        table: pyiceberg.table.Table,\n+        mode: Literal[\"append\", \"overwrite\"],\n+    ) -> None:\n+        \"\"\"\n+        Write DataFrame to an Iceberg table.\n+\n+        Parameters\n+        ----------\n+        table\n+            The pyiceberg.table.Table object representing an Iceberg table.\n+        mode : {'append', 'overwrite'}\n+            How to handle existing data.\n+\n+            - If 'append', will add new data.\n+            - If 'overwrite', will replace table with new data.\n+\n+        \"\"\"\n+        data = self.to_arrow()\n+\n+        if mode == \"append\":\n+            table.append(data)\n+        else:\n+            table.overwrite(data)",
        "comment_created_at": "2024-07-04T19:07:23+00:00",
        "comment_author": "kevinjqliu",
        "comment_body": "> If there is some complex logic required to set up the iceberg table\r\n\r\nThere's a scenario where a user might want to write an Iceberg table to a location in blob store. In such case, the `write_iceberg` function can take care of creating an in-memory catalog and iceberg table object before writing. \r\n\r\nThat was the initial version of this PR \r\nhttps://github.com/pola-rs/polars/pull/15018/commits/08010124c3a67fdd759df83006a7ffef758f4572\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1687381872",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 15018,
        "pr_file": "py-polars/polars/dataframe/frame.py",
        "discussion_id": "1663153115",
        "commented_code": "@@ -3834,6 +3835,32 @@ def unpack_table_name(name: str) -> tuple[str | None, str | None, str]:\n             msg = f\"unrecognised connection type {connection!r}\"\n             raise TypeError(msg)\n \n+    def write_iceberg(\n+        self,\n+        table: pyiceberg.table.Table,\n+        mode: Literal[\"append\", \"overwrite\"],\n+    ) -> None:\n+        \"\"\"\n+        Write DataFrame to an Iceberg table.\n+\n+        Parameters\n+        ----------\n+        table\n+            The pyiceberg.table.Table object representing an Iceberg table.\n+        mode : {'append', 'overwrite'}\n+            How to handle existing data.\n+\n+            - If 'append', will add new data.\n+            - If 'overwrite', will replace table with new data.\n+\n+        \"\"\"\n+        data = self.to_arrow()\n+\n+        if mode == \"append\":\n+            table.append(data)\n+        else:\n+            table.overwrite(data)",
        "comment_created_at": "2024-07-23T03:49:43+00:00",
        "comment_author": "glesperance",
        "comment_body": "Seems like a write_iceberg would be worth it just by virtue of having full support from polars / parity with delta. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1850499081",
    "pr_number": 19884,
    "pr_file": "py-polars/tests/unit/io/test_delta.py",
    "created_at": "2024-11-20T15:14:02+00:00",
    "commented_code": "expected = pl.DataFrame({\"name\": [\"Joey\", \"Ivan\"], \"age\": [14, 32]})\n     assert_frame_equal(expected, ldf.collect(), check_dtypes=False)\n+\n+\n+@pytest.mark.skip(\n+    reason=\"\"\"\\\n+Upstream bug writing empty tables \\\n+\"_internal.DeltaError: Generic error: No data source supplied to write command\"\n+Note this works if we downgrade to deltalake==0.18.2\n+\"\"\"\n+)\n+@pytest.mark.write_disk\n+def test_read_delta_empty(tmp_path: Path) -> None:\n+    tmp_path.mkdir(exist_ok=True)\n+    path = str(tmp_path)\n+    df = pl.DataFrame({}, [(\"p\", pl.Int64)])\n+    df.write_delta(path)",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1850499081",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19884,
        "pr_file": "py-polars/tests/unit/io/test_delta.py",
        "discussion_id": "1850499081",
        "commented_code": "@@ -487,3 +487,20 @@ def test_scan_delta_DT_input(delta_table_path: Path) -> None:\n \n     expected = pl.DataFrame({\"name\": [\"Joey\", \"Ivan\"], \"age\": [14, 32]})\n     assert_frame_equal(expected, ldf.collect(), check_dtypes=False)\n+\n+\n+@pytest.mark.skip(\n+    reason=\"\"\"\\\n+Upstream bug writing empty tables \\\n+\"_internal.DeltaError: Generic error: No data source supplied to write command\"\n+Note this works if we downgrade to deltalake==0.18.2\n+\"\"\"\n+)\n+@pytest.mark.write_disk\n+def test_read_delta_empty(tmp_path: Path) -> None:\n+    tmp_path.mkdir(exist_ok=True)\n+    path = str(tmp_path)\n+    df = pl.DataFrame({}, [(\"p\", pl.Int64)])\n+    df.write_delta(path)",
        "comment_created_at": "2024-11-20T15:14:02+00:00",
        "comment_author": "nameexhaustion",
        "comment_body": "@ion-elgreco , can you check this one? I get an error writing empty tables on deltalake 0.21.0\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1850658594",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19884,
        "pr_file": "py-polars/tests/unit/io/test_delta.py",
        "discussion_id": "1850499081",
        "commented_code": "@@ -487,3 +487,20 @@ def test_scan_delta_DT_input(delta_table_path: Path) -> None:\n \n     expected = pl.DataFrame({\"name\": [\"Joey\", \"Ivan\"], \"age\": [14, 32]})\n     assert_frame_equal(expected, ldf.collect(), check_dtypes=False)\n+\n+\n+@pytest.mark.skip(\n+    reason=\"\"\"\\\n+Upstream bug writing empty tables \\\n+\"_internal.DeltaError: Generic error: No data source supplied to write command\"\n+Note this works if we downgrade to deltalake==0.18.2\n+\"\"\"\n+)\n+@pytest.mark.write_disk\n+def test_read_delta_empty(tmp_path: Path) -> None:\n+    tmp_path.mkdir(exist_ok=True)\n+    path = str(tmp_path)\n+    df = pl.DataFrame({}, [(\"p\", pl.Int64)])\n+    df.write_delta(path)",
        "comment_created_at": "2024-11-20T16:42:53+00:00",
        "comment_author": "ion-elgreco",
        "comment_body": "You should create empty tables with something like this Deltatable.create(path, schema=df.to_arroe().schema)",
        "pr_file_module": null
      },
      {
        "comment_id": "1851275520",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19884,
        "pr_file": "py-polars/tests/unit/io/test_delta.py",
        "discussion_id": "1850499081",
        "commented_code": "@@ -487,3 +487,20 @@ def test_scan_delta_DT_input(delta_table_path: Path) -> None:\n \n     expected = pl.DataFrame({\"name\": [\"Joey\", \"Ivan\"], \"age\": [14, 32]})\n     assert_frame_equal(expected, ldf.collect(), check_dtypes=False)\n+\n+\n+@pytest.mark.skip(\n+    reason=\"\"\"\\\n+Upstream bug writing empty tables \\\n+\"_internal.DeltaError: Generic error: No data source supplied to write command\"\n+Note this works if we downgrade to deltalake==0.18.2\n+\"\"\"\n+)\n+@pytest.mark.write_disk\n+def test_read_delta_empty(tmp_path: Path) -> None:\n+    tmp_path.mkdir(exist_ok=True)\n+    path = str(tmp_path)\n+    df = pl.DataFrame({}, [(\"p\", pl.Int64)])\n+    df.write_delta(path)",
        "comment_created_at": "2024-11-21T03:49:37+00:00",
        "comment_author": "nameexhaustion",
        "comment_body": "I see - thanks for the tip!\r\n",
        "pr_file_module": null
      }
    ]
  }
]