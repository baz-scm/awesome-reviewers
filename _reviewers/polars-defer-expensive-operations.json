[
  {
    "discussion_id": "1860872791",
    "pr_number": 20019,
    "pr_file": "py-polars/polars/functions/eager.py",
    "created_at": "2024-11-27T15:31:59+00:00",
    "commented_code": ")\n             )\n         elif how == \"horizontal\":\n+            if strict:\n+                nrows = first.select(F.len()).collect()[0, 0]",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1860872791",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20019,
        "pr_file": "py-polars/polars/functions/eager.py",
        "discussion_id": "1860872791",
        "commented_code": "@@ -231,6 +240,14 @@ def concat(\n                 )\n             )\n         elif how == \"horizontal\":\n+            if strict:\n+                nrows = first.select(F.len()).collect()[0, 0]",
        "comment_created_at": "2024-11-27T15:31:59+00:00",
        "comment_author": "mcrumiller",
        "comment_body": "The reason this should be implemented on the rust side is that this `collect` here could trigger a massive computation if the query plan is complex, which then gets tossed. The check should be performed when the concatenation operation is actually applied.",
        "pr_file_module": null
      },
      {
        "comment_id": "1862741427",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20019,
        "pr_file": "py-polars/polars/functions/eager.py",
        "discussion_id": "1860872791",
        "commented_code": "@@ -231,6 +240,14 @@ def concat(\n                 )\n             )\n         elif how == \"horizontal\":\n+            if strict:\n+                nrows = first.select(F.len()).collect()[0, 0]",
        "comment_created_at": "2024-11-28T22:54:17+00:00",
        "comment_author": "nimit",
        "comment_body": "Understood. When I initially thought about it, I failed to take into account how I would compare the number of rows on Lazyframes.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2059957956",
    "pr_number": 22405,
    "pr_file": "py-polars/tests/unit/io/test_iceberg.py",
    "created_at": "2025-04-25T10:09:27+00:00",
    "commented_code": "def test_scan_iceberg_snapshot_id_not_found(self, iceberg_path: str) -> None:\n         with pytest.raises(ValueError, match=\"Snapshot ID not found\"):\n-            pl.scan_iceberg(iceberg_path, snapshot_id=1234567890)\n+            pl.scan_iceberg(iceberg_path, snapshot_id=1234567890).collect()",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "2059957956",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 22405,
        "pr_file": "py-polars/tests/unit/io/test_iceberg.py",
        "discussion_id": "2059957956",
        "commented_code": "@@ -56,7 +56,7 @@ def test_scan_iceberg_snapshot_id(self, iceberg_path: str) -> None:\n \n     def test_scan_iceberg_snapshot_id_not_found(self, iceberg_path: str) -> None:\n         with pytest.raises(ValueError, match=\"Snapshot ID not found\"):\n-            pl.scan_iceberg(iceberg_path, snapshot_id=1234567890)\n+            pl.scan_iceberg(iceberg_path, snapshot_id=1234567890).collect()",
        "comment_created_at": "2025-04-25T10:09:27+00:00",
        "comment_author": "nameexhaustion",
        "comment_body": "We currently eagerly query the dataset for the snapshot ID, but this PR defers all IO operations until collection time.\r\n\r\nNote I had to change the exception type because we are going through the Rust, I plan to change it back after https://github.com/pola-rs/polars/issues/22410.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1938285645",
    "pr_number": 20964,
    "pr_file": "py-polars/tests/benchmark/test_list_op.py",
    "created_at": "2025-02-01T14:37:09+00:00",
    "commented_code": "+from typing import Callable",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1938285645",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20964,
        "pr_file": "py-polars/tests/benchmark/test_list_op.py",
        "discussion_id": "1938285645",
        "commented_code": "@@ -0,0 +1,23 @@\n+from typing import Callable",
        "comment_created_at": "2025-02-01T14:37:09+00:00",
        "comment_author": "ritchie46",
        "comment_body": "I don't think we need an extra benchmark for this.",
        "pr_file_module": null
      },
      {
        "comment_id": "1939441535",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20964,
        "pr_file": "py-polars/tests/benchmark/test_list_op.py",
        "discussion_id": "1938285645",
        "commented_code": "@@ -0,0 +1,23 @@\n+from typing import Callable",
        "comment_created_at": "2025-02-03T14:11:11+00:00",
        "comment_author": "itamarst",
        "comment_body": "As far as I can tell there is no other Python benchmarks that covers list or array operations, at least. Is there a benchmark that covers `amortized_iter()` elsewhere?",
        "pr_file_module": null
      },
      {
        "comment_id": "1939760835",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20964,
        "pr_file": "py-polars/tests/benchmark/test_list_op.py",
        "discussion_id": "1938285645",
        "commented_code": "@@ -0,0 +1,23 @@\n+from typing import Callable",
        "comment_created_at": "2025-02-03T17:22:26+00:00",
        "comment_author": "ritchie46",
        "comment_body": "No, but the goal isn't to hit everything with benchmarks. ",
        "pr_file_module": null
      },
      {
        "comment_id": "1939780174",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20964,
        "pr_file": "py-polars/tests/benchmark/test_list_op.py",
        "discussion_id": "1938285645",
        "commented_code": "@@ -0,0 +1,23 @@\n+from typing import Callable",
        "comment_created_at": "2025-02-03T17:37:44+00:00",
        "comment_author": "itamarst",
        "comment_body": "I need to write an article about this, but just to give the gist of the argument: if you want to keep code fast, you really do want benchmarks on everything.\r\n\r\nIf the impacts of changes aren't visible, inevitably any project with significant ongoing development will result in some of those impacts happening. That's why I've written the memory usage tests, too, for example: bugs where scanning a CSV reads the whole thing into memory first are really easy to create because semantically it's identical.\r\n\r\nSame thing applies to performance.\r\n\r\nWhen I added benchmarks to a different project, mostly as part of adding optimizations, the end result was that it caught performance _regressions_ in PRs and prevented things from getting worse. Consider this particular PR: it's a subtle change, it's very easy for someone to tweak that code path in a way that makes things slow again. Maybe the comment will prevent this change, but there's no doubt plenty of other seemingly minor changes in e.g. `Series` APIs that won't matter most of the time, but will trash performance for anything using `amortized_iter()`.\r\n\r\nSo I would suggest that broad performance benchmark coverage is important to ensure performance doesn't get worse.\r\n\r\nThis particular test may not be the best way to do that, of course. In any case, if this still isn't convincing I'll go delete the benchmark.",
        "pr_file_module": null
      },
      {
        "comment_id": "1939842519",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20964,
        "pr_file": "py-polars/tests/benchmark/test_list_op.py",
        "discussion_id": "1938285645",
        "commented_code": "@@ -0,0 +1,23 @@\n+from typing import Callable",
        "comment_created_at": "2025-02-03T18:29:02+00:00",
        "comment_author": "ritchie46",
        "comment_body": "I understand what benchmarks do. :) I think the change is good, but I want to get rid of the in-repo benchmarks, I am not happy with them on the shared runners. We have on-premise benchmarks running. \r\n\r\nCan you remove this?",
        "pr_file_module": null
      },
      {
        "comment_id": "1939912713",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20964,
        "pr_file": "py-polars/tests/benchmark/test_list_op.py",
        "discussion_id": "1938285645",
        "commented_code": "@@ -0,0 +1,23 @@\n+from typing import Callable",
        "comment_created_at": "2025-02-03T19:25:40+00:00",
        "comment_author": "itamarst",
        "comment_body": "Done. Are those benchmarks public somewhere?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1846230143",
    "pr_number": 19844,
    "pr_file": "py-polars/tests/benchmark/test_with_columns.py",
    "created_at": "2024-11-18T10:04:32+00:00",
    "commented_code": "-import time\n+from time import perf_counter\n \n import pytest\n \n import polars as pl\n+import polars.selectors as cs\n \n \n # TODO: this is slow in streaming\n @pytest.mark.may_fail_auto_streaming\n+@pytest.mark.slow\n def test_with_columns_quadratic_19503() -> None:\n-    num_columns = 2000\n+    num_columns = 10_000\n     data1 = {f\"col_{i}\": [0] for i in range(num_columns)}\n     df1 = pl.DataFrame(data1)\n \n     data2 = {f\"feature_{i}\": [0] for i in range(num_columns)}\n     df2 = pl.DataFrame(data2)\n \n-    t0 = time.time()\n-    df1.with_columns(df2)\n-    t1 = time.time()\n-    assert t1 - t0 < 0.2\n+    df3 = df2.select(cs.by_index(range(num_columns // 1000)))\n+\n+    times = []  # [slow, fast]\n+\n+    class _:\n+        t = perf_counter()\n+        df1.with_columns(df2)\n+        times.append(perf_counter() - t)\n+\n+    class _:  # type: ignore[no-redef]\n+        t = perf_counter()\n+        df1.with_columns(df3)\n+        times.append(perf_counter() - t)\n+\n+    # Assert the relative rather than exact runtime to avoid flakiness in CI\n+    # From local testing, the fixed version was roughly 20x, while the quadratic\n+    # version was roughly 200x.",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1846230143",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19844,
        "pr_file": "py-polars/tests/benchmark/test_with_columns.py",
        "discussion_id": "1846230143",
        "commented_code": "@@ -1,21 +1,39 @@\n-import time\n+from time import perf_counter\n \n import pytest\n \n import polars as pl\n+import polars.selectors as cs\n \n \n # TODO: this is slow in streaming\n @pytest.mark.may_fail_auto_streaming\n+@pytest.mark.slow\n def test_with_columns_quadratic_19503() -> None:\n-    num_columns = 2000\n+    num_columns = 10_000\n     data1 = {f\"col_{i}\": [0] for i in range(num_columns)}\n     df1 = pl.DataFrame(data1)\n \n     data2 = {f\"feature_{i}\": [0] for i in range(num_columns)}\n     df2 = pl.DataFrame(data2)\n \n-    t0 = time.time()\n-    df1.with_columns(df2)\n-    t1 = time.time()\n-    assert t1 - t0 < 0.2\n+    df3 = df2.select(cs.by_index(range(num_columns // 1000)))\n+\n+    times = []  # [slow, fast]\n+\n+    class _:\n+        t = perf_counter()\n+        df1.with_columns(df2)\n+        times.append(perf_counter() - t)\n+\n+    class _:  # type: ignore[no-redef]\n+        t = perf_counter()\n+        df1.with_columns(df3)\n+        times.append(perf_counter() - t)\n+\n+    # Assert the relative rather than exact runtime to avoid flakiness in CI\n+    # From local testing, the fixed version was roughly 20x, while the quadratic\n+    # version was roughly 200x.",
        "comment_created_at": "2024-11-18T10:04:32+00:00",
        "comment_author": "nameexhaustion",
        "comment_body": "Side note: On EC2 I got ~13x vs ~150x for `factor`\r\n",
        "pr_file_module": null
      }
    ]
  }
]