[
  {
    "discussion_id": "1854665109",
    "pr_number": 19932,
    "pr_file": "crates/polars-lazy/src/frame/mod.rs",
    "created_at": "2024-11-22T21:11:49+00:00",
    "commented_code": "if let Expr::Column(name) = index_column {\n             options.index_column = name;\n         } else {\n+            fn is_int_range(expr: &Expr) -> bool {\n+                match expr {\n+                    Expr::Alias(input, _) => is_int_range(input),\n+                    Expr::Function {\n+                        input, function, ..\n+                    } => {\n+                        matches!(function, FunctionExpr::Range(f) if f.to_string() == \"int_range\")",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1854665109",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19932,
        "pr_file": "crates/polars-lazy/src/frame/mod.rs",
        "discussion_id": "1854665109",
        "commented_code": "@@ -1138,6 +1138,18 @@ impl LazyFrame {\n         if let Expr::Column(name) = index_column {\n             options.index_column = name;\n         } else {\n+            fn is_int_range(expr: &Expr) -> bool {\n+                match expr {\n+                    Expr::Alias(input, _) => is_int_range(input),\n+                    Expr::Function {\n+                        input, function, ..\n+                    } => {\n+                        matches!(function, FunctionExpr::Range(f) if f.to_string() == \"int_range\")",
        "comment_created_at": "2024-11-22T21:11:49+00:00",
        "comment_author": "jvdd",
        "comment_body": "A bit awkward that I have to check against the `.to_string` method.",
        "pr_file_module": null
      },
      {
        "comment_id": "1855199021",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19932,
        "pr_file": "crates/polars-lazy/src/frame/mod.rs",
        "discussion_id": "1854665109",
        "commented_code": "@@ -1138,6 +1138,18 @@ impl LazyFrame {\n         if let Expr::Column(name) = index_column {\n             options.index_column = name;\n         } else {\n+            fn is_int_range(expr: &Expr) -> bool {\n+                match expr {\n+                    Expr::Alias(input, _) => is_int_range(input),\n+                    Expr::Function {\n+                        input, function, ..\n+                    } => {\n+                        matches!(function, FunctionExpr::Range(f) if f.to_string() == \"int_range\")",
        "comment_created_at": "2024-11-23T13:56:34+00:00",
        "comment_author": "ritchie46",
        "comment_body": "You don't have to:\r\n\r\n\r\n```rust\r\n matches!(\r\n        function,\r\n        FunctionExpr::Range(RangeFunction::IntRange { .. })\r\n    );\r\n```\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1855199594",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19932,
        "pr_file": "crates/polars-lazy/src/frame/mod.rs",
        "discussion_id": "1854665109",
        "commented_code": "@@ -1138,6 +1138,18 @@ impl LazyFrame {\n         if let Expr::Column(name) = index_column {\n             options.index_column = name;\n         } else {\n+            fn is_int_range(expr: &Expr) -> bool {\n+                match expr {\n+                    Expr::Alias(input, _) => is_int_range(input),\n+                    Expr::Function {\n+                        input, function, ..\n+                    } => {\n+                        matches!(function, FunctionExpr::Range(f) if f.to_string() == \"int_range\")",
        "comment_created_at": "2024-11-23T14:01:23+00:00",
        "comment_author": "ritchie46",
        "comment_body": "I think we should do this check during the IR::conversion, in `resolve_groupby` here:\r\n\r\nhttps://github.com/pola-rs/polars/blob/05f2abbf1b7f76f0b34c3c552fc33aa6da186561/crates/polars-plan/src/plans/conversion/dsl_to_ir.rs#L1013",
        "pr_file_module": null
      },
      {
        "comment_id": "1855391159",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19932,
        "pr_file": "crates/polars-lazy/src/frame/mod.rs",
        "discussion_id": "1854665109",
        "commented_code": "@@ -1138,6 +1138,18 @@ impl LazyFrame {\n         if let Expr::Column(name) = index_column {\n             options.index_column = name;\n         } else {\n+            fn is_int_range(expr: &Expr) -> bool {\n+                match expr {\n+                    Expr::Alias(input, _) => is_int_range(input),\n+                    Expr::Function {\n+                        input, function, ..\n+                    } => {\n+                        matches!(function, FunctionExpr::Range(f) if f.to_string() == \"int_range\")",
        "comment_created_at": "2024-11-24T08:12:07+00:00",
        "comment_author": "jvdd",
        "comment_body": "Sure! I am just unsure how I should check in `resolve_group_by` whether the index of the group_by_dynamic is an int_range. Can I get the expression for this index (conveniently) from the `expr_arena`?",
        "pr_file_module": null
      },
      {
        "comment_id": "1855409134",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19932,
        "pr_file": "crates/polars-lazy/src/frame/mod.rs",
        "discussion_id": "1854665109",
        "commented_code": "@@ -1138,6 +1138,18 @@ impl LazyFrame {\n         if let Expr::Column(name) = index_column {\n             options.index_column = name;\n         } else {\n+            fn is_int_range(expr: &Expr) -> bool {\n+                match expr {\n+                    Expr::Alias(input, _) => is_int_range(input),\n+                    Expr::Function {\n+                        input, function, ..\n+                    } => {\n+                        matches!(function, FunctionExpr::Range(f) if f.to_string() == \"int_range\")",
        "comment_created_at": "2024-11-24T09:50:49+00:00",
        "comment_author": "ritchie46",
        "comment_body": "Ah, right. I see that we first do a `with_columns` here. We should store the `index` column on line 1155 and do the `with_column` rewrite during IR conversion.\r\n\r\nI understand it's a bit more than you anticipated. I can do the pre-work for that later if you like?",
        "pr_file_module": null
      },
      {
        "comment_id": "1860114047",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 19932,
        "pr_file": "crates/polars-lazy/src/frame/mod.rs",
        "discussion_id": "1854665109",
        "commented_code": "@@ -1138,6 +1138,18 @@ impl LazyFrame {\n         if let Expr::Column(name) = index_column {\n             options.index_column = name;\n         } else {\n+            fn is_int_range(expr: &Expr) -> bool {\n+                match expr {\n+                    Expr::Alias(input, _) => is_int_range(input),\n+                    Expr::Function {\n+                        input, function, ..\n+                    } => {\n+                        matches!(function, FunctionExpr::Range(f) if f.to_string() == \"int_range\")",
        "comment_created_at": "2024-11-27T07:52:13+00:00",
        "comment_author": "jvdd",
        "comment_body": "I suppose this involves passing the index_column `Expr` from the `lazy_frame.group_by_dynamic` method to the dsl_to_ir functions? However, the best approach to do this is not clear to me..\r\n\r\nWould thus be very helpful if you could do the pre-work for this! :)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2126083232",
    "pr_number": 22718,
    "pr_file": "crates/polars-ops/src/frame/pivot/mod.rs",
    "created_at": "2025-06-04T09:02:39+00:00",
    "commented_code": "let name = expr.root_name()?.clone();\n                             let mut value_col = value_col.clone();\n                             value_col.rename(name);\n-                            let tmp_df = value_col.into_frame();\n+                            let tmp_df = value_col\n+                                .into_frame()\n+                                .hstack(pivot_df.get_columns())\n+                                .unwrap();",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "2126083232",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 22718,
        "pr_file": "crates/polars-ops/src/frame/pivot/mod.rs",
        "discussion_id": "2126083232",
        "commented_code": "@@ -321,7 +321,10 @@ fn pivot_impl_single_column(\n                             let name = expr.root_name()?.clone();\n                             let mut value_col = value_col.clone();\n                             value_col.rename(name);\n-                            let tmp_df = value_col.into_frame();\n+                            let tmp_df = value_col\n+                                .into_frame()\n+                                .hstack(pivot_df.get_columns())\n+                                .unwrap();",
        "comment_created_at": "2025-06-04T09:02:39+00:00",
        "comment_author": "ritchie46",
        "comment_body": "Don't create `tmp_df` as what it does now.\r\n\r\nLet's say we have `pivot_df: A, B, C` and we create `value_col: A` from `pivot_df`. By concatting/hstacking we create:\r\n\r\n`tmp_df: A, A, B, C`. Where we just want to pass the context of `pivot_df` to the aggregation. So we can pass  `pivot_df` directly to `expr.evaluate`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2002747401",
    "pr_number": 21812,
    "pr_file": "crates/polars-stream/src/nodes/io_sources/multi_file_reader/extra_ops/apply.rs",
    "created_at": "2025-03-19T08:38:00+00:00",
    "commented_code": "+//! Implementation of applying the operations during execution.\n+use std::sync::Arc;\n+\n+use polars_core::frame::DataFrame;\n+use polars_core::frame::column::ScalarColumn;\n+use polars_core::prelude::{AnyValue, Column, DataType, IntoColumn};\n+use polars_core::scalar::Scalar;\n+use polars_core::schema::SchemaRef;\n+use polars_error::PolarsResult;\n+use polars_io::RowIndex;\n+use polars_io::predicates::ScanIOPredicate;\n+use polars_plan::dsl::ScanSource;\n+use polars_plan::plans::hive::HivePartitionsDf;\n+use polars_utils::IdxSize;\n+\n+use super::ExtraOperations;\n+use super::cast_columns::CastColumns;\n+use super::reorder_columns::ReorderColumns;\n+\n+/// Apply extra operations onto morsels originating from a reader. This should be initialized\n+/// per-reader (it contains e.g. file path).\n+#[derive(Debug)]\n+pub enum ApplyExtraOps {\n+    /// Intended to be initialized once, as we expect all morsels coming from a single reader to have\n+    /// the same schema. The initialized state can then be executed potentially in parallel by wrapping\n+    /// in Arc.\n+    Uninitialized {\n+        final_output_schema: SchemaRef,\n+        projected_file_schema: SchemaRef,\n+        extra_ops: ExtraOperations,\n+        /// This here so that we can get the include file path name if needed.\n+        scan_source: ScanSource,\n+        scan_source_idx: usize,\n+        hive_parts: Option<Arc<HivePartitionsDf>>,\n+    },\n+\n+    Initialized {\n+        // Note: These fields are ordered according to when they (should be) applied.\n+        row_index: Option<RowIndex>,\n+        cast_columns: Option<CastColumns>,\n+        /// This will have include_file_paths, hive columns, missing columns.\n+        extra_columns: Vec<ScalarColumn>,\n+        predicate: Option<ScanIOPredicate>,\n+        reorder: ReorderColumns,\n+    },\n+\n+    /// No-op.\n+    Noop,\n+}\n+\n+impl ApplyExtraOps {\n+    pub fn initialize(\n+        self,\n+        // Schema of the incoming morsels.\n+        incoming_schema: &SchemaRef,\n+    ) -> PolarsResult<Self> {\n+        use ApplyExtraOps::*;\n+        match self {\n+            Initialized { .. } => panic!(\"ApplyExtraOps already initialized\"),\n+            Noop => Ok(Noop),\n+\n+            Uninitialized {\n+                final_output_schema,\n+                projected_file_schema,\n+                extra_ops:\n+                    ExtraOperations {\n+                        row_index,\n+                        pre_slice,\n+                        cast_columns,\n+                        missing_columns,\n+                        include_file_paths,\n+                        predicate,\n+                    },\n+                scan_source,\n+                scan_source_idx,\n+                hive_parts,\n+            } => {\n+                // This should always be pushed to the reader, or otherwise handled separately.\n+                assert!(pre_slice.is_none());\n+\n+                let cast_columns = if let Some(policy) = cast_columns {\n+                    CastColumns::try_init_from_policy(\n+                        policy,\n+                        &final_output_schema,\n+                        incoming_schema,\n+                    )?\n+                } else {\n+                    None\n+                };\n+\n+                let n_expected_extra_columns = final_output_schema.len()\n+                    - incoming_schema.len()\n+                    - row_index.is_some() as usize;\n+\n+                let mut extra_columns: Vec<ScalarColumn> =\n+                    Vec::with_capacity(n_expected_extra_columns);\n+\n+                if let Some(policy) = missing_columns {\n+                    policy.initialize_policy(\n+                        &projected_file_schema,\n+                        incoming_schema,\n+                        &mut extra_columns,\n+                    )?;\n+                }\n+\n+                if let Some(hive_parts) = hive_parts {\n+                    extra_columns.extend(hive_parts.df().get_columns().iter().map(|c| {\n+                        c.new_from_index(scan_source_idx, 1)\n+                            .as_scalar_column()\n+                            .unwrap()\n+                            .clone()\n+                    }))\n+                }\n+\n+                if let Some(file_path_col) = include_file_paths {\n+                    extra_columns.push(ScalarColumn::new(\n+                        file_path_col,\n+                        Scalar::new(\n+                            DataType::String,\n+                            AnyValue::StringOwned(\n+                                scan_source\n+                                    .as_scan_source_ref()\n+                                    .to_include_path_name()\n+                                    .into(),\n+                            ),\n+                        ),\n+                        1,\n+                    ))\n+                }\n+\n+                // debug_assert_eq!(extra_columns.len(), n_expected_extra_columns);\n+\n+                let mut slf = Self::Initialized {\n+                    row_index,\n+                    cast_columns,\n+                    extra_columns,\n+                    predicate,\n+                    // Initialized below\n+                    reorder: ReorderColumns::Passthrough,\n+                };\n+\n+                let schema_before_reorder = if incoming_schema.len() == final_output_schema.len() {\n+                    // Incoming schema already has all of the columns, either because no extra columns were needed, or\n+                    // the extra columns were attached by the reader (which is just Parquet when it has a predicate).\n+                    incoming_schema.clone()\n+                } else {\n+                    // We use a trick to determine our schema state before reordering by applying onto an empty DataFrame.\n+                    // This is much less error prone compared determining it separately.\n+                    let mut df = DataFrame::empty_with_schema(incoming_schema);\n+                    slf.apply_to_df(&mut df, IdxSize::MAX)?;\n+                    df.schema().clone()\n+                };\n+\n+                if cfg!(debug_assertions)\n+                    && schema_before_reorder.len() != final_output_schema.len()\n+                {\n+                    assert_eq!(schema_before_reorder, final_output_schema);\n+                    unreachable!()\n+                }\n+\n+                let initialized_reorder =\n+                    ReorderColumns::initialize(&final_output_schema, &schema_before_reorder);\n+\n+                let Self::Initialized { reorder, .. } = &mut slf else {\n+                    unreachable!()\n+                };\n+\n+                *reorder = initialized_reorder;\n+\n+                // Return a `Noop` if our initialized state does not have any operations. Downstream\n+                // can see the `Noop` and avoid running through an extra distributor pipeline.\n+                let slf = match slf {\n+                    Initialized {\n+                        row_index: None,\n+                        cast_columns: None,\n+                        extra_columns,\n+                        predicate: None,\n+                        reorder: ReorderColumns::Passthrough,\n+                    } if extra_columns.is_empty() => Self::Noop,\n+\n+                    Initialized { .. } => slf,\n+\n+                    _ => unreachable!(),\n+                };\n+\n+                Ok(slf)\n+            },\n+        }\n+    }\n+\n+    /// # Panics\n+    /// Panics if `self` is `Uninitialized`\n+    pub fn apply_to_df(\n+        &self,\n+        df: &mut DataFrame,\n+        current_row_position: IdxSize,\n+    ) -> PolarsResult<()> {\n+        let Self::Initialized {\n+            row_index,\n+            cast_columns,\n+            extra_columns,\n+            predicate,\n+            reorder,\n+        } = ({\n+            use ApplyExtraOps::*;\n+\n+            match self {\n+                Noop => return Ok(()),\n+                Uninitialized { .. } => panic!(\"ApplyExtraOps not initialized\"),\n+                Initialized { .. } => self,\n+            }\n+        })\n+        else {\n+            unreachable!();\n+        };\n+\n+        if let Some(ri) = row_index {\n+            unsafe {\n+                df.with_column_unchecked(Column::new_row_index(\n+                    ri.name.clone(),\n+                    ri.offset.saturating_add(current_row_position),\n+                    df.height(),\n+                )?)\n+            };\n+            df.clear_schema();\n+        }\n+\n+        if let Some(cast_columns) = cast_columns {\n+            cast_columns.apply_cast(df)?;\n+        }\n+\n+        if !extra_columns.is_empty() {\n+            let h = df.height();\n+            let cols = unsafe { df.get_columns_mut() };\n+            cols.extend(extra_columns.iter().map(|c| c.resize(h).into_column()));\n+            df.clear_schema();\n+        }\n+\n+        if let Some(predicate) = predicate {\n+            let mask = predicate.predicate.evaluate_io(df)?;\n+            *df = df._filter_seq(mask.bool().expect(\"predicate not boolean\"))?;\n+        }\n+\n+        reorder.reorder_columns(df);\n+\n+        df.clear_schema();",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "2002747401",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 21812,
        "pr_file": "crates/polars-stream/src/nodes/io_sources/multi_file_reader/extra_ops/apply.rs",
        "discussion_id": "2002747401",
        "commented_code": "@@ -0,0 +1,250 @@\n+//! Implementation of applying the operations during execution.\n+use std::sync::Arc;\n+\n+use polars_core::frame::DataFrame;\n+use polars_core::frame::column::ScalarColumn;\n+use polars_core::prelude::{AnyValue, Column, DataType, IntoColumn};\n+use polars_core::scalar::Scalar;\n+use polars_core::schema::SchemaRef;\n+use polars_error::PolarsResult;\n+use polars_io::RowIndex;\n+use polars_io::predicates::ScanIOPredicate;\n+use polars_plan::dsl::ScanSource;\n+use polars_plan::plans::hive::HivePartitionsDf;\n+use polars_utils::IdxSize;\n+\n+use super::ExtraOperations;\n+use super::cast_columns::CastColumns;\n+use super::reorder_columns::ReorderColumns;\n+\n+/// Apply extra operations onto morsels originating from a reader. This should be initialized\n+/// per-reader (it contains e.g. file path).\n+#[derive(Debug)]\n+pub enum ApplyExtraOps {\n+    /// Intended to be initialized once, as we expect all morsels coming from a single reader to have\n+    /// the same schema. The initialized state can then be executed potentially in parallel by wrapping\n+    /// in Arc.\n+    Uninitialized {\n+        final_output_schema: SchemaRef,\n+        projected_file_schema: SchemaRef,\n+        extra_ops: ExtraOperations,\n+        /// This here so that we can get the include file path name if needed.\n+        scan_source: ScanSource,\n+        scan_source_idx: usize,\n+        hive_parts: Option<Arc<HivePartitionsDf>>,\n+    },\n+\n+    Initialized {\n+        // Note: These fields are ordered according to when they (should be) applied.\n+        row_index: Option<RowIndex>,\n+        cast_columns: Option<CastColumns>,\n+        /// This will have include_file_paths, hive columns, missing columns.\n+        extra_columns: Vec<ScalarColumn>,\n+        predicate: Option<ScanIOPredicate>,\n+        reorder: ReorderColumns,\n+    },\n+\n+    /// No-op.\n+    Noop,\n+}\n+\n+impl ApplyExtraOps {\n+    pub fn initialize(\n+        self,\n+        // Schema of the incoming morsels.\n+        incoming_schema: &SchemaRef,\n+    ) -> PolarsResult<Self> {\n+        use ApplyExtraOps::*;\n+        match self {\n+            Initialized { .. } => panic!(\"ApplyExtraOps already initialized\"),\n+            Noop => Ok(Noop),\n+\n+            Uninitialized {\n+                final_output_schema,\n+                projected_file_schema,\n+                extra_ops:\n+                    ExtraOperations {\n+                        row_index,\n+                        pre_slice,\n+                        cast_columns,\n+                        missing_columns,\n+                        include_file_paths,\n+                        predicate,\n+                    },\n+                scan_source,\n+                scan_source_idx,\n+                hive_parts,\n+            } => {\n+                // This should always be pushed to the reader, or otherwise handled separately.\n+                assert!(pre_slice.is_none());\n+\n+                let cast_columns = if let Some(policy) = cast_columns {\n+                    CastColumns::try_init_from_policy(\n+                        policy,\n+                        &final_output_schema,\n+                        incoming_schema,\n+                    )?\n+                } else {\n+                    None\n+                };\n+\n+                let n_expected_extra_columns = final_output_schema.len()\n+                    - incoming_schema.len()\n+                    - row_index.is_some() as usize;\n+\n+                let mut extra_columns: Vec<ScalarColumn> =\n+                    Vec::with_capacity(n_expected_extra_columns);\n+\n+                if let Some(policy) = missing_columns {\n+                    policy.initialize_policy(\n+                        &projected_file_schema,\n+                        incoming_schema,\n+                        &mut extra_columns,\n+                    )?;\n+                }\n+\n+                if let Some(hive_parts) = hive_parts {\n+                    extra_columns.extend(hive_parts.df().get_columns().iter().map(|c| {\n+                        c.new_from_index(scan_source_idx, 1)\n+                            .as_scalar_column()\n+                            .unwrap()\n+                            .clone()\n+                    }))\n+                }\n+\n+                if let Some(file_path_col) = include_file_paths {\n+                    extra_columns.push(ScalarColumn::new(\n+                        file_path_col,\n+                        Scalar::new(\n+                            DataType::String,\n+                            AnyValue::StringOwned(\n+                                scan_source\n+                                    .as_scan_source_ref()\n+                                    .to_include_path_name()\n+                                    .into(),\n+                            ),\n+                        ),\n+                        1,\n+                    ))\n+                }\n+\n+                // debug_assert_eq!(extra_columns.len(), n_expected_extra_columns);\n+\n+                let mut slf = Self::Initialized {\n+                    row_index,\n+                    cast_columns,\n+                    extra_columns,\n+                    predicate,\n+                    // Initialized below\n+                    reorder: ReorderColumns::Passthrough,\n+                };\n+\n+                let schema_before_reorder = if incoming_schema.len() == final_output_schema.len() {\n+                    // Incoming schema already has all of the columns, either because no extra columns were needed, or\n+                    // the extra columns were attached by the reader (which is just Parquet when it has a predicate).\n+                    incoming_schema.clone()\n+                } else {\n+                    // We use a trick to determine our schema state before reordering by applying onto an empty DataFrame.\n+                    // This is much less error prone compared determining it separately.\n+                    let mut df = DataFrame::empty_with_schema(incoming_schema);\n+                    slf.apply_to_df(&mut df, IdxSize::MAX)?;\n+                    df.schema().clone()\n+                };\n+\n+                if cfg!(debug_assertions)\n+                    && schema_before_reorder.len() != final_output_schema.len()\n+                {\n+                    assert_eq!(schema_before_reorder, final_output_schema);\n+                    unreachable!()\n+                }\n+\n+                let initialized_reorder =\n+                    ReorderColumns::initialize(&final_output_schema, &schema_before_reorder);\n+\n+                let Self::Initialized { reorder, .. } = &mut slf else {\n+                    unreachable!()\n+                };\n+\n+                *reorder = initialized_reorder;\n+\n+                // Return a `Noop` if our initialized state does not have any operations. Downstream\n+                // can see the `Noop` and avoid running through an extra distributor pipeline.\n+                let slf = match slf {\n+                    Initialized {\n+                        row_index: None,\n+                        cast_columns: None,\n+                        extra_columns,\n+                        predicate: None,\n+                        reorder: ReorderColumns::Passthrough,\n+                    } if extra_columns.is_empty() => Self::Noop,\n+\n+                    Initialized { .. } => slf,\n+\n+                    _ => unreachable!(),\n+                };\n+\n+                Ok(slf)\n+            },\n+        }\n+    }\n+\n+    /// # Panics\n+    /// Panics if `self` is `Uninitialized`\n+    pub fn apply_to_df(\n+        &self,\n+        df: &mut DataFrame,\n+        current_row_position: IdxSize,\n+    ) -> PolarsResult<()> {\n+        let Self::Initialized {\n+            row_index,\n+            cast_columns,\n+            extra_columns,\n+            predicate,\n+            reorder,\n+        } = ({\n+            use ApplyExtraOps::*;\n+\n+            match self {\n+                Noop => return Ok(()),\n+                Uninitialized { .. } => panic!(\"ApplyExtraOps not initialized\"),\n+                Initialized { .. } => self,\n+            }\n+        })\n+        else {\n+            unreachable!();\n+        };\n+\n+        if let Some(ri) = row_index {\n+            unsafe {\n+                df.with_column_unchecked(Column::new_row_index(\n+                    ri.name.clone(),\n+                    ri.offset.saturating_add(current_row_position),\n+                    df.height(),\n+                )?)\n+            };\n+            df.clear_schema();\n+        }\n+\n+        if let Some(cast_columns) = cast_columns {\n+            cast_columns.apply_cast(df)?;\n+        }\n+\n+        if !extra_columns.is_empty() {\n+            let h = df.height();\n+            let cols = unsafe { df.get_columns_mut() };\n+            cols.extend(extra_columns.iter().map(|c| c.resize(h).into_column()));\n+            df.clear_schema();\n+        }\n+\n+        if let Some(predicate) = predicate {\n+            let mask = predicate.predicate.evaluate_io(df)?;\n+            *df = df._filter_seq(mask.bool().expect(\"predicate not boolean\"))?;\n+        }\n+\n+        reorder.reorder_columns(df);\n+\n+        df.clear_schema();",
        "comment_created_at": "2025-03-19T08:38:00+00:00",
        "comment_author": "coastalwhite",
        "comment_body": "same here. Shouldn't reorder_columns clear the schema?",
        "pr_file_module": null
      },
      {
        "comment_id": "2002830843",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 21812,
        "pr_file": "crates/polars-stream/src/nodes/io_sources/multi_file_reader/extra_ops/apply.rs",
        "discussion_id": "2002747401",
        "commented_code": "@@ -0,0 +1,250 @@\n+//! Implementation of applying the operations during execution.\n+use std::sync::Arc;\n+\n+use polars_core::frame::DataFrame;\n+use polars_core::frame::column::ScalarColumn;\n+use polars_core::prelude::{AnyValue, Column, DataType, IntoColumn};\n+use polars_core::scalar::Scalar;\n+use polars_core::schema::SchemaRef;\n+use polars_error::PolarsResult;\n+use polars_io::RowIndex;\n+use polars_io::predicates::ScanIOPredicate;\n+use polars_plan::dsl::ScanSource;\n+use polars_plan::plans::hive::HivePartitionsDf;\n+use polars_utils::IdxSize;\n+\n+use super::ExtraOperations;\n+use super::cast_columns::CastColumns;\n+use super::reorder_columns::ReorderColumns;\n+\n+/// Apply extra operations onto morsels originating from a reader. This should be initialized\n+/// per-reader (it contains e.g. file path).\n+#[derive(Debug)]\n+pub enum ApplyExtraOps {\n+    /// Intended to be initialized once, as we expect all morsels coming from a single reader to have\n+    /// the same schema. The initialized state can then be executed potentially in parallel by wrapping\n+    /// in Arc.\n+    Uninitialized {\n+        final_output_schema: SchemaRef,\n+        projected_file_schema: SchemaRef,\n+        extra_ops: ExtraOperations,\n+        /// This here so that we can get the include file path name if needed.\n+        scan_source: ScanSource,\n+        scan_source_idx: usize,\n+        hive_parts: Option<Arc<HivePartitionsDf>>,\n+    },\n+\n+    Initialized {\n+        // Note: These fields are ordered according to when they (should be) applied.\n+        row_index: Option<RowIndex>,\n+        cast_columns: Option<CastColumns>,\n+        /// This will have include_file_paths, hive columns, missing columns.\n+        extra_columns: Vec<ScalarColumn>,\n+        predicate: Option<ScanIOPredicate>,\n+        reorder: ReorderColumns,\n+    },\n+\n+    /// No-op.\n+    Noop,\n+}\n+\n+impl ApplyExtraOps {\n+    pub fn initialize(\n+        self,\n+        // Schema of the incoming morsels.\n+        incoming_schema: &SchemaRef,\n+    ) -> PolarsResult<Self> {\n+        use ApplyExtraOps::*;\n+        match self {\n+            Initialized { .. } => panic!(\"ApplyExtraOps already initialized\"),\n+            Noop => Ok(Noop),\n+\n+            Uninitialized {\n+                final_output_schema,\n+                projected_file_schema,\n+                extra_ops:\n+                    ExtraOperations {\n+                        row_index,\n+                        pre_slice,\n+                        cast_columns,\n+                        missing_columns,\n+                        include_file_paths,\n+                        predicate,\n+                    },\n+                scan_source,\n+                scan_source_idx,\n+                hive_parts,\n+            } => {\n+                // This should always be pushed to the reader, or otherwise handled separately.\n+                assert!(pre_slice.is_none());\n+\n+                let cast_columns = if let Some(policy) = cast_columns {\n+                    CastColumns::try_init_from_policy(\n+                        policy,\n+                        &final_output_schema,\n+                        incoming_schema,\n+                    )?\n+                } else {\n+                    None\n+                };\n+\n+                let n_expected_extra_columns = final_output_schema.len()\n+                    - incoming_schema.len()\n+                    - row_index.is_some() as usize;\n+\n+                let mut extra_columns: Vec<ScalarColumn> =\n+                    Vec::with_capacity(n_expected_extra_columns);\n+\n+                if let Some(policy) = missing_columns {\n+                    policy.initialize_policy(\n+                        &projected_file_schema,\n+                        incoming_schema,\n+                        &mut extra_columns,\n+                    )?;\n+                }\n+\n+                if let Some(hive_parts) = hive_parts {\n+                    extra_columns.extend(hive_parts.df().get_columns().iter().map(|c| {\n+                        c.new_from_index(scan_source_idx, 1)\n+                            .as_scalar_column()\n+                            .unwrap()\n+                            .clone()\n+                    }))\n+                }\n+\n+                if let Some(file_path_col) = include_file_paths {\n+                    extra_columns.push(ScalarColumn::new(\n+                        file_path_col,\n+                        Scalar::new(\n+                            DataType::String,\n+                            AnyValue::StringOwned(\n+                                scan_source\n+                                    .as_scan_source_ref()\n+                                    .to_include_path_name()\n+                                    .into(),\n+                            ),\n+                        ),\n+                        1,\n+                    ))\n+                }\n+\n+                // debug_assert_eq!(extra_columns.len(), n_expected_extra_columns);\n+\n+                let mut slf = Self::Initialized {\n+                    row_index,\n+                    cast_columns,\n+                    extra_columns,\n+                    predicate,\n+                    // Initialized below\n+                    reorder: ReorderColumns::Passthrough,\n+                };\n+\n+                let schema_before_reorder = if incoming_schema.len() == final_output_schema.len() {\n+                    // Incoming schema already has all of the columns, either because no extra columns were needed, or\n+                    // the extra columns were attached by the reader (which is just Parquet when it has a predicate).\n+                    incoming_schema.clone()\n+                } else {\n+                    // We use a trick to determine our schema state before reordering by applying onto an empty DataFrame.\n+                    // This is much less error prone compared determining it separately.\n+                    let mut df = DataFrame::empty_with_schema(incoming_schema);\n+                    slf.apply_to_df(&mut df, IdxSize::MAX)?;\n+                    df.schema().clone()\n+                };\n+\n+                if cfg!(debug_assertions)\n+                    && schema_before_reorder.len() != final_output_schema.len()\n+                {\n+                    assert_eq!(schema_before_reorder, final_output_schema);\n+                    unreachable!()\n+                }\n+\n+                let initialized_reorder =\n+                    ReorderColumns::initialize(&final_output_schema, &schema_before_reorder);\n+\n+                let Self::Initialized { reorder, .. } = &mut slf else {\n+                    unreachable!()\n+                };\n+\n+                *reorder = initialized_reorder;\n+\n+                // Return a `Noop` if our initialized state does not have any operations. Downstream\n+                // can see the `Noop` and avoid running through an extra distributor pipeline.\n+                let slf = match slf {\n+                    Initialized {\n+                        row_index: None,\n+                        cast_columns: None,\n+                        extra_columns,\n+                        predicate: None,\n+                        reorder: ReorderColumns::Passthrough,\n+                    } if extra_columns.is_empty() => Self::Noop,\n+\n+                    Initialized { .. } => slf,\n+\n+                    _ => unreachable!(),\n+                };\n+\n+                Ok(slf)\n+            },\n+        }\n+    }\n+\n+    /// # Panics\n+    /// Panics if `self` is `Uninitialized`\n+    pub fn apply_to_df(\n+        &self,\n+        df: &mut DataFrame,\n+        current_row_position: IdxSize,\n+    ) -> PolarsResult<()> {\n+        let Self::Initialized {\n+            row_index,\n+            cast_columns,\n+            extra_columns,\n+            predicate,\n+            reorder,\n+        } = ({\n+            use ApplyExtraOps::*;\n+\n+            match self {\n+                Noop => return Ok(()),\n+                Uninitialized { .. } => panic!(\"ApplyExtraOps not initialized\"),\n+                Initialized { .. } => self,\n+            }\n+        })\n+        else {\n+            unreachable!();\n+        };\n+\n+        if let Some(ri) = row_index {\n+            unsafe {\n+                df.with_column_unchecked(Column::new_row_index(\n+                    ri.name.clone(),\n+                    ri.offset.saturating_add(current_row_position),\n+                    df.height(),\n+                )?)\n+            };\n+            df.clear_schema();\n+        }\n+\n+        if let Some(cast_columns) = cast_columns {\n+            cast_columns.apply_cast(df)?;\n+        }\n+\n+        if !extra_columns.is_empty() {\n+            let h = df.height();\n+            let cols = unsafe { df.get_columns_mut() };\n+            cols.extend(extra_columns.iter().map(|c| c.resize(h).into_column()));\n+            df.clear_schema();\n+        }\n+\n+        if let Some(predicate) = predicate {\n+            let mask = predicate.predicate.evaluate_io(df)?;\n+            *df = df._filter_seq(mask.bool().expect(\"predicate not boolean\"))?;\n+        }\n+\n+        reorder.reorder_columns(df);\n+\n+        df.clear_schema();",
        "comment_created_at": "2025-03-19T09:09:34+00:00",
        "comment_author": "nameexhaustion",
        "comment_body": "It won't if `reorder_columns` is a no-op, but in general this final clear_schema is just to be safe in terms of all the other things we did above.\r\n\r\nI'd prefer to just have it there, it helps me sleep at night 😄.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1899975579",
    "pr_number": 20508,
    "pr_file": "crates/polars-io/src/parquet/read/read_impl.rs",
    "created_at": "2024-12-31T08:12:04+00:00",
    "commented_code": ".sum();\n     let slice_end = slice.0 + slice.1;\n \n+    // we distinguish between the number of rows scanned and the number of rows actually",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1899975579",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20508,
        "pr_file": "crates/polars-io/src/parquet/read/read_impl.rs",
        "discussion_id": "1899975579",
        "commented_code": "@@ -689,15 +689,24 @@ fn rg_to_dfs_par_over_rg(\n         .sum();\n     let slice_end = slice.0 + slice.1;\n \n+    // we distinguish between the number of rows scanned and the number of rows actually",
        "comment_created_at": "2024-12-31T08:12:04+00:00",
        "comment_author": "ritchie46",
        "comment_body": "Could you clarify how you distinct \"scanned\" from \"read\"? \r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1900165649",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20508,
        "pr_file": "crates/polars-io/src/parquet/read/read_impl.rs",
        "discussion_id": "1899975579",
        "commented_code": "@@ -689,15 +689,24 @@ fn rg_to_dfs_par_over_rg(\n         .sum();\n     let slice_end = slice.0 + slice.1;\n \n+    // we distinguish between the number of rows scanned and the number of rows actually",
        "comment_created_at": "2024-12-31T15:32:45+00:00",
        "comment_author": "brifitz",
        "comment_body": "Maybe my terminology is not so clear, I'm happy to modify it to make it clearer if it's confusing. In the following, I'll explain what I mean by distinguishing between rows 'scanned' and 'read''\r\n\r\nNote: The 'slice' input parameter to `rg_to_dfs_par_over_rg()` varies depending on whether slice pushdown is enabled or not. If slice pushdown is enabled, the `slice` input parameter actually represents the rows we want to slice, i.e., starting index and number of rows to slice. If slice pushdown is not enabled, the `slice` parameter represents all the rows, and the slicing will be performed later on in the physical plan (`SliceExec`).\r\n\r\n**i) When slice_pushdown is not enabled:**\r\nWe iterate from `row_group_start` to `row_group_end`, and 'scan' through rg_md.num_rows() on each iteration. As slice pushdown is not enabled, in this case we are going to (later) 'read' all the rows into dataframes, which eventually get concatenated into a single dataframe. This dataframe will get sliced later on in the physical plan (`SliceExec`).\r\n\r\n**ii) When slice_pushdown is enabled:**\r\nWe iterate from `row_group_start` to `row_group_end`, and 'scan' through rg_md.num_rows() on each iteration. In this case however, since slice pushdown is enabled, `split_slice_at_file()` lets us know that we need to only 'read' the number of rows that overlap with the row groups into dataframes later on.\r\n\r\nThe number of rows 'scanned' and 'read' can differ in this case, i.e., we may have scanned through 10,000 rows but found that only 10 rows need to be read, as only 10 rows from the slice overlap with a given row group.\r\n\r\nOn the other hand, the number of rows scanned (`rows_scanned`) is needed to correctly set the offset the row index should start from for each row group, when adding the row index column to the dataframe (which is why we use it when setting `row_count_start`). I.e., we may have have read only a subset of rows into a dataframe out of all the rows scanned, but we need to account for all the rows that been scanned when specifying the row index offset or it will be incorrect.\r\n\r\n\r\n**iii) When slice_pushdown and async Parquet reader are enabled:**\r\nThis case is similar to ii), but there is an added complication. That is, `row_group_start` and `row_group_end` may have been modified due to processing that happens before `rg_to_dfs_par_over_rg()` executes (see call to `compute_row_group_range()` in `BatchedParquetReader.next_batches()`). Specifically, the number of row groups that need to be checked for overlap with the slice in `rg_to_dfs_par_over_rg()` may have been reduced by this (pre-)processing.\r\n\r\nThis occurs because each row group's start/end is compared against the slice start/end (which is essentially also an overlap check) in `compute_row_group_range()`, and this can reduce the range of row groups that need to be considered in `rg_to_dfs_par_over_rg()` to only those that overlap with the slice.\r\n\r\nSo, for example, `row_count_start` may not equal to `0` in `rg_to_dfs_par_over_rg()`. Essentially row groups have been skipped because of the earlier processing. But we need to account for that skipping in the number of rows scanned, or the row index that gets added to the dataframe will be incorrect. We need to sum up `file_metadata.row_groups[i]num_rows()` for `i in (0..row_group_start)`, to account for rows scanned/skipped due to the earlier processing.\r\n",
        "pr_file_module": null
      }
    ]
  }
]