[
  {
    "discussion_id": "2166679252",
    "pr_number": 23131,
    "pr_file": "crates/polars-stream/src/nodes/shift.rs",
    "created_at": "2025-06-25T13:09:07+00:00",
    "commented_code": "+use std::collections::VecDeque;\n+use std::sync::Arc;\n+\n+use polars_core::schema::Schema;\n+use polars_core::utils::Container;\n+use polars_error::polars_ensure;\n+\n+use super::compute_node_prelude::*;\n+use crate::async_primitives::connector::{Receiver, Sender};\n+use crate::morsel::SourceToken;\n+\n+pub struct ShiftNode {\n+    state: State,\n+    output_schema: Arc<Schema>,\n+}\n+\n+#[derive(Default)]\n+enum State {\n+    #[default]\n+    Uninit,\n+    PositiveHead {\n+        buffer: Buffer,\n+        head: usize,\n+        offset: usize,\n+        seq: u64,\n+    },\n+    PositiveFlush {\n+        buffer: Buffer,\n+        tail_shift: usize,\n+        seq: u64,\n+    },\n+    PositivePass {\n+        buffer: Buffer,\n+        tail_shift: usize,\n+        seq: u64,\n+    },\n+    Negative {\n+        // Rows to skip at the start of the DataFrame\n+        // This state will be decremented until it reaches 0\n+        // and we don't have to skip any rows anymore\n+        skip: usize,\n+        // Nulls to append at the end of the DataFrame\n+        tail: usize,\n+        seq: u64,\n+    },\n+    NegativeFlush {\n+        tail: usize,\n+        seq: u64,\n+    },\n+    Done,\n+}\n+\n+// Keeps track of the total number rows in the buffers\n+#[derive(Default)]\n+struct Buffer {\n+    inner: VecDeque<Morsel>,\n+    rows: usize,\n+}\n+\n+impl Buffer {\n+    fn new() -> Self {\n+        Self {\n+            inner: Default::default(),\n+            rows: 0,\n+        }\n+    }\n+\n+    fn len(&self) -> usize {\n+        self.rows\n+    }\n+\n+    fn len_after_pop_front(&self) -> usize {\n+        self.len()\n+            - self\n+                .inner\n+                .front()\n+                .map(|morsel| morsel.df().height())\n+                .unwrap_or(0)\n+    }\n+\n+    fn add(&mut self, morsel: &mut Morsel) {\n+        self.rows += morsel.df().height();\n+        let _ = morsel.take_consume_token();\n+    }\n+\n+    fn push_back(&mut self, mut morsel: Morsel) {\n+        self.add(&mut morsel);\n+        self.inner.push_back(morsel);\n+    }\n+    fn push_front(&mut self, mut morsel: Morsel) {\n+        self.add(&mut morsel);\n+        self.inner.push_front(morsel);\n+    }\n+    fn pop_front(&mut self) -> Morsel {\n+        let morsel = self.inner.pop_front().unwrap();\n+        self.rows -= morsel.df().height();\n+        morsel\n+    }\n+}\n+\n+/// Send a morsel and update the MorselSeq\n+async fn send_morsel(\n+    mut morsel: Morsel,\n+    sender: &mut Sender<Morsel>,\n+    seq: &mut u64,\n+) -> Result<(), Morsel> {\n+    *seq += 1;\n+\n+    morsel.set_seq(MorselSeq::new(*seq));\n+    sender.send(morsel).await\n+}\n+\n+impl ShiftNode {\n+    pub fn new(output_schema: Arc<Schema>) -> Self {\n+        Self {\n+            state: State::Uninit,\n+            output_schema,\n+        }\n+    }\n+}\n+\n+impl ComputeNode for ShiftNode {\n+    fn name(&self) -> &str {\n+        \"shift\"\n+    }\n+\n+    fn update_state(\n+        &mut self,\n+        recv: &mut [PortState],\n+        send: &mut [PortState],\n+        _state: &StreamingExecutionState,\n+    ) -> PolarsResult<()> {\n+        assert!(recv.len() == 2 && send.len() == 1);\n+\n+        if send[0] == PortState::Done {\n+            self.state = State::Done;\n+            return Ok(());\n+        }\n+\n+        // Handle offset\n+        if matches!(&self.state, State::Uninit) {\n+            recv[1] = PortState::Ready\n+        } else {\n+            recv[1] = PortState::Done\n+        }\n+\n+        match &mut self.state {",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "2166679252",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 23131,
        "pr_file": "crates/polars-stream/src/nodes/shift.rs",
        "discussion_id": "2166679252",
        "commented_code": "@@ -0,0 +1,474 @@\n+use std::collections::VecDeque;\n+use std::sync::Arc;\n+\n+use polars_core::schema::Schema;\n+use polars_core::utils::Container;\n+use polars_error::polars_ensure;\n+\n+use super::compute_node_prelude::*;\n+use crate::async_primitives::connector::{Receiver, Sender};\n+use crate::morsel::SourceToken;\n+\n+pub struct ShiftNode {\n+    state: State,\n+    output_schema: Arc<Schema>,\n+}\n+\n+#[derive(Default)]\n+enum State {\n+    #[default]\n+    Uninit,\n+    PositiveHead {\n+        buffer: Buffer,\n+        head: usize,\n+        offset: usize,\n+        seq: u64,\n+    },\n+    PositiveFlush {\n+        buffer: Buffer,\n+        tail_shift: usize,\n+        seq: u64,\n+    },\n+    PositivePass {\n+        buffer: Buffer,\n+        tail_shift: usize,\n+        seq: u64,\n+    },\n+    Negative {\n+        // Rows to skip at the start of the DataFrame\n+        // This state will be decremented until it reaches 0\n+        // and we don't have to skip any rows anymore\n+        skip: usize,\n+        // Nulls to append at the end of the DataFrame\n+        tail: usize,\n+        seq: u64,\n+    },\n+    NegativeFlush {\n+        tail: usize,\n+        seq: u64,\n+    },\n+    Done,\n+}\n+\n+// Keeps track of the total number rows in the buffers\n+#[derive(Default)]\n+struct Buffer {\n+    inner: VecDeque<Morsel>,\n+    rows: usize,\n+}\n+\n+impl Buffer {\n+    fn new() -> Self {\n+        Self {\n+            inner: Default::default(),\n+            rows: 0,\n+        }\n+    }\n+\n+    fn len(&self) -> usize {\n+        self.rows\n+    }\n+\n+    fn len_after_pop_front(&self) -> usize {\n+        self.len()\n+            - self\n+                .inner\n+                .front()\n+                .map(|morsel| morsel.df().height())\n+                .unwrap_or(0)\n+    }\n+\n+    fn add(&mut self, morsel: &mut Morsel) {\n+        self.rows += morsel.df().height();\n+        let _ = morsel.take_consume_token();\n+    }\n+\n+    fn push_back(&mut self, mut morsel: Morsel) {\n+        self.add(&mut morsel);\n+        self.inner.push_back(morsel);\n+    }\n+    fn push_front(&mut self, mut morsel: Morsel) {\n+        self.add(&mut morsel);\n+        self.inner.push_front(morsel);\n+    }\n+    fn pop_front(&mut self) -> Morsel {\n+        let morsel = self.inner.pop_front().unwrap();\n+        self.rows -= morsel.df().height();\n+        morsel\n+    }\n+}\n+\n+/// Send a morsel and update the MorselSeq\n+async fn send_morsel(\n+    mut morsel: Morsel,\n+    sender: &mut Sender<Morsel>,\n+    seq: &mut u64,\n+) -> Result<(), Morsel> {\n+    *seq += 1;\n+\n+    morsel.set_seq(MorselSeq::new(*seq));\n+    sender.send(morsel).await\n+}\n+\n+impl ShiftNode {\n+    pub fn new(output_schema: Arc<Schema>) -> Self {\n+        Self {\n+            state: State::Uninit,\n+            output_schema,\n+        }\n+    }\n+}\n+\n+impl ComputeNode for ShiftNode {\n+    fn name(&self) -> &str {\n+        \"shift\"\n+    }\n+\n+    fn update_state(\n+        &mut self,\n+        recv: &mut [PortState],\n+        send: &mut [PortState],\n+        _state: &StreamingExecutionState,\n+    ) -> PolarsResult<()> {\n+        assert!(recv.len() == 2 && send.len() == 1);\n+\n+        if send[0] == PortState::Done {\n+            self.state = State::Done;\n+            return Ok(());\n+        }\n+\n+        // Handle offset\n+        if matches!(&self.state, State::Uninit) {\n+            recv[1] = PortState::Ready\n+        } else {\n+            recv[1] = PortState::Done\n+        }\n+\n+        match &mut self.state {",
        "comment_created_at": "2025-06-25T13:09:07+00:00",
        "comment_author": "orlp",
        "comment_body": "You don't propagate information from your send port to the receive port. If the send port is blocked this will result in you essentially stating \"yes I'm ready to receive information\" even though you can't send it through.\r\n\r\nSimilarly you don't propagate information from your receive port to the send port. If the receive port is blocked you will try to pass information through which isn't there.\r\n\r\nI believe there are the following states in this state machine relevant for the send/receive ports:\r\n\r\n1. You are uninit (still waiting on the offset). Put `recv[0]/send[0]` to blocked and let the `InMemorySinkNode` handle `recv[1]`.\r\n\r\n2. You are flushing. Put `send[0]` to `Ready` (and assert that `recv[0]` is `Done`, because if not, why are you flushing?).\r\n\r\n3. In any other state `send[0]` and `recv[0]` should essentially be directly connected to each other. Just like the `update_state` of simple `MapNode`, in this case you simply swap `send[0]` with `recv[0]`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1980819175",
    "pr_number": 21598,
    "pr_file": "crates/polars-stream/src/nodes/io_sinks/csv.rs",
    "created_at": "2025-03-05T07:07:15+00:00",
    "commented_code": "writer.write_batch(&df)?;\n \n                     allocation_size = allocation_size.max(buffer.len());\n+\n+                    // Must drop before linearizer insert or will deadlock.\n+                    drop(consume_token); // Keep the consume_token until here to increase the",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1980819175",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 21598,
        "pr_file": "crates/polars-stream/src/nodes/io_sinks/csv.rs",
        "discussion_id": "1980819175",
        "commented_code": "@@ -122,11 +122,14 @@ impl SinkNode for CsvSinkNode {\n                     writer.write_batch(&df)?;\n \n                     allocation_size = allocation_size.max(buffer.len());\n+\n+                    // Must drop before linearizer insert or will deadlock.\n+                    drop(consume_token); // Keep the consume_token until here to increase the",
        "comment_created_at": "2025-03-05T07:07:15+00:00",
        "comment_author": "nameexhaustion",
        "comment_body": "If the consume token is sent into the linearizer, we can end up in a deadlock state for a `scan->sink` query  - given 2 workers from the source node (1 and 2):\r\n\r\n* Worker 1 inserts the consume token into the linearizer buffer\r\n* The linearizer is waiting for an insertion from worker 2\r\n* Worker 2 is waiting for the channel of the next source phase\r\n* Worker 1 is waiting for the consume token to be dropped while holding a ref to the current source phase, preventing it from ending",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1966701512",
    "pr_number": 20693,
    "pr_file": "crates/polars-python/src/lazyframe/general.rs",
    "created_at": "2025-02-23T08:05:15+00:00",
    "commented_code": "ldf.cache().into()\n     }\n \n-    fn profile(&self, py: Python) -> PyResult<(PyDataFrame, PyDataFrame)> {\n-        let (df, time_df) = py.enter_polars(|| self.ldf.clone().profile())?;\n+    #[pyo3(signature = (lambda_post_opt=None))]\n+    fn profile(\n+        &self,\n+        py: Python,\n+        lambda_post_opt: Option<PyObject>,\n+    ) -> PyResult<(PyDataFrame, PyDataFrame)> {\n+        // if we don't allow threads and we have udfs trying to acquire the gil from different\n+        // threads we deadlock.\n+        let (df, time_df) = py.allow_threads(|| {",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1966701512",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20693,
        "pr_file": "crates/polars-python/src/lazyframe/general.rs",
        "discussion_id": "1966701512",
        "commented_code": "@@ -613,8 +613,47 @@ impl PyLazyFrame {\n         ldf.cache().into()\n     }\n \n-    fn profile(&self, py: Python) -> PyResult<(PyDataFrame, PyDataFrame)> {\n-        let (df, time_df) = py.enter_polars(|| self.ldf.clone().profile())?;\n+    #[pyo3(signature = (lambda_post_opt=None))]\n+    fn profile(\n+        &self,\n+        py: Python,\n+        lambda_post_opt: Option<PyObject>,\n+    ) -> PyResult<(PyDataFrame, PyDataFrame)> {\n+        // if we don't allow threads and we have udfs trying to acquire the gil from different\n+        // threads we deadlock.\n+        let (df, time_df) = py.allow_threads(|| {",
        "comment_created_at": "2025-02-23T08:05:15+00:00",
        "comment_author": "ritchie46",
        "comment_body": "We should use the `enter_polars` which handles the `allow_threads`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1895657135",
    "pr_number": 20372,
    "pr_file": "crates/polars-stream/src/nodes/io_sources/parquet/init.rs",
    "created_at": "2024-12-23T11:42:43+00:00",
    "commented_code": "eprintln!(\"[ParquetSource]: ideal_morsel_size: {}\", ideal_morsel_size);\n         }\n \n-        // Distributes morsels across pipelines. This does not perform any CPU or I/O bound work -\n-        // it is purely a dispatch loop.\n-        let raw_morsel_distributor_task_handle = io_runtime.spawn(async move {\n+        // Prefetch loop (spawns prefetches on the tokio scheduler).\n+        let (prefetch_send, mut prefetch_recv) =\n+            tokio::sync::mpsc::channel(row_group_prefetch_size);\n+        let prefetch_task = io_runtime.spawn(async move {\n             let slice_range = {\n                 let Ok(slice) = normalized_slice_oneshot_rx.await else {\n                     // If we are here then the producer probably errored.\n                     drop(row_group_data_fetcher);\n-                    return metadata_task_handle.await.unwrap();\n+                    return PolarsResult::Ok(());\n                 };\n \n                 slice.map(|(offset, len)| offset..offset + len)\n             };\n \n             row_group_data_fetcher.slice_range = slice_range;\n \n-            // Ensure proper backpressure by only polling the buffered iterator when a wait group\n-            // is free.\n-            let mut wait_groups = (0..num_pipelines)\n-                .map(|index| IndexedWaitGroup::new(index).wait())\n-                .collect::<FuturesUnordered<_>>();\n-\n-            let mut df_stream = row_group_data_fetcher\n-                .into_stream()\n-                .map(|x| async {\n-                    match x {\n-                        Ok(handle) => handle.await.unwrap(),\n-                        Err(e) => Err(e),\n-                    }\n-                })\n-                .buffered(row_group_prefetch_size)\n-                .map(|x| async {\n-                    let row_group_decoder = row_group_decoder.clone();\n-\n-                    match x {\n-                        Ok(row_group_data) => {\n-                            async_executor::spawn(TaskPriority::Low, async move {\n-                                row_group_decoder.row_group_data_to_df(row_group_data).await\n-                            })\n-                            .await\n-                        },\n-                        Err(e) => Err(e),\n-                    }\n-                })\n-                .buffered(\n-                    // Because we are using an ordered buffer, we may suffer from head-of-line blocking,\n-                    // so we add a small amount of buffer.\n-                    num_pipelines + 4,\n-                );\n-\n-            let morsel_seq_ref = &mut MorselSeq::default();\n-            let mut dfs = VecDeque::with_capacity(1);\n-\n-            'main: loop {\n-                let Some(mut indexed_wait_group) = wait_groups.next().await else {\n+            loop {\n+                let Some(prefetch) = row_group_data_fetcher.next().await else {\n                     break;\n                 };\n+                if prefetch_send.send(prefetch?).await.is_err() {\n+                    break;\n+                }\n+            }\n+            PolarsResult::Ok(())\n+        });\n \n-                while dfs.is_empty() {\n-                    let Some(v) = df_stream.next().await else {\n-                        break 'main;\n-                    };\n-\n-                    let df = v?;\n-\n-                    if df.is_empty() {\n-                        continue;\n-                    }\n-\n-                    let (iter, n) = split_to_morsels(&df, ideal_morsel_size);\n+        // Decode loop (spawns decodes on the computational executor).\n+        let (decode_send, mut decode_recv) = tokio::sync::mpsc::channel(self.config.num_pipelines);\n+        let decode_task = io_runtime.spawn(async move {\n+            while let Some(prefetch) = prefetch_recv.recv().await {\n+                let row_group_data = prefetch.await.unwrap()?;\n+                let row_group_decoder = row_group_decoder.clone();\n+                let decode_fut = async_executor::spawn(TaskPriority::High, async move {\n+                    row_group_decoder.row_group_data_to_df(row_group_data).await\n+                });\n+                if decode_send.send(decode_fut).await.is_err() {\n+                    break;\n+                }\n+            }\n+            PolarsResult::Ok(())\n+        });\n \n-                    dfs.reserve(n);\n-                    dfs.extend(iter);\n+        // Distributes morsels across pipelines. This does not perform any CPU or I/O bound work -\n+        // it is purely a dispatch loop.\n+        let distribute_task = io_runtime.spawn(async move {\n+            let mut morsel_seq = MorselSeq::default();\n+            while let Some(decode_fut) = decode_recv.recv().await {\n+                let df = decode_fut.await?;\n+                if df.is_empty() {\n+                    continue;\n                 }\n \n-                let mut df = dfs.pop_front().unwrap();\n-                let morsel_seq = *morsel_seq_ref;\n-                *morsel_seq_ref = morsel_seq.successor();\n-\n-                loop {\n-                    use crate::async_primitives::connector::SendError;\n-\n-                    let channel_index = indexed_wait_group.index();\n-                    let wait_token = indexed_wait_group.token();\n-\n-                    match raw_morsel_senders[channel_index].try_send((df, morsel_seq, wait_token)) {\n-                        Ok(_) => {\n-                            wait_groups.push(indexed_wait_group.wait());\n-                            break;\n-                        },\n-                        Err(SendError::Closed(v)) => {\n-                            // The channel assigned to this wait group has been closed, so we will not\n-                            // add it back to the list of wait groups, and we will try to send this\n-                            // across another channel.\n-                            df = v.0\n-                        },\n-                        Err(SendError::Full(_)) => unreachable!(),\n+                for df in split_to_morsels(&df, ideal_morsel_size) {\n+                    if raw_morsel_sender.send((df, morsel_seq)).await.is_err() {\n+                        return Ok(());\n                     }\n-\n-                    let Some(v) = wait_groups.next().await else {\n-                        // All channels have closed\n-                        break 'main;\n-                    };\n-\n-                    indexed_wait_group = v;\n+                    morsel_seq = morsel_seq.successor();\n                 }\n             }\n-\n-            // Join on the producer handle to catch errors/panics.\n-            drop(df_stream);\n-            metadata_task_handle.await.unwrap()\n+            PolarsResult::Ok(())\n         });\n \n-        let raw_morsel_distributor_task_handle =\n-            task_handles_ext::AbortOnDropHandle(raw_morsel_distributor_task_handle);\n+        let join_task = io_runtime.spawn(async move {\n+            metadata_task.await.unwrap()?;\n+            prefetch_task.await.unwrap()?;\n+            decode_task.await.unwrap()?;\n+            distribute_task.await.unwrap()?;\n+            Ok(())\n+        });\n \n-        (raw_morsel_receivers, raw_morsel_distributor_task_handle)\n+        (raw_morsel_receivers, AbortOnDropHandle(join_task))",
    "repo_full_name": "pola-rs/polars",
    "discussion_comments": [
      {
        "comment_id": "1895657135",
        "repo_full_name": "pola-rs/polars",
        "pr_number": 20372,
        "pr_file": "crates/polars-stream/src/nodes/io_sources/parquet/init.rs",
        "discussion_id": "1895657135",
        "commented_code": "@@ -120,122 +114,78 @@ impl ParquetSourceNode {\n             eprintln!(\"[ParquetSource]: ideal_morsel_size: {}\", ideal_morsel_size);\n         }\n \n-        // Distributes morsels across pipelines. This does not perform any CPU or I/O bound work -\n-        // it is purely a dispatch loop.\n-        let raw_morsel_distributor_task_handle = io_runtime.spawn(async move {\n+        // Prefetch loop (spawns prefetches on the tokio scheduler).\n+        let (prefetch_send, mut prefetch_recv) =\n+            tokio::sync::mpsc::channel(row_group_prefetch_size);\n+        let prefetch_task = io_runtime.spawn(async move {\n             let slice_range = {\n                 let Ok(slice) = normalized_slice_oneshot_rx.await else {\n                     // If we are here then the producer probably errored.\n                     drop(row_group_data_fetcher);\n-                    return metadata_task_handle.await.unwrap();\n+                    return PolarsResult::Ok(());\n                 };\n \n                 slice.map(|(offset, len)| offset..offset + len)\n             };\n \n             row_group_data_fetcher.slice_range = slice_range;\n \n-            // Ensure proper backpressure by only polling the buffered iterator when a wait group\n-            // is free.\n-            let mut wait_groups = (0..num_pipelines)\n-                .map(|index| IndexedWaitGroup::new(index).wait())\n-                .collect::<FuturesUnordered<_>>();\n-\n-            let mut df_stream = row_group_data_fetcher\n-                .into_stream()\n-                .map(|x| async {\n-                    match x {\n-                        Ok(handle) => handle.await.unwrap(),\n-                        Err(e) => Err(e),\n-                    }\n-                })\n-                .buffered(row_group_prefetch_size)\n-                .map(|x| async {\n-                    let row_group_decoder = row_group_decoder.clone();\n-\n-                    match x {\n-                        Ok(row_group_data) => {\n-                            async_executor::spawn(TaskPriority::Low, async move {\n-                                row_group_decoder.row_group_data_to_df(row_group_data).await\n-                            })\n-                            .await\n-                        },\n-                        Err(e) => Err(e),\n-                    }\n-                })\n-                .buffered(\n-                    // Because we are using an ordered buffer, we may suffer from head-of-line blocking,\n-                    // so we add a small amount of buffer.\n-                    num_pipelines + 4,\n-                );\n-\n-            let morsel_seq_ref = &mut MorselSeq::default();\n-            let mut dfs = VecDeque::with_capacity(1);\n-\n-            'main: loop {\n-                let Some(mut indexed_wait_group) = wait_groups.next().await else {\n+            loop {\n+                let Some(prefetch) = row_group_data_fetcher.next().await else {\n                     break;\n                 };\n+                if prefetch_send.send(prefetch?).await.is_err() {\n+                    break;\n+                }\n+            }\n+            PolarsResult::Ok(())\n+        });\n \n-                while dfs.is_empty() {\n-                    let Some(v) = df_stream.next().await else {\n-                        break 'main;\n-                    };\n-\n-                    let df = v?;\n-\n-                    if df.is_empty() {\n-                        continue;\n-                    }\n-\n-                    let (iter, n) = split_to_morsels(&df, ideal_morsel_size);\n+        // Decode loop (spawns decodes on the computational executor).\n+        let (decode_send, mut decode_recv) = tokio::sync::mpsc::channel(self.config.num_pipelines);\n+        let decode_task = io_runtime.spawn(async move {\n+            while let Some(prefetch) = prefetch_recv.recv().await {\n+                let row_group_data = prefetch.await.unwrap()?;\n+                let row_group_decoder = row_group_decoder.clone();\n+                let decode_fut = async_executor::spawn(TaskPriority::High, async move {\n+                    row_group_decoder.row_group_data_to_df(row_group_data).await\n+                });\n+                if decode_send.send(decode_fut).await.is_err() {\n+                    break;\n+                }\n+            }\n+            PolarsResult::Ok(())\n+        });\n \n-                    dfs.reserve(n);\n-                    dfs.extend(iter);\n+        // Distributes morsels across pipelines. This does not perform any CPU or I/O bound work -\n+        // it is purely a dispatch loop.\n+        let distribute_task = io_runtime.spawn(async move {\n+            let mut morsel_seq = MorselSeq::default();\n+            while let Some(decode_fut) = decode_recv.recv().await {\n+                let df = decode_fut.await?;\n+                if df.is_empty() {\n+                    continue;\n                 }\n \n-                let mut df = dfs.pop_front().unwrap();\n-                let morsel_seq = *morsel_seq_ref;\n-                *morsel_seq_ref = morsel_seq.successor();\n-\n-                loop {\n-                    use crate::async_primitives::connector::SendError;\n-\n-                    let channel_index = indexed_wait_group.index();\n-                    let wait_token = indexed_wait_group.token();\n-\n-                    match raw_morsel_senders[channel_index].try_send((df, morsel_seq, wait_token)) {\n-                        Ok(_) => {\n-                            wait_groups.push(indexed_wait_group.wait());\n-                            break;\n-                        },\n-                        Err(SendError::Closed(v)) => {\n-                            // The channel assigned to this wait group has been closed, so we will not\n-                            // add it back to the list of wait groups, and we will try to send this\n-                            // across another channel.\n-                            df = v.0\n-                        },\n-                        Err(SendError::Full(_)) => unreachable!(),\n+                for df in split_to_morsels(&df, ideal_morsel_size) {\n+                    if raw_morsel_sender.send((df, morsel_seq)).await.is_err() {\n+                        return Ok(());\n                     }\n-\n-                    let Some(v) = wait_groups.next().await else {\n-                        // All channels have closed\n-                        break 'main;\n-                    };\n-\n-                    indexed_wait_group = v;\n+                    morsel_seq = morsel_seq.successor();\n                 }\n             }\n-\n-            // Join on the producer handle to catch errors/panics.\n-            drop(df_stream);\n-            metadata_task_handle.await.unwrap()\n+            PolarsResult::Ok(())\n         });\n \n-        let raw_morsel_distributor_task_handle =\n-            task_handles_ext::AbortOnDropHandle(raw_morsel_distributor_task_handle);\n+        let join_task = io_runtime.spawn(async move {\n+            metadata_task.await.unwrap()?;\n+            prefetch_task.await.unwrap()?;\n+            decode_task.await.unwrap()?;\n+            distribute_task.await.unwrap()?;\n+            Ok(())\n+        });\n \n-        (raw_morsel_receivers, raw_morsel_distributor_task_handle)\n+        (raw_morsel_receivers, AbortOnDropHandle(join_task))",
        "comment_created_at": "2024-12-23T11:42:43+00:00",
        "comment_author": "nameexhaustion",
        "comment_body": "Wrapping this `join_task` `AbortOnDropHandle` here doesn't cancel all of the previous tasks - the handles for those tasks should be individually wrapped in `AbortOnDropHandle` as soon as they are created.\r\n",
        "pr_file_module": null
      }
    ]
  }
]