[
  {
    "discussion_id": "2275986490",
    "pr_number": 36622,
    "pr_file": "plugin-server/src/cdp/consumers/cdp-aggregation-writer.consumer.ts",
    "created_at": "2025-08-14T08:59:40+00:00",
    "commented_code": "return\n         }\n \n-        let query = ''\n-        try {\n-            const ctes: string[] = []\n-            const allParams: any[] = []\n-            let paramOffset = 1\n-\n-            // Add CTEs for each event type\n-            if (personEvents.length > 0) {\n-                const { cte, params } = this.buildPersonEventsCTE(personEvents, paramOffset)\n-                ctes.push(cte)\n-                allParams.push(...params)\n-                paramOffset += params.length // person events use 3 array parameters\n+        // Group events by teamId to hit only one partition per query\n+        const personEventsByTeam = new Map<number, PersonEventPayload[]>()\n+        const behaviouralEventsByTeam = new Map<number, AggregatedBehaviouralEvent[]>()\n+\n+        for (const event of personEvents) {\n+            if (!personEventsByTeam.has(event.teamId)) {\n+                personEventsByTeam.set(event.teamId, [])\n             }\n-            if (behaviouralEvents.length > 0) {\n-                const { cte, params } = this.buildBehaviouralEventsCTE(behaviouralEvents, paramOffset)\n-                ctes.push(cte)\n-                allParams.push(...params)\n-                // No need to update paramOffset here since we're done\n+            personEventsByTeam.get(event.teamId)!.push(event)\n+        }\n+\n+        for (const event of behaviouralEvents) {\n+            if (!behaviouralEventsByTeam.has(event.teamId)) {\n+                behaviouralEventsByTeam.set(event.teamId, [])\n             }\n+            behaviouralEventsByTeam.get(event.teamId)!.push(event)\n+        }\n \n-            // Build and execute the single combined query with parameters\n-            query = `WITH ${ctes.join(', ')} SELECT 1`\n-            await this.hub.postgres.query(PostgresUse.COUNTERS_RW, query, allParams, 'counters-batch-upsert')\n-        } catch (error: any) {\n-            logger.error('Failed to write to COUNTERS postgres', {\n-                error: error.message,\n-                errorCode: error.code,\n-                query: query,\n-                personEventsCount: personEvents.length,\n-                behaviouralEventsCount: behaviouralEvents.length,\n-            })\n-            throw error\n+        // Get all unique team IDs\n+        const allTeamIds = new Set([...personEventsByTeam.keys(), ...behaviouralEventsByTeam.keys()])",
    "repo_full_name": "PostHog/posthog",
    "discussion_comments": [
      {
        "comment_id": "2275986490",
        "repo_full_name": "PostHog/posthog",
        "pr_number": 36622,
        "pr_file": "plugin-server/src/cdp/consumers/cdp-aggregation-writer.consumer.ts",
        "discussion_id": "2275986490",
        "commented_code": "@@ -169,38 +169,70 @@ export class CdpAggregationWriterConsumer extends CdpConsumerBase {\n             return\n         }\n \n-        let query = ''\n-        try {\n-            const ctes: string[] = []\n-            const allParams: any[] = []\n-            let paramOffset = 1\n-\n-            // Add CTEs for each event type\n-            if (personEvents.length > 0) {\n-                const { cte, params } = this.buildPersonEventsCTE(personEvents, paramOffset)\n-                ctes.push(cte)\n-                allParams.push(...params)\n-                paramOffset += params.length // person events use 3 array parameters\n+        // Group events by teamId to hit only one partition per query\n+        const personEventsByTeam = new Map<number, PersonEventPayload[]>()\n+        const behaviouralEventsByTeam = new Map<number, AggregatedBehaviouralEvent[]>()\n+\n+        for (const event of personEvents) {\n+            if (!personEventsByTeam.has(event.teamId)) {\n+                personEventsByTeam.set(event.teamId, [])\n             }\n-            if (behaviouralEvents.length > 0) {\n-                const { cte, params } = this.buildBehaviouralEventsCTE(behaviouralEvents, paramOffset)\n-                ctes.push(cte)\n-                allParams.push(...params)\n-                // No need to update paramOffset here since we're done\n+            personEventsByTeam.get(event.teamId)!.push(event)\n+        }\n+\n+        for (const event of behaviouralEvents) {\n+            if (!behaviouralEventsByTeam.has(event.teamId)) {\n+                behaviouralEventsByTeam.set(event.teamId, [])\n             }\n+            behaviouralEventsByTeam.get(event.teamId)!.push(event)\n+        }\n \n-            // Build and execute the single combined query with parameters\n-            query = `WITH ${ctes.join(', ')} SELECT 1`\n-            await this.hub.postgres.query(PostgresUse.COUNTERS_RW, query, allParams, 'counters-batch-upsert')\n-        } catch (error: any) {\n-            logger.error('Failed to write to COUNTERS postgres', {\n-                error: error.message,\n-                errorCode: error.code,\n-                query: query,\n-                personEventsCount: personEvents.length,\n-                behaviouralEventsCount: behaviouralEvents.length,\n-            })\n-            throw error\n+        // Get all unique team IDs\n+        const allTeamIds = new Set([...personEventsByTeam.keys(), ...behaviouralEventsByTeam.keys()])",
        "comment_created_at": "2025-08-14T08:59:40+00:00",
        "comment_author": "pauldambra",
        "comment_body": "we've had a problem with spread and large arrays elsewhere... could it hit us here...\r\nwe have the team ids up above as we group things so we could gather team ids at the same time and avoid this operation?",
        "pr_file_module": null
      },
      {
        "comment_id": "2276003999",
        "repo_full_name": "PostHog/posthog",
        "pr_number": 36622,
        "pr_file": "plugin-server/src/cdp/consumers/cdp-aggregation-writer.consumer.ts",
        "discussion_id": "2275986490",
        "commented_code": "@@ -169,38 +169,70 @@ export class CdpAggregationWriterConsumer extends CdpConsumerBase {\n             return\n         }\n \n-        let query = ''\n-        try {\n-            const ctes: string[] = []\n-            const allParams: any[] = []\n-            let paramOffset = 1\n-\n-            // Add CTEs for each event type\n-            if (personEvents.length > 0) {\n-                const { cte, params } = this.buildPersonEventsCTE(personEvents, paramOffset)\n-                ctes.push(cte)\n-                allParams.push(...params)\n-                paramOffset += params.length // person events use 3 array parameters\n+        // Group events by teamId to hit only one partition per query\n+        const personEventsByTeam = new Map<number, PersonEventPayload[]>()\n+        const behaviouralEventsByTeam = new Map<number, AggregatedBehaviouralEvent[]>()\n+\n+        for (const event of personEvents) {\n+            if (!personEventsByTeam.has(event.teamId)) {\n+                personEventsByTeam.set(event.teamId, [])\n             }\n-            if (behaviouralEvents.length > 0) {\n-                const { cte, params } = this.buildBehaviouralEventsCTE(behaviouralEvents, paramOffset)\n-                ctes.push(cte)\n-                allParams.push(...params)\n-                // No need to update paramOffset here since we're done\n+            personEventsByTeam.get(event.teamId)!.push(event)\n+        }\n+\n+        for (const event of behaviouralEvents) {\n+            if (!behaviouralEventsByTeam.has(event.teamId)) {\n+                behaviouralEventsByTeam.set(event.teamId, [])\n             }\n+            behaviouralEventsByTeam.get(event.teamId)!.push(event)\n+        }\n \n-            // Build and execute the single combined query with parameters\n-            query = `WITH ${ctes.join(', ')} SELECT 1`\n-            await this.hub.postgres.query(PostgresUse.COUNTERS_RW, query, allParams, 'counters-batch-upsert')\n-        } catch (error: any) {\n-            logger.error('Failed to write to COUNTERS postgres', {\n-                error: error.message,\n-                errorCode: error.code,\n-                query: query,\n-                personEventsCount: personEvents.length,\n-                behaviouralEventsCount: behaviouralEvents.length,\n-            })\n-            throw error\n+        // Get all unique team IDs\n+        const allTeamIds = new Set([...personEventsByTeam.keys(), ...behaviouralEventsByTeam.keys()])",
        "comment_created_at": "2025-08-14T09:04:05+00:00",
        "comment_author": "meikelmosby",
        "comment_body": "not worried about too much as the batch size is only 500 but fair point. changed that! :) ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2272901507",
    "pr_number": 36474,
    "pr_file": "frontend/src/scenes/data-warehouse/externalDataSourcesLogic.ts",
    "created_at": "2025-08-13T10:36:55+00:00",
    "commented_code": "return {\n                         ...values.dataWarehouseSources,\n                         results:\n-                            values.dataWarehouseSources?.results.map((s) =>\n+                            values.dataWarehouseSources?.results.map((s: ExternalDataSource) =>\n                                 s.id === updatedSource.id ? updatedSource : s\n                             ) || [],\n                     }\n                 },\n             },\n         ],\n+        totalRowsProcessed: [\n+            0 as number,\n+            {\n+                loadTotalRowsProcessed: async ({ materializedViews }: { materializedViews: any[] }) => {\n+                    const dataSources = values.dataWarehouseSources?.results || []\n+\n+                    const monthStartISO = getMonthStartISO()\n+\n+                    const [schemaResults, materializationResults] = await Promise.all([\n+                        Promise.all(\n+                            dataSources.map(async (source: ExternalDataSource) => {\n+                                try {\n+                                    const jobs = await api.externalDataSources.jobs(source.id, monthStartISO, null)\n+                                    return sumMTDRows(jobs, monthStartISO)\n+                                } catch (error) {\n+                                    posthog.captureException(error)\n+                                    return 0\n+                                }\n+                            })\n+                        ),\n+\n+                        Promise.all(\n+                            materializedViews.map(async (view: any) => {\n+                                try {\n+                                    const res = await api.dataWarehouseSavedQueries.dataWarehouseDataModelingJobs.list(\n+                                        view.id,\n+                                        DATA_WAREHOUSE_CONFIG.maxJobsForMTD,",
    "repo_full_name": "PostHog/posthog",
    "discussion_comments": [
      {
        "comment_id": "2272901507",
        "repo_full_name": "PostHog/posthog",
        "pr_number": 36474,
        "pr_file": "frontend/src/scenes/data-warehouse/externalDataSourcesLogic.ts",
        "discussion_id": "2272901507",
        "commented_code": "@@ -35,13 +76,143 @@ export const externalDataSourcesLogic = kea<externalDataSourcesLogicType>([\n                     return {\n                         ...values.dataWarehouseSources,\n                         results:\n-                            values.dataWarehouseSources?.results.map((s) =>\n+                            values.dataWarehouseSources?.results.map((s: ExternalDataSource) =>\n                                 s.id === updatedSource.id ? updatedSource : s\n                             ) || [],\n                     }\n                 },\n             },\n         ],\n+        totalRowsProcessed: [\n+            0 as number,\n+            {\n+                loadTotalRowsProcessed: async ({ materializedViews }: { materializedViews: any[] }) => {\n+                    const dataSources = values.dataWarehouseSources?.results || []\n+\n+                    const monthStartISO = getMonthStartISO()\n+\n+                    const [schemaResults, materializationResults] = await Promise.all([\n+                        Promise.all(\n+                            dataSources.map(async (source: ExternalDataSource) => {\n+                                try {\n+                                    const jobs = await api.externalDataSources.jobs(source.id, monthStartISO, null)\n+                                    return sumMTDRows(jobs, monthStartISO)\n+                                } catch (error) {\n+                                    posthog.captureException(error)\n+                                    return 0\n+                                }\n+                            })\n+                        ),\n+\n+                        Promise.all(\n+                            materializedViews.map(async (view: any) => {\n+                                try {\n+                                    const res = await api.dataWarehouseSavedQueries.dataWarehouseDataModelingJobs.list(\n+                                        view.id,\n+                                        DATA_WAREHOUSE_CONFIG.maxJobsForMTD,",
        "comment_created_at": "2025-08-13T10:36:55+00:00",
        "comment_author": "Gilbert09",
        "comment_body": "This will only load the last 200 jobs for the mat view - what if there are more than that in the current month/billing cycle?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2272942496",
    "pr_number": 36474,
    "pr_file": "frontend/src/scenes/data-warehouse/externalDataSourcesLogic.ts",
    "created_at": "2025-08-13T10:49:30+00:00",
    "commented_code": "return {\n                         ...values.dataWarehouseSources,\n                         results:\n-                            values.dataWarehouseSources?.results.map((s) =>\n+                            values.dataWarehouseSources?.results.map((s: ExternalDataSource) =>\n                                 s.id === updatedSource.id ? updatedSource : s\n                             ) || [],\n                     }\n                 },\n             },\n         ],\n+        totalRowsProcessed: [\n+            0 as number,\n+            {\n+                loadTotalRowsProcessed: async ({ materializedViews }: { materializedViews: any[] }) => {\n+                    const dataSources = values.dataWarehouseSources?.results || []\n+\n+                    const monthStartISO = getMonthStartISO()\n+\n+                    const [schemaResults, materializationResults] = await Promise.all([\n+                        Promise.all(\n+                            dataSources.map(async (source: ExternalDataSource) => {\n+                                try {\n+                                    const jobs = await api.externalDataSources.jobs(source.id, monthStartISO, null)\n+                                    return sumMTDRows(jobs, monthStartISO)\n+                                } catch (error) {\n+                                    posthog.captureException(error)\n+                                    return 0\n+                                }\n+                            })\n+                        ),\n+\n+                        Promise.all(\n+                            materializedViews.map(async (view: any) => {\n+                                try {\n+                                    const res = await api.dataWarehouseSavedQueries.dataWarehouseDataModelingJobs.list(\n+                                        view.id,\n+                                        DATA_WAREHOUSE_CONFIG.maxJobsForMTD,\n+                                        0\n+                                    )\n+                                    return sumMTDRows(res.results || [], monthStartISO)\n+                                } catch (error) {\n+                                    posthog.captureException(error)\n+                                    return 0\n+                                }\n+                            })\n+                        ),\n+                    ])\n+\n+                    return [...schemaResults, ...materializationResults].reduce((sum, total) => sum + total, 0)\n+                },\n+            },\n+        ],\n+        recentActivity: [\n+            [] as UnifiedRecentActivity[],\n+            {\n+                loadRecentActivity: async ({ materializedViews }: { materializedViews: any[] }) => {\n+                    const dataSources = values.dataWarehouseSources?.results || []\n+                    const cutoffDate = new Date()\n+                    cutoffDate.setDate(cutoffDate.getDate() - DATA_WAREHOUSE_CONFIG.recentActivityDays)\n+\n+                    const [schemaResults, materializationResults] = await Promise.all([\n+                        Promise.all(\n+                            dataSources.map(async (source: ExternalDataSource) => {\n+                                const allJobs: ExternalDataJob[] = []\n+                                let lastJobTimestamp: string | null = null\n+\n+                                while (true) {\n+                                    try {\n+                                        const jobs: ExternalDataJob[] = await (async () => {\n+                                            const res: unknown = await api.externalDataSources.jobs(\n+                                                source.id,\n+                                                lastJobTimestamp ?? null,\n+                                                null\n+                                            )\n+                                            return (\n+                                                Array.isArray(res) ? res : (res as any)?.results || []\n+                                            ) as ExternalDataJob[]\n+                                        })()\n+\n+                                        if (jobs.length === 0) {\n+                                            break\n+                                        }\n+\n+                                        allJobs.push(...jobs)\n+\n+                                        const oldestJob: ExternalDataJob | undefined = jobs[jobs.length - 1]\n+                                        lastJobTimestamp = oldestJob?.created_at || null\n+                                    } catch (error) {\n+                                        posthog.captureException(error)\n+                                        break\n+                                    }\n+                                }\n+\n+                                return allJobs.map((job) => ({\n+                                    id: job.id,\n+                                    name: job.schema.name,\n+                                    type: 'Data Sync' as const,\n+                                    status: job.status,\n+                                    created_at: job.created_at,\n+                                    rowCount: job.rows_synced,\n+                                    sourceId: source.id,\n+                                    sourceName: source.source_type,\n+                                }))\n+                            })",
    "repo_full_name": "PostHog/posthog",
    "discussion_comments": [
      {
        "comment_id": "2272942496",
        "repo_full_name": "PostHog/posthog",
        "pr_number": 36474,
        "pr_file": "frontend/src/scenes/data-warehouse/externalDataSourcesLogic.ts",
        "discussion_id": "2272942496",
        "commented_code": "@@ -35,13 +76,143 @@ export const externalDataSourcesLogic = kea<externalDataSourcesLogicType>([\n                     return {\n                         ...values.dataWarehouseSources,\n                         results:\n-                            values.dataWarehouseSources?.results.map((s) =>\n+                            values.dataWarehouseSources?.results.map((s: ExternalDataSource) =>\n                                 s.id === updatedSource.id ? updatedSource : s\n                             ) || [],\n                     }\n                 },\n             },\n         ],\n+        totalRowsProcessed: [\n+            0 as number,\n+            {\n+                loadTotalRowsProcessed: async ({ materializedViews }: { materializedViews: any[] }) => {\n+                    const dataSources = values.dataWarehouseSources?.results || []\n+\n+                    const monthStartISO = getMonthStartISO()\n+\n+                    const [schemaResults, materializationResults] = await Promise.all([\n+                        Promise.all(\n+                            dataSources.map(async (source: ExternalDataSource) => {\n+                                try {\n+                                    const jobs = await api.externalDataSources.jobs(source.id, monthStartISO, null)\n+                                    return sumMTDRows(jobs, monthStartISO)\n+                                } catch (error) {\n+                                    posthog.captureException(error)\n+                                    return 0\n+                                }\n+                            })\n+                        ),\n+\n+                        Promise.all(\n+                            materializedViews.map(async (view: any) => {\n+                                try {\n+                                    const res = await api.dataWarehouseSavedQueries.dataWarehouseDataModelingJobs.list(\n+                                        view.id,\n+                                        DATA_WAREHOUSE_CONFIG.maxJobsForMTD,\n+                                        0\n+                                    )\n+                                    return sumMTDRows(res.results || [], monthStartISO)\n+                                } catch (error) {\n+                                    posthog.captureException(error)\n+                                    return 0\n+                                }\n+                            })\n+                        ),\n+                    ])\n+\n+                    return [...schemaResults, ...materializationResults].reduce((sum, total) => sum + total, 0)\n+                },\n+            },\n+        ],\n+        recentActivity: [\n+            [] as UnifiedRecentActivity[],\n+            {\n+                loadRecentActivity: async ({ materializedViews }: { materializedViews: any[] }) => {\n+                    const dataSources = values.dataWarehouseSources?.results || []\n+                    const cutoffDate = new Date()\n+                    cutoffDate.setDate(cutoffDate.getDate() - DATA_WAREHOUSE_CONFIG.recentActivityDays)\n+\n+                    const [schemaResults, materializationResults] = await Promise.all([\n+                        Promise.all(\n+                            dataSources.map(async (source: ExternalDataSource) => {\n+                                const allJobs: ExternalDataJob[] = []\n+                                let lastJobTimestamp: string | null = null\n+\n+                                while (true) {\n+                                    try {\n+                                        const jobs: ExternalDataJob[] = await (async () => {\n+                                            const res: unknown = await api.externalDataSources.jobs(\n+                                                source.id,\n+                                                lastJobTimestamp ?? null,\n+                                                null\n+                                            )\n+                                            return (\n+                                                Array.isArray(res) ? res : (res as any)?.results || []\n+                                            ) as ExternalDataJob[]\n+                                        })()\n+\n+                                        if (jobs.length === 0) {\n+                                            break\n+                                        }\n+\n+                                        allJobs.push(...jobs)\n+\n+                                        const oldestJob: ExternalDataJob | undefined = jobs[jobs.length - 1]\n+                                        lastJobTimestamp = oldestJob?.created_at || null\n+                                    } catch (error) {\n+                                        posthog.captureException(error)\n+                                        break\n+                                    }\n+                                }\n+\n+                                return allJobs.map((job) => ({\n+                                    id: job.id,\n+                                    name: job.schema.name,\n+                                    type: 'Data Sync' as const,\n+                                    status: job.status,\n+                                    created_at: job.created_at,\n+                                    rowCount: job.rows_synced,\n+                                    sourceId: source.id,\n+                                    sourceName: source.source_type,\n+                                }))\n+                            })",
        "comment_created_at": "2025-08-13T10:49:30+00:00",
        "comment_author": "Gilbert09",
        "comment_body": "This is currently getting _all jobs_ for _all sources_. This will generate an array of size `283,914` for team 2. I'm presuming you're missing the filtering for the `cutoffDate` here, but that would still return an array of `5,485` jobs. Not only will this take forever to load, but I doubt we need all of this data?",
        "pr_file_module": null
      }
    ]
  }
]