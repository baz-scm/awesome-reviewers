[
  {
    "discussion_id": "2074758886",
    "pr_number": 16559,
    "pr_file": "scrape/target.go",
    "created_at": "2025-05-06T05:36:46+00:00",
    "commented_code": "// URL returns a copy of the target's URL.\n func (t *Target) URL() *url.URL {\n \tt.mtx.Lock()\n+\tdefer t.mtx.Unlock()",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2074758886",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16559,
        "pr_file": "scrape/target.go",
        "discussion_id": "2074758886",
        "commented_code": "@@ -209,8 +209,8 @@ func (t *Target) SetScrapeConfig(scrapeConfig *config.ScrapeConfig, tLabels, tgL\n // URL returns a copy of the target's URL.\n func (t *Target) URL() *url.URL {\n \tt.mtx.Lock()\n+\tdefer t.mtx.Unlock()",
        "comment_created_at": "2025-05-06T05:36:46+00:00",
        "comment_author": "machine424",
        "comment_body": "IIUC, we only want to replace `.Lock()` with a `.Rlock()`\r\nWhy do we want to defer the release?\r\n\r\nalso could we do the same for https://github.com/prometheus/prometheus/blob/c3ce1f1927483364cae2503ba8c1a2e9693d721e/scrape/target.go#L191-L195?\r\nthnaks",
        "pr_file_module": null
      },
      {
        "comment_id": "2074801048",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16559,
        "pr_file": "scrape/target.go",
        "discussion_id": "2074758886",
        "commented_code": "@@ -209,8 +209,8 @@ func (t *Target) SetScrapeConfig(scrapeConfig *config.ScrapeConfig, tLabels, tgL\n // URL returns a copy of the target's URL.\n func (t *Target) URL() *url.URL {\n \tt.mtx.Lock()\n+\tdefer t.mtx.Unlock()",
        "comment_created_at": "2025-05-06T06:23:44+00:00",
        "comment_author": "Shuimo03",
        "comment_body": "OK, Rlock is used because the target file is a configuration file, which is a case of more reads and fewer writes.DiscoveredLabels was not modified because of performance issues, and several previous tests found it to be slower. If  use Rlock.",
        "pr_file_module": null
      },
      {
        "comment_id": "2078105316",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16559,
        "pr_file": "scrape/target.go",
        "discussion_id": "2074758886",
        "commented_code": "@@ -209,8 +209,8 @@ func (t *Target) SetScrapeConfig(scrapeConfig *config.ScrapeConfig, tLabels, tgL\n // URL returns a copy of the target's URL.\n func (t *Target) URL() *url.URL {\n \tt.mtx.Lock()\n+\tdefer t.mtx.Unlock()",
        "comment_created_at": "2025-05-07T17:13:19+00:00",
        "comment_author": "machine424",
        "comment_body": "I don't see how acquiring the lock for the whole function helps here.\r\ncould you elaborate on those \"performance issues and several previous tests\"?\r\n\r\nFor me what should be done is:\r\n- Replace Lock with an Rlock in DiscoveredLabels() and URL()\r\n- Add a comment in `SetScrapeConfig` just before `t.scrapeConfig = scrapeConfig` that we rely on scrapeConfig being swapped to allow such \"localized locking\" on the readers side",
        "pr_file_module": null
      },
      {
        "comment_id": "2078779405",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16559,
        "pr_file": "scrape/target.go",
        "discussion_id": "2074758886",
        "commented_code": "@@ -209,8 +209,8 @@ func (t *Target) SetScrapeConfig(scrapeConfig *config.ScrapeConfig, tLabels, tgL\n // URL returns a copy of the target's URL.\n func (t *Target) URL() *url.URL {\n \tt.mtx.Lock()\n+\tdefer t.mtx.Unlock()",
        "comment_created_at": "2025-05-08T02:19:39+00:00",
        "comment_author": "Shuimo03",
        "comment_body": "Previous tests utilized RLock, which resulted in significant performance improvements as this represents a read-heavy, write-light scenario. RLock permits multiple goroutines to simultaneously access data for reading operations, whereas Lock completely blocks access from other goroutines regardless of their intent. I inadvertently overlooked this critical distinction when making the commit.",
        "pr_file_module": null
      },
      {
        "comment_id": "2081047267",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16559,
        "pr_file": "scrape/target.go",
        "discussion_id": "2074758886",
        "commented_code": "@@ -209,8 +209,8 @@ func (t *Target) SetScrapeConfig(scrapeConfig *config.ScrapeConfig, tLabels, tgL\n // URL returns a copy of the target's URL.\n func (t *Target) URL() *url.URL {\n \tt.mtx.Lock()\n+\tdefer t.mtx.Unlock()",
        "comment_created_at": "2025-05-09T06:52:39+00:00",
        "comment_author": "machine424",
        "comment_body": "I still feel you're pasting in text from somewhere without really taking time to understand it.\r\nYou added the comment about \"localized locking\" yet you changed the local unlock to a defer unlock.\r\nDid you happen to ask an LLM to find an improvement in the code for you and you cannot/don't want to spend time understanding how things work?\r\n\r\nAlso the PR changes 31 files now.\r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1683231074",
    "pr_number": 14478,
    "pr_file": "rules/manager.go",
    "created_at": "2024-07-18T17:23:37+00:00",
    "commented_code": "g.setEvaluationTime(timeSinceStart)\n \tg.setLastEvaluation(start)\n \tg.setLastEvalTimestamp(evalTimestamp)\n+\n+\tif g.alertStore != nil {\n+\t\t// feature enabled.\n+\t\tgo func() {",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "1683231074",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 14478,
        "pr_file": "rules/manager.go",
        "discussion_id": "1683231074",
        "commented_code": "@@ -85,6 +85,37 @@ func DefaultEvalIterationFunc(ctx context.Context, g *Group, evalTimestamp time.\n \tg.setEvaluationTime(timeSinceStart)\n \tg.setLastEvaluation(start)\n \tg.setLastEvalTimestamp(evalTimestamp)\n+\n+\tif g.alertStore != nil {\n+\t\t// feature enabled.\n+\t\tgo func() {",
        "comment_created_at": "2024-07-18T17:23:37+00:00",
        "comment_author": "bboreham",
        "comment_body": "Why is this on a separate goroutine?",
        "pr_file_module": null
      },
      {
        "comment_id": "1683799664",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 14478,
        "pr_file": "rules/manager.go",
        "discussion_id": "1683231074",
        "commented_code": "@@ -85,6 +85,37 @@ func DefaultEvalIterationFunc(ctx context.Context, g *Group, evalTimestamp time.\n \tg.setEvaluationTime(timeSinceStart)\n \tg.setLastEvaluation(start)\n \tg.setLastEvalTimestamp(evalTimestamp)\n+\n+\tif g.alertStore != nil {\n+\t\t// feature enabled.\n+\t\tgo func() {",
        "comment_created_at": "2024-07-19T05:18:07+00:00",
        "comment_author": "mustafain117",
        "comment_body": "In order to store the state asynchronously without blocking the group evaluation.",
        "pr_file_module": null
      },
      {
        "comment_id": "1684054594",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 14478,
        "pr_file": "rules/manager.go",
        "discussion_id": "1683231074",
        "commented_code": "@@ -85,6 +85,37 @@ func DefaultEvalIterationFunc(ctx context.Context, g *Group, evalTimestamp time.\n \tg.setEvaluationTime(timeSinceStart)\n \tg.setLastEvaluation(start)\n \tg.setLastEvalTimestamp(evalTimestamp)\n+\n+\tif g.alertStore != nil {\n+\t\t// feature enabled.\n+\t\tgo func() {",
        "comment_created_at": "2024-07-19T08:40:45+00:00",
        "comment_author": "bboreham",
        "comment_body": "Is that safe?\r\nWhat happens if it takes a long time?",
        "pr_file_module": null
      },
      {
        "comment_id": "1686717388",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 14478,
        "pr_file": "rules/manager.go",
        "discussion_id": "1683231074",
        "commented_code": "@@ -85,6 +85,37 @@ func DefaultEvalIterationFunc(ctx context.Context, g *Group, evalTimestamp time.\n \tg.setEvaluationTime(timeSinceStart)\n \tg.setLastEvaluation(start)\n \tg.setLastEvalTimestamp(evalTimestamp)\n+\n+\tif g.alertStore != nil {\n+\t\t// feature enabled.\n+\t\tgo func() {",
        "comment_created_at": "2024-07-22T15:09:54+00:00",
        "comment_author": "mustafain117",
        "comment_body": "Tested by adding random sleeps and did not notice any performance/latency issues.\r\nIf it takes a long time, a risk with this approach is incomplete work/loss of state from latest evaluation if Prometheus terminates before this goroutine since we do not use a WaitGroup.",
        "pr_file_module": null
      },
      {
        "comment_id": "1698280414",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 14478,
        "pr_file": "rules/manager.go",
        "discussion_id": "1683231074",
        "commented_code": "@@ -85,6 +85,37 @@ func DefaultEvalIterationFunc(ctx context.Context, g *Group, evalTimestamp time.\n \tg.setEvaluationTime(timeSinceStart)\n \tg.setLastEvaluation(start)\n \tg.setLastEvalTimestamp(evalTimestamp)\n+\n+\tif g.alertStore != nil {\n+\t\t// feature enabled.\n+\t\tgo func() {",
        "comment_created_at": "2024-07-31T10:27:05+00:00",
        "comment_author": "bboreham",
        "comment_body": "I'm worried that these goroutines will pile up.\r\nPutting it more plainly, you must not do this.",
        "pr_file_module": null
      },
      {
        "comment_id": "1702320175",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 14478,
        "pr_file": "rules/manager.go",
        "discussion_id": "1683231074",
        "commented_code": "@@ -85,6 +85,37 @@ func DefaultEvalIterationFunc(ctx context.Context, g *Group, evalTimestamp time.\n \tg.setEvaluationTime(timeSinceStart)\n \tg.setLastEvaluation(start)\n \tg.setLastEvalTimestamp(evalTimestamp)\n+\n+\tif g.alertStore != nil {\n+\t\t// feature enabled.\n+\t\tgo func() {",
        "comment_created_at": "2024-08-02T21:06:24+00:00",
        "comment_author": "mustafain117",
        "comment_body": "I see, thanks for the feedback. Removed, rule evaluation duration does not change significantly even if this is done on the same goroutine.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2066593914",
    "pr_number": 16355,
    "pr_file": "notifier/alertmanagerset.go",
    "created_at": "2025-04-29T14:03:23+00:00",
    "commented_code": "hash := md5.Sum(b)\n \treturn hex.EncodeToString(hash[:]), nil\n }\n+\n+func (s *alertmanagerSet) send(alerts ...*Alert) map[string]int {\n+\tch := make(chan map[string]int, len(s.buffers))\n+\twg := sync.WaitGroup{}\n+\tfor am, q := range s.buffers {\n+\t\twg.Add(1)\n+\t\tgo func(am string, wg *sync.WaitGroup, ch chan<- map[string]int) {\n+\t\t\tdefer wg.Done()\n+\t\t\tif d := q.push(alerts...); d > 0 {\n+\t\t\t\tch <- map[string]int{am: d}\n+\t\t\t}\n+\t\t}(am, &wg, ch)\n+\t}\n+\twg.Wait()\n+\tclose(ch)\n+\n+\tdropped := make(map[string]int)\n+\tfor d := range ch {\n+\t\tmaps.Copy(dropped, d)\n+\t}\n+\treturn dropped\n+}\n+\n+func (s *alertmanagerSet) sendLoop(am alertmanager, more chan struct{}) {\n+\turl := am.url().String()\n+\tfor {\n+\t\t_, ok := <-more\n+\t\tif !ok {\n+\t\t\treturn\n+\t\t}\n+\n+\t\tb := s.getBuffer(url)\n+\t\tif b == nil {\n+\t\t\treturn\n+\t\t}\n+\t\tbatch := b.pop(s.opts.MaxBatchSize)\n+\n+\t\tif !s.postNotifications(am, batch) {\n+\t\t\ts.mtx.Lock()\n+\t\t\ts.metrics.dropped.WithLabelValues(url).Add(float64(len(batch)))\n+\t\t\ts.mtx.Unlock()",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2066593914",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16355,
        "pr_file": "notifier/alertmanagerset.go",
        "discussion_id": "2066593914",
        "commented_code": "@@ -126,3 +144,142 @@ func (s *alertmanagerSet) configHash() (string, error) {\n \thash := md5.Sum(b)\n \treturn hex.EncodeToString(hash[:]), nil\n }\n+\n+func (s *alertmanagerSet) send(alerts ...*Alert) map[string]int {\n+\tch := make(chan map[string]int, len(s.buffers))\n+\twg := sync.WaitGroup{}\n+\tfor am, q := range s.buffers {\n+\t\twg.Add(1)\n+\t\tgo func(am string, wg *sync.WaitGroup, ch chan<- map[string]int) {\n+\t\t\tdefer wg.Done()\n+\t\t\tif d := q.push(alerts...); d > 0 {\n+\t\t\t\tch <- map[string]int{am: d}\n+\t\t\t}\n+\t\t}(am, &wg, ch)\n+\t}\n+\twg.Wait()\n+\tclose(ch)\n+\n+\tdropped := make(map[string]int)\n+\tfor d := range ch {\n+\t\tmaps.Copy(dropped, d)\n+\t}\n+\treturn dropped\n+}\n+\n+func (s *alertmanagerSet) sendLoop(am alertmanager, more chan struct{}) {\n+\turl := am.url().String()\n+\tfor {\n+\t\t_, ok := <-more\n+\t\tif !ok {\n+\t\t\treturn\n+\t\t}\n+\n+\t\tb := s.getBuffer(url)\n+\t\tif b == nil {\n+\t\t\treturn\n+\t\t}\n+\t\tbatch := b.pop(s.opts.MaxBatchSize)\n+\n+\t\tif !s.postNotifications(am, batch) {\n+\t\t\ts.mtx.Lock()\n+\t\t\ts.metrics.dropped.WithLabelValues(url).Add(float64(len(batch)))\n+\t\t\ts.mtx.Unlock()",
        "comment_created_at": "2025-04-29T14:03:23+00:00",
        "comment_author": "krajorama",
        "comment_body": "```suggestion\r\n\t\t\ts.metrics.dropped.WithLabelValues(url).Add(float64(len(batch)))\r\n```\r\n\r\nAs far as I know client_golang is thread safe.",
        "pr_file_module": null
      },
      {
        "comment_id": "2070372542",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16355,
        "pr_file": "notifier/alertmanagerset.go",
        "discussion_id": "2066593914",
        "commented_code": "@@ -126,3 +144,142 @@ func (s *alertmanagerSet) configHash() (string, error) {\n \thash := md5.Sum(b)\n \treturn hex.EncodeToString(hash[:]), nil\n }\n+\n+func (s *alertmanagerSet) send(alerts ...*Alert) map[string]int {\n+\tch := make(chan map[string]int, len(s.buffers))\n+\twg := sync.WaitGroup{}\n+\tfor am, q := range s.buffers {\n+\t\twg.Add(1)\n+\t\tgo func(am string, wg *sync.WaitGroup, ch chan<- map[string]int) {\n+\t\t\tdefer wg.Done()\n+\t\t\tif d := q.push(alerts...); d > 0 {\n+\t\t\t\tch <- map[string]int{am: d}\n+\t\t\t}\n+\t\t}(am, &wg, ch)\n+\t}\n+\twg.Wait()\n+\tclose(ch)\n+\n+\tdropped := make(map[string]int)\n+\tfor d := range ch {\n+\t\tmaps.Copy(dropped, d)\n+\t}\n+\treturn dropped\n+}\n+\n+func (s *alertmanagerSet) sendLoop(am alertmanager, more chan struct{}) {\n+\turl := am.url().String()\n+\tfor {\n+\t\t_, ok := <-more\n+\t\tif !ok {\n+\t\t\treturn\n+\t\t}\n+\n+\t\tb := s.getBuffer(url)\n+\t\tif b == nil {\n+\t\t\treturn\n+\t\t}\n+\t\tbatch := b.pop(s.opts.MaxBatchSize)\n+\n+\t\tif !s.postNotifications(am, batch) {\n+\t\t\ts.mtx.Lock()\n+\t\t\ts.metrics.dropped.WithLabelValues(url).Add(float64(len(batch)))\n+\t\t\ts.mtx.Unlock()",
        "comment_created_at": "2025-05-01T14:57:15+00:00",
        "comment_author": "siavashs",
        "comment_body": "Removing the locks causes a race condition as captured by the tests: https://github.com/prometheus/prometheus/actions/runs/14777171450/job/41487872345?pr=16355",
        "pr_file_module": null
      },
      {
        "comment_id": "2070389409",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16355,
        "pr_file": "notifier/alertmanagerset.go",
        "discussion_id": "2066593914",
        "commented_code": "@@ -126,3 +144,142 @@ func (s *alertmanagerSet) configHash() (string, error) {\n \thash := md5.Sum(b)\n \treturn hex.EncodeToString(hash[:]), nil\n }\n+\n+func (s *alertmanagerSet) send(alerts ...*Alert) map[string]int {\n+\tch := make(chan map[string]int, len(s.buffers))\n+\twg := sync.WaitGroup{}\n+\tfor am, q := range s.buffers {\n+\t\twg.Add(1)\n+\t\tgo func(am string, wg *sync.WaitGroup, ch chan<- map[string]int) {\n+\t\t\tdefer wg.Done()\n+\t\t\tif d := q.push(alerts...); d > 0 {\n+\t\t\t\tch <- map[string]int{am: d}\n+\t\t\t}\n+\t\t}(am, &wg, ch)\n+\t}\n+\twg.Wait()\n+\tclose(ch)\n+\n+\tdropped := make(map[string]int)\n+\tfor d := range ch {\n+\t\tmaps.Copy(dropped, d)\n+\t}\n+\treturn dropped\n+}\n+\n+func (s *alertmanagerSet) sendLoop(am alertmanager, more chan struct{}) {\n+\turl := am.url().String()\n+\tfor {\n+\t\t_, ok := <-more\n+\t\tif !ok {\n+\t\t\treturn\n+\t\t}\n+\n+\t\tb := s.getBuffer(url)\n+\t\tif b == nil {\n+\t\t\treturn\n+\t\t}\n+\t\tbatch := b.pop(s.opts.MaxBatchSize)\n+\n+\t\tif !s.postNotifications(am, batch) {\n+\t\t\ts.mtx.Lock()\n+\t\t\ts.metrics.dropped.WithLabelValues(url).Add(float64(len(batch)))\n+\t\t\ts.mtx.Unlock()",
        "comment_created_at": "2025-05-01T15:11:38+00:00",
        "comment_author": "siavashs",
        "comment_body": "Seems to be caused by the tests themselves, trying to reset the metrics: https://github.com/prometheus/prometheus/pull/16355/files#diff-2f5b03e4b050cbcf68f8cb71506ba38b896df2c70e0277dd9e924d984c1d6d1eR492-R498\r\nI'll try to find a safe reset method.",
        "pr_file_module": null
      },
      {
        "comment_id": "2070398733",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16355,
        "pr_file": "notifier/alertmanagerset.go",
        "discussion_id": "2066593914",
        "commented_code": "@@ -126,3 +144,142 @@ func (s *alertmanagerSet) configHash() (string, error) {\n \thash := md5.Sum(b)\n \treturn hex.EncodeToString(hash[:]), nil\n }\n+\n+func (s *alertmanagerSet) send(alerts ...*Alert) map[string]int {\n+\tch := make(chan map[string]int, len(s.buffers))\n+\twg := sync.WaitGroup{}\n+\tfor am, q := range s.buffers {\n+\t\twg.Add(1)\n+\t\tgo func(am string, wg *sync.WaitGroup, ch chan<- map[string]int) {\n+\t\t\tdefer wg.Done()\n+\t\t\tif d := q.push(alerts...); d > 0 {\n+\t\t\t\tch <- map[string]int{am: d}\n+\t\t\t}\n+\t\t}(am, &wg, ch)\n+\t}\n+\twg.Wait()\n+\tclose(ch)\n+\n+\tdropped := make(map[string]int)\n+\tfor d := range ch {\n+\t\tmaps.Copy(dropped, d)\n+\t}\n+\treturn dropped\n+}\n+\n+func (s *alertmanagerSet) sendLoop(am alertmanager, more chan struct{}) {\n+\turl := am.url().String()\n+\tfor {\n+\t\t_, ok := <-more\n+\t\tif !ok {\n+\t\t\treturn\n+\t\t}\n+\n+\t\tb := s.getBuffer(url)\n+\t\tif b == nil {\n+\t\t\treturn\n+\t\t}\n+\t\tbatch := b.pop(s.opts.MaxBatchSize)\n+\n+\t\tif !s.postNotifications(am, batch) {\n+\t\t\ts.mtx.Lock()\n+\t\t\ts.metrics.dropped.WithLabelValues(url).Add(float64(len(batch)))\n+\t\t\ts.mtx.Unlock()",
        "comment_created_at": "2025-05-01T15:19:52+00:00",
        "comment_author": "siavashs",
        "comment_body": "Well, it was easier than I assumed: `h.metrics.errors.Reset()`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2096545983",
    "pr_number": 16355,
    "pr_file": "notifier/alert.go",
    "created_at": "2025-05-19T22:04:20+00:00",
    "commented_code": "return !a.EndsAt.After(ts)\n }\n \n+// Copy returns a copy of the alert.\n+func (a *Alert) Copy() *Alert {\n+\treturn &Alert{\n+\t\tLabels:       a.Labels.Copy(),\n+\t\tAnnotations:  a.Annotations.Copy(),\n+\t\tStartsAt:     a.StartsAt,\n+\t\tEndsAt:       a.EndsAt,\n+\t\tGeneratorURL: a.GeneratorURL,\n+\t}\n+}\n+\n func relabelAlerts(relabelConfigs []*relabel.Config, externalLabels labels.Labels, alerts []*Alert) []*Alert {\n \tlb := labels.NewBuilder(labels.EmptyLabels())\n \tvar relabeledAlerts []*Alert\n \n-\tfor _, a := range alerts {\n+\tfor _, s := range alerts {\n+\t\t// Copy the alert to avoid race condition.",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2096545983",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16355,
        "pr_file": "notifier/alert.go",
        "discussion_id": "2096545983",
        "commented_code": "@@ -68,11 +68,25 @@ func (a *Alert) ResolvedAt(ts time.Time) bool {\n \treturn !a.EndsAt.After(ts)\n }\n \n+// Copy returns a copy of the alert.\n+func (a *Alert) Copy() *Alert {\n+\treturn &Alert{\n+\t\tLabels:       a.Labels.Copy(),\n+\t\tAnnotations:  a.Annotations.Copy(),\n+\t\tStartsAt:     a.StartsAt,\n+\t\tEndsAt:       a.EndsAt,\n+\t\tGeneratorURL: a.GeneratorURL,\n+\t}\n+}\n+\n func relabelAlerts(relabelConfigs []*relabel.Config, externalLabels labels.Labels, alerts []*Alert) []*Alert {\n \tlb := labels.NewBuilder(labels.EmptyLabels())\n \tvar relabeledAlerts []*Alert\n \n-\tfor _, a := range alerts {\n+\tfor _, s := range alerts {\n+\t\t// Copy the alert to avoid race condition.",
        "comment_created_at": "2025-05-19T22:04:20+00:00",
        "comment_author": "machine424",
        "comment_body": "Could you elaborate?\r\nwas it the case before or is it related to the change? It'd be great to make sure we have a clear regression test for this.",
        "pr_file_module": null
      },
      {
        "comment_id": "2137456780",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16355,
        "pr_file": "notifier/alert.go",
        "discussion_id": "2096545983",
        "commented_code": "@@ -68,11 +68,25 @@ func (a *Alert) ResolvedAt(ts time.Time) bool {\n \treturn !a.EndsAt.After(ts)\n }\n \n+// Copy returns a copy of the alert.\n+func (a *Alert) Copy() *Alert {\n+\treturn &Alert{\n+\t\tLabels:       a.Labels.Copy(),\n+\t\tAnnotations:  a.Annotations.Copy(),\n+\t\tStartsAt:     a.StartsAt,\n+\t\tEndsAt:       a.EndsAt,\n+\t\tGeneratorURL: a.GeneratorURL,\n+\t}\n+}\n+\n func relabelAlerts(relabelConfigs []*relabel.Config, externalLabels labels.Labels, alerts []*Alert) []*Alert {\n \tlb := labels.NewBuilder(labels.EmptyLabels())\n \tvar relabeledAlerts []*Alert\n \n-\tfor _, a := range alerts {\n+\tfor _, s := range alerts {\n+\t\t// Copy the alert to avoid race condition.",
        "comment_created_at": "2025-06-10T10:06:08+00:00",
        "comment_author": "siavashs",
        "comment_body": "Since we have multiple send loops which try to `send()` the same alerts and relabel them, they end up modifying the labels of the same alert pointers and it results in a race condition:\r\n\r\n<details>\r\n\r\n```go\r\n> go test -race ./notifier\r\n# github.com/prometheus/prometheus/notifier.test\r\nld: warning: '/private/var/folders/bm/_9901zqd4dvb59nf2bg4sdwh0000gn/T/go-link-3700774005/000013.o' has malformed LC_DYSYMTAB, expected 98 undefined symbols to start at index 1626, found 95 undefined symbols starting at index 1626\r\n==================\r\nWARNING: DATA RACE\r\nRead at 0x00c000489ce0 by goroutine 78:\r\n  github.com/prometheus/prometheus/notifier.alertsToOpenAPIAlerts()\r\n      /Users/siavash/Developer/prometheus/notifier/util.go:34 +0x128\r\n  github.com/prometheus/prometheus/notifier.(*alertmanagerSet).postNotifications()\r\n      /Users/siavash/Developer/prometheus/notifier/alertmanagerset.go:209 +0x100\r\n  github.com/prometheus/prometheus/notifier.(*alertmanagerSet).sendLoop()\r\n      /Users/siavash/Developer/prometheus/notifier/alertmanagerset.go:184 +0x174\r\n  github.com/prometheus/prometheus/notifier.TestHandlerSendAllRemapPerAm.gowrap4()\r\n      /Users/siavash/Developer/prometheus/notifier/manager_test.go:456 +0x58\r\n\r\nPrevious write at 0x00c000489ce0 by goroutine 74:\r\n  github.com/prometheus/prometheus/notifier.relabelAlerts()\r\n      /Users/siavash/Developer/prometheus/notifier/alert.go:101 +0x1c0\r\n  github.com/prometheus/prometheus/notifier.(*alertmanagerSet).send()\r\n      /Users/siavash/Developer/prometheus/notifier/alertmanagerset.go:152 +0xd4\r\n  github.com/prometheus/prometheus/notifier.(*Manager).Send()\r\n      /Users/siavash/Developer/prometheus/notifier/manager.go:271 +0x1f4\r\n  github.com/prometheus/prometheus/notifier.TestHandlerSendAllRemapPerAm()\r\n      /Users/siavash/Developer/prometheus/notifier/manager_test.go:461 +0x2a88\r\n  testing.tRunner()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1792 +0x180\r\n  testing.(*T).Run.gowrap1()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1851 +0x40\r\n\r\nGoroutine 78 (running) created at:\r\n  github.com/prometheus/prometheus/notifier.TestHandlerSendAllRemapPerAm()\r\n      /Users/siavash/Developer/prometheus/notifier/manager_test.go:456 +0x3718\r\n  testing.tRunner()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1792 +0x180\r\n  testing.(*T).Run.gowrap1()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1851 +0x40\r\n\r\nGoroutine 74 (running) created at:\r\n  testing.(*T).Run()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1851 +0x684\r\n  testing.runTests.func1()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:2279 +0x7c\r\n  testing.tRunner()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1792 +0x180\r\n  testing.runTests()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:2277 +0x77c\r\n  testing.(*M).Run()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:2142 +0xb68\r\n  main.main()\r\n      _testmain.go:85 +0x110\r\n==================\r\n==================\r\nWARNING: DATA RACE\r\nRead at 0x00c000489ce0 by goroutine 78:\r\n  github.com/prometheus/prometheus/notifier.alertsToOpenAPIAlerts()\r\n      /Users/siavash/Developer/prometheus/notifier/util.go:34 +0x128\r\n  github.com/prometheus/prometheus/notifier.(*alertmanagerSet).postNotifications()\r\n      /Users/siavash/Developer/prometheus/notifier/alertmanagerset.go:209 +0x100\r\n  github.com/prometheus/prometheus/notifier.(*alertmanagerSet).sendLoop()\r\n      /Users/siavash/Developer/prometheus/notifier/alertmanagerset.go:184 +0x174\r\n  github.com/prometheus/prometheus/notifier.TestHandlerSendAllRemapPerAm.gowrap4()\r\n      /Users/siavash/Developer/prometheus/notifier/manager_test.go:456 +0x58\r\n\r\nPrevious write at 0x00c000489ce0 by goroutine 74:\r\n  github.com/prometheus/prometheus/notifier.relabelAlerts()\r\n      /Users/siavash/Developer/prometheus/notifier/alert.go:101 +0x1c0\r\n  github.com/prometheus/prometheus/notifier.(*alertmanagerSet).send()\r\n      /Users/siavash/Developer/prometheus/notifier/alertmanagerset.go:152 +0xd4\r\n  github.com/prometheus/prometheus/notifier.(*Manager).Send()\r\n      /Users/siavash/Developer/prometheus/notifier/manager.go:271 +0x1f4\r\n  github.com/prometheus/prometheus/notifier.TestHandlerSendAllRemapPerAm()\r\n      /Users/siavash/Developer/prometheus/notifier/manager_test.go:495 +0x3150\r\n  testing.tRunner()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1792 +0x180\r\n  testing.(*T).Run.gowrap1()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1851 +0x40\r\n\r\nGoroutine 78 (running) created at:\r\n  github.com/prometheus/prometheus/notifier.TestHandlerSendAllRemapPerAm()\r\n      /Users/siavash/Developer/prometheus/notifier/manager_test.go:456 +0x3718\r\n  testing.tRunner()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1792 +0x180\r\n  testing.(*T).Run.gowrap1()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1851 +0x40\r\n\r\nGoroutine 74 (running) created at:\r\n  testing.(*T).Run()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1851 +0x684\r\n  testing.runTests.func1()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:2279 +0x7c\r\n  testing.tRunner()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:1792 +0x180\r\n  testing.runTests()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:2277 +0x77c\r\n  testing.(*M).Run()\r\n      /opt/homebrew/Cellar/go/1.24.3/libexec/src/testing/testing.go:2142 +0xb68\r\n  main.main()\r\n      _testmain.go:85 +0x110\r\n==================\r\n--- FAIL: TestHandlerSendAllRemapPerAm (5.05s)\r\n    testing.go:1490: race detected during execution of test\r\nFAIL\r\nFAIL\tgithub.com/prometheus/prometheus/notifier\t12.102s\r\nFAIL\r\n```\r\n</details>",
        "pr_file_module": null
      },
      {
        "comment_id": "2137465277",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16355,
        "pr_file": "notifier/alert.go",
        "discussion_id": "2096545983",
        "commented_code": "@@ -68,11 +68,25 @@ func (a *Alert) ResolvedAt(ts time.Time) bool {\n \treturn !a.EndsAt.After(ts)\n }\n \n+// Copy returns a copy of the alert.\n+func (a *Alert) Copy() *Alert {\n+\treturn &Alert{\n+\t\tLabels:       a.Labels.Copy(),\n+\t\tAnnotations:  a.Annotations.Copy(),\n+\t\tStartsAt:     a.StartsAt,\n+\t\tEndsAt:       a.EndsAt,\n+\t\tGeneratorURL: a.GeneratorURL,\n+\t}\n+}\n+\n func relabelAlerts(relabelConfigs []*relabel.Config, externalLabels labels.Labels, alerts []*Alert) []*Alert {\n \tlb := labels.NewBuilder(labels.EmptyLabels())\n \tvar relabeledAlerts []*Alert\n \n-\tfor _, a := range alerts {\n+\tfor _, s := range alerts {\n+\t\t// Copy the alert to avoid race condition.",
        "comment_created_at": "2025-06-10T10:09:31+00:00",
        "comment_author": "siavashs",
        "comment_body": "Maybe we should relabel once per `alertmanagerset` and only send already relabeled alerts, avoiding copying.",
        "pr_file_module": null
      },
      {
        "comment_id": "2137487008",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16355,
        "pr_file": "notifier/alert.go",
        "discussion_id": "2096545983",
        "commented_code": "@@ -68,11 +68,25 @@ func (a *Alert) ResolvedAt(ts time.Time) bool {\n \treturn !a.EndsAt.After(ts)\n }\n \n+// Copy returns a copy of the alert.\n+func (a *Alert) Copy() *Alert {\n+\treturn &Alert{\n+\t\tLabels:       a.Labels.Copy(),\n+\t\tAnnotations:  a.Annotations.Copy(),\n+\t\tStartsAt:     a.StartsAt,\n+\t\tEndsAt:       a.EndsAt,\n+\t\tGeneratorURL: a.GeneratorURL,\n+\t}\n+}\n+\n func relabelAlerts(relabelConfigs []*relabel.Config, externalLabels labels.Labels, alerts []*Alert) []*Alert {\n \tlb := labels.NewBuilder(labels.EmptyLabels())\n \tvar relabeledAlerts []*Alert\n \n-\tfor _, a := range alerts {\n+\tfor _, s := range alerts {\n+\t\t// Copy the alert to avoid race condition.",
        "comment_created_at": "2025-06-10T10:21:02+00:00",
        "comment_author": "siavashs",
        "comment_body": "Ah, we already do this and actually the race happens between multiple `alertmanagerset`s so there is no way around copying.\r\nBut let me know if you have an idea how we can avoid this issue.\r\nI'll improve the documentation in the relabel function.",
        "pr_file_module": null
      }
    ]
  }
]