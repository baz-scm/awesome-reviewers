[
  {
    "discussion_id": "1890526516",
    "pr_number": 15577,
    "pr_file": "util/annotations/annotations.go",
    "created_at": "2024-12-18T16:32:10+00:00",
    "commented_code": "HistogramIgnoredInAggregationInfo       = fmt.Errorf(\"%w: ignored histogram in\", PromQLInfo)\n )\n \n-type annoErr struct {\n+type annoErr interface {\n+\terror\n+\t// We can define custom merge functions to merge annoErrs with the same raw error string.\n+\tmerge(annoErr) annoErr",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "1890526516",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 15577,
        "pr_file": "util/annotations/annotations.go",
        "discussion_id": "1890526516",
        "commented_code": "@@ -150,45 +160,83 @@ var (\n \tHistogramIgnoredInAggregationInfo       = fmt.Errorf(\"%w: ignored histogram in\", PromQLInfo)\n )\n \n-type annoErr struct {\n+type annoErr interface {\n+\terror\n+\t// We can define custom merge functions to merge annoErrs with the same raw error string.\n+\tmerge(annoErr) annoErr",
        "comment_created_at": "2024-12-18T16:32:10+00:00",
        "comment_author": "beorn7",
        "comment_body": "I think I understand now what you are up to here, but having two different levels of merging is complex enough so that we need to explain this better here. So there is the exported `Merge` to merge two instances of `Annotations` with each other, but then there is this unexported `merge` to merge one individual annotation with another one.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2136780878",
    "pr_number": 16593,
    "pr_file": "promql/parser/ast.go",
    "created_at": "2025-06-10T02:08:21+00:00",
    "commented_code": "if v, err = v.Visit(node, path); v == nil || err != nil {\n \t\treturn err\n \t}\n-\tpath = append(path, node)\n+\tvar pathToHere []Node // Initialized only when needed.",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2136780878",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16593,
        "pr_file": "promql/parser/ast.go",
        "discussion_id": "2136780878",
        "commented_code": "@@ -333,10 +333,13 @@ func Walk(v Visitor, node Node, path []Node) error {\n \tif v, err = v.Visit(node, path); v == nil || err != nil {\n \t\treturn err\n \t}\n-\tpath = append(path, node)\n+\tvar pathToHere []Node // Initialized only when needed.",
        "comment_created_at": "2025-06-10T02:08:21+00:00",
        "comment_author": "charleskorn",
        "comment_body": "How many consumers of `Walk` make use of the path? I'm wondering if it's worth having another method that doesn't provide the path to avoid needing to allocate it.",
        "pr_file_module": null
      },
      {
        "comment_id": "2161353899",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16593,
        "pr_file": "promql/parser/ast.go",
        "discussion_id": "2136780878",
        "commented_code": "@@ -333,10 +333,13 @@ func Walk(v Visitor, node Node, path []Node) error {\n \tif v, err = v.Visit(node, path); v == nil || err != nil {\n \t\treturn err\n \t}\n-\tpath = append(path, node)\n+\tvar pathToHere []Node // Initialized only when needed.",
        "comment_created_at": "2025-06-23T11:15:20+00:00",
        "comment_author": "bboreham",
        "comment_body": "In Prometheus repo there is 1 non-test call to `Walk()` and 10 calls via `Inspect()`.  Only in `promql/engine.go`, 4 out of 5 calls (`FindMinMaxTime`, `populateSeries`, `setOffsetForAtModifier` and `detectHistogramStatsDecoding`) use the path.\r\n\r\nIn downstream project Thanos, [one call](https://github.com/thanos-io/thanos/blob/9c955d21df19e82735b7d3c2906a61a0429825aa/pkg/querysharding/analyzer.go#L103) uses the path. \r\n\r\nIn downstream project Cortex, [two](https://github.com/cortexproject/cortex/blob/e551a2e7673b0a78c27344f782dc599a66ec9022/pkg/util/promql/promql.go#L27) [calls](https://github.com/cortexproject/cortex/blob/e551a2e7673b0a78c27344f782dc599a66ec9022/pkg/querier/tripperware/queryrange/split_by_interval.go#L320)  (out of about five) use the path.\r\n \r\nIn downstream project Mimir, no calls to `Walk()` or `Inspect()` use the path.\r\n\r\nIf Prometheus merges this PR then it is fairly straightforward for Mimir to implement its own non-allocating `Walk` via `ChildrenIter`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2094090634",
    "pr_number": 16330,
    "pr_file": "promql/engine.go",
    "created_at": "2025-05-17T10:47:33+00:00",
    "commented_code": "panic(fmt.Errorf(\"operator %q not allowed for Scalar operations\", op))\n }\n \n+// processCustomBucket handles custom bucket processing for histogram trimming.\n+// It returns the count to keep and the bucket midpoint for sum calculations.\n+func processCustomBucket(\n+\tbucket histogram.Bucket[float64],\n+\trhs float64,\n+\tmode string,\n+) (keepCount, bucketMidpoint float64) {\n+\t// Midpoint calculation\n+\tswitch {\n+\tcase math.IsInf(bucket.Lower, -1):\n+\t\tif bucket.Upper <= 0 {\n+\t\t\tbucketMidpoint = bucket.Upper\n+\t\t} else {\n+\t\t\tbucketMidpoint = 0\n+\t\t}\n+\tcase math.IsInf(bucket.Upper, 1):\n+\t\tif bucket.Lower >= 0 {\n+\t\t\tbucketMidpoint = bucket.Lower\n+\t\t} else {\n+\t\t\tbucketMidpoint = 0\n+\t\t}\n+\tdefault:\n+\t\tbucketMidpoint = (bucket.Lower + bucket.Upper) / 2\n+\t}\n+\n+\t// Fractional keepCount calculation\n+\tswitch mode {\n+\tcase \"TRIM_UPPER\":\n+\t\tswitch {\n+\t\tcase math.IsInf(bucket.Lower, -1):\n+\t\t\t// Special case for -Inf lower bound\n+\t\t\tif rhs >= bucket.Upper {\n+\t\t\t\t// Trim point is above bucket upper bound, keep all\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\t} else {\n+\t\t\t\t// Trim point is within bucket or below, keep none\n+\t\t\t\tkeepCount = 0",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2094090634",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16330,
        "pr_file": "promql/engine.go",
        "discussion_id": "2094090634",
        "commented_code": "@@ -2887,6 +2887,334 @@ func scalarBinop(op parser.ItemType, lhs, rhs float64) float64 {\n \tpanic(fmt.Errorf(\"operator %q not allowed for Scalar operations\", op))\n }\n \n+// processCustomBucket handles custom bucket processing for histogram trimming.\n+// It returns the count to keep and the bucket midpoint for sum calculations.\n+func processCustomBucket(\n+\tbucket histogram.Bucket[float64],\n+\trhs float64,\n+\tmode string,\n+) (keepCount, bucketMidpoint float64) {\n+\t// Midpoint calculation\n+\tswitch {\n+\tcase math.IsInf(bucket.Lower, -1):\n+\t\tif bucket.Upper <= 0 {\n+\t\t\tbucketMidpoint = bucket.Upper\n+\t\t} else {\n+\t\t\tbucketMidpoint = 0\n+\t\t}\n+\tcase math.IsInf(bucket.Upper, 1):\n+\t\tif bucket.Lower >= 0 {\n+\t\t\tbucketMidpoint = bucket.Lower\n+\t\t} else {\n+\t\t\tbucketMidpoint = 0\n+\t\t}\n+\tdefault:\n+\t\tbucketMidpoint = (bucket.Lower + bucket.Upper) / 2\n+\t}\n+\n+\t// Fractional keepCount calculation\n+\tswitch mode {\n+\tcase \"TRIM_UPPER\":\n+\t\tswitch {\n+\t\tcase math.IsInf(bucket.Lower, -1):\n+\t\t\t// Special case for -Inf lower bound\n+\t\t\tif rhs >= bucket.Upper {\n+\t\t\t\t// Trim point is above bucket upper bound, keep all\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\t} else {\n+\t\t\t\t// Trim point is within bucket or below, keep none\n+\t\t\t\tkeepCount = 0",
        "comment_created_at": "2025-05-17T10:47:33+00:00",
        "comment_author": "NeerajGartia21",
        "comment_body": "In the `+Inf` upper bound case below, we heuristically keep half the count assuming the trim point is within the bucket. Should we apply a similar heuristic for the `-Inf` lower bound case for symmetry and clarity?",
        "pr_file_module": null
      },
      {
        "comment_id": "2175183798",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16330,
        "pr_file": "promql/engine.go",
        "discussion_id": "2094090634",
        "commented_code": "@@ -2887,6 +2887,334 @@ func scalarBinop(op parser.ItemType, lhs, rhs float64) float64 {\n \tpanic(fmt.Errorf(\"operator %q not allowed for Scalar operations\", op))\n }\n \n+// processCustomBucket handles custom bucket processing for histogram trimming.\n+// It returns the count to keep and the bucket midpoint for sum calculations.\n+func processCustomBucket(\n+\tbucket histogram.Bucket[float64],\n+\trhs float64,\n+\tmode string,\n+) (keepCount, bucketMidpoint float64) {\n+\t// Midpoint calculation\n+\tswitch {\n+\tcase math.IsInf(bucket.Lower, -1):\n+\t\tif bucket.Upper <= 0 {\n+\t\t\tbucketMidpoint = bucket.Upper\n+\t\t} else {\n+\t\t\tbucketMidpoint = 0\n+\t\t}\n+\tcase math.IsInf(bucket.Upper, 1):\n+\t\tif bucket.Lower >= 0 {\n+\t\t\tbucketMidpoint = bucket.Lower\n+\t\t} else {\n+\t\t\tbucketMidpoint = 0\n+\t\t}\n+\tdefault:\n+\t\tbucketMidpoint = (bucket.Lower + bucket.Upper) / 2\n+\t}\n+\n+\t// Fractional keepCount calculation\n+\tswitch mode {\n+\tcase \"TRIM_UPPER\":\n+\t\tswitch {\n+\t\tcase math.IsInf(bucket.Lower, -1):\n+\t\t\t// Special case for -Inf lower bound\n+\t\t\tif rhs >= bucket.Upper {\n+\t\t\t\t// Trim point is above bucket upper bound, keep all\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\t} else {\n+\t\t\t\t// Trim point is within bucket or below, keep none\n+\t\t\t\tkeepCount = 0",
        "comment_created_at": "2025-06-30T14:13:56+00:00",
        "comment_author": "krajorama",
        "comment_body": "We definitely need at least a  test case for this.\r\n\r\nI can imagine two ways of handling this:\r\n- just do the bucket.Count*0.5 as for +Inf.\r\n- do bucket.Count*0.5  if the bucket.Upper is <= 0.0 otherwise assume this histogram is for positive values only (which we [assume](https://github.com/prometheus/prometheus/blob/2a88f562d12f309944cd5d0c462e3f6f5694dd15/promql/quantile.go#L148) in histogram_quantile as well) and treat bucket.Lower as 0, which means if rhs < 0 then keepCount == 0 otherwise 0 <= rhs < bucket.Upper so we can do linear interpolation.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190156376",
    "pr_number": 16330,
    "pr_file": "promql/engine.go",
    "created_at": "2025-07-07T13:42:47+00:00",
    "commented_code": "panic(fmt.Errorf(\"operator %q not allowed for Scalar operations\", op))\n }\n \n+// computeLinearTrim handles custom bucket processing for histogram trimming.\n+// It returns the count to keep and the bucket midpoint for sum calculations.\n+func computeLinearTrim(\n+\tbucket histogram.Bucket[float64],\n+\trhs float64,\n+\top parser.ItemType,\n+) (keepCount, bucketMidpoint float64) {\n+\t// Midpoint calculation\n+\tswitch {\n+\tcase math.IsInf(bucket.Lower, -1):\n+\t\t// First bucket: no lower bound, assume midpoint is near upper bound.\n+\t\tbucketMidpoint = bucket.Upper\n+\tcase math.IsInf(bucket.Upper, 1):\n+\t\tbucketMidpoint = bucket.Lower\n+\tdefault:\n+\t\tbucketMidpoint = (bucket.Lower + bucket.Upper) / 2\n+\t}\n+\n+\t// Fractional keepCount calculation\n+\tswitch op {\n+\tcase parser.TRIM_UPPER:\n+\t\tswitch {\n+\t\tcase math.IsInf(bucket.Lower, -1):\n+\t\t\t// Special case for -Inf lower bound\n+\t\t\tif rhs >= bucket.Upper {\n+\t\t\t\t// Trim point is above bucket upper bound, keep all\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\t} else {\n+\t\t\t\tif bucket.Upper <= 0 {\n+\t\t\t\t\tkeepCount = bucket.Count * 0.5\n+\t\t\t\t} else {\n+\t\t\t\t\t// Bucket spans from -Inf to some positive value\n+\t\t\t\t\t// Treat -Inf as if it were 0, then do linear interpolation\n+\t\t\t\t\tif rhs < 0 {\n+\t\t\t\t\t\t// Trim point is negative, keep nothing\n+\t\t\t\t\t\tkeepCount = 0\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tfraction := rhs / bucket.Upper\n+\t\t\t\t\t\tkeepCount = bucket.Count * fraction\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\tcase math.IsInf(bucket.Upper, 1):\n+\t\t\t// Special case for +Inf upper bound\n+\t\t\tif rhs <= bucket.Lower {\n+\t\t\t\t// Trim point is below bucket lower bound, keep none\n+\t\t\t\tkeepCount = 0\n+\t\t\t} else {\n+\t\t\t\t// Trim point is within the bucket, keep a portion\n+\t\t\t\t// Since we can't interpolate with +Inf, assume keep half for simplicity\n+\t\t\t\t// Another approach would be to use a different interpolation scheme\n+\t\t\t\tkeepCount = bucket.Count * 0.5\n+\t\t\t}\n+\t\tdefault:\n+\t\t\t// Normal case - finite bounds\n+\t\t\tswitch {\n+\t\t\tcase bucket.Upper <= rhs:\n+\t\t\t\t// Bucket entirely below trim point - keep all\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\tcase bucket.Lower < rhs:\n+\t\t\t\t// Bucket contains trim point - interpolate\n+\t\t\t\tfraction := (rhs - bucket.Lower) / (bucket.Upper - bucket.Lower)\n+\t\t\t\tkeepCount = bucket.Count * fraction\n+\t\t\tdefault:\n+\t\t\t\t// Bucket entirely above trim point - discard\n+\t\t\t\tkeepCount = 0\n+\t\t\t}\n+\t\t}\n+\n+\tcase parser.TRIM_LOWER:\n+\t\tswitch {\n+\t\tcase math.IsInf(bucket.Upper, 1):\n+\t\t\t// Special case for +Inf upper bound\n+\t\t\tif rhs <= bucket.Lower {\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\t} else {\n+\t\t\t\tkeepCount = 0\n+\t\t\t}\n+\t\tcase math.IsInf(bucket.Lower, -1):\n+\t\t\t// Special case for -Inf lower bound\n+\t\t\tif rhs >= bucket.Upper {\n+\t\t\t\tkeepCount = 0\n+\t\t\t} else {\n+\t\t\t\tkeepCount = bucket.Count * 0.5\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tswitch {\n+\t\t\tcase bucket.Lower >= rhs:\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\tcase bucket.Upper > rhs:\n+\t\t\t\tfraction := (bucket.Upper - rhs) / (bucket.Upper - bucket.Lower)\n+\t\t\t\tkeepCount = bucket.Count * fraction\n+\t\t\tdefault:\n+\t\t\t\tkeepCount = 0\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn keepCount, bucketMidpoint\n+}\n+\n+func computeBucketTrim(op parser.ItemType, bucket histogram.Bucket[float64], rhs float64, isPostive, isCustomBucket bool) (float64, float64) {\n+\tif isCustomBucket {\n+\t\treturn computeLinearTrim(bucket, rhs, op)\n+\t}\n+\treturn computeExponentialTrim(bucket, rhs, isPostive, op)\n+}\n+\n+// Helper function to trim native histogram buckets.\n+func trimHistogram(trimmedHist *histogram.FloatHistogram, rhs float64, op parser.ItemType) {",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2190156376",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16330,
        "pr_file": "promql/engine.go",
        "discussion_id": "2190156376",
        "commented_code": "@@ -2900,6 +2900,309 @@ func scalarBinop(op parser.ItemType, lhs, rhs float64) float64 {\n \tpanic(fmt.Errorf(\"operator %q not allowed for Scalar operations\", op))\n }\n \n+// computeLinearTrim handles custom bucket processing for histogram trimming.\n+// It returns the count to keep and the bucket midpoint for sum calculations.\n+func computeLinearTrim(\n+\tbucket histogram.Bucket[float64],\n+\trhs float64,\n+\top parser.ItemType,\n+) (keepCount, bucketMidpoint float64) {\n+\t// Midpoint calculation\n+\tswitch {\n+\tcase math.IsInf(bucket.Lower, -1):\n+\t\t// First bucket: no lower bound, assume midpoint is near upper bound.\n+\t\tbucketMidpoint = bucket.Upper\n+\tcase math.IsInf(bucket.Upper, 1):\n+\t\tbucketMidpoint = bucket.Lower\n+\tdefault:\n+\t\tbucketMidpoint = (bucket.Lower + bucket.Upper) / 2\n+\t}\n+\n+\t// Fractional keepCount calculation\n+\tswitch op {\n+\tcase parser.TRIM_UPPER:\n+\t\tswitch {\n+\t\tcase math.IsInf(bucket.Lower, -1):\n+\t\t\t// Special case for -Inf lower bound\n+\t\t\tif rhs >= bucket.Upper {\n+\t\t\t\t// Trim point is above bucket upper bound, keep all\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\t} else {\n+\t\t\t\tif bucket.Upper <= 0 {\n+\t\t\t\t\tkeepCount = bucket.Count * 0.5\n+\t\t\t\t} else {\n+\t\t\t\t\t// Bucket spans from -Inf to some positive value\n+\t\t\t\t\t// Treat -Inf as if it were 0, then do linear interpolation\n+\t\t\t\t\tif rhs < 0 {\n+\t\t\t\t\t\t// Trim point is negative, keep nothing\n+\t\t\t\t\t\tkeepCount = 0\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tfraction := rhs / bucket.Upper\n+\t\t\t\t\t\tkeepCount = bucket.Count * fraction\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\tcase math.IsInf(bucket.Upper, 1):\n+\t\t\t// Special case for +Inf upper bound\n+\t\t\tif rhs <= bucket.Lower {\n+\t\t\t\t// Trim point is below bucket lower bound, keep none\n+\t\t\t\tkeepCount = 0\n+\t\t\t} else {\n+\t\t\t\t// Trim point is within the bucket, keep a portion\n+\t\t\t\t// Since we can't interpolate with +Inf, assume keep half for simplicity\n+\t\t\t\t// Another approach would be to use a different interpolation scheme\n+\t\t\t\tkeepCount = bucket.Count * 0.5\n+\t\t\t}\n+\t\tdefault:\n+\t\t\t// Normal case - finite bounds\n+\t\t\tswitch {\n+\t\t\tcase bucket.Upper <= rhs:\n+\t\t\t\t// Bucket entirely below trim point - keep all\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\tcase bucket.Lower < rhs:\n+\t\t\t\t// Bucket contains trim point - interpolate\n+\t\t\t\tfraction := (rhs - bucket.Lower) / (bucket.Upper - bucket.Lower)\n+\t\t\t\tkeepCount = bucket.Count * fraction\n+\t\t\tdefault:\n+\t\t\t\t// Bucket entirely above trim point - discard\n+\t\t\t\tkeepCount = 0\n+\t\t\t}\n+\t\t}\n+\n+\tcase parser.TRIM_LOWER:\n+\t\tswitch {\n+\t\tcase math.IsInf(bucket.Upper, 1):\n+\t\t\t// Special case for +Inf upper bound\n+\t\t\tif rhs <= bucket.Lower {\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\t} else {\n+\t\t\t\tkeepCount = 0\n+\t\t\t}\n+\t\tcase math.IsInf(bucket.Lower, -1):\n+\t\t\t// Special case for -Inf lower bound\n+\t\t\tif rhs >= bucket.Upper {\n+\t\t\t\tkeepCount = 0\n+\t\t\t} else {\n+\t\t\t\tkeepCount = bucket.Count * 0.5\n+\t\t\t}\n+\t\tdefault:\n+\t\t\tswitch {\n+\t\t\tcase bucket.Lower >= rhs:\n+\t\t\t\tkeepCount = bucket.Count\n+\t\t\tcase bucket.Upper > rhs:\n+\t\t\t\tfraction := (bucket.Upper - rhs) / (bucket.Upper - bucket.Lower)\n+\t\t\t\tkeepCount = bucket.Count * fraction\n+\t\t\tdefault:\n+\t\t\t\tkeepCount = 0\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\treturn keepCount, bucketMidpoint\n+}\n+\n+func computeBucketTrim(op parser.ItemType, bucket histogram.Bucket[float64], rhs float64, isPostive, isCustomBucket bool) (float64, float64) {\n+\tif isCustomBucket {\n+\t\treturn computeLinearTrim(bucket, rhs, op)\n+\t}\n+\treturn computeExponentialTrim(bucket, rhs, isPostive, op)\n+}\n+\n+// Helper function to trim native histogram buckets.\n+func trimHistogram(trimmedHist *histogram.FloatHistogram, rhs float64, op parser.ItemType) {",
        "comment_created_at": "2025-07-07T13:42:47+00:00",
        "comment_author": "krajorama",
        "comment_body": "This function and the compute*Trim functions got a little bit complicated. Also as I note later, the computeExponentialTrim function is missing handling of overflow -Inf, +Inf buckets.\r\n\r\nI'd like to suggest a simplification (we can discuss putting it in a follow-up PR)\r\nlet's have a function that has this signature:\r\n```\r\ncomputeSplit(bucket histogram.Bucket[float64], le float64, isPostive, isCustomBucket bool) (underCount, bucketMidPoint)\r\n```\r\n\r\nThis would return the portion of the bucket count that's under or equal to the le and the midPoint of the bucket values (if possible).\r\n\r\nNote that this doesn't have to take the Op. Since the result is directly usable as keepCount for TRIM_UPPER and you just need to subtract the underCount from the bucket count to get the keepCount value for TRIM_LOWER.\r\n\r\nAlso this would make the algorithm closer to https://github.com/prometheus/prometheus/blob/ffcba01c5a4e7f5f35a15a30ba283409e8cd0138/promql/quantile.go#L386 which does something very similar.\r\n\r\nAlso also, the special handling for (-Inf, positive] bucket for custom buckets will never kick in for exponential histograms, since they don't have such bucket.\r\n\r\nThe resulting function should be possible to reuse here easily.\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1993705651",
    "pr_number": 16192,
    "pr_file": "promql/functions.go",
    "created_at": "2025-03-13T14:52:52+00:00",
    "commented_code": "durationToStart = durationToZero\n \t\t}\n \t}\n+\t// Histogram total count clamping logic\n+\t// Using the first sample's count as the master metric, we ensure that\n+\t// if extrapolation would take the count below zero, we clamp the extrapolation.\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 {\n+\t\t// Calculate the duration over which, at the current total rate, the first sample's count would become zero.\n+\t\tdurationToZero := sampledInterval * (samples.Histograms[0].H.Count / resultHistogram.Count)\n+\t\t// Clamp durationToStart so we don't extrapolate beyond the point where the count would be zero.\n+\t\tif durationToZero < durationToStart {\n+\t\t\tdurationToStart = durationToZero\n+\t\t}\n+\t}\n+\n \textrapolateToInterval += durationToStart\n \n+\t// Histogram bucket clamping logic\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 && resultHistogram.Sum > 0 {",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "1993705651",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "1993705651",
        "commented_code": "@@ -162,8 +162,43 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\t\tdurationToStart = durationToZero\n \t\t}\n \t}\n+\t// Histogram total count clamping logic\n+\t// Using the first sample's count as the master metric, we ensure that\n+\t// if extrapolation would take the count below zero, we clamp the extrapolation.\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 {\n+\t\t// Calculate the duration over which, at the current total rate, the first sample's count would become zero.\n+\t\tdurationToZero := sampledInterval * (samples.Histograms[0].H.Count / resultHistogram.Count)\n+\t\t// Clamp durationToStart so we don't extrapolate beyond the point where the count would be zero.\n+\t\tif durationToZero < durationToStart {\n+\t\t\tdurationToStart = durationToZero\n+\t\t}\n+\t}\n+\n \textrapolateToInterval += durationToStart\n \n+\t// Histogram bucket clamping logic\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 && resultHistogram.Sum > 0 {",
        "comment_created_at": "2025-03-13T14:52:52+00:00",
        "comment_author": "beorn7",
        "comment_body": "```suggestion\r\n\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 {\r\n```\r\n\r\nSum can be negative. That's fine.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1993767596",
    "pr_number": 16192,
    "pr_file": "promql/functions.go",
    "created_at": "2025-03-13T15:21:50+00:00",
    "commented_code": "durationToStart = durationToZero\n \t\t}\n \t}\n+\t// Histogram total count clamping logic\n+\t// Using the first sample's count as the master metric, we ensure that\n+\t// if extrapolation would take the count below zero, we clamp the extrapolation.\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 {\n+\t\t// Calculate the duration over which, at the current total rate, the first sample's count would become zero.\n+\t\tdurationToZero := sampledInterval * (samples.Histograms[0].H.Count / resultHistogram.Count)\n+\t\t// Clamp durationToStart so we don't extrapolate beyond the point where the count would be zero.\n+\t\tif durationToZero < durationToStart {\n+\t\t\tdurationToStart = durationToZero\n+\t\t}\n+\t}\n+\n \textrapolateToInterval += durationToStart\n \n+\t// Histogram bucket clamping logic\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 && resultHistogram.Sum > 0 {\n+\t\t// If we leave Bucket rates as is, they will hit 0 either to early or too late.\n+\t\t// To fix this, we take last sample and new zero point and claculate what rate we need.\n+\t\tlastHistogram := samples.Histograms[len(samples.Histograms)-1].H\n+\t\tvar totalSumDelta float64\n+\t\tfor i, bucketRate := range resultHistogram.PositiveBuckets {\n+\t\t\t// Calculate this bucket's proportion of the total rate\n+\t\t\tbucketProportion := bucketRate / resultHistogram.Sum",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "1993767596",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "1993767596",
        "commented_code": "@@ -162,8 +162,43 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\t\tdurationToStart = durationToZero\n \t\t}\n \t}\n+\t// Histogram total count clamping logic\n+\t// Using the first sample's count as the master metric, we ensure that\n+\t// if extrapolation would take the count below zero, we clamp the extrapolation.\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 {\n+\t\t// Calculate the duration over which, at the current total rate, the first sample's count would become zero.\n+\t\tdurationToZero := sampledInterval * (samples.Histograms[0].H.Count / resultHistogram.Count)\n+\t\t// Clamp durationToStart so we don't extrapolate beyond the point where the count would be zero.\n+\t\tif durationToZero < durationToStart {\n+\t\t\tdurationToStart = durationToZero\n+\t\t}\n+\t}\n+\n \textrapolateToInterval += durationToStart\n \n+\t// Histogram bucket clamping logic\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 && resultHistogram.Sum > 0 {\n+\t\t// If we leave Bucket rates as is, they will hit 0 either to early or too late.\n+\t\t// To fix this, we take last sample and new zero point and claculate what rate we need.\n+\t\tlastHistogram := samples.Histograms[len(samples.Histograms)-1].H\n+\t\tvar totalSumDelta float64\n+\t\tfor i, bucketRate := range resultHistogram.PositiveBuckets {\n+\t\t\t// Calculate this bucket's proportion of the total rate\n+\t\t\tbucketProportion := bucketRate / resultHistogram.Sum",
        "comment_created_at": "2025-03-13T15:21:50+00:00",
        "comment_author": "beorn7",
        "comment_body": "Why Sum? If at all, it should be Count.\r\nBut I don't understand the logic of the \"proportion of the rate\" in the first place.\r\n\r\nMaybe I haven't wrapped my head sufficiently far around it. But broadly, I think we have to do something different depending on whether we have clamped the durationToStart above.\r\n\r\nIf `durationToZero > durationToStart`, we only have to manipulate the buckets if they would be extrapolated below zero at `durationToStart`.\r\n\r\nIf `durationToZero == durationToStart`, we have to manipulate all the buckets.\r\n\r\nAfter the manipulation, the manipulated buckets (only) should extrapolate te exactly zero at `durationToStart`.\r\n\r\nI can try to cobble together the equation, but maybe this is already enough to put you on track (or put me on track if I have missed something).",
        "pr_file_module": null
      },
      {
        "comment_id": "2028787746",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "1993767596",
        "commented_code": "@@ -162,8 +162,43 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\t\tdurationToStart = durationToZero\n \t\t}\n \t}\n+\t// Histogram total count clamping logic\n+\t// Using the first sample's count as the master metric, we ensure that\n+\t// if extrapolation would take the count below zero, we clamp the extrapolation.\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 {\n+\t\t// Calculate the duration over which, at the current total rate, the first sample's count would become zero.\n+\t\tdurationToZero := sampledInterval * (samples.Histograms[0].H.Count / resultHistogram.Count)\n+\t\t// Clamp durationToStart so we don't extrapolate beyond the point where the count would be zero.\n+\t\tif durationToZero < durationToStart {\n+\t\t\tdurationToStart = durationToZero\n+\t\t}\n+\t}\n+\n \textrapolateToInterval += durationToStart\n \n+\t// Histogram bucket clamping logic\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 && resultHistogram.Sum > 0 {\n+\t\t// If we leave Bucket rates as is, they will hit 0 either to early or too late.\n+\t\t// To fix this, we take last sample and new zero point and claculate what rate we need.\n+\t\tlastHistogram := samples.Histograms[len(samples.Histograms)-1].H\n+\t\tvar totalSumDelta float64\n+\t\tfor i, bucketRate := range resultHistogram.PositiveBuckets {\n+\t\t\t// Calculate this bucket's proportion of the total rate\n+\t\t\tbucketProportion := bucketRate / resultHistogram.Sum",
        "comment_created_at": "2025-04-04T13:10:33+00:00",
        "comment_author": "gen1321",
        "comment_body": "Yes I think you are totally right, I missed that we can actually make histograms schemas match(in hindsight it's obvious), and this is why I actually did this whole thing.\r\n\r\nSo we actually can match resultHistogram rate with first histogram, and then do all the calculations easily. \r\n\r\nThe only downside I think is that we will lose some precision in case where when resultHistogram.Schema > firstHistogram.Schema \r\n\r\nPlease let me know it is a problem or incorrect and I am missing something :) ",
        "pr_file_module": null
      },
      {
        "comment_id": "2037097378",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "1993767596",
        "commented_code": "@@ -162,8 +162,43 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\t\tdurationToStart = durationToZero\n \t\t}\n \t}\n+\t// Histogram total count clamping logic\n+\t// Using the first sample's count as the master metric, we ensure that\n+\t// if extrapolation would take the count below zero, we clamp the extrapolation.\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 {\n+\t\t// Calculate the duration over which, at the current total rate, the first sample's count would become zero.\n+\t\tdurationToZero := sampledInterval * (samples.Histograms[0].H.Count / resultHistogram.Count)\n+\t\t// Clamp durationToStart so we don't extrapolate beyond the point where the count would be zero.\n+\t\tif durationToZero < durationToStart {\n+\t\t\tdurationToStart = durationToZero\n+\t\t}\n+\t}\n+\n \textrapolateToInterval += durationToStart\n \n+\t// Histogram bucket clamping logic\n+\tif isCounter && resultHistogram != nil && resultHistogram.Count > 0 && resultHistogram.Sum > 0 {\n+\t\t// If we leave Bucket rates as is, they will hit 0 either to early or too late.\n+\t\t// To fix this, we take last sample and new zero point and claculate what rate we need.\n+\t\tlastHistogram := samples.Histograms[len(samples.Histograms)-1].H\n+\t\tvar totalSumDelta float64\n+\t\tfor i, bucketRate := range resultHistogram.PositiveBuckets {\n+\t\t\t// Calculate this bucket's proportion of the total rate\n+\t\t\tbucketProportion := bucketRate / resultHistogram.Sum",
        "comment_created_at": "2025-04-10T11:05:18+00:00",
        "comment_author": "beorn7",
        "comment_body": "Not quite sure.\r\n\r\nMaking all the schemas match is part of the normal rate calculation anyway. First we find the largest common schema, merge buckets in higher-res histograms to match that schema, and only then we start to do the math on all those histograms that now have the same schema.\r\n\r\nThe extrapolation logic only happens in that second part, so different schemas should not be an issue.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2157267948",
    "pr_number": 16192,
    "pr_file": "promql/functions.go",
    "created_at": "2025-06-19T15:23:58+00:00",
    "commented_code": "// The histograms are not compatible with each other.\n \t\t\treturn enh.Out, annos\n \t\t}\n+\t\t// Find the first histogram sample compatible with our result so we can extrapolate correctly.\n+\t\tfor _, h := range samples.Histograms {\n+\t\t\tif h.H.UsesCustomBuckets() == resultHistogram.UsesCustomBuckets() {\n+\t\t\t\tfirstH = h.H\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2157267948",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157267948",
        "commented_code": "@@ -103,6 +104,13 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\t\t// The histograms are not compatible with each other.\n \t\t\treturn enh.Out, annos\n \t\t}\n+\t\t// Find the first histogram sample compatible with our result so we can extrapolate correctly.\n+\t\tfor _, h := range samples.Histograms {\n+\t\t\tif h.H.UsesCustomBuckets() == resultHistogram.UsesCustomBuckets() {\n+\t\t\t\tfirstH = h.H\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}",
        "comment_created_at": "2025-06-19T15:23:58+00:00",
        "comment_author": "beorn7",
        "comment_body": "Oooh, this is a good catch. Since we effectively ignore the 1st histogram, if there is a counter reset between the 1st and 2nd, we also allow an incompatible bucket layout between the 1st and 2nd histogram.\r\n\r\nHowever, I think we cannot just take the 2nd histogram as the 1st and go on.\r\n\r\nWe do have a similar situation with floats, just that we don't have incompatible buckets there (obviously). While we effectively ignore the value of the 1st float (if there is a counter reset between the 1st and 2nd float), we _still_ take the 1st float into account when checking for extrapolation below zero. And that's actually important. A counter reset sets the counter to zero at some point (in this case between the 1st and 2nd float), but this also means that the counter was greater than zero _before_ the reset. We have to limit extrapolation only if the extrapolation would go below zero when extrapolating from the value of the _1st_ sample.\r\n\r\nSo we need to do the same for histograms. However, if the bucket layout is incompatible, we cannot take buckets into account.\r\n\r\n\"Luckily\", I had a lot of thoughts about all of this, inspired by your code in this PR, and by now I believe, we should actually only limit extrapolation based on the count, and not on the buckets, so we don't even have to take buckets into account.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2157726029",
    "pr_number": 16192,
    "pr_file": "promql/functions.go",
    "created_at": "2025-06-19T22:32:35+00:00",
    "commented_code": "resultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2157726029",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-06-19T22:32:35+00:00",
        "comment_author": "beorn7",
        "comment_body": "After a lot of thinking about this, I ultimately came to the conclusion that we should not manipulate buckets after all. Sorry for laying this false trail. (Your work was not in vein, because I would not have been able to think about it in this way without it.)\r\n\r\nIn short, we must keep histograms consistent. For one, that means that the sum of all buckets is equal to the count. (Unless there are NaN observations, but that's a story for another day.) If we scale individual buckets, we have to adjust the count accordingly. And that's what you are doing here. However, I think that creates more harm than good. An individual bucket with some weird behavior might throw off the extrapolation for the whole histogram. I also think there is value if the count alone behaves as we are used to with counters.\r\n\r\nAt the end of the day, this whole heuristics is only a little tweak to avoid overestimating rates and increases. I think using just the count for that is simple, easy to reason with, and probably just right in most cases. In particular, in the new brave world with created-at timestamps, we will have synthetic samples that are zero everywhere (in the count and sum and all buckets), and we'll then stop extrapolation exactly an that sample, which is an exact and perfect result.\r\n\r\nLong story short, I think this code can all go, and we are all good with the simple code above that does the cut-off based on the count.",
        "pr_file_module": null
      },
      {
        "comment_id": "2163225744",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-06-24T07:56:52+00:00",
        "comment_author": "krajorama",
        "comment_body": "I agree that we should base the decision on the overall count, for two reasons:\r\n1. it has information on all buckets, so it should be the truth\r\n2. when we calculate `histogram_count(rate(...))` in PromQL we do an optimization to not load the buckets, so we only have the count and we should be consistent between that and `rate(...)` without `histogram_count`.\r\n\r\nSo your suggested approach to only use the overall count I think is good step in the right direction.\r\n\r\nHowever, maybe we should do a second step because it's pretty easy to make an example where this approach gives big skew. In my example I have a histogram where bucket1 goes from 10 to 15 and bucket 2 goes from 15 to 60. The overall count is 25 to 75 and samples interval is 55.\r\n\r\nIn this case time to zero is `55*25/50` = 27.5. Which gives a factor of `(55+27.5)/55` = 1.5.\r\n\r\nInterpolation will say that the buckets increase is: 7.5 and 67.5 , but in reality it's 15 and 60.\r\n\r\nI've drawn it up:\r\n![image](https://github.com/user-attachments/assets/cbdc10ff-5977-4888-b64c-561f60a95b91)\r\n\r\nYou can see that the way to actually calculate this is to not use `Mul`, but calculate the result directly based on all the information. Figure out the duration to zero from the overall count. If duration to zero inside the range, then iterate all buckets and for each bucket calculate the duration to zero for that bucket. If the duration to zero is greater or equal to the overall duration to zero, than we can just take the second value as increase (bucket 1). If it's less then we need to calculate the increase with the adjusted duration to zero for that bucket (bucket 2).\r\n\r\nIn my case:\r\n1. bucket 1 duration to zero : `55*10/5` = 110 way greater than 27.5, so we take the second value: 15.\r\n2. bucket 2 duration to zero : `55*15/45` = 18.33, which is smaller so we calculate: `45*(55+18.33)/55` = 60.",
        "pr_file_module": null
      },
      {
        "comment_id": "2167691168",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-06-25T21:45:14+00:00",
        "comment_author": "beorn7",
        "comment_body": "Thanks for the drawing. I was about to do that myself, but then stopped myself when I thought I had understood the problem sufficiently. Let's see how far this holds.\r\n\r\nI see a problem with your example: You have put the end of the range at the same time as the 2nd (and last) sample in the range. However, that is generally not the case. The range usually goes on a bit after the last sample. If you modify your example accordingly, I'm pretty sure that you won't arrive at a consistent histogram, i.e. where the sum of the buckets is still the same as the count. (This is the problem that @gen1321  fought with in the current state of the code, where he modified the count value to again match the sum of the buckets.)\r\n\r\nWith your lucky coincidence, the outcome of the `increase` operation is essentially the same as the last point in the range, i.e. `count=75 bucket1=15 bucket2=60`. But if you go on a bit to the right with the slopes you have calculated, you'll run into a deviation between the count and the sum of the buckets.\r\n\r\nBut I think you are up to something here. I guess the following could be a working algorithm:\r\n\r\n1. Calculate the zero point solely based on the count, for all the reasons stated, including your very good added point about the optimization for `histogram_count(rate(foo[1m]))`.\r\n2. At that zero point, we virtually add a zero sample, where count and sum and all buckets are zero.\r\n3. Now we effectively redo the whole calculation including the new zero point but extrapolate from the zero point to the previously determined end point (usually the end of the range).\r\n\r\n(This is just the high level description. The detailed flow has to be different for best efficiency.)\r\n\r\nWDYT?",
        "pr_file_module": null
      },
      {
        "comment_id": "2169152989",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-06-26T14:01:44+00:00",
        "comment_author": "krajorama",
        "comment_body": "I wanted to keep the calculation simpler by not taking into account the right hand side.\r\n\r\nFor the beginning of the range:\r\nIf the duration to zero calculated from the overall count is inside the duration to start, then it sounds like a good idea to add a zero point, since we do want to extrapolate to zero, but this way we make the zero point for ourselves. This also solves what I wanted to solve which is individual buckets going to negative.\r\n\r\nIf the duration to zero is outside the duration to start, an individual bucket may still extrapolate to less than zero because it's slope could be steeper and zero point fall inside the duration to start. I assume you wouldn't insert a zero sample in this case. My algorithm I guess could still work by looking at the duration to zero for each bucket.\r\n\r\nI need to understand how I would take the right hand side extrapolation into account.\r\n\r\nIn general I think people expect native histograms to be better and I think that's still the case if we take a bit more CPU. Also maybe we can do an algorithm that's \"just\" slower than current with a const, as in 2*O(n) instead of O(n) which should be fine.",
        "pr_file_module": null
      },
      {
        "comment_id": "2169207177",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-06-26T14:25:57+00:00",
        "comment_author": "beorn7",
        "comment_body": "> I wanted to keep the calculation simpler by not taking into account the right hand side.\r\n\r\nYou have to explain to me how to do that if the right boundary of the range does not coincide with a sample.\r\n\r\n> If the duration to zero is outside the duration to start, an individual bucket may still extrapolate to less than zero because it's slope could be steeper and zero point fall inside the duration to start. I assume you wouldn't insert a zero sample in this case. My algorithm I guess could still work by looking at the duration to zero for each bucket.\r\n\r\nIndeed no zero point if count doesn't extrapolate below zero within the relevant range.\r\n\r\nHowever, it will be hard to manipulate individual buckets and still keep the histogram consistent. See what @gen1321 tried in this PR (which attempts to also manipulate count, but that's what we definitely don't want, for all the reasons stated). \r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2172047777",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-06-27T13:26:49+00:00",
        "comment_author": "krajorama",
        "comment_body": "I've made a POC that sounds about right to me intuitively. Haven't done formal proof yet: https://github.com/prometheus/prometheus/pull/16791\r\n\r\nThe idea is to extrapolate the right hand side (towards end of range) as currently, but compensate for the \"bad\" cases on the left (towards the start of the range).\r\n\r\nIt matches the expectations on my synthetic test case.\r\n\r\nIt will reduce the extrapolation if the zero time of an individual bucket is inside the start time (do not go below zero).\r\nIt will increase the extrapolation if the zero time of the whole histogram (from overall count) is later than the zero time of  an individual bucket.",
        "pr_file_module": null
      },
      {
        "comment_id": "2173891130",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-06-29T19:37:18+00:00",
        "comment_author": "gen1321",
        "comment_body": "Thanks both for the great discussion! @beorn7 You definitely didn't lay any false\r\ntrails, in fact, your support on that topic was a really critical :)\r\nAnd @krajorama thanks for the POC, was really helpful for my\r\nunderstanding of this new approach\r\n\r\nIf we are able to produce greater precision with \"POC\" approach, that's\r\ngreat, my only concern is things like custom schemas, resets with different layout, etc. All of those\r\nmake approaches with bucket manipulation really complex.\r\n\r\nOne maybe other benefit of going with simpler \"total count\" approach is that it is much easier to understand, which i feel in itself carries a lot of value.\r\nI think the approach with inserting zero point would be most intuitive one\r\n\r\nBut I would be happy to go either way ",
        "pr_file_module": null
      },
      {
        "comment_id": "2174672364",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-06-30T09:46:54+00:00",
        "comment_author": "krajorama",
        "comment_body": "My take on the three approaches:\r\n- Calculate factor (that we multiply the whole result with) from the zero point of the overall count only: simplest approach. Can go below zero for individual buckets if bucket zero point is inside the zero point of the overall count. Result is consistent.\r\n- Insert artificial zero sample at the zero point based on the overall count, if it's inside duration to start. (If it isn't then we cannot do this as we'd do something different from when we only take a count from the histogram and doing float extrapolation):    this would be relatively simple, but means we redo the calculation again. Can go below zero for individual buckets if overall zero point is outside the duration to start, but a bucket zero point is inside the zero point of the overall count. Result is consistent.\r\n- Calculation on bucket level (my POC): extrapolate the end of the range as now , but deal with the start of the range per bucket. Requires a rewrite of the algorithm to keep track of the first sample that the normal rate calculation used and to deal with all the cases. I think the result would be accurate and consistent, but no proof yet.\r\n\r\nI think I'd go with the first option for now and work on the last option for the long term. wdyt @beorn7 ?",
        "pr_file_module": null
      },
      {
        "comment_id": "2180071366",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-07-02T13:28:12+00:00",
        "comment_author": "beorn7",
        "comment_body": "I fully agree with @krajorama's description of the first two options. I think both are spot on. I will look into @krajorama's POC ASAP (ideally right now, if nothing happens that distracts me or I faint because of the heat :sweat:).\r\n\r\nIdeally, we find an option now that is likely to stay (so we can declare NH stable). (Alternatively, we do something preliminary now and come back to this once it is the last thing that keeps us from declaring stability. Then we can still decide to change our mind or to make an exception from the stability guarantees (\"Will change extrapolation behavior later\"). Needless to say that I would prefer to not make that exception. But I'm getting ahead of ourselves.)",
        "pr_file_module": null
      },
      {
        "comment_id": "2180082001",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-07-02T13:33:03+00:00",
        "comment_author": "beorn7",
        "comment_body": "> my only concern is things like custom schemas, resets with different layout, etc. All of those\r\n> make approaches with bucket manipulation really complex.\r\n\r\nI believe that these problems are (with the current state of things) separate from the extrapolation (fortunately). Currently, if there is a mix of buckets with NHCB and standard exponential buckets, we don't return anything (and warn about it). If there are different NHCB bucket boundaries, we don't return anything either. Finally, if we have different standard exponential buckets, we first convert all involved histograms to the lowest-resolution common schema. In other words: Once the rate calculation kicks in, all histograms have the same bucket schema.",
        "pr_file_module": null
      },
      {
        "comment_id": "2180148418",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-07-02T14:02:12+00:00",
        "comment_author": "beorn7",
        "comment_body": "About the counter reset between the first two samples: This is actually easy to deal with because we effectively assume a zero sample at the time of the first sample in that case.\r\n(I guess @krajorama note in the POC is only there to let us know that those special cases aren't handled yet, not that these special cases are problematic.)",
        "pr_file_module": null
      },
      {
        "comment_id": "2180661799",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16192,
        "pr_file": "promql/functions.go",
        "discussion_id": "2157726029",
        "commented_code": "@@ -179,6 +199,44 @@ func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNod\n \t\tresultHistogram.Mul(factor)\n \t}\n \n+\tif isCounter && resultHistogram != nil && firstH != nil && resultHistogram.Count > 0 {\n+\t\trangeSeconds := ms.Range.Seconds()\n+\t\t// How much we \"scaled\" sampled interval.\n+\t\tdeltaScale := sampledInterval / extrapolateToInterval\n+\t\tif isRate {\n+\t\t\tdeltaScale *= rangeSeconds\n+\t\t}\n+\t\tfor bucketIndex, bucketVal := range resultHistogram.PositiveBuckets {\n+\t\t\tif bucketVal <= 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Get bucket value from the first sample.\n+\t\t\tbucketStartVal := firstH.PositiveBuckets[bucketIndex]\n+\t\t\t// Raw change in bucket value between last and first samples.\n+\t\t\tbucketDelta := bucketVal * deltaScale\n+\t\t\t// Predict bucket value at the lower boundary of the range.\n+\t\t\tpredictedAtStart := bucketStartVal - (bucketDelta / sampledInterval * durationToStart)\n+\t\t\tif durationToStart == durationToZeroCount || (predictedAtStart < 0 && bucketStartVal > 0) {\n+\t\t\t\t// Predict bucket value at the upper boundary of the range.\n+\t\t\t\tpredictedAtEnd := bucketStartVal + (bucketDelta * (1 + durationToEnd/sampledInterval))\n+\t\t\t\tvar countDiff float64\n+\t\t\t\tif isRate {\n+\t\t\t\t\t// Calculate the adjusted per-second rate based on the predicted count at the range end.\n+\t\t\t\t\t// This rate reflects the increase from the estimated zero point (clamped at range start)\n+\t\t\t\t\t// to the range end, preventing negative extrapolation artifacts.\n+\t\t\t\t\tadjustedRate := predictedAtEnd / rangeSeconds\n+\t\t\t\t\tcountDiff = adjustedRate - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = adjustedRate\n+\t\t\t\t} else {\n+\t\t\t\t\t// For non-rate functions, calculate the total increase from start to end\n+\t\t\t\t\t// and use the predicted end value as the adjusted value\n+\t\t\t\t\tcountDiff = predictedAtEnd - bucketVal\n+\t\t\t\t\tresultHistogram.PositiveBuckets[bucketIndex] = predictedAtEnd\n+\t\t\t\t}\n+\t\t\t\tresultHistogram.Count += countDiff\n+\t\t\t}\n+\t\t}\n+\t}",
        "comment_created_at": "2025-07-02T18:00:34+00:00",
        "comment_author": "beorn7",
        "comment_body": "> About the counter reset between the first two samples: This is actually easy to deal with because we effectively assume a zero sample at the time of the first sample in that case.\r\n\r\nI have to correct myself. This _is_ actually a problem. While it's true that we assume that right after the 1st sample, everything was reset to zero, we will _not_ stop extrapolation there. In the float case, we take the value of the 1st sample and go down from there. We could still stop with extrapolation if we hit zero from that higher starting point, but we generally do not stop extrapolation at the time of the 1st sample.\r\n\r\nTherefore, if we want to take into account individual buckets, we either have to stop ignoring the bucket layout for the 1st sample, or we have to treat this specific case (counter reset between 1st and 2nd sample AND 1st and 2nd sample have incompatible buckets) specially by not checking for individual buckets in this case.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2166491037",
    "pr_number": 16773,
    "pr_file": "promql/functions.go",
    "created_at": "2025-06-25T11:31:32+00:00",
    "commented_code": "return vec, nil\n \t}\n \treturn aggrOverTime(vals, enh, func(s Series) float64 {\n-\t\tvar mean, kahanC float64\n-\t\tfor i, f := range s.Floats {\n+\t\tvar (\n+\t\t\tsum             = s.Floats[0].F\n+\t\t\tmean, kahanC    float64\n+\t\t\tincrementalMean bool\n+\t\t)\n+\t\tfor i, f := range s.Floats[1:] {\n \t\t\tcount := float64(i + 1)",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2166491037",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16773,
        "pr_file": "promql/functions.go",
        "discussion_id": "2166491037",
        "commented_code": "@@ -716,13 +723,35 @@ func funcAvgOverTime(vals []parser.Value, args parser.Expressions, enh *EvalNode\n \t\treturn vec, nil\n \t}\n \treturn aggrOverTime(vals, enh, func(s Series) float64 {\n-\t\tvar mean, kahanC float64\n-\t\tfor i, f := range s.Floats {\n+\t\tvar (\n+\t\t\tsum             = s.Floats[0].F\n+\t\t\tmean, kahanC    float64\n+\t\t\tincrementalMean bool\n+\t\t)\n+\t\tfor i, f := range s.Floats[1:] {\n \t\t\tcount := float64(i + 1)",
        "comment_created_at": "2025-06-25T11:31:32+00:00",
        "comment_author": "krajorama",
        "comment_body": "It was really hard to reconcile the algorithm in `avg` and this `avg_over_time`.\r\n```suggestion\r\n\t\t\tcount := float64(i + 2)\r\n```\r\nIn avg_over_time we create groups with count==1 and the first number added makes it count==2. \r\n\r\nHere with this change we'd consider the 0th element in sum as count==1 and first element from the rest of the list as count==2. And also this way the `count` at the end as calculated by `len(s.Floats)` would mean the same thing.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2166577923",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16773,
        "pr_file": "promql/functions.go",
        "discussion_id": "2166491037",
        "commented_code": "@@ -716,13 +723,35 @@ func funcAvgOverTime(vals []parser.Value, args parser.Expressions, enh *EvalNode\n \t\treturn vec, nil\n \t}\n \treturn aggrOverTime(vals, enh, func(s Series) float64 {\n-\t\tvar mean, kahanC float64\n-\t\tfor i, f := range s.Floats {\n+\t\tvar (\n+\t\t\tsum             = s.Floats[0].F\n+\t\t\tmean, kahanC    float64\n+\t\t\tincrementalMean bool\n+\t\t)\n+\t\tfor i, f := range s.Floats[1:] {\n \t\t\tcount := float64(i + 1)",
        "comment_created_at": "2025-06-25T12:19:01+00:00",
        "comment_author": "beorn7",
        "comment_body": "Not sure if I fully understand. Could you check the commit I have just pushed to see if it is what you are fishing for?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2045629781",
    "pr_number": 16404,
    "pr_file": "promql/engine.go",
    "created_at": "2025-04-15T22:23:22+00:00",
    "commented_code": "}\n \tgroups := make([]groupedAggregation, groupCount)\n \n-\tvar k int64\n-\tvar ratio float64\n \tvar seriess map[uint64]Series\n-\tswitch aggExpr.Op {\n-\tcase parser.TOPK, parser.BOTTOMK, parser.LIMITK:\n-\t\tif !convertibleToInt64(param) {\n-\t\t\tev.errorf(\"Scalar value %v overflows int64\", param)\n-\t\t}\n-\t\tk = int64(param)\n-\t\tif k > int64(len(inputMatrix)) {\n-\t\t\tk = int64(len(inputMatrix))\n+\t// allParamsMatchCondition returns true if the provided condition holds for every float params.\n+\tallParamsMatchCondition := func(params []FPoint, constParam bool, condition func(float64) bool) bool {",
    "repo_full_name": "prometheus/prometheus",
    "discussion_comments": [
      {
        "comment_id": "2045629781",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16404,
        "pr_file": "promql/engine.go",
        "discussion_id": "2045629781",
        "commented_code": "@@ -1359,46 +1359,46 @@ func (ev *evaluator) rangeEvalAgg(ctx context.Context, aggExpr *parser.Aggregate\n \t}\n \tgroups := make([]groupedAggregation, groupCount)\n \n-\tvar k int64\n-\tvar ratio float64\n \tvar seriess map[uint64]Series\n-\tswitch aggExpr.Op {\n-\tcase parser.TOPK, parser.BOTTOMK, parser.LIMITK:\n-\t\tif !convertibleToInt64(param) {\n-\t\t\tev.errorf(\"Scalar value %v overflows int64\", param)\n-\t\t}\n-\t\tk = int64(param)\n-\t\tif k > int64(len(inputMatrix)) {\n-\t\t\tk = int64(len(inputMatrix))\n+\t// allParamsMatchCondition returns true if the provided condition holds for every float params.\n+\tallParamsMatchCondition := func(params []FPoint, constParam bool, condition func(float64) bool) bool {",
        "comment_created_at": "2025-04-15T22:23:22+00:00",
        "comment_author": "beorn7",
        "comment_body": "While looking at this, I had another thought how we might want to encapsulate the param handling a bit better. Instead of this local function and some other bells and whistles, how about having an `fParams` interface like this:\r\n\r\n```Go\r\ntype fParams interface  {\r\n\tNext(ts int64) float64\r\n\tMax() float64\r\n\tMin() float64\r\n\tHasAnyNaN() bool\r\n}\r\n```\r\n\r\nThere could be one implementation for the variable param series, and one for the constant parameter. The former would pre-populate internal fields to to be used for `Max`, `Min`, `HasAnyNaN`. The latter would store one float value and return it always (or just do `math.IsNaN` for the const value).\r\n\r\nThen you could do many operations in an efficient way without using separate `if` statements everywhere.\r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2046969373",
        "repo_full_name": "prometheus/prometheus",
        "pr_number": 16404,
        "pr_file": "promql/engine.go",
        "discussion_id": "2045629781",
        "commented_code": "@@ -1359,46 +1359,46 @@ func (ev *evaluator) rangeEvalAgg(ctx context.Context, aggExpr *parser.Aggregate\n \t}\n \tgroups := make([]groupedAggregation, groupCount)\n \n-\tvar k int64\n-\tvar ratio float64\n \tvar seriess map[uint64]Series\n-\tswitch aggExpr.Op {\n-\tcase parser.TOPK, parser.BOTTOMK, parser.LIMITK:\n-\t\tif !convertibleToInt64(param) {\n-\t\t\tev.errorf(\"Scalar value %v overflows int64\", param)\n-\t\t}\n-\t\tk = int64(param)\n-\t\tif k > int64(len(inputMatrix)) {\n-\t\t\tk = int64(len(inputMatrix))\n+\t// allParamsMatchCondition returns true if the provided condition holds for every float params.\n+\tallParamsMatchCondition := func(params []FPoint, constParam bool, condition func(float64) bool) bool {",
        "comment_created_at": "2025-04-16T13:41:17+00:00",
        "comment_author": "NeerajGartia21",
        "comment_body": "That's a good idea. It would make the code more readable. I'll implement it as soon as possible.",
        "pr_file_module": null
      }
    ]
  }
]