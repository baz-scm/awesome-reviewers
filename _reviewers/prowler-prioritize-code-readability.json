[
  {
    "discussion_id": "2161675396",
    "pr_number": 8086,
    "pr_file": "prowler/lib/mutelist/mutelist.py",
    "created_at": "2025-06-23T13:43:40+00:00",
    "commented_code": "validate(mutelist, schema=mutelist_schema)\n             return mutelist\n         except Exception as error:\n-            logger.error(\n-                f\"{error.__class__.__name__} -- Mutelist YAML is malformed - {error}[{error.__traceback__.tb_lineno}]\"\n-            )\n+            if raise_on_exception:\n+                raise error\n+            else:\n+                logger.error(\n+                    f\"{error.__class__.__name__} -- Mutelist YAML is malformed - {error}[{error.__traceback__.tb_lineno}]\"\n+                )\n             return {}",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2161675396",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8086,
        "pr_file": "prowler/lib/mutelist/mutelist.py",
        "discussion_id": "2161675396",
        "commented_code": "@@ -453,7 +454,10 @@ def validate_mutelist(mutelist: dict) -> dict:\n             validate(mutelist, schema=mutelist_schema)\n             return mutelist\n         except Exception as error:\n-            logger.error(\n-                f\"{error.__class__.__name__} -- Mutelist YAML is malformed - {error}[{error.__traceback__.tb_lineno}]\"\n-            )\n+            if raise_on_exception:\n+                raise error\n+            else:\n+                logger.error(\n+                    f\"{error.__class__.__name__} -- Mutelist YAML is malformed - {error}[{error.__traceback__.tb_lineno}]\"\n+                )\n             return {}",
        "comment_created_at": "2025-06-23T13:43:40+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "```suggestion\r\n                return {}\r\n```\r\n\r\nI think this is missing one level indentation, right?",
        "pr_file_module": null
      },
      {
        "comment_id": "2161736397",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8086,
        "pr_file": "prowler/lib/mutelist/mutelist.py",
        "discussion_id": "2161675396",
        "commented_code": "@@ -453,7 +454,10 @@ def validate_mutelist(mutelist: dict) -> dict:\n             validate(mutelist, schema=mutelist_schema)\n             return mutelist\n         except Exception as error:\n-            logger.error(\n-                f\"{error.__class__.__name__} -- Mutelist YAML is malformed - {error}[{error.__traceback__.tb_lineno}]\"\n-            )\n+            if raise_on_exception:\n+                raise error\n+            else:\n+                logger.error(\n+                    f\"{error.__class__.__name__} -- Mutelist YAML is malformed - {error}[{error.__traceback__.tb_lineno}]\"\n+                )\n             return {}",
        "comment_created_at": "2025-06-23T14:11:01+00:00",
        "comment_author": "drewkerrigan",
        "comment_body": "Functionally it's the exact same behavior either way, as long as `raise_on_exception` is false, it will always return `{}` in the case that there is an exception. I prefer it this way just to make that clear that we want to return an empty dict in that case.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2073256937",
    "pr_number": 7653,
    "pr_file": "api/src/backend/tasks/tasks.py",
    "created_at": "2025-05-05T11:15:13+00:00",
    "commented_code": "logger.info(f\"No findings found for scan {scan_id}\")\n         return {\"upload\": False}\n \n-    # Initialize the prowler provider\n-    prowler_provider = initialize_prowler_provider(Provider.objects.get(id=provider_id))\n+    provider_obj = Provider.objects.get(id=provider_id)\n+    prowler_provider = initialize_prowler_provider(provider_obj)\n+    provider_uid = provider_obj.uid\n+    provider_type = provider_obj.provider\n \n-    # Get the provider UID\n-    provider_uid = Provider.objects.get(id=provider_id).uid\n-\n-    # Generate and ensure the output directory exists\n-    output_directory = _generate_output_directory(\n+    frameworks_bulk = Compliance.get_bulk(provider_type)\n+    frameworks_avail = get_available_compliance_frameworks(provider_type)\n+    out_dir = _generate_output_directory(\n+        DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n+    )\n+    comp_dir = _generate_compliance_output_directory(\n         DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n     )\n \n-    # Define auxiliary variables\n+    def get_writer(writer_map, name, factory, is_last):\n+        \"\"\"\n+        Return existing writer_map[name] or create via factory().\n+        In both cases set `.close_file = is_last`.\n+        \"\"\"\n+        initialization = False\n+        if name not in writer_map:\n+            writer_map[name] = factory()\n+            initialization = True\n+        w = writer_map[name]\n+        w.close_file = is_last\n+\n+        return w, initialization\n+\n     output_writers = {}\n+    compliance_writers = {}\n+\n     scan_summary = FindingOutput._transform_findings_stats(\n         ScanSummary.objects.filter(scan_id=scan_id)\n     )\n \n-    # Retrieve findings queryset\n-    findings_qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\")\n-\n-    # Process findings in batches\n-    for batch, is_last_batch in batched(\n-        findings_qs.iterator(), DJANGO_FINDINGS_BATCH_SIZE\n-    ):\n-        finding_outputs = [\n-            FindingOutput.transform_api_finding(finding, prowler_provider)\n-            for finding in batch\n-        ]\n-\n-        # Generate output files\n-        for mode, config in OUTPUT_FORMATS_MAPPING.items():\n-            kwargs = dict(config.get(\"kwargs\", {}))\n+    qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\").iterator()\n+    for batch, is_last in batched(qs, DJANGO_FINDINGS_BATCH_SIZE):\n+        fos = [FindingOutput.transform_api_finding(f, prowler_provider) for f in batch]\n+\n+        # Outputs\n+        for mode, cfg in OUTPUT_FORMATS_MAPPING.items():\n+            cls = cfg[\"class\"]\n+            suffix = cfg[\"suffix\"]\n+            extra = cfg.get(\"kwargs\", {}).copy()\n             if mode == \"html\":\n-                kwargs[\"provider\"] = prowler_provider\n-                kwargs[\"stats\"] = scan_summary\n-\n-            writer_class = config[\"class\"]\n-            if writer_class in output_writers:\n-                writer = output_writers[writer_class]\n-                writer.transform(finding_outputs)\n-                writer.close_file = is_last_batch\n-            else:\n-                writer = writer_class(\n-                    findings=finding_outputs,\n-                    file_path=output_directory,\n-                    file_extension=config[\"suffix\"],\n+                extra.update(provider=prowler_provider, stats=scan_summary)\n+\n+            writer, initialization = get_writer(\n+                output_writers,\n+                cls,\n+                lambda cls=cls, fos=fos, suffix=suffix: cls(\n+                    findings=fos,\n+                    file_path=out_dir,\n+                    file_extension=suffix,\n                     from_cli=False,\n-                )\n-                writer.close_file = is_last_batch\n-                output_writers[writer_class] = writer\n-\n-            # Write the current batch using the writer\n-            writer.batch_write_data_to_file(**kwargs)\n-\n-            # TODO: Refactor the output classes to avoid this manual reset\n-            writer._data = []\n-\n-    # Compress output files\n-    output_directory = _compress_output_files(output_directory)\n+                ),\n+                is_last,\n+            )\n+            if not initialization:\n+                writer.transform(fos)\n+            writer.batch_write_data_to_file(**extra)\n+            writer._data.clear()\n+\n+        # Compliance CSVs\n+        for name in frameworks_avail:\n+            compliance_obj = frameworks_bulk[name]\n+            klass = next(\n+                (\n+                    c\n+                    for cond, c in COMPLIANCE_CLASS_MAP.get(provider_type, [])\n+                    if cond(name)\n+                ),\n+                GenericCompliance,\n+            )\n+            filename = f\"{comp_dir}_{name}.csv\"",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2073256937",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "2073256937",
        "commented_code": "@@ -251,84 +256,110 @@ def generate_outputs(scan_id: str, provider_id: str, tenant_id: str):\n         logger.info(f\"No findings found for scan {scan_id}\")\n         return {\"upload\": False}\n \n-    # Initialize the prowler provider\n-    prowler_provider = initialize_prowler_provider(Provider.objects.get(id=provider_id))\n+    provider_obj = Provider.objects.get(id=provider_id)\n+    prowler_provider = initialize_prowler_provider(provider_obj)\n+    provider_uid = provider_obj.uid\n+    provider_type = provider_obj.provider\n \n-    # Get the provider UID\n-    provider_uid = Provider.objects.get(id=provider_id).uid\n-\n-    # Generate and ensure the output directory exists\n-    output_directory = _generate_output_directory(\n+    frameworks_bulk = Compliance.get_bulk(provider_type)\n+    frameworks_avail = get_available_compliance_frameworks(provider_type)\n+    out_dir = _generate_output_directory(\n+        DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n+    )\n+    comp_dir = _generate_compliance_output_directory(\n         DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n     )\n \n-    # Define auxiliary variables\n+    def get_writer(writer_map, name, factory, is_last):\n+        \"\"\"\n+        Return existing writer_map[name] or create via factory().\n+        In both cases set `.close_file = is_last`.\n+        \"\"\"\n+        initialization = False\n+        if name not in writer_map:\n+            writer_map[name] = factory()\n+            initialization = True\n+        w = writer_map[name]\n+        w.close_file = is_last\n+\n+        return w, initialization\n+\n     output_writers = {}\n+    compliance_writers = {}\n+\n     scan_summary = FindingOutput._transform_findings_stats(\n         ScanSummary.objects.filter(scan_id=scan_id)\n     )\n \n-    # Retrieve findings queryset\n-    findings_qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\")\n-\n-    # Process findings in batches\n-    for batch, is_last_batch in batched(\n-        findings_qs.iterator(), DJANGO_FINDINGS_BATCH_SIZE\n-    ):\n-        finding_outputs = [\n-            FindingOutput.transform_api_finding(finding, prowler_provider)\n-            for finding in batch\n-        ]\n-\n-        # Generate output files\n-        for mode, config in OUTPUT_FORMATS_MAPPING.items():\n-            kwargs = dict(config.get(\"kwargs\", {}))\n+    qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\").iterator()\n+    for batch, is_last in batched(qs, DJANGO_FINDINGS_BATCH_SIZE):\n+        fos = [FindingOutput.transform_api_finding(f, prowler_provider) for f in batch]\n+\n+        # Outputs\n+        for mode, cfg in OUTPUT_FORMATS_MAPPING.items():\n+            cls = cfg[\"class\"]\n+            suffix = cfg[\"suffix\"]\n+            extra = cfg.get(\"kwargs\", {}).copy()\n             if mode == \"html\":\n-                kwargs[\"provider\"] = prowler_provider\n-                kwargs[\"stats\"] = scan_summary\n-\n-            writer_class = config[\"class\"]\n-            if writer_class in output_writers:\n-                writer = output_writers[writer_class]\n-                writer.transform(finding_outputs)\n-                writer.close_file = is_last_batch\n-            else:\n-                writer = writer_class(\n-                    findings=finding_outputs,\n-                    file_path=output_directory,\n-                    file_extension=config[\"suffix\"],\n+                extra.update(provider=prowler_provider, stats=scan_summary)\n+\n+            writer, initialization = get_writer(\n+                output_writers,\n+                cls,\n+                lambda cls=cls, fos=fos, suffix=suffix: cls(\n+                    findings=fos,\n+                    file_path=out_dir,\n+                    file_extension=suffix,\n                     from_cli=False,\n-                )\n-                writer.close_file = is_last_batch\n-                output_writers[writer_class] = writer\n-\n-            # Write the current batch using the writer\n-            writer.batch_write_data_to_file(**kwargs)\n-\n-            # TODO: Refactor the output classes to avoid this manual reset\n-            writer._data = []\n-\n-    # Compress output files\n-    output_directory = _compress_output_files(output_directory)\n+                ),\n+                is_last,\n+            )\n+            if not initialization:\n+                writer.transform(fos)\n+            writer.batch_write_data_to_file(**extra)\n+            writer._data.clear()\n+\n+        # Compliance CSVs\n+        for name in frameworks_avail:\n+            compliance_obj = frameworks_bulk[name]\n+            klass = next(\n+                (\n+                    c\n+                    for cond, c in COMPLIANCE_CLASS_MAP.get(provider_type, [])\n+                    if cond(name)\n+                ),\n+                GenericCompliance,\n+            )\n+            filename = f\"{comp_dir}_{name}.csv\"",
        "comment_created_at": "2025-05-05T11:15:13+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Again, to me this is really hard to follow.",
        "pr_file_module": null
      },
      {
        "comment_id": "2073705981",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "2073256937",
        "commented_code": "@@ -251,84 +256,110 @@ def generate_outputs(scan_id: str, provider_id: str, tenant_id: str):\n         logger.info(f\"No findings found for scan {scan_id}\")\n         return {\"upload\": False}\n \n-    # Initialize the prowler provider\n-    prowler_provider = initialize_prowler_provider(Provider.objects.get(id=provider_id))\n+    provider_obj = Provider.objects.get(id=provider_id)\n+    prowler_provider = initialize_prowler_provider(provider_obj)\n+    provider_uid = provider_obj.uid\n+    provider_type = provider_obj.provider\n \n-    # Get the provider UID\n-    provider_uid = Provider.objects.get(id=provider_id).uid\n-\n-    # Generate and ensure the output directory exists\n-    output_directory = _generate_output_directory(\n+    frameworks_bulk = Compliance.get_bulk(provider_type)\n+    frameworks_avail = get_available_compliance_frameworks(provider_type)\n+    out_dir = _generate_output_directory(\n+        DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n+    )\n+    comp_dir = _generate_compliance_output_directory(\n         DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n     )\n \n-    # Define auxiliary variables\n+    def get_writer(writer_map, name, factory, is_last):\n+        \"\"\"\n+        Return existing writer_map[name] or create via factory().\n+        In both cases set `.close_file = is_last`.\n+        \"\"\"\n+        initialization = False\n+        if name not in writer_map:\n+            writer_map[name] = factory()\n+            initialization = True\n+        w = writer_map[name]\n+        w.close_file = is_last\n+\n+        return w, initialization\n+\n     output_writers = {}\n+    compliance_writers = {}\n+\n     scan_summary = FindingOutput._transform_findings_stats(\n         ScanSummary.objects.filter(scan_id=scan_id)\n     )\n \n-    # Retrieve findings queryset\n-    findings_qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\")\n-\n-    # Process findings in batches\n-    for batch, is_last_batch in batched(\n-        findings_qs.iterator(), DJANGO_FINDINGS_BATCH_SIZE\n-    ):\n-        finding_outputs = [\n-            FindingOutput.transform_api_finding(finding, prowler_provider)\n-            for finding in batch\n-        ]\n-\n-        # Generate output files\n-        for mode, config in OUTPUT_FORMATS_MAPPING.items():\n-            kwargs = dict(config.get(\"kwargs\", {}))\n+    qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\").iterator()\n+    for batch, is_last in batched(qs, DJANGO_FINDINGS_BATCH_SIZE):\n+        fos = [FindingOutput.transform_api_finding(f, prowler_provider) for f in batch]\n+\n+        # Outputs\n+        for mode, cfg in OUTPUT_FORMATS_MAPPING.items():\n+            cls = cfg[\"class\"]\n+            suffix = cfg[\"suffix\"]\n+            extra = cfg.get(\"kwargs\", {}).copy()\n             if mode == \"html\":\n-                kwargs[\"provider\"] = prowler_provider\n-                kwargs[\"stats\"] = scan_summary\n-\n-            writer_class = config[\"class\"]\n-            if writer_class in output_writers:\n-                writer = output_writers[writer_class]\n-                writer.transform(finding_outputs)\n-                writer.close_file = is_last_batch\n-            else:\n-                writer = writer_class(\n-                    findings=finding_outputs,\n-                    file_path=output_directory,\n-                    file_extension=config[\"suffix\"],\n+                extra.update(provider=prowler_provider, stats=scan_summary)\n+\n+            writer, initialization = get_writer(\n+                output_writers,\n+                cls,\n+                lambda cls=cls, fos=fos, suffix=suffix: cls(\n+                    findings=fos,\n+                    file_path=out_dir,\n+                    file_extension=suffix,\n                     from_cli=False,\n-                )\n-                writer.close_file = is_last_batch\n-                output_writers[writer_class] = writer\n-\n-            # Write the current batch using the writer\n-            writer.batch_write_data_to_file(**kwargs)\n-\n-            # TODO: Refactor the output classes to avoid this manual reset\n-            writer._data = []\n-\n-    # Compress output files\n-    output_directory = _compress_output_files(output_directory)\n+                ),\n+                is_last,\n+            )\n+            if not initialization:\n+                writer.transform(fos)\n+            writer.batch_write_data_to_file(**extra)\n+            writer._data.clear()\n+\n+        # Compliance CSVs\n+        for name in frameworks_avail:\n+            compliance_obj = frameworks_bulk[name]\n+            klass = next(\n+                (\n+                    c\n+                    for cond, c in COMPLIANCE_CLASS_MAP.get(provider_type, [])\n+                    if cond(name)\n+                ),\n+                GenericCompliance,\n+            )\n+            filename = f\"{comp_dir}_{name}.csv\"",
        "comment_created_at": "2025-05-05T15:51:34+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "If you want we can change to this version, it's the same:\r\n\r\n```\r\nklass = GenericCompliance\r\nfor condition, cls in COMPLIANCE_CLASS_MAP.get(provider_type, []):\r\n        if condition(framework_name):\r\n            klass = cls\r\n            break\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2073759305",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "2073256937",
        "commented_code": "@@ -251,84 +256,110 @@ def generate_outputs(scan_id: str, provider_id: str, tenant_id: str):\n         logger.info(f\"No findings found for scan {scan_id}\")\n         return {\"upload\": False}\n \n-    # Initialize the prowler provider\n-    prowler_provider = initialize_prowler_provider(Provider.objects.get(id=provider_id))\n+    provider_obj = Provider.objects.get(id=provider_id)\n+    prowler_provider = initialize_prowler_provider(provider_obj)\n+    provider_uid = provider_obj.uid\n+    provider_type = provider_obj.provider\n \n-    # Get the provider UID\n-    provider_uid = Provider.objects.get(id=provider_id).uid\n-\n-    # Generate and ensure the output directory exists\n-    output_directory = _generate_output_directory(\n+    frameworks_bulk = Compliance.get_bulk(provider_type)\n+    frameworks_avail = get_available_compliance_frameworks(provider_type)\n+    out_dir = _generate_output_directory(\n+        DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n+    )\n+    comp_dir = _generate_compliance_output_directory(\n         DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n     )\n \n-    # Define auxiliary variables\n+    def get_writer(writer_map, name, factory, is_last):\n+        \"\"\"\n+        Return existing writer_map[name] or create via factory().\n+        In both cases set `.close_file = is_last`.\n+        \"\"\"\n+        initialization = False\n+        if name not in writer_map:\n+            writer_map[name] = factory()\n+            initialization = True\n+        w = writer_map[name]\n+        w.close_file = is_last\n+\n+        return w, initialization\n+\n     output_writers = {}\n+    compliance_writers = {}\n+\n     scan_summary = FindingOutput._transform_findings_stats(\n         ScanSummary.objects.filter(scan_id=scan_id)\n     )\n \n-    # Retrieve findings queryset\n-    findings_qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\")\n-\n-    # Process findings in batches\n-    for batch, is_last_batch in batched(\n-        findings_qs.iterator(), DJANGO_FINDINGS_BATCH_SIZE\n-    ):\n-        finding_outputs = [\n-            FindingOutput.transform_api_finding(finding, prowler_provider)\n-            for finding in batch\n-        ]\n-\n-        # Generate output files\n-        for mode, config in OUTPUT_FORMATS_MAPPING.items():\n-            kwargs = dict(config.get(\"kwargs\", {}))\n+    qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\").iterator()\n+    for batch, is_last in batched(qs, DJANGO_FINDINGS_BATCH_SIZE):\n+        fos = [FindingOutput.transform_api_finding(f, prowler_provider) for f in batch]\n+\n+        # Outputs\n+        for mode, cfg in OUTPUT_FORMATS_MAPPING.items():\n+            cls = cfg[\"class\"]\n+            suffix = cfg[\"suffix\"]\n+            extra = cfg.get(\"kwargs\", {}).copy()\n             if mode == \"html\":\n-                kwargs[\"provider\"] = prowler_provider\n-                kwargs[\"stats\"] = scan_summary\n-\n-            writer_class = config[\"class\"]\n-            if writer_class in output_writers:\n-                writer = output_writers[writer_class]\n-                writer.transform(finding_outputs)\n-                writer.close_file = is_last_batch\n-            else:\n-                writer = writer_class(\n-                    findings=finding_outputs,\n-                    file_path=output_directory,\n-                    file_extension=config[\"suffix\"],\n+                extra.update(provider=prowler_provider, stats=scan_summary)\n+\n+            writer, initialization = get_writer(\n+                output_writers,\n+                cls,\n+                lambda cls=cls, fos=fos, suffix=suffix: cls(\n+                    findings=fos,\n+                    file_path=out_dir,\n+                    file_extension=suffix,\n                     from_cli=False,\n-                )\n-                writer.close_file = is_last_batch\n-                output_writers[writer_class] = writer\n-\n-            # Write the current batch using the writer\n-            writer.batch_write_data_to_file(**kwargs)\n-\n-            # TODO: Refactor the output classes to avoid this manual reset\n-            writer._data = []\n-\n-    # Compress output files\n-    output_directory = _compress_output_files(output_directory)\n+                ),\n+                is_last,\n+            )\n+            if not initialization:\n+                writer.transform(fos)\n+            writer.batch_write_data_to_file(**extra)\n+            writer._data.clear()\n+\n+        # Compliance CSVs\n+        for name in frameworks_avail:\n+            compliance_obj = frameworks_bulk[name]\n+            klass = next(\n+                (\n+                    c\n+                    for cond, c in COMPLIANCE_CLASS_MAP.get(provider_type, [])\n+                    if cond(name)\n+                ),\n+                GenericCompliance,\n+            )\n+            filename = f\"{comp_dir}_{name}.csv\"",
        "comment_created_at": "2025-05-05T16:20:56+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "To me @vicferpoy has the final word on this PR.",
        "pr_file_module": null
      },
      {
        "comment_id": "2075164584",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "2073256937",
        "commented_code": "@@ -251,84 +256,110 @@ def generate_outputs(scan_id: str, provider_id: str, tenant_id: str):\n         logger.info(f\"No findings found for scan {scan_id}\")\n         return {\"upload\": False}\n \n-    # Initialize the prowler provider\n-    prowler_provider = initialize_prowler_provider(Provider.objects.get(id=provider_id))\n+    provider_obj = Provider.objects.get(id=provider_id)\n+    prowler_provider = initialize_prowler_provider(provider_obj)\n+    provider_uid = provider_obj.uid\n+    provider_type = provider_obj.provider\n \n-    # Get the provider UID\n-    provider_uid = Provider.objects.get(id=provider_id).uid\n-\n-    # Generate and ensure the output directory exists\n-    output_directory = _generate_output_directory(\n+    frameworks_bulk = Compliance.get_bulk(provider_type)\n+    frameworks_avail = get_available_compliance_frameworks(provider_type)\n+    out_dir = _generate_output_directory(\n+        DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n+    )\n+    comp_dir = _generate_compliance_output_directory(\n         DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n     )\n \n-    # Define auxiliary variables\n+    def get_writer(writer_map, name, factory, is_last):\n+        \"\"\"\n+        Return existing writer_map[name] or create via factory().\n+        In both cases set `.close_file = is_last`.\n+        \"\"\"\n+        initialization = False\n+        if name not in writer_map:\n+            writer_map[name] = factory()\n+            initialization = True\n+        w = writer_map[name]\n+        w.close_file = is_last\n+\n+        return w, initialization\n+\n     output_writers = {}\n+    compliance_writers = {}\n+\n     scan_summary = FindingOutput._transform_findings_stats(\n         ScanSummary.objects.filter(scan_id=scan_id)\n     )\n \n-    # Retrieve findings queryset\n-    findings_qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\")\n-\n-    # Process findings in batches\n-    for batch, is_last_batch in batched(\n-        findings_qs.iterator(), DJANGO_FINDINGS_BATCH_SIZE\n-    ):\n-        finding_outputs = [\n-            FindingOutput.transform_api_finding(finding, prowler_provider)\n-            for finding in batch\n-        ]\n-\n-        # Generate output files\n-        for mode, config in OUTPUT_FORMATS_MAPPING.items():\n-            kwargs = dict(config.get(\"kwargs\", {}))\n+    qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\").iterator()\n+    for batch, is_last in batched(qs, DJANGO_FINDINGS_BATCH_SIZE):\n+        fos = [FindingOutput.transform_api_finding(f, prowler_provider) for f in batch]\n+\n+        # Outputs\n+        for mode, cfg in OUTPUT_FORMATS_MAPPING.items():\n+            cls = cfg[\"class\"]\n+            suffix = cfg[\"suffix\"]\n+            extra = cfg.get(\"kwargs\", {}).copy()\n             if mode == \"html\":\n-                kwargs[\"provider\"] = prowler_provider\n-                kwargs[\"stats\"] = scan_summary\n-\n-            writer_class = config[\"class\"]\n-            if writer_class in output_writers:\n-                writer = output_writers[writer_class]\n-                writer.transform(finding_outputs)\n-                writer.close_file = is_last_batch\n-            else:\n-                writer = writer_class(\n-                    findings=finding_outputs,\n-                    file_path=output_directory,\n-                    file_extension=config[\"suffix\"],\n+                extra.update(provider=prowler_provider, stats=scan_summary)\n+\n+            writer, initialization = get_writer(\n+                output_writers,\n+                cls,\n+                lambda cls=cls, fos=fos, suffix=suffix: cls(\n+                    findings=fos,\n+                    file_path=out_dir,\n+                    file_extension=suffix,\n                     from_cli=False,\n-                )\n-                writer.close_file = is_last_batch\n-                output_writers[writer_class] = writer\n-\n-            # Write the current batch using the writer\n-            writer.batch_write_data_to_file(**kwargs)\n-\n-            # TODO: Refactor the output classes to avoid this manual reset\n-            writer._data = []\n-\n-    # Compress output files\n-    output_directory = _compress_output_files(output_directory)\n+                ),\n+                is_last,\n+            )\n+            if not initialization:\n+                writer.transform(fos)\n+            writer.batch_write_data_to_file(**extra)\n+            writer._data.clear()\n+\n+        # Compliance CSVs\n+        for name in frameworks_avail:\n+            compliance_obj = frameworks_bulk[name]\n+            klass = next(\n+                (\n+                    c\n+                    for cond, c in COMPLIANCE_CLASS_MAP.get(provider_type, [])\n+                    if cond(name)\n+                ),\n+                GenericCompliance,\n+            )\n+            filename = f\"{comp_dir}_{name}.csv\"",
        "comment_created_at": "2025-05-06T10:18:47+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "I believe Adri already changed this. I agree it's better this way. We're getting older and I would have had issues trying to understand that piece of code in a few months 🤣 ",
        "pr_file_module": null
      },
      {
        "comment_id": "2077245520",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "2073256937",
        "commented_code": "@@ -251,84 +256,110 @@ def generate_outputs(scan_id: str, provider_id: str, tenant_id: str):\n         logger.info(f\"No findings found for scan {scan_id}\")\n         return {\"upload\": False}\n \n-    # Initialize the prowler provider\n-    prowler_provider = initialize_prowler_provider(Provider.objects.get(id=provider_id))\n+    provider_obj = Provider.objects.get(id=provider_id)\n+    prowler_provider = initialize_prowler_provider(provider_obj)\n+    provider_uid = provider_obj.uid\n+    provider_type = provider_obj.provider\n \n-    # Get the provider UID\n-    provider_uid = Provider.objects.get(id=provider_id).uid\n-\n-    # Generate and ensure the output directory exists\n-    output_directory = _generate_output_directory(\n+    frameworks_bulk = Compliance.get_bulk(provider_type)\n+    frameworks_avail = get_available_compliance_frameworks(provider_type)\n+    out_dir = _generate_output_directory(\n+        DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n+    )\n+    comp_dir = _generate_compliance_output_directory(\n         DJANGO_TMP_OUTPUT_DIRECTORY, provider_uid, tenant_id, scan_id\n     )\n \n-    # Define auxiliary variables\n+    def get_writer(writer_map, name, factory, is_last):\n+        \"\"\"\n+        Return existing writer_map[name] or create via factory().\n+        In both cases set `.close_file = is_last`.\n+        \"\"\"\n+        initialization = False\n+        if name not in writer_map:\n+            writer_map[name] = factory()\n+            initialization = True\n+        w = writer_map[name]\n+        w.close_file = is_last\n+\n+        return w, initialization\n+\n     output_writers = {}\n+    compliance_writers = {}\n+\n     scan_summary = FindingOutput._transform_findings_stats(\n         ScanSummary.objects.filter(scan_id=scan_id)\n     )\n \n-    # Retrieve findings queryset\n-    findings_qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\")\n-\n-    # Process findings in batches\n-    for batch, is_last_batch in batched(\n-        findings_qs.iterator(), DJANGO_FINDINGS_BATCH_SIZE\n-    ):\n-        finding_outputs = [\n-            FindingOutput.transform_api_finding(finding, prowler_provider)\n-            for finding in batch\n-        ]\n-\n-        # Generate output files\n-        for mode, config in OUTPUT_FORMATS_MAPPING.items():\n-            kwargs = dict(config.get(\"kwargs\", {}))\n+    qs = Finding.all_objects.filter(scan_id=scan_id).order_by(\"uid\").iterator()\n+    for batch, is_last in batched(qs, DJANGO_FINDINGS_BATCH_SIZE):\n+        fos = [FindingOutput.transform_api_finding(f, prowler_provider) for f in batch]\n+\n+        # Outputs\n+        for mode, cfg in OUTPUT_FORMATS_MAPPING.items():\n+            cls = cfg[\"class\"]\n+            suffix = cfg[\"suffix\"]\n+            extra = cfg.get(\"kwargs\", {}).copy()\n             if mode == \"html\":\n-                kwargs[\"provider\"] = prowler_provider\n-                kwargs[\"stats\"] = scan_summary\n-\n-            writer_class = config[\"class\"]\n-            if writer_class in output_writers:\n-                writer = output_writers[writer_class]\n-                writer.transform(finding_outputs)\n-                writer.close_file = is_last_batch\n-            else:\n-                writer = writer_class(\n-                    findings=finding_outputs,\n-                    file_path=output_directory,\n-                    file_extension=config[\"suffix\"],\n+                extra.update(provider=prowler_provider, stats=scan_summary)\n+\n+            writer, initialization = get_writer(\n+                output_writers,\n+                cls,\n+                lambda cls=cls, fos=fos, suffix=suffix: cls(\n+                    findings=fos,\n+                    file_path=out_dir,\n+                    file_extension=suffix,\n                     from_cli=False,\n-                )\n-                writer.close_file = is_last_batch\n-                output_writers[writer_class] = writer\n-\n-            # Write the current batch using the writer\n-            writer.batch_write_data_to_file(**kwargs)\n-\n-            # TODO: Refactor the output classes to avoid this manual reset\n-            writer._data = []\n-\n-    # Compress output files\n-    output_directory = _compress_output_files(output_directory)\n+                ),\n+                is_last,\n+            )\n+            if not initialization:\n+                writer.transform(fos)\n+            writer.batch_write_data_to_file(**extra)\n+            writer._data.clear()\n+\n+        # Compliance CSVs\n+        for name in frameworks_avail:\n+            compliance_obj = frameworks_bulk[name]\n+            klass = next(\n+                (\n+                    c\n+                    for cond, c in COMPLIANCE_CLASS_MAP.get(provider_type, [])\n+                    if cond(name)\n+                ),\n+                GenericCompliance,\n+            )\n+            filename = f\"{comp_dir}_{name}.csv\"",
        "comment_created_at": "2025-05-07T09:40:41+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Totally agree, I'm getting older.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1883422923",
    "pr_number": 6165,
    "pr_file": "prowler/providers/gcp/gcp_provider.py",
    "created_at": "2024-12-13T07:05:28+00:00",
    "commented_code": "file=__file__, original_exception=error\n                     )\n \n+            if service_account_key:",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1883422923",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6165,
        "pr_file": "prowler/providers/gcp/gcp_provider.py",
        "discussion_id": "1883422923",
        "commented_code": "@@ -351,6 +368,24 @@ def setup_session(\n                         file=__file__, original_exception=error\n                     )\n \n+            if service_account_key:",
        "comment_created_at": "2024-12-13T07:05:28+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "`if` or `elif`?",
        "pr_file_module": null
      },
      {
        "comment_id": "1883605708",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6165,
        "pr_file": "prowler/providers/gcp/gcp_provider.py",
        "discussion_id": "1883422923",
        "commented_code": "@@ -351,6 +368,24 @@ def setup_session(\n                         file=__file__, original_exception=error\n                     )\n \n+            if service_account_key:",
        "comment_created_at": "2024-12-13T09:12:42+00:00",
        "comment_author": "pedrooot",
        "comment_body": "The elif is not needed, If it matches any statement the return will be triggered.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2120819550",
    "pr_number": 7848,
    "pr_file": "api/src/backend/api/v1/serializers.py",
    "created_at": "2025-06-02T11:09:08+00:00",
    "commented_code": "IntegrationProviderRelationship.objects.bulk_create(new_relationships)\n \n         return super().update(instance, validated_data)\n+\n+\n+class LighthouseConfigSerializer(RLSSerializer):\n+    \"\"\"\n+    Serializer for the LighthouseConfig model.\n+    \"\"\"\n+\n+    api_key = serializers.CharField(required=False)\n+\n+    class Meta:\n+        model = LighthouseConfig\n+        fields = [\n+            \"id\",\n+            \"name\",\n+            \"api_key\",\n+            \"model\",\n+            \"temperature\",\n+            \"max_tokens\",\n+            \"business_context\",\n+            \"is_active\",\n+            \"inserted_at\",\n+            \"updated_at\",\n+            \"url\",\n+        ]\n+        extra_kwargs = {\n+            \"id\": {\"read_only\": True},\n+            \"inserted_at\": {\"read_only\": True},\n+            \"updated_at\": {\"read_only\": True},\n+        }\n+\n+    def to_representation(self, instance):\n+        data = super().to_representation(instance)\n+        # Check if api_key is specifically requested in fields param\n+        fields_param = self.context.get(\"request\", None) and self.context[\n+            \"request\"\n+        ].query_params.get(\"fields[lighthouse-config]\", \"\")\n+        if fields_param == \"api_key\":\n+            # Return decrypted key if specifically requested\n+            data[\"api_key\"] = instance.api_key_decoded if instance.api_key else None\n+        else:\n+            # Return masked key for general requests\n+            data[\"api_key\"] = \"*\" * len(instance.api_key) if instance.api_key else None\n+        return data\n+\n+\n+class LighthouseConfigCreateSerializer(RLSSerializer, BaseWriteSerializer):\n+    \"\"\"Serializer for creating new Lighthouse configurations.\"\"\"\n+\n+    api_key = serializers.CharField(write_only=True, required=True)\n+\n+    class Meta:\n+        model = LighthouseConfig\n+        fields = [\n+            \"name\",\n+            \"api_key\",\n+            \"model\",\n+            \"temperature\",\n+            \"max_tokens\",\n+            \"business_context\",\n+            \"is_active\",",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2120819550",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7848,
        "pr_file": "api/src/backend/api/v1/serializers.py",
        "discussion_id": "2120819550",
        "commented_code": "@@ -2128,3 +2130,129 @@ def update(self, instance, validated_data):\n             IntegrationProviderRelationship.objects.bulk_create(new_relationships)\n \n         return super().update(instance, validated_data)\n+\n+\n+class LighthouseConfigSerializer(RLSSerializer):\n+    \"\"\"\n+    Serializer for the LighthouseConfig model.\n+    \"\"\"\n+\n+    api_key = serializers.CharField(required=False)\n+\n+    class Meta:\n+        model = LighthouseConfig\n+        fields = [\n+            \"id\",\n+            \"name\",\n+            \"api_key\",\n+            \"model\",\n+            \"temperature\",\n+            \"max_tokens\",\n+            \"business_context\",\n+            \"is_active\",\n+            \"inserted_at\",\n+            \"updated_at\",\n+            \"url\",\n+        ]\n+        extra_kwargs = {\n+            \"id\": {\"read_only\": True},\n+            \"inserted_at\": {\"read_only\": True},\n+            \"updated_at\": {\"read_only\": True},\n+        }\n+\n+    def to_representation(self, instance):\n+        data = super().to_representation(instance)\n+        # Check if api_key is specifically requested in fields param\n+        fields_param = self.context.get(\"request\", None) and self.context[\n+            \"request\"\n+        ].query_params.get(\"fields[lighthouse-config]\", \"\")\n+        if fields_param == \"api_key\":\n+            # Return decrypted key if specifically requested\n+            data[\"api_key\"] = instance.api_key_decoded if instance.api_key else None\n+        else:\n+            # Return masked key for general requests\n+            data[\"api_key\"] = \"*\" * len(instance.api_key) if instance.api_key else None\n+        return data\n+\n+\n+class LighthouseConfigCreateSerializer(RLSSerializer, BaseWriteSerializer):\n+    \"\"\"Serializer for creating new Lighthouse configurations.\"\"\"\n+\n+    api_key = serializers.CharField(write_only=True, required=True)\n+\n+    class Meta:\n+        model = LighthouseConfig\n+        fields = [\n+            \"name\",\n+            \"api_key\",\n+            \"model\",\n+            \"temperature\",\n+            \"max_tokens\",\n+            \"business_context\",\n+            \"is_active\",",
        "comment_created_at": "2025-06-02T11:09:08+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "I think for consistency with other serializers, you should define all the expected fields as in the read serializer and then using `extra_kwargs` to define them as `read_only`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1888444149",
    "pr_number": 6226,
    "pr_file": "api/src/backend/api/v1/views.py",
    "created_at": "2024-12-17T12:35:13+00:00",
    "commented_code": "filterset_class = UserFilter\n     ordering = [\"-date_joined\"]\n     ordering_fields = [\"name\", \"email\", \"company_name\", \"date_joined\", \"is_active\"]\n+    # RBAC required permissions\n     required_permissions = [Permissions.MANAGE_USERS]\n-    permission_classes = BaseRLSViewSet.permission_classes + [HasPermissions]\n-\n-    def initial(self, request, *args, **kwargs):\n-        \"\"\"\n-        Sets required_permissions before permissions are checked.\n-        \"\"\"\n-        self.required_permissions = self.get_required_permissions()\n-        super().initial(request, *args, **kwargs)\n \n     def get_required_permissions(self):\n         \"\"\"\n         Returns the required permissions based on the request method.\n         \"\"\"\n         if self.action == \"me\":\n             # No permissions required for me request\n-            return []\n+            self.required_permissions = []\n         else:\n             # Require permission for the rest of the requests\n-            return [Permissions.MANAGE_USERS]\n+            self.required_permissions = [Permissions.MANAGE_USERS]",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1888444149",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6226,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "1888444149",
        "commented_code": "@@ -290,26 +290,19 @@ class UserViewSet(BaseUserViewset):\n     filterset_class = UserFilter\n     ordering = [\"-date_joined\"]\n     ordering_fields = [\"name\", \"email\", \"company_name\", \"date_joined\", \"is_active\"]\n+    # RBAC required permissions\n     required_permissions = [Permissions.MANAGE_USERS]\n-    permission_classes = BaseRLSViewSet.permission_classes + [HasPermissions]\n-\n-    def initial(self, request, *args, **kwargs):\n-        \"\"\"\n-        Sets required_permissions before permissions are checked.\n-        \"\"\"\n-        self.required_permissions = self.get_required_permissions()\n-        super().initial(request, *args, **kwargs)\n \n     def get_required_permissions(self):\n         \"\"\"\n         Returns the required permissions based on the request method.\n         \"\"\"\n         if self.action == \"me\":\n             # No permissions required for me request\n-            return []\n+            self.required_permissions = []\n         else:\n             # Require permission for the rest of the requests\n-            return [Permissions.MANAGE_USERS]\n+            self.required_permissions = [Permissions.MANAGE_USERS]",
        "comment_created_at": "2024-12-17T12:35:13+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "I believe this is not correct. This function should return something, not update a internal attribute. Why did you modify the former logic? Since you are setting the base required permissions as `required_permissions = [Permissions.MANAGE_USERS]`, I believe this function should be something like:\r\n\r\n```python\r\n    def get_required_permissions(self):\r\n        \"\"\"\r\n        Returns the required permissions based on the request method.\r\n        \"\"\"\r\n        if self.action == \"me\":\r\n            # No permissions required for me request\r\n            return []\r\n        else:\r\n            # Require permission for the rest of the requests\r\n            return self.required_permissions\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1888466474",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6226,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "1888444149",
        "commented_code": "@@ -290,26 +290,19 @@ class UserViewSet(BaseUserViewset):\n     filterset_class = UserFilter\n     ordering = [\"-date_joined\"]\n     ordering_fields = [\"name\", \"email\", \"company_name\", \"date_joined\", \"is_active\"]\n+    # RBAC required permissions\n     required_permissions = [Permissions.MANAGE_USERS]\n-    permission_classes = BaseRLSViewSet.permission_classes + [HasPermissions]\n-\n-    def initial(self, request, *args, **kwargs):\n-        \"\"\"\n-        Sets required_permissions before permissions are checked.\n-        \"\"\"\n-        self.required_permissions = self.get_required_permissions()\n-        super().initial(request, *args, **kwargs)\n \n     def get_required_permissions(self):\n         \"\"\"\n         Returns the required permissions based on the request method.\n         \"\"\"\n         if self.action == \"me\":\n             # No permissions required for me request\n-            return []\n+            self.required_permissions = []\n         else:\n             # Require permission for the rest of the requests\n-            return [Permissions.MANAGE_USERS]\n+            self.required_permissions = [Permissions.MANAGE_USERS]",
        "comment_created_at": "2024-12-17T12:49:24+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "Making this change implies modifying code and having to define the function get_required_permissions in all views, for me the interesting thing would be to change the name to set_required_permissions.",
        "pr_file_module": null
      },
      {
        "comment_id": "1888555286",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6226,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "1888444149",
        "commented_code": "@@ -290,26 +290,19 @@ class UserViewSet(BaseUserViewset):\n     filterset_class = UserFilter\n     ordering = [\"-date_joined\"]\n     ordering_fields = [\"name\", \"email\", \"company_name\", \"date_joined\", \"is_active\"]\n+    # RBAC required permissions\n     required_permissions = [Permissions.MANAGE_USERS]\n-    permission_classes = BaseRLSViewSet.permission_classes + [HasPermissions]\n-\n-    def initial(self, request, *args, **kwargs):\n-        \"\"\"\n-        Sets required_permissions before permissions are checked.\n-        \"\"\"\n-        self.required_permissions = self.get_required_permissions()\n-        super().initial(request, *args, **kwargs)\n \n     def get_required_permissions(self):\n         \"\"\"\n         Returns the required permissions based on the request method.\n         \"\"\"\n         if self.action == \"me\":\n             # No permissions required for me request\n-            return []\n+            self.required_permissions = []\n         else:\n             # Require permission for the rest of the requests\n-            return [Permissions.MANAGE_USERS]\n+            self.required_permissions = [Permissions.MANAGE_USERS]",
        "comment_created_at": "2024-12-17T13:51:01+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "I thought this was one of the Django viewset functions. If it is auxiliary and our own, it's fine. Maybe consider renaming it as you suggested in other comment. ",
        "pr_file_module": null
      }
    ]
  }
]