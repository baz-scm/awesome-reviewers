[
  {
    "discussion_id": "2156999908",
    "pr_number": 8056,
    "pr_file": "prowler/providers/aws/lib/s3/s3.py",
    "created_at": "2025-06-19T13:16:04+00:00",
    "commented_code": ")\n         return uploaded_objects\n \n+    def upload_file(self, filename: str, bucket_name: str, key: str) -> None:",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2156999908",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "prowler/providers/aws/lib/s3/s3.py",
        "discussion_id": "2156999908",
        "commented_code": "@@ -218,6 +218,27 @@ def send_to_bucket(\n             )\n         return uploaded_objects\n \n+    def upload_file(self, filename: str, bucket_name: str, key: str) -> None:",
        "comment_created_at": "2025-06-19T13:16:04+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Since you created this, could you add it as a replacement in lines 185-192?\r\n\r\nAlso please add tests to this function using `moto`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160951171",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "prowler/providers/aws/lib/s3/s3.py",
        "discussion_id": "2156999908",
        "commented_code": "@@ -218,6 +218,27 @@ def send_to_bucket(\n             )\n         return uploaded_objects\n \n+    def upload_file(self, filename: str, bucket_name: str, key: str) -> None:",
        "comment_created_at": "2025-06-23T07:52:02+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "It hasn’t been changed because I didn’t see the need to explicitly set the content-type. We can talk about this, as it’s possible I’m missing something or we’re not fully aligned",
        "pr_file_module": null
      },
      {
        "comment_id": "2161093010",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "prowler/providers/aws/lib/s3/s3.py",
        "discussion_id": "2156999908",
        "commented_code": "@@ -218,6 +218,27 @@ def send_to_bucket(\n             )\n         return uploaded_objects\n \n+    def upload_file(self, filename: str, bucket_name: str, key: str) -> None:",
        "comment_created_at": "2025-06-23T09:02:14+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "The `Content-Type` needs to be set because in AWS by default is `binary/octet-stream`. That's another reason why I recommended you to use the current SDK's S3 methods. We can adapt whatever is needed to reutilize code.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2157026929",
    "pr_number": 8056,
    "pr_file": "api/src/backend/api/utils.py",
    "created_at": "2025-06-19T13:30:41+00:00",
    "commented_code": ")\n \n \n+def prowler_integration_connection_test(integration: Integration) -> Connection:\n+    \"\"\"Test the connection to a Prowler integration based on the given integration type.\n+\n+    Args:\n+        integration (Integration): The integration object containing the integration type and associated credentials.\n+\n+    Returns:\n+        Connection: A connection object representing the result of the connection test for the specified integration.\n+    \"\"\"\n+    if integration.integration_type in [\n+        Integration.IntegrationChoices.S3,\n+        Integration.IntegrationChoices.AWS_SECURITY_HUB,\n+    ]:\n+        try:\n+            session = AwsProvider(**integration.credentials).session.current_session",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2157026929",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "api/src/backend/api/utils.py",
        "discussion_id": "2157026929",
        "commented_code": "@@ -147,6 +151,47 @@ def prowler_provider_connection_test(provider: Provider) -> Connection:\n     )\n \n \n+def prowler_integration_connection_test(integration: Integration) -> Connection:\n+    \"\"\"Test the connection to a Prowler integration based on the given integration type.\n+\n+    Args:\n+        integration (Integration): The integration object containing the integration type and associated credentials.\n+\n+    Returns:\n+        Connection: A connection object representing the result of the connection test for the specified integration.\n+    \"\"\"\n+    if integration.integration_type in [\n+        Integration.IntegrationChoices.S3,\n+        Integration.IntegrationChoices.AWS_SECURITY_HUB,\n+    ]:\n+        try:\n+            session = AwsProvider(**integration.credentials).session.current_session",
        "comment_created_at": "2025-06-19T13:30:41+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "S3 integration allows you to pass the same credentials as the provider and has a `test_connection` method, so you don't need to setup the AWS provider.\r\n```suggestion\r\n            S3.test_connection(**integration.credentials)\r\n```\r\n\r\nI see the `test_connection` does not support passing the raw credentials and it should. I'm going to add support to that.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160971772",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "api/src/backend/api/utils.py",
        "discussion_id": "2157026929",
        "commented_code": "@@ -147,6 +151,47 @@ def prowler_provider_connection_test(provider: Provider) -> Connection:\n     )\n \n \n+def prowler_integration_connection_test(integration: Integration) -> Connection:\n+    \"\"\"Test the connection to a Prowler integration based on the given integration type.\n+\n+    Args:\n+        integration (Integration): The integration object containing the integration type and associated credentials.\n+\n+    Returns:\n+        Connection: A connection object representing the result of the connection test for the specified integration.\n+    \"\"\"\n+    if integration.integration_type in [\n+        Integration.IntegrationChoices.S3,\n+        Integration.IntegrationChoices.AWS_SECURITY_HUB,\n+    ]:\n+        try:\n+            session = AwsProvider(**integration.credentials).session.current_session",
        "comment_created_at": "2025-06-23T08:01:44+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "Thanks, let me know when this is available",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2157100530",
    "pr_number": 8056,
    "pr_file": "api/src/backend/tasks/jobs/integrations.py",
    "created_at": "2025-06-19T14:00:04+00:00",
    "commented_code": "+import os\n+\n+from celery.utils.log import get_task_logger\n+\n+from api.db_utils import rls_transaction\n+from api.models import Integration\n+from prowler.providers.aws.aws_provider import AwsProvider\n+from prowler.providers.aws.lib.s3.s3 import S3\n+from prowler.providers.common.models import Connection\n+\n+logger = get_task_logger(__name__)\n+\n+\n+def get_s3_client_from_integration(\n+    integration: Integration,\n+) -> tuple[bool, S3 | Connection]:\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from an integration.\n+\n+    Args:\n+        integration (Integration): The integration to get the S3 client from.\n+\n+    Returns:\n+        tuple[bool, S3 | Connection]: A tuple containing a boolean indicating if the connection was successful and the S3 client or connection object.\n+    \"\"\"\n+    session = AwsProvider(**integration.credentials).session.current_session\n+    s3 = S3(\n+        session=session,\n+        bucket_name=integration.configuration[\"bucket_name\"],\n+        output_directory=integration.configuration[\"output_directory\"],\n+    )\n+    connection = s3.test_connection(\n+        session=session,\n+        bucket_name=integration.configuration[\"bucket_name\"],\n+    )\n+\n+    if connection.is_connected:\n+        return True, s3\n+\n+    return False, connection\n+\n+\n+def upload_s3_integration(tenant_id: str, provider_id: str, file_path: str) -> bool:\n+    \"\"\"\n+    Upload the specified output files to an S3 bucket from an integration.\n+    If the S3 bucket environment variables are not configured,\n+    the function returns None without performing an upload.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier, used as part of the S3 key prefix.\n+        provider_id (str): The provider identifier, used as part of the S3 key prefix.\n+        file_path (str): The local file system path to the output files to be uploaded.\n+\n+    Returns:\n+        bool: True if all integrations were executed, False otherwise.\n+\n+    Raises:\n+        botocore.exceptions.ClientError: If the upload attempt to S3 fails for any reason.\n+    \"\"\"\n+    with rls_transaction(tenant_id):\n+        integrations = list(\n+            Integration.objects.filter(\n+                integrationproviderrelationship__provider_id=provider_id,\n+                integration_type=Integration.IntegrationChoices.S3,\n+            )\n+        )\n+\n+    if not integrations:\n+        logger.error(f\"No S3 integrations found for provider {provider_id}\")\n+        return False\n+\n+    integration_executions = 0\n+    for integration in integrations:\n+        integration_configuration = integration.configuration\n+        integration_bucket_name = integration_configuration.get(\"bucket_name\")\n+        integration_output_directory = integration_configuration.get(\"output_directory\")\n+\n+        try:\n+            connected, s3 = get_s3_client_from_integration(integration)\n+        except Exception as e:\n+            logger.error(f\"S3 connection failed for integration {integration.id}: {e}\")\n+            continue\n+\n+        if connected:\n+            try:\n+                for filename in os.listdir(file_path):\n+                    local_path = os.path.join(file_path, filename)\n+                    if not os.path.isfile(local_path):\n+                        continue\n+                    file_key = f\"{integration_output_directory}/{filename}\"\n+                    s3.upload_file(",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2157100530",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "api/src/backend/tasks/jobs/integrations.py",
        "discussion_id": "2157100530",
        "commented_code": "@@ -0,0 +1,123 @@\n+import os\n+\n+from celery.utils.log import get_task_logger\n+\n+from api.db_utils import rls_transaction\n+from api.models import Integration\n+from prowler.providers.aws.aws_provider import AwsProvider\n+from prowler.providers.aws.lib.s3.s3 import S3\n+from prowler.providers.common.models import Connection\n+\n+logger = get_task_logger(__name__)\n+\n+\n+def get_s3_client_from_integration(\n+    integration: Integration,\n+) -> tuple[bool, S3 | Connection]:\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from an integration.\n+\n+    Args:\n+        integration (Integration): The integration to get the S3 client from.\n+\n+    Returns:\n+        tuple[bool, S3 | Connection]: A tuple containing a boolean indicating if the connection was successful and the S3 client or connection object.\n+    \"\"\"\n+    session = AwsProvider(**integration.credentials).session.current_session\n+    s3 = S3(\n+        session=session,\n+        bucket_name=integration.configuration[\"bucket_name\"],\n+        output_directory=integration.configuration[\"output_directory\"],\n+    )\n+    connection = s3.test_connection(\n+        session=session,\n+        bucket_name=integration.configuration[\"bucket_name\"],\n+    )\n+\n+    if connection.is_connected:\n+        return True, s3\n+\n+    return False, connection\n+\n+\n+def upload_s3_integration(tenant_id: str, provider_id: str, file_path: str) -> bool:\n+    \"\"\"\n+    Upload the specified output files to an S3 bucket from an integration.\n+    If the S3 bucket environment variables are not configured,\n+    the function returns None without performing an upload.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier, used as part of the S3 key prefix.\n+        provider_id (str): The provider identifier, used as part of the S3 key prefix.\n+        file_path (str): The local file system path to the output files to be uploaded.\n+\n+    Returns:\n+        bool: True if all integrations were executed, False otherwise.\n+\n+    Raises:\n+        botocore.exceptions.ClientError: If the upload attempt to S3 fails for any reason.\n+    \"\"\"\n+    with rls_transaction(tenant_id):\n+        integrations = list(\n+            Integration.objects.filter(\n+                integrationproviderrelationship__provider_id=provider_id,\n+                integration_type=Integration.IntegrationChoices.S3,\n+            )\n+        )\n+\n+    if not integrations:\n+        logger.error(f\"No S3 integrations found for provider {provider_id}\")\n+        return False\n+\n+    integration_executions = 0\n+    for integration in integrations:\n+        integration_configuration = integration.configuration\n+        integration_bucket_name = integration_configuration.get(\"bucket_name\")\n+        integration_output_directory = integration_configuration.get(\"output_directory\")\n+\n+        try:\n+            connected, s3 = get_s3_client_from_integration(integration)\n+        except Exception as e:\n+            logger.error(f\"S3 connection failed for integration {integration.id}: {e}\")\n+            continue\n+\n+        if connected:\n+            try:\n+                for filename in os.listdir(file_path):\n+                    local_path = os.path.join(file_path, filename)\n+                    if not os.path.isfile(local_path):\n+                        continue\n+                    file_key = f\"{integration_output_directory}/{filename}\"\n+                    s3.upload_file(",
        "comment_created_at": "2025-06-19T14:00:04+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Why are you not using SDK's `send_to_bucket`? We should aim to have all this login in the SDK and to adapt whatever is missing for the API to use it. If not you will need to replicate all the logic in there to handle content-type and file paths.",
        "pr_file_module": null
      },
      {
        "comment_id": "2160944679",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "api/src/backend/tasks/jobs/integrations.py",
        "discussion_id": "2157100530",
        "commented_code": "@@ -0,0 +1,123 @@\n+import os\n+\n+from celery.utils.log import get_task_logger\n+\n+from api.db_utils import rls_transaction\n+from api.models import Integration\n+from prowler.providers.aws.aws_provider import AwsProvider\n+from prowler.providers.aws.lib.s3.s3 import S3\n+from prowler.providers.common.models import Connection\n+\n+logger = get_task_logger(__name__)\n+\n+\n+def get_s3_client_from_integration(\n+    integration: Integration,\n+) -> tuple[bool, S3 | Connection]:\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from an integration.\n+\n+    Args:\n+        integration (Integration): The integration to get the S3 client from.\n+\n+    Returns:\n+        tuple[bool, S3 | Connection]: A tuple containing a boolean indicating if the connection was successful and the S3 client or connection object.\n+    \"\"\"\n+    session = AwsProvider(**integration.credentials).session.current_session\n+    s3 = S3(\n+        session=session,\n+        bucket_name=integration.configuration[\"bucket_name\"],\n+        output_directory=integration.configuration[\"output_directory\"],\n+    )\n+    connection = s3.test_connection(\n+        session=session,\n+        bucket_name=integration.configuration[\"bucket_name\"],\n+    )\n+\n+    if connection.is_connected:\n+        return True, s3\n+\n+    return False, connection\n+\n+\n+def upload_s3_integration(tenant_id: str, provider_id: str, file_path: str) -> bool:\n+    \"\"\"\n+    Upload the specified output files to an S3 bucket from an integration.\n+    If the S3 bucket environment variables are not configured,\n+    the function returns None without performing an upload.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier, used as part of the S3 key prefix.\n+        provider_id (str): The provider identifier, used as part of the S3 key prefix.\n+        file_path (str): The local file system path to the output files to be uploaded.\n+\n+    Returns:\n+        bool: True if all integrations were executed, False otherwise.\n+\n+    Raises:\n+        botocore.exceptions.ClientError: If the upload attempt to S3 fails for any reason.\n+    \"\"\"\n+    with rls_transaction(tenant_id):\n+        integrations = list(\n+            Integration.objects.filter(\n+                integrationproviderrelationship__provider_id=provider_id,\n+                integration_type=Integration.IntegrationChoices.S3,\n+            )\n+        )\n+\n+    if not integrations:\n+        logger.error(f\"No S3 integrations found for provider {provider_id}\")\n+        return False\n+\n+    integration_executions = 0\n+    for integration in integrations:\n+        integration_configuration = integration.configuration\n+        integration_bucket_name = integration_configuration.get(\"bucket_name\")\n+        integration_output_directory = integration_configuration.get(\"output_directory\")\n+\n+        try:\n+            connected, s3 = get_s3_client_from_integration(integration)\n+        except Exception as e:\n+            logger.error(f\"S3 connection failed for integration {integration.id}: {e}\")\n+            continue\n+\n+        if connected:\n+            try:\n+                for filename in os.listdir(file_path):\n+                    local_path = os.path.join(file_path, filename)\n+                    if not os.path.isfile(local_path):\n+                        continue\n+                    file_key = f\"{integration_output_directory}/{filename}\"\n+                    s3.upload_file(",
        "comment_created_at": "2025-06-23T07:48:26+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "This method was created because the API uses a completely different system to manage outputs. While we could replicate what’s in the CLI, it’s not the most efficient approach for the API. So instead of introducing changes just to mirror the CLI, a new function was created. Regarding the content-type handling, in my tests, I haven’t had to worry about it, files are uploaded correctly, the type is set properly, and everything can be viewed and downloaded without any issues.\r\n\r\nWe can discuss this in more detail and reach an agreement since it’s an important decision",
        "pr_file_module": null
      },
      {
        "comment_id": "2160959700",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "api/src/backend/tasks/jobs/integrations.py",
        "discussion_id": "2157100530",
        "commented_code": "@@ -0,0 +1,123 @@\n+import os\n+\n+from celery.utils.log import get_task_logger\n+\n+from api.db_utils import rls_transaction\n+from api.models import Integration\n+from prowler.providers.aws.aws_provider import AwsProvider\n+from prowler.providers.aws.lib.s3.s3 import S3\n+from prowler.providers.common.models import Connection\n+\n+logger = get_task_logger(__name__)\n+\n+\n+def get_s3_client_from_integration(\n+    integration: Integration,\n+) -> tuple[bool, S3 | Connection]:\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from an integration.\n+\n+    Args:\n+        integration (Integration): The integration to get the S3 client from.\n+\n+    Returns:\n+        tuple[bool, S3 | Connection]: A tuple containing a boolean indicating if the connection was successful and the S3 client or connection object.\n+    \"\"\"\n+    session = AwsProvider(**integration.credentials).session.current_session\n+    s3 = S3(\n+        session=session,\n+        bucket_name=integration.configuration[\"bucket_name\"],\n+        output_directory=integration.configuration[\"output_directory\"],\n+    )\n+    connection = s3.test_connection(\n+        session=session,\n+        bucket_name=integration.configuration[\"bucket_name\"],\n+    )\n+\n+    if connection.is_connected:\n+        return True, s3\n+\n+    return False, connection\n+\n+\n+def upload_s3_integration(tenant_id: str, provider_id: str, file_path: str) -> bool:\n+    \"\"\"\n+    Upload the specified output files to an S3 bucket from an integration.\n+    If the S3 bucket environment variables are not configured,\n+    the function returns None without performing an upload.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier, used as part of the S3 key prefix.\n+        provider_id (str): The provider identifier, used as part of the S3 key prefix.\n+        file_path (str): The local file system path to the output files to be uploaded.\n+\n+    Returns:\n+        bool: True if all integrations were executed, False otherwise.\n+\n+    Raises:\n+        botocore.exceptions.ClientError: If the upload attempt to S3 fails for any reason.\n+    \"\"\"\n+    with rls_transaction(tenant_id):\n+        integrations = list(\n+            Integration.objects.filter(\n+                integrationproviderrelationship__provider_id=provider_id,\n+                integration_type=Integration.IntegrationChoices.S3,\n+            )\n+        )\n+\n+    if not integrations:\n+        logger.error(f\"No S3 integrations found for provider {provider_id}\")\n+        return False\n+\n+    integration_executions = 0\n+    for integration in integrations:\n+        integration_configuration = integration.configuration\n+        integration_bucket_name = integration_configuration.get(\"bucket_name\")\n+        integration_output_directory = integration_configuration.get(\"output_directory\")\n+\n+        try:\n+            connected, s3 = get_s3_client_from_integration(integration)\n+        except Exception as e:\n+            logger.error(f\"S3 connection failed for integration {integration.id}: {e}\")\n+            continue\n+\n+        if connected:\n+            try:\n+                for filename in os.listdir(file_path):\n+                    local_path = os.path.join(file_path, filename)\n+                    if not os.path.isfile(local_path):\n+                        continue\n+                    file_key = f\"{integration_output_directory}/{filename}\"\n+                    s3.upload_file(",
        "comment_created_at": "2025-06-23T07:56:33+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "I get that. I think we should first analyse the SDK's status prior starting the work and make all the required changes. \r\n\r\nRegarding the `content-type`, we need to set it for each file because AWS sets only the file type.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1940930503",
    "pr_number": 6746,
    "pr_file": "api/src/backend/api/v1/views.py",
    "created_at": "2025-02-04T10:45:01+00:00",
    "commented_code": "},\n         )\n \n+    @extend_schema(\n+        tags=[\"Scan\"],\n+        summary=\"Download ZIP report\",\n+        description=\"Returns a ZIP file containing the requested report\",\n+        request=ScanReportSerializer,\n+        responses={\n+            200: OpenApiResponse(description=\"Report obtanined successfully\"),\n+            404: OpenApiResponse(description=\"Report not found\"),\n+        },\n+    )\n+    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n+    def report(self, request, pk=None):\n+        s3_client = None\n+        try:\n+            s3_client = boto3.client(\"s3\")\n+            s3_client.list_buckets()\n+        except (ClientError, NoCredentialsError, ParamValidationError):\n+            try:\n+                s3_client = boto3.client(\n+                    \"s3\",\n+                    aws_access_key_id=env.str(\"ARTIFACTS_AWS_ACCESS_KEY_ID\"),\n+                    aws_secret_access_key=env.str(\"ARTIFACTS_AWS_SECRET_ACCESS_KEY\"),\n+                    aws_session_token=env.str(\"ARTIFACTS_AWS_SESSION_TOKEN\"),\n+                    region_name=env.str(\"ARTIFACTS_AWS_DEFAULT_REGION\"),\n+                )\n+                s3_client.list_buckets()\n+            except (ClientError, NoCredentialsError, ParamValidationError):\n+                s3_client = None\n+\n+        if s3_client:\n+            bucket_name = env.str(\"ARTIFACTS_AWS_S3_OUTPUT_BUCKET\")\n+            s3_prefix = f\"{request.tenant_id}/{pk}/\"\n+\n+            try:\n+                response = s3_client.list_objects_v2(\n+                    Bucket=bucket_name, Prefix=s3_prefix\n+                )\n+                if response[\"KeyCount\"] == 0:\n+                    return Response(\n+                        {\"detail\": \"No files found in S3 storage\"},\n+                        status=status.HTTP_404_NOT_FOUND,\n+                    )\n+\n+                zip_files = [\n+                    obj[\"Key\"]\n+                    for obj in response.get(\"Contents\", [])\n+                    if obj[\"Key\"].endswith(\".zip\")\n+                ]\n+                if not zip_files:\n+                    return Response(\n+                        {\"detail\": \"No ZIP files found in S3 storage\"},\n+                        status=status.HTTP_404_NOT_FOUND,\n+                    )\n+\n+                s3_key = zip_files[0]\n+                s3_object = s3_client.get_object(Bucket=bucket_name, Key=s3_key)\n+                file_content = s3_object[\"Body\"].read()\n+                filename = os.path.basename(s3_key)\n+\n+            except ClientError:\n+                return Response(\n+                    {\"detail\": \"Error accessing cloud storage\"},\n+                    status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n+                )\n+\n+        else:\n+            local_path = os.path.join(\n+                tmp_output_directory,\n+                str(request.tenant_id),\n+                str(pk),\n+                \"*.zip\",\n+            )\n+            zip_files = glob.glob(local_path)\n+            if not zip_files:\n+                return Response(\n+                    {\"detail\": \"No local files found\"}, status=status.HTTP_404_NOT_FOUND\n+                )\n+\n+            try:\n+                file_path = zip_files[0]\n+                with open(file_path, \"rb\") as f:\n+                    file_content = f.read()\n+                filename = os.path.basename(file_path)\n+            except IOError:\n+                return Response(\n+                    {\"detail\": \"Error reading local file\"},",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1940930503",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6746,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "1940930503",
        "commented_code": "@@ -1127,6 +1138,101 @@ def create(self, request, *args, **kwargs):\n             },\n         )\n \n+    @extend_schema(\n+        tags=[\"Scan\"],\n+        summary=\"Download ZIP report\",\n+        description=\"Returns a ZIP file containing the requested report\",\n+        request=ScanReportSerializer,\n+        responses={\n+            200: OpenApiResponse(description=\"Report obtanined successfully\"),\n+            404: OpenApiResponse(description=\"Report not found\"),\n+        },\n+    )\n+    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n+    def report(self, request, pk=None):\n+        s3_client = None\n+        try:\n+            s3_client = boto3.client(\"s3\")\n+            s3_client.list_buckets()\n+        except (ClientError, NoCredentialsError, ParamValidationError):\n+            try:\n+                s3_client = boto3.client(\n+                    \"s3\",\n+                    aws_access_key_id=env.str(\"ARTIFACTS_AWS_ACCESS_KEY_ID\"),\n+                    aws_secret_access_key=env.str(\"ARTIFACTS_AWS_SECRET_ACCESS_KEY\"),\n+                    aws_session_token=env.str(\"ARTIFACTS_AWS_SESSION_TOKEN\"),\n+                    region_name=env.str(\"ARTIFACTS_AWS_DEFAULT_REGION\"),\n+                )\n+                s3_client.list_buckets()\n+            except (ClientError, NoCredentialsError, ParamValidationError):\n+                s3_client = None\n+\n+        if s3_client:\n+            bucket_name = env.str(\"ARTIFACTS_AWS_S3_OUTPUT_BUCKET\")\n+            s3_prefix = f\"{request.tenant_id}/{pk}/\"\n+\n+            try:\n+                response = s3_client.list_objects_v2(\n+                    Bucket=bucket_name, Prefix=s3_prefix\n+                )\n+                if response[\"KeyCount\"] == 0:\n+                    return Response(\n+                        {\"detail\": \"No files found in S3 storage\"},\n+                        status=status.HTTP_404_NOT_FOUND,\n+                    )\n+\n+                zip_files = [\n+                    obj[\"Key\"]\n+                    for obj in response.get(\"Contents\", [])\n+                    if obj[\"Key\"].endswith(\".zip\")\n+                ]\n+                if not zip_files:\n+                    return Response(\n+                        {\"detail\": \"No ZIP files found in S3 storage\"},\n+                        status=status.HTTP_404_NOT_FOUND,\n+                    )\n+\n+                s3_key = zip_files[0]\n+                s3_object = s3_client.get_object(Bucket=bucket_name, Key=s3_key)\n+                file_content = s3_object[\"Body\"].read()\n+                filename = os.path.basename(s3_key)\n+\n+            except ClientError:\n+                return Response(\n+                    {\"detail\": \"Error accessing cloud storage\"},\n+                    status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n+                )\n+\n+        else:\n+            local_path = os.path.join(\n+                tmp_output_directory,\n+                str(request.tenant_id),\n+                str(pk),\n+                \"*.zip\",\n+            )\n+            zip_files = glob.glob(local_path)\n+            if not zip_files:\n+                return Response(\n+                    {\"detail\": \"No local files found\"}, status=status.HTTP_404_NOT_FOUND\n+                )\n+\n+            try:\n+                file_path = zip_files[0]\n+                with open(file_path, \"rb\") as f:\n+                    file_content = f.read()\n+                filename = os.path.basename(file_path)\n+            except IOError:\n+                return Response(\n+                    {\"detail\": \"Error reading local file\"},",
        "comment_created_at": "2025-02-04T10:45:01+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "I don't think this belongs in here. It also makes unit testing way more difficult. Please create a service layer to encapsulate all the logic related to the s3 integration.",
        "pr_file_module": null
      }
    ]
  }
]