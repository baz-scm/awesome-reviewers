[
  {
    "discussion_id": "2157380369",
    "pr_number": 8056,
    "pr_file": "api/src/backend/tasks/tasks.py",
    "created_at": "2025-06-19T16:38:01+00:00",
    "commented_code": "- 'available_models' (list): List of available models if connection is successful.\n     \"\"\"\n     return check_lighthouse_connection(lighthouse_config_id=lighthouse_config_id)\n+\n+\n+@shared_task(\n+    name=\"integration-check\",\n+    queue=\"integrations\",\n+)\n+def check_integrations_task(\n+    generate_outputs_result: dict, tenant_id: str, provider_id: str\n+):\n+    \"\"\"\n+    Check and execute all configured integrations for a provider.\n+\n+    Args:\n+        generate_outputs_result (dict): Result from generate_outputs task (contains output_directory)\n+        tenant_id (str): The tenant identifier\n+        provider_id (str): The provider identifier\n+    \"\"\"\n+    logger.info(f\"Checking integrations for provider {provider_id}\")\n+    with rls_transaction(tenant_id):\n+        integrations = Integration.objects.filter(\n+            integrationproviderrelationship__provider_id=provider_id\n+        )\n+\n+        if not integrations.exists():\n+            logger.info(f\"No integrations configured for provider {provider_id}\")\n+            return {\"integrations_processed\": 0}\n+\n+    integration_tasks = []\n+\n+    # S3 integrations (need output_directory)\n+    output_directory = generate_outputs_result.get(\"output_directory\")\n+    with rls_transaction(tenant_id):\n+        s3_integrations = list(\n+            integrations.filter(integration_type=Integration.IntegrationChoices.S3)\n+        )\n+\n+    if s3_integrations and output_directory:\n+        logger.info(f\"Found {len(s3_integrations)} S3 integration(s)\")\n+        integration_tasks.append(\n+            s3_integration_task.s(\n+                tenant_id=tenant_id,\n+                provider_id=provider_id,\n+                output_directory=output_directory,\n+            )\n+        )\n+\n+    # TODO: Add other integration types here\n+    # slack_integrations = integrations.filter(\n+    #     integration_type=Integration.IntegrationChoices.SLACK\n+    # )\n+    # if slack_integrations.exists():\n+    #     integration_tasks.append(\n+    #        slack_integration_task.s(\n+    #            tenant_id=tenant_id,\n+    #            provider_id=provider_id,\n+    #        )\n+    #     )\n+\n+    # Execute all integration tasks in parallel\n+    if integration_tasks:\n+        job = group(integration_tasks)\n+        job.apply_async()\n+        logger.info(f\"Launched {len(integration_tasks)} integration task(s)\")\n+\n+    return {\"integrations_processed\": len(integration_tasks)}\n+\n+\n+@shared_task(\n+    base=RLSTask,\n+    name=\"integration-s3\",\n+    queue=\"integrations\",\n+)\n+def s3_integration_task(tenant_id: str, provider_id: str, output_directory: str):\n+    \"\"\"\n+    Process S3 integrations for a provider.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier\n+        provider_id (str): The provider identifier\n+        output_directory (str): Directory containing the output files\n+    \"\"\"\n+    logger.info(f\"Processing S3 integrations for provider {provider_id}\")\n+\n+    try:\n+        result = upload_s3_integration(tenant_id, provider_id, output_directory)\n+        if result:\n+            logger.info(\n+                f\"All the S3 integrations completed successfully for provider {provider_id}\"",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2157380369",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "2157380369",
        "commented_code": "@@ -414,3 +444,102 @@ def check_lighthouse_connection_task(lighthouse_config_id: str, tenant_id: str =\n             - 'available_models' (list): List of available models if connection is successful.\n     \"\"\"\n     return check_lighthouse_connection(lighthouse_config_id=lighthouse_config_id)\n+\n+\n+@shared_task(\n+    name=\"integration-check\",\n+    queue=\"integrations\",\n+)\n+def check_integrations_task(\n+    generate_outputs_result: dict, tenant_id: str, provider_id: str\n+):\n+    \"\"\"\n+    Check and execute all configured integrations for a provider.\n+\n+    Args:\n+        generate_outputs_result (dict): Result from generate_outputs task (contains output_directory)\n+        tenant_id (str): The tenant identifier\n+        provider_id (str): The provider identifier\n+    \"\"\"\n+    logger.info(f\"Checking integrations for provider {provider_id}\")\n+    with rls_transaction(tenant_id):\n+        integrations = Integration.objects.filter(\n+            integrationproviderrelationship__provider_id=provider_id\n+        )\n+\n+        if not integrations.exists():\n+            logger.info(f\"No integrations configured for provider {provider_id}\")\n+            return {\"integrations_processed\": 0}\n+\n+    integration_tasks = []\n+\n+    # S3 integrations (need output_directory)\n+    output_directory = generate_outputs_result.get(\"output_directory\")\n+    with rls_transaction(tenant_id):\n+        s3_integrations = list(\n+            integrations.filter(integration_type=Integration.IntegrationChoices.S3)\n+        )\n+\n+    if s3_integrations and output_directory:\n+        logger.info(f\"Found {len(s3_integrations)} S3 integration(s)\")\n+        integration_tasks.append(\n+            s3_integration_task.s(\n+                tenant_id=tenant_id,\n+                provider_id=provider_id,\n+                output_directory=output_directory,\n+            )\n+        )\n+\n+    # TODO: Add other integration types here\n+    # slack_integrations = integrations.filter(\n+    #     integration_type=Integration.IntegrationChoices.SLACK\n+    # )\n+    # if slack_integrations.exists():\n+    #     integration_tasks.append(\n+    #        slack_integration_task.s(\n+    #            tenant_id=tenant_id,\n+    #            provider_id=provider_id,\n+    #        )\n+    #     )\n+\n+    # Execute all integration tasks in parallel\n+    if integration_tasks:\n+        job = group(integration_tasks)\n+        job.apply_async()\n+        logger.info(f\"Launched {len(integration_tasks)} integration task(s)\")\n+\n+    return {\"integrations_processed\": len(integration_tasks)}\n+\n+\n+@shared_task(\n+    base=RLSTask,\n+    name=\"integration-s3\",\n+    queue=\"integrations\",\n+)\n+def s3_integration_task(tenant_id: str, provider_id: str, output_directory: str):\n+    \"\"\"\n+    Process S3 integrations for a provider.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier\n+        provider_id (str): The provider identifier\n+        output_directory (str): Directory containing the output files\n+    \"\"\"\n+    logger.info(f\"Processing S3 integrations for provider {provider_id}\")\n+\n+    try:\n+        result = upload_s3_integration(tenant_id, provider_id, output_directory)\n+        if result:\n+            logger.info(\n+                f\"All the S3 integrations completed successfully for provider {provider_id}\"",
        "comment_created_at": "2025-06-19T16:38:01+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "I forgot to point the SDK to this branch and I got this error:\r\n```\r\nworker-dev-1   | [2025-06-19 16:35:02,487: ERROR/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: S3 output upload failed for integration 8740eeb1-789c-4890-93a9-f41d8c71a742: 'S3' object has no attribute 'upload_file'\r\nworker-dev-1   | [2025-06-19 16:35:02,503: ERROR/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: S3 compliance upload failed for integration 8740eeb1-789c-4890-93a9-f41d8c71a742: 'S3' object has no attribute 'upload_file'\r\nworker-dev-1   | [2025-06-19 16:35:02,517: INFO/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: All the S3 integrations completed successfully for provider 1370501e-80a3-4b1b-aad8-bdc4ce26066f\r\n```\r\n\r\nI think we should not print this line because there was an error:\r\n```\r\nworker-dev-1   | [2025-06-19 16:35:02,517: INFO/ForkPoolWorker-6] integration-s3[fae1321a-ae7c-40c3-af9d-02cf12de8f8e]: All the S3 integrations completed successfully for provider 1370501e-80a3-4b1b-aad8-bdc4ce26066f\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2160917206",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "2157380369",
        "commented_code": "@@ -414,3 +444,102 @@ def check_lighthouse_connection_task(lighthouse_config_id: str, tenant_id: str =\n             - 'available_models' (list): List of available models if connection is successful.\n     \"\"\"\n     return check_lighthouse_connection(lighthouse_config_id=lighthouse_config_id)\n+\n+\n+@shared_task(\n+    name=\"integration-check\",\n+    queue=\"integrations\",\n+)\n+def check_integrations_task(\n+    generate_outputs_result: dict, tenant_id: str, provider_id: str\n+):\n+    \"\"\"\n+    Check and execute all configured integrations for a provider.\n+\n+    Args:\n+        generate_outputs_result (dict): Result from generate_outputs task (contains output_directory)\n+        tenant_id (str): The tenant identifier\n+        provider_id (str): The provider identifier\n+    \"\"\"\n+    logger.info(f\"Checking integrations for provider {provider_id}\")\n+    with rls_transaction(tenant_id):\n+        integrations = Integration.objects.filter(\n+            integrationproviderrelationship__provider_id=provider_id\n+        )\n+\n+        if not integrations.exists():\n+            logger.info(f\"No integrations configured for provider {provider_id}\")\n+            return {\"integrations_processed\": 0}\n+\n+    integration_tasks = []\n+\n+    # S3 integrations (need output_directory)\n+    output_directory = generate_outputs_result.get(\"output_directory\")\n+    with rls_transaction(tenant_id):\n+        s3_integrations = list(\n+            integrations.filter(integration_type=Integration.IntegrationChoices.S3)\n+        )\n+\n+    if s3_integrations and output_directory:\n+        logger.info(f\"Found {len(s3_integrations)} S3 integration(s)\")\n+        integration_tasks.append(\n+            s3_integration_task.s(\n+                tenant_id=tenant_id,\n+                provider_id=provider_id,\n+                output_directory=output_directory,\n+            )\n+        )\n+\n+    # TODO: Add other integration types here\n+    # slack_integrations = integrations.filter(\n+    #     integration_type=Integration.IntegrationChoices.SLACK\n+    # )\n+    # if slack_integrations.exists():\n+    #     integration_tasks.append(\n+    #        slack_integration_task.s(\n+    #            tenant_id=tenant_id,\n+    #            provider_id=provider_id,\n+    #        )\n+    #     )\n+\n+    # Execute all integration tasks in parallel\n+    if integration_tasks:\n+        job = group(integration_tasks)\n+        job.apply_async()\n+        logger.info(f\"Launched {len(integration_tasks)} integration task(s)\")\n+\n+    return {\"integrations_processed\": len(integration_tasks)}\n+\n+\n+@shared_task(\n+    base=RLSTask,\n+    name=\"integration-s3\",\n+    queue=\"integrations\",\n+)\n+def s3_integration_task(tenant_id: str, provider_id: str, output_directory: str):\n+    \"\"\"\n+    Process S3 integrations for a provider.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier\n+        provider_id (str): The provider identifier\n+        output_directory (str): Directory containing the output files\n+    \"\"\"\n+    logger.info(f\"Processing S3 integrations for provider {provider_id}\")\n+\n+    try:\n+        result = upload_s3_integration(tenant_id, provider_id, output_directory)\n+        if result:\n+            logger.info(\n+                f\"All the S3 integrations completed successfully for provider {provider_id}\"",
        "comment_created_at": "2025-06-23T07:33:03+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "Good catch, I\u2019ll push the fix later ",
        "pr_file_module": null
      },
      {
        "comment_id": "2161099242",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8056,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "2157380369",
        "commented_code": "@@ -414,3 +444,102 @@ def check_lighthouse_connection_task(lighthouse_config_id: str, tenant_id: str =\n             - 'available_models' (list): List of available models if connection is successful.\n     \"\"\"\n     return check_lighthouse_connection(lighthouse_config_id=lighthouse_config_id)\n+\n+\n+@shared_task(\n+    name=\"integration-check\",\n+    queue=\"integrations\",\n+)\n+def check_integrations_task(\n+    generate_outputs_result: dict, tenant_id: str, provider_id: str\n+):\n+    \"\"\"\n+    Check and execute all configured integrations for a provider.\n+\n+    Args:\n+        generate_outputs_result (dict): Result from generate_outputs task (contains output_directory)\n+        tenant_id (str): The tenant identifier\n+        provider_id (str): The provider identifier\n+    \"\"\"\n+    logger.info(f\"Checking integrations for provider {provider_id}\")\n+    with rls_transaction(tenant_id):\n+        integrations = Integration.objects.filter(\n+            integrationproviderrelationship__provider_id=provider_id\n+        )\n+\n+        if not integrations.exists():\n+            logger.info(f\"No integrations configured for provider {provider_id}\")\n+            return {\"integrations_processed\": 0}\n+\n+    integration_tasks = []\n+\n+    # S3 integrations (need output_directory)\n+    output_directory = generate_outputs_result.get(\"output_directory\")\n+    with rls_transaction(tenant_id):\n+        s3_integrations = list(\n+            integrations.filter(integration_type=Integration.IntegrationChoices.S3)\n+        )\n+\n+    if s3_integrations and output_directory:\n+        logger.info(f\"Found {len(s3_integrations)} S3 integration(s)\")\n+        integration_tasks.append(\n+            s3_integration_task.s(\n+                tenant_id=tenant_id,\n+                provider_id=provider_id,\n+                output_directory=output_directory,\n+            )\n+        )\n+\n+    # TODO: Add other integration types here\n+    # slack_integrations = integrations.filter(\n+    #     integration_type=Integration.IntegrationChoices.SLACK\n+    # )\n+    # if slack_integrations.exists():\n+    #     integration_tasks.append(\n+    #        slack_integration_task.s(\n+    #            tenant_id=tenant_id,\n+    #            provider_id=provider_id,\n+    #        )\n+    #     )\n+\n+    # Execute all integration tasks in parallel\n+    if integration_tasks:\n+        job = group(integration_tasks)\n+        job.apply_async()\n+        logger.info(f\"Launched {len(integration_tasks)} integration task(s)\")\n+\n+    return {\"integrations_processed\": len(integration_tasks)}\n+\n+\n+@shared_task(\n+    base=RLSTask,\n+    name=\"integration-s3\",\n+    queue=\"integrations\",\n+)\n+def s3_integration_task(tenant_id: str, provider_id: str, output_directory: str):\n+    \"\"\"\n+    Process S3 integrations for a provider.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier\n+        provider_id (str): The provider identifier\n+        output_directory (str): Directory containing the output files\n+    \"\"\"\n+    logger.info(f\"Processing S3 integrations for provider {provider_id}\")\n+\n+    try:\n+        result = upload_s3_integration(tenant_id, provider_id, output_directory)\n+        if result:\n+            logger.info(\n+                f\"All the S3 integrations completed successfully for provider {provider_id}\"",
        "comment_created_at": "2025-06-23T09:05:20+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2150357241",
    "pr_number": 8035,
    "pr_file": "prowler/lib/check/models.py",
    "created_at": "2025-06-16T15:58:38+00:00",
    "commented_code": "check_metadata = CheckMetadata.parse_file(metadata_file)\n     except ValidationError as error:\n         logger.critical(f\"Metadata from {metadata_file} is not valid: {error}\")\n-        # TODO: remove this exit and raise an exception\n-        sys.exit(1)\n+        raise error",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2150357241",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8035,
        "pr_file": "prowler/lib/check/models.py",
        "discussion_id": "2150357241",
        "commented_code": "@@ -646,7 +653,6 @@ def load_check_metadata(metadata_file: str) -> CheckMetadata:\n         check_metadata = CheckMetadata.parse_file(metadata_file)\n     except ValidationError as error:\n         logger.critical(f\"Metadata from {metadata_file} is not valid: {error}\")\n-        # TODO: remove this exit and raise an exception\n-        sys.exit(1)\n+        raise error",
        "comment_created_at": "2025-06-16T15:58:38+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Why this? Are we sure that this is not going to break anything?",
        "pr_file_module": null
      },
      {
        "comment_id": "2150365373",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8035,
        "pr_file": "prowler/lib/check/models.py",
        "discussion_id": "2150357241",
        "commented_code": "@@ -646,7 +653,6 @@ def load_check_metadata(metadata_file: str) -> CheckMetadata:\n         check_metadata = CheckMetadata.parse_file(metadata_file)\n     except ValidationError as error:\n         logger.critical(f\"Metadata from {metadata_file} is not valid: {error}\")\n-        # TODO: remove this exit and raise an exception\n-        sys.exit(1)\n+        raise error",
        "comment_created_at": "2025-06-16T16:03:06+00:00",
        "comment_author": "MrCloudSec",
        "comment_body": "Raising the exception is actually safer, it allows errors to be handled or tested, instead of abruptly stopping the whole process with `sys.exit(1)`. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2150367638",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8035,
        "pr_file": "prowler/lib/check/models.py",
        "discussion_id": "2150357241",
        "commented_code": "@@ -646,7 +653,6 @@ def load_check_metadata(metadata_file: str) -> CheckMetadata:\n         check_metadata = CheckMetadata.parse_file(metadata_file)\n     except ValidationError as error:\n         logger.critical(f\"Metadata from {metadata_file} is not valid: {error}\")\n-        # TODO: remove this exit and raise an exception\n-        sys.exit(1)\n+        raise error",
        "comment_created_at": "2025-06-16T16:04:21+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "I knew that, only concerned about this changes because you know what happened in other cases, but better if this fails because something was not right.\r\n\r\nThanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2052173271",
    "pr_number": 7508,
    "pr_file": "api/src/backend/api/models.py",
    "created_at": "2025-04-21T09:18:38+00:00",
    "commented_code": "statements=[\"SELECT\", \"INSERT\", \"UPDATE\", \"DELETE\"],\n             ),\n         ]\n+\n+\n+class LighthouseConfig(RowLevelSecurityProtectedModel):\n+    \"\"\"\n+    Stores configuration and API keys for LLM services.\n+    \"\"\"\n+\n+    id = models.UUIDField(primary_key=True, default=uuid4, editable=False)\n+    inserted_at = models.DateTimeField(auto_now_add=True, editable=False)\n+    updated_at = models.DateTimeField(auto_now=True, editable=False)\n+\n+    name = models.CharField(max_length=100, validators=[MinLengthValidator(3)])\n+    api_key = models.BinaryField(blank=False, null=False)\n+    model = models.CharField(max_length=50, default=\"gpt-4o\")\n+    temperature = models.FloatField(default=0.7)\n+    max_tokens = models.IntegerField(default=4000)\n+    business_context = models.TextField(\n+        blank=True,\n+        null=True,\n+        help_text=\"Additional business context for this AI model configuration\",\n+    )\n+    is_active = models.BooleanField(default=True)\n+\n+    def __str__(self):\n+        return self.name\n+\n+    @property\n+    def api_key_decoded(self):\n+        \"\"\"Return the decrypted API key.\"\"\"\n+        try:\n+            return fernet.decrypt(self.api_key).decode()\n+        except Exception:\n+            return None",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2052173271",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7508,
        "pr_file": "api/src/backend/api/models.py",
        "discussion_id": "2052173271",
        "commented_code": "@@ -1222,3 +1222,64 @@ class Meta:\n                 statements=[\"SELECT\", \"INSERT\", \"UPDATE\", \"DELETE\"],\n             ),\n         ]\n+\n+\n+class LighthouseConfig(RowLevelSecurityProtectedModel):\n+    \"\"\"\n+    Stores configuration and API keys for LLM services.\n+    \"\"\"\n+\n+    id = models.UUIDField(primary_key=True, default=uuid4, editable=False)\n+    inserted_at = models.DateTimeField(auto_now_add=True, editable=False)\n+    updated_at = models.DateTimeField(auto_now=True, editable=False)\n+\n+    name = models.CharField(max_length=100, validators=[MinLengthValidator(3)])\n+    api_key = models.BinaryField(blank=False, null=False)\n+    model = models.CharField(max_length=50, default=\"gpt-4o\")\n+    temperature = models.FloatField(default=0.7)\n+    max_tokens = models.IntegerField(default=4000)\n+    business_context = models.TextField(\n+        blank=True,\n+        null=True,\n+        help_text=\"Additional business context for this AI model configuration\",\n+    )\n+    is_active = models.BooleanField(default=True)\n+\n+    def __str__(self):\n+        return self.name\n+\n+    @property\n+    def api_key_decoded(self):\n+        \"\"\"Return the decrypted API key.\"\"\"\n+        try:\n+            return fernet.decrypt(self.api_key).decode()\n+        except Exception:\n+            return None",
        "comment_created_at": "2025-04-21T09:18:38+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "I think that since the user can add this value it is important to verify and track what is happening, I propose a more secure version of this function with more information to detect anomalous cases:\r\n```suggestion\r\n        try:\r\n            decrypted_key = fernet.decrypt(self.api_key)\r\n            return decrypted_key.decode()\r\n        except InvalidToken:\r\n            logger.warning(\"Failed to decrypt API key: invalid token.\")\r\n            return None\r\n        except Exception as e:\r\n            logger.error(f\"Unexpected error while decrypting API key: {e}\")\r\n            return None\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1967313773",
    "pr_number": 6878,
    "pr_file": "api/src/backend/api/v1/views.py",
    "created_at": "2025-02-24T09:53:39+00:00",
    "commented_code": ")\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n+    @extend_schema(\n+        tags=[\"Scan\"],\n+        summary=\"Download ZIP report\",\n+        description=\"Returns a ZIP file containing the requested report\",\n+        request=ScanReportSerializer,\n+        responses={\n+            200: OpenApiResponse(description=\"Report obtained successfully\"),\n+            202: OpenApiResponse(description=\"The task is in progress\"),\n+            403: OpenApiResponse(\n+                description=\"There is a problem with the AWS credentials\"\n+            ),\n+            404: OpenApiResponse(description=\"The scan has not generated reports\"),\n+        },\n+    )\n+    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n+    def report(self, request, pk=None):\n+        scan_instance = self.get_object()\n+\n+        if scan_instance.state == StateChoices.EXECUTING:\n+            # If the scan is still running, return the task\n+            prowler_task = TaskSerializer(scan_instance.task)\n+            return Response(\n+                data=prowler_task.data,\n+                status=status.HTTP_202_ACCEPTED,\n+                headers={\n+                    \"Content-Location\": reverse(\n+                        \"task-detail\", kwargs={\"pk\": prowler_task.data[\"id\"]}\n+                    )\n+                },\n+            )\n+\n+        try:\n+            output_celery_task = Task.objects.get(\n+                task_runner_task__task_name=\"scan-report\",\n+                task_runner_task__task_args__contains=pk,\n+            )\n+            prowler_task_data = TaskSerializer(output_celery_task).data\n+            if prowler_task_data[\"state\"] == StateChoices.EXECUTING:\n+                # If the task is still running, return the task\n+                return Response(\n+                    data=prowler_task_data,\n+                    status=status.HTTP_202_ACCEPTED,\n+                    headers={\n+                        \"Content-Location\": reverse(\n+                            \"task-detail\", kwargs={\"pk\": prowler_task_data[\"id\"]}\n+                        )\n+                    },\n+                )\n+        except Task.DoesNotExist:\n+            # If the task does not exist, it means that the task is removed from the database\n+            pass\n+\n+        output_path = scan_instance.output_path\n+        if not output_path:\n+            return Response(\n+                {\"detail\": \"The scan has not generated reports.\"},\n+                status=status.HTTP_404_NOT_FOUND,\n+            )\n+\n+        if scan_instance.output_path.startswith(\"s3://\"):\n+            try:\n+                s3_client = get_s3_client()\n+            except (ClientError, NoCredentialsError, ParamValidationError):\n+                return Response(\n+                    {\"detail\": \"There is a problem with the AWS credentials.\"},\n+                    status=status.HTTP_403_FORBIDDEN,\n+                )\n+\n+            bucket_name = env.str(\"DJANGO_OUTPUT_AWS_S3_OUTPUT_BUCKET\")\n+            key = output_path[len(f\"s3://{bucket_name}/\") :]\n+            s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\n+            file_content = s3_object[\"Body\"].read()\n+            filename = os.path.basename(output_path.split(\"/\")[-1])",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1967313773",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "1967313773",
        "commented_code": "@@ -1181,6 +1190,92 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n+    @extend_schema(\n+        tags=[\"Scan\"],\n+        summary=\"Download ZIP report\",\n+        description=\"Returns a ZIP file containing the requested report\",\n+        request=ScanReportSerializer,\n+        responses={\n+            200: OpenApiResponse(description=\"Report obtained successfully\"),\n+            202: OpenApiResponse(description=\"The task is in progress\"),\n+            403: OpenApiResponse(\n+                description=\"There is a problem with the AWS credentials\"\n+            ),\n+            404: OpenApiResponse(description=\"The scan has not generated reports\"),\n+        },\n+    )\n+    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n+    def report(self, request, pk=None):\n+        scan_instance = self.get_object()\n+\n+        if scan_instance.state == StateChoices.EXECUTING:\n+            # If the scan is still running, return the task\n+            prowler_task = TaskSerializer(scan_instance.task)\n+            return Response(\n+                data=prowler_task.data,\n+                status=status.HTTP_202_ACCEPTED,\n+                headers={\n+                    \"Content-Location\": reverse(\n+                        \"task-detail\", kwargs={\"pk\": prowler_task.data[\"id\"]}\n+                    )\n+                },\n+            )\n+\n+        try:\n+            output_celery_task = Task.objects.get(\n+                task_runner_task__task_name=\"scan-report\",\n+                task_runner_task__task_args__contains=pk,\n+            )\n+            prowler_task_data = TaskSerializer(output_celery_task).data\n+            if prowler_task_data[\"state\"] == StateChoices.EXECUTING:\n+                # If the task is still running, return the task\n+                return Response(\n+                    data=prowler_task_data,\n+                    status=status.HTTP_202_ACCEPTED,\n+                    headers={\n+                        \"Content-Location\": reverse(\n+                            \"task-detail\", kwargs={\"pk\": prowler_task_data[\"id\"]}\n+                        )\n+                    },\n+                )\n+        except Task.DoesNotExist:\n+            # If the task does not exist, it means that the task is removed from the database\n+            pass\n+\n+        output_path = scan_instance.output_path\n+        if not output_path:\n+            return Response(\n+                {\"detail\": \"The scan has not generated reports.\"},\n+                status=status.HTTP_404_NOT_FOUND,\n+            )\n+\n+        if scan_instance.output_path.startswith(\"s3://\"):\n+            try:\n+                s3_client = get_s3_client()\n+            except (ClientError, NoCredentialsError, ParamValidationError):\n+                return Response(\n+                    {\"detail\": \"There is a problem with the AWS credentials.\"},\n+                    status=status.HTTP_403_FORBIDDEN,\n+                )\n+\n+            bucket_name = env.str(\"DJANGO_OUTPUT_AWS_S3_OUTPUT_BUCKET\")\n+            key = output_path[len(f\"s3://{bucket_name}/\") :]\n+            s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\n+            file_content = s3_object[\"Body\"].read()\n+            filename = os.path.basename(output_path.split(\"/\")[-1])",
        "comment_created_at": "2025-02-24T09:53:39+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "We need to handle `NoSuchKey` if the file got deleted from S3.",
        "pr_file_module": null
      },
      {
        "comment_id": "1969803964",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "1967313773",
        "commented_code": "@@ -1181,6 +1190,92 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n+    @extend_schema(\n+        tags=[\"Scan\"],\n+        summary=\"Download ZIP report\",\n+        description=\"Returns a ZIP file containing the requested report\",\n+        request=ScanReportSerializer,\n+        responses={\n+            200: OpenApiResponse(description=\"Report obtained successfully\"),\n+            202: OpenApiResponse(description=\"The task is in progress\"),\n+            403: OpenApiResponse(\n+                description=\"There is a problem with the AWS credentials\"\n+            ),\n+            404: OpenApiResponse(description=\"The scan has not generated reports\"),\n+        },\n+    )\n+    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n+    def report(self, request, pk=None):\n+        scan_instance = self.get_object()\n+\n+        if scan_instance.state == StateChoices.EXECUTING:\n+            # If the scan is still running, return the task\n+            prowler_task = TaskSerializer(scan_instance.task)\n+            return Response(\n+                data=prowler_task.data,\n+                status=status.HTTP_202_ACCEPTED,\n+                headers={\n+                    \"Content-Location\": reverse(\n+                        \"task-detail\", kwargs={\"pk\": prowler_task.data[\"id\"]}\n+                    )\n+                },\n+            )\n+\n+        try:\n+            output_celery_task = Task.objects.get(\n+                task_runner_task__task_name=\"scan-report\",\n+                task_runner_task__task_args__contains=pk,\n+            )\n+            prowler_task_data = TaskSerializer(output_celery_task).data\n+            if prowler_task_data[\"state\"] == StateChoices.EXECUTING:\n+                # If the task is still running, return the task\n+                return Response(\n+                    data=prowler_task_data,\n+                    status=status.HTTP_202_ACCEPTED,\n+                    headers={\n+                        \"Content-Location\": reverse(\n+                            \"task-detail\", kwargs={\"pk\": prowler_task_data[\"id\"]}\n+                        )\n+                    },\n+                )\n+        except Task.DoesNotExist:\n+            # If the task does not exist, it means that the task is removed from the database\n+            pass\n+\n+        output_path = scan_instance.output_path\n+        if not output_path:\n+            return Response(\n+                {\"detail\": \"The scan has not generated reports.\"},\n+                status=status.HTTP_404_NOT_FOUND,\n+            )\n+\n+        if scan_instance.output_path.startswith(\"s3://\"):\n+            try:\n+                s3_client = get_s3_client()\n+            except (ClientError, NoCredentialsError, ParamValidationError):\n+                return Response(\n+                    {\"detail\": \"There is a problem with the AWS credentials.\"},\n+                    status=status.HTTP_403_FORBIDDEN,\n+                )\n+\n+            bucket_name = env.str(\"DJANGO_OUTPUT_AWS_S3_OUTPUT_BUCKET\")\n+            key = output_path[len(f\"s3://{bucket_name}/\") :]\n+            s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\n+            file_content = s3_object[\"Body\"].read()\n+            filename = os.path.basename(output_path.split(\"/\")[-1])",
        "comment_created_at": "2025-02-25T13:39:39+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Tested and fixed \u2705  ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1955994884",
    "pr_number": 6874,
    "pr_file": "api/src/backend/config/django/base.py",
    "created_at": "2025-02-14T11:26:35+00:00",
    "commented_code": "TESTING = False\n+\n FINDINGS_MAX_DAYS_IN_RANGE = env.int(\"DJANGO_FINDINGS_MAX_DAYS_IN_RANGE\", 7)\n+\n+sentry_sdk.init(",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1955994884",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6874,
        "pr_file": "api/src/backend/config/django/base.py",
        "discussion_id": "1955994884",
        "commented_code": "@@ -207,4 +209,18 @@\n \n \n TESTING = False\n+\n FINDINGS_MAX_DAYS_IN_RANGE = env.int(\"DJANGO_FINDINGS_MAX_DAYS_IN_RANGE\", 7)\n+\n+sentry_sdk.init(",
        "comment_created_at": "2025-02-14T11:26:35+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "We should also add `before_send=before_send` not to send several credential errors coming from the SDK.\r\n\r\n\r\n```python\r\nignored_exceptions = [\r\n    # Authentication Errors from AWS\r\n    \"InvalidToken\",\r\n    \"AccessDeniedException\",\r\n    \"AuthorizationErrorException\",\r\n    \"UnrecognizedClientException\",\r\n    \"UnauthorizedOperation\",\r\n    \"AuthFailure\",\r\n    \"InvalidClientTokenId\",\r\n    \"AccessDenied\",\r\n    # Shodan Check\r\n    \"No Shodan API Key\",\r\n    # For now we don't want to log the RequestLimitExceeded errors\r\n    \"RequestLimitExceeded\",\r\n    \"ThrottlingException\",\r\n    \"Rate exceeded\",\r\n    # The following comes from urllib3\r\n    # eu-west-1 -- HTTPClientError[126]: An HTTP Client raised an unhandled exception: AWSHTTPSConnectionPool(host='hostname.s3.eu-west-1.amazonaws.com', port=443): Pool is closed.\r\n    \"Pool is closed\",\r\n]\r\n\r\n\r\ndef before_send(event, hint):\r\n    \"\"\"\r\n    before_send handles the Sentry events in order to sent them or not\r\n    \"\"\"\r\n    # Ignore logs with the ignored_exceptions\r\n    # https://docs.python.org/3/library/logging.html#logrecord-objects\r\n    if \"log_record\" in hint:\r\n        log_msg = hint[\"log_record\"].msg\r\n        log_lvl = hint[\"log_record\"].levelno\r\n\r\n        # Handle Error events and discard the rest\r\n        if log_lvl == 40 and any(ignored in log_msg for ignored in ignored_exceptions):\r\n            return\r\n    return event\r\n```\r\n\r\nThis is not tested \ud83d\ude04 ",
        "pr_file_module": null
      },
      {
        "comment_id": "1956042305",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6874,
        "pr_file": "api/src/backend/config/django/base.py",
        "discussion_id": "1955994884",
        "commented_code": "@@ -207,4 +209,18 @@\n \n \n TESTING = False\n+\n FINDINGS_MAX_DAYS_IN_RANGE = env.int(\"DJANGO_FINDINGS_MAX_DAYS_IN_RANGE\", 7)\n+\n+sentry_sdk.init(",
        "comment_created_at": "2025-02-14T12:05:08+00:00",
        "comment_author": "cesararroba",
        "comment_body": "Added and tested \ud83d\udc4d ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2073211883",
    "pr_number": 7653,
    "pr_file": "api/src/backend/api/v1/views.py",
    "created_at": "2025-05-05T10:36:13+00:00",
    "commented_code": ")\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n-    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n-    def report(self, request, pk=None):\n-        scan_instance = self.get_object()\n-\n-        if scan_instance.state == StateChoices.EXECUTING:\n-            # If the scan is still running, return the task\n-            prowler_task = Task.objects.get(id=scan_instance.task.id)\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(prowler_task)\n-            return Response(\n-                data=output_serializer.data,\n-                status=status.HTTP_202_ACCEPTED,\n-                headers={\n-                    \"Content-Location\": reverse(\n-                        \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                    )\n-                },\n-            )\n+    def _get_running_task_response(self, scan_instance):\n+        \"\"\"\n+        If the scan or its report-generation task is still executing,\n+        return a 202 Response with the task payload and Content-Location.\n+        \"\"\"\n+        task = None\n \n-        try:\n-            output_celery_task = Task.objects.get(\n-                task_runner_task__task_name=\"scan-report\",\n-                task_runner_task__task_args__contains=pk,\n-            )\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(output_celery_task)\n-            if output_serializer.data[\"state\"] == StateChoices.EXECUTING:\n-                # If the task is still running, return the task\n-                return Response(\n-                    data=output_serializer.data,\n-                    status=status.HTTP_202_ACCEPTED,\n-                    headers={\n-                        \"Content-Location\": reverse(\n-                            \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                        )\n-                    },\n+        if scan_instance.state == StateChoices.EXECUTING and scan_instance.task:\n+            task = scan_instance.task\n+        else:\n+            try:\n+                task = Task.objects.get(\n+                    task_runner_task__task_name=\"scan-report\",\n+                    task_runner_task__task_args__contains=str(scan_instance.id),\n                 )\n-        except Task.DoesNotExist:\n-            # If the task does not exist, it means that the task is removed from the database\n-            pass\n+            except Task.DoesNotExist:\n+                return None\n \n-        output_location = scan_instance.output_location\n-        if not output_location:\n-            return Response(\n-                {\"detail\": \"The scan has no reports.\"},\n-                status=status.HTTP_404_NOT_FOUND,\n-            )\n+        self.response_serializer_class = TaskSerializer\n+        serializer = self.get_serializer(task)\n \n-        if scan_instance.output_location.startswith(\"s3://\"):\n+        if serializer.data.get(\"state\") != StateChoices.EXECUTING:\n+            return None\n+\n+        return Response(\n+            data=serializer.data,\n+            status=status.HTTP_202_ACCEPTED,\n+            headers={\n+                \"Content-Location\": reverse(\n+                    \"task-detail\", kwargs={\"pk\": serializer.data[\"id\"]}\n+                )\n+            },\n+        )\n+\n+    def _load_file(self, path_pattern, s3=False, bucket=None, list_objects=False):\n+        \"\"\"\n+        Load binary content and filename.\n+        If s3=True and list_objects=False: treat path_pattern as exact key.\n+        If s3=True and list_objects=True: list by prefix, then pick first matching key.\n+        Else: treat path_pattern as glob pattern on local FS.\n+        Returns (content, filename) or Response on error.\n+        \"\"\"\n+        if s3:\n             try:\n-                s3_client = get_s3_client()\n+                client = get_s3_client()\n             except (ClientError, NoCredentialsError, ParamValidationError):\n                 return Response(\n                     {\"detail\": \"There is a problem with credentials.\"},\n                     status=status.HTTP_403_FORBIDDEN,\n                 )\n-\n-            bucket_name = env.str(\"DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET\")\n-            key = output_location[len(f\"s3://{bucket_name}/\") :]\n+            if list_objects:\n+                # list keys under prefix then match suffix\n+                prefix = os.path.dirname(path_pattern)\n+                suffix = os.path.basename(path_pattern)\n+                try:\n+                    resp = client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n+                except ClientError:\n+                    return Response(",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2073211883",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2073211883",
        "commented_code": "@@ -1219,70 +1246,86 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n-    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n-    def report(self, request, pk=None):\n-        scan_instance = self.get_object()\n-\n-        if scan_instance.state == StateChoices.EXECUTING:\n-            # If the scan is still running, return the task\n-            prowler_task = Task.objects.get(id=scan_instance.task.id)\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(prowler_task)\n-            return Response(\n-                data=output_serializer.data,\n-                status=status.HTTP_202_ACCEPTED,\n-                headers={\n-                    \"Content-Location\": reverse(\n-                        \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                    )\n-                },\n-            )\n+    def _get_running_task_response(self, scan_instance):\n+        \"\"\"\n+        If the scan or its report-generation task is still executing,\n+        return a 202 Response with the task payload and Content-Location.\n+        \"\"\"\n+        task = None\n \n-        try:\n-            output_celery_task = Task.objects.get(\n-                task_runner_task__task_name=\"scan-report\",\n-                task_runner_task__task_args__contains=pk,\n-            )\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(output_celery_task)\n-            if output_serializer.data[\"state\"] == StateChoices.EXECUTING:\n-                # If the task is still running, return the task\n-                return Response(\n-                    data=output_serializer.data,\n-                    status=status.HTTP_202_ACCEPTED,\n-                    headers={\n-                        \"Content-Location\": reverse(\n-                            \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                        )\n-                    },\n+        if scan_instance.state == StateChoices.EXECUTING and scan_instance.task:\n+            task = scan_instance.task\n+        else:\n+            try:\n+                task = Task.objects.get(\n+                    task_runner_task__task_name=\"scan-report\",\n+                    task_runner_task__task_args__contains=str(scan_instance.id),\n                 )\n-        except Task.DoesNotExist:\n-            # If the task does not exist, it means that the task is removed from the database\n-            pass\n+            except Task.DoesNotExist:\n+                return None\n \n-        output_location = scan_instance.output_location\n-        if not output_location:\n-            return Response(\n-                {\"detail\": \"The scan has no reports.\"},\n-                status=status.HTTP_404_NOT_FOUND,\n-            )\n+        self.response_serializer_class = TaskSerializer\n+        serializer = self.get_serializer(task)\n \n-        if scan_instance.output_location.startswith(\"s3://\"):\n+        if serializer.data.get(\"state\") != StateChoices.EXECUTING:\n+            return None\n+\n+        return Response(\n+            data=serializer.data,\n+            status=status.HTTP_202_ACCEPTED,\n+            headers={\n+                \"Content-Location\": reverse(\n+                    \"task-detail\", kwargs={\"pk\": serializer.data[\"id\"]}\n+                )\n+            },\n+        )\n+\n+    def _load_file(self, path_pattern, s3=False, bucket=None, list_objects=False):\n+        \"\"\"\n+        Load binary content and filename.\n+        If s3=True and list_objects=False: treat path_pattern as exact key.\n+        If s3=True and list_objects=True: list by prefix, then pick first matching key.\n+        Else: treat path_pattern as glob pattern on local FS.\n+        Returns (content, filename) or Response on error.\n+        \"\"\"\n+        if s3:\n             try:\n-                s3_client = get_s3_client()\n+                client = get_s3_client()\n             except (ClientError, NoCredentialsError, ParamValidationError):\n                 return Response(\n                     {\"detail\": \"There is a problem with credentials.\"},\n                     status=status.HTTP_403_FORBIDDEN,\n                 )\n-\n-            bucket_name = env.str(\"DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET\")\n-            key = output_location[len(f\"s3://{bucket_name}/\") :]\n+            if list_objects:\n+                # list keys under prefix then match suffix\n+                prefix = os.path.dirname(path_pattern)\n+                suffix = os.path.basename(path_pattern)\n+                try:\n+                    resp = client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n+                except ClientError:\n+                    return Response(",
        "comment_created_at": "2025-05-05T10:36:13+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "I think this needs to be refined as we previously did\r\n```python\r\ntry:\r\n                s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\r\n            except ClientError as e:\r\n                error_code = e.response.get(\"Error\", {}).get(\"Code\")\r\n                if error_code == \"NoSuchKey\":\r\n                    return Response(\r\n                        {\"detail\": \"The scan has no reports.\"},\r\n                        status=status.HTTP_404_NOT_FOUND,\r\n                    )\r\n                return Response(\r\n                    {\"detail\": \"There is a problem with credentials.\"},\r\n                    status=status.HTTP_403_FORBIDDEN,\r\n                )\r\n```\r\n\r\nWe should not raise `HTTP 500 Internal Server Error`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2073274202",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2073211883",
        "commented_code": "@@ -1219,70 +1246,86 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n-    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n-    def report(self, request, pk=None):\n-        scan_instance = self.get_object()\n-\n-        if scan_instance.state == StateChoices.EXECUTING:\n-            # If the scan is still running, return the task\n-            prowler_task = Task.objects.get(id=scan_instance.task.id)\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(prowler_task)\n-            return Response(\n-                data=output_serializer.data,\n-                status=status.HTTP_202_ACCEPTED,\n-                headers={\n-                    \"Content-Location\": reverse(\n-                        \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                    )\n-                },\n-            )\n+    def _get_running_task_response(self, scan_instance):\n+        \"\"\"\n+        If the scan or its report-generation task is still executing,\n+        return a 202 Response with the task payload and Content-Location.\n+        \"\"\"\n+        task = None\n \n-        try:\n-            output_celery_task = Task.objects.get(\n-                task_runner_task__task_name=\"scan-report\",\n-                task_runner_task__task_args__contains=pk,\n-            )\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(output_celery_task)\n-            if output_serializer.data[\"state\"] == StateChoices.EXECUTING:\n-                # If the task is still running, return the task\n-                return Response(\n-                    data=output_serializer.data,\n-                    status=status.HTTP_202_ACCEPTED,\n-                    headers={\n-                        \"Content-Location\": reverse(\n-                            \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                        )\n-                    },\n+        if scan_instance.state == StateChoices.EXECUTING and scan_instance.task:\n+            task = scan_instance.task\n+        else:\n+            try:\n+                task = Task.objects.get(\n+                    task_runner_task__task_name=\"scan-report\",\n+                    task_runner_task__task_args__contains=str(scan_instance.id),\n                 )\n-        except Task.DoesNotExist:\n-            # If the task does not exist, it means that the task is removed from the database\n-            pass\n+            except Task.DoesNotExist:\n+                return None\n \n-        output_location = scan_instance.output_location\n-        if not output_location:\n-            return Response(\n-                {\"detail\": \"The scan has no reports.\"},\n-                status=status.HTTP_404_NOT_FOUND,\n-            )\n+        self.response_serializer_class = TaskSerializer\n+        serializer = self.get_serializer(task)\n \n-        if scan_instance.output_location.startswith(\"s3://\"):\n+        if serializer.data.get(\"state\") != StateChoices.EXECUTING:\n+            return None\n+\n+        return Response(\n+            data=serializer.data,\n+            status=status.HTTP_202_ACCEPTED,\n+            headers={\n+                \"Content-Location\": reverse(\n+                    \"task-detail\", kwargs={\"pk\": serializer.data[\"id\"]}\n+                )\n+            },\n+        )\n+\n+    def _load_file(self, path_pattern, s3=False, bucket=None, list_objects=False):\n+        \"\"\"\n+        Load binary content and filename.\n+        If s3=True and list_objects=False: treat path_pattern as exact key.\n+        If s3=True and list_objects=True: list by prefix, then pick first matching key.\n+        Else: treat path_pattern as glob pattern on local FS.\n+        Returns (content, filename) or Response on error.\n+        \"\"\"\n+        if s3:\n             try:\n-                s3_client = get_s3_client()\n+                client = get_s3_client()\n             except (ClientError, NoCredentialsError, ParamValidationError):\n                 return Response(\n                     {\"detail\": \"There is a problem with credentials.\"},\n                     status=status.HTTP_403_FORBIDDEN,\n                 )\n-\n-            bucket_name = env.str(\"DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET\")\n-            key = output_location[len(f\"s3://{bucket_name}/\") :]\n+            if list_objects:\n+                # list keys under prefix then match suffix\n+                prefix = os.path.dirname(path_pattern)\n+                suffix = os.path.basename(path_pattern)\n+                try:\n+                    resp = client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n+                except ClientError:\n+                    return Response(",
        "comment_created_at": "2025-05-05T11:29:51+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "I set a 500 because at that point, if a ClientError really appears, the error is not on the part of the user, it is something internal to us. In my opinion the errors of type 400 are a problem of configuration or in the request, in short, a user problem. The errors 500 are our errors and should never happen but if they happen in this case it is controlled. We can discuss this and change it as we decide",
        "pr_file_module": null
      },
      {
        "comment_id": "2073378746",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2073211883",
        "commented_code": "@@ -1219,70 +1246,86 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n-    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n-    def report(self, request, pk=None):\n-        scan_instance = self.get_object()\n-\n-        if scan_instance.state == StateChoices.EXECUTING:\n-            # If the scan is still running, return the task\n-            prowler_task = Task.objects.get(id=scan_instance.task.id)\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(prowler_task)\n-            return Response(\n-                data=output_serializer.data,\n-                status=status.HTTP_202_ACCEPTED,\n-                headers={\n-                    \"Content-Location\": reverse(\n-                        \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                    )\n-                },\n-            )\n+    def _get_running_task_response(self, scan_instance):\n+        \"\"\"\n+        If the scan or its report-generation task is still executing,\n+        return a 202 Response with the task payload and Content-Location.\n+        \"\"\"\n+        task = None\n \n-        try:\n-            output_celery_task = Task.objects.get(\n-                task_runner_task__task_name=\"scan-report\",\n-                task_runner_task__task_args__contains=pk,\n-            )\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(output_celery_task)\n-            if output_serializer.data[\"state\"] == StateChoices.EXECUTING:\n-                # If the task is still running, return the task\n-                return Response(\n-                    data=output_serializer.data,\n-                    status=status.HTTP_202_ACCEPTED,\n-                    headers={\n-                        \"Content-Location\": reverse(\n-                            \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                        )\n-                    },\n+        if scan_instance.state == StateChoices.EXECUTING and scan_instance.task:\n+            task = scan_instance.task\n+        else:\n+            try:\n+                task = Task.objects.get(\n+                    task_runner_task__task_name=\"scan-report\",\n+                    task_runner_task__task_args__contains=str(scan_instance.id),\n                 )\n-        except Task.DoesNotExist:\n-            # If the task does not exist, it means that the task is removed from the database\n-            pass\n+            except Task.DoesNotExist:\n+                return None\n \n-        output_location = scan_instance.output_location\n-        if not output_location:\n-            return Response(\n-                {\"detail\": \"The scan has no reports.\"},\n-                status=status.HTTP_404_NOT_FOUND,\n-            )\n+        self.response_serializer_class = TaskSerializer\n+        serializer = self.get_serializer(task)\n \n-        if scan_instance.output_location.startswith(\"s3://\"):\n+        if serializer.data.get(\"state\") != StateChoices.EXECUTING:\n+            return None\n+\n+        return Response(\n+            data=serializer.data,\n+            status=status.HTTP_202_ACCEPTED,\n+            headers={\n+                \"Content-Location\": reverse(\n+                    \"task-detail\", kwargs={\"pk\": serializer.data[\"id\"]}\n+                )\n+            },\n+        )\n+\n+    def _load_file(self, path_pattern, s3=False, bucket=None, list_objects=False):\n+        \"\"\"\n+        Load binary content and filename.\n+        If s3=True and list_objects=False: treat path_pattern as exact key.\n+        If s3=True and list_objects=True: list by prefix, then pick first matching key.\n+        Else: treat path_pattern as glob pattern on local FS.\n+        Returns (content, filename) or Response on error.\n+        \"\"\"\n+        if s3:\n             try:\n-                s3_client = get_s3_client()\n+                client = get_s3_client()\n             except (ClientError, NoCredentialsError, ParamValidationError):\n                 return Response(\n                     {\"detail\": \"There is a problem with credentials.\"},\n                     status=status.HTTP_403_FORBIDDEN,\n                 )\n-\n-            bucket_name = env.str(\"DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET\")\n-            key = output_location[len(f\"s3://{bucket_name}/\") :]\n+            if list_objects:\n+                # list keys under prefix then match suffix\n+                prefix = os.path.dirname(path_pattern)\n+                suffix = os.path.basename(path_pattern)\n+                try:\n+                    resp = client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n+                except ClientError:\n+                    return Response(",
        "comment_created_at": "2025-05-05T12:49:31+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "I do not agree with you. A `ClientError` in `botocore`/`boto3` could not end up in an `HTTP 500 Internal Server Error` because is it a wrapper on top of different types of errors. If we are doing a `list_objects_v2` we should handle what's happening by:\r\n- Logging the error\r\n- Sending the exception to Sentry\r\n\r\nWe can maybe raise 500's for some cases but not for all the possible exceptions under `ClientError`.",
        "pr_file_module": null
      },
      {
        "comment_id": "2073680099",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2073211883",
        "commented_code": "@@ -1219,70 +1246,86 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n-    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n-    def report(self, request, pk=None):\n-        scan_instance = self.get_object()\n-\n-        if scan_instance.state == StateChoices.EXECUTING:\n-            # If the scan is still running, return the task\n-            prowler_task = Task.objects.get(id=scan_instance.task.id)\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(prowler_task)\n-            return Response(\n-                data=output_serializer.data,\n-                status=status.HTTP_202_ACCEPTED,\n-                headers={\n-                    \"Content-Location\": reverse(\n-                        \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                    )\n-                },\n-            )\n+    def _get_running_task_response(self, scan_instance):\n+        \"\"\"\n+        If the scan or its report-generation task is still executing,\n+        return a 202 Response with the task payload and Content-Location.\n+        \"\"\"\n+        task = None\n \n-        try:\n-            output_celery_task = Task.objects.get(\n-                task_runner_task__task_name=\"scan-report\",\n-                task_runner_task__task_args__contains=pk,\n-            )\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(output_celery_task)\n-            if output_serializer.data[\"state\"] == StateChoices.EXECUTING:\n-                # If the task is still running, return the task\n-                return Response(\n-                    data=output_serializer.data,\n-                    status=status.HTTP_202_ACCEPTED,\n-                    headers={\n-                        \"Content-Location\": reverse(\n-                            \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                        )\n-                    },\n+        if scan_instance.state == StateChoices.EXECUTING and scan_instance.task:\n+            task = scan_instance.task\n+        else:\n+            try:\n+                task = Task.objects.get(\n+                    task_runner_task__task_name=\"scan-report\",\n+                    task_runner_task__task_args__contains=str(scan_instance.id),\n                 )\n-        except Task.DoesNotExist:\n-            # If the task does not exist, it means that the task is removed from the database\n-            pass\n+            except Task.DoesNotExist:\n+                return None\n \n-        output_location = scan_instance.output_location\n-        if not output_location:\n-            return Response(\n-                {\"detail\": \"The scan has no reports.\"},\n-                status=status.HTTP_404_NOT_FOUND,\n-            )\n+        self.response_serializer_class = TaskSerializer\n+        serializer = self.get_serializer(task)\n \n-        if scan_instance.output_location.startswith(\"s3://\"):\n+        if serializer.data.get(\"state\") != StateChoices.EXECUTING:\n+            return None\n+\n+        return Response(\n+            data=serializer.data,\n+            status=status.HTTP_202_ACCEPTED,\n+            headers={\n+                \"Content-Location\": reverse(\n+                    \"task-detail\", kwargs={\"pk\": serializer.data[\"id\"]}\n+                )\n+            },\n+        )\n+\n+    def _load_file(self, path_pattern, s3=False, bucket=None, list_objects=False):\n+        \"\"\"\n+        Load binary content and filename.\n+        If s3=True and list_objects=False: treat path_pattern as exact key.\n+        If s3=True and list_objects=True: list by prefix, then pick first matching key.\n+        Else: treat path_pattern as glob pattern on local FS.\n+        Returns (content, filename) or Response on error.\n+        \"\"\"\n+        if s3:\n             try:\n-                s3_client = get_s3_client()\n+                client = get_s3_client()\n             except (ClientError, NoCredentialsError, ParamValidationError):\n                 return Response(\n                     {\"detail\": \"There is a problem with credentials.\"},\n                     status=status.HTTP_403_FORBIDDEN,\n                 )\n-\n-            bucket_name = env.str(\"DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET\")\n-            key = output_location[len(f\"s3://{bucket_name}/\") :]\n+            if list_objects:\n+                # list keys under prefix then match suffix\n+                prefix = os.path.dirname(path_pattern)\n+                suffix = os.path.basename(path_pattern)\n+                try:\n+                    resp = client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n+                except ClientError:\n+                    return Response(",
        "comment_created_at": "2025-05-05T15:35:27+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "I will change it but in my opinion even if boto fails it is still not the user's fault and in any case it is the server that has failed. At that point there is no way the user could have caused the crash",
        "pr_file_module": null
      },
      {
        "comment_id": "2073695617",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2073211883",
        "commented_code": "@@ -1219,70 +1246,86 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n-    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n-    def report(self, request, pk=None):\n-        scan_instance = self.get_object()\n-\n-        if scan_instance.state == StateChoices.EXECUTING:\n-            # If the scan is still running, return the task\n-            prowler_task = Task.objects.get(id=scan_instance.task.id)\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(prowler_task)\n-            return Response(\n-                data=output_serializer.data,\n-                status=status.HTTP_202_ACCEPTED,\n-                headers={\n-                    \"Content-Location\": reverse(\n-                        \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                    )\n-                },\n-            )\n+    def _get_running_task_response(self, scan_instance):\n+        \"\"\"\n+        If the scan or its report-generation task is still executing,\n+        return a 202 Response with the task payload and Content-Location.\n+        \"\"\"\n+        task = None\n \n-        try:\n-            output_celery_task = Task.objects.get(\n-                task_runner_task__task_name=\"scan-report\",\n-                task_runner_task__task_args__contains=pk,\n-            )\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(output_celery_task)\n-            if output_serializer.data[\"state\"] == StateChoices.EXECUTING:\n-                # If the task is still running, return the task\n-                return Response(\n-                    data=output_serializer.data,\n-                    status=status.HTTP_202_ACCEPTED,\n-                    headers={\n-                        \"Content-Location\": reverse(\n-                            \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                        )\n-                    },\n+        if scan_instance.state == StateChoices.EXECUTING and scan_instance.task:\n+            task = scan_instance.task\n+        else:\n+            try:\n+                task = Task.objects.get(\n+                    task_runner_task__task_name=\"scan-report\",\n+                    task_runner_task__task_args__contains=str(scan_instance.id),\n                 )\n-        except Task.DoesNotExist:\n-            # If the task does not exist, it means that the task is removed from the database\n-            pass\n+            except Task.DoesNotExist:\n+                return None\n \n-        output_location = scan_instance.output_location\n-        if not output_location:\n-            return Response(\n-                {\"detail\": \"The scan has no reports.\"},\n-                status=status.HTTP_404_NOT_FOUND,\n-            )\n+        self.response_serializer_class = TaskSerializer\n+        serializer = self.get_serializer(task)\n \n-        if scan_instance.output_location.startswith(\"s3://\"):\n+        if serializer.data.get(\"state\") != StateChoices.EXECUTING:\n+            return None\n+\n+        return Response(\n+            data=serializer.data,\n+            status=status.HTTP_202_ACCEPTED,\n+            headers={\n+                \"Content-Location\": reverse(\n+                    \"task-detail\", kwargs={\"pk\": serializer.data[\"id\"]}\n+                )\n+            },\n+        )\n+\n+    def _load_file(self, path_pattern, s3=False, bucket=None, list_objects=False):\n+        \"\"\"\n+        Load binary content and filename.\n+        If s3=True and list_objects=False: treat path_pattern as exact key.\n+        If s3=True and list_objects=True: list by prefix, then pick first matching key.\n+        Else: treat path_pattern as glob pattern on local FS.\n+        Returns (content, filename) or Response on error.\n+        \"\"\"\n+        if s3:\n             try:\n-                s3_client = get_s3_client()\n+                client = get_s3_client()\n             except (ClientError, NoCredentialsError, ParamValidationError):\n                 return Response(\n                     {\"detail\": \"There is a problem with credentials.\"},\n                     status=status.HTTP_403_FORBIDDEN,\n                 )\n-\n-            bucket_name = env.str(\"DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET\")\n-            key = output_location[len(f\"s3://{bucket_name}/\") :]\n+            if list_objects:\n+                # list keys under prefix then match suffix\n+                prefix = os.path.dirname(path_pattern)\n+                suffix = os.path.basename(path_pattern)\n+                try:\n+                    resp = client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n+                except ClientError:\n+                    return Response(",
        "comment_created_at": "2025-05-05T15:44:43+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "What I mean is that at this point all errors are unexpected",
        "pr_file_module": null
      },
      {
        "comment_id": "2073700834",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2073211883",
        "commented_code": "@@ -1219,70 +1246,86 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n-    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n-    def report(self, request, pk=None):\n-        scan_instance = self.get_object()\n-\n-        if scan_instance.state == StateChoices.EXECUTING:\n-            # If the scan is still running, return the task\n-            prowler_task = Task.objects.get(id=scan_instance.task.id)\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(prowler_task)\n-            return Response(\n-                data=output_serializer.data,\n-                status=status.HTTP_202_ACCEPTED,\n-                headers={\n-                    \"Content-Location\": reverse(\n-                        \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                    )\n-                },\n-            )\n+    def _get_running_task_response(self, scan_instance):\n+        \"\"\"\n+        If the scan or its report-generation task is still executing,\n+        return a 202 Response with the task payload and Content-Location.\n+        \"\"\"\n+        task = None\n \n-        try:\n-            output_celery_task = Task.objects.get(\n-                task_runner_task__task_name=\"scan-report\",\n-                task_runner_task__task_args__contains=pk,\n-            )\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(output_celery_task)\n-            if output_serializer.data[\"state\"] == StateChoices.EXECUTING:\n-                # If the task is still running, return the task\n-                return Response(\n-                    data=output_serializer.data,\n-                    status=status.HTTP_202_ACCEPTED,\n-                    headers={\n-                        \"Content-Location\": reverse(\n-                            \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                        )\n-                    },\n+        if scan_instance.state == StateChoices.EXECUTING and scan_instance.task:\n+            task = scan_instance.task\n+        else:\n+            try:\n+                task = Task.objects.get(\n+                    task_runner_task__task_name=\"scan-report\",\n+                    task_runner_task__task_args__contains=str(scan_instance.id),\n                 )\n-        except Task.DoesNotExist:\n-            # If the task does not exist, it means that the task is removed from the database\n-            pass\n+            except Task.DoesNotExist:\n+                return None\n \n-        output_location = scan_instance.output_location\n-        if not output_location:\n-            return Response(\n-                {\"detail\": \"The scan has no reports.\"},\n-                status=status.HTTP_404_NOT_FOUND,\n-            )\n+        self.response_serializer_class = TaskSerializer\n+        serializer = self.get_serializer(task)\n \n-        if scan_instance.output_location.startswith(\"s3://\"):\n+        if serializer.data.get(\"state\") != StateChoices.EXECUTING:\n+            return None\n+\n+        return Response(\n+            data=serializer.data,\n+            status=status.HTTP_202_ACCEPTED,\n+            headers={\n+                \"Content-Location\": reverse(\n+                    \"task-detail\", kwargs={\"pk\": serializer.data[\"id\"]}\n+                )\n+            },\n+        )\n+\n+    def _load_file(self, path_pattern, s3=False, bucket=None, list_objects=False):\n+        \"\"\"\n+        Load binary content and filename.\n+        If s3=True and list_objects=False: treat path_pattern as exact key.\n+        If s3=True and list_objects=True: list by prefix, then pick first matching key.\n+        Else: treat path_pattern as glob pattern on local FS.\n+        Returns (content, filename) or Response on error.\n+        \"\"\"\n+        if s3:\n             try:\n-                s3_client = get_s3_client()\n+                client = get_s3_client()\n             except (ClientError, NoCredentialsError, ParamValidationError):\n                 return Response(\n                     {\"detail\": \"There is a problem with credentials.\"},\n                     status=status.HTTP_403_FORBIDDEN,\n                 )\n-\n-            bucket_name = env.str(\"DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET\")\n-            key = output_location[len(f\"s3://{bucket_name}/\") :]\n+            if list_objects:\n+                # list keys under prefix then match suffix\n+                prefix = os.path.dirname(path_pattern)\n+                suffix = os.path.basename(path_pattern)\n+                try:\n+                    resp = client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n+                except ClientError:\n+                    return Response(",
        "comment_created_at": "2025-05-05T15:48:20+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "Change applied",
        "pr_file_module": null
      },
      {
        "comment_id": "2077241477",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7653,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2073211883",
        "commented_code": "@@ -1219,70 +1246,86 @@ def partial_update(self, request, *args, **kwargs):\n         )\n         return Response(data=read_serializer.data, status=status.HTTP_200_OK)\n \n-    @action(detail=True, methods=[\"get\"], url_name=\"report\")\n-    def report(self, request, pk=None):\n-        scan_instance = self.get_object()\n-\n-        if scan_instance.state == StateChoices.EXECUTING:\n-            # If the scan is still running, return the task\n-            prowler_task = Task.objects.get(id=scan_instance.task.id)\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(prowler_task)\n-            return Response(\n-                data=output_serializer.data,\n-                status=status.HTTP_202_ACCEPTED,\n-                headers={\n-                    \"Content-Location\": reverse(\n-                        \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                    )\n-                },\n-            )\n+    def _get_running_task_response(self, scan_instance):\n+        \"\"\"\n+        If the scan or its report-generation task is still executing,\n+        return a 202 Response with the task payload and Content-Location.\n+        \"\"\"\n+        task = None\n \n-        try:\n-            output_celery_task = Task.objects.get(\n-                task_runner_task__task_name=\"scan-report\",\n-                task_runner_task__task_args__contains=pk,\n-            )\n-            self.response_serializer_class = TaskSerializer\n-            output_serializer = self.get_serializer(output_celery_task)\n-            if output_serializer.data[\"state\"] == StateChoices.EXECUTING:\n-                # If the task is still running, return the task\n-                return Response(\n-                    data=output_serializer.data,\n-                    status=status.HTTP_202_ACCEPTED,\n-                    headers={\n-                        \"Content-Location\": reverse(\n-                            \"task-detail\", kwargs={\"pk\": output_serializer.data[\"id\"]}\n-                        )\n-                    },\n+        if scan_instance.state == StateChoices.EXECUTING and scan_instance.task:\n+            task = scan_instance.task\n+        else:\n+            try:\n+                task = Task.objects.get(\n+                    task_runner_task__task_name=\"scan-report\",\n+                    task_runner_task__task_args__contains=str(scan_instance.id),\n                 )\n-        except Task.DoesNotExist:\n-            # If the task does not exist, it means that the task is removed from the database\n-            pass\n+            except Task.DoesNotExist:\n+                return None\n \n-        output_location = scan_instance.output_location\n-        if not output_location:\n-            return Response(\n-                {\"detail\": \"The scan has no reports.\"},\n-                status=status.HTTP_404_NOT_FOUND,\n-            )\n+        self.response_serializer_class = TaskSerializer\n+        serializer = self.get_serializer(task)\n \n-        if scan_instance.output_location.startswith(\"s3://\"):\n+        if serializer.data.get(\"state\") != StateChoices.EXECUTING:\n+            return None\n+\n+        return Response(\n+            data=serializer.data,\n+            status=status.HTTP_202_ACCEPTED,\n+            headers={\n+                \"Content-Location\": reverse(\n+                    \"task-detail\", kwargs={\"pk\": serializer.data[\"id\"]}\n+                )\n+            },\n+        )\n+\n+    def _load_file(self, path_pattern, s3=False, bucket=None, list_objects=False):\n+        \"\"\"\n+        Load binary content and filename.\n+        If s3=True and list_objects=False: treat path_pattern as exact key.\n+        If s3=True and list_objects=True: list by prefix, then pick first matching key.\n+        Else: treat path_pattern as glob pattern on local FS.\n+        Returns (content, filename) or Response on error.\n+        \"\"\"\n+        if s3:\n             try:\n-                s3_client = get_s3_client()\n+                client = get_s3_client()\n             except (ClientError, NoCredentialsError, ParamValidationError):\n                 return Response(\n                     {\"detail\": \"There is a problem with credentials.\"},\n                     status=status.HTTP_403_FORBIDDEN,\n                 )\n-\n-            bucket_name = env.str(\"DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET\")\n-            key = output_location[len(f\"s3://{bucket_name}/\") :]\n+            if list_objects:\n+                # list keys under prefix then match suffix\n+                prefix = os.path.dirname(path_pattern)\n+                suffix = os.path.basename(path_pattern)\n+                try:\n+                    resp = client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n+                except ClientError:\n+                    return Response(",
        "comment_created_at": "2025-05-07T09:38:39+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Thanks, I know that we need to improve the way we handle this things, but I'm not sure about it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2068611451",
    "pr_number": 7618,
    "pr_file": "prowler/lib/powershell/powershell.py",
    "created_at": "2025-04-30T12:56:40+00:00",
    "commented_code": "error_thread.daemon = True\n         error_thread.start()\n \n-        result = result_queue.get(timeout=timeout) or default\n-        error_result = error_queue.get(timeout=1)\n+        try:\n+            result = result_queue.get(timeout=timeout) or default\n+        except queue.Empty:\n+            result = default\n+\n+        try:\n+            error_result = error_queue.get(timeout=1)\n+        except queue.Empty:\n+            error_result = \"\"",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2068611451",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7618,
        "pr_file": "prowler/lib/powershell/powershell.py",
        "discussion_id": "2068611451",
        "commented_code": "@@ -191,8 +191,15 @@ def error_reader_thread():\n         error_thread.daemon = True\n         error_thread.start()\n \n-        result = result_queue.get(timeout=timeout) or default\n-        error_result = error_queue.get(timeout=1)\n+        try:\n+            result = result_queue.get(timeout=timeout) or default\n+        except queue.Empty:\n+            result = default\n+\n+        try:\n+            error_result = error_queue.get(timeout=1)\n+        except queue.Empty:\n+            error_result = \"\"",
        "comment_created_at": "2025-04-30T12:56:40+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "Maybe we can change the code like this?:\r\n\r\n```suggestion\r\n        try:\r\n            result = result_queue.get(timeout=timeout) or default\r\n            error_result = error_queue.get(timeout=1)\r\n        except queue.Empty:\r\n            result = default\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2068642886",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7618,
        "pr_file": "prowler/lib/powershell/powershell.py",
        "discussion_id": "2068611451",
        "commented_code": "@@ -191,8 +191,15 @@ def error_reader_thread():\n         error_thread.daemon = True\n         error_thread.start()\n \n-        result = result_queue.get(timeout=timeout) or default\n-        error_result = error_queue.get(timeout=1)\n+        try:\n+            result = result_queue.get(timeout=timeout) or default\n+        except queue.Empty:\n+            result = default\n+\n+        try:\n+            error_result = error_queue.get(timeout=1)\n+        except queue.Empty:\n+            error_result = \"\"",
        "comment_created_at": "2025-04-30T13:15:29+00:00",
        "comment_author": "HugoPBrito",
        "comment_body": "Of course. Thanks for the suggestion.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1862542972",
    "pr_number": 5939,
    "pr_file": "api/src/backend/api/models.py",
    "created_at": "2024-11-28T17:42:06+00:00",
    "commented_code": "@staticmethod\n     def validate_kubernetes_uid(value):\n-        if not re.match(r\"^[a-z0-9]([-a-z0-9]{1,61}[a-z0-9])?$\", value):\n-            raise ModelValidationError(\n-                detail=\"K8s provider ID must be up to 63 characters, start and end with a lowercase letter or number, \"\n-                \"and contain only lowercase alphanumeric characters and hyphens.\",\n-                code=\"kubernetes-uid\",\n-                pointer=\"/data/attributes/uid\",\n+        if not re.match(\n+            r\"(^[a-z0-9]([-a-z0-9]{1,61}[a-z0-9])?$)|(^arn:aws(-cn|-us-gov|-iso|-iso-b)?:[a-zA-Z0-9\\-]+:([a-z]{2}-[a-z]+-\\d{1})?:(\\d{12})?:[a-zA-Z0-9\\-_\\/:\\.\\*]+(:\\d+)?$)\",\n+            value,\n+        ):\n+            raise ValueError(\n+                \"The value must either be a valid Kubernetes UID (up to 63 characters, \"\n+                \"starting and ending with a lowercase letter or number, containing only \"\n+                \"lowercase alphanumeric characters and hyphens) or a valid EKS ARN.\"",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1862542972",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 5939,
        "pr_file": "api/src/backend/api/models.py",
        "discussion_id": "1862542972",
        "commented_code": "@@ -187,12 +187,14 @@ def validate_gcp_uid(value):\n \n     @staticmethod\n     def validate_kubernetes_uid(value):\n-        if not re.match(r\"^[a-z0-9]([-a-z0-9]{1,61}[a-z0-9])?$\", value):\n-            raise ModelValidationError(\n-                detail=\"K8s provider ID must be up to 63 characters, start and end with a lowercase letter or number, \"\n-                \"and contain only lowercase alphanumeric characters and hyphens.\",\n-                code=\"kubernetes-uid\",\n-                pointer=\"/data/attributes/uid\",\n+        if not re.match(\n+            r\"(^[a-z0-9]([-a-z0-9]{1,61}[a-z0-9])?$)|(^arn:aws(-cn|-us-gov|-iso|-iso-b)?:[a-zA-Z0-9\\-]+:([a-z]{2}-[a-z]+-\\d{1})?:(\\d{12})?:[a-zA-Z0-9\\-_\\/:\\.\\*]+(:\\d+)?$)\",\n+            value,\n+        ):\n+            raise ValueError(\n+                \"The value must either be a valid Kubernetes UID (up to 63 characters, \"\n+                \"starting and ending with a lowercase letter or number, containing only \"\n+                \"lowercase alphanumeric characters and hyphens) or a valid EKS ARN.\"",
        "comment_created_at": "2024-11-28T17:42:06+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "Please, update regexes and detail messages, but don't modify the current logic. Use `ModelValidationError` with all the required fields. This breaks this exception.",
        "pr_file_module": null
      },
      {
        "comment_id": "1862547476",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 5939,
        "pr_file": "api/src/backend/api/models.py",
        "discussion_id": "1862542972",
        "commented_code": "@@ -187,12 +187,14 @@ def validate_gcp_uid(value):\n \n     @staticmethod\n     def validate_kubernetes_uid(value):\n-        if not re.match(r\"^[a-z0-9]([-a-z0-9]{1,61}[a-z0-9])?$\", value):\n-            raise ModelValidationError(\n-                detail=\"K8s provider ID must be up to 63 characters, start and end with a lowercase letter or number, \"\n-                \"and contain only lowercase alphanumeric characters and hyphens.\",\n-                code=\"kubernetes-uid\",\n-                pointer=\"/data/attributes/uid\",\n+        if not re.match(\n+            r\"(^[a-z0-9]([-a-z0-9]{1,61}[a-z0-9])?$)|(^arn:aws(-cn|-us-gov|-iso|-iso-b)?:[a-zA-Z0-9\\-]+:([a-z]{2}-[a-z]+-\\d{1})?:(\\d{12})?:[a-zA-Z0-9\\-_\\/:\\.\\*]+(:\\d+)?$)\",\n+            value,\n+        ):\n+            raise ValueError(\n+                \"The value must either be a valid Kubernetes UID (up to 63 characters, \"\n+                \"starting and ending with a lowercase letter or number, containing only \"\n+                \"lowercase alphanumeric characters and hyphens) or a valid EKS ARN.\"",
        "comment_created_at": "2024-11-28T17:47:47+00:00",
        "comment_author": "MrCloudSec",
        "comment_body": "Good catch, I will change it now!",
        "pr_file_module": null
      }
    ]
  }
]