[
  {
    "discussion_id": "2058685837",
    "pr_number": 7508,
    "pr_file": "api/src/backend/api/v1/views.py",
    "created_at": "2025-04-24T15:14:51+00:00",
    "commented_code": "context = super().get_serializer_context()\n         context[\"allowed_providers\"] = self.allowed_providers\n         return context\n+\n+\n+class LighthouseConfigViewSet(BaseRLSViewSet):\n+    \"\"\"\n+    API endpoint for managing OpenAI API configuration.\n+    \"\"\"\n+\n+    filterset_fields = {\n+        \"name\": [\"exact\", \"icontains\"],\n+        \"model\": [\"exact\", \"icontains\"],\n+        \"is_active\": [\"exact\"],\n+        \"inserted_at\": [\"gte\", \"lte\"],\n+    }\n+    ordering_fields = [\"name\", \"inserted_at\", \"updated_at\", \"is_active\"]\n+    ordering = [\"-inserted_at\"]\n+\n+    def get_queryset(self):\n+        return LighthouseConfig.objects.all()",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2058685837",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7508,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2058685837",
        "commented_code": "@@ -2565,3 +2572,126 @@ def get_serializer_context(self):\n         context = super().get_serializer_context()\n         context[\"allowed_providers\"] = self.allowed_providers\n         return context\n+\n+\n+class LighthouseConfigViewSet(BaseRLSViewSet):\n+    \"\"\"\n+    API endpoint for managing OpenAI API configuration.\n+    \"\"\"\n+\n+    filterset_fields = {\n+        \"name\": [\"exact\", \"icontains\"],\n+        \"model\": [\"exact\", \"icontains\"],\n+        \"is_active\": [\"exact\"],\n+        \"inserted_at\": [\"gte\", \"lte\"],\n+    }\n+    ordering_fields = [\"name\", \"inserted_at\", \"updated_at\", \"is_active\"]\n+    ordering = [\"-inserted_at\"]\n+\n+    def get_queryset(self):\n+        return LighthouseConfig.objects.all()",
        "comment_created_at": "2025-04-24T15:14:51+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "```suggestion\r\n        return LighthouseConfig.objects.filter(tenant_id=self.request.tenant_id)\r\n```\r\n\r\nEvery tenant would have its own config, isn't it? This would improve performance.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1913417855",
    "pr_number": 6460,
    "pr_file": "api/src/backend/tasks/jobs/scan.py",
    "created_at": "2025-01-13T15:53:45+00:00",
    "commented_code": "status = FindingStatus[finding.status]\n                     delta = _create_finding_delta(last_status, status)\n+                    first_seen = datetime.now(tz=timezone.utc)\n+                    # When the delta attribute is different from new it means that previously there was a finding with the same uid and delta new, so we will obtain the first_seen attribute of that first finding\n+                    if delta != Finding.DeltaChoices.NEW:\n+                        first_seen = (\n+                            Finding.objects.filter(uid=finding_uid, delta=Finding.DeltaChoices.NEW.value)\n+                            .values(\"first_seen\")\n+                            .first()[\"first_seen\"]\n+                        )",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1913417855",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6460,
        "pr_file": "api/src/backend/tasks/jobs/scan.py",
        "discussion_id": "1913417855",
        "commented_code": "@@ -237,6 +237,14 @@ def perform_prowler_scan(\n \n                     status = FindingStatus[finding.status]\n                     delta = _create_finding_delta(last_status, status)\n+                    first_seen = datetime.now(tz=timezone.utc)\n+                    # When the delta attribute is different from new it means that previously there was a finding with the same uid and delta new, so we will obtain the first_seen attribute of that first finding\n+                    if delta != Finding.DeltaChoices.NEW:\n+                        first_seen = (\n+                            Finding.objects.filter(uid=finding_uid, delta=Finding.DeltaChoices.NEW.value)\n+                            .values(\"first_seen\")\n+                            .first()[\"first_seen\"]\n+                        )",
        "comment_created_at": "2025-01-13T15:53:45+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "Can't we move this logic inside the `_create_finding_delta` (consider renaming it if necessary) to avoid making extra queries when not needed? In that function, we retrieve the latest finding with the same `uid` to get the `delta` value. If that finding's `first_seen` is not `None`, we can just copy it from there without having to query again.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1940978574",
    "pr_number": 6746,
    "pr_file": "api/src/backend/tasks/jobs/scan.py",
    "created_at": "2025-02-04T11:16:40+00:00",
    "commented_code": ")\n                 provider_instance.save()\n \n-        prowler_scan = ProwlerScan(provider=prowler_provider, checks=checks_to_execute)\n-\n-        resource_cache = {}\n-        tag_cache = {}\n-        last_status_cache = {}\n+        # Scan configuration\n+        prowler_scan = ProwlerScan(\n+            provider=prowler_provider, checks=checks_to_execute or []\n+        )\n+        output_directory = _generate_output_directory(\n+            prowler_provider, tenant_id, scan_id\n+        )\n+        # Create the output directory\n+        os.makedirs(\"/\".join(output_directory.split(\"/\")[:-1]), exist_ok=True)\n \n-        for progress, findings in prowler_scan.scan():\n+        all_findings = []\n+        for progress, findings, stats in prowler_scan.scan():\n             for finding in findings:\n                 if finding is None:\n                     logger.error(f\"None finding detected on scan {scan_id}.\")\n                     continue\n                 for attempt in range(CELERY_DEADLOCK_ATTEMPTS):\n                     try:\n-                        with rls_transaction(tenant_id):\n-                            # Process resource\n-                            resource_uid = finding.resource_uid\n-                            if resource_uid not in resource_cache:\n-                                # Get or create the resource\n-                                resource_instance, _ = Resource.objects.get_or_create(\n-                                    tenant_id=tenant_id,\n-                                    provider=provider_instance,\n-                                    uid=resource_uid,\n-                                    defaults={\n-                                        \"region\": finding.region,\n-                                        \"service\": finding.service_name,\n-                                        \"type\": finding.resource_type,\n-                                        \"name\": finding.resource_name,\n-                                    },\n-                                )\n-                                resource_cache[resource_uid] = resource_instance\n-                            else:\n-                                resource_instance = resource_cache[resource_uid]\n-\n-                        # Update resource fields if necessary\n-                        updated_fields = []\n-                        if (\n-                            finding.region\n-                            and resource_instance.region != finding.region\n-                        ):\n-                            resource_instance.region = finding.region\n-                            updated_fields.append(\"region\")\n-                        if resource_instance.service != finding.service_name:\n-                            resource_instance.service = finding.service_name\n-                            updated_fields.append(\"service\")\n-                        if resource_instance.type != finding.resource_type:\n-                            resource_instance.type = finding.resource_type\n-                            updated_fields.append(\"type\")\n-                        if updated_fields:\n-                            with rls_transaction(tenant_id):\n-                                resource_instance.save(update_fields=updated_fields)\n+                        resource_instance, resource_uid_region = _store_resources(\n+                            finding,\n+                            tenant_id,\n+                            provider_instance,\n+                            resource_cache,\n+                            tag_cache,\n+                        )\n+                        unique_resources.add(resource_uid_region)\n+                        break\n                     except (OperationalError, IntegrityError) as db_err:\n                         if attempt < CELERY_DEADLOCK_ATTEMPTS - 1:\n                             logger.warning(\n-                                f\"{'Deadlock error' if isinstance(db_err, OperationalError) else 'Integrity error'} \"\n-                                f\"detected when processing resource {resource_uid} on scan {scan_id}. Retrying...\"\n+                                f\"Database error ({type(db_err).__name__}) \"\n+                                f\"processing resource {finding.resource_uid}, retrying...\"\n                             )\n                             time.sleep(0.1 * (2**attempt))\n-                            continue\n                         else:\n                             raise db_err\n \n-                # Update tags\n-                tags = []\n-                with rls_transaction(tenant_id):\n-                    for key, value in finding.resource_tags.items():\n-                        tag_key = (key, value)\n-                        if tag_key not in tag_cache:\n-                            tag_instance, _ = ResourceTag.objects.get_or_create(\n-                                tenant_id=tenant_id, key=key, value=value\n-                            )\n-                            tag_cache[tag_key] = tag_instance\n-                        else:\n-                            tag_instance = tag_cache[tag_key]\n-                        tags.append(tag_instance)\n-                    resource_instance.upsert_or_delete_tags(tags=tags)\n-\n-                unique_resources.add((resource_instance.uid, resource_instance.region))\n-\n-                # Process finding\n+                # Finding processing\n                 with rls_transaction(tenant_id):\n                     finding_uid = finding.uid\n-                    last_first_seen_at = None\n                     if finding_uid not in last_status_cache:\n-                        most_recent_finding = (\n-                            Finding.all_objects.filter(\n-                                tenant_id=tenant_id, uid=finding_uid\n-                            )\n+                        most_recent = (\n+                            Finding.objects.filter(uid=finding_uid)",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1940978574",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6746,
        "pr_file": "api/src/backend/tasks/jobs/scan.py",
        "discussion_id": "1940978574",
        "commented_code": "@@ -144,116 +310,66 @@ def perform_prowler_scan(\n                 )\n                 provider_instance.save()\n \n-        prowler_scan = ProwlerScan(provider=prowler_provider, checks=checks_to_execute)\n-\n-        resource_cache = {}\n-        tag_cache = {}\n-        last_status_cache = {}\n+        # Scan configuration\n+        prowler_scan = ProwlerScan(\n+            provider=prowler_provider, checks=checks_to_execute or []\n+        )\n+        output_directory = _generate_output_directory(\n+            prowler_provider, tenant_id, scan_id\n+        )\n+        # Create the output directory\n+        os.makedirs(\"/\".join(output_directory.split(\"/\")[:-1]), exist_ok=True)\n \n-        for progress, findings in prowler_scan.scan():\n+        all_findings = []\n+        for progress, findings, stats in prowler_scan.scan():\n             for finding in findings:\n                 if finding is None:\n                     logger.error(f\"None finding detected on scan {scan_id}.\")\n                     continue\n                 for attempt in range(CELERY_DEADLOCK_ATTEMPTS):\n                     try:\n-                        with rls_transaction(tenant_id):\n-                            # Process resource\n-                            resource_uid = finding.resource_uid\n-                            if resource_uid not in resource_cache:\n-                                # Get or create the resource\n-                                resource_instance, _ = Resource.objects.get_or_create(\n-                                    tenant_id=tenant_id,\n-                                    provider=provider_instance,\n-                                    uid=resource_uid,\n-                                    defaults={\n-                                        \"region\": finding.region,\n-                                        \"service\": finding.service_name,\n-                                        \"type\": finding.resource_type,\n-                                        \"name\": finding.resource_name,\n-                                    },\n-                                )\n-                                resource_cache[resource_uid] = resource_instance\n-                            else:\n-                                resource_instance = resource_cache[resource_uid]\n-\n-                        # Update resource fields if necessary\n-                        updated_fields = []\n-                        if (\n-                            finding.region\n-                            and resource_instance.region != finding.region\n-                        ):\n-                            resource_instance.region = finding.region\n-                            updated_fields.append(\"region\")\n-                        if resource_instance.service != finding.service_name:\n-                            resource_instance.service = finding.service_name\n-                            updated_fields.append(\"service\")\n-                        if resource_instance.type != finding.resource_type:\n-                            resource_instance.type = finding.resource_type\n-                            updated_fields.append(\"type\")\n-                        if updated_fields:\n-                            with rls_transaction(tenant_id):\n-                                resource_instance.save(update_fields=updated_fields)\n+                        resource_instance, resource_uid_region = _store_resources(\n+                            finding,\n+                            tenant_id,\n+                            provider_instance,\n+                            resource_cache,\n+                            tag_cache,\n+                        )\n+                        unique_resources.add(resource_uid_region)\n+                        break\n                     except (OperationalError, IntegrityError) as db_err:\n                         if attempt < CELERY_DEADLOCK_ATTEMPTS - 1:\n                             logger.warning(\n-                                f\"{'Deadlock error' if isinstance(db_err, OperationalError) else 'Integrity error'} \"\n-                                f\"detected when processing resource {resource_uid} on scan {scan_id}. Retrying...\"\n+                                f\"Database error ({type(db_err).__name__}) \"\n+                                f\"processing resource {finding.resource_uid}, retrying...\"\n                             )\n                             time.sleep(0.1 * (2**attempt))\n-                            continue\n                         else:\n                             raise db_err\n \n-                # Update tags\n-                tags = []\n-                with rls_transaction(tenant_id):\n-                    for key, value in finding.resource_tags.items():\n-                        tag_key = (key, value)\n-                        if tag_key not in tag_cache:\n-                            tag_instance, _ = ResourceTag.objects.get_or_create(\n-                                tenant_id=tenant_id, key=key, value=value\n-                            )\n-                            tag_cache[tag_key] = tag_instance\n-                        else:\n-                            tag_instance = tag_cache[tag_key]\n-                        tags.append(tag_instance)\n-                    resource_instance.upsert_or_delete_tags(tags=tags)\n-\n-                unique_resources.add((resource_instance.uid, resource_instance.region))\n-\n-                # Process finding\n+                # Finding processing\n                 with rls_transaction(tenant_id):\n                     finding_uid = finding.uid\n-                    last_first_seen_at = None\n                     if finding_uid not in last_status_cache:\n-                        most_recent_finding = (\n-                            Finding.all_objects.filter(\n-                                tenant_id=tenant_id, uid=finding_uid\n-                            )\n+                        most_recent = (\n+                            Finding.objects.filter(uid=finding_uid)",
        "comment_created_at": "2025-02-04T11:16:40+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "That's concerning... why are you removing the `tenant_id` filter? It is a key part of our multi tenant system with RLS, we could not remove it unless there is a strong reason behind.",
        "pr_file_module": null
      }
    ]
  }
]