[
  {
    "discussion_id": "1750190869",
    "pr_number": 4892,
    "pr_file": "prowler/lib/persistence/manager.py",
    "created_at": "2024-09-09T12:43:44+00:00",
    "commented_code": "+import os\n+\n+from .sqlite import SQLiteList, SQLiteDict\n+\n+default_list = SQLiteList\n+\n+\n+def mklist() -> list:\n+    \"\"\"\n+    Create a new list with the given name and data.\n+    \"\"\"\n+    prowler_db_connection = os.environ.get('PROWLER_DB_CONNECTION')",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1750190869",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 4892,
        "pr_file": "prowler/lib/persistence/manager.py",
        "discussion_id": "1750190869",
        "commented_code": "@@ -0,0 +1,52 @@\n+import os\n+\n+from .sqlite import SQLiteList, SQLiteDict\n+\n+default_list = SQLiteList\n+\n+\n+def mklist() -> list:\n+    \"\"\"\n+    Create a new list with the given name and data.\n+    \"\"\"\n+    prowler_db_connection = os.environ.get('PROWLER_DB_CONNECTION')",
        "comment_created_at": "2024-09-09T12:43:44+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "If the default value is `in-memory` we'd need to set that default.\r\n```suggestion\r\n    prowler_db_connection = os.environ.get('PROWLER_DB_CONNECTION', \"memory://\")\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2164315501",
    "pr_number": 8095,
    "pr_file": "api/src/backend/api/v1/views.py",
    "created_at": "2025-06-24T15:23:14+00:00",
    "commented_code": "serializer = TokenSocialLoginSerializer(data={\"email\": user.email})\n         serializer.is_valid(raise_exception=True)\n-        return JsonResponse(\n-            {\n-                \"type\": \"saml-social-tokens\",\n-                \"attributes\": serializer.validated_data,\n-            }\n-        )\n+\n+        token_data = serializer.validated_data\n+        access_token = token_data.get(\"access\")\n+        refresh_token = token_data.get(\"refresh\")\n+        auth_url = os.getenv(\"AUTH_URL\", \"http://localhost:3000\")\n+        callback_url = f\"{auth_url}/api/auth/callback/saml\"",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2164315501",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8095,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2164315501",
        "commented_code": "@@ -560,12 +562,15 @@ def dispatch(self, request, organization_slug):\n \n         serializer = TokenSocialLoginSerializer(data={\"email\": user.email})\n         serializer.is_valid(raise_exception=True)\n-        return JsonResponse(\n-            {\n-                \"type\": \"saml-social-tokens\",\n-                \"attributes\": serializer.validated_data,\n-            }\n-        )\n+\n+        token_data = serializer.validated_data\n+        access_token = token_data.get(\"access\")\n+        refresh_token = token_data.get(\"refresh\")\n+        auth_url = os.getenv(\"AUTH_URL\", \"http://localhost:3000\")\n+        callback_url = f\"{auth_url}/api/auth/callback/saml\"",
        "comment_created_at": "2025-06-24T15:23:14+00:00",
        "comment_author": "vicferpoy",
        "comment_body": "I think this should come from an environment variable that takes `AUTH_URL` as base. But it shouldn't be hardcoded in the backend since this is a URL from the UI.",
        "pr_file_module": null
      },
      {
        "comment_id": "2164614466",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8095,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2164315501",
        "commented_code": "@@ -560,12 +562,15 @@ def dispatch(self, request, organization_slug):\n \n         serializer = TokenSocialLoginSerializer(data={\"email\": user.email})\n         serializer.is_valid(raise_exception=True)\n-        return JsonResponse(\n-            {\n-                \"type\": \"saml-social-tokens\",\n-                \"attributes\": serializer.validated_data,\n-            }\n-        )\n+\n+        token_data = serializer.validated_data\n+        access_token = token_data.get(\"access\")\n+        refresh_token = token_data.get(\"refresh\")\n+        auth_url = os.getenv(\"AUTH_URL\", \"http://localhost:3000\")\n+        callback_url = f\"{auth_url}/api/auth/callback/saml\"",
        "comment_created_at": "2025-06-24T18:09:59+00:00",
        "comment_author": "alejandrobailo",
        "comment_body": "Got it",
        "pr_file_module": null
      },
      {
        "comment_id": "2166090789",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8095,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2164315501",
        "commented_code": "@@ -560,12 +562,15 @@ def dispatch(self, request, organization_slug):\n \n         serializer = TokenSocialLoginSerializer(data={\"email\": user.email})\n         serializer.is_valid(raise_exception=True)\n-        return JsonResponse(\n-            {\n-                \"type\": \"saml-social-tokens\",\n-                \"attributes\": serializer.validated_data,\n-            }\n-        )\n+\n+        token_data = serializer.validated_data\n+        access_token = token_data.get(\"access\")\n+        refresh_token = token_data.get(\"refresh\")\n+        auth_url = os.getenv(\"AUTH_URL\", \"http://localhost:3000\")\n+        callback_url = f\"{auth_url}/api/auth/callback/saml\"",
        "comment_created_at": "2025-06-25T08:14:21+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "If you\u2019re okay with it, I\u2019ll continue working on this PR myself, focusing on the DB changes since we can\u2019t pass the tokens through the URL, that approach isn\u2019t secure. So if it sounds good to you, I\u2019ll take it from here and update the PR accordingly. For now, this PR should have the `no-merge` label",
        "pr_file_module": null
      },
      {
        "comment_id": "2166093144",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 8095,
        "pr_file": "api/src/backend/api/v1/views.py",
        "discussion_id": "2164315501",
        "commented_code": "@@ -560,12 +562,15 @@ def dispatch(self, request, organization_slug):\n \n         serializer = TokenSocialLoginSerializer(data={\"email\": user.email})\n         serializer.is_valid(raise_exception=True)\n-        return JsonResponse(\n-            {\n-                \"type\": \"saml-social-tokens\",\n-                \"attributes\": serializer.validated_data,\n-            }\n-        )\n+\n+        token_data = serializer.validated_data\n+        access_token = token_data.get(\"access\")\n+        refresh_token = token_data.get(\"refresh\")\n+        auth_url = os.getenv(\"AUTH_URL\", \"http://localhost:3000\")\n+        callback_url = f\"{auth_url}/api/auth/callback/saml\"",
        "comment_created_at": "2025-06-25T08:15:29+00:00",
        "comment_author": "alejandrobailo",
        "comment_body": "Perfect!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1954193192",
    "pr_number": 6878,
    "pr_file": "api/src/backend/tasks/tasks.py",
    "created_at": "2025-02-13T09:59:58+00:00",
    "commented_code": "@shared_task(name=\"tenant-deletion\")\n def delete_tenant_task(tenant_id: str):\n     return delete_tenant(pk=tenant_id)\n+\n+\n+def batched(iterable, batch_size):\n+    \"\"\"\n+    Yield successive batches from an iterable.\n+\n+    Args:\n+        iterable: An iterable source of items.\n+        batch_size (int): The number of items per batch.\n+\n+    Yields:\n+        tuple: A pair (batch, is_last_batch) where:\n+            - batch (list): A list of items (with length equal to batch_size,\n+              except possibly for the last batch).\n+            - is_last_batch (bool): True if this is the final batch, False otherwise.\n+    \"\"\"\n+    batch = []\n+    for item in iterable:\n+        batch.append(item)\n+        if len(batch) == batch_size:\n+            yield batch, False\n+            batch = []\n+\n+    yield batch, True\n+\n+\n+@shared_task(base=RLSTask, name=\"scan-output\", queue=\"scans\")\n+@set_tenant(keep_tenant=True)\n+def generate_outputs(scan_id: str, provider_id: str, tenant_id: str):\n+    \"\"\"\n+    Process findings in batches and generate output files in multiple formats.\n+\n+    This function retrieves findings associated with a scan, processes them\n+    in batches of 50, and writes each batch to the corresponding output files.\n+    It reuses output writer instances across batches, updates them with each\n+    batch of transformed findings, and uses a flag to indicate when the final\n+    batch is being processed. Finally, the output files are compressed and\n+    uploaded to S3.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier.\n+        scan_id (str): The scan identifier.\n+        provider_id (str): The provider_id id to be used in generating outputs.\n+    \"\"\"\n+    # Generate and ensure the output directory exists\n+    output_directory = _generate_output_directory(provider_id, tenant_id, scan_id)\n+    os.makedirs(\"/\".join(output_directory.split(\"/\")[:-1]), exist_ok=True)\n+\n+    # Define auxiliary variables\n+    output_writers = {}\n+    scan_summary = FindingOutput._transform_findings_stats(\n+        ScanSummary.objects.filter(scan_id=scan_id)\n+    )\n+\n+    # Retrieve findings queryset\n+    findings_qs = Finding.objects.filter(scan_id=scan_id).order_by(\"uid\")\n+\n+    # Process findings in batches\n+    for batch, is_last_batch in batched(findings_qs.iterator(), 50):",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1954193192",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/tasks/tasks.py",
        "discussion_id": "1954193192",
        "commented_code": "@@ -152,3 +168,116 @@ def perform_scan_summary_task(tenant_id: str, scan_id: str):\n @shared_task(name=\"tenant-deletion\")\n def delete_tenant_task(tenant_id: str):\n     return delete_tenant(pk=tenant_id)\n+\n+\n+def batched(iterable, batch_size):\n+    \"\"\"\n+    Yield successive batches from an iterable.\n+\n+    Args:\n+        iterable: An iterable source of items.\n+        batch_size (int): The number of items per batch.\n+\n+    Yields:\n+        tuple: A pair (batch, is_last_batch) where:\n+            - batch (list): A list of items (with length equal to batch_size,\n+              except possibly for the last batch).\n+            - is_last_batch (bool): True if this is the final batch, False otherwise.\n+    \"\"\"\n+    batch = []\n+    for item in iterable:\n+        batch.append(item)\n+        if len(batch) == batch_size:\n+            yield batch, False\n+            batch = []\n+\n+    yield batch, True\n+\n+\n+@shared_task(base=RLSTask, name=\"scan-output\", queue=\"scans\")\n+@set_tenant(keep_tenant=True)\n+def generate_outputs(scan_id: str, provider_id: str, tenant_id: str):\n+    \"\"\"\n+    Process findings in batches and generate output files in multiple formats.\n+\n+    This function retrieves findings associated with a scan, processes them\n+    in batches of 50, and writes each batch to the corresponding output files.\n+    It reuses output writer instances across batches, updates them with each\n+    batch of transformed findings, and uses a flag to indicate when the final\n+    batch is being processed. Finally, the output files are compressed and\n+    uploaded to S3.\n+\n+    Args:\n+        tenant_id (str): The tenant identifier.\n+        scan_id (str): The scan identifier.\n+        provider_id (str): The provider_id id to be used in generating outputs.\n+    \"\"\"\n+    # Generate and ensure the output directory exists\n+    output_directory = _generate_output_directory(provider_id, tenant_id, scan_id)\n+    os.makedirs(\"/\".join(output_directory.split(\"/\")[:-1]), exist_ok=True)\n+\n+    # Define auxiliary variables\n+    output_writers = {}\n+    scan_summary = FindingOutput._transform_findings_stats(\n+        ScanSummary.objects.filter(scan_id=scan_id)\n+    )\n+\n+    # Retrieve findings queryset\n+    findings_qs = Finding.objects.filter(scan_id=scan_id).order_by(\"uid\")\n+\n+    # Process findings in batches\n+    for batch, is_last_batch in batched(findings_qs.iterator(), 50):",
        "comment_created_at": "2025-02-13T09:59:58+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Can we add an environment variable for the batch size?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1955953211",
    "pr_number": 6878,
    "pr_file": "api/src/backend/config/django/base.py",
    "created_at": "2025-02-14T10:54:45+00:00",
    "commented_code": "TESTING = False\n FINDINGS_MAX_DAYS_IN_RANGE = env.int(\"DJANGO_FINDINGS_MAX_DAYS_IN_RANGE\", 7)\n+\n+\n+# API export settings\n+TMP_OUTPUT_DIRECTORY = \"/tmp/prowler_api_output\"\n+FINDINGS_BATCH_SIZE = 1000",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1955953211",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/config/django/base.py",
        "discussion_id": "1955953211",
        "commented_code": "@@ -208,3 +208,14 @@\n \n TESTING = False\n FINDINGS_MAX_DAYS_IN_RANGE = env.int(\"DJANGO_FINDINGS_MAX_DAYS_IN_RANGE\", 7)\n+\n+\n+# API export settings\n+TMP_OUTPUT_DIRECTORY = \"/tmp/prowler_api_output\"\n+FINDINGS_BATCH_SIZE = 1000",
        "comment_created_at": "2025-02-14T10:54:45+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "What about `env.int(\"DJANGO_FINDINGS_BATCH_SIZE\", 1000)`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1967286584",
    "pr_number": 6878,
    "pr_file": "api/src/backend/tasks/jobs/export.py",
    "created_at": "2025-02-24T09:37:03+00:00",
    "commented_code": "+import os\n+import zipfile\n+\n+import boto3\n+import config.django.base as base\n+from botocore.exceptions import ClientError, NoCredentialsError, ParamValidationError\n+from celery.utils.log import get_task_logger\n+from config.env import env\n+\n+from prowler.config.config import (\n+    csv_file_suffix,\n+    html_file_suffix,\n+    json_ocsf_file_suffix,\n+    output_file_timestamp,\n+)\n+from prowler.lib.outputs.csv.csv import CSV\n+from prowler.lib.outputs.html.html import HTML\n+from prowler.lib.outputs.ocsf.ocsf import OCSF\n+\n+logger = get_task_logger(__name__)\n+\n+\n+# Predefined mapping for output formats and their configurations\n+OUTPUT_FORMATS_MAPPING = {\n+    \"csv\": {\n+        \"class\": CSV,\n+        \"suffix\": csv_file_suffix,\n+        \"kwargs\": {},\n+    },\n+    \"json-ocsf\": {\"class\": OCSF, \"suffix\": json_ocsf_file_suffix, \"kwargs\": {}},\n+    \"html\": {\"class\": HTML, \"suffix\": html_file_suffix, \"kwargs\": {\"stats\": {}}},\n+}\n+\n+\n+def _compress_output_files(output_directory: str) -> str:\n+    \"\"\"\n+    Compress output files from all configured output formats into a ZIP archive.\n+    Args:\n+        output_directory (str): The directory where the output files are located.\n+            The function looks up all known suffixes in OUTPUT_FORMATS_MAPPING\n+            and compresses those files into a single ZIP.\n+    Returns:\n+        str: The full path to the newly created ZIP archive.\n+    \"\"\"\n+    zip_path = f\"{output_directory}.zip\"\n+\n+    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n+        for suffix in [config[\"suffix\"] for config in OUTPUT_FORMATS_MAPPING.values()]:\n+            zipf.write(\n+                f\"{output_directory}{suffix}\",\n+                f\"output/{output_directory.split('/')[-1]}{suffix}\",\n+            )\n+\n+    return zip_path\n+\n+\n+def get_s3_client():\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from environment variables.\n+\n+    This function attempts to initialize an S3 client by reading the AWS access key, secret key,\n+    session token, and region from environment variables. It then validates the client by listing\n+    available S3 buckets. If an error occurs during this process (for example, due to missing or\n+    invalid credentials), it falls back to creating an S3 client without explicitly provided credentials,\n+    which may rely on other configuration sources (e.g., IAM roles).\n+\n+    Returns:\n+        boto3.client: A configured S3 client instance.\n+\n+    Raises:\n+        ClientError, NoCredentialsError, or ParamValidationError if both attempts to create a client fail.\n+    \"\"\"\n+    s3_client = None\n+    try:\n+        s3_client = boto3.client(\n+            \"s3\",\n+            aws_access_key_id=env.str(\"DJANGO_OUTPUT_AWS_ACCESS_KEY_ID\"),\n+            aws_secret_access_key=env.str(\"DJANGO_OUTPUT_AWS_SECRET_ACCESS_KEY\"),\n+            aws_session_token=env.str(\"DJANGO_OUTPUT_AWS_SESSION_TOKEN\"),\n+            region_name=env.str(\"DJANGO_OUTPUT_AWS_DEFAULT_REGION\"),",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1967286584",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/tasks/jobs/export.py",
        "discussion_id": "1967286584",
        "commented_code": "@@ -0,0 +1,156 @@\n+import os\n+import zipfile\n+\n+import boto3\n+import config.django.base as base\n+from botocore.exceptions import ClientError, NoCredentialsError, ParamValidationError\n+from celery.utils.log import get_task_logger\n+from config.env import env\n+\n+from prowler.config.config import (\n+    csv_file_suffix,\n+    html_file_suffix,\n+    json_ocsf_file_suffix,\n+    output_file_timestamp,\n+)\n+from prowler.lib.outputs.csv.csv import CSV\n+from prowler.lib.outputs.html.html import HTML\n+from prowler.lib.outputs.ocsf.ocsf import OCSF\n+\n+logger = get_task_logger(__name__)\n+\n+\n+# Predefined mapping for output formats and their configurations\n+OUTPUT_FORMATS_MAPPING = {\n+    \"csv\": {\n+        \"class\": CSV,\n+        \"suffix\": csv_file_suffix,\n+        \"kwargs\": {},\n+    },\n+    \"json-ocsf\": {\"class\": OCSF, \"suffix\": json_ocsf_file_suffix, \"kwargs\": {}},\n+    \"html\": {\"class\": HTML, \"suffix\": html_file_suffix, \"kwargs\": {\"stats\": {}}},\n+}\n+\n+\n+def _compress_output_files(output_directory: str) -> str:\n+    \"\"\"\n+    Compress output files from all configured output formats into a ZIP archive.\n+    Args:\n+        output_directory (str): The directory where the output files are located.\n+            The function looks up all known suffixes in OUTPUT_FORMATS_MAPPING\n+            and compresses those files into a single ZIP.\n+    Returns:\n+        str: The full path to the newly created ZIP archive.\n+    \"\"\"\n+    zip_path = f\"{output_directory}.zip\"\n+\n+    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n+        for suffix in [config[\"suffix\"] for config in OUTPUT_FORMATS_MAPPING.values()]:\n+            zipf.write(\n+                f\"{output_directory}{suffix}\",\n+                f\"output/{output_directory.split('/')[-1]}{suffix}\",\n+            )\n+\n+    return zip_path\n+\n+\n+def get_s3_client():\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from environment variables.\n+\n+    This function attempts to initialize an S3 client by reading the AWS access key, secret key,\n+    session token, and region from environment variables. It then validates the client by listing\n+    available S3 buckets. If an error occurs during this process (for example, due to missing or\n+    invalid credentials), it falls back to creating an S3 client without explicitly provided credentials,\n+    which may rely on other configuration sources (e.g., IAM roles).\n+\n+    Returns:\n+        boto3.client: A configured S3 client instance.\n+\n+    Raises:\n+        ClientError, NoCredentialsError, or ParamValidationError if both attempts to create a client fail.\n+    \"\"\"\n+    s3_client = None\n+    try:\n+        s3_client = boto3.client(\n+            \"s3\",\n+            aws_access_key_id=env.str(\"DJANGO_OUTPUT_AWS_ACCESS_KEY_ID\"),\n+            aws_secret_access_key=env.str(\"DJANGO_OUTPUT_AWS_SECRET_ACCESS_KEY\"),\n+            aws_session_token=env.str(\"DJANGO_OUTPUT_AWS_SESSION_TOKEN\"),\n+            region_name=env.str(\"DJANGO_OUTPUT_AWS_DEFAULT_REGION\"),",
        "comment_created_at": "2025-02-24T09:37:03+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Not having `DJANGO_OUTPUT_AWS_DEFAULT_REGION` makes the endpoint to raise a `HTTP 500 Internal Server Error`. I think we should handle that too.",
        "pr_file_module": null
      },
      {
        "comment_id": "1969814436",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/tasks/jobs/export.py",
        "discussion_id": "1967286584",
        "commented_code": "@@ -0,0 +1,156 @@\n+import os\n+import zipfile\n+\n+import boto3\n+import config.django.base as base\n+from botocore.exceptions import ClientError, NoCredentialsError, ParamValidationError\n+from celery.utils.log import get_task_logger\n+from config.env import env\n+\n+from prowler.config.config import (\n+    csv_file_suffix,\n+    html_file_suffix,\n+    json_ocsf_file_suffix,\n+    output_file_timestamp,\n+)\n+from prowler.lib.outputs.csv.csv import CSV\n+from prowler.lib.outputs.html.html import HTML\n+from prowler.lib.outputs.ocsf.ocsf import OCSF\n+\n+logger = get_task_logger(__name__)\n+\n+\n+# Predefined mapping for output formats and their configurations\n+OUTPUT_FORMATS_MAPPING = {\n+    \"csv\": {\n+        \"class\": CSV,\n+        \"suffix\": csv_file_suffix,\n+        \"kwargs\": {},\n+    },\n+    \"json-ocsf\": {\"class\": OCSF, \"suffix\": json_ocsf_file_suffix, \"kwargs\": {}},\n+    \"html\": {\"class\": HTML, \"suffix\": html_file_suffix, \"kwargs\": {\"stats\": {}}},\n+}\n+\n+\n+def _compress_output_files(output_directory: str) -> str:\n+    \"\"\"\n+    Compress output files from all configured output formats into a ZIP archive.\n+    Args:\n+        output_directory (str): The directory where the output files are located.\n+            The function looks up all known suffixes in OUTPUT_FORMATS_MAPPING\n+            and compresses those files into a single ZIP.\n+    Returns:\n+        str: The full path to the newly created ZIP archive.\n+    \"\"\"\n+    zip_path = f\"{output_directory}.zip\"\n+\n+    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n+        for suffix in [config[\"suffix\"] for config in OUTPUT_FORMATS_MAPPING.values()]:\n+            zipf.write(\n+                f\"{output_directory}{suffix}\",\n+                f\"output/{output_directory.split('/')[-1]}{suffix}\",\n+            )\n+\n+    return zip_path\n+\n+\n+def get_s3_client():\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from environment variables.\n+\n+    This function attempts to initialize an S3 client by reading the AWS access key, secret key,\n+    session token, and region from environment variables. It then validates the client by listing\n+    available S3 buckets. If an error occurs during this process (for example, due to missing or\n+    invalid credentials), it falls back to creating an S3 client without explicitly provided credentials,\n+    which may rely on other configuration sources (e.g., IAM roles).\n+\n+    Returns:\n+        boto3.client: A configured S3 client instance.\n+\n+    Raises:\n+        ClientError, NoCredentialsError, or ParamValidationError if both attempts to create a client fail.\n+    \"\"\"\n+    s3_client = None\n+    try:\n+        s3_client = boto3.client(\n+            \"s3\",\n+            aws_access_key_id=env.str(\"DJANGO_OUTPUT_AWS_ACCESS_KEY_ID\"),\n+            aws_secret_access_key=env.str(\"DJANGO_OUTPUT_AWS_SECRET_ACCESS_KEY\"),\n+            aws_session_token=env.str(\"DJANGO_OUTPUT_AWS_SESSION_TOKEN\"),\n+            region_name=env.str(\"DJANGO_OUTPUT_AWS_DEFAULT_REGION\"),",
        "comment_created_at": "2025-02-25T13:45:36+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Fixed and tested ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1971094580",
    "pr_number": 6878,
    "pr_file": "api/src/backend/tasks/jobs/export.py",
    "created_at": "2025-02-26T07:49:27+00:00",
    "commented_code": "+import os\n+import zipfile\n+\n+import boto3\n+import config.django.base as base\n+from botocore.exceptions import ClientError, NoCredentialsError, ParamValidationError\n+from celery.utils.log import get_task_logger\n+from django.conf import settings\n+\n+from prowler.config.config import (\n+    csv_file_suffix,\n+    html_file_suffix,\n+    json_ocsf_file_suffix,\n+    output_file_timestamp,\n+)\n+from prowler.lib.outputs.csv.csv import CSV\n+from prowler.lib.outputs.html.html import HTML\n+from prowler.lib.outputs.ocsf.ocsf import OCSF\n+\n+logger = get_task_logger(__name__)\n+\n+\n+# Predefined mapping for output formats and their configurations\n+OUTPUT_FORMATS_MAPPING = {\n+    \"csv\": {\n+        \"class\": CSV,\n+        \"suffix\": csv_file_suffix,\n+        \"kwargs\": {},\n+    },\n+    \"json-ocsf\": {\"class\": OCSF, \"suffix\": json_ocsf_file_suffix, \"kwargs\": {}},\n+    \"html\": {\"class\": HTML, \"suffix\": html_file_suffix, \"kwargs\": {\"stats\": {}}},\n+}\n+\n+\n+def _compress_output_files(output_directory: str) -> str:\n+    \"\"\"\n+    Compress output files from all configured output formats into a ZIP archive.\n+    Args:\n+        output_directory (str): The directory where the output files are located.\n+            The function looks up all known suffixes in OUTPUT_FORMATS_MAPPING\n+            and compresses those files into a single ZIP.\n+    Returns:\n+        str: The full path to the newly created ZIP archive.\n+    \"\"\"\n+    zip_path = f\"{output_directory}.zip\"\n+\n+    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n+        for suffix in [config[\"suffix\"] for config in OUTPUT_FORMATS_MAPPING.values()]:\n+            zipf.write(\n+                f\"{output_directory}{suffix}\",\n+                f\"output/{output_directory.split('/')[-1]}{suffix}\",\n+            )\n+\n+    return zip_path\n+\n+\n+def get_s3_client():\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from environment variables.\n+\n+    This function attempts to initialize an S3 client by reading the AWS access key, secret key,\n+    session token, and region from environment variables. It then validates the client by listing\n+    available S3 buckets. If an error occurs during this process (for example, due to missing or\n+    invalid credentials), it falls back to creating an S3 client without explicitly provided credentials,\n+    which may rely on other configuration sources (e.g., IAM roles).\n+\n+    Returns:\n+        boto3.client: A configured S3 client instance.\n+\n+    Raises:\n+        ClientError, NoCredentialsError, or ParamValidationError if both attempts to create a client fail.\n+    \"\"\"\n+    s3_client = None\n+    try:\n+        s3_client = boto3.client(\n+            \"s3\",\n+            aws_access_key_id=settings.DJANGO_OUTPUT_S3_AWS_ACCESS_KEY_ID,\n+            aws_secret_access_key=settings.DJANGO_OUTPUT_S3_AWS_SECRET_ACCESS_KEY,\n+            aws_session_token=settings.DJANGO_OUTPUT_S3_AWS_SESSION_TOKEN,\n+            region_name=settings.DJANGO_OUTPUT_S3_AWS_DEFAULT_REGION,\n+        )\n+        s3_client.list_buckets()\n+    except (ClientError, NoCredentialsError, ParamValidationError, ValueError):\n+        s3_client = boto3.client(\"s3\")",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1971094580",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/tasks/jobs/export.py",
        "discussion_id": "1971094580",
        "commented_code": "@@ -0,0 +1,157 @@\n+import os\n+import zipfile\n+\n+import boto3\n+import config.django.base as base\n+from botocore.exceptions import ClientError, NoCredentialsError, ParamValidationError\n+from celery.utils.log import get_task_logger\n+from django.conf import settings\n+\n+from prowler.config.config import (\n+    csv_file_suffix,\n+    html_file_suffix,\n+    json_ocsf_file_suffix,\n+    output_file_timestamp,\n+)\n+from prowler.lib.outputs.csv.csv import CSV\n+from prowler.lib.outputs.html.html import HTML\n+from prowler.lib.outputs.ocsf.ocsf import OCSF\n+\n+logger = get_task_logger(__name__)\n+\n+\n+# Predefined mapping for output formats and their configurations\n+OUTPUT_FORMATS_MAPPING = {\n+    \"csv\": {\n+        \"class\": CSV,\n+        \"suffix\": csv_file_suffix,\n+        \"kwargs\": {},\n+    },\n+    \"json-ocsf\": {\"class\": OCSF, \"suffix\": json_ocsf_file_suffix, \"kwargs\": {}},\n+    \"html\": {\"class\": HTML, \"suffix\": html_file_suffix, \"kwargs\": {\"stats\": {}}},\n+}\n+\n+\n+def _compress_output_files(output_directory: str) -> str:\n+    \"\"\"\n+    Compress output files from all configured output formats into a ZIP archive.\n+    Args:\n+        output_directory (str): The directory where the output files are located.\n+            The function looks up all known suffixes in OUTPUT_FORMATS_MAPPING\n+            and compresses those files into a single ZIP.\n+    Returns:\n+        str: The full path to the newly created ZIP archive.\n+    \"\"\"\n+    zip_path = f\"{output_directory}.zip\"\n+\n+    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n+        for suffix in [config[\"suffix\"] for config in OUTPUT_FORMATS_MAPPING.values()]:\n+            zipf.write(\n+                f\"{output_directory}{suffix}\",\n+                f\"output/{output_directory.split('/')[-1]}{suffix}\",\n+            )\n+\n+    return zip_path\n+\n+\n+def get_s3_client():\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from environment variables.\n+\n+    This function attempts to initialize an S3 client by reading the AWS access key, secret key,\n+    session token, and region from environment variables. It then validates the client by listing\n+    available S3 buckets. If an error occurs during this process (for example, due to missing or\n+    invalid credentials), it falls back to creating an S3 client without explicitly provided credentials,\n+    which may rely on other configuration sources (e.g., IAM roles).\n+\n+    Returns:\n+        boto3.client: A configured S3 client instance.\n+\n+    Raises:\n+        ClientError, NoCredentialsError, or ParamValidationError if both attempts to create a client fail.\n+    \"\"\"\n+    s3_client = None\n+    try:\n+        s3_client = boto3.client(\n+            \"s3\",\n+            aws_access_key_id=settings.DJANGO_OUTPUT_S3_AWS_ACCESS_KEY_ID,\n+            aws_secret_access_key=settings.DJANGO_OUTPUT_S3_AWS_SECRET_ACCESS_KEY,\n+            aws_session_token=settings.DJANGO_OUTPUT_S3_AWS_SESSION_TOKEN,\n+            region_name=settings.DJANGO_OUTPUT_S3_AWS_DEFAULT_REGION,\n+        )\n+        s3_client.list_buckets()\n+    except (ClientError, NoCredentialsError, ParamValidationError, ValueError):\n+        s3_client = boto3.client(\"s3\")",
        "comment_created_at": "2025-02-26T07:49:27+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "If credentials are not configured this is raising an exception, could you please check it when you get a chance?\r\n\r\nWhen this happens the call to _upload_to_s3 within generate_outputs raises an exception and the output location is not stored nor locally.",
        "pr_file_module": null
      },
      {
        "comment_id": "1971129036",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/tasks/jobs/export.py",
        "discussion_id": "1971094580",
        "commented_code": "@@ -0,0 +1,157 @@\n+import os\n+import zipfile\n+\n+import boto3\n+import config.django.base as base\n+from botocore.exceptions import ClientError, NoCredentialsError, ParamValidationError\n+from celery.utils.log import get_task_logger\n+from django.conf import settings\n+\n+from prowler.config.config import (\n+    csv_file_suffix,\n+    html_file_suffix,\n+    json_ocsf_file_suffix,\n+    output_file_timestamp,\n+)\n+from prowler.lib.outputs.csv.csv import CSV\n+from prowler.lib.outputs.html.html import HTML\n+from prowler.lib.outputs.ocsf.ocsf import OCSF\n+\n+logger = get_task_logger(__name__)\n+\n+\n+# Predefined mapping for output formats and their configurations\n+OUTPUT_FORMATS_MAPPING = {\n+    \"csv\": {\n+        \"class\": CSV,\n+        \"suffix\": csv_file_suffix,\n+        \"kwargs\": {},\n+    },\n+    \"json-ocsf\": {\"class\": OCSF, \"suffix\": json_ocsf_file_suffix, \"kwargs\": {}},\n+    \"html\": {\"class\": HTML, \"suffix\": html_file_suffix, \"kwargs\": {\"stats\": {}}},\n+}\n+\n+\n+def _compress_output_files(output_directory: str) -> str:\n+    \"\"\"\n+    Compress output files from all configured output formats into a ZIP archive.\n+    Args:\n+        output_directory (str): The directory where the output files are located.\n+            The function looks up all known suffixes in OUTPUT_FORMATS_MAPPING\n+            and compresses those files into a single ZIP.\n+    Returns:\n+        str: The full path to the newly created ZIP archive.\n+    \"\"\"\n+    zip_path = f\"{output_directory}.zip\"\n+\n+    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n+        for suffix in [config[\"suffix\"] for config in OUTPUT_FORMATS_MAPPING.values()]:\n+            zipf.write(\n+                f\"{output_directory}{suffix}\",\n+                f\"output/{output_directory.split('/')[-1]}{suffix}\",\n+            )\n+\n+    return zip_path\n+\n+\n+def get_s3_client():\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from environment variables.\n+\n+    This function attempts to initialize an S3 client by reading the AWS access key, secret key,\n+    session token, and region from environment variables. It then validates the client by listing\n+    available S3 buckets. If an error occurs during this process (for example, due to missing or\n+    invalid credentials), it falls back to creating an S3 client without explicitly provided credentials,\n+    which may rely on other configuration sources (e.g., IAM roles).\n+\n+    Returns:\n+        boto3.client: A configured S3 client instance.\n+\n+    Raises:\n+        ClientError, NoCredentialsError, or ParamValidationError if both attempts to create a client fail.\n+    \"\"\"\n+    s3_client = None\n+    try:\n+        s3_client = boto3.client(\n+            \"s3\",\n+            aws_access_key_id=settings.DJANGO_OUTPUT_S3_AWS_ACCESS_KEY_ID,\n+            aws_secret_access_key=settings.DJANGO_OUTPUT_S3_AWS_SECRET_ACCESS_KEY,\n+            aws_session_token=settings.DJANGO_OUTPUT_S3_AWS_SESSION_TOKEN,\n+            region_name=settings.DJANGO_OUTPUT_S3_AWS_DEFAULT_REGION,\n+        )\n+        s3_client.list_buckets()\n+    except (ClientError, NoCredentialsError, ParamValidationError, ValueError):\n+        s3_client = boto3.client(\"s3\")",
        "comment_created_at": "2025-02-26T08:16:53+00:00",
        "comment_author": "AdriiiPRodri",
        "comment_body": "`get_s3_client is` only invoked if you have configured `DJANGO_OUTPUT_S3_AWS_OUTPUT_BUCKET`, which indicates that you want the outputs to be uploaded to S3, then it gives an exception because if a user only configures this it is understood that he is running Prowler in AWS (it is not necessary to set the credentials) or he has made a mistake because he has not set the other variables if he wants access through credentials",
        "pr_file_module": null
      },
      {
        "comment_id": "1971143270",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 6878,
        "pr_file": "api/src/backend/tasks/jobs/export.py",
        "discussion_id": "1971094580",
        "commented_code": "@@ -0,0 +1,157 @@\n+import os\n+import zipfile\n+\n+import boto3\n+import config.django.base as base\n+from botocore.exceptions import ClientError, NoCredentialsError, ParamValidationError\n+from celery.utils.log import get_task_logger\n+from django.conf import settings\n+\n+from prowler.config.config import (\n+    csv_file_suffix,\n+    html_file_suffix,\n+    json_ocsf_file_suffix,\n+    output_file_timestamp,\n+)\n+from prowler.lib.outputs.csv.csv import CSV\n+from prowler.lib.outputs.html.html import HTML\n+from prowler.lib.outputs.ocsf.ocsf import OCSF\n+\n+logger = get_task_logger(__name__)\n+\n+\n+# Predefined mapping for output formats and their configurations\n+OUTPUT_FORMATS_MAPPING = {\n+    \"csv\": {\n+        \"class\": CSV,\n+        \"suffix\": csv_file_suffix,\n+        \"kwargs\": {},\n+    },\n+    \"json-ocsf\": {\"class\": OCSF, \"suffix\": json_ocsf_file_suffix, \"kwargs\": {}},\n+    \"html\": {\"class\": HTML, \"suffix\": html_file_suffix, \"kwargs\": {\"stats\": {}}},\n+}\n+\n+\n+def _compress_output_files(output_directory: str) -> str:\n+    \"\"\"\n+    Compress output files from all configured output formats into a ZIP archive.\n+    Args:\n+        output_directory (str): The directory where the output files are located.\n+            The function looks up all known suffixes in OUTPUT_FORMATS_MAPPING\n+            and compresses those files into a single ZIP.\n+    Returns:\n+        str: The full path to the newly created ZIP archive.\n+    \"\"\"\n+    zip_path = f\"{output_directory}.zip\"\n+\n+    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n+        for suffix in [config[\"suffix\"] for config in OUTPUT_FORMATS_MAPPING.values()]:\n+            zipf.write(\n+                f\"{output_directory}{suffix}\",\n+                f\"output/{output_directory.split('/')[-1]}{suffix}\",\n+            )\n+\n+    return zip_path\n+\n+\n+def get_s3_client():\n+    \"\"\"\n+    Create and return a boto3 S3 client using AWS credentials from environment variables.\n+\n+    This function attempts to initialize an S3 client by reading the AWS access key, secret key,\n+    session token, and region from environment variables. It then validates the client by listing\n+    available S3 buckets. If an error occurs during this process (for example, due to missing or\n+    invalid credentials), it falls back to creating an S3 client without explicitly provided credentials,\n+    which may rely on other configuration sources (e.g., IAM roles).\n+\n+    Returns:\n+        boto3.client: A configured S3 client instance.\n+\n+    Raises:\n+        ClientError, NoCredentialsError, or ParamValidationError if both attempts to create a client fail.\n+    \"\"\"\n+    s3_client = None\n+    try:\n+        s3_client = boto3.client(\n+            \"s3\",\n+            aws_access_key_id=settings.DJANGO_OUTPUT_S3_AWS_ACCESS_KEY_ID,\n+            aws_secret_access_key=settings.DJANGO_OUTPUT_S3_AWS_SECRET_ACCESS_KEY,\n+            aws_session_token=settings.DJANGO_OUTPUT_S3_AWS_SESSION_TOKEN,\n+            region_name=settings.DJANGO_OUTPUT_S3_AWS_DEFAULT_REGION,\n+        )\n+        s3_client.list_buckets()\n+    except (ClientError, NoCredentialsError, ParamValidationError, ValueError):\n+        s3_client = boto3.client(\"s3\")",
        "comment_created_at": "2025-02-26T08:27:34+00:00",
        "comment_author": "jfagoagas",
        "comment_body": "Good point, thanks for pointing that out. The only issue I see with that is that if something fails there is no fallback to the local storage.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2104114573",
    "pr_number": 7786,
    "pr_file": "prowler/providers/github/services/repository/repository_inactive_not_archived/repository_inactive_not_archived.py",
    "created_at": "2025-05-23T08:39:27+00:00",
    "commented_code": "+from datetime import datetime, timezone\n+from typing import List\n+\n+from prowler.lib.check.models import Check, CheckReportGithub\n+from prowler.providers.github.services.repository.repository_client import (\n+    repository_client,\n+)\n+\n+\n+class repository_inactive_not_archived(Check):\n+    \"\"\"Check if unarchived repositories have been inactive for more than 6 months.\"\"\"\n+\n+    def execute(self) -> List[CheckReportGithub]:\n+        findings = []\n+\n+        now = datetime.now(timezone.utc)\n+\n+        for repo in repository_client.repositories.values():\n+            report = CheckReportGithub(\n+                metadata=self.metadata(), resource=repo, repository=repo.name\n+            )\n+\n+            if repo.archived:\n+                report.status = \"PASS\"\n+                report.status_extended = f\"Repository {repo.name} is properly archived.\"\n+                findings.append(report)\n+                continue\n+\n+            latest_activity = repo.pushed_at\n+            months_inactive = (\n+                now - latest_activity\n+            ).days / 30.44  # Average days per month\n+\n+            if months_inactive >= 6:",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "2104114573",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 7786,
        "pr_file": "prowler/providers/github/services/repository/repository_inactive_not_archived/repository_inactive_not_archived.py",
        "discussion_id": "2104114573",
        "commented_code": "@@ -0,0 +1,45 @@\n+from datetime import datetime, timezone\n+from typing import List\n+\n+from prowler.lib.check.models import Check, CheckReportGithub\n+from prowler.providers.github.services.repository.repository_client import (\n+    repository_client,\n+)\n+\n+\n+class repository_inactive_not_archived(Check):\n+    \"\"\"Check if unarchived repositories have been inactive for more than 6 months.\"\"\"\n+\n+    def execute(self) -> List[CheckReportGithub]:\n+        findings = []\n+\n+        now = datetime.now(timezone.utc)\n+\n+        for repo in repository_client.repositories.values():\n+            report = CheckReportGithub(\n+                metadata=self.metadata(), resource=repo, repository=repo.name\n+            )\n+\n+            if repo.archived:\n+                report.status = \"PASS\"\n+                report.status_extended = f\"Repository {repo.name} is properly archived.\"\n+                findings.append(report)\n+                continue\n+\n+            latest_activity = repo.pushed_at\n+            months_inactive = (\n+                now - latest_activity\n+            ).days / 30.44  # Average days per month\n+\n+            if months_inactive >= 6:",
        "comment_created_at": "2025-05-23T08:39:27+00:00",
        "comment_author": "MrCloudSec",
        "comment_body": "Can we make this configurable in days?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1843515964",
    "pr_number": 5745,
    "pr_file": "prowler/providers/azure/services/sqlserver/sqlserver_minimal_tls_version/sqlserver_minimal_tls_version.py",
    "created_at": "2024-11-15T09:55:56+00:00",
    "commented_code": "+from prowler.lib.check.models import Check, Check_Report_Azure\n+from prowler.providers.azure.services.sqlserver.sqlserver_client import sqlserver_client\n+\n+\n+class sqlserver_minimal_tls_version(Check):\n+    def execute(self) -> Check_Report_Azure:\n+        findings = []\n+        for subscription, sql_servers in sqlserver_client.sql_servers.items():\n+            for sql_server in sql_servers:\n+                report = Check_Report_Azure(self.metadata())\n+                report.subscription = subscription\n+                report.resource_name = sql_server.name\n+                report.resource_id = sql_server.id\n+                report.status = \"FAIL\"\n+                report.location = sql_server.location\n+                report.status_extended = f\"SQL Server {sql_server.name} from subscription {subscription} has no or an deprecated minimal TLS version set.\"\n+                if sql_server.minimal_tls_version in (\"1.2\", \"1.3\"):",
    "repo_full_name": "prowler-cloud/prowler",
    "discussion_comments": [
      {
        "comment_id": "1843515964",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 5745,
        "pr_file": "prowler/providers/azure/services/sqlserver/sqlserver_minimal_tls_version/sqlserver_minimal_tls_version.py",
        "discussion_id": "1843515964",
        "commented_code": "@@ -0,0 +1,22 @@\n+from prowler.lib.check.models import Check, Check_Report_Azure\n+from prowler.providers.azure.services.sqlserver.sqlserver_client import sqlserver_client\n+\n+\n+class sqlserver_minimal_tls_version(Check):\n+    def execute(self) -> Check_Report_Azure:\n+        findings = []\n+        for subscription, sql_servers in sqlserver_client.sql_servers.items():\n+            for sql_server in sql_servers:\n+                report = Check_Report_Azure(self.metadata())\n+                report.subscription = subscription\n+                report.resource_name = sql_server.name\n+                report.resource_id = sql_server.id\n+                report.status = \"FAIL\"\n+                report.location = sql_server.location\n+                report.status_extended = f\"SQL Server {sql_server.name} from subscription {subscription} has no or an deprecated minimal TLS version set.\"\n+                if sql_server.minimal_tls_version in (\"1.2\", \"1.3\"):",
        "comment_created_at": "2024-11-15T09:55:56+00:00",
        "comment_author": "puchy22",
        "comment_body": "Could you make this check configurable? So if in the future version 1.2 is not recommended, you only have to change the configuration file and not the logic of the check. Here are examples of other checks that are configurable: https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/configuration_file/#azure",
        "pr_file_module": null
      },
      {
        "comment_id": "1843940344",
        "repo_full_name": "prowler-cloud/prowler",
        "pr_number": 5745,
        "pr_file": "prowler/providers/azure/services/sqlserver/sqlserver_minimal_tls_version/sqlserver_minimal_tls_version.py",
        "discussion_id": "1843515964",
        "commented_code": "@@ -0,0 +1,22 @@\n+from prowler.lib.check.models import Check, Check_Report_Azure\n+from prowler.providers.azure.services.sqlserver.sqlserver_client import sqlserver_client\n+\n+\n+class sqlserver_minimal_tls_version(Check):\n+    def execute(self) -> Check_Report_Azure:\n+        findings = []\n+        for subscription, sql_servers in sqlserver_client.sql_servers.items():\n+            for sql_server in sql_servers:\n+                report = Check_Report_Azure(self.metadata())\n+                report.subscription = subscription\n+                report.resource_name = sql_server.name\n+                report.resource_id = sql_server.id\n+                report.status = \"FAIL\"\n+                report.location = sql_server.location\n+                report.status_extended = f\"SQL Server {sql_server.name} from subscription {subscription} has no or an deprecated minimal TLS version set.\"\n+                if sql_server.minimal_tls_version in (\"1.2\", \"1.3\"):",
        "comment_created_at": "2024-11-15T14:47:07+00:00",
        "comment_author": "johannes-engler-mw",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  }
]