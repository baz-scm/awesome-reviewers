[
  {
    "discussion_id": "1914654183",
    "pr_number": 10863,
    "pr_file": "pydantic/main.py",
    "created_at": "2025-01-14T11:20:16+00:00",
    "commented_code": "@classmethod\n     def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:\n-        \"\"\"Hook into generating the model's CoreSchema.\n-\n-        Args:\n-            source: The class we are generating a schema for.\n-                This will generally be the same as the `cls` argument if this is a classmethod.\n-            handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n-\n-        Returns:\n-            A `pydantic-core` `CoreSchema`.\n-        \"\"\"\n-        # Only use the cached value from this _exact_ class; we don't want one from a parent class\n-        # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\n+        warnings.warn(\n+            'The `__get_pydantic_core_schema__` method of the `BaseModel` class is deprecated. If you are calling '\n+            '`super().__get_pydantic_core_schema__` when overriding the method on a Pydantic model, consider using '\n+            '`handler(source)` instead. However, note that overriding this method on models can lead to unexpected '\n+            'side effects.',\n+            PydanticDeprecatedSince211,\n+            stacklevel=2,\n+        )",
    "repo_full_name": "pydantic/pydantic",
    "discussion_comments": [
      {
        "comment_id": "1914654183",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10863,
        "pr_file": "pydantic/main.py",
        "discussion_id": "1914654183",
        "commented_code": "@@ -687,23 +687,17 @@ def model_validate_strings(\n \n     @classmethod\n     def __get_pydantic_core_schema__(cls, source: type[BaseModel], handler: GetCoreSchemaHandler, /) -> CoreSchema:\n-        \"\"\"Hook into generating the model's CoreSchema.\n-\n-        Args:\n-            source: The class we are generating a schema for.\n-                This will generally be the same as the `cls` argument if this is a classmethod.\n-            handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n-\n-        Returns:\n-            A `pydantic-core` `CoreSchema`.\n-        \"\"\"\n-        # Only use the cached value from this _exact_ class; we don't want one from a parent class\n-        # This is why we check `cls.__dict__` and don't use `cls.__pydantic_core_schema__` or similar.\n+        warnings.warn(\n+            'The `__get_pydantic_core_schema__` method of the `BaseModel` class is deprecated. If you are calling '\n+            '`super().__get_pydantic_core_schema__` when overriding the method on a Pydantic model, consider using '\n+            '`handler(source)` instead. However, note that overriding this method on models can lead to unexpected '\n+            'side effects.',\n+            PydanticDeprecatedSince211,\n+            stacklevel=2,\n+        )",
        "comment_created_at": "2025-01-14T11:20:16+00:00",
        "comment_author": "sydney-runkle",
        "comment_body": "As discussed in person, let's add a comment that this warning is only omitted when calling super. I think we need to make it more clear what the consequences of this deprecation are.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2143334773",
    "pr_number": 11957,
    "pr_file": "pydantic/functional_validators.py",
    "created_at": "2025-06-12T17:55:58+00:00",
    "commented_code": "def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n         # auto apply the @classmethod decorator\n-        f = _decorators.ensure_classmethod_based_on_signature(f)\n+        if mode != 'after':",
    "repo_full_name": "pydantic/pydantic",
    "discussion_comments": [
      {
        "comment_id": "2143334773",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 11957,
        "pr_file": "pydantic/functional_validators.py",
        "discussion_id": "2143334773",
        "commented_code": "@@ -713,7 +713,8 @@ def verify_square(self) -> Self:\n \n     def dec(f: Any) -> _decorators.PydanticDescriptorProxy[Any]:\n         # auto apply the @classmethod decorator\n-        f = _decorators.ensure_classmethod_based_on_signature(f)\n+        if mode != 'after':",
        "comment_created_at": "2025-06-12T17:55:58+00:00",
        "comment_author": "DouweM",
        "comment_body": "I think it's worth putting a comment in to explain why we want this in all modes except for `after`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1949401408",
    "pr_number": 11388,
    "pr_file": "pydantic/main.py",
    "created_at": "2025-02-10T16:02:12+00:00",
    "commented_code": "__pydantic_complete__: ClassVar[bool] = False\n     \"\"\"Whether model building is completed, or if there are still undefined fields.\"\"\"\n \n+    __pydantic_fields_complete__: ClassVar[bool] = False\n+    \"\"\"Whether the fields where successfully collected. This is a private attribute, not meant\n+    to be used outside Pydantic.",
    "repo_full_name": "pydantic/pydantic",
    "discussion_comments": [
      {
        "comment_id": "1949401408",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 11388,
        "pr_file": "pydantic/main.py",
        "discussion_id": "1949401408",
        "commented_code": "@@ -147,6 +147,11 @@ class BaseModel(metaclass=_model_construction.ModelMetaclass):\n     __pydantic_complete__: ClassVar[bool] = False\n     \"\"\"Whether model building is completed, or if there are still undefined fields.\"\"\"\n \n+    __pydantic_fields_complete__: ClassVar[bool] = False\n+    \"\"\"Whether the fields where successfully collected. This is a private attribute, not meant\n+    to be used outside Pydantic.",
        "comment_created_at": "2025-02-10T16:02:12+00:00",
        "comment_author": "sydney-runkle",
        "comment_body": "Maybe add a note about what successfully collected means - specifically, that annotations were successfully evaluated?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1755521208",
    "pr_number": 10367,
    "pr_file": "pydantic/_internal/_validate_call.py",
    "created_at": "2024-09-11T20:10:01+00:00",
    "commented_code": "if self.__return_pydantic_validator__:\n             return self.__return_pydantic_validator__(res)\n         return res\n+\n+\n+class ValidateCallInfo(TypedDict):\n+    validate_return: bool\n+    config: ConfigDict | None\n+    function: Callable[..., Any]\n+    local_namspace: dict[str, Any] | None\n+\n+\n+def _is_wrapped_by_validate_call(obj: object) -> bool:\n+    return hasattr(obj, '__pydantic_validate_call_info__')\n+\n+\n+def collect_validate_call_info(namespace: dict[str, Any]):\n+    validate_call_infos = {}\n+    for name, func in namespace.items():\n+        if _is_wrapped_by_validate_call(func):\n+            validate_call_infos[name] = func.__pydantic_validate_call_info__\n+\n+    return validate_call_infos\n+\n+\n+@lru_cache(maxsize=None)\n+def _add_unique_postfix(name: str):\n+    \"\"\"Used to prevent namespace collision.\"\"\"",
    "repo_full_name": "pydantic/pydantic",
    "discussion_comments": [
      {
        "comment_id": "1755521208",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10367,
        "pr_file": "pydantic/_internal/_validate_call.py",
        "discussion_id": "1755521208",
        "commented_code": "@@ -97,3 +102,105 @@ def __call__(self, *args: Any, **kwargs: Any) -> Any:\n         if self.__return_pydantic_validator__:\n             return self.__return_pydantic_validator__(res)\n         return res\n+\n+\n+class ValidateCallInfo(TypedDict):\n+    validate_return: bool\n+    config: ConfigDict | None\n+    function: Callable[..., Any]\n+    local_namspace: dict[str, Any] | None\n+\n+\n+def _is_wrapped_by_validate_call(obj: object) -> bool:\n+    return hasattr(obj, '__pydantic_validate_call_info__')\n+\n+\n+def collect_validate_call_info(namespace: dict[str, Any]):\n+    validate_call_infos = {}\n+    for name, func in namespace.items():\n+        if _is_wrapped_by_validate_call(func):\n+            validate_call_infos[name] = func.__pydantic_validate_call_info__\n+\n+    return validate_call_infos\n+\n+\n+@lru_cache(maxsize=None)\n+def _add_unique_postfix(name: str):\n+    \"\"\"Used to prevent namespace collision.\"\"\"",
        "comment_created_at": "2024-09-11T20:10:01+00:00",
        "comment_author": "dmontagu",
        "comment_body": "Here or where this function is used, could you expand on why this is necessary? It's not immediately obvious to me reading the code why it would be a problem to just use the original names. (Even if I could figure it out by thinking harder, I think it should be in a comment so that future readers don't have to use as much brain power to understand.)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1846870325",
    "pr_number": 10655,
    "pr_file": "pydantic/_internal/_generate_schema.py",
    "created_at": "2024-11-18T16:08:17+00:00",
    "commented_code": "discriminator,\n             )\n         except _discriminated_union.MissingDefinitionForUnionRef:\n-            # defer until defs are resolved\n-            _discriminated_union.set_discriminator_in_metadata(\n-                schema,\n-                discriminator,\n-            )\n+            # defer until defs are resolved.\n+            _discriminated_union.set_discriminator_in_metadata(schema, discriminator)\n+            self.defs.used_deferred_discriminators = True\n             return schema\n \n-    class CollectedInvalid(Exception):\n-        pass\n-\n-    def clean_schema(self, schema: CoreSchema) -> CoreSchema:\n-        schema = self.collect_definitions(schema)\n-        schema = simplify_schema_references(schema)\n-        if collect_invalid_schemas(schema):\n-            raise self.CollectedInvalid()\n-        schema = _discriminated_union.apply_discriminators(schema)\n+    def clean_schema(self, schema: CoreSchema, *, deep_copy: bool) -> CoreSchema:\n+        \"\"\"Cleans the schema contents by substituting definition-refs and discriminators. Given schema must be the\n+        one just given by generate_schema. Does in place substituting to the schema by mutating it for performance\n+        reasons. Use deep_copy to avoid mutating the original schema.\n+        \"\"\"",
    "repo_full_name": "pydantic/pydantic",
    "discussion_comments": [
      {
        "comment_id": "1846870325",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10655,
        "pr_file": "pydantic/_internal/_generate_schema.py",
        "discussion_id": "1846870325",
        "commented_code": "@@ -533,36 +530,21 @@ def _apply_discriminator_to_union(\n                 discriminator,\n             )\n         except _discriminated_union.MissingDefinitionForUnionRef:\n-            # defer until defs are resolved\n-            _discriminated_union.set_discriminator_in_metadata(\n-                schema,\n-                discriminator,\n-            )\n+            # defer until defs are resolved.\n+            _discriminated_union.set_discriminator_in_metadata(schema, discriminator)\n+            self.defs.used_deferred_discriminators = True\n             return schema\n \n-    class CollectedInvalid(Exception):\n-        pass\n-\n-    def clean_schema(self, schema: CoreSchema) -> CoreSchema:\n-        schema = self.collect_definitions(schema)\n-        schema = simplify_schema_references(schema)\n-        if collect_invalid_schemas(schema):\n-            raise self.CollectedInvalid()\n-        schema = _discriminated_union.apply_discriminators(schema)\n+    def clean_schema(self, schema: CoreSchema, *, deep_copy: bool) -> CoreSchema:\n+        \"\"\"Cleans the schema contents by substituting definition-refs and discriminators. Given schema must be the\n+        one just given by generate_schema. Does in place substituting to the schema by mutating it for performance\n+        reasons. Use deep_copy to avoid mutating the original schema.\n+        \"\"\"",
        "comment_created_at": "2024-11-18T16:08:17+00:00",
        "comment_author": "sydney-runkle",
        "comment_body": "Let's be even more explicit here.\r\n\r\nCan we:\r\n* Explain subsituting definition refs\r\n* Explain why / how we add tagged info to discriminators?\r\n\r\nThis isn't your doing at all, but our internal `GenerateSchema` logic is poorly documented, so I think it's good to have thorough improvements as a standard for PRs here going forward.",
        "pr_file_module": null
      },
      {
        "comment_id": "1847177686",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10655,
        "pr_file": "pydantic/_internal/_generate_schema.py",
        "discussion_id": "1846870325",
        "commented_code": "@@ -533,36 +530,21 @@ def _apply_discriminator_to_union(\n                 discriminator,\n             )\n         except _discriminated_union.MissingDefinitionForUnionRef:\n-            # defer until defs are resolved\n-            _discriminated_union.set_discriminator_in_metadata(\n-                schema,\n-                discriminator,\n-            )\n+            # defer until defs are resolved.\n+            _discriminated_union.set_discriminator_in_metadata(schema, discriminator)\n+            self.defs.used_deferred_discriminators = True\n             return schema\n \n-    class CollectedInvalid(Exception):\n-        pass\n-\n-    def clean_schema(self, schema: CoreSchema) -> CoreSchema:\n-        schema = self.collect_definitions(schema)\n-        schema = simplify_schema_references(schema)\n-        if collect_invalid_schemas(schema):\n-            raise self.CollectedInvalid()\n-        schema = _discriminated_union.apply_discriminators(schema)\n+    def clean_schema(self, schema: CoreSchema, *, deep_copy: bool) -> CoreSchema:\n+        \"\"\"Cleans the schema contents by substituting definition-refs and discriminators. Given schema must be the\n+        one just given by generate_schema. Does in place substituting to the schema by mutating it for performance\n+        reasons. Use deep_copy to avoid mutating the original schema.\n+        \"\"\"",
        "comment_created_at": "2024-11-18T19:59:36+00:00",
        "comment_author": "MarkusSintonen",
        "comment_body": "Done, added some basic examples",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1840554549",
    "pr_number": 10537,
    "pr_file": "pydantic/type_adapter.py",
    "created_at": "2024-11-13T15:15:22+00:00",
    "commented_code": "self._type = type\n         self._config = config\n         self._parent_depth = _parent_depth\n-        if module is None:\n-            f = sys._getframe(1)\n-            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n-        else:\n-            self._module_name = module\n-\n-        self._core_schema: CoreSchema | None = None\n-        self._validator: SchemaValidator | PluggableSchemaValidator | None = None\n-        self._serializer: SchemaSerializer | None = None\n-\n-        if not self._defer_build():\n-            # Immediately initialize the core schema, validator and serializer\n-            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n-                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n-                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n-                self._init_core_attrs(rebuild_mocks=False)\n-\n-    @contextmanager\n-    def _with_frame_depth(self, depth: int) -> Iterator[None]:\n-        self._parent_depth += depth\n-        try:\n-            yield\n-        finally:\n-            self._parent_depth -= depth\n \n-    @_frame_depth(1)\n-    def _init_core_attrs(self, rebuild_mocks: bool) -> None:\n+        self.core_schema: CoreSchema\n+        self.validator: SchemaValidator | PluggableSchemaValidator\n+        self.serializer: SchemaSerializer\n+        self.pydantic_complete: bool = False\n+\n+        localns: _namespace_utils.MappingNamespace = (\n+            _typing_extra.parent_frame_namespace(parent_depth=self._parent_depth) or {}\n+        )\n+        globalns: _namespace_utils.MappingNamespace = sys._getframe(max(self._parent_depth - 1, 1)).f_globals\n+        self._module_name = module or cast(str, globalns.get('__name__', ''))\n+        self._init_core_attrs(\n+            ns_resolver=_namespace_utils.NsResolver(\n+                namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),\n+                parent_namespace=localns,\n+            ),\n+            force=False,\n+        )\n+\n+    def _init_core_attrs(\n+        self, ns_resolver: _namespace_utils.NsResolver, force: bool, raise_errors: bool = False\n+    ) -> bool:\n+        \"\"\"Initialize the core schema, validator, and serializer for the type.\n+\n+        Args:\n+            ns_resolver: The namespace resolver to use when building the core schema for the adapted type.\n+            force: Whether to force the construction of the core schema, validator, and serializer.\n+                If `force` is set to `False` and `_defer_build` is `True`, the core schema, validator, and serializer will be set to mocks.\n+            raise_errors: Whether to raise errors if initializing any of the core attrs fails.\n+\n+        Returns:\n+            `True` if the core schema, validator, and serializer were successfully initialized, otherwise `False`.\n+\n+        Raises:\n+            PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n+                and `raise_errors=True`.\n+\n+        Notes on namespace management, and subtle differences from `BaseModel`:\n+            This method is very similar to `_model_construction.complete_model_class`, used for finalizing `BaseModel` subclasses.\n+\n+            `BaseModel` uses its own `__module__` to find out where it was defined\n+            and then looks for symbols to resolve forward references in those globals.\n+            On the other hand, `TypeAdapter` can be initialized with arbitrary objects,\n+            which may not be types and thus do not have a `__module__` available.\n+            So instead we look at the globals in our parent stack frame.\n+\n+            It is expected that the `ns_resolver` passed to this function will have the correct\n+            namespace for the type we're adapting. See `TypeAdapter.__init__` and `TypeAdapter.rebuild`\n+            for various ways to construct this namespace.\n+\n+            This works for the case where this function is called in a module that\n+            has the target of forward references in its scope, but\n+            does not always work for more complex cases.\n+\n+            For example, take the following:\n+\n+            a.py\n+            ```python\n+            from typing import Dict, List\n+\n+            IntList = List[int]\n+            OuterDict = Dict[str, 'IntList']\n+            ```\n+\n+            b.py\n+            ```python test=\"skip\"\n+            from a import OuterDict\n+\n+            from pydantic import TypeAdapter\n+\n+            IntList = int  # replaces the symbol the forward reference is looking for\n+            v = TypeAdapter(OuterDict)\n+            v({'x': 1})  # should fail but doesn't",
    "repo_full_name": "pydantic/pydantic",
    "discussion_comments": [
      {
        "comment_id": "1840554549",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10537,
        "pr_file": "pydantic/type_adapter.py",
        "discussion_id": "1840554549",
        "commented_code": "@@ -233,100 +159,203 @@ def __init__(\n         self._type = type\n         self._config = config\n         self._parent_depth = _parent_depth\n-        if module is None:\n-            f = sys._getframe(1)\n-            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n-        else:\n-            self._module_name = module\n-\n-        self._core_schema: CoreSchema | None = None\n-        self._validator: SchemaValidator | PluggableSchemaValidator | None = None\n-        self._serializer: SchemaSerializer | None = None\n-\n-        if not self._defer_build():\n-            # Immediately initialize the core schema, validator and serializer\n-            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n-                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n-                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n-                self._init_core_attrs(rebuild_mocks=False)\n-\n-    @contextmanager\n-    def _with_frame_depth(self, depth: int) -> Iterator[None]:\n-        self._parent_depth += depth\n-        try:\n-            yield\n-        finally:\n-            self._parent_depth -= depth\n \n-    @_frame_depth(1)\n-    def _init_core_attrs(self, rebuild_mocks: bool) -> None:\n+        self.core_schema: CoreSchema\n+        self.validator: SchemaValidator | PluggableSchemaValidator\n+        self.serializer: SchemaSerializer\n+        self.pydantic_complete: bool = False\n+\n+        localns: _namespace_utils.MappingNamespace = (\n+            _typing_extra.parent_frame_namespace(parent_depth=self._parent_depth) or {}\n+        )\n+        globalns: _namespace_utils.MappingNamespace = sys._getframe(max(self._parent_depth - 1, 1)).f_globals\n+        self._module_name = module or cast(str, globalns.get('__name__', ''))\n+        self._init_core_attrs(\n+            ns_resolver=_namespace_utils.NsResolver(\n+                namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),\n+                parent_namespace=localns,\n+            ),\n+            force=False,\n+        )\n+\n+    def _init_core_attrs(\n+        self, ns_resolver: _namespace_utils.NsResolver, force: bool, raise_errors: bool = False\n+    ) -> bool:\n+        \"\"\"Initialize the core schema, validator, and serializer for the type.\n+\n+        Args:\n+            ns_resolver: The namespace resolver to use when building the core schema for the adapted type.\n+            force: Whether to force the construction of the core schema, validator, and serializer.\n+                If `force` is set to `False` and `_defer_build` is `True`, the core schema, validator, and serializer will be set to mocks.\n+            raise_errors: Whether to raise errors if initializing any of the core attrs fails.\n+\n+        Returns:\n+            `True` if the core schema, validator, and serializer were successfully initialized, otherwise `False`.\n+\n+        Raises:\n+            PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n+                and `raise_errors=True`.\n+\n+        Notes on namespace management, and subtle differences from `BaseModel`:\n+            This method is very similar to `_model_construction.complete_model_class`, used for finalizing `BaseModel` subclasses.\n+\n+            `BaseModel` uses its own `__module__` to find out where it was defined\n+            and then looks for symbols to resolve forward references in those globals.\n+            On the other hand, `TypeAdapter` can be initialized with arbitrary objects,\n+            which may not be types and thus do not have a `__module__` available.\n+            So instead we look at the globals in our parent stack frame.\n+\n+            It is expected that the `ns_resolver` passed to this function will have the correct\n+            namespace for the type we're adapting. See `TypeAdapter.__init__` and `TypeAdapter.rebuild`\n+            for various ways to construct this namespace.\n+\n+            This works for the case where this function is called in a module that\n+            has the target of forward references in its scope, but\n+            does not always work for more complex cases.\n+\n+            For example, take the following:\n+\n+            a.py\n+            ```python\n+            from typing import Dict, List\n+\n+            IntList = List[int]\n+            OuterDict = Dict[str, 'IntList']\n+            ```\n+\n+            b.py\n+            ```python test=\"skip\"\n+            from a import OuterDict\n+\n+            from pydantic import TypeAdapter\n+\n+            IntList = int  # replaces the symbol the forward reference is looking for\n+            v = TypeAdapter(OuterDict)\n+            v({'x': 1})  # should fail but doesn't",
        "comment_created_at": "2024-11-13T15:15:22+00:00",
        "comment_author": "MarkusSintonen",
        "comment_body": "Were the examples etc here meant to be in public `__init__` side (which has the experimental-looking `_parent_depth` argument for doing the namespace things) for documenting the ForwardRefs quirks?",
        "pr_file_module": null
      },
      {
        "comment_id": "1840566221",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10537,
        "pr_file": "pydantic/type_adapter.py",
        "discussion_id": "1840554549",
        "commented_code": "@@ -233,100 +159,203 @@ def __init__(\n         self._type = type\n         self._config = config\n         self._parent_depth = _parent_depth\n-        if module is None:\n-            f = sys._getframe(1)\n-            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n-        else:\n-            self._module_name = module\n-\n-        self._core_schema: CoreSchema | None = None\n-        self._validator: SchemaValidator | PluggableSchemaValidator | None = None\n-        self._serializer: SchemaSerializer | None = None\n-\n-        if not self._defer_build():\n-            # Immediately initialize the core schema, validator and serializer\n-            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n-                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n-                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n-                self._init_core_attrs(rebuild_mocks=False)\n-\n-    @contextmanager\n-    def _with_frame_depth(self, depth: int) -> Iterator[None]:\n-        self._parent_depth += depth\n-        try:\n-            yield\n-        finally:\n-            self._parent_depth -= depth\n \n-    @_frame_depth(1)\n-    def _init_core_attrs(self, rebuild_mocks: bool) -> None:\n+        self.core_schema: CoreSchema\n+        self.validator: SchemaValidator | PluggableSchemaValidator\n+        self.serializer: SchemaSerializer\n+        self.pydantic_complete: bool = False\n+\n+        localns: _namespace_utils.MappingNamespace = (\n+            _typing_extra.parent_frame_namespace(parent_depth=self._parent_depth) or {}\n+        )\n+        globalns: _namespace_utils.MappingNamespace = sys._getframe(max(self._parent_depth - 1, 1)).f_globals\n+        self._module_name = module or cast(str, globalns.get('__name__', ''))\n+        self._init_core_attrs(\n+            ns_resolver=_namespace_utils.NsResolver(\n+                namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),\n+                parent_namespace=localns,\n+            ),\n+            force=False,\n+        )\n+\n+    def _init_core_attrs(\n+        self, ns_resolver: _namespace_utils.NsResolver, force: bool, raise_errors: bool = False\n+    ) -> bool:\n+        \"\"\"Initialize the core schema, validator, and serializer for the type.\n+\n+        Args:\n+            ns_resolver: The namespace resolver to use when building the core schema for the adapted type.\n+            force: Whether to force the construction of the core schema, validator, and serializer.\n+                If `force` is set to `False` and `_defer_build` is `True`, the core schema, validator, and serializer will be set to mocks.\n+            raise_errors: Whether to raise errors if initializing any of the core attrs fails.\n+\n+        Returns:\n+            `True` if the core schema, validator, and serializer were successfully initialized, otherwise `False`.\n+\n+        Raises:\n+            PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n+                and `raise_errors=True`.\n+\n+        Notes on namespace management, and subtle differences from `BaseModel`:\n+            This method is very similar to `_model_construction.complete_model_class`, used for finalizing `BaseModel` subclasses.\n+\n+            `BaseModel` uses its own `__module__` to find out where it was defined\n+            and then looks for symbols to resolve forward references in those globals.\n+            On the other hand, `TypeAdapter` can be initialized with arbitrary objects,\n+            which may not be types and thus do not have a `__module__` available.\n+            So instead we look at the globals in our parent stack frame.\n+\n+            It is expected that the `ns_resolver` passed to this function will have the correct\n+            namespace for the type we're adapting. See `TypeAdapter.__init__` and `TypeAdapter.rebuild`\n+            for various ways to construct this namespace.\n+\n+            This works for the case where this function is called in a module that\n+            has the target of forward references in its scope, but\n+            does not always work for more complex cases.\n+\n+            For example, take the following:\n+\n+            a.py\n+            ```python\n+            from typing import Dict, List\n+\n+            IntList = List[int]\n+            OuterDict = Dict[str, 'IntList']\n+            ```\n+\n+            b.py\n+            ```python test=\"skip\"\n+            from a import OuterDict\n+\n+            from pydantic import TypeAdapter\n+\n+            IntList = int  # replaces the symbol the forward reference is looking for\n+            v = TypeAdapter(OuterDict)\n+            v({'x': 1})  # should fail but doesn't",
        "comment_created_at": "2024-11-13T15:19:10+00:00",
        "comment_author": "sydney-runkle",
        "comment_body": "Yeah, that probably makes more sense, given that's where we do the namespace fetching anyways. Happy to move there.",
        "pr_file_module": null
      },
      {
        "comment_id": "1840567440",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10537,
        "pr_file": "pydantic/type_adapter.py",
        "discussion_id": "1840554549",
        "commented_code": "@@ -233,100 +159,203 @@ def __init__(\n         self._type = type\n         self._config = config\n         self._parent_depth = _parent_depth\n-        if module is None:\n-            f = sys._getframe(1)\n-            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n-        else:\n-            self._module_name = module\n-\n-        self._core_schema: CoreSchema | None = None\n-        self._validator: SchemaValidator | PluggableSchemaValidator | None = None\n-        self._serializer: SchemaSerializer | None = None\n-\n-        if not self._defer_build():\n-            # Immediately initialize the core schema, validator and serializer\n-            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n-                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n-                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n-                self._init_core_attrs(rebuild_mocks=False)\n-\n-    @contextmanager\n-    def _with_frame_depth(self, depth: int) -> Iterator[None]:\n-        self._parent_depth += depth\n-        try:\n-            yield\n-        finally:\n-            self._parent_depth -= depth\n \n-    @_frame_depth(1)\n-    def _init_core_attrs(self, rebuild_mocks: bool) -> None:\n+        self.core_schema: CoreSchema\n+        self.validator: SchemaValidator | PluggableSchemaValidator\n+        self.serializer: SchemaSerializer\n+        self.pydantic_complete: bool = False\n+\n+        localns: _namespace_utils.MappingNamespace = (\n+            _typing_extra.parent_frame_namespace(parent_depth=self._parent_depth) or {}\n+        )\n+        globalns: _namespace_utils.MappingNamespace = sys._getframe(max(self._parent_depth - 1, 1)).f_globals\n+        self._module_name = module or cast(str, globalns.get('__name__', ''))\n+        self._init_core_attrs(\n+            ns_resolver=_namespace_utils.NsResolver(\n+                namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),\n+                parent_namespace=localns,\n+            ),\n+            force=False,\n+        )\n+\n+    def _init_core_attrs(\n+        self, ns_resolver: _namespace_utils.NsResolver, force: bool, raise_errors: bool = False\n+    ) -> bool:\n+        \"\"\"Initialize the core schema, validator, and serializer for the type.\n+\n+        Args:\n+            ns_resolver: The namespace resolver to use when building the core schema for the adapted type.\n+            force: Whether to force the construction of the core schema, validator, and serializer.\n+                If `force` is set to `False` and `_defer_build` is `True`, the core schema, validator, and serializer will be set to mocks.\n+            raise_errors: Whether to raise errors if initializing any of the core attrs fails.\n+\n+        Returns:\n+            `True` if the core schema, validator, and serializer were successfully initialized, otherwise `False`.\n+\n+        Raises:\n+            PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n+                and `raise_errors=True`.\n+\n+        Notes on namespace management, and subtle differences from `BaseModel`:\n+            This method is very similar to `_model_construction.complete_model_class`, used for finalizing `BaseModel` subclasses.\n+\n+            `BaseModel` uses its own `__module__` to find out where it was defined\n+            and then looks for symbols to resolve forward references in those globals.\n+            On the other hand, `TypeAdapter` can be initialized with arbitrary objects,\n+            which may not be types and thus do not have a `__module__` available.\n+            So instead we look at the globals in our parent stack frame.\n+\n+            It is expected that the `ns_resolver` passed to this function will have the correct\n+            namespace for the type we're adapting. See `TypeAdapter.__init__` and `TypeAdapter.rebuild`\n+            for various ways to construct this namespace.\n+\n+            This works for the case where this function is called in a module that\n+            has the target of forward references in its scope, but\n+            does not always work for more complex cases.\n+\n+            For example, take the following:\n+\n+            a.py\n+            ```python\n+            from typing import Dict, List\n+\n+            IntList = List[int]\n+            OuterDict = Dict[str, 'IntList']\n+            ```\n+\n+            b.py\n+            ```python test=\"skip\"\n+            from a import OuterDict\n+\n+            from pydantic import TypeAdapter\n+\n+            IntList = int  # replaces the symbol the forward reference is looking for\n+            v = TypeAdapter(OuterDict)\n+            v({'x': 1})  # should fail but doesn't",
        "comment_created_at": "2024-11-13T15:19:33+00:00",
        "comment_author": "sydney-runkle",
        "comment_body": "Might put this under a closed by default block as to not overwhelm users not using this complex functionality.",
        "pr_file_module": null
      },
      {
        "comment_id": "1840577766",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10537,
        "pr_file": "pydantic/type_adapter.py",
        "discussion_id": "1840554549",
        "commented_code": "@@ -233,100 +159,203 @@ def __init__(\n         self._type = type\n         self._config = config\n         self._parent_depth = _parent_depth\n-        if module is None:\n-            f = sys._getframe(1)\n-            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n-        else:\n-            self._module_name = module\n-\n-        self._core_schema: CoreSchema | None = None\n-        self._validator: SchemaValidator | PluggableSchemaValidator | None = None\n-        self._serializer: SchemaSerializer | None = None\n-\n-        if not self._defer_build():\n-            # Immediately initialize the core schema, validator and serializer\n-            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n-                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n-                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n-                self._init_core_attrs(rebuild_mocks=False)\n-\n-    @contextmanager\n-    def _with_frame_depth(self, depth: int) -> Iterator[None]:\n-        self._parent_depth += depth\n-        try:\n-            yield\n-        finally:\n-            self._parent_depth -= depth\n \n-    @_frame_depth(1)\n-    def _init_core_attrs(self, rebuild_mocks: bool) -> None:\n+        self.core_schema: CoreSchema\n+        self.validator: SchemaValidator | PluggableSchemaValidator\n+        self.serializer: SchemaSerializer\n+        self.pydantic_complete: bool = False\n+\n+        localns: _namespace_utils.MappingNamespace = (\n+            _typing_extra.parent_frame_namespace(parent_depth=self._parent_depth) or {}\n+        )\n+        globalns: _namespace_utils.MappingNamespace = sys._getframe(max(self._parent_depth - 1, 1)).f_globals\n+        self._module_name = module or cast(str, globalns.get('__name__', ''))\n+        self._init_core_attrs(\n+            ns_resolver=_namespace_utils.NsResolver(\n+                namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),\n+                parent_namespace=localns,\n+            ),\n+            force=False,\n+        )\n+\n+    def _init_core_attrs(\n+        self, ns_resolver: _namespace_utils.NsResolver, force: bool, raise_errors: bool = False\n+    ) -> bool:\n+        \"\"\"Initialize the core schema, validator, and serializer for the type.\n+\n+        Args:\n+            ns_resolver: The namespace resolver to use when building the core schema for the adapted type.\n+            force: Whether to force the construction of the core schema, validator, and serializer.\n+                If `force` is set to `False` and `_defer_build` is `True`, the core schema, validator, and serializer will be set to mocks.\n+            raise_errors: Whether to raise errors if initializing any of the core attrs fails.\n+\n+        Returns:\n+            `True` if the core schema, validator, and serializer were successfully initialized, otherwise `False`.\n+\n+        Raises:\n+            PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n+                and `raise_errors=True`.\n+\n+        Notes on namespace management, and subtle differences from `BaseModel`:\n+            This method is very similar to `_model_construction.complete_model_class`, used for finalizing `BaseModel` subclasses.\n+\n+            `BaseModel` uses its own `__module__` to find out where it was defined\n+            and then looks for symbols to resolve forward references in those globals.\n+            On the other hand, `TypeAdapter` can be initialized with arbitrary objects,\n+            which may not be types and thus do not have a `__module__` available.\n+            So instead we look at the globals in our parent stack frame.\n+\n+            It is expected that the `ns_resolver` passed to this function will have the correct\n+            namespace for the type we're adapting. See `TypeAdapter.__init__` and `TypeAdapter.rebuild`\n+            for various ways to construct this namespace.\n+\n+            This works for the case where this function is called in a module that\n+            has the target of forward references in its scope, but\n+            does not always work for more complex cases.\n+\n+            For example, take the following:\n+\n+            a.py\n+            ```python\n+            from typing import Dict, List\n+\n+            IntList = List[int]\n+            OuterDict = Dict[str, 'IntList']\n+            ```\n+\n+            b.py\n+            ```python test=\"skip\"\n+            from a import OuterDict\n+\n+            from pydantic import TypeAdapter\n+\n+            IntList = int  # replaces the symbol the forward reference is looking for\n+            v = TypeAdapter(OuterDict)\n+            v({'x': 1})  # should fail but doesn't",
        "comment_created_at": "2024-11-13T15:22:53+00:00",
        "comment_author": "MarkusSintonen",
        "comment_body": "> Might put this under a closed by default block as to not overwhelm users not using this complex functionality.\r\n\r\nYeah the ForwardRefs are such a can of worms... Not really necessarily relevant for the usual simple use-cases.",
        "pr_file_module": null
      },
      {
        "comment_id": "1840590030",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10537,
        "pr_file": "pydantic/type_adapter.py",
        "discussion_id": "1840554549",
        "commented_code": "@@ -233,100 +159,203 @@ def __init__(\n         self._type = type\n         self._config = config\n         self._parent_depth = _parent_depth\n-        if module is None:\n-            f = sys._getframe(1)\n-            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n-        else:\n-            self._module_name = module\n-\n-        self._core_schema: CoreSchema | None = None\n-        self._validator: SchemaValidator | PluggableSchemaValidator | None = None\n-        self._serializer: SchemaSerializer | None = None\n-\n-        if not self._defer_build():\n-            # Immediately initialize the core schema, validator and serializer\n-            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n-                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n-                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n-                self._init_core_attrs(rebuild_mocks=False)\n-\n-    @contextmanager\n-    def _with_frame_depth(self, depth: int) -> Iterator[None]:\n-        self._parent_depth += depth\n-        try:\n-            yield\n-        finally:\n-            self._parent_depth -= depth\n \n-    @_frame_depth(1)\n-    def _init_core_attrs(self, rebuild_mocks: bool) -> None:\n+        self.core_schema: CoreSchema\n+        self.validator: SchemaValidator | PluggableSchemaValidator\n+        self.serializer: SchemaSerializer\n+        self.pydantic_complete: bool = False\n+\n+        localns: _namespace_utils.MappingNamespace = (\n+            _typing_extra.parent_frame_namespace(parent_depth=self._parent_depth) or {}\n+        )\n+        globalns: _namespace_utils.MappingNamespace = sys._getframe(max(self._parent_depth - 1, 1)).f_globals\n+        self._module_name = module or cast(str, globalns.get('__name__', ''))\n+        self._init_core_attrs(\n+            ns_resolver=_namespace_utils.NsResolver(\n+                namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),\n+                parent_namespace=localns,\n+            ),\n+            force=False,\n+        )\n+\n+    def _init_core_attrs(\n+        self, ns_resolver: _namespace_utils.NsResolver, force: bool, raise_errors: bool = False\n+    ) -> bool:\n+        \"\"\"Initialize the core schema, validator, and serializer for the type.\n+\n+        Args:\n+            ns_resolver: The namespace resolver to use when building the core schema for the adapted type.\n+            force: Whether to force the construction of the core schema, validator, and serializer.\n+                If `force` is set to `False` and `_defer_build` is `True`, the core schema, validator, and serializer will be set to mocks.\n+            raise_errors: Whether to raise errors if initializing any of the core attrs fails.\n+\n+        Returns:\n+            `True` if the core schema, validator, and serializer were successfully initialized, otherwise `False`.\n+\n+        Raises:\n+            PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n+                and `raise_errors=True`.\n+\n+        Notes on namespace management, and subtle differences from `BaseModel`:\n+            This method is very similar to `_model_construction.complete_model_class`, used for finalizing `BaseModel` subclasses.\n+\n+            `BaseModel` uses its own `__module__` to find out where it was defined\n+            and then looks for symbols to resolve forward references in those globals.\n+            On the other hand, `TypeAdapter` can be initialized with arbitrary objects,\n+            which may not be types and thus do not have a `__module__` available.\n+            So instead we look at the globals in our parent stack frame.\n+\n+            It is expected that the `ns_resolver` passed to this function will have the correct\n+            namespace for the type we're adapting. See `TypeAdapter.__init__` and `TypeAdapter.rebuild`\n+            for various ways to construct this namespace.\n+\n+            This works for the case where this function is called in a module that\n+            has the target of forward references in its scope, but\n+            does not always work for more complex cases.\n+\n+            For example, take the following:\n+\n+            a.py\n+            ```python\n+            from typing import Dict, List\n+\n+            IntList = List[int]\n+            OuterDict = Dict[str, 'IntList']\n+            ```\n+\n+            b.py\n+            ```python test=\"skip\"\n+            from a import OuterDict\n+\n+            from pydantic import TypeAdapter\n+\n+            IntList = int  # replaces the symbol the forward reference is looking for\n+            v = TypeAdapter(OuterDict)\n+            v({'x': 1})  # should fail but doesn't",
        "comment_created_at": "2024-11-13T15:26:35+00:00",
        "comment_author": "MarkusSintonen",
        "comment_body": "`rebuild` could be also one place to have this, or the class itself, dont know which is best...",
        "pr_file_module": null
      },
      {
        "comment_id": "1840608964",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10537,
        "pr_file": "pydantic/type_adapter.py",
        "discussion_id": "1840554549",
        "commented_code": "@@ -233,100 +159,203 @@ def __init__(\n         self._type = type\n         self._config = config\n         self._parent_depth = _parent_depth\n-        if module is None:\n-            f = sys._getframe(1)\n-            self._module_name = cast(str, f.f_globals.get('__name__', ''))\n-        else:\n-            self._module_name = module\n-\n-        self._core_schema: CoreSchema | None = None\n-        self._validator: SchemaValidator | PluggableSchemaValidator | None = None\n-        self._serializer: SchemaSerializer | None = None\n-\n-        if not self._defer_build():\n-            # Immediately initialize the core schema, validator and serializer\n-            with self._with_frame_depth(1):  # +1 frame depth for this __init__\n-                # Model itself may be using deferred building. For backward compatibility we don't rebuild model mocks\n-                # here as part of __init__ even though TypeAdapter itself is not using deferred building.\n-                self._init_core_attrs(rebuild_mocks=False)\n-\n-    @contextmanager\n-    def _with_frame_depth(self, depth: int) -> Iterator[None]:\n-        self._parent_depth += depth\n-        try:\n-            yield\n-        finally:\n-            self._parent_depth -= depth\n \n-    @_frame_depth(1)\n-    def _init_core_attrs(self, rebuild_mocks: bool) -> None:\n+        self.core_schema: CoreSchema\n+        self.validator: SchemaValidator | PluggableSchemaValidator\n+        self.serializer: SchemaSerializer\n+        self.pydantic_complete: bool = False\n+\n+        localns: _namespace_utils.MappingNamespace = (\n+            _typing_extra.parent_frame_namespace(parent_depth=self._parent_depth) or {}\n+        )\n+        globalns: _namespace_utils.MappingNamespace = sys._getframe(max(self._parent_depth - 1, 1)).f_globals\n+        self._module_name = module or cast(str, globalns.get('__name__', ''))\n+        self._init_core_attrs(\n+            ns_resolver=_namespace_utils.NsResolver(\n+                namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns, globals=globalns),\n+                parent_namespace=localns,\n+            ),\n+            force=False,\n+        )\n+\n+    def _init_core_attrs(\n+        self, ns_resolver: _namespace_utils.NsResolver, force: bool, raise_errors: bool = False\n+    ) -> bool:\n+        \"\"\"Initialize the core schema, validator, and serializer for the type.\n+\n+        Args:\n+            ns_resolver: The namespace resolver to use when building the core schema for the adapted type.\n+            force: Whether to force the construction of the core schema, validator, and serializer.\n+                If `force` is set to `False` and `_defer_build` is `True`, the core schema, validator, and serializer will be set to mocks.\n+            raise_errors: Whether to raise errors if initializing any of the core attrs fails.\n+\n+        Returns:\n+            `True` if the core schema, validator, and serializer were successfully initialized, otherwise `False`.\n+\n+        Raises:\n+            PydanticUndefinedAnnotation: If `PydanticUndefinedAnnotation` occurs in`__get_pydantic_core_schema__`\n+                and `raise_errors=True`.\n+\n+        Notes on namespace management, and subtle differences from `BaseModel`:\n+            This method is very similar to `_model_construction.complete_model_class`, used for finalizing `BaseModel` subclasses.\n+\n+            `BaseModel` uses its own `__module__` to find out where it was defined\n+            and then looks for symbols to resolve forward references in those globals.\n+            On the other hand, `TypeAdapter` can be initialized with arbitrary objects,\n+            which may not be types and thus do not have a `__module__` available.\n+            So instead we look at the globals in our parent stack frame.\n+\n+            It is expected that the `ns_resolver` passed to this function will have the correct\n+            namespace for the type we're adapting. See `TypeAdapter.__init__` and `TypeAdapter.rebuild`\n+            for various ways to construct this namespace.\n+\n+            This works for the case where this function is called in a module that\n+            has the target of forward references in its scope, but\n+            does not always work for more complex cases.\n+\n+            For example, take the following:\n+\n+            a.py\n+            ```python\n+            from typing import Dict, List\n+\n+            IntList = List[int]\n+            OuterDict = Dict[str, 'IntList']\n+            ```\n+\n+            b.py\n+            ```python test=\"skip\"\n+            from a import OuterDict\n+\n+            from pydantic import TypeAdapter\n+\n+            IntList = int  # replaces the symbol the forward reference is looking for\n+            v = TypeAdapter(OuterDict)\n+            v({'x': 1})  # should fail but doesn't",
        "comment_created_at": "2024-11-13T15:32:59+00:00",
        "comment_author": "sydney-runkle",
        "comment_body": "Maybe the class itself, honestly. Bc it applies to both `__init__` and `rebuild`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1825751124",
    "pr_number": 10725,
    "pr_file": "pydantic/_internal/_typing_extra.py",
    "created_at": "2024-11-01T12:11:24+00:00",
    "commented_code": "-\"\"\"Logic for interacting with type annotations, mostly extensions, shims and hacks to wrap python's typing module.\"\"\"\n+\"\"\"Logic for interacting with type annotations, mostly extensions, shims and hacks to wrap Python's typing module.\"\"\"\n \n-from __future__ import annotations as _annotations\n+from __future__ import annotations\n \n-import dataclasses\n+import collections.abc\n import re\n import sys\n import types\n import typing\n import warnings\n-from collections.abc import Callable\n-from functools import partial\n-from types import GetSetDescriptorType\n-from typing import TYPE_CHECKING, Any, Final\n+from functools import lru_cache, partial\n+from typing import Any, Callable\n \n-from typing_extensions import Annotated, Literal, TypeAliasType, TypeGuard, Unpack, deprecated, get_args, get_origin\n+import typing_extensions\n+from typing_extensions import TypeIs, deprecated, get_args, get_origin\n \n from ._namespace_utils import GlobalsNamespace, MappingNamespace, NsResolver, get_module_ns_of\n \n-if TYPE_CHECKING:\n-    from ._dataclasses import StandardDataclass\n+if sys.version_info < (3, 10):\n+    NoneType = type(None)\n+    EllipsisType = type(Ellipsis)\n+else:\n+    from types import EllipsisType as EllipsisType\n+    from types import NoneType as NoneType\n \n-try:\n-    from typing import _TypingBase  # type: ignore[attr-defined]\n-except ImportError:\n-    from typing import _Final as _TypingBase  # type: ignore[attr-defined]\n \n-typing_base = _TypingBase\n+# See https://typing-extensions.readthedocs.io/en/latest/#runtime-use-of-types:\n \n \n-if sys.version_info < (3, 9):\n-    # python < 3.9 does not have GenericAlias (list[int], tuple[str, ...] and so on)\n-    TypingGenericAlias = ()\n-else:\n-    from typing import GenericAlias as TypingGenericAlias  # type: ignore\n+@lru_cache(maxsize=None)\n+def _get_typing_objects_by_name_of(name: str) -> tuple[Any, ...]:\n+    result = tuple(getattr(module, name) for module in (typing, typing_extensions) if hasattr(module, name))\n+    if not result:\n+        raise ValueError(f'Neither typing nor typing_extensions has an object called {name!r}')\n+    return result\n \n \n-if sys.version_info < (3, 11):\n-    from typing_extensions import NotRequired, Required\n-else:\n-    from typing import NotRequired, Required  # noqa: F401\n+# As suggested by the `typing-extensions` documentation, we could apply caching to this method,\n+# but it doesn't seem to improve performance. This also requires `obj` to be hashable, which\n+# might not be always the case:\n+def _is_typing_name(obj: object, name: str) -> bool:\n+    # Using `any()` is slower:",
    "repo_full_name": "pydantic/pydantic",
    "discussion_comments": [
      {
        "comment_id": "1825751124",
        "repo_full_name": "pydantic/pydantic",
        "pr_number": 10725,
        "pr_file": "pydantic/_internal/_typing_extra.py",
        "discussion_id": "1825751124",
        "commented_code": "@@ -1,192 +1,414 @@\n-\"\"\"Logic for interacting with type annotations, mostly extensions, shims and hacks to wrap python's typing module.\"\"\"\n+\"\"\"Logic for interacting with type annotations, mostly extensions, shims and hacks to wrap Python's typing module.\"\"\"\n \n-from __future__ import annotations as _annotations\n+from __future__ import annotations\n \n-import dataclasses\n+import collections.abc\n import re\n import sys\n import types\n import typing\n import warnings\n-from collections.abc import Callable\n-from functools import partial\n-from types import GetSetDescriptorType\n-from typing import TYPE_CHECKING, Any, Final\n+from functools import lru_cache, partial\n+from typing import Any, Callable\n \n-from typing_extensions import Annotated, Literal, TypeAliasType, TypeGuard, Unpack, deprecated, get_args, get_origin\n+import typing_extensions\n+from typing_extensions import TypeIs, deprecated, get_args, get_origin\n \n from ._namespace_utils import GlobalsNamespace, MappingNamespace, NsResolver, get_module_ns_of\n \n-if TYPE_CHECKING:\n-    from ._dataclasses import StandardDataclass\n+if sys.version_info < (3, 10):\n+    NoneType = type(None)\n+    EllipsisType = type(Ellipsis)\n+else:\n+    from types import EllipsisType as EllipsisType\n+    from types import NoneType as NoneType\n \n-try:\n-    from typing import _TypingBase  # type: ignore[attr-defined]\n-except ImportError:\n-    from typing import _Final as _TypingBase  # type: ignore[attr-defined]\n \n-typing_base = _TypingBase\n+# See https://typing-extensions.readthedocs.io/en/latest/#runtime-use-of-types:\n \n \n-if sys.version_info < (3, 9):\n-    # python < 3.9 does not have GenericAlias (list[int], tuple[str, ...] and so on)\n-    TypingGenericAlias = ()\n-else:\n-    from typing import GenericAlias as TypingGenericAlias  # type: ignore\n+@lru_cache(maxsize=None)\n+def _get_typing_objects_by_name_of(name: str) -> tuple[Any, ...]:\n+    result = tuple(getattr(module, name) for module in (typing, typing_extensions) if hasattr(module, name))\n+    if not result:\n+        raise ValueError(f'Neither typing nor typing_extensions has an object called {name!r}')\n+    return result\n \n \n-if sys.version_info < (3, 11):\n-    from typing_extensions import NotRequired, Required\n-else:\n-    from typing import NotRequired, Required  # noqa: F401\n+# As suggested by the `typing-extensions` documentation, we could apply caching to this method,\n+# but it doesn't seem to improve performance. This also requires `obj` to be hashable, which\n+# might not be always the case:\n+def _is_typing_name(obj: object, name: str) -> bool:\n+    # Using `any()` is slower:",
        "comment_created_at": "2024-11-01T12:11:24+00:00",
        "comment_author": "sydney-runkle",
        "comment_body": "Can we add a docstring to this function and the one above? If you're switching contexts into the world of typing and types management, a little description here could really help.",
        "pr_file_module": null
      }
    ]
  }
]