[
  {
    "discussion_id": "2088107795",
    "pr_number": 150218,
    "pr_file": "aten/src/ATen/DLConvertor.cpp",
    "created_at": "2025-05-14T06:03:19+00:00",
    "commented_code": "return fromDLPackImpl<DLManagedTensorVersioned>(src, std::move(deleter));\n }\n \n+Tensor maybeCopyTensor(\n+    const Tensor& data,\n+    std::optional<DLDevice> optional_dl_device,\n+    std::optional<bool> copy) {\n+  bool force_copy = copy.has_value() && *copy;\n+  bool force_move = copy.has_value() && !*copy;\n+\n+  if (optional_dl_device.has_value()) {\n+    auto device = at::getATenDevice(\n+        optional_dl_device->device_type,\n+        static_cast<c10::DeviceIndex>(optional_dl_device->device_id));\n+\n+    if (device != data.device()) {\n+      TORCH_CHECK(\n+          !force_move,\n+          \"cannot move tensor from \",\n+          data.device(),\n+          \" to \",\n+          device,\n+          \" without copying. Set copy=True is needed.\");",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2088107795",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "aten/src/ATen/DLConvertor.cpp",
        "discussion_id": "2088107795",
        "commented_code": "@@ -388,4 +389,35 @@ Tensor fromDLPackVersioned(DLManagedTensorVersioned* src, std::function<void(voi\n   return fromDLPackImpl<DLManagedTensorVersioned>(src, std::move(deleter));\n }\n \n+Tensor maybeCopyTensor(\n+    const Tensor& data,\n+    std::optional<DLDevice> optional_dl_device,\n+    std::optional<bool> copy) {\n+  bool force_copy = copy.has_value() && *copy;\n+  bool force_move = copy.has_value() && !*copy;\n+\n+  if (optional_dl_device.has_value()) {\n+    auto device = at::getATenDevice(\n+        optional_dl_device->device_type,\n+        static_cast<c10::DeviceIndex>(optional_dl_device->device_id));\n+\n+    if (device != data.device()) {\n+      TORCH_CHECK(\n+          !force_move,\n+          \"cannot move tensor from \",\n+          data.device(),\n+          \" to \",\n+          device,\n+          \" without copying. Set copy=True is needed.\");",
        "comment_created_at": "2025-05-14T06:03:19+00:00",
        "comment_author": "msaroufim",
        "comment_body": "just double checking this check would make it clear that users would need to set the copy flag in\r\n\r\n`torch.from_dlpack(..., copy=copy)`",
        "pr_file_module": null
      },
      {
        "comment_id": "2105836795",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "aten/src/ATen/DLConvertor.cpp",
        "discussion_id": "2088107795",
        "commented_code": "@@ -388,4 +389,35 @@ Tensor fromDLPackVersioned(DLManagedTensorVersioned* src, std::function<void(voi\n   return fromDLPackImpl<DLManagedTensorVersioned>(src, std::move(deleter));\n }\n \n+Tensor maybeCopyTensor(\n+    const Tensor& data,\n+    std::optional<DLDevice> optional_dl_device,\n+    std::optional<bool> copy) {\n+  bool force_copy = copy.has_value() && *copy;\n+  bool force_move = copy.has_value() && !*copy;\n+\n+  if (optional_dl_device.has_value()) {\n+    auto device = at::getATenDevice(\n+        optional_dl_device->device_type,\n+        static_cast<c10::DeviceIndex>(optional_dl_device->device_id));\n+\n+    if (device != data.device()) {\n+      TORCH_CHECK(\n+          !force_move,\n+          \"cannot move tensor from \",\n+          data.device(),\n+          \" to \",\n+          device,\n+          \" without copying. Set copy=True is needed.\");",
        "comment_created_at": "2025-05-24T14:18:41+00:00",
        "comment_author": "ysiraichi",
        "comment_body": "Ah, no. Not specifying `copy` should also work, here. Will fix.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089790531",
    "pr_number": 150218,
    "pr_file": "aten/src/ATen/DLConvertor.cpp",
    "created_at": "2025-05-14T21:36:27+00:00",
    "commented_code": "ctx.device_type = DLDeviceType::kDLExtDev;\n       break;\n     default:\n-      TORCH_CHECK(false, \"Cannot pack tensors on \" + tensor.device().str());\n+      TORCH_CHECK(false, \"Cannot pack tensors on \" + device.str());\n   }\n+\n   return ctx;\n }\n \n-static Device getATenDevice(const DLDevice& ctx, void* data) {\n-  switch (ctx.device_type) {\n+static Device getATenDevice(DLDeviceType type, c10::DeviceIndex index, void* data = nullptr) {\n+  switch (type) {\n     case DLDeviceType::kDLCPU:\n       return at::Device(DeviceType::CPU);\n #ifndef USE_ROCM\n     // if we are compiled under HIP, we cannot do cuda\n     case DLDeviceType::kDLCUDA:\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #endif\n     case DLDeviceType::kDLOpenCL:\n-      return at::Device(DeviceType::OPENCL, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::OPENCL, index);\n     case DLDeviceType::kDLROCM:\n #ifdef USE_ROCM\n       // this looks funny, we need to return CUDA here to masquerade\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #else\n-      return at::Device(DeviceType::HIP, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::HIP, index);\n #endif\n     case DLDeviceType::kDLOneAPI:\n+      TORCH_CHECK(data != nullptr, \"Can't get ATen device for XPU without XPU data.\");",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2089790531",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "aten/src/ATen/DLConvertor.cpp",
        "discussion_id": "2089790531",
        "commented_code": "@@ -130,38 +133,40 @@ static DLDevice getDLDevice(const Tensor& tensor, c10::DeviceIndex device_id) {\n       ctx.device_type = DLDeviceType::kDLExtDev;\n       break;\n     default:\n-      TORCH_CHECK(false, \"Cannot pack tensors on \" + tensor.device().str());\n+      TORCH_CHECK(false, \"Cannot pack tensors on \" + device.str());\n   }\n+\n   return ctx;\n }\n \n-static Device getATenDevice(const DLDevice& ctx, void* data) {\n-  switch (ctx.device_type) {\n+static Device getATenDevice(DLDeviceType type, c10::DeviceIndex index, void* data = nullptr) {\n+  switch (type) {\n     case DLDeviceType::kDLCPU:\n       return at::Device(DeviceType::CPU);\n #ifndef USE_ROCM\n     // if we are compiled under HIP, we cannot do cuda\n     case DLDeviceType::kDLCUDA:\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #endif\n     case DLDeviceType::kDLOpenCL:\n-      return at::Device(DeviceType::OPENCL, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::OPENCL, index);\n     case DLDeviceType::kDLROCM:\n #ifdef USE_ROCM\n       // this looks funny, we need to return CUDA here to masquerade\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #else\n-      return at::Device(DeviceType::HIP, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::HIP, index);\n #endif\n     case DLDeviceType::kDLOneAPI:\n+      TORCH_CHECK(data != nullptr, \"Can't get ATen device for XPU without XPU data.\");",
        "comment_created_at": "2025-05-14T21:36:27+00:00",
        "comment_author": "albanD",
        "comment_body": "nit: Why do we use data here and not the passed in type and index?",
        "pr_file_module": null
      },
      {
        "comment_id": "2105835687",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "aten/src/ATen/DLConvertor.cpp",
        "discussion_id": "2089790531",
        "commented_code": "@@ -130,38 +133,40 @@ static DLDevice getDLDevice(const Tensor& tensor, c10::DeviceIndex device_id) {\n       ctx.device_type = DLDeviceType::kDLExtDev;\n       break;\n     default:\n-      TORCH_CHECK(false, \"Cannot pack tensors on \" + tensor.device().str());\n+      TORCH_CHECK(false, \"Cannot pack tensors on \" + device.str());\n   }\n+\n   return ctx;\n }\n \n-static Device getATenDevice(const DLDevice& ctx, void* data) {\n-  switch (ctx.device_type) {\n+static Device getATenDevice(DLDeviceType type, c10::DeviceIndex index, void* data = nullptr) {\n+  switch (type) {\n     case DLDeviceType::kDLCPU:\n       return at::Device(DeviceType::CPU);\n #ifndef USE_ROCM\n     // if we are compiled under HIP, we cannot do cuda\n     case DLDeviceType::kDLCUDA:\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #endif\n     case DLDeviceType::kDLOpenCL:\n-      return at::Device(DeviceType::OPENCL, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::OPENCL, index);\n     case DLDeviceType::kDLROCM:\n #ifdef USE_ROCM\n       // this looks funny, we need to return CUDA here to masquerade\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #else\n-      return at::Device(DeviceType::HIP, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::HIP, index);\n #endif\n     case DLDeviceType::kDLOneAPI:\n+      TORCH_CHECK(data != nullptr, \"Can't get ATen device for XPU without XPU data.\");",
        "comment_created_at": "2025-05-24T14:15:52+00:00",
        "comment_author": "ysiraichi",
        "comment_body": "I'm not really sure myself. I just added this check because I needed to use this function without `data` when copying the tensor to some other device (see `maybeCopyTensor`).",
        "pr_file_module": null
      },
      {
        "comment_id": "2150365480",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "aten/src/ATen/DLConvertor.cpp",
        "discussion_id": "2089790531",
        "commented_code": "@@ -130,38 +133,40 @@ static DLDevice getDLDevice(const Tensor& tensor, c10::DeviceIndex device_id) {\n       ctx.device_type = DLDeviceType::kDLExtDev;\n       break;\n     default:\n-      TORCH_CHECK(false, \"Cannot pack tensors on \" + tensor.device().str());\n+      TORCH_CHECK(false, \"Cannot pack tensors on \" + device.str());\n   }\n+\n   return ctx;\n }\n \n-static Device getATenDevice(const DLDevice& ctx, void* data) {\n-  switch (ctx.device_type) {\n+static Device getATenDevice(DLDeviceType type, c10::DeviceIndex index, void* data = nullptr) {\n+  switch (type) {\n     case DLDeviceType::kDLCPU:\n       return at::Device(DeviceType::CPU);\n #ifndef USE_ROCM\n     // if we are compiled under HIP, we cannot do cuda\n     case DLDeviceType::kDLCUDA:\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #endif\n     case DLDeviceType::kDLOpenCL:\n-      return at::Device(DeviceType::OPENCL, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::OPENCL, index);\n     case DLDeviceType::kDLROCM:\n #ifdef USE_ROCM\n       // this looks funny, we need to return CUDA here to masquerade\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #else\n-      return at::Device(DeviceType::HIP, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::HIP, index);\n #endif\n     case DLDeviceType::kDLOneAPI:\n+      TORCH_CHECK(data != nullptr, \"Can't get ATen device for XPU without XPU data.\");",
        "comment_created_at": "2025-06-16T16:03:08+00:00",
        "comment_author": "albanD",
        "comment_body": "cc @EikanWang do you know by any chance?",
        "pr_file_module": null
      },
      {
        "comment_id": "2182300680",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "aten/src/ATen/DLConvertor.cpp",
        "discussion_id": "2089790531",
        "commented_code": "@@ -130,38 +133,40 @@ static DLDevice getDLDevice(const Tensor& tensor, c10::DeviceIndex device_id) {\n       ctx.device_type = DLDeviceType::kDLExtDev;\n       break;\n     default:\n-      TORCH_CHECK(false, \"Cannot pack tensors on \" + tensor.device().str());\n+      TORCH_CHECK(false, \"Cannot pack tensors on \" + device.str());\n   }\n+\n   return ctx;\n }\n \n-static Device getATenDevice(const DLDevice& ctx, void* data) {\n-  switch (ctx.device_type) {\n+static Device getATenDevice(DLDeviceType type, c10::DeviceIndex index, void* data = nullptr) {\n+  switch (type) {\n     case DLDeviceType::kDLCPU:\n       return at::Device(DeviceType::CPU);\n #ifndef USE_ROCM\n     // if we are compiled under HIP, we cannot do cuda\n     case DLDeviceType::kDLCUDA:\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #endif\n     case DLDeviceType::kDLOpenCL:\n-      return at::Device(DeviceType::OPENCL, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::OPENCL, index);\n     case DLDeviceType::kDLROCM:\n #ifdef USE_ROCM\n       // this looks funny, we need to return CUDA here to masquerade\n-      return at::Device(DeviceType::CUDA, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::CUDA, index);\n #else\n-      return at::Device(DeviceType::HIP, static_cast<c10::DeviceIndex>(ctx.device_id));\n+      return at::Device(DeviceType::HIP, index);\n #endif\n     case DLDeviceType::kDLOneAPI:\n+      TORCH_CHECK(data != nullptr, \"Can't get ATen device for XPU without XPU data.\");",
        "comment_created_at": "2025-07-03T09:22:26+00:00",
        "comment_author": "guangyey",
        "comment_body": "Currently, XPU always gets its device index from data since device index in PyTorch might not match the order of the xpu driver enumerated.\r\nWe will refine this issue in the following PR when the appropriate xpu toolchain is available.\r\nBTW, here `TORCH_CHECK` is not needed since `getDeviceFromPtr` will check data's validity inside itself.",
        "pr_file_module": null
      }
    ]
  }
]