[
  {
    "discussion_id": "2191084365",
    "pr_number": 157699,
    "pr_file": "torch/_inductor/lookup_table.py",
    "created_at": "2025-07-07T22:12:19+00:00",
    "commented_code": "+import json\n+import logging\n+from functools import lru_cache\n+from typing import Any, Optional\n+\n+import torch\n+from torch._inductor.virtualized import V\n+\n+from . import config as inductor_config\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def _in_use() -> bool:\n+    \"\"\"\n+    Determine if the gemm config lookup table should be used.\n+    This system is only used when cuda is available\n+    Returns True if either:\n+    1. The kernel_config_lookup_table_path is set, OR\n+    2. The kernel_config_lookup_table global has been set\n+    \"\"\"\n+    return torch.cuda.is_available() and (\n+        inductor_config.triton.kernel_config_lookup_table_path is not None\n+        or kernel_config_lookup_table is not None\n+    )\n+\n+\n+@lru_cache(0)\n+def _read_lookup_table(\n+    path: str,\n+) -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Handle actual file reading, so this can be cached depending on the input path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+    # Assert supported file extensions\n+    if path.endswith(\".json\"):\n+        with open(path) as f:\n+            table = json.load(f)\n+    elif path.endswith(\".yaml\") or path.endswith(\".yml\"):\n+        try:\n+            import yaml\n+        except ImportError:\n+            raise ImportError(\n+                \"PyYAML is required to load YAML files. Install with: pip install PyYAML, or convert your file to JSON format instead.\"\n+            )\n+        with open(path) as f:\n+            table = yaml.safe_load(f)\n+    else:\n+        raise AssertionError(\n+            f\"Unsupported file format. Only .json and .yaml/.yml files are supported. Got: {path}\"\n+        )\n+    return table\n+\n+\n+def _get_lookup_table() -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Load and return the gemm config lookup table from file if configured.\n+\n+    If the table is already defined, this takes precedence over the file path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+\n+    # If table is directly defined, use that\n+    if kernel_config_lookup_table is not None:\n+        # Log warning if both table and path are defined\n+        if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+            log.warning(\n+                \"Both kernel_config_lookup_table and kernel_config_lookup_table_path are defined. \"\n+                \"Ignoring the path because a table is already defined.\"\n+            )\n+        return kernel_config_lookup_table\n+\n+    # Otherwise use the path if it's defined\n+    if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+        return _read_lookup_table(\n+            inductor_config.triton.kernel_config_lookup_table_path\n+        )\n+\n+    return None\n+\n+\n+# A static lookup table for (triton) configurations for GEMMs.\n+# This is a lookup table in the form of\n+# [op][device name][input_nodes_key][backend_key] = JSON_string\n+# where the JSON string format is operation-dependent:\n+#\n+# For MM family operations (mm, bmm, addmm, mm_plus_mm):\n+\n+# for triton and tma backend_key:\n+# - \"config\": a list of GemmConfig parameters [BLOCK_M, BLOCK_N, BLOCK_K, num_stages, num_warps]\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [128, 128, 64, 2, 2], \"kwargs\": {\"allow_tf32\": \"True\"}}'\n+#\n+# for decompose_k backend_key:\n+# - \"config\": a list with a single value representing the k decomposition factor\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [4], \"kwargs\": {}}'\n+#\n+# Other operations may have different config structures as needed.\n+kernel_config_lookup_table: Optional[\n+    dict[str, dict[str, dict[str, dict[str, str]]]]\n+] = None\n+\n+\n+def _gemm_lookup_key(input_nodes: list[Any]) -> str:\n+    return str(\n+        tuple(\n+            # List here, because we want a standard encoding e.g. [] instead of ()\n+            (node.get_dtype(), list(get_size_hint(node)), list(get_stride_hint(node)))\n+            for node in input_nodes\n+        )\n+    )\n+\n+\n+def get_gemm_lookup_table(\n+    input_nodes: list[Any], method: str\n+) -> Optional[dict[str, str]]:\n+    lookup_dict = None\n+    lookup_table = _get_lookup_table()\n+    if _in_use() and lookup_table is not None:\n+        # Assume the first input parameter is used to determine the device\n+        device = input_nodes[0].get_device()\n+        device_name = (\n+            torch.cuda.get_device_name(device.index) if device.type == \"cuda\" else \"cpu\"",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2191084365",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157699,
        "pr_file": "torch/_inductor/lookup_table.py",
        "discussion_id": "2191084365",
        "commented_code": "@@ -0,0 +1,224 @@\n+import json\n+import logging\n+from functools import lru_cache\n+from typing import Any, Optional\n+\n+import torch\n+from torch._inductor.virtualized import V\n+\n+from . import config as inductor_config\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def _in_use() -> bool:\n+    \"\"\"\n+    Determine if the gemm config lookup table should be used.\n+    This system is only used when cuda is available\n+    Returns True if either:\n+    1. The kernel_config_lookup_table_path is set, OR\n+    2. The kernel_config_lookup_table global has been set\n+    \"\"\"\n+    return torch.cuda.is_available() and (\n+        inductor_config.triton.kernel_config_lookup_table_path is not None\n+        or kernel_config_lookup_table is not None\n+    )\n+\n+\n+@lru_cache(0)\n+def _read_lookup_table(\n+    path: str,\n+) -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Handle actual file reading, so this can be cached depending on the input path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+    # Assert supported file extensions\n+    if path.endswith(\".json\"):\n+        with open(path) as f:\n+            table = json.load(f)\n+    elif path.endswith(\".yaml\") or path.endswith(\".yml\"):\n+        try:\n+            import yaml\n+        except ImportError:\n+            raise ImportError(\n+                \"PyYAML is required to load YAML files. Install with: pip install PyYAML, or convert your file to JSON format instead.\"\n+            )\n+        with open(path) as f:\n+            table = yaml.safe_load(f)\n+    else:\n+        raise AssertionError(\n+            f\"Unsupported file format. Only .json and .yaml/.yml files are supported. Got: {path}\"\n+        )\n+    return table\n+\n+\n+def _get_lookup_table() -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Load and return the gemm config lookup table from file if configured.\n+\n+    If the table is already defined, this takes precedence over the file path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+\n+    # If table is directly defined, use that\n+    if kernel_config_lookup_table is not None:\n+        # Log warning if both table and path are defined\n+        if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+            log.warning(\n+                \"Both kernel_config_lookup_table and kernel_config_lookup_table_path are defined. \"\n+                \"Ignoring the path because a table is already defined.\"\n+            )\n+        return kernel_config_lookup_table\n+\n+    # Otherwise use the path if it's defined\n+    if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+        return _read_lookup_table(\n+            inductor_config.triton.kernel_config_lookup_table_path\n+        )\n+\n+    return None\n+\n+\n+# A static lookup table for (triton) configurations for GEMMs.\n+# This is a lookup table in the form of\n+# [op][device name][input_nodes_key][backend_key] = JSON_string\n+# where the JSON string format is operation-dependent:\n+#\n+# For MM family operations (mm, bmm, addmm, mm_plus_mm):\n+\n+# for triton and tma backend_key:\n+# - \"config\": a list of GemmConfig parameters [BLOCK_M, BLOCK_N, BLOCK_K, num_stages, num_warps]\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [128, 128, 64, 2, 2], \"kwargs\": {\"allow_tf32\": \"True\"}}'\n+#\n+# for decompose_k backend_key:\n+# - \"config\": a list with a single value representing the k decomposition factor\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [4], \"kwargs\": {}}'\n+#\n+# Other operations may have different config structures as needed.\n+kernel_config_lookup_table: Optional[\n+    dict[str, dict[str, dict[str, dict[str, str]]]]\n+] = None\n+\n+\n+def _gemm_lookup_key(input_nodes: list[Any]) -> str:\n+    return str(\n+        tuple(\n+            # List here, because we want a standard encoding e.g. [] instead of ()\n+            (node.get_dtype(), list(get_size_hint(node)), list(get_stride_hint(node)))\n+            for node in input_nodes\n+        )\n+    )\n+\n+\n+def get_gemm_lookup_table(\n+    input_nodes: list[Any], method: str\n+) -> Optional[dict[str, str]]:\n+    lookup_dict = None\n+    lookup_table = _get_lookup_table()\n+    if _in_use() and lookup_table is not None:\n+        # Assume the first input parameter is used to determine the device\n+        device = input_nodes[0].get_device()\n+        device_name = (\n+            torch.cuda.get_device_name(device.index) if device.type == \"cuda\" else \"cpu\"",
        "comment_created_at": "2025-07-07T22:12:19+00:00",
        "comment_author": "coconutruben",
        "comment_body": "@jansel right now we're indexing on `torch.cuda.get_device_name` which I assume is a concise way of getting the information we need. We might need `torch.cuda.get_device_properties(device).gcnArchName` to properly handle this on AMD as well. Is your feedback here, that we should do this, rather than on the name, on the entire `torch.cuda.get_device_properties(device)` instead? do we suspect that there could be two GPUs with the same name but with different properties?",
        "pr_file_module": null
      },
      {
        "comment_id": "2191111034",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157699,
        "pr_file": "torch/_inductor/lookup_table.py",
        "discussion_id": "2191084365",
        "commented_code": "@@ -0,0 +1,224 @@\n+import json\n+import logging\n+from functools import lru_cache\n+from typing import Any, Optional\n+\n+import torch\n+from torch._inductor.virtualized import V\n+\n+from . import config as inductor_config\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def _in_use() -> bool:\n+    \"\"\"\n+    Determine if the gemm config lookup table should be used.\n+    This system is only used when cuda is available\n+    Returns True if either:\n+    1. The kernel_config_lookup_table_path is set, OR\n+    2. The kernel_config_lookup_table global has been set\n+    \"\"\"\n+    return torch.cuda.is_available() and (\n+        inductor_config.triton.kernel_config_lookup_table_path is not None\n+        or kernel_config_lookup_table is not None\n+    )\n+\n+\n+@lru_cache(0)\n+def _read_lookup_table(\n+    path: str,\n+) -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Handle actual file reading, so this can be cached depending on the input path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+    # Assert supported file extensions\n+    if path.endswith(\".json\"):\n+        with open(path) as f:\n+            table = json.load(f)\n+    elif path.endswith(\".yaml\") or path.endswith(\".yml\"):\n+        try:\n+            import yaml\n+        except ImportError:\n+            raise ImportError(\n+                \"PyYAML is required to load YAML files. Install with: pip install PyYAML, or convert your file to JSON format instead.\"\n+            )\n+        with open(path) as f:\n+            table = yaml.safe_load(f)\n+    else:\n+        raise AssertionError(\n+            f\"Unsupported file format. Only .json and .yaml/.yml files are supported. Got: {path}\"\n+        )\n+    return table\n+\n+\n+def _get_lookup_table() -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Load and return the gemm config lookup table from file if configured.\n+\n+    If the table is already defined, this takes precedence over the file path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+\n+    # If table is directly defined, use that\n+    if kernel_config_lookup_table is not None:\n+        # Log warning if both table and path are defined\n+        if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+            log.warning(\n+                \"Both kernel_config_lookup_table and kernel_config_lookup_table_path are defined. \"\n+                \"Ignoring the path because a table is already defined.\"\n+            )\n+        return kernel_config_lookup_table\n+\n+    # Otherwise use the path if it's defined\n+    if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+        return _read_lookup_table(\n+            inductor_config.triton.kernel_config_lookup_table_path\n+        )\n+\n+    return None\n+\n+\n+# A static lookup table for (triton) configurations for GEMMs.\n+# This is a lookup table in the form of\n+# [op][device name][input_nodes_key][backend_key] = JSON_string\n+# where the JSON string format is operation-dependent:\n+#\n+# For MM family operations (mm, bmm, addmm, mm_plus_mm):\n+\n+# for triton and tma backend_key:\n+# - \"config\": a list of GemmConfig parameters [BLOCK_M, BLOCK_N, BLOCK_K, num_stages, num_warps]\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [128, 128, 64, 2, 2], \"kwargs\": {\"allow_tf32\": \"True\"}}'\n+#\n+# for decompose_k backend_key:\n+# - \"config\": a list with a single value representing the k decomposition factor\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [4], \"kwargs\": {}}'\n+#\n+# Other operations may have different config structures as needed.\n+kernel_config_lookup_table: Optional[\n+    dict[str, dict[str, dict[str, dict[str, str]]]]\n+] = None\n+\n+\n+def _gemm_lookup_key(input_nodes: list[Any]) -> str:\n+    return str(\n+        tuple(\n+            # List here, because we want a standard encoding e.g. [] instead of ()\n+            (node.get_dtype(), list(get_size_hint(node)), list(get_stride_hint(node)))\n+            for node in input_nodes\n+        )\n+    )\n+\n+\n+def get_gemm_lookup_table(\n+    input_nodes: list[Any], method: str\n+) -> Optional[dict[str, str]]:\n+    lookup_dict = None\n+    lookup_table = _get_lookup_table()\n+    if _in_use() and lookup_table is not None:\n+        # Assume the first input parameter is used to determine the device\n+        device = input_nodes[0].get_device()\n+        device_name = (\n+            torch.cuda.get_device_name(device.index) if device.type == \"cuda\" else \"cpu\"",
        "comment_created_at": "2025-07-07T22:36:09+00:00",
        "comment_author": "coconutruben",
        "comment_body": "```\r\n|  L2_cache_size\r\n |  gcnArchName\r\n |  is_integrated\r\n |  is_multi_gpu_board\r\n |  major\r\n |  max_threads_per_multi_processor\r\n |  minor\r\n |  multi_processor_count\r\n |  name\r\n |  pci_bus_id\r\n |  pci_device_id\r\n |  pci_domain_id\r\n |  regs_per_multiprocessor\r\n |  shared_memory_per_block\r\n |  shared_memory_per_block_optin\r\n |  shared_memory_per_multiprocessor\r\n |  total_memory\r\n |  uuid\r\n |  warp_size\r\n ```\r\n is the full list of properties on H100. Same on A100. \r\n ```\r\n  |  shared_memory_per_block\r\n |  shared_memory_per_block_optin\r\n |  shared_memory_per_multiprocessor\r\n ```\r\n are not available on AMD\r\n \r\n If we don't want to key on just the name we can do one of 2 things by defining a static list of which properties to key on (if available)\r\n - have a sorted string of key/value pairs as a dict of sorts that we can then generate a key with\r\n - make a hash out of it\r\n \r\n thoughts? hash is prettier but harder to debug",
        "pr_file_module": null
      },
      {
        "comment_id": "2191113390",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157699,
        "pr_file": "torch/_inductor/lookup_table.py",
        "discussion_id": "2191084365",
        "commented_code": "@@ -0,0 +1,224 @@\n+import json\n+import logging\n+from functools import lru_cache\n+from typing import Any, Optional\n+\n+import torch\n+from torch._inductor.virtualized import V\n+\n+from . import config as inductor_config\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def _in_use() -> bool:\n+    \"\"\"\n+    Determine if the gemm config lookup table should be used.\n+    This system is only used when cuda is available\n+    Returns True if either:\n+    1. The kernel_config_lookup_table_path is set, OR\n+    2. The kernel_config_lookup_table global has been set\n+    \"\"\"\n+    return torch.cuda.is_available() and (\n+        inductor_config.triton.kernel_config_lookup_table_path is not None\n+        or kernel_config_lookup_table is not None\n+    )\n+\n+\n+@lru_cache(0)\n+def _read_lookup_table(\n+    path: str,\n+) -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Handle actual file reading, so this can be cached depending on the input path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+    # Assert supported file extensions\n+    if path.endswith(\".json\"):\n+        with open(path) as f:\n+            table = json.load(f)\n+    elif path.endswith(\".yaml\") or path.endswith(\".yml\"):\n+        try:\n+            import yaml\n+        except ImportError:\n+            raise ImportError(\n+                \"PyYAML is required to load YAML files. Install with: pip install PyYAML, or convert your file to JSON format instead.\"\n+            )\n+        with open(path) as f:\n+            table = yaml.safe_load(f)\n+    else:\n+        raise AssertionError(\n+            f\"Unsupported file format. Only .json and .yaml/.yml files are supported. Got: {path}\"\n+        )\n+    return table\n+\n+\n+def _get_lookup_table() -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Load and return the gemm config lookup table from file if configured.\n+\n+    If the table is already defined, this takes precedence over the file path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+\n+    # If table is directly defined, use that\n+    if kernel_config_lookup_table is not None:\n+        # Log warning if both table and path are defined\n+        if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+            log.warning(\n+                \"Both kernel_config_lookup_table and kernel_config_lookup_table_path are defined. \"\n+                \"Ignoring the path because a table is already defined.\"\n+            )\n+        return kernel_config_lookup_table\n+\n+    # Otherwise use the path if it's defined\n+    if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+        return _read_lookup_table(\n+            inductor_config.triton.kernel_config_lookup_table_path\n+        )\n+\n+    return None\n+\n+\n+# A static lookup table for (triton) configurations for GEMMs.\n+# This is a lookup table in the form of\n+# [op][device name][input_nodes_key][backend_key] = JSON_string\n+# where the JSON string format is operation-dependent:\n+#\n+# For MM family operations (mm, bmm, addmm, mm_plus_mm):\n+\n+# for triton and tma backend_key:\n+# - \"config\": a list of GemmConfig parameters [BLOCK_M, BLOCK_N, BLOCK_K, num_stages, num_warps]\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [128, 128, 64, 2, 2], \"kwargs\": {\"allow_tf32\": \"True\"}}'\n+#\n+# for decompose_k backend_key:\n+# - \"config\": a list with a single value representing the k decomposition factor\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [4], \"kwargs\": {}}'\n+#\n+# Other operations may have different config structures as needed.\n+kernel_config_lookup_table: Optional[\n+    dict[str, dict[str, dict[str, dict[str, str]]]]\n+] = None\n+\n+\n+def _gemm_lookup_key(input_nodes: list[Any]) -> str:\n+    return str(\n+        tuple(\n+            # List here, because we want a standard encoding e.g. [] instead of ()\n+            (node.get_dtype(), list(get_size_hint(node)), list(get_stride_hint(node)))\n+            for node in input_nodes\n+        )\n+    )\n+\n+\n+def get_gemm_lookup_table(\n+    input_nodes: list[Any], method: str\n+) -> Optional[dict[str, str]]:\n+    lookup_dict = None\n+    lookup_table = _get_lookup_table()\n+    if _in_use() and lookup_table is not None:\n+        # Assume the first input parameter is used to determine the device\n+        device = input_nodes[0].get_device()\n+        device_name = (\n+            torch.cuda.get_device_name(device.index) if device.type == \"cuda\" else \"cpu\"",
        "comment_created_at": "2025-07-07T22:37:13+00:00",
        "comment_author": "coconutruben",
        "comment_body": "presumably the things we'd want to key on are\r\n\r\n```\r\n |  L2_cache_size\r\n |  gcnArchName\r\n |  major\r\n |  max_threads_per_multi_processor\r\n |  minor\r\n |  multi_processor_count\r\n |  regs_per_multiprocessor\r\n |  shared_memory_per_block\r\n |  shared_memory_per_block_optin\r\n |  shared_memory_per_multiprocessor\r\n |  total_memory\r\n |  warp_size\r\n ```",
        "pr_file_module": null
      },
      {
        "comment_id": "2191229691",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157699,
        "pr_file": "torch/_inductor/lookup_table.py",
        "discussion_id": "2191084365",
        "commented_code": "@@ -0,0 +1,224 @@\n+import json\n+import logging\n+from functools import lru_cache\n+from typing import Any, Optional\n+\n+import torch\n+from torch._inductor.virtualized import V\n+\n+from . import config as inductor_config\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def _in_use() -> bool:\n+    \"\"\"\n+    Determine if the gemm config lookup table should be used.\n+    This system is only used when cuda is available\n+    Returns True if either:\n+    1. The kernel_config_lookup_table_path is set, OR\n+    2. The kernel_config_lookup_table global has been set\n+    \"\"\"\n+    return torch.cuda.is_available() and (\n+        inductor_config.triton.kernel_config_lookup_table_path is not None\n+        or kernel_config_lookup_table is not None\n+    )\n+\n+\n+@lru_cache(0)\n+def _read_lookup_table(\n+    path: str,\n+) -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Handle actual file reading, so this can be cached depending on the input path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+    # Assert supported file extensions\n+    if path.endswith(\".json\"):\n+        with open(path) as f:\n+            table = json.load(f)\n+    elif path.endswith(\".yaml\") or path.endswith(\".yml\"):\n+        try:\n+            import yaml\n+        except ImportError:\n+            raise ImportError(\n+                \"PyYAML is required to load YAML files. Install with: pip install PyYAML, or convert your file to JSON format instead.\"\n+            )\n+        with open(path) as f:\n+            table = yaml.safe_load(f)\n+    else:\n+        raise AssertionError(\n+            f\"Unsupported file format. Only .json and .yaml/.yml files are supported. Got: {path}\"\n+        )\n+    return table\n+\n+\n+def _get_lookup_table() -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Load and return the gemm config lookup table from file if configured.\n+\n+    If the table is already defined, this takes precedence over the file path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+\n+    # If table is directly defined, use that\n+    if kernel_config_lookup_table is not None:\n+        # Log warning if both table and path are defined\n+        if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+            log.warning(\n+                \"Both kernel_config_lookup_table and kernel_config_lookup_table_path are defined. \"\n+                \"Ignoring the path because a table is already defined.\"\n+            )\n+        return kernel_config_lookup_table\n+\n+    # Otherwise use the path if it's defined\n+    if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+        return _read_lookup_table(\n+            inductor_config.triton.kernel_config_lookup_table_path\n+        )\n+\n+    return None\n+\n+\n+# A static lookup table for (triton) configurations for GEMMs.\n+# This is a lookup table in the form of\n+# [op][device name][input_nodes_key][backend_key] = JSON_string\n+# where the JSON string format is operation-dependent:\n+#\n+# For MM family operations (mm, bmm, addmm, mm_plus_mm):\n+\n+# for triton and tma backend_key:\n+# - \"config\": a list of GemmConfig parameters [BLOCK_M, BLOCK_N, BLOCK_K, num_stages, num_warps]\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [128, 128, 64, 2, 2], \"kwargs\": {\"allow_tf32\": \"True\"}}'\n+#\n+# for decompose_k backend_key:\n+# - \"config\": a list with a single value representing the k decomposition factor\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [4], \"kwargs\": {}}'\n+#\n+# Other operations may have different config structures as needed.\n+kernel_config_lookup_table: Optional[\n+    dict[str, dict[str, dict[str, dict[str, str]]]]\n+] = None\n+\n+\n+def _gemm_lookup_key(input_nodes: list[Any]) -> str:\n+    return str(\n+        tuple(\n+            # List here, because we want a standard encoding e.g. [] instead of ()\n+            (node.get_dtype(), list(get_size_hint(node)), list(get_stride_hint(node)))\n+            for node in input_nodes\n+        )\n+    )\n+\n+\n+def get_gemm_lookup_table(\n+    input_nodes: list[Any], method: str\n+) -> Optional[dict[str, str]]:\n+    lookup_dict = None\n+    lookup_table = _get_lookup_table()\n+    if _in_use() and lookup_table is not None:\n+        # Assume the first input parameter is used to determine the device\n+        device = input_nodes[0].get_device()\n+        device_name = (\n+            torch.cuda.get_device_name(device.index) if device.type == \"cuda\" else \"cpu\"",
        "comment_created_at": "2025-07-08T00:12:07+00:00",
        "comment_author": "coconutruben",
        "comment_body": "in the latest version, I have swapped this out for _dev_key which is now a dict of sorted keys of those capabilities and that works quite well. I also swapped the order from `[op][device key]` to `[device key][op]` as the device key is more verbose now, lmk if you see improvement opportunities there",
        "pr_file_module": null
      },
      {
        "comment_id": "2191503734",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157699,
        "pr_file": "torch/_inductor/lookup_table.py",
        "discussion_id": "2191084365",
        "commented_code": "@@ -0,0 +1,224 @@\n+import json\n+import logging\n+from functools import lru_cache\n+from typing import Any, Optional\n+\n+import torch\n+from torch._inductor.virtualized import V\n+\n+from . import config as inductor_config\n+\n+\n+log = logging.getLogger(__name__)\n+\n+\n+def _in_use() -> bool:\n+    \"\"\"\n+    Determine if the gemm config lookup table should be used.\n+    This system is only used when cuda is available\n+    Returns True if either:\n+    1. The kernel_config_lookup_table_path is set, OR\n+    2. The kernel_config_lookup_table global has been set\n+    \"\"\"\n+    return torch.cuda.is_available() and (\n+        inductor_config.triton.kernel_config_lookup_table_path is not None\n+        or kernel_config_lookup_table is not None\n+    )\n+\n+\n+@lru_cache(0)\n+def _read_lookup_table(\n+    path: str,\n+) -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Handle actual file reading, so this can be cached depending on the input path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+    # Assert supported file extensions\n+    if path.endswith(\".json\"):\n+        with open(path) as f:\n+            table = json.load(f)\n+    elif path.endswith(\".yaml\") or path.endswith(\".yml\"):\n+        try:\n+            import yaml\n+        except ImportError:\n+            raise ImportError(\n+                \"PyYAML is required to load YAML files. Install with: pip install PyYAML, or convert your file to JSON format instead.\"\n+            )\n+        with open(path) as f:\n+            table = yaml.safe_load(f)\n+    else:\n+        raise AssertionError(\n+            f\"Unsupported file format. Only .json and .yaml/.yml files are supported. Got: {path}\"\n+        )\n+    return table\n+\n+\n+def _get_lookup_table() -> Optional[dict[str, dict[str, dict[str, dict[str, str]]]]]:\n+    \"\"\"\n+    Load and return the gemm config lookup table from file if configured.\n+\n+    If the table is already defined, this takes precedence over the file path\n+    \"\"\"\n+    if not _in_use():\n+        return None\n+\n+    # If table is directly defined, use that\n+    if kernel_config_lookup_table is not None:\n+        # Log warning if both table and path are defined\n+        if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+            log.warning(\n+                \"Both kernel_config_lookup_table and kernel_config_lookup_table_path are defined. \"\n+                \"Ignoring the path because a table is already defined.\"\n+            )\n+        return kernel_config_lookup_table\n+\n+    # Otherwise use the path if it's defined\n+    if inductor_config.triton.kernel_config_lookup_table_path is not None:\n+        return _read_lookup_table(\n+            inductor_config.triton.kernel_config_lookup_table_path\n+        )\n+\n+    return None\n+\n+\n+# A static lookup table for (triton) configurations for GEMMs.\n+# This is a lookup table in the form of\n+# [op][device name][input_nodes_key][backend_key] = JSON_string\n+# where the JSON string format is operation-dependent:\n+#\n+# For MM family operations (mm, bmm, addmm, mm_plus_mm):\n+\n+# for triton and tma backend_key:\n+# - \"config\": a list of GemmConfig parameters [BLOCK_M, BLOCK_N, BLOCK_K, num_stages, num_warps]\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [128, 128, 64, 2, 2], \"kwargs\": {\"allow_tf32\": \"True\"}}'\n+#\n+# for decompose_k backend_key:\n+# - \"config\": a list with a single value representing the k decomposition factor\n+# - \"kwargs\": an optional dictionary of additional keyword arguments (defaults to empty dict if not present)\n+# Example: '{\"config\": [4], \"kwargs\": {}}'\n+#\n+# Other operations may have different config structures as needed.\n+kernel_config_lookup_table: Optional[\n+    dict[str, dict[str, dict[str, dict[str, str]]]]\n+] = None\n+\n+\n+def _gemm_lookup_key(input_nodes: list[Any]) -> str:\n+    return str(\n+        tuple(\n+            # List here, because we want a standard encoding e.g. [] instead of ()\n+            (node.get_dtype(), list(get_size_hint(node)), list(get_stride_hint(node)))\n+            for node in input_nodes\n+        )\n+    )\n+\n+\n+def get_gemm_lookup_table(\n+    input_nodes: list[Any], method: str\n+) -> Optional[dict[str, str]]:\n+    lookup_dict = None\n+    lookup_table = _get_lookup_table()\n+    if _in_use() and lookup_table is not None:\n+        # Assume the first input parameter is used to determine the device\n+        device = input_nodes[0].get_device()\n+        device_name = (\n+            torch.cuda.get_device_name(device.index) if device.type == \"cuda\" else \"cpu\"",
        "comment_created_at": "2025-07-08T05:22:46+00:00",
        "comment_author": "jansel",
        "comment_body": "Maybe we could use `torch.cuda.get_device_capability()` -- that or the gcnArchName that should be enough to tell major hardware versions apart.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1861218181",
    "pr_number": 140979,
    "pr_file": "torch/_higher_order_ops/cond.py",
    "created_at": "2024-11-27T20:33:38+00:00",
    "commented_code": "), f\"Dense implementation operands must be a list of tensors and ints {operands}\"\n     mode = _get_current_dispatch_mode()\n     assert mode is None, \"Mode should never be enabled for CPU/CUDA key\"\n-    if not torch.cuda.is_current_stream_capturing():\n+    if not (torch.cuda.is_available() and torch.cuda.is_current_stream_capturing()):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1861218181",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 140979,
        "pr_file": "torch/_higher_order_ops/cond.py",
        "discussion_id": "1861218181",
        "commented_code": "@@ -371,7 +371,7 @@ def cond_op_dense(pred, true_fn, false_fn, operands):\n     ), f\"Dense implementation operands must be a list of tensors and ints {operands}\"\n     mode = _get_current_dispatch_mode()\n     assert mode is None, \"Mode should never be enabled for CPU/CUDA key\"\n-    if not torch.cuda.is_current_stream_capturing():\n+    if not (torch.cuda.is_available() and torch.cuda.is_current_stream_capturing()):",
        "comment_created_at": "2024-11-27T20:33:38+00:00",
        "comment_author": "eellison",
        "comment_body": "Since we're already making a custom TorchDispatchMode to do the warmup, could we put this logic in that class to avoid cudagraphs leaking here ?",
        "pr_file_module": null
      },
      {
        "comment_id": "1865300817",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 140979,
        "pr_file": "torch/_higher_order_ops/cond.py",
        "discussion_id": "1861218181",
        "commented_code": "@@ -371,7 +371,7 @@ def cond_op_dense(pred, true_fn, false_fn, operands):\n     ), f\"Dense implementation operands must be a list of tensors and ints {operands}\"\n     mode = _get_current_dispatch_mode()\n     assert mode is None, \"Mode should never be enabled for CPU/CUDA key\"\n-    if not torch.cuda.is_current_stream_capturing():\n+    if not (torch.cuda.is_available() and torch.cuda.is_current_stream_capturing()):",
        "comment_created_at": "2024-12-02T06:39:10+00:00",
        "comment_author": "galv",
        "comment_body": "Good idea. I think I encountered some weirdness, that is described here: https://github.com/pytorch/pytorch/pull/140979/files#diff-4093d24068320f82997aecfaf4260aa495e3b4322e178cb5625f9555e3f8b0f5R388-R392\r\n\r\nand here:\r\n\r\nhttps://github.com/pytorch/pytorch/issues/140322\r\n\r\nBut let me take a second look. I would be happy to make that change, assuming there is nothing standing in the way.",
        "pr_file_module": null
      },
      {
        "comment_id": "1866808555",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 140979,
        "pr_file": "torch/_higher_order_ops/cond.py",
        "discussion_id": "1861218181",
        "commented_code": "@@ -371,7 +371,7 @@ def cond_op_dense(pred, true_fn, false_fn, operands):\n     ), f\"Dense implementation operands must be a list of tensors and ints {operands}\"\n     mode = _get_current_dispatch_mode()\n     assert mode is None, \"Mode should never be enabled for CPU/CUDA key\"\n-    if not torch.cuda.is_current_stream_capturing():\n+    if not (torch.cuda.is_available() and torch.cuda.is_current_stream_capturing()):",
        "comment_created_at": "2024-12-03T00:36:16+00:00",
        "comment_author": "eellison",
        "comment_body": "If https://github.com/pytorch/pytorch/issues/140322 is blocking we can do as a follow up",
        "pr_file_module": null
      },
      {
        "comment_id": "1868639365",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 140979,
        "pr_file": "torch/_higher_order_ops/cond.py",
        "discussion_id": "1861218181",
        "commented_code": "@@ -371,7 +371,7 @@ def cond_op_dense(pred, true_fn, false_fn, operands):\n     ), f\"Dense implementation operands must be a list of tensors and ints {operands}\"\n     mode = _get_current_dispatch_mode()\n     assert mode is None, \"Mode should never be enabled for CPU/CUDA key\"\n-    if not torch.cuda.is_current_stream_capturing():\n+    if not (torch.cuda.is_available() and torch.cuda.is_current_stream_capturing()):",
        "comment_created_at": "2024-12-04T02:41:41+00:00",
        "comment_author": "galv",
        "comment_body": "I did what you suggested. Unfortunately we cannot use code within `__torch_dispatch__` right now to dispatch ops in a different way, but we can use py_impl(MyTorchDispatchMode) to do this right now, so I went with that workaround, though it is a little bit ugly because it causes some circular dependencies and separates code that should all sit in one single class. It should probably be redone once #140322 is handled. You can see how I do it for cond here: https://github.com/pytorch/pytorch/pull/140979/files#diff-4093d24068320f82997aecfaf4260aa495e3b4322e178cb5625f9555e3f8b0f5R384-R408\r\n\r\nNote that I use two TorchDispatchModes, one for warmup, and one for making sure that I do stream capture appropriately for conditional nodes. I automatically enter CUDAGraphCaptureControlFlowOpDispatchMode in the __enter__ method of torch.cuda.graph. This unfortunately means that users who use CUDAGraph.capture_begin() and CUDAGraph.capture_end() instead of torch.cuda.graph() won't be able to capture conditional ops. I don't think this is a problem to be honest. Do you think it is?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2042240684",
    "pr_number": 151205,
    "pr_file": "torch/masked/maskedtensor/core.py",
    "created_at": "2025-04-14T14:18:32+00:00",
    "commented_code": "@property\n     def is_sparse(self):\n         return self.is_sparse_coo() or self.is_sparse_csr()\n+\n+    def to(self, *args, **kwargs):\n+        current_device = self._masked_data.device\n+        device_to = current_device\n+        if len(args) == 1:\n+            arg = args[0]\n+            if isinstance(arg, (torch.device, str, int)):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2042240684",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 151205,
        "pr_file": "torch/masked/maskedtensor/core.py",
        "discussion_id": "2042240684",
        "commented_code": "@@ -357,3 +357,17 @@ def is_sparse_csr(self):  # type: ignore[override]\n     @property\n     def is_sparse(self):\n         return self.is_sparse_coo() or self.is_sparse_csr()\n+\n+    def to(self, *args, **kwargs):\n+        current_device = self._masked_data.device\n+        device_to = current_device\n+        if len(args) == 1:\n+            arg = args[0]\n+            if isinstance(arg, (torch.device, str, int)):",
        "comment_created_at": "2025-04-14T14:18:32+00:00",
        "comment_author": "Skylion007",
        "comment_body": "Use DeviceLIke from torch._prims_common and maybe use cannonicalize_device below?",
        "pr_file_module": null
      },
      {
        "comment_id": "2043400103",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 151205,
        "pr_file": "torch/masked/maskedtensor/core.py",
        "discussion_id": "2042240684",
        "commented_code": "@@ -357,3 +357,17 @@ def is_sparse_csr(self):  # type: ignore[override]\n     @property\n     def is_sparse(self):\n         return self.is_sparse_coo() or self.is_sparse_csr()\n+\n+    def to(self, *args, **kwargs):\n+        current_device = self._masked_data.device\n+        device_to = current_device\n+        if len(args) == 1:\n+            arg = args[0]\n+            if isinstance(arg, (torch.device, str, int)):",
        "comment_created_at": "2025-04-15T02:43:24+00:00",
        "comment_author": "zeshengzong",
        "comment_body": "Hi, do you mean using `DeviceLikeType` like this?\r\n\r\n```python\r\n    def to(self, *args, **kwargs):\r\n        current_device = self._masked_data.device\r\n        device_to = current_device\r\n        if len(args) == 1:\r\n            arg = args[0]\r\n            if isinstance(arg, DeviceLikeType):\r\n                device_to = canonicalize_device(arg)\r\n```\r\n\r\nBut mypy seems unhapply about it\r\n\r\n```bash\r\n>>> Lint for torch/masked/maskedtensor/core.py:\r\n\r\n  Error (MYPY) [misc]\r\n    Parameterized generics cannot be used with class or instance checks\r\n\r\n        364  |        device_to = current_device\r\n        365  |        if len(args) == 1:\r\n        366  |            arg = args[0]\r\n    >>> 367  |            if isinstance(arg, DeviceLikeType):\r\n        368  |                device_to = canonicalize_device(arg)\r\n        369  |            elif isinstance(arg, torch.Tensor):\r\n        370  |                device_to = arg.device\r\n\r\n  Error (MYPY) [arg-type]\r\n    Argument 2 to \"isinstance\" has incompatible type \"<typing special form>\";\r\n    expected \"_ClassInfo\"\r\n\r\n        364  |        device_to = current_device\r\n        365  |        if len(args) == 1:\r\n        366  |            arg = args[0]\r\n    >>> 367  |            if isinstance(arg, DeviceLikeType):\r\n        368  |                device_to = canonicalize_device(arg)\r\n        369  |            elif isinstance(arg, torch.Tensor):\r\n        370  |                device_to = arg.device\r\n\r\n```\r\n\r\nAnd found a solution in here https://github.com/python/mypy/issues/12155#issuecomment-1660655381",
        "pr_file_module": null
      }
    ]
  }
]