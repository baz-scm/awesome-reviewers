[
  {
    "discussion_id": "2191326859",
    "pr_number": 157658,
    "pr_file": "aten/src/ATen/native/Distributions.cpp",
    "created_at": "2025-07-08T02:04:11+00:00",
    "commented_code": "*/\n \n Tensor _s_binomial_cpu(const Tensor& count, const Tensor& prob, std::optional<Generator> gen) {\n+  TORCH_CHECK(\n+      at::isFloatingType(count.scalar_type()),\n+      \"binomial only supports floating-point dtypes for count, got: \",\n+      count.scalar_type());\n+  TORCH_CHECK(",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2191326859",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157658,
        "pr_file": "aten/src/ATen/native/Distributions.cpp",
        "discussion_id": "2191326859",
        "commented_code": "@@ -424,6 +424,14 @@ Tensor _dirichlet_grad_cpu(const Tensor& x, const Tensor& alpha, const Tensor& t\n  */\n \n Tensor _s_binomial_cpu(const Tensor& count, const Tensor& prob, std::optional<Generator> gen) {\n+  TORCH_CHECK(\n+      at::isFloatingType(count.scalar_type()),\n+      \"binomial only supports floating-point dtypes for count, got: \",\n+      count.scalar_type());\n+  TORCH_CHECK(",
        "comment_created_at": "2025-07-08T02:04:11+00:00",
        "comment_author": "malfet",
        "comment_body": "If some values are unsupported, than ValueError is more appropriate than RuntimeError, don't you think?\r\n\r\n```suggestion\r\n  TORCH_CHECK_VALUE(\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2117901542",
    "pr_number": 154759,
    "pr_file": "aten/src/ATen/native/sparse/SparseTensor.cpp",
    "created_at": "2025-05-31T14:24:00+00:00",
    "commented_code": "\"), but got \",\n       size.size());\n \n-  TORCH_CHECK(\n-      indices.is_pinned() == values.is_pinned(),\n-      \"memory pinning of indices (=\",\n-      indices.is_pinned(),\n-      \") must match memory pinning of values (=\",\n-      values.is_pinned(),\n-      \")\");\n+  if (check_pinning) {\n+    TORCH_CHECK(",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2117901542",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154759,
        "pr_file": "aten/src/ATen/native/sparse/SparseTensor.cpp",
        "discussion_id": "2117901542",
        "commented_code": "@@ -397,13 +399,15 @@ void _validate_sparse_coo_tensor_args(\n       \"), but got \",\n       size.size());\n \n-  TORCH_CHECK(\n-      indices.is_pinned() == values.is_pinned(),\n-      \"memory pinning of indices (=\",\n-      indices.is_pinned(),\n-      \") must match memory pinning of values (=\",\n-      values.is_pinned(),\n-      \")\");\n+  if (check_pinning) {\n+    TORCH_CHECK(",
        "comment_created_at": "2025-05-31T14:24:00+00:00",
        "comment_author": "Skylion007",
        "comment_body": "Nit should probably be an assert error more than a RuntimeError, right?\r\n```suggestion\r\n    TORCH_CHECK_ASSERT(",
        "pr_file_module": null
      },
      {
        "comment_id": "2117936787",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154759,
        "pr_file": "aten/src/ATen/native/sparse/SparseTensor.cpp",
        "discussion_id": "2117901542",
        "commented_code": "@@ -397,13 +399,15 @@ void _validate_sparse_coo_tensor_args(\n       \"), but got \",\n       size.size());\n \n-  TORCH_CHECK(\n-      indices.is_pinned() == values.is_pinned(),\n-      \"memory pinning of indices (=\",\n-      indices.is_pinned(),\n-      \") must match memory pinning of values (=\",\n-      values.is_pinned(),\n-      \")\");\n+  if (check_pinning) {\n+    TORCH_CHECK(",
        "comment_created_at": "2025-05-31T14:59:22+00:00",
        "comment_author": "pearu",
        "comment_body": "`TORCH_CHECK` should be appropriate here as we are checking user inputs rather than the coherence of pytorch internals.",
        "pr_file_module": null
      },
      {
        "comment_id": "2118133945",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154759,
        "pr_file": "aten/src/ATen/native/sparse/SparseTensor.cpp",
        "discussion_id": "2117901542",
        "commented_code": "@@ -397,13 +399,15 @@ void _validate_sparse_coo_tensor_args(\n       \"), but got \",\n       size.size());\n \n-  TORCH_CHECK(\n-      indices.is_pinned() == values.is_pinned(),\n-      \"memory pinning of indices (=\",\n-      indices.is_pinned(),\n-      \") must match memory pinning of values (=\",\n-      values.is_pinned(),\n-      \")\");\n+  if (check_pinning) {\n+    TORCH_CHECK(",
        "comment_created_at": "2025-05-31T18:03:06+00:00",
        "comment_author": "Skylion007",
        "comment_body": "@pearu If we are cehcking user inputs, shouldn't we raise a ValueError?",
        "pr_file_module": null
      },
      {
        "comment_id": "2118738286",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154759,
        "pr_file": "aten/src/ATen/native/sparse/SparseTensor.cpp",
        "discussion_id": "2117901542",
        "commented_code": "@@ -397,13 +399,15 @@ void _validate_sparse_coo_tensor_args(\n       \"), but got \",\n       size.size());\n \n-  TORCH_CHECK(\n-      indices.is_pinned() == values.is_pinned(),\n-      \"memory pinning of indices (=\",\n-      indices.is_pinned(),\n-      \") must match memory pinning of values (=\",\n-      values.is_pinned(),\n-      \")\");\n+  if (check_pinning) {\n+    TORCH_CHECK(",
        "comment_created_at": "2025-06-01T05:25:38+00:00",
        "comment_author": "pearu",
        "comment_body": "No. User inputs can be invalid in many ways, not just with respect to the user provided values. In this particular case, user input is invalid if the memory pinning property of `indices` and `values` is different. This case could be compared to if the devices of `indices` and `values` are different. Within pytorch, such errors are typically exposed as RuntimeError exceptions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089785335",
    "pr_number": 150218,
    "pr_file": "torch/csrc/Module.cpp",
    "created_at": "2025-05-14T21:31:27+00:00",
    "commented_code": "END_HANDLE_TH_ERRORS\n }\n \n+static PyObject* THPModule_torchDeviceToDLDevice(\n+    PyObject* _unused,\n+    PyObject* data) {\n+  HANDLE_TH_ERRORS\n+  TORCH_CHECK(\n+      THPDevice_Check(data),\n+      \"torchDeviceToDLDevice: expected torch.device argument.\");\n+  auto device = reinterpret_cast<THPDevice*>(data)->device;\n+  auto dl_device = at::torchDeviceToDLDevice(device);\n+  auto tuple = PyTuple_New(2);",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2089785335",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "2089785335",
        "commented_code": "@@ -636,6 +668,22 @@ static PyObject* THPModule_fromDLPack(PyObject* _unused, PyObject* data) {\n   END_HANDLE_TH_ERRORS\n }\n \n+static PyObject* THPModule_torchDeviceToDLDevice(\n+    PyObject* _unused,\n+    PyObject* data) {\n+  HANDLE_TH_ERRORS\n+  TORCH_CHECK(\n+      THPDevice_Check(data),\n+      \"torchDeviceToDLDevice: expected torch.device argument.\");\n+  auto device = reinterpret_cast<THPDevice*>(data)->device;\n+  auto dl_device = at::torchDeviceToDLDevice(device);\n+  auto tuple = PyTuple_New(2);",
        "comment_created_at": "2025-05-14T21:31:27+00:00",
        "comment_author": "albanD",
        "comment_body": "Add error checking in case this failed",
        "pr_file_module": null
      },
      {
        "comment_id": "2105840117",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "2089785335",
        "commented_code": "@@ -636,6 +668,22 @@ static PyObject* THPModule_fromDLPack(PyObject* _unused, PyObject* data) {\n   END_HANDLE_TH_ERRORS\n }\n \n+static PyObject* THPModule_torchDeviceToDLDevice(\n+    PyObject* _unused,\n+    PyObject* data) {\n+  HANDLE_TH_ERRORS\n+  TORCH_CHECK(\n+      THPDevice_Check(data),\n+      \"torchDeviceToDLDevice: expected torch.device argument.\");\n+  auto device = reinterpret_cast<THPDevice*>(data)->device;\n+  auto dl_device = at::torchDeviceToDLDevice(device);\n+  auto tuple = PyTuple_New(2);",
        "comment_created_at": "2025-05-24T14:28:55+00:00",
        "comment_author": "ysiraichi",
        "comment_body": "Not sure I get what you mean. If `at::torchDeviceToDLDevice` errors, the error will bubble up to Python, won't it?",
        "pr_file_module": null
      },
      {
        "comment_id": "2150360414",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "2089785335",
        "commented_code": "@@ -636,6 +668,22 @@ static PyObject* THPModule_fromDLPack(PyObject* _unused, PyObject* data) {\n   END_HANDLE_TH_ERRORS\n }\n \n+static PyObject* THPModule_torchDeviceToDLDevice(\n+    PyObject* _unused,\n+    PyObject* data) {\n+  HANDLE_TH_ERRORS\n+  TORCH_CHECK(\n+      THPDevice_Check(data),\n+      \"torchDeviceToDLDevice: expected torch.device argument.\");\n+  auto device = reinterpret_cast<THPDevice*>(data)->device;\n+  auto dl_device = at::torchDeviceToDLDevice(device);\n+  auto tuple = PyTuple_New(2);",
        "comment_created_at": "2025-06-16T16:00:19+00:00",
        "comment_author": "albanD",
        "comment_body": "CPython is C API, not C++. There is no error throwing here, you need to check the tuple is not null and if so, make this a c++ error\r\n\r\n```cpp\r\nif (!tuple) {\r\n  throw python_error();\r\n}\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2089799016",
    "pr_number": 150218,
    "pr_file": "torch/csrc/Module.cpp",
    "created_at": "2025-05-14T21:44:38+00:00",
    "commented_code": "}\n \n template <class T>\n-PyObject* THPModule_toDLPackImpl(PyObject* _unused, PyObject* data) {\n+PyObject* THPModule_toDLPackImpl(\n+    PyObject* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n   HANDLE_TH_ERRORS\n-  TORCH_CHECK(THPVariable_Check(data), \"data must be a Tensor\");\n-  auto tensor = at::DLPackTraits<T>::toDLPack(THPVariable_Unpack(data));\n-  return PyCapsule_New(\n-      tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  static torch::PythonArgParser parser(\n+      {\"_to_dlpack(Tensor data, *, IntArrayRef? dl_device=None, bool? copy=None)\"});\n+  torch::ParsedArgs<3> parsed_args{};\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+\n+  if (r.idx == 0) {\n+    auto data = r.tensor(0);\n+    auto dl_device = r.intlist(1);\n+    auto copy = r.toBoolOptional(2);\n+\n+    // Parse the int list into a tuple.\n+    std::optional<DLDevice> optional_dl_device;\n+\n+    if (!dl_device.empty()) {\n+      TORCH_CHECK(\n+          dl_device.size() == 2,\n+          \"dl_device must be either None or a tuple of ints\");\n+      optional_dl_device = DLDevice{\n+          static_cast<DLDeviceType>(dl_device[0]),\n+          static_cast<int32_t>(dl_device[1])};\n+    }\n+\n+    auto tensor = at::DLPackTraits<T>::toDLPack(\n+        at::maybeCopyTensor(data, optional_dl_device, copy));\n+    return PyCapsule_New(\n+        tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  }\n+\n+  return nullptr;",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2089799016",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "2089799016",
        "commented_code": "@@ -607,25 +607,57 @@ void DLPack_Capsule_Destructor(PyObject* data) {\n }\n \n template <class T>\n-PyObject* THPModule_toDLPackImpl(PyObject* _unused, PyObject* data) {\n+PyObject* THPModule_toDLPackImpl(\n+    PyObject* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n   HANDLE_TH_ERRORS\n-  TORCH_CHECK(THPVariable_Check(data), \"data must be a Tensor\");\n-  auto tensor = at::DLPackTraits<T>::toDLPack(THPVariable_Unpack(data));\n-  return PyCapsule_New(\n-      tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  static torch::PythonArgParser parser(\n+      {\"_to_dlpack(Tensor data, *, IntArrayRef? dl_device=None, bool? copy=None)\"});\n+  torch::ParsedArgs<3> parsed_args{};\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+\n+  if (r.idx == 0) {\n+    auto data = r.tensor(0);\n+    auto dl_device = r.intlist(1);\n+    auto copy = r.toBoolOptional(2);\n+\n+    // Parse the int list into a tuple.\n+    std::optional<DLDevice> optional_dl_device;\n+\n+    if (!dl_device.empty()) {\n+      TORCH_CHECK(\n+          dl_device.size() == 2,\n+          \"dl_device must be either None or a tuple of ints\");\n+      optional_dl_device = DLDevice{\n+          static_cast<DLDeviceType>(dl_device[0]),\n+          static_cast<int32_t>(dl_device[1])};\n+    }\n+\n+    auto tensor = at::DLPackTraits<T>::toDLPack(\n+        at::maybeCopyTensor(data, optional_dl_device, copy));\n+    return PyCapsule_New(\n+        tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  }\n+\n+  return nullptr;",
        "comment_created_at": "2025-05-14T21:44:38+00:00",
        "comment_author": "albanD",
        "comment_body": "I'm don't recall, will this have the right python err set?",
        "pr_file_module": null
      },
      {
        "comment_id": "2105839664",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "2089799016",
        "commented_code": "@@ -607,25 +607,57 @@ void DLPack_Capsule_Destructor(PyObject* data) {\n }\n \n template <class T>\n-PyObject* THPModule_toDLPackImpl(PyObject* _unused, PyObject* data) {\n+PyObject* THPModule_toDLPackImpl(\n+    PyObject* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n   HANDLE_TH_ERRORS\n-  TORCH_CHECK(THPVariable_Check(data), \"data must be a Tensor\");\n-  auto tensor = at::DLPackTraits<T>::toDLPack(THPVariable_Unpack(data));\n-  return PyCapsule_New(\n-      tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  static torch::PythonArgParser parser(\n+      {\"_to_dlpack(Tensor data, *, IntArrayRef? dl_device=None, bool? copy=None)\"});\n+  torch::ParsedArgs<3> parsed_args{};\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+\n+  if (r.idx == 0) {\n+    auto data = r.tensor(0);\n+    auto dl_device = r.intlist(1);\n+    auto copy = r.toBoolOptional(2);\n+\n+    // Parse the int list into a tuple.\n+    std::optional<DLDevice> optional_dl_device;\n+\n+    if (!dl_device.empty()) {\n+      TORCH_CHECK(\n+          dl_device.size() == 2,\n+          \"dl_device must be either None or a tuple of ints\");\n+      optional_dl_device = DLDevice{\n+          static_cast<DLDeviceType>(dl_device[0]),\n+          static_cast<int32_t>(dl_device[1])};\n+    }\n+\n+    auto tensor = at::DLPackTraits<T>::toDLPack(\n+        at::maybeCopyTensor(data, optional_dl_device, copy));\n+    return PyCapsule_New(\n+        tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  }\n+\n+  return nullptr;",
        "comment_created_at": "2025-05-24T14:26:49+00:00",
        "comment_author": "ysiraichi",
        "comment_body": "I think so. At least, it will throw an error inside the parser. I will replace it with `Py_RETURN_NONE`, just for consistency.",
        "pr_file_module": null
      },
      {
        "comment_id": "2147035848",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "2089799016",
        "commented_code": "@@ -607,25 +607,57 @@ void DLPack_Capsule_Destructor(PyObject* data) {\n }\n \n template <class T>\n-PyObject* THPModule_toDLPackImpl(PyObject* _unused, PyObject* data) {\n+PyObject* THPModule_toDLPackImpl(\n+    PyObject* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n   HANDLE_TH_ERRORS\n-  TORCH_CHECK(THPVariable_Check(data), \"data must be a Tensor\");\n-  auto tensor = at::DLPackTraits<T>::toDLPack(THPVariable_Unpack(data));\n-  return PyCapsule_New(\n-      tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  static torch::PythonArgParser parser(\n+      {\"_to_dlpack(Tensor data, *, IntArrayRef? dl_device=None, bool? copy=None)\"});\n+  torch::ParsedArgs<3> parsed_args{};\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+\n+  if (r.idx == 0) {\n+    auto data = r.tensor(0);\n+    auto dl_device = r.intlist(1);\n+    auto copy = r.toBoolOptional(2);\n+\n+    // Parse the int list into a tuple.\n+    std::optional<DLDevice> optional_dl_device;\n+\n+    if (!dl_device.empty()) {\n+      TORCH_CHECK(\n+          dl_device.size() == 2,\n+          \"dl_device must be either None or a tuple of ints\");\n+      optional_dl_device = DLDevice{\n+          static_cast<DLDeviceType>(dl_device[0]),\n+          static_cast<int32_t>(dl_device[1])};\n+    }\n+\n+    auto tensor = at::DLPackTraits<T>::toDLPack(\n+        at::maybeCopyTensor(data, optional_dl_device, copy));\n+    return PyCapsule_New(\n+        tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  }\n+\n+  return nullptr;",
        "comment_created_at": "2025-06-14T17:36:14+00:00",
        "comment_author": "ysiraichi",
        "comment_body": "I think I hadn't understood you question earlier. If you are asking whether a C++ exception will be mapped the correct Python error set, the answer is: yes! `END_HANDLE_TH_ERRORS` will take care of that. Specifically, `torch::translate_exception_to_python(std::current_exception())` call will do the job.",
        "pr_file_module": null
      },
      {
        "comment_id": "2150371678",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "2089799016",
        "commented_code": "@@ -607,25 +607,57 @@ void DLPack_Capsule_Destructor(PyObject* data) {\n }\n \n template <class T>\n-PyObject* THPModule_toDLPackImpl(PyObject* _unused, PyObject* data) {\n+PyObject* THPModule_toDLPackImpl(\n+    PyObject* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n   HANDLE_TH_ERRORS\n-  TORCH_CHECK(THPVariable_Check(data), \"data must be a Tensor\");\n-  auto tensor = at::DLPackTraits<T>::toDLPack(THPVariable_Unpack(data));\n-  return PyCapsule_New(\n-      tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  static torch::PythonArgParser parser(\n+      {\"_to_dlpack(Tensor data, *, IntArrayRef? dl_device=None, bool? copy=None)\"});\n+  torch::ParsedArgs<3> parsed_args{};\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+\n+  if (r.idx == 0) {\n+    auto data = r.tensor(0);\n+    auto dl_device = r.intlist(1);\n+    auto copy = r.toBoolOptional(2);\n+\n+    // Parse the int list into a tuple.\n+    std::optional<DLDevice> optional_dl_device;\n+\n+    if (!dl_device.empty()) {\n+      TORCH_CHECK(\n+          dl_device.size() == 2,\n+          \"dl_device must be either None or a tuple of ints\");\n+      optional_dl_device = DLDevice{\n+          static_cast<DLDeviceType>(dl_device[0]),\n+          static_cast<int32_t>(dl_device[1])};\n+    }\n+\n+    auto tensor = at::DLPackTraits<T>::toDLPack(\n+        at::maybeCopyTensor(data, optional_dl_device, copy));\n+    return PyCapsule_New(\n+        tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  }\n+\n+  return nullptr;",
        "comment_created_at": "2025-06-16T16:06:38+00:00",
        "comment_author": "albanD",
        "comment_body": "Returning None here is completely different. These are in no way interchangeable.\r\nReturning `nullptr` from these APIs mean that something went wrong and the caller should check the globally set error for more info. Returning None means that all went well and the result is \"None\".\r\nYou either want one or the other :D \r\nOr maybe you're saying this is dead code?",
        "pr_file_module": null
      },
      {
        "comment_id": "2158931025",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "2089799016",
        "commented_code": "@@ -607,25 +607,57 @@ void DLPack_Capsule_Destructor(PyObject* data) {\n }\n \n template <class T>\n-PyObject* THPModule_toDLPackImpl(PyObject* _unused, PyObject* data) {\n+PyObject* THPModule_toDLPackImpl(\n+    PyObject* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n   HANDLE_TH_ERRORS\n-  TORCH_CHECK(THPVariable_Check(data), \"data must be a Tensor\");\n-  auto tensor = at::DLPackTraits<T>::toDLPack(THPVariable_Unpack(data));\n-  return PyCapsule_New(\n-      tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  static torch::PythonArgParser parser(\n+      {\"_to_dlpack(Tensor data, *, IntArrayRef? dl_device=None, bool? copy=None)\"});\n+  torch::ParsedArgs<3> parsed_args{};\n+  auto r = parser.parse(args, kwargs, parsed_args);\n+\n+  if (r.idx == 0) {\n+    auto data = r.tensor(0);\n+    auto dl_device = r.intlist(1);\n+    auto copy = r.toBoolOptional(2);\n+\n+    // Parse the int list into a tuple.\n+    std::optional<DLDevice> optional_dl_device;\n+\n+    if (!dl_device.empty()) {\n+      TORCH_CHECK(\n+          dl_device.size() == 2,\n+          \"dl_device must be either None or a tuple of ints\");\n+      optional_dl_device = DLDevice{\n+          static_cast<DLDeviceType>(dl_device[0]),\n+          static_cast<int32_t>(dl_device[1])};\n+    }\n+\n+    auto tensor = at::DLPackTraits<T>::toDLPack(\n+        at::maybeCopyTensor(data, optional_dl_device, copy));\n+    return PyCapsule_New(\n+        tensor, at::DLPackTraits<T>::capsule, DLPack_Capsule_Destructor<T>);\n+  }\n+\n+  return nullptr;",
        "comment_created_at": "2025-06-20T13:08:36+00:00",
        "comment_author": "ysiraichi",
        "comment_body": "This is essentially deadcode.\r\nOn second thoughts, it would be better to just `TORCH_INTERNAL_ASSERT(r.idx == 0);`. Then, there would be no need to return None, since the bug would be in the arg parser.",
        "pr_file_module": null
      }
    ]
  }
]