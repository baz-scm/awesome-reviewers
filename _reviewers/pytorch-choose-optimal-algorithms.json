[
  {
    "discussion_id": "2145816443",
    "pr_number": 151547,
    "pr_file": "aten/src/ATen/native/CPUBlas.cpp",
    "created_at": "2025-06-13T18:29:32+00:00",
    "commented_code": "b, &ldb_,\n              &beta_,\n              float_v.data(), &ldc_);\n      for (auto cv: float_v) {\n        *(c++) = c10::convert<at::BFloat16>(cv);\n\n      for (const auto j : c10::irange(n)) {\n        for (const auto i : c10::irange(m)) {\n          auto offset = j * ldc + i;\n          c[offset] = c10::convert<c10::BFloat16>(float_v[j * m + i]);",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2145816443",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 151547,
        "pr_file": "aten/src/ATen/native/CPUBlas.cpp",
        "discussion_id": "2145816443",
        "commented_code": "@@ -368,8 +368,12 @@ void gemm(\n               b, &ldb_,\n               &beta_,\n               float_v.data(), &ldc_);\n-      for (auto cv: float_v) {\n-        *(c++) = c10::convert<at::BFloat16>(cv);\n+\n+      for (const auto j : c10::irange(n)) {\n+        for (const auto i : c10::irange(m)) {\n+          auto offset = j * ldc + i;\n+          c[offset] = c10::convert<c10::BFloat16>(float_v[j * m + i]);",
        "comment_created_at": "2025-06-13T18:29:32+00:00",
        "comment_author": "taoye9",
        "comment_body": "should it be `float_v[j * ldc_ + i]`  instead of `float_v[j * m + i]` given sbgemm leading dim is ldc?\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178739355",
    "pr_number": 157267,
    "pr_file": "aten/src/ATen/cpu/vec/vec512/vec512_qint.h",
    "created_at": "2025-07-01T23:53:23+00:00",
    "commented_code": "return _mm512_permutexvar_epi32(permute_mask_v, xyzw_clamped_v);\n}\n\ntemplate <>\nat::vec::Vectorized<uint8_t> inline convert_float_to_int8(\n    at::vec::Vectorized<float> src) {\n  // The type of min_val should be int8_t to ensure correct clamping behavior.\n  constexpr auto min_val = std::numeric_limits<int8_t>::min();\n  constexpr auto max_val = std::numeric_limits<uint8_t>::max();\n  __m512 float32_min_val = _mm512_set1_ps(float(min_val));\n  __m512 float32_max_val = _mm512_set1_ps(float(max_val));\n  __m512 float32_src = _mm512_max_ps(src, float32_min_val);\n  float32_src = _mm512_min_ps(float32_src, float32_max_val);\n  __m512i int32_src = _mm512_cvttps_epi32(float32_src);\n  uint8_t output_mem[Vectorized<int8_t>::size()];\n  _mm512_mask_cvtepi32_storeu_epi8(output_mem, 0xffff, int32_src);",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2178739355",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157267,
        "pr_file": "aten/src/ATen/cpu/vec/vec512/vec512_qint.h",
        "discussion_id": "2178739355",
        "commented_code": "@@ -159,6 +161,22 @@ typename std::enable_if_t<\n   return _mm512_permutexvar_epi32(permute_mask_v, xyzw_clamped_v);\n }\n \n+template <>\n+at::vec::Vectorized<uint8_t> inline convert_float_to_int8(\n+    at::vec::Vectorized<float> src) {\n+  // The type of min_val should be int8_t to ensure correct clamping behavior.\n+  constexpr auto min_val = std::numeric_limits<int8_t>::min();\n+  constexpr auto max_val = std::numeric_limits<uint8_t>::max();\n+  __m512 float32_min_val = _mm512_set1_ps(float(min_val));\n+  __m512 float32_max_val = _mm512_set1_ps(float(max_val));\n+  __m512 float32_src = _mm512_max_ps(src, float32_min_val);\n+  float32_src = _mm512_min_ps(float32_src, float32_max_val);\n+  __m512i int32_src = _mm512_cvttps_epi32(float32_src);\n+  uint8_t output_mem[Vectorized<int8_t>::size()];\n+  _mm512_mask_cvtepi32_storeu_epi8(output_mem, 0xffff, int32_src);",
        "comment_created_at": "2025-07-01T23:53:23+00:00",
        "comment_author": "leslie-fang-intel",
        "comment_body": "Could we use\r\n```\r\n__m128i int8_src = _mm512_cvtepi32_epi8(int32_src);\r\nreturn _mm512_castsi128_si512(int8_src);\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2178869263",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157267,
        "pr_file": "aten/src/ATen/cpu/vec/vec512/vec512_qint.h",
        "discussion_id": "2178739355",
        "commented_code": "@@ -159,6 +161,22 @@ typename std::enable_if_t<\n   return _mm512_permutexvar_epi32(permute_mask_v, xyzw_clamped_v);\n }\n \n+template <>\n+at::vec::Vectorized<uint8_t> inline convert_float_to_int8(\n+    at::vec::Vectorized<float> src) {\n+  // The type of min_val should be int8_t to ensure correct clamping behavior.\n+  constexpr auto min_val = std::numeric_limits<int8_t>::min();\n+  constexpr auto max_val = std::numeric_limits<uint8_t>::max();\n+  __m512 float32_min_val = _mm512_set1_ps(float(min_val));\n+  __m512 float32_max_val = _mm512_set1_ps(float(max_val));\n+  __m512 float32_src = _mm512_max_ps(src, float32_min_val);\n+  float32_src = _mm512_min_ps(float32_src, float32_max_val);\n+  __m512i int32_src = _mm512_cvttps_epi32(float32_src);\n+  uint8_t output_mem[Vectorized<int8_t>::size()];\n+  _mm512_mask_cvtepi32_storeu_epi8(output_mem, 0xffff, int32_src);",
        "comment_created_at": "2025-07-02T02:39:04+00:00",
        "comment_author": "thenumberouscode",
        "comment_body": "I've made the changes as per your suggestions. Please let me know if you have any further comments or questions.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2109952742",
    "pr_number": 153666,
    "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
    "created_at": "2025-05-27T18:59:36+00:00",
    "commented_code": "vec_t data = X_vec[i];\n      #pragma unroll\n      for (int ii=0; ii < vec_size; ii++){\n        wd = cuWelfordOnlineSum(static_cast<acc_t>(data.val[ii]), wd);\n        if(!rms_norm){\n          wd = cuWelfordOnlineSum(static_cast<acc_t>(data.val[ii]), wd);\n        } else{\n          wd = cuRMSOnlineSum(static_cast<acc_t>(data.val[ii]), wd);\n        }\n      }\n    }\n    // intra-warp reduction\n    for (int offset = (C10_WARP_SIZE >> 1); offset > 0; offset >>= 1) {\n        WelfordDataLN wdB{WARP_SHFL_DOWN(wd.mean, offset),\n        WARP_SHFL_DOWN(wd.sigma2, offset), WARP_SHFL_DOWN(wd.count, offset)};\n        wd = cuWelfordCombine(wd, wdB);\n        WelfordDataLN wdB{WARP_SHFL_DOWN(wd.mean, offset), WARP_SHFL_DOWN(wd.sigma2, offset), WARP_SHFL_DOWN(wd.count, offset)};\n        if(!rms_norm){",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2109952742",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153666,
        "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
        "discussion_id": "2109952742",
        "commented_code": "@@ -171,14 +197,21 @@ __device__ WelfordDataLN compute_stats(\n       vec_t data = X_vec[i];\n       #pragma unroll\n       for (int ii=0; ii < vec_size; ii++){\n-        wd = cuWelfordOnlineSum(static_cast<acc_t>(data.val[ii]), wd);\n+        if(!rms_norm){\n+          wd = cuWelfordOnlineSum(static_cast<acc_t>(data.val[ii]), wd);\n+        } else{\n+          wd = cuRMSOnlineSum(static_cast<acc_t>(data.val[ii]), wd);\n+        }\n       }\n     }\n     // intra-warp reduction\n     for (int offset = (C10_WARP_SIZE >> 1); offset > 0; offset >>= 1) {\n-        WelfordDataLN wdB{WARP_SHFL_DOWN(wd.mean, offset),\n-        WARP_SHFL_DOWN(wd.sigma2, offset), WARP_SHFL_DOWN(wd.count, offset)};\n-        wd = cuWelfordCombine(wd, wdB);\n+        WelfordDataLN wdB{WARP_SHFL_DOWN(wd.mean, offset), WARP_SHFL_DOWN(wd.sigma2, offset), WARP_SHFL_DOWN(wd.count, offset)};\n+        if(!rms_norm){",
        "comment_created_at": "2025-05-27T18:59:36+00:00",
        "comment_author": "jeffdaily",
        "comment_body": "`if constexpr`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178606011",
    "pr_number": 157290,
    "pr_file": "torch/nativert/executor/memory/LayoutManager.cpp",
    "created_at": "2025-07-01T21:56:40+00:00",
    "commented_code": "}\n}\n\n#ifndef NDEBUG\nvoid LayoutManager::assert_no_overlapping_storages(const Node& node, size_t t)\n    const {\n  if (state_ != LayoutManagerState::Running) {\n    return;\n  }\n\n  /*\n    for each value\n    (either an input or output)\n    ensure that the associated storage\n    slice lies within the allocated slice\n    if it is managed (or if it is an alias,\n    we can use the slice allocated to its source)\n    ---\n    also ensure that the current index lies\n    within the lifetime of this value\n  */\n\n  const auto& alias_analyzer = planner_.get_alias_analyzer();\n  const auto& alive_values = alias_analyzer.alive_values_at_time(t);\n\n  // make sure active memory intervals are non-overlapping\n  // by sorting them by start, and ensuring",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2178606011",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157290,
        "pr_file": "torch/nativert/executor/memory/LayoutManager.cpp",
        "discussion_id": "2178606011",
        "commented_code": "@@ -157,6 +160,123 @@ void LayoutManager::populate_tensor_values() {\n   }\n }\n \n+#ifndef NDEBUG\n+void LayoutManager::assert_no_overlapping_storages(const Node& node, size_t t)\n+    const {\n+  if (state_ != LayoutManagerState::Running) {\n+    return;\n+  }\n+\n+  /*\n+    for each value\n+    (either an input or output)\n+    ensure that the associated storage\n+    slice lies within the allocated slice\n+    if it is managed (or if it is an alias,\n+    we can use the slice allocated to its source)\n+    ---\n+    also ensure that the current index lies\n+    within the lifetime of this value\n+  */\n+\n+  const auto& alias_analyzer = planner_.get_alias_analyzer();\n+  const auto& alive_values = alias_analyzer.alive_values_at_time(t);\n+\n+  // make sure active memory intervals are non-overlapping\n+  // by sorting them by start, and ensuring",
        "comment_created_at": "2025-07-01T21:56:40+00:00",
        "comment_author": "SherlockNoMad",
        "comment_body": "How is the sorting achieved? Do you need a custom comparator? \r\nIf you are relying on the default std::set\\<std::pair\\> behavior, make a remark mentioning so, coz I don't how c++ compare pairs by default. \r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2178737089",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157290,
        "pr_file": "torch/nativert/executor/memory/LayoutManager.cpp",
        "discussion_id": "2178606011",
        "commented_code": "@@ -157,6 +160,123 @@ void LayoutManager::populate_tensor_values() {\n   }\n }\n \n+#ifndef NDEBUG\n+void LayoutManager::assert_no_overlapping_storages(const Node& node, size_t t)\n+    const {\n+  if (state_ != LayoutManagerState::Running) {\n+    return;\n+  }\n+\n+  /*\n+    for each value\n+    (either an input or output)\n+    ensure that the associated storage\n+    slice lies within the allocated slice\n+    if it is managed (or if it is an alias,\n+    we can use the slice allocated to its source)\n+    ---\n+    also ensure that the current index lies\n+    within the lifetime of this value\n+  */\n+\n+  const auto& alias_analyzer = planner_.get_alias_analyzer();\n+  const auto& alive_values = alias_analyzer.alive_values_at_time(t);\n+\n+  // make sure active memory intervals are non-overlapping\n+  // by sorting them by start, and ensuring",
        "comment_created_at": "2025-07-01T23:50:34+00:00",
        "comment_author": "dolpm",
        "comment_body": "on comparison of pairs: https://cplusplus.com/reference/utility/pair/operators/\r\n\r\ni will add this to the comment.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172431501",
    "pr_number": 156203,
    "pr_file": "aten/src/ATen/native/cuda/GroupMM.cu",
    "created_at": "2025-06-27T16:40:54+00:00",
    "commented_code": "#include <cutlass/gemm/dispatch_policy.hpp>\n#include <cutlass/gemm/kernel/gemm_universal.hpp>\n\n#include <ATen/native/cuda/cutlass_common.cuh>\n\nnamespace {\nusing Strides = at::cuda::detail::Strides; // std::array<int64_t, 3>;\n\ntemplate <bool PONG, typename TB_M, typename TB_N, typename TB_K>\ntemplate <typename ArchTag, bool PONGOr2SM, typename TB_M, typename TB_N, typename TB_K>\nstruct Schedule {\n  // SM90\n  using CooperativeSchedule =\n      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedCooperative;\n  using PongSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpong;\n  using CooperativeEpilogueSchedule =\n      cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n  using PongEpilogueSchedule =\n      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n  using KernelSchedule =\n      cute::conditional_t<PONG, PongSchedule, CooperativeSchedule>;\n  using EpilogueSchedule = cute::\n      conditional_t<PONG, PongEpilogueSchedule, CooperativeEpilogueSchedule>;\n  // SM100\n  using MMA1SMKernelSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecialized1SmSm100;\n  using MMA1SMEpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;\n  using MMA2SMKernelSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecialized2SmSm100;\n  using MMA2SMEpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized2Sm;",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2172431501",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156203,
        "pr_file": "aten/src/ATen/native/cuda/GroupMM.cu",
        "discussion_id": "2172431501",
        "commented_code": "@@ -43,22 +44,34 @@ C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(\"-Wunused-but-set-parameter\")\n #include <cutlass/gemm/dispatch_policy.hpp>\n #include <cutlass/gemm/kernel/gemm_universal.hpp>\n \n+#include <ATen/native/cuda/cutlass_common.cuh>\n+\n namespace {\n using Strides = at::cuda::detail::Strides; // std::array<int64_t, 3>;\n \n-template <bool PONG, typename TB_M, typename TB_N, typename TB_K>\n+template <typename ArchTag, bool PONGOr2SM, typename TB_M, typename TB_N, typename TB_K>\n struct Schedule {\n+  // SM90\n   using CooperativeSchedule =\n       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedCooperative;\n   using PongSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpong;\n   using CooperativeEpilogueSchedule =\n       cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n   using PongEpilogueSchedule =\n       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n-  using KernelSchedule =\n-      cute::conditional_t<PONG, PongSchedule, CooperativeSchedule>;\n-  using EpilogueSchedule = cute::\n-      conditional_t<PONG, PongEpilogueSchedule, CooperativeEpilogueSchedule>;\n+  // SM100\n+  using MMA1SMKernelSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecialized1SmSm100;\n+  using MMA1SMEpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;\n+  using MMA2SMKernelSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecialized2SmSm100;\n+  using MMA2SMEpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized2Sm;\n+",
        "comment_created_at": "2025-06-27T16:40:54+00:00",
        "comment_author": "drisspg",
        "comment_body": "Is there no pingpong schedule for sm100?",
        "pr_file_module": null
      },
      {
        "comment_id": "2172484226",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156203,
        "pr_file": "aten/src/ATen/native/cuda/GroupMM.cu",
        "discussion_id": "2172431501",
        "commented_code": "@@ -43,22 +44,34 @@ C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(\"-Wunused-but-set-parameter\")\n #include <cutlass/gemm/dispatch_policy.hpp>\n #include <cutlass/gemm/kernel/gemm_universal.hpp>\n \n+#include <ATen/native/cuda/cutlass_common.cuh>\n+\n namespace {\n using Strides = at::cuda::detail::Strides; // std::array<int64_t, 3>;\n \n-template <bool PONG, typename TB_M, typename TB_N, typename TB_K>\n+template <typename ArchTag, bool PONGOr2SM, typename TB_M, typename TB_N, typename TB_K>\n struct Schedule {\n+  // SM90\n   using CooperativeSchedule =\n       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedCooperative;\n   using PongSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpong;\n   using CooperativeEpilogueSchedule =\n       cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n   using PongEpilogueSchedule =\n       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n-  using KernelSchedule =\n-      cute::conditional_t<PONG, PongSchedule, CooperativeSchedule>;\n-  using EpilogueSchedule = cute::\n-      conditional_t<PONG, PongEpilogueSchedule, CooperativeEpilogueSchedule>;\n+  // SM100\n+  using MMA1SMKernelSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecialized1SmSm100;\n+  using MMA1SMEpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;\n+  using MMA2SMKernelSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecialized2SmSm100;\n+  using MMA2SMEpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized2Sm;\n+",
        "comment_created_at": "2025-06-27T17:04:10+00:00",
        "comment_author": "AaronWang04",
        "comment_body": "Nope, this is what the CUTLASS team said when I asked: \"with SM100, there are no more cooperative & pingpong scheduling policies (in the name, everything is pingpong)\"",
        "pr_file_module": null
      },
      {
        "comment_id": "2172549319",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156203,
        "pr_file": "aten/src/ATen/native/cuda/GroupMM.cu",
        "discussion_id": "2172431501",
        "commented_code": "@@ -43,22 +44,34 @@ C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(\"-Wunused-but-set-parameter\")\n #include <cutlass/gemm/dispatch_policy.hpp>\n #include <cutlass/gemm/kernel/gemm_universal.hpp>\n \n+#include <ATen/native/cuda/cutlass_common.cuh>\n+\n namespace {\n using Strides = at::cuda::detail::Strides; // std::array<int64_t, 3>;\n \n-template <bool PONG, typename TB_M, typename TB_N, typename TB_K>\n+template <typename ArchTag, bool PONGOr2SM, typename TB_M, typename TB_N, typename TB_K>\n struct Schedule {\n+  // SM90\n   using CooperativeSchedule =\n       cutlass::gemm::KernelPtrArrayTmaWarpSpecializedCooperative;\n   using PongSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpong;\n   using CooperativeEpilogueSchedule =\n       cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative;\n   using PongEpilogueSchedule =\n       cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;\n-  using KernelSchedule =\n-      cute::conditional_t<PONG, PongSchedule, CooperativeSchedule>;\n-  using EpilogueSchedule = cute::\n-      conditional_t<PONG, PongEpilogueSchedule, CooperativeEpilogueSchedule>;\n+  // SM100\n+  using MMA1SMKernelSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecialized1SmSm100;\n+  using MMA1SMEpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized1Sm;\n+  using MMA2SMKernelSchedule = cutlass::gemm::KernelPtrArrayTmaWarpSpecialized2SmSm100;\n+  using MMA2SMEpilogueSchedule = cutlass::epilogue::PtrArrayTmaWarpSpecialized2Sm;\n+",
        "comment_created_at": "2025-06-27T17:46:06+00:00",
        "comment_author": "drisspg",
        "comment_body": "Sounds good",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2084689345",
    "pr_number": 153373,
    "pr_file": "aten/src/ATen/cuda/cub.cu",
    "created_at": "2025-05-12T13:33:03+00:00",
    "commented_code": "template <typename input_t, typename output_t>\nvoid inclusive_sum_truncating(const input_t *input, output_t *output, int64_t num_items) {\n#if CUB_V3_PLUS()\n  inclusive_scan(input, output, ::cuda::std::plus<>{}, num_items);\n#else\n  using NO_ROCM(at_cuda_detail)::cub::Sum;\n  inclusive_scan(input, output, Sum{}, num_items);\n#endif",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2084689345",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153373,
        "pr_file": "aten/src/ATen/cuda/cub.cu",
        "discussion_id": "2084689345",
        "commented_code": "@@ -15,8 +20,12 @@ struct SumOp {\n \n template <typename input_t, typename output_t>\n void inclusive_sum_truncating(const input_t *input, output_t *output, int64_t num_items) {\n+#if CUB_V3_PLUS()\n+  inclusive_scan(input, output, ::cuda::std::plus<>{}, num_items);\n+#else\n   using NO_ROCM(at_cuda_detail)::cub::Sum;\n   inclusive_scan(input, output, Sum{}, num_items);\n+#endif",
        "comment_created_at": "2025-05-12T13:33:03+00:00",
        "comment_author": "miscco",
        "comment_body": "This change is valid unconditionally\r\n```suggestion\r\n  inclusive_scan(input, output, ::cuda::std::plus<>{}, num_items);\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2084690364",
    "pr_number": 153373,
    "pr_file": "aten/src/ATen/cuda/cub.cu",
    "created_at": "2025-05-12T13:33:36+00:00",
    "commented_code": "void mask_exclusive_sum(const uint8_t *mask, int64_t *output_idx, int64_t n) {\n  CountMaskOp op{};\n#if CUB_V3_PLUS()\n  auto iter = thrust::transform_iterator<decltype(op), decltype(mask)>(mask, op);\n#else\n  auto iter = NO_ROCM(at_cuda_detail)::cub::TransformInputIterator<\n      bool, decltype(op), decltype(mask)>(mask, op);\n#endif",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2084690364",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153373,
        "pr_file": "aten/src/ATen/cuda/cub.cu",
        "discussion_id": "2084690364",
        "commented_code": "@@ -42,8 +51,12 @@ struct CountMaskOp {\n \n void mask_exclusive_sum(const uint8_t *mask, int64_t *output_idx, int64_t n) {\n   CountMaskOp op{};\n+#if CUB_V3_PLUS()\n+  auto iter = thrust::transform_iterator<decltype(op), decltype(mask)>(mask, op);\n+#else\n   auto iter = NO_ROCM(at_cuda_detail)::cub::TransformInputIterator<\n       bool, decltype(op), decltype(mask)>(mask, op);\n+#endif",
        "comment_created_at": "2025-05-12T13:33:36+00:00",
        "comment_author": "miscco",
        "comment_body": "Ditto, I believe this can be applied unconditionally. Also we can leverage `thrust::make_transform_iterator`\r\n```suggestion\r\n  auto iter = thrust::make_transform_iterator(mask, op);\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2085100379",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153373,
        "pr_file": "aten/src/ATen/cuda/cub.cu",
        "discussion_id": "2084690364",
        "commented_code": "@@ -42,8 +51,12 @@ struct CountMaskOp {\n \n void mask_exclusive_sum(const uint8_t *mask, int64_t *output_idx, int64_t n) {\n   CountMaskOp op{};\n+#if CUB_V3_PLUS()\n+  auto iter = thrust::transform_iterator<decltype(op), decltype(mask)>(mask, op);\n+#else\n   auto iter = NO_ROCM(at_cuda_detail)::cub::TransformInputIterator<\n       bool, decltype(op), decltype(mask)>(mask, op);\n+#endif",
        "comment_created_at": "2025-05-12T17:03:15+00:00",
        "comment_author": "ngimel",
        "comment_body": "NO_ROCM wrappers are there for a reason, to enable rocm builds, that said it's true that it would be better to enable this with a macro similar to NO_ROCM instead of ifdefs",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2084693962",
    "pr_number": 153373,
    "pr_file": "aten/src/ATen/cuda/cub.cuh",
    "created_at": "2025-05-12T13:35:33+00:00",
    "commented_code": "aggT data[ITEMS_PER_THREAD];\n    aggT agg_val = 0;\n    TransformFunctor<T, aggT, nonzero> transform_functor;\n#if CUB_V3_PLUS()\n    auto iter_in = thrust::transform_iterator<TransformFunctor<T, aggT, nonzero>, const T*>(d_in, transform_functor);\n#else\n    auto iter_in = ROCM_HIPCUB(at_cuda_detail::cub)::TransformInputIterator<aggT, TransformFunctor<T, aggT, nonzero>, const T*>(d_in, transform_functor);\n#endif",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2084693962",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153373,
        "pr_file": "aten/src/ATen/cuda/cub.cuh",
        "discussion_id": "2084693962",
        "commented_code": "@@ -425,7 +435,11 @@ __global__ void calc_block_sums(const T * d_in, aggT * agg, int64_t nelem, int i\n     aggT data[ITEMS_PER_THREAD];\n     aggT agg_val = 0;\n     TransformFunctor<T, aggT, nonzero> transform_functor;\n+#if CUB_V3_PLUS()\n+    auto iter_in = thrust::transform_iterator<TransformFunctor<T, aggT, nonzero>, const T*>(d_in, transform_functor);\n+#else\n     auto iter_in = ROCM_HIPCUB(at_cuda_detail::cub)::TransformInputIterator<aggT, TransformFunctor<T, aggT, nonzero>, const T*>(d_in, transform_functor);\n+#endif",
        "comment_created_at": "2025-05-12T13:35:33+00:00",
        "comment_author": "miscco",
        "comment_body": "```suggestion\r\n    auto iter_in = thrust::make_transform_iterator(d_in, transform_functor);\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2084695693",
    "pr_number": 153373,
    "pr_file": "aten/src/ATen/cuda/cub.cuh",
    "created_at": "2025-05-12T13:36:13+00:00",
    "commented_code": "TORCH_CHECK(num_items <= std::numeric_limits<int>::max(),\n    \"cub InclusiveSumByKey does not support more than INT_MAX elements\");\n#if !defined(USE_ROCM)\n#if CUB_V3_PLUS()\n  CUB_WRAPPER(at_cuda_detail::cub::DeviceScan::InclusiveSumByKey,\n      keys, input, output, num_items, ::cuda::std::equal_to<>(), at::cuda::getCurrentCUDAStream());\n#else\n  CUB_WRAPPER(at_cuda_detail::cub::DeviceScan::InclusiveSumByKey,\n      keys, input, output, num_items, at_cuda_detail::cub::Equality(), at::cuda::getCurrentCUDAStream());\n#endif // CUB_V3_PLUS()",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2084695693",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153373,
        "pr_file": "aten/src/ATen/cuda/cub.cuh",
        "discussion_id": "2084695693",
        "commented_code": "@@ -567,8 +581,13 @@ inline void inclusive_sum_by_key(KeysInputIteratorT keys, ValuesInputIteratorT i\n   TORCH_CHECK(num_items <= std::numeric_limits<int>::max(),\n     \"cub InclusiveSumByKey does not support more than INT_MAX elements\");\n #if !defined(USE_ROCM)\n+#if CUB_V3_PLUS()\n+  CUB_WRAPPER(at_cuda_detail::cub::DeviceScan::InclusiveSumByKey,\n+      keys, input, output, num_items, ::cuda::std::equal_to<>(), at::cuda::getCurrentCUDAStream());\n+#else\n   CUB_WRAPPER(at_cuda_detail::cub::DeviceScan::InclusiveSumByKey,\n       keys, input, output, num_items, at_cuda_detail::cub::Equality(), at::cuda::getCurrentCUDAStream());\n+#endif // CUB_V3_PLUS()",
        "comment_created_at": "2025-05-12T13:36:13+00:00",
        "comment_author": "miscco",
        "comment_body": "`cuda::std::equal_to` is available unconditionally\r\n```suggestion\r\n  CUB_WRAPPER(at_cuda_detail::cub::DeviceScan::InclusiveSumByKey,\r\n      keys, input, output, num_items, ::cuda::std::equal_to<>(), at::cuda::getCurrentCUDAStream());\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2084703977",
    "pr_number": 153373,
    "pr_file": "aten/src/ATen/native/cuda/Nonzero.cu",
    "created_at": "2025-05-12T13:40:06+00:00",
    "commented_code": "for (int64_t idx = 0; idx < num_chunks; idx++) {\n      int remaining = std::min(chunk_size, self.numel() - idx * chunk_size);\n\n#if CUB_V3_PLUS()\n      thrust::counting_iterator<int64_t> counting_itr(idx * chunk_size);\n      thrust::transform_iterator<NonZeroOp<scalar_t>, const scalar_t*>\n          itr(self_.const_data_ptr<scalar_t>() + idx * chunk_size,\n              NonZeroOp<scalar_t>());\n#else\n      cub::CountingInputIterator<int64_t> counting_itr(idx * chunk_size);\n      cub::TransformInputIterator<bool, NonZeroOp<scalar_t>, const scalar_t*>\n          itr(self_.const_data_ptr<scalar_t>() + idx * chunk_size,\n              NonZeroOp<scalar_t>());\n#endif",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2084703977",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153373,
        "pr_file": "aten/src/ATen/native/cuda/Nonzero.cu",
        "discussion_id": "2084703977",
        "commented_code": "@@ -243,10 +258,17 @@ void nonzero_cuda_out_impl(const Tensor& self, Tensor& out) {\n     for (int64_t idx = 0; idx < num_chunks; idx++) {\n       int remaining = std::min(chunk_size, self.numel() - idx * chunk_size);\n \n+#if CUB_V3_PLUS()\n+      thrust::counting_iterator<int64_t> counting_itr(idx * chunk_size);\n+      thrust::transform_iterator<NonZeroOp<scalar_t>, const scalar_t*>\n+          itr(self_.const_data_ptr<scalar_t>() + idx * chunk_size,\n+              NonZeroOp<scalar_t>());\n+#else\n       cub::CountingInputIterator<int64_t> counting_itr(idx * chunk_size);\n       cub::TransformInputIterator<bool, NonZeroOp<scalar_t>, const scalar_t*>\n           itr(self_.const_data_ptr<scalar_t>() + idx * chunk_size,\n               NonZeroOp<scalar_t>());\n+#endif",
        "comment_created_at": "2025-05-12T13:40:06+00:00",
        "comment_author": "miscco",
        "comment_body": "```suggestion\r\n      thrust::counting_iterator<int64_t> counting_itr(idx * chunk_size);\r\n      thrust::transform_iterator<NonZeroOp<scalar_t>, const scalar_t*>\r\n          itr(self_.const_data_ptr<scalar_t>() + idx * chunk_size,\r\n              NonZeroOp<scalar_t>());\r\n```",
        "pr_file_module": null
      }
    ]
  }
]