[
  {
    "discussion_id": "2099094811",
    "pr_number": 153977,
    "pr_file": "torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp",
    "created_at": "2025-05-21T00:51:53+00:00",
    "commented_code": "std::string traceKeyEnd_;\n   };\n \n+  // Class that runs as part of a separate thread aside from watchdog\n+  // thread because we need to check the heartbeat from watchdog thread\n+  // so that when we get stuck in some NCCL/CUDA calls,\n+  // we can dump the debugging information and abort the process./\n+  // The thread is per class not per instance.\n+  class HeartbeatMonitor {\n+   public:\n+    // Return the singleton instance of HeartbeatMonitor.\n+    static HeartbeatMonitor* get() {\n+      static HeartbeatMonitor instance;",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2099094811",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153977,
        "pr_file": "torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp",
        "discussion_id": "2099094811",
        "commented_code": "@@ -596,6 +596,116 @@ class TORCH_API ProcessGroupNCCL : public Backend {\n     std::string traceKeyEnd_;\n   };\n \n+  // Class that runs as part of a separate thread aside from watchdog\n+  // thread because we need to check the heartbeat from watchdog thread\n+  // so that when we get stuck in some NCCL/CUDA calls,\n+  // we can dump the debugging information and abort the process./\n+  // The thread is per class not per instance.\n+  class HeartbeatMonitor {\n+   public:\n+    // Return the singleton instance of HeartbeatMonitor.\n+    static HeartbeatMonitor* get() {\n+      static HeartbeatMonitor instance;",
        "comment_created_at": "2025-05-21T00:51:53+00:00",
        "comment_author": "d4l3k",
        "comment_body": "nit: should put static in a .cpp file otherwise you could end up with multiple if you reference this function from different CPP files. I assume that's not intentional?",
        "pr_file_module": null
      },
      {
        "comment_id": "2099348969",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153977,
        "pr_file": "torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp",
        "discussion_id": "2099094811",
        "commented_code": "@@ -596,6 +596,116 @@ class TORCH_API ProcessGroupNCCL : public Backend {\n     std::string traceKeyEnd_;\n   };\n \n+  // Class that runs as part of a separate thread aside from watchdog\n+  // thread because we need to check the heartbeat from watchdog thread\n+  // so that when we get stuck in some NCCL/CUDA calls,\n+  // we can dump the debugging information and abort the process./\n+  // The thread is per class not per instance.\n+  class HeartbeatMonitor {\n+   public:\n+    // Return the singleton instance of HeartbeatMonitor.\n+    static HeartbeatMonitor* get() {\n+      static HeartbeatMonitor instance;",
        "comment_created_at": "2025-05-21T05:05:43+00:00",
        "comment_author": "fduwjj",
        "comment_body": "I can move it to cpp file but since it is static, different CPP files will still get the same singleton.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2125279484",
    "pr_number": 154977,
    "pr_file": "aten/src/ATen/native/mkldnn/xpu/detail/MatmulHelpers.h",
    "created_at": "2025-06-04T01:54:51+00:00",
    "commented_code": "+#pragma once\n+\n+#include <ATen/ATen.h>\n+\n+#include <ATen/native/mkldnn/xpu/detail/Attr.h>\n+#include <ATen/native/mkldnn/xpu/detail/Utils.h>\n+\n+#include <oneapi/dnnl/dnnl.hpp>\n+\n+namespace at::native::onednn {\n+\n+class BiasHandler {\n+  // This class handles the bias tensor for matmul operations.\n+  // It checks the dimensions of the bias tensor and expands it if necessary\n+  // for broadcasting.\n+ public:\n+  BiasHandler() = delete;\n+  BiasHandler(\n+      at::Tensor& bias,\n+      int64_t mb,\n+      int64_t m,\n+      int64_t k,\n+      int64_t n,\n+      int64_t gemm_dim)\n+      : b(bias),\n+        mb(mb),\n+        m(m),\n+        k(k),\n+        n(n),\n+        gemm_dim(gemm_dim),\n+        with_bias(bias.defined()) {}\n+",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2125279484",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154977,
        "pr_file": "aten/src/ATen/native/mkldnn/xpu/detail/MatmulHelpers.h",
        "discussion_id": "2125279484",
        "commented_code": "@@ -0,0 +1,231 @@\n+#pragma once\n+\n+#include <ATen/ATen.h>\n+\n+#include <ATen/native/mkldnn/xpu/detail/Attr.h>\n+#include <ATen/native/mkldnn/xpu/detail/Utils.h>\n+\n+#include <oneapi/dnnl/dnnl.hpp>\n+\n+namespace at::native::onednn {\n+\n+class BiasHandler {\n+  // This class handles the bias tensor for matmul operations.\n+  // It checks the dimensions of the bias tensor and expands it if necessary\n+  // for broadcasting.\n+ public:\n+  BiasHandler() = delete;\n+  BiasHandler(\n+      at::Tensor& bias,\n+      int64_t mb,\n+      int64_t m,\n+      int64_t k,\n+      int64_t n,\n+      int64_t gemm_dim)\n+      : b(bias),\n+        mb(mb),\n+        m(m),\n+        k(k),\n+        n(n),\n+        gemm_dim(gemm_dim),\n+        with_bias(bias.defined()) {}\n+",
        "comment_created_at": "2025-06-04T01:54:51+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n  C10_DISABLE_COPY_AND_ASSIGN(BiasHandler);\r\n\r\n  BiasHandler(BiasHandler&&) = default;\r\n  BiasHandler& operator=(BiasHandler&&) = default;\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175616884",
    "pr_number": 156572,
    "pr_file": "aten/src/ATen/native/mkldnn/xpu/qlinear.h",
    "created_at": "2025-06-30T18:00:16+00:00",
    "commented_code": "+#pragma once\n+\n+#include <ATen/Config.h>\n+#include <ATen/Tensor.h>\n+#include <ATen/core/List.h>\n+\n+namespace at::native::xpu {\n+C10_API Tensor q_linear_pointwise_tensor(",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2175616884",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156572,
        "pr_file": "aten/src/ATen/native/mkldnn/xpu/qlinear.h",
        "discussion_id": "2175616884",
        "commented_code": "@@ -0,0 +1,23 @@\n+#pragma once\n+\n+#include <ATen/Config.h>\n+#include <ATen/Tensor.h>\n+#include <ATen/core/List.h>\n+\n+namespace at::native::xpu {\n+C10_API Tensor q_linear_pointwise_tensor(",
        "comment_created_at": "2025-06-30T18:00:16+00:00",
        "comment_author": "desertfire",
        "comment_body": "Nit: Looking at qconv.h, what is decision for having these functions as static class methods vs. normal functions?",
        "pr_file_module": null
      },
      {
        "comment_id": "2176291566",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156572,
        "pr_file": "aten/src/ATen/native/mkldnn/xpu/qlinear.h",
        "discussion_id": "2175616884",
        "commented_code": "@@ -0,0 +1,23 @@\n+#pragma once\n+\n+#include <ATen/Config.h>\n+#include <ATen/Tensor.h>\n+#include <ATen/core/List.h>\n+\n+namespace at::native::xpu {\n+C10_API Tensor q_linear_pointwise_tensor(",
        "comment_created_at": "2025-07-01T02:27:21+00:00",
        "comment_author": "EikanWang",
        "comment_body": "@etaf , let's unify the design. For linear, the implementation is Function-based, while the implementation of qconv is class-based. Let's align with the CPU design.",
        "pr_file_module": null
      },
      {
        "comment_id": "2178776380",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156572,
        "pr_file": "aten/src/ATen/native/mkldnn/xpu/qlinear.h",
        "discussion_id": "2175616884",
        "commented_code": "@@ -0,0 +1,23 @@\n+#pragma once\n+\n+#include <ATen/Config.h>\n+#include <ATen/Tensor.h>\n+#include <ATen/core/List.h>\n+\n+namespace at::native::xpu {\n+C10_API Tensor q_linear_pointwise_tensor(",
        "comment_created_at": "2025-07-02T00:49:41+00:00",
        "comment_author": "etaf",
        "comment_body": "Hi\uff0c @desertfire\uff1a  Following the design of the CPU part, we use a class to group these functions together to make the code structure clearer. Therefore, we do not need to create an instance of the class when using these methods; instead, we can call them directly, for example: QConvoneDNNXPU::run_pointwise.",
        "pr_file_module": null
      },
      {
        "comment_id": "2178782759",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156572,
        "pr_file": "aten/src/ATen/native/mkldnn/xpu/qlinear.h",
        "discussion_id": "2175616884",
        "commented_code": "@@ -0,0 +1,23 @@\n+#pragma once\n+\n+#include <ATen/Config.h>\n+#include <ATen/Tensor.h>\n+#include <ATen/core/List.h>\n+\n+namespace at::native::xpu {\n+C10_API Tensor q_linear_pointwise_tensor(",
        "comment_created_at": "2025-07-02T00:57:36+00:00",
        "comment_author": "etaf",
        "comment_body": "> @etaf , let's unify the design. For linear, the implementation is Function-based, while the implementation of qconv is class-based. Let's align with the CPU design.\r\n\r\nSure, I'll stack an other PR to do that.",
        "pr_file_module": null
      }
    ]
  }
]