[
  {
    "discussion_id": "2167258024",
    "pr_number": 156097,
    "pr_file": "c10/cuda/driver_api.cpp",
    "created_at": "2025-06-25T17:30:58+00:00",
    "commented_code": "return &singleton;\n }\n \n+void* get_symbol(const char* name) {\n+  // CUDA 12.5+ supports version-based lookup\n+#if defined(CUDA_VERSION) && (CUDA_VERSION >= 12050)\n+  // We hardcode the version to 12.0 to ensure we always use a consistent\n+  // version of the driver API.\n+  unsigned int req_ver = 12000;\n+  if (auto st = cudaGetDriverEntryPointByVersion(",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2167258024",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156097,
        "pr_file": "c10/cuda/driver_api.cpp",
        "discussion_id": "2167258024",
        "commented_code": "@@ -47,6 +43,32 @@ C10_EXPORT DriverAPI* DriverAPI::get() {\n   return &singleton;\n }\n \n+void* get_symbol(const char* name) {\n+  // CUDA 12.5+ supports version-based lookup\n+#if defined(CUDA_VERSION) && (CUDA_VERSION >= 12050)\n+  // We hardcode the version to 12.0 to ensure we always use a consistent\n+  // version of the driver API.\n+  unsigned int req_ver = 12000;\n+  if (auto st = cudaGetDriverEntryPointByVersion(",
        "comment_created_at": "2025-06-25T17:30:58+00:00",
        "comment_author": "ngimel",
        "comment_body": "how will this work to return symbols that are added after 12000, like `cuMulticastAddDevice`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2167876661",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156097,
        "pr_file": "c10/cuda/driver_api.cpp",
        "discussion_id": "2167258024",
        "commented_code": "@@ -47,6 +43,32 @@ C10_EXPORT DriverAPI* DriverAPI::get() {\n   return &singleton;\n }\n \n+void* get_symbol(const char* name) {\n+  // CUDA 12.5+ supports version-based lookup\n+#if defined(CUDA_VERSION) && (CUDA_VERSION >= 12050)\n+  // We hardcode the version to 12.0 to ensure we always use a consistent\n+  // version of the driver API.\n+  unsigned int req_ver = 12000;\n+  if (auto st = cudaGetDriverEntryPointByVersion(",
        "comment_created_at": "2025-06-26T00:53:06+00:00",
        "comment_author": "eee4017",
        "comment_body": "Originally, I thought we should use the fallback API `cudaGetDriverEntryPoint`. However, I believe @wujingyue intended to remove all uses of `cudaGetDriverEntryPoint` and replace them with `cudaGetDriverEntryPointByVersion`, specifying a version number for each API.\r\n\r\n`cudaGetDriverEntryPoint` is still kept as a backup for now, and we resolve symbols using `cudaGetDriverEntryPointByVersion` whenever possible.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2167940898",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156097,
        "pr_file": "c10/cuda/driver_api.cpp",
        "discussion_id": "2167258024",
        "commented_code": "@@ -47,6 +43,32 @@ C10_EXPORT DriverAPI* DriverAPI::get() {\n   return &singleton;\n }\n \n+void* get_symbol(const char* name) {\n+  // CUDA 12.5+ supports version-based lookup\n+#if defined(CUDA_VERSION) && (CUDA_VERSION >= 12050)\n+  // We hardcode the version to 12.0 to ensure we always use a consistent\n+  // version of the driver API.\n+  unsigned int req_ver = 12000;\n+  if (auto st = cudaGetDriverEntryPointByVersion(",
        "comment_created_at": "2025-06-26T01:58:21+00:00",
        "comment_author": "wujingyue",
        "comment_body": "> how will this work to return symbols that are added after 12000, like `cuMulticastAddDevice`?\r\n\r\nIn that case, the requested version should be the version that introduced cuMulticastAddDevice, which I believe is 12090. Any use of cuMulticastAddDevice has to be version guarded (e.g. `#if (CUDA_VERSION >= 12090)`) as needed anyway. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2167953227",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156097,
        "pr_file": "c10/cuda/driver_api.cpp",
        "discussion_id": "2167258024",
        "commented_code": "@@ -47,6 +43,32 @@ C10_EXPORT DriverAPI* DriverAPI::get() {\n   return &singleton;\n }\n \n+void* get_symbol(const char* name) {\n+  // CUDA 12.5+ supports version-based lookup\n+#if defined(CUDA_VERSION) && (CUDA_VERSION >= 12050)\n+  // We hardcode the version to 12.0 to ensure we always use a consistent\n+  // version of the driver API.\n+  unsigned int req_ver = 12000;\n+  if (auto st = cudaGetDriverEntryPointByVersion(",
        "comment_created_at": "2025-06-26T02:15:42+00:00",
        "comment_author": "eee4017",
        "comment_body": "Hi @wujingyue, I believe `cuMulticastAddDevice` was introduced in CUDA 12.1.0 (see: [https://docs.nvidia.com/cuda/archive/12.1.0/cuda-driver-api/group\\_\\_CUDA\\_\\_MULTICAST.html](https://docs.nvidia.com/cuda/archive/12.1.0/cuda-driver-api/group__CUDA__MULTICAST.html)).\r\n\r\nI've updated the implementation so that we always request version 12.3.0 for `cuMulticastAddDevice` (This is the version in `c10/cuda/driver_api.h` originally).\r\nCould you please check the current implementation and let me know if this aligns with what you had in mind?\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2145461056",
    "pr_number": 155900,
    "pr_file": "c10/cuda/CUDAFunctions.cpp",
    "created_at": "2025-06-13T16:18:46+00:00",
    "commented_code": "C10_CUDA_CHECK(c10::cuda::SetDevice(device));\n }\n \n+void set_device(DeviceIndex device, const bool force) {",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2145461056",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "c10/cuda/CUDAFunctions.cpp",
        "discussion_id": "2145461056",
        "commented_code": "@@ -134,6 +134,10 @@ void set_device(DeviceIndex device) {\n   C10_CUDA_CHECK(c10::cuda::SetDevice(device));\n }\n \n+void set_device(DeviceIndex device, const bool force) {",
        "comment_created_at": "2025-06-13T16:18:46+00:00",
        "comment_author": "ngimel",
        "comment_body": "instead of creating a new overload you can default `force` argument to `false`?",
        "pr_file_module": null
      },
      {
        "comment_id": "2145559611",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "c10/cuda/CUDAFunctions.cpp",
        "discussion_id": "2145461056",
        "commented_code": "@@ -134,6 +134,10 @@ void set_device(DeviceIndex device) {\n   C10_CUDA_CHECK(c10::cuda::SetDevice(device));\n }\n \n+void set_device(DeviceIndex device, const bool force) {",
        "comment_created_at": "2025-06-13T17:14:08+00:00",
        "comment_author": "Aidyn-A",
        "comment_body": "Yes, I certainly can, but the third-party extensions might fail at runtime due to missing symbols. Should we take the risk?",
        "pr_file_module": null
      },
      {
        "comment_id": "2147305266",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "c10/cuda/CUDAFunctions.cpp",
        "discussion_id": "2145461056",
        "commented_code": "@@ -134,6 +134,10 @@ void set_device(DeviceIndex device) {\n   C10_CUDA_CHECK(c10::cuda::SetDevice(device));\n }\n \n+void set_device(DeviceIndex device, const bool force) {",
        "comment_created_at": "2025-06-14T20:49:21+00:00",
        "comment_author": "ngimel",
        "comment_body": "cc @janeyx99 but I think for things that are not part of stable ABI we don't provide any guarantees. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2183303263",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "c10/cuda/CUDAFunctions.cpp",
        "discussion_id": "2145461056",
        "commented_code": "@@ -134,6 +134,10 @@ void set_device(DeviceIndex device) {\n   C10_CUDA_CHECK(c10::cuda::SetDevice(device));\n }\n \n+void set_device(DeviceIndex device, const bool force) {",
        "comment_created_at": "2025-07-03T17:13:14+00:00",
        "comment_author": "jerryzh168",
        "comment_body": "this is a BC breaking change? why break BC? can you add a default arg for this function? @Aidyn-A ",
        "pr_file_module": null
      },
      {
        "comment_id": "2190748591",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "c10/cuda/CUDAFunctions.cpp",
        "discussion_id": "2145461056",
        "commented_code": "@@ -134,6 +134,10 @@ void set_device(DeviceIndex device) {\n   C10_CUDA_CHECK(c10::cuda::SetDevice(device));\n }\n \n+void set_device(DeviceIndex device, const bool force) {",
        "comment_created_at": "2025-07-07T18:07:12+00:00",
        "comment_author": "minosfuture",
        "comment_body": "default arg doesn't help. it's the signature parameter incompatibility.\r\nthere is already default arg: https://github.com/pytorch/pytorch/blob/main/c10/cuda/CUDAFunctions.h#L42",
        "pr_file_module": null
      },
      {
        "comment_id": "2190760360",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "c10/cuda/CUDAFunctions.cpp",
        "discussion_id": "2145461056",
        "commented_code": "@@ -134,6 +134,10 @@ void set_device(DeviceIndex device) {\n   C10_CUDA_CHECK(c10::cuda::SetDevice(device));\n }\n \n+void set_device(DeviceIndex device, const bool force) {",
        "comment_created_at": "2025-07-07T18:14:51+00:00",
        "comment_author": "janeyx99",
        "comment_body": "Is it possible to just have both \r\n```\r\nvoid set_device(DeviceIndex device) {\r\n  C10_CUDA_CHECK(c10::cuda::SetDevice(device));\r\n```\r\n\r\nand the new one to avoid breaking BC?",
        "pr_file_module": null
      },
      {
        "comment_id": "2190762441",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "c10/cuda/CUDAFunctions.cpp",
        "discussion_id": "2145461056",
        "commented_code": "@@ -134,6 +134,10 @@ void set_device(DeviceIndex device) {\n   C10_CUDA_CHECK(c10::cuda::SetDevice(device));\n }\n \n+void set_device(DeviceIndex device, const bool force) {",
        "comment_created_at": "2025-07-07T18:16:06+00:00",
        "comment_author": "janeyx99",
        "comment_body": "~This PR does seem to break BC and will need a BC breaking note if we don't forward fix this https://docs.google.com/document/d/14OmgGBr1w6gl1VO47GGGdwrIaUNr92DFhQbY_NEk8mQ/edit?tab=t.0#heading=h.a9htwgvvec1m  (also cc @jbschlosser)~\r\n\r\nSorry I take it back! I didn't realize the default arg already existed. Our BC policy is for API today, not ABI.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2149898697",
    "pr_number": 155900,
    "pr_file": "torch/csrc/cuda/Module.cpp",
    "created_at": "2025-06-16T12:30:38+00:00",
    "commented_code": "auto device = THPUtils_unpackLong(arg);\n \n   torch::utils::device_lazy_init(at::kCUDA);\n-  c10::cuda::set_device(static_cast<c10::DeviceIndex>(device));\n+  c10::cuda::set_device(static_cast<c10::DeviceIndex>(device), /*force*/ true);",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2149898697",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "torch/csrc/cuda/Module.cpp",
        "discussion_id": "2149898697",
        "commented_code": "@@ -64,7 +64,7 @@ PyObject* THCPModule_setDevice_wrap(PyObject* self, PyObject* arg) {\n   auto device = THPUtils_unpackLong(arg);\n \n   torch::utils::device_lazy_init(at::kCUDA);\n-  c10::cuda::set_device(static_cast<c10::DeviceIndex>(device));\n+  c10::cuda::set_device(static_cast<c10::DeviceIndex>(device), /*force*/ true);",
        "comment_created_at": "2025-06-16T12:30:38+00:00",
        "comment_author": "youkaichao",
        "comment_body": "do we need to expose the `force` argument to python-side API?",
        "pr_file_module": null
      },
      {
        "comment_id": "2149902930",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "torch/csrc/cuda/Module.cpp",
        "discussion_id": "2149898697",
        "commented_code": "@@ -64,7 +64,7 @@ PyObject* THCPModule_setDevice_wrap(PyObject* self, PyObject* arg) {\n   auto device = THPUtils_unpackLong(arg);\n \n   torch::utils::device_lazy_init(at::kCUDA);\n-  c10::cuda::set_device(static_cast<c10::DeviceIndex>(device));\n+  c10::cuda::set_device(static_cast<c10::DeviceIndex>(device), /*force*/ true);",
        "comment_created_at": "2025-06-16T12:32:53+00:00",
        "comment_author": "Aidyn-A",
        "comment_body": "No, this should be an internal feature only.",
        "pr_file_module": null
      },
      {
        "comment_id": "2149915806",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155900,
        "pr_file": "torch/csrc/cuda/Module.cpp",
        "discussion_id": "2149898697",
        "commented_code": "@@ -64,7 +64,7 @@ PyObject* THCPModule_setDevice_wrap(PyObject* self, PyObject* arg) {\n   auto device = THPUtils_unpackLong(arg);\n \n   torch::utils::device_lazy_init(at::kCUDA);\n-  c10::cuda::set_device(static_cast<c10::DeviceIndex>(device));\n+  c10::cuda::set_device(static_cast<c10::DeviceIndex>(device), /*force*/ true);",
        "comment_created_at": "2025-06-16T12:40:04+00:00",
        "comment_author": "youkaichao",
        "comment_body": "sounds good, thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1939924463",
    "pr_number": 145000,
    "pr_file": "torch/csrc/Module.cpp",
    "created_at": "2025-02-03T19:35:59+00:00",
    "commented_code": "METH_NOARGS,\n      nullptr},\n     {\"_to_dlpack\", THPModule_toDLPack, METH_O, nullptr},\n+    {\"_to_dlpack_unversioned\", THPModule_toDLPackUnversioned, METH_O, nullptr},",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1939924463",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 145000,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "1939924463",
        "commented_code": "@@ -1599,6 +1616,7 @@ static std::initializer_list<PyMethodDef> TorchMethods = {\n      METH_NOARGS,\n      nullptr},\n     {\"_to_dlpack\", THPModule_toDLPack, METH_O, nullptr},\n+    {\"_to_dlpack_unversioned\", THPModule_toDLPackUnversioned, METH_O, nullptr},",
        "comment_created_at": "2025-02-03T19:35:59+00:00",
        "comment_author": "albanD",
        "comment_body": "Why do we still need this one?",
        "pr_file_module": null
      },
      {
        "comment_id": "1941463598",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 145000,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "1939924463",
        "commented_code": "@@ -1599,6 +1616,7 @@ static std::initializer_list<PyMethodDef> TorchMethods = {\n      METH_NOARGS,\n      nullptr},\n     {\"_to_dlpack\", THPModule_toDLPack, METH_O, nullptr},\n+    {\"_to_dlpack_unversioned\", THPModule_toDLPackUnversioned, METH_O, nullptr},",
        "comment_created_at": "2025-02-04T16:05:23+00:00",
        "comment_author": "rgommers",
        "comment_body": "This does look necessary to me. For DLPack to change the ABI once, there's a dance that needs doing to be not-super-disruptive: continue returning the old (0.8) version, unless the consumer indicates it can handle the new (1.X) version by passing in `max_version=(1, 0)` (or `(1, x)` in the future).",
        "pr_file_module": null
      }
    ]
  }
]