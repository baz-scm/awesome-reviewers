[
  {
    "discussion_id": "2160529392",
    "pr_number": 156187,
    "pr_file": "torch/_inductor/compile_fx.py",
    "created_at": "2025-06-22T23:38:09+00:00",
    "commented_code": "NB: This function TAKES OWNERSHIP of the input ``model_`` and can potentially\n     mutate it!  Make a copy if you need to preserve the original GraphModule.\n     \"\"\"\n+    # Wake up the AsyncCompile subproc pool as early as possible (if there's cuda).\n+    if any(\n+        isinstance(e, torch.Tensor) and e.device.type == \"cuda\" for e in example_inputs_",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2160529392",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156187,
        "pr_file": "torch/_inductor/compile_fx.py",
        "discussion_id": "2160529392",
        "commented_code": "@@ -1993,6 +1993,11 @@ def compile_fx(\n     NB: This function TAKES OWNERSHIP of the input ``model_`` and can potentially\n     mutate it!  Make a copy if you need to preserve the original GraphModule.\n     \"\"\"\n+    # Wake up the AsyncCompile subproc pool as early as possible (if there's cuda).\n+    if any(\n+        isinstance(e, torch.Tensor) and e.device.type == \"cuda\" for e in example_inputs_",
        "comment_created_at": "2025-06-22T23:38:09+00:00",
        "comment_author": "etaf",
        "comment_body": "Hi, \"xpu\" is also the one with triton backend, would you like to add it here?",
        "pr_file_module": null
      },
      {
        "comment_id": "2164974113",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156187,
        "pr_file": "torch/_inductor/compile_fx.py",
        "discussion_id": "2160529392",
        "commented_code": "@@ -1993,6 +1993,11 @@ def compile_fx(\n     NB: This function TAKES OWNERSHIP of the input ``model_`` and can potentially\n     mutate it!  Make a copy if you need to preserve the original GraphModule.\n     \"\"\"\n+    # Wake up the AsyncCompile subproc pool as early as possible (if there's cuda).\n+    if any(\n+        isinstance(e, torch.Tensor) and e.device.type == \"cuda\" for e in example_inputs_",
        "comment_created_at": "2025-06-24T21:45:22+00:00",
        "comment_author": "masnesral",
        "comment_body": "Sure can. I actually don't even know if checking the device on the inputs is a reasonable way to infer that we're going to need the Triton compilation pool. Hoping @jansel can educate me.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2188847546",
    "pr_number": 157679,
    "pr_file": "test/inductor/test_multi_kernel.py",
    "created_at": "2025-07-07T02:30:28+00:00",
    "commented_code": "else:\n             self.assertFalse(_contains_multi_kernel_code(wrapper_code))\n \n+    @requires_triton()\n+    @unittest.skipIf(not IS_BIG_GPU, \"templates require big gpu\")\n+    def test_triton_gemm(self):\n+        def fn(x, y):\n+            return x @ y\n+\n+        compiled_fn = torch.compile(\n+            fn,\n+            options={\n+                \"max_autotune\": True,\n+                \"max_autotune_gemm_backends\": \"TRITON\",\n+            },\n+        )\n+        x = torch.randn(4096, 4096, device=\"cuda\")",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2188847546",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157679,
        "pr_file": "test/inductor/test_multi_kernel.py",
        "discussion_id": "2188847546",
        "commented_code": "@@ -91,6 +97,56 @@ def test_softmax(self, expect_multi_kernel=True):\n         else:\n             self.assertFalse(_contains_multi_kernel_code(wrapper_code))\n \n+    @requires_triton()\n+    @unittest.skipIf(not IS_BIG_GPU, \"templates require big gpu\")\n+    def test_triton_gemm(self):\n+        def fn(x, y):\n+            return x @ y\n+\n+        compiled_fn = torch.compile(\n+            fn,\n+            options={\n+                \"max_autotune\": True,\n+                \"max_autotune_gemm_backends\": \"TRITON\",\n+            },\n+        )\n+        x = torch.randn(4096, 4096, device=\"cuda\")",
        "comment_created_at": "2025-07-07T02:30:28+00:00",
        "comment_author": "etaf",
        "comment_body": "Hi, could you please replace the hardcode `\"cuda\"` with `GPU_TYPE` in this case to avoid `no cuda` crash when runing on other GPU.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1861233895",
    "pr_number": 140979,
    "pr_file": "torch/cuda/graphs.py",
    "created_at": "2024-11-27T20:52:21+00:00",
    "commented_code": "may be unsafe. \"global\" will error on actions in other threads, \"thread_local\" will only error for\n             actions in the current thread, and \"relaxed\" will not error on actions. Do NOT change this setting\n             unless you're familiar with `cudaStreamCaptureMode <https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85>`_\n+        collect_garbage (bool, optional): If True, call torch.cuda.synchronize() followed by gc.collect() to free",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1861233895",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 140979,
        "pr_file": "torch/cuda/graphs.py",
        "discussion_id": "1861233895",
        "commented_code": "@@ -132,6 +135,11 @@ class graph:\n             may be unsafe. \"global\" will error on actions in other threads, \"thread_local\" will only error for\n             actions in the current thread, and \"relaxed\" will not error on actions. Do NOT change this setting\n             unless you're familiar with `cudaStreamCaptureMode <https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85>`_\n+        collect_garbage (bool, optional): If True, call torch.cuda.synchronize() followed by gc.collect() to free",
        "comment_created_at": "2024-11-27T20:52:21+00:00",
        "comment_author": "eellison",
        "comment_body": "This is currently only set to False in `ControlFlowOpWarmupDispatchMode`.  So, I guess the use case there is that someone if capturing a cuda graph in eager and calls into a torch.compile wrapped function ? Wouldn't we expect that they should have already warmed up the function once prior to capture ? \r\n\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "1865296932",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 140979,
        "pr_file": "torch/cuda/graphs.py",
        "discussion_id": "1861233895",
        "commented_code": "@@ -132,6 +135,11 @@ class graph:\n             may be unsafe. \"global\" will error on actions in other threads, \"thread_local\" will only error for\n             actions in the current thread, and \"relaxed\" will not error on actions. Do NOT change this setting\n             unless you're familiar with `cudaStreamCaptureMode <https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85>`_\n+        collect_garbage (bool, optional): If True, call torch.cuda.synchronize() followed by gc.collect() to free",
        "comment_created_at": "2024-12-02T06:34:40+00:00",
        "comment_author": "galv",
        "comment_body": "Fundamentally, the challenge is that my current approach to this can cause two distinct cuda stream captures to happen at the same time. \r\n\r\nFirst of all, pytorch never actually intended for more than one stream capture to be possible, from what I can see. This is clearly stated in the documentation of torch.cuda.graph() See here: https://github.com/pytorch/pytorch/pull/140979/files#diff-39e542d87359e7d5381d036cbcea9ec759fbe469578bcdc5693ce6cfab7f1a54R187-R188 If torch.cuda.graph is constructed without a stream, every call to __init__ will use the same single cuda stream. And calling cudaStreamBeginCapture() on an already capturing stream will fail.\r\n\r\nAs you see, entering the torch.cuda.graph() context manager will call torch.cuda.synchronize(). Unfortunately, torch.cuda.synchronize() will basically call cudaStreamSynchronize() on every stream in the cuda context, and cudaStreamSynchronize() will invalidate stream capture for a capturing stream. \r\n\r\nThat is why I added this boolean flag, since my \"warmup\" TorchDispatchMode will always start a second stream capture when another stream capture is already going on.\r\n\r\nUnfortunately, there is more to it than that.\r\n\r\nThe warmup TorchDispatchMode does two things:\r\n\r\n1. It ensures that all conditionally-executed code paths are \"warmed up\" by running them at least once.\r\n2. It avoids side effects while doing (1) by \"stubbing\" out all kernel launches via doing relaxed stream capture  (side effects are not acceptable because I don't want both sides of an if-else statement to execute. That would never happen during normal execution, and could therefore put the program into an undefined state).\r\n\r\nUnfortunately, even though I do relaxed stream capture on the second stream, the first stream caputre may be in global or thread local stream capture mode. There is no way to temporarily \"turn off\" stream capture on a stream and then turn it back on right now (though in principle, I see no reason why we couldn't do it). This means that unsafe actions done during the warmup will still invalidate the first stream's capture. Thus, I need to change the thread's capture mode to relaxed at various points, which I'm not happy with.\r\n\r\nPytorch has succeeded so far with just using the torch.cuda.graph() context manager for capturing (though someone could use torch.cuda.CUDAGraph()'s APIs directly). This is the first time that what code we want to execute will differ depending upon whether a stream is capturing or not.\r\n\r\nI can probably keep running with pytorch's current assumption that only one stream can do capture at a time if the user passed me a function describing the work that they wanted to do. I would then make a wrapper function over that uses torch.cuda.graph() twice, sequentially, once in relaxed mode, and then again in the user-requested mode.\r\n\r\nAs you can see, there are unfortunately quite a few annoying details here. But I am inclined to agree with you that there is probably an easier, safer way to do this.",
        "pr_file_module": null
      },
      {
        "comment_id": "1874096286",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 140979,
        "pr_file": "torch/cuda/graphs.py",
        "discussion_id": "1861233895",
        "commented_code": "@@ -132,6 +135,11 @@ class graph:\n             may be unsafe. \"global\" will error on actions in other threads, \"thread_local\" will only error for\n             actions in the current thread, and \"relaxed\" will not error on actions. Do NOT change this setting\n             unless you're familiar with `cudaStreamCaptureMode <https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85>`_\n+        collect_garbage (bool, optional): If True, call torch.cuda.synchronize() followed by gc.collect() to free",
        "comment_created_at": "2024-12-06T22:51:52+00:00",
        "comment_author": "eellison",
        "comment_body": "> That is why I added this boolean flag, since my \"warmup\" TorchDispatchMode will always start a second stream capture when another stream capture is already going on.\r\n\r\nI think a lot of these problems stem from trying to do warmup when stream capture is already on.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2115182133",
    "pr_number": 154679,
    "pr_file": "test/inductor/test_torchinductor.py",
    "created_at": "2025-05-30T05:45:40+00:00",
    "commented_code": "compiled_out = f_compiled(x, y)\n             self.assertEqual(compiled_out, f(x, y))\n \n+        @torch._inductor.config.patch(\"graph_partition\", True)\n+        def test_graph_partition_symint_cat_backward(self):\n+            def f(x, w):\n+                y = torch.cat((x, x), dim=0)\n+                z = y @ w\n+                return z @ z.T\n+\n+            compiled_f = torch.compile(f)\n+\n+            for shape in (2, 3):\n+                torch.manual_seed(42)\n+                eager_x = torch.randn(shape, 2, device=\"cuda\")",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2115182133",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154679,
        "pr_file": "test/inductor/test_torchinductor.py",
        "discussion_id": "2115182133",
        "commented_code": "@@ -14824,6 +14824,27 @@ def f(x, y):\n             compiled_out = f_compiled(x, y)\n             self.assertEqual(compiled_out, f(x, y))\n \n+        @torch._inductor.config.patch(\"graph_partition\", True)\n+        def test_graph_partition_symint_cat_backward(self):\n+            def f(x, w):\n+                y = torch.cat((x, x), dim=0)\n+                z = y @ w\n+                return z @ z.T\n+\n+            compiled_f = torch.compile(f)\n+\n+            for shape in (2, 3):\n+                torch.manual_seed(42)\n+                eager_x = torch.randn(shape, 2, device=\"cuda\")",
        "comment_created_at": "2025-05-30T05:45:40+00:00",
        "comment_author": "etaf",
        "comment_body": "Hi, May I suggest to replace the hard code `\"cuda\"` with `GPU_TYPE` in this case ? Otherwise the `\"cuda\"` will fail on other GPUs.",
        "pr_file_module": null
      },
      {
        "comment_id": "2115187030",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154679,
        "pr_file": "test/inductor/test_torchinductor.py",
        "discussion_id": "2115182133",
        "commented_code": "@@ -14824,6 +14824,27 @@ def f(x, y):\n             compiled_out = f_compiled(x, y)\n             self.assertEqual(compiled_out, f(x, y))\n \n+        @torch._inductor.config.patch(\"graph_partition\", True)\n+        def test_graph_partition_symint_cat_backward(self):\n+            def f(x, w):\n+                y = torch.cat((x, x), dim=0)\n+                z = y @ w\n+                return z @ z.T\n+\n+            compiled_f = torch.compile(f)\n+\n+            for shape in (2, 3):\n+                torch.manual_seed(42)\n+                eager_x = torch.randn(shape, 2, device=\"cuda\")",
        "comment_created_at": "2025-05-30T05:50:30+00:00",
        "comment_author": "BoyuanFeng",
        "comment_body": "yes thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2166162800",
    "pr_number": 156796,
    "pr_file": "test/distributed/checkpoint/test_state_dict.py",
    "created_at": "2025-06-25T08:43:36+00:00",
    "commented_code": "from torch.utils._pytree import tree_all, tree_all_only\n \n \n+device_type = torch.accelerator.current_accelerator().type",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2166162800",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156796,
        "pr_file": "test/distributed/checkpoint/test_state_dict.py",
        "discussion_id": "2166162800",
        "commented_code": "@@ -62,6 +62,9 @@\n from torch.utils._pytree import tree_all, tree_all_only\n \n \n+device_type = torch.accelerator.current_accelerator().type",
        "comment_created_at": "2025-06-25T08:43:36+00:00",
        "comment_author": "zeshengzong",
        "comment_body": "Hi, `torch.accelerator.current_accelerator()` will return `None` when accelerator not detected, is there a check before running these tests? Thanks!",
        "pr_file_module": null
      },
      {
        "comment_id": "2167354355",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156796,
        "pr_file": "test/distributed/checkpoint/test_state_dict.py",
        "discussion_id": "2166162800",
        "commented_code": "@@ -62,6 +62,9 @@\n from torch.utils._pytree import tree_all, tree_all_only\n \n \n+device_type = torch.accelerator.current_accelerator().type",
        "comment_created_at": "2025-06-25T18:31:47+00:00",
        "comment_author": "harikodali",
        "comment_body": "thanks for spotting, it's an oversight, changed it to use get_devtype from common_fsdp instead",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2176247028",
    "pr_number": 157317,
    "pr_file": "torch/distributed/tensor/examples/visualize_sharding_example.py",
    "created_at": "2025-07-01T01:56:31+00:00",
    "commented_code": "assert int(os.getenv(\"WORLD_SIZE\", \"1\")) >= 4, \"We need at least 4 devices\"\n rank = int(os.environ[\"RANK\"])\n \n+def get_device_type() -> str:\n+    return (\n+        torch.accelerator.current_accelerator().type\n+        if  torch.accelerator.current_accelerator() and torch.accelerator.device_count()\n+        else \"cpu\"\n+    )",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2176247028",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157317,
        "pr_file": "torch/distributed/tensor/examples/visualize_sharding_example.py",
        "discussion_id": "2176247028",
        "commented_code": "@@ -17,6 +17,15 @@\n assert int(os.getenv(\"WORLD_SIZE\", \"1\")) >= 4, \"We need at least 4 devices\"\n rank = int(os.environ[\"RANK\"])\n \n+def get_device_type() -> str:\n+    return (\n+        torch.accelerator.current_accelerator().type\n+        if  torch.accelerator.current_accelerator() and torch.accelerator.device_count()\n+        else \"cpu\"\n+    )",
        "comment_created_at": "2025-07-01T01:56:31+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n    return acc.type if (acc := torch.accelerator.current_accelerator(True)) else \"cpu\"\r\n\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2176248163",
    "pr_number": 157317,
    "pr_file": "torch/distributed/tensor/examples/comm_mode_features_example.py",
    "created_at": "2025-07-01T01:58:07+00:00",
    "commented_code": "def get_device_type() -> str:\n     return (\n-        \"cuda\"\n-        if torch.cuda.is_available() and torch.cuda.device_count() >= 4\n+        torch.accelerator.current_accelerator().type\n+        if  torch.accelerator.current_accelerator() and torch.accelerator.device_count() >= 4",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2176248163",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157317,
        "pr_file": "torch/distributed/tensor/examples/comm_mode_features_example.py",
        "discussion_id": "2176248163",
        "commented_code": "@@ -28,12 +28,13 @@\n \n def get_device_type() -> str:\n     return (\n-        \"cuda\"\n-        if torch.cuda.is_available() and torch.cuda.device_count() >= 4\n+        torch.accelerator.current_accelerator().type\n+        if  torch.accelerator.current_accelerator() and torch.accelerator.device_count() >= 4",
        "comment_created_at": "2025-07-01T01:58:07+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n        if torch.accelerator.device_count() >= 4\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2116967854",
    "pr_number": 153751,
    "pr_file": "test/inductor/test_loop_ordering.py",
    "created_at": "2025-05-31T01:18:56+00:00",
    "commented_code": "arg0_1 = torch.randn([XDIM, YDIM], device=GPU_TYPE, dtype=torch.bfloat16)\n             permute = torch.ops.aten.permute.default(arg0_1, [1, 0])\n \n-            torch.compile(forward)(permute)\n+            out, code = run_and_get_code(torch.compile(forward), (permute))\n+\n+            self.assertEqual(out, forward(permute))\n+            FileCheck().check(\"YBLOCK\").check(\"XBLOCK\").run(code[0])\n+\n+\n+layouts = (\"cont\", \"NHWC\", \"T\")\n+\n+\n+@inductor_config.patch(\n+    {\n+        \"triton.unique_kernel_names\": True,\n+        \"loop_ordering_after_fusion\": True,\n+        \"test_configs.global_tiling_analysis\": True,\n+        \"triton.max_tiles\" : 3,\n+    }\n+)\n+@instantiate_parametrized_tests\n+class TestTiling(TestCase):\n+    def T(self, layout: str):\n+        SIZE_A = 128\n+        SIZE_B = 256\n+        SIZE_C = 512\n+\n+        if layout == \"cont\":\n+            return torch.rand(SIZE_A, SIZE_B, SIZE_C, device=GPU_TYPE).unsqueeze(0)\n+        elif layout == \"T\":\n+            return (\n+                torch.rand(SIZE_A, SIZE_B, SIZE_C, device=GPU_TYPE)\n+                .transpose(1, 2)\n+                .contiguous()\n+                .transpose(1, 2)\n+                .unsqueeze(0)\n+            )\n+        else:\n+            assert layout == \"NHWC\"\n+            return torch.rand([1, SIZE_A, SIZE_B, SIZE_C], device=GPU_TYPE).to(\n+                memory_format=torch.channels_last\n+            )\n+\n+    @parametrize(\"a\", layouts)\n+    @parametrize(\"b\", layouts)\n+    def test_pointwise(self, a, b):\n+        def foo(x, y):\n+            return x + y\n+\n+        x, y = self.T(a), self.T(b)\n+        res, code = run_and_get_code(torch.compile(foo), x, y)\n+\n+        if a != b:\n+            FileCheck().check(\"ynumel\").run(code[0])\n+        else:\n+            FileCheck().check_not(\"ynumel\").run(code[0])\n+\n+        self.assertEqual(res, foo(x, y))\n+\n+    def test_tiled_reduction(self):\n+        def f(a, b):\n+            return (a * b).sum(dim=-1)\n+\n+        N = 512\n+        inps = (\n+            torch.randn(N, N, N, device=GPU_TYPE).permute(2, 1, 0),\n+            torch.randn(N, N, N, device=GPU_TYPE).permute(1, 2, 0),\n+        )\n+        f_c = torch.compile(f)\n+        out, code = run_and_get_code(f_c, *inps)\n+\n+        FileCheck().check_dag(\"xnumel = 512\").check_dag(\"ynumel = 512\").check_dag(\n+            \"rnumel\"\n+        ).run(code[0])\n+        self.assertEqual(out, f(*inps), atol=0.001, rtol=0.04)\n+\n+    def test_3d_pointwise(self):\n+        inps = (self.T(\"cont\"), self.T(\"T\"), self.T(\"NHWC\"))\n+\n+        def f(x, y, z):\n+            return x + y + z\n+\n+        f_c = torch.compile(f)\n+        out, code = run_and_get_code(f_c, *inps)\n+\n+        FileCheck().check_dag(\"znumel\").check_dag(\"ynumel\").check_dag(\"xnumel\").run(\n+            code[0]\n+        )\n+        self.assertEqual(out, f(*inps))\n+\n+    def test_cat(self):\n+        # test unwrapping Identity\n+\n+        def f(x, y):\n+            return torch.cat((x, y)) + 1\n+\n+        x = self.T(\"cont\")\n+        y = self.T(\"T\")\n+\n+        inps = (x, y)\n+\n+        f_c = torch.compile(f)\n+        out, code = run_and_get_code(f_c, *inps)\n+        FileCheck().check_dag(\"ynumel\").check_dag(\"xnumel\").run(code[0])\n+        self.assertEqual(out, f(*inps))\n+\n+    def test_penalized_small_dim(self):\n+        x = torch.rand([2000, 1], device=\"cuda\")",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2116967854",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153751,
        "pr_file": "test/inductor/test_loop_ordering.py",
        "discussion_id": "2116967854",
        "commented_code": "@@ -893,7 +895,155 @@ def forward(permute):\n             arg0_1 = torch.randn([XDIM, YDIM], device=GPU_TYPE, dtype=torch.bfloat16)\n             permute = torch.ops.aten.permute.default(arg0_1, [1, 0])\n \n-            torch.compile(forward)(permute)\n+            out, code = run_and_get_code(torch.compile(forward), (permute))\n+\n+            self.assertEqual(out, forward(permute))\n+            FileCheck().check(\"YBLOCK\").check(\"XBLOCK\").run(code[0])\n+\n+\n+layouts = (\"cont\", \"NHWC\", \"T\")\n+\n+\n+@inductor_config.patch(\n+    {\n+        \"triton.unique_kernel_names\": True,\n+        \"loop_ordering_after_fusion\": True,\n+        \"test_configs.global_tiling_analysis\": True,\n+        \"triton.max_tiles\" : 3,\n+    }\n+)\n+@instantiate_parametrized_tests\n+class TestTiling(TestCase):\n+    def T(self, layout: str):\n+        SIZE_A = 128\n+        SIZE_B = 256\n+        SIZE_C = 512\n+\n+        if layout == \"cont\":\n+            return torch.rand(SIZE_A, SIZE_B, SIZE_C, device=GPU_TYPE).unsqueeze(0)\n+        elif layout == \"T\":\n+            return (\n+                torch.rand(SIZE_A, SIZE_B, SIZE_C, device=GPU_TYPE)\n+                .transpose(1, 2)\n+                .contiguous()\n+                .transpose(1, 2)\n+                .unsqueeze(0)\n+            )\n+        else:\n+            assert layout == \"NHWC\"\n+            return torch.rand([1, SIZE_A, SIZE_B, SIZE_C], device=GPU_TYPE).to(\n+                memory_format=torch.channels_last\n+            )\n+\n+    @parametrize(\"a\", layouts)\n+    @parametrize(\"b\", layouts)\n+    def test_pointwise(self, a, b):\n+        def foo(x, y):\n+            return x + y\n+\n+        x, y = self.T(a), self.T(b)\n+        res, code = run_and_get_code(torch.compile(foo), x, y)\n+\n+        if a != b:\n+            FileCheck().check(\"ynumel\").run(code[0])\n+        else:\n+            FileCheck().check_not(\"ynumel\").run(code[0])\n+\n+        self.assertEqual(res, foo(x, y))\n+\n+    def test_tiled_reduction(self):\n+        def f(a, b):\n+            return (a * b).sum(dim=-1)\n+\n+        N = 512\n+        inps = (\n+            torch.randn(N, N, N, device=GPU_TYPE).permute(2, 1, 0),\n+            torch.randn(N, N, N, device=GPU_TYPE).permute(1, 2, 0),\n+        )\n+        f_c = torch.compile(f)\n+        out, code = run_and_get_code(f_c, *inps)\n+\n+        FileCheck().check_dag(\"xnumel = 512\").check_dag(\"ynumel = 512\").check_dag(\n+            \"rnumel\"\n+        ).run(code[0])\n+        self.assertEqual(out, f(*inps), atol=0.001, rtol=0.04)\n+\n+    def test_3d_pointwise(self):\n+        inps = (self.T(\"cont\"), self.T(\"T\"), self.T(\"NHWC\"))\n+\n+        def f(x, y, z):\n+            return x + y + z\n+\n+        f_c = torch.compile(f)\n+        out, code = run_and_get_code(f_c, *inps)\n+\n+        FileCheck().check_dag(\"znumel\").check_dag(\"ynumel\").check_dag(\"xnumel\").run(\n+            code[0]\n+        )\n+        self.assertEqual(out, f(*inps))\n+\n+    def test_cat(self):\n+        # test unwrapping Identity\n+\n+        def f(x, y):\n+            return torch.cat((x, y)) + 1\n+\n+        x = self.T(\"cont\")\n+        y = self.T(\"T\")\n+\n+        inps = (x, y)\n+\n+        f_c = torch.compile(f)\n+        out, code = run_and_get_code(f_c, *inps)\n+        FileCheck().check_dag(\"ynumel\").check_dag(\"xnumel\").run(code[0])\n+        self.assertEqual(out, f(*inps))\n+\n+    def test_penalized_small_dim(self):\n+        x = torch.rand([2000, 1], device=\"cuda\")",
        "comment_created_at": "2025-05-31T01:18:56+00:00",
        "comment_author": "etaf",
        "comment_body": "Hi\uff0c may I suggest to replace the hard code `\"cuda\"` in this case so that it won't fail on XPU, thanks.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2093785117",
    "pr_number": 153730,
    "pr_file": "test/inductor/test_loop_ordering.py",
    "created_at": "2025-05-17T00:07:13+00:00",
    "commented_code": "foo(torch.rand(1024, device=\"cuda\"))\n \n+    def test_coalescing(self):\n+        from torch._inductor import tiling_utils\n+\n+        # Define symbolic variables\n+        i, j, n, m = sympy.symbols(\"i j n m\", integer=True)\n+\n+        # Test cases: (expression, var_ranges, expected_result)\n+        test_cases = [\n+            # Simple direct case\n+            (i + j * 5, {i: 10, j: 8}, i),\n+            # Floor division case\n+            (i + FloorDiv(j, 2), {i: 4, j: 8}, i),\n+            # Modular indexing\n+            (i * 10 + ModularIndexing(j, 1, 3), {i: 5, j: 10}, j),\n+            # Case with no coalescing variable\n+            (i * 2 + j * 3, {i: 8, j: 5}, None),\n+            # Division case\n+            (i / 2, {i: 10}, None),\n+            # More complex floor division\n+            (j + FloorDiv(i, 3), {i: 6, j: 12}, j),\n+            # Addition inside modular indexing\n+            (ModularIndexing(i + 3, 1, 5), {i: 8, j: 12}, i),\n+        ]\n+\n+        for expr, var_ranges, expected in test_cases:\n+            # Test the function\n+            result = tiling_utils.find_coalesced_var(expr, var_ranges)\n+            self.assertEqual(result, expected)\n+\n+    @parametrize(\"downcast_transposed_v\", (False, True))\n+    def test_tiled_coalesce_analysis(self, downcast_transposed_v):\n+        # test one pw var, one red var\n+        from torch._inductor import tiling_utils\n+\n+        def fn(nodes):\n+            self.assertTrue(len(nodes) == 1)\n+\n+            coalesce_analysis = tiling_utils.analyze_memory_coalescing(nodes[0])\n+\n+            i_vars = coalesce_analysis.norm_read_writes.index_vars\n+\n+            # because output is contiguous, second dimension should\n+            # coalesce twice as many bytes as first dimension\n+            # if not downcasted\n+            # if downcasted, should be equal, bc larger dtype size\n+            cont_reads = coalesce_analysis.coalesced_by_var[i_vars[1]]\n+            t_reads = coalesce_analysis.coalesced_by_var[i_vars[0]]\n+\n+            if not downcast_transposed_v:\n+                self.assertEqual(cont_reads, t_reads * 2)\n+            else:\n+                self.assertEqual(cont_reads, t_reads)\n+\n+            return nodes\n+\n+        with torch._inductor.config.patch(_post_fusion_custom_pass=fn), torch.no_grad():\n+\n+            @torch.compile()\n+            def foo(x, y):\n+                return x + y.to(x.dtype)\n+\n+            y_dtype = torch.float if not downcast_transposed_v else torch.float64\n+            foo(\n+                torch.rand(256, 256, device=\"cuda\"),",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2093785117",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153730,
        "pr_file": "test/inductor/test_loop_ordering.py",
        "discussion_id": "2093785117",
        "commented_code": "@@ -741,6 +742,73 @@ def foo(x):\n \n             foo(torch.rand(1024, device=\"cuda\"))\n \n+    def test_coalescing(self):\n+        from torch._inductor import tiling_utils\n+\n+        # Define symbolic variables\n+        i, j, n, m = sympy.symbols(\"i j n m\", integer=True)\n+\n+        # Test cases: (expression, var_ranges, expected_result)\n+        test_cases = [\n+            # Simple direct case\n+            (i + j * 5, {i: 10, j: 8}, i),\n+            # Floor division case\n+            (i + FloorDiv(j, 2), {i: 4, j: 8}, i),\n+            # Modular indexing\n+            (i * 10 + ModularIndexing(j, 1, 3), {i: 5, j: 10}, j),\n+            # Case with no coalescing variable\n+            (i * 2 + j * 3, {i: 8, j: 5}, None),\n+            # Division case\n+            (i / 2, {i: 10}, None),\n+            # More complex floor division\n+            (j + FloorDiv(i, 3), {i: 6, j: 12}, j),\n+            # Addition inside modular indexing\n+            (ModularIndexing(i + 3, 1, 5), {i: 8, j: 12}, i),\n+        ]\n+\n+        for expr, var_ranges, expected in test_cases:\n+            # Test the function\n+            result = tiling_utils.find_coalesced_var(expr, var_ranges)\n+            self.assertEqual(result, expected)\n+\n+    @parametrize(\"downcast_transposed_v\", (False, True))\n+    def test_tiled_coalesce_analysis(self, downcast_transposed_v):\n+        # test one pw var, one red var\n+        from torch._inductor import tiling_utils\n+\n+        def fn(nodes):\n+            self.assertTrue(len(nodes) == 1)\n+\n+            coalesce_analysis = tiling_utils.analyze_memory_coalescing(nodes[0])\n+\n+            i_vars = coalesce_analysis.norm_read_writes.index_vars\n+\n+            # because output is contiguous, second dimension should\n+            # coalesce twice as many bytes as first dimension\n+            # if not downcasted\n+            # if downcasted, should be equal, bc larger dtype size\n+            cont_reads = coalesce_analysis.coalesced_by_var[i_vars[1]]\n+            t_reads = coalesce_analysis.coalesced_by_var[i_vars[0]]\n+\n+            if not downcast_transposed_v:\n+                self.assertEqual(cont_reads, t_reads * 2)\n+            else:\n+                self.assertEqual(cont_reads, t_reads)\n+\n+            return nodes\n+\n+        with torch._inductor.config.patch(_post_fusion_custom_pass=fn), torch.no_grad():\n+\n+            @torch.compile()\n+            def foo(x, y):\n+                return x + y.to(x.dtype)\n+\n+            y_dtype = torch.float if not downcast_transposed_v else torch.float64\n+            foo(\n+                torch.rand(256, 256, device=\"cuda\"),",
        "comment_created_at": "2025-05-17T00:07:13+00:00",
        "comment_author": "etaf",
        "comment_body": "Hi, May I suggest we mark this case as requires_cuda or replace the hardcode `cuda` with GPU_TYPE here? The `cuda` will  fail on XPU.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2177758844",
    "pr_number": 156287,
    "pr_file": "torch/_functorch/partitioners.py",
    "created_at": "2025-07-01T14:28:29+00:00",
    "commented_code": "):\n         with no_dispatch(), unset_fake_temporarily():\n             objects = [[x.name for x in saved_values]]\n-            # TODO: maybe use a different process group for this\n-            torch.distributed.broadcast_object_list(objects, src=0)\n-            saved_values_names = objects[0]\n+            saved_ops_names_all_ranks: list[list[str]] = [\n+                [] for _ in range(torch.distributed.get_world_size())\n+            ]\n+\n+            torch.distributed.all_gather_object(saved_ops_names_all_ranks, objects[0])\n             name_to_node = get_name_to_node(joint_graph)\n-            saved_values = [name_to_node[n] for n in saved_values_names]\n+            saved_sizes: list[int] = []\n+            for saved_ops_names in saved_ops_names_all_ranks:\n+                saved_nodes = [name_to_node[op_name] for op_name in saved_ops_names]\n+                saved_size = 0\n+                for node in saved_nodes:\n+                    saved_size += _size_of(node)\n+                saved_sizes.append(saved_size)\n+\n+            saved_sizes_tensor = torch.tensor(\n+                saved_sizes, device=torch.cuda.current_device()",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2177758844",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156287,
        "pr_file": "torch/_functorch/partitioners.py",
        "discussion_id": "2177758844",
        "commented_code": "@@ -2515,11 +2515,38 @@ def has_same_nodes(joint_graph):\n     ):\n         with no_dispatch(), unset_fake_temporarily():\n             objects = [[x.name for x in saved_values]]\n-            # TODO: maybe use a different process group for this\n-            torch.distributed.broadcast_object_list(objects, src=0)\n-            saved_values_names = objects[0]\n+            saved_ops_names_all_ranks: list[list[str]] = [\n+                [] for _ in range(torch.distributed.get_world_size())\n+            ]\n+\n+            torch.distributed.all_gather_object(saved_ops_names_all_ranks, objects[0])\n             name_to_node = get_name_to_node(joint_graph)\n-            saved_values = [name_to_node[n] for n in saved_values_names]\n+            saved_sizes: list[int] = []\n+            for saved_ops_names in saved_ops_names_all_ranks:\n+                saved_nodes = [name_to_node[op_name] for op_name in saved_ops_names]\n+                saved_size = 0\n+                for node in saved_nodes:\n+                    saved_size += _size_of(node)\n+                saved_sizes.append(saved_size)\n+\n+            saved_sizes_tensor = torch.tensor(\n+                saved_sizes, device=torch.cuda.current_device()",
        "comment_created_at": "2025-07-01T14:28:29+00:00",
        "comment_author": "bdhirsh",
        "comment_body": "Instead of hardcoding the cuda device here, can we try to use the same device that our collectives are going to use?\r\n\r\nMaybe @wconstab knows of a cleaner way, but it looks like `all_gather_object` figures out what device to use with [this](https://github.com/pytorch/pytorch/blob/main/torch/distributed/distributed_c10d.py#L3153C5-L3153C52)\r\n```\r\ncurrent_device = _get_object_coll_device(_group_or_default_group())\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2147458264",
    "pr_number": 155497,
    "pr_file": "torch/nn/parallel/distributed.py",
    "created_at": "2025-06-15T06:41:37+00:00",
    "commented_code": "To use ``DistributedDataParallel`` on a host with N GPUs, you should spawn\n     up ``N`` processes, ensuring that each process exclusively works on a single\n     GPU from 0 to N-1. This can be done by either setting\n-    ``CUDA_VISIBLE_DEVICES`` for every process or by calling:\n+    ``CUDA_VISIBLE_DEVICES`` for every process or by calling the following API for GPUs,\n \n         >>> # xdoctest: +SKIP(\"undefined variables\")\n         >>> torch.cuda.set_device(i)\n \n+    and XPUs,\n+\n+        >>> # xdoctest: +SKIP(\"undefined variables\")\n+        >>> torch.xpu.set_device(i)\n+\n     where i is from 0 to N-1. In each process, you should refer the following\n     to construct this module:\n \n         >>> # xdoctest: +SKIP(\"undefined variables\")\n+        >>> if torch.cuda.is_available():\n+        >>>     vendor_backend = 'nccl'\n+        >>> elif torch.xpu.is_available():\n+        >>>     vendor_backend = 'xccl'\n+        >>>",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2147458264",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155497,
        "pr_file": "torch/nn/parallel/distributed.py",
        "discussion_id": "2147458264",
        "commented_code": "@@ -347,17 +347,27 @@ class DistributedDataParallel(Module, Joinable):\n     To use ``DistributedDataParallel`` on a host with N GPUs, you should spawn\n     up ``N`` processes, ensuring that each process exclusively works on a single\n     GPU from 0 to N-1. This can be done by either setting\n-    ``CUDA_VISIBLE_DEVICES`` for every process or by calling:\n+    ``CUDA_VISIBLE_DEVICES`` for every process or by calling the following API for GPUs,\n \n         >>> # xdoctest: +SKIP(\"undefined variables\")\n         >>> torch.cuda.set_device(i)\n \n+    and XPUs,\n+\n+        >>> # xdoctest: +SKIP(\"undefined variables\")\n+        >>> torch.xpu.set_device(i)\n+\n     where i is from 0 to N-1. In each process, you should refer the following\n     to construct this module:\n \n         >>> # xdoctest: +SKIP(\"undefined variables\")\n+        >>> if torch.cuda.is_available():\n+        >>>     vendor_backend = 'nccl'\n+        >>> elif torch.xpu.is_available():\n+        >>>     vendor_backend = 'xccl'\n+        >>>",
        "comment_created_at": "2025-06-15T06:41:37+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n        >>> if torch.accelerator.is_available():\r\n        >>>     device_type = torch.accelerator.current_accelerator().type\r\n        >>>     vendor_backend = torch.distributed.get_default_backend_for_device(device_type)\r\n        >>>\r\n```\r\nFollowing @wconstab 's comments, we could generalize this code a bit. \r\n@wconstab, may I know if this approach looks reasonable to you?",
        "pr_file_module": null
      }
    ]
  }
]