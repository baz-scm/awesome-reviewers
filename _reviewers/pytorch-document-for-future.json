[
  {
    "discussion_id": "2172807638",
    "pr_number": 157130,
    "pr_file": "torch/onnx/_internal/exporter/_torchlib/ops/nn.py",
    "created_at": "2025-06-27T20:11:29+00:00",
    "commented_code": "\"SDPA (MHA) requires q_num_heads = kv_num_heads\"\n            )\n\n        # NOTE: There was extended discussion on whether the num_heads attributes (q_num_heads/kv_num_heads)\n        # should be set as ONNX attributes or inferred from the tensor shape. In ONNX, num_heads is needed\n        # for 3D attention inputs (shape: [B, S, N*H]), but not for 4D ([B, N, S, H]), which is the only\n        # input accepted by this exporter. Thus, the attribute is not strictly necessary here, but adding it\n        # may ease future optimization or conversion to 3D formats (e.g., GQA ops)\n        # NOTE: num_heads attributes (q_num_heads/kv_num_heads) are not mandatory for 4D.",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2172807638",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157130,
        "pr_file": "torch/onnx/_internal/exporter/_torchlib/ops/nn.py",
        "discussion_id": "2172807638",
        "commented_code": "@@ -119,19 +119,19 @@ def aten_scaled_dot_product_attention_23(\n                 \"SDPA (MHA) requires q_num_heads = kv_num_heads\"\n             )\n \n-        # NOTE: There was extended discussion on whether the num_heads attributes (q_num_heads/kv_num_heads)\n-        # should be set as ONNX attributes or inferred from the tensor shape. In ONNX, num_heads is needed\n-        # for 3D attention inputs (shape: [B, S, N*H]), but not for 4D ([B, N, S, H]), which is the only\n-        # input accepted by this exporter. Thus, the attribute is not strictly necessary here, but adding it\n-        # may ease future optimization or conversion to 3D formats (e.g., GQA ops)\n+        # NOTE: num_heads attributes (q_num_heads/kv_num_heads) are not mandatory for 4D.",
        "comment_created_at": "2025-06-27T20:11:29+00:00",
        "comment_author": "gramalingam",
        "comment_body": "nit: may be clearer if we change \"are not mandatory for 4D\" to \"should NOT be specified for 4D (even though the ONNX op documentation does not make this clear)\"",
        "pr_file_module": null
      },
      {
        "comment_id": "2175067772",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157130,
        "pr_file": "torch/onnx/_internal/exporter/_torchlib/ops/nn.py",
        "discussion_id": "2172807638",
        "commented_code": "@@ -119,19 +119,19 @@ def aten_scaled_dot_product_attention_23(\n                 \"SDPA (MHA) requires q_num_heads = kv_num_heads\"\n             )\n \n-        # NOTE: There was extended discussion on whether the num_heads attributes (q_num_heads/kv_num_heads)\n-        # should be set as ONNX attributes or inferred from the tensor shape. In ONNX, num_heads is needed\n-        # for 3D attention inputs (shape: [B, S, N*H]), but not for 4D ([B, N, S, H]), which is the only\n-        # input accepted by this exporter. Thus, the attribute is not strictly necessary here, but adding it\n-        # may ease future optimization or conversion to 3D formats (e.g., GQA ops)\n+        # NOTE: num_heads attributes (q_num_heads/kv_num_heads) are not mandatory for 4D.",
        "comment_created_at": "2025-06-30T13:21:34+00:00",
        "comment_author": "xadupre",
        "comment_body": "changed, I removed the part in (...) in case the documentation of onnx is changed.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175940579",
    "pr_number": 156705,
    "pr_file": "torch/distributed/checkpoint/hf_storage.py",
    "created_at": "2025-06-30T21:01:09+00:00",
    "commented_code": "return super()._write_data(planner, file_queue)\n\n    def finish(self, metadata: Metadata, results: list[list[WriteResult]]) -> None:\n        if self._save_sharded:\n        if self.save_distributed and not self.consolidated_output_path:\n            logger.info(\"Not consolidating sharded checkpoint in finish step.\")",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2175940579",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156705,
        "pr_file": "torch/distributed/checkpoint/hf_storage.py",
        "discussion_id": "2175940579",
        "commented_code": "@@ -136,8 +154,16 @@ def write_data(\n         return super()._write_data(planner, file_queue)\n \n     def finish(self, metadata: Metadata, results: list[list[WriteResult]]) -> None:\n-        if self._save_sharded:\n+        if self.save_distributed and not self.consolidated_output_path:\n+            logger.info(\"Not consolidating sharded checkpoint in finish step.\")",
        "comment_created_at": "2025-06-30T21:01:09+00:00",
        "comment_author": "saumishr",
        "comment_body": "It would be worth adding a documentation here explaining different scenarios and why the metadata write is also being skipped. ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2153270155",
    "pr_number": 156207,
    "pr_file": "torch/distributed/checkpoint/state_dict_saver.py",
    "created_at": "2025-06-17T22:25:11+00:00",
    "commented_code": "planner=planner,\n        )\n\ndef default_stage(state_dict: STATE_DICT_TYPE, cpu_state_dict: STATE_DICT_TYPE) -> STATE_DICT_TYPE:\n    \"\"\"Default stage function. This function will stage the state_dict on to the\n    staging storage (defaults to CPU memory).\n\n    Args:\n        state_dict STATE_DICT_TYPE: The state_dict to stage.\n\n    Returns:\n       STATE_DICT_TYPE: staged state_dict\n    \"\"\"\n    staging_stream = torch.cuda.Stream()\n    with staging_stream:\n        staged_state_dict = _copy_state_dict(state_dict, cpu_state_dict, True, type_check=False)\n    staging_stream.synchronize()\n    return staged_state_dict\n\n@dataclass\nclass AsyncSaveResponse:",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2153270155",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156207,
        "pr_file": "torch/distributed/checkpoint/state_dict_saver.py",
        "discussion_id": "2153270155",
        "commented_code": "@@ -181,6 +189,36 @@ def save(\n             planner=planner,\n         )\n \n+def default_stage(state_dict: STATE_DICT_TYPE, cpu_state_dict: STATE_DICT_TYPE) -> STATE_DICT_TYPE:\n+    \"\"\"Default stage function. This function will stage the state_dict on to the\n+    staging storage (defaults to CPU memory).\n+\n+    Args:\n+        state_dict STATE_DICT_TYPE: The state_dict to stage.\n+\n+    Returns:\n+       STATE_DICT_TYPE: staged state_dict\n+    \"\"\"\n+    staging_stream = torch.cuda.Stream()\n+    with staging_stream:\n+        staged_state_dict = _copy_state_dict(state_dict, cpu_state_dict, True, type_check=False)\n+    staging_stream.synchronize()\n+    return staged_state_dict\n+\n+@dataclass\n+class AsyncSaveResponse:",
        "comment_created_at": "2025-06-17T22:25:11+00:00",
        "comment_author": "fegin",
        "comment_body": "Need a good docstring since this is a public dataclass.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2094381930",
    "pr_number": 153198,
    "pr_file": "torch/_refs/__init__.py",
    "created_at": "2025-05-18T05:51:33+00:00",
    "commented_code": "return permuted_result.reshape(target_shape)\n\n\ndef _reshape_view_helper(a: TensorLikeType, *shape, allow_copy: bool) -> TensorLikeType:\n    from torch._dynamo.exc import UserError, UserErrorType\n# this function is python match of computeStride_impl in TensorUtils.cpp\ndef _compute_stride(old_shape, old_stride, new_shape):\n    from torch.fx.experimental.symbolic_shapes import (\n        guard_or_false,\n        guard_or_true,\n        GuardOnDataDependentSymNode,\n        sym_eq,\n    )\n\n    if len(old_shape) == 0:\n        return [1] * len(new_shape)\n\n    numel = reduce(operator.mul, old_shape, 1)\n    zero_numel = guard_or_false(numel == 0)\n    if zero_numel and guard_or_false(sym_eq(old_shape, new_shape)):\n        return old_stride\n\n    new_stride = [0] * len(new_shape)\n\n    if zero_numel:\n        for view_d in range(len(new_shape) - 1, -1, -1):\n            if view_d == len(new_shape) - 1:\n                new_stride[view_d] = 1\n            else:\n                new_stride[view_d] = (\n                    max(new_shape[view_d + 1], 1) * new_stride[view_d + 1]\n                )\n        return new_stride\n\n    view_d = len(new_shape) - 1\n    chunk_base_stride = old_stride[-1]\n    tensor_numel = 1\n    view_numel = 1\n\n    for tensor_d in range(len(old_shape) - 1, -1, -1):\n        tensor_numel *= old_shape[tensor_d]\n\n        if tensor_d == 0 or (\n            guard_or_true(old_shape[tensor_d - 1] != 1)\n            and guard_or_true(\n                old_stride[tensor_d - 1] != tensor_numel * chunk_base_stride\n            )\n        ):\n            while view_d >= 0 and (\n                guard_or_true(view_numel < tensor_numel)\n                or guard_or_false(new_shape[view_d] == 1)\n            ):\n                new_stride[view_d] = view_numel * chunk_base_stride\n                view_numel *= new_shape[view_d]\n                view_d -= 1\n\n            if guard_or_true(view_numel != tensor_numel):\n                return None\n\n            if tensor_d > 0:\n                chunk_base_stride = old_stride[tensor_d - 1]\n                tensor_numel = 1\n                view_numel = 1\n    if view_d != -1:\n        return None\n    return new_stride\n\n\n# if a is contiguous, we can always reshape with as_strided and contiguous_strides.\n# if a is not contiguous and _compute_stride succeed we also use as_strided without clone.",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2094381930",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153198,
        "pr_file": "torch/_refs/__init__.py",
        "discussion_id": "2094381930",
        "commented_code": "@@ -3729,62 +3729,111 @@ def repeat(a: Tensor, *repeat_shape) -> Tensor:\n     return permuted_result.reshape(target_shape)\n \n \n-def _reshape_view_helper(a: TensorLikeType, *shape, allow_copy: bool) -> TensorLikeType:\n-    from torch._dynamo.exc import UserError, UserErrorType\n+# this function is python match of computeStride_impl in TensorUtils.cpp\n+def _compute_stride(old_shape, old_stride, new_shape):\n     from torch.fx.experimental.symbolic_shapes import (\n         guard_or_false,\n         guard_or_true,\n-        GuardOnDataDependentSymNode,\n+        sym_eq,\n     )\n \n+    if len(old_shape) == 0:\n+        return [1] * len(new_shape)\n+\n+    numel = reduce(operator.mul, old_shape, 1)\n+    zero_numel = guard_or_false(numel == 0)\n+    if zero_numel and guard_or_false(sym_eq(old_shape, new_shape)):\n+        return old_stride\n+\n+    new_stride = [0] * len(new_shape)\n+\n+    if zero_numel:\n+        for view_d in range(len(new_shape) - 1, -1, -1):\n+            if view_d == len(new_shape) - 1:\n+                new_stride[view_d] = 1\n+            else:\n+                new_stride[view_d] = (\n+                    max(new_shape[view_d + 1], 1) * new_stride[view_d + 1]\n+                )\n+        return new_stride\n+\n+    view_d = len(new_shape) - 1\n+    chunk_base_stride = old_stride[-1]\n+    tensor_numel = 1\n+    view_numel = 1\n+\n+    for tensor_d in range(len(old_shape) - 1, -1, -1):\n+        tensor_numel *= old_shape[tensor_d]\n+\n+        if tensor_d == 0 or (\n+            guard_or_true(old_shape[tensor_d - 1] != 1)\n+            and guard_or_true(\n+                old_stride[tensor_d - 1] != tensor_numel * chunk_base_stride\n+            )\n+        ):\n+            while view_d >= 0 and (\n+                guard_or_true(view_numel < tensor_numel)\n+                or guard_or_false(new_shape[view_d] == 1)\n+            ):\n+                new_stride[view_d] = view_numel * chunk_base_stride\n+                view_numel *= new_shape[view_d]\n+                view_d -= 1\n+\n+            if guard_or_true(view_numel != tensor_numel):\n+                return None\n+\n+            if tensor_d > 0:\n+                chunk_base_stride = old_stride[tensor_d - 1]\n+                tensor_numel = 1\n+                view_numel = 1\n+    if view_d != -1:\n+        return None\n+    return new_stride\n+\n+\n+# if a is contiguous, we can always reshape with as_strided and contiguous_strides.\n+# if a is not contiguous and _compute_stride succeed we also use as_strided without clone.",
        "comment_created_at": "2025-05-18T05:51:33+00:00",
        "comment_author": "bobrenjc93",
        "comment_body": "Can you please elaborate on these cases? For these correctness comments, you don't need to be frugal with your words. Most of the comments sound like notes for yourself, but for these comments you should really imagine what another core pytorch dev reading this comment 6 months from now will think.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2098995525",
    "pr_number": 153991,
    "pr_file": "torch/vulkan/__init__.py",
    "created_at": "2025-05-20T22:58:06+00:00",
    "commented_code": "# mypy: allow-untyped-defs\nr\"\"\"\nThis package enables an interface for accessing the Vulkan backend in Python.\n\"\"\"\nfrom typing import Union\n\nimport torch\nfrom torch import Tensor\n\ndef device_count() -> int:\n    r\"\"\"Returns the number of available Vulkan devices.\"\"\"\n    # TODO: actually get the number!\n    return int(torch.is_vulkan_available())\n\n\ndef compile_shader(name: str, source: str):\n    r\"\"\"Compiles compute shader from source and allows one to invoke kernels\n    defined there from the comfort of Python runtime\n    Example::\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n        >>> lib = torch.mps.compile_shader(\n        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n        ...  )\n        >>> x = torch.zeros(16, device=\"mps\")\n        >>> lib.full(x, 3.14)\n    \"\"\"\n    from pathlib import Path\n\n    from torch.utils._cpp_embed_headers import _embed_headers\n\n    if not hasattr(torch._C, \"_vulkan_compileShader\"):\n        raise RuntimeError(\"Vulkan is not available\")\n    source = _embed_headers(\n        [l + \"\\n\" for l in source.split(\"\\n\")],\n        [Path(__file__).parent.parent / \"include\"],\n        set(),\n    )\n    return torch._C._vulkan_compileShader(name, source)\n\n\ndef is_available() -> bool:\n    return device_count() > 0\n\n\n__all__ = [",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2098995525",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153991,
        "pr_file": "torch/vulkan/__init__.py",
        "discussion_id": "2098995525",
        "commented_code": "@@ -0,0 +1,51 @@\n+# mypy: allow-untyped-defs\n+r\"\"\"\n+This package enables an interface for accessing the Vulkan backend in Python.\n+\"\"\"\n+from typing import Union\n+\n+import torch\n+from torch import Tensor\n+\n+def device_count() -> int:\n+    r\"\"\"Returns the number of available Vulkan devices.\"\"\"\n+    # TODO: actually get the number!\n+    return int(torch.is_vulkan_available())\n+\n+\n+def compile_shader(name: str, source: str):\n+    r\"\"\"Compiles compute shader from source and allows one to invoke kernels\n+    defined there from the comfort of Python runtime\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n+        >>> lib = torch.mps.compile_shader(\n+        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n+        ...  )\n+        >>> x = torch.zeros(16, device=\"mps\")\n+        >>> lib.full(x, 3.14)\n+    \"\"\"\n+    from pathlib import Path\n+\n+    from torch.utils._cpp_embed_headers import _embed_headers\n+\n+    if not hasattr(torch._C, \"_vulkan_compileShader\"):\n+        raise RuntimeError(\"Vulkan is not available\")\n+    source = _embed_headers(\n+        [l + \"\\n\" for l in source.split(\"\\n\")],\n+        [Path(__file__).parent.parent / \"include\"],\n+        set(),\n+    )\n+    return torch._C._vulkan_compileShader(name, source)\n+\n+\n+def is_available() -> bool:\n+    return device_count() > 0\n+\n+\n+__all__ = [",
        "comment_created_at": "2025-05-20T22:58:06+00:00",
        "comment_author": "albanD",
        "comment_body": "We're really making these public APIs that we will maintain for the foreseeable future?\r\nIf so they need doc and tests!",
        "pr_file_module": null
      },
      {
        "comment_id": "2099003823",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153991,
        "pr_file": "torch/vulkan/__init__.py",
        "discussion_id": "2098995525",
        "commented_code": "@@ -0,0 +1,51 @@\n+# mypy: allow-untyped-defs\n+r\"\"\"\n+This package enables an interface for accessing the Vulkan backend in Python.\n+\"\"\"\n+from typing import Union\n+\n+import torch\n+from torch import Tensor\n+\n+def device_count() -> int:\n+    r\"\"\"Returns the number of available Vulkan devices.\"\"\"\n+    # TODO: actually get the number!\n+    return int(torch.is_vulkan_available())\n+\n+\n+def compile_shader(name: str, source: str):\n+    r\"\"\"Compiles compute shader from source and allows one to invoke kernels\n+    defined there from the comfort of Python runtime\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n+        >>> lib = torch.mps.compile_shader(\n+        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n+        ...  )\n+        >>> x = torch.zeros(16, device=\"mps\")\n+        >>> lib.full(x, 3.14)\n+    \"\"\"\n+    from pathlib import Path\n+\n+    from torch.utils._cpp_embed_headers import _embed_headers\n+\n+    if not hasattr(torch._C, \"_vulkan_compileShader\"):\n+        raise RuntimeError(\"Vulkan is not available\")\n+    source = _embed_headers(\n+        [l + \"\\n\" for l in source.split(\"\\n\")],\n+        [Path(__file__).parent.parent / \"include\"],\n+        set(),\n+    )\n+    return torch._C._vulkan_compileShader(name, source)\n+\n+\n+def is_available() -> bool:\n+    return device_count() > 0\n+\n+\n+__all__ = [",
        "comment_created_at": "2025-05-20T23:07:58+00:00",
        "comment_author": "albanD",
        "comment_body": "Actually is_available, device_count and synchronize should be ok if they do have a semantic that match other backends.\r\nThe shader compiler is most likely more private.",
        "pr_file_module": null
      },
      {
        "comment_id": "2099011455",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153991,
        "pr_file": "torch/vulkan/__init__.py",
        "discussion_id": "2098995525",
        "commented_code": "@@ -0,0 +1,51 @@\n+# mypy: allow-untyped-defs\n+r\"\"\"\n+This package enables an interface for accessing the Vulkan backend in Python.\n+\"\"\"\n+from typing import Union\n+\n+import torch\n+from torch import Tensor\n+\n+def device_count() -> int:\n+    r\"\"\"Returns the number of available Vulkan devices.\"\"\"\n+    # TODO: actually get the number!\n+    return int(torch.is_vulkan_available())\n+\n+\n+def compile_shader(name: str, source: str):\n+    r\"\"\"Compiles compute shader from source and allows one to invoke kernels\n+    defined there from the comfort of Python runtime\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n+        >>> lib = torch.mps.compile_shader(\n+        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n+        ...  )\n+        >>> x = torch.zeros(16, device=\"mps\")\n+        >>> lib.full(x, 3.14)\n+    \"\"\"\n+    from pathlib import Path\n+\n+    from torch.utils._cpp_embed_headers import _embed_headers\n+\n+    if not hasattr(torch._C, \"_vulkan_compileShader\"):\n+        raise RuntimeError(\"Vulkan is not available\")\n+    source = _embed_headers(\n+        [l + \"\\n\" for l in source.split(\"\\n\")],\n+        [Path(__file__).parent.parent / \"include\"],\n+        set(),\n+    )\n+    return torch._C._vulkan_compileShader(name, source)\n+\n+\n+def is_available() -> bool:\n+    return device_count() > 0\n+\n+\n+__all__ = [",
        "comment_created_at": "2025-05-20T23:14:54+00:00",
        "comment_author": "swolchok",
        "comment_body": "> The shader compiler is most likely more private.\r\n\r\nThis is way smaller than `torch/mps/__init__.py`! torch.mps.compile_shader is public https://github.com/pytorch/pytorch/blob/main/torch/mps/__init__.py#L143\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2099016867",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153991,
        "pr_file": "torch/vulkan/__init__.py",
        "discussion_id": "2098995525",
        "commented_code": "@@ -0,0 +1,51 @@\n+# mypy: allow-untyped-defs\n+r\"\"\"\n+This package enables an interface for accessing the Vulkan backend in Python.\n+\"\"\"\n+from typing import Union\n+\n+import torch\n+from torch import Tensor\n+\n+def device_count() -> int:\n+    r\"\"\"Returns the number of available Vulkan devices.\"\"\"\n+    # TODO: actually get the number!\n+    return int(torch.is_vulkan_available())\n+\n+\n+def compile_shader(name: str, source: str):\n+    r\"\"\"Compiles compute shader from source and allows one to invoke kernels\n+    defined there from the comfort of Python runtime\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n+        >>> lib = torch.mps.compile_shader(\n+        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n+        ...  )\n+        >>> x = torch.zeros(16, device=\"mps\")\n+        >>> lib.full(x, 3.14)\n+    \"\"\"\n+    from pathlib import Path\n+\n+    from torch.utils._cpp_embed_headers import _embed_headers\n+\n+    if not hasattr(torch._C, \"_vulkan_compileShader\"):\n+        raise RuntimeError(\"Vulkan is not available\")\n+    source = _embed_headers(\n+        [l + \"\\n\" for l in source.split(\"\\n\")],\n+        [Path(__file__).parent.parent / \"include\"],\n+        set(),\n+    )\n+    return torch._C._vulkan_compileShader(name, source)\n+\n+\n+def is_available() -> bool:\n+    return device_count() > 0\n+\n+\n+__all__ = [",
        "comment_created_at": "2025-05-20T23:18:23+00:00",
        "comment_author": "swolchok",
        "comment_body": "btw, @albanD, do we have a process or mechanism that would, if we decided this was even promising enough to commit, allow committing this stuff *without* promising to maintain or never change it? Is the \"prototype\" status from https://pytorch.org/blog/pytorch-feature-classification-changes/ still available?",
        "pr_file_module": null
      },
      {
        "comment_id": "2099068635",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153991,
        "pr_file": "torch/vulkan/__init__.py",
        "discussion_id": "2098995525",
        "commented_code": "@@ -0,0 +1,51 @@\n+# mypy: allow-untyped-defs\n+r\"\"\"\n+This package enables an interface for accessing the Vulkan backend in Python.\n+\"\"\"\n+from typing import Union\n+\n+import torch\n+from torch import Tensor\n+\n+def device_count() -> int:\n+    r\"\"\"Returns the number of available Vulkan devices.\"\"\"\n+    # TODO: actually get the number!\n+    return int(torch.is_vulkan_available())\n+\n+\n+def compile_shader(name: str, source: str):\n+    r\"\"\"Compiles compute shader from source and allows one to invoke kernels\n+    defined there from the comfort of Python runtime\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n+        >>> lib = torch.mps.compile_shader(\n+        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n+        ...  )\n+        >>> x = torch.zeros(16, device=\"mps\")\n+        >>> lib.full(x, 3.14)\n+    \"\"\"\n+    from pathlib import Path\n+\n+    from torch.utils._cpp_embed_headers import _embed_headers\n+\n+    if not hasattr(torch._C, \"_vulkan_compileShader\"):\n+        raise RuntimeError(\"Vulkan is not available\")\n+    source = _embed_headers(\n+        [l + \"\\n\" for l in source.split(\"\\n\")],\n+        [Path(__file__).parent.parent / \"include\"],\n+        set(),\n+    )\n+    return torch._C._vulkan_compileShader(name, source)\n+\n+\n+def is_available() -> bool:\n+    return device_count() > 0\n+\n+\n+__all__ = [",
        "comment_created_at": "2025-05-21T00:16:16+00:00",
        "comment_author": "albanD",
        "comment_body": "You should make it a private API (via prepending _*) to mark it as \"should not be used\".",
        "pr_file_module": null
      },
      {
        "comment_id": "2099069584",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153991,
        "pr_file": "torch/vulkan/__init__.py",
        "discussion_id": "2098995525",
        "commented_code": "@@ -0,0 +1,51 @@\n+# mypy: allow-untyped-defs\n+r\"\"\"\n+This package enables an interface for accessing the Vulkan backend in Python.\n+\"\"\"\n+from typing import Union\n+\n+import torch\n+from torch import Tensor\n+\n+def device_count() -> int:\n+    r\"\"\"Returns the number of available Vulkan devices.\"\"\"\n+    # TODO: actually get the number!\n+    return int(torch.is_vulkan_available())\n+\n+\n+def compile_shader(name: str, source: str):\n+    r\"\"\"Compiles compute shader from source and allows one to invoke kernels\n+    defined there from the comfort of Python runtime\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n+        >>> lib = torch.mps.compile_shader(\n+        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n+        ...  )\n+        >>> x = torch.zeros(16, device=\"mps\")\n+        >>> lib.full(x, 3.14)\n+    \"\"\"\n+    from pathlib import Path\n+\n+    from torch.utils._cpp_embed_headers import _embed_headers\n+\n+    if not hasattr(torch._C, \"_vulkan_compileShader\"):\n+        raise RuntimeError(\"Vulkan is not available\")\n+    source = _embed_headers(\n+        [l + \"\\n\" for l in source.split(\"\\n\")],\n+        [Path(__file__).parent.parent / \"include\"],\n+        set(),\n+    )\n+    return torch._C._vulkan_compileShader(name, source)\n+\n+\n+def is_available() -> bool:\n+    return device_count() > 0\n+\n+\n+__all__ = [",
        "comment_created_at": "2025-05-21T00:17:53+00:00",
        "comment_author": "albanD",
        "comment_body": "> This is way smaller than torch/mps/__init__.py! torch.mps.compile_shader is public\r\n\r\nI'm not saying I don't think we should make it public. I'm asking if we're confident that this API is stable and we will maintain it for the foreseeable future? If the answer is yes, let's document all of this and go for it !\r\nIf not, let's just make it private for now and see later once it's more mature.",
        "pr_file_module": null
      },
      {
        "comment_id": "2099088683",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153991,
        "pr_file": "torch/vulkan/__init__.py",
        "discussion_id": "2098995525",
        "commented_code": "@@ -0,0 +1,51 @@\n+# mypy: allow-untyped-defs\n+r\"\"\"\n+This package enables an interface for accessing the Vulkan backend in Python.\n+\"\"\"\n+from typing import Union\n+\n+import torch\n+from torch import Tensor\n+\n+def device_count() -> int:\n+    r\"\"\"Returns the number of available Vulkan devices.\"\"\"\n+    # TODO: actually get the number!\n+    return int(torch.is_vulkan_available())\n+\n+\n+def compile_shader(name: str, source: str):\n+    r\"\"\"Compiles compute shader from source and allows one to invoke kernels\n+    defined there from the comfort of Python runtime\n+    Example::\n+\n+        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_MPS)\n+        >>> lib = torch.mps.compile_shader(\n+        ... \"kernel void full(device float* out, constant float& val, uint idx [[thread_position_in_grid]]) { out[idx] = val; }\"\n+        ...  )\n+        >>> x = torch.zeros(16, device=\"mps\")\n+        >>> lib.full(x, 3.14)\n+    \"\"\"\n+    from pathlib import Path\n+\n+    from torch.utils._cpp_embed_headers import _embed_headers\n+\n+    if not hasattr(torch._C, \"_vulkan_compileShader\"):\n+        raise RuntimeError(\"Vulkan is not available\")\n+    source = _embed_headers(\n+        [l + \"\\n\" for l in source.split(\"\\n\")],\n+        [Path(__file__).parent.parent / \"include\"],\n+        set(),\n+    )\n+    return torch._C._vulkan_compileShader(name, source)\n+\n+\n+def is_available() -> bool:\n+    return device_count() > 0\n+\n+\n+__all__ = [",
        "comment_created_at": "2025-05-21T00:43:15+00:00",
        "comment_author": "swolchok",
        "comment_body": "> we're confident that this API is stable and we will maintain it for the foreseeable future\r\n\r\ndefinitely not. this is currently experimental status, not clearly motivated, timeboxed (and I extended the timebox because I was close to getting this out). Hence the \"experimental\" label and draft PR.\r\n\r\nI'll go ahead and underscore-prefix everything. ",
        "pr_file_module": null
      }
    ]
  }
]