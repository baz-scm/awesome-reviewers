[
  {
    "discussion_id": "2148213674",
    "pr_number": 156017,
    "pr_file": "tools/linter/adapters/pyproject_linter.py",
    "created_at": "2025-06-15T14:56:38+00:00",
    "commented_code": "+from __future__ import annotations\n+\n+import argparse\n+import concurrent.futures\n+import json\n+import logging\n+import operator\n+import os\n+import re\n+import sys\n+from enum import Enum\n+from pathlib import Path\n+from typing import NamedTuple\n+\n+from packaging.version import Version\n+\n+\n+if sys.version_info >= (3, 11):\n+    import tomllib\n+else:\n+    import tomli as tomllib  # type: ignore[import-not-found]\n+\n+\n+REQUIRES_PYTHON_PATTERN = re.compile(\n+    r\"\"\"\n+    ^\\s*\n+    (?P<min_ver_op>>=)\n+    \\s*\n+    (?P<min_ver>\\d+\\.\\d+(?:\\.\\d+)?)\n+    \\s*,\\s*\n+    (?P<max_ver_op><(=?))\n+    \\s*\n+    (?P<max_ver>\\d+\\.\\d+(?:\\.\\d+)?)\n+    \\s*$\n+    \"\"\",\n+    flags=re.VERBOSE,\n+)\n+COMPARE_OPS = {\n+    \"<\": operator.lt,\n+    \"<=\": operator.le,\n+    \">\": operator.gt,\n+    \">=\": operator.ge,\n+    \"==\": operator.eq,\n+    \"!=\": operator.ne,\n+}\n+\n+\n+class LintSeverity(str, Enum):\n+    ERROR = \"error\"\n+    WARNING = \"warning\"\n+    ADVICE = \"advice\"\n+    DISABLED = \"disabled\"\n+\n+\n+class LintMessage(NamedTuple):\n+    path: str | None\n+    line: int | None\n+    char: int | None\n+    code: str\n+    severity: LintSeverity\n+    name: str\n+    original: str | None\n+    replacement: str | None\n+    description: str | None\n+\n+\n+def format_error_message(\n+    filename: str,\n+    error: Exception | None = None,\n+    *,\n+    message: str | None = None,\n+) -> LintMessage:\n+    if message is None and error is not None:\n+        message = f\"Failed due to {error.__class__.__name__}:\n{error}\"\n+    return LintMessage(\n+        path=filename,\n+        line=None,\n+        char=None,\n+        code=\"PYPROJECT\",\n+        severity=LintSeverity.ERROR,\n+        name=\"pyproject.toml consistency\",\n+        original=None,\n+        replacement=None,\n+        description=message,\n+    )\n+\n+\n+def check_file(filename: str) -> list[LintMessage]:\n+    path = Path(filename).absolute()\n+    content = path.read_text(encoding=\"utf-8\")\n+    try:\n+        pyproject = tomllib.loads(content)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2148213674",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156017,
        "pr_file": "tools/linter/adapters/pyproject_linter.py",
        "discussion_id": "2148213674",
        "commented_code": "@@ -0,0 +1,277 @@\n+from __future__ import annotations\n+\n+import argparse\n+import concurrent.futures\n+import json\n+import logging\n+import operator\n+import os\n+import re\n+import sys\n+from enum import Enum\n+from pathlib import Path\n+from typing import NamedTuple\n+\n+from packaging.version import Version\n+\n+\n+if sys.version_info >= (3, 11):\n+    import tomllib\n+else:\n+    import tomli as tomllib  # type: ignore[import-not-found]\n+\n+\n+REQUIRES_PYTHON_PATTERN = re.compile(\n+    r\"\"\"\n+    ^\\s*\n+    (?P<min_ver_op>>=)\n+    \\s*\n+    (?P<min_ver>\\d+\\.\\d+(?:\\.\\d+)?)\n+    \\s*,\\s*\n+    (?P<max_ver_op><(=?))\n+    \\s*\n+    (?P<max_ver>\\d+\\.\\d+(?:\\.\\d+)?)\n+    \\s*$\n+    \"\"\",\n+    flags=re.VERBOSE,\n+)\n+COMPARE_OPS = {\n+    \"<\": operator.lt,\n+    \"<=\": operator.le,\n+    \">\": operator.gt,\n+    \">=\": operator.ge,\n+    \"==\": operator.eq,\n+    \"!=\": operator.ne,\n+}\n+\n+\n+class LintSeverity(str, Enum):\n+    ERROR = \"error\"\n+    WARNING = \"warning\"\n+    ADVICE = \"advice\"\n+    DISABLED = \"disabled\"\n+\n+\n+class LintMessage(NamedTuple):\n+    path: str | None\n+    line: int | None\n+    char: int | None\n+    code: str\n+    severity: LintSeverity\n+    name: str\n+    original: str | None\n+    replacement: str | None\n+    description: str | None\n+\n+\n+def format_error_message(\n+    filename: str,\n+    error: Exception | None = None,\n+    *,\n+    message: str | None = None,\n+) -> LintMessage:\n+    if message is None and error is not None:\n+        message = f\"Failed due to {error.__class__.__name__}:\\n{error}\"\n+    return LintMessage(\n+        path=filename,\n+        line=None,\n+        char=None,\n+        code=\"PYPROJECT\",\n+        severity=LintSeverity.ERROR,\n+        name=\"pyproject.toml consistency\",\n+        original=None,\n+        replacement=None,\n+        description=message,\n+    )\n+\n+\n+def check_file(filename: str) -> list[LintMessage]:\n+    path = Path(filename).absolute()\n+    content = path.read_text(encoding=\"utf-8\")\n+    try:\n+        pyproject = tomllib.loads(content)",
        "comment_created_at": "2025-06-15T14:56:38+00:00",
        "comment_author": "Skylion007",
        "comment_body": "Just load it from the file descriptor / path lib file directly. No need for temporary string.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2115901124",
    "pr_number": 154695,
    "pr_file": "torch/_inductor/select_algorithm.py",
    "created_at": "2025-05-30T13:09:33+00:00",
    "commented_code": ")\n \n         timings = do_autotuning(choices, precompile_fn)\n-        if timings == {} or choices[0] not in timings:\n-            return choices[0].output_node()\n \n-        selected_key = builtins.min(timings, key=timings.__getitem__)\n-        selected_choice = selected_key.output_node()\n-        log.debug(\"selected choice: %s\", str(selected_choice))\n-        return selected_choice\n+        # if timings is empty, we really have no choice but to return a semi-random\n+        # choice. returning the first `ExternKernelCaller` is probably the safest bet\n+        # in this case, since it will generally be the ATen kernel. if there are no\n+        # `ExternKernelCaller`s to return, then returning the 0th kernel is our next\n+        # best option (ideally we'd fail whenever there is no ATen kernel to fallback\n+        # to, but that's not trivial to figure out)\n+        if timings == {}:\n+            for choice in choices:\n+                if isinstance(choice, ExternKernelCaller):\n+                    node = choice.output_node()\n+                    log.debug(\n+                        \"Autotuning returned empty timings, falling back to first `ExternKernelCaller`: %s\",\n+                        str(node),\n+                    )\n+                    return node\n+            node = choices[0].output_node()\n+            log.debug(\n+                \"Autotuning returned empty timings, falling back to first choice: %s\",\n+                str(node),",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2115901124",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154695,
        "pr_file": "torch/_inductor/select_algorithm.py",
        "discussion_id": "2115901124",
        "commented_code": "@@ -2324,13 +2325,34 @@ def get_timings():\n             )\n \n         timings = do_autotuning(choices, precompile_fn)\n-        if timings == {} or choices[0] not in timings:\n-            return choices[0].output_node()\n \n-        selected_key = builtins.min(timings, key=timings.__getitem__)\n-        selected_choice = selected_key.output_node()\n-        log.debug(\"selected choice: %s\", str(selected_choice))\n-        return selected_choice\n+        # if timings is empty, we really have no choice but to return a semi-random\n+        # choice. returning the first `ExternKernelCaller` is probably the safest bet\n+        # in this case, since it will generally be the ATen kernel. if there are no\n+        # `ExternKernelCaller`s to return, then returning the 0th kernel is our next\n+        # best option (ideally we'd fail whenever there is no ATen kernel to fallback\n+        # to, but that's not trivial to figure out)\n+        if timings == {}:\n+            for choice in choices:\n+                if isinstance(choice, ExternKernelCaller):\n+                    node = choice.output_node()\n+                    log.debug(\n+                        \"Autotuning returned empty timings, falling back to first `ExternKernelCaller`: %s\",\n+                        str(node),\n+                    )\n+                    return node\n+            node = choices[0].output_node()\n+            log.debug(\n+                \"Autotuning returned empty timings, falling back to first choice: %s\",\n+                str(node),",
        "comment_created_at": "2025-05-30T13:09:33+00:00",
        "comment_author": "Skylion007",
        "comment_body": "Calling str on node isn't unnecessary \"%s\" implicitly casts it to str anyway. It's also less efficient as node will be converted to str even in debug mode!",
        "pr_file_module": null
      },
      {
        "comment_id": "2122306980",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154695,
        "pr_file": "torch/_inductor/select_algorithm.py",
        "discussion_id": "2115901124",
        "commented_code": "@@ -2324,13 +2325,34 @@ def get_timings():\n             )\n \n         timings = do_autotuning(choices, precompile_fn)\n-        if timings == {} or choices[0] not in timings:\n-            return choices[0].output_node()\n \n-        selected_key = builtins.min(timings, key=timings.__getitem__)\n-        selected_choice = selected_key.output_node()\n-        log.debug(\"selected choice: %s\", str(selected_choice))\n-        return selected_choice\n+        # if timings is empty, we really have no choice but to return a semi-random\n+        # choice. returning the first `ExternKernelCaller` is probably the safest bet\n+        # in this case, since it will generally be the ATen kernel. if there are no\n+        # `ExternKernelCaller`s to return, then returning the 0th kernel is our next\n+        # best option (ideally we'd fail whenever there is no ATen kernel to fallback\n+        # to, but that's not trivial to figure out)\n+        if timings == {}:\n+            for choice in choices:\n+                if isinstance(choice, ExternKernelCaller):\n+                    node = choice.output_node()\n+                    log.debug(\n+                        \"Autotuning returned empty timings, falling back to first `ExternKernelCaller`: %s\",\n+                        str(node),\n+                    )\n+                    return node\n+            node = choices[0].output_node()\n+            log.debug(\n+                \"Autotuning returned empty timings, falling back to first choice: %s\",\n+                str(node),",
        "comment_created_at": "2025-06-02T23:04:14+00:00",
        "comment_author": "nmacchioni",
        "comment_body": "good catch",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178725079",
    "pr_number": 153343,
    "pr_file": "torch/_higher_order_ops/map.py",
    "created_at": "2025-07-01T23:36:52+00:00",
    "commented_code": "class MapAutogradOp(torch.autograd.Function):\n     @staticmethod\n-    def forward(ctx, fw_graph, joint_graph, num_mapped_args, *flat_args):\n-        save_tensors_and_symints_for_backward(ctx, flat_args)\n-        ctx._joint_graph = joint_graph\n+    def forward(ctx, f, num_mapped_args, *flat_args):\n+        ctx._f = f\n         ctx._num_mapped_args = num_mapped_args\n+        ctx._num_pos_args = len(flat_args) - num_mapped_args\n+\n+        # We snapshot the dispatch keys in forward for materializing the\n+        # the bw_graph in backward.\n+        ctx._fw_include_key_set = torch._C._dispatch_tls_local_include_set()\n+        ctx._fw_exclude_key_set = torch._C._dispatch_tls_local_exclude_set()\n+        save_tensors_and_symints_for_backward(ctx, flat_args)\n         with torch._C._AutoDispatchBelowAutograd():\n             return (\n-                *map_impl(\n-                    fw_graph, flat_args[:num_mapped_args], flat_args[num_mapped_args:]\n-                ),\n+                *map_impl(f, flat_args[:num_mapped_args], flat_args[num_mapped_args:]),\n             )\n \n     @staticmethod\n     def backward(ctx, *flat_grads):\n         fw_args = saved_tensors_and_symints(ctx)\n-        fw_mapped_args = fw_args[: ctx._num_mapped_args]\n-        pos_args = fw_args[ctx._num_mapped_args :]\n+        num_mapped_args = ctx._num_mapped_args\n+        num_pos_args = ctx._num_pos_args\n+        num_grads = len(flat_grads)\n \n-        grads = map_impl(\n-            ctx._joint_graph,\n-            fw_mapped_args + flat_grads,\n-            pos_args,\n+        with disable_functional_mode():\n+            fw_mapped_args, pos_args = split_into_chunks(\n+                fw_args, [num_mapped_args, num_pos_args]\n+            )\n+\n+        # Prepare the backward graph:\n+        # The fw_mapped_args need to be sliced along the first dimension\n+        fw_args_prepared = (*[first_slice_copy(x) for x in fw_mapped_args], *pos_args)\n+        bw_f = create_bw_fn(ctx._f, fw_args_prepared)\n+\n+        # Create a wrapper around thefor the bw_f\n+        def bw_f_wrapper(*args):\n+            # Dissect args and re-order them for the ``ctx._bw_f``\n+            # args provided to the wrapper are composed of [*fw_mapped_args, *flat_grads, *pos_args]\n+            # The content of ``bw_f_tangents`` are the upstream gradients, i.e. flat_grads\n+            # The content of ``bw_f_primals`` are the fw_args, i.e., [*fw_mapped_args, *pos_args]\n+            # The bw_f requires *bw_f_primals, *bw_f_tangents\n+            fw_m_args, bw_f_tangents, pos_args = split_into_chunks(\n+                args, [num_mapped_args, num_grads, num_pos_args]\n+            )\n+            bw_f_primals = *fw_m_args, *pos_args\n+            return bw_f(*bw_f_primals, *bw_f_tangents)\n+\n+        def construct_args_single_step_bw():\n+            example_xs = first_slice_copy_with_grad(fw_mapped_args)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2178725079",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153343,
        "pr_file": "torch/_higher_order_ops/map.py",
        "discussion_id": "2178725079",
        "commented_code": "@@ -193,33 +118,75 @@ def wrapped_fn(*flat_args, f, xs_tree_spec, args_tree_spec, num_xs):\n \n class MapAutogradOp(torch.autograd.Function):\n     @staticmethod\n-    def forward(ctx, fw_graph, joint_graph, num_mapped_args, *flat_args):\n-        save_tensors_and_symints_for_backward(ctx, flat_args)\n-        ctx._joint_graph = joint_graph\n+    def forward(ctx, f, num_mapped_args, *flat_args):\n+        ctx._f = f\n         ctx._num_mapped_args = num_mapped_args\n+        ctx._num_pos_args = len(flat_args) - num_mapped_args\n+\n+        # We snapshot the dispatch keys in forward for materializing the\n+        # the bw_graph in backward.\n+        ctx._fw_include_key_set = torch._C._dispatch_tls_local_include_set()\n+        ctx._fw_exclude_key_set = torch._C._dispatch_tls_local_exclude_set()\n+        save_tensors_and_symints_for_backward(ctx, flat_args)\n         with torch._C._AutoDispatchBelowAutograd():\n             return (\n-                *map_impl(\n-                    fw_graph, flat_args[:num_mapped_args], flat_args[num_mapped_args:]\n-                ),\n+                *map_impl(f, flat_args[:num_mapped_args], flat_args[num_mapped_args:]),\n             )\n \n     @staticmethod\n     def backward(ctx, *flat_grads):\n         fw_args = saved_tensors_and_symints(ctx)\n-        fw_mapped_args = fw_args[: ctx._num_mapped_args]\n-        pos_args = fw_args[ctx._num_mapped_args :]\n+        num_mapped_args = ctx._num_mapped_args\n+        num_pos_args = ctx._num_pos_args\n+        num_grads = len(flat_grads)\n \n-        grads = map_impl(\n-            ctx._joint_graph,\n-            fw_mapped_args + flat_grads,\n-            pos_args,\n+        with disable_functional_mode():\n+            fw_mapped_args, pos_args = split_into_chunks(\n+                fw_args, [num_mapped_args, num_pos_args]\n+            )\n+\n+        # Prepare the backward graph:\n+        # The fw_mapped_args need to be sliced along the first dimension\n+        fw_args_prepared = (*[first_slice_copy(x) for x in fw_mapped_args], *pos_args)\n+        bw_f = create_bw_fn(ctx._f, fw_args_prepared)\n+\n+        # Create a wrapper around thefor the bw_f\n+        def bw_f_wrapper(*args):\n+            # Dissect args and re-order them for the ``ctx._bw_f``\n+            # args provided to the wrapper are composed of [*fw_mapped_args, *flat_grads, *pos_args]\n+            # The content of ``bw_f_tangents`` are the upstream gradients, i.e. flat_grads\n+            # The content of ``bw_f_primals`` are the fw_args, i.e., [*fw_mapped_args, *pos_args]\n+            # The bw_f requires *bw_f_primals, *bw_f_tangents\n+            fw_m_args, bw_f_tangents, pos_args = split_into_chunks(\n+                args, [num_mapped_args, num_grads, num_pos_args]\n+            )\n+            bw_f_primals = *fw_m_args, *pos_args\n+            return bw_f(*bw_f_primals, *bw_f_tangents)\n+\n+        def construct_args_single_step_bw():\n+            example_xs = first_slice_copy_with_grad(fw_mapped_args)",
        "comment_created_at": "2025-07-01T23:36:52+00:00",
        "comment_author": "ydwu4",
        "comment_body": "can we reuse example_xs be for fw_args_prepared? don't want to slice twice. Maybe we could remove the construct_args_single_step_bw.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1975607971",
    "pr_number": 148180,
    "pr_file": "torch/pytree.py",
    "created_at": "2025-02-28T15:31:03+00:00",
    "commented_code": "+# Owner(s): [\"module: pytree\"]\n+\n+\"\"\"\n+Contains utility functions for working with nested python data structures.\n+\n+A *pytree* is Python nested data structure. It is a tree in the sense that\n+nodes are Python collections (e.g., list, tuple, dict) and the leaves are\n+Python values. Furthermore, a pytree should not contain reference cycles.\n+\n+pytrees are useful for working with nested collections of Tensors. For example,\n+one can use `map` to map a function over all Tensors inside some nested\n+collection of Tensors and `leaves` to get a flat list of all Tensors\n+inside some nested collection. pytrees are helpful for implementing nested\n+collection support for PyTorch APIs.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Any as _Any, TYPE_CHECKING as _TYPE_CHECKING\n+\n+import torch\n+from torch.utils.pytree import (\n+    register_pytree_node as register_node,\n+    tree_all as all,\n+    tree_all_only as all_only,\n+    tree_any as any,\n+    tree_any_only as any_only,\n+    tree_flatten as flatten,\n+    tree_iter as iter,\n+    tree_leaves as leaves,\n+    tree_map as map,\n+    tree_map_ as map_,\n+    tree_map_only as map_only,\n+    tree_map_only_ as map_only_,\n+    tree_structure as structure,\n+)\n+\n+\n+if _TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n+    from torch.utils._cxx_pytree import PyTree as PyTree, PyTreeSpec as PyTreeSpec\n+\n+\n+__all__ = [\n+    \"PyTreeSpec\",\n+    \"register_node\",\n+    \"flatten\",\n+    \"unflatten\",\n+    \"iter\",\n+    \"leaves\",\n+    \"structure\",\n+    \"map\",\n+    \"map_\",\n+    \"map_only\",\n+    \"map_only_\",\n+    \"all\",\n+    \"any\",\n+    \"all_only\",\n+    \"any_only\",\n+]\n+\n+\n+def unflatten(treespec: PyTreeSpec, leaves: Iterable[_Any]) -> PyTree:\n+    \"\"\"Reconstruct a pytree from the treespec and the leaves.\n+\n+    The inverse of :func:`flatten`.\n+\n+    >>> tree = {\"b\": (2, [3, 4]), \"a\": 1, \"c\": None, \"d\": 5}\n+    >>> leaves, treespec = torch.pytree.flatten(tree)\n+    >>> tree == torch.pytree.unflatten(treespec, leaves)\n+    True\n+\n+    .. note::\n+\n+        This function has a different signature than :func:`torch.utils.pytree.tree_unflatten`.\n+        The ``treespec`` argument comes first to have a better :class:`functools.partial` support:\n+\n+        .. code-block:: python\n+\n+            import functools\n+\n+            unflatten_fn = functools.partial(unflatten, treespec)\n+            tree1 = unflatten_fn(leaves1)\n+            tree2 = unflatten_fn(leaves2)\n+\n+    Args:\n+        treespec (PyTreeSpec): The treespec to reconstruct.\n+        leaves (iterable): The list of leaves to use for reconstruction. The list must match the\n+            number of leaves of the treespec.\n+\n+    Returns:\n+        The reconstructed pytree, containing the ``leaves`` placed in the structure described by\n+        ``treespec``.\n+    \"\"\"\n+    return torch.utils.pytree.tree_unflatten(leaves, treespec)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1975607971",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 148180,
        "pr_file": "torch/pytree.py",
        "discussion_id": "1975607971",
        "commented_code": "@@ -0,0 +1,102 @@\n+# Owner(s): [\"module: pytree\"]\n+\n+\"\"\"\n+Contains utility functions for working with nested python data structures.\n+\n+A *pytree* is Python nested data structure. It is a tree in the sense that\n+nodes are Python collections (e.g., list, tuple, dict) and the leaves are\n+Python values. Furthermore, a pytree should not contain reference cycles.\n+\n+pytrees are useful for working with nested collections of Tensors. For example,\n+one can use `map` to map a function over all Tensors inside some nested\n+collection of Tensors and `leaves` to get a flat list of all Tensors\n+inside some nested collection. pytrees are helpful for implementing nested\n+collection support for PyTorch APIs.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Any as _Any, TYPE_CHECKING as _TYPE_CHECKING\n+\n+import torch\n+from torch.utils.pytree import (\n+    register_pytree_node as register_node,\n+    tree_all as all,\n+    tree_all_only as all_only,\n+    tree_any as any,\n+    tree_any_only as any_only,\n+    tree_flatten as flatten,\n+    tree_iter as iter,\n+    tree_leaves as leaves,\n+    tree_map as map,\n+    tree_map_ as map_,\n+    tree_map_only as map_only,\n+    tree_map_only_ as map_only_,\n+    tree_structure as structure,\n+)\n+\n+\n+if _TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n+    from torch.utils._cxx_pytree import PyTree as PyTree, PyTreeSpec as PyTreeSpec\n+\n+\n+__all__ = [\n+    \"PyTreeSpec\",\n+    \"register_node\",\n+    \"flatten\",\n+    \"unflatten\",\n+    \"iter\",\n+    \"leaves\",\n+    \"structure\",\n+    \"map\",\n+    \"map_\",\n+    \"map_only\",\n+    \"map_only_\",\n+    \"all\",\n+    \"any\",\n+    \"all_only\",\n+    \"any_only\",\n+]\n+\n+\n+def unflatten(treespec: PyTreeSpec, leaves: Iterable[_Any]) -> PyTree:\n+    \"\"\"Reconstruct a pytree from the treespec and the leaves.\n+\n+    The inverse of :func:`flatten`.\n+\n+    >>> tree = {\"b\": (2, [3, 4]), \"a\": 1, \"c\": None, \"d\": 5}\n+    >>> leaves, treespec = torch.pytree.flatten(tree)\n+    >>> tree == torch.pytree.unflatten(treespec, leaves)\n+    True\n+\n+    .. note::\n+\n+        This function has a different signature than :func:`torch.utils.pytree.tree_unflatten`.\n+        The ``treespec`` argument comes first to have a better :class:`functools.partial` support:\n+\n+        .. code-block:: python\n+\n+            import functools\n+\n+            unflatten_fn = functools.partial(unflatten, treespec)\n+            tree1 = unflatten_fn(leaves1)\n+            tree2 = unflatten_fn(leaves2)\n+\n+    Args:\n+        treespec (PyTreeSpec): The treespec to reconstruct.\n+        leaves (iterable): The list of leaves to use for reconstruction. The list must match the\n+            number of leaves of the treespec.\n+\n+    Returns:\n+        The reconstructed pytree, containing the ``leaves`` placed in the structure described by\n+        ``treespec``.\n+    \"\"\"\n+    return torch.utils.pytree.tree_unflatten(leaves, treespec)",
        "comment_created_at": "2025-02-28T15:31:03+00:00",
        "comment_author": "vmoens",
        "comment_body": "Nit: Is there a reason why we don't just\r\n```suggestion\r\n    return tree_unflatten(leaves, treespec)\r\n```\r\nand add the import?\r\nThis call comes at [the price](https://gist.github.com/vmoens/5c9fb9d1572ddb10b15d762b1dea3e49) of 3 `__getattribute__`",
        "pr_file_module": null
      }
    ]
  }
]