[
  {
    "discussion_id": "2079807853",
    "pr_number": 147300,
    "pr_file": "cmake/public/utils.cmake",
    "created_at": "2025-05-08T14:13:36+00:00",
    "commented_code": "string(REPLACE \" \" \";\" ${store_var} \"${_TMP}\")\n endmacro()\n \n+# cuda_select_nvcc_arch_flags is part of find_package(CUDA), but not find_package(CUDAToolkit);\n+# vendor it from https://github.com/Kitware/CMake/blob/master/Modules/FindCUDA/select_compute_arch.cmake\n+# but disable CUDA_DETECT_INSTALLED_GPUS\n+################################################################################################\n+# Function for selecting GPU arch flags for nvcc based on CUDA architectures from parameter list\n+# Usage:\n+#   SELECT_NVCC_ARCH_FLAGS(out_variable [list of CUDA compute archs])\n+function(CUDA_SELECT_NVCC_ARCH_FLAGS out_variable)\n+  set(CUDA_ARCH_LIST \"${ARGN}\")\n+\n+  if(\"X${CUDA_ARCH_LIST}\" STREQUAL \"X\" )\n+    set(CUDA_ARCH_LIST \"Auto\")\n+  endif()\n+\n+  set(cuda_arch_bin)\n+  set(cuda_arch_ptx)\n+\n+  if(\"${CUDA_ARCH_LIST}\" STREQUAL \"All\")\n+    set(CUDA_ARCH_LIST ${CUDA_KNOWN_GPU_ARCHITECTURES})\n+  elseif(\"${CUDA_ARCH_LIST}\" STREQUAL \"Common\")\n+    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})\n+  elseif(\"${CUDA_ARCH_LIST}\" STREQUAL \"Auto\")\n+    # disabled, replaced by common architectures\n+    # CUDA_DETECT_INSTALLED_GPUS(CUDA_ARCH_LIST)\n+    # message(STATUS \"Autodetected CUDA architecture(s): ${CUDA_ARCH_LIST}\")\n+    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})\n+  endif()\n+\n+  # Now process the list and look for names\n+  string(REGEX REPLACE \"[ \\t]+\" \";\" CUDA_ARCH_LIST \"${CUDA_ARCH_LIST}\")\n+  list(REMOVE_DUPLICATES CUDA_ARCH_LIST)\n+  foreach(arch_name ${CUDA_ARCH_LIST})\n+    set(arch_bin)\n+    set(arch_ptx)\n+    set(add_ptx FALSE)\n+    # Check to see if we are compiling PTX\n+    if(arch_name MATCHES \"(.*)\\\\+PTX$\")\n+      set(add_ptx TRUE)\n+      set(arch_name ${CMAKE_MATCH_1})\n+    endif()\n+    if(arch_name MATCHES \"^([0-9]\\\\.[0-9](\\\\([0-9]\\\\.[0-9]\\\\))?)$\")\n+      set(arch_bin ${CMAKE_MATCH_1})\n+      set(arch_ptx ${arch_bin})\n+    else()\n+      # Look for it in our list of known architectures\n+      if(${arch_name} STREQUAL \"Fermi\")\n+        set(arch_bin 2.0 \"2.1(2.0)\")\n+      elseif(${arch_name} STREQUAL \"Kepler+Tegra\")\n+        set(arch_bin 3.2)\n+      elseif(${arch_name} STREQUAL \"Kepler+Tesla\")\n+        set(arch_bin 3.7)\n+      elseif(${arch_name} STREQUAL \"Kepler\")\n+        set(arch_bin 3.0 3.5)\n+        set(arch_ptx 3.5)\n+      elseif(${arch_name} STREQUAL \"Maxwell+Tegra\")\n+        set(arch_bin 5.3)\n+      elseif(${arch_name} STREQUAL \"Maxwell\")\n+        set(arch_bin 5.0 5.2)\n+        set(arch_ptx 5.2)\n+      elseif(${arch_name} STREQUAL \"Pascal\")\n+        set(arch_bin 6.0 6.1)\n+        set(arch_ptx 6.1)\n+      elseif(${arch_name} STREQUAL \"Volta\")\n+        set(arch_bin 7.0 7.0)\n+        set(arch_ptx 7.0)\n+      elseif(${arch_name} STREQUAL \"Turing\")\n+        set(arch_bin 7.5)\n+        set(arch_ptx 7.5)\n+      elseif(${arch_name} STREQUAL \"Ampere\")\n+        set(arch_bin 8.0)\n+        set(arch_ptx 8.0)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2079807853",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 147300,
        "pr_file": "cmake/public/utils.cmake",
        "discussion_id": "2079807853",
        "commented_code": "@@ -306,6 +306,133 @@ macro(torch_hip_get_arch_list store_var)\n   string(REPLACE \" \" \";\" ${store_var} \"${_TMP}\")\n endmacro()\n \n+# cuda_select_nvcc_arch_flags is part of find_package(CUDA), but not find_package(CUDAToolkit);\n+# vendor it from https://github.com/Kitware/CMake/blob/master/Modules/FindCUDA/select_compute_arch.cmake\n+# but disable CUDA_DETECT_INSTALLED_GPUS\n+################################################################################################\n+# Function for selecting GPU arch flags for nvcc based on CUDA architectures from parameter list\n+# Usage:\n+#   SELECT_NVCC_ARCH_FLAGS(out_variable [list of CUDA compute archs])\n+function(CUDA_SELECT_NVCC_ARCH_FLAGS out_variable)\n+  set(CUDA_ARCH_LIST \"${ARGN}\")\n+\n+  if(\"X${CUDA_ARCH_LIST}\" STREQUAL \"X\" )\n+    set(CUDA_ARCH_LIST \"Auto\")\n+  endif()\n+\n+  set(cuda_arch_bin)\n+  set(cuda_arch_ptx)\n+\n+  if(\"${CUDA_ARCH_LIST}\" STREQUAL \"All\")\n+    set(CUDA_ARCH_LIST ${CUDA_KNOWN_GPU_ARCHITECTURES})\n+  elseif(\"${CUDA_ARCH_LIST}\" STREQUAL \"Common\")\n+    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})\n+  elseif(\"${CUDA_ARCH_LIST}\" STREQUAL \"Auto\")\n+    # disabled, replaced by common architectures\n+    # CUDA_DETECT_INSTALLED_GPUS(CUDA_ARCH_LIST)\n+    # message(STATUS \"Autodetected CUDA architecture(s): ${CUDA_ARCH_LIST}\")\n+    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})\n+  endif()\n+\n+  # Now process the list and look for names\n+  string(REGEX REPLACE \"[ \\t]+\" \";\" CUDA_ARCH_LIST \"${CUDA_ARCH_LIST}\")\n+  list(REMOVE_DUPLICATES CUDA_ARCH_LIST)\n+  foreach(arch_name ${CUDA_ARCH_LIST})\n+    set(arch_bin)\n+    set(arch_ptx)\n+    set(add_ptx FALSE)\n+    # Check to see if we are compiling PTX\n+    if(arch_name MATCHES \"(.*)\\\\+PTX$\")\n+      set(add_ptx TRUE)\n+      set(arch_name ${CMAKE_MATCH_1})\n+    endif()\n+    if(arch_name MATCHES \"^([0-9]\\\\.[0-9](\\\\([0-9]\\\\.[0-9]\\\\))?)$\")\n+      set(arch_bin ${CMAKE_MATCH_1})\n+      set(arch_ptx ${arch_bin})\n+    else()\n+      # Look for it in our list of known architectures\n+      if(${arch_name} STREQUAL \"Fermi\")\n+        set(arch_bin 2.0 \"2.1(2.0)\")\n+      elseif(${arch_name} STREQUAL \"Kepler+Tegra\")\n+        set(arch_bin 3.2)\n+      elseif(${arch_name} STREQUAL \"Kepler+Tesla\")\n+        set(arch_bin 3.7)\n+      elseif(${arch_name} STREQUAL \"Kepler\")\n+        set(arch_bin 3.0 3.5)\n+        set(arch_ptx 3.5)\n+      elseif(${arch_name} STREQUAL \"Maxwell+Tegra\")\n+        set(arch_bin 5.3)\n+      elseif(${arch_name} STREQUAL \"Maxwell\")\n+        set(arch_bin 5.0 5.2)\n+        set(arch_ptx 5.2)\n+      elseif(${arch_name} STREQUAL \"Pascal\")\n+        set(arch_bin 6.0 6.1)\n+        set(arch_ptx 6.1)\n+      elseif(${arch_name} STREQUAL \"Volta\")\n+        set(arch_bin 7.0 7.0)\n+        set(arch_ptx 7.0)\n+      elseif(${arch_name} STREQUAL \"Turing\")\n+        set(arch_bin 7.5)\n+        set(arch_ptx 7.5)\n+      elseif(${arch_name} STREQUAL \"Ampere\")\n+        set(arch_bin 8.0)\n+        set(arch_ptx 8.0)",
        "comment_created_at": "2025-05-08T14:13:36+00:00",
        "comment_author": "Skylion007",
        "comment_body": "We are missing a bunch here for Blackwell",
        "pr_file_module": null
      },
      {
        "comment_id": "2080651489",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 147300,
        "pr_file": "cmake/public/utils.cmake",
        "discussion_id": "2079807853",
        "commented_code": "@@ -306,6 +306,133 @@ macro(torch_hip_get_arch_list store_var)\n   string(REPLACE \" \" \";\" ${store_var} \"${_TMP}\")\n endmacro()\n \n+# cuda_select_nvcc_arch_flags is part of find_package(CUDA), but not find_package(CUDAToolkit);\n+# vendor it from https://github.com/Kitware/CMake/blob/master/Modules/FindCUDA/select_compute_arch.cmake\n+# but disable CUDA_DETECT_INSTALLED_GPUS\n+################################################################################################\n+# Function for selecting GPU arch flags for nvcc based on CUDA architectures from parameter list\n+# Usage:\n+#   SELECT_NVCC_ARCH_FLAGS(out_variable [list of CUDA compute archs])\n+function(CUDA_SELECT_NVCC_ARCH_FLAGS out_variable)\n+  set(CUDA_ARCH_LIST \"${ARGN}\")\n+\n+  if(\"X${CUDA_ARCH_LIST}\" STREQUAL \"X\" )\n+    set(CUDA_ARCH_LIST \"Auto\")\n+  endif()\n+\n+  set(cuda_arch_bin)\n+  set(cuda_arch_ptx)\n+\n+  if(\"${CUDA_ARCH_LIST}\" STREQUAL \"All\")\n+    set(CUDA_ARCH_LIST ${CUDA_KNOWN_GPU_ARCHITECTURES})\n+  elseif(\"${CUDA_ARCH_LIST}\" STREQUAL \"Common\")\n+    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})\n+  elseif(\"${CUDA_ARCH_LIST}\" STREQUAL \"Auto\")\n+    # disabled, replaced by common architectures\n+    # CUDA_DETECT_INSTALLED_GPUS(CUDA_ARCH_LIST)\n+    # message(STATUS \"Autodetected CUDA architecture(s): ${CUDA_ARCH_LIST}\")\n+    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})\n+  endif()\n+\n+  # Now process the list and look for names\n+  string(REGEX REPLACE \"[ \\t]+\" \";\" CUDA_ARCH_LIST \"${CUDA_ARCH_LIST}\")\n+  list(REMOVE_DUPLICATES CUDA_ARCH_LIST)\n+  foreach(arch_name ${CUDA_ARCH_LIST})\n+    set(arch_bin)\n+    set(arch_ptx)\n+    set(add_ptx FALSE)\n+    # Check to see if we are compiling PTX\n+    if(arch_name MATCHES \"(.*)\\\\+PTX$\")\n+      set(add_ptx TRUE)\n+      set(arch_name ${CMAKE_MATCH_1})\n+    endif()\n+    if(arch_name MATCHES \"^([0-9]\\\\.[0-9](\\\\([0-9]\\\\.[0-9]\\\\))?)$\")\n+      set(arch_bin ${CMAKE_MATCH_1})\n+      set(arch_ptx ${arch_bin})\n+    else()\n+      # Look for it in our list of known architectures\n+      if(${arch_name} STREQUAL \"Fermi\")\n+        set(arch_bin 2.0 \"2.1(2.0)\")\n+      elseif(${arch_name} STREQUAL \"Kepler+Tegra\")\n+        set(arch_bin 3.2)\n+      elseif(${arch_name} STREQUAL \"Kepler+Tesla\")\n+        set(arch_bin 3.7)\n+      elseif(${arch_name} STREQUAL \"Kepler\")\n+        set(arch_bin 3.0 3.5)\n+        set(arch_ptx 3.5)\n+      elseif(${arch_name} STREQUAL \"Maxwell+Tegra\")\n+        set(arch_bin 5.3)\n+      elseif(${arch_name} STREQUAL \"Maxwell\")\n+        set(arch_bin 5.0 5.2)\n+        set(arch_ptx 5.2)\n+      elseif(${arch_name} STREQUAL \"Pascal\")\n+        set(arch_bin 6.0 6.1)\n+        set(arch_ptx 6.1)\n+      elseif(${arch_name} STREQUAL \"Volta\")\n+        set(arch_bin 7.0 7.0)\n+        set(arch_ptx 7.0)\n+      elseif(${arch_name} STREQUAL \"Turing\")\n+        set(arch_bin 7.5)\n+        set(arch_ptx 7.5)\n+      elseif(${arch_name} STREQUAL \"Ampere\")\n+        set(arch_bin 8.0)\n+        set(arch_ptx 8.0)",
        "comment_created_at": "2025-05-08T23:58:12+00:00",
        "comment_author": "h-vetinari",
        "comment_body": "As I noted in the OP, this should (very likely) not be used as-is:\r\n\r\n> `torch_cuda_get_nvcc_gencode_flag` relies on functionality (`cuda_select_nvcc_arch_flags`) that's not available anymore when switching to `find_package(CUDAToolkit)` - AFAIU, because it should be replaced by `CMAKE_CUDA_ARCHITECTURES`.\r\n> \r\n> To avoid auditing and rewriting all the call-sites of `torch_cuda_get_nvcc_gencode_flag`, I just vendored `cuda_select_nvcc_arch_flags` [from CMake](https://github.com/Kitware/CMake/blob/master/Modules/FindCUDA/select_compute_arch.cmake). A proper solution would of course mean deeper surgery on this.\r\n\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2184107053",
    "pr_number": 157498,
    "pr_file": "aten/src/ATen/native/mps/kernels/Pooling.metal",
    "created_at": "2025-07-04T01:32:45+00:00",
    "commented_code": "dilation);\n }\n \n+// Finds the element in the grad input which corresponds to the index into the\n+// pool, and then adds the grad output element to it.\n+template <typename T>\n+void max_pool_backward_impl(\n+    device AtomicType_t<T>* grad_input,\n+    T grad_output_element,\n+    int32_t input_index,\n+    constant int64_t* grad_input_sizes,\n+    constant int64_t* grad_input_strides,\n+    int32_t grad_input_leading_offset,\n+    int32_t pooling_dims) {\n+  int32_t size_prod = 1;\n+  int32_t pool_offset = 0;\n+\n+  for (int32_t dim = pooling_dims - 1; dim >= 0; dim--) {\n+    int32_t next_size_prod = grad_input_sizes[dim] * size_prod;\n+    pool_offset +=\n+        grad_input_strides[dim] * ((input_index % next_size_prod) / size_prod);\n+    size_prod *= grad_input_sizes[dim];\n+  }\n+\n+  AtomicType<T>::atomic_add(",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2184107053",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157498,
        "pr_file": "aten/src/ATen/native/mps/kernels/Pooling.metal",
        "discussion_id": "2184107053",
        "commented_code": "@@ -153,6 +188,69 @@ kernel void max_pool(\n       dilation);\n }\n \n+// Finds the element in the grad input which corresponds to the index into the\n+// pool, and then adds the grad output element to it.\n+template <typename T>\n+void max_pool_backward_impl(\n+    device AtomicType_t<T>* grad_input,\n+    T grad_output_element,\n+    int32_t input_index,\n+    constant int64_t* grad_input_sizes,\n+    constant int64_t* grad_input_strides,\n+    int32_t grad_input_leading_offset,\n+    int32_t pooling_dims) {\n+  int32_t size_prod = 1;\n+  int32_t pool_offset = 0;\n+\n+  for (int32_t dim = pooling_dims - 1; dim >= 0; dim--) {\n+    int32_t next_size_prod = grad_input_sizes[dim] * size_prod;\n+    pool_offset +=\n+        grad_input_strides[dim] * ((input_index % next_size_prod) / size_prod);\n+    size_prod *= grad_input_sizes[dim];\n+  }\n+\n+  AtomicType<T>::atomic_add(",
        "comment_created_at": "2025-07-04T01:32:45+00:00",
        "comment_author": "kurtamohler",
        "comment_body": "Is `AtomicType::atomic_add` deterministic? My guess is no, in which case I think I should mark this op nondeterministic for `torch.use_deterministic_algorithms`. I think we would only need the nondeterministic alert to be raised if the input requires grad and the stride is less than kernel size in any of the dimensions.",
        "pr_file_module": null
      },
      {
        "comment_id": "2184236772",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157498,
        "pr_file": "aten/src/ATen/native/mps/kernels/Pooling.metal",
        "discussion_id": "2184107053",
        "commented_code": "@@ -153,6 +188,69 @@ kernel void max_pool(\n       dilation);\n }\n \n+// Finds the element in the grad input which corresponds to the index into the\n+// pool, and then adds the grad output element to it.\n+template <typename T>\n+void max_pool_backward_impl(\n+    device AtomicType_t<T>* grad_input,\n+    T grad_output_element,\n+    int32_t input_index,\n+    constant int64_t* grad_input_sizes,\n+    constant int64_t* grad_input_strides,\n+    int32_t grad_input_leading_offset,\n+    int32_t pooling_dims) {\n+  int32_t size_prod = 1;\n+  int32_t pool_offset = 0;\n+\n+  for (int32_t dim = pooling_dims - 1; dim >= 0; dim--) {\n+    int32_t next_size_prod = grad_input_sizes[dim] * size_prod;\n+    pool_offset +=\n+        grad_input_strides[dim] * ((input_index % next_size_prod) / size_prod);\n+    size_prod *= grad_input_sizes[dim];\n+  }\n+\n+  AtomicType<T>::atomic_add(",
        "comment_created_at": "2025-07-04T03:31:15+00:00",
        "comment_author": "kurtamohler",
        "comment_body": "If there is need for it, I could write a deterministic alternative that doesn't use atomic add. The CUDA impl doesn't have this yet either",
        "pr_file_module": null
      }
    ]
  }
]