[
  {
    "discussion_id": "2168080662",
    "pr_number": 156923,
    "pr_file": "torch/utils/cpp_extension.py",
    "created_at": "2025-06-26T04:30:00+00:00",
    "commented_code": "# See cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake\n     _arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST', None)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2168080662",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156923,
        "pr_file": "torch/utils/cpp_extension.py",
        "discussion_id": "2168080662",
        "commented_code": "@@ -2420,11 +2420,12 @@ def _get_cuda_arch_flags(cflags: Optional[list[str]] = None) -> list[str]:\n     # See cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake\n     _arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST', None)",
        "comment_created_at": "2025-06-26T04:30:00+00:00",
        "comment_author": "Copilot",
        "comment_body": "Normalize the environment variable with `_arch_list = _arch_list.strip().lower()` before comparison to ensure case-insensitive matching and avoid unintended whitespace issues.\n```suggestion\n    _arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST', None)\n    if _arch_list:\n        _arch_list = _arch_list.strip().lower()\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189815338",
    "pr_number": 150312,
    "pr_file": "test/test_cuda.py",
    "created_at": "2025-07-07T11:43:35+00:00",
    "commented_code": "reg_mem = torch.cuda.memory_stats()[key_allocated]\n         self.assertEqual(reg_mem - start_mem, nbytes)\n \n-        with self.assertRaises(RuntimeError):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2189815338",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150312,
        "pr_file": "test/test_cuda.py",
        "discussion_id": "2189815338",
        "commented_code": "@@ -4441,9 +4441,6 @@ def power2_div(size, div_factor):\n         reg_mem = torch.cuda.memory_stats()[key_allocated]\n         self.assertEqual(reg_mem - start_mem, nbytes)\n \n-        with self.assertRaises(RuntimeError):",
        "comment_created_at": "2025-07-07T11:43:35+00:00",
        "comment_author": "albanD",
        "comment_body": "This should still error out?",
        "pr_file_module": null
      },
      {
        "comment_id": "2191335150",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150312,
        "pr_file": "test/test_cuda.py",
        "discussion_id": "2189815338",
        "commented_code": "@@ -4441,9 +4441,6 @@ def power2_div(size, div_factor):\n         reg_mem = torch.cuda.memory_stats()[key_allocated]\n         self.assertEqual(reg_mem - start_mem, nbytes)\n \n-        with self.assertRaises(RuntimeError):",
        "comment_created_at": "2025-07-08T02:14:10+00:00",
        "comment_author": "guangyey",
        "comment_body": "Since `foo:1,bar:2` may represent key-value configuration pairs for other accelerator backends, raising an error is inappropriate if `CUDAAllocatorConfig` does not recognize them. So, the logic is that all allocator configurations only parse the `key-value` pairs that they recognize and skip or ignore the others.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2183009838",
    "pr_number": 144554,
    "pr_file": "torch/cuda/tunable.py",
    "created_at": "2025-07-03T14:58:09+00:00",
    "commented_code": ".. code-block:: python\n \n-   PYTORCH_TUNABLEOP_ENABLED=1\n-   PYTORCH_TUNABLEOP_TUNING=0\n-   PYTORCH_TUNABLEOP_RECORD_UNTUNED=1\n+   PYTORCH_TUNABLEOP_ENABLED = 1\n+   PYTORCH_TUNABLEOP_TUNING = 0\n+   PYTORCH_TUNABLEOP_RECORD_UNTUNED = 1",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2183009838",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 144554,
        "pr_file": "torch/cuda/tunable.py",
        "discussion_id": "2183009838",
        "commented_code": "@@ -126,9 +126,9 @@\n \n .. code-block:: python\n \n-   PYTORCH_TUNABLEOP_ENABLED=1\n-   PYTORCH_TUNABLEOP_TUNING=0\n-   PYTORCH_TUNABLEOP_RECORD_UNTUNED=1\n+   PYTORCH_TUNABLEOP_ENABLED = 1\n+   PYTORCH_TUNABLEOP_TUNING = 0\n+   PYTORCH_TUNABLEOP_RECORD_UNTUNED = 1",
        "comment_created_at": "2025-07-03T14:58:09+00:00",
        "comment_author": "soulitzer",
        "comment_body": "We probably don't want a space if these are setting environment variables",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178160440",
    "pr_number": 156628,
    "pr_file": "torch/_inductor/config.py",
    "created_at": "2025-07-01T17:20:15+00:00",
    "commented_code": "True if is_fbcode() else os.environ.get(\"TORCHINDUCTOR_FORCE_SAME_PRECISION\") == \"1\"\n )\n \n+# Size hints for multi-kernel dispatch.\n+# A reasonable default value of this config would be [64, 256, 4096]\n+# TODO: @bobrenjc93 to roll this out to a few internal models to ensure this works\n+# as expected before turning it on for everyone.\n+multi_kernel_hints: list[int] = []",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2178160440",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156628,
        "pr_file": "torch/_inductor/config.py",
        "discussion_id": "2178160440",
        "commented_code": "@@ -432,6 +432,12 @@ def prologue_fusion_enabled() -> bool:\n     True if is_fbcode() else os.environ.get(\"TORCHINDUCTOR_FORCE_SAME_PRECISION\") == \"1\"\n )\n \n+# Size hints for multi-kernel dispatch.\n+# A reasonable default value of this config would be [64, 256, 4096]\n+# TODO: @bobrenjc93 to roll this out to a few internal models to ensure this works\n+# as expected before turning it on for everyone.\n+multi_kernel_hints: list[int] = []",
        "comment_created_at": "2025-07-01T17:20:15+00:00",
        "comment_author": "jansel",
        "comment_body": "Is one global config enough here?  Or do we need something per-call site?  What about non-squaere sizes?",
        "pr_file_module": null
      },
      {
        "comment_id": "2178304909",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156628,
        "pr_file": "torch/_inductor/config.py",
        "discussion_id": "2178160440",
        "commented_code": "@@ -432,6 +432,12 @@ def prologue_fusion_enabled() -> bool:\n     True if is_fbcode() else os.environ.get(\"TORCHINDUCTOR_FORCE_SAME_PRECISION\") == \"1\"\n )\n \n+# Size hints for multi-kernel dispatch.\n+# A reasonable default value of this config would be [64, 256, 4096]\n+# TODO: @bobrenjc93 to roll this out to a few internal models to ensure this works\n+# as expected before turning it on for everyone.\n+multi_kernel_hints: list[int] = []",
        "comment_created_at": "2025-07-01T18:33:46+00:00",
        "comment_author": "bobrenjc93",
        "comment_body": "There\u2019s a tradeoff between configuration complexity and performance. Since this is in PT2, where kernel generation is mostly opaque to the user, I figured a simple approach like this is preferable. If someone really needs fine-grained control over per-kernel parameters, Helion might be a better fit for that level of customization.\r\n\r\nAs for non-square shapes, I\u2019m not entirely sure what you\u2019re referring to, but here are a few examples to clarify what happens with multi_kernel_hints = [64, 256] and a runtime hint of 16:\r\n\r\n[s0, 100] @ [100, s0]\r\n\u2192 First kernel: tuned with [16, 100] @ [100, 16]\r\n\u2192 Second kernel: [64, 100] @ [100, 64]\r\n\u2192 Third kernel: [256, 100] @ [100, 256]\r\n\r\n[s0, 100] @ [100, 200]\r\n\u2192 First kernel: tuned with [16, 100] @ [100, 200]\r\n\u2192 Second kernel: [64, 100] @ [100, 200]\r\n\u2192 Third kernel: [256, 100] @ [100, 200]\r\n\r\n[s0, s1] @ [s2, s3]\r\n\u2192 First kernel: [16, 16] @ [16, 16]\r\n\u2192 Second kernel: [64, 64] @ [64, 64]\r\n\u2192 Third kernel: [256, 256] @ [256, 256]\r\n\r\nNB: we enforce that s1 == s2 via a guard.",
        "pr_file_module": null
      }
    ]
  }
]