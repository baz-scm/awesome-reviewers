[
  {
    "discussion_id": "2125312388",
    "pr_number": 154977,
    "pr_file": "aten/src/ATen/native/mkldnn/xpu/detail/MatmulHelpers.h",
    "created_at": "2025-06-04T02:08:13+00:00",
    "commented_code": "+#pragma once\n+\n+#include <ATen/ATen.h>\n+\n+#include <ATen/native/mkldnn/xpu/detail/Attr.h>\n+#include <ATen/native/mkldnn/xpu/detail/Utils.h>\n+\n+#include <oneapi/dnnl/dnnl.hpp>\n+\n+namespace at::native::onednn {\n+\n+class BiasHandler {\n+  // This class handles the bias tensor for matmul operations.\n+  // It checks the dimensions of the bias tensor and expands it if necessary\n+  // for broadcasting.\n+ public:\n+  BiasHandler() = delete;\n+  BiasHandler(\n+      at::Tensor& bias,\n+      int64_t mb,\n+      int64_t m,\n+      int64_t k,\n+      int64_t n,\n+      int64_t gemm_dim)\n+      : b(bias),\n+        mb(mb),\n+        m(m),\n+        k(k),\n+        n(n),\n+        gemm_dim(gemm_dim),\n+        with_bias(bias.defined()) {}\n+\n+  void handle0D() {\n+    TORCH_CHECK(\n+        b.numel() == 1, \"matmul supports 1 numel when bias dim is [] ...\");\n+    if (gemm_dim == 3) {\n+      b = b.expand({mb, m, n}).contiguous();\n+    } else {\n+      b = b.expand({1, n}).contiguous();\n+    }\n+  }\n+\n+  void handle1D() {\n+    TORCH_CHECK(\n+        b.size(0) == n || b.size(0) == 1,\n+        \"matmul supports [n] or [1] when bias dim is 1...\");\n+    if (b.size(0) == 0) {\n+      with_bias = false;\n+    } else if (gemm_dim == 3) {\n+      b = b.expand({mb, m, n}).contiguous();\n+    } else if (gemm_dim == 2) {\n+      b = b.expand({1, n}).contiguous();\n+    }\n+  }\n+\n+  void handle2D() {\n+    TORCH_CHECK(\n+        (b.size(0) == m && b.size(1) == n) ||\n+            (b.size(0) == 1 && b.size(1) == n) ||\n+            (b.size(0) == m && b.size(1) == 1) ||\n+            (b.size(0) == 1 && b.size(1) == 1),\n+        \"matmul supports [m, n] or [1, n] or [m, 1] or [1, 1] when bias dim is 2 ...\");\n+    if (b.size(0) == 1 && b.size(1) == 1)\n+      b = b.expand({1, n}).contiguous();\n+  }\n+\n+  void handle3D() {\n+    TORCH_CHECK(\n+        at::are_expandable({mb, m, n}, b.sizes()),\n+        \"matmul bias must be expandable to:\",\n+        \"{mb, m, n} where mb is the batch size, m is the number of rows, and n is the number of columns.\",\n+        \" but got:\",\n+        b.sizes());\n+    b = b.expand({mb, m, n}).contiguous();\n+  }\n+\n+  bool is_with_bias() {\n+    return with_bias;\n+  }\n+\n+  void handle() {\n+    with_bias = b.defined();\n+    if (!with_bias)\n+      return;\n+    using HandlerFn = void (BiasHandler::*)();\n+    std::unordered_map<int, HandlerFn> handler_map = {",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2125312388",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154977,
        "pr_file": "aten/src/ATen/native/mkldnn/xpu/detail/MatmulHelpers.h",
        "discussion_id": "2125312388",
        "commented_code": "@@ -0,0 +1,231 @@\n+#pragma once\n+\n+#include <ATen/ATen.h>\n+\n+#include <ATen/native/mkldnn/xpu/detail/Attr.h>\n+#include <ATen/native/mkldnn/xpu/detail/Utils.h>\n+\n+#include <oneapi/dnnl/dnnl.hpp>\n+\n+namespace at::native::onednn {\n+\n+class BiasHandler {\n+  // This class handles the bias tensor for matmul operations.\n+  // It checks the dimensions of the bias tensor and expands it if necessary\n+  // for broadcasting.\n+ public:\n+  BiasHandler() = delete;\n+  BiasHandler(\n+      at::Tensor& bias,\n+      int64_t mb,\n+      int64_t m,\n+      int64_t k,\n+      int64_t n,\n+      int64_t gemm_dim)\n+      : b(bias),\n+        mb(mb),\n+        m(m),\n+        k(k),\n+        n(n),\n+        gemm_dim(gemm_dim),\n+        with_bias(bias.defined()) {}\n+\n+  void handle0D() {\n+    TORCH_CHECK(\n+        b.numel() == 1, \"matmul supports 1 numel when bias dim is [] ...\");\n+    if (gemm_dim == 3) {\n+      b = b.expand({mb, m, n}).contiguous();\n+    } else {\n+      b = b.expand({1, n}).contiguous();\n+    }\n+  }\n+\n+  void handle1D() {\n+    TORCH_CHECK(\n+        b.size(0) == n || b.size(0) == 1,\n+        \"matmul supports [n] or [1] when bias dim is 1...\");\n+    if (b.size(0) == 0) {\n+      with_bias = false;\n+    } else if (gemm_dim == 3) {\n+      b = b.expand({mb, m, n}).contiguous();\n+    } else if (gemm_dim == 2) {\n+      b = b.expand({1, n}).contiguous();\n+    }\n+  }\n+\n+  void handle2D() {\n+    TORCH_CHECK(\n+        (b.size(0) == m && b.size(1) == n) ||\n+            (b.size(0) == 1 && b.size(1) == n) ||\n+            (b.size(0) == m && b.size(1) == 1) ||\n+            (b.size(0) == 1 && b.size(1) == 1),\n+        \"matmul supports [m, n] or [1, n] or [m, 1] or [1, 1] when bias dim is 2 ...\");\n+    if (b.size(0) == 1 && b.size(1) == 1)\n+      b = b.expand({1, n}).contiguous();\n+  }\n+\n+  void handle3D() {\n+    TORCH_CHECK(\n+        at::are_expandable({mb, m, n}, b.sizes()),\n+        \"matmul bias must be expandable to:\",\n+        \"{mb, m, n} where mb is the batch size, m is the number of rows, and n is the number of columns.\",\n+        \" but got:\",\n+        b.sizes());\n+    b = b.expand({mb, m, n}).contiguous();\n+  }\n+\n+  bool is_with_bias() {\n+    return with_bias;\n+  }\n+\n+  void handle() {\n+    with_bias = b.defined();\n+    if (!with_bias)\n+      return;\n+    using HandlerFn = void (BiasHandler::*)();\n+    std::unordered_map<int, HandlerFn> handler_map = {",
        "comment_created_at": "2025-06-04T02:08:13+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n    static std::unordered_map<int, HandlerFn> handler_map = {\r\n```\r\nAvoid unordered_map constructor overhead each time.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2125344316",
    "pr_number": 154977,
    "pr_file": "aten/src/ATen/native/mkldnn/xpu/detail/MatmulHelpers.h",
    "created_at": "2025-06-04T02:26:28+00:00",
    "commented_code": "+#pragma once\n+\n+#include <ATen/ATen.h>\n+\n+#include <ATen/native/mkldnn/xpu/detail/Attr.h>\n+#include <ATen/native/mkldnn/xpu/detail/Utils.h>\n+\n+#include <oneapi/dnnl/dnnl.hpp>\n+\n+namespace at::native::onednn {\n+\n+class BiasHandler {\n+  // This class handles the bias tensor for matmul operations.\n+  // It checks the dimensions of the bias tensor and expands it if necessary\n+  // for broadcasting.\n+ public:\n+  BiasHandler() = delete;\n+  BiasHandler(\n+      at::Tensor& bias,\n+      int64_t mb,\n+      int64_t m,\n+      int64_t k,\n+      int64_t n,\n+      int64_t gemm_dim)\n+      : b(bias),\n+        mb(mb),\n+        m(m),\n+        k(k),\n+        n(n),\n+        gemm_dim(gemm_dim),\n+        with_bias(bias.defined()) {}\n+\n+  void handle0D() {\n+    TORCH_CHECK(\n+        b.numel() == 1, \"matmul supports 1 numel when bias dim is [] ...\");\n+    if (gemm_dim == 3) {\n+      b = b.expand({mb, m, n}).contiguous();\n+    } else {\n+      b = b.expand({1, n}).contiguous();\n+    }\n+  }\n+\n+  void handle1D() {\n+    TORCH_CHECK(\n+        b.size(0) == n || b.size(0) == 1,\n+        \"matmul supports [n] or [1] when bias dim is 1...\");\n+    if (b.size(0) == 0) {\n+      with_bias = false;\n+    } else if (gemm_dim == 3) {\n+      b = b.expand({mb, m, n}).contiguous();\n+    } else if (gemm_dim == 2) {\n+      b = b.expand({1, n}).contiguous();\n+    }\n+  }\n+\n+  void handle2D() {\n+    TORCH_CHECK(\n+        (b.size(0) == m && b.size(1) == n) ||\n+            (b.size(0) == 1 && b.size(1) == n) ||\n+            (b.size(0) == m && b.size(1) == 1) ||\n+            (b.size(0) == 1 && b.size(1) == 1),\n+        \"matmul supports [m, n] or [1, n] or [m, 1] or [1, 1] when bias dim is 2 ...\");\n+    if (b.size(0) == 1 && b.size(1) == 1)\n+      b = b.expand({1, n}).contiguous();\n+  }\n+\n+  void handle3D() {\n+    TORCH_CHECK(\n+        at::are_expandable({mb, m, n}, b.sizes()),\n+        \"matmul bias must be expandable to:\",\n+        \"{mb, m, n} where mb is the batch size, m is the number of rows, and n is the number of columns.\",\n+        \" but got:\",\n+        b.sizes());\n+    b = b.expand({mb, m, n}).contiguous();\n+  }\n+\n+  bool is_with_bias() {\n+    return with_bias;\n+  }\n+\n+  void handle() {\n+    with_bias = b.defined();\n+    if (!with_bias)\n+      return;\n+    using HandlerFn = void (BiasHandler::*)();\n+    std::unordered_map<int, HandlerFn> handler_map = {\n+        {0, &BiasHandler::handle0D},\n+        {1, &BiasHandler::handle1D},\n+        {2, &BiasHandler::handle2D},\n+        {3, &BiasHandler::handle3D}};\n+\n+    auto iter = handler_map.find(b.dim());\n+    TORCH_CHECK(iter != handler_map.end(), \"invalid bias dim:\", b.dim());\n+    (this->*(iter->second))();\n+  }\n+\n+ private:\n+  at::Tensor& b;\n+  int mb, m, k, n, gemm_dim;\n+  bool with_bias;\n+};\n+\n+class GEMMMemoryCreator {\n+  // This class creates memory descriptors and memory objects for GEMM\n+  // operations.\n+ public:\n+  GEMMMemoryCreator() = delete;\n+  GEMMMemoryCreator(\n+      int64_t dim,\n+      int64_t m,\n+      int64_t k,\n+      int64_t n,\n+      int64_t mb,\n+      int64_t bsA,\n+      int64_t bsB,\n+      bool m2_trans,\n+      bool with_bias)\n+      : ndim(dim),\n+        m(m),\n+        k(k),\n+        n{n},\n+        mb{mb},\n+        bsA(bsA),\n+        bsB(bsB),\n+        m2_trans(m2_trans),\n+        with_bias(with_bias) {}\n+\n+  using arg_map_t = std::unordered_map<int, dnnl::memory>;\n+  using md_t = dnnl::memory::desc;\n+\n+  void handle2D(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst) {\n+    m1_dims = {m, k};\n+    m2_dims = {k, n};\n+    dst_dims = {m, n};\n+\n+    m1_strides = {m1.stride(0), m1.stride(1)};\n+    m2_strides = m2_trans ? std::vector<int64_t>{m2.stride(0), m2.stride(1)}\n+                          : std::vector<int64_t>{m2.stride(1), m2.stride(0)};\n+    dst_strides = {dst.stride(0), dst.stride(1)};\n+  }\n+\n+  void handle3D(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst) {\n+    m1_dims = dnnl::memory::dims({bsA, m, k});\n+    m2_dims = dnnl::memory::dims({bsB, k, n});\n+    dst_dims = dnnl::memory::dims({mb, m, n});\n+\n+    m1_strides = {m1.stride(0), m1.stride(1), m1.stride(2)};\n+    m2_strides = m2_trans\n+        ? std::vector<int64_t>{m2.stride(0), m2.stride(1), m2.stride(2)}\n+        : std::vector<int64_t>{m2.stride(0), m2.stride(2), m2.stride(1)};\n+    dst_strides = {dst.stride(0), dst.stride(1), dst.stride(2)};\n+  }\n+\n+  void initialize_md(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst) {\n+    m1_md = create_md(m1, m1_dims, m1_strides);\n+    m2_md = create_md(m2, m2_dims, m2_strides);\n+    dst_md = create_md(dst, dst_dims, dst_strides);\n+  }\n+\n+  std::tuple<md_t, md_t, md_t, md_t> query_md() {\n+    return {m1_md, m2_md, dst_md, bias_md};\n+  }\n+\n+  void initialize(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst,\n+      const at::Tensor& bias) {\n+    if (ndim == 2) {\n+      handle2D(m1, m2, dst);\n+    } else if (ndim == 3) {\n+      handle3D(m1, m2, dst);\n+    } else {\n+      TORCH_CHECK(false, \"only support 2D or 3D matmul, got ndim:\", ndim);\n+    }\n+    initialize_md(m1, m2, dst);\n+\n+    if (with_bias) {\n+      bias_dims = get_onednn_dims(bias);\n+      bias_strides = get_onednn_strides(bias);\n+      bias_md = dnnl::memory::desc(\n+          bias_dims, get_onednn_dtype_include_double(bias), bias_strides);\n+    } else {\n+      bias_md = dnnl::memory::desc();\n+    }\n+  }\n+\n+  arg_map_t create_memory(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst,\n+      const at::Tensor& bias) {\n+    std::unordered_map<int, dnnl::memory> args;\n+    auto& eng = GpuEngineManager::Instance().get_engine();\n+    args.insert({DNNL_ARG_SRC, dnnl::memory(m1_md, eng, m1.data_ptr())});\n+    args.insert({DNNL_ARG_WEIGHTS, dnnl::memory(m2_md, eng, m2.data_ptr())});\n+    args.insert({DNNL_ARG_DST, dnnl::memory(dst_md, eng, dst.data_ptr())});",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2125344316",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154977,
        "pr_file": "aten/src/ATen/native/mkldnn/xpu/detail/MatmulHelpers.h",
        "discussion_id": "2125344316",
        "commented_code": "@@ -0,0 +1,231 @@\n+#pragma once\n+\n+#include <ATen/ATen.h>\n+\n+#include <ATen/native/mkldnn/xpu/detail/Attr.h>\n+#include <ATen/native/mkldnn/xpu/detail/Utils.h>\n+\n+#include <oneapi/dnnl/dnnl.hpp>\n+\n+namespace at::native::onednn {\n+\n+class BiasHandler {\n+  // This class handles the bias tensor for matmul operations.\n+  // It checks the dimensions of the bias tensor and expands it if necessary\n+  // for broadcasting.\n+ public:\n+  BiasHandler() = delete;\n+  BiasHandler(\n+      at::Tensor& bias,\n+      int64_t mb,\n+      int64_t m,\n+      int64_t k,\n+      int64_t n,\n+      int64_t gemm_dim)\n+      : b(bias),\n+        mb(mb),\n+        m(m),\n+        k(k),\n+        n(n),\n+        gemm_dim(gemm_dim),\n+        with_bias(bias.defined()) {}\n+\n+  void handle0D() {\n+    TORCH_CHECK(\n+        b.numel() == 1, \"matmul supports 1 numel when bias dim is [] ...\");\n+    if (gemm_dim == 3) {\n+      b = b.expand({mb, m, n}).contiguous();\n+    } else {\n+      b = b.expand({1, n}).contiguous();\n+    }\n+  }\n+\n+  void handle1D() {\n+    TORCH_CHECK(\n+        b.size(0) == n || b.size(0) == 1,\n+        \"matmul supports [n] or [1] when bias dim is 1...\");\n+    if (b.size(0) == 0) {\n+      with_bias = false;\n+    } else if (gemm_dim == 3) {\n+      b = b.expand({mb, m, n}).contiguous();\n+    } else if (gemm_dim == 2) {\n+      b = b.expand({1, n}).contiguous();\n+    }\n+  }\n+\n+  void handle2D() {\n+    TORCH_CHECK(\n+        (b.size(0) == m && b.size(1) == n) ||\n+            (b.size(0) == 1 && b.size(1) == n) ||\n+            (b.size(0) == m && b.size(1) == 1) ||\n+            (b.size(0) == 1 && b.size(1) == 1),\n+        \"matmul supports [m, n] or [1, n] or [m, 1] or [1, 1] when bias dim is 2 ...\");\n+    if (b.size(0) == 1 && b.size(1) == 1)\n+      b = b.expand({1, n}).contiguous();\n+  }\n+\n+  void handle3D() {\n+    TORCH_CHECK(\n+        at::are_expandable({mb, m, n}, b.sizes()),\n+        \"matmul bias must be expandable to:\",\n+        \"{mb, m, n} where mb is the batch size, m is the number of rows, and n is the number of columns.\",\n+        \" but got:\",\n+        b.sizes());\n+    b = b.expand({mb, m, n}).contiguous();\n+  }\n+\n+  bool is_with_bias() {\n+    return with_bias;\n+  }\n+\n+  void handle() {\n+    with_bias = b.defined();\n+    if (!with_bias)\n+      return;\n+    using HandlerFn = void (BiasHandler::*)();\n+    std::unordered_map<int, HandlerFn> handler_map = {\n+        {0, &BiasHandler::handle0D},\n+        {1, &BiasHandler::handle1D},\n+        {2, &BiasHandler::handle2D},\n+        {3, &BiasHandler::handle3D}};\n+\n+    auto iter = handler_map.find(b.dim());\n+    TORCH_CHECK(iter != handler_map.end(), \"invalid bias dim:\", b.dim());\n+    (this->*(iter->second))();\n+  }\n+\n+ private:\n+  at::Tensor& b;\n+  int mb, m, k, n, gemm_dim;\n+  bool with_bias;\n+};\n+\n+class GEMMMemoryCreator {\n+  // This class creates memory descriptors and memory objects for GEMM\n+  // operations.\n+ public:\n+  GEMMMemoryCreator() = delete;\n+  GEMMMemoryCreator(\n+      int64_t dim,\n+      int64_t m,\n+      int64_t k,\n+      int64_t n,\n+      int64_t mb,\n+      int64_t bsA,\n+      int64_t bsB,\n+      bool m2_trans,\n+      bool with_bias)\n+      : ndim(dim),\n+        m(m),\n+        k(k),\n+        n{n},\n+        mb{mb},\n+        bsA(bsA),\n+        bsB(bsB),\n+        m2_trans(m2_trans),\n+        with_bias(with_bias) {}\n+\n+  using arg_map_t = std::unordered_map<int, dnnl::memory>;\n+  using md_t = dnnl::memory::desc;\n+\n+  void handle2D(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst) {\n+    m1_dims = {m, k};\n+    m2_dims = {k, n};\n+    dst_dims = {m, n};\n+\n+    m1_strides = {m1.stride(0), m1.stride(1)};\n+    m2_strides = m2_trans ? std::vector<int64_t>{m2.stride(0), m2.stride(1)}\n+                          : std::vector<int64_t>{m2.stride(1), m2.stride(0)};\n+    dst_strides = {dst.stride(0), dst.stride(1)};\n+  }\n+\n+  void handle3D(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst) {\n+    m1_dims = dnnl::memory::dims({bsA, m, k});\n+    m2_dims = dnnl::memory::dims({bsB, k, n});\n+    dst_dims = dnnl::memory::dims({mb, m, n});\n+\n+    m1_strides = {m1.stride(0), m1.stride(1), m1.stride(2)};\n+    m2_strides = m2_trans\n+        ? std::vector<int64_t>{m2.stride(0), m2.stride(1), m2.stride(2)}\n+        : std::vector<int64_t>{m2.stride(0), m2.stride(2), m2.stride(1)};\n+    dst_strides = {dst.stride(0), dst.stride(1), dst.stride(2)};\n+  }\n+\n+  void initialize_md(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst) {\n+    m1_md = create_md(m1, m1_dims, m1_strides);\n+    m2_md = create_md(m2, m2_dims, m2_strides);\n+    dst_md = create_md(dst, dst_dims, dst_strides);\n+  }\n+\n+  std::tuple<md_t, md_t, md_t, md_t> query_md() {\n+    return {m1_md, m2_md, dst_md, bias_md};\n+  }\n+\n+  void initialize(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst,\n+      const at::Tensor& bias) {\n+    if (ndim == 2) {\n+      handle2D(m1, m2, dst);\n+    } else if (ndim == 3) {\n+      handle3D(m1, m2, dst);\n+    } else {\n+      TORCH_CHECK(false, \"only support 2D or 3D matmul, got ndim:\", ndim);\n+    }\n+    initialize_md(m1, m2, dst);\n+\n+    if (with_bias) {\n+      bias_dims = get_onednn_dims(bias);\n+      bias_strides = get_onednn_strides(bias);\n+      bias_md = dnnl::memory::desc(\n+          bias_dims, get_onednn_dtype_include_double(bias), bias_strides);\n+    } else {\n+      bias_md = dnnl::memory::desc();\n+    }\n+  }\n+\n+  arg_map_t create_memory(\n+      const at::Tensor& m1,\n+      const at::Tensor& m2,\n+      const at::Tensor& dst,\n+      const at::Tensor& bias) {\n+    std::unordered_map<int, dnnl::memory> args;\n+    auto& eng = GpuEngineManager::Instance().get_engine();\n+    args.insert({DNNL_ARG_SRC, dnnl::memory(m1_md, eng, m1.data_ptr())});\n+    args.insert({DNNL_ARG_WEIGHTS, dnnl::memory(m2_md, eng, m2.data_ptr())});\n+    args.insert({DNNL_ARG_DST, dnnl::memory(dst_md, eng, dst.data_ptr())});",
        "comment_created_at": "2025-06-04T02:26:28+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n    args.emplace({DNNL_ARG_SRC, dnnl::memory(m1_md, eng, m1.data_ptr())});\r\n    args.emplace({DNNL_ARG_WEIGHTS, dnnl::memory(m2_md, eng, m2.data_ptr())});\r\n    args.emplace({DNNL_ARG_DST, dnnl::memory(dst_md, eng, dst.data_ptr())});\r\n```\r\nAvoid pair constructor overhead.",
        "pr_file_module": null
      }
    ]
  }
]