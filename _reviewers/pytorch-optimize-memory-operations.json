[
  {
    "discussion_id": "2145822363",
    "pr_number": 151547,
    "pr_file": "aten/src/ATen/native/CPUBlas.cpp",
    "created_at": "2025-06-13T18:30:57+00:00",
    "commented_code": "b, &ldb_,\n              &beta_,\n              float_v.data(), &ldc_);\n      for (auto cv: float_v) {\n        *(c++) = c10::convert<at::BFloat16>(cv);\n\n      for (const auto j : c10::irange(n)) {\n        for (const auto i : c10::irange(m)) {\n          auto offset = j * ldc + i;",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2145822363",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 151547,
        "pr_file": "aten/src/ATen/native/CPUBlas.cpp",
        "discussion_id": "2145822363",
        "commented_code": "@@ -368,8 +368,12 @@ void gemm(\n               b, &ldb_,\n               &beta_,\n               float_v.data(), &ldc_);\n-      for (auto cv: float_v) {\n-        *(c++) = c10::convert<at::BFloat16>(cv);\n+\n+      for (const auto j : c10::irange(n)) {\n+        for (const auto i : c10::irange(m)) {\n+          auto offset = j * ldc + i;",
        "comment_created_at": "2025-06-13T18:30:57+00:00",
        "comment_author": "taoye9",
        "comment_body": "it's better to move j * ldc out of inner loop to reduce unnecessary multiplication.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1996412678",
    "pr_number": 148605,
    "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
    "created_at": "2025-03-14T23:11:11+00:00",
    "commented_code": "}\n}\n\n\ntemplate <typename T, typename T_ACC>\n__global__ void GammaBetaBackwardSimpleCUDAKernel(\n// When the data size is a multiple of the tile size we can use fewer\n// instructions and registers. Example, this is the case when M=256 N=256.\ntemplate <typename T, typename T_ACC,\nunsigned int block_dim_x,\nunsigned int block_dim_y,\nunsigned int rows_per_block_y,\nbool aligned_grid>\n__device__\n__forceinline__\nvoid\nblockReduceGammaBetaBackwardsAligned(\n    int64_t M,\n    int64_t N,\n    const T* dY,\n    const T* X,\n    const T_ACC* mean,\n    const T_ACC* rstd,\n    T* dg,\n    T* db) {\n  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (j < N) {\n    T_ACC sum1 = 0;\n    T_ACC sum2 = 0;\n    for (int64_t i = 0; i < M; ++i) {\n      const int64_t index = i * N + j;\n      sum1 += dg == nullptr ? T_ACC(0)\n                            : static_cast<T_ACC>(dY[index]) *\n              (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n              static_cast<T_ACC>(rstd[i]);\n      sum2 += db == nullptr ? T_ACC(0) : static_cast<T_ACC>(dY[index]);\n    const T* __restrict__ dY,\n    const T* __restrict__ X,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T* __restrict__ dg,\n    T* __restrict__ db,\n    T_ACC &dg_sum,\n    T_ACC &db_sum\n) {\n  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n        M_start < M;\n        M_start += rows_per_block_y * gridDim.y) {\n    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n    T_ACC warp_mean = 0, warp_rstd = 0;\n    if (lane_id < rows_per_thread_y) {\n      warp_mean = mean[mean_index + lane_id];\n      warp_rstd = rstd[mean_index + lane_id];\n    }\n    if (dg != nullptr) {\n      dg[j] = sum1;\n    WARP_SYNC();\n    T_ACC dY_regs[rows_per_thread_y] = {0};\n    T_ACC X_regs[rows_per_thread_y] = {0};\n    #pragma unroll\n    for (int i = 0; i < rows_per_thread_y; ++i) {\n      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n      if (aligned_grid || (current_y < M && thread_x < N)) {\n        dY_regs[i] = dY[current_y * N + thread_x];\n        X_regs[i] = X[current_y * N + thread_x];\n      }\n    }\n    if (db != nullptr) {\n      db[j] = sum2;\n\n    #pragma unroll\n    for (int i = 0; i < rows_per_thread_y; ++i) {\n      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n      db_sum += dY_regs[i];\n    }\n  }\n}\n\n// This implementation gets called if M and N divide with 32. This case should\n// be the most common. We can then make better use of warp level intrinsics\n// to improve performance.\n\ntemplate <typename T, typename T_ACC>\n__global__ void GammaBetaBackwardCUDAKernel_32x32(\ntemplate <typename T, typename T_ACC,\nunsigned int block_dim_x,\nunsigned int block_dim_y,\nunsigned int rows_per_block_y,\nbool check_x,\nbool check_y>\n__device__\n__forceinline__\nvoid\nblockReduceGammaBetaBackwardsHelper(\n    int64_t M_start,\n    int64_t M,\n    int64_t N,\n    const T* dY,\n    const T* X,\n    const T_ACC* mean,\n    const T_ACC* rstd,\n    T* dg,\n    T* db) {\n  alignas(sizeof(double)) extern __shared__ char s_data1[];\n  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n  T_ACC* s_dg;\n  T_ACC* s_db;\n\n  T_ACC dg_sum = 0;\n  T_ACC db_sum = 0;\n\n  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n\n  if (j < N) {\n    constexpr int unroll_factor = 8;\n    int laneId = threadIdx.x & (C10_WARP_SIZE - 1);\n    const T* __restrict__ dY,\n    const T* __restrict__ X,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T* __restrict__ dg,\n    T* __restrict__ db,\n    T_ACC &dg_sum,\n    T_ACC &db_sum\n) {\n  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n\n    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n    T_ACC warp_mean = 0, warp_rstd = 0;\n    if (lane_id < rows_per_thread_y && mean_index + lane_id < M) {\n      warp_mean = mean[mean_index + lane_id];\n      warp_rstd = rstd[mean_index + lane_id];\n    }\n    WARP_SYNC();\n\n    T_ACC mean_reg, mean_reg_tmp;\n    T_ACC rstd_reg, rstd_reg_tmp;\n    T dY_reg;\n    T X_reg;\n\n    // Main loop\n    int bcounter;\n    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor);\n         bcounter++) {\n      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n\n      if (laneId < unroll_factor) {\n        mean_reg_tmp = mean[offset + laneId];\n        rstd_reg_tmp = rstd[offset + laneId];\n    T_ACC dY_regs[rows_per_thread_y] = {0};\n    T_ACC X_regs[rows_per_thread_y] = {0};\n    #pragma unroll\n    for (int i = 0; i < rows_per_thread_y; ++i) {\n      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n      bool active = true;\n      if (check_x && thread_x >= N) {\n        active = false;\n      }\n      WARP_SYNC();\n\n      #pragma unroll\n      for (int ii = 0; ii < unroll_factor; ++ii) {\n        dY_reg = dY[(offset + ii) * N + j];\n        X_reg = X[(offset + ii) * N + j];\n        mean_reg = WARP_SHFL(mean_reg_tmp, ii, kWarpSize);\n        rstd_reg = WARP_SHFL(rstd_reg_tmp, ii, kWarpSize);\n        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n        db_sum += dY_reg;\n      if (check_y && current_y >= M) {\n        active = false;\n      }\n    }\n\n    // Remainder loop\n    int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n    for (int ii = 0; ii < unroll_factor; ii++) {\n      if ((offset + ii) < M) {\n        mean_reg = mean[offset + ii];\n        rstd_reg = rstd[offset + ii];\n        dY_reg = dY[(offset + ii) * N + j];\n        X_reg = X[(offset + ii) * N + j];\n        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n        db_sum += dY_reg;\n      if (active) {\n        dY_regs[i] = dY[current_y * N + thread_x];\n        X_regs[i] = X[current_y * N + thread_x];\n      }\n    }\n\n    // This kernel uses a block of (C10_WARP_SIZE x C10_WARP_SIZE) and\n    // gets called when M; N divide by 32. We can use warp shuffles\n    // for the final reduction step. This removes 4 shmem loads and\n    // stores with their corresponding __syncthreads()\n\n    // This greatly reduces bank conflicts at the expense of a little\n    // extra shared memory. It does not impact occupancy\n    int padded_bx = (1 + blockDim.x);\n\n    s_dg = s_data_typed;\n    s_db = s_data_typed + (padded_bx * blockDim.y);\n    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n    __syncthreads();\n\n    // Load transposed so that a warp holds an entire column\n    T_ACC reg_dg = s_dg[threadIdx.x * padded_bx + threadIdx.y];\n    T_ACC reg_db = s_db[threadIdx.x * padded_bx + threadIdx.y];\n    for (unsigned delta = C10_WARP_SIZE >> 1; delta >= 1; delta >>= 1) {\n      reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n      reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n    #pragma unroll\n    for (int i = 0; i < rows_per_thread_y; ++i) {\n      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n      db_sum += dY_regs[i];\n    }\n}\n\n    if (threadIdx.x == 0) {\n      const int64_t j = blockIdx.x * blockDim.x + threadIdx.y;\n      if (dg) {\n        dg[j] = reg_dg;\n      }\n      if (db) {\n        db[j] = reg_db;\n      }\ntemplate <typename T, typename T_ACC,\nunsigned int block_dim_x,\nunsigned int block_dim_y,\nunsigned int rows_per_block_y,\nbool check_x,\nbool check_y>\n__device__\n__forceinline__\nvoid\nblockReduceGammaBetaBackwardsWithChecks(\n    int64_t M,\n    int64_t N,\n    const T* __restrict__ dY,\n    const T* __restrict__ X,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T* __restrict__ dg,\n    T* __restrict__ db,\n    T_ACC &dg_sum,\n    T_ACC &db_sum\n) {\n  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n        M_start < M;\n        M_start += rows_per_block_y * gridDim.y) {\n    int64_t M_end = M_start + rows_per_block_y - 1;\n    if (M_end < M) {\n      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, false>\n      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    } else {\n      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, true>\n      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    }\n  }\n}\n\ntemplate <typename T, typename T_ACC>\n__global__ void GammaBetaBackwardCUDAKernel(\n// block_dim_x is the number of threads in the x dimension per block.\n// block_dim_y is the number of threads in the y dimension per block.\n// rows_per_block_y is the size of the tile (number of data elements)\n// in the y dimension per block.\n// partial_reduction indicates whether we need to reduce across threads\n// or not. If set to true, we will not reduce across threads. This can\n// be faster in the M >> N case but requires another kernel to do a full\n// final reduction.\n// aligned_grid means the data size is a multiple of tile size. In that\n// case we don't need to check for boundary conditions which can provide\n// a further speedup by not needing instructions to check for edge cases\n// and not needing predicate registers.\ntemplate <typename T, typename T_ACC,\nunsigned int block_dim_x, unsigned int block_dim_y,\nunsigned int rows_per_block_y,\nbool partial_reduction,\nbool aligned_grid\n>\n__global__\nvoid\n GammaBetaBackwardCUDAKernelTemplate(\n    int64_t M,\n    int64_t N,\n    const T* dY,\n    const T* X,\n    const T_ACC* mean,\n    const T_ACC* rstd,\n    T* dg,\n    T* db) {\n  alignas(sizeof(double)) extern __shared__ char s_data1[];\n  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n  T_ACC* s_dg;\n  T_ACC* s_db;\n\n  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n    const T* __restrict__ dY,\n    const T* __restrict__ X,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T* __restrict__ dg,\n    T* __restrict__ db) {\n  // This assert is a compile-time check only.\n  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n  static_assert(rows_per_thread_y <= kWarpSize);\n\n  T_ACC dg_sum = 0;\n  T_ACC db_sum = 0;\n\n  if (j < N) {\n    constexpr int unroll_factor = 8;\n\n    T_ACC mean_reg;\n    T_ACC rstd_reg;\n    T dY_reg;\n    T X_reg;\n  if (aligned_grid) {\n    // When N and M align perfectly with block_dim_x and block_dim_y, we\n    // can skip boundary condition checks that waste instruction issue slots.\n    blockReduceGammaBetaBackwardsAligned\n        <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>\n        (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n  } else {\n    // In the general case we need to check boundary conditions in the M\n    // dimension. However, we can still avoid boundary checks in the N dimension\n    // for the inner blocks. So try to avoid those checks when possible.\n    if (blockIdx.x * block_dim_x + block_dim_x - 1 < N) {\n      blockReduceGammaBetaBackwardsWithChecks\n          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, true>\n          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    } else {\n      blockReduceGammaBetaBackwardsWithChecks\n          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true, true>\n          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    }\n  }\n\n    // Main Loop\n    int bcounter;\n    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor); bcounter++){\n      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n\n      #pragma unroll\n      for (int ii = 0; ii < unroll_factor; ++ii) {\n        dY_reg = dY[(offset + ii) * N + j];\n        X_reg = X[(offset + ii) * N + j];\n        mean_reg = mean[offset + ii];\n        rstd_reg = rstd[offset + ii];\n        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n        db_sum += dY_reg;\n  // When partial_reduction is requested, we don't reduce within a block.\n  // We also don't reduce if we are only a single block in the y dimension.\n  if (partial_reduction || (blockDim.y == 1 && gridDim.y == 1)) {\n    if (aligned_grid || thread_x < N) {\n      int64_t thread_y = blockIdx.y * blockDim.y + threadIdx.y;\n      if (dg) {\n        dg[thread_y * N + thread_x] = dg_sum;\n      }\n    }\n\n    // Remainder loop\n    int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n    for (int ii = 0; ii < unroll_factor; ii++ ){\n      if ((offset + ii) < M) {\n        dY_reg = dY[(offset + ii) * N + j ];\n        X_reg = X[(offset + ii) * N + j];\n        mean_reg = mean[offset + ii];\n        rstd_reg = rstd[offset + ii];\n        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n        db_sum += dY_reg;\n      if (db) {\n        db[thread_y * N + thread_x] = db_sum;\n      }\n    }\n\n    // Do the final reduction in shared memory\n  } else {\n    // The caller requested a full reduction so we must reduce across\n    // warps using shared memory and warp shuffles.\n    static_assert(rows_per_thread_y <= C10_WARP_SIZE);\n    alignas(sizeof(double)) extern __shared__ char s_data1[];\n    T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n    T_ACC* s_dg;\n    T_ACC* s_db;\n    int padded_bx = (block_dim_x + 1);\n    // Transpose dg and db.\n    s_dg = s_data_typed;\n    s_db = s_data_typed + blockDim.x * blockDim.y;\n    s_dg[threadIdx.y * blockDim.x + threadIdx.x] = dg_sum;\n    s_db[threadIdx.y * blockDim.x + threadIdx.x] = db_sum;\n    s_db = s_data_typed + (padded_bx * block_dim_y);\n    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n    __syncthreads();\n\n    for (int offset = blockDim.y / 2; offset >= 1; offset /= 2) {\n      if (threadIdx.y < offset) {\n        s_dg[threadIdx.y * blockDim.x + threadIdx.x] +=\n            s_dg[(threadIdx.y + offset) * blockDim.x + threadIdx.x];\n        s_db[threadIdx.y * blockDim.x + threadIdx.x] +=\n            s_db[(threadIdx.y + offset) * blockDim.x + threadIdx.x];\n    // Load transposed so that a warp holds an entire column\n    // Because block_dim_x != block_dim_y in the general case, we need\n    // some code to handle the general case.\n    static_assert(block_dim_x * block_dim_y % C10_WARP_SIZE == 0);\n    constexpr int warps_available_to_reduce = block_dim_x * block_dim_y / C10_WARP_SIZE;\n    int thread_id = threadIdx.y * block_dim_x + threadIdx.x;\n    int warp_id = thread_id / C10_WARP_SIZE;\n    int lane_id = thread_id & (C10_WARP_SIZE - 1);\n    #pragma unroll\n    for (int i = warp_id; i < block_dim_x; i += warps_available_to_reduce) {\n      T_ACC reg_db, reg_dg;\n      if (lane_id < block_dim_y) {\n        reg_dg = s_dg[lane_id * padded_bx + i];\n        reg_db = s_db[lane_id * padded_bx + i];\n      }\n      #pragma unroll\n      for (unsigned delta = block_dim_y >> 1; delta >= 1; delta >>= 1) {\n        reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n        reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n      }\n      // Reduce is done. Now write it out to global memory.\n      int64_t out_index = blockIdx.x * block_dim_x + i;\n      if (threadIdx.x == 0 && (aligned_grid || out_index < N)) {\n        if (dg) {\n          dg[out_index] = reg_dg;\n        }\n      __syncthreads();\n        if (db) {\n          db[out_index] = reg_db;\n        }\n      }\n    }\n  }\n}\n\n    if (threadIdx.y == 0) {\n      if (dg) {\n        dg[j] = s_dg[threadIdx.x];\n      }\n      if (db) {\n        db[j] = s_db[threadIdx.x];\n      }\ntemplate<typename T, typename T_ACC,\nint block_dim_x, int block_dim_y,\nint rows_per_block_y,\nbool partial_reduction>\nvoid LaunchAndCheckGammaBetaBackwardKernel(\n  bool aligned_grid,\n  dim3 blocks,\n  dim3 threads,\n  size_t shmem_sz,\n  cudaStream_t cuda_stream,\n  const T* dY_data,\n  const T* X_data,\n  const T_ACC* mean_data,\n  const T_ACC* rstd_data,\n  int64_t M,\n  int64_t N,\n  T* dgamma_data,\n  T* dbeta_data) {\nif (aligned_grid) {\n    GammaBetaBackwardCUDAKernelTemplate<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, partial_reduction, true>\n        <<<blocks, threads, shmem_sz, cuda_stream>>>(\n            M,\n            N,\n            dY_data,\n            X_data,\n            mean_data,\n            rstd_data,\n            dgamma_data,\n            dbeta_data);\n  } else {\n    GammaBetaBackwardCUDAKernelTemplate<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, partial_reduction, false>\n        <<<blocks, threads, shmem_sz, cuda_stream>>>(\n            M,\n            N,\n            dY_data,\n            X_data,\n            mean_data,\n            rstd_data,\n            dgamma_data,\n            dbeta_data);\n  }\n  C10_CUDA_KERNEL_LAUNCH_CHECK();\n}\n\ntemplate<typename T, typename T_ACC,\nint block_dim_x, int block_dim_y,\nint rows_per_block_y>\nvoid ConfigureAndLaunchGammaBetaBackwardKernel(\n    const T* dY_data,\n    const T* X_data,\n    const T_ACC* mean_data,\n    const T_ACC* rstd_data,\n    int64_t M,\n    int64_t N,\n    Tensor* dgamma,\n    Tensor* dbeta,\n    cudaStream_t cuda_stream) {\n  T* dgamma_data =\n    dgamma->defined() ? dgamma->template data_ptr<T>() : nullptr;\n  T* dbeta_data = dbeta->defined() ? dbeta->template data_ptr<T>() : nullptr;\n  bool aligned_grid = (M % rows_per_block_y == 0) && (N % block_dim_x == 0);\n  dim3 threads{block_dim_x, block_dim_y};\n  dim3 blocks;\n  blocks.x = (N + block_dim_x - 1) / block_dim_x;\n  blocks.y = 1;\n  size_t shmem_sz = (block_dim_x + 1) * block_dim_y * sizeof(T_ACC) * 2;\n  if (blocks.y == 1 && threads.y == 1) {\n    // Optimization: since there is just one thread doing all the summation, we don't need a reduction\n    // across threads. So we set partial_reduction to true.\n    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>(\n      aligned_grid, blocks, threads, shmem_sz, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_data, dbeta_data);\n  } else {\n    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false>(\n      aligned_grid, blocks, threads, shmem_sz, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_data, dbeta_data);\n  }\n\n}\n\ntemplate<typename T, typename T_ACC>\nvoid LaunchGammaBetaBackwardCUDAKernel(\n    const T* dY_data,\n    const T* X_data,\n    const T_ACC* mean_data,\n    const T_ACC* rstd_data,\n    int64_t M,\n    int64_t N,\n    Tensor* dgamma,\n    Tensor* dbeta,\n    cudaStream_t cuda_stream) {\n  if (M > 64 * 1024 && N / kWarpSize < 64) {\n    // We have a situation where M >> N and N is small.\n    // In this case we can speed up the computation by parallelizing in the M dimension.\n    // We launch multiple blocks in the y-dimension, and compute partial sums for the\n    // gradient in the first pass. Then we do a .sum(0) to do a final reduction.\n    // Although we launch 2 kernels, we can get up to a 10x speedup for large M.\n    constexpr int block_dim_x = 32;\n    constexpr int block_dim_y = 1;\n    constexpr int rows_per_block_y = 32;\n    bool aligned_grid = (M % rows_per_block_y == 0) && (N % block_dim_x == 0);\n    dim3 threads{block_dim_x, block_dim_y};\n    dim3 blocks;\n    blocks.x = (N + block_dim_x - 1) / block_dim_x;\n    // int rows_per_block = my_gamma_beta_unroll_factor *\n    blocks.y = (M + rows_per_block_y - 1) / rows_per_block_y;\n    constexpr int max_grid_size = 64 * 1024 / 2;\n    blocks.y = std::min<unsigned int>(max_grid_size / blocks.x, blocks.y);\n    auto options = dgamma->options();\n    Tensor dgamma_blocks;\n    Tensor dbeta_blocks;\n    T * dgamma_blocks_ptr = nullptr;\n    T * dbeta_blocks_ptr = nullptr;\n    if (dgamma->defined()) {\n      dgamma_blocks = at::zeros({blocks.y * threads.y, dgamma->size(-1)}, options);\n      dgamma_blocks_ptr = dgamma_blocks.data_ptr<T>();\n    }\n    if (dbeta->defined()) {\n      dbeta_blocks = at::zeros({blocks.y * threads.y, dgamma->size(-1)}, options);\n      dbeta_blocks_ptr = dbeta_blocks.data_ptr<T>();\n    }\n    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>(\n      aligned_grid, blocks, threads, 0, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_blocks_ptr, dbeta_blocks_ptr);\n\n    *dgamma = dgamma_blocks.sum(0);\n    *dbeta = dbeta_blocks.sum(0);\n  } else {\n    // We are in the normal case where M is not that large.\n    // We can change the tile shape (which is the last template parameter) in accordance with M.\n    // For small M it is faster to have a smaller tile, otherwise we could have idle threads.\n    // For larger M we use a bigger tile size.\n    if (M < 64) {",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1996412678",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 148605,
        "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
        "discussion_id": "1996412678",
        "commented_code": "@@ -508,223 +509,418 @@ __global__ void layer_norm_grad_input_kernel_vectorized(\n   }\n }\n \n-\n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardSimpleCUDAKernel(\n+// When the data size is a multiple of the tile size we can use fewer\n+// instructions and registers. Example, this is the case when M=256 N=256.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool aligned_grid>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsAligned(\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-  if (j < N) {\n-    T_ACC sum1 = 0;\n-    T_ACC sum2 = 0;\n-    for (int64_t i = 0; i < M; ++i) {\n-      const int64_t index = i * N + j;\n-      sum1 += dg == nullptr ? T_ACC(0)\n-                            : static_cast<T_ACC>(dY[index]) *\n-              (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n-              static_cast<T_ACC>(rstd[i]);\n-      sum2 += db == nullptr ? T_ACC(0) : static_cast<T_ACC>(dY[index]);\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n+  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n+        M_start < M;\n+        M_start += rows_per_block_y * gridDim.y) {\n+    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n+    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n+    T_ACC warp_mean = 0, warp_rstd = 0;\n+    if (lane_id < rows_per_thread_y) {\n+      warp_mean = mean[mean_index + lane_id];\n+      warp_rstd = rstd[mean_index + lane_id];\n     }\n-    if (dg != nullptr) {\n-      dg[j] = sum1;\n+    WARP_SYNC();\n+    T_ACC dY_regs[rows_per_thread_y] = {0};\n+    T_ACC X_regs[rows_per_thread_y] = {0};\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n+      if (aligned_grid || (current_y < M && thread_x < N)) {\n+        dY_regs[i] = dY[current_y * N + thread_x];\n+        X_regs[i] = X[current_y * N + thread_x];\n+      }\n     }\n-    if (db != nullptr) {\n-      db[j] = sum2;\n+\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n+      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n+      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n+      db_sum += dY_regs[i];\n     }\n   }\n }\n \n-// This implementation gets called if M and N divide with 32. This case should\n-// be the most common. We can then make better use of warp level intrinsics\n-// to improve performance.\n-\n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardCUDAKernel_32x32(\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsHelper(\n+    int64_t M_start,\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  alignas(sizeof(double)) extern __shared__ char s_data1[];\n-  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n-  T_ACC* s_dg;\n-  T_ACC* s_db;\n-\n-  T_ACC dg_sum = 0;\n-  T_ACC db_sum = 0;\n-\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-\n-  if (j < N) {\n-    constexpr int unroll_factor = 8;\n-    int laneId = threadIdx.x & (C10_WARP_SIZE - 1);\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n+\n+    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n+    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n+    T_ACC warp_mean = 0, warp_rstd = 0;\n+    if (lane_id < rows_per_thread_y && mean_index + lane_id < M) {\n+      warp_mean = mean[mean_index + lane_id];\n+      warp_rstd = rstd[mean_index + lane_id];\n+    }\n+    WARP_SYNC();\n \n-    T_ACC mean_reg, mean_reg_tmp;\n-    T_ACC rstd_reg, rstd_reg_tmp;\n-    T dY_reg;\n-    T X_reg;\n \n-    // Main loop\n-    int bcounter;\n-    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor);\n-         bcounter++) {\n-      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n \n-      if (laneId < unroll_factor) {\n-        mean_reg_tmp = mean[offset + laneId];\n-        rstd_reg_tmp = rstd[offset + laneId];\n+    T_ACC dY_regs[rows_per_thread_y] = {0};\n+    T_ACC X_regs[rows_per_thread_y] = {0};\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n+      bool active = true;\n+      if (check_x && thread_x >= N) {\n+        active = false;\n       }\n-      WARP_SYNC();\n-\n-      #pragma unroll\n-      for (int ii = 0; ii < unroll_factor; ++ii) {\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = WARP_SHFL(mean_reg_tmp, ii, kWarpSize);\n-        rstd_reg = WARP_SHFL(rstd_reg_tmp, ii, kWarpSize);\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (check_y && current_y >= M) {\n+        active = false;\n       }\n-    }\n-\n-    // Remainder loop\n-    int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n-    for (int ii = 0; ii < unroll_factor; ii++) {\n-      if ((offset + ii) < M) {\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (active) {\n+        dY_regs[i] = dY[current_y * N + thread_x];\n+        X_regs[i] = X[current_y * N + thread_x];\n       }\n     }\n \n-    // This kernel uses a block of (C10_WARP_SIZE x C10_WARP_SIZE) and\n-    // gets called when M; N divide by 32. We can use warp shuffles\n-    // for the final reduction step. This removes 4 shmem loads and\n-    // stores with their corresponding __syncthreads()\n-\n-    // This greatly reduces bank conflicts at the expense of a little\n-    // extra shared memory. It does not impact occupancy\n-    int padded_bx = (1 + blockDim.x);\n-\n-    s_dg = s_data_typed;\n-    s_db = s_data_typed + (padded_bx * blockDim.y);\n-    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n-    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n-    __syncthreads();\n-\n-    // Load transposed so that a warp holds an entire column\n-    T_ACC reg_dg = s_dg[threadIdx.x * padded_bx + threadIdx.y];\n-    T_ACC reg_db = s_db[threadIdx.x * padded_bx + threadIdx.y];\n-    for (unsigned delta = C10_WARP_SIZE >> 1; delta >= 1; delta >>= 1) {\n-      reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n-      reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n+      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n+      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n+      db_sum += dY_regs[i];\n     }\n+}\n \n-    if (threadIdx.x == 0) {\n-      const int64_t j = blockIdx.x * blockDim.x + threadIdx.y;\n-      if (dg) {\n-        dg[j] = reg_dg;\n-      }\n-      if (db) {\n-        db[j] = reg_db;\n-      }\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsWithChecks(\n+    int64_t M,\n+    int64_t N,\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n+        M_start < M;\n+        M_start += rows_per_block_y * gridDim.y) {\n+    int64_t M_end = M_start + rows_per_block_y - 1;\n+    if (M_end < M) {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, false>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, true>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n     }\n   }\n }\n \n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardCUDAKernel(\n+// block_dim_x is the number of threads in the x dimension per block.\n+// block_dim_y is the number of threads in the y dimension per block.\n+// rows_per_block_y is the size of the tile (number of data elements)\n+// in the y dimension per block.\n+// partial_reduction indicates whether we need to reduce across threads\n+// or not. If set to true, we will not reduce across threads. This can\n+// be faster in the M >> N case but requires another kernel to do a full\n+// final reduction.\n+// aligned_grid means the data size is a multiple of tile size. In that\n+// case we don't need to check for boundary conditions which can provide\n+// a further speedup by not needing instructions to check for edge cases\n+// and not needing predicate registers.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x, unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool partial_reduction,\n+bool aligned_grid\n+>\n+__global__\n+void\n+ GammaBetaBackwardCUDAKernelTemplate(\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  alignas(sizeof(double)) extern __shared__ char s_data1[];\n-  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n-  T_ACC* s_dg;\n-  T_ACC* s_db;\n-\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db) {\n+  // This assert is a compile-time check only.\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  static_assert(rows_per_thread_y <= kWarpSize);\n \n   T_ACC dg_sum = 0;\n   T_ACC db_sum = 0;\n \n-  if (j < N) {\n-    constexpr int unroll_factor = 8;\n-\n-    T_ACC mean_reg;\n-    T_ACC rstd_reg;\n-    T dY_reg;\n-    T X_reg;\n+  if (aligned_grid) {\n+    // When N and M align perfectly with block_dim_x and block_dim_y, we\n+    // can skip boundary condition checks that waste instruction issue slots.\n+    blockReduceGammaBetaBackwardsAligned\n+        <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>\n+        (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+  } else {\n+    // In the general case we need to check boundary conditions in the M\n+    // dimension. However, we can still avoid boundary checks in the N dimension\n+    // for the inner blocks. So try to avoid those checks when possible.\n+    if (blockIdx.x * block_dim_x + block_dim_x - 1 < N) {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    }\n+  }\n \n-    // Main Loop\n-    int bcounter;\n-    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor); bcounter++){\n-      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n \n-      #pragma unroll\n-      for (int ii = 0; ii < unroll_factor; ++ii) {\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+  // When partial_reduction is requested, we don't reduce within a block.\n+  // We also don't reduce if we are only a single block in the y dimension.\n+  if (partial_reduction || (blockDim.y == 1 && gridDim.y == 1)) {\n+    if (aligned_grid || thread_x < N) {\n+      int64_t thread_y = blockIdx.y * blockDim.y + threadIdx.y;\n+      if (dg) {\n+        dg[thread_y * N + thread_x] = dg_sum;\n       }\n-    }\n-\n-    // Remainder loop\n-    int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n-    for (int ii = 0; ii < unroll_factor; ii++ ){\n-      if ((offset + ii) < M) {\n-        dY_reg = dY[(offset + ii) * N + j ];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (db) {\n+        db[thread_y * N + thread_x] = db_sum;\n       }\n     }\n-\n-    // Do the final reduction in shared memory\n+  } else {\n+    // The caller requested a full reduction so we must reduce across\n+    // warps using shared memory and warp shuffles.\n+    static_assert(rows_per_thread_y <= C10_WARP_SIZE);\n+    alignas(sizeof(double)) extern __shared__ char s_data1[];\n+    T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n+    T_ACC* s_dg;\n+    T_ACC* s_db;\n+    int padded_bx = (block_dim_x + 1);\n+    // Transpose dg and db.\n     s_dg = s_data_typed;\n-    s_db = s_data_typed + blockDim.x * blockDim.y;\n-    s_dg[threadIdx.y * blockDim.x + threadIdx.x] = dg_sum;\n-    s_db[threadIdx.y * blockDim.x + threadIdx.x] = db_sum;\n+    s_db = s_data_typed + (padded_bx * block_dim_y);\n+    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n+    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n     __syncthreads();\n \n-    for (int offset = blockDim.y / 2; offset >= 1; offset /= 2) {\n-      if (threadIdx.y < offset) {\n-        s_dg[threadIdx.y * blockDim.x + threadIdx.x] +=\n-            s_dg[(threadIdx.y + offset) * blockDim.x + threadIdx.x];\n-        s_db[threadIdx.y * blockDim.x + threadIdx.x] +=\n-            s_db[(threadIdx.y + offset) * blockDim.x + threadIdx.x];\n+    // Load transposed so that a warp holds an entire column\n+    // Because block_dim_x != block_dim_y in the general case, we need\n+    // some code to handle the general case.\n+    static_assert(block_dim_x * block_dim_y % C10_WARP_SIZE == 0);\n+    constexpr int warps_available_to_reduce = block_dim_x * block_dim_y / C10_WARP_SIZE;\n+    int thread_id = threadIdx.y * block_dim_x + threadIdx.x;\n+    int warp_id = thread_id / C10_WARP_SIZE;\n+    int lane_id = thread_id & (C10_WARP_SIZE - 1);\n+    #pragma unroll\n+    for (int i = warp_id; i < block_dim_x; i += warps_available_to_reduce) {\n+      T_ACC reg_db, reg_dg;\n+      if (lane_id < block_dim_y) {\n+        reg_dg = s_dg[lane_id * padded_bx + i];\n+        reg_db = s_db[lane_id * padded_bx + i];\n+      }\n+      #pragma unroll\n+      for (unsigned delta = block_dim_y >> 1; delta >= 1; delta >>= 1) {\n+        reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n+        reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n+      }\n+      // Reduce is done. Now write it out to global memory.\n+      int64_t out_index = blockIdx.x * block_dim_x + i;\n+      if (threadIdx.x == 0 && (aligned_grid || out_index < N)) {\n+        if (dg) {\n+          dg[out_index] = reg_dg;\n         }\n-      __syncthreads();\n+        if (db) {\n+          db[out_index] = reg_db;\n+        }\n+      }\n     }\n+  }\n+}\n \n-    if (threadIdx.y == 0) {\n-      if (dg) {\n-        dg[j] = s_dg[threadIdx.x];\n-      }\n-      if (db) {\n-        db[j] = s_db[threadIdx.x];\n-      }\n+template<typename T, typename T_ACC,\n+int block_dim_x, int block_dim_y,\n+int rows_per_block_y,\n+bool partial_reduction>\n+void LaunchAndCheckGammaBetaBackwardKernel(\n+  bool aligned_grid,\n+  dim3 blocks,\n+  dim3 threads,\n+  size_t shmem_sz,\n+  cudaStream_t cuda_stream,\n+  const T* dY_data,\n+  const T* X_data,\n+  const T_ACC* mean_data,\n+  const T_ACC* rstd_data,\n+  int64_t M,\n+  int64_t N,\n+  T* dgamma_data,\n+  T* dbeta_data) {\n+if (aligned_grid) {\n+    GammaBetaBackwardCUDAKernelTemplate<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, partial_reduction, true>\n+        <<<blocks, threads, shmem_sz, cuda_stream>>>(\n+            M,\n+            N,\n+            dY_data,\n+            X_data,\n+            mean_data,\n+            rstd_data,\n+            dgamma_data,\n+            dbeta_data);\n+  } else {\n+    GammaBetaBackwardCUDAKernelTemplate<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, partial_reduction, false>\n+        <<<blocks, threads, shmem_sz, cuda_stream>>>(\n+            M,\n+            N,\n+            dY_data,\n+            X_data,\n+            mean_data,\n+            rstd_data,\n+            dgamma_data,\n+            dbeta_data);\n+  }\n+  C10_CUDA_KERNEL_LAUNCH_CHECK();\n+}\n+\n+template<typename T, typename T_ACC,\n+int block_dim_x, int block_dim_y,\n+int rows_per_block_y>\n+void ConfigureAndLaunchGammaBetaBackwardKernel(\n+    const T* dY_data,\n+    const T* X_data,\n+    const T_ACC* mean_data,\n+    const T_ACC* rstd_data,\n+    int64_t M,\n+    int64_t N,\n+    Tensor* dgamma,\n+    Tensor* dbeta,\n+    cudaStream_t cuda_stream) {\n+  T* dgamma_data =\n+    dgamma->defined() ? dgamma->template data_ptr<T>() : nullptr;\n+  T* dbeta_data = dbeta->defined() ? dbeta->template data_ptr<T>() : nullptr;\n+  bool aligned_grid = (M % rows_per_block_y == 0) && (N % block_dim_x == 0);\n+  dim3 threads{block_dim_x, block_dim_y};\n+  dim3 blocks;\n+  blocks.x = (N + block_dim_x - 1) / block_dim_x;\n+  blocks.y = 1;\n+  size_t shmem_sz = (block_dim_x + 1) * block_dim_y * sizeof(T_ACC) * 2;\n+  if (blocks.y == 1 && threads.y == 1) {\n+    // Optimization: since there is just one thread doing all the summation, we don't need a reduction\n+    // across threads. So we set partial_reduction to true.\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>(\n+      aligned_grid, blocks, threads, shmem_sz, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_data, dbeta_data);\n+  } else {\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false>(\n+      aligned_grid, blocks, threads, shmem_sz, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_data, dbeta_data);\n+  }\n+\n+}\n+\n+template<typename T, typename T_ACC>\n+void LaunchGammaBetaBackwardCUDAKernel(\n+    const T* dY_data,\n+    const T* X_data,\n+    const T_ACC* mean_data,\n+    const T_ACC* rstd_data,\n+    int64_t M,\n+    int64_t N,\n+    Tensor* dgamma,\n+    Tensor* dbeta,\n+    cudaStream_t cuda_stream) {\n+  if (M > 64 * 1024 && N / kWarpSize < 64) {\n+    // We have a situation where M >> N and N is small.\n+    // In this case we can speed up the computation by parallelizing in the M dimension.\n+    // We launch multiple blocks in the y-dimension, and compute partial sums for the\n+    // gradient in the first pass. Then we do a .sum(0) to do a final reduction.\n+    // Although we launch 2 kernels, we can get up to a 10x speedup for large M.\n+    constexpr int block_dim_x = 32;\n+    constexpr int block_dim_y = 1;\n+    constexpr int rows_per_block_y = 32;\n+    bool aligned_grid = (M % rows_per_block_y == 0) && (N % block_dim_x == 0);\n+    dim3 threads{block_dim_x, block_dim_y};\n+    dim3 blocks;\n+    blocks.x = (N + block_dim_x - 1) / block_dim_x;\n+    // int rows_per_block = my_gamma_beta_unroll_factor *\n+    blocks.y = (M + rows_per_block_y - 1) / rows_per_block_y;\n+    constexpr int max_grid_size = 64 * 1024 / 2;\n+    blocks.y = std::min<unsigned int>(max_grid_size / blocks.x, blocks.y);\n+    auto options = dgamma->options();\n+    Tensor dgamma_blocks;\n+    Tensor dbeta_blocks;\n+    T * dgamma_blocks_ptr = nullptr;\n+    T * dbeta_blocks_ptr = nullptr;\n+    if (dgamma->defined()) {\n+      dgamma_blocks = at::zeros({blocks.y * threads.y, dgamma->size(-1)}, options);\n+      dgamma_blocks_ptr = dgamma_blocks.data_ptr<T>();\n+    }\n+    if (dbeta->defined()) {\n+      dbeta_blocks = at::zeros({blocks.y * threads.y, dgamma->size(-1)}, options);\n+      dbeta_blocks_ptr = dbeta_blocks.data_ptr<T>();\n+    }\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>(\n+      aligned_grid, blocks, threads, 0, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_blocks_ptr, dbeta_blocks_ptr);\n+\n+    *dgamma = dgamma_blocks.sum(0);\n+    *dbeta = dbeta_blocks.sum(0);\n+  } else {\n+    // We are in the normal case where M is not that large.\n+    // We can change the tile shape (which is the last template parameter) in accordance with M.\n+    // For small M it is faster to have a smaller tile, otherwise we could have idle threads.\n+    // For larger M we use a bigger tile size.\n+    if (M < 64) {",
        "comment_created_at": "2025-03-14T23:11:11+00:00",
        "comment_author": "ngimel",
        "comment_body": "do we need all 4 instantiations here? How much is the perf impact?",
        "pr_file_module": null
      },
      {
        "comment_id": "2000093553",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 148605,
        "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
        "discussion_id": "1996412678",
        "commented_code": "@@ -508,223 +509,418 @@ __global__ void layer_norm_grad_input_kernel_vectorized(\n   }\n }\n \n-\n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardSimpleCUDAKernel(\n+// When the data size is a multiple of the tile size we can use fewer\n+// instructions and registers. Example, this is the case when M=256 N=256.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool aligned_grid>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsAligned(\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-  if (j < N) {\n-    T_ACC sum1 = 0;\n-    T_ACC sum2 = 0;\n-    for (int64_t i = 0; i < M; ++i) {\n-      const int64_t index = i * N + j;\n-      sum1 += dg == nullptr ? T_ACC(0)\n-                            : static_cast<T_ACC>(dY[index]) *\n-              (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n-              static_cast<T_ACC>(rstd[i]);\n-      sum2 += db == nullptr ? T_ACC(0) : static_cast<T_ACC>(dY[index]);\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n+  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n+        M_start < M;\n+        M_start += rows_per_block_y * gridDim.y) {\n+    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n+    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n+    T_ACC warp_mean = 0, warp_rstd = 0;\n+    if (lane_id < rows_per_thread_y) {\n+      warp_mean = mean[mean_index + lane_id];\n+      warp_rstd = rstd[mean_index + lane_id];\n     }\n-    if (dg != nullptr) {\n-      dg[j] = sum1;\n+    WARP_SYNC();\n+    T_ACC dY_regs[rows_per_thread_y] = {0};\n+    T_ACC X_regs[rows_per_thread_y] = {0};\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n+      if (aligned_grid || (current_y < M && thread_x < N)) {\n+        dY_regs[i] = dY[current_y * N + thread_x];\n+        X_regs[i] = X[current_y * N + thread_x];\n+      }\n     }\n-    if (db != nullptr) {\n-      db[j] = sum2;\n+\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n+      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n+      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n+      db_sum += dY_regs[i];\n     }\n   }\n }\n \n-// This implementation gets called if M and N divide with 32. This case should\n-// be the most common. We can then make better use of warp level intrinsics\n-// to improve performance.\n-\n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardCUDAKernel_32x32(\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsHelper(\n+    int64_t M_start,\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  alignas(sizeof(double)) extern __shared__ char s_data1[];\n-  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n-  T_ACC* s_dg;\n-  T_ACC* s_db;\n-\n-  T_ACC dg_sum = 0;\n-  T_ACC db_sum = 0;\n-\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-\n-  if (j < N) {\n-    constexpr int unroll_factor = 8;\n-    int laneId = threadIdx.x & (C10_WARP_SIZE - 1);\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n+\n+    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n+    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n+    T_ACC warp_mean = 0, warp_rstd = 0;\n+    if (lane_id < rows_per_thread_y && mean_index + lane_id < M) {\n+      warp_mean = mean[mean_index + lane_id];\n+      warp_rstd = rstd[mean_index + lane_id];\n+    }\n+    WARP_SYNC();\n \n-    T_ACC mean_reg, mean_reg_tmp;\n-    T_ACC rstd_reg, rstd_reg_tmp;\n-    T dY_reg;\n-    T X_reg;\n \n-    // Main loop\n-    int bcounter;\n-    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor);\n-         bcounter++) {\n-      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n \n-      if (laneId < unroll_factor) {\n-        mean_reg_tmp = mean[offset + laneId];\n-        rstd_reg_tmp = rstd[offset + laneId];\n+    T_ACC dY_regs[rows_per_thread_y] = {0};\n+    T_ACC X_regs[rows_per_thread_y] = {0};\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n+      bool active = true;\n+      if (check_x && thread_x >= N) {\n+        active = false;\n       }\n-      WARP_SYNC();\n-\n-      #pragma unroll\n-      for (int ii = 0; ii < unroll_factor; ++ii) {\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = WARP_SHFL(mean_reg_tmp, ii, kWarpSize);\n-        rstd_reg = WARP_SHFL(rstd_reg_tmp, ii, kWarpSize);\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (check_y && current_y >= M) {\n+        active = false;\n       }\n-    }\n-\n-    // Remainder loop\n-    int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n-    for (int ii = 0; ii < unroll_factor; ii++) {\n-      if ((offset + ii) < M) {\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (active) {\n+        dY_regs[i] = dY[current_y * N + thread_x];\n+        X_regs[i] = X[current_y * N + thread_x];\n       }\n     }\n \n-    // This kernel uses a block of (C10_WARP_SIZE x C10_WARP_SIZE) and\n-    // gets called when M; N divide by 32. We can use warp shuffles\n-    // for the final reduction step. This removes 4 shmem loads and\n-    // stores with their corresponding __syncthreads()\n-\n-    // This greatly reduces bank conflicts at the expense of a little\n-    // extra shared memory. It does not impact occupancy\n-    int padded_bx = (1 + blockDim.x);\n-\n-    s_dg = s_data_typed;\n-    s_db = s_data_typed + (padded_bx * blockDim.y);\n-    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n-    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n-    __syncthreads();\n-\n-    // Load transposed so that a warp holds an entire column\n-    T_ACC reg_dg = s_dg[threadIdx.x * padded_bx + threadIdx.y];\n-    T_ACC reg_db = s_db[threadIdx.x * padded_bx + threadIdx.y];\n-    for (unsigned delta = C10_WARP_SIZE >> 1; delta >= 1; delta >>= 1) {\n-      reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n-      reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n+      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n+      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n+      db_sum += dY_regs[i];\n     }\n+}\n \n-    if (threadIdx.x == 0) {\n-      const int64_t j = blockIdx.x * blockDim.x + threadIdx.y;\n-      if (dg) {\n-        dg[j] = reg_dg;\n-      }\n-      if (db) {\n-        db[j] = reg_db;\n-      }\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsWithChecks(\n+    int64_t M,\n+    int64_t N,\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n+        M_start < M;\n+        M_start += rows_per_block_y * gridDim.y) {\n+    int64_t M_end = M_start + rows_per_block_y - 1;\n+    if (M_end < M) {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, false>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, true>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n     }\n   }\n }\n \n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardCUDAKernel(\n+// block_dim_x is the number of threads in the x dimension per block.\n+// block_dim_y is the number of threads in the y dimension per block.\n+// rows_per_block_y is the size of the tile (number of data elements)\n+// in the y dimension per block.\n+// partial_reduction indicates whether we need to reduce across threads\n+// or not. If set to true, we will not reduce across threads. This can\n+// be faster in the M >> N case but requires another kernel to do a full\n+// final reduction.\n+// aligned_grid means the data size is a multiple of tile size. In that\n+// case we don't need to check for boundary conditions which can provide\n+// a further speedup by not needing instructions to check for edge cases\n+// and not needing predicate registers.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x, unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool partial_reduction,\n+bool aligned_grid\n+>\n+__global__\n+void\n+ GammaBetaBackwardCUDAKernelTemplate(\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  alignas(sizeof(double)) extern __shared__ char s_data1[];\n-  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n-  T_ACC* s_dg;\n-  T_ACC* s_db;\n-\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db) {\n+  // This assert is a compile-time check only.\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  static_assert(rows_per_thread_y <= kWarpSize);\n \n   T_ACC dg_sum = 0;\n   T_ACC db_sum = 0;\n \n-  if (j < N) {\n-    constexpr int unroll_factor = 8;\n-\n-    T_ACC mean_reg;\n-    T_ACC rstd_reg;\n-    T dY_reg;\n-    T X_reg;\n+  if (aligned_grid) {\n+    // When N and M align perfectly with block_dim_x and block_dim_y, we\n+    // can skip boundary condition checks that waste instruction issue slots.\n+    blockReduceGammaBetaBackwardsAligned\n+        <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>\n+        (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+  } else {\n+    // In the general case we need to check boundary conditions in the M\n+    // dimension. However, we can still avoid boundary checks in the N dimension\n+    // for the inner blocks. So try to avoid those checks when possible.\n+    if (blockIdx.x * block_dim_x + block_dim_x - 1 < N) {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    }\n+  }\n \n-    // Main Loop\n-    int bcounter;\n-    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor); bcounter++){\n-      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n \n-      #pragma unroll\n-      for (int ii = 0; ii < unroll_factor; ++ii) {\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+  // When partial_reduction is requested, we don't reduce within a block.\n+  // We also don't reduce if we are only a single block in the y dimension.\n+  if (partial_reduction || (blockDim.y == 1 && gridDim.y == 1)) {\n+    if (aligned_grid || thread_x < N) {\n+      int64_t thread_y = blockIdx.y * blockDim.y + threadIdx.y;\n+      if (dg) {\n+        dg[thread_y * N + thread_x] = dg_sum;\n       }\n-    }\n-\n-    // Remainder loop\n-    int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n-    for (int ii = 0; ii < unroll_factor; ii++ ){\n-      if ((offset + ii) < M) {\n-        dY_reg = dY[(offset + ii) * N + j ];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (db) {\n+        db[thread_y * N + thread_x] = db_sum;\n       }\n     }\n-\n-    // Do the final reduction in shared memory\n+  } else {\n+    // The caller requested a full reduction so we must reduce across\n+    // warps using shared memory and warp shuffles.\n+    static_assert(rows_per_thread_y <= C10_WARP_SIZE);\n+    alignas(sizeof(double)) extern __shared__ char s_data1[];\n+    T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n+    T_ACC* s_dg;\n+    T_ACC* s_db;\n+    int padded_bx = (block_dim_x + 1);\n+    // Transpose dg and db.\n     s_dg = s_data_typed;\n-    s_db = s_data_typed + blockDim.x * blockDim.y;\n-    s_dg[threadIdx.y * blockDim.x + threadIdx.x] = dg_sum;\n-    s_db[threadIdx.y * blockDim.x + threadIdx.x] = db_sum;\n+    s_db = s_data_typed + (padded_bx * block_dim_y);\n+    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n+    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n     __syncthreads();\n \n-    for (int offset = blockDim.y / 2; offset >= 1; offset /= 2) {\n-      if (threadIdx.y < offset) {\n-        s_dg[threadIdx.y * blockDim.x + threadIdx.x] +=\n-            s_dg[(threadIdx.y + offset) * blockDim.x + threadIdx.x];\n-        s_db[threadIdx.y * blockDim.x + threadIdx.x] +=\n-            s_db[(threadIdx.y + offset) * blockDim.x + threadIdx.x];\n+    // Load transposed so that a warp holds an entire column\n+    // Because block_dim_x != block_dim_y in the general case, we need\n+    // some code to handle the general case.\n+    static_assert(block_dim_x * block_dim_y % C10_WARP_SIZE == 0);\n+    constexpr int warps_available_to_reduce = block_dim_x * block_dim_y / C10_WARP_SIZE;\n+    int thread_id = threadIdx.y * block_dim_x + threadIdx.x;\n+    int warp_id = thread_id / C10_WARP_SIZE;\n+    int lane_id = thread_id & (C10_WARP_SIZE - 1);\n+    #pragma unroll\n+    for (int i = warp_id; i < block_dim_x; i += warps_available_to_reduce) {\n+      T_ACC reg_db, reg_dg;\n+      if (lane_id < block_dim_y) {\n+        reg_dg = s_dg[lane_id * padded_bx + i];\n+        reg_db = s_db[lane_id * padded_bx + i];\n+      }\n+      #pragma unroll\n+      for (unsigned delta = block_dim_y >> 1; delta >= 1; delta >>= 1) {\n+        reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n+        reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n+      }\n+      // Reduce is done. Now write it out to global memory.\n+      int64_t out_index = blockIdx.x * block_dim_x + i;\n+      if (threadIdx.x == 0 && (aligned_grid || out_index < N)) {\n+        if (dg) {\n+          dg[out_index] = reg_dg;\n         }\n-      __syncthreads();\n+        if (db) {\n+          db[out_index] = reg_db;\n+        }\n+      }\n     }\n+  }\n+}\n \n-    if (threadIdx.y == 0) {\n-      if (dg) {\n-        dg[j] = s_dg[threadIdx.x];\n-      }\n-      if (db) {\n-        db[j] = s_db[threadIdx.x];\n-      }\n+template<typename T, typename T_ACC,\n+int block_dim_x, int block_dim_y,\n+int rows_per_block_y,\n+bool partial_reduction>\n+void LaunchAndCheckGammaBetaBackwardKernel(\n+  bool aligned_grid,\n+  dim3 blocks,\n+  dim3 threads,\n+  size_t shmem_sz,\n+  cudaStream_t cuda_stream,\n+  const T* dY_data,\n+  const T* X_data,\n+  const T_ACC* mean_data,\n+  const T_ACC* rstd_data,\n+  int64_t M,\n+  int64_t N,\n+  T* dgamma_data,\n+  T* dbeta_data) {\n+if (aligned_grid) {\n+    GammaBetaBackwardCUDAKernelTemplate<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, partial_reduction, true>\n+        <<<blocks, threads, shmem_sz, cuda_stream>>>(\n+            M,\n+            N,\n+            dY_data,\n+            X_data,\n+            mean_data,\n+            rstd_data,\n+            dgamma_data,\n+            dbeta_data);\n+  } else {\n+    GammaBetaBackwardCUDAKernelTemplate<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, partial_reduction, false>\n+        <<<blocks, threads, shmem_sz, cuda_stream>>>(\n+            M,\n+            N,\n+            dY_data,\n+            X_data,\n+            mean_data,\n+            rstd_data,\n+            dgamma_data,\n+            dbeta_data);\n+  }\n+  C10_CUDA_KERNEL_LAUNCH_CHECK();\n+}\n+\n+template<typename T, typename T_ACC,\n+int block_dim_x, int block_dim_y,\n+int rows_per_block_y>\n+void ConfigureAndLaunchGammaBetaBackwardKernel(\n+    const T* dY_data,\n+    const T* X_data,\n+    const T_ACC* mean_data,\n+    const T_ACC* rstd_data,\n+    int64_t M,\n+    int64_t N,\n+    Tensor* dgamma,\n+    Tensor* dbeta,\n+    cudaStream_t cuda_stream) {\n+  T* dgamma_data =\n+    dgamma->defined() ? dgamma->template data_ptr<T>() : nullptr;\n+  T* dbeta_data = dbeta->defined() ? dbeta->template data_ptr<T>() : nullptr;\n+  bool aligned_grid = (M % rows_per_block_y == 0) && (N % block_dim_x == 0);\n+  dim3 threads{block_dim_x, block_dim_y};\n+  dim3 blocks;\n+  blocks.x = (N + block_dim_x - 1) / block_dim_x;\n+  blocks.y = 1;\n+  size_t shmem_sz = (block_dim_x + 1) * block_dim_y * sizeof(T_ACC) * 2;\n+  if (blocks.y == 1 && threads.y == 1) {\n+    // Optimization: since there is just one thread doing all the summation, we don't need a reduction\n+    // across threads. So we set partial_reduction to true.\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>(\n+      aligned_grid, blocks, threads, shmem_sz, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_data, dbeta_data);\n+  } else {\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false>(\n+      aligned_grid, blocks, threads, shmem_sz, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_data, dbeta_data);\n+  }\n+\n+}\n+\n+template<typename T, typename T_ACC>\n+void LaunchGammaBetaBackwardCUDAKernel(\n+    const T* dY_data,\n+    const T* X_data,\n+    const T_ACC* mean_data,\n+    const T_ACC* rstd_data,\n+    int64_t M,\n+    int64_t N,\n+    Tensor* dgamma,\n+    Tensor* dbeta,\n+    cudaStream_t cuda_stream) {\n+  if (M > 64 * 1024 && N / kWarpSize < 64) {\n+    // We have a situation where M >> N and N is small.\n+    // In this case we can speed up the computation by parallelizing in the M dimension.\n+    // We launch multiple blocks in the y-dimension, and compute partial sums for the\n+    // gradient in the first pass. Then we do a .sum(0) to do a final reduction.\n+    // Although we launch 2 kernels, we can get up to a 10x speedup for large M.\n+    constexpr int block_dim_x = 32;\n+    constexpr int block_dim_y = 1;\n+    constexpr int rows_per_block_y = 32;\n+    bool aligned_grid = (M % rows_per_block_y == 0) && (N % block_dim_x == 0);\n+    dim3 threads{block_dim_x, block_dim_y};\n+    dim3 blocks;\n+    blocks.x = (N + block_dim_x - 1) / block_dim_x;\n+    // int rows_per_block = my_gamma_beta_unroll_factor *\n+    blocks.y = (M + rows_per_block_y - 1) / rows_per_block_y;\n+    constexpr int max_grid_size = 64 * 1024 / 2;\n+    blocks.y = std::min<unsigned int>(max_grid_size / blocks.x, blocks.y);\n+    auto options = dgamma->options();\n+    Tensor dgamma_blocks;\n+    Tensor dbeta_blocks;\n+    T * dgamma_blocks_ptr = nullptr;\n+    T * dbeta_blocks_ptr = nullptr;\n+    if (dgamma->defined()) {\n+      dgamma_blocks = at::zeros({blocks.y * threads.y, dgamma->size(-1)}, options);\n+      dgamma_blocks_ptr = dgamma_blocks.data_ptr<T>();\n+    }\n+    if (dbeta->defined()) {\n+      dbeta_blocks = at::zeros({blocks.y * threads.y, dgamma->size(-1)}, options);\n+      dbeta_blocks_ptr = dbeta_blocks.data_ptr<T>();\n+    }\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>(\n+      aligned_grid, blocks, threads, 0, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_blocks_ptr, dbeta_blocks_ptr);\n+\n+    *dgamma = dgamma_blocks.sum(0);\n+    *dbeta = dbeta_blocks.sum(0);\n+  } else {\n+    // We are in the normal case where M is not that large.\n+    // We can change the tile shape (which is the last template parameter) in accordance with M.\n+    // For small M it is faster to have a smaller tile, otherwise we could have idle threads.\n+    // For larger M we use a bigger tile size.\n+    if (M < 64) {",
        "comment_created_at": "2025-03-18T03:03:26+00:00",
        "comment_author": "ahmadsharif1",
        "comment_body": "<img width=\"1510\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3bbd6550-5ea8-4644-a192-b8f3963424dc\" />\r\n\r\nThe perf impact is substantial (20% regression) even for powers of 2 without this specialization. Note that even before my change we were specializing for M < 128. I hyper-specialize a little bit (keeping the same code -- just different template parameters) which results in a speed-up over the baseline:\r\n\r\n<img width=\"1469\" alt=\"image\" src=\"https://github.com/user-attachments/assets/70f02463-d769-461a-b3fc-3536c0cb38f2\" />\r\n\r\nMy code has each thread handling 8 rows so it's pointless launching threads that don't handle any rows at all for \"short and wide\" cases (i.e. for low values of M). They just cause wasted occupancy.\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2006461566",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 148605,
        "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
        "discussion_id": "1996412678",
        "commented_code": "@@ -508,223 +509,418 @@ __global__ void layer_norm_grad_input_kernel_vectorized(\n   }\n }\n \n-\n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardSimpleCUDAKernel(\n+// When the data size is a multiple of the tile size we can use fewer\n+// instructions and registers. Example, this is the case when M=256 N=256.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool aligned_grid>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsAligned(\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-  if (j < N) {\n-    T_ACC sum1 = 0;\n-    T_ACC sum2 = 0;\n-    for (int64_t i = 0; i < M; ++i) {\n-      const int64_t index = i * N + j;\n-      sum1 += dg == nullptr ? T_ACC(0)\n-                            : static_cast<T_ACC>(dY[index]) *\n-              (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n-              static_cast<T_ACC>(rstd[i]);\n-      sum2 += db == nullptr ? T_ACC(0) : static_cast<T_ACC>(dY[index]);\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n+  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n+        M_start < M;\n+        M_start += rows_per_block_y * gridDim.y) {\n+    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n+    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n+    T_ACC warp_mean = 0, warp_rstd = 0;\n+    if (lane_id < rows_per_thread_y) {\n+      warp_mean = mean[mean_index + lane_id];\n+      warp_rstd = rstd[mean_index + lane_id];\n     }\n-    if (dg != nullptr) {\n-      dg[j] = sum1;\n+    WARP_SYNC();\n+    T_ACC dY_regs[rows_per_thread_y] = {0};\n+    T_ACC X_regs[rows_per_thread_y] = {0};\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n+      if (aligned_grid || (current_y < M && thread_x < N)) {\n+        dY_regs[i] = dY[current_y * N + thread_x];\n+        X_regs[i] = X[current_y * N + thread_x];\n+      }\n     }\n-    if (db != nullptr) {\n-      db[j] = sum2;\n+\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n+      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n+      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n+      db_sum += dY_regs[i];\n     }\n   }\n }\n \n-// This implementation gets called if M and N divide with 32. This case should\n-// be the most common. We can then make better use of warp level intrinsics\n-// to improve performance.\n-\n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardCUDAKernel_32x32(\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsHelper(\n+    int64_t M_start,\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  alignas(sizeof(double)) extern __shared__ char s_data1[];\n-  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n-  T_ACC* s_dg;\n-  T_ACC* s_db;\n-\n-  T_ACC dg_sum = 0;\n-  T_ACC db_sum = 0;\n-\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-\n-  if (j < N) {\n-    constexpr int unroll_factor = 8;\n-    int laneId = threadIdx.x & (C10_WARP_SIZE - 1);\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n+\n+    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n+    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n+    T_ACC warp_mean = 0, warp_rstd = 0;\n+    if (lane_id < rows_per_thread_y && mean_index + lane_id < M) {\n+      warp_mean = mean[mean_index + lane_id];\n+      warp_rstd = rstd[mean_index + lane_id];\n+    }\n+    WARP_SYNC();\n \n-    T_ACC mean_reg, mean_reg_tmp;\n-    T_ACC rstd_reg, rstd_reg_tmp;\n-    T dY_reg;\n-    T X_reg;\n \n-    // Main loop\n-    int bcounter;\n-    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor);\n-         bcounter++) {\n-      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n \n-      if (laneId < unroll_factor) {\n-        mean_reg_tmp = mean[offset + laneId];\n-        rstd_reg_tmp = rstd[offset + laneId];\n+    T_ACC dY_regs[rows_per_thread_y] = {0};\n+    T_ACC X_regs[rows_per_thread_y] = {0};\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n+      bool active = true;\n+      if (check_x && thread_x >= N) {\n+        active = false;\n       }\n-      WARP_SYNC();\n-\n-      #pragma unroll\n-      for (int ii = 0; ii < unroll_factor; ++ii) {\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = WARP_SHFL(mean_reg_tmp, ii, kWarpSize);\n-        rstd_reg = WARP_SHFL(rstd_reg_tmp, ii, kWarpSize);\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (check_y && current_y >= M) {\n+        active = false;\n       }\n-    }\n-\n-    // Remainder loop\n-    int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n-    for (int ii = 0; ii < unroll_factor; ii++) {\n-      if ((offset + ii) < M) {\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (active) {\n+        dY_regs[i] = dY[current_y * N + thread_x];\n+        X_regs[i] = X[current_y * N + thread_x];\n       }\n     }\n \n-    // This kernel uses a block of (C10_WARP_SIZE x C10_WARP_SIZE) and\n-    // gets called when M; N divide by 32. We can use warp shuffles\n-    // for the final reduction step. This removes 4 shmem loads and\n-    // stores with their corresponding __syncthreads()\n-\n-    // This greatly reduces bank conflicts at the expense of a little\n-    // extra shared memory. It does not impact occupancy\n-    int padded_bx = (1 + blockDim.x);\n-\n-    s_dg = s_data_typed;\n-    s_db = s_data_typed + (padded_bx * blockDim.y);\n-    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n-    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n-    __syncthreads();\n-\n-    // Load transposed so that a warp holds an entire column\n-    T_ACC reg_dg = s_dg[threadIdx.x * padded_bx + threadIdx.y];\n-    T_ACC reg_db = s_db[threadIdx.x * padded_bx + threadIdx.y];\n-    for (unsigned delta = C10_WARP_SIZE >> 1; delta >= 1; delta >>= 1) {\n-      reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n-      reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n+      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n+      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n+      db_sum += dY_regs[i];\n     }\n+}\n \n-    if (threadIdx.x == 0) {\n-      const int64_t j = blockIdx.x * blockDim.x + threadIdx.y;\n-      if (dg) {\n-        dg[j] = reg_dg;\n-      }\n-      if (db) {\n-        db[j] = reg_db;\n-      }\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsWithChecks(\n+    int64_t M,\n+    int64_t N,\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n+        M_start < M;\n+        M_start += rows_per_block_y * gridDim.y) {\n+    int64_t M_end = M_start + rows_per_block_y - 1;\n+    if (M_end < M) {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, false>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, true>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n     }\n   }\n }\n \n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardCUDAKernel(\n+// block_dim_x is the number of threads in the x dimension per block.\n+// block_dim_y is the number of threads in the y dimension per block.\n+// rows_per_block_y is the size of the tile (number of data elements)\n+// in the y dimension per block.\n+// partial_reduction indicates whether we need to reduce across threads\n+// or not. If set to true, we will not reduce across threads. This can\n+// be faster in the M >> N case but requires another kernel to do a full\n+// final reduction.\n+// aligned_grid means the data size is a multiple of tile size. In that\n+// case we don't need to check for boundary conditions which can provide\n+// a further speedup by not needing instructions to check for edge cases\n+// and not needing predicate registers.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x, unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool partial_reduction,\n+bool aligned_grid\n+>\n+__global__\n+void\n+ GammaBetaBackwardCUDAKernelTemplate(\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  alignas(sizeof(double)) extern __shared__ char s_data1[];\n-  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n-  T_ACC* s_dg;\n-  T_ACC* s_db;\n-\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db) {\n+  // This assert is a compile-time check only.\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  static_assert(rows_per_thread_y <= kWarpSize);\n \n   T_ACC dg_sum = 0;\n   T_ACC db_sum = 0;\n \n-  if (j < N) {\n-    constexpr int unroll_factor = 8;\n-\n-    T_ACC mean_reg;\n-    T_ACC rstd_reg;\n-    T dY_reg;\n-    T X_reg;\n+  if (aligned_grid) {\n+    // When N and M align perfectly with block_dim_x and block_dim_y, we\n+    // can skip boundary condition checks that waste instruction issue slots.\n+    blockReduceGammaBetaBackwardsAligned\n+        <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>\n+        (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+  } else {\n+    // In the general case we need to check boundary conditions in the M\n+    // dimension. However, we can still avoid boundary checks in the N dimension\n+    // for the inner blocks. So try to avoid those checks when possible.\n+    if (blockIdx.x * block_dim_x + block_dim_x - 1 < N) {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    }\n+  }\n \n-    // Main Loop\n-    int bcounter;\n-    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor); bcounter++){\n-      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n \n-      #pragma unroll\n-      for (int ii = 0; ii < unroll_factor; ++ii) {\n-        dY_reg = dY[(offset + ii) * N + j];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+  // When partial_reduction is requested, we don't reduce within a block.\n+  // We also don't reduce if we are only a single block in the y dimension.\n+  if (partial_reduction || (blockDim.y == 1 && gridDim.y == 1)) {\n+    if (aligned_grid || thread_x < N) {\n+      int64_t thread_y = blockIdx.y * blockDim.y + threadIdx.y;\n+      if (dg) {\n+        dg[thread_y * N + thread_x] = dg_sum;\n       }\n-    }\n-\n-    // Remainder loop\n-    int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n-    for (int ii = 0; ii < unroll_factor; ii++ ){\n-      if ((offset + ii) < M) {\n-        dY_reg = dY[(offset + ii) * N + j ];\n-        X_reg = X[(offset + ii) * N + j];\n-        mean_reg = mean[offset + ii];\n-        rstd_reg = rstd[offset + ii];\n-        dg_sum += dY_reg * (X_reg - mean_reg) * rstd_reg;\n-        db_sum += dY_reg;\n+      if (db) {\n+        db[thread_y * N + thread_x] = db_sum;\n       }\n     }\n-\n-    // Do the final reduction in shared memory\n+  } else {\n+    // The caller requested a full reduction so we must reduce across\n+    // warps using shared memory and warp shuffles.\n+    static_assert(rows_per_thread_y <= C10_WARP_SIZE);\n+    alignas(sizeof(double)) extern __shared__ char s_data1[];\n+    T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n+    T_ACC* s_dg;\n+    T_ACC* s_db;\n+    int padded_bx = (block_dim_x + 1);\n+    // Transpose dg and db.\n     s_dg = s_data_typed;\n-    s_db = s_data_typed + blockDim.x * blockDim.y;\n-    s_dg[threadIdx.y * blockDim.x + threadIdx.x] = dg_sum;\n-    s_db[threadIdx.y * blockDim.x + threadIdx.x] = db_sum;\n+    s_db = s_data_typed + (padded_bx * block_dim_y);\n+    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n+    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n     __syncthreads();\n \n-    for (int offset = blockDim.y / 2; offset >= 1; offset /= 2) {\n-      if (threadIdx.y < offset) {\n-        s_dg[threadIdx.y * blockDim.x + threadIdx.x] +=\n-            s_dg[(threadIdx.y + offset) * blockDim.x + threadIdx.x];\n-        s_db[threadIdx.y * blockDim.x + threadIdx.x] +=\n-            s_db[(threadIdx.y + offset) * blockDim.x + threadIdx.x];\n+    // Load transposed so that a warp holds an entire column\n+    // Because block_dim_x != block_dim_y in the general case, we need\n+    // some code to handle the general case.\n+    static_assert(block_dim_x * block_dim_y % C10_WARP_SIZE == 0);\n+    constexpr int warps_available_to_reduce = block_dim_x * block_dim_y / C10_WARP_SIZE;\n+    int thread_id = threadIdx.y * block_dim_x + threadIdx.x;\n+    int warp_id = thread_id / C10_WARP_SIZE;\n+    int lane_id = thread_id & (C10_WARP_SIZE - 1);\n+    #pragma unroll\n+    for (int i = warp_id; i < block_dim_x; i += warps_available_to_reduce) {\n+      T_ACC reg_db, reg_dg;\n+      if (lane_id < block_dim_y) {\n+        reg_dg = s_dg[lane_id * padded_bx + i];\n+        reg_db = s_db[lane_id * padded_bx + i];\n+      }\n+      #pragma unroll\n+      for (unsigned delta = block_dim_y >> 1; delta >= 1; delta >>= 1) {\n+        reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n+        reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n+      }\n+      // Reduce is done. Now write it out to global memory.\n+      int64_t out_index = blockIdx.x * block_dim_x + i;\n+      if (threadIdx.x == 0 && (aligned_grid || out_index < N)) {\n+        if (dg) {\n+          dg[out_index] = reg_dg;\n         }\n-      __syncthreads();\n+        if (db) {\n+          db[out_index] = reg_db;\n+        }\n+      }\n     }\n+  }\n+}\n \n-    if (threadIdx.y == 0) {\n-      if (dg) {\n-        dg[j] = s_dg[threadIdx.x];\n-      }\n-      if (db) {\n-        db[j] = s_db[threadIdx.x];\n-      }\n+template<typename T, typename T_ACC,\n+int block_dim_x, int block_dim_y,\n+int rows_per_block_y,\n+bool partial_reduction>\n+void LaunchAndCheckGammaBetaBackwardKernel(\n+  bool aligned_grid,\n+  dim3 blocks,\n+  dim3 threads,\n+  size_t shmem_sz,\n+  cudaStream_t cuda_stream,\n+  const T* dY_data,\n+  const T* X_data,\n+  const T_ACC* mean_data,\n+  const T_ACC* rstd_data,\n+  int64_t M,\n+  int64_t N,\n+  T* dgamma_data,\n+  T* dbeta_data) {\n+if (aligned_grid) {\n+    GammaBetaBackwardCUDAKernelTemplate<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, partial_reduction, true>\n+        <<<blocks, threads, shmem_sz, cuda_stream>>>(\n+            M,\n+            N,\n+            dY_data,\n+            X_data,\n+            mean_data,\n+            rstd_data,\n+            dgamma_data,\n+            dbeta_data);\n+  } else {\n+    GammaBetaBackwardCUDAKernelTemplate<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, partial_reduction, false>\n+        <<<blocks, threads, shmem_sz, cuda_stream>>>(\n+            M,\n+            N,\n+            dY_data,\n+            X_data,\n+            mean_data,\n+            rstd_data,\n+            dgamma_data,\n+            dbeta_data);\n+  }\n+  C10_CUDA_KERNEL_LAUNCH_CHECK();\n+}\n+\n+template<typename T, typename T_ACC,\n+int block_dim_x, int block_dim_y,\n+int rows_per_block_y>\n+void ConfigureAndLaunchGammaBetaBackwardKernel(\n+    const T* dY_data,\n+    const T* X_data,\n+    const T_ACC* mean_data,\n+    const T_ACC* rstd_data,\n+    int64_t M,\n+    int64_t N,\n+    Tensor* dgamma,\n+    Tensor* dbeta,\n+    cudaStream_t cuda_stream) {\n+  T* dgamma_data =\n+    dgamma->defined() ? dgamma->template data_ptr<T>() : nullptr;\n+  T* dbeta_data = dbeta->defined() ? dbeta->template data_ptr<T>() : nullptr;\n+  bool aligned_grid = (M % rows_per_block_y == 0) && (N % block_dim_x == 0);\n+  dim3 threads{block_dim_x, block_dim_y};\n+  dim3 blocks;\n+  blocks.x = (N + block_dim_x - 1) / block_dim_x;\n+  blocks.y = 1;\n+  size_t shmem_sz = (block_dim_x + 1) * block_dim_y * sizeof(T_ACC) * 2;\n+  if (blocks.y == 1 && threads.y == 1) {\n+    // Optimization: since there is just one thread doing all the summation, we don't need a reduction\n+    // across threads. So we set partial_reduction to true.\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>(\n+      aligned_grid, blocks, threads, shmem_sz, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_data, dbeta_data);\n+  } else {\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false>(\n+      aligned_grid, blocks, threads, shmem_sz, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_data, dbeta_data);\n+  }\n+\n+}\n+\n+template<typename T, typename T_ACC>\n+void LaunchGammaBetaBackwardCUDAKernel(\n+    const T* dY_data,\n+    const T* X_data,\n+    const T_ACC* mean_data,\n+    const T_ACC* rstd_data,\n+    int64_t M,\n+    int64_t N,\n+    Tensor* dgamma,\n+    Tensor* dbeta,\n+    cudaStream_t cuda_stream) {\n+  if (M > 64 * 1024 && N / kWarpSize < 64) {\n+    // We have a situation where M >> N and N is small.\n+    // In this case we can speed up the computation by parallelizing in the M dimension.\n+    // We launch multiple blocks in the y-dimension, and compute partial sums for the\n+    // gradient in the first pass. Then we do a .sum(0) to do a final reduction.\n+    // Although we launch 2 kernels, we can get up to a 10x speedup for large M.\n+    constexpr int block_dim_x = 32;\n+    constexpr int block_dim_y = 1;\n+    constexpr int rows_per_block_y = 32;\n+    bool aligned_grid = (M % rows_per_block_y == 0) && (N % block_dim_x == 0);\n+    dim3 threads{block_dim_x, block_dim_y};\n+    dim3 blocks;\n+    blocks.x = (N + block_dim_x - 1) / block_dim_x;\n+    // int rows_per_block = my_gamma_beta_unroll_factor *\n+    blocks.y = (M + rows_per_block_y - 1) / rows_per_block_y;\n+    constexpr int max_grid_size = 64 * 1024 / 2;\n+    blocks.y = std::min<unsigned int>(max_grid_size / blocks.x, blocks.y);\n+    auto options = dgamma->options();\n+    Tensor dgamma_blocks;\n+    Tensor dbeta_blocks;\n+    T * dgamma_blocks_ptr = nullptr;\n+    T * dbeta_blocks_ptr = nullptr;\n+    if (dgamma->defined()) {\n+      dgamma_blocks = at::zeros({blocks.y * threads.y, dgamma->size(-1)}, options);\n+      dgamma_blocks_ptr = dgamma_blocks.data_ptr<T>();\n+    }\n+    if (dbeta->defined()) {\n+      dbeta_blocks = at::zeros({blocks.y * threads.y, dgamma->size(-1)}, options);\n+      dbeta_blocks_ptr = dbeta_blocks.data_ptr<T>();\n+    }\n+    LaunchAndCheckGammaBetaBackwardKernel<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true>(\n+      aligned_grid, blocks, threads, 0, cuda_stream, dY_data, X_data, mean_data, rstd_data, M, N, dgamma_blocks_ptr, dbeta_blocks_ptr);\n+\n+    *dgamma = dgamma_blocks.sum(0);\n+    *dbeta = dbeta_blocks.sum(0);\n+  } else {\n+    // We are in the normal case where M is not that large.\n+    // We can change the tile shape (which is the last template parameter) in accordance with M.\n+    // For small M it is faster to have a smaller tile, otherwise we could have idle threads.\n+    // For larger M we use a bigger tile size.\n+    if (M < 64) {",
        "comment_created_at": "2025-03-20T21:12:03+00:00",
        "comment_author": "ahmadsharif1",
        "comment_body": "For completeness, I did more experiments with smaller values of M and N and a lot more datapoints.\r\n\r\nWithout specialization:\r\n![image](https://github.com/user-attachments/assets/1fc62865-3f7b-4f12-97ed-01e2eaba4cc1)\r\n\r\n\r\nWith specialization:\r\n![image](https://github.com/user-attachments/assets/5325ed92-cc36-4f2a-a137-0edbeeb0ccbd)\r\n\r\n\r\nYou can observe that the \"short and wide\" region regressions go away with specialization (basically large N and small M which is at the bottom right of the chart).\r\n\r\nThere are still some regressions left, but those are with very small values of N (like 4 or 8 -- not sure if those shapes are used much).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2033537678",
    "pr_number": 150218,
    "pr_file": "aten/src/ATen/DLConvertor.cpp",
    "created_at": "2025-04-08T16:03:52+00:00",
    "commented_code": "return fromDLPackImpl<DLManagedTensorVersioned>(src, std::move(deleter));\n}\n\nTensor maybeCopyTensor(\n    const Tensor& data,\n    std::optional<DLDevice> optional_dl_device,\n    std::optional<bool> copy) {\n  bool force_copy = copy.has_value() && *copy;\n  bool force_move = copy.has_value() && !*copy;\n\n  if (optional_dl_device.has_value()) {\n    auto device = at::getATenDevice(optional_dl_device->device_type, static_cast<c10::DeviceIndex>(optional_dl_device->device_id));\n\n    if (device != data.device()) {\n      TORCH_CHECK(\n          !force_move,\n          \"cannot move tensor from \",\n          data.device(),\n          \" to \",\n          device,\n          \" without copying. Set copy=True is needed.\");\n      return data.to(device);\n    }\n  }\n\n  if (force_copy) {\n    return data.clone().detach();",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2033537678",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150218,
        "pr_file": "aten/src/ATen/DLConvertor.cpp",
        "discussion_id": "2033537678",
        "commented_code": "@@ -388,4 +389,33 @@ Tensor fromDLPackVersioned(DLManagedTensorVersioned* src, std::function<void(voi\n   return fromDLPackImpl<DLManagedTensorVersioned>(src, std::move(deleter));\n }\n \n+Tensor maybeCopyTensor(\n+    const Tensor& data,\n+    std::optional<DLDevice> optional_dl_device,\n+    std::optional<bool> copy) {\n+  bool force_copy = copy.has_value() && *copy;\n+  bool force_move = copy.has_value() && !*copy;\n+\n+  if (optional_dl_device.has_value()) {\n+    auto device = at::getATenDevice(optional_dl_device->device_type, static_cast<c10::DeviceIndex>(optional_dl_device->device_id));\n+\n+    if (device != data.device()) {\n+      TORCH_CHECK(\n+          !force_move,\n+          \"cannot move tensor from \",\n+          data.device(),\n+          \" to \",\n+          device,\n+          \" without copying. Set copy=True is needed.\");\n+      return data.to(device);\n+    }\n+  }\n+\n+  if (force_copy) {\n+    return data.clone().detach();",
        "comment_created_at": "2025-04-08T16:03:52+00:00",
        "comment_author": "Skylion007",
        "comment_body": "```suggestion\r\n    return data.detach().clone();\r\n```\r\nThis will copy less data / be more efficient.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2063591267",
    "pr_number": 152089,
    "pr_file": "aten/src/ATen/core/dispatch/OperatorEntry.cpp",
    "created_at": "2025-04-28T12:51:29+00:00",
    "commented_code": "#endif\n}\n\nvoid OperatorEntry::unsafeSetTags(const std::vector<at::Tag>& tags) {\n  #ifndef C10_MOBILE\n    tags_ = std::move(tags);\n  #endif",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2063591267",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 152089,
        "pr_file": "aten/src/ATen/core/dispatch/OperatorEntry.cpp",
        "discussion_id": "2063591267",
        "commented_code": "@@ -157,6 +157,12 @@ void OperatorEntry::registerSchema(FunctionSchema&& schema, std::string&& debug,\n   #endif\n }\n \n+void OperatorEntry::unsafeSetTags(const std::vector<at::Tag>& tags) {\n+  #ifndef C10_MOBILE\n+    tags_ = std::move(tags);\n+  #endif",
        "comment_created_at": "2025-04-28T12:51:29+00:00",
        "comment_author": "zou3519",
        "comment_body": "I'm not that good with C++, why is this a std::move?",
        "pr_file_module": null
      },
      {
        "comment_id": "2065568732",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 152089,
        "pr_file": "aten/src/ATen/core/dispatch/OperatorEntry.cpp",
        "discussion_id": "2063591267",
        "commented_code": "@@ -157,6 +157,12 @@ void OperatorEntry::registerSchema(FunctionSchema&& schema, std::string&& debug,\n   #endif\n }\n \n+void OperatorEntry::unsafeSetTags(const std::vector<at::Tag>& tags) {\n+  #ifndef C10_MOBILE\n+    tags_ = std::move(tags);\n+  #endif",
        "comment_created_at": "2025-04-29T06:17:35+00:00",
        "comment_author": "jijiew",
        "comment_body": "it avoids unnecessary copy",
        "pr_file_module": null
      }
    ]
  }
]