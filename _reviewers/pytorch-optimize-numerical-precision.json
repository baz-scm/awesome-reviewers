[
  {
    "discussion_id": "2181446162",
    "pr_number": 157520,
    "pr_file": "aten/src/ATen/Context.cpp",
    "created_at": "2025-07-03T02:40:34+00:00",
    "commented_code": "} else if (s_ == \"high\") {\n       float32_matmul_precision = at::Float32MatmulPrecision::HIGH;\n       setFloat32Precision(\"cuda\", \"matmul\", \"tf32\");\n-      setFloat32Precision(\"mkldnn\", \"matmul\", \"ieee\");\n+      setFloat32Precision(\"mkldnn\", \"matmul\", \"tf32\");",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2181446162",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157520,
        "pr_file": "aten/src/ATen/Context.cpp",
        "discussion_id": "2181446162",
        "commented_code": "@@ -401,7 +404,7 @@ void Context::setFloat32MatmulPrecision(const std::string &s) {\n     } else if (s_ == \"high\") {\n       float32_matmul_precision = at::Float32MatmulPrecision::HIGH;\n       setFloat32Precision(\"cuda\", \"matmul\", \"tf32\");\n-      setFloat32Precision(\"mkldnn\", \"matmul\", \"ieee\");\n+      setFloat32Precision(\"mkldnn\", \"matmul\", \"tf32\");",
        "comment_created_at": "2025-07-03T02:40:34+00:00",
        "comment_author": "mingfeima",
        "comment_body": "i don't quite understand what this change means, \"ieee\" to \"tf32\"",
        "pr_file_module": null
      },
      {
        "comment_id": "2181459381",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157520,
        "pr_file": "aten/src/ATen/Context.cpp",
        "discussion_id": "2181446162",
        "commented_code": "@@ -401,7 +404,7 @@ void Context::setFloat32MatmulPrecision(const std::string &s) {\n     } else if (s_ == \"high\") {\n       float32_matmul_precision = at::Float32MatmulPrecision::HIGH;\n       setFloat32Precision(\"cuda\", \"matmul\", \"tf32\");\n-      setFloat32Precision(\"mkldnn\", \"matmul\", \"ieee\");\n+      setFloat32Precision(\"mkldnn\", \"matmul\", \"tf32\");",
        "comment_created_at": "2025-07-03T02:50:30+00:00",
        "comment_author": "yanbing-j",
        "comment_body": "In the description of https://github.com/pytorch/pytorch/pull/125888, it says,\r\n\r\nWe provide 3 fp32 compute precision can be set:\r\n**\"ieee\"**: Not allowed to use any other internal computation data types .\r\n**\"tf32\"**: Allowed to use tf32 as internal computation data types.\r\n**\"bf16\"**: Allowed to use bf16 as internal computation data types.\r\n**\"none\"**: Precision's are not set. Can be override by its father node.\r\n\r\n`\"HIGHEST, HIGH, MEDIUM\"` is a legacy representation, means `ieee`, `tf32`, and `bf16`.\r\n\r\nSo without this PR, mkldnn backend only supports `ieee`, `bf16` and `none`. If set to `HIGH`, `tf32` is not supported in mkldnn, use `ieee` instead. With this PR, `tf32` is supported in mkldnn, so we can use `tf32` directly.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2181463970",
    "pr_number": 157520,
    "pr_file": "aten/src/ATen/native/mkldnn/Matmul.cpp",
    "created_at": "2025-07-03T02:53:01+00:00",
    "commented_code": "checksize(mat1, mat2));\n }\n \n+bool use_mkldnn_tf32_matmul(\n+    const Tensor& mat1,\n+    const Tensor& mat2,\n+    const Tensor& result) {\n+\n+    return (\n+      use_mkldnn_tf32_matmul() &&\n+      mat1.scalar_type() == kFloat &&\n+      mat2.scalar_type() == kFloat &&\n+      (!result.defined() || result.scalar_type() == kFloat) &&\n+      mat1.numel() != 0 &&\n+      mat2.numel() != 0 &&\n+      checksize(mat1, mat2));\n+}\n+\n bool use_mkldnn_matmul(\n     const Tensor& mat1,\n     const Tensor& mat2,\n     const Tensor& result) {\n-  return (use_mkldnn_bf16_matmul(mat1, mat2, result) || use_mkldnn_fp16_matmul(mat1, mat2, result) || use_mkldnn_bf32_matmul(mat1, mat2, result));\n+  return (use_mkldnn_bf16_matmul(mat1, mat2, result) || use_mkldnn_fp16_matmul(mat1, mat2, result) || use_mkldnn_bf32_matmul(mat1, mat2, result) || use_mkldnn_tf32_matmul(mat1, mat2, result));\n }",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2181463970",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157520,
        "pr_file": "aten/src/ATen/native/mkldnn/Matmul.cpp",
        "discussion_id": "2181463970",
        "commented_code": "@@ -471,11 +486,26 @@ bool use_mkldnn_bf32_matmul(\n       checksize(mat1, mat2));\n }\n \n+bool use_mkldnn_tf32_matmul(\n+    const Tensor& mat1,\n+    const Tensor& mat2,\n+    const Tensor& result) {\n+\n+    return (\n+      use_mkldnn_tf32_matmul() &&\n+      mat1.scalar_type() == kFloat &&\n+      mat2.scalar_type() == kFloat &&\n+      (!result.defined() || result.scalar_type() == kFloat) &&\n+      mat1.numel() != 0 &&\n+      mat2.numel() != 0 &&\n+      checksize(mat1, mat2));\n+}\n+\n bool use_mkldnn_matmul(\n     const Tensor& mat1,\n     const Tensor& mat2,\n     const Tensor& result) {\n-  return (use_mkldnn_bf16_matmul(mat1, mat2, result) || use_mkldnn_fp16_matmul(mat1, mat2, result) || use_mkldnn_bf32_matmul(mat1, mat2, result));\n+  return (use_mkldnn_bf16_matmul(mat1, mat2, result) || use_mkldnn_fp16_matmul(mat1, mat2, result) || use_mkldnn_bf32_matmul(mat1, mat2, result) || use_mkldnn_tf32_matmul(mat1, mat2, result));\n }",
        "comment_created_at": "2025-07-03T02:53:01+00:00",
        "comment_author": "mingfeima",
        "comment_body": "if you can template use_mkldnn_matmul<>, then you can do something like:\r\n```\r\nAT_DISPATCH_FLOATING_AND2(kBFloat16, kHalf, ..., [&] {\r\n  return use_mkldnn_matmul<scalar_t>(...);\r\n});\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2191234858",
    "pr_number": 157076,
    "pr_file": "aten/src/ATen/native/quantized/cpu/qconv.cpp",
    "created_at": "2025-07-08T00:18:40+00:00",
    "commented_code": "dilation.size() == (decltype(dilation.size()))kSpatialDim,\n     func_name, \": dilation should contain \", kSpatialDim, \" elements for \",\n     kSpatialDim, \"D convolution.\");\n+  bool is_fp8 = weight.scalar_type() == c10::ScalarType::Float8_e4m3fn;\n+  if (is_fp8) {\n+    TORCH_CHECK(act_dtype == c10::ScalarType::Float8_e4m3fn,\n+      func_name, \": expect input tensor to have fp8 data type, but got \", act_dtype);\n+    TORCH_CHECK(act_zero_point == 0,\n+      func_name, \": fp8 input should not have zero point.\");\n+    bool use_ref = false;\n+#if !IDEEP_PREREQ(3, 9, 0, 0)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2191234858",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157076,
        "pr_file": "aten/src/ATen/native/quantized/cpu/qconv.cpp",
        "discussion_id": "2191234858",
        "commented_code": "@@ -1502,6 +1619,41 @@ static at::Tensor _quantized_convolution_onednn(\n     dilation.size() == (decltype(dilation.size()))kSpatialDim,\n     func_name, \": dilation should contain \", kSpatialDim, \" elements for \",\n     kSpatialDim, \"D convolution.\");\n+  bool is_fp8 = weight.scalar_type() == c10::ScalarType::Float8_e4m3fn;\n+  if (is_fp8) {\n+    TORCH_CHECK(act_dtype == c10::ScalarType::Float8_e4m3fn,\n+      func_name, \": expect input tensor to have fp8 data type, but got \", act_dtype);\n+    TORCH_CHECK(act_zero_point == 0,\n+      func_name, \": fp8 input should not have zero point.\");\n+    bool use_ref = false;\n+#if !IDEEP_PREREQ(3, 9, 0, 0)",
        "comment_created_at": "2025-07-08T00:18:40+00:00",
        "comment_author": "leslie-fang-intel",
        "comment_body": "It seems better to upgrade oneDNN first. Otherwise, the oneDNN path wonâ€™t be covered by CI when the related code is merged.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2152830210",
    "pr_number": 155964,
    "pr_file": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
    "created_at": "2025-06-17T17:45:07+00:00",
    "commented_code": "return unpackPreMulSum<at::Half, ncclHalf>(reduceOp, comm);\n         case ncclFloat:\n           return unpackPreMulSum<float, ncclFloat>(reduceOp, comm);\n+        case ncclBfloat16:",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2152830210",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155964,
        "pr_file": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
        "discussion_id": "2152830210",
        "commented_code": "@@ -139,11 +139,14 @@ ncclRedOpRAII getNcclReduceOp(\n           return unpackPreMulSum<at::Half, ncclHalf>(reduceOp, comm);\n         case ncclFloat:\n           return unpackPreMulSum<float, ncclFloat>(reduceOp, comm);\n+        case ncclBfloat16:",
        "comment_created_at": "2025-06-17T17:45:07+00:00",
        "comment_author": "weifengpy",
        "comment_body": "add ProcessGroupNCCL unit test here? https://github.com/pytorch/pytorch/blob/2625c70aecc6eced1dbe108279feab7509733bef/test/distributed/test_c10d_ops_nccl.py#L816",
        "pr_file_module": null
      },
      {
        "comment_id": "2152841302",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155964,
        "pr_file": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
        "discussion_id": "2152830210",
        "commented_code": "@@ -139,11 +139,14 @@ ncclRedOpRAII getNcclReduceOp(\n           return unpackPreMulSum<at::Half, ncclHalf>(reduceOp, comm);\n         case ncclFloat:\n           return unpackPreMulSum<float, ncclFloat>(reduceOp, comm);\n+        case ncclBfloat16:",
        "comment_created_at": "2025-06-17T17:51:17+00:00",
        "comment_author": "weifengpy",
        "comment_body": "@kwen2501 seems nccl can take all the datatype they support. that's why we added ncclBfloat16 https://github.com/NVIDIA/nccl/blob/72d2432094d6ae36abd6e511c3a16a2d052dbf94/src/enqueue.cc#L2467-L2480",
        "pr_file_module": null
      },
      {
        "comment_id": "2155531913",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155964,
        "pr_file": "torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp",
        "discussion_id": "2152830210",
        "commented_code": "@@ -139,11 +139,14 @@ ncclRedOpRAII getNcclReduceOp(\n           return unpackPreMulSum<at::Half, ncclHalf>(reduceOp, comm);\n         case ncclFloat:\n           return unpackPreMulSum<float, ncclFloat>(reduceOp, comm);\n+        case ncclBfloat16:",
        "comment_created_at": "2025-06-18T21:22:26+00:00",
        "comment_author": "weifengpy",
        "comment_body": "rebase on top of https://github.com/pytorch/pytorch/pull/155915",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2027321480",
    "pr_number": 150278,
    "pr_file": "aten/src/ATen/native/SoftMax.cpp",
    "created_at": "2025-04-03T15:59:55+00:00",
    "commented_code": "Tensor softmax(const Tensor& input_, const int64_t dim_, std::optional<ScalarType> dtype) {\n   auto result = [&]() {\n     NoNamesGuard guard;\n-    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){\n+    if ((input_.is_cuda()||input_.is_xpu()) && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2027321480",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150278,
        "pr_file": "aten/src/ATen/native/SoftMax.cpp",
        "discussion_id": "2027321480",
        "commented_code": "@@ -411,7 +411,7 @@ TORCH_IMPL_FUNC(log_softmax_backward_cpu_out) (\n Tensor softmax(const Tensor& input_, const int64_t dim_, std::optional<ScalarType> dtype) {\n   auto result = [&]() {\n     NoNamesGuard guard;\n-    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){\n+    if ((input_.is_cuda()||input_.is_xpu()) && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){",
        "comment_created_at": "2025-04-03T15:59:55+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n    if ((input_.is_cuda() || input_.is_xpu()) && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float) {\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2027322124",
    "pr_number": 150278,
    "pr_file": "aten/src/ATen/native/SoftMax.cpp",
    "created_at": "2025-04-03T16:00:19+00:00",
    "commented_code": "std::optional<ScalarType> dtype,\n     Tensor& output_) {\n   Tensor output_temp;\n-  if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half &&\n+  if ((input_.is_cuda()||input_.is_xpu()) && input_.scalar_type() == ScalarType::Half &&",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2027322124",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150278,
        "pr_file": "aten/src/ATen/native/SoftMax.cpp",
        "discussion_id": "2027322124",
        "commented_code": "@@ -428,7 +428,7 @@ Tensor& softmax_out(\n     std::optional<ScalarType> dtype,\n     Tensor& output_) {\n   Tensor output_temp;\n-  if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half &&\n+  if ((input_.is_cuda()||input_.is_xpu()) && input_.scalar_type() == ScalarType::Half &&",
        "comment_created_at": "2025-04-03T16:00:19+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n  if ((input_.is_cuda() || input_.is_xpu()) && input_.scalar_type() == ScalarType::Half &&\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2027322401",
    "pr_number": 150278,
    "pr_file": "aten/src/ATen/native/SoftMax.cpp",
    "created_at": "2025-04-03T16:00:29+00:00",
    "commented_code": "Tensor log_softmax(const Tensor& input_, const int64_t dim_, std::optional<ScalarType> dtype) {\n   auto result = [&]() {\n     NoNamesGuard guard;\n-    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){\n+    if ((input_.is_cuda()||input_.is_xpu()) && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2027322401",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150278,
        "pr_file": "aten/src/ATen/native/SoftMax.cpp",
        "discussion_id": "2027322401",
        "commented_code": "@@ -467,7 +467,7 @@ Tensor special_softmax(const Tensor& input_, const int64_t dim_, std::optional<S\n Tensor log_softmax(const Tensor& input_, const int64_t dim_, std::optional<ScalarType> dtype) {\n   auto result = [&]() {\n     NoNamesGuard guard;\n-    if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){\n+    if ((input_.is_cuda()||input_.is_xpu()) && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float){",
        "comment_created_at": "2025-04-03T16:00:29+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n    if ((input_.is_cuda() || input_.is_xpu()) && input_.scalar_type() == ScalarType::Half && dtype == ScalarType::Float) {\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2027323071",
    "pr_number": 150278,
    "pr_file": "aten/src/ATen/native/SoftMax.cpp",
    "created_at": "2025-04-03T16:00:54+00:00",
    "commented_code": "std::optional<ScalarType> dtype,\n     Tensor& output_) {\n   Tensor output_temp;\n-  if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half &&\n+  if ((input_.is_cuda()||input_.is_xpu()) && input_.scalar_type() == ScalarType::Half &&",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2027323071",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 150278,
        "pr_file": "aten/src/ATen/native/SoftMax.cpp",
        "discussion_id": "2027323071",
        "commented_code": "@@ -484,7 +484,7 @@ Tensor& log_softmax_out(\n     std::optional<ScalarType> dtype,\n     Tensor& output_) {\n   Tensor output_temp;\n-  if (input_.is_cuda() && input_.scalar_type() == ScalarType::Half &&\n+  if ((input_.is_cuda()||input_.is_xpu()) && input_.scalar_type() == ScalarType::Half &&",
        "comment_created_at": "2025-04-03T16:00:54+00:00",
        "comment_author": "guangyey",
        "comment_body": "```suggestion\r\n  if ((input_.is_cuda() || input_.is_xpu()) && input_.scalar_type() == ScalarType::Half &&\r\n```",
        "pr_file_module": null
      }
    ]
  }
]