[
  {
    "discussion_id": "2191322701",
    "pr_number": 157658,
    "pr_file": "test/distributions/test_distributions.py",
    "created_at": "2025-07-08T01:59:11+00:00",
    "commented_code": "assert (vals == 0.0).sum() > 4000\n         assert (vals == 1.0).sum() > 4000\n \n+    def test_binomial_dtype_error(self):\n+        dtypes = [torch.int, torch.long, torch.short]\n+        for count_dtype in dtypes:\n+            for prob_dtype in dtypes:\n+                total_count = torch.tensor([10, 10], dtype=count_dtype)\n+                total_prob = torch.tensor([0.5, 0.5], dtype=prob_dtype)\n+\n+                with self.assertRaisesRegex(\n+                    RuntimeError,\n+                    \"binomial only supports floating-point dtypes for .*\",",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2191322701",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157658,
        "pr_file": "test/distributions/test_distributions.py",
        "discussion_id": "2191322701",
        "commented_code": "@@ -1805,6 +1805,19 @@ def test_zero_excluded_binomial(self):\n         assert (vals == 0.0).sum() > 4000\n         assert (vals == 1.0).sum() > 4000\n \n+    def test_binomial_dtype_error(self):\n+        dtypes = [torch.int, torch.long, torch.short]\n+        for count_dtype in dtypes:\n+            for prob_dtype in dtypes:\n+                total_count = torch.tensor([10, 10], dtype=count_dtype)\n+                total_prob = torch.tensor([0.5, 0.5], dtype=prob_dtype)\n+\n+                with self.assertRaisesRegex(\n+                    RuntimeError,\n+                    \"binomial only supports floating-point dtypes for .*\",",
        "comment_created_at": "2025-07-08T01:59:11+00:00",
        "comment_author": "malfet",
        "comment_body": "In all fairness, this test will only hit first exception. IMO instead of nested loop you'll need to add two loops: one for count dtypes and another for prob dtypes",
        "pr_file_module": null
      },
      {
        "comment_id": "2191343496",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157658,
        "pr_file": "test/distributions/test_distributions.py",
        "discussion_id": "2191322701",
        "commented_code": "@@ -1805,6 +1805,19 @@ def test_zero_excluded_binomial(self):\n         assert (vals == 0.0).sum() > 4000\n         assert (vals == 1.0).sum() > 4000\n \n+    def test_binomial_dtype_error(self):\n+        dtypes = [torch.int, torch.long, torch.short]\n+        for count_dtype in dtypes:\n+            for prob_dtype in dtypes:\n+                total_count = torch.tensor([10, 10], dtype=count_dtype)\n+                total_prob = torch.tensor([0.5, 0.5], dtype=prob_dtype)\n+\n+                with self.assertRaisesRegex(\n+                    RuntimeError,\n+                    \"binomial only supports floating-point dtypes for .*\",",
        "comment_created_at": "2025-07-08T02:22:09+00:00",
        "comment_author": "michellemadubuike",
        "comment_body": "Oh wow, I missed this. I'll make the fix. Thanks!",
        "pr_file_module": null
      },
      {
        "comment_id": "2191494726",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157658,
        "pr_file": "test/distributions/test_distributions.py",
        "discussion_id": "2191322701",
        "commented_code": "@@ -1805,6 +1805,19 @@ def test_zero_excluded_binomial(self):\n         assert (vals == 0.0).sum() > 4000\n         assert (vals == 1.0).sum() > 4000\n \n+    def test_binomial_dtype_error(self):\n+        dtypes = [torch.int, torch.long, torch.short]\n+        for count_dtype in dtypes:\n+            for prob_dtype in dtypes:\n+                total_count = torch.tensor([10, 10], dtype=count_dtype)\n+                total_prob = torch.tensor([0.5, 0.5], dtype=prob_dtype)\n+\n+                with self.assertRaisesRegex(\n+                    RuntimeError,\n+                    \"binomial only supports floating-point dtypes for .*\",",
        "comment_created_at": "2025-07-08T05:14:02+00:00",
        "comment_author": "michellemadubuike",
        "comment_body": "I have made the fix in the new commit",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2122382389",
    "pr_number": 154698,
    "pr_file": "test/inductor/test_codecache.py",
    "created_at": "2025-06-03T00:35:23+00:00",
    "commented_code": "self.assertEqual(counters[\"inductor\"][\"fxgraph_cache_hit\"], 1)\n \n+    @config.patch({\"fx_graph_cache\": True})\n+    @config.patch({\"fx_graph_remote_cache\": False})\n+    @functorch_config.patch({\"enable_autograd_cache\": True})\n+    @parametrize(\"device\", (GPU_TYPE, \"cpu\"))\n+    @parametrize(\"format\", (\"binary\", \"unpacked\"))\n+    @parametrize(\"dynamic\", (False, True))\n+    def test_basic(self, device: str, format: str, dynamic: bool) -> None:\n+        self._test_basic(device, format, dynamic)\n+\n+    @config.patch({\"fx_graph_cache\": True})\n+    @config.patch({\"fx_graph_remote_cache\": False})\n+    @functorch_config.patch({\"enable_autograd_cache\": True})\n+    @parametrize(\"device\", (GPU_TYPE, \"cpu\"))\n+    @parametrize(\"format\", (\"binary\", \"unpacked\"))\n+    @parametrize(\"dynamic\", (False, True))\n+    @config.patch(\"graph_partition\", True)\n+    def test_basic_with_graph_partition(\n+        self, device: str, format: str, dynamic: bool\n+    ) -> None:\n+        self._test_basic(device, format, dynamic)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2122382389",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154698,
        "pr_file": "test/inductor/test_codecache.py",
        "discussion_id": "2122382389",
        "commented_code": "@@ -1596,6 +1590,27 @@ def f(x):\n \n             self.assertEqual(counters[\"inductor\"][\"fxgraph_cache_hit\"], 1)\n \n+    @config.patch({\"fx_graph_cache\": True})\n+    @config.patch({\"fx_graph_remote_cache\": False})\n+    @functorch_config.patch({\"enable_autograd_cache\": True})\n+    @parametrize(\"device\", (GPU_TYPE, \"cpu\"))\n+    @parametrize(\"format\", (\"binary\", \"unpacked\"))\n+    @parametrize(\"dynamic\", (False, True))\n+    def test_basic(self, device: str, format: str, dynamic: bool) -> None:\n+        self._test_basic(device, format, dynamic)\n+\n+    @config.patch({\"fx_graph_cache\": True})\n+    @config.patch({\"fx_graph_remote_cache\": False})\n+    @functorch_config.patch({\"enable_autograd_cache\": True})\n+    @parametrize(\"device\", (GPU_TYPE, \"cpu\"))\n+    @parametrize(\"format\", (\"binary\", \"unpacked\"))\n+    @parametrize(\"dynamic\", (False, True))\n+    @config.patch(\"graph_partition\", True)\n+    def test_basic_with_graph_partition(\n+        self, device: str, format: str, dynamic: bool\n+    ) -> None:\n+        self._test_basic(device, format, dynamic)",
        "comment_created_at": "2025-06-03T00:35:23+00:00",
        "comment_author": "oulgen",
        "comment_body": "instead of breaking this test into two tests, make graph partition be another boolean parameter, and when use `with config.patch()`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2070477835",
    "pr_number": 137400,
    "pr_file": "test/dynamo/test_misc.py",
    "created_at": "2025-05-01T16:25:42+00:00",
    "commented_code": "opt = torch.compile(fn, backend=\"eager\")\n         opt()\n \n-    def test_tracing_py_tree(self):\n-        def fn(xs):\n-            flat_xs, spec = python_pytree.tree_flatten(xs)\n-            res = [x.clone() for x in flat_xs]\n-            return python_pytree.tree_unflatten(res, spec)\n-\n-        xs = [torch.tensor(i) for i in range(3)]\n-\n-        counter = CompileCounter()\n-        torch.compile(fn, backend=counter, fullgraph=True)(xs)\n-        self.assertEqual(counter.frame_count, 1)\n-        self.assertEqual(counter.op_count, 3)\n-\n-    def test_tracing_nested_py_tree(self):\n-        def fn(xs):\n-            flat_xs, spec = python_pytree.tree_flatten(xs)\n-            res = [x.clone() for x in flat_xs]\n-            return python_pytree.tree_unflatten(res, spec)\n-\n-        xs = [torch.tensor(i) for i in range(3)]\n-        xsl = [xs, xs, xs, xs]\n-\n-        counter = CompileCounter()\n-        comp_out = torch.compile(fn, backend=counter, fullgraph=True)(xsl)\n-        real_out = fn(xsl)\n-        self.assertEqual(comp_out, real_out)\n-        self.assertEqual(counter.frame_count, 1)\n-        self.assertEqual(counter.op_count, 12)\n-\n-    def test_tracing_nested_py_tree_tuples(self):\n-        def fn(xs):\n-            flat_xs, spec = python_pytree.tree_flatten(xs)\n-            res = [x.clone() for x in flat_xs]\n-            return python_pytree.tree_unflatten(res, spec)\n-\n-        xs = [torch.tensor(i) for i in range(3)]\n-        xsl = (xs, xs, xs, xs)\n-\n-        counter = CompileCounter()\n-        comp_out = torch.compile(fn, backend=counter, fullgraph=True)(xsl)\n-        real_out = fn(xsl)\n-        self.assertEqual(comp_out, real_out)\n-        self.assertEqual(counter.frame_count, 1)\n-        self.assertEqual(counter.op_count, 12)\n-\n-    def test_tracing_nested_py_tree_dicts(self):\n-        def fn(xs):\n-            flat_xs, spec = python_pytree.tree_flatten(xs)\n-            res = [x.clone() for x in flat_xs]\n-            return python_pytree.tree_unflatten(res, spec)\n-\n-        xs = [torch.tensor(i) for i in range(3)]\n-        xsl = {\n-            \"a\": xs,\n-            \"b\": xs,\n-            \"c\": xs,\n-        }\n+    def test_tracing_pytree(self):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2070477835",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 137400,
        "pr_file": "test/dynamo/test_misc.py",
        "discussion_id": "2070477835",
        "commented_code": "@@ -8660,70 +8668,82 @@ def fn():\n         opt = torch.compile(fn, backend=\"eager\")\n         opt()\n \n-    def test_tracing_py_tree(self):\n-        def fn(xs):\n-            flat_xs, spec = python_pytree.tree_flatten(xs)\n-            res = [x.clone() for x in flat_xs]\n-            return python_pytree.tree_unflatten(res, spec)\n-\n-        xs = [torch.tensor(i) for i in range(3)]\n-\n-        counter = CompileCounter()\n-        torch.compile(fn, backend=counter, fullgraph=True)(xs)\n-        self.assertEqual(counter.frame_count, 1)\n-        self.assertEqual(counter.op_count, 3)\n-\n-    def test_tracing_nested_py_tree(self):\n-        def fn(xs):\n-            flat_xs, spec = python_pytree.tree_flatten(xs)\n-            res = [x.clone() for x in flat_xs]\n-            return python_pytree.tree_unflatten(res, spec)\n-\n-        xs = [torch.tensor(i) for i in range(3)]\n-        xsl = [xs, xs, xs, xs]\n-\n-        counter = CompileCounter()\n-        comp_out = torch.compile(fn, backend=counter, fullgraph=True)(xsl)\n-        real_out = fn(xsl)\n-        self.assertEqual(comp_out, real_out)\n-        self.assertEqual(counter.frame_count, 1)\n-        self.assertEqual(counter.op_count, 12)\n-\n-    def test_tracing_nested_py_tree_tuples(self):\n-        def fn(xs):\n-            flat_xs, spec = python_pytree.tree_flatten(xs)\n-            res = [x.clone() for x in flat_xs]\n-            return python_pytree.tree_unflatten(res, spec)\n-\n-        xs = [torch.tensor(i) for i in range(3)]\n-        xsl = (xs, xs, xs, xs)\n-\n-        counter = CompileCounter()\n-        comp_out = torch.compile(fn, backend=counter, fullgraph=True)(xsl)\n-        real_out = fn(xsl)\n-        self.assertEqual(comp_out, real_out)\n-        self.assertEqual(counter.frame_count, 1)\n-        self.assertEqual(counter.op_count, 12)\n-\n-    def test_tracing_nested_py_tree_dicts(self):\n-        def fn(xs):\n-            flat_xs, spec = python_pytree.tree_flatten(xs)\n-            res = [x.clone() for x in flat_xs]\n-            return python_pytree.tree_unflatten(res, spec)\n-\n-        xs = [torch.tensor(i) for i in range(3)]\n-        xsl = {\n-            \"a\": xs,\n-            \"b\": xs,\n-            \"c\": xs,\n-        }\n+    def test_tracing_pytree(self):",
        "comment_created_at": "2025-05-01T16:25:42+00:00",
        "comment_author": "angelayi",
        "comment_body": "Maybe you can use something like [`parametrize`](https://github.com/pytorch/pytorch/blob/4c8dee7986d0da5cd8485b8d84323c425d228891/test/dynamo/test_export.py#L2206) to loop over the different pytree modules?",
        "pr_file_module": null
      }
    ]
  }
]