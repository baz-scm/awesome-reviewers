[
  {
    "discussion_id": "1939920168",
    "pr_number": 145000,
    "pr_file": "aten/src/ATen/DLConvertor.h",
    "created_at": "2025-02-03T19:32:11+00:00",
    "commented_code": "namespace at {\n \n TORCH_API ScalarType toScalarType(const DLDataType& dtype);\n-TORCH_API DLManagedTensor* toDLPack(const Tensor& src);\n-TORCH_API Tensor fromDLPack(DLManagedTensor* src);\n-TORCH_API Tensor\n-fromDLPack(DLManagedTensor* src, std::function<void(void*)> deleter);\n+TORCH_API DLManagedTensorVersioned* toDLPack(const Tensor& src);",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1939920168",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 145000,
        "pr_file": "aten/src/ATen/DLConvertor.h",
        "discussion_id": "1939920168",
        "commented_code": "@@ -11,11 +11,50 @@\n namespace at {\n \n TORCH_API ScalarType toScalarType(const DLDataType& dtype);\n-TORCH_API DLManagedTensor* toDLPack(const Tensor& src);\n-TORCH_API Tensor fromDLPack(DLManagedTensor* src);\n-TORCH_API Tensor\n-fromDLPack(DLManagedTensor* src, std::function<void(void*)> deleter);\n+TORCH_API DLManagedTensorVersioned* toDLPack(const Tensor& src);",
        "comment_created_at": "2025-02-03T19:32:11+00:00",
        "comment_author": "albanD",
        "comment_body": "Is this API used by C++ libraries? This would be a BC-breaking change for these users right?",
        "pr_file_module": null
      },
      {
        "comment_id": "1946617614",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 145000,
        "pr_file": "aten/src/ATen/DLConvertor.h",
        "discussion_id": "1939920168",
        "commented_code": "@@ -11,11 +11,50 @@\n namespace at {\n \n TORCH_API ScalarType toScalarType(const DLDataType& dtype);\n-TORCH_API DLManagedTensor* toDLPack(const Tensor& src);\n-TORCH_API Tensor fromDLPack(DLManagedTensor* src);\n-TORCH_API Tensor\n-fromDLPack(DLManagedTensor* src, std::function<void(void*)> deleter);\n+TORCH_API DLManagedTensorVersioned* toDLPack(const Tensor& src);",
        "comment_created_at": "2025-02-07T14:29:00+00:00",
        "comment_author": "ysiraichi",
        "comment_body": "I think it's being used by PyTorch/XLA. Yes, it's definitely BC-breaking. I was thinking that, since DLPack 1.0 should be the new default, the old version should have the name with a suffix. However, now that you brought this up, not being BC-breaking sounds more important.\r\n\r\nIn summary, I will change the names so that we are not BC-breaking.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2189799288",
    "pr_number": 149601,
    "pr_file": "c10/core/AllocatorConfig.h",
    "created_at": "2025-07-07T11:36:39+00:00",
    "commented_code": "+#pragma once\n+\n+#include <c10/core/DeviceType.h>\n+#include <c10/util/Exception.h>\n+#include <c10/util/llvmMathExtras.h>\n+\n+#include <atomic>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n+\n+namespace c10::CachingAllocator {\n+\n+// \"large\" allocations may be packed in 20 MiB blocks\n+const size_t kLargeBuffer = 20971520;\n+\n+// A utility class for tokenizing allocator configuration strings into discrete\n+// parts. For example, the config string:\n+//   \"key1:val1,key2:[val2,val3]\"\n+// is tokenized into:\n+//   \"key1\", \":\", \"val1\", \",\", \"key2\", \":\", \"[\", \"val2\", \",\", \"val3\", \"]\",\n+//\n+// Tokens include keys, values, and special characters (':', ',', '[', ']').\n+// Whitespace is ignored.\n+class C10_API ConfigTokenizer {",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2189799288",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 149601,
        "pr_file": "c10/core/AllocatorConfig.h",
        "discussion_id": "2189799288",
        "commented_code": "@@ -0,0 +1,337 @@\n+#pragma once\n+\n+#include <c10/core/DeviceType.h>\n+#include <c10/util/Exception.h>\n+#include <c10/util/llvmMathExtras.h>\n+\n+#include <atomic>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n+\n+namespace c10::CachingAllocator {\n+\n+// \"large\" allocations may be packed in 20 MiB blocks\n+const size_t kLargeBuffer = 20971520;\n+\n+// A utility class for tokenizing allocator configuration strings into discrete\n+// parts. For example, the config string:\n+//   \"key1:val1,key2:[val2,val3]\"\n+// is tokenized into:\n+//   \"key1\", \":\", \"val1\", \",\", \"key2\", \":\", \"[\", \"val2\", \",\", \"val3\", \"]\",\n+//\n+// Tokens include keys, values, and special characters (':', ',', '[', ']').\n+// Whitespace is ignored.\n+class C10_API ConfigTokenizer {",
        "comment_created_at": "2025-07-07T11:36:39+00:00",
        "comment_author": "albanD",
        "comment_body": "Do we really need this to be public API? Which context do you expect this to be used?",
        "pr_file_module": null
      },
      {
        "comment_id": "2191349861",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 149601,
        "pr_file": "c10/core/AllocatorConfig.h",
        "discussion_id": "2189799288",
        "commented_code": "@@ -0,0 +1,337 @@\n+#pragma once\n+\n+#include <c10/core/DeviceType.h>\n+#include <c10/util/Exception.h>\n+#include <c10/util/llvmMathExtras.h>\n+\n+#include <atomic>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n+\n+namespace c10::CachingAllocator {\n+\n+// \"large\" allocations may be packed in 20 MiB blocks\n+const size_t kLargeBuffer = 20971520;\n+\n+// A utility class for tokenizing allocator configuration strings into discrete\n+// parts. For example, the config string:\n+//   \"key1:val1,key2:[val2,val3]\"\n+// is tokenized into:\n+//   \"key1\", \":\", \"val1\", \",\", \"key2\", \":\", \"[\", \"val2\", \",\", \"val3\", \"]\",\n+//\n+// Tokens include keys, values, and special characters (':', ',', '[', ']').\n+// Whitespace is ignored.\n+class C10_API ConfigTokenizer {",
        "comment_created_at": "2025-07-08T02:29:47+00:00",
        "comment_author": "guangyey",
        "comment_body": "`ConfigTokenizer` is intended to be reused by other accelerator backends, such as `CUDAAllocatorConfig` or out-of-tree allocator configurations. Let's remove `C10_API` for now, and reintroduce it later only if necessary.",
        "pr_file_module": null
      }
    ]
  }
]