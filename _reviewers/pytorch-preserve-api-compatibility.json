[
  {
    "discussion_id": "2101258835",
    "pr_number": 154042,
    "pr_file": "torch/csrc/cuda/CUDAPluggableAllocator.h",
    "created_at": "2025-05-21T22:11:21+00:00",
    "commented_code": "bool initialized() override;\n  double getMemoryFraction(c10::DeviceIndex device) override;\n  void setMemoryFraction(double fraction, c10::DeviceIndex device) override;\n  void emptyCache() override;\n  void emptyCache(c10::cuda::MempoolId_t) override;",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2101258835",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154042,
        "pr_file": "torch/csrc/cuda/CUDAPluggableAllocator.h",
        "discussion_id": "2101258835",
        "commented_code": "@@ -114,7 +114,7 @@ struct TORCH_CUDA_CPP_API CUDAPluggableAllocator\n   bool initialized() override;\n   double getMemoryFraction(c10::DeviceIndex device) override;\n   void setMemoryFraction(double fraction, c10::DeviceIndex device) override;\n-  void emptyCache() override;\n+  void emptyCache(c10::cuda::MempoolId_t) override;",
        "comment_created_at": "2025-05-21T22:11:21+00:00",
        "comment_author": "albanD",
        "comment_body": "Are these public APIs? Should we add default value to not BC-break?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1939920168",
    "pr_number": 145000,
    "pr_file": "aten/src/ATen/DLConvertor.h",
    "created_at": "2025-02-03T19:32:11+00:00",
    "commented_code": "namespace at {\n\nTORCH_API ScalarType toScalarType(const DLDataType& dtype);\nTORCH_API DLManagedTensor* toDLPack(const Tensor& src);\nTORCH_API Tensor fromDLPack(DLManagedTensor* src);\nTORCH_API Tensor\nfromDLPack(DLManagedTensor* src, std::function<void(void*)> deleter);\nTORCH_API DLManagedTensorVersioned* toDLPack(const Tensor& src);",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1939920168",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 145000,
        "pr_file": "aten/src/ATen/DLConvertor.h",
        "discussion_id": "1939920168",
        "commented_code": "@@ -11,11 +11,50 @@\n namespace at {\n \n TORCH_API ScalarType toScalarType(const DLDataType& dtype);\n-TORCH_API DLManagedTensor* toDLPack(const Tensor& src);\n-TORCH_API Tensor fromDLPack(DLManagedTensor* src);\n-TORCH_API Tensor\n-fromDLPack(DLManagedTensor* src, std::function<void(void*)> deleter);\n+TORCH_API DLManagedTensorVersioned* toDLPack(const Tensor& src);",
        "comment_created_at": "2025-02-03T19:32:11+00:00",
        "comment_author": "albanD",
        "comment_body": "Is this API used by C++ libraries? This would be a BC-breaking change for these users right?",
        "pr_file_module": null
      },
      {
        "comment_id": "1946617614",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 145000,
        "pr_file": "aten/src/ATen/DLConvertor.h",
        "discussion_id": "1939920168",
        "commented_code": "@@ -11,11 +11,50 @@\n namespace at {\n \n TORCH_API ScalarType toScalarType(const DLDataType& dtype);\n-TORCH_API DLManagedTensor* toDLPack(const Tensor& src);\n-TORCH_API Tensor fromDLPack(DLManagedTensor* src);\n-TORCH_API Tensor\n-fromDLPack(DLManagedTensor* src, std::function<void(void*)> deleter);\n+TORCH_API DLManagedTensorVersioned* toDLPack(const Tensor& src);",
        "comment_created_at": "2025-02-07T14:29:00+00:00",
        "comment_author": "ysiraichi",
        "comment_body": "I think it's being used by PyTorch/XLA. Yes, it's definitely BC-breaking. I was thinking that, since DLPack 1.0 should be the new default, the old version should have the name with a suffix. However, now that you brought this up, not being BC-breaking sounds more important.\r\n\r\nIn summary, I will change the names so that we are not BC-breaking.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1939924463",
    "pr_number": 145000,
    "pr_file": "torch/csrc/Module.cpp",
    "created_at": "2025-02-03T19:35:59+00:00",
    "commented_code": "METH_NOARGS,\n     nullptr},\n    {\"_to_dlpack\", THPModule_toDLPack, METH_O, nullptr},\n    {\"_to_dlpack_unversioned\", THPModule_toDLPackUnversioned, METH_O, nullptr},",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1939924463",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 145000,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "1939924463",
        "commented_code": "@@ -1599,6 +1616,7 @@ static std::initializer_list<PyMethodDef> TorchMethods = {\n      METH_NOARGS,\n      nullptr},\n     {\"_to_dlpack\", THPModule_toDLPack, METH_O, nullptr},\n+    {\"_to_dlpack_unversioned\", THPModule_toDLPackUnversioned, METH_O, nullptr},",
        "comment_created_at": "2025-02-03T19:35:59+00:00",
        "comment_author": "albanD",
        "comment_body": "Why do we still need this one?",
        "pr_file_module": null
      },
      {
        "comment_id": "1941463598",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 145000,
        "pr_file": "torch/csrc/Module.cpp",
        "discussion_id": "1939924463",
        "commented_code": "@@ -1599,6 +1616,7 @@ static std::initializer_list<PyMethodDef> TorchMethods = {\n      METH_NOARGS,\n      nullptr},\n     {\"_to_dlpack\", THPModule_toDLPack, METH_O, nullptr},\n+    {\"_to_dlpack_unversioned\", THPModule_toDLPackUnversioned, METH_O, nullptr},",
        "comment_created_at": "2025-02-04T16:05:23+00:00",
        "comment_author": "rgommers",
        "comment_body": "This does look necessary to me. For DLPack to change the ABI once, there's a dance that needs doing to be not-super-disruptive: continue returning the old (0.8) version, unless the consumer indicates it can handle the new (1.X) version by passing in `max_version=(1, 0)` (or `(1, x)` in the future).",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1978540718",
    "pr_number": 147095,
    "pr_file": "torch/csrc/jit/serialization/pickler.h",
    "created_at": "2025-03-04T03:17:50+00:00",
    "commented_code": "void(const at::Tensor&, std::unordered_map<std::string, bool>&)>;\n\n// A allowlist of device type, currently available is PrivateUse1\ninline std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist() {\n  static std::unordered_set<c10::DeviceType> DeviceTypeAllowlist{\n      c10::DeviceType::PrivateUse1};\n  return DeviceTypeAllowlist;\n}\nTORCH_API std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist();\n\n// Dynamically obtain serialization function pairs\n// that require the corresponding backend.\ninline std::array<\nTORCH_API std::array<",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1978540718",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 147095,
        "pr_file": "torch/csrc/jit/serialization/pickler.h",
        "discussion_id": "1978540718",
        "commented_code": "@@ -299,27 +299,14 @@ using BackendMetaPtr = std::function<\n     void(const at::Tensor&, std::unordered_map<std::string, bool>&)>;\n \n // A allowlist of device type, currently available is PrivateUse1\n-inline std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist() {\n-  static std::unordered_set<c10::DeviceType> DeviceTypeAllowlist{\n-      c10::DeviceType::PrivateUse1};\n-  return DeviceTypeAllowlist;\n-}\n+TORCH_API std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist();\n \n // Dynamically obtain serialization function pairs\n // that require the corresponding backend.\n-inline std::array<\n+TORCH_API std::array<",
        "comment_created_at": "2025-03-04T03:17:50+00:00",
        "comment_author": "cyyever",
        "comment_body": "Why added TORCH_API?",
        "pr_file_module": null
      },
      {
        "comment_id": "1997886240",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 147095,
        "pr_file": "torch/csrc/jit/serialization/pickler.h",
        "discussion_id": "1978540718",
        "commented_code": "@@ -299,27 +299,14 @@ using BackendMetaPtr = std::function<\n     void(const at::Tensor&, std::unordered_map<std::string, bool>&)>;\n \n // A allowlist of device type, currently available is PrivateUse1\n-inline std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist() {\n-  static std::unordered_set<c10::DeviceType> DeviceTypeAllowlist{\n-      c10::DeviceType::PrivateUse1};\n-  return DeviceTypeAllowlist;\n-}\n+TORCH_API std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist();\n \n // Dynamically obtain serialization function pairs\n // that require the corresponding backend.\n-inline std::array<\n+TORCH_API std::array<",
        "comment_created_at": "2025-03-17T03:39:20+00:00",
        "comment_author": "FFFrog",
        "comment_body": "Sorry for the late reply.\r\nï»¿\r\nGetBackendMetaSerialization can be called by some inline function like [setTensorMetadata](https://github.com/pytorch/pytorch/blob/916e8979d3e0d651a9091732ce3e59da32e72b0e/torch/csrc/jit/serialization/pickler.h#L386), which can be called in other [.cpp](https://github.com/pytorch/pytorch/blob/916e8979d3e0d651a9091732ce3e59da32e72b0e/torch/csrc/Module.cpp#L2438) and the cpp can be built into another libtorch_python.so, therefore we need to add TORCH_API for GetBackendMetaSerialization to make it visible for other .so.",
        "pr_file_module": null
      },
      {
        "comment_id": "1997956969",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 147095,
        "pr_file": "torch/csrc/jit/serialization/pickler.h",
        "discussion_id": "1978540718",
        "commented_code": "@@ -299,27 +299,14 @@ using BackendMetaPtr = std::function<\n     void(const at::Tensor&, std::unordered_map<std::string, bool>&)>;\n \n // A allowlist of device type, currently available is PrivateUse1\n-inline std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist() {\n-  static std::unordered_set<c10::DeviceType> DeviceTypeAllowlist{\n-      c10::DeviceType::PrivateUse1};\n-  return DeviceTypeAllowlist;\n-}\n+TORCH_API std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist();\n \n // Dynamically obtain serialization function pairs\n // that require the corresponding backend.\n-inline std::array<\n+TORCH_API std::array<",
        "comment_created_at": "2025-03-17T05:29:58+00:00",
        "comment_author": "cyyever",
        "comment_body": "It's not need in your description, it's just an inner function. What error did you encounter without TORCH_API? Post it? ",
        "pr_file_module": null
      },
      {
        "comment_id": "1998032914",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 147095,
        "pr_file": "torch/csrc/jit/serialization/pickler.h",
        "discussion_id": "1978540718",
        "commented_code": "@@ -299,27 +299,14 @@ using BackendMetaPtr = std::function<\n     void(const at::Tensor&, std::unordered_map<std::string, bool>&)>;\n \n // A allowlist of device type, currently available is PrivateUse1\n-inline std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist() {\n-  static std::unordered_set<c10::DeviceType> DeviceTypeAllowlist{\n-      c10::DeviceType::PrivateUse1};\n-  return DeviceTypeAllowlist;\n-}\n+TORCH_API std::unordered_set<c10::DeviceType>& GetBackendMetaAllowlist();\n \n // Dynamically obtain serialization function pairs\n // that require the corresponding backend.\n-inline std::array<\n+TORCH_API std::array<",
        "comment_created_at": "2025-03-17T06:37:30+00:00",
        "comment_author": "FFFrog",
        "comment_body": "![image](https://github.com/user-attachments/assets/6c8814d2-20b3-4029-9db7-9108a3c71378)\r\n\r\nThe **pickler.h** file is included by **Module.cpp**, so the function `getTensorMetadata` will exist in Module.cpp (although it is marked as inline, the compiler will probably treat it as a normal function, but to avoid symbol conflicts, it is additionally marked as a weak symbol). Then `getTensorMetadata` calls `GetBackendMetaSerialization` internally, and the default hidden behavior makes the symbol invisible.",
        "pr_file_module": null
      }
    ]
  }
]