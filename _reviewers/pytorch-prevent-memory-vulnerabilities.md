---
title: Prevent memory vulnerabilities
description: Ensure code is protected against memory-related security vulnerabilities
  by properly handling type conversions and resource management. When working with
  different integer widths (e.g., mixing 32-bit and 64-bit values), explicitly cast
  to the wider type before performing operations to prevent integer overflow. Similarly,
  when working with external APIs that...
repository: pytorch/pytorch
label: Security
language: Other
comments_count: 2
repository_stars: 91169
---

Ensure code is protected against memory-related security vulnerabilities by properly handling type conversions and resource management. When working with different integer widths (e.g., mixing 32-bit and 64-bit values), explicitly cast to the wider type before performing operations to prevent integer overflow. Similarly, when working with external APIs that require manual memory management (like Python C API), either use RAII wrappers or explicitly handle resource acquisition and release.

Example 1 - Safe integer width handling:
```cpp
// Unsafe: May cause integer overflow if large values
int64_t offset = blockIdx.y * rows_per_block_y;

// Safe: Explicit cast to int64_t prevents overflow
int64_t offset = static_cast<int64_t>(blockIdx.y) * rows_per_block_y;
```

Example 2 - Safe memory management:
```cpp
// Unsafe: No reference management for PyObject
PyObject* dict = ...;

// Safe option 1: Use RAII wrapper
py::object dict = ...;

// Safe option 2: Explicit reference management
PyObject* dict = ...;
// Use dict
Py_XDECREF(dict);
```


[
  {
    "discussion_id": "2012679126",
    "pr_number": 148605,
    "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
    "created_at": "2025-03-25T18:10:26+00:00",
    "commented_code": "}\n}\n\n\ntemplate <typename T, typename T_ACC>\n__global__ void GammaBetaBackwardSimpleCUDAKernel(\ntemplate <typename T, typename T_ACC,\nunsigned int block_dim_x,\nunsigned int block_dim_y,\nunsigned int rows_per_block_y,\nbool check_x,\nbool check_y>\n__device__\n__forceinline__\nvoid\nblockReduceGammaBetaBackwardsHelper(\n    int64_t M_start,\n    int64_t M,\n    int64_t N,\n    const T* dY,\n    const T* X,\n    const T_ACC* mean,\n    const T_ACC* rstd,\n    T* dg,\n    T* db) {\n  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (j < N) {\n    T_ACC sum1 = 0;\n    T_ACC sum2 = 0;\n    for (int64_t i = 0; i < M; ++i) {\n      const int64_t index = i * N + j;\n      sum1 += dg == nullptr ? T_ACC(0)\n                            : static_cast<T_ACC>(dY[index]) *\n              (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n              static_cast<T_ACC>(rstd[i]);\n      sum2 += db == nullptr ? T_ACC(0) : static_cast<T_ACC>(dY[index]);\n    const T* __restrict__ dY,\n    const T* __restrict__ X,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T* __restrict__ dg,\n    T* __restrict__ db,\n    T_ACC &dg_sum,\n    T_ACC &db_sum\n) {\n  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n\n    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n    T_ACC warp_mean = 0, warp_rstd = 0;\n    if (lane_id < rows_per_thread_y && mean_index + lane_id < M) {\n      warp_mean = mean[mean_index + lane_id];\n      warp_rstd = rstd[mean_index + lane_id];\n    }\n    if (dg != nullptr) {\n      dg[j] = sum1;\n    // We do a WARP_SYNC() here because we use WARP_SHFL below to access\n    // warp_mean and warp_rstd.\n    WARP_SYNC();\n\n    T_ACC dY_regs[rows_per_thread_y] = {0};\n    T_ACC X_regs[rows_per_thread_y] = {0};\n    #pragma unroll\n    for (int i = 0; i < rows_per_thread_y; ++i) {\n      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n      bool active = true;\n      if (check_x && thread_x >= N) {\n        active = false;\n      }\n      if (check_y && current_y >= M) {\n        active = false;\n      }\n      if (active) {\n        dY_regs[i] = dY[current_y * N + thread_x];\n        X_regs[i] = X[current_y * N + thread_x];\n      }\n    }\n    if (db != nullptr) {\n      db[j] = sum2;\n\n    #pragma unroll\n    for (int i = 0; i < rows_per_thread_y; ++i) {\n      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n      db_sum += dY_regs[i];\n    }\n  }\n}\n\n// This implementation gets called if M and N divide with 32. This case should\n// be the most common. We can then make better use of warp level intrinsics\n// to improve performance.\ntemplate <typename T, typename T_ACC,\nunsigned int block_dim_x,\nunsigned int block_dim_y,\nunsigned int rows_per_block_y,\nbool check_x,\nbool check_y>\n__device__\n__forceinline__\nvoid\nblockReduceGammaBetaBackwardsWithChecks(\n    int64_t M,\n    int64_t N,\n    const T* __restrict__ dY,\n    const T* __restrict__ X,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T* __restrict__ dg,\n    T* __restrict__ db,\n    T_ACC &dg_sum,\n    T_ACC &db_sum\n) {\n  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n        M_start < M;\n        M_start += rows_per_block_y * gridDim.y) {\n    int64_t M_end = M_start + rows_per_block_y - 1;\n    if (!check_y || M_end < M) {\n      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, false>\n      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    } else {\n      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, true>\n      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    }\n  }\n}\n\ntemplate <typename T, typename T_ACC>\n__global__ void GammaBetaBackwardCUDAKernel_32x32(\n// block_dim_x is the number of threads in the x dimension per block.\n// block_dim_y is the number of threads in the y dimension per block.\n// rows_per_block_y is the size of the tile (number of data elements)\n// in the y dimension per block.\n// partial_reduction indicates whether we need to reduce across threads\n// or not. If set to true, we will not reduce across threads. This can\n// be faster in the M >> N case but requires another kernel to do a full\n// final reduction.\n// aligned_grid means the data size is a multiple of tile size. In that\n// case we don't need to check for boundary conditions which can provide\n// a further speedup by not needing instructions to check for edge cases\n// and not needing predicate registers.\ntemplate <typename T, typename T_ACC,\nunsigned int block_dim_x, unsigned int block_dim_y,\nunsigned int rows_per_block_y,\nbool partial_reduction,\nbool aligned_grid\n>\n__global__\nvoid\n GammaBetaBackwardCUDAKernelTemplate(\n    int64_t M,\n    int64_t N,\n    const T* dY,\n    const T* X,\n    const T_ACC* mean,\n    const T_ACC* rstd,\n    T* dg,\n    T* db) {\n  alignas(sizeof(double)) extern __shared__ char s_data1[];\n  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n  T_ACC* s_dg;\n  T_ACC* s_db;\n    const T* __restrict__ dY,\n    const T* __restrict__ X,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T* __restrict__ dg,\n    T* __restrict__ db) {\n  // This assert is a compile-time check only.\n  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n  static_assert(rows_per_thread_y <= kWarpSize);\n\n  T_ACC dg_sum = 0;\n  T_ACC db_sum = 0;\n\n  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n\n  if (j < N) {\n    constexpr int unroll_factor = 8;\n    int laneId = threadIdx.x & (C10_WARP_SIZE - 1);\n\n    T_ACC mean_reg, mean_reg_tmp;\n    T_ACC rstd_reg, rstd_reg_tmp;\n    T dY_reg;\n    T X_reg;\n\n    // Main loop\n    int bcounter;\n    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor);\n         bcounter++) {\n      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n  if (aligned_grid) {\n    // When N and M align perfectly with block_dim_x and block_dim_y, we\n    // can skip boundary condition checks that waste instruction issue slots.\n    blockReduceGammaBetaBackwardsWithChecks\n          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, false>\n          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n  } else {\n    // In the general case we need to check boundary conditions in the M\n    // dimension. However, we can still avoid boundary checks in the N dimension\n    // for the inner blocks. So try to avoid those checks when possible.\n    if (blockIdx.x * block_dim_x + block_dim_x - 1 < N) {\n      blockReduceGammaBetaBackwardsWithChecks\n          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, true>\n          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    } else {\n      blockReduceGammaBetaBackwardsWithChecks\n          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true, true>\n          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    }\n  }\n\n      if (laneId < unroll_factor) {\n        mean_reg_tmp = mean[offset + laneId];\n        rstd_reg_tmp = rstd[offset + laneId];\n      }\n      WARP_SYNC();\n  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2012679126",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 148605,
        "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
        "discussion_id": "2012679126",
        "commented_code": "@@ -508,223 +509,362 @@ __global__ void layer_norm_grad_input_kernel_vectorized(\n   }\n }\n \n-\n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardSimpleCUDAKernel(\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsHelper(\n+    int64_t M_start,\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-  if (j < N) {\n-    T_ACC sum1 = 0;\n-    T_ACC sum2 = 0;\n-    for (int64_t i = 0; i < M; ++i) {\n-      const int64_t index = i * N + j;\n-      sum1 += dg == nullptr ? T_ACC(0)\n-                            : static_cast<T_ACC>(dY[index]) *\n-              (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n-              static_cast<T_ACC>(rstd[i]);\n-      sum2 += db == nullptr ? T_ACC(0) : static_cast<T_ACC>(dY[index]);\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n+\n+    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n+    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n+    T_ACC warp_mean = 0, warp_rstd = 0;\n+    if (lane_id < rows_per_thread_y && mean_index + lane_id < M) {\n+      warp_mean = mean[mean_index + lane_id];\n+      warp_rstd = rstd[mean_index + lane_id];\n     }\n-    if (dg != nullptr) {\n-      dg[j] = sum1;\n+    // We do a WARP_SYNC() here because we use WARP_SHFL below to access\n+    // warp_mean and warp_rstd.\n+    WARP_SYNC();\n+\n+    T_ACC dY_regs[rows_per_thread_y] = {0};\n+    T_ACC X_regs[rows_per_thread_y] = {0};\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n+      bool active = true;\n+      if (check_x && thread_x >= N) {\n+        active = false;\n+      }\n+      if (check_y && current_y >= M) {\n+        active = false;\n+      }\n+      if (active) {\n+        dY_regs[i] = dY[current_y * N + thread_x];\n+        X_regs[i] = X[current_y * N + thread_x];\n+      }\n     }\n-    if (db != nullptr) {\n-      db[j] = sum2;\n+\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n+      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n+      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n+      db_sum += dY_regs[i];\n     }\n-  }\n }\n \n-// This implementation gets called if M and N divide with 32. This case should\n-// be the most common. We can then make better use of warp level intrinsics\n-// to improve performance.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsWithChecks(\n+    int64_t M,\n+    int64_t N,\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n+        M_start < M;\n+        M_start += rows_per_block_y * gridDim.y) {\n+    int64_t M_end = M_start + rows_per_block_y - 1;\n+    if (!check_y || M_end < M) {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, false>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, true>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    }\n+  }\n+}\n \n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardCUDAKernel_32x32(\n+// block_dim_x is the number of threads in the x dimension per block.\n+// block_dim_y is the number of threads in the y dimension per block.\n+// rows_per_block_y is the size of the tile (number of data elements)\n+// in the y dimension per block.\n+// partial_reduction indicates whether we need to reduce across threads\n+// or not. If set to true, we will not reduce across threads. This can\n+// be faster in the M >> N case but requires another kernel to do a full\n+// final reduction.\n+// aligned_grid means the data size is a multiple of tile size. In that\n+// case we don't need to check for boundary conditions which can provide\n+// a further speedup by not needing instructions to check for edge cases\n+// and not needing predicate registers.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x, unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool partial_reduction,\n+bool aligned_grid\n+>\n+__global__\n+void\n+ GammaBetaBackwardCUDAKernelTemplate(\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  alignas(sizeof(double)) extern __shared__ char s_data1[];\n-  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n-  T_ACC* s_dg;\n-  T_ACC* s_db;\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db) {\n+  // This assert is a compile-time check only.\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  static_assert(rows_per_thread_y <= kWarpSize);\n \n   T_ACC dg_sum = 0;\n   T_ACC db_sum = 0;\n \n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-\n-  if (j < N) {\n-    constexpr int unroll_factor = 8;\n-    int laneId = threadIdx.x & (C10_WARP_SIZE - 1);\n-\n-    T_ACC mean_reg, mean_reg_tmp;\n-    T_ACC rstd_reg, rstd_reg_tmp;\n-    T dY_reg;\n-    T X_reg;\n-\n-    // Main loop\n-    int bcounter;\n-    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor);\n-         bcounter++) {\n-      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n+  if (aligned_grid) {\n+    // When N and M align perfectly with block_dim_x and block_dim_y, we\n+    // can skip boundary condition checks that waste instruction issue slots.\n+    blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, false>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+  } else {\n+    // In the general case we need to check boundary conditions in the M\n+    // dimension. However, we can still avoid boundary checks in the N dimension\n+    // for the inner blocks. So try to avoid those checks when possible.\n+    if (blockIdx.x * block_dim_x + block_dim_x - 1 < N) {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    }\n+  }\n \n-      if (laneId < unroll_factor) {\n-        mean_reg_tmp = mean[offset + laneId];\n-        rstd_reg_tmp = rstd[offset + laneId];\n-      }\n-      WARP_SYNC();\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;",
        "comment_created_at": "2025-03-25T18:10:26+00:00",
        "comment_author": "eqy",
        "comment_body": "nit: I think this is potentially unsafe if you intend this to do 64-bit indexing. Overflow will still happen as blockIDx.y and threadIdx.y are 32-bit wide, so one of the scalars here should be cast to the wider type first.",
        "pr_file_module": null
      },
      {
        "comment_id": "2012809564",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 148605,
        "pr_file": "aten/src/ATen/native/cuda/layer_norm_kernel.cu",
        "discussion_id": "2012679126",
        "commented_code": "@@ -508,223 +509,362 @@ __global__ void layer_norm_grad_input_kernel_vectorized(\n   }\n }\n \n-\n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardSimpleCUDAKernel(\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsHelper(\n+    int64_t M_start,\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-  if (j < N) {\n-    T_ACC sum1 = 0;\n-    T_ACC sum2 = 0;\n-    for (int64_t i = 0; i < M; ++i) {\n-      const int64_t index = i * N + j;\n-      sum1 += dg == nullptr ? T_ACC(0)\n-                            : static_cast<T_ACC>(dY[index]) *\n-              (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n-              static_cast<T_ACC>(rstd[i]);\n-      sum2 += db == nullptr ? T_ACC(0) : static_cast<T_ACC>(dY[index]);\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;\n+\n+    int lane_id = (threadIdx.y * blockDim.x + threadIdx.x) & (kWarpSize - 1);\n+    int64_t mean_index = M_start + threadIdx.y * rows_per_thread_y;\n+    T_ACC warp_mean = 0, warp_rstd = 0;\n+    if (lane_id < rows_per_thread_y && mean_index + lane_id < M) {\n+      warp_mean = mean[mean_index + lane_id];\n+      warp_rstd = rstd[mean_index + lane_id];\n     }\n-    if (dg != nullptr) {\n-      dg[j] = sum1;\n+    // We do a WARP_SYNC() here because we use WARP_SHFL below to access\n+    // warp_mean and warp_rstd.\n+    WARP_SYNC();\n+\n+    T_ACC dY_regs[rows_per_thread_y] = {0};\n+    T_ACC X_regs[rows_per_thread_y] = {0};\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      int64_t current_y = M_start + threadIdx.y * rows_per_thread_y + i;\n+      bool active = true;\n+      if (check_x && thread_x >= N) {\n+        active = false;\n+      }\n+      if (check_y && current_y >= M) {\n+        active = false;\n+      }\n+      if (active) {\n+        dY_regs[i] = dY[current_y * N + thread_x];\n+        X_regs[i] = X[current_y * N + thread_x];\n+      }\n     }\n-    if (db != nullptr) {\n-      db[j] = sum2;\n+\n+    #pragma unroll\n+    for (int i = 0; i < rows_per_thread_y; ++i) {\n+      T_ACC mean_reg = WARP_SHFL(warp_mean, i, kWarpSize);\n+      T_ACC rstd_reg = WARP_SHFL(warp_rstd, i, kWarpSize);\n+      dg_sum += dY_regs[i] * (X_regs[i] - mean_reg) * rstd_reg;\n+      db_sum += dY_regs[i];\n     }\n-  }\n }\n \n-// This implementation gets called if M and N divide with 32. This case should\n-// be the most common. We can then make better use of warp level intrinsics\n-// to improve performance.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x,\n+unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool check_x,\n+bool check_y>\n+__device__\n+__forceinline__\n+void\n+blockReduceGammaBetaBackwardsWithChecks(\n+    int64_t M,\n+    int64_t N,\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db,\n+    T_ACC &dg_sum,\n+    T_ACC &db_sum\n+) {\n+  for (int64_t M_start = blockIdx.y * rows_per_block_y;\n+        M_start < M;\n+        M_start += rows_per_block_y * gridDim.y) {\n+    int64_t M_end = M_start + rows_per_block_y - 1;\n+    if (!check_y || M_end < M) {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, false>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsHelper<T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, check_x, true>\n+      (M_start, M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    }\n+  }\n+}\n \n-template <typename T, typename T_ACC>\n-__global__ void GammaBetaBackwardCUDAKernel_32x32(\n+// block_dim_x is the number of threads in the x dimension per block.\n+// block_dim_y is the number of threads in the y dimension per block.\n+// rows_per_block_y is the size of the tile (number of data elements)\n+// in the y dimension per block.\n+// partial_reduction indicates whether we need to reduce across threads\n+// or not. If set to true, we will not reduce across threads. This can\n+// be faster in the M >> N case but requires another kernel to do a full\n+// final reduction.\n+// aligned_grid means the data size is a multiple of tile size. In that\n+// case we don't need to check for boundary conditions which can provide\n+// a further speedup by not needing instructions to check for edge cases\n+// and not needing predicate registers.\n+template <typename T, typename T_ACC,\n+unsigned int block_dim_x, unsigned int block_dim_y,\n+unsigned int rows_per_block_y,\n+bool partial_reduction,\n+bool aligned_grid\n+>\n+__global__\n+void\n+ GammaBetaBackwardCUDAKernelTemplate(\n     int64_t M,\n     int64_t N,\n-    const T* dY,\n-    const T* X,\n-    const T_ACC* mean,\n-    const T_ACC* rstd,\n-    T* dg,\n-    T* db) {\n-  alignas(sizeof(double)) extern __shared__ char s_data1[];\n-  T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n-  T_ACC* s_dg;\n-  T_ACC* s_db;\n+    const T* __restrict__ dY,\n+    const T* __restrict__ X,\n+    const T_ACC* __restrict__ mean,\n+    const T_ACC* __restrict__ rstd,\n+    T* __restrict__ dg,\n+    T* __restrict__ db) {\n+  // This assert is a compile-time check only.\n+  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n+  static_assert(rows_per_thread_y <= kWarpSize);\n \n   T_ACC dg_sum = 0;\n   T_ACC db_sum = 0;\n \n-  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n-\n-  if (j < N) {\n-    constexpr int unroll_factor = 8;\n-    int laneId = threadIdx.x & (C10_WARP_SIZE - 1);\n-\n-    T_ACC mean_reg, mean_reg_tmp;\n-    T_ACC rstd_reg, rstd_reg_tmp;\n-    T dY_reg;\n-    T X_reg;\n-\n-    // Main loop\n-    int bcounter;\n-    for (bcounter = 0; bcounter < M / (blockDim.y * unroll_factor);\n-         bcounter++) {\n-      int offset = (bcounter * blockDim.y + threadIdx.y) * unroll_factor;\n+  if (aligned_grid) {\n+    // When N and M align perfectly with block_dim_x and block_dim_y, we\n+    // can skip boundary condition checks that waste instruction issue slots.\n+    blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, false>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+  } else {\n+    // In the general case we need to check boundary conditions in the M\n+    // dimension. However, we can still avoid boundary checks in the N dimension\n+    // for the inner blocks. So try to avoid those checks when possible.\n+    if (blockIdx.x * block_dim_x + block_dim_x - 1 < N) {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    } else {\n+      blockReduceGammaBetaBackwardsWithChecks\n+          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true, true>\n+          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n+    }\n+  }\n \n-      if (laneId < unroll_factor) {\n-        mean_reg_tmp = mean[offset + laneId];\n-        rstd_reg_tmp = rstd[offset + laneId];\n-      }\n-      WARP_SYNC();\n+  int64_t thread_x = blockIdx.x * block_dim_x + threadIdx.x;",
        "comment_created_at": "2025-03-25T19:31:32+00:00",
        "comment_author": "ahmadsharif1",
        "comment_body": "Great point. Done!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2109841015",
    "pr_number": 153150,
    "pr_file": "torch/csrc/dynamo/guards.cpp",
    "created_at": "2025-05-27T18:06:14+00:00",
    "commented_code": "Py_ssize_t _index{-1};\n};\n\n/**\n * Represents set[index] accessor by converting the set into a dictionary.\n */\nclass SetGetItemGuardAccessor : public GuardAccessor {\n public:\n  SetGetItemGuardAccessor(\n      RootGuardManager* root,\n      const py::object& index,\n      std::string source,\n      py::handle example_value,\n      py::handle guard_manager_enum)\n      : GuardAccessor(\n            root,\n            index,\n            std::move(source),\n            example_value,\n            guard_manager_enum),\n        _index(py::cast<Py_ssize_t>(index)) {}\n\n  // NB: Intentional duplication between check_nopybind and\n  // check_verbose_nopybind.\n  bool check_nopybind(PyObject* obj, bool matches_dict_tag = false)\n      override { // borrowed ref\n    PyObject* PyDict = (PyObject*)&PyDict_Type;\n    PyObject* dict =",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2109841015",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153150,
        "pr_file": "torch/csrc/dynamo/guards.cpp",
        "discussion_id": "2109841015",
        "commented_code": "@@ -4117,6 +4141,85 @@ class ListGetItemGuardAccessor : public GuardAccessor {\n   Py_ssize_t _index{-1};\n };\n \n+/**\n+ * Represents set[index] accessor by converting the set into a dictionary.\n+ */\n+class SetGetItemGuardAccessor : public GuardAccessor {\n+ public:\n+  SetGetItemGuardAccessor(\n+      RootGuardManager* root,\n+      const py::object& index,\n+      std::string source,\n+      py::handle example_value,\n+      py::handle guard_manager_enum)\n+      : GuardAccessor(\n+            root,\n+            index,\n+            std::move(source),\n+            example_value,\n+            guard_manager_enum),\n+        _index(py::cast<Py_ssize_t>(index)) {}\n+\n+  // NB: Intentional duplication between check_nopybind and\n+  // check_verbose_nopybind.\n+  bool check_nopybind(PyObject* obj, bool matches_dict_tag = false)\n+      override { // borrowed ref\n+    PyObject* PyDict = (PyObject*)&PyDict_Type;\n+    PyObject* dict =",
        "comment_created_at": "2025-05-27T18:06:14+00:00",
        "comment_author": "williamwen42",
        "comment_body": "Borrowed/new references don't seem to be handled correctly here. Either use `py::object` to automatically delete or manually call `Py_[X]DECREF`.",
        "pr_file_module": null
      }
    ]
  }
]
