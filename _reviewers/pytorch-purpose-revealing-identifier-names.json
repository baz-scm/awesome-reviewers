[
  {
    "discussion_id": "2190446406",
    "pr_number": 157706,
    "pr_file": "torch/_inductor/comms.py",
    "created_at": "2025-07-07T15:45:53+00:00",
    "commented_code": "if total_moves >= MOVE_LIMIT:\n                 reorder_info.limiting_factor = \"move limit\"\n                 continue\n+\n             for j in range(i - 1, -1, -1):\n-                prev_snode = snodes[j]\n+                prev_gsnode = gsnodes[j]\n+                if len(prev_gsnode.snodes) == 0:\n+                    continue\n+\n                 if j < max(0, i - PER_COLLECTIVE_PREFETCH_LIMIT):\n                     reorder_info.limiting_factor = \"prefetch limit\"\n                     break\n-                if contains_collective(prev_snode):\n+                if contains_collective(prev_gsnode):\n                     reorder_info.limiting_factor = \"collective ordering\"\n                     break\n+\n                 dep_names = OrderedSet([s.name for s in snode.unmet_dependencies])\n-                if any(o.get_name() in dep_names for o in prev_snode.get_outputs()):\n-                    reorder_info.limiting_factor = \"data dependency\"\n-                    break\n+                prev_outs = prev_gsnode.get_outputs()\n+                data_dep = None\n+                for o in prev_outs:\n+                    if o.get_name() in dep_names:\n+                        data_dep = o.get_name()\n+                        break\n+\n+                if data_dep is not None:\n+\n+                    def is_groupable(prev_gsnode):\n+                        # preserve ordering\n+                        if contains_collective(prev_gsnode):\n+                            return False\n+\n+                        if contains_ungroupable(prev_gsnode):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2190446406",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157706,
        "pr_file": "torch/_inductor/comms.py",
        "discussion_id": "2190446406",
        "commented_code": "@@ -183,18 +253,53 @@ def exposed_communication_time(collective_snode, remaining_snodes):\n             if total_moves >= MOVE_LIMIT:\n                 reorder_info.limiting_factor = \"move limit\"\n                 continue\n+\n             for j in range(i - 1, -1, -1):\n-                prev_snode = snodes[j]\n+                prev_gsnode = gsnodes[j]\n+                if len(prev_gsnode.snodes) == 0:\n+                    continue\n+\n                 if j < max(0, i - PER_COLLECTIVE_PREFETCH_LIMIT):\n                     reorder_info.limiting_factor = \"prefetch limit\"\n                     break\n-                if contains_collective(prev_snode):\n+                if contains_collective(prev_gsnode):\n                     reorder_info.limiting_factor = \"collective ordering\"\n                     break\n+\n                 dep_names = OrderedSet([s.name for s in snode.unmet_dependencies])\n-                if any(o.get_name() in dep_names for o in prev_snode.get_outputs()):\n-                    reorder_info.limiting_factor = \"data dependency\"\n-                    break\n+                prev_outs = prev_gsnode.get_outputs()\n+                data_dep = None\n+                for o in prev_outs:\n+                    if o.get_name() in dep_names:\n+                        data_dep = o.get_name()\n+                        break\n+\n+                if data_dep is not None:\n+\n+                    def is_groupable(prev_gsnode):\n+                        # preserve ordering\n+                        if contains_collective(prev_gsnode):\n+                            return False\n+\n+                        if contains_ungroupable(prev_gsnode):",
        "comment_created_at": "2025-07-07T15:45:53+00:00",
        "comment_author": "wconstab",
        "comment_body": "nit: a bit arbitrary that \"contains_collective\" is a separate catetory than '\"contains_ungroupable\".  Would it be more readable to rename \"contains_ungroupable\" to \"contains_gemm_like\" ?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2176488019",
    "pr_number": 157333,
    "pr_file": "torch/torch_version.py",
    "created_at": "2025-07-01T05:52:52+00:00",
    "commented_code": "from torch.version import __version__ as internal_version\n \n \n-__all__ = [\"TorchVersion\"]\n+__all__ = [\"VersionParser\"]\n \n \n-class TorchVersion(str):\n+class VersionParser(str):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2176488019",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157333,
        "pr_file": "torch/torch_version.py",
        "discussion_id": "2176488019",
        "commented_code": "@@ -5,25 +5,25 @@\n from torch.version import __version__ as internal_version\n \n \n-__all__ = [\"TorchVersion\"]\n+__all__ = [\"VersionParser\"]\n \n \n-class TorchVersion(str):\n+class VersionParser(str):",
        "comment_created_at": "2025-07-01T05:52:52+00:00",
        "comment_author": "XuehaiPan",
        "comment_body": "`VersionString` should be a better name.",
        "pr_file_module": null
      },
      {
        "comment_id": "2178304037",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157333,
        "pr_file": "torch/torch_version.py",
        "discussion_id": "2176488019",
        "commented_code": "@@ -5,25 +5,25 @@\n from torch.version import __version__ as internal_version\n \n \n-__all__ = [\"TorchVersion\"]\n+__all__ = [\"VersionParser\"]\n \n \n-class TorchVersion(str):\n+class VersionParser(str):",
        "comment_created_at": "2025-07-01T18:33:10+00:00",
        "comment_author": "davidberard98",
        "comment_body": "Thanks, that's definitely a better name!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "657378727",
    "pr_number": 60497,
    "pr_file": "torch/fx/tensor_type.py",
    "created_at": "2021-06-23T18:51:05+00:00",
    "commented_code": "+class Tensor_Type:\n+    \"\"\"\n+    Tensor_Type defines a type for tensors, which consists of a list of dimensions.\n+\n+    Example:\n+        class M(torch.nn.Module):\n+            def forward(self, x:Tensor_Type((1,2,3, Dyn)), y:Tensor_Type((1,2,3, Dyn))):\n+                return torch.add(x, y)\n+    \"\"\"\n+\n+    def __init__(self, dim):\n+        self.__origin__ = Tensor_Type\n+        self.__args__ = dim\n+\n+    def __repr__(self):\n+        return f'Tensor_Type[{self.__args__}]'\n+\n+    def __eq__(self, other):\n+        if isinstance(other, self.__class__):\n+            return list(self.__args__) == list(other.__args__)\n+        else:\n+            return False\n+\n+    @staticmethod\n+    def __class_getitem__(*args):\n+        assert isinstance(args[0], tuple)\n+        return Tensor_Type(args[0])\n+\n+class Dyn_Type:\n+    \"\"\"\n+    Dyn_Type defines a type which stands for the absence of type information.\n+    \"\"\"\n+    def __init__(self):\n+        self.__name__ = 'Dyn_Type'\n+\n+    def __eq__(self, other):\n+        return isinstance(other, self.__class__)\n+\n+Dyn = Dyn_Type()\n+\n+\n+def consistency(t1, t2):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "657378727",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 60497,
        "pr_file": "torch/fx/tensor_type.py",
        "discussion_id": "657378727",
        "commented_code": "@@ -0,0 +1,62 @@\n+class Tensor_Type:\n+    \"\"\"\n+    Tensor_Type defines a type for tensors, which consists of a list of dimensions.\n+\n+    Example:\n+        class M(torch.nn.Module):\n+            def forward(self, x:Tensor_Type((1,2,3, Dyn)), y:Tensor_Type((1,2,3, Dyn))):\n+                return torch.add(x, y)\n+    \"\"\"\n+\n+    def __init__(self, dim):\n+        self.__origin__ = Tensor_Type\n+        self.__args__ = dim\n+\n+    def __repr__(self):\n+        return f'Tensor_Type[{self.__args__}]'\n+\n+    def __eq__(self, other):\n+        if isinstance(other, self.__class__):\n+            return list(self.__args__) == list(other.__args__)\n+        else:\n+            return False\n+\n+    @staticmethod\n+    def __class_getitem__(*args):\n+        assert isinstance(args[0], tuple)\n+        return Tensor_Type(args[0])\n+\n+class Dyn_Type:\n+    \"\"\"\n+    Dyn_Type defines a type which stands for the absence of type information.\n+    \"\"\"\n+    def __init__(self):\n+        self.__name__ = 'Dyn_Type'\n+\n+    def __eq__(self, other):\n+        return isinstance(other, self.__class__)\n+\n+Dyn = Dyn_Type()\n+\n+\n+def consistency(t1, t2):",
        "comment_created_at": "2021-06-23T18:51:05+00:00",
        "comment_author": "jamesr66a",
        "comment_body": "nit: can we name this as an action? e.g. `is_consistent` or `are_types_consistent`",
        "pr_file_module": null
      },
      {
        "comment_id": "657435136",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 60497,
        "pr_file": "torch/fx/tensor_type.py",
        "discussion_id": "657378727",
        "commented_code": "@@ -0,0 +1,62 @@\n+class Tensor_Type:\n+    \"\"\"\n+    Tensor_Type defines a type for tensors, which consists of a list of dimensions.\n+\n+    Example:\n+        class M(torch.nn.Module):\n+            def forward(self, x:Tensor_Type((1,2,3, Dyn)), y:Tensor_Type((1,2,3, Dyn))):\n+                return torch.add(x, y)\n+    \"\"\"\n+\n+    def __init__(self, dim):\n+        self.__origin__ = Tensor_Type\n+        self.__args__ = dim\n+\n+    def __repr__(self):\n+        return f'Tensor_Type[{self.__args__}]'\n+\n+    def __eq__(self, other):\n+        if isinstance(other, self.__class__):\n+            return list(self.__args__) == list(other.__args__)\n+        else:\n+            return False\n+\n+    @staticmethod\n+    def __class_getitem__(*args):\n+        assert isinstance(args[0], tuple)\n+        return Tensor_Type(args[0])\n+\n+class Dyn_Type:\n+    \"\"\"\n+    Dyn_Type defines a type which stands for the absence of type information.\n+    \"\"\"\n+    def __init__(self):\n+        self.__name__ = 'Dyn_Type'\n+\n+    def __eq__(self, other):\n+        return isinstance(other, self.__class__)\n+\n+Dyn = Dyn_Type()\n+\n+\n+def consistency(t1, t2):",
        "comment_created_at": "2021-06-23T20:22:54+00:00",
        "comment_author": "migeed-z",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  }
]