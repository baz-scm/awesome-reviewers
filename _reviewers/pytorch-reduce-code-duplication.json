[
  {
    "discussion_id": "2181456948",
    "pr_number": 157520,
    "pr_file": "aten/src/ATen/native/mkldnn/Matmul.cpp",
    "created_at": "2025-07-03T02:48:38+00:00",
    "commented_code": "checksize(mat1, mat2));\n }\n \n+bool use_mkldnn_tf32_matmul(\n+    const Tensor& mat1,\n+    const Tensor& mat2,\n+    const Tensor& result) {\n+\n+    return (\n+      use_mkldnn_tf32_matmul() &&\n+      mat1.scalar_type() == kFloat &&\n+      mat2.scalar_type() == kFloat &&\n+      (!result.defined() || result.scalar_type() == kFloat) &&\n+      mat1.numel() != 0 &&\n+      mat2.numel() != 0 &&\n+      checksize(mat1, mat2));\n+}\n+",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2181456948",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157520,
        "pr_file": "aten/src/ATen/native/mkldnn/Matmul.cpp",
        "discussion_id": "2181456948",
        "commented_code": "@@ -471,11 +486,26 @@ bool use_mkldnn_bf32_matmul(\n       checksize(mat1, mat2));\n }\n \n+bool use_mkldnn_tf32_matmul(\n+    const Tensor& mat1,\n+    const Tensor& mat2,\n+    const Tensor& result) {\n+\n+    return (\n+      use_mkldnn_tf32_matmul() &&\n+      mat1.scalar_type() == kFloat &&\n+      mat2.scalar_type() == kFloat &&\n+      (!result.defined() || result.scalar_type() == kFloat) &&\n+      mat1.numel() != 0 &&\n+      mat2.numel() != 0 &&\n+      checksize(mat1, mat2));\n+}\n+",
        "comment_created_at": "2025-07-03T02:48:38+00:00",
        "comment_author": "mingfeima",
        "comment_body": "this file has multiple functions that have similar usage:\r\n`use_mkldnn_bf16_matmul`, `use_mkldnn_fp16_matmul`, `use_mkldnn_bf32_matmul` and `use_mkldnn_tf32_matmul`\r\n\r\ncan we templatize it to simplify the code?\r\n\r\n```\r\ntemplate <typename T>\r\nbool use_mkldnn_matmul();\r\n\r\n#if defined(__aarch64__)\r\nbool use_mkldnn_matmul<at::BFloat16>();\r\n#endif\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2180500483",
    "pr_number": 157290,
    "pr_file": "torch/nativert/executor/memory/LayoutManager.cpp",
    "created_at": "2025-07-02T16:31:15+00:00",
    "commented_code": "}\n }\n \n+#ifndef NDEBUG\n+void LayoutManager::assert_no_overlapping_storages(const Node& node, size_t t)\n+    const {\n+  if (state_ != LayoutManagerState::Running) {\n+    return;\n+  }\n+\n+  /*\n+    for each value\n+    (either an input or output)\n+    ensure that the associated storage\n+    slice lies within the allocated slice\n+    if it is managed (or if it is an alias,\n+    we can use the slice allocated to its source)\n+    ---\n+    also ensure that the current index lies\n+    within the lifetime of this value\n+  */\n+\n+  const auto& alias_analyzer = planner_.get_alias_analyzer();\n+  const auto& alive_values = alias_analyzer.alive_values_at_time(t);\n+\n+  // make sure active memory intervals are non-overlapping\n+  // by sorting them by start, and ensuring\n+  // cur.start > prev.end for each\n+  std::set<std::pair<size_t, size_t>> intervals;\n+\n+  planner_.with_plan([&](const LayoutPlan& plan) {\n+    for (auto v : alive_values) {\n+      // sanity check lifetimes\n+      const auto& lt = alias_analyzer.lifetime(v);\n+      TORCH_CHECK_GE(t, lt.start);\n+      TORCH_CHECK_LE(t, lt.end);\n+\n+      // prevent recomputation from occuring\n+      c10::FastSet<ValueId> checked_values;\n+\n+      const auto& iv = parent_frame_.getIValue(v->id());\n+      if (!iv.isTensor()) {\n+        continue;\n+      }\n+\n+      const auto& storage_impl = iv.toTensor().storage().unsafeGetStorageImpl();\n+      const auto storage_nbytes = storage_impl->nbytes();\n+\n+      const auto start =\n+          layout_buffer_.get_offset_from_ptr(storage_impl->data_ptr().get());\n+      if (C10_UNLIKELY(!start.has_value() /* not in managed buffer */)) {\n+        continue;\n+      }\n+\n+      const auto* src_ptr = alias_analyzer.get_source_of_alias(v);\n+\n+      if (!src_ptr /* v isn't an alias */) {\n+        if (checked_values.emplace(v->id())\n+                .second /* if not already checked */) {\n+          // sanity check that the storage lies within the slice allocated\n+          // during planning\n+          auto& alloc =\n+              plan.allocations[value_to_vector_idx_map_.at(src_ptr->id())];\n+          TORCH_CHECK_GE(*start, alloc.offset);\n+          TORCH_CHECK_LE(*start + storage_nbytes, alloc.offset + alloc.size);\n+          intervals.emplace(*start, *start + storage_nbytes);\n+        }\n+        continue;\n+      }\n+\n+      const auto& src_iv = parent_frame_.getIValue(src_ptr->id());",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2180500483",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157290,
        "pr_file": "torch/nativert/executor/memory/LayoutManager.cpp",
        "discussion_id": "2180500483",
        "commented_code": "@@ -157,6 +160,123 @@ void LayoutManager::populate_tensor_values() {\n   }\n }\n \n+#ifndef NDEBUG\n+void LayoutManager::assert_no_overlapping_storages(const Node& node, size_t t)\n+    const {\n+  if (state_ != LayoutManagerState::Running) {\n+    return;\n+  }\n+\n+  /*\n+    for each value\n+    (either an input or output)\n+    ensure that the associated storage\n+    slice lies within the allocated slice\n+    if it is managed (or if it is an alias,\n+    we can use the slice allocated to its source)\n+    ---\n+    also ensure that the current index lies\n+    within the lifetime of this value\n+  */\n+\n+  const auto& alias_analyzer = planner_.get_alias_analyzer();\n+  const auto& alive_values = alias_analyzer.alive_values_at_time(t);\n+\n+  // make sure active memory intervals are non-overlapping\n+  // by sorting them by start, and ensuring\n+  // cur.start > prev.end for each\n+  std::set<std::pair<size_t, size_t>> intervals;\n+\n+  planner_.with_plan([&](const LayoutPlan& plan) {\n+    for (auto v : alive_values) {\n+      // sanity check lifetimes\n+      const auto& lt = alias_analyzer.lifetime(v);\n+      TORCH_CHECK_GE(t, lt.start);\n+      TORCH_CHECK_LE(t, lt.end);\n+\n+      // prevent recomputation from occuring\n+      c10::FastSet<ValueId> checked_values;\n+\n+      const auto& iv = parent_frame_.getIValue(v->id());\n+      if (!iv.isTensor()) {\n+        continue;\n+      }\n+\n+      const auto& storage_impl = iv.toTensor().storage().unsafeGetStorageImpl();\n+      const auto storage_nbytes = storage_impl->nbytes();\n+\n+      const auto start =\n+          layout_buffer_.get_offset_from_ptr(storage_impl->data_ptr().get());\n+      if (C10_UNLIKELY(!start.has_value() /* not in managed buffer */)) {\n+        continue;\n+      }\n+\n+      const auto* src_ptr = alias_analyzer.get_source_of_alias(v);\n+\n+      if (!src_ptr /* v isn't an alias */) {\n+        if (checked_values.emplace(v->id())\n+                .second /* if not already checked */) {\n+          // sanity check that the storage lies within the slice allocated\n+          // during planning\n+          auto& alloc =\n+              plan.allocations[value_to_vector_idx_map_.at(src_ptr->id())];\n+          TORCH_CHECK_GE(*start, alloc.offset);\n+          TORCH_CHECK_LE(*start + storage_nbytes, alloc.offset + alloc.size);\n+          intervals.emplace(*start, *start + storage_nbytes);\n+        }\n+        continue;\n+      }\n+\n+      const auto& src_iv = parent_frame_.getIValue(src_ptr->id());",
        "comment_created_at": "2025-07-02T16:31:15+00:00",
        "comment_author": "SherlockNoMad",
        "comment_body": "checking logic looks duplicated for iv and src_iv? \r\n\r\nrefactor them into a function? ",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2190366214",
    "pr_number": 157633,
    "pr_file": "torch/nativert/executor/ModelRunnerBase.cpp",
    "created_at": "2025-07-07T15:09:55+00:00",
    "commented_code": "+#include <torch/nativert/executor/ModelRunnerBase.h> // @manual\n+\n+#include <fmt/format.h>\n+\n+#include <ATen/record_function.h> // @manual\n+#include <c10/util/Logging.h>\n+#include <caffe2/serialize/inline_container.h> // @manual=//caffe2/caffe2/serialize:inline_container\n+#include <torch/csrc/export/pt2_archive_constants.h> // @manual\n+#include <torch/csrc/inductor/aoti_torch/oss_proxy_executor.h> // @manual\n+#include <torch/csrc/jit/serialization/pickle.h> // @manual=//caffe2:torch-cpp-cpu\n+#include <torch/nativert/executor/Weights.h> // @manual\n+#include <torch/nativert/graph/TensorMeta.h> // @manual\n+\n+namespace torch::nativert {\n+\n+using torch::nativert::TensorMeta;\n+using torch::nativert::detail::itreeFlattenFromArgs;\n+using torch::nativert::detail::itreeMap;\n+using torch::nativert::detail::ITreeSpec;\n+using torch::nativert::detail::itreeUnflatten;\n+\n+ModelRunnerBase::ModelRunnerBase(\n+    std::shared_ptr<caffe2::serialize::PyTorchStreamReader> pytorchStreamReader,\n+    const std::string& modelName,\n+    ExecutorType executorType,\n+    const BaseRuntimeConfigs& runtimeConfigs,\n+    const std::function<Placement(const torch::nativert::Graph& graph)>&\n+        buildPlacementFn)\n+    : modelName_(modelName),\n+      executorType_(executorType),\n+      runtimeConfigs_(runtimeConfigs) {}\n+\n+void ModelRunnerBase::setExecutorType(\n+    ExecutorType type,\n+    const std::string& platformArch) {\n+  LOG(INFO) << fmt::format(\n+      \"Setting executor type to {} with platformArch='{}'\", type, platformArch);\n+  executorType_ = type;\n+  if (type == ExecutorType::AOTINDUCTOR) {\n+    runtimeConfigs_.platformArch = platformArch;\n+  } else if (type == ExecutorType::MTIA) {\n+    // TODO: hardcoded for now (nativert packages specify platformArch as\n+    // \"mtia\")\n+    runtimeConfigs_.platformArch = \"mtia\";\n+  }\n+}\n+\n+const std::string& ModelRunnerBase::getModelName() const {\n+  return modelName_;\n+}\n+\n+std::shared_ptr<Weights> ModelRunnerBase::getWeights() {\n+  if (executor_ != nullptr) {\n+    return executor_->getWeights();\n+  } else if (newWeights_ != nullptr) {\n+    return newWeights_;\n+  } else {\n+    TORCH_CHECK(\n+        false, \"ModelRunner is not initialized, and no weights are loaded.\");\n+  }\n+}\n+\n+std::shared_ptr<Weights> ModelRunnerBase::loadNewWeights(\n+    std::shared_ptr<caffe2::serialize::PyTorchStreamReader> packageStreamReader,\n+    std::function<bool(const std::string&)> skipSizeCheck,\n+    std::function<bool(const std::string&)> skipDtypeCheck) {\n+  LOG(INFO) << \"ModelRunner loading new weights\";\n+  newWeights_ = std::make_shared<Weights>(\n+      graph_.get(),\n+      packageStreamReader,\n+      stateDictPath_,\n+      torch::_export::archive_spec::WEIGHTS_DIR,\n+      constantPaths_,\n+      torch::_export::archive_spec::CONSTANTS_DIR,\n+      placement_,\n+      std::move(skipSizeCheck),\n+      std::move(skipDtypeCheck));\n+\n+  return newWeights_;\n+}\n+\n+void ModelRunnerBase::commitNewWeights() {\n+  CHECK(newWeights_) << \"No new weights loaded\";\n+  CHECK(executor_) << \"ModelRunner not initialized\";\n+  LOG(INFO) << \"ModelRunner committing new weights\";\n+\n+  executor_->processWeights(newWeights_);\n+\n+  executor_->atomicSwapWeights(std::move(newWeights_));\n+\n+  newWeights_ = nullptr;\n+}\n+\n+bool loadExtraFiles(\n+    ExtraFilesMap& extraFiles,\n+    std::shared_ptr<caffe2::serialize::PyTorchStreamReader>\n+        pytorchStreamReader) {\n+  auto filesExist = false;\n+  for (const auto& kv : extraFiles) {\n+    const auto key =\n+        std::string{torch::_export::archive_spec::EXTRA_DIR} + kv.first;\n+    if (pytorchStreamReader->hasRecord(key)) {\n+      auto [metaPtr, metaSize] = pytorchStreamReader->getRecord(key);\n+      extraFiles[kv.first] =\n+          std::string(static_cast<char*>(metaPtr.get()), metaSize);\n+      filesExist = true;\n+    }\n+  }\n+  return filesExist;\n+}\n+\n+c10::IValue ModelRunnerBase::run(\n+    const std::unordered_map<std::string, c10::IValue>& kwargs,\n+    const RunConfigs& runConfigs) {\n+  return run({}, kwargs, runConfigs);\n+}\n+\n+c10::IValue ModelRunnerBase::run(\n+    const std::vector<c10::IValue>& args,\n+    const std::unordered_map<std::string, c10::IValue>& kwargs,\n+    const RunConfigs& runConfigs) {\n+  RECORD_USER_SCOPE(\"nativert::ModelRunner::run\");\n+  CHECK(executor_) << \"ModelRunner not initialized\";\n+\n+  // ModelRunner is only used for inference\n+  c10::InferenceMode mode;\n+\n+  return itreeUnflatten(\n+      executor_->execute(args, kwargs, inputSpec_), outputSpec_);\n+}\n+\n+std::vector<c10::IValue> ModelRunnerBase::runWithFlatInputsAndOutputs(\n+    std::vector<c10::IValue>&& flatInputs,\n+    const RunConfigs& /* runConfigs */) {\n+  RECORD_USER_SCOPE(\"nativert::ModelRunner::runWithFlatInputsAndOutputs\");\n+  CHECK(executor_) << \"ModelRunner not initialized\";\n+\n+  // ModelRunner is only used for inference\n+  c10::InferenceMode mode;\n+\n+  return executor_->execute(std::move(flatInputs));\n+}\n+\n+ProfileMetrics ModelRunnerBase::benchmarkIndividualNodes(\n+    const std::vector<std::vector<c10::IValue>>& argsList,\n+    const std::vector<std::unordered_map<std::string, c10::IValue>>& kwargsList,\n+    const uint32_t warmupRuns,\n+    const uint32_t mainRuns,\n+    const bool printPerNodeTime,\n+    const RunConfigs& runConfigs) {\n+  std::vector<std::vector<c10::IValue>> flatInputsList;\n+  for (const auto& args : argsList) {\n+    if (!kwargsList.empty()) {\n+      for (const auto& kwargs : kwargsList) {\n+        flatInputsList.emplace_back(\n+            itreeFlattenFromArgs(args, kwargs, inputSpec_));\n+      }\n+    } else {\n+      flatInputsList.emplace_back(itreeFlattenFromArgs(args, {}, inputSpec_));\n+    }\n+  }\n+  c10::InferenceMode mode;\n+  auto results =\n+      executor_->benchmarkIndividualNodes(flatInputsList, warmupRuns, mainRuns);\n+\n+  if (printPerNodeTime) {\n+    size_t i = 0;\n+    for (const auto& node : graph_->nodes()) {\n+      LOG(INFO) << \"Node #\" << i << \": \" << node.toString()\n+                << \"\n Time: \" << results.timePerNode[i] << \" ms/iter, \";\n+      i++;\n+    }",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2190366214",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157633,
        "pr_file": "torch/nativert/executor/ModelRunnerBase.cpp",
        "discussion_id": "2190366214",
        "commented_code": "@@ -0,0 +1,438 @@\n+#include <torch/nativert/executor/ModelRunnerBase.h> // @manual\n+\n+#include <fmt/format.h>\n+\n+#include <ATen/record_function.h> // @manual\n+#include <c10/util/Logging.h>\n+#include <caffe2/serialize/inline_container.h> // @manual=//caffe2/caffe2/serialize:inline_container\n+#include <torch/csrc/export/pt2_archive_constants.h> // @manual\n+#include <torch/csrc/inductor/aoti_torch/oss_proxy_executor.h> // @manual\n+#include <torch/csrc/jit/serialization/pickle.h> // @manual=//caffe2:torch-cpp-cpu\n+#include <torch/nativert/executor/Weights.h> // @manual\n+#include <torch/nativert/graph/TensorMeta.h> // @manual\n+\n+namespace torch::nativert {\n+\n+using torch::nativert::TensorMeta;\n+using torch::nativert::detail::itreeFlattenFromArgs;\n+using torch::nativert::detail::itreeMap;\n+using torch::nativert::detail::ITreeSpec;\n+using torch::nativert::detail::itreeUnflatten;\n+\n+ModelRunnerBase::ModelRunnerBase(\n+    std::shared_ptr<caffe2::serialize::PyTorchStreamReader> pytorchStreamReader,\n+    const std::string& modelName,\n+    ExecutorType executorType,\n+    const BaseRuntimeConfigs& runtimeConfigs,\n+    const std::function<Placement(const torch::nativert::Graph& graph)>&\n+        buildPlacementFn)\n+    : modelName_(modelName),\n+      executorType_(executorType),\n+      runtimeConfigs_(runtimeConfigs) {}\n+\n+void ModelRunnerBase::setExecutorType(\n+    ExecutorType type,\n+    const std::string& platformArch) {\n+  LOG(INFO) << fmt::format(\n+      \"Setting executor type to {} with platformArch='{}'\", type, platformArch);\n+  executorType_ = type;\n+  if (type == ExecutorType::AOTINDUCTOR) {\n+    runtimeConfigs_.platformArch = platformArch;\n+  } else if (type == ExecutorType::MTIA) {\n+    // TODO: hardcoded for now (nativert packages specify platformArch as\n+    // \"mtia\")\n+    runtimeConfigs_.platformArch = \"mtia\";\n+  }\n+}\n+\n+const std::string& ModelRunnerBase::getModelName() const {\n+  return modelName_;\n+}\n+\n+std::shared_ptr<Weights> ModelRunnerBase::getWeights() {\n+  if (executor_ != nullptr) {\n+    return executor_->getWeights();\n+  } else if (newWeights_ != nullptr) {\n+    return newWeights_;\n+  } else {\n+    TORCH_CHECK(\n+        false, \"ModelRunner is not initialized, and no weights are loaded.\");\n+  }\n+}\n+\n+std::shared_ptr<Weights> ModelRunnerBase::loadNewWeights(\n+    std::shared_ptr<caffe2::serialize::PyTorchStreamReader> packageStreamReader,\n+    std::function<bool(const std::string&)> skipSizeCheck,\n+    std::function<bool(const std::string&)> skipDtypeCheck) {\n+  LOG(INFO) << \"ModelRunner loading new weights\";\n+  newWeights_ = std::make_shared<Weights>(\n+      graph_.get(),\n+      packageStreamReader,\n+      stateDictPath_,\n+      torch::_export::archive_spec::WEIGHTS_DIR,\n+      constantPaths_,\n+      torch::_export::archive_spec::CONSTANTS_DIR,\n+      placement_,\n+      std::move(skipSizeCheck),\n+      std::move(skipDtypeCheck));\n+\n+  return newWeights_;\n+}\n+\n+void ModelRunnerBase::commitNewWeights() {\n+  CHECK(newWeights_) << \"No new weights loaded\";\n+  CHECK(executor_) << \"ModelRunner not initialized\";\n+  LOG(INFO) << \"ModelRunner committing new weights\";\n+\n+  executor_->processWeights(newWeights_);\n+\n+  executor_->atomicSwapWeights(std::move(newWeights_));\n+\n+  newWeights_ = nullptr;\n+}\n+\n+bool loadExtraFiles(\n+    ExtraFilesMap& extraFiles,\n+    std::shared_ptr<caffe2::serialize::PyTorchStreamReader>\n+        pytorchStreamReader) {\n+  auto filesExist = false;\n+  for (const auto& kv : extraFiles) {\n+    const auto key =\n+        std::string{torch::_export::archive_spec::EXTRA_DIR} + kv.first;\n+    if (pytorchStreamReader->hasRecord(key)) {\n+      auto [metaPtr, metaSize] = pytorchStreamReader->getRecord(key);\n+      extraFiles[kv.first] =\n+          std::string(static_cast<char*>(metaPtr.get()), metaSize);\n+      filesExist = true;\n+    }\n+  }\n+  return filesExist;\n+}\n+\n+c10::IValue ModelRunnerBase::run(\n+    const std::unordered_map<std::string, c10::IValue>& kwargs,\n+    const RunConfigs& runConfigs) {\n+  return run({}, kwargs, runConfigs);\n+}\n+\n+c10::IValue ModelRunnerBase::run(\n+    const std::vector<c10::IValue>& args,\n+    const std::unordered_map<std::string, c10::IValue>& kwargs,\n+    const RunConfigs& runConfigs) {\n+  RECORD_USER_SCOPE(\"nativert::ModelRunner::run\");\n+  CHECK(executor_) << \"ModelRunner not initialized\";\n+\n+  // ModelRunner is only used for inference\n+  c10::InferenceMode mode;\n+\n+  return itreeUnflatten(\n+      executor_->execute(args, kwargs, inputSpec_), outputSpec_);\n+}\n+\n+std::vector<c10::IValue> ModelRunnerBase::runWithFlatInputsAndOutputs(\n+    std::vector<c10::IValue>&& flatInputs,\n+    const RunConfigs& /* runConfigs */) {\n+  RECORD_USER_SCOPE(\"nativert::ModelRunner::runWithFlatInputsAndOutputs\");\n+  CHECK(executor_) << \"ModelRunner not initialized\";\n+\n+  // ModelRunner is only used for inference\n+  c10::InferenceMode mode;\n+\n+  return executor_->execute(std::move(flatInputs));\n+}\n+\n+ProfileMetrics ModelRunnerBase::benchmarkIndividualNodes(\n+    const std::vector<std::vector<c10::IValue>>& argsList,\n+    const std::vector<std::unordered_map<std::string, c10::IValue>>& kwargsList,\n+    const uint32_t warmupRuns,\n+    const uint32_t mainRuns,\n+    const bool printPerNodeTime,\n+    const RunConfigs& runConfigs) {\n+  std::vector<std::vector<c10::IValue>> flatInputsList;\n+  for (const auto& args : argsList) {\n+    if (!kwargsList.empty()) {\n+      for (const auto& kwargs : kwargsList) {\n+        flatInputsList.emplace_back(\n+            itreeFlattenFromArgs(args, kwargs, inputSpec_));\n+      }\n+    } else {\n+      flatInputsList.emplace_back(itreeFlattenFromArgs(args, {}, inputSpec_));\n+    }\n+  }\n+  c10::InferenceMode mode;\n+  auto results =\n+      executor_->benchmarkIndividualNodes(flatInputsList, warmupRuns, mainRuns);\n+\n+  if (printPerNodeTime) {\n+    size_t i = 0;\n+    for (const auto& node : graph_->nodes()) {\n+      LOG(INFO) << \"Node #\" << i << \": \" << node.toString()\n+                << \"\\n Time: \" << results.timePerNode[i] << \" ms/iter, \";\n+      i++;\n+    }",
        "comment_created_at": "2025-07-07T15:09:55+00:00",
        "comment_author": "dolpm",
        "comment_body": "nit: opt for enumerate\r\n\r\n```suggestion\r\n    for (const auto&& [i, node] : c10::enumerate(graph_->nodes())) {\r\n      LOG(INFO) << \"Node #\" << i << \": \" << node.toString()\r\n                << \"\\n Time: \" << results.timePerNode[i] << \" ms/iter, \";\r\n    }\r\n```",
        "pr_file_module": null
      }
    ]
  }
]