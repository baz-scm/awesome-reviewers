[
  {
    "discussion_id": "2164098880",
    "pr_number": 156689,
    "pr_file": "torch/_subclasses/fake_tensor.py",
    "created_at": "2025-06-24T13:59:52+00:00",
    "commented_code": "nonlocal flat_arg_fake_tensors\n            if not self.is_our_fake(x):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2164098880",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156689,
        "pr_file": "torch/_subclasses/fake_tensor.py",
        "discussion_id": "2164098880",
        "commented_code": "@@ -2774,7 +2774,7 @@ def validate(x: T) -> Union[T, FakeTensor]:\n \n             nonlocal flat_arg_fake_tensors\n             if not self.is_our_fake(x):",
        "comment_created_at": "2025-06-24T13:59:52+00:00",
        "comment_author": "atalman",
        "comment_body": "Hi @Valentine233 could you please also add unit test for this change ?",
        "pr_file_module": null
      },
      {
        "comment_id": "2165804339",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156689,
        "pr_file": "torch/_subclasses/fake_tensor.py",
        "discussion_id": "2164098880",
        "commented_code": "@@ -2774,7 +2774,7 @@ def validate(x: T) -> Union[T, FakeTensor]:\n \n             nonlocal flat_arg_fake_tensors\n             if not self.is_our_fake(x):",
        "comment_created_at": "2025-06-25T05:22:03+00:00",
        "comment_author": "Valentine233",
        "comment_body": "Thanks @atalman , the UT has been added.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2109527687",
    "pr_number": 154333,
    "pr_file": "test/test_export_serde.py",
    "created_at": "2025-05-27T15:34:17+00:00",
    "commented_code": "import io\nimport torch\nfrom torch._export.serde.serialize import deserialize_torch_artifact\n\ndef test_deserialize_torch_artifact_dict():",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2109527687",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154333,
        "pr_file": "test/test_export_serde.py",
        "discussion_id": "2109527687",
        "commented_code": "@@ -0,0 +1,13 @@\n+import io\n+import torch\n+from torch._export.serde.serialize import deserialize_torch_artifact\n+\n+def test_deserialize_torch_artifact_dict():",
        "comment_created_at": "2025-05-27T15:34:17+00:00",
        "comment_author": "angelayi",
        "comment_body": "Can you move your test case into [test_serialize.py](https://github.com/pytorch/pytorch/blob/7ae204c3b67e58405127b9758f7a7da3c3b51b83/test/export/test_serialize.py#L959)?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2169264519",
    "pr_number": 156339,
    "pr_file": "torch/_dynamo/variables/lists.py",
    "created_at": "2025-06-26T14:51:49+00:00",
    "commented_code": "else:\n                self.items[key.as_python_constant()] = value\n            return ConstantVariable.create(None)\n        elif name == \"__delitem__\" and self.is_mutable():\n            if kwargs or len(args) != 1:\n                raise_args_mismatch(tx, name)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2169264519",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156339,
        "pr_file": "torch/_dynamo/variables/lists.py",
        "discussion_id": "2169264519",
        "commented_code": "@@ -531,6 +531,29 @@ def call_method(\n             else:\n                 self.items[key.as_python_constant()] = value\n             return ConstantVariable.create(None)\n+        elif name == \"__delitem__\" and self.is_mutable():\n+            if kwargs or len(args) != 1:\n+                raise_args_mismatch(tx, name)",
        "comment_created_at": "2025-06-26T14:51:49+00:00",
        "comment_author": "zou3519",
        "comment_body": "Can you add a test for what happens if someone does `__delitem__` on a global list? I'm hoping dynamo generates the right bytecode for that.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2114123690",
    "pr_number": 153150,
    "pr_file": "test/dynamo/test_sets.py",
    "created_at": "2025-05-29T14:43:53+00:00",
    "commented_code": "# Owner(s): [\"module: dynamo\"]\n\n# TODO: move set tests from test_functions.py/test_misc.py to this file\n\n\nimport torch\nimport torch._dynamo.test_case\nfrom torch._dynamo.testing import CompileCounter\n\n\nclass SetGuardsSet(torch._dynamo.test_case.TestCase):\n    def test_set_recompile_on_key_pop(self):\n        s = {\n            torch._C._set_grad_enabled,\n            torch.amp._enter_autocast,\n            torch.amp._exit_autocast,\n        }\n\n        cnts = CompileCounter()\n\n        def fn(x, s):\n            if torch.amp._exit_autocast in s:\n                return x.sin()\n            return x.cos()\n\n        x = torch.randn(4)\n        opt_fn = torch.compile(fn, backend=cnts, fullgraph=True)\n        res = opt_fn(x, s)\n        opt_fn(x, s)\n        self.assertEqual(res, fn(x, s))\n        # No recompilation\n        self.assertEqual(cnts.frame_count, 1)\n\n        # Pop a value\n        s.remove(torch.amp._exit_autocast)\n\n        res = opt_fn(x, s)\n        # Check recompilation\n        self.assertEqual(cnts.frame_count, 2)\n        self.assertEqual(res, fn(x, s))\n\n    def test_set_recompile_on_key_change(self):\n        s = {\n            torch._C._set_grad_enabled,\n            torch.amp._enter_autocast,\n            torch.amp._exit_autocast,\n        }\n\n        cnts = CompileCounter()\n\n        def fn(x, s):\n            if torch.amp._exit_autocast in s:\n                return x.sin()\n            return x.cos()\n\n        x = torch.randn(4)\n        opt_fn = torch.compile(fn, backend=cnts, fullgraph=True)\n        res = opt_fn(x, s)\n        opt_fn(x, s)\n        self.assertEqual(res, fn(x, s))\n        # No recompilation\n        self.assertEqual(cnts.frame_count, 1)\n\n        # Pop a value\n        s.remove(torch.amp._exit_autocast)\n        # Add a different value\n        s.add(torch._C._set_autograd_fallback_mode)\n\n        res = opt_fn(x, s)\n        # Check recompilation\n        self.assertEqual(cnts.frame_count, 2)\n        self.assertEqual(res, fn(x, s))\n\n    def test_set_guard_on_keys_change(self):\n        # This test guarantee that we're not triggering any of the dict guards\n        # on sets\n        s = {\n            torch._C._set_grad_enabled,\n            torch.amp._enter_autocast,\n            torch.amp._exit_autocast,\n        }\n\n        cnts = CompileCounter()\n\n        def fn(x, s):\n            for e in s:\n                x = x * len(str(e))\n            return x\n\n        opt_fn = torch.compile(fn, backend=cnts, fullgraph=True)\n        opt_fn(torch.randn(4), s)\n        opt_fn(torch.randn(4), s)\n        # No recompilation\n        self.assertEqual(cnts.frame_count, 1)\n\n        # pop and add the same item\n        s.remove(torch.amp._exit_autocast)\n        s.add(torch.amp._exit_autocast)\n\n        x = torch.randn(4)\n        res = opt_fn(x, s)\n        # Check Dynamo don't recompile\n        self.assertEqual(cnts.frame_count, 1)\n        self.assertEqual(res, fn(x, s))",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2114123690",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153150,
        "pr_file": "test/dynamo/test_sets.py",
        "discussion_id": "2114123690",
        "commented_code": "@@ -0,0 +1,110 @@\n+# Owner(s): [\"module: dynamo\"]\n+\n+# TODO: move set tests from test_functions.py/test_misc.py to this file\n+\n+\n+import torch\n+import torch._dynamo.test_case\n+from torch._dynamo.testing import CompileCounter\n+\n+\n+class SetGuardsSet(torch._dynamo.test_case.TestCase):\n+    def test_set_recompile_on_key_pop(self):\n+        s = {\n+            torch._C._set_grad_enabled,\n+            torch.amp._enter_autocast,\n+            torch.amp._exit_autocast,\n+        }\n+\n+        cnts = CompileCounter()\n+\n+        def fn(x, s):\n+            if torch.amp._exit_autocast in s:\n+                return x.sin()\n+            return x.cos()\n+\n+        x = torch.randn(4)\n+        opt_fn = torch.compile(fn, backend=cnts, fullgraph=True)\n+        res = opt_fn(x, s)\n+        opt_fn(x, s)\n+        self.assertEqual(res, fn(x, s))\n+        # No recompilation\n+        self.assertEqual(cnts.frame_count, 1)\n+\n+        # Pop a value\n+        s.remove(torch.amp._exit_autocast)\n+\n+        res = opt_fn(x, s)\n+        # Check recompilation\n+        self.assertEqual(cnts.frame_count, 2)\n+        self.assertEqual(res, fn(x, s))\n+\n+    def test_set_recompile_on_key_change(self):\n+        s = {\n+            torch._C._set_grad_enabled,\n+            torch.amp._enter_autocast,\n+            torch.amp._exit_autocast,\n+        }\n+\n+        cnts = CompileCounter()\n+\n+        def fn(x, s):\n+            if torch.amp._exit_autocast in s:\n+                return x.sin()\n+            return x.cos()\n+\n+        x = torch.randn(4)\n+        opt_fn = torch.compile(fn, backend=cnts, fullgraph=True)\n+        res = opt_fn(x, s)\n+        opt_fn(x, s)\n+        self.assertEqual(res, fn(x, s))\n+        # No recompilation\n+        self.assertEqual(cnts.frame_count, 1)\n+\n+        # Pop a value\n+        s.remove(torch.amp._exit_autocast)\n+        # Add a different value\n+        s.add(torch._C._set_autograd_fallback_mode)\n+\n+        res = opt_fn(x, s)\n+        # Check recompilation\n+        self.assertEqual(cnts.frame_count, 2)\n+        self.assertEqual(res, fn(x, s))\n+\n+    def test_set_guard_on_keys_change(self):\n+        # This test guarantee that we're not triggering any of the dict guards\n+        # on sets\n+        s = {\n+            torch._C._set_grad_enabled,\n+            torch.amp._enter_autocast,\n+            torch.amp._exit_autocast,\n+        }\n+\n+        cnts = CompileCounter()\n+\n+        def fn(x, s):\n+            for e in s:\n+                x = x * len(str(e))\n+            return x\n+\n+        opt_fn = torch.compile(fn, backend=cnts, fullgraph=True)\n+        opt_fn(torch.randn(4), s)\n+        opt_fn(torch.randn(4), s)\n+        # No recompilation\n+        self.assertEqual(cnts.frame_count, 1)\n+\n+        # pop and add the same item\n+        s.remove(torch.amp._exit_autocast)\n+        s.add(torch.amp._exit_autocast)\n+\n+        x = torch.randn(4)\n+        res = opt_fn(x, s)\n+        # Check Dynamo don't recompile\n+        self.assertEqual(cnts.frame_count, 1)\n+        self.assertEqual(res, fn(x, s))\n+\n+",
        "comment_created_at": "2025-05-29T14:43:53+00:00",
        "comment_author": "zou3519",
        "comment_body": "Do we support sets of Tensors? If so, can we add some tests for those? I expect some of the guards might be too conservative but that's OK",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2064433335",
    "pr_number": 152161,
    "pr_file": "test/test_indexing.py",
    "created_at": "2025-04-28T19:52:50+00:00",
    "commented_code": "self.assertEqual(v[0].tolist(), [0, 3, 0, 4])\n        self.assertEqual(v[1:].sum(), 0)\n\n    def test_take_along_dim_negative_indices(self) -> None:",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2064433335",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 152161,
        "pr_file": "test/test_indexing.py",
        "discussion_id": "2064433335",
        "commented_code": "@@ -843,6 +843,13 @@ def test_step_assignment(self, device):\n         self.assertEqual(v[0].tolist(), [0, 3, 0, 4])\n         self.assertEqual(v[1:].sum(), 0)\n \n+    def test_take_along_dim_negative_indices(self) -> None:",
        "comment_created_at": "2025-04-28T19:52:50+00:00",
        "comment_author": "albanD",
        "comment_body": "You can update the generic test for take_along_dim to test these values in https://github.com/pytorch/pytorch/blob/5f4c8e4c896f9a1d89e1bf9b3bd92b3e7db5c3b9/torch/testing/_internal/common_methods_invocations.py#L2988\r\nThis is better than a one-off test.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2175646168",
    "pr_number": 156572,
    "pr_file": "test/inductor/test_gpu_cpp_wrapper.py",
    "created_at": "2025-06-30T18:22:03+00:00",
    "commented_code": "device=None,\n            tests=test_select_algorithm.TestSelectAlgorithm(),\n        ),\n        BaseTest(",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2175646168",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156572,
        "pr_file": "test/inductor/test_gpu_cpp_wrapper.py",
        "discussion_id": "2175646168",
        "commented_code": "@@ -291,6 +294,36 @@ class BaseTest(NamedTuple):\n             device=None,\n             tests=test_select_algorithm.TestSelectAlgorithm(),\n         ),\n+        BaseTest(",
        "comment_created_at": "2025-06-30T18:22:03+00:00",
        "comment_author": "desertfire",
        "comment_body": "We actually test way more cpp-wrapper mode than tests in this file, see https://github.com/pytorch/pytorch/blob/c7b6c98d1097bec9dc92bde2fe324aa126a5daa2/.ci/pytorch/test.sh#L462-L464. Can we do the same for XPU and stop adding tests to this file?",
        "pr_file_module": null
      },
      {
        "comment_id": "2178781741",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156572,
        "pr_file": "test/inductor/test_gpu_cpp_wrapper.py",
        "discussion_id": "2175646168",
        "commented_code": "@@ -291,6 +294,36 @@ class BaseTest(NamedTuple):\n             device=None,\n             tests=test_select_algorithm.TestSelectAlgorithm(),\n         ),\n+        BaseTest(",
        "comment_created_at": "2025-07-02T00:56:07+00:00",
        "comment_author": "etaf",
        "comment_body": "@desertfire: Sure, thanks for your advice.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2172786579",
    "pr_number": 157136,
    "pr_file": "test/dynamo/test_fx_graph_runnable.py",
    "created_at": "2025-06-27T19:59:14+00:00",
    "commented_code": "torch.compile(f)(torch.randn(5))\n        self._exec_and_verify_payload()\n\n    # testing dynamic shapes\n    def test_dynamic_shapes_run(self):\n        torch._dynamo.reset()\n        torch._dynamo.config.dynamic_shapes = True\n\n        def f(x):\n            return (x @ x.transpose(0, 1)).relu()\n\n        a = torch.randn(10, 12)\n        torch._dynamo.mark_dynamic(a, 0)\n        torch._dynamo.mark_dynamic(a, 1)\n\n        torch.compile(f)(a)\n        self._exec_and_verify_payload()\n\n    def test_broadcast_add_dynamic(self):\n        torch._dynamo.reset()\n        torch._dynamo.config.dynamic_shapes = True\n\n        def f(x, y):\n            return x + y * 2\n\n        x = torch.randn(5, 1)\n        y = torch.randn(1, 8)\n        torch._dynamo.mark_dynamic(x, 0)\n        torch._dynamo.mark_dynamic(y, 1)\n\n        torch.compile(f)(x, y)\n        self._exec_and_verify_payload()\n\n    def test_toy_model_basic(self):\n        model = ToyModel(input_size=8, hidden_size=16, output_size=4)\n        model.eval()  # Set to eval mode to avoid dropout randomness\n\n        x = torch.randn(3, 8)\n        torch.compile(model)(x)\n        self._exec_and_verify_payload()\n\n    def test_toy_model_batch_processing(self):\n        model = ToyModel(input_size=12, hidden_size=24, output_size=6)\n        model.eval()\n\n        x = torch.randn(16, 12)\n        torch.compile(model)(x)\n        self._exec_and_verify_payload()\n\n    def test_toy_model_dynamic_batch(self):\n        torch._dynamo.reset()",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2172786579",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157136,
        "pr_file": "test/dynamo/test_fx_graph_runnable.py",
        "discussion_id": "2172786579",
        "commented_code": "@@ -90,6 +106,65 @@ def f(x):\n         torch.compile(f)(torch.randn(5))\n         self._exec_and_verify_payload()\n \n+    # testing dynamic shapes\n+    def test_dynamic_shapes_run(self):\n+        torch._dynamo.reset()\n+        torch._dynamo.config.dynamic_shapes = True\n+\n+        def f(x):\n+            return (x @ x.transpose(0, 1)).relu()\n+\n+        a = torch.randn(10, 12)\n+        torch._dynamo.mark_dynamic(a, 0)\n+        torch._dynamo.mark_dynamic(a, 1)\n+\n+        torch.compile(f)(a)\n+        self._exec_and_verify_payload()\n+\n+    def test_broadcast_add_dynamic(self):\n+        torch._dynamo.reset()\n+        torch._dynamo.config.dynamic_shapes = True\n+\n+        def f(x, y):\n+            return x + y * 2\n+\n+        x = torch.randn(5, 1)\n+        y = torch.randn(1, 8)\n+        torch._dynamo.mark_dynamic(x, 0)\n+        torch._dynamo.mark_dynamic(y, 1)\n+\n+        torch.compile(f)(x, y)\n+        self._exec_and_verify_payload()\n+\n+    def test_toy_model_basic(self):\n+        model = ToyModel(input_size=8, hidden_size=16, output_size=4)\n+        model.eval()  # Set to eval mode to avoid dropout randomness\n+\n+        x = torch.randn(3, 8)\n+        torch.compile(model)(x)\n+        self._exec_and_verify_payload()\n+\n+    def test_toy_model_batch_processing(self):\n+        model = ToyModel(input_size=12, hidden_size=24, output_size=6)\n+        model.eval()\n+\n+        x = torch.randn(16, 12)\n+        torch.compile(model)(x)\n+        self._exec_and_verify_payload()\n+\n+    def test_toy_model_dynamic_batch(self):\n+        torch._dynamo.reset()",
        "comment_created_at": "2025-06-27T19:59:14+00:00",
        "comment_author": "xmfan",
        "comment_body": "let's move this to the test `setUp`",
        "pr_file_module": null
      }
    ]
  }
]