[
  {
    "discussion_id": "1994632241",
    "pr_number": 146821,
    "pr_file": "torch/utils/data/_utils/pin_memory.py",
    "created_at": "2025-03-14T03:12:11+00:00",
    "commented_code": "from . import MP_STATUS_CHECK_INTERVAL\n \n \n-def _pin_memory_loop(in_queue, out_queue, device_id, done_event, device):\n+def _pin_memory_loop(in_queue, out_queue, done_event, device):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1994632241",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 146821,
        "pr_file": "torch/utils/data/_utils/pin_memory.py",
        "discussion_id": "1994632241",
        "commented_code": "@@ -15,22 +15,13 @@\n from . import MP_STATUS_CHECK_INTERVAL\n \n \n-def _pin_memory_loop(in_queue, out_queue, device_id, done_event, device):\n+def _pin_memory_loop(in_queue, out_queue, done_event, device):",
        "comment_created_at": "2025-03-14T03:12:11+00:00",
        "comment_author": "zeshengzong",
        "comment_body": "I think `device_id` param may not needed, the caller pass value of `torch.accelerator.current_device_index()` and here we set it to `torch.accelerator.set_device_index(device_id)`, looks like its the same value, be like\r\n\r\n```python\r\ncurrent_device_index = torch.accelerator.current_device_index()\r\ntorch.accelerator.set_device_index(current_device_index)\r\n```\r\n\r\nSo remove the set logic in below",
        "pr_file_module": null
      },
      {
        "comment_id": "2035667982",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 146821,
        "pr_file": "torch/utils/data/_utils/pin_memory.py",
        "discussion_id": "1994632241",
        "commented_code": "@@ -15,22 +15,13 @@\n from . import MP_STATUS_CHECK_INTERVAL\n \n \n-def _pin_memory_loop(in_queue, out_queue, device_id, done_event, device):\n+def _pin_memory_loop(in_queue, out_queue, done_event, device):",
        "comment_created_at": "2025-04-09T15:52:13+00:00",
        "comment_author": "albanD",
        "comment_body": "But this is called from a different thread. So we have to set the device again !\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2036363100",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 146821,
        "pr_file": "torch/utils/data/_utils/pin_memory.py",
        "discussion_id": "1994632241",
        "commented_code": "@@ -15,22 +15,13 @@\n from . import MP_STATUS_CHECK_INTERVAL\n \n \n-def _pin_memory_loop(in_queue, out_queue, device_id, done_event, device):\n+def _pin_memory_loop(in_queue, out_queue, done_event, device):",
        "comment_created_at": "2025-04-10T01:39:31+00:00",
        "comment_author": "zeshengzong",
        "comment_body": "Oh, not aware of this point, restored change, thanks!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2186284433",
    "pr_number": 157650,
    "pr_file": "test/inductor/test_compile_subprocess.py",
    "created_at": "2025-07-05T01:00:47+00:00",
    "commented_code": "self.assertGreater(\n             do_bench(lambda: baseline(x, y)), do_bench(lambda: optimized(x, y))\n         )\n+        self.assertTrue(\"'max_autotune': True\" in source_codes[-1])",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2186284433",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157650,
        "pr_file": "test/inductor/test_compile_subprocess.py",
        "discussion_id": "2186284433",
        "commented_code": "@@ -155,6 +165,7 @@ def baseline(x, y):\n         self.assertGreater(\n             do_bench(lambda: baseline(x, y)), do_bench(lambda: optimized(x, y))\n         )\n+        self.assertTrue(\"'max_autotune': True\" in source_codes[-1])",
        "comment_created_at": "2025-07-05T01:00:47+00:00",
        "comment_author": "aorenste",
        "comment_body": "I guess it would be a race condition to expect there to always be two...\r\nConsider something like:\r\n```\r\nself.assertTrue(len(source_codes) == 1 or \"'max_autotune': True\" not in source_codes[0])\r\nself.assertTrue(\"'max_autotune': True\" in source_codes[-1])\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2186608885",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157650,
        "pr_file": "test/inductor/test_compile_subprocess.py",
        "discussion_id": "2186284433",
        "commented_code": "@@ -155,6 +165,7 @@ def baseline(x, y):\n         self.assertGreater(\n             do_bench(lambda: baseline(x, y)), do_bench(lambda: optimized(x, y))\n         )\n+        self.assertTrue(\"'max_autotune': True\" in source_codes[-1])",
        "comment_created_at": "2025-07-05T04:29:39+00:00",
        "comment_author": "bobrenjc93",
        "comment_body": "It actually isn't a race condition since we barrier on the first compile finishing before checking the futures:\r\n\r\n```\r\n        if self.post_compile_data:\r\n            for i, future in enumerate(self.progression_futures):\r\n                if future.done():\r\n                    stage_index = i\r\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2181068885",
    "pr_number": 157396,
    "pr_file": "torch/_inductor/fx_passes/bucketing.py",
    "created_at": "2025-07-02T22:15:17+00:00",
    "commented_code": "+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2181068885",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T22:15:17+00:00",
        "comment_author": "wconstab",
        "comment_body": "this assumption is unfortunately too narrow.  In the FSDP + TP case, there will be all_fathers on multiple process groups, and this assert fails.\r\n\r\nI wonder if anybody has considered TP all_gathers yet in bucketing pass design?\r\ncc @ruisizhang123 ",
        "pr_file_module": null
      },
      {
        "comment_id": "2181079256",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T22:23:55+00:00",
        "comment_author": "wconstab",
        "comment_body": "There might be a simple fix here.\r\nWhat if instead of asserting on 'same group', we just tweak the logic a bit:\r\n- one \"bucket\" can now contain allgathers from any number of processgroups\r\n- when it comes time to issue the 'bucketed allgather', instead of issuing one allgather per bucket, we issue one allgather per process-group per bucket\r\n- bucketing assignment logic could be left alone for now\r\n\r\nIn the future, bucketing assignment logic could be improved, for example if we have the pattern of \r\n(TP-allgather, FSDP-allgather) for each param, it probably makes sense to either put both of those into a bucket or neither of them.  But for now we can let them be separated, i think.",
        "pr_file_module": null
      },
      {
        "comment_id": "2181108895",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T22:51:39+00:00",
        "comment_author": "ruisizhang123",
        "comment_body": "1. In inductor, we look into the metadata trace of each IR node. If it contains `Reparameterization`, which is why SimpleFSDP ensures the graph traceable, we consdier the node as FSDP all-gather node. \r\n\r\n> considered TP all_gathers yet in bucketing pass design\r\n\r\n2. You mean bucket TP all-gathers?  I didn't try, but seems doable to me lolll.",
        "pr_file_module": null
      },
      {
        "comment_id": "2181117046",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T22:57:55+00:00",
        "comment_author": "wconstab",
        "comment_body": "when we combine TP + FSDP and perform bucketing, it seems pointless / impossible to perform any bucketing on FSDP collectives if we do not also perform bucketing on TP collectives.\r\n\r\nor am i missing something? did we handle TP a different way in the simple-fsdp experiments?",
        "pr_file_module": null
      },
      {
        "comment_id": "2181118611",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T22:59:38+00:00",
        "comment_author": "ruisizhang123",
        "comment_body": "yes, we rewrote tp re-distribute logic here: https://github.com/pytorch/torchtitan/blob/a04f6bdb4a8b38bcce1157156de7f9b2af734fc6/torchtitan/experiments/simple_fsdp/simple_fsdp.py#L52-L125",
        "pr_file_module": null
      },
      {
        "comment_id": "2181123539",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T23:05:22+00:00",
        "comment_author": "wconstab",
        "comment_body": "hmm even in that code, i think i see a 2-D DTensor with stridedshard.  So I assume for FSDP to work it would trigger DTensor to issue 2 separate all_gathers per parameter, one per mesh dimension.  If so, then doesn't my above claim still apply?",
        "pr_file_module": null
      },
      {
        "comment_id": "2181144658",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T23:26:23+00:00",
        "comment_author": "ruisizhang123",
        "comment_body": "Yes, it triggers 2 separate AGs. Not sure there are any difference in doing this in FX-level versus Inductor-level. In inductor, we just pick out the FSDP AGs, and bucket them. It prefetches the one-dim parameters ahead of time. The rest TP AGs gathers things in their original order.  Not sure there will be any blockers for doing this? \r\n\r\nThis is what the profile trace looks like (The upper one is FSDP, the lower one is TP)\r\n\r\n<img width=\"1185\" alt=\"Screenshot 2025-07-02 at 4 17 48PM\" src=\"https://github.com/user-attachments/assets/672fbab8-b6cb-4f47-bb0f-b48a8386d53a\" />\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2181148809",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T23:30:23+00:00",
        "comment_author": "wconstab",
        "comment_body": "sorry, i have just been saying nonsense. \r\n\r\nthere shouldn't even be allgathers for TP, should there?\r\n\r\nI think what was happening to me is the autoparallel solver was picking to do FSDP on the TP dim, and so it was really like fsdp(fsdp(param)).  If there is TP, we should be doing the sharded matmul and an allreduce.. ",
        "pr_file_module": null
      },
      {
        "comment_id": "2181166719",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157396,
        "pr_file": "torch/_inductor/fx_passes/bucketing.py",
        "discussion_id": "2181068885",
        "commented_code": "@@ -0,0 +1,431 @@\n+import logging\n+import operator\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch._dispatch.python import enable_python_dispatcher\n+from torch._inductor.virtualized import V\n+from torch.utils._ordered_set import OrderedSet\n+\n+\n+logger: logging.Logger = logging.getLogger(__name__)\n+logger.setLevel(logging.INFO)\n+\n+\n+def bucket_size_determinator(bucket_id: int) -> float:\n+    \"\"\"\n+    Determine the size of a bucket based on its ID.\n+\n+    Args:\n+    bucket_id (int): The ID of the bucket.\n+\n+    Returns:\n+    float: The size of the bucket.\n+    \"\"\"\n+    return 2000.0\n+\n+\n+def bucket_all_gather(\n+    gm: torch.fx.GraphModule, all_gather_bucket_cap_mb_callback: Callable[[int], float]\n+) -> None:\n+    ag_buckets = bucket_all_gather_by_mb(gm, all_gather_bucket_cap_mb_callback)\n+    if len(ag_buckets) == 0:\n+        return\n+    merge_all_gather(gm, ag_buckets)\n+\n+\n+def is_all_gather_into_tensor(node: torch.fx.Node) -> bool:  # type: ignore[arg-type]\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.all_gather_into_tensor.default\n+    )\n+\n+\n+def is_wait_tensor(node: torch.fx.Node) -> bool:\n+    return (\n+        node.op == \"call_function\"\n+        and node.target == torch.ops._c10d_functional.wait_tensor.default\n+    )\n+\n+\n+def is_wait_tensor_from_all_gather_into_tensor(node: torch.fx.Node) -> bool:\n+    return is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0])  # type: ignore[arg-type]\n+\n+\n+def bucket_all_gather_by_mb(\n+    gm: torch.fx.GraphModule,\n+    all_gather_bucket_cap_mb_callback: Callable[[int], float],\n+    filter_wait_node: Optional[Callable[[torch.fx.Node], bool]] = None,\n+) -> list[list[torch.fx.Node]]:\n+    \"\"\"\n+    Identifies all all_gather nodes and groups them into buckets based on size limit `all_gather_bucket_cap_mb_callback`.\n+\n+\n+    Returns a list of buckets, where each bucket is a list of all_gather nodes.\n+    \"\"\"\n+\n+    node_list = gm.graph.nodes\n+\n+    # Prerequisite: Check if there is any all_gather node\n+    found_all_gather = False\n+    for node in node_list:\n+        if is_all_gather_into_tensor(node):\n+            found_all_gather = True\n+            break\n+    if not found_all_gather:\n+        return []\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+\n+    # Step 1: Find all all_gather nodes\n+    for node in node_list:\n+        if is_wait_tensor(node) and is_all_gather_into_tensor(node.args[0]):\n+            if (filter_wait_node is None) or filter_wait_node(node):\n+                ag_node = node.args[0]\n+                ag_nodes.append(ag_node)\n+\n+    # Step 2: Put all_gather nodes into buckets\n+    ag_buckets: list[list[torch.fx.Node]] = []\n+    cur_bucket: list[torch.fx.Node] = []\n+    cur_bucket_size_bytes: int = 0\n+    cur_bucket_id: int = 0\n+    # Convert MiB to bytes\n+    all_gather_bucket_size_bytes = int(\n+        all_gather_bucket_cap_mb_callback(cur_bucket_id) * 1024 * 1024\n+    )\n+    for ag_node in ag_nodes:\n+        assert is_all_gather_into_tensor(ag_node)\n+        assert \"val\" in ag_node.meta\n+        ag_output_size_bytes = (\n+            ag_node.meta[\"val\"].numel()\n+            * torch.finfo(ag_node.meta[\"val\"].dtype).bits\n+            // 8\n+        )\n+        if (\n+            cur_bucket_size_bytes + ag_output_size_bytes > all_gather_bucket_size_bytes\n+            and cur_bucket\n+        ):\n+            # Current bucket is full, create new bucket\n+            ag_buckets.append(cur_bucket)\n+            cur_bucket = []\n+            cur_bucket_size_bytes = 0\n+            cur_bucket_id += 1\n+        cur_bucket_size_bytes += ag_output_size_bytes\n+        cur_bucket.append(ag_node)\n+    if cur_bucket:\n+        # add remaining nodes in the last bucket\n+        ag_buckets.append(cur_bucket)\n+\n+    return ag_buckets\n+\n+\n+def node_copy(  # type: ignore[no-untyped-def]\n+    env,\n+    new_graph,\n+    node: torch.fx.Node,\n+    arg_transform: Callable[[torch.fx.Node], torch.fx.node.Argument],\n+) -> torch.fx.Node:\n+    if node not in env:\n+        new_node = new_graph.node_copy(node, arg_transform=arg_transform)\n+        env[node] = new_node\n+    else:\n+        new_node = env[node]\n+    return new_node\n+\n+\n+def new_graph_call_function(  # type: ignore[no-untyped-def]\n+    new_graph,\n+    target: Callable[..., Any],\n+    args: Optional[tuple[torch.fx.node.Argument, ...]] = None,\n+    kwargs: Optional[dict[str, torch.fx.node.Argument]] = None,\n+    type_expr: Optional[Any] = None,\n+) -> torch.fx.Node:\n+    from torch.utils._pytree import tree_map_only\n+\n+    new_node = new_graph.call_function(target, args, kwargs)\n+    args_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], args)\n+    kwargs_val = tree_map_only(torch.fx.Node, lambda x: x.meta[\"val\"], kwargs)\n+    with V.fake_mode, enable_python_dispatcher():\n+        new_fake_tensor = target(*args_val, **kwargs_val)\n+    new_node.meta[\"val\"] = new_fake_tensor\n+    return new_node\n+\n+\n+def env_lookup(  # type: ignore[no-untyped-def]\n+    env, x: torch.fx.Node, node_user: Union[torch.fx.Node, str]\n+) -> torch.fx.Node:\n+    assert x in env, (\n+        f\"Dependent node {x} not in env when creating downstream node {node_user}\"\n+    )\n+    return env[x]\n+\n+\n+def merge_all_gather(\n+    gm: torch.fx.GraphModule, ag_buckets: list[list[torch.fx.Node]]\n+) -> None:\n+    \"\"\"\n+    Transforms the graph to use bucketed all_gather operations based on `ag_buckets`.\n+    \"\"\"\n+    assert len(ag_buckets) > 0\n+\n+    ag_nodes: list[torch.fx.Node] = []\n+    cast_nodes: list[torch.fx.Node] = []\n+    ag_node_to_wait_node: dict[torch.fx.Node, torch.fx.Node] = {}\n+    ag_node_to_bucket_id = {}\n+    cast_node_to_bucket_id = {}\n+\n+    # Map nodes to buckets and identify wait nodes\n+    for bucket_id, bucket in enumerate(ag_buckets):\n+        for ag_node in bucket:\n+            assert len(ag_node.users) == 1, (\n+                f\"Expect only one user for {ag_node}, but got {ag_node.users}\"\n+            )\n+            wait_node = next(iter(ag_node.users))\n+            ag_node_to_wait_node[ag_node] = wait_node\n+            ag_nodes.append(ag_node)\n+            ag_node_to_bucket_id[ag_node] = bucket_id\n+            if (\n+                ag_node.args[0].op == \"call_function\"  # type: ignore[union-attr]\n+                and ag_node.args[0].target  # type: ignore[union-attr]\n+                == torch.ops.prims.convert_element_type.default\n+            ):\n+                cast_nodes.append(ag_node.args[0])  # type: ignore[arg-type]\n+                cast_node_to_bucket_id[ag_node.args[0]] = bucket_id  # type: ignore[arg-type]\n+\n+    # Step 3: Create new (bucketed) all_gather nodes\n+    bucket_id_to_bucketed_op_info = {}\n+    bucket_id_is_scheduled = {}\n+    cast_bucket_id_is_scheduled = {}\n+    _, group_size, group_name = next(iter(ag_node_to_wait_node.keys())).args\n+    for bucket_id, ag_bucket in enumerate(ag_buckets):\n+        ag_input_nodes = []\n+        wait_nodes = []\n+        for ag_node in ag_bucket:\n+            assert (\n+                ag_node in ag_node_to_wait_node\n+                and ag_node.args[1] == group_size\n+                and ag_node.args[2] == group_name",
        "comment_created_at": "2025-07-02T23:47:07+00:00",
        "comment_author": "ruisizhang123",
        "comment_body": "yes, sorry for not familiar with tp comm details....",
        "pr_file_module": null
      }
    ]
  }
]