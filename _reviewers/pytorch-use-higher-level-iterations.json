[
  {
    "discussion_id": "2191320362",
    "pr_number": 157658,
    "pr_file": "test/distributions/test_distributions.py",
    "created_at": "2025-07-08T01:56:13+00:00",
    "commented_code": "assert (vals == 0.0).sum() > 4000\n         assert (vals == 1.0).sum() > 4000\n \n+    def test_binomial_dtype_error(self):\n+        dtypes = [torch.int, torch.long, torch.short]\n+        for count_dtype in dtypes:\n+            for prob_dtype in dtypes:",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2191320362",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157658,
        "pr_file": "test/distributions/test_distributions.py",
        "discussion_id": "2191320362",
        "commented_code": "@@ -1805,6 +1805,19 @@ def test_zero_excluded_binomial(self):\n         assert (vals == 0.0).sum() > 4000\n         assert (vals == 1.0).sum() > 4000\n \n+    def test_binomial_dtype_error(self):\n+        dtypes = [torch.int, torch.long, torch.short]\n+        for count_dtype in dtypes:\n+            for prob_dtype in dtypes:",
        "comment_created_at": "2025-07-08T01:56:13+00:00",
        "comment_author": "malfet",
        "comment_body": "Nit\r\n```suggestion\r\n        for count_dtype, prob_dtype in itertools.product(dtypes, repeat=2):\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2191494016",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157658,
        "pr_file": "test/distributions/test_distributions.py",
        "discussion_id": "2191320362",
        "commented_code": "@@ -1805,6 +1805,19 @@ def test_zero_excluded_binomial(self):\n         assert (vals == 0.0).sum() > 4000\n         assert (vals == 1.0).sum() > 4000\n \n+    def test_binomial_dtype_error(self):\n+        dtypes = [torch.int, torch.long, torch.short]\n+        for count_dtype in dtypes:\n+            for prob_dtype in dtypes:",
        "comment_created_at": "2025-07-08T05:13:13+00:00",
        "comment_author": "michellemadubuike",
        "comment_body": "Changed to using 2 different for loops for count and prob",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178578471",
    "pr_number": 157305,
    "pr_file": "torch/_inductor/compile_fx_async.py",
    "created_at": "2025-07-01T21:34:01+00:00",
    "commented_code": "return output.graph\n \n         return _AsyncOutputCode(eager_output_code, f, callback)\n+\n+\n+# _ProgressiveOutputCode handles running a fast compile first, then hot-swapping\n+# to a more optimized version when the expensive compile finishes.\n+@final\n+class _ProgressiveOutputCode(OutputCode):\n+    _fast_output_code: Optional[OutputCode]\n+    _optimized_output_code: Optional[OutputCode]\n+    _progression_futures: list[Future[_WireProtocolPickledOutput]]\n+    _callback: Callable[[_WireProtocolPickledOutput], OutputCode]\n+    _post_compile_data: Optional[_PostCompileData] = None\n+    _boxed_call: bool\n+    _current_progression_index: int\n+\n+    def __init__(\n+        self,\n+        # Fast compile that runs faster than the progressive compiles\n+        fast_output_code: OutputCode,\n+        # Futures for the progressive optimized compiles\n+        progression_futures: list[Future[_WireProtocolPickledOutput]],\n+        # Callback to convert the optimized result to OutputCode\n+        callback: Callable[[_WireProtocolPickledOutput], OutputCode],\n+    ) -> None:\n+        self._fast_output_code = fast_output_code\n+        self._optimized_output_code = None\n+        self._progression_futures = progression_futures\n+        self._callback = callback\n+        self._boxed_call = getattr(fast_output_code, \"_boxed_call\", False)\n+        self._current_progression_index = -1\n+\n+    @override\n+    def __call__(self, *args: Any) -> Any:\n+        # Check if any newer progression stage is ready and switch to it\n+        args = self._check_and_switch_progression(args)\n+\n+        if self._optimized_output_code is not None:\n+            _ProgressiveFxCompile._stat_optimized_runs += 1\n+            return self._optimized_output_code.__call__(*args)\n+        else:\n+            _ProgressiveFxCompile._stat_fast_runs += 1\n+            assert self._fast_output_code is not None\n+            return self._fast_output_code.__call__(*args)\n+\n+    def _check_and_switch_progression(self, args: tuple[Any, ...]) -> tuple[Any, ...]:\n+        # Check if any newer progression stage is ready (in order from latest to earliest)\n+        for i in range(\n+            len(self._progression_futures) - 1, self._current_progression_index, -1\n+        ):\n+            future = self._progression_futures[i]",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2178578471",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157305,
        "pr_file": "torch/_inductor/compile_fx_async.py",
        "discussion_id": "2178578471",
        "commented_code": "@@ -179,3 +179,183 @@ def callback(pickled_output: _WireProtocolPickledOutput) -> OutputCode:\n             return output.graph\n \n         return _AsyncOutputCode(eager_output_code, f, callback)\n+\n+\n+# _ProgressiveOutputCode handles running a fast compile first, then hot-swapping\n+# to a more optimized version when the expensive compile finishes.\n+@final\n+class _ProgressiveOutputCode(OutputCode):\n+    _fast_output_code: Optional[OutputCode]\n+    _optimized_output_code: Optional[OutputCode]\n+    _progression_futures: list[Future[_WireProtocolPickledOutput]]\n+    _callback: Callable[[_WireProtocolPickledOutput], OutputCode]\n+    _post_compile_data: Optional[_PostCompileData] = None\n+    _boxed_call: bool\n+    _current_progression_index: int\n+\n+    def __init__(\n+        self,\n+        # Fast compile that runs faster than the progressive compiles\n+        fast_output_code: OutputCode,\n+        # Futures for the progressive optimized compiles\n+        progression_futures: list[Future[_WireProtocolPickledOutput]],\n+        # Callback to convert the optimized result to OutputCode\n+        callback: Callable[[_WireProtocolPickledOutput], OutputCode],\n+    ) -> None:\n+        self._fast_output_code = fast_output_code\n+        self._optimized_output_code = None\n+        self._progression_futures = progression_futures\n+        self._callback = callback\n+        self._boxed_call = getattr(fast_output_code, \"_boxed_call\", False)\n+        self._current_progression_index = -1\n+\n+    @override\n+    def __call__(self, *args: Any) -> Any:\n+        # Check if any newer progression stage is ready and switch to it\n+        args = self._check_and_switch_progression(args)\n+\n+        if self._optimized_output_code is not None:\n+            _ProgressiveFxCompile._stat_optimized_runs += 1\n+            return self._optimized_output_code.__call__(*args)\n+        else:\n+            _ProgressiveFxCompile._stat_fast_runs += 1\n+            assert self._fast_output_code is not None\n+            return self._fast_output_code.__call__(*args)\n+\n+    def _check_and_switch_progression(self, args: tuple[Any, ...]) -> tuple[Any, ...]:\n+        # Check if any newer progression stage is ready (in order from latest to earliest)\n+        for i in range(\n+            len(self._progression_futures) - 1, self._current_progression_index, -1\n+        ):\n+            future = self._progression_futures[i]",
        "comment_created_at": "2025-07-01T21:34:01+00:00",
        "comment_author": "aorenste",
        "comment_body": "nit:\r\n```suggestion\r\n        for i, future in reversed(enumerate(self._progression_futures)):\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2178758584",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157305,
        "pr_file": "torch/_inductor/compile_fx_async.py",
        "discussion_id": "2178578471",
        "commented_code": "@@ -179,3 +179,183 @@ def callback(pickled_output: _WireProtocolPickledOutput) -> OutputCode:\n             return output.graph\n \n         return _AsyncOutputCode(eager_output_code, f, callback)\n+\n+\n+# _ProgressiveOutputCode handles running a fast compile first, then hot-swapping\n+# to a more optimized version when the expensive compile finishes.\n+@final\n+class _ProgressiveOutputCode(OutputCode):\n+    _fast_output_code: Optional[OutputCode]\n+    _optimized_output_code: Optional[OutputCode]\n+    _progression_futures: list[Future[_WireProtocolPickledOutput]]\n+    _callback: Callable[[_WireProtocolPickledOutput], OutputCode]\n+    _post_compile_data: Optional[_PostCompileData] = None\n+    _boxed_call: bool\n+    _current_progression_index: int\n+\n+    def __init__(\n+        self,\n+        # Fast compile that runs faster than the progressive compiles\n+        fast_output_code: OutputCode,\n+        # Futures for the progressive optimized compiles\n+        progression_futures: list[Future[_WireProtocolPickledOutput]],\n+        # Callback to convert the optimized result to OutputCode\n+        callback: Callable[[_WireProtocolPickledOutput], OutputCode],\n+    ) -> None:\n+        self._fast_output_code = fast_output_code\n+        self._optimized_output_code = None\n+        self._progression_futures = progression_futures\n+        self._callback = callback\n+        self._boxed_call = getattr(fast_output_code, \"_boxed_call\", False)\n+        self._current_progression_index = -1\n+\n+    @override\n+    def __call__(self, *args: Any) -> Any:\n+        # Check if any newer progression stage is ready and switch to it\n+        args = self._check_and_switch_progression(args)\n+\n+        if self._optimized_output_code is not None:\n+            _ProgressiveFxCompile._stat_optimized_runs += 1\n+            return self._optimized_output_code.__call__(*args)\n+        else:\n+            _ProgressiveFxCompile._stat_fast_runs += 1\n+            assert self._fast_output_code is not None\n+            return self._fast_output_code.__call__(*args)\n+\n+    def _check_and_switch_progression(self, args: tuple[Any, ...]) -> tuple[Any, ...]:\n+        # Check if any newer progression stage is ready (in order from latest to earliest)\n+        for i in range(\n+            len(self._progression_futures) - 1, self._current_progression_index, -1\n+        ):\n+            future = self._progression_futures[i]",
        "comment_created_at": "2025-07-02T00:22:43+00:00",
        "comment_author": "bobrenjc93",
        "comment_body": "This is technically less efficient right?\r\n\r\n- enumerate() returns an iterator\r\n- reversed() requires a sequence (not just an iterator), so it converts the iterator to a list internally\r\n- Then it reverses that materialized list\r\n\r\nwhereas range uses an iterator with no materialization? I guess the big O is negligible and we get readability benefits?\r\n",
        "pr_file_module": null
      },
      {
        "comment_id": "2180456097",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157305,
        "pr_file": "torch/_inductor/compile_fx_async.py",
        "discussion_id": "2178578471",
        "commented_code": "@@ -179,3 +179,183 @@ def callback(pickled_output: _WireProtocolPickledOutput) -> OutputCode:\n             return output.graph\n \n         return _AsyncOutputCode(eager_output_code, f, callback)\n+\n+\n+# _ProgressiveOutputCode handles running a fast compile first, then hot-swapping\n+# to a more optimized version when the expensive compile finishes.\n+@final\n+class _ProgressiveOutputCode(OutputCode):\n+    _fast_output_code: Optional[OutputCode]\n+    _optimized_output_code: Optional[OutputCode]\n+    _progression_futures: list[Future[_WireProtocolPickledOutput]]\n+    _callback: Callable[[_WireProtocolPickledOutput], OutputCode]\n+    _post_compile_data: Optional[_PostCompileData] = None\n+    _boxed_call: bool\n+    _current_progression_index: int\n+\n+    def __init__(\n+        self,\n+        # Fast compile that runs faster than the progressive compiles\n+        fast_output_code: OutputCode,\n+        # Futures for the progressive optimized compiles\n+        progression_futures: list[Future[_WireProtocolPickledOutput]],\n+        # Callback to convert the optimized result to OutputCode\n+        callback: Callable[[_WireProtocolPickledOutput], OutputCode],\n+    ) -> None:\n+        self._fast_output_code = fast_output_code\n+        self._optimized_output_code = None\n+        self._progression_futures = progression_futures\n+        self._callback = callback\n+        self._boxed_call = getattr(fast_output_code, \"_boxed_call\", False)\n+        self._current_progression_index = -1\n+\n+    @override\n+    def __call__(self, *args: Any) -> Any:\n+        # Check if any newer progression stage is ready and switch to it\n+        args = self._check_and_switch_progression(args)\n+\n+        if self._optimized_output_code is not None:\n+            _ProgressiveFxCompile._stat_optimized_runs += 1\n+            return self._optimized_output_code.__call__(*args)\n+        else:\n+            _ProgressiveFxCompile._stat_fast_runs += 1\n+            assert self._fast_output_code is not None\n+            return self._fast_output_code.__call__(*args)\n+\n+    def _check_and_switch_progression(self, args: tuple[Any, ...]) -> tuple[Any, ...]:\n+        # Check if any newer progression stage is ready (in order from latest to earliest)\n+        for i in range(\n+            len(self._progression_futures) - 1, self._current_progression_index, -1\n+        ):\n+            future = self._progression_futures[i]",
        "comment_created_at": "2025-07-02T16:09:24+00:00",
        "comment_author": "aorenste",
        "comment_body": "Yeah - I was going for readability. But apparently you can't do `reversed(enumerate(...))` (you need to convert to a list) so that makes it less appealing...\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2118105891",
    "pr_number": 154761,
    "pr_file": "torch/_inductor/codegen/cuda/gemm_template.py",
    "created_at": "2025-05-31T17:45:25+00:00",
    "commented_code": "Returns:\n             bool: True if layouts are GEMM compatible, otherwise False.\n         \"\"\"\n-        assert len(layouts) == 2 or len(layouts) == 3 or len(layouts) == 4\n+        assert len(layouts) >= 2 and len(layouts) <= 5",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2118105891",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 154761,
        "pr_file": "torch/_inductor/codegen/cuda/gemm_template.py",
        "discussion_id": "2118105891",
        "commented_code": "@@ -1308,7 +1308,7 @@ def _are_inputs_layout_compatible(self, layouts: list[Layout]) -> bool:\n         Returns:\n             bool: True if layouts are GEMM compatible, otherwise False.\n         \"\"\"\n-        assert len(layouts) == 2 or len(layouts) == 3 or len(layouts) == 4\n+        assert len(layouts) >= 2 and len(layouts) <= 5",
        "comment_created_at": "2025-05-31T17:45:25+00:00",
        "comment_author": "Skylion007",
        "comment_body": "```suggestion\r\n        assert 2<= len(layouts) <= 5",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2116629641",
    "pr_number": 153150,
    "pr_file": "torch/_dynamo/utils.py",
    "created_at": "2025-05-30T21:00:17+00:00",
    "commented_code": "return next(itertools.islice(dict_class.keys(d), n, n + 1))\n \n \n+def set_getitem(s, n):\n+    # Mimic set.__getitem__ by sorting the set using the hash\n+    return sorted(s, key=hash)[n]",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2116629641",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153150,
        "pr_file": "torch/_dynamo/utils.py",
        "discussion_id": "2116629641",
        "commented_code": "@@ -2530,6 +2530,11 @@ def dict_keys_getitem(d, n):\n     return next(itertools.islice(dict_class.keys(d), n, n + 1))\n \n \n+def set_getitem(s, n):\n+    # Mimic set.__getitem__ by sorting the set using the hash\n+    return sorted(s, key=hash)[n]",
        "comment_created_at": "2025-05-30T21:00:17+00:00",
        "comment_author": "williamwen42",
        "comment_body": "How will we deal with hash collisions? (What if the object has a custom hash that always returns 0?)",
        "pr_file_module": null
      },
      {
        "comment_id": "2122042749",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153150,
        "pr_file": "torch/_dynamo/utils.py",
        "discussion_id": "2116629641",
        "commented_code": "@@ -2530,6 +2530,11 @@ def dict_keys_getitem(d, n):\n     return next(itertools.islice(dict_class.keys(d), n, n + 1))\n \n \n+def set_getitem(s, n):\n+    # Mimic set.__getitem__ by sorting the set using the hash\n+    return sorted(s, key=hash)[n]",
        "comment_created_at": "2025-06-02T20:00:34+00:00",
        "comment_author": "zou3519",
        "comment_body": "Does this lead to anything worse than a recompile in the current PR?",
        "pr_file_module": null
      },
      {
        "comment_id": "2122108696",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153150,
        "pr_file": "torch/_dynamo/utils.py",
        "discussion_id": "2116629641",
        "commented_code": "@@ -2530,6 +2530,11 @@ def dict_keys_getitem(d, n):\n     return next(itertools.islice(dict_class.keys(d), n, n + 1))\n \n \n+def set_getitem(s, n):\n+    # Mimic set.__getitem__ by sorting the set using the hash\n+    return sorted(s, key=hash)[n]",
        "comment_created_at": "2025-06-02T20:37:57+00:00",
        "comment_author": "guilhermeleobas",
        "comment_body": "I don't think so. I couldn't actually create a test case for this because we don't support set of objects with custom hash functions:\r\nhttps://github.com/pytorch/pytorch/blob/48807d568ec5d4ad654fff27aaa1cec3b715c473/torch/_dynamo/variables/dicts.py#L101-L106",
        "pr_file_module": null
      },
      {
        "comment_id": "2122437629",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153150,
        "pr_file": "torch/_dynamo/utils.py",
        "discussion_id": "2116629641",
        "commented_code": "@@ -2530,6 +2530,11 @@ def dict_keys_getitem(d, n):\n     return next(itertools.islice(dict_class.keys(d), n, n + 1))\n \n \n+def set_getitem(s, n):\n+    # Mimic set.__getitem__ by sorting the set using the hash\n+    return sorted(s, key=hash)[n]",
        "comment_created_at": "2025-06-03T01:41:11+00:00",
        "comment_author": "guilhermeleobas",
        "comment_body": "If recompilation is the worst thing we can have, then one can just do `list(set_obj)[index]` instead. It will be faster than sorting the elements by the hash or calling `list(dict.fromkeys(set_object))[index]`",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2140304805",
    "pr_number": 153150,
    "pr_file": "torch/_dynamo/variables/builder.py",
    "created_at": "2025-06-11T14:11:47+00:00",
    "commented_code": "var = TorchFunctionModeVariable(value, source=self.source)\n             self.tx.output.side_effects.track_object_existing(value, var)\n             return var\n+        elif istype(value, set):\n+            self.install_guards(GuardBuilder.TYPE_MATCH)\n+            self.install_guards(GuardBuilder.SEQUENCE_LENGTH)\n+\n+            # The dictionary gives a ordering for the set items\n+            L = list(value)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2140304805",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 153150,
        "pr_file": "torch/_dynamo/variables/builder.py",
        "discussion_id": "2140304805",
        "commented_code": "@@ -755,6 +756,18 @@ def build_key_value(i, k, v):\n             var = TorchFunctionModeVariable(value, source=self.source)\n             self.tx.output.side_effects.track_object_existing(value, var)\n             return var\n+        elif istype(value, set):\n+            self.install_guards(GuardBuilder.TYPE_MATCH)\n+            self.install_guards(GuardBuilder.SEQUENCE_LENGTH)\n+\n+            # The dictionary gives a ordering for the set items\n+            L = list(value)",
        "comment_created_at": "2025-06-11T14:11:47+00:00",
        "comment_author": "zou3519",
        "comment_body": "Can you leave a comment here spifying:\r\n1) set ordering in cpython is based on hash value order\r\n2) the order being incorrect would just lead to a recompilation (no silent incorrectness)",
        "pr_file_module": null
      }
    ]
  }
]