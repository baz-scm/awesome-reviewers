[
  {
    "discussion_id": "2148479631",
    "pr_number": 155998,
    "pr_file": "setup.py",
    "created_at": "2025-06-15T16:41:57+00:00",
    "commented_code": "os.path.isdir(folder) and len(os.listdir(folder)) == 0\n        )\n\n    if bool(os.getenv(\"USE_SYSTEM_LIBS\", False)):\n    if os.getenv(\"USE_SYSTEM_LIBS\"):",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2148479631",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155998,
        "pr_file": "setup.py",
        "discussion_id": "2148479631",
        "commented_code": "@@ -413,7 +426,7 @@ def not_exists_or_empty(folder):\n             os.path.isdir(folder) and len(os.listdir(folder)) == 0\n         )\n \n-    if bool(os.getenv(\"USE_SYSTEM_LIBS\", False)):\n+    if os.getenv(\"USE_SYSTEM_LIBS\"):",
        "comment_created_at": "2025-06-15T16:41:57+00:00",
        "comment_author": "malfet",
        "comment_body": "Again, this is unrelated to the PR in question and also kind of semantically broken (it should use `str2bool` or something helper) that would convert 0 and \"false\" strings to false",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2178277997",
    "pr_number": 157374,
    "pr_file": "torch/cuda/_memory_viz.py",
    "created_at": "2025-07-01T18:18:51+00:00",
    "commented_code": "def format_flamegraph(flamegraph_lines, flamegraph_script=None):\n    if flamegraph_script is None:\n        flamegraph_script = f\"/tmp/{os.getuid()}_flamegraph.pl\"\n        cache_dir = os.path.expanduser(\"~/.cache/\")",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2178277997",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157374,
        "pr_file": "torch/cuda/_memory_viz.py",
        "discussion_id": "2178277997",
        "commented_code": "@@ -89,7 +89,9 @@ def _block_extra(b):\n \n def format_flamegraph(flamegraph_lines, flamegraph_script=None):\n     if flamegraph_script is None:\n-        flamegraph_script = f\"/tmp/{os.getuid()}_flamegraph.pl\"\n+        cache_dir = os.path.expanduser(\"~/.cache/\")",
        "comment_created_at": "2025-07-01T18:18:51+00:00",
        "comment_author": "XuehaiPan",
        "comment_body": "```suggestion\r\n        cache_dir = os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2178493565",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157374,
        "pr_file": "torch/cuda/_memory_viz.py",
        "discussion_id": "2178277997",
        "commented_code": "@@ -89,7 +89,9 @@ def _block_extra(b):\n \n def format_flamegraph(flamegraph_lines, flamegraph_script=None):\n     if flamegraph_script is None:\n-        flamegraph_script = f\"/tmp/{os.getuid()}_flamegraph.pl\"\n+        cache_dir = os.path.expanduser(\"~/.cache/\")",
        "comment_created_at": "2025-07-01T20:53:10+00:00",
        "comment_author": "malfet",
        "comment_body": "I though about it. Do you know when home would not be sufficient?",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2112886671",
    "pr_number": 137846,
    "pr_file": "torch/utils/collect_env.py",
    "created_at": "2025-05-28T22:47:29+00:00",
    "commented_code": "return smi\n\n\ndef get_pkg_version(run_lambda, pkg):\n    ret = \"\"\n    index = -1\n    if get_platform() == \"linux\":",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2112886671",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 137846,
        "pr_file": "torch/utils/collect_env.py",
        "discussion_id": "2112886671",
        "commented_code": "@@ -243,6 +248,128 @@ def get_nvidia_smi():\n     return smi\n \n \n+def get_pkg_version(run_lambda, pkg):\n+    ret = \"\"\n+    index = -1\n+    if get_platform() == \"linux\":",
        "comment_created_at": "2025-05-28T22:47:29+00:00",
        "comment_author": "malfet",
        "comment_body": "Why not replace it with something like:\r\n```\r\nif get_platform() != \"linux\": return \"N/A\"\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "2113890020",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 137846,
        "pr_file": "torch/utils/collect_env.py",
        "discussion_id": "2112886671",
        "commented_code": "@@ -243,6 +248,128 @@ def get_nvidia_smi():\n     return smi\n \n \n+def get_pkg_version(run_lambda, pkg):\n+    ret = \"\"\n+    index = -1\n+    if get_platform() == \"linux\":",
        "comment_created_at": "2025-05-29T12:57:14+00:00",
        "comment_author": "jingxu10",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2153261028",
    "pr_number": 156207,
    "pr_file": "torch/distributed/_pin_memory_utils.py",
    "created_at": "2025-06-17T22:15:49+00:00",
    "commented_code": "# mypy: allow-untyped-defs\nimport logging\nimport os\nfrom logging import getLogger\n\nimport torch\n\nlogger = getLogger()\nlogger.setLevel(logging.INFO)\n\n\n# Allows retrying cudaHostRegister if it fails\nCKPT_PIN_ALLOW_RETRY = os.environ.get(\"CKPT_PIN_ALLOW_RETRY\", \"1\") == \"1\"\n# Peeks last cudaError before pinning shared memory\nCKPT_PIN_PEEK_CUDA_ERROR = os.environ.get(\"CKPT_PIN_PEEK_CUDA_ERROR\", \"0\") == \"1\"\n# Pops last cudaError before pinning shared memory\nCKPT_PIN_POP_CUDA_ERROR = os.environ.get(\"CKPT_PIN_POP_CUDA_ERROR\", \"0\") == \"1\"",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2153261028",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156207,
        "pr_file": "torch/distributed/_pin_memory_utils.py",
        "discussion_id": "2153261028",
        "commented_code": "@@ -0,0 +1,66 @@\n+# mypy: allow-untyped-defs\n+import logging\n+import os\n+from logging import getLogger\n+\n+import torch\n+\n+logger = getLogger()\n+logger.setLevel(logging.INFO)\n+\n+\n+# Allows retrying cudaHostRegister if it fails\n+CKPT_PIN_ALLOW_RETRY = os.environ.get(\"CKPT_PIN_ALLOW_RETRY\", \"1\") == \"1\"\n+# Peeks last cudaError before pinning shared memory\n+CKPT_PIN_PEEK_CUDA_ERROR = os.environ.get(\"CKPT_PIN_PEEK_CUDA_ERROR\", \"0\") == \"1\"\n+# Pops last cudaError before pinning shared memory\n+CKPT_PIN_POP_CUDA_ERROR = os.environ.get(\"CKPT_PIN_POP_CUDA_ERROR\", \"0\") == \"1\"",
        "comment_created_at": "2025-06-17T22:15:49+00:00",
        "comment_author": "fegin",
        "comment_body": "Do we want to export these variables? I guess not. We can just move before where they are used. For example, `CKPT_PIN_ALLOW_RETRY` is not used at all.",
        "pr_file_module": null
      },
      {
        "comment_id": "2153276222",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156207,
        "pr_file": "torch/distributed/_pin_memory_utils.py",
        "discussion_id": "2153261028",
        "commented_code": "@@ -0,0 +1,66 @@\n+# mypy: allow-untyped-defs\n+import logging\n+import os\n+from logging import getLogger\n+\n+import torch\n+\n+logger = getLogger()\n+logger.setLevel(logging.INFO)\n+\n+\n+# Allows retrying cudaHostRegister if it fails\n+CKPT_PIN_ALLOW_RETRY = os.environ.get(\"CKPT_PIN_ALLOW_RETRY\", \"1\") == \"1\"\n+# Peeks last cudaError before pinning shared memory\n+CKPT_PIN_PEEK_CUDA_ERROR = os.environ.get(\"CKPT_PIN_PEEK_CUDA_ERROR\", \"0\") == \"1\"\n+# Pops last cudaError before pinning shared memory\n+CKPT_PIN_POP_CUDA_ERROR = os.environ.get(\"CKPT_PIN_POP_CUDA_ERROR\", \"0\") == \"1\"",
        "comment_created_at": "2025-06-17T22:31:58+00:00",
        "comment_author": "vadimkantorov",
        "comment_body": "Also, should these env vars be `TORCH_`-prefixed?",
        "pr_file_module": null
      },
      {
        "comment_id": "2153356898",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156207,
        "pr_file": "torch/distributed/_pin_memory_utils.py",
        "discussion_id": "2153261028",
        "commented_code": "@@ -0,0 +1,66 @@\n+# mypy: allow-untyped-defs\n+import logging\n+import os\n+from logging import getLogger\n+\n+import torch\n+\n+logger = getLogger()\n+logger.setLevel(logging.INFO)\n+\n+\n+# Allows retrying cudaHostRegister if it fails\n+CKPT_PIN_ALLOW_RETRY = os.environ.get(\"CKPT_PIN_ALLOW_RETRY\", \"1\") == \"1\"\n+# Peeks last cudaError before pinning shared memory\n+CKPT_PIN_PEEK_CUDA_ERROR = os.environ.get(\"CKPT_PIN_PEEK_CUDA_ERROR\", \"0\") == \"1\"\n+# Pops last cudaError before pinning shared memory\n+CKPT_PIN_POP_CUDA_ERROR = os.environ.get(\"CKPT_PIN_POP_CUDA_ERROR\", \"0\") == \"1\"",
        "comment_created_at": "2025-06-17T23:58:19+00:00",
        "comment_author": "Saiteja64",
        "comment_body": "Removed and simplified implementation. Can add these separately. Not necessary for ZOC",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2168080662",
    "pr_number": 156923,
    "pr_file": "torch/utils/cpp_extension.py",
    "created_at": "2025-06-26T04:30:00+00:00",
    "commented_code": "# See cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake\n    _arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST', None)",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2168080662",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156923,
        "pr_file": "torch/utils/cpp_extension.py",
        "discussion_id": "2168080662",
        "commented_code": "@@ -2420,11 +2420,12 @@ def _get_cuda_arch_flags(cflags: Optional[list[str]] = None) -> list[str]:\n     # See cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake\n     _arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST', None)",
        "comment_created_at": "2025-06-26T04:30:00+00:00",
        "comment_author": "Copilot",
        "comment_body": "Normalize the environment variable with `_arch_list = _arch_list.strip().lower()` before comparison to ensure case-insensitive matching and avoid unintended whitespace issues.\n```suggestion\n    _arch_list = os.environ.get('TORCH_CUDA_ARCH_LIST', None)\n    if _arch_list:\n        _arch_list = _arch_list.strip().lower()\n```",
        "pr_file_module": null
      }
    ]
  }
]