[
  {
    "discussion_id": "2176926064",
    "pr_number": 156742,
    "pr_file": "setup.py",
    "created_at": "2025-07-01T09:04:42+00:00",
    "commented_code": "\"lib/*.lib\",\n            ]\n        )\n        aotriton_image_path = os.path.join(lib_path, \"aotriton.images\")\n        aks2_files = []\n        for root, dirs, files in os.walk(aotriton_image_path):\n            subpath = os.path.relpath(root, start=aotriton_image_path)\n            for fn in files:\n                aks2_files.append(os.path.join(\"lib/aotriton.images\", subpath, fn))\n        # XXX: Why not use wildcards [\"lib/aotriton.images/*\", \"lib/aotriton.images/**/*\"] here?\n        aotriton_image_path = TORCH_DIR / \"lib\" / \"aotriton.images\"\n        aks2_files = [\n            file.relative_to(TORCH_DIR).as_posix()\n            for file in aotriton_image_path.rglob(\"*\")\n            if file.is_file()\n        ]",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2176926064",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 156742,
        "pr_file": "setup.py",
        "discussion_id": "2176926064",
        "commented_code": "@@ -1313,12 +1333,13 @@ def main() -> None:\n                 \"lib/*.lib\",\n             ]\n         )\n-        aotriton_image_path = os.path.join(lib_path, \"aotriton.images\")\n-        aks2_files = []\n-        for root, dirs, files in os.walk(aotriton_image_path):\n-            subpath = os.path.relpath(root, start=aotriton_image_path)\n-            for fn in files:\n-                aks2_files.append(os.path.join(\"lib/aotriton.images\", subpath, fn))\n+        # XXX: Why not use wildcards [\"lib/aotriton.images/*\", \"lib/aotriton.images/**/*\"] here?\n+        aotriton_image_path = TORCH_DIR / \"lib\" / \"aotriton.images\"\n+        aks2_files = [\n+            file.relative_to(TORCH_DIR).as_posix()\n+            for file in aotriton_image_path.rglob(\"*\")\n+            if file.is_file()\n+        ]",
        "comment_created_at": "2025-07-01T09:04:42+00:00",
        "comment_author": "XuehaiPan",
        "comment_body": "The previous code gets `aks2_files = []` and breaks the ROCm build on trunk. I'm not sure why.\r\n\r\nI think the following code seems equivalent.\r\n\r\n```diff\r\n  aotriton_image_path = TORCH_DIR / \"lib\" / \"aotriton.images\"\r\n- aks2_files: list[str] = []\r\n- for file in filter(lambda p: p.is_file(), aotriton_image_path.glob(\"**\")):\r\n-     subpath = file.relative_to(aotriton_image_path)\r\n-     aks2_files.append(os.path.join(\"lib/aotriton.images\", subpath))\r\n+ aks2_files = [\r\n+     file.relative_to(TORCH_DIR).as_posix()\r\n+     for file in aotriton_image_path.rglob(\"*\")\r\n+     if file.is_file()\r\n+ ]\r\n```\r\n\r\nPS: `path.rglob(\"*\")` is equivalent to `path.glob(\"**/*\")`. `path.glob(\"**\")` will yield `path` while `path.glob(\"**/*\")` will not. But we are looking for files only, so they are the same.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2148467367",
    "pr_number": 155998,
    "pr_file": "setup.py",
    "created_at": "2025-06-15T16:37:04+00:00",
    "commented_code": "import platform\n\n\nBUILD_LIBTORCH_WHL = os.getenv(\"BUILD_LIBTORCH_WHL\", \"0\") == \"1\"\nBUILD_PYTHON_ONLY = os.getenv(\"BUILD_PYTHON_ONLY\", \"0\") == \"1\"\n\n# Also update `project.requires-python` in pyproject.toml when changing this\npython_min_version = (3, 9, 0)\npython_min_version_str = \".\".join(map(str, python_min_version))\nif sys.version_info < python_min_version:\n    print(\n        f\"You are using Python {platform.python_version()}. Python >={python_min_version_str} is required.\"\n        f\"You are using Python {platform.python_version()}. \"\n        f\"Python >={python_min_version_str} is required.\"",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2148467367",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 155998,
        "pr_file": "setup.py",
        "discussion_id": "2148467367",
        "commented_code": "@@ -234,14 +234,13 @@\n import platform\n \n \n-BUILD_LIBTORCH_WHL = os.getenv(\"BUILD_LIBTORCH_WHL\", \"0\") == \"1\"\n-BUILD_PYTHON_ONLY = os.getenv(\"BUILD_PYTHON_ONLY\", \"0\") == \"1\"\n-\n+# Also update `project.requires-python` in pyproject.toml when changing this\n python_min_version = (3, 9, 0)\n python_min_version_str = \".\".join(map(str, python_min_version))\n if sys.version_info < python_min_version:\n     print(\n-        f\"You are using Python {platform.python_version()}. Python >={python_min_version_str} is required.\"\n+        f\"You are using Python {platform.python_version()}. \"\n+        f\"Python >={python_min_version_str} is required.\"",
        "comment_created_at": "2025-06-15T16:37:04+00:00",
        "comment_author": "malfet",
        "comment_body": "Please don't mix unrelated formatting changes with some functional changes",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2019529371",
    "pr_number": 149697,
    "pr_file": "torch/_inductor/scheduler.py",
    "created_at": "2025-03-28T22:54:18+00:00",
    "commented_code": "return buf_byte_accesses\n\n    @cache_on_self\n    def estimate_flops(self) -> int | None:\n        from torch._subclasses.fake_tensor import FakeTensorMode\n        from torch.utils.flop_counter import FlopCounterMode\n\n        op = kernel_name_to_op.get(getattr(self.node, \"python_kernel_name\", \"\"), None)\n\n        if isinstance(self, ExternKernel):\n            if op is not None:",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2019529371",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 149697,
        "pr_file": "torch/_inductor/scheduler.py",
        "discussion_id": "2019529371",
        "commented_code": "@@ -736,6 +737,48 @@ def get_buf_bytes(\n \n         return buf_byte_accesses\n \n+    @cache_on_self\n+    def estimate_flops(self) -> int | None:\n+        from torch._subclasses.fake_tensor import FakeTensorMode\n+        from torch.utils.flop_counter import FlopCounterMode\n+\n+        op = kernel_name_to_op.get(getattr(self.node, \"python_kernel_name\", \"\"), None)\n+\n+        if isinstance(self, ExternKernel):\n+            if op is not None:",
        "comment_created_at": "2025-03-28T22:54:18+00:00",
        "comment_author": "eellison",
        "comment_body": " nit? can we reduce nesting. \r\n\r\n```\r\nif not isinstance(self, ExternKernel) or op is None:\r\n     return None\r\n```\r\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2019531149",
    "pr_number": 149697,
    "pr_file": "torch/_inductor/scheduler.py",
    "created_at": "2025-03-28T22:57:08+00:00",
    "commented_code": "return buf_byte_accesses\n\n    @cache_on_self\n    def estimate_flops(self) -> int | None:\n        from torch._subclasses.fake_tensor import FakeTensorMode\n        from torch.utils.flop_counter import FlopCounterMode\n\n        op = kernel_name_to_op.get(getattr(self.node, \"python_kernel_name\", \"\"), None)\n\n        if isinstance(self, ExternKernel):\n            if op is not None:\n                # make mypy happy\n                # mypy isn't smart enough to infer from InputsKernel that self.node.inputs\n                # and self.node.fx_node exists\n                kern: ExternKernel = self\n                if kern.node is None:\n                    return None\n                if any(\n                    len(free_unbacked_symbols(n.get_numel())) > 0\n                    for n in kern.node.inputs\n                ):\n                    # Tensor has unbacked symints, we don't know how to estimate\n                    # runtime for that today\n                    return None\n\n                with (\n                    FakeTensorMode() as fake_mode,\n                    FlopCounterMode(display=False) as flop_counter_mode,\n                    V.set_current_node(kern.node.fx_node),  # type ignore[attr-defined]\n                    V.set_fake_mode(fake_mode),\n                ):\n                    from .ir import ir_node_to_tensor\n\n                    fake_inputs = [\n                        ir_node_to_tensor(input, guard_shape=False)\n                        for input in kern.node.inputs  # type: ignore[attr-defined]\n                    ]\n                    cls = kern.node.__class__\n                    cls.process_kernel(op, *fake_inputs, **kern.node.kwargs)\n\n                    ret = flop_counter_mode.get_total_flops()",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2019531149",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 149697,
        "pr_file": "torch/_inductor/scheduler.py",
        "discussion_id": "2019531149",
        "commented_code": "@@ -736,6 +737,48 @@ def get_buf_bytes(\n \n         return buf_byte_accesses\n \n+    @cache_on_self\n+    def estimate_flops(self) -> int | None:\n+        from torch._subclasses.fake_tensor import FakeTensorMode\n+        from torch.utils.flop_counter import FlopCounterMode\n+\n+        op = kernel_name_to_op.get(getattr(self.node, \"python_kernel_name\", \"\"), None)\n+\n+        if isinstance(self, ExternKernel):\n+            if op is not None:\n+                # make mypy happy\n+                # mypy isn't smart enough to infer from InputsKernel that self.node.inputs\n+                # and self.node.fx_node exists\n+                kern: ExternKernel = self\n+                if kern.node is None:\n+                    return None\n+                if any(\n+                    len(free_unbacked_symbols(n.get_numel())) > 0\n+                    for n in kern.node.inputs\n+                ):\n+                    # Tensor has unbacked symints, we don't know how to estimate\n+                    # runtime for that today\n+                    return None\n+\n+                with (\n+                    FakeTensorMode() as fake_mode,\n+                    FlopCounterMode(display=False) as flop_counter_mode,\n+                    V.set_current_node(kern.node.fx_node),  # type ignore[attr-defined]\n+                    V.set_fake_mode(fake_mode),\n+                ):\n+                    from .ir import ir_node_to_tensor\n+\n+                    fake_inputs = [\n+                        ir_node_to_tensor(input, guard_shape=False)\n+                        for input in kern.node.inputs  # type: ignore[attr-defined]\n+                    ]\n+                    cls = kern.node.__class__\n+                    cls.process_kernel(op, *fake_inputs, **kern.node.kwargs)\n+\n+                    ret = flop_counter_mode.get_total_flops()",
        "comment_created_at": "2025-03-28T22:57:08+00:00",
        "comment_author": "eellison",
        "comment_body": "nit: no need for `ret` var..",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2173356037",
    "pr_number": 157178,
    "pr_file": "torch/distributed/tensor/_sharding_prop.py",
    "created_at": "2025-06-28T15:31:34+00:00",
    "commented_code": "f\"Operator {op_schema.op} does not have a sharding strategy registered.\"\n            )\n\n    def _select_strategy(self, strategy: OpStrategy) -> OpSpec:\n    def _select_strategy(self, op_name, strategy: OpStrategy) -> OpSpec:\n        print(f\"{op_name}, {strategy.strategies}\")",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "2173356037",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157178,
        "pr_file": "torch/distributed/tensor/_sharding_prop.py",
        "discussion_id": "2173356037",
        "commented_code": "@@ -487,15 +487,23 @@ def propagate_op_sharding_non_cached(self, op_schema: OpSchema) -> OutputShardin\n                 f\"Operator {op_schema.op} does not have a sharding strategy registered.\"\n             )\n \n-    def _select_strategy(self, strategy: OpStrategy) -> OpSpec:\n+    def _select_strategy(self, op_name, strategy: OpStrategy) -> OpSpec:\n+        print(f\"{op_name}, {strategy.strategies}\")",
        "comment_created_at": "2025-06-28T15:31:34+00:00",
        "comment_author": "Skylion007",
        "comment_body": "Print statement left in",
        "pr_file_module": null
      },
      {
        "comment_id": "2173563701",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 157178,
        "pr_file": "torch/distributed/tensor/_sharding_prop.py",
        "discussion_id": "2173356037",
        "commented_code": "@@ -487,15 +487,23 @@ def propagate_op_sharding_non_cached(self, op_schema: OpSchema) -> OutputShardin\n                 f\"Operator {op_schema.op} does not have a sharding strategy registered.\"\n             )\n \n-    def _select_strategy(self, strategy: OpStrategy) -> OpSpec:\n+    def _select_strategy(self, op_name, strategy: OpStrategy) -> OpSpec:\n+        print(f\"{op_name}, {strategy.strategies}\")",
        "comment_created_at": "2025-06-29T00:53:52+00:00",
        "comment_author": "zpcore",
        "comment_body": "Oops, removed!",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1805708823",
    "pr_number": 137846,
    "pr_file": "torch/utils/collect_env.py",
    "created_at": "2024-10-18T01:47:18+00:00",
    "commented_code": "return smi\n\n\ndef get_pkg_version(run_lambda, pkg):\n    ret = \"\"\n    index = -1\n    if get_platform() == \"linux\":\n        mgr_name = \"\"\n        if mgr_name == \"\":\n            rc, _, _ = run(\"which dpkg\")\n            if rc == 0:\n                mgr_name = \"dpkg\"\n        if mgr_name == \"\":\n            rc, _, _ = run(\"which dnf\")\n            if rc == 0:\n                mgr_name = \"dnf\"\n        if mgr_name == \"\":\n            rc, _, _ = run(\"which yum\")\n            if rc == 0:\n                mgr_name = \"yum\"\n        if mgr_name == \"\":\n            rc, _, _ = run(\"which zypper\")\n            if rc == 0:\n                mgr_name = \"zypper\"",
    "repo_full_name": "pytorch/pytorch",
    "discussion_comments": [
      {
        "comment_id": "1805708823",
        "repo_full_name": "pytorch/pytorch",
        "pr_number": 137846,
        "pr_file": "torch/utils/collect_env.py",
        "discussion_id": "1805708823",
        "commented_code": "@@ -225,6 +230,139 @@ def get_nvidia_smi():\n     return smi\n \n \n+def get_pkg_version(run_lambda, pkg):\n+    ret = \"\"\n+    index = -1\n+    if get_platform() == \"linux\":\n+        mgr_name = \"\"\n+        if mgr_name == \"\":\n+            rc, _, _ = run(\"which dpkg\")\n+            if rc == 0:\n+                mgr_name = \"dpkg\"\n+        if mgr_name == \"\":\n+            rc, _, _ = run(\"which dnf\")\n+            if rc == 0:\n+                mgr_name = \"dnf\"\n+        if mgr_name == \"\":\n+            rc, _, _ = run(\"which yum\")\n+            if rc == 0:\n+                mgr_name = \"yum\"\n+        if mgr_name == \"\":\n+            rc, _, _ = run(\"which zypper\")\n+            if rc == 0:\n+                mgr_name = \"zypper\"",
        "comment_created_at": "2024-10-18T01:47:18+00:00",
        "comment_author": "malfet",
        "comment_body": "Please Avoid code duplication, use loops\r\n```suggestion\r\n        for mgr_name in [\"dpkg\", \"dnf\", \"yum\", \"zypper\", \"\"]:\r\n            if mgr_name == \"\":\r\n                continue\r\n            rc, _, _ = run(f\"which {mgr_name}\")\r\n            if rc == 0:\r\n                break\r\n```",
        "pr_file_module": null
      }
    ]
  }
]