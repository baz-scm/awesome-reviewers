[
  {
    "discussion_id": "1824057091",
    "pr_number": 1045,
    "pr_file": "docs/locales/zh_CN/LC_MESSAGES/benchmark/speed_benchmark.po",
    "created_at": "2024-10-31T08:19:45+00:00",
    "commented_code": "\"MIME-Version: 1.0\n\"\n \"Content-Type: text/plain; charset=utf-8\n\"\n \"Content-Transfer-Encoding: 8bit\n\"\n-\"Generated-By: Babel 2.15.0\n\"\n+\"Generated-By: Babel 2.16.0\n\"\n \n #: ../../source/benchmark/speed_benchmark.rst:2\n-#: 96f9c969f82049efbaf7b70525976649\n-msgid \"Speed Benchmark\"\n+#: c37062a883c842a2b89fc3971b2209cb\n+#, fuzzy\n+msgid \"Qwen2.5 Speed Benchmark\"\n msgstr \"\u6548\u7387\u8bc4\u4f30\"\n \n #: ../../source/benchmark/speed_benchmark.rst:5\n-#: 3e97857c19314350b1d6686ad9776d35\n-msgid \"To be updated for Qwen2.5.\"\n-msgstr \"Qwen2.5\u7ed3\u679c\u5f85\u66f4\u65b0\uff0c\u7531\u4e8e\u6a21\u578b\u7ed3\u6784\u5dee\u5f02\u6709\u9650\uff0cQwen2\u7ed3\u679c\u53ef\u4f9b\u53c2\u8003\u3002\"\n+#: 5577386104e04ce0820d75b8d4a4b9bb\n+#, fuzzy\n+msgid \"This section reports the speed performance of bf16 models, quantized models (including GPTQ-Int4, GPTQ-Int8 and AWQ) of the Qwen2.5 series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under the conditions of different context lengths.\"\n+msgstr \"\u672c\u90e8\u5206\u4ecb\u7ecdQwen2.5\u7cfb\u5217\u6a21\u578b\uff08\u539f\u59cb\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\uff09\u7684\u6548\u7387\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5305\u62ec\u63a8\u7406\u901f\u5ea6(tokens/s)\u4e0e\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u7684\u663e\u5b58\u5360\u7528(GB)\u3002\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:7\n-#: 4f0e196db456466997765e4b93b873be\n-msgid \"This section reports the speed performance of bf16 models, quantized models (including GPTQ-Int4, GPTQ-Int8 and AWQ) of the Qwen2 series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under the conditions of different context lengths.\"\n-msgstr \"\u672c\u90e8\u5206\u4ecb\u7ecdQwen2\u6a21\u578b\uff08\u539f\u59cb\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\uff09\u7684\u6548\u7387\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5305\u62ec\u63a8\u7406\u901f\u5ea6(tokens/s)\u4e0e\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u7684\u663e\u5b58\u5360\u7528(GB)\u3002\"\n-\n-#: ../../source/benchmark/speed_benchmark.rst:12\n-#: d3a3a79f4010466f882bd52955780253\n+#: ../../source/benchmark/speed_benchmark.rst:10\n+#: 9edf3184b2694e6d9dee05c519bea1ae\n msgid \"The environment of the evaluation with huggingface transformers is:\"\n msgstr \"\u6d4b\u8bd5HuggingFace ``transformers`` \u65f6\u7684\u73af\u5883\u914d\u7f6e\uff1a\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:14\n-#: ../../source/benchmark/speed_benchmark.rst:24\n-#: 8e1e5f8b79c54381b4bf00c8637954c8\n+#: ../../source/benchmark/speed_benchmark.rst:12\n+#: ../../source/benchmark/speed_benchmark.rst:23\n+#: 5929629b0bf143ab983efd4e2aa964c8 b619da3afa86420ba7e2583d9a5e7c39\n msgid \"NVIDIA A100 80GB\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:15\n-#: ../../source/benchmark/speed_benchmark.rst:25\n-#: 79bb2a6aea064df79c0819b9c966b867\n-msgid \"CUDA 11.8\"\n+#: ../../source/benchmark/speed_benchmark.rst:13\n+#: ../../source/benchmark/speed_benchmark.rst:24\n+#: 6986d9f22df54554a9e830b3828a5ed2 a4e87ae3bd2042429b8df23c779f6373\n+msgid \"CUDA 12.1\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:16\n-#: 6ed8b5fb474842c18b4e319eebbbb73f\n-msgid \"Pytorch 2.1.2+cu118\"\n+#: ../../source/benchmark/speed_benchmark.rst:14\n+#: 190e255dcd1e469294188508b49bf98c\n+msgid \"Pytorch 2.3.1+cu121\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:17\n-#: e6429404fdc543e1a80c811b9ef32e2a\n-msgid \"Flash Attention 2.3.3\"\n+#: ../../source/benchmark/speed_benchmark.rst:15\n+#: c693c6e715074b2daa95c62064b4e79e\n+msgid \"Flash Attention 2.5.8\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:18\n-#: a02f0bd8337949288a07caa4704aa55a\n-msgid \"Transformers 4.38.2\"\n+#: ../../source/benchmark/speed_benchmark.rst:16\n+#: ../../source/benchmark/speed_benchmark.rst:28\n+#: 3796f99ed359444da30190e7a3b86428 bfc7d82414fa46a58a09553f4c703af6\n+msgid \"Transformers 4.46.0\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:19\n-#: 7072898a11164f7ca15acca7edaca4f9\n-msgid \"AutoGPTQ 0.7.1\"\n+#: ../../source/benchmark/speed_benchmark.rst:17\n+#: 427aa447657849cba460032041380f2e\n+msgid \"AutoGPTQ 0.7.1+cu121 (Compiled from source code)\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:20\n-#: 76bdca0175824567908b4cbc83c02731\n-msgid \"AutoAWQ 0.2.4\"\n+#: ../../source/benchmark/speed_benchmark.rst:18\n+#: aabddb4e2b0244ea9c27788ce453f30e\n+msgid \"AutoAWQ 0.2.6\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:22\n-#: 568b1b0c821d4af199a3d6122f38d1ea\n+#: ../../source/benchmark/speed_benchmark.rst:21\n+#: 8f4e975fbc9d48f18cb30d75a9f335db\n msgid \"The environment of the evaluation with vLLM is:\"\n msgstr \"\u6d4b\u8bd5vLLM\u65f6\u7684\u73af\u5883\u914d\u7f6e\uff1a\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:26\n-#: 73515f5745e148cc8ddf1e1ae1c9da3b\n-msgid \"Pytorch 2.3.0+cu118\"\n-msgstr \"\"\n-\n-#: ../../source/benchmark/speed_benchmark.rst:27\n-#: 3a291f04fa1f4c86b646c28625f36868\n-msgid \"Flash Attention 2.5.6\"\n+#: ../../source/benchmark/speed_benchmark.rst:25\n+#: 4fd3b3a5e61747f6b1577593d144efe0\n+msgid \"vLLM 0.6.3\"",
    "repo_full_name": "QwenLM/Qwen3",
    "discussion_comments": [
      {
        "comment_id": "1824057091",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1045,
        "pr_file": "docs/locales/zh_CN/LC_MESSAGES/benchmark/speed_benchmark.po",
        "discussion_id": "1824057091",
        "commented_code": "@@ -16,219 +16,250 @@ msgstr \"\"\n \"MIME-Version: 1.0\\n\"\n \"Content-Type: text/plain; charset=utf-8\\n\"\n \"Content-Transfer-Encoding: 8bit\\n\"\n-\"Generated-By: Babel 2.15.0\\n\"\n+\"Generated-By: Babel 2.16.0\\n\"\n \n #: ../../source/benchmark/speed_benchmark.rst:2\n-#: 96f9c969f82049efbaf7b70525976649\n-msgid \"Speed Benchmark\"\n+#: c37062a883c842a2b89fc3971b2209cb\n+#, fuzzy\n+msgid \"Qwen2.5 Speed Benchmark\"\n msgstr \"\u6548\u7387\u8bc4\u4f30\"\n \n #: ../../source/benchmark/speed_benchmark.rst:5\n-#: 3e97857c19314350b1d6686ad9776d35\n-msgid \"To be updated for Qwen2.5.\"\n-msgstr \"Qwen2.5\u7ed3\u679c\u5f85\u66f4\u65b0\uff0c\u7531\u4e8e\u6a21\u578b\u7ed3\u6784\u5dee\u5f02\u6709\u9650\uff0cQwen2\u7ed3\u679c\u53ef\u4f9b\u53c2\u8003\u3002\"\n+#: 5577386104e04ce0820d75b8d4a4b9bb\n+#, fuzzy\n+msgid \"This section reports the speed performance of bf16 models, quantized models (including GPTQ-Int4, GPTQ-Int8 and AWQ) of the Qwen2.5 series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under the conditions of different context lengths.\"\n+msgstr \"\u672c\u90e8\u5206\u4ecb\u7ecdQwen2.5\u7cfb\u5217\u6a21\u578b\uff08\u539f\u59cb\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\uff09\u7684\u6548\u7387\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5305\u62ec\u63a8\u7406\u901f\u5ea6(tokens/s)\u4e0e\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u7684\u663e\u5b58\u5360\u7528(GB)\u3002\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:7\n-#: 4f0e196db456466997765e4b93b873be\n-msgid \"This section reports the speed performance of bf16 models, quantized models (including GPTQ-Int4, GPTQ-Int8 and AWQ) of the Qwen2 series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under the conditions of different context lengths.\"\n-msgstr \"\u672c\u90e8\u5206\u4ecb\u7ecdQwen2\u6a21\u578b\uff08\u539f\u59cb\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\uff09\u7684\u6548\u7387\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5305\u62ec\u63a8\u7406\u901f\u5ea6(tokens/s)\u4e0e\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u7684\u663e\u5b58\u5360\u7528(GB)\u3002\"\n-\n-#: ../../source/benchmark/speed_benchmark.rst:12\n-#: d3a3a79f4010466f882bd52955780253\n+#: ../../source/benchmark/speed_benchmark.rst:10\n+#: 9edf3184b2694e6d9dee05c519bea1ae\n msgid \"The environment of the evaluation with huggingface transformers is:\"\n msgstr \"\u6d4b\u8bd5HuggingFace ``transformers`` \u65f6\u7684\u73af\u5883\u914d\u7f6e\uff1a\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:14\n-#: ../../source/benchmark/speed_benchmark.rst:24\n-#: 8e1e5f8b79c54381b4bf00c8637954c8\n+#: ../../source/benchmark/speed_benchmark.rst:12\n+#: ../../source/benchmark/speed_benchmark.rst:23\n+#: 5929629b0bf143ab983efd4e2aa964c8 b619da3afa86420ba7e2583d9a5e7c39\n msgid \"NVIDIA A100 80GB\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:15\n-#: ../../source/benchmark/speed_benchmark.rst:25\n-#: 79bb2a6aea064df79c0819b9c966b867\n-msgid \"CUDA 11.8\"\n+#: ../../source/benchmark/speed_benchmark.rst:13\n+#: ../../source/benchmark/speed_benchmark.rst:24\n+#: 6986d9f22df54554a9e830b3828a5ed2 a4e87ae3bd2042429b8df23c779f6373\n+msgid \"CUDA 12.1\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:16\n-#: 6ed8b5fb474842c18b4e319eebbbb73f\n-msgid \"Pytorch 2.1.2+cu118\"\n+#: ../../source/benchmark/speed_benchmark.rst:14\n+#: 190e255dcd1e469294188508b49bf98c\n+msgid \"Pytorch 2.3.1+cu121\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:17\n-#: e6429404fdc543e1a80c811b9ef32e2a\n-msgid \"Flash Attention 2.3.3\"\n+#: ../../source/benchmark/speed_benchmark.rst:15\n+#: c693c6e715074b2daa95c62064b4e79e\n+msgid \"Flash Attention 2.5.8\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:18\n-#: a02f0bd8337949288a07caa4704aa55a\n-msgid \"Transformers 4.38.2\"\n+#: ../../source/benchmark/speed_benchmark.rst:16\n+#: ../../source/benchmark/speed_benchmark.rst:28\n+#: 3796f99ed359444da30190e7a3b86428 bfc7d82414fa46a58a09553f4c703af6\n+msgid \"Transformers 4.46.0\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:19\n-#: 7072898a11164f7ca15acca7edaca4f9\n-msgid \"AutoGPTQ 0.7.1\"\n+#: ../../source/benchmark/speed_benchmark.rst:17\n+#: 427aa447657849cba460032041380f2e\n+msgid \"AutoGPTQ 0.7.1+cu121 (Compiled from source code)\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:20\n-#: 76bdca0175824567908b4cbc83c02731\n-msgid \"AutoAWQ 0.2.4\"\n+#: ../../source/benchmark/speed_benchmark.rst:18\n+#: aabddb4e2b0244ea9c27788ce453f30e\n+msgid \"AutoAWQ 0.2.6\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:22\n-#: 568b1b0c821d4af199a3d6122f38d1ea\n+#: ../../source/benchmark/speed_benchmark.rst:21\n+#: 8f4e975fbc9d48f18cb30d75a9f335db\n msgid \"The environment of the evaluation with vLLM is:\"\n msgstr \"\u6d4b\u8bd5vLLM\u65f6\u7684\u73af\u5883\u914d\u7f6e\uff1a\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:26\n-#: 73515f5745e148cc8ddf1e1ae1c9da3b\n-msgid \"Pytorch 2.3.0+cu118\"\n-msgstr \"\"\n-\n-#: ../../source/benchmark/speed_benchmark.rst:27\n-#: 3a291f04fa1f4c86b646c28625f36868\n-msgid \"Flash Attention 2.5.6\"\n+#: ../../source/benchmark/speed_benchmark.rst:25\n+#: 4fd3b3a5e61747f6b1577593d144efe0\n+msgid \"vLLM 0.6.3\"",
        "comment_created_at": "2024-10-31T08:19:45+00:00",
        "comment_author": "jklj077",
        "comment_body": "does this work? some users reported that 0.6.3 failed.",
        "pr_file_module": null
      },
      {
        "comment_id": "1830791012",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1045,
        "pr_file": "docs/locales/zh_CN/LC_MESSAGES/benchmark/speed_benchmark.po",
        "discussion_id": "1824057091",
        "commented_code": "@@ -16,219 +16,250 @@ msgstr \"\"\n \"MIME-Version: 1.0\\n\"\n \"Content-Type: text/plain; charset=utf-8\\n\"\n \"Content-Transfer-Encoding: 8bit\\n\"\n-\"Generated-By: Babel 2.15.0\\n\"\n+\"Generated-By: Babel 2.16.0\\n\"\n \n #: ../../source/benchmark/speed_benchmark.rst:2\n-#: 96f9c969f82049efbaf7b70525976649\n-msgid \"Speed Benchmark\"\n+#: c37062a883c842a2b89fc3971b2209cb\n+#, fuzzy\n+msgid \"Qwen2.5 Speed Benchmark\"\n msgstr \"\u6548\u7387\u8bc4\u4f30\"\n \n #: ../../source/benchmark/speed_benchmark.rst:5\n-#: 3e97857c19314350b1d6686ad9776d35\n-msgid \"To be updated for Qwen2.5.\"\n-msgstr \"Qwen2.5\u7ed3\u679c\u5f85\u66f4\u65b0\uff0c\u7531\u4e8e\u6a21\u578b\u7ed3\u6784\u5dee\u5f02\u6709\u9650\uff0cQwen2\u7ed3\u679c\u53ef\u4f9b\u53c2\u8003\u3002\"\n+#: 5577386104e04ce0820d75b8d4a4b9bb\n+#, fuzzy\n+msgid \"This section reports the speed performance of bf16 models, quantized models (including GPTQ-Int4, GPTQ-Int8 and AWQ) of the Qwen2.5 series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under the conditions of different context lengths.\"\n+msgstr \"\u672c\u90e8\u5206\u4ecb\u7ecdQwen2.5\u7cfb\u5217\u6a21\u578b\uff08\u539f\u59cb\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\uff09\u7684\u6548\u7387\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5305\u62ec\u63a8\u7406\u901f\u5ea6(tokens/s)\u4e0e\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u7684\u663e\u5b58\u5360\u7528(GB)\u3002\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:7\n-#: 4f0e196db456466997765e4b93b873be\n-msgid \"This section reports the speed performance of bf16 models, quantized models (including GPTQ-Int4, GPTQ-Int8 and AWQ) of the Qwen2 series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under the conditions of different context lengths.\"\n-msgstr \"\u672c\u90e8\u5206\u4ecb\u7ecdQwen2\u6a21\u578b\uff08\u539f\u59cb\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\uff09\u7684\u6548\u7387\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5305\u62ec\u63a8\u7406\u901f\u5ea6(tokens/s)\u4e0e\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u7684\u663e\u5b58\u5360\u7528(GB)\u3002\"\n-\n-#: ../../source/benchmark/speed_benchmark.rst:12\n-#: d3a3a79f4010466f882bd52955780253\n+#: ../../source/benchmark/speed_benchmark.rst:10\n+#: 9edf3184b2694e6d9dee05c519bea1ae\n msgid \"The environment of the evaluation with huggingface transformers is:\"\n msgstr \"\u6d4b\u8bd5HuggingFace ``transformers`` \u65f6\u7684\u73af\u5883\u914d\u7f6e\uff1a\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:14\n-#: ../../source/benchmark/speed_benchmark.rst:24\n-#: 8e1e5f8b79c54381b4bf00c8637954c8\n+#: ../../source/benchmark/speed_benchmark.rst:12\n+#: ../../source/benchmark/speed_benchmark.rst:23\n+#: 5929629b0bf143ab983efd4e2aa964c8 b619da3afa86420ba7e2583d9a5e7c39\n msgid \"NVIDIA A100 80GB\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:15\n-#: ../../source/benchmark/speed_benchmark.rst:25\n-#: 79bb2a6aea064df79c0819b9c966b867\n-msgid \"CUDA 11.8\"\n+#: ../../source/benchmark/speed_benchmark.rst:13\n+#: ../../source/benchmark/speed_benchmark.rst:24\n+#: 6986d9f22df54554a9e830b3828a5ed2 a4e87ae3bd2042429b8df23c779f6373\n+msgid \"CUDA 12.1\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:16\n-#: 6ed8b5fb474842c18b4e319eebbbb73f\n-msgid \"Pytorch 2.1.2+cu118\"\n+#: ../../source/benchmark/speed_benchmark.rst:14\n+#: 190e255dcd1e469294188508b49bf98c\n+msgid \"Pytorch 2.3.1+cu121\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:17\n-#: e6429404fdc543e1a80c811b9ef32e2a\n-msgid \"Flash Attention 2.3.3\"\n+#: ../../source/benchmark/speed_benchmark.rst:15\n+#: c693c6e715074b2daa95c62064b4e79e\n+msgid \"Flash Attention 2.5.8\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:18\n-#: a02f0bd8337949288a07caa4704aa55a\n-msgid \"Transformers 4.38.2\"\n+#: ../../source/benchmark/speed_benchmark.rst:16\n+#: ../../source/benchmark/speed_benchmark.rst:28\n+#: 3796f99ed359444da30190e7a3b86428 bfc7d82414fa46a58a09553f4c703af6\n+msgid \"Transformers 4.46.0\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:19\n-#: 7072898a11164f7ca15acca7edaca4f9\n-msgid \"AutoGPTQ 0.7.1\"\n+#: ../../source/benchmark/speed_benchmark.rst:17\n+#: 427aa447657849cba460032041380f2e\n+msgid \"AutoGPTQ 0.7.1+cu121 (Compiled from source code)\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:20\n-#: 76bdca0175824567908b4cbc83c02731\n-msgid \"AutoAWQ 0.2.4\"\n+#: ../../source/benchmark/speed_benchmark.rst:18\n+#: aabddb4e2b0244ea9c27788ce453f30e\n+msgid \"AutoAWQ 0.2.6\"\n msgstr \"\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:22\n-#: 568b1b0c821d4af199a3d6122f38d1ea\n+#: ../../source/benchmark/speed_benchmark.rst:21\n+#: 8f4e975fbc9d48f18cb30d75a9f335db\n msgid \"The environment of the evaluation with vLLM is:\"\n msgstr \"\u6d4b\u8bd5vLLM\u65f6\u7684\u73af\u5883\u914d\u7f6e\uff1a\"\n \n-#: ../../source/benchmark/speed_benchmark.rst:26\n-#: 73515f5745e148cc8ddf1e1ae1c9da3b\n-msgid \"Pytorch 2.3.0+cu118\"\n-msgstr \"\"\n-\n-#: ../../source/benchmark/speed_benchmark.rst:27\n-#: 3a291f04fa1f4c86b646c28625f36868\n-msgid \"Flash Attention 2.5.6\"\n+#: ../../source/benchmark/speed_benchmark.rst:25\n+#: 4fd3b3a5e61747f6b1577593d144efe0\n+msgid \"vLLM 0.6.3\"",
        "comment_created_at": "2024-11-06T10:40:09+00:00",
        "comment_author": "wangxingjun778",
        "comment_body": "For Qwen2.5 7B\u300114B\u300132B, we use vLLM==0.6.3 to deploy these models with 128k context-length, got output issue: \r\n```!!!!!!!!!!!!!!!!!!!!!!!```\r\n\r\nvLLM==0.6.2 IS OK for above settings, otherwise we use vLLM==0.6.3  by default.\r\n\r\nRefer to speed_benchmark.rst for more details.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1824068171",
    "pr_number": 1045,
    "pr_file": "docs/source/benchmark/speed_benchmark.rst",
    "created_at": "2024-10-31T08:29:10+00:00",
    "commented_code": "-  0.5B (Transformer)\n \n-+-------------------------+--------------+--------------+---------+-----------------+----------------+\n-| Model                   | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) |\n-+=========================+==============+==============+=========+=================+================+\n-| Qwen2.5-0.5B-Instruct   | 1            | BF16         | 1       | 47.40           | 0.97           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 35.17           | 0.64           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 50.60           | 0.48           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.09           | 0.68           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 6144         | BF16         | 1       | 47.45           | 1.23           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 36.47           | 0.90           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 48.89           | 0.73           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.04           | 0.72           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 14336        | BF16         | 1       | 47.11           | 1.60           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 35.44           | 1.26           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 48.26           | 1.10           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.14           | 1.10           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 30720        | BF16         | 1       | 47.16           | 2.34           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 36.25           | 2.01           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 49.22           | 1.85           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 36.90           | 1.84           |\n-+-------------------------+--------------+--------------+---------+-----------------+----------------+\n-\n++-------------------------+--------------+--------------+---------+-----------------+----------------+---------------------------+\n+| Model                   | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) | Note                      |",
    "repo_full_name": "QwenLM/Qwen3",
    "discussion_comments": [
      {
        "comment_id": "1824068171",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1045,
        "pr_file": "docs/source/benchmark/speed_benchmark.rst",
        "discussion_id": "1824068171",
        "commented_code": "@@ -43,42 +43,41 @@ Notes:\n \n -  0.5B (Transformer)\n \n-+-------------------------+--------------+--------------+---------+-----------------+----------------+\n-| Model                   | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) |\n-+=========================+==============+==============+=========+=================+================+\n-| Qwen2.5-0.5B-Instruct   | 1            | BF16         | 1       | 47.40           | 0.97           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 35.17           | 0.64           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 50.60           | 0.48           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.09           | 0.68           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 6144         | BF16         | 1       | 47.45           | 1.23           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 36.47           | 0.90           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 48.89           | 0.73           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.04           | 0.72           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 14336        | BF16         | 1       | 47.11           | 1.60           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 35.44           | 1.26           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 48.26           | 1.10           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.14           | 1.10           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 30720        | BF16         | 1       | 47.16           | 2.34           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 36.25           | 2.01           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 49.22           | 1.85           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 36.90           | 1.84           |\n-+-------------------------+--------------+--------------+---------+-----------------+----------------+\n-\n++-------------------------+--------------+--------------+---------+-----------------+----------------+---------------------------+\n+| Model                   | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) | Note                      |",
        "comment_created_at": "2024-10-31T08:29:10+00:00",
        "comment_author": "jklj077",
        "comment_body": "any reason 0.6.0 is used?",
        "pr_file_module": null
      },
      {
        "comment_id": "1830735227",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1045,
        "pr_file": "docs/source/benchmark/speed_benchmark.rst",
        "discussion_id": "1824068171",
        "commented_code": "@@ -43,42 +43,41 @@ Notes:\n \n -  0.5B (Transformer)\n \n-+-------------------------+--------------+--------------+---------+-----------------+----------------+\n-| Model                   | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) |\n-+=========================+==============+==============+=========+=================+================+\n-| Qwen2.5-0.5B-Instruct   | 1            | BF16         | 1       | 47.40           | 0.97           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 35.17           | 0.64           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 50.60           | 0.48           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.09           | 0.68           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 6144         | BF16         | 1       | 47.45           | 1.23           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 36.47           | 0.90           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 48.89           | 0.73           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.04           | 0.72           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 14336        | BF16         | 1       | 47.11           | 1.60           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 35.44           | 1.26           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 48.26           | 1.10           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 37.14           | 1.10           |\n-+                         +--------------+--------------+---------+-----------------+----------------+\n-|                         | 30720        | BF16         | 1       | 47.16           | 2.34           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int8    | 1       | 36.25           | 2.01           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | GPTQ-Int4    | 1       | 49.22           | 1.85           |\n-+                         +              +--------------+---------+-----------------+----------------+\n-|                         |              | AWQ          | 1       | 36.90           | 1.84           |\n-+-------------------------+--------------+--------------+---------+-----------------+----------------+\n-\n++-------------------------+--------------+--------------+---------+-----------------+----------------+---------------------------+\n+| Model                   | Input Length | Quantization | GPU Num | Speed(tokens/s) | GPU Memory(GB) | Note                      |",
        "comment_created_at": "2024-11-06T10:15:40+00:00",
        "comment_author": "wangxingjun778",
        "comment_body": "Got infer error for all qwen2.5 int8 models:\r\n``` RuntimeError: probability tensor contains either `inf`, `nan` or element < 0  ```\r\nunder settings:\r\n``` auto_gptq>=0.7.0  cu1210 ```\r\n\r\nThe env: ```auto_gptq==0.6.0+cu1210``` can run the infer code successfully.",
        "pr_file_module": null
      }
    ]
  }
]