[
  {
    "discussion_id": "1837550123",
    "pr_number": 1068,
    "pr_file": "examples/speed-benchmark/README.md",
    "created_at": "2024-11-12T06:34:41+00:00",
    "commented_code": "+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt\n+\n+# Note: For auto_gptq, you may need to install from the source code.\n+```\n+\n+For inference using vLLM:\n+\n+```shell\n+pip install -r requirements/perf_vllm.txt\n+\n+```\n+\n+\n+### 3. Run experiments\n+\n+#### 3.1 Inference using HuggingFace Transformers\n+\n+```shell\n+python speed_benchmark_transformer.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --use_modelscope --outputs_dir outputs/transformer\n+\n+```\n+\n+Parameters:\n+\n+    `--model_id_or_path`: Model ID or path, optional values refer to the Model Resources section.  \n+    `--context_length`: Input length in tokens; optional values are 1, 6144, 14336, 30720, 63488, 129024; Refer to the `Qwen2.5 SpeedBenchmark`.  \n+    `--gpus`: Number of GPUs to use, e.g., 0,1.  \n+    `--use_modelscope`: Whether to use ModelScope; if False, HuggingFace is used; default is True.",
    "repo_full_name": "QwenLM/Qwen3",
    "discussion_comments": [
      {
        "comment_id": "1837550123",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/README.md",
        "discussion_id": "1837550123",
        "commented_code": "@@ -0,0 +1,75 @@\n+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt\n+\n+# Note: For auto_gptq, you may need to install from the source code.\n+```\n+\n+For inference using vLLM:\n+\n+```shell\n+pip install -r requirements/perf_vllm.txt\n+\n+```\n+\n+\n+### 3. Run experiments\n+\n+#### 3.1 Inference using HuggingFace Transformers\n+\n+```shell\n+python speed_benchmark_transformer.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --use_modelscope --outputs_dir outputs/transformer\n+\n+```\n+\n+Parameters:\n+\n+    `--model_id_or_path`: Model ID or path, optional values refer to the Model Resources section.  \n+    `--context_length`: Input length in tokens; optional values are 1, 6144, 14336, 30720, 63488, 129024; Refer to the `Qwen2.5 SpeedBenchmark`.  \n+    `--gpus`: Number of GPUs to use, e.g., 0,1.  \n+    `--use_modelscope`: Whether to use ModelScope; if False, HuggingFace is used; default is True.  ",
        "comment_created_at": "2024-11-12T06:34:41+00:00",
        "comment_author": "jklj077",
        "comment_body": "please check the usage of `action='store_true'`. it should be paired with a default value of `False`.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1837555551",
    "pr_number": 1068,
    "pr_file": "examples/speed-benchmark/README.md",
    "created_at": "2024-11-12T06:41:21+00:00",
    "commented_code": "+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt\n+\n+# Note: For auto_gptq, you may need to install from the source code.\n+```\n+\n+For inference using vLLM:\n+\n+```shell\n+pip install -r requirements/perf_vllm.txt\n+\n+```\n+\n+\n+### 3. Run experiments\n+\n+#### 3.1 Inference using HuggingFace Transformers\n+\n+```shell\n+python speed_benchmark_transformer.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --use_modelscope --outputs_dir outputs/transformer",
    "repo_full_name": "QwenLM/Qwen3",
    "discussion_comments": [
      {
        "comment_id": "1837555551",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/README.md",
        "discussion_id": "1837555551",
        "commented_code": "@@ -0,0 +1,75 @@\n+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt\n+\n+# Note: For auto_gptq, you may need to install from the source code.\n+```\n+\n+For inference using vLLM:\n+\n+```shell\n+pip install -r requirements/perf_vllm.txt\n+\n+```\n+\n+\n+### 3. Run experiments\n+\n+#### 3.1 Inference using HuggingFace Transformers\n+\n+```shell\n+python speed_benchmark_transformer.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --use_modelscope --outputs_dir outputs/transformer",
        "comment_created_at": "2024-11-12T06:41:21+00:00",
        "comment_author": "jklj077",
        "comment_body": "please state somewhere the number of the generated tokens is fixed and is 2048. or just move this file to the speed benchmark doc",
        "pr_file_module": null
      },
      {
        "comment_id": "1839760276",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/README.md",
        "discussion_id": "1837555551",
        "commented_code": "@@ -0,0 +1,75 @@\n+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt\n+\n+# Note: For auto_gptq, you may need to install from the source code.\n+```\n+\n+For inference using vLLM:\n+\n+```shell\n+pip install -r requirements/perf_vllm.txt\n+\n+```\n+\n+\n+### 3. Run experiments\n+\n+#### 3.1 Inference using HuggingFace Transformers\n+\n+```shell\n+python speed_benchmark_transformer.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --use_modelscope --outputs_dir outputs/transformer",
        "comment_created_at": "2024-11-13T08:46:14+00:00",
        "comment_author": "wangxingjun778",
        "comment_body": "Add `--generate_length` for transformers and vllm, set 2048 by default.",
        "pr_file_module": null
      },
      {
        "comment_id": "1839761280",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/README.md",
        "discussion_id": "1837555551",
        "commented_code": "@@ -0,0 +1,75 @@\n+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt\n+\n+# Note: For auto_gptq, you may need to install from the source code.\n+```\n+\n+For inference using vLLM:\n+\n+```shell\n+pip install -r requirements/perf_vllm.txt\n+\n+```\n+\n+\n+### 3. Run experiments\n+\n+#### 3.1 Inference using HuggingFace Transformers\n+\n+```shell\n+python speed_benchmark_transformer.py --model_id_or_path Qwen/Qwen2.5-0.5B-Instruct --context_length 1 --gpus 0 --use_modelscope --outputs_dir outputs/transformer",
        "comment_created_at": "2024-11-13T08:46:53+00:00",
        "comment_author": "wangxingjun778",
        "comment_body": "Added desc of `--generate_length` in readme",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1837567161",
    "pr_number": 1068,
    "pr_file": "examples/speed-benchmark/README.md",
    "created_at": "2024-11-12T06:54:23+00:00",
    "commented_code": "+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt",
    "repo_full_name": "QwenLM/Qwen3",
    "discussion_comments": [
      {
        "comment_id": "1837567161",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/README.md",
        "discussion_id": "1837567161",
        "commented_code": "@@ -0,0 +1,75 @@\n+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt",
        "comment_created_at": "2024-11-12T06:54:23+00:00",
        "comment_author": "jklj077",
        "comment_body": "i don't think the requirements will work out of the box. there seem to be a lot of incompatible versions. please separate them into different requirement files or use git+:... to force a source install or http link to force a wheel install.",
        "pr_file_module": null
      },
      {
        "comment_id": "1839727491",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/README.md",
        "discussion_id": "1837567161",
        "commented_code": "@@ -0,0 +1,75 @@\n+## Speed Benchmark\n+\n+This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the [Qwen2.5 SpeedBenchmark](../../docs/source/benchmark/speed_benchmark.rst)\n+\n+### 1. Model Collections\n+\n+For models hosted on HuggingFace, please refer to [Qwen2.5 Collections-HuggingFace](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e)\n+\n+For models hosted on ModelScope, please refer to [Qwen2.5 Collections-ModelScope](https://modelscope.cn/collections/Qwen25-dbc4d30adb768)\n+\n+### 2. Environment Installation\n+\n+For inference using HuggingFace transformers:\n+\n+```shell\n+pip install -r requirements/perf_transformer.txt",
        "comment_created_at": "2024-11-13T08:25:14+00:00",
        "comment_author": "wangxingjun778",
        "comment_body": "done\r\n",
        "pr_file_module": null
      }
    ]
  }
]