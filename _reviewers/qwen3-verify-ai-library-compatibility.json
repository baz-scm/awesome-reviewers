[
  {
    "discussion_id": "1837558276",
    "pr_number": 1068,
    "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
    "created_at": "2024-11-12T06:44:00+00:00",
    "commented_code": "+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1",
    "repo_full_name": "QwenLM/Qwen3",
    "discussion_comments": [
      {
        "comment_id": "1837558276",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
        "discussion_id": "1837558276",
        "commented_code": "@@ -0,0 +1,9 @@\n+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1",
        "comment_created_at": "2024-11-12T06:44:00+00:00",
        "comment_author": "jklj077",
        "comment_body": "this does not seem right. the prebuilt auto_gptq 0.7.1 is compiled against torch 2.2.1 and cuda 12.1.  torch 2.3.1 is not suitable if cuda extensions are needed.",
        "pr_file_module": null
      },
      {
        "comment_id": "1838239698",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
        "discussion_id": "1837558276",
        "commented_code": "@@ -0,0 +1,9 @@\n+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1",
        "comment_created_at": "2024-11-12T14:50:14+00:00",
        "comment_author": "wangxingjun778",
        "comment_body": "We can use:\r\n`pip install torch==2.3.1`\r\n`pip install git+https://github.com/AutoGPTQ/AutoGPTQ.git@v0.7.1`\r\n\r\nto run the experiment successfully.\r\n\r\ntorch==2.2.1 is incompatible with autoawq==0.2.6, and got error during running the experiment.",
        "pr_file_module": null
      },
      {
        "comment_id": "1840166853",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
        "discussion_id": "1837558276",
        "commented_code": "@@ -0,0 +1,9 @@\n+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1",
        "comment_created_at": "2024-11-13T12:20:21+00:00",
        "comment_author": "jklj077",
        "comment_body": "If that's the case, I think we should make a separate requirements file for autoawq or auto_gptq.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1837561995",
    "pr_number": 1068,
    "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
    "created_at": "2024-11-12T06:48:11+00:00",
    "commented_code": "+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1\n+autoawq==0.2.6\n+flash_attn==2.5.8",
    "repo_full_name": "QwenLM/Qwen3",
    "discussion_comments": [
      {
        "comment_id": "1837561995",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
        "discussion_id": "1837561995",
        "commented_code": "@@ -0,0 +1,9 @@\n+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1\n+autoawq==0.2.6\n+flash_attn==2.5.8",
        "comment_created_at": "2024-11-12T06:48:11+00:00",
        "comment_author": "jklj077",
        "comment_body": "this will build from source. please warn the users they should have a cuda compiler installed or use the wheels from the github page https://github.com/Dao-AILab/flash-attention/releases/tag/v2.5.8 (no ones for cuda12.1 though)",
        "pr_file_module": null
      },
      {
        "comment_id": "1839716478",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
        "discussion_id": "1837561995",
        "commented_code": "@@ -0,0 +1,9 @@\n+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1\n+autoawq==0.2.6\n+flash_attn==2.5.8",
        "comment_created_at": "2024-11-13T08:18:28+00:00",
        "comment_author": "wangxingjun778",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1837564209",
    "pr_number": 1068,
    "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
    "created_at": "2024-11-12T06:50:38+00:00",
    "commented_code": "+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1\n+autoawq==0.2.6",
    "repo_full_name": "QwenLM/Qwen3",
    "discussion_comments": [
      {
        "comment_id": "1837564209",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
        "discussion_id": "1837564209",
        "commented_code": "@@ -0,0 +1,9 @@\n+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1\n+autoawq==0.2.6",
        "comment_created_at": "2024-11-12T06:50:38+00:00",
        "comment_author": "jklj077",
        "comment_body": "i don't think this will install autoawq_extensions (the prebuilt binary is built against torch2.4.1+cu124 https://github.com/casper-hansen/AutoAWQ_kernels/releases/tag/v0.0.8)\r\n\r\nif autoawq_extensions is not installed, triton is used, which also requires a cuda compiler.",
        "pr_file_module": null
      },
      {
        "comment_id": "1839622989",
        "repo_full_name": "QwenLM/Qwen3",
        "pr_number": 1068,
        "pr_file": "examples/speed-benchmark/requirements/perf_transformer.txt",
        "discussion_id": "1837564209",
        "commented_code": "@@ -0,0 +1,9 @@\n+torch==2.3.1\n+transformers==4.46.0\n+auto_gptq==0.7.1\n+autoawq==0.2.6",
        "comment_created_at": "2024-11-13T07:10:56+00:00",
        "comment_author": "wangxingjun778",
        "comment_body": "The installation: pip install autoawq\uff08verified on autoawq>=0.2.4, CUDA>=11.8\uff09\r\n\r\ncan be installed autoawq-kernels prebuilt wheel automatically, it means Triton will NOT be used.",
        "pr_file_module": null
      }
    ]
  }
]