[
  {
    "discussion_id": "2149283784",
    "pr_number": 822,
    "pr_file": "CONTRIBUTING.md",
    "created_at": "2025-06-16T07:55:39+00:00",
    "commented_code": "-# \ud83c\udf1f Contribute to Refact.ai Agent\n+# \ud83e\udd16 Contributing Models and Providers to Refact BYOK\n \n-Welcome to the Refact.ai project! We're excited to have you join our community. Whether you're a first-time contributor or a seasoned developer, here are some impactful ways to get involved.\n+Welcome to the comprehensive guide for adding new models and providers to Refact's Bring Your Own Key (BYOK) functionality! This guide will walk you through everything you need to know to contribute new models and providers to the Refact ecosystem.\n+\n+\n+**Note:** At the moment, we are only accepting contributions related to **adding new models**.\n+Stay tuned for updates on other contribution opportunities!\n \n \n ## \ud83d\udcda Table of Contents\n-- [\u2764\ufe0f Ways to Contribute](#%EF%B8%8F-ways-to-contribute)\n-  - [\ud83d\udc1b Report Bugs](#-report-bugs)\n-  - [Contributing To Code](#contributing-to-code)\n-    - [ What Can I Do](#what-can-i-do)\n-    - [Coding Standards](#coding-standards)\n-    - [Testing](#testing)\n-    - [Contact](#contact)\n \n+- [\ud83d\ude80 Quick Start](#-quick-start)\n+- [\ud83d\udee0\ufe0f Development Environment Setup](#\ufe0f-development-environment-setup)\n+- [\ud83e\udde0 Adding Chat Models](#-adding-chat-models)\n+- [\u26a1 Adding Completion Models](#-adding-completion-models)\n+- [\ud83d\udd0c Adding New Providers](#-adding-new-providers)\n+- [\ud83e\uddea Testing Your Contributions](#-testing-your-contributions)\n+- [\ud83d\udccb Best Practices](#-best-practices)\n+- [\ud83d\udc1b Troubleshooting](#-troubleshooting)\n+- [\ud83d\udca1 Examples](#-examples)\n \n-### \u2764\ufe0f Ways to Contribute\n+## \ud83d\ude80 Quick Start\n \n-* Fork the repository\n-* Create a feature branch\n-* Do the work\n-* Create a pull request\n-* Maintainers will review it\n+Before diving deep, here's what you need to know:\n \n-### \ud83d\udc1b Report Bugs\n-Encountered an issue? Help us improve Refact.ai by reporting bugs in issue section, make sure you label the issue with correct tag [here](https://github.com/smallcloudai/refact-lsp/issues)! \n+1. **Chat Models** are for conversational AI (like GPT-4, Claude)\n+2. **Completion Models** are for code completion (preferably FIM models) like qwen-2.5-coder-base, starcoder2 and deepseek-coder\n+3. **Providers** are services that host these models (OpenAI, Anthropic, etc.)\n \n-### \ud83d\udcd6 Improving Documentation\n-Help us make Refact.ai more accessible by contributing to our documentation, make sure you label the issue with correct tag! Create issues [here](https://github.com/smallcloudai/web_docs_refact_ai/issues).\n+## \ud83d\udee0\ufe0f Development Environment Setup\n \n-### Contributing To Code\n+### Prerequisites\n \n-#### What Can I Do?\n+- **Rust** (latest stable version)\n+- **Node.js** and **npm** (for React frontend)\n+- **Chrome/Chromium** (dev dependency)\n+- **Git**\n \n-Before you start, create an issue with a title that begins with `[idea] ...`. The field of AI and agents is vast,\n-and not every idea will benefit the project, even if it is a good idea in itself.\n+### Setting Up the Rust Backend (Engine)\n \n-Another rule of thumb: Only implement a feature you can test thoroughly.\n+```bash\n+# Clone the repository\n+git clone https://github.com/smallcloudai/refact.git\n+cd refact\n \n+# Install Rust if you haven't already\n+curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n+source ~/.cargo/env\n \n-#### Coding Standards\n+# Navigate to the engine directory\n+cd refact-agent/engine/\n \n-Good practices for Rust are generally applicable to this project. There are a few points however:\n+# Build the project\n+cargo build\n \n-1. Naming. Use \"Find in files...\" to check if a name you give to your structs, fields, functions is too\n-generic. If a name is already all over the project, be more specific. For example \"IntegrationGitHub\" is a good\n-name, but \"Integration\" is not, even if it's in `github.rs` and files work as namespaces in Rust. It's\n-still hard to navigate the project if you can't use search.\n+# Run the engine with your API key\n+cargo run -- --address-url Refact --api-key <YOUR_CLOUD_API_KEY> --http-port 8001 --lsp-port 8002 --logs-stderr --vecdb --ast --workspace-folder .\n+```\n \n-2. Locks. For some reason, it's still hard for most people, and for current AI models, too. Refact is\n-multi-threaded, locks are necessary. But locks need to be activated for the shortest time possible, this\n-is how you use `Arc<AMutex<>>` to do it:\n+### Setting Up the React Frontend (GUI)\n \n-```rust\n-struct BigStruct {\n-    ...\n-    pub small_struct: Arc<AMutex<SmallStruct>>;\n-}\n+```bash\n+# In a new terminal, navigate to the GUI directory\n+cd refact-agent/gui/\n \n-fn some_code(big_struct: Arc<AMutex<BigStruct>>)\n-{\n-    let small_struct = {\n-        let big_struct_locked = big_struct.lock().await;\n-        big_struct_locked.small_struct.clone()  // cloning Arc is cheap\n-        // big_struct_locked is destroyed here because it goes out of scope\n-    };\n-    // use small_struct without holding big_struct_locked\n-}\n+# Install dependencies\n+npm ci\n+\n+# Start the development server\n+npm run dev\n ```\n \n-Another multi-threaded trick, move a member function outside of a class:\n+The frontend will connect to the Rust engine running on port 8001.\n \n-```rust\n-struct MyStruct {\n-    pub data1: i32,\n-    pub data2: i32,\n-}\n+## \ud83e\udde0 Adding Chat Models\n \n-impl MyStruct {\n-    pub fn lengthy_function1(&mut self)  {  }\n-}\n+Chat models are used for conversational AI interactions. Here's how to add them:\n+\n+### Step 1: Add Model to Known Models\n+\n+Edit `refact-agent/engine/src/known_models.json`:\n \n-fn some_code(my_struct: Arc<AMutex<SmallStruct>>)\n+```json\n {\n-    my_struct.lock().await.lengthy_function1();\n-    // Whoops, lengthy_function has the whole structure locked for a long time,\n-    // and Rust won't not let you unlock it\n+  \"chat_models\": {\n+    \"your-new-model\": {\n+      \"n_ctx\": 128000,\n+      \"supports_tools\": true,\n+      \"supports_multimodality\": true,\n+      \"supports_agent\": true,\n+      \"scratchpad\": \"PASSTHROUGH\",\n+      \"tokenizer\": \"hf://your-tokenizer-path\",\n+      \"similar_models\": [\n+        \"similar-model-1\",\n+        \"similar-model-2\"\n+      ]\n+    }\n+  }\n }\n+```\n+\n+### Step 2: Add to Provider Configuration\n+\n+For existing providers, edit the appropriate YAML file in `refact-agent/engine/src/yaml_configs/default_providers/`:\n+\n+```yaml\n+# Example: anthropic.yaml\n+running_models:\n+  - claude-3-7-sonnet-latest\n+  - claude-3-5-sonnet-latest\n+  - your-new-model  # Add your model here\n+\n+chat_models:\n+  your-new-model:\n+    n_ctx: 200000\n+    supports_tools: true\n+    supports_multimodality: true\n+    supports_agent: true\n+    tokenizer: hf://your-tokenizer-path\n+```\n+\n+### Step 3: Test the Model\n+\n+```bash\n+# Test with curl\n+curl http://127.0.0.1:8001/v1/chat/completions -k \\\n+  -H 'Content-Type: application/json' \\\n+  -d '{\n+    \"model\": \"provider/your-new-model\",\n+    \"messages\": [\n+      {\"role\": \"user\", \"content\": \"Hello, test message\"}\n+    ],\n+    \"stream\": false,\n+    \"temperature\": 0.1,\n+    \"max_tokens\": 100\n+  }'\n+```\n+\n+## \u26a1 Adding Completion Models\n+\n+Completion models are used for code completion. FIM (Fill-in-the-Middle) models work best.\n+\n+### Step 1: Understand FIM Tokens\n+\n+FIM models use special tokens:\n+- `fim_prefix`: Text before the cursor\n+- `fim_suffix`: Text after the cursor  \n+- `fim_middle`: Where the completion goes\n+- `eot`: End of text token\n \n-pub fn lengthy_function2(s: Arc<AMutex<SmallStruct>>)\n+### Step 2: Add to Known Models\n+\n+```json\n {\n-    let (data1, data2) = {\n-        let s_locked = s.lock().await;\n-        (s_locked.data1.clone(), s_locked.data2.clone())\n+  \"completion_models\": {\n+    \"your-completion-model\": {\n+      \"n_ctx\": 8192,\n+      \"scratchpad_patch\": {\n+        \"fim_prefix\": \"<|fim_prefix|>\",\n+        \"fim_suffix\": \"<|fim_suffix|>\", \n+        \"fim_middle\": \"<|fim_middle|>\",\n+        \"eot\": \"<|endoftext|>\",\n+        \"extra_stop_tokens\": [\n+          \"<|repo_name|>\",\n+          \"<|file_sep|>\"\n+        ],\n+        \"context_format\": \"your-format\",\n+        \"rag_ratio\": 0.5\n+      },\n+      \"scratchpad\": \"FIM-PSM\",\n+      \"tokenizer\": \"hf://your-tokenizer-path\",\n+      \"similar_models\": []\n     }\n-    // Do lengthy stuff here without locks!\n+  }\n }\n ```\n \n-Avoid nested locks, avoid RwLock unless you know what you are doing.\n+### Step 3: Test Code Completion\n+\n+```bash\n+curl http://127.0.0.1:8001/v1/code-completion -k \\\n+  -H 'Content-Type: application/json' \\\n+  -d '{\n+    \"inputs\": {\n+      \"sources\": {\"test.py\": \"def hello_world():\"},\n+      \"cursor\": {\n+        \"file\": \"test.py\",\n+        \"line\": 0,\n+        \"character\": 18\n+      },\n+      \"multiline\": true\n+    },\n+    \"stream\": false,\n+    \"parameters\": {\n+      \"temperature\": 0.1,\n+      \"max_new_tokens\": 50\n+    }\n+  }'\n+```\n+\n+## \ud83d\udd0c Adding New Providers\n+\n+To add a completely new OpenAI-compatible provider:\n+\n+### Step 1: Create Provider Configuration\n+\n+Create `refact-agent/engine/src/yaml_configs/default_providers/your-provider.yaml`:\n+\n+```yaml\n+chat_endpoint: https://api.your-provider.com/v1/chat/completions\n+completion_endpoint: https://api.your-provider.com/v1/completions\n+embedding_endpoint: https://api.your-provider.com/v1/embeddings\n+supports_completion: true\n+\n+api_key: your-api-key-format\n+\n+running_models:\n+  - your-model-1\n+  - your-model-2\n+\n+model_default_settings_ui:\n+  chat:\n+    n_ctx: 128000\n+    supports_tools: true\n+    supports_multimodality: false\n+    supports_agent: true\n+    tokenizer: hf://your-default-tokenizer\n+  completion:\n+    n_ctx: 8192\n+    tokenizer: hf://your-completion-tokenizer\n+```\n+\n+### Step 2: Add to Provider List\n+\n+Edit `refact-agent/engine/src/caps/providers.rs` and add your provider to the `PROVIDER_TEMPLATES` array:\n+\n+```rust\n+const PROVIDER_TEMPLATES: &[(&str, &str)] = &[\n+    (\"anthropic\", include_str!(\"../yaml_configs/default_providers/anthropic.yaml\")),\n+    (\"openai\", include_str!(\"../yaml_configs/default_providers/openai.yaml\")),\n+    // ... existing providers ...\n+    (\"your-provider\", include_str!(\"../yaml_configs/default_providers/your-provider.yaml\")),\n+];\n+```\n+\n+### Step 3: Test Provider Integration\n+\n+```bash\n+# Check if provider is loaded\n+curl http://127.0.0.1:8001/v1/caps\n+\n+# Test with your provider\n+curl http://127.0.0.1:8001/v1/chat/completions -k \\\n+  -H 'Content-Type: application/json' \\\n+  -d '{\n+    \"model\": \"your-provider/your-model\",\n+    \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n+  }'\n+```\n+\n+## \ud83e\uddea Testing Your Contributions\n+\n+### Unit Tests\n+\n+```bash\n+cd refact-agent/engine/\n+cargo test\n+```\n+\n+### Integration Tests\n+\n+```bash\n+# Start the engine\n+cargo run -- --http-port 8001 --logs-stderr --experimental --workspace-folder .\n+\n+# Run Python integration tests\n+cd tests/\n+python test_your_feature.py\n+```\n+\n+### Manual Testing Checklist\n+\n+-  Model appears in capabilities endpoint (`/v1/caps`)\n+-  Chat functionality works\n+-  Code completion works (for completion models)\n+-  Tool calling works (if supported)\n+-  Multimodality works (if supported)\n+-  Error handling is graceful\n+-  Performance is acceptable\n+\n+### Using xDebug for IDE Testing\n+\n+Enable xDebug in your IDE extension settings to connect to your locally built Rust binary for testing completion models.\n+\n+## \ud83d\udccb Best Practices\n+\n+### Model Configuration\n+\n+1. **Context Windows**: Set realistic `n_ctx` values based on the model's actual capabilities\n+2. **Capabilities**: Only enable features the model actually supports\n+3. **Tokenizers**: Use the correct tokenizer for accurate token counting\n+4. **Similar Models**: Group models with similar capabilities\n+\n+### Provider Configuration\n+\n+1. **API Keys**: Use environment variables for sensitive data\n+2. **Endpoints**: Ensure URLs are correct and follow OpenAI compatibility\n+3. **Error Handling**: Test edge cases and error conditions\n+4. **Rate Limiting**: Consider provider-specific limitations\n \n+### Code Quality\n \n-#### Testing\n+1. **Follow Rust conventions**: Use `cargo fmt` and `cargo clippy`\n+2. **Add tests**: Include unit tests for new functionality\n+3. **Commit messages**: Use clear, descriptive commit messages\n \n-It's a good idea to have tests in source files, and run them using `cargo test`, and we\n-have CI in place to run it automatically.\n-But not everything can be tested solely within Rust tests, for example a Rust test cannot run\n-an AI model inside.\n+## \ud83d\udc1b Troubleshooting\n \n-So we have `tests/*.py` scripts that expect the `refact-lsp` process to be running on port 8001,\n-and the project itself as a workspace dir:\n+### Common Issues\n \n+**Model not appearing in capabilities:**\n+- Check if it's added to `running_models` in provider config\n+- Verify the model exists in `known_models.json`\n+- Ensure provider is properly loaded\n+\n+**Tokenizer errors:**\n+- Verify tokenizer path is correct\n+- Check if tokenizer supports the model's special tokens\n+- Use `fake` tokenizer for testing if needed\n+\n+**API connection issues:**\n+- Verify endpoint URLs are correct\n+- Check API key format and permissions\n+- Test with curl directly first\n+\n+**Completion not working:**\n+- Ensure FIM tokens are correctly configured\n+- Check `scratchpad` type is appropriate\n+- Verify context format matches model expectations\n+\n+### Debug Commands\n \n ```bash\n-cargo build && target/debug/refact-lsp --http-port 8001 --reset-memory --experimental --workspace-folder . --logs-stderr --vecdb --ast\n+# Check logs\n+cargo run -- --logs-stderr --experimental\n+\n+# Test specific endpoints\n+curl http://127.0.0.1:8001/v1/caps\n+curl http://127.0.0.1:8001/v1/rag-status\n+\n+# Validate configuration\n+cargo check\n+```\n+\n+## \ud83d\udca1 Examples\n+\n+### Example 1: Adding Claude 4 (Hypothetical) \n+\n+1. **Add to known_models.json:**\n+```json\n+\"claude-4\": {\n+  \"n_ctx\": 300000,\n+  \"supports_tools\": true,\n+  \"supports_multimodality\": true,\n+  \"supports_agent\": true,\n+  \"scratchpad\": \"PASSTHROUGH\",\n+  \"tokenizer\": \"hf://Xenova/claude-tokenizer\"\n+}\n+```\n+\n+2. **Update anthropic.yaml:**\n+```yaml\n+running_models:",
    "repo_full_name": "smallcloudai/refact",
    "discussion_comments": [
      {
        "comment_id": "2149283784",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 822,
        "pr_file": "CONTRIBUTING.md",
        "discussion_id": "2149283784",
        "commented_code": "@@ -1,124 +1,424 @@\n-# \ud83c\udf1f Contribute to Refact.ai Agent\n+# \ud83e\udd16 Contributing Models and Providers to Refact BYOK\n \n-Welcome to the Refact.ai project! We're excited to have you join our community. Whether you're a first-time contributor or a seasoned developer, here are some impactful ways to get involved.\n+Welcome to the comprehensive guide for adding new models and providers to Refact's Bring Your Own Key (BYOK) functionality! This guide will walk you through everything you need to know to contribute new models and providers to the Refact ecosystem.\n+\n+\n+**Note:** At the moment, we are only accepting contributions related to **adding new models**.\n+Stay tuned for updates on other contribution opportunities!\n \n \n ## \ud83d\udcda Table of Contents\n-- [\u2764\ufe0f Ways to Contribute](#%EF%B8%8F-ways-to-contribute)\n-  - [\ud83d\udc1b Report Bugs](#-report-bugs)\n-  - [Contributing To Code](#contributing-to-code)\n-    - [ What Can I Do](#what-can-i-do)\n-    - [Coding Standards](#coding-standards)\n-    - [Testing](#testing)\n-    - [Contact](#contact)\n \n+- [\ud83d\ude80 Quick Start](#-quick-start)\n+- [\ud83d\udee0\ufe0f Development Environment Setup](#\ufe0f-development-environment-setup)\n+- [\ud83e\udde0 Adding Chat Models](#-adding-chat-models)\n+- [\u26a1 Adding Completion Models](#-adding-completion-models)\n+- [\ud83d\udd0c Adding New Providers](#-adding-new-providers)\n+- [\ud83e\uddea Testing Your Contributions](#-testing-your-contributions)\n+- [\ud83d\udccb Best Practices](#-best-practices)\n+- [\ud83d\udc1b Troubleshooting](#-troubleshooting)\n+- [\ud83d\udca1 Examples](#-examples)\n \n-### \u2764\ufe0f Ways to Contribute\n+## \ud83d\ude80 Quick Start\n \n-* Fork the repository\n-* Create a feature branch\n-* Do the work\n-* Create a pull request\n-* Maintainers will review it\n+Before diving deep, here's what you need to know:\n \n-### \ud83d\udc1b Report Bugs\n-Encountered an issue? Help us improve Refact.ai by reporting bugs in issue section, make sure you label the issue with correct tag [here](https://github.com/smallcloudai/refact-lsp/issues)! \n+1. **Chat Models** are for conversational AI (like GPT-4, Claude)\n+2. **Completion Models** are for code completion (preferably FIM models) like qwen-2.5-coder-base, starcoder2 and deepseek-coder\n+3. **Providers** are services that host these models (OpenAI, Anthropic, etc.)\n \n-### \ud83d\udcd6 Improving Documentation\n-Help us make Refact.ai more accessible by contributing to our documentation, make sure you label the issue with correct tag! Create issues [here](https://github.com/smallcloudai/web_docs_refact_ai/issues).\n+## \ud83d\udee0\ufe0f Development Environment Setup\n \n-### Contributing To Code\n+### Prerequisites\n \n-#### What Can I Do?\n+- **Rust** (latest stable version)\n+- **Node.js** and **npm** (for React frontend)\n+- **Chrome/Chromium** (dev dependency)\n+- **Git**\n \n-Before you start, create an issue with a title that begins with `[idea] ...`. The field of AI and agents is vast,\n-and not every idea will benefit the project, even if it is a good idea in itself.\n+### Setting Up the Rust Backend (Engine)\n \n-Another rule of thumb: Only implement a feature you can test thoroughly.\n+```bash\n+# Clone the repository\n+git clone https://github.com/smallcloudai/refact.git\n+cd refact\n \n+# Install Rust if you haven't already\n+curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n+source ~/.cargo/env\n \n-#### Coding Standards\n+# Navigate to the engine directory\n+cd refact-agent/engine/\n \n-Good practices for Rust are generally applicable to this project. There are a few points however:\n+# Build the project\n+cargo build\n \n-1. Naming. Use \"Find in files...\" to check if a name you give to your structs, fields, functions is too\n-generic. If a name is already all over the project, be more specific. For example \"IntegrationGitHub\" is a good\n-name, but \"Integration\" is not, even if it's in `github.rs` and files work as namespaces in Rust. It's\n-still hard to navigate the project if you can't use search.\n+# Run the engine with your API key\n+cargo run -- --address-url Refact --api-key <YOUR_CLOUD_API_KEY> --http-port 8001 --lsp-port 8002 --logs-stderr --vecdb --ast --workspace-folder .\n+```\n \n-2. Locks. For some reason, it's still hard for most people, and for current AI models, too. Refact is\n-multi-threaded, locks are necessary. But locks need to be activated for the shortest time possible, this\n-is how you use `Arc<AMutex<>>` to do it:\n+### Setting Up the React Frontend (GUI)\n \n-```rust\n-struct BigStruct {\n-    ...\n-    pub small_struct: Arc<AMutex<SmallStruct>>;\n-}\n+```bash\n+# In a new terminal, navigate to the GUI directory\n+cd refact-agent/gui/\n \n-fn some_code(big_struct: Arc<AMutex<BigStruct>>)\n-{\n-    let small_struct = {\n-        let big_struct_locked = big_struct.lock().await;\n-        big_struct_locked.small_struct.clone()  // cloning Arc is cheap\n-        // big_struct_locked is destroyed here because it goes out of scope\n-    };\n-    // use small_struct without holding big_struct_locked\n-}\n+# Install dependencies\n+npm ci\n+\n+# Start the development server\n+npm run dev\n ```\n \n-Another multi-threaded trick, move a member function outside of a class:\n+The frontend will connect to the Rust engine running on port 8001.\n \n-```rust\n-struct MyStruct {\n-    pub data1: i32,\n-    pub data2: i32,\n-}\n+## \ud83e\udde0 Adding Chat Models\n \n-impl MyStruct {\n-    pub fn lengthy_function1(&mut self)  {  }\n-}\n+Chat models are used for conversational AI interactions. Here's how to add them:\n+\n+### Step 1: Add Model to Known Models\n+\n+Edit `refact-agent/engine/src/known_models.json`:\n \n-fn some_code(my_struct: Arc<AMutex<SmallStruct>>)\n+```json\n {\n-    my_struct.lock().await.lengthy_function1();\n-    // Whoops, lengthy_function has the whole structure locked for a long time,\n-    // and Rust won't not let you unlock it\n+  \"chat_models\": {\n+    \"your-new-model\": {\n+      \"n_ctx\": 128000,\n+      \"supports_tools\": true,\n+      \"supports_multimodality\": true,\n+      \"supports_agent\": true,\n+      \"scratchpad\": \"PASSTHROUGH\",\n+      \"tokenizer\": \"hf://your-tokenizer-path\",\n+      \"similar_models\": [\n+        \"similar-model-1\",\n+        \"similar-model-2\"\n+      ]\n+    }\n+  }\n }\n+```\n+\n+### Step 2: Add to Provider Configuration\n+\n+For existing providers, edit the appropriate YAML file in `refact-agent/engine/src/yaml_configs/default_providers/`:\n+\n+```yaml\n+# Example: anthropic.yaml\n+running_models:\n+  - claude-3-7-sonnet-latest\n+  - claude-3-5-sonnet-latest\n+  - your-new-model  # Add your model here\n+\n+chat_models:\n+  your-new-model:\n+    n_ctx: 200000\n+    supports_tools: true\n+    supports_multimodality: true\n+    supports_agent: true\n+    tokenizer: hf://your-tokenizer-path\n+```\n+\n+### Step 3: Test the Model\n+\n+```bash\n+# Test with curl\n+curl http://127.0.0.1:8001/v1/chat/completions -k \\\n+  -H 'Content-Type: application/json' \\\n+  -d '{\n+    \"model\": \"provider/your-new-model\",\n+    \"messages\": [\n+      {\"role\": \"user\", \"content\": \"Hello, test message\"}\n+    ],\n+    \"stream\": false,\n+    \"temperature\": 0.1,\n+    \"max_tokens\": 100\n+  }'\n+```\n+\n+## \u26a1 Adding Completion Models\n+\n+Completion models are used for code completion. FIM (Fill-in-the-Middle) models work best.\n+\n+### Step 1: Understand FIM Tokens\n+\n+FIM models use special tokens:\n+- `fim_prefix`: Text before the cursor\n+- `fim_suffix`: Text after the cursor  \n+- `fim_middle`: Where the completion goes\n+- `eot`: End of text token\n \n-pub fn lengthy_function2(s: Arc<AMutex<SmallStruct>>)\n+### Step 2: Add to Known Models\n+\n+```json\n {\n-    let (data1, data2) = {\n-        let s_locked = s.lock().await;\n-        (s_locked.data1.clone(), s_locked.data2.clone())\n+  \"completion_models\": {\n+    \"your-completion-model\": {\n+      \"n_ctx\": 8192,\n+      \"scratchpad_patch\": {\n+        \"fim_prefix\": \"<|fim_prefix|>\",\n+        \"fim_suffix\": \"<|fim_suffix|>\", \n+        \"fim_middle\": \"<|fim_middle|>\",\n+        \"eot\": \"<|endoftext|>\",\n+        \"extra_stop_tokens\": [\n+          \"<|repo_name|>\",\n+          \"<|file_sep|>\"\n+        ],\n+        \"context_format\": \"your-format\",\n+        \"rag_ratio\": 0.5\n+      },\n+      \"scratchpad\": \"FIM-PSM\",\n+      \"tokenizer\": \"hf://your-tokenizer-path\",\n+      \"similar_models\": []\n     }\n-    // Do lengthy stuff here without locks!\n+  }\n }\n ```\n \n-Avoid nested locks, avoid RwLock unless you know what you are doing.\n+### Step 3: Test Code Completion\n+\n+```bash\n+curl http://127.0.0.1:8001/v1/code-completion -k \\\n+  -H 'Content-Type: application/json' \\\n+  -d '{\n+    \"inputs\": {\n+      \"sources\": {\"test.py\": \"def hello_world():\"},\n+      \"cursor\": {\n+        \"file\": \"test.py\",\n+        \"line\": 0,\n+        \"character\": 18\n+      },\n+      \"multiline\": true\n+    },\n+    \"stream\": false,\n+    \"parameters\": {\n+      \"temperature\": 0.1,\n+      \"max_new_tokens\": 50\n+    }\n+  }'\n+```\n+\n+## \ud83d\udd0c Adding New Providers\n+\n+To add a completely new OpenAI-compatible provider:\n+\n+### Step 1: Create Provider Configuration\n+\n+Create `refact-agent/engine/src/yaml_configs/default_providers/your-provider.yaml`:\n+\n+```yaml\n+chat_endpoint: https://api.your-provider.com/v1/chat/completions\n+completion_endpoint: https://api.your-provider.com/v1/completions\n+embedding_endpoint: https://api.your-provider.com/v1/embeddings\n+supports_completion: true\n+\n+api_key: your-api-key-format\n+\n+running_models:\n+  - your-model-1\n+  - your-model-2\n+\n+model_default_settings_ui:\n+  chat:\n+    n_ctx: 128000\n+    supports_tools: true\n+    supports_multimodality: false\n+    supports_agent: true\n+    tokenizer: hf://your-default-tokenizer\n+  completion:\n+    n_ctx: 8192\n+    tokenizer: hf://your-completion-tokenizer\n+```\n+\n+### Step 2: Add to Provider List\n+\n+Edit `refact-agent/engine/src/caps/providers.rs` and add your provider to the `PROVIDER_TEMPLATES` array:\n+\n+```rust\n+const PROVIDER_TEMPLATES: &[(&str, &str)] = &[\n+    (\"anthropic\", include_str!(\"../yaml_configs/default_providers/anthropic.yaml\")),\n+    (\"openai\", include_str!(\"../yaml_configs/default_providers/openai.yaml\")),\n+    // ... existing providers ...\n+    (\"your-provider\", include_str!(\"../yaml_configs/default_providers/your-provider.yaml\")),\n+];\n+```\n+\n+### Step 3: Test Provider Integration\n+\n+```bash\n+# Check if provider is loaded\n+curl http://127.0.0.1:8001/v1/caps\n+\n+# Test with your provider\n+curl http://127.0.0.1:8001/v1/chat/completions -k \\\n+  -H 'Content-Type: application/json' \\\n+  -d '{\n+    \"model\": \"your-provider/your-model\",\n+    \"messages\": [{\"role\": \"user\", \"content\": \"Test\"}]\n+  }'\n+```\n+\n+## \ud83e\uddea Testing Your Contributions\n+\n+### Unit Tests\n+\n+```bash\n+cd refact-agent/engine/\n+cargo test\n+```\n+\n+### Integration Tests\n+\n+```bash\n+# Start the engine\n+cargo run -- --http-port 8001 --logs-stderr --experimental --workspace-folder .\n+\n+# Run Python integration tests\n+cd tests/\n+python test_your_feature.py\n+```\n+\n+### Manual Testing Checklist\n+\n+-  Model appears in capabilities endpoint (`/v1/caps`)\n+-  Chat functionality works\n+-  Code completion works (for completion models)\n+-  Tool calling works (if supported)\n+-  Multimodality works (if supported)\n+-  Error handling is graceful\n+-  Performance is acceptable\n+\n+### Using xDebug for IDE Testing\n+\n+Enable xDebug in your IDE extension settings to connect to your locally built Rust binary for testing completion models.\n+\n+## \ud83d\udccb Best Practices\n+\n+### Model Configuration\n+\n+1. **Context Windows**: Set realistic `n_ctx` values based on the model's actual capabilities\n+2. **Capabilities**: Only enable features the model actually supports\n+3. **Tokenizers**: Use the correct tokenizer for accurate token counting\n+4. **Similar Models**: Group models with similar capabilities\n+\n+### Provider Configuration\n+\n+1. **API Keys**: Use environment variables for sensitive data\n+2. **Endpoints**: Ensure URLs are correct and follow OpenAI compatibility\n+3. **Error Handling**: Test edge cases and error conditions\n+4. **Rate Limiting**: Consider provider-specific limitations\n \n+### Code Quality\n \n-#### Testing\n+1. **Follow Rust conventions**: Use `cargo fmt` and `cargo clippy`\n+2. **Add tests**: Include unit tests for new functionality\n+3. **Commit messages**: Use clear, descriptive commit messages\n \n-It's a good idea to have tests in source files, and run them using `cargo test`, and we\n-have CI in place to run it automatically.\n-But not everything can be tested solely within Rust tests, for example a Rust test cannot run\n-an AI model inside.\n+## \ud83d\udc1b Troubleshooting\n \n-So we have `tests/*.py` scripts that expect the `refact-lsp` process to be running on port 8001,\n-and the project itself as a workspace dir:\n+### Common Issues\n \n+**Model not appearing in capabilities:**\n+- Check if it's added to `running_models` in provider config\n+- Verify the model exists in `known_models.json`\n+- Ensure provider is properly loaded\n+\n+**Tokenizer errors:**\n+- Verify tokenizer path is correct\n+- Check if tokenizer supports the model's special tokens\n+- Use `fake` tokenizer for testing if needed\n+\n+**API connection issues:**\n+- Verify endpoint URLs are correct\n+- Check API key format and permissions\n+- Test with curl directly first\n+\n+**Completion not working:**\n+- Ensure FIM tokens are correctly configured\n+- Check `scratchpad` type is appropriate\n+- Verify context format matches model expectations\n+\n+### Debug Commands\n \n ```bash\n-cargo build && target/debug/refact-lsp --http-port 8001 --reset-memory --experimental --workspace-folder . --logs-stderr --vecdb --ast\n+# Check logs\n+cargo run -- --logs-stderr --experimental\n+\n+# Test specific endpoints\n+curl http://127.0.0.1:8001/v1/caps\n+curl http://127.0.0.1:8001/v1/rag-status\n+\n+# Validate configuration\n+cargo check\n+```\n+\n+## \ud83d\udca1 Examples\n+\n+### Example 1: Adding Claude 4 (Hypothetical) \n+\n+1. **Add to known_models.json:**\n+```json\n+\"claude-4\": {\n+  \"n_ctx\": 300000,\n+  \"supports_tools\": true,\n+  \"supports_multimodality\": true,\n+  \"supports_agent\": true,\n+  \"scratchpad\": \"PASSTHROUGH\",\n+  \"tokenizer\": \"hf://Xenova/claude-tokenizer\"\n+}\n+```\n+\n+2. **Update anthropic.yaml:**\n+```yaml\n+running_models:",
        "comment_created_at": "2025-06-16T07:55:39+00:00",
        "comment_author": "humbertoyusta",
        "comment_body": "then in here, you need to add the n_ctx and all parts in the yaml",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2154319179",
    "pr_number": 822,
    "pr_file": "CONTRIBUTING.md",
    "created_at": "2025-06-18T11:05:23+00:00",
    "commented_code": "-# \ud83c\udf1f Contribute to Refact.ai Agent\n+## \ud83e\udd1d Contributing to Refact Agent\n+Thanks for your interest in contributing to Refact Agent! We\u2019re an open-source agent build with community \u2014 and we\u2019re excited to have you here.\n \n-Welcome to the Refact.ai project! We're excited to have you join our community. Whether you're a first-time contributor or a seasoned developer, here are some impactful ways to get involved.\n+Whether you're fixing a bug, adding a new model, improving the docs, or exploring areas like the MCP catalog \u2014 your contributions help shape the future of AI Agents.\n \n+## \ud83c\udf31 How You Can Contribute\n+There\u2019s no single path to contributing. Here are a few great starting points:\n+\n+- Try Refact out and open issues when you hit bugs or have feature ideas.\n+- Add a new model or provider \u2014 this guide includes an example of how to do that\n+- Explore and extend the MCP catalog\n+- Improve docs\n+\n+Much of the setup info in this doc applies across different areas \u2014 so feel free to contribute where your interest leads you.\n+\n+## \u2728 Got Something Else in Mind?\n+If you're excited about something that\u2019s not listed here \u2014 feel free to reach out on Discord Community (#contribution channel). We're always open to new contributions and ways to improve together.\n \n ## \ud83d\udcda Table of Contents\n-- [\u2764\ufe0f Ways to Contribute](#%EF%B8%8F-ways-to-contribute)\n-  - [\ud83d\udc1b Report Bugs](#-report-bugs)\n-  - [Contributing To Code](#contributing-to-code)\n-    - [ What Can I Do](#what-can-i-do)\n-    - [Coding Standards](#coding-standards)\n-    - [Testing](#testing)\n-    - [Contact](#contact)\n \n+- [\ud83d\ude80 Quick Start](#-quick-start)\n+- [\ud83d\udee0\ufe0f Development Environment Setup](#\ufe0f-development-environment-setup)\n+- [\ud83e\udde0 Adding Chat Models](#-adding-chat-models)\n+- [\u26a1 Adding Completion Models](#-adding-completion-models)\n+- [\ud83d\udd0c Adding New Providers](#-adding-new-providers)\n+- [\ud83e\uddea Testing Your Contributions](#-testing-your-contributions)\n+- [\ud83d\udccb Best Practices](#-best-practices)\n+- [\ud83d\udc1b Troubleshooting](#-troubleshooting)\n+- [\ud83d\udca1 Examples](#-examples)\n \n-### \u2764\ufe0f Ways to Contribute\n+## \ud83d\ude80 Quick Start\n \n-* Fork the repository\n-* Create a feature branch\n-* Do the work\n-* Create a pull request\n-* Maintainers will review it\n+Before diving deep, here's what you need to know:\n \n-### \ud83d\udc1b Report Bugs\n-Encountered an issue? Help us improve Refact.ai by reporting bugs in issue section, make sure you label the issue with correct tag [here](https://github.com/smallcloudai/refact-lsp/issues)! \n+1. **Chat Models** are for conversational AI (like GPT-4, Claude)\n+2. **Completion Models** are for code completion (preferably FIM models) like qwen-2.5-coder-base, starcoder2 and deepseek-coder\n+3. **Providers** are services that host these models (OpenAI, Anthropic, etc.)\n \n-### \ud83d\udcd6 Improving Documentation\n-Help us make Refact.ai more accessible by contributing to our documentation, make sure you label the issue with correct tag! Create issues [here](https://github.com/smallcloudai/web_docs_refact_ai/issues).\n+## \ud83d\udee0\ufe0f Development Environment Setup\n \n-### Contributing To Code\n+### Prerequisites\n \n-#### What Can I Do?\n+- **Rust** (latest stable version)\n+- **Node.js** and **npm** (for React frontend)\n+- **Chrome/Chromium** (dev dependency)\n+- **Git**\n \n-Before you start, create an issue with a title that begins with `[idea] ...`. The field of AI and agents is vast,\n-and not every idea will benefit the project, even if it is a good idea in itself.\n+### Setting Up the Rust Backend (Engine)\n \n-Another rule of thumb: Only implement a feature you can test thoroughly.\n+```bash\n+# Clone the repository\n+git clone https://github.com/smallcloudai/refact.git\n+cd refact\n \n+# Install Rust if you haven't already\n+curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n+source ~/.cargo/env\n \n-#### Coding Standards\n+# Navigate to the engine directory\n+cd refact-agent/engine/\n \n-Good practices for Rust are generally applicable to this project. There are a few points however:\n+# Build the project\n+cargo build\n \n-1. Naming. Use \"Find in files...\" to check if a name you give to your structs, fields, functions is too\n-generic. If a name is already all over the project, be more specific. For example \"IntegrationGitHub\" is a good\n-name, but \"Integration\" is not, even if it's in `github.rs` and files work as namespaces in Rust. It's\n-still hard to navigate the project if you can't use search.\n+# Run the engine with your API key\n+cargo run -- --address-url Refact --api-key <YOUR_CLOUD_API_KEY> --http-port 8001 --lsp-port 8002 --logs-stderr --vecdb --ast --workspace-folder .\n+```\n \n-2. Locks. For some reason, it's still hard for most people, and for current AI models, too. Refact is\n-multi-threaded, locks are necessary. But locks need to be activated for the shortest time possible, this\n-is how you use `Arc<AMutex<>>` to do it:\n+### Setting Up the React Frontend (GUI)\n \n-```rust\n-struct BigStruct {\n-    ...\n-    pub small_struct: Arc<AMutex<SmallStruct>>;\n-}\n+```bash\n+# In a new terminal, navigate to the GUI directory\n+cd refact-agent/gui/\n \n-fn some_code(big_struct: Arc<AMutex<BigStruct>>)\n-{\n-    let small_struct = {\n-        let big_struct_locked = big_struct.lock().await;\n-        big_struct_locked.small_struct.clone()  // cloning Arc is cheap\n-        // big_struct_locked is destroyed here because it goes out of scope\n-    };\n-    // use small_struct without holding big_struct_locked\n-}\n+# Install dependencies\n+npm ci\n+\n+# Start the development server\n+npm run dev\n ```\n \n-Another multi-threaded trick, move a member function outside of a class:\n+The frontend will connect to the Rust engine running on port 8001.\n \n-```rust\n-struct MyStruct {\n-    pub data1: i32,\n-    pub data2: i32,\n-}\n+## \ud83e\udde0 Adding Chat Models\n \n-impl MyStruct {\n-    pub fn lengthy_function1(&mut self)  {  }\n-}\n+Chat models are used for conversational AI interactions. Here's how to add them:\n+\n+### Step 1: Add to Provider Configuration\n+\n+For existing providers, edit the appropriate YAML file in `refact-agent/engine/src/yaml_configs/default_providers/`:\n+\n+```yaml\n+# Example: anthropic.yaml\n+running_models:\n+  - claude-3-7-sonnet-latest\n+  - claude-3-5-sonnet-latest\n+  - your-new-model  # Add your model here\n+\n+chat_models:\n+  your-new-model:\n+    n_ctx: 200000\n+    supports_tools: true\n+    supports_multimodality: true\n+    supports_agent: true\n+    tokenizer: hf://your-tokenizer-path\n+```\n+for more info about which config needs to be set up, you can see known_models.json\n \n-fn some_code(my_struct: Arc<AMutex<SmallStruct>>)\n-{\n-    my_struct.lock().await.lengthy_function1();\n-    // Whoops, lengthy_function has the whole structure locked for a long time,\n-    // and Rust won't not let you unlock it\n-}\n \n-pub fn lengthy_function2(s: Arc<AMutex<SmallStruct>>)\n+### Step 2: Test the Model\n+\n+Once set up, test your model in the Refact frontend:\n+\n+- Can it call tools?\n+- Does it support images (if enabled)?\n+- Do the flags behave as expected?\n+\n+This ensures everything works smoothly end-to-end.\n+\n+\n+## \u26a1 Adding Completion Models\n+\n+Completion models are used for code completion. FIM (Fill-in-the-Middle) models work best.\n+\n+### Step 1: Understand FIM Tokens\n+\n+FIM models use special tokens:\n+- `fim_prefix`: Text before the cursor\n+- `fim_suffix`: Text after the cursor  \n+- `fim_middle`: Where the completion goes\n+- `eot`: End of text token\n+\n+### Step 2: Add to Known Models\n+Add to known models (in json) or provider file (in yaml)\n+```json\n {\n-    let (data1, data2) = {\n-        let s_locked = s.lock().await;\n-        (s_locked.data1.clone(), s_locked.data2.clone())\n+  \"completion_models\": {\n+    \"your-completion-model\": {\n+      \"n_ctx\": 8192,\n+      \"scratchpad_patch\": {\n+        \"fim_prefix\": \"<|fim_prefix|>\",\n+        \"fim_suffix\": \"<|fim_suffix|>\", \n+        \"fim_middle\": \"<|fim_middle|>\",\n+        \"eot\": \"<|endoftext|>\",\n+        \"extra_stop_tokens\": [\n+          \"<|repo_name|>\",\n+          \"<|file_sep|>\"\n+        ],\n+        \"context_format\": \"your-format\",\n+        \"rag_ratio\": 0.5\n+      },\n+      \"scratchpad\": \"FIM-PSM\",\n+      \"tokenizer\": \"hf://your-tokenizer-path\",\n+      \"similar_models\": []\n     }\n-    // Do lengthy stuff here without locks!\n+  }\n }\n ```\n \n-Avoid nested locks, avoid RwLock unless you know what you are doing.\n+### Step 3: Test Code Completion\n+\n+Use the Refact IDE plugin in XDebug mode. It should connect to your local LSP server on port 8001.\n+\n+Try triggering completions in the IDE to make sure everything\u2019s working as expected.\n+\n+\n+## \ud83d\udd0c Adding New Providers\n+\n+To add a completely new OpenAI-compatible provider:\n+\n+### Step 1: Create Provider Configuration\n+\n+Create `refact-agent/engine/src/yaml_configs/default_providers/your-provider.yaml`:\n+\n+```yaml\n+chat_endpoint: https://api.your-provider.com/v1/chat/completions\n+completion_endpoint: https://api.your-provider.com/v1/completions\n+embedding_endpoint: https://api.your-provider.com/v1/embeddings\n+supports_completion: true\n+\n+api_key: your-api-key-format\n+\n+running_models:\n+  - your-model-1\n+  - your-model-2\n+\n+model_default_settings_ui:\n+  chat:\n+    n_ctx: 128000\n+    supports_tools: true\n+    supports_multimodality: false\n+    supports_agent: true\n+    tokenizer: hf://your-default-tokenizer\n+  completion:\n+    n_ctx: 8192\n+    tokenizer: hf://your-completion-tokenizer\n+```\n+\n+### Step 2: Add to Provider List\n+\n+Edit `refact-agent/engine/src/caps/providers.rs` and add your provider to the `PROVIDER_TEMPLATES` array:\n \n+```rust\n+const PROVIDER_TEMPLATES: &[(&str, &str)] = &[\n+    (\"anthropic\", include_str!(\"../yaml_configs/default_providers/anthropic.yaml\")),\n+    (\"openai\", include_str!(\"../yaml_configs/default_providers/openai.yaml\")),\n+    // ... existing providers ...\n+    (\"your-provider\", include_str!(\"../yaml_configs/default_providers/your-provider.yaml\")),\n+];\n+```\n \n-#### Testing\n+### Step 3: Test Provider Integration\n \n-It's a good idea to have tests in source files, and run them using `cargo test`, and we\n-have CI in place to run it automatically.\n-But not everything can be tested solely within Rust tests, for example a Rust test cannot run\n-an AI model inside.\n+Test should be done in UI to see if it can be set up, and if their models work after that.\n \n-So we have `tests/*.py` scripts that expect the `refact-lsp` process to be running on port 8001,\n-and the project itself as a workspace dir:\n+## \ud83e\uddea Testing Your Contributions\n \n+### Unit Tests\n \n ```bash\n-cargo build && target/debug/refact-lsp --http-port 8001 --reset-memory --experimental --workspace-folder . --logs-stderr --vecdb --ast\n+cd refact-agent/engine/\n+cargo test\n ```\n \n-Running those tests is still manual. To make sure your work didn't break other features,\n-run tests for things you might have broken.\n+### Manual Testing Checklist\n+\n+-  Model appears in capabilities endpoint (`/v1/caps`)\n+-  Chat functionality works\n+-  Code completion works (for completion models)\n+-  Tool calling works (if supported)\n+-  Multimodality works (if supported)\n+-  Error handling is graceful\n+-  Performance is acceptable\n+\n+### Using xDebug for IDE Testing\n+\n+Enable xDebug in your IDE extension settings to connect to your locally built Rust binary for testing completion models.\n+\n+## \ud83d\udccb Best Practices\n+\n+### Model Configuration\n+\n+1. **Context Windows**: Set realistic `n_ctx` values based on the model's actual capabilities\n+2. **Capabilities**: Only enable features the model actually supports\n+3. **Tokenizers**: Use the correct tokenizer for accurate token counting\n+4. **Similar Models**: Group models with similar capabilities\n+\n+### Provider Configuration\n+\n+1. **API Keys**: Use environment variables for sensitive data\n+2. **Endpoints**: Ensure URLs are correct and follow OpenAI compatibility\n+3. **Error Handling**: Test edge cases and error conditions\n+4. **Rate Limiting**: Consider provider-specific limitations\n+\n+### Code Quality\n+\n+1. **Commit messages**: Use clear, descriptive commit messages\n+\n+## \ud83d\udc1b Troubleshooting\n+\n+### Common Issues\n+\n+**Model not appearing in capabilities:**\n+- Check if it's added to `running_models` in provider config\n+- Verify the model exists in `known_models.json`\n+- Ensure provider is properly loaded\n+\n+**Tokenizer errors:**\n+- Verify tokenizer path is correct\n+- Use `fake` tokenizer for testing if needed\n+\n+**API connection issues:**\n+- Verify endpoint URLs are correct\n+- Check API key format authorization\n+- Test with curl directly first\n+\n+**Completion not working:**\n+- Ensure FIM tokens are correctly configured\n+- Check `scratchpad` type is appropriate\n+- Verify context format matches model expectations\n+\n+### Debug Commands\n+\n+```bash\n+\n+# Test specific endpoints\n+curl http://127.0.0.1:8001/v1/caps\n+curl http://127.0.0.1:8001/v1/rag-status\n+\n+# Validate configuration\n+cargo check\n+```\n+\n+## \ud83d\udca1 Examples\n+\n+### Example 1: Adding Claude 4 (Hypothetical) \n+\n+Make sure your model is listed in the config with all required fields \u2014 not just running_models, but also things like n_ctx, and any other relevant settings.\n+\n+-  **Update anthropic.yaml:**\n+```yaml\n+running_models:\n+  - claude-4\n+  - claude-3-7-sonnet-latest\n+\n+models:\n+  claude-4:\n+    n_ctx: 200000\n+    generation_params:\n+      temperature: 0.7\n+      top_p: 0.95\n+  claude-3-7-sonnet-latest:\n+    n_ctx: 100000\n+    generation_params:\n+      temperature: 0.5\n+      top_p: 0.9\n+```\n+\n+### Example 2: Adding a New FIM Model\n+\n+```json\n+\"new-coder-model\": {\n+  \"n_ctx\": 16384,\n+  \"scratchpad_patch\": {\n+    \"fim_prefix\": \"<PRE>\",\n+    \"fim_suffix\": \"<SUF>\", \n+    \"fim_middle\": \"<MID>\",\n+    \"eot\": \"<EOT>\"\n+  },\n+  \"scratchpad\": \"FIM-PSM\",\n+  \"tokenizer\": \"hf://company/new-coder-model\"\n+}\n+```\n+\n+### Example 3: Adding a Custom Provider\n+\n+```yaml\n+# custom-ai.yaml\n+chat_endpoint: https://api.custom-ai.com/v1/chat/completions\n+supports_completion: false\n+api_key: ca-...\n+\n+running_models:\n+  - custom-model-pro\n+  - custom-model-lite",
    "repo_full_name": "smallcloudai/refact",
    "discussion_comments": [
      {
        "comment_id": "2154319179",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 822,
        "pr_file": "CONTRIBUTING.md",
        "discussion_id": "2154319179",
        "commented_code": "@@ -1,124 +1,365 @@\n-# \ud83c\udf1f Contribute to Refact.ai Agent\n+## \ud83e\udd1d Contributing to Refact Agent\n+Thanks for your interest in contributing to Refact Agent! We\u2019re an open-source agent build with community \u2014 and we\u2019re excited to have you here.\n \n-Welcome to the Refact.ai project! We're excited to have you join our community. Whether you're a first-time contributor or a seasoned developer, here are some impactful ways to get involved.\n+Whether you're fixing a bug, adding a new model, improving the docs, or exploring areas like the MCP catalog \u2014 your contributions help shape the future of AI Agents.\n \n+## \ud83c\udf31 How You Can Contribute\n+There\u2019s no single path to contributing. Here are a few great starting points:\n+\n+- Try Refact out and open issues when you hit bugs or have feature ideas.\n+- Add a new model or provider \u2014 this guide includes an example of how to do that\n+- Explore and extend the MCP catalog\n+- Improve docs\n+\n+Much of the setup info in this doc applies across different areas \u2014 so feel free to contribute where your interest leads you.\n+\n+## \u2728 Got Something Else in Mind?\n+If you're excited about something that\u2019s not listed here \u2014 feel free to reach out on Discord Community (#contribution channel). We're always open to new contributions and ways to improve together.\n \n ## \ud83d\udcda Table of Contents\n-- [\u2764\ufe0f Ways to Contribute](#%EF%B8%8F-ways-to-contribute)\n-  - [\ud83d\udc1b Report Bugs](#-report-bugs)\n-  - [Contributing To Code](#contributing-to-code)\n-    - [ What Can I Do](#what-can-i-do)\n-    - [Coding Standards](#coding-standards)\n-    - [Testing](#testing)\n-    - [Contact](#contact)\n \n+- [\ud83d\ude80 Quick Start](#-quick-start)\n+- [\ud83d\udee0\ufe0f Development Environment Setup](#\ufe0f-development-environment-setup)\n+- [\ud83e\udde0 Adding Chat Models](#-adding-chat-models)\n+- [\u26a1 Adding Completion Models](#-adding-completion-models)\n+- [\ud83d\udd0c Adding New Providers](#-adding-new-providers)\n+- [\ud83e\uddea Testing Your Contributions](#-testing-your-contributions)\n+- [\ud83d\udccb Best Practices](#-best-practices)\n+- [\ud83d\udc1b Troubleshooting](#-troubleshooting)\n+- [\ud83d\udca1 Examples](#-examples)\n \n-### \u2764\ufe0f Ways to Contribute\n+## \ud83d\ude80 Quick Start\n \n-* Fork the repository\n-* Create a feature branch\n-* Do the work\n-* Create a pull request\n-* Maintainers will review it\n+Before diving deep, here's what you need to know:\n \n-### \ud83d\udc1b Report Bugs\n-Encountered an issue? Help us improve Refact.ai by reporting bugs in issue section, make sure you label the issue with correct tag [here](https://github.com/smallcloudai/refact-lsp/issues)! \n+1. **Chat Models** are for conversational AI (like GPT-4, Claude)\n+2. **Completion Models** are for code completion (preferably FIM models) like qwen-2.5-coder-base, starcoder2 and deepseek-coder\n+3. **Providers** are services that host these models (OpenAI, Anthropic, etc.)\n \n-### \ud83d\udcd6 Improving Documentation\n-Help us make Refact.ai more accessible by contributing to our documentation, make sure you label the issue with correct tag! Create issues [here](https://github.com/smallcloudai/web_docs_refact_ai/issues).\n+## \ud83d\udee0\ufe0f Development Environment Setup\n \n-### Contributing To Code\n+### Prerequisites\n \n-#### What Can I Do?\n+- **Rust** (latest stable version)\n+- **Node.js** and **npm** (for React frontend)\n+- **Chrome/Chromium** (dev dependency)\n+- **Git**\n \n-Before you start, create an issue with a title that begins with `[idea] ...`. The field of AI and agents is vast,\n-and not every idea will benefit the project, even if it is a good idea in itself.\n+### Setting Up the Rust Backend (Engine)\n \n-Another rule of thumb: Only implement a feature you can test thoroughly.\n+```bash\n+# Clone the repository\n+git clone https://github.com/smallcloudai/refact.git\n+cd refact\n \n+# Install Rust if you haven't already\n+curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n+source ~/.cargo/env\n \n-#### Coding Standards\n+# Navigate to the engine directory\n+cd refact-agent/engine/\n \n-Good practices for Rust are generally applicable to this project. There are a few points however:\n+# Build the project\n+cargo build\n \n-1. Naming. Use \"Find in files...\" to check if a name you give to your structs, fields, functions is too\n-generic. If a name is already all over the project, be more specific. For example \"IntegrationGitHub\" is a good\n-name, but \"Integration\" is not, even if it's in `github.rs` and files work as namespaces in Rust. It's\n-still hard to navigate the project if you can't use search.\n+# Run the engine with your API key\n+cargo run -- --address-url Refact --api-key <YOUR_CLOUD_API_KEY> --http-port 8001 --lsp-port 8002 --logs-stderr --vecdb --ast --workspace-folder .\n+```\n \n-2. Locks. For some reason, it's still hard for most people, and for current AI models, too. Refact is\n-multi-threaded, locks are necessary. But locks need to be activated for the shortest time possible, this\n-is how you use `Arc<AMutex<>>` to do it:\n+### Setting Up the React Frontend (GUI)\n \n-```rust\n-struct BigStruct {\n-    ...\n-    pub small_struct: Arc<AMutex<SmallStruct>>;\n-}\n+```bash\n+# In a new terminal, navigate to the GUI directory\n+cd refact-agent/gui/\n \n-fn some_code(big_struct: Arc<AMutex<BigStruct>>)\n-{\n-    let small_struct = {\n-        let big_struct_locked = big_struct.lock().await;\n-        big_struct_locked.small_struct.clone()  // cloning Arc is cheap\n-        // big_struct_locked is destroyed here because it goes out of scope\n-    };\n-    // use small_struct without holding big_struct_locked\n-}\n+# Install dependencies\n+npm ci\n+\n+# Start the development server\n+npm run dev\n ```\n \n-Another multi-threaded trick, move a member function outside of a class:\n+The frontend will connect to the Rust engine running on port 8001.\n \n-```rust\n-struct MyStruct {\n-    pub data1: i32,\n-    pub data2: i32,\n-}\n+## \ud83e\udde0 Adding Chat Models\n \n-impl MyStruct {\n-    pub fn lengthy_function1(&mut self)  {  }\n-}\n+Chat models are used for conversational AI interactions. Here's how to add them:\n+\n+### Step 1: Add to Provider Configuration\n+\n+For existing providers, edit the appropriate YAML file in `refact-agent/engine/src/yaml_configs/default_providers/`:\n+\n+```yaml\n+# Example: anthropic.yaml\n+running_models:\n+  - claude-3-7-sonnet-latest\n+  - claude-3-5-sonnet-latest\n+  - your-new-model  # Add your model here\n+\n+chat_models:\n+  your-new-model:\n+    n_ctx: 200000\n+    supports_tools: true\n+    supports_multimodality: true\n+    supports_agent: true\n+    tokenizer: hf://your-tokenizer-path\n+```\n+for more info about which config needs to be set up, you can see known_models.json\n \n-fn some_code(my_struct: Arc<AMutex<SmallStruct>>)\n-{\n-    my_struct.lock().await.lengthy_function1();\n-    // Whoops, lengthy_function has the whole structure locked for a long time,\n-    // and Rust won't not let you unlock it\n-}\n \n-pub fn lengthy_function2(s: Arc<AMutex<SmallStruct>>)\n+### Step 2: Test the Model\n+\n+Once set up, test your model in the Refact frontend:\n+\n+- Can it call tools?\n+- Does it support images (if enabled)?\n+- Do the flags behave as expected?\n+\n+This ensures everything works smoothly end-to-end.\n+\n+\n+## \u26a1 Adding Completion Models\n+\n+Completion models are used for code completion. FIM (Fill-in-the-Middle) models work best.\n+\n+### Step 1: Understand FIM Tokens\n+\n+FIM models use special tokens:\n+- `fim_prefix`: Text before the cursor\n+- `fim_suffix`: Text after the cursor  \n+- `fim_middle`: Where the completion goes\n+- `eot`: End of text token\n+\n+### Step 2: Add to Known Models\n+Add to known models (in json) or provider file (in yaml)\n+```json\n {\n-    let (data1, data2) = {\n-        let s_locked = s.lock().await;\n-        (s_locked.data1.clone(), s_locked.data2.clone())\n+  \"completion_models\": {\n+    \"your-completion-model\": {\n+      \"n_ctx\": 8192,\n+      \"scratchpad_patch\": {\n+        \"fim_prefix\": \"<|fim_prefix|>\",\n+        \"fim_suffix\": \"<|fim_suffix|>\", \n+        \"fim_middle\": \"<|fim_middle|>\",\n+        \"eot\": \"<|endoftext|>\",\n+        \"extra_stop_tokens\": [\n+          \"<|repo_name|>\",\n+          \"<|file_sep|>\"\n+        ],\n+        \"context_format\": \"your-format\",\n+        \"rag_ratio\": 0.5\n+      },\n+      \"scratchpad\": \"FIM-PSM\",\n+      \"tokenizer\": \"hf://your-tokenizer-path\",\n+      \"similar_models\": []\n     }\n-    // Do lengthy stuff here without locks!\n+  }\n }\n ```\n \n-Avoid nested locks, avoid RwLock unless you know what you are doing.\n+### Step 3: Test Code Completion\n+\n+Use the Refact IDE plugin in XDebug mode. It should connect to your local LSP server on port 8001.\n+\n+Try triggering completions in the IDE to make sure everything\u2019s working as expected.\n+\n+\n+## \ud83d\udd0c Adding New Providers\n+\n+To add a completely new OpenAI-compatible provider:\n+\n+### Step 1: Create Provider Configuration\n+\n+Create `refact-agent/engine/src/yaml_configs/default_providers/your-provider.yaml`:\n+\n+```yaml\n+chat_endpoint: https://api.your-provider.com/v1/chat/completions\n+completion_endpoint: https://api.your-provider.com/v1/completions\n+embedding_endpoint: https://api.your-provider.com/v1/embeddings\n+supports_completion: true\n+\n+api_key: your-api-key-format\n+\n+running_models:\n+  - your-model-1\n+  - your-model-2\n+\n+model_default_settings_ui:\n+  chat:\n+    n_ctx: 128000\n+    supports_tools: true\n+    supports_multimodality: false\n+    supports_agent: true\n+    tokenizer: hf://your-default-tokenizer\n+  completion:\n+    n_ctx: 8192\n+    tokenizer: hf://your-completion-tokenizer\n+```\n+\n+### Step 2: Add to Provider List\n+\n+Edit `refact-agent/engine/src/caps/providers.rs` and add your provider to the `PROVIDER_TEMPLATES` array:\n \n+```rust\n+const PROVIDER_TEMPLATES: &[(&str, &str)] = &[\n+    (\"anthropic\", include_str!(\"../yaml_configs/default_providers/anthropic.yaml\")),\n+    (\"openai\", include_str!(\"../yaml_configs/default_providers/openai.yaml\")),\n+    // ... existing providers ...\n+    (\"your-provider\", include_str!(\"../yaml_configs/default_providers/your-provider.yaml\")),\n+];\n+```\n \n-#### Testing\n+### Step 3: Test Provider Integration\n \n-It's a good idea to have tests in source files, and run them using `cargo test`, and we\n-have CI in place to run it automatically.\n-But not everything can be tested solely within Rust tests, for example a Rust test cannot run\n-an AI model inside.\n+Test should be done in UI to see if it can be set up, and if their models work after that.\n \n-So we have `tests/*.py` scripts that expect the `refact-lsp` process to be running on port 8001,\n-and the project itself as a workspace dir:\n+## \ud83e\uddea Testing Your Contributions\n \n+### Unit Tests\n \n ```bash\n-cargo build && target/debug/refact-lsp --http-port 8001 --reset-memory --experimental --workspace-folder . --logs-stderr --vecdb --ast\n+cd refact-agent/engine/\n+cargo test\n ```\n \n-Running those tests is still manual. To make sure your work didn't break other features,\n-run tests for things you might have broken.\n+### Manual Testing Checklist\n+\n+-  Model appears in capabilities endpoint (`/v1/caps`)\n+-  Chat functionality works\n+-  Code completion works (for completion models)\n+-  Tool calling works (if supported)\n+-  Multimodality works (if supported)\n+-  Error handling is graceful\n+-  Performance is acceptable\n+\n+### Using xDebug for IDE Testing\n+\n+Enable xDebug in your IDE extension settings to connect to your locally built Rust binary for testing completion models.\n+\n+## \ud83d\udccb Best Practices\n+\n+### Model Configuration\n+\n+1. **Context Windows**: Set realistic `n_ctx` values based on the model's actual capabilities\n+2. **Capabilities**: Only enable features the model actually supports\n+3. **Tokenizers**: Use the correct tokenizer for accurate token counting\n+4. **Similar Models**: Group models with similar capabilities\n+\n+### Provider Configuration\n+\n+1. **API Keys**: Use environment variables for sensitive data\n+2. **Endpoints**: Ensure URLs are correct and follow OpenAI compatibility\n+3. **Error Handling**: Test edge cases and error conditions\n+4. **Rate Limiting**: Consider provider-specific limitations\n+\n+### Code Quality\n+\n+1. **Commit messages**: Use clear, descriptive commit messages\n+\n+## \ud83d\udc1b Troubleshooting\n+\n+### Common Issues\n+\n+**Model not appearing in capabilities:**\n+- Check if it's added to `running_models` in provider config\n+- Verify the model exists in `known_models.json`\n+- Ensure provider is properly loaded\n+\n+**Tokenizer errors:**\n+- Verify tokenizer path is correct\n+- Use `fake` tokenizer for testing if needed\n+\n+**API connection issues:**\n+- Verify endpoint URLs are correct\n+- Check API key format authorization\n+- Test with curl directly first\n+\n+**Completion not working:**\n+- Ensure FIM tokens are correctly configured\n+- Check `scratchpad` type is appropriate\n+- Verify context format matches model expectations\n+\n+### Debug Commands\n+\n+```bash\n+\n+# Test specific endpoints\n+curl http://127.0.0.1:8001/v1/caps\n+curl http://127.0.0.1:8001/v1/rag-status\n+\n+# Validate configuration\n+cargo check\n+```\n+\n+## \ud83d\udca1 Examples\n+\n+### Example 1: Adding Claude 4 (Hypothetical) \n+\n+Make sure your model is listed in the config with all required fields \u2014 not just running_models, but also things like n_ctx, and any other relevant settings.\n+\n+-  **Update anthropic.yaml:**\n+```yaml\n+running_models:\n+  - claude-4\n+  - claude-3-7-sonnet-latest\n+\n+models:\n+  claude-4:\n+    n_ctx: 200000\n+    generation_params:\n+      temperature: 0.7\n+      top_p: 0.95\n+  claude-3-7-sonnet-latest:\n+    n_ctx: 100000\n+    generation_params:\n+      temperature: 0.5\n+      top_p: 0.9\n+```\n+\n+### Example 2: Adding a New FIM Model\n+\n+```json\n+\"new-coder-model\": {\n+  \"n_ctx\": 16384,\n+  \"scratchpad_patch\": {\n+    \"fim_prefix\": \"<PRE>\",\n+    \"fim_suffix\": \"<SUF>\", \n+    \"fim_middle\": \"<MID>\",\n+    \"eot\": \"<EOT>\"\n+  },\n+  \"scratchpad\": \"FIM-PSM\",\n+  \"tokenizer\": \"hf://company/new-coder-model\"\n+}\n+```\n+\n+### Example 3: Adding a Custom Provider\n+\n+```yaml\n+# custom-ai.yaml\n+chat_endpoint: https://api.custom-ai.com/v1/chat/completions\n+supports_completion: false\n+api_key: ca-...\n+\n+running_models:\n+  - custom-model-pro\n+  - custom-model-lite",
        "comment_created_at": "2025-06-18T11:05:23+00:00",
        "comment_author": "humbertoyusta",
        "comment_body": "here apart from these, the models should be configured, example, like claude-4",
        "pr_file_module": null
      },
      {
        "comment_id": "2154576221",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 822,
        "pr_file": "CONTRIBUTING.md",
        "discussion_id": "2154319179",
        "commented_code": "@@ -1,124 +1,365 @@\n-# \ud83c\udf1f Contribute to Refact.ai Agent\n+## \ud83e\udd1d Contributing to Refact Agent\n+Thanks for your interest in contributing to Refact Agent! We\u2019re an open-source agent build with community \u2014 and we\u2019re excited to have you here.\n \n-Welcome to the Refact.ai project! We're excited to have you join our community. Whether you're a first-time contributor or a seasoned developer, here are some impactful ways to get involved.\n+Whether you're fixing a bug, adding a new model, improving the docs, or exploring areas like the MCP catalog \u2014 your contributions help shape the future of AI Agents.\n \n+## \ud83c\udf31 How You Can Contribute\n+There\u2019s no single path to contributing. Here are a few great starting points:\n+\n+- Try Refact out and open issues when you hit bugs or have feature ideas.\n+- Add a new model or provider \u2014 this guide includes an example of how to do that\n+- Explore and extend the MCP catalog\n+- Improve docs\n+\n+Much of the setup info in this doc applies across different areas \u2014 so feel free to contribute where your interest leads you.\n+\n+## \u2728 Got Something Else in Mind?\n+If you're excited about something that\u2019s not listed here \u2014 feel free to reach out on Discord Community (#contribution channel). We're always open to new contributions and ways to improve together.\n \n ## \ud83d\udcda Table of Contents\n-- [\u2764\ufe0f Ways to Contribute](#%EF%B8%8F-ways-to-contribute)\n-  - [\ud83d\udc1b Report Bugs](#-report-bugs)\n-  - [Contributing To Code](#contributing-to-code)\n-    - [ What Can I Do](#what-can-i-do)\n-    - [Coding Standards](#coding-standards)\n-    - [Testing](#testing)\n-    - [Contact](#contact)\n \n+- [\ud83d\ude80 Quick Start](#-quick-start)\n+- [\ud83d\udee0\ufe0f Development Environment Setup](#\ufe0f-development-environment-setup)\n+- [\ud83e\udde0 Adding Chat Models](#-adding-chat-models)\n+- [\u26a1 Adding Completion Models](#-adding-completion-models)\n+- [\ud83d\udd0c Adding New Providers](#-adding-new-providers)\n+- [\ud83e\uddea Testing Your Contributions](#-testing-your-contributions)\n+- [\ud83d\udccb Best Practices](#-best-practices)\n+- [\ud83d\udc1b Troubleshooting](#-troubleshooting)\n+- [\ud83d\udca1 Examples](#-examples)\n \n-### \u2764\ufe0f Ways to Contribute\n+## \ud83d\ude80 Quick Start\n \n-* Fork the repository\n-* Create a feature branch\n-* Do the work\n-* Create a pull request\n-* Maintainers will review it\n+Before diving deep, here's what you need to know:\n \n-### \ud83d\udc1b Report Bugs\n-Encountered an issue? Help us improve Refact.ai by reporting bugs in issue section, make sure you label the issue with correct tag [here](https://github.com/smallcloudai/refact-lsp/issues)! \n+1. **Chat Models** are for conversational AI (like GPT-4, Claude)\n+2. **Completion Models** are for code completion (preferably FIM models) like qwen-2.5-coder-base, starcoder2 and deepseek-coder\n+3. **Providers** are services that host these models (OpenAI, Anthropic, etc.)\n \n-### \ud83d\udcd6 Improving Documentation\n-Help us make Refact.ai more accessible by contributing to our documentation, make sure you label the issue with correct tag! Create issues [here](https://github.com/smallcloudai/web_docs_refact_ai/issues).\n+## \ud83d\udee0\ufe0f Development Environment Setup\n \n-### Contributing To Code\n+### Prerequisites\n \n-#### What Can I Do?\n+- **Rust** (latest stable version)\n+- **Node.js** and **npm** (for React frontend)\n+- **Chrome/Chromium** (dev dependency)\n+- **Git**\n \n-Before you start, create an issue with a title that begins with `[idea] ...`. The field of AI and agents is vast,\n-and not every idea will benefit the project, even if it is a good idea in itself.\n+### Setting Up the Rust Backend (Engine)\n \n-Another rule of thumb: Only implement a feature you can test thoroughly.\n+```bash\n+# Clone the repository\n+git clone https://github.com/smallcloudai/refact.git\n+cd refact\n \n+# Install Rust if you haven't already\n+curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n+source ~/.cargo/env\n \n-#### Coding Standards\n+# Navigate to the engine directory\n+cd refact-agent/engine/\n \n-Good practices for Rust are generally applicable to this project. There are a few points however:\n+# Build the project\n+cargo build\n \n-1. Naming. Use \"Find in files...\" to check if a name you give to your structs, fields, functions is too\n-generic. If a name is already all over the project, be more specific. For example \"IntegrationGitHub\" is a good\n-name, but \"Integration\" is not, even if it's in `github.rs` and files work as namespaces in Rust. It's\n-still hard to navigate the project if you can't use search.\n+# Run the engine with your API key\n+cargo run -- --address-url Refact --api-key <YOUR_CLOUD_API_KEY> --http-port 8001 --lsp-port 8002 --logs-stderr --vecdb --ast --workspace-folder .\n+```\n \n-2. Locks. For some reason, it's still hard for most people, and for current AI models, too. Refact is\n-multi-threaded, locks are necessary. But locks need to be activated for the shortest time possible, this\n-is how you use `Arc<AMutex<>>` to do it:\n+### Setting Up the React Frontend (GUI)\n \n-```rust\n-struct BigStruct {\n-    ...\n-    pub small_struct: Arc<AMutex<SmallStruct>>;\n-}\n+```bash\n+# In a new terminal, navigate to the GUI directory\n+cd refact-agent/gui/\n \n-fn some_code(big_struct: Arc<AMutex<BigStruct>>)\n-{\n-    let small_struct = {\n-        let big_struct_locked = big_struct.lock().await;\n-        big_struct_locked.small_struct.clone()  // cloning Arc is cheap\n-        // big_struct_locked is destroyed here because it goes out of scope\n-    };\n-    // use small_struct without holding big_struct_locked\n-}\n+# Install dependencies\n+npm ci\n+\n+# Start the development server\n+npm run dev\n ```\n \n-Another multi-threaded trick, move a member function outside of a class:\n+The frontend will connect to the Rust engine running on port 8001.\n \n-```rust\n-struct MyStruct {\n-    pub data1: i32,\n-    pub data2: i32,\n-}\n+## \ud83e\udde0 Adding Chat Models\n \n-impl MyStruct {\n-    pub fn lengthy_function1(&mut self)  {  }\n-}\n+Chat models are used for conversational AI interactions. Here's how to add them:\n+\n+### Step 1: Add to Provider Configuration\n+\n+For existing providers, edit the appropriate YAML file in `refact-agent/engine/src/yaml_configs/default_providers/`:\n+\n+```yaml\n+# Example: anthropic.yaml\n+running_models:\n+  - claude-3-7-sonnet-latest\n+  - claude-3-5-sonnet-latest\n+  - your-new-model  # Add your model here\n+\n+chat_models:\n+  your-new-model:\n+    n_ctx: 200000\n+    supports_tools: true\n+    supports_multimodality: true\n+    supports_agent: true\n+    tokenizer: hf://your-tokenizer-path\n+```\n+for more info about which config needs to be set up, you can see known_models.json\n \n-fn some_code(my_struct: Arc<AMutex<SmallStruct>>)\n-{\n-    my_struct.lock().await.lengthy_function1();\n-    // Whoops, lengthy_function has the whole structure locked for a long time,\n-    // and Rust won't not let you unlock it\n-}\n \n-pub fn lengthy_function2(s: Arc<AMutex<SmallStruct>>)\n+### Step 2: Test the Model\n+\n+Once set up, test your model in the Refact frontend:\n+\n+- Can it call tools?\n+- Does it support images (if enabled)?\n+- Do the flags behave as expected?\n+\n+This ensures everything works smoothly end-to-end.\n+\n+\n+## \u26a1 Adding Completion Models\n+\n+Completion models are used for code completion. FIM (Fill-in-the-Middle) models work best.\n+\n+### Step 1: Understand FIM Tokens\n+\n+FIM models use special tokens:\n+- `fim_prefix`: Text before the cursor\n+- `fim_suffix`: Text after the cursor  \n+- `fim_middle`: Where the completion goes\n+- `eot`: End of text token\n+\n+### Step 2: Add to Known Models\n+Add to known models (in json) or provider file (in yaml)\n+```json\n {\n-    let (data1, data2) = {\n-        let s_locked = s.lock().await;\n-        (s_locked.data1.clone(), s_locked.data2.clone())\n+  \"completion_models\": {\n+    \"your-completion-model\": {\n+      \"n_ctx\": 8192,\n+      \"scratchpad_patch\": {\n+        \"fim_prefix\": \"<|fim_prefix|>\",\n+        \"fim_suffix\": \"<|fim_suffix|>\", \n+        \"fim_middle\": \"<|fim_middle|>\",\n+        \"eot\": \"<|endoftext|>\",\n+        \"extra_stop_tokens\": [\n+          \"<|repo_name|>\",\n+          \"<|file_sep|>\"\n+        ],\n+        \"context_format\": \"your-format\",\n+        \"rag_ratio\": 0.5\n+      },\n+      \"scratchpad\": \"FIM-PSM\",\n+      \"tokenizer\": \"hf://your-tokenizer-path\",\n+      \"similar_models\": []\n     }\n-    // Do lengthy stuff here without locks!\n+  }\n }\n ```\n \n-Avoid nested locks, avoid RwLock unless you know what you are doing.\n+### Step 3: Test Code Completion\n+\n+Use the Refact IDE plugin in XDebug mode. It should connect to your local LSP server on port 8001.\n+\n+Try triggering completions in the IDE to make sure everything\u2019s working as expected.\n+\n+\n+## \ud83d\udd0c Adding New Providers\n+\n+To add a completely new OpenAI-compatible provider:\n+\n+### Step 1: Create Provider Configuration\n+\n+Create `refact-agent/engine/src/yaml_configs/default_providers/your-provider.yaml`:\n+\n+```yaml\n+chat_endpoint: https://api.your-provider.com/v1/chat/completions\n+completion_endpoint: https://api.your-provider.com/v1/completions\n+embedding_endpoint: https://api.your-provider.com/v1/embeddings\n+supports_completion: true\n+\n+api_key: your-api-key-format\n+\n+running_models:\n+  - your-model-1\n+  - your-model-2\n+\n+model_default_settings_ui:\n+  chat:\n+    n_ctx: 128000\n+    supports_tools: true\n+    supports_multimodality: false\n+    supports_agent: true\n+    tokenizer: hf://your-default-tokenizer\n+  completion:\n+    n_ctx: 8192\n+    tokenizer: hf://your-completion-tokenizer\n+```\n+\n+### Step 2: Add to Provider List\n+\n+Edit `refact-agent/engine/src/caps/providers.rs` and add your provider to the `PROVIDER_TEMPLATES` array:\n \n+```rust\n+const PROVIDER_TEMPLATES: &[(&str, &str)] = &[\n+    (\"anthropic\", include_str!(\"../yaml_configs/default_providers/anthropic.yaml\")),\n+    (\"openai\", include_str!(\"../yaml_configs/default_providers/openai.yaml\")),\n+    // ... existing providers ...\n+    (\"your-provider\", include_str!(\"../yaml_configs/default_providers/your-provider.yaml\")),\n+];\n+```\n \n-#### Testing\n+### Step 3: Test Provider Integration\n \n-It's a good idea to have tests in source files, and run them using `cargo test`, and we\n-have CI in place to run it automatically.\n-But not everything can be tested solely within Rust tests, for example a Rust test cannot run\n-an AI model inside.\n+Test should be done in UI to see if it can be set up, and if their models work after that.\n \n-So we have `tests/*.py` scripts that expect the `refact-lsp` process to be running on port 8001,\n-and the project itself as a workspace dir:\n+## \ud83e\uddea Testing Your Contributions\n \n+### Unit Tests\n \n ```bash\n-cargo build && target/debug/refact-lsp --http-port 8001 --reset-memory --experimental --workspace-folder . --logs-stderr --vecdb --ast\n+cd refact-agent/engine/\n+cargo test\n ```\n \n-Running those tests is still manual. To make sure your work didn't break other features,\n-run tests for things you might have broken.\n+### Manual Testing Checklist\n+\n+-  Model appears in capabilities endpoint (`/v1/caps`)\n+-  Chat functionality works\n+-  Code completion works (for completion models)\n+-  Tool calling works (if supported)\n+-  Multimodality works (if supported)\n+-  Error handling is graceful\n+-  Performance is acceptable\n+\n+### Using xDebug for IDE Testing\n+\n+Enable xDebug in your IDE extension settings to connect to your locally built Rust binary for testing completion models.\n+\n+## \ud83d\udccb Best Practices\n+\n+### Model Configuration\n+\n+1. **Context Windows**: Set realistic `n_ctx` values based on the model's actual capabilities\n+2. **Capabilities**: Only enable features the model actually supports\n+3. **Tokenizers**: Use the correct tokenizer for accurate token counting\n+4. **Similar Models**: Group models with similar capabilities\n+\n+### Provider Configuration\n+\n+1. **API Keys**: Use environment variables for sensitive data\n+2. **Endpoints**: Ensure URLs are correct and follow OpenAI compatibility\n+3. **Error Handling**: Test edge cases and error conditions\n+4. **Rate Limiting**: Consider provider-specific limitations\n+\n+### Code Quality\n+\n+1. **Commit messages**: Use clear, descriptive commit messages\n+\n+## \ud83d\udc1b Troubleshooting\n+\n+### Common Issues\n+\n+**Model not appearing in capabilities:**\n+- Check if it's added to `running_models` in provider config\n+- Verify the model exists in `known_models.json`\n+- Ensure provider is properly loaded\n+\n+**Tokenizer errors:**\n+- Verify tokenizer path is correct\n+- Use `fake` tokenizer for testing if needed\n+\n+**API connection issues:**\n+- Verify endpoint URLs are correct\n+- Check API key format authorization\n+- Test with curl directly first\n+\n+**Completion not working:**\n+- Ensure FIM tokens are correctly configured\n+- Check `scratchpad` type is appropriate\n+- Verify context format matches model expectations\n+\n+### Debug Commands\n+\n+```bash\n+\n+# Test specific endpoints\n+curl http://127.0.0.1:8001/v1/caps\n+curl http://127.0.0.1:8001/v1/rag-status\n+\n+# Validate configuration\n+cargo check\n+```\n+\n+## \ud83d\udca1 Examples\n+\n+### Example 1: Adding Claude 4 (Hypothetical) \n+\n+Make sure your model is listed in the config with all required fields \u2014 not just running_models, but also things like n_ctx, and any other relevant settings.\n+\n+-  **Update anthropic.yaml:**\n+```yaml\n+running_models:\n+  - claude-4\n+  - claude-3-7-sonnet-latest\n+\n+models:\n+  claude-4:\n+    n_ctx: 200000\n+    generation_params:\n+      temperature: 0.7\n+      top_p: 0.95\n+  claude-3-7-sonnet-latest:\n+    n_ctx: 100000\n+    generation_params:\n+      temperature: 0.5\n+      top_p: 0.9\n+```\n+\n+### Example 2: Adding a New FIM Model\n+\n+```json\n+\"new-coder-model\": {\n+  \"n_ctx\": 16384,\n+  \"scratchpad_patch\": {\n+    \"fim_prefix\": \"<PRE>\",\n+    \"fim_suffix\": \"<SUF>\", \n+    \"fim_middle\": \"<MID>\",\n+    \"eot\": \"<EOT>\"\n+  },\n+  \"scratchpad\": \"FIM-PSM\",\n+  \"tokenizer\": \"hf://company/new-coder-model\"\n+}\n+```\n+\n+### Example 3: Adding a Custom Provider\n+\n+```yaml\n+# custom-ai.yaml\n+chat_endpoint: https://api.custom-ai.com/v1/chat/completions\n+supports_completion: false\n+api_key: ca-...\n+\n+running_models:\n+  - custom-model-pro\n+  - custom-model-lite",
        "comment_created_at": "2025-06-18T13:14:23+00:00",
        "comment_author": "humbertoyusta",
        "comment_body": "No no, don't add it to default settings ui, example should look like\r\n\r\n\r\n```\r\nchat_endpoint: https://api.anthropic.com/v1/chat/completions\r\nsupports_completion: false\r\n\r\napi_key: sk-ant-...\r\n\r\nchat_models:\r\n  claude-3-7-sonnet-latest:\r\n    n_ctx: 200000\r\n    supports_tools: true\r\n    supports_multimodality: true\r\n    supports_clicks: true\r\n    supports_agent: true\r\n    supports_reasoning: anthropic\r\n    tokenizer: hf://Xenova/claude-tokenizer\r\n\r\nrunning_models:\r\n  - claude-3-7-sonnet-latest\r\n\r\nmodel_default_settings_ui:\r\n  chat:\r\n    n_ctx: 200000\r\n    supports_tools: true\r\n    supports_multimodality: true\r\n    supports_agent: true\r\n    tokenizer: hf://Xenova/claude-tokenizer\r\n```",
        "pr_file_module": null
      }
    ]
  }
]