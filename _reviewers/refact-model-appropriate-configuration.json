[
  {
    "discussion_id": "1518535193",
    "pr_number": 331,
    "pr_file": "self_hosting_machinery/finetune/configuration/supported_models.py",
    "created_at": "2024-03-09T08:25:41+00:00",
    "commented_code": "\"train_model_modifiers\": [\n         \"flash_sa.apply_flash_mha_to_starcoder2_model\"\n     ],\n-    \"force_enable_checkpointing\": False\n+    \"force_enable_checkpointing\": True",
    "repo_full_name": "smallcloudai/refact",
    "discussion_comments": [
      {
        "comment_id": "1518535193",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 331,
        "pr_file": "self_hosting_machinery/finetune/configuration/supported_models.py",
        "discussion_id": "1518535193",
        "commented_code": "@@ -58,7 +58,7 @@\n     \"train_model_modifiers\": [\n         \"flash_sa.apply_flash_mha_to_starcoder2_model\"\n     ],\n-    \"force_enable_checkpointing\": False\n+    \"force_enable_checkpointing\": True",
        "comment_created_at": "2024-03-09T08:25:41+00:00",
        "comment_author": "JegernOUTT",
        "comment_body": "otherwise starcoder2 models are too large while being in training",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1366470443",
    "pr_number": 194,
    "pr_file": "refact_data_pipeline/finetune/supported_models.py",
    "created_at": "2023-10-20T05:03:22+00:00",
    "commented_code": "},\n         \"train_model_modifiers\": [],\n         \"force_enable_checkpointing\": True\n+    },\n+\n+    \"codellama/7b\": {\n+        \"lora_target_modules_mapping\": {\n+            \"qkv\": [\"attn.q_attn\", \"attn.c_attn\"],\n+            \"out\": [\"attn.c_proj\"],\n+            \"backproj\": [\"attn.c_proj\"],\n+            \"mlp\": [\"mlp.c_fc\", \"mlp.c_proj\"],\n+        },\n+        \"freeze_exceptions_mapping\": {\n+            \"wte\": \"wte\",\n+            \"lm_head\": \"lm_head\",\n+            \"lora\": \"lora\"\n+        },\n+        \"tokenizer\": {\n+            \"eot_idx\": 0,",
    "repo_full_name": "smallcloudai/refact",
    "discussion_comments": [
      {
        "comment_id": "1366470443",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 194,
        "pr_file": "refact_data_pipeline/finetune/supported_models.py",
        "discussion_id": "1366470443",
        "commented_code": "@@ -151,5 +151,43 @@\n         },\n         \"train_model_modifiers\": [],\n         \"force_enable_checkpointing\": True\n+    },\n+\n+    \"codellama/7b\": {\n+        \"lora_target_modules_mapping\": {\n+            \"qkv\": [\"attn.q_attn\", \"attn.c_attn\"],\n+            \"out\": [\"attn.c_proj\"],\n+            \"backproj\": [\"attn.c_proj\"],\n+            \"mlp\": [\"mlp.c_fc\", \"mlp.c_proj\"],\n+        },\n+        \"freeze_exceptions_mapping\": {\n+            \"wte\": \"wte\",\n+            \"lm_head\": \"lm_head\",\n+            \"lora\": \"lora\"\n+        },\n+        \"tokenizer\": {\n+            \"eot_idx\": 0,",
        "comment_created_at": "2023-10-20T05:03:22+00:00",
        "comment_author": "JegernOUTT",
        "comment_body": "I think there is a problem since those indexes are for `bigcode` encoding, and you are using a llama encoding\r\n\r\nThey also use slightly different encoding format (with `bos` token) and maybe the `FIM` format is also different",
        "pr_file_module": null
      },
      {
        "comment_id": "1371774701",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 194,
        "pr_file": "refact_data_pipeline/finetune/supported_models.py",
        "discussion_id": "1366470443",
        "commented_code": "@@ -151,5 +151,43 @@\n         },\n         \"train_model_modifiers\": [],\n         \"force_enable_checkpointing\": True\n+    },\n+\n+    \"codellama/7b\": {\n+        \"lora_target_modules_mapping\": {\n+            \"qkv\": [\"attn.q_attn\", \"attn.c_attn\"],\n+            \"out\": [\"attn.c_proj\"],\n+            \"backproj\": [\"attn.c_proj\"],\n+            \"mlp\": [\"mlp.c_fc\", \"mlp.c_proj\"],\n+        },\n+        \"freeze_exceptions_mapping\": {\n+            \"wte\": \"wte\",\n+            \"lm_head\": \"lm_head\",\n+            \"lora\": \"lora\"\n+        },\n+        \"tokenizer\": {\n+            \"eot_idx\": 0,",
        "comment_created_at": "2023-10-25T13:34:57+00:00",
        "comment_author": "adam-weinberger",
        "comment_body": "Sorry, just saw this. I understand what you are saying, but I'm a little unclear on how to find the correct encoding. This is the [Code Llama tokenizer](https://huggingface.co/docs/transformers/main/model_doc/code_llama#transformers.CodeLlamaTokenizer). Should I create a new `code_llama.json` In the refact_encoding folder with these tokens? Where did the `bigcode_largemodel.json` file come from? Where did the vocab part of the file come from? When I google the entire file for Code Llama the best result is [this](https://huggingface.co/MarioPenguin/beto_amazon/commit/83bd1c1ba9422bc57e90b92f93b4bd112afea8be.diff?file=tokenizer.json) which is for BERT.",
        "pr_file_module": null
      },
      {
        "comment_id": "1371805506",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 194,
        "pr_file": "refact_data_pipeline/finetune/supported_models.py",
        "discussion_id": "1366470443",
        "commented_code": "@@ -151,5 +151,43 @@\n         },\n         \"train_model_modifiers\": [],\n         \"force_enable_checkpointing\": True\n+    },\n+\n+    \"codellama/7b\": {\n+        \"lora_target_modules_mapping\": {\n+            \"qkv\": [\"attn.q_attn\", \"attn.c_attn\"],\n+            \"out\": [\"attn.c_proj\"],\n+            \"backproj\": [\"attn.c_proj\"],\n+            \"mlp\": [\"mlp.c_fc\", \"mlp.c_proj\"],\n+        },\n+        \"freeze_exceptions_mapping\": {\n+            \"wte\": \"wte\",\n+            \"lm_head\": \"lm_head\",\n+            \"lora\": \"lora\"\n+        },\n+        \"tokenizer\": {\n+            \"eot_idx\": 0,",
        "comment_created_at": "2023-10-25T13:54:05+00:00",
        "comment_author": "JegernOUTT",
        "comment_body": "We are using encodings which are downloaded from HF directly. Since you're using https://huggingface.co/TheBloke/CodeLlama-7B-fp16, try to study the [tokenizer file](https://huggingface.co/TheBloke/CodeLlama-7B-fp16/raw/main/tokenizer.json), [code llama FIM format](https://github.com/ggerganov/llama.cpp/issues/2818). Compare them with some official sources. \r\nAs an `escape` token you can choose any \"empty\" token, treat it like some kind of \"control\" symbol\r\n\r\nAlso, don't forget about `BOS` token, as I remember llama models are using it as a starting label. If it's true - make sure our datapipeline prepares data correctly",
        "pr_file_module": null
      },
      {
        "comment_id": "1372089573",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 194,
        "pr_file": "refact_data_pipeline/finetune/supported_models.py",
        "discussion_id": "1366470443",
        "commented_code": "@@ -151,5 +151,43 @@\n         },\n         \"train_model_modifiers\": [],\n         \"force_enable_checkpointing\": True\n+    },\n+\n+    \"codellama/7b\": {\n+        \"lora_target_modules_mapping\": {\n+            \"qkv\": [\"attn.q_attn\", \"attn.c_attn\"],\n+            \"out\": [\"attn.c_proj\"],\n+            \"backproj\": [\"attn.c_proj\"],\n+            \"mlp\": [\"mlp.c_fc\", \"mlp.c_proj\"],\n+        },\n+        \"freeze_exceptions_mapping\": {\n+            \"wte\": \"wte\",\n+            \"lm_head\": \"lm_head\",\n+            \"lora\": \"lora\"\n+        },\n+        \"tokenizer\": {\n+            \"eot_idx\": 0,",
        "comment_created_at": "2023-10-25T17:19:26+00:00",
        "comment_author": "adam-weinberger",
        "comment_body": "Yes, that makes sense. I will look into it.\r\n\r\nAlternatively, you are sure that lines 135-143 in encoding.py, are not a valid method of applying the llama tokenization?\r\n\r\n```\r\n        elif name in ['llama']:\r\n            from sentencepiece import SentencePieceProcessor\r\n            filename = Path(__file__).resolve().parent / f\"{name}.tokenizer.model\"\r\n            self._sentencepiece_tokenizer = SentencePieceProcessor(str(filename))\r\n            self.n_vocab = self._sentencepiece_tokenizer.vocab_size()\r\n            self.bos_id: int = self._sentencepiece_tokenizer.bos_id()\r\n            self.DIAMOND = self._sentencepiece_tokenizer.unk_id()\r\n            self.EOT = self._sentencepiece_tokenizer.eos_id()\r\n            self.LF = 13\r\n```",
        "pr_file_module": null
      },
      {
        "comment_id": "1372638820",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 194,
        "pr_file": "refact_data_pipeline/finetune/supported_models.py",
        "discussion_id": "1366470443",
        "commented_code": "@@ -151,5 +151,43 @@\n         },\n         \"train_model_modifiers\": [],\n         \"force_enable_checkpointing\": True\n+    },\n+\n+    \"codellama/7b\": {\n+        \"lora_target_modules_mapping\": {\n+            \"qkv\": [\"attn.q_attn\", \"attn.c_attn\"],\n+            \"out\": [\"attn.c_proj\"],\n+            \"backproj\": [\"attn.c_proj\"],\n+            \"mlp\": [\"mlp.c_fc\", \"mlp.c_proj\"],\n+        },\n+        \"freeze_exceptions_mapping\": {\n+            \"wte\": \"wte\",\n+            \"lm_head\": \"lm_head\",\n+            \"lora\": \"lora\"\n+        },\n+        \"tokenizer\": {\n+            \"eot_idx\": 0,",
        "comment_created_at": "2023-10-26T06:20:47+00:00",
        "comment_author": "JegernOUTT",
        "comment_body": "It should be valid. \r\nHowever, we don't use this code unless you're using an old `codify` model\r\nSee https://github.com/smallcloudai/refact/blob/main/refact_data_pipeline/finetune/model_handling.py#L127 to get the idea\r\n\r\n\r\nI must mention, that the logic behind constructing all these objects is weird, that's why we are refactoring it right now (https://github.com/smallcloudai/refact/pull/182)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1489569453",
    "pr_number": 299,
    "pr_file": "self_hosting_machinery/webgui/selfhost_fastapi_completions.py",
    "created_at": "2024-02-14T14:33:29+00:00",
    "commented_code": "\"endpoint_embeddings_style\": \"openai\",\n             \"size_embeddings\": 768,\n \n-            \"tokenizer_path_template\": \"https://huggingface.co/$MODEL/resolve/main/tokenizer.json\",\n+            \"tokenizer_path_template\": \"/get-tokenizer/$MODEL/tokenizer.json\",\n             \"tokenizer_rewrite_path\": {\n-                model: self._model_assigner.models_db[model][\"model_path\"]\n+                model: file.replace(env.DIR_WEIGHTS + \"/\", \"\").replace(\"/tokenizer.json\", \"\")\n                 for model in models_available\n-                if model in self._model_assigner.models_db\n+                if (file := tokenizer_json_file_for_model(self._model_assigner.models_db.get(model, {})))\n             },",
    "repo_full_name": "smallcloudai/refact",
    "discussion_comments": [
      {
        "comment_id": "1489569453",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 299,
        "pr_file": "self_hosting_machinery/webgui/selfhost_fastapi_completions.py",
        "discussion_id": "1489569453",
        "commented_code": "@@ -281,11 +298,11 @@ async def _coding_assistant_caps(self):\n             \"endpoint_embeddings_style\": \"openai\",\n             \"size_embeddings\": 768,\n \n-            \"tokenizer_path_template\": \"https://huggingface.co/$MODEL/resolve/main/tokenizer.json\",\n+            \"tokenizer_path_template\": \"/get-tokenizer/$MODEL/tokenizer.json\",\n             \"tokenizer_rewrite_path\": {\n-                model: self._model_assigner.models_db[model][\"model_path\"]\n+                model: file.replace(env.DIR_WEIGHTS + \"/\", \"\").replace(\"/tokenizer.json\", \"\")\n                 for model in models_available\n-                if model in self._model_assigner.models_db\n+                if (file := tokenizer_json_file_for_model(self._model_assigner.models_db.get(model, {})))\n             },",
        "comment_created_at": "2024-02-14T14:33:29+00:00",
        "comment_author": "mitya52",
        "comment_body": "looks like we don't need to rewrite tokenizer path because backend knows it's place for given model\r\nthis hack was for loading tokenizer from huggingface where models have different names",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "1418845446",
    "pr_number": 238,
    "pr_file": "self_hosting_machinery/webgui/selfhost_fastapi_completions.py",
    "created_at": "2023-12-07T11:50:25+00:00",
    "commented_code": "async def _embeddings(self, post: Embeddings, request: Request, account: str = \"XXX\"):\n         account = \"XXX\"\n         ticket = Ticket(\"embed-\")\n-        model_name, err_msg = static_resolve_model(post.model_name, self._inference_queue)\n+        model_name, err_msg = static_resolve_model(\"thenlper/gte-base\", self._inference_queue)",
    "repo_full_name": "smallcloudai/refact",
    "discussion_comments": [
      {
        "comment_id": "1418845446",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 238,
        "pr_file": "self_hosting_machinery/webgui/selfhost_fastapi_completions.py",
        "discussion_id": "1418845446",
        "commented_code": "@@ -466,11 +464,11 @@ async def _chat(self, post: ChatContext, request: Request, account: str = \"XXX\")\n     async def _embeddings(self, post: Embeddings, request: Request, account: str = \"XXX\"):\n         account = \"XXX\"\n         ticket = Ticket(\"embed-\")\n-        model_name, err_msg = static_resolve_model(post.model_name, self._inference_queue)\n+        model_name, err_msg = static_resolve_model(\"thenlper/gte-base\", self._inference_queue)",
        "comment_created_at": "2023-12-07T11:50:25+00:00",
        "comment_author": "mitya52",
        "comment_body": "I don't think you need static_resolve, just check if model_name is in available_models\r\nand of course you SHOULD check model's availability",
        "pr_file_module": null
      }
    ]
  }
]