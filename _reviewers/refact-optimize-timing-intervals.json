[
  {
    "discussion_id": "2071480259",
    "pr_number": 732,
    "pr_file": "refact-agent/engine/src/indexing_utils.rs",
    "created_at": "2025-05-02T11:30:22+00:00",
    "commented_code": "+use std::time::Duration;\n+use tracing::info;\n+use tokio::sync::RwLock as ARwLock;\n+use std::sync::Arc;\n+\n+use crate::global_context::GlobalContext;\n+use crate::http::routers::v1::status::get_rag_status;\n+\n+/// Waits for both AST and VecDB indexing to complete based on --wait-ast and --wait-vecdb.\n+pub async fn wait_for_indexing(\n+    gcx: Arc<ARwLock<GlobalContext>>,\n+) {\n+    let cmdline = {\n+        let gcx_locked = gcx.read().await;\n+        gcx_locked.cmdline.clone()\n+    };\n+\n+    let ast_done = async || get_rag_status(gcx.clone()).await.ast.is_some_and(|ast_status| ast_status.astate == \"done\");\n+    #[cfg(feature = \"vecdb\")]\n+    let vecdb_done = async || get_rag_status(gcx.clone()).await.vecdb.is_some_and(|vecdb_status| vecdb_status.state == \"done\");\n+\n+    let mut waiting_ast = cmdline.wait_ast && !ast_done().await;\n+    #[cfg(feature = \"vecdb\")]\n+    let mut waiting_vecdb = cmdline.wait_vecdb && !vecdb_done().await;\n+    #[cfg(not(feature = \"vecdb\"))]\n+    let mut waiting_vecdb = false;\n+\n+    if waiting_ast {\n+        info!(\"Waiting for AST to finish indexing.\");\n+    }\n+    if waiting_vecdb {\n+        info!(\"Waiting for Vecdb to finish indexing.\");\n+    }\n+\n+    while waiting_ast || waiting_vecdb {\n+        if waiting_ast && ast_done().await {\n+            info!(\"Ast finished indexing.\");\n+            waiting_ast = false;\n+        }\n+        #[cfg(feature = \"vecdb\")]\n+        if waiting_vecdb && vecdb_done().await {\n+            info!(\"Vecdb finished indexing.\");\n+            waiting_vecdb = false;\n+        }\n+\n+        if waiting_ast || waiting_vecdb {\n+            tokio::time::sleep(Duration::from_secs(1)).await;",
    "repo_full_name": "smallcloudai/refact",
    "discussion_comments": [
      {
        "comment_id": "2071480259",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 732,
        "pr_file": "refact-agent/engine/src/indexing_utils.rs",
        "discussion_id": "2071480259",
        "commented_code": "@@ -0,0 +1,50 @@\n+use std::time::Duration;\n+use tracing::info;\n+use tokio::sync::RwLock as ARwLock;\n+use std::sync::Arc;\n+\n+use crate::global_context::GlobalContext;\n+use crate::http::routers::v1::status::get_rag_status;\n+\n+/// Waits for both AST and VecDB indexing to complete based on --wait-ast and --wait-vecdb.\n+pub async fn wait_for_indexing(\n+    gcx: Arc<ARwLock<GlobalContext>>,\n+) {\n+    let cmdline = {\n+        let gcx_locked = gcx.read().await;\n+        gcx_locked.cmdline.clone()\n+    };\n+\n+    let ast_done = async || get_rag_status(gcx.clone()).await.ast.is_some_and(|ast_status| ast_status.astate == \"done\");\n+    #[cfg(feature = \"vecdb\")]\n+    let vecdb_done = async || get_rag_status(gcx.clone()).await.vecdb.is_some_and(|vecdb_status| vecdb_status.state == \"done\");\n+\n+    let mut waiting_ast = cmdline.wait_ast && !ast_done().await;\n+    #[cfg(feature = \"vecdb\")]\n+    let mut waiting_vecdb = cmdline.wait_vecdb && !vecdb_done().await;\n+    #[cfg(not(feature = \"vecdb\"))]\n+    let mut waiting_vecdb = false;\n+\n+    if waiting_ast {\n+        info!(\"Waiting for AST to finish indexing.\");\n+    }\n+    if waiting_vecdb {\n+        info!(\"Waiting for Vecdb to finish indexing.\");\n+    }\n+\n+    while waiting_ast || waiting_vecdb {\n+        if waiting_ast && ast_done().await {\n+            info!(\"Ast finished indexing.\");\n+            waiting_ast = false;\n+        }\n+        #[cfg(feature = \"vecdb\")]\n+        if waiting_vecdb && vecdb_done().await {\n+            info!(\"Vecdb finished indexing.\");\n+            waiting_vecdb = false;\n+        }\n+\n+        if waiting_ast || waiting_vecdb {\n+            tokio::time::sleep(Duration::from_secs(1)).await;",
        "comment_created_at": "2025-05-02T11:30:22+00:00",
        "comment_author": "humbertoyusta",
        "comment_body": "check status is cheap, 200 ms or something less would be better I think",
        "pr_file_module": null
      },
      {
        "comment_id": "2071503555",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 732,
        "pr_file": "refact-agent/engine/src/indexing_utils.rs",
        "discussion_id": "2071480259",
        "commented_code": "@@ -0,0 +1,50 @@\n+use std::time::Duration;\n+use tracing::info;\n+use tokio::sync::RwLock as ARwLock;\n+use std::sync::Arc;\n+\n+use crate::global_context::GlobalContext;\n+use crate::http::routers::v1::status::get_rag_status;\n+\n+/// Waits for both AST and VecDB indexing to complete based on --wait-ast and --wait-vecdb.\n+pub async fn wait_for_indexing(\n+    gcx: Arc<ARwLock<GlobalContext>>,\n+) {\n+    let cmdline = {\n+        let gcx_locked = gcx.read().await;\n+        gcx_locked.cmdline.clone()\n+    };\n+\n+    let ast_done = async || get_rag_status(gcx.clone()).await.ast.is_some_and(|ast_status| ast_status.astate == \"done\");\n+    #[cfg(feature = \"vecdb\")]\n+    let vecdb_done = async || get_rag_status(gcx.clone()).await.vecdb.is_some_and(|vecdb_status| vecdb_status.state == \"done\");\n+\n+    let mut waiting_ast = cmdline.wait_ast && !ast_done().await;\n+    #[cfg(feature = \"vecdb\")]\n+    let mut waiting_vecdb = cmdline.wait_vecdb && !vecdb_done().await;\n+    #[cfg(not(feature = \"vecdb\"))]\n+    let mut waiting_vecdb = false;\n+\n+    if waiting_ast {\n+        info!(\"Waiting for AST to finish indexing.\");\n+    }\n+    if waiting_vecdb {\n+        info!(\"Waiting for Vecdb to finish indexing.\");\n+    }\n+\n+    while waiting_ast || waiting_vecdb {\n+        if waiting_ast && ast_done().await {\n+            info!(\"Ast finished indexing.\");\n+            waiting_ast = false;\n+        }\n+        #[cfg(feature = \"vecdb\")]\n+        if waiting_vecdb && vecdb_done().await {\n+            info!(\"Vecdb finished indexing.\");\n+            waiting_vecdb = false;\n+        }\n+\n+        if waiting_ast || waiting_vecdb {\n+            tokio::time::sleep(Duration::from_secs(1)).await;",
        "comment_created_at": "2025-05-02T11:51:42+00:00",
        "comment_author": "MDario123",
        "comment_body": "done",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2131547750",
    "pr_number": 812,
    "pr_file": "refact-agent/engine/src/git/cleanup.rs",
    "created_at": "2025-06-06T05:46:04+00:00",
    "commented_code": "+use std::path::Path;\n+use std::sync::Arc;\n+use std::time::SystemTime;\n+use std::collections::HashSet;\n+use tokio::time::Duration;\n+use tokio::sync::RwLock as ARwLock;\n+use git2::{Repository, Oid, ObjectType};\n+\n+use crate::ast::chunk_utils::official_text_hashing_function;\n+use crate::custom_error::trace_and_default;\n+use crate::files_correction::get_project_dirs;\n+use crate::global_context::GlobalContext;\n+\n+const MAX_INACTIVE_REPO_DURATION: Duration = Duration::from_secs(7 * 24 * 3600); // 1 week\n+pub const RECENT_COMMITS_DURATION: Duration = Duration::from_secs(7 * 24 * 60); // 1 week\n+const CLEANUP_INTERVAL_DURATION: Duration = Duration::from_secs(24 * 60 * 60); // 1 day\n+\n+pub async fn git_shadow_cleanup_background_task(gcx: Arc<ARwLock<GlobalContext>>) {\n+    loop {\n+        // wait 2 mins before cleanup; lower priority than other startup tasks\n+        tokio::time::sleep(tokio::time::Duration::from_secs(2 * 60 / 10)).await;\n+\n+        let cache_dir = gcx.read().await.cache_dir.clone();\n+        let workspace_folders = get_project_dirs(gcx.clone()).await;\n+        let workspace_folder_hashes: Vec<_> = workspace_folders.into_iter()\n+            .map(|f| official_text_hashing_function(&f.to_string_lossy())).collect();\n+\n+        let dirs_to_check: Vec<_> = [\n+            cache_dir.join(\"shadow_git\"),\n+            cache_dir.join(\"shadow_git\").join(\"nested\")\n+        ].into_iter().filter(|dir| dir.exists()).collect();\n+\n+        for dir in dirs_to_check {\n+            match cleanup_inactive_shadow_repositories(&dir, &workspace_folder_hashes).await {\n+                Ok(cleanup_count) => {\n+                    if cleanup_count > 0 {\n+                        tracing::info!(\"Git shadow cleanup: removed {} old repositories\", cleanup_count);\n+                    }\n+                }\n+                Err(e) => {\n+                    tracing::error!(\"Git shadow cleanup failed: {}\", e);\n+                }\n+            }\n+        }\n+\n+        match cleanup_old_objects_from_repos(&cache_dir.join(\"shadow_git\"), &workspace_folder_hashes).await {\n+            Ok(objects_cleaned) => {\n+                if objects_cleaned > 0 {\n+                    tracing::info!(\"Git object cleanup: removed {} old objects from active repositories\", objects_cleaned);\n+                }\n+            }\n+            Err(e) => {\n+                tracing::error!(\"Git object cleanup failed: {}\", e);\n+            }\n+        }\n+\n+        tokio::time::sleep(CLEANUP_INTERVAL_DURATION).await;\n+    }\n+}\n+\n+async fn cleanup_inactive_shadow_repositories(dir: &Path, workspace_folder_hashes: &[String]) -> Result<usize, String> {\n+    let mut cleanup_count = 0;\n+\n+    let mut entries = tokio::fs::read_dir(dir).await\n+        .map_err(|e| format!(\"Failed to read shadow_git directory: {}\", e))?;\n+\n+    while let Some(entry) = entries.next_entry().await\n+        .map_err(|e| format!(\"Failed to read directory entry: {}\", e))? {\n+\n+        let path = entry.path();\n+        let dir_name = path.file_name().unwrap_or_default().to_string_lossy().to_string();\n+        if !path.is_dir() || !path.join(\".git\").exists() || workspace_folder_hashes.contains(&dir_name) {\n+            continue;\n+        }\n+\n+        if repo_is_inactive(&path).await {\n+            match tokio::fs::remove_dir_all(&path).await {",
    "repo_full_name": "smallcloudai/refact",
    "discussion_comments": [
      {
        "comment_id": "2131547750",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 812,
        "pr_file": "refact-agent/engine/src/git/cleanup.rs",
        "discussion_id": "2131547750",
        "commented_code": "@@ -0,0 +1,302 @@\n+use std::path::Path;\n+use std::sync::Arc;\n+use std::time::SystemTime;\n+use std::collections::HashSet;\n+use tokio::time::Duration;\n+use tokio::sync::RwLock as ARwLock;\n+use git2::{Repository, Oid, ObjectType};\n+\n+use crate::ast::chunk_utils::official_text_hashing_function;\n+use crate::custom_error::trace_and_default;\n+use crate::files_correction::get_project_dirs;\n+use crate::global_context::GlobalContext;\n+\n+const MAX_INACTIVE_REPO_DURATION: Duration = Duration::from_secs(7 * 24 * 3600); // 1 week\n+pub const RECENT_COMMITS_DURATION: Duration = Duration::from_secs(7 * 24 * 60); // 1 week\n+const CLEANUP_INTERVAL_DURATION: Duration = Duration::from_secs(24 * 60 * 60); // 1 day\n+\n+pub async fn git_shadow_cleanup_background_task(gcx: Arc<ARwLock<GlobalContext>>) {\n+    loop {\n+        // wait 2 mins before cleanup; lower priority than other startup tasks\n+        tokio::time::sleep(tokio::time::Duration::from_secs(2 * 60 / 10)).await;\n+\n+        let cache_dir = gcx.read().await.cache_dir.clone();\n+        let workspace_folders = get_project_dirs(gcx.clone()).await;\n+        let workspace_folder_hashes: Vec<_> = workspace_folders.into_iter()\n+            .map(|f| official_text_hashing_function(&f.to_string_lossy())).collect();\n+\n+        let dirs_to_check: Vec<_> = [\n+            cache_dir.join(\"shadow_git\"),\n+            cache_dir.join(\"shadow_git\").join(\"nested\")\n+        ].into_iter().filter(|dir| dir.exists()).collect();\n+\n+        for dir in dirs_to_check {\n+            match cleanup_inactive_shadow_repositories(&dir, &workspace_folder_hashes).await {\n+                Ok(cleanup_count) => {\n+                    if cleanup_count > 0 {\n+                        tracing::info!(\"Git shadow cleanup: removed {} old repositories\", cleanup_count);\n+                    }\n+                }\n+                Err(e) => {\n+                    tracing::error!(\"Git shadow cleanup failed: {}\", e);\n+                }\n+            }\n+        }\n+\n+        match cleanup_old_objects_from_repos(&cache_dir.join(\"shadow_git\"), &workspace_folder_hashes).await {\n+            Ok(objects_cleaned) => {\n+                if objects_cleaned > 0 {\n+                    tracing::info!(\"Git object cleanup: removed {} old objects from active repositories\", objects_cleaned);\n+                }\n+            }\n+            Err(e) => {\n+                tracing::error!(\"Git object cleanup failed: {}\", e);\n+            }\n+        }\n+\n+        tokio::time::sleep(CLEANUP_INTERVAL_DURATION).await;\n+    }\n+}\n+\n+async fn cleanup_inactive_shadow_repositories(dir: &Path, workspace_folder_hashes: &[String]) -> Result<usize, String> {\n+    let mut cleanup_count = 0;\n+\n+    let mut entries = tokio::fs::read_dir(dir).await\n+        .map_err(|e| format!(\"Failed to read shadow_git directory: {}\", e))?;\n+\n+    while let Some(entry) = entries.next_entry().await\n+        .map_err(|e| format!(\"Failed to read directory entry: {}\", e))? {\n+\n+        let path = entry.path();\n+        let dir_name = path.file_name().unwrap_or_default().to_string_lossy().to_string();\n+        if !path.is_dir() || !path.join(\".git\").exists() || workspace_folder_hashes.contains(&dir_name) {\n+            continue;\n+        }\n+\n+        if repo_is_inactive(&path).await {\n+            match tokio::fs::remove_dir_all(&path).await {",
        "comment_created_at": "2025-06-06T05:46:04+00:00",
        "comment_author": "mitya52",
        "comment_body": "it's take some time to remove dir\r\nbetter approach:\r\n* rename (mv): repo -> repo.to-remove\r\n* then remove: rm -rf repo.to-remove",
        "pr_file_module": null
      },
      {
        "comment_id": "2131548866",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 812,
        "pr_file": "refact-agent/engine/src/git/cleanup.rs",
        "discussion_id": "2131547750",
        "commented_code": "@@ -0,0 +1,302 @@\n+use std::path::Path;\n+use std::sync::Arc;\n+use std::time::SystemTime;\n+use std::collections::HashSet;\n+use tokio::time::Duration;\n+use tokio::sync::RwLock as ARwLock;\n+use git2::{Repository, Oid, ObjectType};\n+\n+use crate::ast::chunk_utils::official_text_hashing_function;\n+use crate::custom_error::trace_and_default;\n+use crate::files_correction::get_project_dirs;\n+use crate::global_context::GlobalContext;\n+\n+const MAX_INACTIVE_REPO_DURATION: Duration = Duration::from_secs(7 * 24 * 3600); // 1 week\n+pub const RECENT_COMMITS_DURATION: Duration = Duration::from_secs(7 * 24 * 60); // 1 week\n+const CLEANUP_INTERVAL_DURATION: Duration = Duration::from_secs(24 * 60 * 60); // 1 day\n+\n+pub async fn git_shadow_cleanup_background_task(gcx: Arc<ARwLock<GlobalContext>>) {\n+    loop {\n+        // wait 2 mins before cleanup; lower priority than other startup tasks\n+        tokio::time::sleep(tokio::time::Duration::from_secs(2 * 60 / 10)).await;\n+\n+        let cache_dir = gcx.read().await.cache_dir.clone();\n+        let workspace_folders = get_project_dirs(gcx.clone()).await;\n+        let workspace_folder_hashes: Vec<_> = workspace_folders.into_iter()\n+            .map(|f| official_text_hashing_function(&f.to_string_lossy())).collect();\n+\n+        let dirs_to_check: Vec<_> = [\n+            cache_dir.join(\"shadow_git\"),\n+            cache_dir.join(\"shadow_git\").join(\"nested\")\n+        ].into_iter().filter(|dir| dir.exists()).collect();\n+\n+        for dir in dirs_to_check {\n+            match cleanup_inactive_shadow_repositories(&dir, &workspace_folder_hashes).await {\n+                Ok(cleanup_count) => {\n+                    if cleanup_count > 0 {\n+                        tracing::info!(\"Git shadow cleanup: removed {} old repositories\", cleanup_count);\n+                    }\n+                }\n+                Err(e) => {\n+                    tracing::error!(\"Git shadow cleanup failed: {}\", e);\n+                }\n+            }\n+        }\n+\n+        match cleanup_old_objects_from_repos(&cache_dir.join(\"shadow_git\"), &workspace_folder_hashes).await {\n+            Ok(objects_cleaned) => {\n+                if objects_cleaned > 0 {\n+                    tracing::info!(\"Git object cleanup: removed {} old objects from active repositories\", objects_cleaned);\n+                }\n+            }\n+            Err(e) => {\n+                tracing::error!(\"Git object cleanup failed: {}\", e);\n+            }\n+        }\n+\n+        tokio::time::sleep(CLEANUP_INTERVAL_DURATION).await;\n+    }\n+}\n+\n+async fn cleanup_inactive_shadow_repositories(dir: &Path, workspace_folder_hashes: &[String]) -> Result<usize, String> {\n+    let mut cleanup_count = 0;\n+\n+    let mut entries = tokio::fs::read_dir(dir).await\n+        .map_err(|e| format!(\"Failed to read shadow_git directory: {}\", e))?;\n+\n+    while let Some(entry) = entries.next_entry().await\n+        .map_err(|e| format!(\"Failed to read directory entry: {}\", e))? {\n+\n+        let path = entry.path();\n+        let dir_name = path.file_name().unwrap_or_default().to_string_lossy().to_string();\n+        if !path.is_dir() || !path.join(\".git\").exists() || workspace_folder_hashes.contains(&dir_name) {\n+            continue;\n+        }\n+\n+        if repo_is_inactive(&path).await {\n+            match tokio::fs::remove_dir_all(&path).await {",
        "comment_created_at": "2025-06-06T05:47:24+00:00",
        "comment_author": "mitya52",
        "comment_body": "and we can perform it as 2 steps: first rename all with some postfix (*.to-remove) and then remove all dirs with than prefix",
        "pr_file_module": null
      },
      {
        "comment_id": "2131906184",
        "repo_full_name": "smallcloudai/refact",
        "pr_number": 812,
        "pr_file": "refact-agent/engine/src/git/cleanup.rs",
        "discussion_id": "2131547750",
        "commented_code": "@@ -0,0 +1,302 @@\n+use std::path::Path;\n+use std::sync::Arc;\n+use std::time::SystemTime;\n+use std::collections::HashSet;\n+use tokio::time::Duration;\n+use tokio::sync::RwLock as ARwLock;\n+use git2::{Repository, Oid, ObjectType};\n+\n+use crate::ast::chunk_utils::official_text_hashing_function;\n+use crate::custom_error::trace_and_default;\n+use crate::files_correction::get_project_dirs;\n+use crate::global_context::GlobalContext;\n+\n+const MAX_INACTIVE_REPO_DURATION: Duration = Duration::from_secs(7 * 24 * 3600); // 1 week\n+pub const RECENT_COMMITS_DURATION: Duration = Duration::from_secs(7 * 24 * 60); // 1 week\n+const CLEANUP_INTERVAL_DURATION: Duration = Duration::from_secs(24 * 60 * 60); // 1 day\n+\n+pub async fn git_shadow_cleanup_background_task(gcx: Arc<ARwLock<GlobalContext>>) {\n+    loop {\n+        // wait 2 mins before cleanup; lower priority than other startup tasks\n+        tokio::time::sleep(tokio::time::Duration::from_secs(2 * 60 / 10)).await;\n+\n+        let cache_dir = gcx.read().await.cache_dir.clone();\n+        let workspace_folders = get_project_dirs(gcx.clone()).await;\n+        let workspace_folder_hashes: Vec<_> = workspace_folders.into_iter()\n+            .map(|f| official_text_hashing_function(&f.to_string_lossy())).collect();\n+\n+        let dirs_to_check: Vec<_> = [\n+            cache_dir.join(\"shadow_git\"),\n+            cache_dir.join(\"shadow_git\").join(\"nested\")\n+        ].into_iter().filter(|dir| dir.exists()).collect();\n+\n+        for dir in dirs_to_check {\n+            match cleanup_inactive_shadow_repositories(&dir, &workspace_folder_hashes).await {\n+                Ok(cleanup_count) => {\n+                    if cleanup_count > 0 {\n+                        tracing::info!(\"Git shadow cleanup: removed {} old repositories\", cleanup_count);\n+                    }\n+                }\n+                Err(e) => {\n+                    tracing::error!(\"Git shadow cleanup failed: {}\", e);\n+                }\n+            }\n+        }\n+\n+        match cleanup_old_objects_from_repos(&cache_dir.join(\"shadow_git\"), &workspace_folder_hashes).await {\n+            Ok(objects_cleaned) => {\n+                if objects_cleaned > 0 {\n+                    tracing::info!(\"Git object cleanup: removed {} old objects from active repositories\", objects_cleaned);\n+                }\n+            }\n+            Err(e) => {\n+                tracing::error!(\"Git object cleanup failed: {}\", e);\n+            }\n+        }\n+\n+        tokio::time::sleep(CLEANUP_INTERVAL_DURATION).await;\n+    }\n+}\n+\n+async fn cleanup_inactive_shadow_repositories(dir: &Path, workspace_folder_hashes: &[String]) -> Result<usize, String> {\n+    let mut cleanup_count = 0;\n+\n+    let mut entries = tokio::fs::read_dir(dir).await\n+        .map_err(|e| format!(\"Failed to read shadow_git directory: {}\", e))?;\n+\n+    while let Some(entry) = entries.next_entry().await\n+        .map_err(|e| format!(\"Failed to read directory entry: {}\", e))? {\n+\n+        let path = entry.path();\n+        let dir_name = path.file_name().unwrap_or_default().to_string_lossy().to_string();\n+        if !path.is_dir() || !path.join(\".git\").exists() || workspace_folder_hashes.contains(&dir_name) {\n+            continue;\n+        }\n+\n+        if repo_is_inactive(&path).await {\n+            match tokio::fs::remove_dir_all(&path).await {",
        "comment_created_at": "2025-06-06T09:58:29+00:00",
        "comment_author": "humbertoyusta",
        "comment_body": "Done",
        "pr_file_module": null
      }
    ]
  }
]