[
  {
    "discussion_id": "2213945377",
    "pr_number": 5828,
    "pr_file": "src/services/code-index/manager.ts",
    "created_at": "2025-07-17T17:49:24+00:00",
    "commented_code": "/**\n \t * Initiates the indexing process (initial scan and starts watcher).\n+\t * Enhanced with error recovery to prevent users from needing to reload workspace.\n \t */\n-\n \tpublic async startIndexing(): Promise<void> {\n \t\tif (!this.isFeatureEnabled) {\n \t\t\treturn\n \t\t}\n \t\tthis.assertInitialized()\n-\t\tawait this._orchestrator!.startIndexing()\n+\n+\t\t// If we're in an error state, attempt to recover by resetting the state\n+\t\tif (this._stateManager.state === \"Error\") {\n+\t\t\tconsole.log(\"[CodeIndexManager] Attempting to recover from error state before starting indexing.\")",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2213945377",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5828,
        "pr_file": "src/services/code-index/manager.ts",
        "discussion_id": "2213945377",
        "commented_code": "@@ -157,14 +157,36 @@ export class CodeIndexManager {\n \n \t/**\n \t * Initiates the indexing process (initial scan and starts watcher).\n+\t * Enhanced with error recovery to prevent users from needing to reload workspace.\n \t */\n-\n \tpublic async startIndexing(): Promise<void> {\n \t\tif (!this.isFeatureEnabled) {\n \t\t\treturn\n \t\t}\n \t\tthis.assertInitialized()\n-\t\tawait this._orchestrator!.startIndexing()\n+\n+\t\t// If we're in an error state, attempt to recover by resetting the state\n+\t\tif (this._stateManager.state === \"Error\") {\n+\t\t\tconsole.log(\"[CodeIndexManager] Attempting to recover from error state before starting indexing.\")",
        "comment_created_at": "2025-07-17T17:49:24+00:00",
        "comment_author": "ellipsis-dev[bot]",
        "comment_body": "Consider using a centralized logging abstraction instead of raw console.log/console.error for consistency across the codebase.\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2183941333",
    "pr_number": 5356,
    "pr_file": "src/core/tools/executeCommandTool.ts",
    "created_at": "2025-07-03T23:42:21+00:00",
    "commented_code": ") {\n \tlet command: string | undefined = block.params.command\n \tconst customCwd: string | undefined = block.params.cwd\n+\tconsole.log(\"executeCommandTool\", command)",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2183941333",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5356,
        "pr_file": "src/core/tools/executeCommandTool.ts",
        "discussion_id": "2183941333",
        "commented_code": "@@ -27,6 +27,7 @@ export async function executeCommandTool(\n ) {\n \tlet command: string | undefined = block.params.command\n \tconst customCwd: string | undefined = block.params.cwd\n+\tconsole.log(\"executeCommandTool\", command)",
        "comment_created_at": "2025-07-03T23:42:21+00:00",
        "comment_author": "ellipsis-dev[bot]",
        "comment_body": "Please remove or replace the debug console.log with a proper logging mechanism (e.g., a debug logger or TelemetryService) to avoid leaving debug statements in production.\n```suggestion\n\n```\n",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2177869237",
    "pr_number": 5304,
    "pr_file": "webview-ui/src/services/mermaidSyntaxFixer.ts",
    "created_at": "2025-07-01T15:13:12+00:00",
    "commented_code": "+import { vscode } from \"@src/utils/vscode\"\n+import i18next from \"i18next\"\n+\n+export interface MermaidFixResult {\n+\tsuccess: boolean\n+\tfixedCode?: string\n+\terror?: string\n+\tattempts?: number\n+}\n+\n+export interface MermaidValidationResult {\n+\tisValid: boolean\n+\terror?: string\n+}\n+\n+/**\n+ * Service for validating and fixing Mermaid syntax using LLM assistance\n+ */\n+export class MermaidSyntaxFixer {\n+\tprivate static readonly MAX_FIX_ATTEMPTS = 2\n+\tprivate static readonly FIX_TIMEOUT = 30000 // 30 seconds\n+\n+\t/**\n+\t * Applies deterministic fixes for common LLM errors before validation\n+\t */\n+\tstatic applyDeterministicFixes(code: string): string {\n+\t\t// Fix HTML entity encoding: --&gt; should be -->;\n+\t\t// surprisingly, this does most of the heavy lifting in the MermaidSyntaxFixer\n+\t\t// sometimes the llm prepends ```mermaid, remove that\n+\t\treturn code.replace(/--&gt;/g, \"-->\").replace(/```mermaid/, \"\")\n+\t}\n+\n+\t/**\n+\t * Validates Mermaid syntax using the mermaid library\n+\t */\n+\tstatic async validateSyntax(code: string): Promise<MermaidValidationResult> {\n+\t\ttry {\n+\t\t\tconst mermaid = (await import(\"mermaid\")).default\n+\t\t\tawait mermaid.parse(code)\n+\t\t\treturn { isValid: true }\n+\t\t} catch (error) {\n+\t\t\treturn {\n+\t\t\t\tisValid: false,\n+\t\t\t\terror: error instanceof Error ? error.message : i18next.t(\"common:mermaid.errors.unknown_syntax\"),\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Requests the LLM to fix the Mermaid syntax via the extension\n+\t */\n+\tprivate static requestLLMFix(\n+\t\tcode: string,\n+\t\terror: string,\n+\t): Promise<{ fixedCode: string } | { requestError: string }> {\n+\t\treturn new Promise((resolve, _reject) => {\n+\t\t\tconst requestId = `mermaid-fix-${Date.now()}-${Math.random().toString(36).substring(2, 11)}`\n+\n+\t\t\tconst timeout = setTimeout(() => {\n+\t\t\t\tcleanup()\n+\t\t\t\tresolve({ requestError: i18next.t(\"common:mermaid.errors.fix_timeout\") })\n+\t\t\t}, this.FIX_TIMEOUT)\n+\n+\t\t\tconst messageListener = (event: MessageEvent) => {\n+\t\t\t\tconst message = event.data\n+\t\t\t\tif (message.type === \"mermaidFixResponse\" && message.requestId === requestId) {\n+\t\t\t\t\tcleanup()\n+\n+\t\t\t\t\tif (message.success) {\n+\t\t\t\t\t\tresolve({ fixedCode: message.fixedCode })\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tresolve({\n+\t\t\t\t\t\t\trequestError: message.error || i18next.t(\"common:mermaid.errors.fix_request_failed\"),\n+\t\t\t\t\t\t})\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tconst cleanup = () => {\n+\t\t\t\tclearTimeout(timeout)\n+\t\t\t\twindow.removeEventListener(\"message\", messageListener)\n+\t\t\t}\n+\n+\t\t\twindow.addEventListener(\"message\", messageListener)\n+\n+\t\t\tvscode.postMessage({\n+\t\t\t\ttype: \"fixMermaidSyntax\",\n+\t\t\t\trequestId,\n+\t\t\t\ttext: code,\n+\t\t\t\tvalues: { error },\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t/**\n+\t * Attempts to fix Mermaid syntax with automatic retry and fallback\n+\t * Always returns the best attempt at fixing the code, even if not completely successful\n+\t */\n+\tstatic async autoFixSyntax(code: string): Promise<MermaidFixResult> {\n+\t\tlet currentCode = code\n+\t\tlet llmAttempts = 0\n+\t\tlet finalError: string | undefined\n+\n+\t\twhile (true) {\n+\t\t\tconsole.info(\"attempt \", llmAttempts)",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2177869237",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5304,
        "pr_file": "webview-ui/src/services/mermaidSyntaxFixer.ts",
        "discussion_id": "2177869237",
        "commented_code": "@@ -0,0 +1,154 @@\n+import { vscode } from \"@src/utils/vscode\"\n+import i18next from \"i18next\"\n+\n+export interface MermaidFixResult {\n+\tsuccess: boolean\n+\tfixedCode?: string\n+\terror?: string\n+\tattempts?: number\n+}\n+\n+export interface MermaidValidationResult {\n+\tisValid: boolean\n+\terror?: string\n+}\n+\n+/**\n+ * Service for validating and fixing Mermaid syntax using LLM assistance\n+ */\n+export class MermaidSyntaxFixer {\n+\tprivate static readonly MAX_FIX_ATTEMPTS = 2\n+\tprivate static readonly FIX_TIMEOUT = 30000 // 30 seconds\n+\n+\t/**\n+\t * Applies deterministic fixes for common LLM errors before validation\n+\t */\n+\tstatic applyDeterministicFixes(code: string): string {\n+\t\t// Fix HTML entity encoding: --&gt; should be -->;\n+\t\t// surprisingly, this does most of the heavy lifting in the MermaidSyntaxFixer\n+\t\t// sometimes the llm prepends ```mermaid, remove that\n+\t\treturn code.replace(/--&gt;/g, \"-->\").replace(/```mermaid/, \"\")\n+\t}\n+\n+\t/**\n+\t * Validates Mermaid syntax using the mermaid library\n+\t */\n+\tstatic async validateSyntax(code: string): Promise<MermaidValidationResult> {\n+\t\ttry {\n+\t\t\tconst mermaid = (await import(\"mermaid\")).default\n+\t\t\tawait mermaid.parse(code)\n+\t\t\treturn { isValid: true }\n+\t\t} catch (error) {\n+\t\t\treturn {\n+\t\t\t\tisValid: false,\n+\t\t\t\terror: error instanceof Error ? error.message : i18next.t(\"common:mermaid.errors.unknown_syntax\"),\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Requests the LLM to fix the Mermaid syntax via the extension\n+\t */\n+\tprivate static requestLLMFix(\n+\t\tcode: string,\n+\t\terror: string,\n+\t): Promise<{ fixedCode: string } | { requestError: string }> {\n+\t\treturn new Promise((resolve, _reject) => {\n+\t\t\tconst requestId = `mermaid-fix-${Date.now()}-${Math.random().toString(36).substring(2, 11)}`\n+\n+\t\t\tconst timeout = setTimeout(() => {\n+\t\t\t\tcleanup()\n+\t\t\t\tresolve({ requestError: i18next.t(\"common:mermaid.errors.fix_timeout\") })\n+\t\t\t}, this.FIX_TIMEOUT)\n+\n+\t\t\tconst messageListener = (event: MessageEvent) => {\n+\t\t\t\tconst message = event.data\n+\t\t\t\tif (message.type === \"mermaidFixResponse\" && message.requestId === requestId) {\n+\t\t\t\t\tcleanup()\n+\n+\t\t\t\t\tif (message.success) {\n+\t\t\t\t\t\tresolve({ fixedCode: message.fixedCode })\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tresolve({\n+\t\t\t\t\t\t\trequestError: message.error || i18next.t(\"common:mermaid.errors.fix_request_failed\"),\n+\t\t\t\t\t\t})\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tconst cleanup = () => {\n+\t\t\t\tclearTimeout(timeout)\n+\t\t\t\twindow.removeEventListener(\"message\", messageListener)\n+\t\t\t}\n+\n+\t\t\twindow.addEventListener(\"message\", messageListener)\n+\n+\t\t\tvscode.postMessage({\n+\t\t\t\ttype: \"fixMermaidSyntax\",\n+\t\t\t\trequestId,\n+\t\t\t\ttext: code,\n+\t\t\t\tvalues: { error },\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t/**\n+\t * Attempts to fix Mermaid syntax with automatic retry and fallback\n+\t * Always returns the best attempt at fixing the code, even if not completely successful\n+\t */\n+\tstatic async autoFixSyntax(code: string): Promise<MermaidFixResult> {\n+\t\tlet currentCode = code\n+\t\tlet llmAttempts = 0\n+\t\tlet finalError: string | undefined\n+\n+\t\twhile (true) {\n+\t\t\tconsole.info(\"attempt \", llmAttempts)",
        "comment_created_at": "2025-07-01T15:13:12+00:00",
        "comment_author": "daniel-lxs",
        "comment_body": "We should clean this up before merging.\r\n\r\nThe `console.info` statements currently in the code (including the one at line 144 where `finalError` is logged) should be removed. If logging is still needed, consider using a proper logging service instead of `console` calls.",
        "pr_file_module": null
      },
      {
        "comment_id": "2179573006",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5304,
        "pr_file": "webview-ui/src/services/mermaidSyntaxFixer.ts",
        "discussion_id": "2177869237",
        "commented_code": "@@ -0,0 +1,154 @@\n+import { vscode } from \"@src/utils/vscode\"\n+import i18next from \"i18next\"\n+\n+export interface MermaidFixResult {\n+\tsuccess: boolean\n+\tfixedCode?: string\n+\terror?: string\n+\tattempts?: number\n+}\n+\n+export interface MermaidValidationResult {\n+\tisValid: boolean\n+\terror?: string\n+}\n+\n+/**\n+ * Service for validating and fixing Mermaid syntax using LLM assistance\n+ */\n+export class MermaidSyntaxFixer {\n+\tprivate static readonly MAX_FIX_ATTEMPTS = 2\n+\tprivate static readonly FIX_TIMEOUT = 30000 // 30 seconds\n+\n+\t/**\n+\t * Applies deterministic fixes for common LLM errors before validation\n+\t */\n+\tstatic applyDeterministicFixes(code: string): string {\n+\t\t// Fix HTML entity encoding: --&gt; should be -->;\n+\t\t// surprisingly, this does most of the heavy lifting in the MermaidSyntaxFixer\n+\t\t// sometimes the llm prepends ```mermaid, remove that\n+\t\treturn code.replace(/--&gt;/g, \"-->\").replace(/```mermaid/, \"\")\n+\t}\n+\n+\t/**\n+\t * Validates Mermaid syntax using the mermaid library\n+\t */\n+\tstatic async validateSyntax(code: string): Promise<MermaidValidationResult> {\n+\t\ttry {\n+\t\t\tconst mermaid = (await import(\"mermaid\")).default\n+\t\t\tawait mermaid.parse(code)\n+\t\t\treturn { isValid: true }\n+\t\t} catch (error) {\n+\t\t\treturn {\n+\t\t\t\tisValid: false,\n+\t\t\t\terror: error instanceof Error ? error.message : i18next.t(\"common:mermaid.errors.unknown_syntax\"),\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Requests the LLM to fix the Mermaid syntax via the extension\n+\t */\n+\tprivate static requestLLMFix(\n+\t\tcode: string,\n+\t\terror: string,\n+\t): Promise<{ fixedCode: string } | { requestError: string }> {\n+\t\treturn new Promise((resolve, _reject) => {\n+\t\t\tconst requestId = `mermaid-fix-${Date.now()}-${Math.random().toString(36).substring(2, 11)}`\n+\n+\t\t\tconst timeout = setTimeout(() => {\n+\t\t\t\tcleanup()\n+\t\t\t\tresolve({ requestError: i18next.t(\"common:mermaid.errors.fix_timeout\") })\n+\t\t\t}, this.FIX_TIMEOUT)\n+\n+\t\t\tconst messageListener = (event: MessageEvent) => {\n+\t\t\t\tconst message = event.data\n+\t\t\t\tif (message.type === \"mermaidFixResponse\" && message.requestId === requestId) {\n+\t\t\t\t\tcleanup()\n+\n+\t\t\t\t\tif (message.success) {\n+\t\t\t\t\t\tresolve({ fixedCode: message.fixedCode })\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tresolve({\n+\t\t\t\t\t\t\trequestError: message.error || i18next.t(\"common:mermaid.errors.fix_request_failed\"),\n+\t\t\t\t\t\t})\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tconst cleanup = () => {\n+\t\t\t\tclearTimeout(timeout)\n+\t\t\t\twindow.removeEventListener(\"message\", messageListener)\n+\t\t\t}\n+\n+\t\t\twindow.addEventListener(\"message\", messageListener)\n+\n+\t\t\tvscode.postMessage({\n+\t\t\t\ttype: \"fixMermaidSyntax\",\n+\t\t\t\trequestId,\n+\t\t\t\ttext: code,\n+\t\t\t\tvalues: { error },\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t/**\n+\t * Attempts to fix Mermaid syntax with automatic retry and fallback\n+\t * Always returns the best attempt at fixing the code, even if not completely successful\n+\t */\n+\tstatic async autoFixSyntax(code: string): Promise<MermaidFixResult> {\n+\t\tlet currentCode = code\n+\t\tlet llmAttempts = 0\n+\t\tlet finalError: string | undefined\n+\n+\t\twhile (true) {\n+\t\t\tconsole.info(\"attempt \", llmAttempts)",
        "comment_created_at": "2025-07-02T09:25:10+00:00",
        "comment_author": "markijbema",
        "comment_body": "Oh damn, that's sloppy, apologies. Should I also remove the console.warn/error in mermaidblock? (they were already there, but I do not think they're necessary either)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2076473224",
    "pr_number": 3243,
    "pr_file": "src/integrations/editor/DiffViewProvider.ts",
    "created_at": "2025-05-06T22:55:47+00:00",
    "commented_code": "const fileExists = this.editType === \"modify\"\n \t\tconst absolutePath = path.resolve(this.cwd, relPath)\n \t\tthis.isEditing = true\n-\t\t// if the file is already open, ensure it's not dirty before getting its contents\n+\n+\t\t// If the file is already open, ensure it's not dirty before getting its\n+\t\t// contents.\n \t\tif (fileExists) {\n \t\t\tconst existingDocument = vscode.workspace.textDocuments.find((doc) =>\n \t\t\t\tarePathsEqual(doc.uri.fsPath, absolutePath),\n \t\t\t)\n+\n \t\t\tif (existingDocument && existingDocument.isDirty) {\n \t\t\t\tawait existingDocument.save()\n \t\t\t}\n \t\t}\n \n-\t\t// get diagnostics before editing the file, we'll compare to diagnostics after editing to see if cline needs to fix anything\n+\t\t// Get diagnostics before editing the file, we'll compare to diagnostics\n+\t\t// after editing to see if Roo needs to fix anything.\n \t\tthis.preDiagnostics = vscode.languages.getDiagnostics()\n+\t\tconsole.log(`preDiagnostics: ${JSON.stringify(this.preDiagnostics)}`)",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2076473224",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 3243,
        "pr_file": "src/integrations/editor/DiffViewProvider.ts",
        "discussion_id": "2076473224",
        "commented_code": "@@ -32,110 +40,141 @@ export class DiffViewProvider {\n \t\tconst fileExists = this.editType === \"modify\"\n \t\tconst absolutePath = path.resolve(this.cwd, relPath)\n \t\tthis.isEditing = true\n-\t\t// if the file is already open, ensure it's not dirty before getting its contents\n+\n+\t\t// If the file is already open, ensure it's not dirty before getting its\n+\t\t// contents.\n \t\tif (fileExists) {\n \t\t\tconst existingDocument = vscode.workspace.textDocuments.find((doc) =>\n \t\t\t\tarePathsEqual(doc.uri.fsPath, absolutePath),\n \t\t\t)\n+\n \t\t\tif (existingDocument && existingDocument.isDirty) {\n \t\t\t\tawait existingDocument.save()\n \t\t\t}\n \t\t}\n \n-\t\t// get diagnostics before editing the file, we'll compare to diagnostics after editing to see if cline needs to fix anything\n+\t\t// Get diagnostics before editing the file, we'll compare to diagnostics\n+\t\t// after editing to see if Roo needs to fix anything.\n \t\tthis.preDiagnostics = vscode.languages.getDiagnostics()\n+\t\tconsole.log(`preDiagnostics: ${JSON.stringify(this.preDiagnostics)}`)",
        "comment_created_at": "2025-05-06T22:55:47+00:00",
        "comment_author": "ellipsis-dev[bot]",
        "comment_body": "Consider replacing `console.log` with structured logging for consistency and production-readiness.\n```suggestion\n\t\tlogger.info(`preDiagnostics: ${JSON.stringify(this.preDiagnostics)}`)\n```\n\n<sup>This comment was generated because it violated a code review rule: [mrule_OR1S8PRRHcvbdFib](https://app.ellipsis.dev/RooVetGit/code-review/rules?id=mrule_OR1S8PRRHcvbdFib).</sup>",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2133942365",
    "pr_number": 4314,
    "pr_file": "src/api/providers/fetchers/ollama.ts",
    "created_at": "2025-06-07T15:33:56+00:00",
    "commented_code": "+import axios from \"axios\"\n+import { ModelInfo, ollamaDefaultModelInfo } from \"@roo-code/types\"\n+import { z } from \"zod\"\n+\n+const OllamaModelDetailsSchema = z.object({\n+\tfamily: z.string(),\n+\tfamilies: z.array(z.string()),\n+\tformat: z.string(),\n+\tparameter_size: z.string(),\n+\tparent_model: z.string(),\n+\tquantization_level: z.string(),\n+})\n+\n+const OllamaModelSchema = z.object({\n+\tdetails: OllamaModelDetailsSchema,\n+\tdigest: z.string(),\n+\tmodel: z.string(),\n+\tmodified_at: z.string(),\n+\tname: z.string(),\n+\tsize: z.number(),\n+})\n+\n+const OllamaModelInfoResponseSchema = z.object({\n+\tmodelfile: z.string(),\n+\tparameters: z.string(),\n+\ttemplate: z.string(),\n+\tdetails: OllamaModelDetailsSchema,\n+\tmodel_info: z.record(z.string(), z.any()),\n+\tcapabilities: z.array(z.string()).optional(),\n+})\n+\n+const OllamaModelsResponseSchema = z.object({\n+\tmodels: z.array(OllamaModelSchema),\n+})\n+\n+type OllamaModelsResponse = z.infer<typeof OllamaModelsResponseSchema>\n+\n+type OllamaModelInfoResponse = z.infer<typeof OllamaModelInfoResponseSchema>\n+\n+export const parseOllamaModel = (rawModel: OllamaModelInfoResponse): ModelInfo => {\n+\tconst contextKey = Object.keys(rawModel.model_info).find((k) => k.includes(\"context_length\"))\n+\tconst contextWindow = contextKey ? rawModel.model_info[contextKey] : undefined\n+\n+\tconst modelInfo: ModelInfo = Object.assign({}, ollamaDefaultModelInfo, {\n+\t\tdescription: `Family: ${rawModel.details.family}, Context: ${contextWindow}, Size: ${rawModel.details.parameter_size}`,\n+\t\tcontextWindow: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t\tsupportsPromptCache: true,\n+\t\tsupportsImages: rawModel.capabilities?.includes(\"vision\"),\n+\t\tsupportsComputerUse: false,\n+\t\tmaxTokens: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t})\n+\n+\treturn modelInfo\n+}\n+\n+export async function getOllamaModels(baseUrl = \"http://localhost:11434\"): Promise<Record<string, ModelInfo>> {\n+\tconst models: Record<string, ModelInfo> = {}\n+\n+\t// clearing the input can leave an empty string; use the default in that case\n+\tbaseUrl = baseUrl === \"\" ? \"http://localhost:11434\" : baseUrl\n+\n+\ttry {\n+\t\tif (!URL.canParse(baseUrl)) {\n+\t\t\treturn models\n+\t\t}\n+\n+\t\tconst response = await axios.get<OllamaModelsResponse>(`${baseUrl}/api/tags`)\n+\t\tconst parsedResponse = OllamaModelsResponseSchema.safeParse(response.data)\n+\t\tlet modelInfoPromises = []\n+\n+\t\tif (parsedResponse.success) {\n+\t\t\tfor (const ollamaModel of parsedResponse.data.models) {\n+\t\t\t\tmodelInfoPromises.push(\n+\t\t\t\t\taxios\n+\t\t\t\t\t\t.post<OllamaModelInfoResponse>(`${baseUrl}/api/show`, {\n+\t\t\t\t\t\t\tmodel: ollamaModel.model,\n+\t\t\t\t\t\t})\n+\t\t\t\t\t\t.then((ollamaModelInfo) => {\n+\t\t\t\t\t\t\tmodels[ollamaModel.name] = parseOllamaModel(ollamaModelInfo.data)\n+\t\t\t\t\t\t}),\n+\t\t\t\t)\n+\t\t\t}\n+\n+\t\t\tawait Promise.all(modelInfoPromises)\n+\t\t} else {\n+\t\t\tconsole.error(`Error parsing Ollama models response: ${JSON.stringify(parsedResponse.error, null, 2)}`)\n+\t\t}\n+\t} catch (error) {\n+\t\tif (error.code === \"ECONNREFUSED\") {\n+\t\t\tconsole.info(`Failed connecting to Ollama at ${baseUrl}`)",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2133942365",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 4314,
        "pr_file": "src/api/providers/fetchers/ollama.ts",
        "discussion_id": "2133942365",
        "commented_code": "@@ -0,0 +1,97 @@\n+import axios from \"axios\"\n+import { ModelInfo, ollamaDefaultModelInfo } from \"@roo-code/types\"\n+import { z } from \"zod\"\n+\n+const OllamaModelDetailsSchema = z.object({\n+\tfamily: z.string(),\n+\tfamilies: z.array(z.string()),\n+\tformat: z.string(),\n+\tparameter_size: z.string(),\n+\tparent_model: z.string(),\n+\tquantization_level: z.string(),\n+})\n+\n+const OllamaModelSchema = z.object({\n+\tdetails: OllamaModelDetailsSchema,\n+\tdigest: z.string(),\n+\tmodel: z.string(),\n+\tmodified_at: z.string(),\n+\tname: z.string(),\n+\tsize: z.number(),\n+})\n+\n+const OllamaModelInfoResponseSchema = z.object({\n+\tmodelfile: z.string(),\n+\tparameters: z.string(),\n+\ttemplate: z.string(),\n+\tdetails: OllamaModelDetailsSchema,\n+\tmodel_info: z.record(z.string(), z.any()),\n+\tcapabilities: z.array(z.string()).optional(),\n+})\n+\n+const OllamaModelsResponseSchema = z.object({\n+\tmodels: z.array(OllamaModelSchema),\n+})\n+\n+type OllamaModelsResponse = z.infer<typeof OllamaModelsResponseSchema>\n+\n+type OllamaModelInfoResponse = z.infer<typeof OllamaModelInfoResponseSchema>\n+\n+export const parseOllamaModel = (rawModel: OllamaModelInfoResponse): ModelInfo => {\n+\tconst contextKey = Object.keys(rawModel.model_info).find((k) => k.includes(\"context_length\"))\n+\tconst contextWindow = contextKey ? rawModel.model_info[contextKey] : undefined\n+\n+\tconst modelInfo: ModelInfo = Object.assign({}, ollamaDefaultModelInfo, {\n+\t\tdescription: `Family: ${rawModel.details.family}, Context: ${contextWindow}, Size: ${rawModel.details.parameter_size}`,\n+\t\tcontextWindow: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t\tsupportsPromptCache: true,\n+\t\tsupportsImages: rawModel.capabilities?.includes(\"vision\"),\n+\t\tsupportsComputerUse: false,\n+\t\tmaxTokens: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t})\n+\n+\treturn modelInfo\n+}\n+\n+export async function getOllamaModels(baseUrl = \"http://localhost:11434\"): Promise<Record<string, ModelInfo>> {\n+\tconst models: Record<string, ModelInfo> = {}\n+\n+\t// clearing the input can leave an empty string; use the default in that case\n+\tbaseUrl = baseUrl === \"\" ? \"http://localhost:11434\" : baseUrl\n+\n+\ttry {\n+\t\tif (!URL.canParse(baseUrl)) {\n+\t\t\treturn models\n+\t\t}\n+\n+\t\tconst response = await axios.get<OllamaModelsResponse>(`${baseUrl}/api/tags`)\n+\t\tconst parsedResponse = OllamaModelsResponseSchema.safeParse(response.data)\n+\t\tlet modelInfoPromises = []\n+\n+\t\tif (parsedResponse.success) {\n+\t\t\tfor (const ollamaModel of parsedResponse.data.models) {\n+\t\t\t\tmodelInfoPromises.push(\n+\t\t\t\t\taxios\n+\t\t\t\t\t\t.post<OllamaModelInfoResponse>(`${baseUrl}/api/show`, {\n+\t\t\t\t\t\t\tmodel: ollamaModel.model,\n+\t\t\t\t\t\t})\n+\t\t\t\t\t\t.then((ollamaModelInfo) => {\n+\t\t\t\t\t\t\tmodels[ollamaModel.name] = parseOllamaModel(ollamaModelInfo.data)\n+\t\t\t\t\t\t}),\n+\t\t\t\t)\n+\t\t\t}\n+\n+\t\t\tawait Promise.all(modelInfoPromises)\n+\t\t} else {\n+\t\t\tconsole.error(`Error parsing Ollama models response: ${JSON.stringify(parsedResponse.error, null, 2)}`)\n+\t\t}\n+\t} catch (error) {\n+\t\tif (error.code === \"ECONNREFUSED\") {\n+\t\t\tconsole.info(`Failed connecting to Ollama at ${baseUrl}`)",
        "comment_created_at": "2025-06-07T15:33:56+00:00",
        "comment_author": "daniel-lxs",
        "comment_body": "I notice the error logging here uses different levels - `console.info` for connection failures (line 90) vs `console.warn` for other errors (line 92). In contrast, the LM Studio fetcher uses `console.error` for connection failures. Would it make sense to standardize the logging approach across both fetchers?",
        "pr_file_module": null
      },
      {
        "comment_id": "2135040219",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 4314,
        "pr_file": "src/api/providers/fetchers/ollama.ts",
        "discussion_id": "2133942365",
        "commented_code": "@@ -0,0 +1,97 @@\n+import axios from \"axios\"\n+import { ModelInfo, ollamaDefaultModelInfo } from \"@roo-code/types\"\n+import { z } from \"zod\"\n+\n+const OllamaModelDetailsSchema = z.object({\n+\tfamily: z.string(),\n+\tfamilies: z.array(z.string()),\n+\tformat: z.string(),\n+\tparameter_size: z.string(),\n+\tparent_model: z.string(),\n+\tquantization_level: z.string(),\n+})\n+\n+const OllamaModelSchema = z.object({\n+\tdetails: OllamaModelDetailsSchema,\n+\tdigest: z.string(),\n+\tmodel: z.string(),\n+\tmodified_at: z.string(),\n+\tname: z.string(),\n+\tsize: z.number(),\n+})\n+\n+const OllamaModelInfoResponseSchema = z.object({\n+\tmodelfile: z.string(),\n+\tparameters: z.string(),\n+\ttemplate: z.string(),\n+\tdetails: OllamaModelDetailsSchema,\n+\tmodel_info: z.record(z.string(), z.any()),\n+\tcapabilities: z.array(z.string()).optional(),\n+})\n+\n+const OllamaModelsResponseSchema = z.object({\n+\tmodels: z.array(OllamaModelSchema),\n+})\n+\n+type OllamaModelsResponse = z.infer<typeof OllamaModelsResponseSchema>\n+\n+type OllamaModelInfoResponse = z.infer<typeof OllamaModelInfoResponseSchema>\n+\n+export const parseOllamaModel = (rawModel: OllamaModelInfoResponse): ModelInfo => {\n+\tconst contextKey = Object.keys(rawModel.model_info).find((k) => k.includes(\"context_length\"))\n+\tconst contextWindow = contextKey ? rawModel.model_info[contextKey] : undefined\n+\n+\tconst modelInfo: ModelInfo = Object.assign({}, ollamaDefaultModelInfo, {\n+\t\tdescription: `Family: ${rawModel.details.family}, Context: ${contextWindow}, Size: ${rawModel.details.parameter_size}`,\n+\t\tcontextWindow: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t\tsupportsPromptCache: true,\n+\t\tsupportsImages: rawModel.capabilities?.includes(\"vision\"),\n+\t\tsupportsComputerUse: false,\n+\t\tmaxTokens: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t})\n+\n+\treturn modelInfo\n+}\n+\n+export async function getOllamaModels(baseUrl = \"http://localhost:11434\"): Promise<Record<string, ModelInfo>> {\n+\tconst models: Record<string, ModelInfo> = {}\n+\n+\t// clearing the input can leave an empty string; use the default in that case\n+\tbaseUrl = baseUrl === \"\" ? \"http://localhost:11434\" : baseUrl\n+\n+\ttry {\n+\t\tif (!URL.canParse(baseUrl)) {\n+\t\t\treturn models\n+\t\t}\n+\n+\t\tconst response = await axios.get<OllamaModelsResponse>(`${baseUrl}/api/tags`)\n+\t\tconst parsedResponse = OllamaModelsResponseSchema.safeParse(response.data)\n+\t\tlet modelInfoPromises = []\n+\n+\t\tif (parsedResponse.success) {\n+\t\t\tfor (const ollamaModel of parsedResponse.data.models) {\n+\t\t\t\tmodelInfoPromises.push(\n+\t\t\t\t\taxios\n+\t\t\t\t\t\t.post<OllamaModelInfoResponse>(`${baseUrl}/api/show`, {\n+\t\t\t\t\t\t\tmodel: ollamaModel.model,\n+\t\t\t\t\t\t})\n+\t\t\t\t\t\t.then((ollamaModelInfo) => {\n+\t\t\t\t\t\t\tmodels[ollamaModel.name] = parseOllamaModel(ollamaModelInfo.data)\n+\t\t\t\t\t\t}),\n+\t\t\t\t)\n+\t\t\t}\n+\n+\t\t\tawait Promise.all(modelInfoPromises)\n+\t\t} else {\n+\t\t\tconsole.error(`Error parsing Ollama models response: ${JSON.stringify(parsedResponse.error, null, 2)}`)\n+\t\t}\n+\t} catch (error) {\n+\t\tif (error.code === \"ECONNREFUSED\") {\n+\t\t\tconsole.info(`Failed connecting to Ollama at ${baseUrl}`)",
        "comment_created_at": "2025-06-09T05:11:49+00:00",
        "comment_author": "thecolorblue",
        "comment_body": "My thought was that `ECONNREFUSED` is most likely caused by ollama/lm studio not running. This is not really an error. \r\n\r\nThat being said, I think it makes more sense to use warn here. I'll update them both to be more consistent.",
        "pr_file_module": null
      }
    ]
  }
]