[
  {
    "discussion_id": "2177869237",
    "pr_number": 5304,
    "pr_file": "webview-ui/src/services/mermaidSyntaxFixer.ts",
    "created_at": "2025-07-01T15:13:12+00:00",
    "commented_code": "+import { vscode } from \"@src/utils/vscode\"\n+import i18next from \"i18next\"\n+\n+export interface MermaidFixResult {\n+\tsuccess: boolean\n+\tfixedCode?: string\n+\terror?: string\n+\tattempts?: number\n+}\n+\n+export interface MermaidValidationResult {\n+\tisValid: boolean\n+\terror?: string\n+}\n+\n+/**\n+ * Service for validating and fixing Mermaid syntax using LLM assistance\n+ */\n+export class MermaidSyntaxFixer {\n+\tprivate static readonly MAX_FIX_ATTEMPTS = 2\n+\tprivate static readonly FIX_TIMEOUT = 30000 // 30 seconds\n+\n+\t/**\n+\t * Applies deterministic fixes for common LLM errors before validation\n+\t */\n+\tstatic applyDeterministicFixes(code: string): string {\n+\t\t// Fix HTML entity encoding: --&gt; should be -->;\n+\t\t// surprisingly, this does most of the heavy lifting in the MermaidSyntaxFixer\n+\t\t// sometimes the llm prepends ```mermaid, remove that\n+\t\treturn code.replace(/--&gt;/g, \"-->\").replace(/```mermaid/, \"\")\n+\t}\n+\n+\t/**\n+\t * Validates Mermaid syntax using the mermaid library\n+\t */\n+\tstatic async validateSyntax(code: string): Promise<MermaidValidationResult> {\n+\t\ttry {\n+\t\t\tconst mermaid = (await import(\"mermaid\")).default\n+\t\t\tawait mermaid.parse(code)\n+\t\t\treturn { isValid: true }\n+\t\t} catch (error) {\n+\t\t\treturn {\n+\t\t\t\tisValid: false,\n+\t\t\t\terror: error instanceof Error ? error.message : i18next.t(\"common:mermaid.errors.unknown_syntax\"),\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Requests the LLM to fix the Mermaid syntax via the extension\n+\t */\n+\tprivate static requestLLMFix(\n+\t\tcode: string,\n+\t\terror: string,\n+\t): Promise<{ fixedCode: string } | { requestError: string }> {\n+\t\treturn new Promise((resolve, _reject) => {\n+\t\t\tconst requestId = `mermaid-fix-${Date.now()}-${Math.random().toString(36).substring(2, 11)}`\n+\n+\t\t\tconst timeout = setTimeout(() => {\n+\t\t\t\tcleanup()\n+\t\t\t\tresolve({ requestError: i18next.t(\"common:mermaid.errors.fix_timeout\") })\n+\t\t\t}, this.FIX_TIMEOUT)\n+\n+\t\t\tconst messageListener = (event: MessageEvent) => {\n+\t\t\t\tconst message = event.data\n+\t\t\t\tif (message.type === \"mermaidFixResponse\" && message.requestId === requestId) {\n+\t\t\t\t\tcleanup()\n+\n+\t\t\t\t\tif (message.success) {\n+\t\t\t\t\t\tresolve({ fixedCode: message.fixedCode })\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tresolve({\n+\t\t\t\t\t\t\trequestError: message.error || i18next.t(\"common:mermaid.errors.fix_request_failed\"),\n+\t\t\t\t\t\t})\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tconst cleanup = () => {\n+\t\t\t\tclearTimeout(timeout)\n+\t\t\t\twindow.removeEventListener(\"message\", messageListener)\n+\t\t\t}\n+\n+\t\t\twindow.addEventListener(\"message\", messageListener)\n+\n+\t\t\tvscode.postMessage({\n+\t\t\t\ttype: \"fixMermaidSyntax\",\n+\t\t\t\trequestId,\n+\t\t\t\ttext: code,\n+\t\t\t\tvalues: { error },\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t/**\n+\t * Attempts to fix Mermaid syntax with automatic retry and fallback\n+\t * Always returns the best attempt at fixing the code, even if not completely successful\n+\t */\n+\tstatic async autoFixSyntax(code: string): Promise<MermaidFixResult> {\n+\t\tlet currentCode = code\n+\t\tlet llmAttempts = 0\n+\t\tlet finalError: string | undefined\n+\n+\t\twhile (true) {\n+\t\t\tconsole.info(\"attempt \", llmAttempts)",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2177869237",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5304,
        "pr_file": "webview-ui/src/services/mermaidSyntaxFixer.ts",
        "discussion_id": "2177869237",
        "commented_code": "@@ -0,0 +1,154 @@\n+import { vscode } from \"@src/utils/vscode\"\n+import i18next from \"i18next\"\n+\n+export interface MermaidFixResult {\n+\tsuccess: boolean\n+\tfixedCode?: string\n+\terror?: string\n+\tattempts?: number\n+}\n+\n+export interface MermaidValidationResult {\n+\tisValid: boolean\n+\terror?: string\n+}\n+\n+/**\n+ * Service for validating and fixing Mermaid syntax using LLM assistance\n+ */\n+export class MermaidSyntaxFixer {\n+\tprivate static readonly MAX_FIX_ATTEMPTS = 2\n+\tprivate static readonly FIX_TIMEOUT = 30000 // 30 seconds\n+\n+\t/**\n+\t * Applies deterministic fixes for common LLM errors before validation\n+\t */\n+\tstatic applyDeterministicFixes(code: string): string {\n+\t\t// Fix HTML entity encoding: --&gt; should be -->;\n+\t\t// surprisingly, this does most of the heavy lifting in the MermaidSyntaxFixer\n+\t\t// sometimes the llm prepends ```mermaid, remove that\n+\t\treturn code.replace(/--&gt;/g, \"-->\").replace(/```mermaid/, \"\")\n+\t}\n+\n+\t/**\n+\t * Validates Mermaid syntax using the mermaid library\n+\t */\n+\tstatic async validateSyntax(code: string): Promise<MermaidValidationResult> {\n+\t\ttry {\n+\t\t\tconst mermaid = (await import(\"mermaid\")).default\n+\t\t\tawait mermaid.parse(code)\n+\t\t\treturn { isValid: true }\n+\t\t} catch (error) {\n+\t\t\treturn {\n+\t\t\t\tisValid: false,\n+\t\t\t\terror: error instanceof Error ? error.message : i18next.t(\"common:mermaid.errors.unknown_syntax\"),\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Requests the LLM to fix the Mermaid syntax via the extension\n+\t */\n+\tprivate static requestLLMFix(\n+\t\tcode: string,\n+\t\terror: string,\n+\t): Promise<{ fixedCode: string } | { requestError: string }> {\n+\t\treturn new Promise((resolve, _reject) => {\n+\t\t\tconst requestId = `mermaid-fix-${Date.now()}-${Math.random().toString(36).substring(2, 11)}`\n+\n+\t\t\tconst timeout = setTimeout(() => {\n+\t\t\t\tcleanup()\n+\t\t\t\tresolve({ requestError: i18next.t(\"common:mermaid.errors.fix_timeout\") })\n+\t\t\t}, this.FIX_TIMEOUT)\n+\n+\t\t\tconst messageListener = (event: MessageEvent) => {\n+\t\t\t\tconst message = event.data\n+\t\t\t\tif (message.type === \"mermaidFixResponse\" && message.requestId === requestId) {\n+\t\t\t\t\tcleanup()\n+\n+\t\t\t\t\tif (message.success) {\n+\t\t\t\t\t\tresolve({ fixedCode: message.fixedCode })\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tresolve({\n+\t\t\t\t\t\t\trequestError: message.error || i18next.t(\"common:mermaid.errors.fix_request_failed\"),\n+\t\t\t\t\t\t})\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tconst cleanup = () => {\n+\t\t\t\tclearTimeout(timeout)\n+\t\t\t\twindow.removeEventListener(\"message\", messageListener)\n+\t\t\t}\n+\n+\t\t\twindow.addEventListener(\"message\", messageListener)\n+\n+\t\t\tvscode.postMessage({\n+\t\t\t\ttype: \"fixMermaidSyntax\",\n+\t\t\t\trequestId,\n+\t\t\t\ttext: code,\n+\t\t\t\tvalues: { error },\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t/**\n+\t * Attempts to fix Mermaid syntax with automatic retry and fallback\n+\t * Always returns the best attempt at fixing the code, even if not completely successful\n+\t */\n+\tstatic async autoFixSyntax(code: string): Promise<MermaidFixResult> {\n+\t\tlet currentCode = code\n+\t\tlet llmAttempts = 0\n+\t\tlet finalError: string | undefined\n+\n+\t\twhile (true) {\n+\t\t\tconsole.info(\"attempt \", llmAttempts)",
        "comment_created_at": "2025-07-01T15:13:12+00:00",
        "comment_author": "daniel-lxs",
        "comment_body": "We should clean this up before merging.\r\n\r\nThe `console.info` statements currently in the code (including the one at line 144 where `finalError` is logged) should be removed. If logging is still needed, consider using a proper logging service instead of `console` calls.",
        "pr_file_module": null
      },
      {
        "comment_id": "2179573006",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5304,
        "pr_file": "webview-ui/src/services/mermaidSyntaxFixer.ts",
        "discussion_id": "2177869237",
        "commented_code": "@@ -0,0 +1,154 @@\n+import { vscode } from \"@src/utils/vscode\"\n+import i18next from \"i18next\"\n+\n+export interface MermaidFixResult {\n+\tsuccess: boolean\n+\tfixedCode?: string\n+\terror?: string\n+\tattempts?: number\n+}\n+\n+export interface MermaidValidationResult {\n+\tisValid: boolean\n+\terror?: string\n+}\n+\n+/**\n+ * Service for validating and fixing Mermaid syntax using LLM assistance\n+ */\n+export class MermaidSyntaxFixer {\n+\tprivate static readonly MAX_FIX_ATTEMPTS = 2\n+\tprivate static readonly FIX_TIMEOUT = 30000 // 30 seconds\n+\n+\t/**\n+\t * Applies deterministic fixes for common LLM errors before validation\n+\t */\n+\tstatic applyDeterministicFixes(code: string): string {\n+\t\t// Fix HTML entity encoding: --&gt; should be -->;\n+\t\t// surprisingly, this does most of the heavy lifting in the MermaidSyntaxFixer\n+\t\t// sometimes the llm prepends ```mermaid, remove that\n+\t\treturn code.replace(/--&gt;/g, \"-->\").replace(/```mermaid/, \"\")\n+\t}\n+\n+\t/**\n+\t * Validates Mermaid syntax using the mermaid library\n+\t */\n+\tstatic async validateSyntax(code: string): Promise<MermaidValidationResult> {\n+\t\ttry {\n+\t\t\tconst mermaid = (await import(\"mermaid\")).default\n+\t\t\tawait mermaid.parse(code)\n+\t\t\treturn { isValid: true }\n+\t\t} catch (error) {\n+\t\t\treturn {\n+\t\t\t\tisValid: false,\n+\t\t\t\terror: error instanceof Error ? error.message : i18next.t(\"common:mermaid.errors.unknown_syntax\"),\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Requests the LLM to fix the Mermaid syntax via the extension\n+\t */\n+\tprivate static requestLLMFix(\n+\t\tcode: string,\n+\t\terror: string,\n+\t): Promise<{ fixedCode: string } | { requestError: string }> {\n+\t\treturn new Promise((resolve, _reject) => {\n+\t\t\tconst requestId = `mermaid-fix-${Date.now()}-${Math.random().toString(36).substring(2, 11)}`\n+\n+\t\t\tconst timeout = setTimeout(() => {\n+\t\t\t\tcleanup()\n+\t\t\t\tresolve({ requestError: i18next.t(\"common:mermaid.errors.fix_timeout\") })\n+\t\t\t}, this.FIX_TIMEOUT)\n+\n+\t\t\tconst messageListener = (event: MessageEvent) => {\n+\t\t\t\tconst message = event.data\n+\t\t\t\tif (message.type === \"mermaidFixResponse\" && message.requestId === requestId) {\n+\t\t\t\t\tcleanup()\n+\n+\t\t\t\t\tif (message.success) {\n+\t\t\t\t\t\tresolve({ fixedCode: message.fixedCode })\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tresolve({\n+\t\t\t\t\t\t\trequestError: message.error || i18next.t(\"common:mermaid.errors.fix_request_failed\"),\n+\t\t\t\t\t\t})\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tconst cleanup = () => {\n+\t\t\t\tclearTimeout(timeout)\n+\t\t\t\twindow.removeEventListener(\"message\", messageListener)\n+\t\t\t}\n+\n+\t\t\twindow.addEventListener(\"message\", messageListener)\n+\n+\t\t\tvscode.postMessage({\n+\t\t\t\ttype: \"fixMermaidSyntax\",\n+\t\t\t\trequestId,\n+\t\t\t\ttext: code,\n+\t\t\t\tvalues: { error },\n+\t\t\t})\n+\t\t})\n+\t}\n+\n+\t/**\n+\t * Attempts to fix Mermaid syntax with automatic retry and fallback\n+\t * Always returns the best attempt at fixing the code, even if not completely successful\n+\t */\n+\tstatic async autoFixSyntax(code: string): Promise<MermaidFixResult> {\n+\t\tlet currentCode = code\n+\t\tlet llmAttempts = 0\n+\t\tlet finalError: string | undefined\n+\n+\t\twhile (true) {\n+\t\t\tconsole.info(\"attempt \", llmAttempts)",
        "comment_created_at": "2025-07-02T09:25:10+00:00",
        "comment_author": "markijbema",
        "comment_body": "Oh damn, that's sloppy, apologies. Should I also remove the console.warn/error in mermaidblock? (they were already there, but I do not think they're necessary either)",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2133942365",
    "pr_number": 4314,
    "pr_file": "src/api/providers/fetchers/ollama.ts",
    "created_at": "2025-06-07T15:33:56+00:00",
    "commented_code": "+import axios from \"axios\"\n+import { ModelInfo, ollamaDefaultModelInfo } from \"@roo-code/types\"\n+import { z } from \"zod\"\n+\n+const OllamaModelDetailsSchema = z.object({\n+\tfamily: z.string(),\n+\tfamilies: z.array(z.string()),\n+\tformat: z.string(),\n+\tparameter_size: z.string(),\n+\tparent_model: z.string(),\n+\tquantization_level: z.string(),\n+})\n+\n+const OllamaModelSchema = z.object({\n+\tdetails: OllamaModelDetailsSchema,\n+\tdigest: z.string(),\n+\tmodel: z.string(),\n+\tmodified_at: z.string(),\n+\tname: z.string(),\n+\tsize: z.number(),\n+})\n+\n+const OllamaModelInfoResponseSchema = z.object({\n+\tmodelfile: z.string(),\n+\tparameters: z.string(),\n+\ttemplate: z.string(),\n+\tdetails: OllamaModelDetailsSchema,\n+\tmodel_info: z.record(z.string(), z.any()),\n+\tcapabilities: z.array(z.string()).optional(),\n+})\n+\n+const OllamaModelsResponseSchema = z.object({\n+\tmodels: z.array(OllamaModelSchema),\n+})\n+\n+type OllamaModelsResponse = z.infer<typeof OllamaModelsResponseSchema>\n+\n+type OllamaModelInfoResponse = z.infer<typeof OllamaModelInfoResponseSchema>\n+\n+export const parseOllamaModel = (rawModel: OllamaModelInfoResponse): ModelInfo => {\n+\tconst contextKey = Object.keys(rawModel.model_info).find((k) => k.includes(\"context_length\"))\n+\tconst contextWindow = contextKey ? rawModel.model_info[contextKey] : undefined\n+\n+\tconst modelInfo: ModelInfo = Object.assign({}, ollamaDefaultModelInfo, {\n+\t\tdescription: `Family: ${rawModel.details.family}, Context: ${contextWindow}, Size: ${rawModel.details.parameter_size}`,\n+\t\tcontextWindow: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t\tsupportsPromptCache: true,\n+\t\tsupportsImages: rawModel.capabilities?.includes(\"vision\"),\n+\t\tsupportsComputerUse: false,\n+\t\tmaxTokens: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t})\n+\n+\treturn modelInfo\n+}\n+\n+export async function getOllamaModels(baseUrl = \"http://localhost:11434\"): Promise<Record<string, ModelInfo>> {\n+\tconst models: Record<string, ModelInfo> = {}\n+\n+\t// clearing the input can leave an empty string; use the default in that case\n+\tbaseUrl = baseUrl === \"\" ? \"http://localhost:11434\" : baseUrl\n+\n+\ttry {\n+\t\tif (!URL.canParse(baseUrl)) {\n+\t\t\treturn models\n+\t\t}\n+\n+\t\tconst response = await axios.get<OllamaModelsResponse>(`${baseUrl}/api/tags`)\n+\t\tconst parsedResponse = OllamaModelsResponseSchema.safeParse(response.data)\n+\t\tlet modelInfoPromises = []\n+\n+\t\tif (parsedResponse.success) {\n+\t\t\tfor (const ollamaModel of parsedResponse.data.models) {\n+\t\t\t\tmodelInfoPromises.push(\n+\t\t\t\t\taxios\n+\t\t\t\t\t\t.post<OllamaModelInfoResponse>(`${baseUrl}/api/show`, {\n+\t\t\t\t\t\t\tmodel: ollamaModel.model,\n+\t\t\t\t\t\t})\n+\t\t\t\t\t\t.then((ollamaModelInfo) => {\n+\t\t\t\t\t\t\tmodels[ollamaModel.name] = parseOllamaModel(ollamaModelInfo.data)\n+\t\t\t\t\t\t}),\n+\t\t\t\t)\n+\t\t\t}\n+\n+\t\t\tawait Promise.all(modelInfoPromises)\n+\t\t} else {\n+\t\t\tconsole.error(`Error parsing Ollama models response: ${JSON.stringify(parsedResponse.error, null, 2)}`)\n+\t\t}\n+\t} catch (error) {\n+\t\tif (error.code === \"ECONNREFUSED\") {\n+\t\t\tconsole.info(`Failed connecting to Ollama at ${baseUrl}`)",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2133942365",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 4314,
        "pr_file": "src/api/providers/fetchers/ollama.ts",
        "discussion_id": "2133942365",
        "commented_code": "@@ -0,0 +1,97 @@\n+import axios from \"axios\"\n+import { ModelInfo, ollamaDefaultModelInfo } from \"@roo-code/types\"\n+import { z } from \"zod\"\n+\n+const OllamaModelDetailsSchema = z.object({\n+\tfamily: z.string(),\n+\tfamilies: z.array(z.string()),\n+\tformat: z.string(),\n+\tparameter_size: z.string(),\n+\tparent_model: z.string(),\n+\tquantization_level: z.string(),\n+})\n+\n+const OllamaModelSchema = z.object({\n+\tdetails: OllamaModelDetailsSchema,\n+\tdigest: z.string(),\n+\tmodel: z.string(),\n+\tmodified_at: z.string(),\n+\tname: z.string(),\n+\tsize: z.number(),\n+})\n+\n+const OllamaModelInfoResponseSchema = z.object({\n+\tmodelfile: z.string(),\n+\tparameters: z.string(),\n+\ttemplate: z.string(),\n+\tdetails: OllamaModelDetailsSchema,\n+\tmodel_info: z.record(z.string(), z.any()),\n+\tcapabilities: z.array(z.string()).optional(),\n+})\n+\n+const OllamaModelsResponseSchema = z.object({\n+\tmodels: z.array(OllamaModelSchema),\n+})\n+\n+type OllamaModelsResponse = z.infer<typeof OllamaModelsResponseSchema>\n+\n+type OllamaModelInfoResponse = z.infer<typeof OllamaModelInfoResponseSchema>\n+\n+export const parseOllamaModel = (rawModel: OllamaModelInfoResponse): ModelInfo => {\n+\tconst contextKey = Object.keys(rawModel.model_info).find((k) => k.includes(\"context_length\"))\n+\tconst contextWindow = contextKey ? rawModel.model_info[contextKey] : undefined\n+\n+\tconst modelInfo: ModelInfo = Object.assign({}, ollamaDefaultModelInfo, {\n+\t\tdescription: `Family: ${rawModel.details.family}, Context: ${contextWindow}, Size: ${rawModel.details.parameter_size}`,\n+\t\tcontextWindow: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t\tsupportsPromptCache: true,\n+\t\tsupportsImages: rawModel.capabilities?.includes(\"vision\"),\n+\t\tsupportsComputerUse: false,\n+\t\tmaxTokens: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t})\n+\n+\treturn modelInfo\n+}\n+\n+export async function getOllamaModels(baseUrl = \"http://localhost:11434\"): Promise<Record<string, ModelInfo>> {\n+\tconst models: Record<string, ModelInfo> = {}\n+\n+\t// clearing the input can leave an empty string; use the default in that case\n+\tbaseUrl = baseUrl === \"\" ? \"http://localhost:11434\" : baseUrl\n+\n+\ttry {\n+\t\tif (!URL.canParse(baseUrl)) {\n+\t\t\treturn models\n+\t\t}\n+\n+\t\tconst response = await axios.get<OllamaModelsResponse>(`${baseUrl}/api/tags`)\n+\t\tconst parsedResponse = OllamaModelsResponseSchema.safeParse(response.data)\n+\t\tlet modelInfoPromises = []\n+\n+\t\tif (parsedResponse.success) {\n+\t\t\tfor (const ollamaModel of parsedResponse.data.models) {\n+\t\t\t\tmodelInfoPromises.push(\n+\t\t\t\t\taxios\n+\t\t\t\t\t\t.post<OllamaModelInfoResponse>(`${baseUrl}/api/show`, {\n+\t\t\t\t\t\t\tmodel: ollamaModel.model,\n+\t\t\t\t\t\t})\n+\t\t\t\t\t\t.then((ollamaModelInfo) => {\n+\t\t\t\t\t\t\tmodels[ollamaModel.name] = parseOllamaModel(ollamaModelInfo.data)\n+\t\t\t\t\t\t}),\n+\t\t\t\t)\n+\t\t\t}\n+\n+\t\t\tawait Promise.all(modelInfoPromises)\n+\t\t} else {\n+\t\t\tconsole.error(`Error parsing Ollama models response: ${JSON.stringify(parsedResponse.error, null, 2)}`)\n+\t\t}\n+\t} catch (error) {\n+\t\tif (error.code === \"ECONNREFUSED\") {\n+\t\t\tconsole.info(`Failed connecting to Ollama at ${baseUrl}`)",
        "comment_created_at": "2025-06-07T15:33:56+00:00",
        "comment_author": "daniel-lxs",
        "comment_body": "I notice the error logging here uses different levels - `console.info` for connection failures (line 90) vs `console.warn` for other errors (line 92). In contrast, the LM Studio fetcher uses `console.error` for connection failures. Would it make sense to standardize the logging approach across both fetchers?",
        "pr_file_module": null
      },
      {
        "comment_id": "2135040219",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 4314,
        "pr_file": "src/api/providers/fetchers/ollama.ts",
        "discussion_id": "2133942365",
        "commented_code": "@@ -0,0 +1,97 @@\n+import axios from \"axios\"\n+import { ModelInfo, ollamaDefaultModelInfo } from \"@roo-code/types\"\n+import { z } from \"zod\"\n+\n+const OllamaModelDetailsSchema = z.object({\n+\tfamily: z.string(),\n+\tfamilies: z.array(z.string()),\n+\tformat: z.string(),\n+\tparameter_size: z.string(),\n+\tparent_model: z.string(),\n+\tquantization_level: z.string(),\n+})\n+\n+const OllamaModelSchema = z.object({\n+\tdetails: OllamaModelDetailsSchema,\n+\tdigest: z.string(),\n+\tmodel: z.string(),\n+\tmodified_at: z.string(),\n+\tname: z.string(),\n+\tsize: z.number(),\n+})\n+\n+const OllamaModelInfoResponseSchema = z.object({\n+\tmodelfile: z.string(),\n+\tparameters: z.string(),\n+\ttemplate: z.string(),\n+\tdetails: OllamaModelDetailsSchema,\n+\tmodel_info: z.record(z.string(), z.any()),\n+\tcapabilities: z.array(z.string()).optional(),\n+})\n+\n+const OllamaModelsResponseSchema = z.object({\n+\tmodels: z.array(OllamaModelSchema),\n+})\n+\n+type OllamaModelsResponse = z.infer<typeof OllamaModelsResponseSchema>\n+\n+type OllamaModelInfoResponse = z.infer<typeof OllamaModelInfoResponseSchema>\n+\n+export const parseOllamaModel = (rawModel: OllamaModelInfoResponse): ModelInfo => {\n+\tconst contextKey = Object.keys(rawModel.model_info).find((k) => k.includes(\"context_length\"))\n+\tconst contextWindow = contextKey ? rawModel.model_info[contextKey] : undefined\n+\n+\tconst modelInfo: ModelInfo = Object.assign({}, ollamaDefaultModelInfo, {\n+\t\tdescription: `Family: ${rawModel.details.family}, Context: ${contextWindow}, Size: ${rawModel.details.parameter_size}`,\n+\t\tcontextWindow: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t\tsupportsPromptCache: true,\n+\t\tsupportsImages: rawModel.capabilities?.includes(\"vision\"),\n+\t\tsupportsComputerUse: false,\n+\t\tmaxTokens: contextWindow || ollamaDefaultModelInfo.contextWindow,\n+\t})\n+\n+\treturn modelInfo\n+}\n+\n+export async function getOllamaModels(baseUrl = \"http://localhost:11434\"): Promise<Record<string, ModelInfo>> {\n+\tconst models: Record<string, ModelInfo> = {}\n+\n+\t// clearing the input can leave an empty string; use the default in that case\n+\tbaseUrl = baseUrl === \"\" ? \"http://localhost:11434\" : baseUrl\n+\n+\ttry {\n+\t\tif (!URL.canParse(baseUrl)) {\n+\t\t\treturn models\n+\t\t}\n+\n+\t\tconst response = await axios.get<OllamaModelsResponse>(`${baseUrl}/api/tags`)\n+\t\tconst parsedResponse = OllamaModelsResponseSchema.safeParse(response.data)\n+\t\tlet modelInfoPromises = []\n+\n+\t\tif (parsedResponse.success) {\n+\t\t\tfor (const ollamaModel of parsedResponse.data.models) {\n+\t\t\t\tmodelInfoPromises.push(\n+\t\t\t\t\taxios\n+\t\t\t\t\t\t.post<OllamaModelInfoResponse>(`${baseUrl}/api/show`, {\n+\t\t\t\t\t\t\tmodel: ollamaModel.model,\n+\t\t\t\t\t\t})\n+\t\t\t\t\t\t.then((ollamaModelInfo) => {\n+\t\t\t\t\t\t\tmodels[ollamaModel.name] = parseOllamaModel(ollamaModelInfo.data)\n+\t\t\t\t\t\t}),\n+\t\t\t\t)\n+\t\t\t}\n+\n+\t\t\tawait Promise.all(modelInfoPromises)\n+\t\t} else {\n+\t\t\tconsole.error(`Error parsing Ollama models response: ${JSON.stringify(parsedResponse.error, null, 2)}`)\n+\t\t}\n+\t} catch (error) {\n+\t\tif (error.code === \"ECONNREFUSED\") {\n+\t\t\tconsole.info(`Failed connecting to Ollama at ${baseUrl}`)",
        "comment_created_at": "2025-06-09T05:11:49+00:00",
        "comment_author": "thecolorblue",
        "comment_body": "My thought was that `ECONNREFUSED` is most likely caused by ollama/lm studio not running. This is not really an error. \r\n\r\nThat being said, I think it makes more sense to use warn here. I'll update them both to be more consistent.",
        "pr_file_module": null
      }
    ]
  }
]