[
  {
    "discussion_id": "2186122992",
    "pr_number": 5172,
    "pr_file": "src/core/tools/readFileTool.ts",
    "created_at": "2025-07-04T21:24:06+00:00",
    "commented_code": "}\n \t\t}\n \n+\t\t// Combine all images: feedback images first, then file images\n+\t\tconst allImages = [...feedbackImages, ...fileImageUrls]\n+\n \t\t// Push the result with appropriate formatting\n-\t\tif (statusMessage) {\n-\t\t\tconst result = formatResponse.toolResult(statusMessage, feedbackImages)\n+\t\tif (statusMessage || allImages.length > 0) {\n+\t\t\t// Always use formatResponse.toolResult when we have a status message or images\n+\t\t\tconst result = formatResponse.toolResult(",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2186122992",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5172,
        "pr_file": "src/core/tools/readFileTool.ts",
        "discussion_id": "2186122992",
        "commented_code": "@@ -573,20 +699,35 @@ export async function readFileTool(\n \t\t\t}\n \t\t}\n \n+\t\t// Combine all images: feedback images first, then file images\n+\t\tconst allImages = [...feedbackImages, ...fileImageUrls]\n+\n \t\t// Push the result with appropriate formatting\n-\t\tif (statusMessage) {\n-\t\t\tconst result = formatResponse.toolResult(statusMessage, feedbackImages)\n+\t\tif (statusMessage || allImages.length > 0) {\n+\t\t\t// Always use formatResponse.toolResult when we have a status message or images\n+\t\t\tconst result = formatResponse.toolResult(",
        "comment_created_at": "2025-07-04T21:24:06+00:00",
        "comment_author": "daniel-lxs",
        "comment_body": "Is it intentional that we're returning image data without checking if the current AI model supports images? I noticed that `maybeRemoveImageBlocks` in `src/api/transform/image-cleaning.ts` checks `apiHandler.getModel().info.supportsImages` before processing images. Should we add a similar check here to prevent sending image data to models that can't process it?\n\nFor example, we could check the provider's capability before including images in the response:\n```typescript\nconst provider = await cline.providerRef.deref();\nconst supportsImages = provider?.apiHandler?.getModel()?.info?.supportsImages ?? false;\nconst imagesToInclude = supportsImages ? allImages : [];\n```",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2205952880",
    "pr_number": 5665,
    "pr_file": "src/shared/embeddingModels.ts",
    "created_at": "2025-07-14T22:53:16+00:00",
    "commented_code": "},\n \tgemini: {\n \t\t\"text-embedding-004\": { dimension: 768 },\n+\t\t// ADD: New model with a default dimension.\n+\t\t// The actual dimension will be passed from the configuration at runtime.\n+\t\t\"gemini-embedding-exp-03-07\": { dimension: 768 },",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2205952880",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5665,
        "pr_file": "src/shared/embeddingModels.ts",
        "discussion_id": "2205952880",
        "commented_code": "@@ -48,6 +48,9 @@ export const EMBEDDING_MODEL_PROFILES: EmbeddingModelProfiles = {\n \t},\n \tgemini: {\n \t\t\"text-embedding-004\": { dimension: 768 },\n+\t\t// ADD: New model with a default dimension.\n+\t\t// The actual dimension will be passed from the configuration at runtime.\n+\t\t\"gemini-embedding-exp-03-07\": { dimension: 768 },",
        "comment_created_at": "2025-07-14T22:53:16+00:00",
        "comment_author": "daniel-lxs",
        "comment_body": "The default dimension is set to 768 for `gemini-embedding-exp-03-07`, but this model supports dimensions of 3072, 1536, and 768. Is 768 the intended default? The smaller dimension might impact embedding quality.\n\nAlso, would it be helpful to add validation somewhere to ensure only valid dimensions (3072, 1536, or 768) are accepted for this model?",
        "pr_file_module": null
      },
      {
        "comment_id": "2207309915",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 5665,
        "pr_file": "src/shared/embeddingModels.ts",
        "discussion_id": "2205952880",
        "commented_code": "@@ -48,6 +48,9 @@ export const EMBEDDING_MODEL_PROFILES: EmbeddingModelProfiles = {\n \t},\n \tgemini: {\n \t\t\"text-embedding-004\": { dimension: 768 },\n+\t\t// ADD: New model with a default dimension.\n+\t\t// The actual dimension will be passed from the configuration at runtime.\n+\t\t\"gemini-embedding-exp-03-07\": { dimension: 768 },",
        "comment_created_at": "2025-07-15T12:10:27+00:00",
        "comment_author": "Jopo-JP",
        "comment_body": "Hi @daniel-lxs,\r\n\r\nThanks for the feedback. I've addressed your suggestions.\r\n\r\nRegarding the model dimensions, you're right that the initial implementation was just a placeholder. I've now implemented a more robust solution.\r\n\r\n*   I removed the `gemini-embedding-exp-03-07` profile, as the new `gemini-embedding-001` now handles variable dimensions and was a experimental model. You already did the groundwork.\r\n*   I've added a slider to the UI, allowing the user to flexibly choose the embedding dimension. This value is saved and passed correctly to Qdrant.\r\n\r\nHere is a look at the new UI:\r\n<img width=\"353\" height=\"689\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d7a4ac0c-df5e-43a5-9edc-64a8ed718144\" />\r\n\r\nThis approach provides the flexibility and validation you suggested. The user can now select any dimension supported by the model, and the system will use it.",
        "pr_file_module": null
      }
    ]
  },
  {
    "discussion_id": "2154915734",
    "pr_number": 4314,
    "pr_file": "webview-ui/src/components/ui/hooks/useSelectedModel.ts",
    "created_at": "2025-06-18T15:28:57+00:00",
    "commented_code": "}\n \t\tcase \"ollama\": {\n \t\t\tconst id = apiConfiguration.ollamaModelId ?? \"\"\n-\t\t\tconst info = openAiModelInfoSaneDefaults\n-\t\t\treturn { id, info }\n+\t\t\tconst info = routerModels.ollama && routerModels.ollama[id]\n+\t\t\treturn {\n+\t\t\t\tid,\n+\t\t\t\tinfo: info ? info : ollamaDefaultModelInfo,\n+\t\t\t}",
    "repo_full_name": "RooCodeInc/Roo-Code",
    "discussion_comments": [
      {
        "comment_id": "2154915734",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 4314,
        "pr_file": "webview-ui/src/components/ui/hooks/useSelectedModel.ts",
        "discussion_id": "2154915734",
        "commented_code": "@@ -177,13 +179,19 @@ function getSelectedModel({\n \t\t}\n \t\tcase \"ollama\": {\n \t\t\tconst id = apiConfiguration.ollamaModelId ?? \"\"\n-\t\t\tconst info = openAiModelInfoSaneDefaults\n-\t\t\treturn { id, info }\n+\t\t\tconst info = routerModels.ollama && routerModels.ollama[id]\n+\t\t\treturn {\n+\t\t\t\tid,\n+\t\t\t\tinfo: info ? info : ollamaDefaultModelInfo,\n+\t\t\t}",
        "comment_created_at": "2025-06-18T15:28:57+00:00",
        "comment_author": "hannesrudolph",
        "comment_body": "I noticed that the fallback to default model info here could potentially mask configuration issues. When a user selects a model that doesn't exist locally (which is common with Ollama/LM Studio), returning the default model info might make it appear as if everything is working when it's not.\n\nWould it make sense to either:\n1. Return `undefined` for the info when the model isn't found (similar to other providers)\n2. Add a visual indicator in the UI when using fallback model info\n3. Include a flag in the returned object to indicate when fallback is being used?\n\nThis would help users understand when their selected model isn't actually available.",
        "pr_file_module": null
      },
      {
        "comment_id": "2156895451",
        "repo_full_name": "RooCodeInc/Roo-Code",
        "pr_number": 4314,
        "pr_file": "webview-ui/src/components/ui/hooks/useSelectedModel.ts",
        "discussion_id": "2154915734",
        "commented_code": "@@ -177,13 +179,19 @@ function getSelectedModel({\n \t\t}\n \t\tcase \"ollama\": {\n \t\t\tconst id = apiConfiguration.ollamaModelId ?? \"\"\n-\t\t\tconst info = openAiModelInfoSaneDefaults\n-\t\t\treturn { id, info }\n+\t\t\tconst info = routerModels.ollama && routerModels.ollama[id]\n+\t\t\treturn {\n+\t\t\t\tid,\n+\t\t\t\tinfo: info ? info : ollamaDefaultModelInfo,\n+\t\t\t}",
        "comment_created_at": "2025-06-19T12:24:43+00:00",
        "comment_author": "thecolorblue",
        "comment_body": "I have done a couple iterations on this code. I wanted to keep these providers working as similar to other providers as possible. Returning `undefined` causes other errors from my testing. I wanted to keep UI changes out of this PR to limit its size. \r\n\r\nI have a couple UI updates that I can put into a issue for another PR. Other vs code plugins that work with ollama throw a notification when there is an error connecting to ollama or a model is in correct. More connection information about ollama and lm studio would be useful in the settings pages as well. \r\n\r\nThe flag is a good idea. Is there an example of a flag being used in the code already?",
        "pr_file_module": null
      }
    ]
  }
]